<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>NeO 360&#26159;&#19968;&#31181;&#29992;&#20110;&#31232;&#30095;&#35270;&#35282;&#21512;&#25104;&#25143;&#22806;&#22330;&#26223;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#33719;&#22797;&#26434;&#30340;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20998;&#24067;&#65292;&#20351;&#29992;&#28151;&#21512;&#30340;&#22270;&#20687;&#26465;&#20214;&#24335;&#19977;&#24179;&#38754;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#20174;&#21333;&#20010;&#25110;&#23569;&#25968;&#20960;&#20010;&#22270;&#20687;&#37325;&#24314;360&#24230;&#22330;&#26223;&#65292;&#24182;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#21644;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.12967</link><description>&lt;p&gt;
NeO 360: &#29992;&#20110;&#31232;&#30095;&#35270;&#35282;&#21512;&#25104;&#25143;&#22806;&#22330;&#26223;&#30340;&#31070;&#32463;&#22330;
&lt;/p&gt;
&lt;p&gt;
NeO 360: Neural Fields for Sparse View Synthesis of Outdoor Scenes. (arXiv:2308.12967v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12967
&lt;/p&gt;
&lt;p&gt;
NeO 360&#26159;&#19968;&#31181;&#29992;&#20110;&#31232;&#30095;&#35270;&#35282;&#21512;&#25104;&#25143;&#22806;&#22330;&#26223;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#33719;&#22797;&#26434;&#30340;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20998;&#24067;&#65292;&#20351;&#29992;&#28151;&#21512;&#30340;&#22270;&#20687;&#26465;&#20214;&#24335;&#19977;&#24179;&#38754;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#20174;&#21333;&#20010;&#25110;&#23569;&#25968;&#20960;&#20010;&#22270;&#20687;&#37325;&#24314;360&#24230;&#22330;&#26223;&#65292;&#24182;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#21644;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#26041;&#27861;&#22312;&#26032;&#35270;&#35282;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#22312;&#35768;&#22810;&#35270;&#35282;&#19979;&#36827;&#34892;&#26114;&#36149;&#30340;&#22330;&#26223;&#20248;&#21270;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#26080;&#38480;&#21046;&#30340;&#22478;&#24066;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#24863;&#20852;&#36259;&#30340;&#23545;&#35937;&#25110;&#32972;&#26223;&#21482;&#20174;&#24456;&#23569;&#30340;&#35270;&#35282;&#35266;&#23519;&#21040;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;NeO 360&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#31232;&#30095;&#35270;&#35282;&#21512;&#25104;&#25143;&#22806;&#22330;&#26223;&#12290;NeO 360&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#21333;&#20010;&#25110;&#23569;&#25968;&#20960;&#20010;&#20301;&#32622;&#30340;RGB&#22270;&#20687;&#37325;&#24314;360&#24230;&#22330;&#26223;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#22312;&#20110;&#25429;&#33719;&#22797;&#26434;&#30340;&#30495;&#23454;&#19990;&#30028;&#25143;&#22806;&#19977;&#32500;&#22330;&#26223;&#30340;&#20998;&#24067;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#28151;&#21512;&#30340;&#22270;&#20687;&#26465;&#20214;&#24335;&#19977;&#24179;&#38754;&#34920;&#31034;&#65292;&#21487;&#20197;&#20174;&#20219;&#20309;&#19990;&#30028;&#28857;&#26597;&#35810;&#12290;&#25105;&#20204;&#30340;&#34920;&#31034;&#26041;&#27861;&#32467;&#21512;&#20102;&#20307;&#32032;&#21644;&#40479;&#30640;&#22270;&#34920;&#31034;&#30340;&#20248;&#28857;&#65292;&#27604;&#27599;&#31181;&#34920;&#31034;&#26041;&#27861;&#26356;&#26377;&#25928;&#21644;&#34920;&#36798;&#33021;&#21147;&#26356;&#24378;&#12290;NeO 360&#30340;&#34920;&#31034;&#26041;&#27861;&#20801;&#35768;&#25105;&#20204;&#20174;&#19968;&#20010;&#24222;&#22823;&#30340;&#26080;&#38480;&#21046;&#30340;&#19977;&#32500;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent implicit neural representations have shown great results for novel view synthesis. However, existing methods require expensive per-scene optimization from many views hence limiting their application to real-world unbounded urban settings where the objects of interest or backgrounds are observed from very few views. To mitigate this challenge, we introduce a new approach called NeO 360, Neural fields for sparse view synthesis of outdoor scenes. NeO 360 is a generalizable method that reconstructs 360{\deg} scenes from a single or a few posed RGB images. The essence of our approach is in capturing the distribution of complex real-world outdoor 3D scenes and using a hybrid image-conditional triplanar representation that can be queried from any world point. Our representation combines the best of both voxel-based and bird's-eye-view (BEV) representations and is more effective and expressive than each. NeO 360's representation allows us to learn from a large collection of unbounded 3D
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DLIP&#65292;&#25552;&#21462;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#27169;&#22359;&#30340;&#26550;&#26500;&#29305;&#24615;&#21644;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#20256;&#36882;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;&#22914;&#20309;&#25552;&#21462;&#36731;&#37327;&#32423;&#20294;&#24615;&#33021;&#20248;&#36234;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;DLIP&#33021;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2308.12956</link><description>&lt;p&gt;
DLIP: &#25552;&#21462;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DLIP: Distilling Language-Image Pre-training. (arXiv:2308.12956v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DLIP&#65292;&#25552;&#21462;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#27169;&#22359;&#30340;&#26550;&#26500;&#29305;&#24615;&#21644;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#20256;&#36882;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;&#22914;&#20309;&#25552;&#21462;&#36731;&#37327;&#32423;&#20294;&#24615;&#33021;&#20248;&#36234;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;DLIP&#33021;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451; (VLP) &#22312;&#26497;&#37325;&#30340;&#21442;&#25968;&#36741;&#21161;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#36825;&#20063;&#32473;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#30693;&#35782;&#25552;&#21462;&#20316;&#20026;&#27169;&#22411;&#21387;&#32553;&#20013;&#30340;&#37325;&#35201;&#36807;&#31243;&#34987;&#24191;&#27867;&#35748;&#21487;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30693;&#35782;&#25552;&#21462;&#25216;&#26415;&#23545;&#20110;VLP&#30340;&#28145;&#20837;&#35843;&#26597;&#21644;&#20998;&#26512;&#36824;&#19981;&#22815;&#65292;&#24182;&#19988;VLP&#23548;&#21521;&#30340;&#25552;&#21462;&#23454;&#36341;&#25351;&#21335;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DLIP&#65292;&#19968;&#20010;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#25552;&#21462;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#23427;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#25552;&#21462;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;VLP&#27169;&#22411;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20174;&#22810;&#20010;&#32500;&#24230;&#21078;&#26512;&#20102;&#27169;&#22411;&#25552;&#21462;&#65292;&#22914;&#19981;&#21516;&#27169;&#22359;&#30340;&#26550;&#26500;&#29305;&#24615;&#21644;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#20256;&#36882;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#24182;&#23545;&#22914;&#20309;&#25552;&#21462;&#36731;&#37327;&#32423;&#20294;&#24615;&#33021;&#20248;&#36234;&#30340;VLP&#27169;&#22411;&#25552;&#20379;&#20102;&#28145;&#21051;&#30340;&#35265;&#35299;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DLIP&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#30340;&#26435;&#34913;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Pre-training (VLP) shows remarkable progress with the assistance of extremely heavy parameters, which challenges deployment in real applications. Knowledge distillation is well recognized as the essential procedure in model compression. However, existing knowledge distillation techniques lack an in-depth investigation and analysis of VLP, and practical guidelines for VLP-oriented distillation are still not yet explored. In this paper, we present DLIP, a simple yet efficient Distilling Language-Image Pre-training framework, through which we investigate how to distill a light VLP model. Specifically, we dissect the model distillation from multiple dimensions, such as the architecture characteristics of different modules and the information transfer of different modalities. We conduct comprehensive experiments and provide insights on distilling a light but performant VLP model. Experimental results reveal that DLIP can achieve a state-of-the-art accuracy/efficiency trade-o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#20302;&#35745;&#25968;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#29983;&#25104;&#36807;&#31243;&#26469;&#21019;&#24314;&#21253;&#21547;&#24322;&#24120;&#29255;&#27573;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#35299;&#37322;&#20102;&#24120;&#29992;&#31639;&#27861;&#22312;&#27491;&#24120;&#21644;&#24322;&#24120;&#29255;&#27573;&#20043;&#38388;&#30340;&#20998;&#24067;&#37325;&#21472;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.12925</link><description>&lt;p&gt;
&#20302;&#35745;&#25968;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Low-count Time Series Anomaly Detection. (arXiv:2308.12925v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#20302;&#35745;&#25968;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#29983;&#25104;&#36807;&#31243;&#26469;&#21019;&#24314;&#21253;&#21547;&#24322;&#24120;&#29255;&#27573;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#35299;&#37322;&#20102;&#24120;&#29992;&#31639;&#27861;&#22312;&#27491;&#24120;&#21644;&#24322;&#24120;&#29255;&#27573;&#20043;&#38388;&#30340;&#20998;&#24067;&#37325;&#21472;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#35745;&#25968;&#26102;&#38388;&#24207;&#21015;&#25551;&#36848;&#31232;&#30095;&#25110;&#38388;&#26029;&#20107;&#20214;&#65292;&#36825;&#22312;&#25429;&#33719;&#21644;&#30417;&#25511;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#30340;&#22823;&#35268;&#27169;&#22312;&#32447;&#24179;&#21488;&#20013;&#24456;&#24120;&#35265;&#12290;&#24314;&#27169;&#20302;&#35745;&#25968;&#26102;&#38388;&#24207;&#21015;&#38754;&#20020;&#20960;&#20010;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#20302;&#20449;&#22122;&#27604;&#65288;&#24403;&#24322;&#24120;&#31614;&#21517;&#26080;&#27861;&#26816;&#27979;&#26102;&#65289;&#21644;&#38750;&#22343;&#21248;&#24615;&#33021;&#65288;&#24179;&#22343;&#24230;&#37327;&#25351;&#26631;&#19981;&#33021;&#20195;&#34920;&#23616;&#37096;&#34892;&#20026;&#65289;&#12290;&#24403;&#21069;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#39046;&#22495;&#32570;&#20047;&#26126;&#30830;&#30340;&#24037;&#20855;&#21644;&#27969;&#31243;&#26469;&#24314;&#27169;&#21644;&#21487;&#38752;&#22320;&#26816;&#27979;&#36825;&#20123;&#24773;&#20917;&#19979;&#30340;&#24322;&#24120;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#29992;&#20110;&#21019;&#24314;&#21253;&#21547;&#26377;&#24322;&#24120;&#29255;&#27573;&#30340;&#20302;&#35745;&#25968;&#26102;&#38388;&#24207;&#21015;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#30340;&#28151;&#21512;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#35299;&#37322;&#20102;&#24120;&#29992;&#31639;&#27861;&#22312;&#27491;&#24120;&#21644;&#24322;&#24120;&#29255;&#27573;&#20043;&#38388;&#30340;&#20998;&#24067;&#37325;&#21472;&#20013;&#36935;&#21040;&#30340;&#22256;&#38590;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20010;&#32570;&#28857;&#65292;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#30340;&#21457;&#29616;&#26469;&#23637;&#31034;&#22914;&#20309;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-count time series describe sparse or intermittent events, which are prevalent in large-scale online platforms that capture and monitor diverse data types. Several distinct challenges surface when modelling low-count time series, particularly low signal-to-noise ratios (when anomaly signatures are provably undetectable), and non-uniform performance (when average metrics are not representative of local behaviour). The time series anomaly detection community currently lacks explicit tooling and processes to model and reliably detect anomalies in these settings. We address this gap by introducing a novel generative procedure for creating benchmark datasets comprising of low-count time series with anomalous segments. Via a mixture of theoretical and empirical analysis, our work explains how widely-used algorithms struggle with the distribution overlap between normal and anomalous segments. In order to mitigate this shortcoming, we then leverage our findings to demonstrate how anomaly sc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#38754;&#23545;&#23545;&#25239;&#25915;&#20987;&#26102;&#30340;&#28431;&#27934;&#65292;&#24182;&#35752;&#35770;&#20102;&#28431;&#27934;&#21487;&#33021;&#30340;&#21407;&#22240;&#12289;&#23545;&#25239;&#25915;&#20987;&#19982;&#38543;&#26426;&#21270;&#31034;&#20363;&#30340;&#24046;&#24322;&#20197;&#21450;&#30456;&#20851;&#30340;&#36947;&#24503;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.12918</link><description>&lt;p&gt;
&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#23545;&#25239;&#25915;&#20987;&#26041;&#38754;&#30340;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Vulnerabilities in ML systems in terms of adversarial attacks. (arXiv:2308.12918v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#38754;&#23545;&#23545;&#25239;&#25915;&#20987;&#26102;&#30340;&#28431;&#27934;&#65292;&#24182;&#35752;&#35770;&#20102;&#28431;&#27934;&#21487;&#33021;&#30340;&#21407;&#22240;&#12289;&#23545;&#25239;&#25915;&#20987;&#19982;&#38543;&#26426;&#21270;&#31034;&#20363;&#30340;&#24046;&#24322;&#20197;&#21450;&#30456;&#20851;&#30340;&#36947;&#24503;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20986;&#29616;&#20102;&#19968;&#20123;&#38590;&#20197;&#21457;&#29616;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;&#36825;&#20123;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#21487;&#33021;&#23545;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#38450;&#24481;&#31995;&#32479;&#26500;&#25104;&#25361;&#25112;&#65292;&#24182;&#21487;&#33021;&#24433;&#21709;&#26410;&#26469;&#32593;&#32476;&#25915;&#20987;&#30340;&#38450;&#24481;&#12290;&#26412;&#30740;&#31350;&#35770;&#25991;&#20316;&#32773;&#19987;&#27880;&#20110;&#36825;&#20010;&#39046;&#22495;&#12290;&#20182;&#20204;&#25506;&#35752;&#20102;AI&#31995;&#32479;&#30340;&#28431;&#27934;&#24102;&#26469;&#30340;&#21518;&#26524;&#12290;&#36825;&#21253;&#25324;&#35752;&#35770;&#28431;&#27934;&#21487;&#33021;&#21457;&#29983;&#30340;&#21407;&#22240;&#65292;&#38543;&#26426;&#21270;&#21644;&#23545;&#25239;&#24615;&#31034;&#20363;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20197;&#21450;&#28431;&#27934;&#21487;&#33021;&#24341;&#21457;&#30340;&#36947;&#24503;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#24403;AI&#31995;&#32479;&#22788;&#20110;&#27979;&#35797;&#38454;&#27573;&#24182;&#20934;&#22791;&#36827;&#34892;&#26356;&#24191;&#27867;&#30340;&#20351;&#29992;&#26102;&#65292;&#36866;&#24403;&#36827;&#34892;&#38024;&#23545;&#24615;&#30340;&#35757;&#32451;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
There have been recent adversarial attacks that are difficult to find. These new adversarial attacks methods may pose challenges to current deep learning cyber defense systems and could influence the future defense of cyberattacks. The authors focus on this domain in this research paper. They explore the consequences of vulnerabilities in AI systems. This includes discussing how they might arise, differences between randomized and adversarial examples and also potential ethical implications of vulnerabilities. Moreover, it is important to train the AI systems appropriately when they are in testing phase and getting them ready for broader use.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#27454;&#20197;&#29983;&#25104;AI&#20026;&#22522;&#30784;&#30340;&#21512;&#20316;&#24615;&#25925;&#20107;&#28216;&#25103;&#12298;&#19968;&#21315;&#38646;&#19968;&#22812;&#12299;&#65292;&#29609;&#23478;&#36890;&#36807;&#19982;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#35282;&#33394;&#20849;&#21516;&#21019;&#20316;&#25925;&#20107;&#26469;&#24341;&#23548;&#28216;&#25103;&#20013;&#30340;&#29616;&#23454;&#65292;&#25361;&#25112;&#28216;&#25103;&#19990;&#30028;&#19982;&#29616;&#23454;&#20043;&#38388;&#30340;&#20256;&#32479;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2308.12915</link><description>&lt;p&gt;
&#20197;&#29983;&#25104;AI&#20026;&#22522;&#30784;&#30340;&#21512;&#20316;&#24615;&#25925;&#20107;&#28216;&#25103;&#20307;&#39564;&#65306;&#22312;&#12298;&#19968;&#21315;&#38646;&#19968;&#22812;&#12299;&#20013;&#30340;&#35821;&#35328;&#20316;&#20026;&#29616;&#23454;
&lt;/p&gt;
&lt;p&gt;
Language as Reality: A Co-Creative Storytelling Game Experience in 1001 Nights using Generative AI. (arXiv:2308.12915v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#27454;&#20197;&#29983;&#25104;AI&#20026;&#22522;&#30784;&#30340;&#21512;&#20316;&#24615;&#25925;&#20107;&#28216;&#25103;&#12298;&#19968;&#21315;&#38646;&#19968;&#22812;&#12299;&#65292;&#29609;&#23478;&#36890;&#36807;&#19982;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#35282;&#33394;&#20849;&#21516;&#21019;&#20316;&#25925;&#20107;&#26469;&#24341;&#23548;&#28216;&#25103;&#20013;&#30340;&#29616;&#23454;&#65292;&#25361;&#25112;&#28216;&#25103;&#19990;&#30028;&#19982;&#29616;&#23454;&#20043;&#38388;&#30340;&#20256;&#32479;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#27454;&#21517;&#20026;&#12298;&#19968;&#21315;&#38646;&#19968;&#22812;&#12299;&#30340;AI&#26412;&#22320;&#21270;&#28216;&#25103;&#65292;&#36890;&#36807;&#19982;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#35282;&#33394;&#20849;&#21516;&#21019;&#20316;&#25925;&#20107;&#65292;&#29609;&#23478;&#21487;&#20197;&#24341;&#23548;&#28216;&#25103;&#20013;&#30340;&#29616;&#23454;&#12290;&#35813;&#27010;&#24565;&#21463;&#21040;&#32500;&#29305;&#26681;&#26031;&#22374;&#20851;&#20110;&#35821;&#35328;&#30028;&#38480;&#20915;&#23450;&#19968;&#20010;&#20154;&#19990;&#30028;&#30028;&#38480;&#30340;&#24605;&#24819;&#30340;&#21551;&#21457;&#12290;&#20351;&#29992;GPT-4&#21644;&#31283;&#23450;&#25193;&#25955;&#31561;&#20808;&#36827;&#30340;AI&#24037;&#20855;&#65292;&#28216;&#25103;&#30340;&#31532;&#20108;&#27425;&#36845;&#20195;&#20351;&#24471;&#20027;&#35282;&#27801;&#36203;&#25289;&#33832;&#24503;&#33021;&#22815;&#22312;&#22905;&#30340;&#19990;&#30028;&#20013;&#23454;&#29616;&#25991;&#23383;&#21644;&#25925;&#20107;&#12290;&#29609;&#23478;&#21487;&#20197;&#19982;AI&#22269;&#29579;&#36827;&#34892;&#23545;&#35805;&#65292;&#24341;&#23548;&#23545;&#29305;&#23450;&#20851;&#38190;&#35789;&#30340;&#35752;&#35770;&#65292;&#36825;&#20123;&#20851;&#38190;&#35789;&#38543;&#21518;&#25104;&#20026;&#28216;&#25103;&#20013;&#30340;&#25112;&#26007;&#35013;&#22791;&#12290;&#36825;&#31181;&#20114;&#21160;&#21465;&#20107;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#36716;&#25442;&#30340;&#32467;&#21512;&#36890;&#36807;&#21452;&#37325;&#35270;&#35282;&#25361;&#25112;&#20102;&#28216;&#25103;&#19990;&#30028;&#19982;&#29616;&#23454;&#20043;&#38388;&#30340;&#20256;&#32479;&#36793;&#30028;&#12290;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#35797;&#22270;&#25913;&#21464;&#33258;&#24049;&#21629;&#36816;&#30340;&#27801;&#36203;&#25289;&#33832;&#24503;&#65292;&#20197;&#21450;&#19982;AI&#21512;&#20316;&#21019;&#20316;&#25925;&#20107;&#24182;&#22609;&#36896;&#28216;&#25103;&#19990;&#30028;&#30340;&#29609;&#23478;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#23454;&#29616;&#36825;&#19968;&#27010;&#24565;&#30340;&#25216;&#26415;&#21644;&#35774;&#35745;&#35201;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present "1001 Nights", an AI-native game that allows players lead in-game reality through co-created storytelling with the character driven by large language model. The concept is inspired by Wittgenstein's idea of the limits of one's world being determined by the bounds of their language. Using advanced AI tools like GPT-4 and Stable Diffusion, the second iteration of the game enables the protagonist, Shahrzad, to realize words and stories in her world. The player can steer the conversation with the AI King towards specific keywords, which then become battle equipment in the game. This blend of interactive narrative and text-to-image transformation challenges the conventional border between the game world and reality through a dual perspective. We focus on Shahrzad, who seeks to alter her fate compared to the original folklore, and the player, who collaborates with AI to craft narratives and shape the game world. We explore the technical and design elements of implem
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CDAN&#30340;&#21367;&#31215;&#31264;&#23494;&#27880;&#24847;&#21147;&#24341;&#23548;&#32593;&#32476;&#65292;&#29992;&#20110;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#12290;&#35813;&#32593;&#32476;&#32467;&#21512;&#20102;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#12289;&#21367;&#31215;&#21644;&#31264;&#23494;&#22359;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#36339;&#36291;&#36830;&#25509;&#65292;&#36890;&#36807;&#19987;&#38376;&#30340;&#21518;&#22788;&#29702;&#38454;&#27573;&#36827;&#19968;&#27493;&#25913;&#21892;&#33394;&#24425;&#24179;&#34913;&#21644;&#23545;&#27604;&#24230;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12902</link><description>&lt;p&gt;
CDAN: &#29992;&#20110;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#30340;&#21367;&#31215;&#31264;&#23494;&#27880;&#24847;&#21147;&#24341;&#23548;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
CDAN: Convolutional Dense Attention-guided Network for Low-light Image Enhancement. (arXiv:2308.12902v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CDAN&#30340;&#21367;&#31215;&#31264;&#23494;&#27880;&#24847;&#21147;&#24341;&#23548;&#32593;&#32476;&#65292;&#29992;&#20110;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#12290;&#35813;&#32593;&#32476;&#32467;&#21512;&#20102;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#12289;&#21367;&#31215;&#21644;&#31264;&#23494;&#22359;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#36339;&#36291;&#36830;&#25509;&#65292;&#36890;&#36807;&#19987;&#38376;&#30340;&#21518;&#22788;&#29702;&#38454;&#27573;&#36827;&#19968;&#27493;&#25913;&#21892;&#33394;&#24425;&#24179;&#34913;&#21644;&#23545;&#27604;&#24230;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#20809;&#22270;&#20687;&#20197;&#19981;&#36275;&#30340;&#29031;&#26126;&#20026;&#29305;&#24449;&#65292;&#38754;&#20020;&#28165;&#26224;&#24230;&#20943;&#24369;&#12289;&#39068;&#33394;&#26263;&#28129;&#21644;&#32454;&#33410;&#20943;&#23569;&#30340;&#25361;&#25112;&#12290;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#25913;&#21892;&#20142;&#24230;&#12289;&#23545;&#27604;&#24230;&#21644;&#25972;&#20307;&#24863;&#30693;&#36136;&#37327;&#26469;&#32416;&#27491;&#36825;&#20123;&#38382;&#39064;&#65292;&#20174;&#32780;&#20419;&#36827;&#20934;&#30830;&#30340;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65306;&#21367;&#31215;&#31264;&#23494;&#27880;&#24847;&#21147;&#24341;&#23548;&#32593;&#32476;&#65288;CDAN&#65289;&#65292;&#29992;&#20110;&#22686;&#24378;&#20302;&#20809;&#22270;&#20687;&#12290;CDAN&#23558;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#19982;&#21367;&#31215;&#21644;&#31264;&#23494;&#22359;&#30456;&#32467;&#21512;&#65292;&#37197;&#21512;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#36339;&#36291;&#36830;&#25509;&#12290;&#35813;&#26550;&#26500;&#30830;&#20445;&#20102;&#26377;&#25928;&#30340;&#20449;&#24687;&#20256;&#36882;&#21644;&#29305;&#24449;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#19987;&#38376;&#30340;&#21518;&#22788;&#29702;&#38454;&#27573;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;&#33394;&#24425;&#24179;&#34913;&#21644;&#23545;&#27604;&#24230;&#12290;&#19982;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#39046;&#22495;&#30340;&#26368;&#26032;&#25104;&#26524;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-light images, characterized by inadequate illumination, pose challenges of diminished clarity, muted colors, and reduced details. Low-light image enhancement, an essential task in computer vision, aims to rectify these issues by improving brightness, contrast, and overall perceptual quality, thereby facilitating accurate analysis and interpretation. This paper introduces the Convolutional Dense Attention-guided Network (CDAN), a novel solution for enhancing low-light images. CDAN integrates an autoencoder-based architecture with convolutional and dense blocks, complemented by an attention mechanism and skip connections. This architecture ensures efficient information propagation and feature learning. Furthermore, a dedicated post-processing phase refines color balance and contrast. Our approach demonstrates notable progress compared to state-of-the-art results in low-light image enhancement, showcasing its robustness across a wide range of challenging scenarios. Our model performs 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#30693;&#35782;&#22312;&#22810;&#27169;&#24577;&#23545;&#40784;&#20013;&#30340;&#20316;&#29992;&#65292;&#35774;&#35745;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#23545;&#40784;&#25506;&#27979;&#22522;&#20934;&#26469;&#26816;&#27979;&#20851;&#38190;&#30340;&#35821;&#35328;&#32452;&#25104;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2308.12898</link><description>&lt;p&gt;
&#35821;&#35328;&#30693;&#35782;&#33021;&#25913;&#36827;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Linguistic Knowledge Improve Multimodal Alignment in Vision-Language Pretraining?. (arXiv:2308.12898v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#30693;&#35782;&#22312;&#22810;&#27169;&#24577;&#23545;&#40784;&#20013;&#30340;&#20316;&#29992;&#65292;&#35774;&#35745;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#23545;&#40784;&#25506;&#27979;&#22522;&#20934;&#26469;&#26816;&#27979;&#20851;&#38190;&#30340;&#35821;&#35328;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23186;&#20307;&#39046;&#22495;&#23545;&#36890;&#36807;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24863;&#30693;&#21644;&#34920;&#36798;&#29289;&#29702;&#19990;&#30028;&#23637;&#29616;&#20986;&#20102;&#24378;&#28872;&#30340;&#20852;&#36259;&#65292;&#20854;&#20013;&#35270;&#35273;-&#35821;&#35328;&#30456;&#20851;&#30340;&#30740;&#31350;&#26159;&#24403;&#21069;&#26368;&#21560;&#24341;&#20154;&#30340;&#35805;&#39064;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#20197;&#19979;&#20004;&#20010;&#38382;&#39064;&#30340;&#25506;&#32034;&#38750;&#24120;&#26377;&#38480;&#65306;1&#65289;&#22312;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#26159;&#21542;&#21487;&#20197;&#25552;&#21462;&#20851;&#38190;&#30340;&#35821;&#35328;&#30693;&#35782;&#65288;&#22914;&#35821;&#20041;&#21644;&#21477;&#27861;&#65289;&#65292;2&#65289;&#36825;&#31181;&#35821;&#35328;&#30693;&#35782;&#22914;&#20309;&#24433;&#21709;&#25110;&#22686;&#24378;&#22810;&#27169;&#24577;&#23545;&#40784;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#38416;&#26126;&#20840;&#38754;&#30340;&#35821;&#35328;&#30693;&#35782;&#65292;&#21253;&#25324;&#35821;&#20041;&#34920;&#36798;&#21644;&#21477;&#27861;&#32467;&#26500;&#65292;&#23545;&#22810;&#27169;&#24577;&#23545;&#40784;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#21457;&#24067;&#20102;SNARE&#65292;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#25506;&#27979;&#22522;&#20934;&#65292;&#26469;&#26816;&#27979;&#20851;&#38190;&#30340;&#35821;&#35328;&#32452;&#25104;&#37096;&#20998;&#65292;&#22914;&#35789;&#27719;&#12289;&#35821;&#20041;&#21644;&#21477;&#27861;&#30693;&#35782;&#65292;&#21253;&#21547;&#20102;&#22235;&#20010;&#20219;&#21153;&#65306;&#35821;&#20041;&#32467;&#26500;&#12289;&#21542;&#23450;&#36923;&#36753;&#12289;&#23646;&#24615;&#24402;&#23646;&#21644;&#20851;&#31995;&#32452;&#21512;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;.....
&lt;/p&gt;
&lt;p&gt;
The multimedia community has shown a significant interest in perceiving and representing the physical world with multimodal pretrained neural network models, and among them, the visual-language pertaining (VLP) is, currently, the most captivating topic. However, there have been few endeavors dedicated to the exploration of 1) whether essential linguistic knowledge (e.g., semantics and syntax) can be extracted during VLP, and 2) how such linguistic knowledge impact or enhance the multimodal alignment. In response, here we aim to elucidate the impact of comprehensive linguistic knowledge, including semantic expression and syntactic structure, on multimodal alignment. Specifically, we design and release the SNARE, the first large-scale multimodal alignment probing benchmark, to detect the vital linguistic components, e.g., lexical, semantic, and syntax knowledge, containing four tasks: Semantic structure, Negation logic, Attribute ownership, and Relationship composition. Based on our prop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27169;&#22411;&#25237;&#31080;&#25552;&#31034;(MVP)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;(FSL)&#29615;&#22659;&#19979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#26597;&#35810;&#24615;&#33021;&#12290;MVP&#36890;&#36807;&#25552;&#31034;&#22810;&#20010;LLMs&#25191;&#34892;&#30456;&#21516;&#30340;&#20219;&#21153;&#65292;&#24182;&#23545;&#29983;&#25104;&#30340;&#36755;&#20986;&#36827;&#34892;&#22810;&#25968;&#25237;&#31080;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#32597;&#35265;&#30149;&#30340;&#35782;&#21035;&#21644;&#20998;&#31867;&#20219;&#21153;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.12890</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25237;&#31080;&#65306;&#29992;&#20110;&#32597;&#35265;&#30149;&#35782;&#21035;&#30340;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Vote: Prompting for Rare Disease Identification. (arXiv:2308.12890v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27169;&#22411;&#25237;&#31080;&#25552;&#31034;(MVP)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;(FSL)&#29615;&#22659;&#19979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#26597;&#35810;&#24615;&#33021;&#12290;MVP&#36890;&#36807;&#25552;&#31034;&#22810;&#20010;LLMs&#25191;&#34892;&#30456;&#21516;&#30340;&#20219;&#21153;&#65292;&#24182;&#23545;&#29983;&#25104;&#30340;&#36755;&#20986;&#36827;&#34892;&#22810;&#25968;&#25237;&#31080;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#32597;&#35265;&#30149;&#30340;&#35782;&#21035;&#21644;&#20998;&#31867;&#20219;&#21153;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20986;&#29616;&#24378;&#35843;&#20102;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#25552;&#31034;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;LLMs&#32463;&#24120;&#24212;&#29992;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;(FSL)&#30340;&#24773;&#22659;&#20013;&#65292;&#36825;&#37324;&#20219;&#21153;&#21482;&#20351;&#29992;&#24456;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#25191;&#34892;&#12290;FSL&#22312;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;(AI)&#23376;&#39046;&#22495;&#20013;&#21464;&#24471;&#27969;&#34892;&#65292;&#21253;&#25324;&#29992;&#20110;&#20581;&#24247;&#30340;AI&#12290;&#32597;&#35265;&#30149;&#24433;&#21709;&#20154;&#21475;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#22312;&#25968;&#25454;&#21487;&#29992;&#24615;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979; inherently &#38656;&#35201;FSL&#25216;&#26415;&#65292;&#23613;&#31649;&#20154;&#24037;&#25968;&#25454;&#25910;&#38598;&#21644;&#26631;&#27880;&#36153;&#26102;&#36153;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27169;&#22411;&#25237;&#31080;&#25552;&#31034;(MVP)&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;FSL&#29615;&#22659;&#20013;LLM&#26597;&#35810;&#24615;&#33021;&#30340;&#28789;&#27963;&#25552;&#31034;&#26041;&#27861;&#12290;MVP&#36890;&#36807;&#25552;&#31034;&#22810;&#20010;LLMs&#25191;&#34892;&#30456;&#21516;&#30340;&#20219;&#21153;&#65292;&#28982;&#21518;&#23545;&#29983;&#25104;&#30340;&#36755;&#20986;&#36827;&#34892;&#22810;&#25968;&#25237;&#31080;&#26469;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#22312;&#21333;&#27425;&#32597;&#35265;&#30149;&#35782;&#21035;&#21644;&#20998;&#31867;&#20219;&#21153;&#20013;&#30456;&#23545;&#20110;&#20219;&#20309;&#21333;&#20010;&#27169;&#22411;&#22312;&#38598;&#25104;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32597;&#35265;&#30149;&#25968;&#25454;&#38598;&#29992;&#20110;FSL&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of generative Large Language Models (LLMs) emphasizes the need for accurate and efficient prompting approaches. LLMs are often applied in Few-Shot Learning (FSL) contexts, where tasks are executed with minimal training data. FSL has become popular in many Artificial Intelligence (AI) subdomains, including AI for health. Rare diseases, affecting a small fraction of the population, inherently require FSL techniques due to limited data availability, though manual data collection and annotation is costly and time-consuming. In this paper, we propose Models-Vote Prompting (MVP), a flexible prompting approach for improving the performance of LLM queries in FSL settings. MVP works by prompting numerous LLMs to perform the same tasks and then conducting a majority vote on the resulting outputs. This method achieves improved results to any one model in the ensemble on one-shot rare disease identification and classification tasks. We also release a novel rare disease dataset for FS
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#26469;&#35825;&#23548;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#22240;&#26524;&#32467;&#26500;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#20851;&#31995;&#65292;&#21487;&#20197;&#32531;&#35299;&#35757;&#32451;&#25968;&#25454;&#20013;&#35821;&#35328;&#20808;&#39564;&#30340;&#24178;&#25200;&#65292;&#36827;&#32780;&#25552;&#39640;&#25688;&#35201;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;&#30740;&#31350;&#32773;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#22240;&#26524;&#21551;&#21457;&#24335;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#26469;&#23398;&#20064;&#22240;&#26524;&#34920;&#31034;&#65292;&#20197;&#25351;&#23548;&#25688;&#35201;&#29983;&#25104;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2308.12888</link><description>&lt;p&gt;
&#35825;&#23548;&#22240;&#26524;&#32467;&#26500;&#36827;&#34892;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Inducing Causal Structure for Abstractive Text Summarization. (arXiv:2308.12888v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12888
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#26469;&#35825;&#23548;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#22240;&#26524;&#32467;&#26500;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#20851;&#31995;&#65292;&#21487;&#20197;&#32531;&#35299;&#35757;&#32451;&#25968;&#25454;&#20013;&#35821;&#35328;&#20808;&#39564;&#30340;&#24178;&#25200;&#65292;&#36827;&#32780;&#25552;&#39640;&#25688;&#35201;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;&#30740;&#31350;&#32773;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#22240;&#26524;&#21551;&#21457;&#24335;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#26469;&#23398;&#20064;&#22240;&#26524;&#34920;&#31034;&#65292;&#20197;&#25351;&#23548;&#25688;&#35201;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#20027;&#35201;&#25506;&#32034;&#30456;&#20851;&#24615;&#32780;&#38750;&#22240;&#26524;&#20851;&#31995;&#12290;&#22312;&#36825;&#20123;&#30456;&#20851;&#24615;&#20013;&#65292;&#21487;&#33021;&#23384;&#22312;&#21463;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#35821;&#35328;&#20808;&#39564;&#24178;&#25200;&#30340;&#20266;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#21066;&#24369;&#20102;&#23398;&#20064;&#27169;&#22411;&#30340;&#25972;&#20307;&#25928;&#26524;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#26469;&#35825;&#23548;&#25688;&#35201;&#25968;&#25454;&#30340;&#28508;&#22312;&#22240;&#26524;&#32467;&#26500;&#12290;&#25105;&#20204;&#20551;&#35774;&#23384;&#22312;&#20960;&#20010;&#28508;&#22312;&#30340;&#22240;&#26524;&#22240;&#32032;&#21644;&#38750;&#22240;&#26524;&#22240;&#32032;&#65292;&#20998;&#21035;&#34920;&#31034;&#25991;&#26723;&#21644;&#25688;&#35201;&#30340;&#20869;&#23481;&#21644;&#39118;&#26684;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#30340;SCM&#20013;&#30340;&#28508;&#22312;&#22240;&#32032;&#21487;&#20197;&#36890;&#36807;&#25311;&#21512;&#35266;&#23519;&#21040;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#35782;&#21035;&#20986;&#26469;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#21551;&#21457;&#24335;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#65288;CI-Seq2Seq&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#33021;&#22815;&#27169;&#25311;&#22240;&#26524;&#22240;&#32032;&#30340;&#22240;&#26524;&#34920;&#31034;&#65292;&#25351;&#23548;&#25105;&#20204;&#33719;&#21462;&#29992;&#20110;&#29983;&#25104;&#25688;&#35201;&#30340;&#22240;&#26524;&#20449;&#24687;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#37325;&#26032;&#26500;&#36896;Va&#27169;&#22411;&#65292;&#20197;&#24341;&#20837;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The mainstream of data-driven abstractive summarization models tends to explore the correlations rather than the causal relationships. Among such correlations, there can be spurious ones which suffer from the language prior learned from the training corpus and therefore undermine the overall effectiveness of the learned model. To tackle this issue, we introduce a Structural Causal Model (SCM) to induce the underlying causal structure of the summarization data. We assume several latent causal factors and non-causal factors, representing the content and style of the document and summary. Theoretically, we prove that the latent factors in our SCM can be identified by fitting the observed training data under certain conditions. On the basis of this, we propose a Causality Inspired Sequence-to-Sequence model (CI-Seq2Seq) to learn the causal representations that can mimic the causal factors, guiding us to pursue causal information for summary generation. The key idea is to reformulate the Va
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FaceTouch&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#26694;&#26550;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26816;&#27979;&#22797;&#26434;&#22330;&#26223;&#19979;&#30340;&#25163;&#19982;&#33080;&#25509;&#35302;&#65292;&#20197;&#24110;&#21161;&#36861;&#36394;&#20256;&#26579;&#30149;&#12290;FaceTouch&#36890;&#36807;&#23398;&#20064;&#20154;&#20307;&#21160;&#20316;&#21644;&#22330;&#26223;&#30340;RGB&#34920;&#31034;&#26469;&#26816;&#27979;&#38754;&#37096;&#25509;&#35302;&#65292;&#23545;&#20110;&#35299;&#20915;&#21482;&#33021;&#35782;&#21035;&#25163;&#30340;&#31227;&#21160;&#21644;&#19982;&#33080;&#30340;&#25509;&#36817;&#30340;&#38382;&#39064;&#20855;&#26377;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12840</link><description>&lt;p&gt;
FaceTouch: &#21033;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26816;&#27979;&#25163;&#19982;&#33080;&#25509;&#35302;&#65292;&#20197;&#24110;&#21161;&#36861;&#36394;&#20256;&#26579;&#30149;
&lt;/p&gt;
&lt;p&gt;
FaceTouch: Detecting hand-to-face touch with supervised contrastive learning to assist in tracing infectious disease. (arXiv:2308.12840v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12840
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FaceTouch&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#26694;&#26550;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26816;&#27979;&#22797;&#26434;&#22330;&#26223;&#19979;&#30340;&#25163;&#19982;&#33080;&#25509;&#35302;&#65292;&#20197;&#24110;&#21161;&#36861;&#36394;&#20256;&#26579;&#30149;&#12290;FaceTouch&#36890;&#36807;&#23398;&#20064;&#20154;&#20307;&#21160;&#20316;&#21644;&#22330;&#26223;&#30340;RGB&#34920;&#31034;&#26469;&#26816;&#27979;&#38754;&#37096;&#25509;&#35302;&#65292;&#23545;&#20110;&#35299;&#20915;&#21482;&#33021;&#35782;&#21035;&#25163;&#30340;&#31227;&#21160;&#21644;&#19982;&#33080;&#30340;&#25509;&#36817;&#30340;&#38382;&#39064;&#20855;&#26377;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25105;&#20204;&#30340;&#21628;&#21560;&#31995;&#32479;&#65292;&#35768;&#22810;&#30149;&#27602;&#21644;&#30142;&#30149;&#32463;&#24120;&#22312;&#20154;&#19982;&#20154;&#20043;&#38388;&#20256;&#25773;&#12290;COVID-19&#26159;&#19968;&#20010;&#31034;&#20363;&#65292;&#23637;&#31034;&#20102;&#36861;&#36394;&#21644;&#20943;&#23569;&#25509;&#35302;&#20197;&#38459;&#27490;&#30149;&#27602;&#20256;&#25773;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#23547;&#25214;&#33021;&#22815;&#22312;&#22797;&#26434;&#30340;&#22478;&#24066;&#22330;&#26223;&#25110;&#23460;&#20869;&#26816;&#27979;&#25163;&#19982;&#33080;&#25509;&#35302;&#30340;&#33258;&#21160;&#26041;&#27861;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#30340;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#26694;&#26550;&#65292;&#21517;&#20026;FaceTouch&#12290;&#23427;&#21253;&#25324;&#20102;&#29992;&#20110;&#26816;&#27979;&#20154;&#20307;&#21644;&#20998;&#26512;&#20854;&#21160;&#20316;&#30340;&#28145;&#24230;&#23376;&#27169;&#22411;&#12290;FaceTouch&#26088;&#22312;&#26816;&#27979;&#37326;&#22806;&#29615;&#22659;&#20013;&#30340;&#25163;&#19982;&#33080;&#25509;&#35302;&#65292;&#20363;&#22914;&#35270;&#39057;&#32842;&#22825;&#12289;&#20844;&#20132;&#36710;&#25668;&#20687;&#22836;&#25110;&#38381;&#36335;&#30005;&#35270;&#12290;&#23613;&#31649;&#38754;&#37096;&#37096;&#20998;&#36974;&#25377;&#65292;&#35813;&#31995;&#32479;&#36890;&#36807;&#21033;&#29992;&#22330;&#26223;&#30340;RGB&#34920;&#31034;&#20197;&#21450;&#36523;&#20307;&#25163;&#21183;&#65288;&#22914;&#25163;&#33218;&#36816;&#21160;&#65289;&#30340;&#34920;&#31034;&#23398;&#20064;&#26816;&#27979;&#38754;&#37096;&#25509;&#35302;&#12290;&#36825;&#24050;&#32463;&#22312;&#22797;&#26434;&#30340;&#22478;&#24066;&#22330;&#26223;&#20013;&#35777;&#26126;&#20102;&#23545;&#20110;&#20165;&#20165;&#35782;&#21035;&#25163;&#30340;&#31227;&#21160;&#21644;&#20854;&#19982;&#33080;&#30340;&#25509;&#36817;&#20043;&#22806;&#30340;&#23454;&#29992;&#24615;&#12290;&#22312;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#22522;&#30784;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Through our respiratory system, many viruses and diseases frequently spread and pass from one person to another. Covid-19 served as an example of how crucial it is to track down and cut back on contacts to stop its spread. There is a clear gap in finding automatic methods that can detect hand-to-face contact in complex urban scenes or indoors. In this paper, we introduce a computer vision framework, called FaceTouch, based on deep learning. It comprises deep sub-models to detect humans and analyse their actions. FaceTouch seeks to detect hand-to-face touches in the wild, such as through video chats, bus footage, or CCTV feeds. Despite partial occlusion of faces, the introduced system learns to detect face touches from the RGB representation of a given scene by utilising the representation of the body gestures such as arm movement. This has been demonstrated to be useful in complex urban scenarios beyond simply identifying hand movement and its closeness to faces. Relying on Supervised 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30701;&#36884;&#20844;&#20132;&#32447;&#36335;&#35268;&#21010;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#36890;&#36807;&#35843;&#25972;&#36335;&#32447;&#30340;&#29305;&#23450;&#37096;&#20998;&#65292;&#20943;&#23569;&#26102;&#38388;&#24182;&#25552;&#21319;&#20844;&#20849;&#20132;&#36890;&#26381;&#21153;&#12290;&#21033;&#29992;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#28304;&#65292;&#36890;&#36807;&#39044;&#27979;&#36947;&#36335;&#27573;&#30340;&#24310;&#36831;&#20540;&#20316;&#20026;&#36793;&#26435;&#37325;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#36335;&#24452;&#35268;&#21010;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2308.12828</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21152;&#26435;&#22270;&#30340;&#30701;&#36884;&#20844;&#20132;&#32447;&#36335;&#35268;&#21010;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Short Run Transit Route Planning Decision Support System Using a Deep Learning-Based Weighted Graph. (arXiv:2308.12828v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12828
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30701;&#36884;&#20844;&#20132;&#32447;&#36335;&#35268;&#21010;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#36890;&#36807;&#35843;&#25972;&#36335;&#32447;&#30340;&#29305;&#23450;&#37096;&#20998;&#65292;&#20943;&#23569;&#26102;&#38388;&#24182;&#25552;&#21319;&#20844;&#20849;&#20132;&#36890;&#26381;&#21153;&#12290;&#21033;&#29992;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#28304;&#65292;&#36890;&#36807;&#39044;&#27979;&#36947;&#36335;&#27573;&#30340;&#24310;&#36831;&#20540;&#20316;&#20026;&#36793;&#26435;&#37325;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#36335;&#24452;&#35268;&#21010;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#20849;&#20132;&#36890;&#36335;&#32447;&#35268;&#21010;&#22312;&#20132;&#36890;&#32593;&#32476;&#35774;&#35745;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#30830;&#20445;&#20056;&#23458;&#33719;&#24471;&#28385;&#24847;&#30340;&#26381;&#21153;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#36335;&#32447;&#35268;&#21010;&#35299;&#20915;&#26041;&#26696;&#20381;&#36182;&#20110;&#20256;&#32479;&#30340;&#36816;&#33829;&#30740;&#31350;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#23454;&#26045;&#36215;&#26469;&#32791;&#26102;&#65292;&#24182;&#19988;&#32570;&#20047;&#25552;&#20379;&#24555;&#36895;&#35299;&#20915;&#26041;&#26696;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#20351;&#20844;&#20849;&#20132;&#36890;&#35268;&#21010;&#32773;&#33021;&#22815;&#24555;&#36895;&#30830;&#23450;&#30701;&#26399;&#36335;&#32447;&#25913;&#36827;&#12290;&#36890;&#36807;&#22312;&#29305;&#23450;&#26102;&#38388;&#27573;&#20869;&#26080;&#32541;&#35843;&#25972;&#20004;&#20010;&#31449;&#28857;&#20043;&#38388;&#29305;&#23450;&#36335;&#27573;&#30340;&#36335;&#32447;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#20943;&#23569;&#26102;&#38388;&#24182;&#25552;&#21319;&#20844;&#20849;&#20132;&#36890;&#26381;&#21153;&#12290;&#21033;&#29992;GTFS&#21644;&#26234;&#33021;&#21345;&#25968;&#25454;&#31561;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#28304;&#65292;&#25105;&#20204;&#25552;&#21462;&#29305;&#24449;&#24182;&#23558;&#20132;&#36890;&#32593;&#32476;&#24314;&#27169;&#20026;&#26377;&#21521;&#22270;&#12290;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#36947;&#36335;&#27573;&#30340;&#24310;&#36831;&#20540;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#24310;&#36831;&#20540;&#34987;&#29992;&#20316;&#20132;&#36890;&#22270;&#20013;&#30340;&#36793;&#26435;&#37325;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#36335;&#24452;&#35268;&#21010;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Public transport routing plays a crucial role in transit network design, ensuring a satisfactory level of service for passengers. However, current routing solutions rely on traditional operational research heuristics, which can be time-consuming to implement and lack the ability to provide quick solutions. Here, we propose a novel deep learning-based methodology for a decision support system that enables public transport (PT) planners to identify short-term route improvements rapidly. By seamlessly adjusting specific sections of routes between two stops during specific times of the day, our method effectively reduces times and enhances PT services. Leveraging diverse data sources such as GTFS and smart card data, we extract features and model the transportation network as a directed graph. Using self-supervision, we train a deep learning model for predicting lateness values for road segments.  These lateness values are then utilized as edge weights in the transportation graph, enabling
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#20998;&#26512;&#24202;&#36793;&#30417;&#27979;&#20135;&#29983;&#30340;&#26102;&#38388;&#25968;&#25454;&#65292;&#25104;&#21151;&#26500;&#24314;&#20102;&#29992;&#20110;&#39044;&#27979;ICU&#24739;&#32773;&#27515;&#20129;&#29575;&#21644;&#20303;&#38498;&#26102;&#38388;&#30340;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2308.12800</link><description>&lt;p&gt;
ICU&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#36827;&#34892;&#27515;&#20129;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
ICU Mortality Prediction Using Long Short-Term Memory Networks. (arXiv:2308.12800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12800
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#20998;&#26512;&#24202;&#36793;&#30417;&#27979;&#20135;&#29983;&#30340;&#26102;&#38388;&#25968;&#25454;&#65292;&#25104;&#21151;&#26500;&#24314;&#20102;&#29992;&#20110;&#39044;&#27979;ICU&#24739;&#32773;&#27515;&#20129;&#29575;&#21644;&#20303;&#38498;&#26102;&#38388;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151; (ICU) &#36827;&#34892;&#30340;&#22823;&#37327;&#24202;&#36793;&#30417;&#27979;&#20135;&#29983;&#20102;&#19982;&#24739;&#32773;&#29983;&#29702;&#30456;&#20851;&#30340;&#22797;&#26434;&#26102;&#38388;&#25968;&#25454;&#65292;&#36825;&#20026;&#20020;&#24202;&#25968;&#25454;&#20998;&#26512;&#25552;&#20379;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#32972;&#26223;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#35782;&#21035;&#20986;&#36825;&#20123;&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#24335;&#21487;&#33021;&#20250;&#25552;&#20379;&#39640;&#24230;&#33021;&#21147;&#26469;&#39044;&#27979;&#20020;&#24202;&#20107;&#20214;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23454;&#26045;&#19968;&#20010;&#33258;&#21160;&#25968;&#25454;&#39537;&#21160;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20998;&#26512;&#26469;&#33258;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405; (EHRs) &#30340;&#22823;&#37327;&#22810;&#21464;&#37327;&#26102;&#38388;&#25968;&#25454;&#65292;&#24182;&#25552;&#21462;&#39640;&#32423;&#20449;&#24687;&#20197;&#26089;&#26399;&#39044;&#27979;&#20303;&#38498;&#27515;&#20129;&#29575;&#21644;&#20303;&#38498;&#26102;&#38388;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#26102;&#38388;&#31383;&#21475;&#32553;&#30701;&#21040;6&#23567;&#26102;&#26469;&#30740;&#31350;LSTM&#32593;&#32476;&#30340;&#36866;&#29992;&#24615;&#65292;&#20197;&#22686;&#24378;&#20020;&#24202;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#31361;&#20986;&#20102;LSTM&#27169;&#22411;&#30340;&#25928;&#29575;&#65292;&#36890;&#36807;&#20005;&#26684;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#27979;&#37327;&#26469;&#26500;&#24314;&#30495;&#23454;&#19990;&#30028;&#30340;&#39044;&#27979;&#24341;&#25806;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extensive bedside monitoring in Intensive Care Units (ICUs) has resulted in complex temporal data regarding patient physiology, which presents an upscale context for clinical data analysis. In the other hand, identifying the time-series patterns within these data may provide a high aptitude to predict clinical events. Hence, we investigate, during this work, the implementation of an automatic data-driven system, which analyzes large amounts of multivariate temporal data derived from Electronic Health Records (EHRs), and extracts high-level information so as to predict in-hospital mortality and Length of Stay (LOS) early. Practically, we investigate the applicability of LSTM network by reducing the time-frame to 6-hour so as to enhance clinical tasks. The experimental results highlight the efficiency of LSTM model with rigorous multivariate time-series measurements for building real-world prediction engines.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#24320;&#28304;&#30340;GitHub&#20179;&#24211;&#20026;&#26426;&#22120;&#35843;&#24230;&#38382;&#39064;&#25552;&#20379;&#20102;&#32508;&#21512;&#22522;&#20934;&#65292;&#21253;&#25324;&#22810;&#31181;&#29615;&#22659;&#21644;&#23454;&#20363;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#38598;&#20013;&#30340;&#20013;&#24515;&#12290;</title><link>http://arxiv.org/abs/2308.12794</link><description>&lt;p&gt;
&#24037;&#20316;&#36710;&#38388;&#35843;&#24230;&#22522;&#20934;&#65306;&#29992;&#20110;&#23398;&#20064;&#21644;&#38750;&#23398;&#20064;&#26041;&#27861;&#30340;&#29615;&#22659;&#21644;&#23454;&#20363;
&lt;/p&gt;
&lt;p&gt;
Job Shop Scheduling Benchmark: Environments and Instances for Learning and Non-learning Methods. (arXiv:2308.12794v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12794
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#24320;&#28304;&#30340;GitHub&#20179;&#24211;&#20026;&#26426;&#22120;&#35843;&#24230;&#38382;&#39064;&#25552;&#20379;&#20102;&#32508;&#21512;&#22522;&#20934;&#65292;&#21253;&#25324;&#22810;&#31181;&#29615;&#22659;&#21644;&#23454;&#20363;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#38598;&#20013;&#30340;&#20013;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;GitHub&#20179;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#24191;&#27867;&#30340;&#26426;&#22120;&#35843;&#24230;&#38382;&#39064;&#30340;&#32508;&#21512;&#22522;&#20934;&#65292;&#21253;&#25324;&#24037;&#20316;&#36710;&#38388;&#35843;&#24230;&#65288;JSP&#65289;&#65292;&#27969;&#27700;&#36710;&#38388;&#35843;&#24230;&#65288;FSP&#65289;&#65292;&#28789;&#27963;&#24037;&#20316;&#36710;&#38388;&#35843;&#24230;&#65288;FJSP&#65289;&#65292;&#20855;&#26377;&#35013;&#37197;&#32422;&#26463;&#30340;FJSP&#65288;FAJSP&#65289;&#65292;&#20855;&#26377;&#24207;&#21015;&#20381;&#36182;&#35774;&#32622;&#26102;&#38388;&#30340;FJSP&#65288;FJSP-SDST&#65289;&#21644;&#22312;&#32447;FJSP&#65288;&#22312;&#32447;&#20316;&#19994;&#21040;&#36798;&#65289;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20026;&#23545;&#26426;&#22120;&#35843;&#24230;&#25361;&#25112;&#24863;&#20852;&#36259;&#30340;&#30740;&#31350;&#20154;&#21592;&#65292;&#20174;&#19994;&#32773;&#21644;&#29233;&#22909;&#32773;&#25552;&#20379;&#19968;&#20010;&#38598;&#20013;&#30340;&#20013;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an open-source GitHub repository containing comprehensive benchmarks for a wide range of machine scheduling problems, including Job Shop Scheduling (JSP), Flow Shop Scheduling (FSP), Flexible Job Shop Scheduling (FJSP), FJSP with Assembly constraints (FAJSP), FJSP with Sequence-Dependent Setup Times (FJSP-SDST), and the online FJSP (with online job arrivals). Our primary goal is to provide a centralized hub for researchers, practitioners, and enthusiasts interested in tackling machine scheduling challenges.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#24615;&#21487;&#35299;&#37322;&#22270;(QXG)&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#30340;&#35299;&#37322;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23450;&#24615;&#26102;&#31354;&#25512;&#29702;&#26500;&#24314;&#22270;&#65292;&#33021;&#22815;&#23454;&#26102;&#35745;&#31639;&#65292;&#21344;&#29992;&#31354;&#38388;&#36739;&#23569;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12755</link><description>&lt;p&gt;
&#33719;&#21462;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#35299;&#37322;&#30340;&#23450;&#24615;&#21487;&#35299;&#37322;&#22270;
&lt;/p&gt;
&lt;p&gt;
Acquiring Qualitative Explainable Graphs for Automated Driving Scene Interpretation. (arXiv:2308.12755v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#24615;&#21487;&#35299;&#37322;&#22270;(QXG)&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#30340;&#35299;&#37322;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23450;&#24615;&#26102;&#31354;&#25512;&#29702;&#26500;&#24314;&#22270;&#65292;&#33021;&#22815;&#23454;&#26102;&#35745;&#31639;&#65292;&#21344;&#29992;&#31354;&#38388;&#36739;&#23569;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#65288;AD&#65289;&#26410;&#26469;&#30340;&#21457;&#23637;&#20381;&#36182;&#20110;&#24378;&#22823;&#12289;&#20844;&#24179;&#21644;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#12290;&#26681;&#25454;&#35201;&#27714;&#65292;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#24517;&#39035;&#33021;&#22815;&#21521;&#39550;&#39542;&#21592;&#21644;&#36710;&#19978;&#20056;&#23458;&#12289;&#34892;&#20154;&#20197;&#21450;&#20854;&#20182;&#23481;&#26131;&#21463;&#20260;&#30340;&#36947;&#36335;&#20351;&#29992;&#32773;&#35299;&#37322;&#20854;&#20915;&#31574;&#65292;&#21487;&#33021;&#36824;&#35201;&#21521;&#22806;&#37096;&#23457;&#35745;&#20154;&#21592;&#35299;&#37322;&#20107;&#25925;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22823;&#22810;&#25968;&#21487;&#35299;&#37322;&#26041;&#27861;&#20173;&#20381;&#36182;&#20110;&#23545;&#22810;&#20010;&#20256;&#24863;&#22120;&#25429;&#25417;&#30340;AD&#22330;&#26223;&#34920;&#31034;&#36827;&#34892;&#23450;&#37327;&#20998;&#26512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;AD&#22330;&#26223;&#34920;&#31034;&#26041;&#27861;&#65292;&#31216;&#20026;&#23450;&#24615;&#21487;&#35299;&#37322;&#22270;&#65288;QXG&#65289;&#65292;&#19987;&#29992;&#20110;&#38271;&#26399;&#22330;&#26223;&#30340;&#23450;&#24615;&#26102;&#31354;&#25512;&#29702;&#12290;&#35813;&#22270;&#30340;&#26500;&#24314;&#21033;&#29992;&#20102;&#26368;&#36817;&#30340;&#23450;&#24615;&#32422;&#26463;&#33719;&#21462;&#33539;&#24335;&#12290;&#25105;&#20204;&#22312;NuScenes&#65292;&#19968;&#20010;&#24320;&#25918;&#30340;&#30495;&#23454;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19968;&#20010;&#30001;40&#24103;&#32452;&#25104;&#30340;AD&#22330;&#26223;&#30340;&#23450;&#24615;&#21487;&#35299;&#37322;&#22270;&#21487;&#20197;&#23454;&#26102;&#35745;&#31639;&#65292;&#24182;&#19988;&#21344;&#29992;&#31354;&#38388;&#36739;&#23569;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The future of automated driving (AD) is rooted in the development of robust, fair and explainable artificial intelligence methods. Upon request, automated vehicles must be able to explain their decisions to the driver and the car passengers, to the pedestrians and other vulnerable road users and potentially to external auditors in case of accidents. However, nowadays, most explainable methods still rely on quantitative analysis of the AD scene representations captured by multiple sensors. This paper proposes a novel representation of AD scenes, called Qualitative eXplainable Graph (QXG), dedicated to qualitative spatiotemporal reasoning of long-term scenes. The construction of this graph exploits the recent Qualitative Constraint Acquisition paradigm. Our experimental results on NuScenes, an open real-world multi-modal dataset, show that the qualitative eXplainable graph of an AD scene composed of 40 frames can be computed in real-time and light in space storage which makes it a potent
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#30456;&#20301;&#27969;&#24418;&#30340;&#21160;&#20316;&#25554;&#24103;&#31995;&#32479;&#65292;&#36890;&#36807;&#23398;&#20064;&#30456;&#20301;&#21464;&#37327;&#21644;&#28151;&#21512;&#19987;&#23478;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26469;&#29983;&#25104;&#30446;&#26631;&#23039;&#21183;&#20043;&#38388;&#30340;&#36830;&#32493;&#23039;&#21183;&#24207;&#21015;&#65292;&#21516;&#26102;&#21487;&#20197;&#28385;&#36275;&#21160;&#30011;&#24072;&#25163;&#21160;&#20462;&#25913;&#30340;&#23039;&#21183;&#21644;&#26411;&#31471;&#25928;&#24212;&#22120;&#20316;&#20026;&#32422;&#26463;&#30340;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.12751</link><description>&lt;p&gt;
&#20351;&#29992;&#30456;&#20301;&#27969;&#24418;&#30340;&#21160;&#20316;&#25554;&#24103;
&lt;/p&gt;
&lt;p&gt;
Motion In-Betweening with Phase Manifolds. (arXiv:2308.12751v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#30456;&#20301;&#27969;&#24418;&#30340;&#21160;&#20316;&#25554;&#24103;&#31995;&#32479;&#65292;&#36890;&#36807;&#23398;&#20064;&#30456;&#20301;&#21464;&#37327;&#21644;&#28151;&#21512;&#19987;&#23478;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26469;&#29983;&#25104;&#30446;&#26631;&#23039;&#21183;&#20043;&#38388;&#30340;&#36830;&#32493;&#23039;&#21183;&#24207;&#21015;&#65292;&#21516;&#26102;&#21487;&#20197;&#28385;&#36275;&#21160;&#30011;&#24072;&#25163;&#21160;&#20462;&#25913;&#30340;&#23039;&#21183;&#21644;&#26411;&#31471;&#25928;&#24212;&#22120;&#20316;&#20026;&#32422;&#26463;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39537;&#21160;&#21160;&#20316;&#25554;&#24103;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#21608;&#26399;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#30340;&#30456;&#20301;&#21464;&#37327;&#26469;&#36798;&#21040;&#35282;&#33394;&#30340;&#30446;&#26631;&#23039;&#21183;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#28151;&#21512;&#19987;&#23478;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20854;&#20013;&#30456;&#20301;&#20197;&#19981;&#21516;&#30340;&#19987;&#23478;&#26435;&#37325;&#23558;&#21160;&#20316;&#22312;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#36827;&#34892;&#32858;&#31867;&#12290;&#27599;&#32452;&#29983;&#25104;&#30340;&#26435;&#37325;&#20197;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#22312;&#24403;&#21069;&#29366;&#24577;&#21644;&#30446;&#26631;&#29366;&#24577;&#20043;&#38388;&#20135;&#29983;&#19968;&#31995;&#21015;&#23039;&#21183;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#28385;&#36275;&#21160;&#30011;&#24072;&#25163;&#21160;&#20462;&#25913;&#30340;&#23039;&#21183;&#25110;&#26576;&#20123;&#26411;&#31471;&#25928;&#24212;&#22120;&#20316;&#20026;&#21160;&#30011;&#35201;&#36798;&#21040;&#30340;&#32422;&#26463;&#65292;&#23454;&#26045;&#20102;&#19968;&#31181;&#23398;&#20064;&#30340;&#21452;&#21521;&#25511;&#21046;&#26041;&#26696;&#26469;&#28385;&#36275;&#36825;&#20123;&#32422;&#26463;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#30456;&#20301;&#36827;&#34892;&#21160;&#20316;&#25554;&#24103;&#21487;&#20197;&#25552;&#39640;&#25554;&#20540;&#21160;&#20316;&#30340;&#38160;&#24230;&#65292;&#24182;&#36827;&#19968;&#27493;&#31283;&#23450;&#23398;&#20064;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#30456;&#20301;&#36827;&#34892;&#21160;&#20316;&#25554;&#24103;&#36824;&#21487;&#20197;&#21512;&#25104;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#36816;&#21160;&#65292;&#36229;&#36234;&#20102;&#34892;&#36208;&#31561;&#22522;&#26412;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel data-driven motion in-betweening system to reach target poses of characters by making use of phases variables learned by a Periodic Autoencoder. Our approach utilizes a mixture-of-experts neural network model, in which the phases cluster movements in both space and time with different expert weights. Each generated set of weights then produces a sequence of poses in an autoregressive manner between the current and target state of the character. In addition, to satisfy poses which are manually modified by the animators or where certain end effectors serve as constraints to be reached by the animation, a learned bi-directional control scheme is implemented to satisfy such constraints. The results demonstrate that using phases for motion in-betweening tasks sharpen the interpolated movements, and furthermore stabilizes the learning process. Moreover, using phases for motion in-betweening tasks can also synthesize more challenging movements beyond locomotion b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26356;&#39640;&#30340;&#25209;&#35780;&#21147;&#37327;&#23558;&#20154;&#31867;&#35302;&#25720;&#19982;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#20998;&#31163;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20010;&#22256;&#24785;&#24230;&#27979;&#35797;&#21644;&#32479;&#35745;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#20986;&#21487;&#33021;&#34987;&#32534;&#36753;&#30340;&#37096;&#20998;&#65292;&#24182;&#25506;&#35752;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21450;&#30456;&#20851;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.12747</link><description>&lt;p&gt;
&#20351;&#29992;&#26356;&#39640;&#30340;&#25209;&#35780;&#21147;&#37327;&#23558;&#20154;&#31867;&#35302;&#25720;&#19982;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#20998;&#31163;&#65306;&#19968;&#31181;&#20449;&#24687;&#35770;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Separating the Human Touch from AI-Generated Text using Higher Criticism: An Information-Theoretic Approach. (arXiv:2308.12747v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26356;&#39640;&#30340;&#25209;&#35780;&#21147;&#37327;&#23558;&#20154;&#31867;&#35302;&#25720;&#19982;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#20998;&#31163;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20010;&#22256;&#24785;&#24230;&#27979;&#35797;&#21644;&#32479;&#35745;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#20986;&#21487;&#33021;&#34987;&#32534;&#36753;&#30340;&#37096;&#20998;&#65292;&#24182;&#25506;&#35752;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21450;&#30456;&#20851;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#19968;&#31687;&#32473;&#23450;&#25991;&#31456;&#26159;&#23436;&#20840;&#30001;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#32534;&#20889;&#30340;&#65292;&#36824;&#26159;&#21253;&#21547;&#20102;&#19968;&#20123;&#30001;&#20854;&#20182;&#20316;&#32773;&#65288;&#21487;&#33021;&#26159;&#20154;&#31867;&#65289;&#36827;&#34892;&#20102;&#37325;&#35201;&#32534;&#36753;&#30340;&#26367;&#20195;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#36807;&#31243;&#28041;&#21450;&#23545;&#20010;&#21035;&#21477;&#23376;&#25110;&#20854;&#20182;&#25991;&#26412;&#21333;&#20301;&#36215;&#28304;&#36827;&#34892;&#30340;&#22810;&#20010;&#22256;&#24785;&#24230;&#27979;&#35797;&#65292;&#23558;&#36825;&#20123;&#22810;&#20010;&#27979;&#35797;&#32467;&#21512;&#36215;&#26469;&#20351;&#29992;&#26356;&#39640;&#30340;&#25209;&#35780;&#21147;&#37327;&#65288;HC&#65289;&#12290;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#35813;&#26041;&#27861;&#36824;&#33021;&#22815;&#35782;&#21035;&#20986;&#21487;&#33021;&#34987;&#32534;&#36753;&#30340;&#37096;&#20998;&#12290;&#35813;&#26041;&#27861;&#21463;&#21040;&#20102;&#23545;&#25968;&#22256;&#24785;&#24230;&#25910;&#25947;&#21040;&#20132;&#21449;&#29109;&#29575;&#30340;&#21551;&#21457;&#65292;&#20197;&#21450;&#19968;&#20010;&#32479;&#35745;&#27169;&#22411;&#26469;&#25551;&#36848;&#34987;&#32534;&#36753;&#30340;&#25991;&#26412;&#65292;&#21363;&#21477;&#23376;&#20027;&#35201;&#30001;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#65292;&#20294;&#21487;&#33021;&#26377;&#23569;&#25968;&#21477;&#23376;&#26159;&#36890;&#36807;&#20854;&#20182;&#26426;&#21046;&#20135;&#29983;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#24433;&#21709;&#20854;&#25104;&#21151;&#30340;&#22240;&#32032;&#12290;&#36825;&#20010;&#20998;&#26512;&#25552;&#20986;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#24320;&#25918;&#25361;&#25112;&#65292;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#21487;&#33021;&#20250;&#25913;&#21892;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method to determine whether a given article was entirely written by a generative language model versus an alternative situation in which the article includes some significant edits by a different author, possibly a human. Our process involves many perplexity tests for the origin of individual sentences or other text atoms, combining these multiple tests using Higher Criticism (HC). As a by-product, the method identifies parts suspected to be edited. The method is motivated by the convergence of the log-perplexity to the cross-entropy rate and by a statistical model for edited text saying that sentences are mostly generated by the language model, except perhaps for a few sentences that might have originated via a different mechanism. We demonstrate the effectiveness of our method using real data and analyze the factors affecting its success. This analysis raises several interesting open challenges whose resolution may improve the method's effectiveness.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#22522;&#22240;&#32452;&#35268;&#27169;&#20195;&#35874;&#32593;&#32476;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#22522;&#20110;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;(ILP)&#26694;&#26550;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#20174;&#23454;&#39564;&#20013;&#23398;&#20064;&#26032;&#30340;&#36923;&#36753;&#32467;&#26500;&#65292;&#20197;&#26377;&#25928;&#25506;&#32034;&#20551;&#35774;&#31354;&#38388;&#21644;&#25351;&#23548;&#23454;&#39564;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2308.12740</link><description>&lt;p&gt;
&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#22522;&#22240;&#32452;&#35268;&#27169;&#20195;&#35874;&#32593;&#32476;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Human Comprehensible Active Learning of Genome-Scale Metabolic Networks. (arXiv:2308.12740v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12740
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#22522;&#22240;&#32452;&#35268;&#27169;&#20195;&#35874;&#32593;&#32476;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#22522;&#20110;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;(ILP)&#26694;&#26550;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#20174;&#23454;&#39564;&#20013;&#23398;&#20064;&#26032;&#30340;&#36923;&#36753;&#32467;&#26500;&#65292;&#20197;&#26377;&#25928;&#25506;&#32034;&#20551;&#35774;&#31354;&#38388;&#21644;&#25351;&#23548;&#23454;&#39564;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#29983;&#29289;&#23398;&#30340;&#19968;&#20010;&#37325;&#35201;&#24212;&#29992;&#26159;&#23558;&#23487;&#20027;&#32454;&#32990;&#31995;&#32479;&#24037;&#31243;&#21270;&#20197;&#20135;&#29983;&#26377;&#29992;&#30340;&#20135;&#21697;&#12290;&#28982;&#32780;&#65292;&#23487;&#20027;&#31995;&#32479;&#35268;&#27169;&#30340;&#22686;&#21152;&#23548;&#33268;&#35774;&#35745;&#31354;&#38388;&#24040;&#22823;&#65292;&#24182;&#38656;&#35201;&#22823;&#37327;&#39640;&#26114;&#30340;&#39564;&#35777;&#35797;&#39564;&#12290;&#20026;&#20102;&#23487;&#20027;&#32454;&#32990;&#31995;&#32479;&#30340;&#35774;&#35745;-&#26500;&#24314;-&#27979;&#35797;-&#23398;&#20064;&#65288;Design-Build-Test-Learn&#65292;DBTL&#65289;&#21608;&#26399;&#65292;&#36843;&#20999;&#38656;&#35201;&#19968;&#31181;&#33021;&#26377;&#25928;&#25506;&#32034;&#20551;&#35774;&#31354;&#38388;&#24182;&#25351;&#23548;&#23454;&#39564;&#35774;&#35745;&#30340;&#21487;&#29702;&#35299;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#65288;ILP&#65289;&#30340;&#26032;&#22411;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;ILP-iML1515&#65292;&#23427;&#36890;&#36807;&#35825;&#23548;&#36923;&#36753;&#25512;&#29702;&#21644;&#20174;&#35757;&#32451;&#23454;&#20363;&#20013;&#31215;&#26497;&#23398;&#20064;&#26469;&#25191;&#34892;&#35828;&#26126;&#24615;&#30340;&#36923;&#36753;&#25512;&#29702;&#12290;&#19982;&#25968;&#20540;&#27169;&#22411;&#19981;&#21516;&#65292;ILP-iML1515&#24314;&#31435;&#22312;&#23545;&#22522;&#22240;&#32452;&#35268;&#27169;&#20195;&#35874;&#27169;&#22411;&#30340;&#21487;&#29702;&#35299;&#30340;&#36923;&#36753;&#34920;&#31034;&#19978;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#20174;&#32570;&#20047;&#33829;&#20859;&#30340;&#31361;&#21464;&#20307;&#35797;&#39564;&#20013;&#23398;&#20064;&#26032;&#30340;&#36923;&#36753;&#32467;&#26500;&#26469;&#26356;&#26032;&#27169;&#22411;&#12290;ILP-iML1515&#26694;&#26550;&#20855;&#26377;&#39640;&#36890;&#37327;&#27169;&#25311;&#33021;&#21147;&#65292;&#24182;&#33021;&#20027;&#21160;&#36873;&#25321;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
An important application of Synthetic Biology is the engineering of the host cell system to yield useful products. However, an increase in the scale of the host system leads to huge design space and requires a large number of validation trials with high experimental costs. A comprehensible machine learning approach that efficiently explores the hypothesis space and guides experimental design is urgently needed for the Design-Build-Test-Learn (DBTL) cycle of the host cell system. We introduce a novel machine learning framework ILP-iML1515 based on Inductive Logic Programming (ILP) that performs abductive logical reasoning and actively learns from training examples. In contrast to numerical models, ILP-iML1515 is built on comprehensible logical representations of a genome-scale metabolic model and can update the model by learning new logical structures from auxotrophic mutant trials. The ILP-iML1515 framework 1) allows high-throughput simulations and 2) actively selects experiments that 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#23545;&#31216;&#21327;&#21516;&#35757;&#32451;&#26694;&#26550;&#65292;&#32467;&#21512;&#28145;&#24230;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#22810;&#31867;&#21035;&#32452;&#32455;&#30149;&#29702;&#22270;&#20687;&#20998;&#31867;&#12290;&#36890;&#36807;&#23884;&#20837;&#32454;&#32990;&#30340;&#24418;&#24577;&#21644;&#25299;&#25169;&#20998;&#24067;&#65292;&#36825;&#20010;&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#25972;&#20010;&#31995;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12737</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#32454;&#32990;&#22270;&#38598;&#25104;&#30340;&#38750;&#23545;&#31216;&#21327;&#21516;&#35757;&#32451;&#29992;&#20110;&#32452;&#32455;&#30149;&#29702;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Asymmetric Co-Training with Explainable Cell Graph Ensembling for Histopathological Image Classification. (arXiv:2308.12737v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12737
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#23545;&#31216;&#21327;&#21516;&#35757;&#32451;&#26694;&#26550;&#65292;&#32467;&#21512;&#28145;&#24230;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#22810;&#31867;&#21035;&#32452;&#32455;&#30149;&#29702;&#22270;&#20687;&#20998;&#31867;&#12290;&#36890;&#36807;&#23884;&#20837;&#32454;&#32990;&#30340;&#24418;&#24577;&#21644;&#25299;&#25169;&#20998;&#24067;&#65292;&#36825;&#20010;&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#25972;&#20010;&#31995;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#32452;&#32455;&#30149;&#29702;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#20854;&#20687;&#32032;&#32423;&#30340;&#20851;&#27880;&#65292;&#38480;&#21046;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#30456;&#21453;&#65292;&#26032;&#20852;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#32858;&#28966;&#20110;&#32454;&#32990;&#32423;&#29305;&#24449;&#21644;&#21307;&#23398;&#24847;&#20041;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27973;&#23618;&#32593;&#32476;&#21644;&#39640;&#32500;&#20687;&#32032;&#25968;&#25454;&#20351;&#29992;&#19981;&#20805;&#20998;&#65292;&#22270;&#21367;&#31215;&#32593;&#32476;&#22312;&#22810;&#31867;&#21035;&#32452;&#32455;&#30149;&#29702;&#22270;&#20687;&#20998;&#31867;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#20687;&#32032;&#32423;&#21644;&#32454;&#32990;&#32423;&#29305;&#24449;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#23545;&#31216;&#21327;&#21516;&#35757;&#32451;&#26694;&#26550;&#65292;&#32467;&#21512;&#28145;&#24230;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22810;&#31867;&#21035;&#32452;&#32455;&#30149;&#29702;&#22270;&#20687;&#20998;&#31867;&#12290;&#20026;&#20102;&#25552;&#39640;&#25972;&#20010;&#26694;&#26550;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;14&#23618;&#30340;&#28145;&#24230;&#22270;&#21367;&#31215;&#32593;&#32476;&#26469;&#22788;&#29702;&#32454;&#32990;&#22270;&#25968;&#25454;&#30340;&#24418;&#24577;&#21644;&#25299;&#25169;&#20998;&#24067;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#21033;&#29992;&#20687;&#32032;&#32423;&#21644;&#32454;&#32990;&#32423;&#20449;&#24687;&#20043;&#38388;&#30340;&#21160;&#24577;&#20114;&#20316;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#21327;&#21516;&#35757;&#32451;&#31574;&#30053;&#26469;&#25972;&#21512;&#36825;&#20004;&#32773;&#20043;&#38388;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks excel in histopathological image classification, yet their pixel-level focus hampers explainability. Conversely, emerging graph convolutional networks spotlight cell-level features and medical implications. However, limited by their shallowness and suboptimal use of high-dimensional pixel data, GCNs underperform in multi-class histopathological image classification. To make full use of pixel-level and cell-level features dynamically, we propose an asymmetric co-training framework combining a deep graph convolutional network and a convolutional neural network for multi-class histopathological image classification. To improve the explainability of the entire framework by embedding morphological and topological distribution of cells, we build a 14-layer deep graph convolutional network to handle cell graph data. For the further utilization and dynamic interactions between pixel-level and cell-level information, we also design a co-training strategy to integra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;YOLO&#21644;Swin&#27169;&#22411;&#23454;&#29616;&#25163;&#33109;X&#23556;&#32447;&#22270;&#20687;&#20013;&#39592;&#30149;&#29702;&#30340;&#23450;&#20301;&#21644;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#20934;&#30830;&#23450;&#20301;&#21644;&#31934;&#30830;&#20998;&#31867;&#30340;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.12727</link><description>&lt;p&gt;
DeepLOC: &#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25163;&#33109;X&#23556;&#32447;&#22270;&#20687;&#20013;&#39592;&#30149;&#29702;&#23450;&#20301;&#21644;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
DeepLOC: Deep Learning-based Bone Pathology Localization and Classification in Wrist X-ray Images. (arXiv:2308.12727v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;YOLO&#21644;Swin&#27169;&#22411;&#23454;&#29616;&#25163;&#33109;X&#23556;&#32447;&#22270;&#20687;&#20013;&#39592;&#30149;&#29702;&#30340;&#23450;&#20301;&#21644;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#20934;&#30830;&#23450;&#20301;&#21644;&#31934;&#30830;&#20998;&#31867;&#30340;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#31995;&#32479;&#22312;&#36741;&#21161;&#25918;&#23556;&#31185;&#21307;&#29983;&#36827;&#34892;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#26041;&#38754;&#23637;&#29616;&#20102;&#24040;&#22823;&#28508;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;YOLO (You Only Look Once) &#21644; Shifted Window Transformer (Swin) &#30340;&#32452;&#21512;&#65292;&#24182;&#37197;&#21512;&#19968;&#20010;&#26032;&#25552;&#20986;&#30340;&#27169;&#22359;&#65292;&#23545;&#25163;&#33109;X&#23556;&#32447;&#22270;&#20687;&#20013;&#30340;&#39592;&#30149;&#29702;&#36827;&#34892;&#23450;&#20301;&#21644;&#20998;&#31867;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#25163;&#33109;X&#23556;&#32447;&#20998;&#26512;&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#39592;&#30149;&#29702;&#30340;&#20934;&#30830;&#23450;&#20301;&#21644;&#24322;&#24120;&#30340;&#31934;&#30830;&#20998;&#31867;&#12290;&#21033;&#29992;YOLO&#26694;&#26550;&#36827;&#34892;&#39592;&#30149;&#29702;&#30340;&#26816;&#27979;&#21644;&#23450;&#20301;&#65292;&#20805;&#20998;&#21457;&#25381;&#20854;&#23454;&#26102;&#30446;&#26631;&#26816;&#27979;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;Swin&#27169;&#22359;&#20174;&#24863;&#20852;&#36259;&#21306;&#22495;(ROI)&#20013;&#25552;&#21462;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#23454;&#29616;&#20934;&#30830;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, computer-aided diagnosis systems have shown great potential in assisting radiologists with accurate and efficient medical image analysis. This paper presents a novel approach for bone pathology localization and classification in wrist X-ray images using a combination of YOLO (You Only Look Once) and the Shifted Window Transformer (Swin) with a newly proposed block. The proposed methodology addresses two critical challenges in wrist X-ray analysis: accurate localization of bone pathologies and precise classification of abnormalities. The YOLO framework is employed to detect and localize bone pathologies, leveraging its real-time object detection capabilities. Additionally, the Swin, a transformer-based module, is utilized to extract contextual information from the localized regions of interest (ROIs) for accurate classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#30340;&#21160;&#24577;&#38590;&#24230;&#35843;&#25972;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#35270;&#35273;&#24037;&#20316;&#35760;&#24518;&#28216;&#25103;&#20013;&#30340;&#22797;&#26434;&#38590;&#24230;&#35760;&#24518;&#38382;&#39064;&#12290;&#36890;&#36807;&#26681;&#25454;&#29609;&#23478;&#30340;&#24471;&#20998;&#21644;&#19978;&#19968;&#36718;&#28216;&#25103;&#30340;&#38590;&#24230;&#37327;&#24230;&#26469;&#35843;&#25972;&#28216;&#25103;&#38590;&#24230;&#65292;&#35813;&#26041;&#27861;&#22312;52&#20301;&#21463;&#35797;&#32773;&#30340;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2308.12726</link><description>&lt;p&gt;
&#22522;&#20110;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#30340;&#35270;&#35273;&#24037;&#20316;&#35760;&#24518;&#28216;&#25103;&#21160;&#24577;&#38590;&#24230;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Continuous Reinforcement Learning-based Dynamic Difficulty Adjustment in a Visual Working Memory Game. (arXiv:2308.12726v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#30340;&#21160;&#24577;&#38590;&#24230;&#35843;&#25972;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#35270;&#35273;&#24037;&#20316;&#35760;&#24518;&#28216;&#25103;&#20013;&#30340;&#22797;&#26434;&#38590;&#24230;&#35760;&#24518;&#38382;&#39064;&#12290;&#36890;&#36807;&#26681;&#25454;&#29609;&#23478;&#30340;&#24471;&#20998;&#21644;&#19978;&#19968;&#36718;&#28216;&#25103;&#30340;&#38590;&#24230;&#37327;&#24230;&#26469;&#35843;&#25972;&#28216;&#25103;&#38590;&#24230;&#65292;&#35813;&#26041;&#27861;&#22312;52&#20301;&#21463;&#35797;&#32773;&#30340;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#38590;&#24230;&#35843;&#25972;&#65288;DDA&#65289;&#26159;&#25552;&#21319;&#29609;&#23478;&#22312;&#35270;&#39057;&#28216;&#25103;&#20013;&#20307;&#39564;&#30340;&#19968;&#31181;&#26377;&#25928;&#26041;&#27861;&#12290;&#26368;&#36817;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#24050;&#34987;&#24212;&#29992;&#20110;&#38750;&#31454;&#20105;&#24615;&#28216;&#25103;&#30340;DDA&#65307;&#23613;&#31649;&#22914;&#27492;&#65292;&#23427;&#20204;&#20165;&#20381;&#36182;&#20110;&#20855;&#26377;&#23567;&#30340;&#25628;&#32034;&#31354;&#38388;&#30340;&#31163;&#25955;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#30340;DDA&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#35270;&#35273;&#24037;&#20316;&#35760;&#24518;&#65288;VWM&#65289;&#28216;&#25103;&#20013;&#38590;&#24230;&#35760;&#24518;&#30340;&#22797;&#26434;&#25628;&#32034;&#31354;&#38388;&#12290;&#35813;&#25552;&#20986;&#30340;&#22522;&#20110;RL&#30340;DDA&#26681;&#25454;&#29609;&#23478;&#30340;&#24471;&#20998;&#21644;&#19978;&#19968;&#36718;&#28216;&#25103;&#30340;&#38590;&#24230;&#37327;&#24230;&#26469;&#35843;&#25972;&#28216;&#25103;&#38590;&#24230;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#36830;&#32493;&#30340;&#38590;&#24230;&#35760;&#24518;&#24230;&#37327;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#20219;&#21153;&#38590;&#24230;&#21644;&#38590;&#24230;-&#24471;&#20998;&#21521;&#37327;&#20998;&#21035;&#20316;&#20026;RL&#30340;&#21160;&#20316;&#21644;&#29366;&#24577;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#28041;&#21450;52&#20301;&#21463;&#35797;&#32773;&#30340;&#34987;&#35797;&#20869;&#23454;&#39564;&#26469;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#22312;&#29609;&#23478;&#30340;&#24471;&#20998;&#21644;&#28216;&#25103;&#20307;&#39564;&#24230;&#37327;&#26041;&#38754;&#65292;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#20004;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#38590;&#24230;&#35843;&#25972;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic Difficulty Adjustment (DDA) is a viable approach to enhance a player's experience in video games. Recently, Reinforcement Learning (RL) methods have been employed for DDA in non-competitive games; nevertheless, they rely solely on discrete state-action space with a small search space. In this paper, we propose a continuous RL-based DDA methodology for a visual working memory (VWM) game to handle the complex search space for the difficulty of memorization. The proposed RL-based DDA tailors game difficulty based on the player's score and game difficulty in the last trial. We defined a continuous metric for the difficulty of memorization. Then, we consider the task difficulty and the vector of difficulty-score as the RL's action and state, respectively. We evaluated the proposed method through a within-subject experiment involving 52 subjects. The proposed approach was compared with two rule-based difficulty adjustment methods in terms of player's score and game experience measure
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;VIGC&#26694;&#26550;&#65292;&#20351;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#21644;&#32416;&#27491;&#35270;&#35273;&#25351;&#20196;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#39640;&#36136;&#37327;&#35843;&#25972;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.12714</link><description>&lt;p&gt;
VIGC: &#35270;&#35273;&#25351;&#20196;&#29983;&#25104;&#19982;&#32416;&#27491;
&lt;/p&gt;
&lt;p&gt;
VIGC: Visual Instruction Generation and Correction. (arXiv:2308.12714v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;VIGC&#26694;&#26550;&#65292;&#20351;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#21644;&#32416;&#27491;&#35270;&#35273;&#25351;&#20196;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#39640;&#36136;&#37327;&#35843;&#25972;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25972;&#21512;&#25512;&#21160;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#30340;&#39640;&#36136;&#37327;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#30340;&#31232;&#32570;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#24403;&#21069;&#30340;&#20027;&#23548;&#33539;&#24335;&#65292;&#22914;LLaVA&#65292;&#20381;&#36182;&#20110;&#20165;&#20351;&#29992;&#35821;&#35328;&#30340;GPT-4&#29983;&#25104;&#25968;&#25454;&#65292;&#36825;&#38656;&#35201;&#39044;&#27880;&#37322;&#30340;&#22270;&#20687;&#26631;&#39064;&#21644;&#26816;&#27979;&#21253;&#22260;&#26694;&#65292;&#23548;&#33268;&#23545;&#22270;&#20687;&#32454;&#33410;&#30340;&#29702;&#35299;&#19981;&#36275;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#23454;&#38469;&#26041;&#26696;&#26159;&#21033;&#29992;&#21487;&#29992;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#29983;&#25104;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#30340;&#25351;&#20196;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#24403;&#21069;&#21487;&#35775;&#38382;&#30340;MLLMs&#19981;&#20687;&#23427;&#20204;&#30340;LLM&#23545;&#24212;&#29289;&#37027;&#26679;&#24378;&#22823;&#65292;&#22240;&#20026;&#23427;&#20204;&#24448;&#24448;&#20135;&#29983;&#19981;&#36866;&#24403;&#30340;&#22238;&#24212;&#21644;&#29983;&#25104;&#38169;&#35823;&#20449;&#24687;&#12290;&#20316;&#20026;&#35299;&#20915;&#24403;&#21069;&#38382;&#39064;&#30340;&#26041;&#26696;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Visual Instruction Generation and Correction&#65288;VIGC&#65289;&#26694;&#26550;&#65292;&#20351;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#35270;&#35273;&#25351;&#20196;&#25968;&#25454;&#24182;&#32416;&#27491;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of visual encoders and large language models (LLMs) has driven recent progress in multimodal large language models (MLLMs). However, the scarcity of high-quality instruction-tuning data for vision-language tasks remains a challenge. The current leading paradigm, such as LLaVA, relies on language-only GPT-4 to generate data, which requires pre-annotated image captions and detection bounding boxes, suffering from understanding image details. A practical solution to this problem would be to utilize the available multimodal large language models (MLLMs) to generate instruction data for vision-language tasks. However, it's worth noting that the currently accessible MLLMs are not as powerful as their LLM counterparts, as they tend to produce inadequate responses and generate false information. As a solution for addressing the current issue, this paper proposes the Visual Instruction Generation and Correction (VIGC) framework that enables multimodal large language models to ge
&lt;/p&gt;</description></item><item><title>SayCanPay&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21644;&#21551;&#21457;&#24335;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#30340;&#19990;&#30028;&#30693;&#35782;&#21644;&#21551;&#21457;&#24335;&#25628;&#32034;&#30340;&#21407;&#21017;&#65292;&#29983;&#25104;&#21487;&#34892;&#30340;&#26368;&#20248;&#35745;&#21010;&#12290;</title><link>http://arxiv.org/abs/2308.12682</link><description>&lt;p&gt;
SayCanPay: &#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#39046;&#22495;&#30693;&#35782;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21551;&#21457;&#24335;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
SayCanPay: Heuristic Planning with Large Language Models using Learnable Domain Knowledge. (arXiv:2308.12682v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12682
&lt;/p&gt;
&lt;p&gt;
SayCanPay&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21644;&#21551;&#21457;&#24335;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#30340;&#19990;&#30028;&#30693;&#35782;&#21644;&#21551;&#21457;&#24335;&#25628;&#32034;&#30340;&#21407;&#21017;&#65292;&#29983;&#25104;&#21487;&#34892;&#30340;&#26368;&#20248;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36890;&#36807;&#20854;&#24222;&#22823;&#30340;"&#19990;&#30028;&#30693;&#35782;"&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35268;&#21010;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#33719;&#24471;&#26082;&#21487;&#34892;&#65288;&#22522;&#20110;&#21487;&#29992;&#24615;&#65289;&#21448;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#65288;&#35745;&#21010;&#38271;&#24230;&#26041;&#38754;&#65289;&#30340;&#35745;&#21010;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#36825;&#19982;&#21551;&#21457;&#24335;&#35268;&#21010;&#26041;&#27861;&#24418;&#25104;&#21453;&#24046;&#65292;&#21551;&#21457;&#24335;&#35268;&#21010;&#26041;&#27861;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;(&#22312;&#21160;&#20316;&#27169;&#22411;&#22914;PDDL&#20013;&#24418;&#24335;&#21270;)&#21644;&#21551;&#21457;&#24335;&#25628;&#32034;&#26469;&#29983;&#25104;&#21487;&#34892;&#30340;&#26368;&#20248;&#35745;&#21010;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#21644;&#21551;&#21457;&#24335;&#35268;&#21010;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#30340;&#19990;&#30028;&#30693;&#35782;&#21644;&#21551;&#21457;&#24335;&#25628;&#32034;&#30340;&#21407;&#21017;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;SayCanPay&#21033;&#29992;LLMs&#26469;&#29983;&#25104;&#30001;&#21487;&#23398;&#20064;&#30340;&#39046;&#22495;&#30693;&#35782;&#24341;&#23548;&#30340;&#21160;&#20316;(Say)&#65292;&#35780;&#20272;&#21160;&#20316;&#30340;&#21487;&#34892;&#24615;(Can)&#21644;&#38271;&#26399;&#22238;&#25253;/&#25910;&#30410;(Pay)&#65292;&#21033;&#29992;&#21551;&#21457;&#24335;&#25628;&#32034;&#26469;&#36873;&#25321;&#26368;&#20339;&#30340;&#21160;&#20316;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;(1)&#22312;&#21551;&#21457;&#24335;&#35268;&#21010;&#30340;&#32972;&#26223;&#19979;&#65292;&#23545;LLM&#35268;&#21010;&#38382;&#39064;&#36827;&#34892;&#20102;&#26032;&#39062;&#30340;&#26500;&#24314;&#65292;(2)&#25972;&#21512;&#20102;&#21487;&#29992;&#24615;&#21644;&#25104;&#26412;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated impressive planning abilities due to their vast "world knowledge". Yet, obtaining plans that are both feasible (grounded in affordances) and cost-effective (in plan length), remains a challenge, despite recent progress. This contrasts with heuristic planning methods that employ domain knowledge (formalized in action models such as PDDL) and heuristic search to generate feasible, optimal plans. Inspired by this, we propose to combine the power of LLMs and heuristic planning by leveraging the world knowledge of LLMs and the principles of heuristic search. Our approach, SayCanPay, employs LLMs to generate actions (Say) guided by learnable domain knowledge, that evaluates actions' feasibility (Can) and long-term reward/payoff (Pay), and heuristic search to select the best sequence of actions. Our contributions are (1) a novel framing of the LLM planning problem in the context of heuristic planning, (2) integrating grounding and cost-effective 
&lt;/p&gt;</description></item><item><title>LR-XFL&#26159;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36923;&#36753;&#35268;&#21017;&#21644;&#27169;&#22411;&#26356;&#26032;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#20102;&#23545;FL&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#25552;&#21319;&#21644;&#21152;&#26435;&#32858;&#21512;&#65292;&#24182;&#22312;&#30456;&#20851;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.12681</link><description>&lt;p&gt;
LR-XFL: &#22522;&#20110;&#36923;&#36753;&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LR-XFL: Logical Reasoning-based Explainable Federated Learning. (arXiv:2308.12681v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12681
&lt;/p&gt;
&lt;p&gt;
LR-XFL&#26159;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36923;&#36753;&#35268;&#21017;&#21644;&#27169;&#22411;&#26356;&#26032;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#20102;&#23545;FL&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#25552;&#21319;&#21644;&#21152;&#26435;&#32858;&#21512;&#65292;&#24182;&#22312;&#30456;&#20851;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064; (FL) &#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21327;&#20316;&#35757;&#32451;&#26041;&#27861;&#65292;&#33021;&#22815;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#38544;&#31169;&#20445;&#25252;&#30340;&#38656;&#27714;&#20351;&#24471;FL&#27169;&#22411;&#24456;&#38590;&#23454;&#29616;&#20840;&#23616;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36923;&#36753;&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#32852;&#37030;&#23398;&#20064; (LR-XFL) &#26041;&#27861;&#65292;&#23558;&#36923;&#36753;&#25512;&#29702;&#34701;&#20837;FL&#20013;&#12290;&#22312;LR-XFL&#20013;&#65292;FL&#23458;&#25143;&#31471;&#26681;&#25454;&#20854;&#26412;&#22320;&#25968;&#25454;&#21019;&#24314;&#26412;&#22320;&#36923;&#36753;&#35268;&#21017;&#65292;&#24182;&#23558;&#20854;&#19982;&#27169;&#22411;&#26356;&#26032;&#19968;&#36215;&#21457;&#36865;&#21040;FL&#26381;&#21153;&#22120;&#12290;FL&#26381;&#21153;&#22120;&#36890;&#36807;&#36866;&#24403;&#30340;&#36923;&#36753;&#36830;&#25509;&#31526;&#23558;&#26412;&#22320;&#36923;&#36753;&#35268;&#21017;&#36830;&#25509;&#36215;&#26469;&#65292;&#35813;&#36830;&#25509;&#31526;&#22522;&#20110;&#23458;&#25143;&#31471;&#25968;&#25454;&#30340;&#23646;&#24615;&#36827;&#34892;&#25512;&#23548;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#21407;&#22987;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#26381;&#21153;&#22120;&#36824;&#26681;&#25454;&#23458;&#25143;&#31471;&#19978;&#20256;&#30340;&#36923;&#36753;&#35268;&#21017;&#21453;&#26144;&#30340;&#26412;&#22320;&#25968;&#25454;&#30340;&#36136;&#37327;&#65292;&#20351;&#29992;&#26435;&#37325;&#20540;&#23545;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#36827;&#34892;&#32858;&#21512;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;LR-XFL&#22312;&#26368;&#30456;&#20851;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;1.19&#65285;&#65292;5.81&#65285;&#21644;5.41&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is an emerging approach for training machine learning models collaboratively while preserving data privacy. The need for privacy protection makes it difficult for FL models to achieve global transparency and explainability. To address this limitation, we incorporate logic-based explanations into FL by proposing the Logical Reasoning-based eXplainable Federated Learning (LR-XFL) approach. Under LR-XFL, FL clients create local logic rules based on their local data and send them, along with model updates, to the FL server. The FL server connects the local logic rules through a proper logical connector that is derived based on properties of client data, without requiring access to the raw data. In addition, the server also aggregates the local model updates with weight values determined by the quality of the clients' local data as reflected by their uploaded logic rules. The results show that LR-XFL outperforms the most relevant baseline by 1.19%, 5.81% and 5.41% in
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22686;&#24378;&#25351;&#20196;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#30495;&#23454;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SWIE&#65288;&#20998;&#27573;&#21152;&#26435;&#25351;&#20196;&#23884;&#20837;&#65289;&#21644;&#19968;&#20010;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#38598;OVERMISS&#65292;&#29992;&#20110;&#25913;&#21892;&#27169;&#22411;&#23545;&#25351;&#20196;&#30340;&#29702;&#35299;&#21644;&#25552;&#39640;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12674</link><description>&lt;p&gt;
&#36890;&#36807;&#22686;&#24378;&#25351;&#20196;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#30495;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Translation Faithfulness of Large Language Models via Augmenting Instructions. (arXiv:2308.12674v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12674
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22686;&#24378;&#25351;&#20196;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#30495;&#23454;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SWIE&#65288;&#20998;&#27573;&#21152;&#26435;&#25351;&#20196;&#23884;&#20837;&#65289;&#21644;&#19968;&#20010;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#38598;OVERMISS&#65292;&#29992;&#20110;&#25913;&#21892;&#27169;&#22411;&#23545;&#25351;&#20196;&#30340;&#29702;&#35299;&#21644;&#25552;&#39640;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20855;&#26377;&#24456;&#24378;&#30340;&#36890;&#29992;&#33021;&#21147;&#65292;&#30446;&#21069;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#25361;&#25112;&#26159;&#36890;&#36807;&#20302;&#25104;&#26412;&#30340;&#25351;&#20196;&#35843;&#25972;&#26469;&#28608;&#21457;&#23427;&#20204;&#30340;&#19987;&#38376;&#33021;&#21147;&#65292;&#22914;&#26426;&#22120;&#32763;&#35793;&#12290;&#26631;&#20934;&#30340;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#26159;&#25353;&#39034;&#24207;&#32452;&#32455;&#30340;&#65292;&#21253;&#25324;&#25351;&#20196;&#12289;&#36755;&#20837;&#21644;&#21709;&#24212;&#30340;&#36830;&#25509;&#12290;&#30001;&#20110;LLMs&#30340;&#27880;&#24847;&#26426;&#21046;&#22312;&#23616;&#37096;&#20851;&#27880;&#19978;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;LLMs&#20542;&#21521;&#20110;&#22312;&#27599;&#20010;&#20301;&#32622;&#26356;&#22810;&#22320;&#20851;&#27880;&#38468;&#36817;&#30340;&#21333;&#35789;&#25110;&#21477;&#23376;&#12290;&#36825;&#23548;&#33268;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#36951;&#24536;&#25351;&#20196;&#30340;&#39118;&#38505;&#24456;&#39640;&#12290;&#20026;&#20102;&#32531;&#35299;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SWIE&#65288;&#20998;&#27573;&#21152;&#26435;&#25351;&#20196;&#23884;&#20837;&#65289;&#21644;&#19968;&#20010;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#38598;OVERMISS&#12290;SWIE&#36890;&#36807;&#22312;&#21518;&#32493;&#30340;&#36755;&#20837;&#21644;&#21709;&#24212;&#34920;&#31034;&#19978;&#28155;&#21152;&#20840;&#23616;&#25351;&#20196;&#34920;&#31034;&#26469;&#25913;&#21892;&#27169;&#22411;&#23545;&#25351;&#20196;&#30340;&#29702;&#35299;&#12290;OVERMISS&#36890;&#36807;&#23558;&#36807;&#24230;&#32763;&#35793;&#21644;&#36951;&#28431;&#32763;&#35793;&#32467;&#26524;&#19982;&#27491;&#30830;&#32763;&#35793;&#36827;&#34892;&#27604;&#36739;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#21040;&#20004;&#20010;&#20027;&#35201;&#30340;&#32763;&#35793;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) present strong general capabilities, and a current compelling challenge is stimulating their specialized capabilities, such as machine translation, through low-cost instruction tuning. The standard instruction-following data is sequentially organized as the concatenation of an instruction, an input, and a response. As the attention mechanism of LLMs has limitations on local focus, LLMs tend to focus more on the words or sentences nearby at each position. This leads to a high risk of instruction forgetting during decoding. To alleviate the above issues, We propose SWIE (Segment-Weighted Instruction Embedding) and an instruction-following dataset OVERMISS. SWIE improves the model instruction understanding by adding a global instruction representation on the following input and response representations. OVERMISS improves model faithfulness by comparing over-translation and miss-translation results with the correct translation. We apply our methods to two main-
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#22270;&#20687;&#36827;&#34892;&#26333;&#20809;&#25915;&#20987;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#35777;&#26126;&#35813;&#25915;&#20987;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12661</link><description>&lt;p&gt;
&#19981;&#35201;&#26395;&#21521;&#22826;&#38451;&#65306;&#23545;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#24615;&#26333;&#20809;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Don't Look into the Sun: Adversarial Solarization Attacks on Image Classifiers. (arXiv:2308.12661v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12661
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#22270;&#20687;&#36827;&#34892;&#26333;&#20809;&#25915;&#20987;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#35777;&#26126;&#35813;&#25915;&#20987;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#36755;&#20837;&#30340;&#40065;&#26834;&#24615;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#20687;&#33258;&#21160;&#39550;&#39542;&#36825;&#26679;&#30340;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#65292;&#20294;&#20063;&#36866;&#29992;&#20110;&#24694;&#24847;&#25915;&#20987;&#32773;&#21487;&#20197;&#25968;&#23383;&#22320;&#20462;&#25913;&#36755;&#20837;&#20197;&#32469;&#36807;&#23433;&#20840;&#20445;&#25252;&#30340;&#23433;&#20840;&#31995;&#32479;&#20013;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#26377;&#25928;&#30340;&#36229;&#20986;&#20998;&#24067;&#27979;&#35797;&#65292;&#22312;&#20445;&#25345;&#20934;&#30830;&#30340;&#26631;&#31614;&#20449;&#24687;&#30340;&#21516;&#26102;&#28085;&#30422;&#25152;&#26377;&#21487;&#33021;&#30340;&#22330;&#26223;&#65292;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#24448;&#24448;&#38656;&#35201;&#22312;&#25915;&#20987;&#30340;&#22810;&#26679;&#24615;&#21644;&#38480;&#21046;&#27700;&#24179;&#20043;&#38388;&#20570;&#20986;&#22949;&#21327;&#65292;&#26377;&#26102;&#29978;&#33267;&#20004;&#32773;&#20860;&#32780;&#26377;&#20043;&#12290;&#20316;&#20026;&#23545;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#26356;&#20840;&#38754;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#30340;&#31532;&#19968;&#27493;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#26333;&#20809;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#27010;&#24565;&#19978;&#31616;&#21333;&#26126;&#20102;&#65292;&#20294;&#21448;&#33021;&#36991;&#20813;&#23616;&#37096;&#33539;&#22260;&#20869;&#30340;&#33258;&#28982;&#22270;&#20687;&#30340;&#20840;&#23616;&#32467;&#26500;&#21463;&#21040;&#25439;&#23475;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;ImageNet&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#35813;&#25915;&#20987;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#20934;&#30830;&#24615;&#65292;&#21069;&#25552;&#26159;&#23427;&#27809;&#26377;&#38598;&#25104;&#21040;
&lt;/p&gt;
&lt;p&gt;
Assessing the robustness of deep neural networks against out-of-distribution inputs is crucial, especially in safety-critical domains like autonomous driving, but also in safety systems where malicious actors can digitally alter inputs to circumvent safety guards. However, designing effective out-of-distribution tests that encompass all possible scenarios while preserving accurate label information is a challenging task. Existing methodologies often entail a compromise between variety and constraint levels for attacks and sometimes even both. In a first step towards a more holistic robustness evaluation of image classification models, we introduce an attack method based on image solarization that is conceptually straightforward yet avoids jeopardizing the global structure of natural images independent of the intensity. Through comprehensive evaluations of multiple ImageNet models, we demonstrate the attack's capacity to degrade accuracy significantly, provided it is not integrated into
&lt;/p&gt;</description></item><item><title>kTrans&#26159;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#30340;&#20108;&#36827;&#21046;&#20195;&#30721;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#26174;&#24335;&#30693;&#35782;&#21644;&#38544;&#24335;&#30693;&#35782;&#19982;Transformer&#27169;&#22411;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#23558;&#39046;&#22495;&#30693;&#35782;&#34701;&#20837;Transformer&#26694;&#26550;&#30340;&#26032;&#35270;&#35282;&#12290;&#23427;&#22312;&#20108;&#36827;&#21046;&#20195;&#30721;&#30456;&#20284;&#24615;&#26816;&#27979;&#12289;&#20989;&#25968;&#31867;&#22411;&#24674;&#22797;&#21644;&#38388;&#25509;&#35843;&#29992;&#35782;&#21035;&#31561;&#20219;&#21153;&#20013;&#20855;&#26377;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.12659</link><description>&lt;p&gt;
kTrans:&#30693;&#35782;&#24863;&#30693;&#30340;&#20108;&#36827;&#21046;&#20195;&#30721;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
kTrans: Knowledge-Aware Transformer for Binary Code Embedding. (arXiv:2308.12659v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12659
&lt;/p&gt;
&lt;p&gt;
kTrans&#26159;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#30340;&#20108;&#36827;&#21046;&#20195;&#30721;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#26174;&#24335;&#30693;&#35782;&#21644;&#38544;&#24335;&#30693;&#35782;&#19982;Transformer&#27169;&#22411;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#23558;&#39046;&#22495;&#30693;&#35782;&#34701;&#20837;Transformer&#26694;&#26550;&#30340;&#26032;&#35270;&#35282;&#12290;&#23427;&#22312;&#20108;&#36827;&#21046;&#20195;&#30721;&#30456;&#20284;&#24615;&#26816;&#27979;&#12289;&#20989;&#25968;&#31867;&#22411;&#24674;&#22797;&#21644;&#38388;&#25509;&#35843;&#29992;&#35782;&#21035;&#31561;&#20219;&#21153;&#20013;&#20855;&#26377;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#36827;&#21046;&#20195;&#30721;&#23884;&#20837;(BCE)&#22312;&#21508;&#31181;&#36870;&#21521;&#24037;&#31243;&#20219;&#21153;&#20013;&#26377;&#37325;&#35201;&#24212;&#29992;&#65292;&#22914;&#20108;&#36827;&#21046;&#20195;&#30721;&#30456;&#20284;&#24615;&#26816;&#27979;&#12289;&#31867;&#22411;&#24674;&#22797;&#12289;&#25511;&#21046;&#27969;&#24674;&#22797;&#21644;&#25968;&#25454;&#27969;&#20998;&#26512;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;Transformer&#27169;&#22411;&#21487;&#20197;&#29702;&#35299;&#20108;&#36827;&#21046;&#20195;&#30721;&#30340;&#35821;&#20041;&#20197;&#25903;&#25345;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#24573;&#35270;&#20102;&#27719;&#32534;&#35821;&#35328;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#65292;&#21363;kTrans&#65292;&#29992;&#20110;&#29983;&#25104;&#30693;&#35782;&#24863;&#30693;&#30340;&#20108;&#36827;&#21046;&#20195;&#30721;&#23884;&#20837;&#12290;&#36890;&#36807;&#23558;&#26174;&#24335;&#30693;&#35782;&#20316;&#20026;&#39069;&#22806;&#30340;&#36755;&#20837;&#39304;&#36865;&#32473;Transformer&#65292;&#24182;&#23558;&#38544;&#24335;&#30693;&#35782;&#19982;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#34701;&#21512;&#65292;kTrans&#20026;&#23558;&#39046;&#22495;&#30693;&#35782;&#21512;&#24182;&#21040;Transformer&#26694;&#26550;&#20013;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;&#25105;&#20204;&#29992;&#31163;&#32676;&#20540;&#26816;&#27979;&#21644;&#21487;&#35270;&#21270;&#26469;&#26816;&#26597;&#29983;&#25104;&#30340;&#23884;&#20837;&#65292;&#24182;&#23558;kTrans&#24212;&#29992;&#20110;3&#20010;&#19979;&#28216;&#20219;&#21153;&#65306;&#20108;&#36827;&#21046;&#20195;&#30721;&#30456;&#20284;&#24615;&#26816;&#27979;(BCSD)&#12289;&#20989;&#25968;&#31867;&#22411;&#24674;&#22797;(FTR)&#21644;&#38388;&#25509;&#35843;&#29992;&#35782;&#21035;(ICR)&#12290;
&lt;/p&gt;
&lt;p&gt;
Binary Code Embedding (BCE) has important applications in various reverse engineering tasks such as binary code similarity detection, type recovery, control-flow recovery and data-flow analysis. Recent studies have shown that the Transformer model can comprehend the semantics of binary code to support downstream tasks. However, existing models overlooked the prior knowledge of assembly language. In this paper, we propose a novel Transformer-based approach, namely kTrans, to generate knowledge-aware binary code embedding. By feeding explicit knowledge as additional inputs to the Transformer, and fusing implicit knowledge with a novel pre-training task, kTrans provides a new perspective to incorporating domain knowledge into a Transformer framework. We inspect the generated embeddings with outlier detection and visualization, and also apply kTrans to 3 downstream tasks: Binary Code Similarity Detection (BCSD), Function Type Recovery (FTR) and Indirect Call Recognition (ICR). Evaluation r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;APART&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20840;&#32452;&#23545;&#21028;&#21035;&#22120;&#12289;&#26032;&#39062;&#30340;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#21644;&#20002;&#24323;&#25216;&#26415;&#65292;&#22312;&#26080;&#22870;&#21169;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#22810;&#26679;&#21270;&#25216;&#33021;&#30340;&#21457;&#29616;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#65292;&#24182;&#22312;&#31616;&#21333;&#30340;&#32593;&#26684;&#19990;&#30028;&#29615;&#22659;&#20013;&#21457;&#29616;&#20102;&#25152;&#26377;&#21487;&#33021;&#30340;&#25216;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12649</link><description>&lt;p&gt;
APART: &#20351;&#29992;&#26377;&#21319;&#24207;&#22870;&#21169;&#21644;&#20002;&#24323;&#25216;&#26415;&#30340;&#20840;&#32452;&#23545;&#22810;&#26679;&#21270;&#25216;&#33021;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
APART: Diverse Skill Discovery using All Pairs with Ascending Reward and DropouT. (arXiv:2308.12649v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;APART&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20840;&#32452;&#23545;&#21028;&#21035;&#22120;&#12289;&#26032;&#39062;&#30340;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#21644;&#20002;&#24323;&#25216;&#26415;&#65292;&#22312;&#26080;&#22870;&#21169;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#22810;&#26679;&#21270;&#25216;&#33021;&#30340;&#21457;&#29616;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#65292;&#24182;&#22312;&#31616;&#21333;&#30340;&#32593;&#26684;&#19990;&#30028;&#29615;&#22659;&#20013;&#21457;&#29616;&#20102;&#25152;&#26377;&#21487;&#33021;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#26080;&#22870;&#21169;&#29615;&#22659;&#20013;&#30340;&#22810;&#26679;&#21270;&#25216;&#33021;&#21457;&#29616;&#65292;&#22312;&#31616;&#21333;&#30340;&#32593;&#26684;&#19990;&#30028;&#29615;&#22659;&#20013;&#21457;&#29616;&#25152;&#26377;&#21487;&#33021;&#30340;&#25216;&#33021;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#24456;&#38590;&#25104;&#21151;&#12290;&#36825;&#20010;&#38382;&#39064;&#34987;&#21046;&#23450;&#20026;&#20351;&#29992;&#20869;&#22312;&#22870;&#21169;&#21644;&#19968;&#20010;&#32463;&#36807;&#35757;&#32451;&#30340;&#21028;&#21035;&#22120;&#26469;&#30456;&#20114;&#35757;&#32451;&#25216;&#33021;&#20197;&#39044;&#27979;&#32473;&#23450;&#36712;&#36857;&#30340;&#25216;&#33021;&#12290;&#25105;&#20204;&#30340;&#21021;&#22987;&#35299;&#20915;&#26041;&#26696;&#29992;&#20840;&#32452;&#23545;&#65288;all pairs&#65289;&#21028;&#21035;&#22120;&#26367;&#25442;&#20102;&#26631;&#20934;&#30340;&#19968;&#23545;&#22810;&#65288;softmax&#65289;&#21028;&#21035;&#22120;&#65292;&#24182;&#32467;&#21512;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#21644;&#20002;&#24323;&#65288;dropout&#65289;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;&#36825;&#31181;&#32508;&#21512;&#26041;&#27861;&#34987;&#21629;&#21517;&#20026;APART: &#20351;&#29992;&#26377;&#21319;&#24207;&#22870;&#21169;&#21644;&#20002;&#24323;&#25216;&#26415;&#30340;&#20840;&#32452;&#23545;&#22810;&#26679;&#21270;&#25216;&#33021;&#21457;&#29616;&#12290;&#25105;&#20204;&#35777;&#26126;APART&#27604;&#20808;&#21069;&#30340;&#26041;&#27861;&#26356;&#23569;&#26679;&#26412;&#23601;&#33021;&#21457;&#29616;&#32593;&#26684;&#19990;&#30028;&#20013;&#25152;&#26377;&#21487;&#33021;&#30340;&#25216;&#33021;&#12290;&#21463;&#21040;APART&#30340;&#23454;&#35777;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#25913;&#21464;VIC&#65292;&#37325;&#26032;&#35843;&#25972;&#20854;&#20869;&#22312;&#22870;&#21169;&#65292;&#24182;&#35843;&#33410;&#20854;softmax&#28201;&#24230;&#26469;&#23454;&#29616;&#26368;&#22823;&#21270;&#25216;&#33021;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study diverse skill discovery in reward-free environments, aiming to discover all possible skills in simple grid-world environments where prior methods have struggled to succeed. This problem is formulated as mutual training of skills using an intrinsic reward and a discriminator trained to predict a skill given its trajectory. Our initial solution replaces the standard one-vs-all (softmax) discriminator with a one-vs-one (all pairs) discriminator and combines it with a novel intrinsic reward function and a dropout regularization technique. The combined approach is named APART: Diverse Skill Discovery using All Pairs with Ascending Reward and Dropout. We demonstrate that APART discovers all the possible skills in grid worlds with remarkably fewer samples than previous works. Motivated by the empirical success of APART, we further investigate an even simpler algorithm that achieves maximum skills by altering VIC, rescaling its intrinsic reward, and tuning the temperature of its softm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#22871;&#24037;&#19994;&#32423;&#21256;&#29273;&#21033;&#25991;&#26412;&#22788;&#29702;&#27169;&#22411;&#65292;&#21033;&#29992;HuSpaCy&#26694;&#26550;&#23454;&#29616;&#65292;&#36890;&#36807;&#22810;&#39033;&#25913;&#36827;&#22312;&#36164;&#28304;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#27169;&#22411;&#20855;&#22791;&#39640;&#20934;&#30830;&#24615;&#21644;&#21534;&#21520;&#37327;&#65292;&#24182;&#22312;&#25152;&#26377;&#22522;&#26412;&#25991;&#26412;&#22788;&#29702;&#27493;&#39588;&#20013;&#23637;&#31034;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12635</link><description>&lt;p&gt;
&#20351;&#29992;HuSpaCy&#25512;&#36827;&#21256;&#29273;&#21033;&#25991;&#26412;&#22788;&#29702;&#65306;&#39640;&#25928;&#20934;&#30830;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31649;&#36947;
&lt;/p&gt;
&lt;p&gt;
Advancing Hungarian Text Processing with HuSpaCy: Efficient and Accurate NLP Pipelines. (arXiv:2308.12635v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#22871;&#24037;&#19994;&#32423;&#21256;&#29273;&#21033;&#25991;&#26412;&#22788;&#29702;&#27169;&#22411;&#65292;&#21033;&#29992;HuSpaCy&#26694;&#26550;&#23454;&#29616;&#65292;&#36890;&#36807;&#22810;&#39033;&#25913;&#36827;&#22312;&#36164;&#28304;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#27169;&#22411;&#20855;&#22791;&#39640;&#20934;&#30830;&#24615;&#21644;&#21534;&#21520;&#37327;&#65292;&#24182;&#22312;&#25152;&#26377;&#22522;&#26412;&#25991;&#26412;&#22788;&#29702;&#27493;&#39588;&#20013;&#23637;&#31034;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#22871;&#29992;&#20110;&#21256;&#29273;&#21033;&#25991;&#26412;&#22788;&#29702;&#30340;&#24037;&#19994;&#32423;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#36164;&#28304;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#27169;&#22411;&#26159;&#22522;&#20110;spaCy&#26694;&#26550;&#23454;&#29616;&#30340;&#65292;&#22312;HuSpaCy&#24037;&#20855;&#21253;&#30340;&#26550;&#26500;&#19978;&#36827;&#34892;&#20102;&#22810;&#20010;&#25913;&#36827;&#12290;&#19982;&#29616;&#26377;&#30340;&#21256;&#29273;&#21033;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#30456;&#27604;&#65292;&#25105;&#20204;&#25152;&#26377;&#30340;&#31649;&#36947;&#37117;&#20855;&#22791;&#21253;&#25324;&#26631;&#35760;&#21270;&#12289;&#21477;&#23376;&#36793;&#30028;&#26816;&#27979;&#12289;&#35789;&#24615;&#26631;&#27880;&#12289;&#35789;&#24418;&#29305;&#24449;&#26631;&#27880;&#12289;&#35789;&#24418;&#36824;&#21407;&#12289;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#22312;&#20869;&#30340;&#25152;&#26377;&#22522;&#26412;&#25991;&#26412;&#22788;&#29702;&#27493;&#39588;&#65292;&#24182;&#19988;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#21644;&#39640;&#21534;&#21520;&#37327;&#12290;&#25105;&#20204;&#23545;&#25552;&#20986;&#30340;&#25913;&#36827;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#23558;&#31649;&#36947;&#19982;&#26368;&#20808;&#36827;&#30340;&#24037;&#20855;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#22312;&#25152;&#26377;&#25991;&#26412;&#39044;&#22788;&#29702;&#27493;&#39588;&#20013;&#23637;&#31034;&#20102;&#26032;&#27169;&#22411;&#30340;&#31454;&#20105;&#24615;&#33021;&#12290;&#25152;&#26377;&#23454;&#39564;&#37117;&#21487;&#20197;&#37325;&#29616;&#65292;&#24182;&#19988;&#36825;&#20123;&#31649;&#36947;&#21487;&#20197;&#20813;&#36153;&#20351;&#29992;&#24182;&#37319;&#29992;&#23485;&#26494;&#30340;&#35768;&#21487;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a set of industrial-grade text processing models for Hungarian that achieve near state-of-the-art performance while balancing resource efficiency and accuracy. Models have been implemented in the spaCy framework, extending the HuSpaCy toolkit with several improvements to its architecture. Compared to existing NLP tools for Hungarian, all of our pipelines feature all basic text processing steps including tokenization, sentence-boundary detection, part-of-speech tagging, morphological feature tagging, lemmatization, dependency parsing and named entity recognition with high accuracy and throughput. We thoroughly evaluated the proposed enhancements, compared the pipelines with state-of-the-art tools and demonstrated the competitive performance of the new models in all text preprocessing steps. All experiments are reproducible and the pipelines are freely available under a permissive license.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21306;&#22495;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#34701;&#21512;&#21306;&#22495;&#34917;&#19969;&#20449;&#24687;&#20197;&#24471;&#20986;&#28369;&#29255;&#32423;&#21035;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#22534;&#21472;&#21306;&#22495;&#32858;&#21512;&#26469;&#20998;&#23618;&#22788;&#29702;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#32858;&#28966;&#20110;&#39640;&#20851;&#27880;&#21306;&#22495;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.12634</link><description>&lt;p&gt;
&#38754;&#21521;&#20998;&#23618;&#21306;&#22495;&#21464;&#21387;&#22120;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Hierarchical Regional Transformer-based Multiple Instance Learning. (arXiv:2308.12634v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21306;&#22495;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#34701;&#21512;&#21306;&#22495;&#34917;&#19969;&#20449;&#24687;&#20197;&#24471;&#20986;&#28369;&#29255;&#32423;&#21035;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#22534;&#21472;&#21306;&#22495;&#32858;&#21512;&#26469;&#20998;&#23618;&#22788;&#29702;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#32858;&#28966;&#20110;&#39640;&#20851;&#27880;&#21306;&#22495;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#30149;&#29702;&#23398;&#21644;&#31934;&#30830;&#21307;&#23398;&#20013;&#65292;&#20351;&#29992;&#28145;&#24230;&#22810;&#23454;&#20363;&#23398;&#20064;&#27169;&#22411;&#23545;&#24040;&#20687;&#32032;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#24050;&#25104;&#20026;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#29992;&#21306;&#22495;&#24615;&#30340;&#12289;&#21463;&#21040;&#35270;&#35273;&#21464;&#21387;&#22120;&#21551;&#21457;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#26367;&#20195;&#20102;&#20256;&#32479;&#30340;&#23398;&#20064;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#21306;&#22495;&#34917;&#19969;&#20449;&#24687;&#20197;&#24471;&#20986;&#28369;&#29255;&#32423;&#21035;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#22534;&#21472;&#36825;&#31181;&#21306;&#22495;&#32858;&#21512;&#20197;&#20998;&#23618;&#22320;&#22788;&#29702;&#19981;&#21516;&#36317;&#31163;&#27700;&#24179;&#19978;&#30340;&#29305;&#24449;&#12290;&#20026;&#20102;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20855;&#26377;&#23567;&#30340;&#23616;&#37096;&#24418;&#24577;&#29305;&#24449;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#25512;&#29702;&#26399;&#38388;&#23558;&#22270;&#20687;&#22788;&#29702;&#38598;&#20013;&#22312;&#39640;&#20851;&#27880;&#21306;&#22495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#20004;&#20010;&#32452;&#32455;&#30149;&#29702;&#23398;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#24182;&#25351;&#21521;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The classification of gigapixel histopathology images with deep multiple instance learning models has become a critical task in digital pathology and precision medicine. In this work, we propose a Transformer-based multiple instance learning approach that replaces the traditional learned attention mechanism with a regional, Vision Transformer inspired self-attention mechanism. We present a method that fuses regional patch information to derive slide-level predictions and show how this regional aggregation can be stacked to hierarchically process features on different distance levels. To increase predictive accuracy, especially for datasets with small, local morphological features, we introduce a method to focus the image processing on high attention regions during inference. Our approach is able to significantly improve performance over the baseline on two histopathology datasets and points towards promising directions for further research.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35299;&#20915;&#22871;&#39184;&#20248;&#21270;&#38382;&#39064;&#30340;&#26032;&#39062;&#32452;&#21512;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#38024;&#23545;&#30005;&#20449;&#36816;&#33829;&#21830;&#22312;&#36873;&#25321;&#28608;&#21169;&#22871;&#39184;&#21644;&#30446;&#26631;&#29992;&#25143;&#26102;&#38754;&#20020;&#30340;&#22256;&#38590;&#36827;&#34892;&#20102;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2308.12606</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#21521;&#30005;&#20449;&#29992;&#25143;&#25552;&#20379;&#30340;&#36138;&#24515;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Greedy Approach for Offering to Telecom Subscribers. (arXiv:2308.12606v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35299;&#20915;&#22871;&#39184;&#20248;&#21270;&#38382;&#39064;&#30340;&#26032;&#39062;&#32452;&#21512;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#38024;&#23545;&#30005;&#20449;&#36816;&#33829;&#21830;&#22312;&#36873;&#25321;&#28608;&#21169;&#22871;&#39184;&#21644;&#30446;&#26631;&#29992;&#25143;&#26102;&#38754;&#20020;&#30340;&#22256;&#38590;&#36827;&#34892;&#20102;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23458;&#25143;&#20445;&#30041;&#25110;&#20943;&#23569;&#27969;&#22833;&#26159;&#30005;&#20449;&#36816;&#33829;&#21830;&#38754;&#20020;&#30340;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20854;&#20013;&#19968;&#20010;&#26377;&#25928;&#30340;&#26041;&#27861;&#26159;&#21521;&#29992;&#25143;&#25552;&#20379;&#19968;&#20123;&#26377;&#21560;&#24341;&#21147;&#30340;&#28608;&#21169;&#25514;&#26045;&#25110;&#38468;&#21152;&#26381;&#21153;&#25110;&#37329;&#38065;&#65292;&#20197;&#20445;&#25345;&#20182;&#20204;&#30340;&#21442;&#19982;&#24182;&#30830;&#20445;&#20182;&#20204;&#22312;&#36816;&#33829;&#21830;&#30340;&#32593;&#32476;&#20013;&#20572;&#30041;&#26356;&#38271;&#26102;&#38388;&#12290;&#36890;&#24120;&#65292;&#36816;&#33829;&#21830;&#20250;&#20998;&#37197;&#19968;&#23450;&#37329;&#39069;&#30340;&#39044;&#31639;&#26469;&#36827;&#34892;&#25512;&#24191;&#27963;&#21160;&#12290;&#36825;&#39033;&#27963;&#21160;&#30340;&#22256;&#38590;&#20043;&#22788;&#22312;&#20110;&#20174;&#24222;&#22823;&#30340;&#35746;&#25143;&#32676;&#20307;&#20013;&#36873;&#25321;&#19968;&#32452;&#23458;&#25143;&#65292;&#24182;&#20915;&#23450;&#24212;&#35813;&#21521;&#20010;&#20307;&#25552;&#20379;&#22810;&#23569;&#37329;&#39069;&#65292;&#20197;&#23454;&#29616;&#36816;&#33829;&#21830;&#30340;&#30446;&#26631;&#12290;&#36873;&#25321;&#35746;&#25143;&#21644;&#36873;&#25321;&#25552;&#20379;&#32473;&#34987;&#36873;&#23450;&#35746;&#25143;&#30340;&#22871;&#39184;&#21487;&#33021;&#26377;&#22810;&#20010;&#30446;&#26631;&#65288;&#20363;&#22914;&#65292;&#26368;&#22823;&#21270;&#25910;&#20837;&#65292;&#26368;&#23567;&#21270;&#27969;&#22833;&#25968;&#37327;&#65289;&#12290;&#38500;&#20102;&#37329;&#38065;&#21033;&#30410;&#65292;&#22871;&#39184;&#36824;&#21487;&#20197;&#21253;&#25324;&#39069;&#22806;&#30340;&#25968;&#25454;&#12289;&#30701;&#20449;&#12289;&#25163;&#26426;&#28909;&#28857;&#20849;&#20139;&#31561;&#31561;&#12290;&#36825;&#20010;&#38382;&#39064;&#34987;&#31216;&#20026;&#22871;&#39184;&#20248;&#21270;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32452;&#21512;&#31639;&#27861;&#26469;&#35299;&#20915;&#22871;&#39184;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Customer retention or churn prevention is a challenging task of a telecom operator. One of the effective approaches is to offer some attractive incentive or additional services or money to the subscribers for keeping them engaged and make sure they stay in the operator's network for longer time. Often, operators allocate certain amount of monetary budget to carry out the offer campaign. The difficult part of this campaign is the selection of a set of customers from a large subscriber-base and deciding the amount that should be offered to an individual so that operator's objective is achieved. There may be multiple objectives (e.g., maximizing revenue, minimizing number of churns) for selection of subscriber and selection of an offer to the selected subscriber. Apart from monetary benefit, offers may include additional data, SMS, hots-spot tethering, and many more. This problem is known as offer optimization. In this paper, we propose a novel combinatorial algorithm for solving offer op
&lt;/p&gt;</description></item><item><title>APLA&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#32593;&#32476;&#32467;&#26500;&#65292;&#36890;&#36807;&#24341;&#20837;&#38468;&#21152;&#25668;&#21160;&#30340;&#23618;&#22122;&#22768;&#19982;&#23545;&#25239;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#35270;&#39057;&#29983;&#25104;&#20013;&#19968;&#33268;&#24615;&#32454;&#33410;&#20002;&#22833;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.12605</link><description>&lt;p&gt;
APLA: &#38468;&#21152;&#25668;&#21160;&#30340;&#23618;&#22122;&#22768;&#19982;&#23545;&#25239;&#35757;&#32451;&#20351;&#19968;&#33268;&#24615;&#25104;&#20026;&#21487;&#33021;
&lt;/p&gt;
&lt;p&gt;
APLA: Additional Perturbation for Latent Noise with Adversarial Training Enables Consistency. (arXiv:2308.12605v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12605
&lt;/p&gt;
&lt;p&gt;
APLA&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#32593;&#32476;&#32467;&#26500;&#65292;&#36890;&#36807;&#24341;&#20837;&#38468;&#21152;&#25668;&#21160;&#30340;&#23618;&#22122;&#22768;&#19982;&#23545;&#25239;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#35270;&#39057;&#29983;&#25104;&#20013;&#19968;&#33268;&#24615;&#32454;&#33410;&#20002;&#22833;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#35270;&#39057;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#32463;&#24120;&#38590;&#20197;&#22312;&#24103;&#20043;&#38388;&#20445;&#30041;&#23616;&#37096;&#21306;&#22495;&#30340;&#19968;&#33268;&#32454;&#33410;&#12290;&#20854;&#20013;&#19968;&#20010;&#28508;&#22312;&#21407;&#22240;&#26159;&#20256;&#32479;&#25193;&#25955;&#27169;&#22411;&#22312;&#36924;&#36817;&#39640;&#26031;&#22122;&#22768;&#20998;&#24067;&#26102;&#21033;&#29992;&#20102;&#39044;&#27979;&#22122;&#22768;&#65292;&#27809;&#26377;&#20805;&#20998;&#32771;&#34385;&#36755;&#20837;&#26412;&#36523;&#30340;&#20869;&#22312;&#20449;&#24687;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#24378;&#35843;&#39044;&#27979;&#21644;&#21442;&#32771;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#24573;&#35270;&#20102;&#35270;&#39057;&#26412;&#36523;&#22266;&#26377;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#21463;&#21040;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#65288;T2V&#65289;&#29983;&#25104;&#32593;&#32476;&#32467;&#26500;&#65292;&#21517;&#20026;&#38468;&#21152;&#25668;&#21160;&#30340;&#23618;&#22122;&#22768;&#19982;&#23545;&#25239;&#35757;&#32451;&#65288;APLA&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#38656;&#35201;&#19968;&#20010;&#35270;&#39057;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#24314;&#31435;&#22312;&#39044;&#35757;&#32451;&#31283;&#23450;&#30340;&#25193;&#25955;&#32593;&#32476;&#19978;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#32039;&#20945;&#32593;&#32476;&#65292;&#31216;&#20026;&#35270;&#39057;&#29983;&#25104;&#21464;&#25442;&#22120;&#65288;VGT&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have exhibited promising progress in video generation. However, they often struggle to retain consistent details within local regions across frames. One underlying cause is that traditional diffusion models approximate Gaussian noise distribution by utilizing predictive noise, without fully accounting for the impact of inherent information within the input itself. Additionally, these models emphasize the distinction between predictions and references, neglecting information intrinsic to the videos. To address this limitation, inspired by the self-attention mechanism, we propose a novel text-to-video (T2V) generation network structure based on diffusion models, dubbed Additional Perturbation for Latent noise with Adversarial training (APLA). Our approach only necessitates a single video as input and builds upon pre-trained stable diffusion networks. Notably, we introduce an additional compact network, known as the Video Generation Transformer (VGT). This auxiliary compo
&lt;/p&gt;</description></item><item><title>SICNN&#26159;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22343;&#34913;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#23637;&#24320;&#22522;&#20110;&#27169;&#22411;&#30340;&#36845;&#20195;SIC&#26041;&#27861;&#26469;&#35774;&#35745;&#65292;&#28040;&#38500;&#20102;&#22522;&#20110;&#27169;&#22411;&#26041;&#27861;&#30340;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.12591</link><description>&lt;p&gt;
SICNN: &#21463;&#36719;&#24178;&#25200;&#25269;&#28040;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#22343;&#34913;&#22120;
&lt;/p&gt;
&lt;p&gt;
SICNN: Soft Interference Cancellation Inspired Neural Network Equalizers. (arXiv:2308.12591v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12591
&lt;/p&gt;
&lt;p&gt;
SICNN&#26159;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22343;&#34913;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#23637;&#24320;&#22522;&#20110;&#27169;&#22411;&#30340;&#36845;&#20195;SIC&#26041;&#27861;&#26469;&#35774;&#35745;&#65292;&#28040;&#38500;&#20102;&#22522;&#20110;&#27169;&#22411;&#26041;&#27861;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22343;&#34913;&#26159;&#25968;&#23383;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#25509;&#25910;&#31471;&#30340;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#65292;&#20256;&#32479;&#19978;&#20351;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#20272;&#35745;&#26041;&#27861;&#26469;&#36827;&#34892;&#12290;&#22312;&#20247;&#22810;&#36873;&#39033;&#20013;&#65292;&#36845;&#20195;&#36719;&#24178;&#25200;&#25269;&#28040;&#65288;SIC&#65289;&#26159;&#19968;&#31181;&#34920;&#29616;&#33391;&#22909;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#36991;&#20813;&#20102;&#36845;&#20195;&#20272;&#35745;&#36807;&#31243;&#20013;&#30001;&#30828;&#20915;&#31574;&#25968;&#25454;&#31526;&#21495;&#20272;&#35745;&#24341;&#36215;&#30340;&#38169;&#35823;&#20256;&#25773;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#23384;&#22312;&#39640;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#24615;&#33021;&#38477;&#20302;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#38656;&#35201;&#36827;&#34892;&#36924;&#36817;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;&#22343;&#34913;&#26041;&#27861;&#65292;&#31216;&#20026;SICNN&#65292;&#36890;&#36807;&#23545;&#22522;&#20110;&#27169;&#22411;&#30340;&#36845;&#20195;SIC&#26041;&#27861;&#36827;&#34892;&#28145;&#24230;&#23637;&#24320;&#26469;&#35774;&#35745;&#65292;&#28040;&#38500;&#20102;&#20854;&#22522;&#20110;&#27169;&#22411;&#30340;&#23545;&#24212;&#26041;&#27861;&#30340;&#20027;&#35201;&#32570;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#29256;&#26412;&#30340;SICNN&#12290;SICNNv1&#38750;&#24120;&#31867;&#20284;&#20110;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#19987;&#38376;&#20026;&#21333;&#36733;&#27874;&#39057;&#22495;&#22343;&#34913;&#31995;&#32479;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Equalization is an important task at the receiver side of a digital wireless communication system, which is traditionally conducted with model-based estimation methods. Among the numerous options for model-based equalization, iterative soft interference cancellation (SIC) is a well-performing approach since error propagation caused by hard decision data symbol estimation during the iterative estimation procedure is avoided. However, the model-based method suffers from high computational complexity and performance degradation due to required approximations. In this work, we propose a novel neural network (NN-)based equalization approach, referred to as SICNN, which is designed by deep unfolding of a model-based iterative SIC method, eliminating the main disadvantages of its model-based counterpart. We present different variants of SICNN. SICNNv1 is very similar to the model-based method, and is specifically tailored for single carrier frequency domain equalization systems, which is the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Huber&#25439;&#22833;&#26368;&#23567;&#21270;&#30340;&#26032;&#22411;&#32858;&#21512;&#22120;&#29992;&#20110;&#25308;&#21344;&#24237;&#40065;&#26834;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#22312;&#29420;&#31435;&#21516;&#20998;&#24067;&#20551;&#35774;&#19979;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#30340;&#20248;&#21183;&#65292;&#24182;&#23545;&#38750;i.i.d&#25968;&#25454;&#36827;&#34892;&#20102;&#25193;&#23637;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2308.12581</link><description>&lt;p&gt;
&#19968;&#31181;Huber&#25439;&#22833;&#26368;&#23567;&#21270;&#26041;&#27861;&#29992;&#20110;&#25308;&#21344;&#24237;&#40065;&#26834;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Huber Loss Minimization Approach to Byzantine Robust Federated Learning. (arXiv:2308.12581v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Huber&#25439;&#22833;&#26368;&#23567;&#21270;&#30340;&#26032;&#22411;&#32858;&#21512;&#22120;&#29992;&#20110;&#25308;&#21344;&#24237;&#40065;&#26834;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#22312;&#29420;&#31435;&#21516;&#20998;&#24067;&#20551;&#35774;&#19979;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#30340;&#20248;&#21183;&#65292;&#24182;&#23545;&#38750;i.i.d&#25968;&#25454;&#36827;&#34892;&#20102;&#25193;&#23637;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;Huber&#25439;&#22833;&#26368;&#23567;&#21270;&#30340;&#26032;&#22411;&#32858;&#21512;&#22120;&#65292;&#24182;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#22312;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;i.i.d&#65289;&#20551;&#35774;&#19979;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20960;&#20010;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#23427;&#23545;&#20110;&#34987;&#25915;&#20987;&#23458;&#25143;&#31471;&#27604;&#29575;$\epsilon$&#20855;&#26377;&#26368;&#20248;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#23545;$\epsilon$&#26377;&#31934;&#30830;&#30340;&#30693;&#35782;&#12290;&#31532;&#19977;&#65292;&#23427;&#20801;&#35768;&#19981;&#21516;&#30340;&#23458;&#25143;&#31471;&#20855;&#26377;&#19981;&#22343;&#31561;&#30340;&#25968;&#25454;&#22823;&#23567;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#20998;&#26512;&#25193;&#23637;&#21040;&#21253;&#25324;&#38750;i.i.d&#25968;&#25454;&#65292;&#36825;&#24847;&#21619;&#30528;&#23458;&#25143;&#31471;&#20855;&#26377;&#30053;&#26377;&#19981;&#21516;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning systems are susceptible to adversarial attacks. To combat this, we introduce a novel aggregator based on Huber loss minimization, and provide a comprehensive theoretical analysis. Under independent and identically distributed (i.i.d) assumption, our approach has several advantages compared to existing methods. Firstly, it has optimal dependence on $\epsilon$, which stands for the ratio of attacked clients. Secondly, our approach does not need precise knowledge of $\epsilon$. Thirdly, it allows different clients to have unequal data sizes. We then broaden our analysis to include non-i.i.d data, such that clients have slightly different distributions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#37325;&#26032;&#21028;&#26029;&#19981;&#19968;&#33268;&#24615;&#8221;&#30340;&#29616;&#35937;&#65292;&#21363;&#22312;&#23436;&#25104;&#38472;&#36848;&#21644;&#37325;&#26032;&#35780;&#21028;&#36807;&#31243;&#20013;&#23384;&#22312;&#30683;&#30462;&#12290;&#36825;&#23545;&#20110;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#21644;&#38544;&#21547;&#20559;&#35265;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.12578</link><description>&lt;p&gt;
&#29702;&#26234;&#23545;&#35805;&#22768;&#38899;&#65306;&#20851;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#27979;&#37327;&#31038;&#20250;&#20559;&#35265;&#30340;&#37325;&#26032;&#21028;&#26029;&#19981;&#19968;&#33268;&#24615;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Mind vs. Mouth: On Measuring Re-judge Inconsistency of Social Bias in Large Language Models. (arXiv:2308.12578v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#37325;&#26032;&#21028;&#26029;&#19981;&#19968;&#33268;&#24615;&#8221;&#30340;&#29616;&#35937;&#65292;&#21363;&#22312;&#23436;&#25104;&#38472;&#36848;&#21644;&#37325;&#26032;&#35780;&#21028;&#36807;&#31243;&#20013;&#23384;&#22312;&#30683;&#30462;&#12290;&#36825;&#23545;&#20110;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#21644;&#38544;&#21547;&#20559;&#35265;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20855;&#26377;&#19982;&#20154;&#31867;&#35266;&#23519;&#21040;&#30340;&#35748;&#30693;&#32467;&#26500;&#30456;&#20284;&#30340;&#29305;&#28857;&#65292;&#20419;&#20351;&#30740;&#31350;&#20154;&#21592;&#35843;&#26597;LLMs&#30340;&#35748;&#30693;&#26041;&#38754;&#12290;&#26412;&#25991;&#30528;&#37325;&#30740;&#31350;&#20102;&#26126;&#30830;&#21644;&#38544;&#21547;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#36825;&#26159;&#24515;&#29702;&#23398;&#20013;&#19968;&#31181;&#29420;&#29305;&#30340;&#20004;&#32423;&#35748;&#30693;&#32467;&#26500;&#12290;&#25991;&#20013;&#25552;&#20986;&#65292;&#20010;&#20307;&#30340;&#26126;&#30830;&#31038;&#20250;&#20559;&#35265;&#65292;&#21363;&#20854;&#22312;&#38472;&#36848;&#20013;&#34920;&#36798;&#30340;&#26377;&#24847;&#35782;&#20559;&#35265;&#65292;&#21487;&#33021;&#19982;&#20854;&#38544;&#21547;&#30340;&#31038;&#20250;&#20559;&#35265;&#19981;&#21516;&#65292;&#21518;&#32773;&#20195;&#34920;&#20854;&#26080;&#24847;&#35782;&#20559;&#35265;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;LLMs&#20013;&#30340;&#19968;&#31181;&#24182;&#34892;&#29616;&#35937;&#65292;&#21363;&#31038;&#20250;&#20559;&#35265;&#20013;&#30340;&#8220;&#37325;&#26032;&#21028;&#26029;&#19981;&#19968;&#33268;&#24615;&#8221;&#12290;&#22312;&#21021;&#22987;&#38454;&#27573;&#65292;LLM&#36127;&#36131;&#33258;&#21160;&#23436;&#25104;&#38472;&#36848;&#65292;&#21487;&#33021;&#20250;&#21253;&#21547;&#38544;&#21547;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#22312;&#38543;&#21518;&#30340;&#38454;&#27573;&#65292;&#21516;&#26679;&#30340;LLM&#37325;&#26032;&#35780;&#21028;&#20102;&#20854;&#33258;&#21160;&#29983;&#25104;&#30340;&#26377;&#20559;&#35265;&#30340;&#38472;&#36848;&#65292;&#20294;&#21364;&#19982;&#20043;&#30456;&#30683;&#30462;&#12290;&#25105;&#20204;&#25552;&#20986;&#65292;&#36825;&#31181;&#37325;&#26032;&#21028;&#26029;&#30340;&#19981;&#19968;&#33268;&#24615;&#21487;&#33021;&#31867;&#20284;&#20110;&#20154;&#31867;&#19981;&#30693;&#36947;&#20854;&#20559;&#35265;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent researches indicate that Pre-trained Large Language Models (LLMs) possess cognitive constructs similar to those observed in humans, prompting researchers to investigate the cognitive aspects of LLMs. This paper focuses on explicit and implicit social bias, a distinctive two-level cognitive construct in psychology. It posits that individuals' explicit social bias, which is their conscious expression of bias in the statements, may differ from their implicit social bias, which represents their unconscious bias. We propose a two-stage approach and discover a parallel phenomenon in LLMs known as "re-judge inconsistency" in social bias. In the initial stage, the LLM is tasked with automatically completing statements, potentially incorporating implicit social bias. However, in the subsequent stage, the same LLM re-judges the biased statement generated by itself but contradicts it. We propose that this re-judge inconsistency can be similar to the inconsistency between human's unaware im
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;REB&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#39046;&#22495;&#20559;&#24046;&#21644;&#26500;&#24314;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#20197;&#21450;&#20351;&#29992;&#26412;&#22320;&#23494;&#24230;K&#26368;&#36817;&#37051;&#26041;&#27861;&#65292;&#20174;&#32780;&#38477;&#20302;&#24037;&#19994;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#34920;&#31034;&#20559;&#24046;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.12577</link><description>&lt;p&gt;
REB&#65306;&#38477;&#20302;&#24037;&#19994;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#34920;&#31034;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
REB: Reducing Biases in Representation for Industrial Anomaly Detection. (arXiv:2308.12577v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;REB&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#39046;&#22495;&#20559;&#24046;&#21644;&#26500;&#24314;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#20197;&#21450;&#20351;&#29992;&#26412;&#22320;&#23494;&#24230;K&#26368;&#36817;&#37051;&#26041;&#27861;&#65292;&#20174;&#32780;&#38477;&#20302;&#24037;&#19994;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#34920;&#31034;&#20559;&#24046;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;K&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#26816;&#32034;&#26041;&#27861;&#36890;&#24120;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#36827;&#34892;&#24037;&#19994;&#24322;&#24120;&#26816;&#27979;&#65306;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;CNN&#27169;&#22411;&#33719;&#21462;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#25191;&#34892;&#36317;&#31163;&#24230;&#37327;&#36827;&#34892;&#32570;&#38519;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24573;&#30053;&#20102;&#39046;&#22495;&#20559;&#24046;&#21644;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#23616;&#37096;&#23494;&#24230;&#24046;&#24322;&#65292;&#36825;&#20123;&#29305;&#24449;&#27809;&#26377;&#34987;&#20805;&#20998;&#21033;&#29992;&#65292;&#38480;&#21046;&#20102;&#26816;&#27979;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32771;&#34385;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#39046;&#22495;&#20559;&#24046;&#21644;&#26500;&#24314;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#39046;&#22495;&#36866;&#24212;&#24615;&#30340;&#26041;&#27861;&#8212;&#8212;Reducing Biases&#65288;REB&#65289;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#27169;&#20223;&#33258;&#28982;&#32570;&#38519;&#30340;&#32570;&#38519;&#29983;&#25104;&#31574;&#30053;&#65288;DefectMaker&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26412;&#22320;&#23494;&#24230;K&#26368;&#36817;&#37051;&#65288;LDKNN&#65289;&#26041;&#27861;&#26469;&#20943;&#23569;&#23616;&#37096;&#23494;&#24230;&#20559;&#24046;&#24182;&#23454;&#29616;&#26377;&#25928;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;MVTec AD&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;99.5&#65285;&#30340;AUROC&#65292;&#24182;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;MVTec LOCO AD&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;88.0&#65285;&#30340;AUROC&#65292;&#24182;&#23558;AUROC&#25552;&#39640;&#20102;4.7&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing K-nearest neighbor (KNN) retrieval-based methods usually conduct industrial anomaly detection in two stages: obtain feature representations with a pre-trained CNN model and perform distance measures for defect detection. However, the features are not fully exploited as they ignore domain bias and the difference of local density in feature space, which limits the detection performance. In this paper, we propose Reducing Biases (REB) in representation by considering the domain bias of the pre-trained model and building a self-supervised learning task for better domain adaption with a defect generation strategy (DefectMaker) imitating the natural defects. Additionally, we propose a local density KNN (LDKNN) to reduce the local density bias and obtain effective anomaly detection. We achieve a promising result of 99.5\% AUROC on the widely used MVTec AD benchmark. We also achieve 88.0\% AUROC on the challenging MVTec LOCO AD dataset and bring an improvement of 4.7\% AUROC to the st
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25506;&#32034;&#19981;&#21516;&#30340;&#26816;&#32034;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#26041;&#27861;&#26469;&#22686;&#24378;&#31572;&#26696;&#29983;&#25104;&#65292;&#24182;&#21457;&#29616;&#24120;&#29992;&#30340;&#36830;&#25509;&#26041;&#27861;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22235;&#31181;&#26367;&#20195;&#31574;&#30053;&#65292;&#21253;&#25324;&#20004;&#31181;&#21333;&#36718;&#26041;&#27861;&#21644;&#20004;&#31181;&#22810;&#36718;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2308.12574</link><description>&lt;p&gt;
&#25506;&#32034;&#26816;&#32034;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25972;&#21512;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Exploring the Integration Strategies of Retriever and Large Language Models. (arXiv:2308.12574v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25506;&#32034;&#19981;&#21516;&#30340;&#26816;&#32034;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#26041;&#27861;&#26469;&#22686;&#24378;&#31572;&#26696;&#29983;&#25104;&#65292;&#24182;&#21457;&#29616;&#24120;&#29992;&#30340;&#36830;&#25509;&#26041;&#27861;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22235;&#31181;&#26367;&#20195;&#31574;&#30053;&#65292;&#21253;&#25324;&#20004;&#31181;&#21333;&#36718;&#26041;&#27861;&#21644;&#20004;&#31181;&#22810;&#36718;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#21040;&#30340;&#27573;&#33853;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#30340;&#25972;&#21512;&#20026;&#25552;&#39640;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20316;&#20986;&#20102;&#26174;&#33879;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#23558;&#26816;&#32034;&#21040;&#30340;&#27573;&#33853;&#34701;&#20837;&#31572;&#26696;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#26368;&#20339;&#26041;&#27861;&#20173;&#28982;&#32570;&#20047;&#25506;&#32034;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#32467;&#21512;&#26816;&#32034;&#21040;&#30340;&#27573;&#33853;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#22686;&#24378;&#31572;&#26696;&#29983;&#25104;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#24120;&#29992;&#30340;&#36830;&#25509;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21363;&#20351;&#27491;&#30830;&#30340;&#25991;&#26723;&#22312;&#21069;k&#20010;&#26816;&#32034;&#21040;&#30340;&#27573;&#33853;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#32463;&#24120;&#20250;&#29983;&#25104;&#8220;&#26410;&#30693;&#8221;&#36755;&#20986;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22235;&#31181;&#23558;&#26816;&#32034;&#21040;&#30340;&#27573;&#33853;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#30340;&#26367;&#20195;&#31574;&#30053;&#12290;&#36825;&#20123;&#31574;&#30053;&#21253;&#25324;&#20004;&#31181;&#21033;&#29992;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#21333;&#36718;&#26041;&#27861;&#21644;&#20004;&#31181;&#21033;&#29992;&#21453;&#39304;&#24490;&#29615;&#30340;&#22810;&#36718;&#31574;&#30053;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#20998;&#26512;&#21644;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
The integration of retrieved passages and large language models (LLMs), such as ChatGPTs, has significantly contributed to improving open-domain question answering. However, there is still a lack of exploration regarding the optimal approach for incorporating retrieved passages into the answer generation process. This paper aims to fill this gap by investigating different methods of combining retrieved passages with LLMs to enhance answer generation. We begin by examining the limitations of a commonly-used concatenation approach. Surprisingly, this approach often results in generating "unknown" outputs, even when the correct document is among the top-k retrieved passages. To address this issue, we explore four alternative strategies for integrating the retrieved passages with the LLMs. These strategies include two single-round methods that utilize chain-of-thought reasoning and two multi-round strategies that incorporate feedback loops. Through comprehensive analyses and experiments, w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20197;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#29615;&#22659;&#20026;&#22522;&#30784;&#65292;&#20165;&#20973;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#36827;&#34892;&#20223;&#30495;&#23398;&#20064;&#65292;&#26080;&#38656;&#35775;&#38382;&#36716;&#31227;&#21160;&#21147;&#23398;&#20449;&#24687;&#12289;&#22870;&#21169;&#32467;&#26500;&#65292;&#25110;&#32773;&#20219;&#20309;&#39069;&#22806;&#30340;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2308.12573</link><description>&lt;p&gt;
&#26465;&#20214;&#26680;&#27169;&#20223;&#23398;&#20064;&#22312;&#36830;&#32493;&#29366;&#24577;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Conditional Kernel Imitation Learning for Continuous State Environments. (arXiv:2308.12573v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20197;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#29615;&#22659;&#20026;&#22522;&#30784;&#65292;&#20165;&#20973;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#36827;&#34892;&#20223;&#30495;&#23398;&#20064;&#65292;&#26080;&#38656;&#35775;&#38382;&#36716;&#31227;&#21160;&#21147;&#23398;&#20449;&#24687;&#12289;&#22870;&#21169;&#32467;&#26500;&#65292;&#25110;&#32773;&#20219;&#20309;&#39069;&#22806;&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20223;&#30495;&#23398;&#20064;&#65288;IL&#65289;&#26159;&#22312;&#26356;&#24191;&#27867;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#20013;&#30340;&#37325;&#35201;&#33539;&#20363;&#12290;&#19982;&#22823;&#22810;&#25968;RL&#19981;&#21516;&#65292;&#23427;&#19981;&#20551;&#35774;&#22238;&#39304;&#22870;&#21169;&#30340;&#21487;&#29992;&#24615;&#12290;&#22870;&#21169;&#25512;&#26029;&#21644;&#22609;&#24418;&#24050;&#30693;&#26159;&#22256;&#38590;&#19988;&#23481;&#26131;&#20986;&#38169;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#24403;&#28436;&#31034;&#25968;&#25454;&#26469;&#33258;&#20154;&#31867;&#19987;&#23478;&#26102;&#12290;&#20256;&#32479;&#26041;&#27861;&#22914;&#34892;&#20026;&#20811;&#38534;&#21644;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#23545;&#20272;&#35745;&#35823;&#24046;&#38750;&#24120;&#25935;&#24863;&#65292;&#36825;&#22312;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#38382;&#39064;&#20013;&#23588;&#20026;&#20005;&#37325;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26368;&#20808;&#36827;&#30340;IL&#31639;&#27861;&#23558;&#34892;&#20026;&#31574;&#30053;&#23398;&#20064;&#38382;&#39064;&#36716;&#25442;&#20026;&#20998;&#24067;&#21305;&#37197;&#38382;&#39064;&#65292;&#36890;&#24120;&#38656;&#35201;&#39069;&#22806;&#30340;&#22312;&#32447;&#20132;&#20114;&#25968;&#25454;&#25165;&#33021;&#21457;&#25381;&#20316;&#29992;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#29615;&#22659;&#20013;&#20165;&#22522;&#20110;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#36827;&#34892;&#20223;&#30495;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#19981;&#38656;&#35201;&#35775;&#38382;&#36716;&#31227;&#21160;&#21147;&#23398;&#20449;&#24687;&#12289;&#22870;&#21169;&#32467;&#26500;&#65292;&#25110;&#32773;&#26368;&#37325;&#35201;&#30340;&#26159;&#19981;&#38656;&#35201;&#19982;&#29615;&#22659;&#36827;&#34892;&#20219;&#20309;&#39069;&#22806;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning (IL) is an important paradigm within the broader reinforcement learning (RL) methodology. Unlike most of RL, it does not assume availability of reward-feedback. Reward inference and shaping are known to be difficult and error-prone methods particularly when the demonstration data comes from human experts. Classical methods such as behavioral cloning and inverse reinforcement learning are highly sensitive to estimation errors, a problem that is particularly acute in continuous state space problems. Meanwhile, state-of-the-art IL algorithms convert behavioral policy learning problems into distribution-matching problems which often require additional online interaction data to be effective. In this paper, we consider the problem of imitation learning in continuous state space environments based solely on observed behavior, without access to transition dynamics information, reward structure, or, most importantly, any additional interactions with the environment. Our appr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#23398;&#20064;&#30340;&#21327;&#21516;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#35270;&#22270;&#23398;&#20064;&#26469;&#20943;&#36731;&#25968;&#25454;&#22122;&#22768;&#21644;&#25439;&#22351;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#22522;&#20934;&#19978;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.12551</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#23398;&#20064;&#30340;&#21327;&#21516;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Co-training Approach for Noisy Time Series Learning. (arXiv:2308.12551v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#23398;&#20064;&#30340;&#21327;&#21516;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#35270;&#22270;&#23398;&#20064;&#26469;&#20943;&#36731;&#25968;&#25454;&#22122;&#22768;&#21644;&#25439;&#22351;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#22522;&#20934;&#19978;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#40065;&#26834;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#26159;&#29616;&#23454;&#19990;&#30028;&#30340;&#26102;&#38388;&#24207;&#21015;&#26159;&#22024;&#26434;&#30340;&#65292;&#24182;&#19988;&#26469;&#33258;&#21516;&#19968;&#26102;&#38388;&#24207;&#21015;&#30340;&#19981;&#21516;&#35266;&#28857;&#30340;&#20114;&#34917;&#20449;&#24687;&#22312;&#20998;&#26512;&#22024;&#26434;&#36755;&#20837;&#26102;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#19981;&#21516;&#30340;&#32534;&#30721;&#22120;&#20026;&#36755;&#20837;&#26102;&#38388;&#24207;&#21015;&#21019;&#24314;&#20102;&#20004;&#20010;&#35270;&#22270;&#12290;&#25105;&#20204;&#36890;&#36807;&#21327;&#21516;&#35757;&#32451;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#24335;&#26469;&#36845;&#20195;&#23398;&#20064;&#32534;&#30721;&#22120;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#21327;&#21516;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#19981;&#21516;&#35270;&#22270;&#30340;&#20114;&#34917;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;TS-CoT&#26041;&#27861;&#21487;&#20197;&#20943;&#36731;&#25968;&#25454;&#22122;&#22768;&#21644;&#25439;&#22351;&#30340;&#24433;&#21709;&#12290;&#22312;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#35774;&#32622;&#19979;&#23545;&#22235;&#20010;&#26102;&#38388;&#24207;&#21015;&#22522;&#20934;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;TS-CoT&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#24494;&#35843;&#65292;TS-CoT&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#21487;&#20197;&#24456;&#22909;&#22320;&#36866;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we focus on robust time series representation learning. Our assumption is that real-world time series is noisy and complementary information from different views of the same time series plays an important role while analyzing noisy input. Based on this, we create two views for the input time series through two different encoders. We conduct co-training based contrastive learning iteratively to learn the encoders. Our experiments demonstrate that this co-training approach leads to a significant improvement in performance. Especially, by leveraging the complementary information from different views, our proposed TS-CoT method can mitigate the impact of data noise and corruption. Empirical evaluations on four time series benchmarks in unsupervised and semi-supervised settings reveal that TS-CoT outperforms existing methods. Furthermore, the representations learned by TS-CoT can transfer well to downstream tasks through fine-tuning.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#25903;&#26550;&#26500;SyncTrack&#65292;&#36890;&#36807;&#21516;&#27493;&#29305;&#24449;&#25552;&#21462;&#21644;&#21305;&#37197;&#26469;&#31616;&#21270;3D&#29289;&#20307;&#36319;&#36394;&#20013;&#30340;Siamese&#32593;&#32476;&#65292;&#36991;&#20813;&#20102;&#22810;&#27425;&#36716;&#21457;&#32534;&#30721;&#22120;&#21644;&#24341;&#20837;&#39069;&#22806;&#21442;&#25968;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;Transformer&#23618;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;Attention Points-Sampling&#31574;&#30053;&#65288;APST&#65289;&#36827;&#34892;&#29305;&#24449;&#37319;&#26679;&#12290;</title><link>http://arxiv.org/abs/2308.12549</link><description>&lt;p&gt;
&#21516;&#27493;&#29305;&#24449;&#25552;&#21462;&#21644;&#21305;&#37197;&#65306;&#19968;&#31181;&#29992;&#20110;3D&#29289;&#20307;&#36319;&#36394;&#30340;&#21333;&#25903;&#26550;&#26500;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Synchronize Feature Extracting and Matching: A Single Branch Framework for 3D Object Tracking. (arXiv:2308.12549v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12549
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#25903;&#26550;&#26500;SyncTrack&#65292;&#36890;&#36807;&#21516;&#27493;&#29305;&#24449;&#25552;&#21462;&#21644;&#21305;&#37197;&#26469;&#31616;&#21270;3D&#29289;&#20307;&#36319;&#36394;&#20013;&#30340;Siamese&#32593;&#32476;&#65292;&#36991;&#20813;&#20102;&#22810;&#27425;&#36716;&#21457;&#32534;&#30721;&#22120;&#21644;&#24341;&#20837;&#39069;&#22806;&#21442;&#25968;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;Transformer&#23618;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;Attention Points-Sampling&#31574;&#30053;&#65288;APST&#65289;&#36827;&#34892;&#29305;&#24449;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;3D LiDAR&#29289;&#20307;&#36319;&#36394;&#20013;&#20849;&#20139;&#21442;&#25968;&#30340;&#32534;&#30721;&#22120;&#20174;&#27169;&#26495;&#21644;&#25628;&#32034;&#21306;&#22495;&#20998;&#21035;&#25552;&#21462;&#29305;&#24449;&#30340;Siamese&#32593;&#32476;&#24050;&#25104;&#20026;&#20107;&#23454;&#19978;&#30340;&#22522;&#20934;&#26694;&#26550;&#12290;&#36825;&#20010;&#33539;&#24335;&#20005;&#37325;&#20381;&#36182;&#20110;&#39069;&#22806;&#30340;&#21305;&#37197;&#32593;&#32476;&#26469;&#24314;&#27169;&#27169;&#26495;&#21644;&#25628;&#32034;&#21306;&#22495;&#30340;&#20132;&#21449;&#30456;&#20851;&#24615;/&#30456;&#20284;&#24615;&#12290;&#26412;&#25991;&#19981;&#20877;&#20351;&#29992;&#20256;&#32479;&#30340;Siamese&#33539;&#24335;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;&#25903;&#26550;&#26500;SyncTrack&#65292;&#36890;&#36807;&#21516;&#27493;&#29305;&#24449;&#25552;&#21462;&#21644;&#21305;&#37197;&#65292;&#36991;&#20813;&#20102;&#20026;&#27169;&#26495;&#21644;&#25628;&#32034;&#21306;&#22495;&#20998;&#21035;&#36716;&#21457;&#32534;&#30721;&#22120;&#20004;&#27425;&#65292;&#20063;&#36991;&#20813;&#20102;&#24341;&#20837;&#39069;&#22806;&#30340;&#21305;&#37197;&#32593;&#32476;&#21442;&#25968;&#12290;&#21516;&#27493;&#26426;&#21046;&#22522;&#20110;Transformer&#30340;&#21160;&#24577;&#20146;&#21644;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#30456;&#20851;&#24615;&#28145;&#20837;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#21516;&#27493;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Transformer&#23618;&#20013;&#30340;Attention Points-Sampling&#31574;&#30053;&#65288;APST&#65289;&#65292;&#29992;&#37319;&#26679;&#20195;&#26367;&#20102;&#38543;&#26426;/&#26368;&#36828;&#28857;&#37319;&#26679;&#65288;FPS&#65289;&#26041;&#27861;&#65292;&#24182;&#22312;&#37319;&#26679;&#36807;&#31243;&#20013;&#26377;&#30417;&#30563;&#22320;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Siamese network has been a de facto benchmark framework for 3D LiDAR object tracking with a shared-parametric encoder extracting features from template and search region, respectively. This paradigm relies heavily on an additional matching network to model the cross-correlation/similarity of the template and search region. In this paper, we forsake the conventional Siamese paradigm and propose a novel single-branch framework, SyncTrack, synchronizing the feature extracting and matching to avoid forwarding encoder twice for template and search region as well as introducing extra parameters of matching network. The synchronization mechanism is based on the dynamic affinity of the Transformer, and an in-depth analysis of the relevance is provided theoretically. Moreover, based on the synchronization, we introduce a novel Attentive Points-Sampling strategy into the Transformer layers (APST), replacing the random/Farthest Points Sampling (FPS) method with sampling under the supervision of a
&lt;/p&gt;</description></item><item><title>CALM&#26159;&#19968;&#20010;&#29992;&#20110;&#37327;&#21270;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#30456;&#27604;&#20808;&#21069;&#25968;&#25454;&#38598;&#26356;&#21152;&#22810;&#26679;&#21644;&#21487;&#38752;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#35780;&#20272;&#27169;&#22411;&#20559;&#35265;&#25152;&#38656;&#30340;&#35821;&#35328;&#21464;&#21270;&#30340;&#24191;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.12539</link><description>&lt;p&gt;
CALM: &#19968;&#31181;&#29992;&#20110;&#20840;&#38754;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias. (arXiv:2308.12539v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12539
&lt;/p&gt;
&lt;p&gt;
CALM&#26159;&#19968;&#20010;&#29992;&#20110;&#37327;&#21270;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#30456;&#27604;&#20808;&#21069;&#25968;&#25454;&#38598;&#26356;&#21152;&#22810;&#26679;&#21644;&#21487;&#38752;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#35780;&#20272;&#27169;&#22411;&#20559;&#35265;&#25152;&#38656;&#30340;&#35821;&#35328;&#21464;&#21270;&#30340;&#24191;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#19981;&#26029;&#22686;&#24378;&#65292;&#37327;&#21270;&#21644;&#27604;&#36739;&#23427;&#20204;&#22312;&#31038;&#20250;&#21644;&#20154;&#21475;&#23398;&#20559;&#35265;&#26041;&#38754;&#30340;&#33021;&#21147;&#20197;&#21450;&#28508;&#22312;&#30340;&#21361;&#23475;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#20559;&#35265;&#27979;&#37327;&#25968;&#25454;&#38598;&#23545;&#20110;&#20154;&#24037;&#35774;&#35745;&#27169;&#26495;&#30340;&#25200;&#21160;&#25935;&#24863;&#65292;&#22240;&#27492;&#19981;&#21487;&#38752;&#12290;&#20026;&#20102;&#20445;&#35777;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20840;&#38754;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#65288;CALM&#65289;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#37327;&#21270;LMs&#22312;&#19977;&#20010;&#20219;&#21153;&#19978;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#25972;&#21512;&#20102;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#65288;&#22914;&#32500;&#22522;&#30334;&#31185;&#21644;&#26032;&#38395;&#25991;&#31456;&#65289;&#30340;16&#20010;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#36807;&#28388;&#20986;224&#20010;&#27169;&#26495;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;78,400&#20010;&#31034;&#20363;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#24179;&#22343;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#27169;&#26495;&#38271;&#24230;&#30340;&#21464;&#24322;&#31243;&#24230;&#31561;&#25351;&#26631;&#65292;&#27604;&#36739;CALM&#19982;&#20808;&#21069;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#27979;&#35797;&#20854;&#23545;&#32454;&#24494;&#25200;&#21160;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#30456;&#23545;&#20110;&#20808;&#21069;&#25968;&#25454;&#38598;&#26356;&#21152;&#22810;&#26679;&#21644;&#21487;&#38752;&#65292;&#22240;&#27492;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#35780;&#20272;&#27169;&#22411;&#20559;&#35265;&#25152;&#38656;&#30340;&#35821;&#35328;&#21464;&#21270;&#30340;&#24191;&#24230;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;20&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
As language models (LMs) become increasingly powerful, it is important to quantify and compare them for sociodemographic bias with potential for harm. Prior bias measurement datasets are sensitive to perturbations in their manually designed templates, therefore unreliable. To achieve reliability, we introduce the Comprehensive Assessment of Language Model bias (CALM), a benchmark dataset to quantify bias in LMs across three tasks. We integrate 16 existing datasets across different domains, such as Wikipedia and news articles, to filter 224 templates from which we construct a dataset of 78,400 examples. We compare the diversity of CALM with prior datasets on metrics such as average semantic similarity, and variation in template length, and test the sensitivity to small perturbations. We show that our dataset is more diverse and reliable than previous datasets, thus better capture the breadth of linguistic variation required to reliably evaluate model bias. We evaluate 20 large language 
&lt;/p&gt;</description></item><item><title>FedSoL&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21248;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#24179;&#34913;&#20840;&#23616;&#23545;&#40784;&#21644;&#26412;&#22320;&#19968;&#33324;&#24615;&#26469;&#25913;&#21892;FL&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.12532</link><description>&lt;p&gt;
FedSoL: &#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#20840;&#23616;&#23545;&#40784;&#21644;&#26412;&#22320;&#19968;&#33324;&#24615;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
FedSoL: Bridging Global Alignment and Local Generality in Federated Learning. (arXiv:2308.12532v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12532
&lt;/p&gt;
&lt;p&gt;
FedSoL&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21248;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#24179;&#34913;&#20840;&#23616;&#23545;&#40784;&#21644;&#26412;&#22320;&#19968;&#33324;&#24615;&#26469;&#25913;&#21892;FL&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(Federated Learning, FL)&#36890;&#36807;&#32858;&#21512;&#26469;&#33258;&#20010;&#20307;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#26469;&#26500;&#24314;&#20840;&#23616;&#27169;&#22411;&#12290;&#34429;&#28982;FL&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#24403;&#23458;&#25143;&#31471;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21248;&#26102;&#65292;&#24120;&#24120;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;FL&#31639;&#27861;&#36890;&#36807;&#24341;&#20837;&#21508;&#31181;&#36817;&#20284;&#32422;&#26463;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20123;&#32422;&#26463;&#26088;&#22312;&#36890;&#36807;&#38480;&#21046;&#23616;&#37096;&#23398;&#20064;&#19982;&#20840;&#23616;&#30446;&#26631;&#30340;&#20559;&#31163;&#26469;&#20419;&#36827;&#20840;&#23616;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26412;&#36136;&#19978;&#36890;&#36807;&#24178;&#25200;&#21407;&#22987;&#30340;&#23616;&#37096;&#30446;&#26631;&#32780;&#38480;&#21046;&#20102;&#23616;&#37096;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#26469;&#25913;&#21892;&#26412;&#22320;&#23398;&#20064;&#30340;&#19968;&#33324;&#24615;&#12290;&#36890;&#36807;&#22312;&#24179;&#28369;&#30340;&#25439;&#22833;&#31354;&#38388;&#20013;&#33719;&#24471;&#26412;&#22320;&#27169;&#22411;&#65292;&#36825;&#31181;&#26041;&#27861;&#20943;&#36731;&#20102;&#23458;&#25143;&#31471;&#19981;&#21516;&#26412;&#22320;&#30446;&#26631;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;&#28982;&#32780;&#65292;&#23427;&#19981;&#33021;&#30830;&#20445;&#31283;&#23450;&#30340;&#20840;&#23616;&#23545;&#40784;&#65292;&#22240;&#20026;&#26412;&#22320;&#23398;&#20064;&#19981;&#32771;&#34385;&#20840;&#23616;&#30446;&#26631;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#31283;&#23450;&#24615;(FedSoL)&#26041;&#27861;&#26469;&#22312;FL&#20013;&#35299;&#20915;&#20840;&#23616;&#23545;&#40784;&#21644;&#26412;&#22320;&#19968;&#33324;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) aggregates locally trained models from individual clients to construct a global model. While FL enables learning a model with data privacy, it often suffers from significant performance degradation when client data distributions are heterogeneous. Many previous FL algorithms have addressed this issue by introducing various proximal restrictions. These restrictions aim to encourage global alignment by constraining the deviation of local learning from the global objective. However, they inherently limit local learning by interfering with the original local objectives. Recently, an alternative approach has emerged to improve local learning generality. By obtaining local models within a smooth loss landscape, this approach mitigates conflicts among different local objectives of the clients. Yet, it does not ensure stable global alignment, as local learning does not take the global objective into account. In this study, we propose Federated Stability on Learning (Fed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20026;&#22797;&#26434;&#26426;&#22120;&#20154;&#31995;&#32479;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#22870;&#21169;&#21644;&#32422;&#26463;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#35774;&#35745;&#39640;&#25928;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#26469;&#22788;&#29702;&#32422;&#26463;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;&#19981;&#21516;&#33151;&#24335;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#25511;&#21046;&#22120;&#35757;&#32451;&#20013;&#65292;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12517</link><description>&lt;p&gt;
&#19981;&#20165;&#20165;&#22870;&#21169;&#65292;&#36824;&#26377;&#32422;&#26463;&#65306;&#29992;&#20110;&#33151;&#24335;&#26426;&#22120;&#20154;&#36816;&#21160;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Not Only Rewards But Also Constraints: Applications on Legged Robot Locomotion. (arXiv:2308.12517v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20026;&#22797;&#26434;&#26426;&#22120;&#20154;&#31995;&#32479;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#22870;&#21169;&#21644;&#32422;&#26463;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#35774;&#35745;&#39640;&#25928;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#26469;&#22788;&#29702;&#32422;&#26463;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;&#19981;&#21516;&#33151;&#24335;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#25511;&#21046;&#22120;&#35757;&#32451;&#20013;&#65292;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#30340;&#19968;&#20123;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#24182;&#20351;&#29992;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26469;&#35757;&#32451;&#65292;&#23637;&#31034;&#20102;&#22797;&#26434;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25511;&#21046;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20855;&#26377;&#33258;&#28982;&#21160;&#20316;&#39118;&#26684;&#21644;&#39640;&#20219;&#21153;&#24615;&#33021;&#30340;&#20986;&#33394;&#25511;&#21046;&#22120;&#26159;&#36890;&#36807;&#36827;&#34892;&#22823;&#37327;&#22870;&#21169;&#24037;&#31243;&#32780;&#24320;&#21457;&#30340;&#65292;&#35813;&#36807;&#31243;&#38750;&#24120;&#36153;&#26102;&#36153;&#21147;&#65292;&#38656;&#35201;&#35774;&#35745;&#22823;&#37327;&#22870;&#21169;&#39033;&#24182;&#30830;&#23450;&#21512;&#36866;&#30340;&#22870;&#21169;&#31995;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#21516;&#26102;&#21253;&#21547;&#22870;&#21169;&#21644;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#12290;&#20026;&#20102;&#35753;&#24037;&#31243;&#24072;&#33021;&#22815;&#36866;&#24403;&#22320;&#21453;&#26144;&#20182;&#20204;&#23545;&#32422;&#26463;&#30340;&#24847;&#22270;&#24182;&#20197;&#26368;&#23567;&#30340;&#35745;&#31639;&#24320;&#38144;&#22788;&#29702;&#23427;&#20204;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#32422;&#26463;&#31867;&#22411;&#21644;&#19968;&#31181;&#39640;&#25928;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#12290;&#35813;&#23398;&#20064;&#26694;&#26550;&#34987;&#24212;&#29992;&#20110;&#35757;&#32451;&#19981;&#21516;&#24418;&#24577;&#21644;&#29289;&#29702;&#23646;&#24615;&#30340;&#20960;&#20010;&#33151;&#24335;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several earlier studies have shown impressive control performance in complex robotic systems by designing the controller using a neural network and training it with model-free reinforcement learning. However, these outstanding controllers with natural motion style and high task performance are developed through extensive reward engineering, which is a highly laborious and time-consuming process of designing numerous reward terms and determining suitable reward coefficients. In this work, we propose a novel reinforcement learning framework for training neural network controllers for complex robotic systems consisting of both rewards and constraints. To let the engineers appropriately reflect their intent to constraints and handle them with minimal computation overhead, two constraint types and an efficient policy optimization algorithm are suggested. The learning framework is applied to train locomotion controllers for several legged robots with different morphology and physical attribu
&lt;/p&gt;</description></item><item><title>I3DOD&#26159;&#19968;&#31181;&#22522;&#20110;&#24341;&#23548;&#30340;&#22686;&#37327;&#24335;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#26426;&#21046;&#23398;&#20064;&#23545;&#35937;&#23450;&#20301;&#20449;&#24687;&#21644;&#31867;&#21035;&#35821;&#20041;&#20449;&#24687;&#20043;&#38388;&#30340;&#21305;&#37197;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#26087;&#31867;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.12512</link><description>&lt;p&gt;
I3DOD: &#22522;&#20110;&#24341;&#23548;&#30340;&#22686;&#37327;&#24335;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
I3DOD: Towards Incremental 3D Object Detection via Prompting. (arXiv:2308.12512v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12512
&lt;/p&gt;
&lt;p&gt;
I3DOD&#26159;&#19968;&#31181;&#22522;&#20110;&#24341;&#23548;&#30340;&#22686;&#37327;&#24335;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#26426;&#21046;&#23398;&#20064;&#23545;&#35937;&#23450;&#20301;&#20449;&#24687;&#21644;&#31867;&#21035;&#35821;&#20041;&#20449;&#24687;&#20043;&#38388;&#30340;&#21305;&#37197;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#26087;&#31867;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#22312;&#35768;&#22810;&#39046;&#22495;&#65292;&#22914;&#26426;&#22120;&#20154;&#31995;&#32479;&#12289;&#33258;&#21160;&#39550;&#39542;&#21644;&#22686;&#24378;&#29616;&#23454;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#31867;&#22686;&#37327;&#22411;&#22330;&#26223;&#26102;&#21487;&#33021;&#23548;&#33268;&#26087;&#31867;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#21516;&#26102;&#65292;&#24403;&#21069;&#30340;&#31867;&#22686;&#37327;&#22411;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#26041;&#27861;&#24573;&#35270;&#20102;&#23545;&#35937;&#23450;&#20301;&#20449;&#24687;&#21644;&#31867;&#21035;&#35821;&#20041;&#20449;&#24687;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20551;&#35774;&#26087;&#27169;&#22411;&#30340;&#20840;&#37096;&#30693;&#35782;&#26159;&#21487;&#38752;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22686;&#37327;&#24335;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#26694;&#26550;&#65292;&#21363;I3DOD&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#20849;&#20139;&#30340;&#24341;&#23548;&#26426;&#21046;&#65292;&#29992;&#20110;&#23398;&#20064;&#23545;&#35937;&#23450;&#20301;&#20449;&#24687;&#21644;&#31867;&#21035;&#35821;&#20041;&#20449;&#24687;&#20043;&#38388;&#30340;&#21305;&#37197;&#20851;&#31995;&#12290;&#35757;&#32451;&#23436;&#25104;&#21518;&#65292;&#36825;&#20123;&#24341;&#23548;&#23558;&#34987;&#23384;&#20648;&#22312;&#25105;&#20204;&#30340;&#24341;&#23548;&#27744;&#20013;&#65292;&#24182;&#22312;&#19979;&#19968;&#20010;&#20219;&#21153;&#20013;&#22788;&#29702;&#26087;&#31867;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#33976;&#39311;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
3D object detection has achieved significant performance in many fields, e.g., robotics system, autonomous driving, and augmented reality. However, most existing methods could cause catastrophic forgetting of old classes when performing on the class-incremental scenarios. Meanwhile, the current class-incremental 3D object detection methods neglect the relationships between the object localization information and category semantic information and assume all the knowledge of old model is reliable. To address the above challenge, we present a novel Incremental 3D Object Detection framework with the guidance of prompting, i.e., I3DOD. Specifically, we propose a task-shared prompts mechanism to learn the matching relationships between the object localization information and category semantic information. After training on the current task, these prompts will be stored in our prompt pool, and perform the relationship of old classes in the next task. Moreover, we design a reliable distillatio
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#25513;&#34109;&#33258;&#32534;&#30721;&#22120;(MAEs)&#20316;&#20026;&#39640;&#25928;&#30340;&#20998;&#31867;&#22686;&#37327;&#23398;&#20064;&#22120;&#65292;&#36890;&#36807;&#37325;&#24314;&#21407;&#22987;&#36755;&#20837;&#22270;&#20687;&#21644;&#23398;&#20064;&#22270;&#20687;&#32423;&#21644;&#23884;&#20837;&#32423;&#34701;&#21512;&#26469;&#23384;&#20648;&#21644;&#23398;&#20064;&#36807;&#21435;&#20219;&#21153;&#30340;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#23454;&#65292;&#22312;CIFAR-100&#65292;ImageNet-Subset&#21644;ImageNet-Full&#19978;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.12510</link><description>&lt;p&gt;
Masked Autoencoders&#26159;&#39640;&#25928;&#30340;&#20998;&#31867;&#22686;&#37327;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Masked Autoencoders are Efficient Class Incremental Learners. (arXiv:2308.12510v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#25513;&#34109;&#33258;&#32534;&#30721;&#22120;(MAEs)&#20316;&#20026;&#39640;&#25928;&#30340;&#20998;&#31867;&#22686;&#37327;&#23398;&#20064;&#22120;&#65292;&#36890;&#36807;&#37325;&#24314;&#21407;&#22987;&#36755;&#20837;&#22270;&#20687;&#21644;&#23398;&#20064;&#22270;&#20687;&#32423;&#21644;&#23884;&#20837;&#32423;&#34701;&#21512;&#26469;&#23384;&#20648;&#21644;&#23398;&#20064;&#36807;&#21435;&#20219;&#21153;&#30340;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#23454;&#65292;&#22312;CIFAR-100&#65292;ImageNet-Subset&#21644;ImageNet-Full&#19978;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#22686;&#37327;&#23398;&#20064;(CIL)&#26088;&#22312;&#22312;&#23398;&#20064;&#26032;&#31867;&#21035;&#30340;&#21516;&#26102;&#36991;&#20813;&#23545;&#20043;&#21069;&#30693;&#35782;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#25513;&#34109;&#33258;&#32534;&#30721;&#22120;(MAEs)&#20316;&#20026;CIL&#30340;&#39640;&#25928;&#23398;&#20064;&#22120;&#12290;MAEs&#26368;&#21021;&#26159;&#20026;&#20102;&#36890;&#36807;&#37325;&#24314;&#26080;&#30417;&#30563;&#23398;&#20064;&#26469;&#23398;&#20064;&#26377;&#29992;&#30340;&#34920;&#31034;&#65292;&#24182;&#19988;&#23427;&#20204;&#21487;&#20197;&#36731;&#26494;&#22320;&#19982;&#30417;&#30563;&#25439;&#22833;&#32467;&#21512;&#20197;&#29992;&#20110;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;MAEs&#21487;&#20197;&#21487;&#38752;&#22320;&#20174;&#38543;&#26426;&#36873;&#25321;&#30340;&#22270;&#20687;&#34917;&#19969;&#20013;&#37325;&#24314;&#21407;&#22987;&#36755;&#20837;&#22270;&#20687;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#26356;&#39640;&#25928;&#22320;&#23384;&#20648;&#36807;&#21435;&#20219;&#21153;&#30340;&#33539;&#20363;&#29992;&#20110;CIL&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#21452;&#36793;MAE&#26694;&#26550;&#26469;&#23398;&#20064;&#22270;&#20687;&#32423;&#21644;&#23884;&#20837;&#32423;&#34701;&#21512;&#65292;&#23427;&#21487;&#20197;&#20135;&#29983;&#26356;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#22270;&#20687;&#21644;&#26356;&#31283;&#23450;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#23454;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;CIFAR-100&#65292;ImageNet-Subset&#21644;ImageNet-Full&#19978;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/scok30/MAE-CIL&#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class Incremental Learning (CIL) aims to sequentially learn new classes while avoiding catastrophic forgetting of previous knowledge. We propose to use Masked Autoencoders (MAEs) as efficient learners for CIL. MAEs were originally designed to learn useful representations through reconstructive unsupervised learning, and they can be easily integrated with a supervised loss for classification. Moreover, MAEs can reliably reconstruct original input images from randomly selected patches, which we use to store exemplars from past tasks more efficiently for CIL. We also propose a bilateral MAE framework to learn from image-level and embedding-level fusion, which produces better-quality reconstructed images and more stable representations. Our experiments confirm that our approach performs better than the state-of-the-art on CIFAR-100, ImageNet-Subset, and ImageNet-Full. The code is available at https://github.com/scok30/MAE-CIL .
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CGMI&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#20154;&#38469;&#20132;&#24448;&#12290;&#35813;&#26694;&#26550;&#37319;&#29992;&#20102;&#26641;&#29366;&#32467;&#26500;&#26041;&#27861;&#26469;&#31649;&#29702;&#26234;&#33021;&#20307;&#30340;&#20010;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;ACT*&#27169;&#22411;&#30340;&#35748;&#30693;&#26550;&#26500;&#12290;&#36890;&#36807;&#20351;&#29992;CGMI&#26694;&#26550;&#65292;&#25105;&#20204;&#25104;&#21151;&#27169;&#25311;&#20102;&#25945;&#24072;&#21644;&#23398;&#29983;&#20043;&#38388;&#30340;&#35838;&#22530;&#20114;&#21160;&#12290;</title><link>http://arxiv.org/abs/2308.12503</link><description>&lt;p&gt;
CGMI: &#21487;&#37197;&#32622;&#30340;&#36890;&#29992;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
CGMI: Configurable General Multi-Agent Interaction Framework. (arXiv:2308.12503v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CGMI&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#20154;&#38469;&#20132;&#24448;&#12290;&#35813;&#26694;&#26550;&#37319;&#29992;&#20102;&#26641;&#29366;&#32467;&#26500;&#26041;&#27861;&#26469;&#31649;&#29702;&#26234;&#33021;&#20307;&#30340;&#20010;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;ACT*&#27169;&#22411;&#30340;&#35748;&#30693;&#26550;&#26500;&#12290;&#36890;&#36807;&#20351;&#29992;CGMI&#26694;&#26550;&#65292;&#25105;&#20204;&#25104;&#21151;&#27169;&#25311;&#20102;&#25945;&#24072;&#21644;&#23398;&#29983;&#20043;&#38388;&#30340;&#35838;&#22530;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#20307;&#24050;&#32463;&#23637;&#29616;&#20986;&#35299;&#20915;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#21644;&#27169;&#20223;&#20154;&#31867;&#34892;&#20026;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#26377;&#38480;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#20197;&#21450;&#32570;&#20047;&#26377;&#25928;&#30340;&#35748;&#30693;&#26550;&#26500;&#65292;&#36825;&#20123;&#26234;&#33021;&#20307;&#29983;&#25104;&#30340;&#20869;&#23481;&#20173;&#28982;&#30456;&#23545;&#34920;&#38754;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#37197;&#32622;&#30340;&#36890;&#29992;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#65288;CGMI&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#20154;&#38469;&#20132;&#24448;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26641;&#29366;&#32467;&#26500;&#30340;&#26041;&#27861;&#35770;&#65292;&#29992;&#20110;&#26234;&#33021;&#20307;&#30340;&#20010;&#24615;&#20998;&#37197;&#12289;&#26816;&#27979;&#21644;&#32500;&#25252;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;ACT*&#27169;&#22411;&#30340;&#25216;&#33021;&#24211;&#30340;&#35748;&#30693;&#26550;&#26500;&#65292;&#20854;&#20013;&#21253;&#21547;&#35760;&#24518;&#12289;&#21453;&#24605;&#21644;&#35268;&#21010;&#27169;&#22359;&#12290;&#25105;&#20204;&#36824;&#25972;&#21512;&#20102;&#36890;&#29992;&#26234;&#33021;&#20307;&#26469;&#22686;&#24378;&#34394;&#25311;&#29615;&#22659;&#30340;&#30495;&#23454;&#24863;&#12290;&#21033;&#29992;CGMI&#26694;&#26550;&#65292;&#25105;&#20204;&#27169;&#25311;&#20102;&#22810;&#20010;&#25945;&#24072;&#21644;&#23398;&#29983;&#20043;&#38388;&#30340;&#35838;&#22530;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Benefiting from the powerful capabilities of large language models (LLMs), agents based on LLMs have shown the potential to address domain-specific tasks and emulate human behaviors. However, the content generated by these agents remains somewhat superficial, owing to their limited domain expertise and the absence of an effective cognitive architecture. To address this, we present the Configurable General Multi-Agent Interaction (CGMI) framework, designed to replicate human interactions in real-world scenarios. Specifically, we propose a tree-structured methodology for the assignment, detection, and maintenance of agent personality. Additionally, we designed a cognitive architecture equipped with a skill library based on the ACT* model, which contains memory, reflection, and planning modules. We have also integrated general agents to augment the virtual environment's realism. Using the CGMI framework, we simulated numerous classroom interactions between teacher and students. The experi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26080;&#38656;&#28304;&#25968;&#25454;&#65292;&#20351;&#29992;&#22810;&#35282;&#24230;&#29305;&#24449;&#22686;&#24378;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#28304;&#21327;&#20316;&#39046;&#22495;&#36866;&#24212;&#65288;SCDA&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#38745;&#24687;&#24577;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;rs-fMRI&#65289;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20943;&#23569;&#19981;&#21516;&#25195;&#25551;&#20202;/&#21327;&#35758;&#23548;&#33268;&#30340;&#36328;&#31449;&#28857;/&#39046;&#22495;&#25968;&#25454;&#24322;&#36136;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12495</link><description>&lt;p&gt;
&#26080;&#28304;&#21327;&#20316;&#39046;&#22495;&#36866;&#24212;&#65306;&#22810;&#35282;&#24230;&#29305;&#24449;&#22686;&#24378;&#22312;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Source-Free Collaborative Domain Adaptation via Multi-Perspective Feature Enrichment for Functional MRI Analysis. (arXiv:2308.12495v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12495
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26080;&#38656;&#28304;&#25968;&#25454;&#65292;&#20351;&#29992;&#22810;&#35282;&#24230;&#29305;&#24449;&#22686;&#24378;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#28304;&#21327;&#20316;&#39046;&#22495;&#36866;&#24212;&#65288;SCDA&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#38745;&#24687;&#24577;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;rs-fMRI&#65289;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20943;&#23569;&#19981;&#21516;&#25195;&#25551;&#20202;/&#21327;&#35758;&#23548;&#33268;&#30340;&#36328;&#31449;&#28857;/&#39046;&#22495;&#25968;&#25454;&#24322;&#36136;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38745;&#24687;&#24577;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;rs-fMRI&#65289;&#22312;&#22810;&#22320;&#28857;&#30740;&#31350;&#20013;&#36234;&#26469;&#36234;&#24120;&#29992;&#20110;&#36741;&#21161;&#31070;&#32463;&#23398;&#30142;&#30149;&#20998;&#26512;&#12290;&#29616;&#26377;&#30740;&#31350;&#36890;&#24120;&#21463;&#21040;&#36328;&#31449;&#28857;/&#39046;&#22495;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#24433;&#21709;&#65292;&#36825;&#26159;&#30001;&#20110;&#19981;&#21516;&#30340;&#25195;&#25551;&#20202;/&#21327;&#35758;&#23548;&#33268;&#30340;&#12290;&#35768;&#22810;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#20197;&#20943;&#23569;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;fMRI&#24322;&#36136;&#24615;&#65292;&#20294;&#36825;&#24448;&#24448;&#38656;&#35201;&#28304;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#22320;&#28857;&#30740;&#31350;&#20013;&#30001;&#20110;&#38544;&#31169;&#21644;/&#25110;&#25968;&#25454;&#23384;&#20648;&#36127;&#25285;&#30340;&#38382;&#39064;&#65292;&#33719;&#21462;&#28304;&#25968;&#25454;&#24456;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26080;&#28304;&#21327;&#20316;&#39046;&#22495;&#36866;&#24212;&#65288;SCDA&#65289;&#26694;&#26550;&#29992;&#20110;fMRI&#20998;&#26512;&#65292;&#20854;&#20013;&#20165;&#33021;&#35775;&#38382;&#39044;&#20808;&#35757;&#32451;&#30340;&#28304;&#27169;&#22411;&#21644;&#26080;&#26631;&#31614;&#30340;&#30446;&#26631;&#25968;&#25454;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22810;&#35282;&#24230;&#29305;&#24449;&#22686;&#24378;&#26041;&#27861;&#65288;MFE&#65289;&#29992;&#20110;&#30446;&#26631;fMRI&#20998;&#26512;&#65292;&#30001;&#22810;&#20010;&#21327;&#20316;&#20998;&#25903;&#32452;&#25104;&#65292;&#21160;&#24577;&#22320;&#25429;&#25417;&#26469;&#33258;&#22810;&#20010;&#35270;&#35282;&#30340;&#26080;&#26631;&#31614;&#30446;&#26631;&#25968;&#25454;&#30340;fMRI&#29305;&#24449;&#12290;&#27599;&#20010;&#20998;&#25903;&#37117;&#20855;&#26377;&#19968;&#20010;&#25968;&#25454;&#36827;&#32473;&#27169;&#22359;&#65292;
&lt;/p&gt;
&lt;p&gt;
Resting-state functional MRI (rs-fMRI) is increasingly employed in multi-site research to aid neurological disorder analysis. Existing studies usually suffer from significant cross-site/domain data heterogeneity caused by site effects such as differences in scanners/protocols. Many methods have been proposed to reduce fMRI heterogeneity between source and target domains, heavily relying on the availability of source data. But acquiring source data is challenging due to privacy concerns and/or data storage burdens in multi-site studies. To this end, we design a source-free collaborative domain adaptation (SCDA) framework for fMRI analysis, where only a pretrained source model and unlabeled target data are accessible. Specifically, a multi-perspective feature enrichment method (MFE) is developed for target fMRI analysis, consisting of multiple collaborative branches to dynamically capture fMRI features of unlabeled target data from multiple views. Each branch has a data-feeding module, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;ChatGPT&#21644;GPT-4&#30340;&#20808;&#21069;&#35780;&#20272;&#36827;&#34892;&#20102;&#32508;&#21512;&#20998;&#26512;&#65292;&#20851;&#27880;&#20854;&#35821;&#35328;&#21644;&#25512;&#29702;&#33021;&#21147;&#12289;&#31185;&#23398;&#30693;&#35782;&#21644;&#20262;&#29702;&#32771;&#34385;&#65292;&#25552;&#20986;&#20102;&#20960;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2308.12488</link><description>&lt;p&gt;
GPTEval: &#23545;ChatGPT&#21644;GPT-4&#35780;&#20272;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
GPTEval: A Survey on Assessments of ChatGPT and GPT-4. (arXiv:2308.12488v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;ChatGPT&#21644;GPT-4&#30340;&#20808;&#21069;&#35780;&#20272;&#36827;&#34892;&#20102;&#32508;&#21512;&#20998;&#26512;&#65292;&#20851;&#27880;&#20854;&#35821;&#35328;&#21644;&#25512;&#29702;&#33021;&#21147;&#12289;&#31185;&#23398;&#30693;&#35782;&#21644;&#20262;&#29702;&#32771;&#34385;&#65292;&#25552;&#20986;&#20102;&#20960;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30340;&#20986;&#29616;&#24341;&#21457;&#20102;&#23186;&#20307;&#23545;&#20854;&#25200;&#20081;&#31038;&#20250;&#21644;&#32463;&#27982;&#31995;&#32479;&#28508;&#21147;&#30340;&#35768;&#22810;&#29468;&#27979;&#12290;&#20854;&#24778;&#20154;&#30340;&#35821;&#35328;&#33021;&#21147;&#28608;&#36215;&#23398;&#32773;&#20204;&#23545;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#34920;&#29616;&#30340;&#27987;&#21402;&#20852;&#36259;&#12290;&#24050;&#32463;&#26377;&#35768;&#22810;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#21644;GPT-4&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#23398;&#31185;&#20013;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#39033;&#32508;&#21512;&#24615;&#30340;&#32508;&#36848;&#24635;&#32467;&#38598;&#20307;&#35780;&#20272;&#32467;&#26524;&#12290;&#26412;&#35843;&#26597;&#30340;&#30446;&#26631;&#26159;&#23545;ChatGPT&#21644;GPT-4&#30340;&#20808;&#21069;&#35780;&#20272;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#37325;&#28857;&#20851;&#27880;&#20854;&#35821;&#35328;&#21644;&#25512;&#29702;&#33021;&#21147;&#12289;&#31185;&#23398;&#30693;&#35782;&#21644;&#20262;&#29702;&#32771;&#34385;&#12290;&#27492;&#22806;&#65292;&#23545;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#36827;&#34892;&#20102;&#26816;&#26597;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#20010;&#26410;&#26469;&#30740;&#31350;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of ChatGPT has generated much speculation in the press about its potential to disrupt social and economic systems. Its astonishing language ability has aroused strong curiosity among scholars about its performance in different domains. There have been many studies evaluating the ability of ChatGPT and GPT-4 in different tasks and disciplines. However, a comprehensive review summarizing the collective assessment findings is lacking. The objective of this survey is to thoroughly analyze prior assessments of ChatGPT and GPT-4, focusing on its language and reasoning abilities, scientific knowledge, and ethical considerations. Furthermore, an examination of the existing evaluation methods is conducted, offering several recommendations for future research in evaluating large language models.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38750;&#20844;&#29702;&#36923;&#36753;&#30340;&#39034;&#24207;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#26234;&#33021;&#20195;&#29702;&#30340;&#23398;&#20064;&#21151;&#33021;&#12290;&#23427;&#21253;&#21547;&#20551;&#35774;&#12289;&#20462;&#27491;&#21644;&#24490;&#29615;&#19977;&#20010;&#27493;&#39588;&#65292;&#24182;&#22312;&#30693;&#35782;&#21644;&#36164;&#28304;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#24037;&#20316;&#12290;&#23613;&#31649;&#26377;&#19968;&#20123;&#38480;&#21046;&#65292;&#20294;&#35813;&#27169;&#22411;&#24050;&#22312;&#19968;&#20123;&#31616;&#21333;&#26696;&#20363;&#20013;&#35777;&#26126;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2308.12486</link><description>&lt;p&gt;
&#22522;&#20110;&#38750;&#20844;&#29702;&#36923;&#36753;&#30340;&#39034;&#24207;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Model of Sequential Learning based on Non-Axiomatic Logic. (arXiv:2308.12486v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12486
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38750;&#20844;&#29702;&#36923;&#36753;&#30340;&#39034;&#24207;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#26234;&#33021;&#20195;&#29702;&#30340;&#23398;&#20064;&#21151;&#33021;&#12290;&#23427;&#21253;&#21547;&#20551;&#35774;&#12289;&#20462;&#27491;&#21644;&#24490;&#29615;&#19977;&#20010;&#27493;&#39588;&#65292;&#24182;&#22312;&#30693;&#35782;&#21644;&#36164;&#28304;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#24037;&#20316;&#12290;&#23613;&#31649;&#26377;&#19968;&#20123;&#38480;&#21046;&#65292;&#20294;&#35813;&#27169;&#22411;&#24050;&#22312;&#19968;&#20123;&#31616;&#21333;&#26696;&#20363;&#20013;&#35777;&#26126;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#23398;&#20064;&#26159;&#26234;&#33021;&#20195;&#29702;&#30340;&#22522;&#26412;&#21151;&#33021;&#12290;&#26412;&#25216;&#26415;&#25253;&#21578;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#38750;&#20844;&#29702;&#36923;&#36753;&#30340;&#39034;&#24207;&#23398;&#20064;&#27169;&#22411;&#12290;&#23398;&#20064;&#36807;&#31243;&#21253;&#25324;&#20551;&#35774;&#12289;&#20462;&#27491;&#21644;&#24490;&#29615;&#19977;&#20010;&#27493;&#39588;&#65292;&#24182;&#21487;&#20197;&#36866;&#29992;&#20110;&#30693;&#35782;&#21644;&#36164;&#28304;&#19981;&#36275;&#30340;&#24773;&#20917;&#12290;&#34429;&#28982;&#24403;&#21069;&#35774;&#35745;&#23384;&#22312;&#19968;&#23450;&#38480;&#21046;&#65292;&#20294;&#35813;&#27169;&#22411;&#24050;&#22312;&#19968;&#20123;&#31616;&#21333;&#26696;&#20363;&#20013;&#35777;&#26126;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential learning is a fundamental function of an intelligent agent. This technical report introduces a model of sequential learning, which is interpretable through Non-Axiomatic Logic. The learning procedure includes three steps, hypothesizing, revising, and recycling, and can work under the Assumption of Insufficient Knowledge and Resources. Although there are limitations for the current design, the model has been proven effective in some simple cases.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22768;&#23398;&#29305;&#24449;&#34701;&#21512;&#32593;&#32476;&#65288;ABAFnet&#65289;&#29992;&#20110;&#25233;&#37057;&#30151;&#26816;&#27979;&#65292;&#33021;&#22815;&#26377;&#25928;&#25972;&#21512;&#21644;&#34701;&#21512;&#22810;&#31181;&#35821;&#38899;&#29305;&#24449;&#65292;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12478</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22768;&#23398;&#29305;&#24449;&#34701;&#21512;&#32593;&#32476;&#29992;&#20110;&#25233;&#37057;&#30151;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Attention-Based Acoustic Feature Fusion Network for Depression Detection. (arXiv:2308.12478v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12478
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22768;&#23398;&#29305;&#24449;&#34701;&#21512;&#32593;&#32476;&#65288;ABAFnet&#65289;&#29992;&#20110;&#25233;&#37057;&#30151;&#26816;&#27979;&#65292;&#33021;&#22815;&#26377;&#25928;&#25972;&#21512;&#21644;&#34701;&#21512;&#22810;&#31181;&#35821;&#38899;&#29305;&#24449;&#65292;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25233;&#37057;&#30151;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#24515;&#29702;&#38556;&#30861;&#65292;&#23545;&#20010;&#20307;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#24182;&#23545;&#31038;&#20250;&#36896;&#25104;&#24040;&#22823;&#20914;&#20987;&#12290;&#30001;&#20110;&#35813;&#30142;&#30149;&#30340;&#22797;&#26434;&#24615;&#21644;&#24322;&#36136;&#24615;&#65292;&#36843;&#20999;&#38656;&#35201;&#21450;&#26102;&#26377;&#25928;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#36825;&#20063;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#21033;&#29992;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#21548;&#35273;&#25968;&#25454;&#36827;&#34892;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25216;&#26415;&#20027;&#35201;&#20381;&#36182;&#20110;&#21333;&#19968;&#32500;&#24230;&#30340;&#29305;&#24449;&#27169;&#22411;&#65292;&#21487;&#33021;&#24573;&#30053;&#20102;&#22810;&#31181;&#35821;&#38899;&#29305;&#24449;&#20013;&#38544;&#34255;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22768;&#23398;&#29305;&#24449;&#34701;&#21512;&#32593;&#32476;&#65288;ABAFnet&#65289;&#29992;&#20110;&#25233;&#37057;&#30151;&#26816;&#27979;&#12290;ABAFnet&#23558;&#22235;&#31181;&#19981;&#21516;&#30340;&#22768;&#23398;&#29305;&#24449;&#32467;&#21512;&#21040;&#19968;&#20010;&#32508;&#21512;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#26377;&#25928;&#22320;&#25972;&#21512;&#21644;&#34701;&#21512;&#20102;&#22810;&#23618;&#27425;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Depression, a common mental disorder, significantly influences individuals and imposes considerable societal impacts. The complexity and heterogeneity of the disorder necessitate prompt and effective detection, which nonetheless, poses a difficult challenge. This situation highlights an urgent requirement for improved detection methods. Exploiting auditory data through advanced machine learning paradigms presents promising research directions. Yet, existing techniques mainly rely on single-dimensional feature models, potentially neglecting the abundance of information hidden in various speech characteristics. To rectify this, we present the novel Attention-Based Acoustic Feature Fusion Network (ABAFnet) for depression detection. ABAFnet combines four different acoustic features into a comprehensive deep learning model, thereby effectively integrating and blending multi-tiered features. We present a novel weight adjustment module for late fusion that boosts performance by efficaciously 
&lt;/p&gt;</description></item><item><title>ChatGPT&#21644;GPT-4&#22312;&#25169;&#20811;&#20013;&#26174;&#31034;&#20986;&#39640;&#32423;&#29702;&#35299;&#65292;&#20294;&#19981;&#26159;&#28216;&#25103;&#35770;&#29702;&#26368;&#20248;&#30340;&#25169;&#20811;&#29609;&#23478;&#12290;&#23545;&#27169;&#22411;&#21442;&#25968;&#21644;&#25552;&#31034;&#30340;&#20248;&#21270;&#21487;&#20197;&#25552;&#39640;&#23427;&#20204;&#22312;&#25169;&#20811;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.12466</link><description>&lt;p&gt;
ChatGPT&#21644;GPT-4&#26159;&#20248;&#31168;&#30340;&#25169;&#20811;&#29609;&#23478;&#21527;&#65311;&#8212;&#8212;&#19968;&#39033;Pre-Flop&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are ChatGPT and GPT-4 Good Poker Players? -- A Pre-Flop Analysis. (arXiv:2308.12466v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12466
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#21644;GPT-4&#22312;&#25169;&#20811;&#20013;&#26174;&#31034;&#20986;&#39640;&#32423;&#29702;&#35299;&#65292;&#20294;&#19981;&#26159;&#28216;&#25103;&#35770;&#29702;&#26368;&#20248;&#30340;&#25169;&#20811;&#29609;&#23478;&#12290;&#23545;&#27169;&#22411;&#21442;&#25968;&#21644;&#25552;&#31034;&#30340;&#20248;&#21270;&#21487;&#20197;&#25552;&#39640;&#23427;&#20204;&#22312;&#25169;&#20811;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;ChatGPT&#21644;GPT-4&#38382;&#19990;&#20197;&#26469;&#65292;&#36825;&#20123;&#27169;&#22411;&#24050;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#29087;&#32451;&#31243;&#24230;&#26159;&#26174;&#32780;&#26131;&#35265;&#30340;&#65292;&#20294;&#23427;&#20204;&#22312;&#28216;&#25103;&#20013;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#25169;&#20811;&#39046;&#22495;&#30340;&#33021;&#21147;&#65292;&#36824;&#26410;&#34987;&#25506;&#32034;&#12290;&#25169;&#20811;&#26159;&#19968;&#31181;&#38656;&#35201;&#22312;&#19981;&#30830;&#23450;&#24615;&#21644;&#19981;&#23436;&#20840;&#20449;&#24687;&#19979;&#20570;&#20986;&#20915;&#31574;&#30340;&#28216;&#25103;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;ChatGPT&#21644;GPT-4&#36827;&#34892;&#20102;&#25169;&#20811;&#27979;&#35797;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#25169;&#20811;&#25216;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#34429;&#28982;&#36825;&#20004;&#20010;&#27169;&#22411;&#37117;&#23637;&#31034;&#20102;&#23545;&#25169;&#20811;&#30340;&#39640;&#32423;&#29702;&#35299;&#65292;&#21253;&#25324;&#36215;&#22987;&#25163;&#29260;&#30340;&#20272;&#20540;&#12289;&#25171;&#29260;&#20301;&#32622;&#20197;&#21450;&#28216;&#25103;&#35770;&#29702;&#26368;&#20248;(GTO)&#25169;&#20811;&#30340;&#20854;&#20182;&#22797;&#26434;&#24615;&#65292;&#20294;ChatGPT&#21644;GPT-4&#24182;&#19981;&#26159;&#28216;&#25103;&#35770;&#29702;&#26368;&#20248;&#30340;&#25169;&#20811;&#29609;&#23478;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25105;&#20204;&#39318;&#20808;&#21457;&#29616;&#20102;&#19982;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#29609;&#25169;&#20811;&#30456;&#20851;&#30340;&#26368;&#20339;&#25552;&#31034;&#21644;&#27169;&#22411;&#21442;&#25968;&#30340;&#29305;&#24449;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#36825;&#20004;&#20010;&#27169;&#22411;&#20855;&#26377;&#19981;&#21516;&#30340;&#25171;&#29260;&#39118;&#26684;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65306;GPT-4&#26159;
&lt;/p&gt;
&lt;p&gt;
Since the introduction of ChatGPT and GPT-4, these models have been tested across a large number of tasks. Their adeptness across domains is evident, but their aptitude in playing games and specifically their aptitude in the realm of poker has remained unexplored. Poker is a game that requires decision making under uncertainty and incomplete information. In this paper, we put ChatGPT and GPT-4 through the poker test and evaluate their poker skills. Our findings reveal that while both models display an advanced understanding of poker, encompassing concepts like the valuation of starting hands, playing positions and other intricacies of game theory optimal (GTO) poker, both ChatGPT and GPT-4 are NOT game theory optimal poker players.  Through a series of experiments, we first discover the characteristics of optimal prompts and model parameters for playing poker with these models. Our observations then unveil the distinct playing personas of the two models. We first conclude that GPT-4 is
&lt;/p&gt;</description></item><item><title>PFL-GAN&#26159;&#19968;&#31181;&#22312;&#20010;&#24615;&#21270;&#32852;&#21512;&#23398;&#20064;&#20013;&#35299;&#20915;&#23458;&#25143;&#24322;&#36136;&#24615;&#30340;&#26032;&#22411;GAN&#20849;&#20139;&#21644;&#32858;&#21512;&#31574;&#30053;&#65292;&#36890;&#36807;&#23398;&#20064;&#23458;&#25143;&#38388;&#30340;&#30456;&#20284;&#24230;&#24182;&#37319;&#29992;&#21152;&#26435;&#21327;&#21516;&#25968;&#25454;&#32858;&#21512;&#26041;&#27861;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.12454</link><description>&lt;p&gt;
PFL-GAN&#65306;&#24403;&#23458;&#25143;&#24322;&#36136;&#24615;&#19982;&#29983;&#25104;&#27169;&#22411;&#30456;&#36935;&#22312;&#20010;&#24615;&#21270;&#32852;&#21512;&#23398;&#20064;&#20013;
&lt;/p&gt;
&lt;p&gt;
PFL-GAN: When Client Heterogeneity Meets Generative Models in Personalized Federated Learning. (arXiv:2308.12454v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12454
&lt;/p&gt;
&lt;p&gt;
PFL-GAN&#26159;&#19968;&#31181;&#22312;&#20010;&#24615;&#21270;&#32852;&#21512;&#23398;&#20064;&#20013;&#35299;&#20915;&#23458;&#25143;&#24322;&#36136;&#24615;&#30340;&#26032;&#22411;GAN&#20849;&#20139;&#21644;&#32858;&#21512;&#31574;&#30053;&#65292;&#36890;&#36807;&#23398;&#20064;&#23458;&#25143;&#38388;&#30340;&#30456;&#20284;&#24230;&#24182;&#37319;&#29992;&#21152;&#26435;&#21327;&#21516;&#25968;&#25454;&#32858;&#21512;&#26041;&#27861;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#29983;&#25104;&#23398;&#20064;&#27169;&#22411;&#30340;&#36827;&#23637;&#20276;&#38543;&#30528;&#23545;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#27169;&#22411;&#30340;&#32852;&#21512;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#26085;&#30410;&#20851;&#27880;&#12290;&#22312;FL&#30340;&#32972;&#26223;&#19979;&#65292;GAN&#21487;&#20197;&#25429;&#25417;&#24213;&#23618;&#23458;&#25143;&#25968;&#25454;&#32467;&#26500;&#65292;&#24182;&#37325;&#26032;&#29983;&#25104;&#31867;&#20284;&#20110;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#30340;&#26679;&#26412;&#65292;&#32780;&#19981;&#25439;&#23475;&#31169;&#26377;&#21407;&#22987;&#25968;&#25454;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#29616;&#26377;&#22522;&#20110;GAN&#30340;FL&#24037;&#20316;&#38598;&#20013;&#22312;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#19978;&#65292;&#20294;&#22312;&#23458;&#25143;&#25968;&#25454;&#24322;&#36136;&#24615;&#26041;&#38754;&#65292;&#20010;&#24615;&#21270;FL&#65288;PFL&#65289;&#26377;&#26102;&#21487;&#33021;&#26356;&#21152;&#26377;&#25928;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#21040;&#19981;&#21516;&#25968;&#25454;&#26679;&#26412;&#20998;&#24067;&#12289;&#29305;&#24449;&#31354;&#38388;&#21644;&#26631;&#31614;&#12290;&#20026;&#20102;&#24212;&#23545;GAN-based FL&#20013;&#30340;&#23458;&#25143;&#24322;&#36136;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;PFL&#30340;&#26032;&#22411;GAN&#20849;&#20139;&#21644;&#32858;&#21512;&#31574;&#30053;&#12290;&#25152;&#25552;&#20986;&#30340;PFL-GAN&#35299;&#20915;&#20102;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#23458;&#25143;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23398;&#20064;&#23458;&#25143;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#65292;&#28982;&#21518;&#24320;&#21457;&#19968;&#20010;&#21152;&#26435;&#30340;&#21327;&#21516;&#25968;&#25454;&#32858;&#21512;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#20960;&#20010;&#30693;&#21517;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20005;&#26684;&#30340;&#23454;&#39564;&#26469;&#39564;&#35777;&#27492;&#26041;&#27861;&#30340;&#23454;&#38469;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances of generative learning models are accompanied by the growing interest in federated learning (FL) based on generative adversarial network (GAN) models. In the context of FL, GAN can capture the underlying client data structure, and regenerate samples resembling the original data distribution without compromising the private raw data. Although most existing GAN-based FL works focus on training a global model, Personalized FL (PFL) sometimes can be more effective in view of client data heterogeneity in terms of distinct data sample distributions, feature spaces, and labels. To cope with client heterogeneity in GAN-based FL, we propose a novel GAN sharing and aggregation strategy for PFL. The proposed PFL-GAN addresses the client heterogeneity in different scenarios. More specially, we first learn the similarity among clients and then develop an weighted collaborative data aggregation. The empirical results through the rigorous experimentation on several well-known datasets
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#21487;&#25552;&#39640;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#22312;&#25968;&#25454;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#39281;&#21644;&#25928;&#26524;&#65292;&#27604;&#28155;&#21152;&#30495;&#23454;&#22270;&#20687;&#33719;&#24471;&#30340;&#24615;&#33021;&#25552;&#21319;&#35201;&#23567;&#24471;&#22810;&#12290;</title><link>http://arxiv.org/abs/2308.12453</link><description>&lt;p&gt;
&#21033;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Augmenting medical image classifiers with synthetic data from latent diffusion models. (arXiv:2308.12453v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12453
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#21487;&#25552;&#39640;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#22312;&#25968;&#25454;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#39281;&#21644;&#25928;&#26524;&#65292;&#27604;&#28155;&#21152;&#30495;&#23454;&#22270;&#20687;&#33719;&#24471;&#30340;&#24615;&#33021;&#25552;&#21319;&#35201;&#23567;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#29616;&#22312;&#26377;&#25968;&#30334;&#31181;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#24050;&#33719;&#24471;&#32654;&#22269;&#39135;&#21697;&#21644;&#33647;&#29289;&#31649;&#29702;&#23616;&#30340;&#25209;&#20934;&#25110;&#28165;&#38500;&#65292;&#20294;&#35768;&#22810;&#30740;&#31350;&#26174;&#31034;&#20854;&#19968;&#33268;&#24615;&#27867;&#21270;&#25110;&#28508;&#22312;&#20559;&#24046;&#23384;&#22312;&#19981;&#19968;&#33268;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23569;&#25968;&#32676;&#20307;&#12290;&#26377;&#20154;&#25552;&#20986;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#20943;&#23569;&#23545;&#30495;&#23454;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#20294;&#20854;&#22312;&#27169;&#22411;&#24320;&#21457;&#20013;&#30340;&#25928;&#29992;&#23578;&#19981;&#28165;&#26970;&#12290;&#30382;&#32932;&#30142;&#30149;&#26159;&#21512;&#25104;&#22270;&#20687;&#29983;&#25104;&#30340;&#26377;&#29992;&#26696;&#20363;&#30740;&#31350;&#65292;&#22240;&#20026;&#30142;&#30149;&#22806;&#35266;&#20855;&#26377;&#22810;&#26679;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#30382;&#32932;&#33394;&#35843;&#36825;&#19968;&#20445;&#25252;&#23646;&#24615;&#19978;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#21487;&#25193;&#23637;&#22320;&#29983;&#25104;&#30382;&#32932;&#30142;&#30149;&#22270;&#20687;&#65292;&#24182;&#19988;&#22312;&#25968;&#25454;&#21463;&#38480;&#24773;&#20917;&#19979;&#65292;&#23558;&#36825;&#20123;&#25968;&#25454;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#36825;&#31181;&#24615;&#33021;&#25552;&#21319;&#22312;&#21512;&#25104;&#21040;&#30495;&#23454;&#22270;&#20687;&#27604;&#20363;&#36229;&#36807;10&#65306;1&#21518;&#36798;&#21040;&#39281;&#21644;&#65292;&#24182;&#19988;&#27604;&#28155;&#21152;&#30495;&#23454;&#22270;&#20687;&#25152;&#33719;&#24471;&#30340;&#25552;&#21319;&#35201;&#23567;&#24471;&#22810;&#12290;&#20316;&#20026;&#25105;&#20204;&#20998;&#26512;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#29983;&#25104;&#24182;&#20998;&#26512;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;458,920&#20010;&#21512;&#25104;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
While hundreds of artificial intelligence (AI) algorithms are now approved or cleared by the US Food and Drugs Administration (FDA), many studies have shown inconsistent generalization or latent bias, particularly for underrepresented populations. Some have proposed that generative AI could reduce the need for real data, but its utility in model development remains unclear. Skin disease serves as a useful case study in synthetic image generation due to the diversity of disease appearance, particularly across the protected attribute of skin tone. Here we show that latent diffusion models can scalably generate images of skin disease and that augmenting model training with these data improves performance in data-limited settings. These performance gains saturate at synthetic-to-real image ratios above 10:1 and are substantially smaller than the gains obtained from adding real images. As part of our analysis, we generate and analyze a new dataset of 458,920 synthetic images produced using 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Dr. DRL &#30340;&#33258;&#24840;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#19968;&#20123;&#25928;&#29575;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#24341;&#20837;&#26377;&#24847;&#36951;&#24536;&#30340;&#26426;&#21046;&#26469;&#24212;&#23545;&#29615;&#22659;&#28418;&#31227;&#24341;&#36215;&#30340;&#22256;&#25200;&#12290;</title><link>http://arxiv.org/abs/2308.12445</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#26377;&#24847;&#36951;&#24536;&#39537;&#21160;&#30340;&#33258;&#24840;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Intentional Forgetting-Driven Self-Healing Method For Deep Reinforcement Learning Systems. (arXiv:2308.12445v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Dr. DRL &#30340;&#33258;&#24840;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#19968;&#20123;&#25928;&#29575;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#24341;&#20837;&#26377;&#24847;&#36951;&#24536;&#30340;&#26426;&#21046;&#26469;&#24212;&#23545;&#29615;&#22659;&#28418;&#31227;&#24341;&#36215;&#30340;&#22256;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064; (DRL) &#22312;&#20687; Netflix &#21644; Facebook &#36825;&#26679;&#30340;&#22823;&#35268;&#27169;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#22810;&#12290;&#21644;&#22823;&#22810;&#25968;&#25968;&#25454;&#39537;&#21160;&#31995;&#32479;&#19968;&#26679;&#65292;DRL &#31995;&#32479;&#21487;&#33021;&#30001;&#20110;&#29615;&#22659;&#28418;&#31227;&#23548;&#33268;&#19981;&#33391;&#34892;&#20026;&#65292;&#32780;&#36825;&#31181;&#28418;&#31227;&#32463;&#24120;&#21457;&#29983;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#29983;&#20135;&#29615;&#22659;&#20013;&#12290;&#36830;&#32493;&#23398;&#20064; (CL) &#26159;&#33258;&#24840;&#26041;&#27861;&#65292;&#29992;&#20110;&#26681;&#25454;&#29615;&#22659;&#26465;&#20214;&#30340;&#21464;&#21270;&#35843;&#25972; DRL &#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#30340;&#36830;&#32493;&#21464;&#21270;&#21487;&#33021;&#23548;&#33268;&#29983;&#20135;&#29615;&#22659;&#20174;&#21407;&#22987;&#29366;&#24577;&#20559;&#31163;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#29615;&#22659;&#28418;&#31227;&#24448;&#24448;&#23548;&#33268;&#36830;&#32493;&#23398;&#20064;&#36827;&#20837;&#38271;&#26102;&#38388;&#30340;&#33258;&#24840;&#21608;&#26399;&#65292;&#29978;&#33267;&#26080;&#27861;&#25104;&#21151;&#65292;&#36825;&#26159;&#30001;&#20110;&#28798;&#38590;&#24615;&#36951;&#24536;&#12289;&#28201;&#21644;&#36215;&#22987;&#22833;&#36133;&#21644;&#25910;&#25947;&#32531;&#24930;&#31561;&#25928;&#29575;&#20302;&#19979;&#38382;&#39064;&#24341;&#36215;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986; Dr. DRL&#65292;&#19968;&#31181;&#23545; DRL &#31995;&#32479;&#30340;&#26377;&#25928;&#33258;&#24840;&#26041;&#27861;&#65292;&#23427;&#23558;&#26377;&#24847;&#36951;&#24536;&#30340;&#26032;&#39062;&#26426;&#21046;&#25972;&#21512;&#21040;&#21407;&#22987;&#30340;&#36830;&#32493;&#23398;&#20064;&#20013;&#20197;&#35299;&#20915;&#20854;&#20027;&#35201;&#38382;&#39064;&#12290;Dr. DRL &#26377;&#24847;&#22320;&#25830;&#38500;...
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (DRL) is increasingly applied in large-scale productions like Netflix and Facebook. As with most data-driven systems, DRL systems can exhibit undesirable behaviors due to environmental drifts, which often occur in constantly-changing production settings. Continual Learning (CL) is the inherent self-healing approach for adapting the DRL agent in response to the environment's conditions shifts. However, successive shifts of considerable magnitude may cause the production environment to drift from its original state. Recent studies have shown that these environmental drifts tend to drive CL into long, or even unsuccessful, healing cycles, which arise from inefficiencies such as catastrophic forgetting, warm-starting failure, and slow convergence. In this paper, we propose Dr. DRL, an effective self-healing approach for DRL systems that integrates a novel mechanism of intentional forgetting into vanilla CL to overcome its main issues. Dr. DRL deliberately erases
&lt;/p&gt;</description></item><item><title>BaDExpert&#26159;&#19968;&#31181;&#38450;&#24481;&#21518;&#38376;&#25915;&#20987;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36870;&#21521;&#24037;&#31243;&#25552;&#21462;&#32473;&#23450;&#21518;&#38376;&#27169;&#22411;&#30340;&#21518;&#38376;&#21151;&#33021;&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#19987;&#23478;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21482;&#33021;&#35782;&#21035;&#21518;&#38376;&#36755;&#20837;&#12290;&#21487;&#20197;&#36827;&#19968;&#27493;&#20351;&#29992;&#35813;&#27169;&#22411;&#35774;&#35745;&#39640;&#24230;&#20934;&#30830;&#30340;&#21518;&#38376;&#36755;&#20837;&#26816;&#27979;&#22120;&#12290;</title><link>http://arxiv.org/abs/2308.12439</link><description>&lt;p&gt;
BaDExpert: &#25552;&#21462;&#21518;&#38376;&#21151;&#33021;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#21518;&#38376;&#36755;&#20837;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection. (arXiv:2308.12439v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12439
&lt;/p&gt;
&lt;p&gt;
BaDExpert&#26159;&#19968;&#31181;&#38450;&#24481;&#21518;&#38376;&#25915;&#20987;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36870;&#21521;&#24037;&#31243;&#25552;&#21462;&#32473;&#23450;&#21518;&#38376;&#27169;&#22411;&#30340;&#21518;&#38376;&#21151;&#33021;&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#19987;&#23478;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21482;&#33021;&#35782;&#21035;&#21518;&#38376;&#36755;&#20837;&#12290;&#21487;&#20197;&#36827;&#19968;&#27493;&#20351;&#29992;&#35813;&#27169;&#22411;&#35774;&#35745;&#39640;&#24230;&#20934;&#30830;&#30340;&#21518;&#38376;&#36755;&#20837;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#25239;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#19978;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20854;&#20013;&#23545;&#25163;&#23558;&#24694;&#24847;&#34892;&#20026;&#65288;&#21518;&#38376;&#65289;&#31192;&#23494;&#22320;&#26893;&#20837;DNN&#20013;&#12290;&#25105;&#20204;&#30340;&#38450;&#24481;&#23646;&#20110;&#21518;&#26399;&#24320;&#21457;&#30340;&#38450;&#24481;&#33539;&#30068;&#65292;&#29420;&#31435;&#20110;&#27169;&#22411;&#29983;&#25104;&#30340;&#26041;&#24335;&#12290;&#25152;&#25552;&#20986;&#30340;&#38450;&#24481;&#26041;&#27861;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#36870;&#21521;&#24037;&#31243;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#25552;&#21462;&#32473;&#23450;&#21518;&#38376;&#27169;&#22411;&#30340;&#21518;&#38376;&#21151;&#33021;&#24182;&#29983;&#25104;&#19968;&#20010;&#19987;&#23478;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#24456;&#31616;&#21333; - &#22312;&#19968;&#23567;&#32452;&#26377;&#24847;&#20041;&#30340;&#38169;&#35823;&#26631;&#35760;&#30340;&#24178;&#20928;&#26679;&#26412;&#19978;&#24494;&#35843;&#21518;&#38376;&#27169;&#22411;&#65292;&#20174;&#32780;&#20351;&#20854;&#24536;&#35760;&#27491;&#24120;&#21151;&#33021;&#20294;&#20173;&#20445;&#30041;&#21518;&#38376;&#21151;&#33021;&#65292;&#20174;&#32780;&#29983;&#25104;&#19968;&#20010;&#21482;&#33021;&#35782;&#21035;&#21518;&#38376;&#36755;&#20837;&#30340;&#27169;&#22411;&#65288;&#31216;&#20026;&#21518;&#38376;&#19987;&#23478;&#27169;&#22411;&#65289;&#12290;&#22522;&#20110;&#25552;&#21462;&#30340;&#21518;&#38376;&#19987;&#23478;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35774;&#35745;&#39640;&#24230;&#20934;&#30830;&#30340;&#21518;&#38376;&#36755;&#20837;&#26816;&#27979;&#22120;&#30340;&#21487;&#34892;&#24615;&#65292;&#22312;&#27169;&#22411;&#25512;&#29702;&#36807;&#31243;&#20013;&#36807;&#28388;&#25481;&#21518;&#38376;&#36755;&#20837;&#12290;&#36827;&#19968;&#27493;&#36890;&#36807;...
&lt;/p&gt;
&lt;p&gt;
We present a novel defense, against backdoor attacks on Deep Neural Networks (DNNs), wherein adversaries covertly implant malicious behaviors (backdoors) into DNNs. Our defense falls within the category of post-development defenses that operate independently of how the model was generated. The proposed defense is built upon a novel reverse engineering approach that can directly extract backdoor functionality of a given backdoored model to a backdoor expert model. The approach is straightforward -- finetuning the backdoored model over a small set of intentionally mislabeled clean samples, such that it unlearns the normal functionality while still preserving the backdoor functionality, and thus resulting in a model (dubbed a backdoor expert model) that can only recognize backdoor inputs. Based on the extracted backdoor expert model, we show the feasibility of devising highly accurate backdoor input detectors that filter out the backdoor inputs during model inference. Further augmented by
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#24320;&#21457;&#32773;&#38382;&#31572;&#35770;&#22363;Stack Overflow&#19978;&#30340;&#24086;&#23376;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#24635;&#32467;&#20102;&#37096;&#32626;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#38024;&#23545;&#19981;&#21516;&#30340;&#37096;&#32626;&#24179;&#21488;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2308.12438</link><description>&lt;p&gt;
&#37096;&#32626;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#65306;&#25361;&#25112;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Deploying Deep Reinforcement Learning Systems: A Taxonomy of Challenges. (arXiv:2308.12438v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#24320;&#21457;&#32773;&#38382;&#31572;&#35770;&#22363;Stack Overflow&#19978;&#30340;&#24086;&#23376;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#24635;&#32467;&#20102;&#37096;&#32626;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#38024;&#23545;&#19981;&#21516;&#30340;&#37096;&#32626;&#24179;&#21488;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24050;&#32463;&#22312;&#21253;&#25324;&#26426;&#22120;&#20154;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#30005;&#33041;&#28216;&#25103;&#31561;&#39046;&#22495;&#26174;&#31034;&#20986;&#26174;&#33879;&#28508;&#21147;&#65292;&#20197;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#33258;&#20027;&#33021;&#21147;&#12290;&#36825;&#31181;&#28508;&#21147;&#24341;&#36215;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#23545;DRL&#30340;&#28909;&#24773;&#21644;&#26085;&#30410;&#22686;&#38271;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#31038;&#21306;&#20027;&#35201;&#38598;&#20013;&#22312;DRL&#31995;&#32479;&#24320;&#21457;&#38454;&#27573;&#65292;&#23545;DRL&#37096;&#32626;&#20851;&#27880;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#24320;&#21457;&#32773;&#26368;&#27969;&#34892;&#30340;&#38382;&#31572;&#35770;&#22363;Stack Overflow&#65288;SO&#65289;&#36827;&#34892;&#20102;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;&#65292;&#20197;&#25581;&#31034;&#21644;&#20102;&#35299;&#20174;&#19994;&#20154;&#21592;&#22312;&#37096;&#32626;DRL&#31995;&#32479;&#26102;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25353;&#37096;&#32626;&#24179;&#21488;&#23545;&#30456;&#20851;SO&#24086;&#23376;&#36827;&#34892;&#20102;&#20998;&#31867;&#65306;&#26381;&#21153;&#22120;/&#20113;&#12289;&#31227;&#21160;/&#23884;&#20837;&#24335;&#31995;&#32479;&#12289;&#27983;&#35272;&#22120;&#21644;&#28216;&#25103;&#24341;&#25806;&#12290;&#32463;&#36807;&#36807;&#28388;&#21644;&#25163;&#21160;&#20998;&#26512;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;357&#20010;&#26377;&#20851;DRL&#37096;&#32626;&#30340;SO&#24086;&#23376;&#65292;&#35843;&#26597;&#20102;&#24403;&#21069;&#29366;&#20917;&#65292;&#24182;&#30830;&#23450;&#20102;&#19982;&#37096;&#32626;DRL&#31995;&#32479;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (DRL), leveraging Deep Learning (DL) in reinforcement learning, has shown significant potential in achieving human-level autonomy in a wide range of domains, including robotics, computer vision, and computer games. This potential justifies the enthusiasm and growing interest in DRL in both academia and industry. However, the community currently focuses mostly on the development phase of DRL systems, with little attention devoted to DRL deployment. In this paper, we propose an empirical study on Stack Overflow (SO), the most popular Q&amp;A forum for developers, to uncover and understand the challenges practitioners faced when deploying DRL systems. Specifically, we categorized relevant SO posts by deployment platforms: server/cloud, mobile/embedded system, browser, and game engine. After filtering and manual analysis, we examined 357 SO posts about DRL deployment, investigated the current state, and identified the challenges related to deploying DRL systems. The
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#22609;&#33041;&#40836;&#39044;&#27979;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#23558;&#20854;&#36716;&#21270;&#20026;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#22238;&#24402;&#38382;&#39064;&#65292;&#36890;&#36807;&#20272;&#35745;&#27599;&#20010;&#33041;&#37096;&#20307;&#32032;&#30340;&#33041;&#40836;&#26469;&#25552;&#20379;&#26356;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;&#19982;&#20840;&#23616;&#24180;&#40836;&#39044;&#27979;&#27169;&#22411;&#30456;&#27604;&#65292;&#36880;&#20307;&#32032;&#24180;&#40836;&#39044;&#27979;&#27169;&#22411;&#25552;&#20379;&#20102;&#26356;&#21487;&#35299;&#37322;&#30340;&#31354;&#38388;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.12416</link><description>&lt;p&gt;
&#37325;&#22609;&#33041;&#40836;&#39044;&#27979;&#38382;&#39064;&#20026;&#26356;&#21487;&#35299;&#37322;&#21644;&#37327;&#21270;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reframing the Brain Age Prediction Problem to a More Interpretable and Quantitative Approach. (arXiv:2308.12416v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#22609;&#33041;&#40836;&#39044;&#27979;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#23558;&#20854;&#36716;&#21270;&#20026;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#22238;&#24402;&#38382;&#39064;&#65292;&#36890;&#36807;&#20272;&#35745;&#27599;&#20010;&#33041;&#37096;&#20307;&#32032;&#30340;&#33041;&#40836;&#26469;&#25552;&#20379;&#26356;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;&#19982;&#20840;&#23616;&#24180;&#40836;&#39044;&#27979;&#27169;&#22411;&#30456;&#27604;&#65292;&#36880;&#20307;&#32032;&#24180;&#40836;&#39044;&#27979;&#27169;&#22411;&#25552;&#20379;&#20102;&#26356;&#21487;&#35299;&#37322;&#30340;&#31354;&#38388;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#22312;&#20272;&#35745;&#30913;&#20849;&#25391;&#22270;&#20687;&#20013;&#30340;&#33041;&#40836;&#65288;&#19968;&#20010;&#37325;&#35201;&#30340;&#33041;&#20581;&#24247;&#29983;&#29289;&#26631;&#24535;&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#21482;&#25552;&#20379;&#20840;&#23616;&#30340;&#24180;&#40836;&#39044;&#27979;&#65292;&#24182;&#20381;&#36182;&#20110;&#20687;&#26174;&#33879;&#24615;&#22270;&#36825;&#26679;&#30340;&#25216;&#26415;&#26469;&#35299;&#37322;&#32467;&#26524;&#12290;&#36825;&#20123;&#26174;&#33879;&#24615;&#22270;&#31361;&#20986;&#26174;&#31034;&#20102;&#23545;&#27169;&#22411;&#39044;&#27979;&#26377;&#37325;&#35201;&#24847;&#20041;&#30340;&#36755;&#20837;&#22270;&#20687;&#21306;&#22495;&#65292;&#20294;&#24456;&#38590;&#35299;&#37322;&#65292;&#24182;&#19988;&#26174;&#33879;&#24615;&#22270;&#30340;&#20540;&#22312;&#19981;&#21516;&#26679;&#26412;&#20043;&#38388;&#26080;&#27861;&#30452;&#25509;&#27604;&#36739;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#33041;&#40836;&#39044;&#27979;&#38382;&#39064;&#20174;&#30913;&#20849;&#25391;&#22270;&#20687;&#36716;&#25442;&#20026;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#22238;&#24402;&#38382;&#39064;&#65292;&#20854;&#20013;&#25105;&#20204;&#20272;&#35745;&#27599;&#20010;&#33041;&#37096;&#20307;&#32032;&#30340;&#33041;&#40836;&#12290;&#25105;&#20204;&#23558;&#36880;&#20307;&#32032;&#24180;&#40836;&#39044;&#27979;&#27169;&#22411;&#19982;&#20840;&#23616;&#24180;&#40836;&#39044;&#27979;&#27169;&#22411;&#21450;&#20854;&#23545;&#24212;&#30340;&#26174;&#33879;&#24615;&#22270;&#36827;&#34892;&#23545;&#27604;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36880;&#20307;&#32032;&#24180;&#40836;&#39044;&#27979;&#27169;&#22411;&#26356;&#26131;&#35299;&#37322;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#33041;&#34928;&#32769;&#36807;&#31243;&#30340;&#31354;&#38388;&#20449;&#24687;&#65292;&#24182;&#19988;&#23427;&#20204;&#20174;&#20013;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models have achieved state-of-the-art results in estimating brain age, which is an important brain health biomarker, from magnetic resonance (MR) images. However, most of these models only provide a global age prediction, and rely on techniques, such as saliency maps to interpret their results. These saliency maps highlight regions in the input image that were significant for the model's predictions, but they are hard to be interpreted, and saliency map values are not directly comparable across different samples. In this work, we reframe the age prediction problem from MR images to an image-to-image regression problem where we estimate the brain age for each brain voxel in MR images. We compare voxel-wise age prediction models against global age prediction models and their corresponding saliency maps. The results indicate that voxel-wise age prediction models are more interpretable, since they provide spatial information about the brain aging process, and they benefit fro
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28304;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22240;&#26524;&#30740;&#31350;&#22522;&#20934;&#35780;&#20272;&#65292;&#20197;&#35299;&#20915;&#21253;&#25324;&#22240;&#26524;&#25512;&#26029;&#22312;&#20869;&#30340;&#27169;&#22411;&#24615;&#33021;&#21487;&#35299;&#37322;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.12415</link><description>&lt;p&gt;
&#29992;&#20110;&#28304;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22240;&#26524;&#30740;&#31350;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Causal Study to Interpret Large Language Models for Source Code. (arXiv:2308.12415v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12415
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28304;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22240;&#26524;&#30740;&#31350;&#22522;&#20934;&#35780;&#20272;&#65292;&#20197;&#35299;&#20915;&#21253;&#25324;&#22240;&#26524;&#25512;&#26029;&#22312;&#20869;&#30340;&#27169;&#22411;&#24615;&#33021;&#21487;&#35299;&#37322;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#30740;&#31350;&#20154;&#21592;&#24120;&#37319;&#29992;&#30340;&#35299;&#20915;&#20195;&#30721;&#29983;&#25104;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#36890;&#36807;&#23545;&#22823;&#37327;&#28304;&#20195;&#30721;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35757;&#32451;&#12290;&#23613;&#31649;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;LLM&#22312;&#27969;&#34892;&#30340;&#31934;&#30830;&#24230;&#25351;&#26631;&#65288;&#20363;&#22914;BLEU&#65292;CodeBleu&#65289;&#19978;&#24050;&#32463;&#24471;&#21040;&#20102;&#26377;&#25928;&#35780;&#20272;&#65292;&#20294;&#20197;&#21069;&#30340;&#30740;&#31350;&#22312;LLM&#24615;&#33021;&#30340;&#21487;&#35299;&#37322;&#24615;&#30340;&#26681;&#26412;&#32452;&#25104;&#37096;&#20998;&#8212;&#8212;&#22240;&#26524;&#25512;&#26029;&#30340;&#20316;&#29992;&#26041;&#38754;&#20027;&#35201;&#34987;&#24573;&#35270;&#20102;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#21644;&#25968;&#25454;&#38598;&#26088;&#22312;&#31361;&#20986;&#39044;&#26399;&#32467;&#26524;&#19982;&#29983;&#25104;&#32467;&#26524;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20294;&#27809;&#26377;&#32771;&#34385;&#21516;&#26679;&#24433;&#21709;&#31934;&#30830;&#24230;&#25351;&#26631;&#30340;&#28151;&#28102;&#21464;&#37327;&#65288;&#20363;&#22914;&#20195;&#30721;&#34892;&#25968;&#65292;&#25552;&#31034;&#22823;&#23567;&#65289;&#12290;&#20107;&#23454;&#19978;&#65292;&#22312;&#22788;&#29702;LLM&#30340;&#29983;&#25104;&#36719;&#20214;&#20219;&#21153;&#26102;&#65292;&#27809;&#26377;&#22522;&#20934;&#21487;&#20197;&#21578;&#35785;&#30740;&#31350;&#20154;&#21592;&#22914;&#20309;&#37327;&#21270;&#22522;&#20110;&#36719;&#20214;&#24037;&#31243;&#30340;&#22788;&#29702;&#30340;&#22240;&#26524;&#25928;&#24212;&#20197;&#21450;&#28151;&#28102;&#21464;&#37327;&#19982;&#27169;&#22411;&#24615;&#33021;&#30340;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#22312;&#35780;&#20272;LLM&#26102;&#24341;&#20837;&#32479;&#35745;&#20005;&#35880;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most common solutions adopted by software researchers to address code generation is by training Large Language Models (LLMs) on massive amounts of source code. Although a number of studies have shown that LLMs have been effectively evaluated on popular accuracy metrics (e.g., BLEU, CodeBleu), previous research has largely overlooked the role of Causal Inference as a fundamental component of the interpretability of LLMs' performance. Existing benchmarks and datasets are meant to highlight the difference between the expected and the generated outcome, but do not take into account confounding variables (e.g., lines of code, prompt size) that equally influence the accuracy metrics. The fact remains that, when dealing with generative software tasks by LLMs, no benchmark is available to tell researchers how to quantify neither the causal effect of SE-based treatments nor the correlation of confounders to the model's performance. In an effort to bring statistical rigor to the evalu
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26234;&#33021;&#30340;&#29702;&#35770;&#65292;&#35752;&#35770;&#20102;&#26234;&#33021;&#30340;&#26680;&#24515;&#35201;&#32032;&#21644;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#31532;&#19968;&#21407;&#29702;&#30340;&#29702;&#35770;&#12290;&#30740;&#31350;&#37325;&#28857;&#26159;&#20154;&#31867;&#26234;&#33021;&#65292;&#24182;&#19982;&#26426;&#22120;&#36827;&#34892;&#27604;&#36739;&#65292;&#30446;&#30340;&#26159;&#20026;&#26356;&#24191;&#27867;&#30340;&#29983;&#21629;&#12289;&#38598;&#21512;&#20307;&#21644;&#38750;&#35774;&#35745;&#30340;&#29289;&#29702;&#21270;&#23398;&#31995;&#32479;&#25552;&#20379;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2308.12411</link><description>&lt;p&gt;
&#26234;&#33021;&#30340;&#19968;&#31181;&#29702;&#35770;&#65306;&#27010;&#24565;&#12289;&#27169;&#22411;&#21644;&#24847;&#20041;
&lt;/p&gt;
&lt;p&gt;
A Theory of Intelligences: Concepts, Models, Implications. (arXiv:2308.12411v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12411
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26234;&#33021;&#30340;&#29702;&#35770;&#65292;&#35752;&#35770;&#20102;&#26234;&#33021;&#30340;&#26680;&#24515;&#35201;&#32032;&#21644;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#31532;&#19968;&#21407;&#29702;&#30340;&#29702;&#35770;&#12290;&#30740;&#31350;&#37325;&#28857;&#26159;&#20154;&#31867;&#26234;&#33021;&#65292;&#24182;&#19982;&#26426;&#22120;&#36827;&#34892;&#27604;&#36739;&#65292;&#30446;&#30340;&#26159;&#20026;&#26356;&#24191;&#27867;&#30340;&#29983;&#21629;&#12289;&#38598;&#21512;&#20307;&#21644;&#38750;&#35774;&#35745;&#30340;&#29289;&#29702;&#21270;&#23398;&#31995;&#32479;&#25552;&#20379;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#26159;&#20154;&#31867;&#29992;&#26469;&#34920;&#31034;&#23454;&#29616;&#30446;&#26631;&#33021;&#21147;&#30340;&#27010;&#24565;&#12290;&#32473;&#20104;&#36825;&#20010;&#24191;&#27867;&#30340;&#33539;&#30068;&#65292;&#26234;&#33021;&#24050;&#32463;&#34987;&#26080;&#25968;&#27425;&#23450;&#20041;&#65292;&#20197;&#21508;&#31181;&#26041;&#24335;&#36827;&#34892;&#30740;&#31350;&#65292;&#24182;&#20351;&#29992;&#22810;&#31181;&#27979;&#37327;&#26041;&#27861;&#36827;&#34892;&#37327;&#21270;&#12290;&#29702;&#35299;&#26234;&#33021;&#26368;&#32456;&#38656;&#35201;&#29702;&#35770;&#21644;&#37327;&#21270;&#65292;&#20294;&#36825;&#20004;&#32773;&#37117;&#24456;&#38590;&#25417;&#25720;&#12290;&#25105;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#30830;&#23450;&#26234;&#33021;&#30340;&#19968;&#20123;&#26680;&#24515;&#35201;&#32032;&#65292;&#35752;&#35770;&#20854;&#20013;&#30340;&#19968;&#20123;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#31532;&#19968;&#21407;&#29702;&#30340;&#29702;&#35770;&#12290;&#25105;&#20027;&#35201;&#20851;&#27880;&#20197;&#20154;&#31867;&#20026;&#23450;&#20041;&#21644;&#21442;&#29031;&#23545;&#35937;&#30340;&#26234;&#33021;&#65292;&#24120;&#24120;&#19982;&#26426;&#22120;&#36827;&#34892;&#27604;&#36739;&#65292;&#24847;&#22270;&#20026;&#29983;&#21629;&#12289;&#38598;&#21512;&#20307;&#12289;&#20154;&#24037;&#26234;&#33021;&#31561;&#38750;&#35774;&#35745;&#30340;&#29289;&#29702;&#21644;&#21270;&#23398;&#31995;&#32479;&#25552;&#20379;&#26356;&#19968;&#33324;&#21270;&#30340;&#25551;&#36848;&#12290;&#25105;&#35752;&#35770;&#20102;&#26234;&#33021;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#21253;&#25324;&#36335;&#24452;&#25928;&#29575;&#21644;&#30446;&#26631;&#20934;&#30830;&#24615;&#12289;&#26234;&#33021;&#20316;&#20026;&#40657;&#30418;&#12289;&#29615;&#22659;&#24433;&#21709;&#12289;&#22788;&#29702;&#24847;&#22806;&#24773;&#20917;&#30340;&#28789;&#27963;&#24615;&#12289;&#26234;&#33021;&#30340;&#22238;&#24402;&#21644;&#30456;&#23545;&#35770;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligence is a human construct to represent the ability to achieve goals. Given this wide berth, intelligence has been defined countless times, studied in a variety of ways and quantified using numerous measures. Understanding intelligence ultimately requires theory and quantification, both of which are elusive. My main objectives are to identify some of the central elements in and surrounding intelligence, discuss some of its challenges and propose a theory based on first principles. I focus on intelligence as defined by and for humans, frequently in comparison to machines, with the intention of setting the stage for more general characterizations in life, collectives, human designs such as AI and in non-designed physical and chemical systems. I discuss key features of intelligence, including path efficiency and goal accuracy, intelligence as a Black Box, environmental influences, flexibility to deal with surprisal, the regress of intelligence, the relativistic nature of intelligen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#20869;&#31397;&#38236;&#35270;&#39057;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#20351;&#29992;Masked Siamese Networks&#65288;MSNs&#65289;&#26694;&#26550;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#21019;&#24314;&#22823;&#35268;&#27169;&#26410;&#26631;&#35760;&#30340;&#35270;&#39057;&#25968;&#25454;&#38598;&#26469;&#20805;&#20998;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#22312;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#30340;&#20108;&#27425;&#35757;&#32451;&#19979;&#65292;&#23454;&#29616;&#20102;&#20869;&#31397;&#38236;&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12394</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#20869;&#31397;&#38236;&#35270;&#39057;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning for Endoscopic Video Analysis. (arXiv:2308.12394v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#20869;&#31397;&#38236;&#35270;&#39057;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#20351;&#29992;Masked Siamese Networks&#65288;MSNs&#65289;&#26694;&#26550;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#21019;&#24314;&#22823;&#35268;&#27169;&#26410;&#26631;&#35760;&#30340;&#35270;&#39057;&#25968;&#25454;&#38598;&#26469;&#20805;&#20998;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#22312;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#30340;&#20108;&#27425;&#35757;&#32451;&#19979;&#65292;&#23454;&#29616;&#20102;&#20869;&#31397;&#38236;&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#36890;&#36807;&#20801;&#35768;&#20174;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#24050;&#32463;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#35201;&#31361;&#30772;&#12290;&#22240;&#27492;&#65292;&#22312;&#38656;&#35201;&#39640;&#24230;&#19987;&#38376;&#21270;&#19987;&#19994;&#30693;&#35782;&#26469;&#27880;&#37322;&#25968;&#25454;&#30340;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#65292;SSL&#21487;&#33021;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#21307;&#30103;&#39046;&#22495;&#20013;&#65292;SSL&#30340;&#24212;&#29992;&#23578;&#26410;&#24191;&#27867;&#25506;&#32034;&#65292;&#20854;&#20013;&#20043;&#19968;&#23601;&#26159;&#20869;&#31397;&#38236;&#26816;&#26597;&#65292;&#36825;&#31181;&#24494;&#21019;&#25163;&#26415;&#24120;&#29992;&#20110;&#26816;&#27979;&#21644;&#27835;&#30103;&#24863;&#26579;&#12289;&#24930;&#24615;&#28814;&#30151;&#24615;&#30142;&#30149;&#25110;&#30284;&#30151;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#39046;&#20808;&#30340;SSL&#26694;&#26550;&#65292;&#21363;Masked Siamese Networks&#65288;MSNs&#65289;&#65292;&#29992;&#20110;&#20869;&#31397;&#38236;&#35270;&#39057;&#20998;&#26512;&#65292;&#20363;&#22914;&#32467;&#32928;&#38236;&#26816;&#26597;&#21644;&#33145;&#33108;&#38236;&#26816;&#26597;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;SSL&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#22823;&#35268;&#27169;&#30340;&#26410;&#26631;&#35760;&#20869;&#31397;&#38236;&#35270;&#39057;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;MSN&#12290;&#36825;&#20123;&#24378;&#22823;&#30340;&#22270;&#20687;&#34920;&#31034;&#20316;&#20026;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#20108;&#27425;&#35757;&#32451;&#30340;&#22522;&#30784;&#65292;&#20351;&#24471;&#22312;&#20869;&#31397;&#38236;&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22914;&#25163;&#26415;&#38454;&#27573;&#35782;&#21035;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has led to important breakthroughs in computer vision by allowing learning from large amounts of unlabeled data. As such, it might have a pivotal role to play in biomedicine where annotating data requires a highly specialized expertise. Yet, there are many healthcare domains for which SSL has not been extensively explored. One such domain is endoscopy, minimally invasive procedures which are commonly used to detect and treat infections, chronic inflammatory diseases or cancer. In this work, we study the use of a leading SSL framework, namely Masked Siamese Networks (MSNs), for endoscopic video analysis such as colonoscopy and laparoscopy. To fully exploit the power of SSL, we create sizable unlabeled endoscopic video datasets for training MSNs. These strong image representations serve as a foundation for secondary training with limited annotated datasets, resulting in state-of-the-art performance in endoscopic benchmarks like surgical phase recognition du
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;$\min-\rightarrow$&#27169;&#31946;&#20851;&#31995;&#26041;&#31243;&#32452;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#35745;&#31639;&#19982;&#35813;&#26041;&#31243;&#32452;&#30456;&#20851;&#30340;Chebyshev&#36317;&#31163;&#30340;&#20998;&#26512;&#20844;&#24335;&#12290;&#26368;&#32456;&#35777;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#35813;&#36317;&#31163;&#21487;&#33021;&#20026;&#19979;&#30830;&#30028;&#65292;&#32780;&#22312;&#20219;&#20309;&#24773;&#20917;&#19979;&#37117;&#20026;&#26368;&#23567;&#20540;&#12290;</title><link>http://arxiv.org/abs/2308.12385</link><description>&lt;p&gt;
&#22788;&#29702;$\min\rightarrow$&#27169;&#31946;&#20851;&#31995;&#26041;&#31243;&#32452;&#30340;&#19981;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Handling the inconsistency of systems of $\min\rightarrow$ fuzzy relational equations. (arXiv:2308.12385v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;$\min-\rightarrow$&#27169;&#31946;&#20851;&#31995;&#26041;&#31243;&#32452;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#35745;&#31639;&#19982;&#35813;&#26041;&#31243;&#32452;&#30456;&#20851;&#30340;Chebyshev&#36317;&#31163;&#30340;&#20998;&#26512;&#20844;&#24335;&#12290;&#26368;&#32456;&#35777;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#35813;&#36317;&#31163;&#21487;&#33021;&#20026;&#19979;&#30830;&#30028;&#65292;&#32780;&#22312;&#20219;&#20309;&#24773;&#20917;&#19979;&#37117;&#20026;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;$\min-\rightarrow$&#27169;&#31946;&#20851;&#31995;&#26041;&#31243;&#32452;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#35745;&#31639;&#19982;&#24418;&#24335;&#20026;$\Gamma \Box_{\rightarrow}^{\min} x = \beta$&#30340;$\min-\rightarrow$&#27169;&#31946;&#20851;&#31995;&#26041;&#31243;&#32452;&#30456;&#20851;&#30340;Chebyshev&#36317;&#31163;$\nabla = \inf_{d \in \mathcal{D}} \Vert \beta - d \Vert$&#30340;&#20998;&#26512;&#20844;&#24335;&#65292;&#20854;&#20013;$\rightarrow$&#26159;G\"odel&#34164;&#21547;&#12289;Goguen&#34164;&#21547;&#25110;Lukasiewicz&#34164;&#21547;&#20043;&#19968;&#65292;$\mathcal{D}$&#26159;&#36890;&#36807;&#30456;&#21516;&#30697;&#38453;$\Gamma$&#23450;&#20041;&#30340;&#19968;&#33268;&#26041;&#31243;&#32452;&#30340;&#31532;&#20108;&#25104;&#21592;&#30340;&#38598;&#21512;&#12290;&#20801;&#35768;&#25105;&#20204;&#24471;&#21040;&#36825;&#20123;&#20844;&#24335;&#30340;&#20027;&#35201;&#21021;&#27493;&#32467;&#26524;&#26159;&#65292;&#26080;&#35770;&#20351;&#29992;&#21738;&#31181;&#27531;&#24046;&#34164;&#21547;&#65292;Chebyshev&#36317;&#31163;$\nabla$&#37117;&#26159;&#19968;&#20010;&#21521;&#37327;&#19981;&#31561;&#24335;&#35299;&#30340;&#19979;&#30028;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;$\min-\rightarrow_{G}$&#31995;&#32479;&#30340;&#24773;&#20917;&#19979;&#65292;Chebyshev&#36317;&#31163;$\nabla$&#21487;&#33021;&#26159;&#19968;&#20010;&#19979;&#30830;&#30028;&#65292;&#32780;&#22312;&#20219;&#20309;&#24773;&#20917;&#19979;&#23427;&#37117;&#26159;&#19968;&#20010;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article, we study the inconsistency of systems of $\min-\rightarrow$ fuzzy relational equations. We give analytical formulas for computing the Chebyshev distances $\nabla = \inf_{d \in \mathcal{D}} \Vert \beta - d \Vert$ associated to systems of $\min-\rightarrow$ fuzzy relational equations of the form $\Gamma \Box_{\rightarrow}^{\min} x = \beta$, where $\rightarrow$ is a residual implicator among the G\"odel implication $\rightarrow_G$, the Goguen implication $\rightarrow_{GG}$ or Lukasiewicz's implication $\rightarrow_L$ and $\mathcal{D}$ is the set of second members of consistent systems defined with the same matrix $\Gamma$. The main preliminary result that allows us to obtain these formulas is that the Chebyshev distance $\nabla$ is the lower bound of the solutions of a vector inequality, whatever the residual implicator used. Finally, we show that, in the case of the $\min-\rightarrow_{G}$ system, the Chebyshev distance $\nabla$ may be an infimum, while it is always a min
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20856;&#22411;&#35760;&#24518;&#32593;&#32476;&#65292;&#29992;&#20110;&#25552;&#21462;&#22270;&#20687;&#30340;&#35821;&#20041;&#24182;&#29983;&#25104;&#35821;&#20041;&#36830;&#36143;&#30340;&#25551;&#36848;&#12290;&#36890;&#36807;&#22312;&#22788;&#29702;&#20854;&#20182;&#35757;&#32451;&#26679;&#26412;&#26102;&#25191;&#34892;&#27880;&#24847;&#21147;&#25805;&#20316;&#65292;&#35813;&#32593;&#32476;&#33021;&#22815;&#21033;&#29992;&#20808;&#21069;&#28608;&#27963;&#30340;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;COCO&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#27169;&#22411;&#21644;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.12383</link><description>&lt;p&gt;
&#36890;&#36807;&#36807;&#21435;&#30340;&#35760;&#24518;&#65306;&#29992;&#20110;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#30340;&#20856;&#22411;&#35760;&#24518;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
With a Little Help from your own Past: Prototypical Memory Networks for Image Captioning. (arXiv:2308.12383v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20856;&#22411;&#35760;&#24518;&#32593;&#32476;&#65292;&#29992;&#20110;&#25552;&#21462;&#22270;&#20687;&#30340;&#35821;&#20041;&#24182;&#29983;&#25104;&#35821;&#20041;&#36830;&#36143;&#30340;&#25551;&#36848;&#12290;&#36890;&#36807;&#22312;&#22788;&#29702;&#20854;&#20182;&#35757;&#32451;&#26679;&#26412;&#26102;&#25191;&#34892;&#27880;&#24847;&#21147;&#25805;&#20316;&#65292;&#35813;&#32593;&#32476;&#33021;&#22815;&#21033;&#29992;&#20808;&#21069;&#28608;&#27963;&#30340;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;COCO&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#27169;&#22411;&#21644;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26159;&#19968;&#20010;&#28041;&#21450;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#20219;&#21153;&#65292;&#30446;&#21069;&#20381;&#36182;&#20110;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#26469;&#25552;&#21462;&#22270;&#20687;&#30340;&#35821;&#20041;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#35821;&#20041;&#36830;&#36143;&#30340;&#25551;&#36848;&#12290;&#23613;&#31649;&#25104;&#21151;&#65292;&#20294;&#27880;&#24847;&#21147;&#26426;&#21046;&#21482;&#32771;&#34385;&#24403;&#21069;&#36755;&#20837;&#26679;&#26412;&#30340;&#25237;&#24433;&#21152;&#26435;&#27714;&#21644;&#65292;&#22240;&#27492;&#24573;&#30053;&#20102;&#26469;&#33258;&#20854;&#20182;&#26679;&#26412;&#30340;&#30456;&#20851;&#35821;&#20041;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#20856;&#22411;&#35760;&#24518;&#27169;&#22411;&#22312;&#22788;&#29702;&#20854;&#20182;&#35757;&#32451;&#26679;&#26412;&#26102;&#25191;&#34892;&#27880;&#24847;&#21147;&#25805;&#20316;&#12290;&#25105;&#20204;&#30340;&#35760;&#24518;&#36890;&#36807;&#21407;&#22411;&#21521;&#37327;&#30340;&#23450;&#20041;&#26469;&#24314;&#27169;&#36807;&#21435;&#30340;&#38190;&#21644;&#20540;&#30340;&#20998;&#24067;&#65292;&#36825;&#20123;&#21407;&#22411;&#21521;&#37327;&#26082;&#20855;&#26377;&#21306;&#20998;&#24615;&#21448;&#32039;&#20945;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#27169;&#22411;&#19982;&#31934;&#24515;&#35774;&#35745;&#30340;&#22522;&#20934;&#27169;&#22411;&#21644;&#26368;&#20808;&#36827;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#30740;&#31350;&#27599;&#20010;&#25552;&#20986;&#30340;&#32452;&#20214;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image captioning, like many tasks involving vision and language, currently relies on Transformer-based architectures for extracting the semantics in an image and translating it into linguistically coherent descriptions. Although successful, the attention operator only considers a weighted summation of projections of the current input sample, therefore ignoring the relevant semantic information which can come from the joint observation of other samples. In this paper, we devise a network which can perform attention over activations obtained while processing other training samples, through a prototypical memory model. Our memory models the distribution of past keys and values through the definition of prototype vectors which are both discriminative and compact. Experimentally, we assess the performance of the proposed model on the COCO dataset, in comparison with carefully designed baselines and state-of-the-art approaches, and by investigating the role of each of the proposed components
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20174;&#22995;&#21517;&#20013;&#25512;&#26029;&#24615;&#21035;&#30340;&#24615;&#33021;&#65292;&#35813;&#26041;&#27861;&#22312;&#27809;&#26377;&#24615;&#21035;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#26159;&#19968;&#31181;&#21487;&#34892;&#19988;&#24191;&#27867;&#24212;&#29992;&#30340;&#26041;&#27861;&#12290;&#20854;&#37325;&#35201;&#24615;&#22312;&#20110;&#30740;&#31350;&#21508;&#31181;&#31185;&#23398;&#23398;&#31185;&#20013;&#23545;&#24615;&#21035;&#24046;&#24322;&#30340;&#27169;&#24335;&#21644;&#20915;&#23450;&#22240;&#32032;&#36827;&#34892;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2308.12381</link><description>&lt;p&gt;
&#20174;&#22995;&#21517;&#20013;&#25512;&#26029;&#24615;&#21035;&#65306;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#24615;&#33021;&#35780;&#20272;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Inferring gender from name: a large scale performance evaluation study. (arXiv:2308.12381v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20174;&#22995;&#21517;&#20013;&#25512;&#26029;&#24615;&#21035;&#30340;&#24615;&#33021;&#65292;&#35813;&#26041;&#27861;&#22312;&#27809;&#26377;&#24615;&#21035;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#26159;&#19968;&#31181;&#21487;&#34892;&#19988;&#24191;&#27867;&#24212;&#29992;&#30340;&#26041;&#27861;&#12290;&#20854;&#37325;&#35201;&#24615;&#22312;&#20110;&#30740;&#31350;&#21508;&#31181;&#31185;&#23398;&#23398;&#31185;&#20013;&#23545;&#24615;&#21035;&#24046;&#24322;&#30340;&#27169;&#24335;&#21644;&#20915;&#23450;&#22240;&#32032;&#36827;&#34892;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#20154;&#30340;&#24615;&#21035;&#26159;&#22312;&#36827;&#34892;&#21307;&#23398;&#12289;&#31038;&#20250;&#23398;&#12289;&#25919;&#27835;&#23398;&#21644;&#32463;&#27982;&#23398;&#31561;&#21508;&#31181;&#31185;&#23398;&#23398;&#31185;&#30340;&#30740;&#31350;&#26102;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22823;&#25968;&#25454;&#30340;&#28608;&#22686;&#65292;&#24615;&#21035;&#20449;&#24687;&#30340;&#33719;&#21462;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#30740;&#31350;&#20154;&#21592;&#38656;&#35201;&#20174;&#21487;&#33719;&#24471;&#30340;&#20449;&#24687;&#20013;&#65292;&#20027;&#35201;&#26159;&#20174;&#20154;&#21517;&#20013;&#25512;&#26029;&#24615;&#21035;&#12290;&#23613;&#31649;&#36890;&#36807;&#22995;&#21517;&#26469;&#25512;&#26029;&#24615;&#21035;&#21487;&#33021;&#24341;&#21457;&#19968;&#20123;&#20262;&#29702;&#38382;&#39064;&#65292;&#20294;&#32570;&#20047;&#21487;&#34892;&#30340;&#26367;&#20195;&#26041;&#27861;&#24847;&#21619;&#30528;&#30740;&#31350;&#20154;&#21592;&#19981;&#24471;&#19981;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#24403;&#30446;&#26631;&#20351;&#25163;&#27573;&#21512;&#29702;&#26102;-&#22312;&#22823;&#22810;&#25968;&#36825;&#31867;&#30740;&#31350;&#20013;&#65292;&#30446;&#26631;&#26159;&#30740;&#31350;&#24615;&#21035;&#24046;&#24322;&#30340;&#27169;&#24335;&#21644;&#20915;&#23450;&#22240;&#32032;&#12290;&#22995;&#21517;&#21040;&#24615;&#21035;&#25512;&#26029;&#30340;&#24517;&#35201;&#24615;&#20135;&#29983;&#20102;&#19968;&#20010;&#36234;&#26469;&#36234;&#22810;&#30340;&#31639;&#27861;&#26041;&#27861;&#21644;&#36719;&#20214;&#20135;&#21697;&#30340;&#39046;&#22495;&#12290;&#36825;&#20123;&#26041;&#27861;&#24050;&#32463;&#22312;&#19990;&#30028;&#21508;&#22320;&#30340;&#23398;&#26415;&#30028;&#12289;&#24037;&#19994;&#30028;&#12289;&#25919;&#24220;&#21644;&#38750;&#25919;&#24220;&#32452;&#32455;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
A person's gender is a crucial piece of information when performing research across a wide range of scientific disciplines, such as medicine, sociology, political science, and economics, to name a few. However, in increasing instances, especially given the proliferation of big data, gender information is not readily available. In such cases researchers need to infer gender from readily available information, primarily from persons' names. While inferring gender from name may raise some ethical questions, the lack of viable alternatives means that researchers have to resort to such approaches when the goal justifies the means - in the majority of such studies the goal is to examine patterns and determinants of gender disparities. The necessity of name-to-gender inference has generated an ever-growing domain of algorithmic approaches and software products. These approaches have been used throughout the world in academia, industry, governmental and non-governmental organizations. Neverthe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#19968;&#32452;&#32039;&#20945;&#30340;&#31070;&#32463;&#32593;&#32476;&#19982;&#22522;&#20110;&#36793;&#32536;&#30340;&#25104;&#26412;&#20989;&#25968;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#25506;&#32034;&#38468;&#21152;&#26679;&#26412;&#26469;&#25552;&#39640;&#24320;&#25918;&#38598;&#20154;&#33080;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22806;&#37096;&#25968;&#25454;&#24211;&#33719;&#21462;&#36741;&#21161;&#36127;&#26679;&#26412;&#25110;&#36890;&#36807;&#28151;&#21512;&#29305;&#24449;&#22686;&#24378;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21512;&#25104;&#24314;&#31435;&#36127;&#26679;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#21319;&#23553;&#38381;&#38598;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12371</link><description>&lt;p&gt;
&#24102;&#26377;&#31070;&#32463;&#38598;&#21512;&#12289;&#26368;&#22823;&#29109;&#25439;&#22833;&#21644;&#29305;&#24449;&#22686;&#24378;&#30340;&#24320;&#25918;&#38598;&#20154;&#33080;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Open-set Face Recognition with Neural Ensemble, Maximal Entropy Loss and Feature Augmentation. (arXiv:2308.12371v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#19968;&#32452;&#32039;&#20945;&#30340;&#31070;&#32463;&#32593;&#32476;&#19982;&#22522;&#20110;&#36793;&#32536;&#30340;&#25104;&#26412;&#20989;&#25968;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#25506;&#32034;&#38468;&#21152;&#26679;&#26412;&#26469;&#25552;&#39640;&#24320;&#25918;&#38598;&#20154;&#33080;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22806;&#37096;&#25968;&#25454;&#24211;&#33719;&#21462;&#36741;&#21161;&#36127;&#26679;&#26412;&#25110;&#36890;&#36807;&#28151;&#21512;&#29305;&#24449;&#22686;&#24378;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21512;&#25104;&#24314;&#31435;&#36127;&#26679;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#21319;&#23553;&#38381;&#38598;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#38598;&#20154;&#33080;&#35782;&#21035;&#26159;&#25351;&#29983;&#29289;&#35782;&#21035;&#31995;&#32479;&#23545;&#25152;&#26377;&#24050;&#23384;&#22312;&#20027;&#20307;&#30340;&#30693;&#35782;&#19981;&#23436;&#25972;&#30340;&#24773;&#20917;&#12290;&#22240;&#27492;&#65292;&#20154;&#20204;&#26399;&#26395;&#23427;&#20204;&#33021;&#22815;&#38450;&#27490;&#23558;&#26410;&#27880;&#20876;&#20027;&#20307;&#30340;&#20154;&#33080;&#26679;&#26412;&#35782;&#21035;&#20026;&#20808;&#21069;&#27880;&#20876;&#30340;&#36523;&#20221;&#12290;&#36825;&#31181;&#30417;&#35270;&#21015;&#34920;&#30340;&#32972;&#26223;&#22686;&#21152;&#20102;&#19968;&#20010;&#33392;&#24040;&#30340;&#35201;&#27714;&#65292;&#35201;&#27714;&#20027;&#35201;&#20851;&#27880;&#24863;&#20852;&#36259;&#30340;&#20027;&#20307;&#65292;&#20174;&#32780;&#25490;&#38500;&#19981;&#30456;&#20851;&#30340;&#20154;&#33080;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#19968;&#32452;&#32039;&#20945;&#30340;&#31070;&#32463;&#32593;&#32476;&#19982;&#22522;&#20110;&#36793;&#32536;&#30340;&#25104;&#26412;&#20989;&#25968;&#30456;&#32467;&#21512;&#65292;&#35813;&#20989;&#25968;&#25506;&#32034;&#38468;&#21152;&#26679;&#26412;&#12290;&#36741;&#21161;&#36127;&#26679;&#26412;&#21487;&#20197;&#20174;&#22806;&#37096;&#25968;&#25454;&#24211;&#20013;&#33719;&#24471;&#65292;&#25110;&#32773;&#22312;&#35757;&#32451;&#26102;&#36890;&#36807;&#26032;&#30340;&#28151;&#21512;&#29305;&#24449;&#22686;&#24378;&#26041;&#27861;&#22312;&#34920;&#31034;&#23618;&#27425;&#19978;&#36827;&#34892;&#21512;&#25104;&#26500;&#24314;&#12290;&#39044;&#20808;&#22312;&#22823;&#22411;&#20154;&#33080;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#21021;&#27493;&#29305;&#24449;&#25552;&#21462;&#27169;&#22359;&#12290;&#25105;&#20204;&#22312;&#30693;&#21517;&#30340;LFW&#21644;IJB-C&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#21319;&#23553;&#38381;&#38598;&#30340;
&lt;/p&gt;
&lt;p&gt;
Open-set face recognition refers to a scenario in which biometric systems have incomplete knowledge of all existing subjects. Therefore, they are expected to prevent face samples of unregistered subjects from being identified as previously enrolled identities. This watchlist context adds an arduous requirement that calls for the dismissal of irrelevant faces by focusing mainly on subjects of interest. As a response, this work introduces a novel method that associates an ensemble of compact neural networks with a margin-based cost function that explores additional samples. Supplementary negative samples can be obtained from external databases or synthetically built at the representation level in training time with a new mix-up feature augmentation approach. Deep neural networks pre-trained on large face datasets serve as the preliminary feature extraction module. We carry out experiments on well-known LFW and IJB-C datasets where results show that the approach is able to boost closed an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#23433;&#20840;&#30340;&#31639;&#27861;&#34917;&#25937;&#26041;&#27861;&#65288;SafeAR&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#39118;&#38505;&#22240;&#32032;&#22312;&#35745;&#31639;&#21644;&#35780;&#20272;&#34917;&#25937;&#25514;&#26045;&#26102;&#65292;&#20026;&#37027;&#20123;&#21463;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20915;&#31574;&#19981;&#21033;&#24433;&#21709;&#30340;&#20010;&#20307;&#25552;&#20379;&#26356;&#21487;&#38752;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2308.12367</link><description>&lt;p&gt;
SafeAR: &#36890;&#36807;&#39118;&#38505;&#24863;&#30693;&#31574;&#30053;&#23454;&#29616;&#26356;&#23433;&#20840;&#30340;&#31639;&#27861;&#34917;&#20607;
&lt;/p&gt;
&lt;p&gt;
SafeAR: Towards Safer Algorithmic Recourse by Risk-Aware Policies. (arXiv:2308.12367v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#23433;&#20840;&#30340;&#31639;&#27861;&#34917;&#25937;&#26041;&#27861;&#65288;SafeAR&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#39118;&#38505;&#22240;&#32032;&#22312;&#35745;&#31639;&#21644;&#35780;&#20272;&#34917;&#25937;&#25514;&#26045;&#26102;&#65292;&#20026;&#37027;&#20123;&#21463;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20915;&#31574;&#19981;&#21033;&#24433;&#21709;&#30340;&#20010;&#20307;&#25552;&#20379;&#26356;&#21487;&#38752;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#37329;&#34701;&#21644;&#21307;&#30103;&#31561;&#20851;&#38190;&#39046;&#22495;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#20026;&#37027;&#20123;&#21463;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20915;&#31574;&#19981;&#21033;&#24433;&#21709;&#30340;&#20010;&#20307;&#25552;&#20379;&#34917;&#25937;&#25514;&#26045;&#30340;&#38656;&#27714;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#65307;&#20010;&#20307;&#24212;&#35813;&#33719;&#24471;&#25913;&#21892;&#33258;&#36523;&#24773;&#20917;&#21644;&#33719;&#24471;&#26377;&#21033;&#20915;&#31574;&#30340;&#24314;&#35758;&#12290;&#20043;&#21069;&#20851;&#20110;&#39034;&#24207;&#31639;&#27861;&#34917;&#25937;&#30340;&#24037;&#20316;&#8212;&#8212;&#25512;&#33616;&#19968;&#31995;&#21015;&#21464;&#21270;&#8212;&#8212;&#20027;&#35201;&#20851;&#27880;&#34892;&#21160;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#20351;&#29992;&#29305;&#24449;&#21464;&#21270;&#30340;&#25509;&#36817;&#31243;&#24230;&#30830;&#23450;&#34892;&#21160;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#26410;&#32771;&#34385;&#29305;&#24449;&#21464;&#21270;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#34917;&#25937;&#20013;&#39640;&#20110;&#24179;&#22343;&#25104;&#26412;&#30340;&#39118;&#38505;&#12290;&#22914;&#26524;&#34917;&#25937;&#25514;&#26045;&#21487;&#33021;&#65288;&#20197;&#19968;&#23450;&#27010;&#29575;&#65289;&#23548;&#33268;&#26356;&#31967;&#31957;&#30340;&#24773;&#20917;&#65292;&#32780;&#24674;&#22797;&#38656;&#35201;&#20184;&#20986;&#38750;&#24120;&#39640;&#30340;&#20195;&#20215;&#65292;&#37027;&#23558;&#26159;&#19981;&#21487;&#21462;&#30340;&#12290;&#22312;&#35745;&#31639;&#21644;&#35780;&#20272;&#34917;&#25937;&#25514;&#26045;&#26102;&#65292;&#24517;&#39035;&#32771;&#34385;&#39118;&#38505;&#12290;&#25105;&#20204;&#23558;&#32771;&#34385;&#20102;&#36825;&#31181;&#39118;&#38505;&#22240;&#32032;&#35745;&#31639;&#20986;&#30340;&#34917;&#25937;&#25514;&#26045;&#31216;&#20026;&#26356;&#23433;&#20840;&#30340;&#31639;&#27861;&#34917;&#25937;&#65288;SafeAR&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growing use of machine learning (ML) models in critical domains such as finance and healthcare, the need to offer recourse for those adversely affected by the decisions of ML models has become more important; individuals ought to be provided with recommendations on actions to take for improving their situation and thus receive a favorable decision. Prior work on sequential algorithmic recourse -- which recommends a series of changes -- focuses on action feasibility and uses the proximity of feature changes to determine action costs. However, the uncertainties of feature changes and the risk of higher than average costs in recourse have not been considered. It is undesirable if a recourse could (with some probability) result in a worse situation from which recovery requires an extremely high cost. It is essential to incorporate risks when computing and evaluating recourse. We call the recourse computed with such risk considerations as Safer Algorithmic Recourse (SafeAR). The ob
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;DNN&#25351;&#32441;&#21435;&#38500;&#25915;&#20987;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RemovalNet&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;min-max&#21452;&#23618;&#20248;&#21270;&#26469;&#36867;&#36991;&#27169;&#22411;&#25152;&#26377;&#26435;&#39564;&#35777;&#12290;&#25915;&#20987;&#26041;&#27861;&#26088;&#22312;&#21435;&#38500;&#25351;&#32441;&#29305;&#23450;&#30693;&#35782;&#65292;&#24182;&#25552;&#21462;&#21463;&#23475;&#27169;&#22411;&#30340;&#36890;&#29992;&#35821;&#20041;&#30693;&#35782;&#26469;&#32500;&#25345;&#26367;&#20195;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12319</link><description>&lt;p&gt;
RemovalNet: DNN&#25351;&#32441;&#21435;&#38500;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
RemovalNet: DNN Fingerprint Removal Attacks. (arXiv:2308.12319v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;DNN&#25351;&#32441;&#21435;&#38500;&#25915;&#20987;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RemovalNet&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;min-max&#21452;&#23618;&#20248;&#21270;&#26469;&#36867;&#36991;&#27169;&#22411;&#25152;&#26377;&#26435;&#39564;&#35777;&#12290;&#25915;&#20987;&#26041;&#27861;&#26088;&#22312;&#21435;&#38500;&#25351;&#32441;&#29305;&#23450;&#30693;&#35782;&#65292;&#24182;&#25552;&#21462;&#21463;&#23475;&#27169;&#22411;&#30340;&#36890;&#29992;&#35821;&#20041;&#30693;&#35782;&#26469;&#32500;&#25345;&#26367;&#20195;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#24615;&#33021;&#30340;&#26174;&#33879;&#25552;&#21319;&#65292;DNNs&#22312;&#35768;&#22810;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;DNN&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#39033;&#23453;&#36149;&#30340;&#36164;&#20135;&#65292;&#20854;&#30693;&#35782;&#20135;&#26435;&#36890;&#36807;&#25152;&#26377;&#26435;&#39564;&#35777;&#25216;&#26415;&#65288;&#22914;DNN&#25351;&#32441;&#65289;&#24471;&#21040;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;DNN&#25351;&#32441;&#21435;&#38500;&#25915;&#20987;&#30340;&#21487;&#34892;&#24615;&#21450;&#20854;&#28508;&#22312;&#24433;&#21709;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#39318;&#27425;&#23545;DNN&#25351;&#32441;&#21435;&#38500;&#25915;&#20987;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#12290;&#19968;&#33324;&#32780;&#35328;&#65292;DNN&#27169;&#22411;&#20013;&#21253;&#21547;&#30340;&#30693;&#35782;&#21487;&#20197;&#20998;&#20026;&#36890;&#29992;&#35821;&#20041;&#30693;&#35782;&#21644;&#25351;&#32441;&#29305;&#23450;&#30693;&#35782;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;min-max&#21452;&#23618;&#20248;&#21270;&#30340;DNN&#25351;&#32441;&#21435;&#38500;&#25915;&#20987;&#8212;&#8212;RemovalNet&#65292;&#20197;&#36867;&#36991;&#27169;&#22411;&#25152;&#26377;&#26435;&#39564;&#35777;&#12290;&#19979;&#23618;&#20248;&#21270;&#26088;&#22312;&#21435;&#38500;&#25351;&#32441;&#29305;&#23450;&#30693;&#35782;&#65292;&#32780;&#19978;&#23618;&#20248;&#21270;&#21017;&#22312;&#32500;&#25345;&#26367;&#20195;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#25552;&#21462;&#21463;&#23475;&#27169;&#22411;&#30340;&#36890;&#29992;&#35821;&#20041;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the performance of deep neural networks (DNNs) remarkably improving, DNNs have been widely used in many areas. Consequently, the DNN model has become a valuable asset, and its intellectual property is safeguarded by ownership verification techniques (e.g., DNN fingerprinting). However, the feasibility of the DNN fingerprint removal attack and its potential influence remains an open problem. In this paper, we perform the first comprehensive investigation of DNN fingerprint removal attacks. Generally, the knowledge contained in a DNN model can be categorized into general semantic and fingerprint-specific knowledge. To this end, we propose a min-max bilevel optimization-based DNN fingerprint removal attack named RemovalNet, to evade model ownership verification. The lower-level optimization is designed to remove fingerprint-specific knowledge. While in the upper-level optimization, we distill the victim model's general semantic knowledge to maintain the surrogate model's performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#36328;&#39046;&#22495;&#30340;&#21487;&#20449;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21253;&#25324;&#40065;&#26834;&#24615;&#12289;&#38544;&#31169;&#12289;&#20844;&#24179;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#31561;&#27010;&#24565;&#65292;&#23545;&#35813;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;</title><link>http://arxiv.org/abs/2308.12315</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#30340;&#21487;&#20449;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Trustworthy Representation Learning Across Domains. (arXiv:2308.12315v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#36328;&#39046;&#22495;&#30340;&#21487;&#20449;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21253;&#25324;&#40065;&#26834;&#24615;&#12289;&#38544;&#31169;&#12289;&#20844;&#24179;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#31561;&#27010;&#24565;&#65292;&#23545;&#35813;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#21644;&#20154;&#31867;&#31038;&#20250;&#20013;&#21462;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20154;&#20204;&#26082;&#20139;&#21463;&#21040;&#20102;&#36825;&#20123;&#25216;&#26415;&#24102;&#26469;&#30340;&#22909;&#22788;&#65292;&#20063;&#38754;&#20020;&#22240;&#36825;&#20123;&#31995;&#32479;&#32780;&#24341;&#21457;&#30340;&#35768;&#22810;&#31038;&#20250;&#38382;&#39064;&#12290;&#20026;&#20102;&#20351;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36275;&#22815;&#22909;&#24182;&#19988;&#21487;&#20449;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#24314;&#31435;&#20102;&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#25351;&#21335;&#12290;&#26426;&#22120;&#23398;&#20064;&#26159;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#26368;&#37325;&#35201;&#30340;&#37096;&#20998;&#20043;&#19968;&#65292;&#32780;&#34920;&#31034;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22522;&#30784;&#25216;&#26415;&#12290;&#22914;&#20309;&#20351;&#34920;&#31034;&#23398;&#20064;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#20855;&#26377;&#21487;&#20449;&#24230;&#65292;&#20363;&#22914;&#36328;&#39046;&#22495;&#22330;&#26223;&#65292;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#39046;&#22495;&#37117;&#26159;&#38750;&#24120;&#26377;&#20215;&#20540;&#21644;&#24517;&#35201;&#30340;&#12290;&#22312;&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#30340;&#27010;&#24565;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36328;&#39046;&#22495;&#30340;&#21487;&#20449;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21253;&#25324;&#20102;&#40065;&#26834;&#24615;&#12289;&#38544;&#31169;&#12289;&#20844;&#24179;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#36825;&#22235;&#20010;&#27010;&#24565;&#65292;&#23545;&#36825;&#20010;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
As AI systems have obtained significant performance to be deployed widely in our daily live and human society, people both enjoy the benefits brought by these technologies and suffer many social issues induced by these systems. To make AI systems good enough and trustworthy, plenty of researches have been done to build guidelines for trustworthy AI systems. Machine learning is one of the most important parts for AI systems and representation learning is the fundamental technology in machine learning. How to make the representation learning trustworthy in real-world application, e.g., cross domain scenarios, is very valuable and necessary for both machine learning and AI system fields. Inspired by the concepts in trustworthy AI, we proposed the first trustworthy representation learning across domains framework which includes four concepts, i.e, robustness, privacy, fairness, and explainability, to give a comprehensive literature review on this research direction. Specifically, we first 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#27979;&#35797;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#22312;&#25551;&#36848;&#31561;&#31163;&#23376;&#20307;&#34701;&#21512;&#21160;&#21147;&#23398;&#27169;&#25311;&#20013;&#30340;&#27874;&#31890;&#20849;&#25391;&#38382;&#39064;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;I-PINN&#30340;PINN&#21464;&#20307;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#33258;&#21160;&#24494;&#20998;&#21644;&#33258;&#21160;&#31215;&#20998;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#21644;&#31215;&#20998;&#26041;&#31243;&#12290;</title><link>http://arxiv.org/abs/2308.12312</link><description>&lt;p&gt;
&#24212;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#25551;&#36848;&#31561;&#31163;&#23376;&#20307;&#34701;&#21512;&#21160;&#21147;&#23398;&#27169;&#25311;&#20013;&#30340;&#27874;&#31890;&#20849;&#25391;
&lt;/p&gt;
&lt;p&gt;
Physics informed Neural Networks applied to the description of wave-particle resonance in kinetic simulations of fusion plasmas. (arXiv:2308.12312v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#27979;&#35797;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#22312;&#25551;&#36848;&#31561;&#31163;&#23376;&#20307;&#34701;&#21512;&#21160;&#21147;&#23398;&#27169;&#25311;&#20013;&#30340;&#27874;&#31890;&#20849;&#25391;&#38382;&#39064;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;I-PINN&#30340;PINN&#21464;&#20307;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#33258;&#21160;&#24494;&#20998;&#21644;&#33258;&#21160;&#31215;&#20998;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#21644;&#31215;&#20998;&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;Vlasov-Poisson&#31995;&#32479;&#30340;&#31616;&#21270;&#24418;&#24335;&#65288;1D1V&#65289;&#24212;&#29992;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#23545;&#27874;&#31890;&#20849;&#25391;&#30340;&#36866;&#29992;&#24615;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#30740;&#31350;&#20102;&#20004;&#20010;&#20363;&#23376;&#65306;Landau&#34928;&#20943;&#21644;&#23614;&#24052;&#19978;&#30340;&#39072;&#31800;&#19981;&#31283;&#23450;&#24615;&#12290;&#39318;&#20808;&#65292;&#23558;PINN&#20316;&#20026;Vlasov-Poisson&#31995;&#32479;&#35299;&#30340;&#21387;&#32553;&#26041;&#27861;&#36827;&#34892;&#27979;&#35797;&#65292;&#24182;&#19982;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20854;&#27425;&#65292;&#36824;&#20171;&#32461;&#20102;&#23558;PINN&#24212;&#29992;&#20110;&#35299;&#20915;Vlasov-Poisson&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#24378;&#35843;&#20102;&#31215;&#20998;&#37096;&#20998;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#24494;&#20998;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#21644;&#33258;&#21160;&#31215;&#20998;&#35299;&#20915;&#31215;&#20998;&#26041;&#31243;&#30340;PINN&#21464;&#20307;&#65292;&#31216;&#20026;&#21487;&#31215;PINN&#65288;I-PINN&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Vlasov-Poisson system is employed in its reduced form version (1D1V) as a test bed for the applicability of Physics Informed Neural Network (PINN) to the wave-particle resonance. Two examples are explored: the Landau damping and the bump-on-tail instability. PINN is first tested as a compression method for the solution of the Vlasov-Poisson system and compared to the standard neural networks. Second, the application of PINN to solving the Vlasov-Poisson system is also presented with the special emphasis on the integral part, which motivates the implementation of a PINN variant, called Integrable PINN (I-PINN), based on the automatic-differentiation to solve the partial differential equation and on the automatic-integration to solve the integral equation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27969;&#34892;&#38899;&#20048;&#21513;&#20182;&#35889;&#20013;&#30340;&#24367;&#38899;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#26512;&#24367;&#38899;&#30340;&#36807;&#21435;&#21644;&#26410;&#26469;&#30701;&#26399;&#19978;&#19979;&#25991;&#26469;&#39044;&#27979;&#24367;&#38899;&#21457;&#29983;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2308.12307</link><description>&lt;p&gt;
&#27169;&#25311;&#27969;&#34892;&#38899;&#20048;&#21513;&#20182;&#35889;&#20013;&#30340;&#24367;&#38899;
&lt;/p&gt;
&lt;p&gt;
Modeling Bends in Popular Music Guitar Tablatures. (arXiv:2308.12307v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27969;&#34892;&#38899;&#20048;&#21513;&#20182;&#35889;&#20013;&#30340;&#24367;&#38899;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#26512;&#24367;&#38899;&#30340;&#36807;&#21435;&#21644;&#26410;&#26469;&#30701;&#26399;&#19978;&#19979;&#25991;&#26469;&#39044;&#27979;&#24367;&#38899;&#21457;&#29983;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24377;&#22863;&#35889;&#26631;&#35760;&#24191;&#27867;&#24212;&#29992;&#20110;&#27969;&#34892;&#38899;&#20048;&#20013;&#65292;&#29992;&#20110;&#36716;&#24405;&#21644;&#20998;&#20139;&#21513;&#20182;&#30340;&#38899;&#20048;&#20869;&#23481;&#12290;&#20316;&#20026;&#26631;&#20934;&#38899;&#31526;&#26631;&#35760;&#30340;&#34917;&#20805;&#65292;&#24377;&#22863;&#35889;&#21487;&#20197;&#36716;&#24405;&#28436;&#22863;&#21160;&#20316;&#20449;&#24687;&#65292;&#21253;&#25324;&#25163;&#25351;&#20301;&#32622;&#21644;&#21508;&#31181;&#21513;&#20182;&#29305;&#23450;&#30340;&#28436;&#22863;&#25216;&#24039;&#65292;&#22914;&#28369;&#38899;&#65292;&#25289;&#24358;&#21644;&#24367;&#38899;&#12290;&#26412;&#25991;&#19987;&#27880;&#20110;&#24367;&#38899;&#65292;&#22312;&#35889;&#20013;&#36880;&#28176;&#25913;&#21464;&#38899;&#39640;&#20174;&#32780;&#36991;&#20813;&#31163;&#25955;&#30340;&#25353;&#24358;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#39640;&#32423;&#29305;&#24449;&#65292;&#38024;&#23545;&#35889;&#20013;&#30340;&#27599;&#20010;&#38899;&#31526;&#36827;&#34892;&#35745;&#31639;&#65292;&#30740;&#31350;&#22914;&#20309;&#36890;&#36807;&#36807;&#21435;&#21644;&#26410;&#26469;&#30340;&#30701;&#26399;&#19978;&#19979;&#25991;&#26469;&#39044;&#27979;&#24367;&#38899;&#30340;&#21457;&#29983;&#12290;&#22312;932&#20010;&#27969;&#34892;&#38899;&#20048;&#30340;&#20027;&#21513;&#20182;&#35889;&#26679;&#26412;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#20915;&#31574;&#26641;&#21487;&#20197;&#25104;&#21151;&#39044;&#27979;&#24367;&#38899;&#30340;&#21457;&#29983;&#65292;F1&#20998;&#25968;&#20026;0.71&#65292;&#24182;&#19988;&#26377;&#21487;&#25509;&#21463;&#30340;&#35823;&#25253;&#29575;&#65292;&#23637;&#31034;&#20102;&#23558;&#38750;&#21513;&#20182;&#38899;&#20048;&#36866;&#24212;&#30340;&#26377;&#24076;&#26395;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tablature notation is widely used in popular music to transcribe and share guitar musical content. As a complement to standard score notation, tablatures transcribe performance gesture information including finger positions and a variety of guitar-specific playing techniques such as slides, hammer-on/pull-off or bends.This paper focuses on bends, which enable to progressively shift the pitch of a note, therefore circumventing physical limitations of the discrete fretted fingerboard. In this paper, we propose a set of 25 high-level features, computed for each note of the tablature, to study how bend occurrences can be predicted from their past and future short-term context. Experiments are performed on a corpus of 932 lead guitar tablatures of popular music and show that a decision tree successfully predicts bend occurrences with an F1 score of 0.71 anda limited amount of false positive predictions, demonstrating promising applications to assist the arrangement of non-guitar music into 
&lt;/p&gt;</description></item><item><title>FedDAT&#26159;&#19968;&#31181;&#22312;&#22810;&#27169;&#24577;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#20013;&#36827;&#34892;&#22522;&#30784;&#27169;&#22411;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#26469;&#20943;&#36731;&#23458;&#25143;&#31471;&#35745;&#31639;&#36127;&#25285;&#21644;&#36890;&#20449;&#24320;&#38144;&#65292;&#24182;&#35299;&#20915;&#20102;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.12305</link><description>&lt;p&gt;
FedDAT: &#19968;&#31181;&#22810;&#27169;&#24577;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#20013;&#22522;&#30784;&#27169;&#22411;&#24494;&#35843;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FedDAT: An Approach for Foundation Model Finetuning in Multi-Modal Heterogeneous Federated Learning. (arXiv:2308.12305v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12305
&lt;/p&gt;
&lt;p&gt;
FedDAT&#26159;&#19968;&#31181;&#22312;&#22810;&#27169;&#24577;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#20013;&#36827;&#34892;&#22522;&#30784;&#27169;&#22411;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#26469;&#20943;&#36731;&#23458;&#25143;&#31471;&#35745;&#31639;&#36127;&#25285;&#21644;&#36890;&#20449;&#24320;&#38144;&#65292;&#24182;&#35299;&#20915;&#20102;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#65292;&#22522;&#30784;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#37197;&#22791;&#20102;&#25968;&#30334;&#19975;&#65288;&#25110;&#25968;&#21313;&#20159;&#65289;&#20010;&#21442;&#25968;&#65292;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#21516;&#30340;&#38544;&#31169;&#27861;&#35268;&#65292;&#20174;&#19981;&#21516;&#39046;&#22495;&#25910;&#38598;&#21644;&#38598;&#20013;&#35757;&#32451;&#25968;&#25454;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#20986;&#29616;&#65292;&#21487;&#20197;&#35753;&#22810;&#20010;&#23458;&#25143;&#31471;&#22312;&#19981;&#38598;&#20013;&#20854;&#26412;&#22320;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20849;&#21516;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;&#20026;&#20102;&#20943;&#36731;&#23458;&#25143;&#31471;&#30340;&#35745;&#31639;&#36127;&#25285;&#21644;&#36890;&#20449;&#24320;&#38144;&#65292;&#20043;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#37319;&#29992;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#29992;&#20110;FL&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#21482;&#26377;&#23569;&#37327;&#27169;&#22411;&#21442;&#25968;&#22312;&#32852;&#37030;&#36890;&#20449;&#26399;&#38388;&#36827;&#34892;&#20248;&#21270;&#21644;&#20256;&#36755;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#24037;&#20316;&#37117;&#19987;&#27880;&#20110;&#21333;&#19968;&#24418;&#24577;&#65292;&#24182;&#24573;&#30053;&#20102;&#19968;&#31181;&#24120;&#35265;&#29616;&#35937;&#65292;&#21363;&#23458;&#25143;&#31471;&#20043;&#38388;&#23384;&#22312;&#25968;&#25454;&#24322;&#26500;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24494;&#35843;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, foundation models have exhibited remarkable advancements in multi-modal learning. These models, equipped with millions (or billions) of parameters, typically require a substantial amount of data for finetuning. However, collecting and centralizing training data from diverse sectors becomes challenging due to distinct privacy regulations. Federated Learning (FL) emerges as a promising solution, enabling multiple clients to collaboratively train neural networks without centralizing their local data. To alleviate client computation burdens and communication overheads, previous works have adapted Parameter-efficient Finetuning (PEFT) methods for FL. Hereby, only a small fraction of the model parameters are optimized and communicated during federated communications. Nevertheless, most previous works have focused on a single modality and neglected one common phenomenon, i.e., the presence of data heterogeneity across the clients. Therefore, in this work, we propose a finetuning fra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CLIPN&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31215;&#26497;&#30340;&#35821;&#20041;&#25552;&#31034;&#21644;&#21542;&#23450;&#30340;&#35821;&#20041;&#25552;&#31034;&#65292;&#20026;CLIP&#36171;&#20104;&#20102;&#21306;&#20998;OOD&#21644;ID&#26679;&#26412;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.12213</link><description>&lt;p&gt;
CLIPN&#29992;&#20110;&#38646;&#26679;&#26412;OOD&#26816;&#27979;&#65306;&#25945;CLIP&#35828;&#8220;&#19981;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say No. (arXiv:2308.12213v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CLIPN&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31215;&#26497;&#30340;&#35821;&#20041;&#25552;&#31034;&#21644;&#21542;&#23450;&#30340;&#35821;&#20041;&#25552;&#31034;&#65292;&#20026;CLIP&#36171;&#20104;&#20102;&#21306;&#20998;OOD&#21644;ID&#26679;&#26412;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
OOD&#26816;&#27979;&#26159;&#25351;&#22312;&#20869;&#37096;&#20998;&#24067;&#65288;ID&#65289;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#20998;&#31867;&#36755;&#20837;&#22270;&#20687;&#26469;&#33258;&#26410;&#30693;&#31867;&#21035;&#12290;&#35774;&#35745;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25110;Transformer&#30340;&#21508;&#31181;OOD&#26816;&#27979;&#26041;&#27861;&#24050;&#32463;&#20184;&#20986;&#20102;&#30456;&#24403;&#22810;&#30340;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#20165;&#38656;&#35201;ID&#30340;&#31867;&#21517;&#30340;CLIP&#39537;&#21160;&#30340;&#38646;&#26679;&#26412;OOD&#26816;&#27979;&#26041;&#27861;&#21364;&#21463;&#21040;&#36739;&#23569;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;CLIP&#35828;&#8220;&#19981;&#8221;&#65288;CLIPN&#65289;&#65292;&#23427;&#36171;&#20104;&#20102;CLIP&#22312;&#36923;&#36753;&#19978;&#35828;&#8220;&#19981;&#8221;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21160;&#26426;&#26159;&#36890;&#36807;&#31215;&#26497;&#30340;&#35821;&#20041;&#25552;&#31034;&#21644;&#21542;&#23450;&#30340;&#35821;&#20041;&#25552;&#31034;&#65292;&#20026;CLIP&#25552;&#20379;&#21306;&#20998;OOD&#26679;&#26412;&#21644;ID&#26679;&#26412;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#23398;&#20064;&#30340;&#8220;&#19981;&#8221;&#25552;&#31034;&#31526;&#21644;&#19968;&#20010;&#8220;&#19981;&#8221;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#20197;&#25429;&#33719;&#22270;&#20687;&#20013;&#30340;&#21542;&#23450;&#35821;&#20041;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#25439;&#22833;&#20989;&#25968;&#65306;&#22270;&#20687;-&#25991;&#26412;&#20108;&#20803;&#30456;&#21453;&#25439;&#22833;&#21644;&#25991;&#26412;&#35821;&#20041;&#30456;&#21453;&#25439;&#22833;&#65292;&#29992;&#20110;&#25945;&#25480;CLIPN&#20851;&#32852;OOD&#21644;ID&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection refers to training the model on an in-distribution (ID) dataset to classify whether the input images come from unknown classes. Considerable effort has been invested in designing various OOD detection methods based on either convolutional neural networks or transformers. However, zero-shot OOD detection methods driven by CLIP, which only require class names for ID, have received less attention. This paper presents a novel method, namely CLIP saying "no" (\textbf{CLIPN}), which empowers the logic of saying "no" within CLIP. Our key motivation is to equip CLIP with the capability of distinguishing OOD and ID samples using positive-semantic prompts and negation-semantic prompts. Specifically, we design a novel learnable "no" prompt and a "no" text encoder to capture negation semantics within images. Subsequently, we introduce two loss functions: the image-text binary-opposite loss and the text semantic-opposite loss, which we use to teach CLIPN to assoc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#24310;&#32493;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21017;&#21270;&#36335;&#24452;&#65292;&#20197;&#35299;&#20915;DNNs&#20013;&#31232;&#30095;&#24615;&#21644;&#25968;&#20540;&#25928;&#29575;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;</title><link>http://arxiv.org/abs/2308.12044</link><description>&lt;p&gt;
&#29992;&#20110;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#36335;&#24452;&#30340;&#22810;&#30446;&#26631;&#24310;&#32493;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A multiobjective continuation method to compute the regularization path of deep neural networks. (arXiv:2308.12044v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#24310;&#32493;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21017;&#21270;&#36335;&#24452;&#65292;&#20197;&#35299;&#20915;DNNs&#20013;&#31232;&#30095;&#24615;&#21644;&#25968;&#20540;&#25928;&#29575;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#24615;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#20013;&#38750;&#24120;&#29702;&#24819;&#30340;&#29305;&#24449;&#65292;&#22240;&#20026;&#23427;&#30830;&#20445;&#20102;&#25968;&#20540;&#25928;&#29575;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;(&#30001;&#20110;&#30456;&#20851;&#29305;&#24449;&#30340;&#25968;&#37327;&#36739;&#23569;)&#21644;&#40065;&#26834;&#24615;&#12290;&#22312;&#22522;&#20110;&#32447;&#24615;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#20247;&#25152;&#21608;&#30693;&#22312;$\ell^1$&#33539;&#25968;(&#21363;&#38646;&#26435;&#37325;)&#30340;&#26368;&#31232;&#30095;&#35299;&#21644;&#38750;&#27491;&#21017;&#21270;&#35299;&#20043;&#38388;&#23384;&#22312;&#19968;&#26465;&#36830;&#25509;&#36335;&#24452;&#65292;&#36825;&#26465;&#36335;&#24452;&#34987;&#31216;&#20026;&#27491;&#21017;&#21270;&#36335;&#24452;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#23558;&#32463;&#39564;&#25439;&#22833;&#21644;&#31232;&#30095;&#24615;($\ell^1$&#33539;&#25968;)&#20316;&#20026;&#20004;&#20010;&#20914;&#31361;&#30340;&#26631;&#20934;&#65292;&#24182;&#35299;&#20915;&#30001;&#27492;&#20135;&#29983;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#39318;&#27425;&#23581;&#35797;&#23558;&#27491;&#21017;&#21270;&#36335;&#24452;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;DNNs&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;$\ell^1$&#33539;&#25968;&#30340;&#19981;&#20809;&#28369;&#24615;&#21644;&#21442;&#25968;&#25968;&#37327;&#30340;&#39640;&#24230;&#65292;&#20174;&#35745;&#31639;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#26041;&#27861;&#24182;&#19981;&#26159;&#24456;&#26377;&#25928;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#36817;&#20284;&#35745;&#31639;&#25972;&#20010;&#24085;&#32047;&#25176;&#26354;&#32447;
&lt;/p&gt;
&lt;p&gt;
Sparsity is a highly desired feature in deep neural networks (DNNs) since it ensures numerical efficiency, improves the interpretability of models (due to the smaller number of relevant features), and robustness. In machine learning approaches based on linear models, it is well known that there exists a connecting path between the sparsest solution in terms of the $\ell^1$ norm (i.e., zero weights) and the non-regularized solution, which is called the regularization path. Very recently, there was a first attempt to extend the concept of regularization paths to DNNs by means of treating the empirical loss and sparsity ($\ell^1$ norm) as two conflicting criteria and solving the resulting multiobjective optimization problem. However, due to the non-smoothness of the $\ell^1$ norm and the high number of parameters, this approach is not very efficient from a computational perspective. To overcome this limitation, we present an algorithm that allows for the approximation of the entire Pareto
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#32593;&#32476;&#65292;&#32467;&#21512;&#22270;&#20687;&#21644;&#20301;&#32622;&#20449;&#24687;&#36827;&#34892;&#21019;&#20260;&#20998;&#31867;&#65292;&#36890;&#36807;&#24341;&#20837;&#20307;&#37096;&#22270;&#35889;&#31995;&#32479;&#25552;&#20379;&#31934;&#30830;&#30340;&#21019;&#20260;&#20301;&#32622;&#26631;&#35760;&#65292;&#24182;&#22312;&#26032;&#39062;&#30340;&#26550;&#26500;&#20013;&#25972;&#21512;&#22810;&#20010;&#27169;&#22411;&#20197;&#25552;&#39640;&#20998;&#31867;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.11877</link><description>&lt;p&gt;
&#32508;&#21512;&#22270;&#20687;&#21644;&#20301;&#32622;&#20998;&#26512;&#29992;&#20110;&#21019;&#20260;&#20998;&#31867;&#65306;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Integrated Image and Location Analysis for Wound Classification: A Deep Learning Approach. (arXiv:2308.11877v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#32593;&#32476;&#65292;&#32467;&#21512;&#22270;&#20687;&#21644;&#20301;&#32622;&#20449;&#24687;&#36827;&#34892;&#21019;&#20260;&#20998;&#31867;&#65292;&#36890;&#36807;&#24341;&#20837;&#20307;&#37096;&#22270;&#35889;&#31995;&#32479;&#25552;&#20379;&#31934;&#30830;&#30340;&#21019;&#20260;&#20301;&#32622;&#26631;&#35760;&#65292;&#24182;&#22312;&#26032;&#39062;&#30340;&#26550;&#26500;&#20013;&#25972;&#21512;&#22810;&#20010;&#27169;&#22411;&#20197;&#25552;&#39640;&#20998;&#31867;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24613;&#24615;&#21644;&#24930;&#24615;&#21019;&#20260;&#30340;&#20840;&#29699;&#36127;&#25285;&#20026;&#22686;&#24378;&#21019;&#20260;&#20998;&#31867;&#26041;&#27861;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#26696;&#20363;&#65292;&#36825;&#26159;&#35786;&#26029;&#21644;&#30830;&#23450;&#26368;&#20339;&#27835;&#30103;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#35748;&#35782;&#21040;&#36825;&#19968;&#38656;&#27714;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22810;&#27169;&#24577;&#32593;&#32476;&#65292;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#21019;&#20260;&#20998;&#31867;&#20026;&#22235;&#31867;&#65306;&#31958;&#23615;&#30149;&#24615;&#12289;&#21387;&#21147;&#24615;&#12289;&#22806;&#31185;&#24615;&#21644;&#38745;&#33033;&#28291;&#30113;&#12290;&#25105;&#20204;&#30340;&#22810;&#27169;&#24577;&#32593;&#32476;&#20351;&#29992;&#21019;&#20260;&#22270;&#20687;&#21450;&#20854;&#30456;&#24212;&#30340;&#20301;&#32622;&#20449;&#24687;&#36827;&#34892;&#26356;&#31934;&#30830;&#30340;&#20998;&#31867;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#24341;&#20837;&#20102;&#19968;&#20010;&#20307;&#37096;&#22270;&#35889;&#31995;&#32479;&#65292;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#21019;&#20260;&#20301;&#32622;&#26631;&#35760;&#65292;&#25913;&#36827;&#20102;&#20256;&#32479;&#30340;&#21019;&#20260;&#22270;&#20687;&#20998;&#31867;&#25216;&#26415;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#29420;&#29305;&#29305;&#28857;&#26159;&#22312;&#19968;&#20010;&#26032;&#39062;&#30340;&#26550;&#26500;&#20013;&#25972;&#21512;&#20102;VGG16&#12289;ResNet152&#21644;EfficientNet&#31561;&#27169;&#22411;&#12290;&#35813;&#26550;&#26500;&#21253;&#25324;&#20102;&#31354;&#38388;&#21644;&#36890;&#36947;&#32423;&#30340;Squeeze-and-Excitation&#27169;&#22359;&#12289;Axial Attention&#21644;&#33258;&#36866;&#24212;&#38376;&#25511;&#22810;&#23618;&#24863;&#30693;&#26426;&#31561;&#20803;&#32032;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The global burden of acute and chronic wounds presents a compelling case for enhancing wound classification methods, a vital step in diagnosing and determining optimal treatments. Recognizing this need, we introduce an innovative multi-modal network based on a deep convolutional neural network for categorizing wounds into four categories: diabetic, pressure, surgical, and venous ulcers. Our multi-modal network uses wound images and their corresponding body locations for more precise classification. A unique aspect of our methodology is incorporating a body map system that facilitates accurate wound location tagging, improving upon traditional wound image classification techniques. A distinctive feature of our approach is the integration of models such as VGG16, ResNet152, and EfficientNet within a novel architecture. This architecture includes elements like spatial and channel-wise Squeeze-and-Excitation modules, Axial Attention, and an Adaptive Gated Multi-Layer Perceptron, providing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#21644;&#20943;&#23569;&#24320;&#28304;&#24369;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#24182;&#25506;&#32034;&#20102;&#30693;&#35782;&#27880;&#20837;&#21644;&#24072;&#29983;&#26041;&#27861;&#31561;&#25216;&#26415;&#26469;&#20943;&#36731;&#20302;&#21442;&#25968;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25361;&#25112;&#24615;&#39046;&#22495;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24187;&#35273;&#38382;&#39064;&#24471;&#21040;&#20102;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2308.11764</link><description>&lt;p&gt;
Halo&#65306;&#35780;&#20272;&#21644;&#38477;&#20302;&#24320;&#28304;&#24369;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models. (arXiv:2308.11764v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#21644;&#20943;&#23569;&#24320;&#28304;&#24369;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#24182;&#25506;&#32034;&#20102;&#30693;&#35782;&#27880;&#20837;&#21644;&#24072;&#29983;&#26041;&#27861;&#31561;&#25216;&#26415;&#26469;&#20943;&#36731;&#20302;&#21442;&#25968;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25361;&#25112;&#24615;&#39046;&#22495;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24187;&#35273;&#38382;&#39064;&#24471;&#21040;&#20102;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#39046;&#22495;&#12290;&#34429;&#28982;&#23545;&#20110;&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#26469;&#35828;&#26041;&#20415;&#65292;&#20294;&#26159;&#19982;&#20854;&#26356;&#22823;&#35268;&#27169;&#30340;&#23545;&#24212;&#27169;&#22411;&#30456;&#27604;&#65292;&#24320;&#28304;&#30340;&#21442;&#25968;&#36739;&#23569;&#30340;LLMs&#32463;&#24120;&#20986;&#29616;&#20005;&#37325;&#24187;&#35273;&#38382;&#39064;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#27979;&#37327;&#21644;&#20943;&#23569;BLOOM 7B&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#26159;&#20844;&#24320;&#25552;&#20379;&#32473;&#30740;&#31350;&#21644;&#21830;&#19994;&#24212;&#29992;&#30340;&#24369;&#24320;&#28304;LLMs&#30340;&#20195;&#34920;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;HaloCheck&#65292;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26080;&#38656;&#30693;&#35782;&#30340;&#40657;&#30418;&#23376;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;LLMs&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#30693;&#35782;&#27880;&#20837;&#21644;&#24072;&#29983;&#26041;&#27861;&#31561;&#25216;&#26415;&#65292;&#20197;&#20943;&#36731;&#20302;&#21442;&#25968;LLMs&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#22312;&#36825;&#20123;LLMs&#30340;&#25361;&#25112;&#24615;&#39046;&#22495;&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP). Although convenient for research and practical applications, open-source LLMs with fewer parameters often suffer from severe hallucinations compared to their larger counterparts. This paper focuses on measuring and reducing hallucinations in BLOOM 7B, a representative of such weaker open-source LLMs that are publicly available for research and commercial applications. We introduce HaloCheck, a lightweight BlackBox knowledge-free framework designed to quantify the severity of hallucinations in LLMs. Additionally, we explore techniques like knowledge injection and teacher-student approaches to alleviate hallucinations in low-parameter LLMs. Our experiments effectively demonstrate the reduction of hallucinations in challenging domains for these LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#24320;&#25918;&#35789;&#27719;&#22686;&#24378;&#30340;&#26234;&#33021;&#23433;&#20840;&#30528;&#38470;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#24320;&#25918;&#35789;&#27719;&#22270;&#20687;&#20998;&#21106;&#30340;&#33021;&#21147;&#23454;&#29616;&#26080;&#20154;&#26426;&#30340;&#35270;&#35273;&#20282;&#26381;&#65292;&#36866;&#24212;&#19981;&#21516;&#22330;&#26223;&#19988;&#26080;&#38656;&#22823;&#37327;&#25968;&#25454;&#31215;&#32047;&#36827;&#34892;&#27169;&#22411;&#25913;&#36827;&#65292;&#21487;&#20197;&#22788;&#29702;100&#31859;&#39640;&#24230;&#30340;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2308.11471</link><description>&lt;p&gt;
&#21160;&#24577;&#24320;&#25918;&#35789;&#27719;&#22686;&#24378;&#30340;&#26234;&#33021;&#23433;&#20840;&#30528;&#38470;&#65288;DOVESEI&#65289;
&lt;/p&gt;
&lt;p&gt;
Dynamic Open Vocabulary Enhanced Safe-landing with Intelligence (DOVESEI). (arXiv:2308.11471v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#24320;&#25918;&#35789;&#27719;&#22686;&#24378;&#30340;&#26234;&#33021;&#23433;&#20840;&#30528;&#38470;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#24320;&#25918;&#35789;&#27719;&#22270;&#20687;&#20998;&#21106;&#30340;&#33021;&#21147;&#23454;&#29616;&#26080;&#20154;&#26426;&#30340;&#35270;&#35273;&#20282;&#26381;&#65292;&#36866;&#24212;&#19981;&#21516;&#22330;&#26223;&#19988;&#26080;&#38656;&#22823;&#37327;&#25968;&#25454;&#31215;&#32047;&#36827;&#34892;&#27169;&#22411;&#25913;&#36827;&#65292;&#21487;&#20197;&#22788;&#29702;100&#31859;&#39640;&#24230;&#30340;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22478;&#24066;&#31354;&#20013;&#26426;&#22120;&#20154;&#30340;&#22522;&#30784;&#27493;&#39588;&#20043;&#19968;&#65292;&#21363;&#23433;&#20840;&#30528;&#38470;&#12290;&#25105;&#20204;&#20851;&#27880;&#23433;&#20840;&#30528;&#38470;&#24863;&#30693;&#22534;&#26632;&#20013;&#26368;&#20851;&#38190;&#30340;&#26041;&#38754;&#20043;&#19968;&#65292;&#21363;&#20998;&#21106;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#21453;&#24212;&#24335;&#26080;&#20154;&#26426;&#31995;&#32479;&#65292;&#21033;&#29992;&#24320;&#25918;&#35789;&#27719;&#22270;&#20687;&#20998;&#21106;&#30340;&#33021;&#21147;&#23454;&#29616;&#35270;&#35273;&#20282;&#26381;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#22330;&#26223;&#65292;&#24182;&#36890;&#36807;&#20854;&#24320;&#25918;&#35789;&#27719;&#26041;&#27861;&#65292;&#26368;&#23567;&#21270;&#35843;&#25972;&#38656;&#27714;&#65292;&#32469;&#36807;&#23545;&#20869;&#37096;&#27169;&#22411;&#36827;&#34892;&#22823;&#37327;&#25968;&#25454;&#31215;&#32047;&#20197;&#36827;&#34892;&#25913;&#36827;&#30340;&#24517;&#35201;&#24615;&#12290;&#32771;&#34385;&#21040;&#24403;&#22320;&#24403;&#23616;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;&#20174;100&#31859;&#39640;&#24230;&#36215;&#39134;&#30340;&#25805;&#20316;&#12290;&#36825;&#20010;&#36873;&#25321;&#26159;&#26377;&#24847;&#30340;&#65292;&#22240;&#20026;&#35768;&#22810;&#20043;&#21069;&#30340;&#24037;&#20316;&#22788;&#29702;&#30340;&#39640;&#24230;&#20165;&#38480;&#20110;30&#31859;&#65292;&#19982;&#23567;&#22411;&#31435;&#20307;&#30456;&#26426;&#30340;&#33021;&#21147;&#30456;&#21563;&#21512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#20256;&#32479;&#30340;&#19977;&#32500;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#26469;&#23548;&#33322;&#21097;&#19979;&#30340;20&#31859;&#12290;&#21033;&#29992;&#21333;&#30446;&#30456;&#26426;&#21644;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
This work targets what we consider to be the foundational step for urban airborne robots, a safe landing. Our attention is directed toward what we deem the most crucial aspect of the safe landing perception stack: segmentation. We present a streamlined reactive UAV system that employs visual servoing by harnessing the capabilities of open vocabulary image segmentation. This approach can adapt to various scenarios with minimal adjustments, bypassing the necessity for extensive data accumulation for refining internal models, thanks to its open vocabulary methodology. Given the limitations imposed by local authorities, our primary focus centers on operations originating from altitudes of 100 meters. This choice is deliberate, as numerous preceding works have dealt with altitudes up to 30 meters, aligning with the capabilities of small stereo cameras. Consequently, we leave the remaining 20m to be navigated using conventional 3D path planning methods. Utilizing monocular cameras and image 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#31169;&#26377;&#39046;&#22495;&#25968;&#25454;&#21327;&#21516;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#36328;&#22330;&#26223;&#30340;&#26234;&#33021;&#26381;&#21153;&#12290;&#22312;&#22823;&#27169;&#22411;&#26102;&#20195;&#65292;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#24322;&#26500;&#25968;&#25454;&#12289;&#27169;&#22411;&#32858;&#21512;&#12289;&#24615;&#33021;&#21644;&#25104;&#26412;&#26435;&#34913;&#12289;&#25968;&#25454;&#38544;&#31169;&#20197;&#21450;&#28608;&#21169;&#26426;&#21046;&#31561;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.11217</link><description>&lt;p&gt;
&#22823;&#27169;&#22411;&#26102;&#20195;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#65306;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#22810;&#27169;&#24577;&#22823;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models. (arXiv:2308.11217v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#31169;&#26377;&#39046;&#22495;&#25968;&#25454;&#21327;&#21516;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#36328;&#22330;&#26223;&#30340;&#26234;&#33021;&#26381;&#21153;&#12290;&#22312;&#22823;&#27169;&#22411;&#26102;&#20195;&#65292;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#24322;&#26500;&#25968;&#25454;&#12289;&#27169;&#22411;&#32858;&#21512;&#12289;&#24615;&#33021;&#21644;&#25104;&#26412;&#26435;&#34913;&#12289;&#25968;&#25454;&#38544;&#31169;&#20197;&#21450;&#28608;&#21169;&#26426;&#21046;&#31561;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#25968;&#25454;&#33021;&#22815;&#20840;&#38754;&#24863;&#30693;&#21644;&#35782;&#21035;&#29289;&#29702;&#19990;&#30028;&#65292;&#24050;&#25104;&#20026;&#36890;&#24448;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#36335;&#24452;&#12290;&#28982;&#32780;&#65292;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#22823;&#27169;&#22411;&#22312;&#29305;&#23450;&#24037;&#19994;&#39046;&#22495;&#30340;&#24615;&#33021;&#24448;&#24448;&#19981;&#29702;&#24819;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#22810;&#20010;&#20225;&#19994;&#21033;&#29992;&#31169;&#26377;&#39046;&#22495;&#25968;&#25454;&#21327;&#21516;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#65292;&#23454;&#29616;&#36328;&#22330;&#26223;&#30340;&#26234;&#33021;&#26381;&#21153;&#12290;&#20316;&#32773;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#27169;&#22411;&#26102;&#20195;&#32852;&#37030;&#23398;&#20064;&#30340;&#26234;&#33021;&#22522;&#30784;&#21644;&#30446;&#26631;&#30340;&#25112;&#30053;&#36716;&#21464;&#65292;&#20197;&#21450;&#22312;&#24322;&#26500;&#25968;&#25454;&#12289;&#27169;&#22411;&#32858;&#21512;&#12289;&#24615;&#33021;&#21644;&#25104;&#26412;&#26435;&#34913;&#12289;&#25968;&#25454;&#38544;&#31169;&#21644;&#28608;&#21169;&#26426;&#21046;&#26041;&#38754;&#38754;&#20020;&#30340;&#26032;&#25361;&#25112;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#39046;&#20808;&#20225;&#19994;&#22312;&#22478;&#24066;&#23433;&#20840;&#36816;&#33829;&#31649;&#29702;&#26041;&#38754;&#36129;&#29486;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#19987;&#23478;&#30693;&#35782;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21253;&#25324;&#20998;&#24067;&#24335;&#37096;&#32626;&#21644;&#39640;&#25928;&#24615;&#33021;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal data, which can comprehensively perceive and recognize the physical world, has become an essential path towards general artificial intelligence. However, multimodal large models trained on public datasets often underperform in specific industrial domains. This paper proposes a multimodal federated learning framework that enables multiple enterprises to utilize private domain data to collaboratively train large models for vertical domains, achieving intelligent services across scenarios. The authors discuss in-depth the strategic transformation of federated learning in terms of intelligence foundation and objectives in the era of big model, as well as the new challenges faced in heterogeneous data, model aggregation, performance and cost trade-off, data privacy, and incentive mechanism. The paper elaborates a case study of leading enterprises contributing multimodal data and expert knowledge to city safety operation management , including distributed deployment and efficient 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#21487;&#35270;&#21270;&#20154;&#32676;&#20998;&#26512;&#30340;&#20845;&#20010;&#20027;&#35201;&#39046;&#22495;&#65292;&#24182;&#27010;&#36848;&#20102;&#22312;&#26410;&#26469;&#24037;&#20316;&#20013;&#38656;&#35201;&#35299;&#20915;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#20197;&#30830;&#20445;&#33258;&#21160;&#20154;&#32676;&#30417;&#27979;&#39046;&#22495;&#30340;&#36827;&#23637;&#21644;&#32321;&#33635;&#12290;</title><link>http://arxiv.org/abs/2308.10677</link><description>&lt;p&gt;
&#21487;&#35270;&#21270;&#20154;&#32676;&#20998;&#26512;&#65306;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Visual Crowd Analysis: Open Research Problems. (arXiv:2308.10677v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#21487;&#35270;&#21270;&#20154;&#32676;&#20998;&#26512;&#30340;&#20845;&#20010;&#20027;&#35201;&#39046;&#22495;&#65292;&#24182;&#27010;&#36848;&#20102;&#22312;&#26410;&#26469;&#24037;&#20316;&#20013;&#38656;&#35201;&#35299;&#20915;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#20197;&#30830;&#20445;&#33258;&#21160;&#20154;&#32676;&#30417;&#27979;&#39046;&#22495;&#30340;&#36827;&#23637;&#21644;&#32321;&#33635;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#23545;&#33258;&#21160;&#21270;&#20154;&#32676;&#30417;&#27979;&#30340;&#20852;&#36259;&#36805;&#29467;&#22686;&#38271;&#12290;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20351;&#24471;&#24320;&#21457;&#20840;&#33258;&#21160;&#22522;&#20110;&#35270;&#35273;&#30340;&#20154;&#32676;&#30417;&#27979;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#38382;&#39064;&#30340;&#35268;&#27169;&#24222;&#22823;&#65292;&#25216;&#26415;&#30340;&#26174;&#33879;&#36827;&#27493;&#20197;&#21450;&#30740;&#31350;&#30028;&#30340;&#25345;&#32493;&#20851;&#27880;&#65292;&#20173;&#28982;&#23384;&#22312;&#35768;&#22810;&#38656;&#35201;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#21487;&#35270;&#21270;&#20154;&#32676;&#20998;&#26512;&#30340;&#20845;&#20010;&#20027;&#35201;&#39046;&#22495;&#65292;&#24378;&#35843;&#20102;&#27599;&#20010;&#39046;&#22495;&#30340;&#20851;&#38190;&#21457;&#23637;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#24517;&#39035;&#22312;&#26410;&#26469;&#30340;&#24037;&#20316;&#20013;&#35299;&#20915;&#30340;&#37325;&#22823;&#26410;&#35299;&#20915;&#38382;&#39064;&#65292;&#20197;&#30830;&#20445;&#33258;&#21160;&#20154;&#32676;&#30417;&#27979;&#39046;&#22495;&#32487;&#32493;&#21462;&#24471;&#36827;&#23637;&#21644;&#32321;&#33635;&#12290;&#36807;&#21435;&#24050;&#32463;&#36827;&#34892;&#20102;&#19968;&#20123;&#30456;&#20851;&#20027;&#39064;&#30340;&#35843;&#26597;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#26412;&#25991;&#20840;&#38754;&#32771;&#23519;&#24182;&#21576;&#29616;&#20102;&#26356;&#30452;&#35266;&#30340;&#20316;&#21697;&#20998;&#31867;&#65292;&#21516;&#26102;&#25551;&#36848;&#20102;&#26368;&#26032;&#30340;&#31361;&#30772;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last decade, there has been a remarkable surge in interest in automated crowd monitoring within the computer vision community. Modern deep-learning approaches have made it possible to develop fully-automated vision-based crowd-monitoring applications. However, despite the magnitude of the issue at hand, the significant technological advancements, and the consistent interest of the research community, there are still numerous challenges that need to be overcome. In this article, we delve into six major areas of visual crowd analysis, emphasizing the key developments in each of these areas. We outline the crucial unresolved issues that must be tackled in future works, in order to ensure that the field of automated crowd monitoring continues to progress and thrive. Several surveys related to this topic have been conducted in the past. Nonetheless, this article thoroughly examines and presents a more intuitive categorization of works, while also depicting the latest breakthroughs 
&lt;/p&gt;</description></item><item><title>&#20803;&#23431;&#23449;&#26159;&#19968;&#31181;&#21487;&#20280;&#32553;&#19988;&#23454;&#26102;&#30340;&#34394;&#25311;&#19990;&#30028;&#65292;&#23427;&#36890;&#36807;&#34394;&#25311;&#21644;&#22686;&#24378;&#29616;&#23454;&#25216;&#26415;&#25299;&#23637;&#29289;&#29702;&#19990;&#30028;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#19982;&#30495;&#23454;&#21644;&#34394;&#25311;&#19990;&#30028;&#26080;&#32541;&#20132;&#20114;&#12290;&#23427;&#23545;&#31038;&#20132;&#23186;&#20307;&#12289;&#21512;&#20316;&#24037;&#20316;&#12289;&#24066;&#22330;&#33829;&#38144;&#12289;&#25945;&#23398;&#21644;&#20010;&#24615;&#21270;&#21307;&#30103;&#31561;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#24433;&#21709;&#12290;&#23454;&#29616;&#20803;&#23431;&#23449;&#22312;&#23454;&#26102;&#21644;&#22823;&#35268;&#27169;&#19978;&#30340;&#35201;&#27714;&#23578;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#65292;&#20197;&#23454;&#29616;&#20854;&#21487;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.10559</link><description>&lt;p&gt;
Metaverse&#65306;&#21487;&#20280;&#32553;&#21644;&#23454;&#26102;&#34394;&#25311;&#19990;&#30028;&#30340;&#24895;&#26223;&#12289;&#26550;&#26500;&#35201;&#32032;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Metaverse: A Vision, Architectural Elements, and Future Directions for Scalable and Realtime Virtual Worlds. (arXiv:2308.10559v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10559
&lt;/p&gt;
&lt;p&gt;
&#20803;&#23431;&#23449;&#26159;&#19968;&#31181;&#21487;&#20280;&#32553;&#19988;&#23454;&#26102;&#30340;&#34394;&#25311;&#19990;&#30028;&#65292;&#23427;&#36890;&#36807;&#34394;&#25311;&#21644;&#22686;&#24378;&#29616;&#23454;&#25216;&#26415;&#25299;&#23637;&#29289;&#29702;&#19990;&#30028;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#19982;&#30495;&#23454;&#21644;&#34394;&#25311;&#19990;&#30028;&#26080;&#32541;&#20132;&#20114;&#12290;&#23427;&#23545;&#31038;&#20132;&#23186;&#20307;&#12289;&#21512;&#20316;&#24037;&#20316;&#12289;&#24066;&#22330;&#33829;&#38144;&#12289;&#25945;&#23398;&#21644;&#20010;&#24615;&#21270;&#21307;&#30103;&#31561;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#24433;&#21709;&#12290;&#23454;&#29616;&#20803;&#23431;&#23449;&#22312;&#23454;&#26102;&#21644;&#22823;&#35268;&#27169;&#19978;&#30340;&#35201;&#27714;&#23578;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#65292;&#20197;&#23454;&#29616;&#20854;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20113;&#35745;&#31639;&#12289;&#29289;&#32852;&#32593;&#25216;&#26415;&#21551;&#29992;&#30340;&#20154;&#26426;&#30028;&#38754;&#12289;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#20197;&#21450;&#39640;&#31934;&#24230;&#30340;&#26426;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#35782;&#21035;&#21644;&#39044;&#27979;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#20877;&#21152;&#19978;&#21518;COVID-19&#26102;&#20195;&#31038;&#20132;&#32593;&#32476;&#21644;&#36828;&#31243;&#36890;&#20449;&#30340;&#24191;&#27867;&#26222;&#21450;&#65292;&#20803;&#23431;&#23449;&#21464;&#24471;&#38750;&#24120;&#21463;&#27426;&#36814;&#12290;&#20803;&#23431;&#23449;&#26377;&#28508;&#21147;&#36890;&#36807;&#34394;&#25311;&#21644;&#22686;&#24378;&#29616;&#23454;&#26469;&#25193;&#23637;&#29289;&#29702;&#19990;&#30028;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#34394;&#25311;&#24418;&#35937;&#21644;&#20840;&#24687;&#24433;&#20687;&#19982;&#29616;&#23454;&#21644;&#34394;&#25311;&#19990;&#30028;&#26080;&#32541;&#20132;&#20114;&#12290;&#23427;&#26377;&#21487;&#33021;&#24433;&#21709;&#20154;&#20204;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20114;&#21160;&#26041;&#24335;&#65292;&#24037;&#20316;&#20013;&#30340;&#21327;&#20316;&#65292;&#24066;&#22330;&#33829;&#38144;&#21644;&#21830;&#19994;&#27963;&#21160;&#65292;&#25945;&#23398;&#65292;&#23398;&#20064;&#65292;&#29978;&#33267;&#20010;&#24615;&#21270;&#21307;&#30103;&#25252;&#29702;&#30340;&#33719;&#21462;&#12290;&#25991;&#29486;&#20013;&#30340;&#20960;&#39033;&#30740;&#31350;&#20174;&#21487;&#31359;&#25140;&#30828;&#20214;&#35774;&#22791;&#21644;&#34394;&#25311;&#29616;&#23454;&#28216;&#25103;&#24212;&#29992;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#20803;&#23431;&#23449;&#12290;&#28982;&#32780;&#65292;&#23454;&#26102;&#23454;&#29616;&#24182;&#22312;&#22823;&#35268;&#27169;&#19978;&#23454;&#29616;&#20803;&#23431;&#23449;&#30340;&#35201;&#27714;&#36824;&#26377;&#24453;&#30740;&#31350;&#65292;&#20197;&#20415;&#20351;&#35813;&#25216;&#26415;&#33021;&#22815;&#34987;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the emergence of Cloud computing, Internet of Things-enabled Human-Computer Interfaces, Generative Artificial Intelligence, and high-accurate Machine and Deep-learning recognition and predictive models, along with the Post Covid-19 proliferation of social networking, and remote communications, the Metaverse gained a lot of popularity. Metaverse has the prospective to extend the physical world using virtual and augmented reality so the users can interact seamlessly with the real and virtual worlds using avatars and holograms. It has the potential to impact people in the way they interact on social media, collaborate in their work, perform marketing and business, teach, learn, and even access personalized healthcare. Several works in the literature examine Metaverse in terms of hardware wearable devices, and virtual reality gaming applications. However, the requirements of realizing the Metaverse in realtime and at a large-scale need yet to be examined for the technology to be usabl
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MoCLIM&#30340;&#22810;&#32452;&#23398;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#30284;&#30151;&#20122;&#22411;&#21010;&#20998;&#20013;&#21033;&#29992;&#22810;&#32452;&#23398;&#25968;&#25454;&#30340;&#28508;&#21147;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#30340;&#25311;&#21512;&#24230;&#21644;&#20122;&#22411;&#21010;&#20998;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09725</link><description>&lt;p&gt;
MoCLIM: &#29992;&#22810;&#32452;&#23398;&#23545;&#27604;&#23398;&#20064;&#21644;&#32452;&#23398;&#25512;&#29702;&#24314;&#27169;&#23454;&#29616;&#20934;&#30830;&#30340;&#30284;&#30151;&#20122;&#22411;&#21010;&#20998;
&lt;/p&gt;
&lt;p&gt;
MoCLIM: Towards Accurate Cancer Subtyping via Multi-Omics Contrastive Learning with Omics-Inference Modeling. (arXiv:2308.09725v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MoCLIM&#30340;&#22810;&#32452;&#23398;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#30284;&#30151;&#20122;&#22411;&#21010;&#20998;&#20013;&#21033;&#29992;&#22810;&#32452;&#23398;&#25968;&#25454;&#30340;&#28508;&#21147;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#30340;&#25311;&#21512;&#24230;&#21644;&#20122;&#22411;&#21010;&#20998;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#20934;&#21307;&#23398;&#30340;&#30446;&#26631;&#26159;&#24314;&#31435;&#30284;&#30151;&#20122;&#22411;&#30340;&#29983;&#21270;&#26426;&#21046;&#19982;&#30142;&#30149;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#22522;&#20110;&#32452;&#23398;&#30340;&#30284;&#30151;&#20122;&#22411;&#21010;&#20998;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#38761;&#21629;&#24615;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#19981;&#21516;&#32423;&#21035;&#30340;&#32452;&#23398;&#35760;&#24405;&#20102;&#30284;&#30151;&#20013;&#22810;&#27493;&#39588;&#36807;&#31243;&#30340;&#29983;&#21270;&#20135;&#29289;&#12290;&#26412;&#25991;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;&#22810;&#32452;&#23398;&#25968;&#25454;&#30340;&#28508;&#21147;&#26469;&#25913;&#21892;&#30284;&#30151;&#20122;&#22411;&#21010;&#20998;&#32467;&#26524;&#65292;&#22240;&#27492;&#24320;&#21457;&#20102;MoCLIM&#65292;&#19968;&#31181;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;MoCLIM&#29420;&#31435;&#22320;&#20174;&#19981;&#21516;&#30340;&#32452;&#23398;&#27169;&#24335;&#20013;&#25552;&#21462;&#26377;&#20449;&#24687;&#37327;&#30340;&#29305;&#24449;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#32452;&#23398;&#27169;&#24335;&#20043;&#38388;&#30340;&#23545;&#27604;&#23398;&#20064;&#25152;&#24471;&#21040;&#30340;&#32479;&#19968;&#34920;&#31034;&#65292;&#22312;&#32473;&#23450;&#30284;&#30151;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#33021;&#22815;&#23558;&#20122;&#22411;&#24456;&#22909;&#22320;&#32858;&#31867;&#21040;&#36739;&#20302;&#30340;&#28508;&#31354;&#38388;&#20013;&#12290;&#36825;&#31181;&#23545;&#27604;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#22312;&#29983;&#29289;&#32593;&#32476;&#20013;&#35266;&#23519;&#21040;&#30340;&#32452;&#38469;&#25512;&#29702;&#30340;&#25237;&#24433;&#12290;&#22312;&#20845;&#20010;&#30284;&#30151;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36739;&#23569;&#30340;&#39640;&#32500;&#30284;&#30151;&#25968;&#25454;&#25311;&#21512;&#21644;&#20122;&#22411;&#21010;&#20998;&#24615;&#33021;&#26041;&#38754;&#26174;&#33879;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precision medicine fundamentally aims to establish causality between dysregulated biochemical mechanisms and cancer subtypes. Omics-based cancer subtyping has emerged as a revolutionary approach, as different level of omics records the biochemical products of multistep processes in cancers. This paper focuses on fully exploiting the potential of multi-omics data to improve cancer subtyping outcomes, and hence developed MoCLIM, a representation learning framework. MoCLIM independently extracts the informative features from distinct omics modalities. Using a unified representation informed by contrastive learning of different omics modalities, we can well-cluster the subtypes, given cancer, into a lower latent space. This contrast can be interpreted as a projection of inter-omics inference observed in biological networks. Experimental results on six cancer datasets demonstrate that our approach significantly improves data fit and subtyping performance in fewer high-dimensional cancer ins
&lt;/p&gt;</description></item><item><title>DiagGPT&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25193;&#23637;&#21040;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#22330;&#26223;&#65292;&#25552;&#20379;&#20102;&#22312;&#22797;&#26434;&#35786;&#26029;&#22330;&#26223;&#20013;&#20027;&#21160;&#25552;&#38382;&#21644;&#24341;&#23548;&#29992;&#25143;&#23436;&#25104;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.08043</link><description>&lt;p&gt;
DiagGPT:&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
DiagGPT: An LLM-based Chatbot with Automatic Topic Management for Task-Oriented Dialogue. (arXiv:2308.08043v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08043
&lt;/p&gt;
&lt;p&gt;
DiagGPT&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25193;&#23637;&#21040;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#22330;&#26223;&#65292;&#25552;&#20379;&#20102;&#22312;&#22797;&#26434;&#35786;&#26029;&#22330;&#26223;&#20013;&#20027;&#21160;&#25552;&#38382;&#21644;&#24341;&#23548;&#29992;&#25143;&#23436;&#25104;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22914;ChatGPT&#27491;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#23637;&#31034;&#20986;&#19982;&#20154;&#31867;&#30456;&#20284;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;AI&#27169;&#22411;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#36741;&#21161;&#20154;&#31867;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;AI&#20316;&#20026;&#32842;&#22825;&#20195;&#29702;&#20154;&#30340;&#37325;&#35201;&#24212;&#29992;&#26159;&#22238;&#31572;&#20154;&#31867;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;&#30446;&#21069;&#30340;LLMs&#22312;&#22238;&#31572;&#19968;&#33324;&#38382;&#39064;&#26041;&#38754;&#24050;&#32463;&#26174;&#31034;&#20986;&#29087;&#32451;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#30340;&#35786;&#26029;&#22330;&#26223;(&#22914;&#27861;&#24459;&#25110;&#21307;&#30103;&#21672;&#35810;)&#20013;&#65292;&#22522;&#26412;&#30340;&#38382;&#31572;&#23545;&#35805;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#20123;&#22330;&#26223;&#36890;&#24120;&#38656;&#35201;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;(TOD)&#65292;&#20854;&#20013;AI&#32842;&#22825;&#20195;&#29702;&#38656;&#35201;&#20027;&#21160;&#25552;&#38382;&#24182;&#24341;&#23548;&#29992;&#25143;&#23436;&#25104;&#29305;&#23450;&#20219;&#21153;&#12290;&#20197;&#21069;&#30340;&#24494;&#35843;&#27169;&#22411;&#22312;TOD&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#24403;&#21069;&#30340;LLMs&#24182;&#26410;&#22266;&#26377;&#36825;&#31181;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DiagGPT (Diagnosis GPT)&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#23427;&#23558;LLMs&#25512;&#24191;&#21040;TOD&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as ChatGPT, are becoming increasingly sophisticated, demonstrating capabilities that closely resemble those of humans. These AI models are playing an essential role in assisting humans with a wide array of tasks in daily life. A significant application of AI is its use as a chat agent, responding to human inquiries across various domains. Current LLMs have shown proficiency in answering general questions. However, basic question-answering dialogue often falls short in complex diagnostic scenarios, such as legal or medical consultations. These scenarios typically necessitate Task-Oriented Dialogue (TOD), wherein an AI chat agent needs to proactively pose questions and guide users towards specific task completion. Previous fine-tuning models have underperformed in TOD, and current LLMs do not inherently possess this capability. In this paper, we introduce DiagGPT (Dialogue in Diagnosis GPT), an innovative method that extends LLMs to TOD scenarios. Our e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22312;&#32858;&#31867;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#38598;&#25104;&#27169;&#22411;&#36827;&#34892;&#24320;&#25918;&#38598;&#21512;&#20154;&#33080;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#24863;&#20852;&#36259;&#30340;&#20010;&#20307;&#65292;&#21516;&#26102;&#26377;&#25928;&#22788;&#29702;&#38476;&#29983;&#30340;&#38754;&#23380;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#21363;&#20351;&#22312;&#22823;&#35268;&#27169;&#22270;&#24211;&#20013;&#20063;&#33021;&#21462;&#24471;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07445</link><description>&lt;p&gt;
&#20351;&#29992;&#22312;&#32858;&#31867;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#38598;&#25104;&#27169;&#22411;&#36827;&#34892;&#24320;&#25918;&#38598;&#21512;&#20154;&#33080;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Open-set Face Recognition using Ensembles trained on Clustered Data. (arXiv:2308.07445v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22312;&#32858;&#31867;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#38598;&#25104;&#27169;&#22411;&#36827;&#34892;&#24320;&#25918;&#38598;&#21512;&#20154;&#33080;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#24863;&#20852;&#36259;&#30340;&#20010;&#20307;&#65292;&#21516;&#26102;&#26377;&#25928;&#22788;&#29702;&#38476;&#29983;&#30340;&#38754;&#23380;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#21363;&#20351;&#22312;&#22823;&#35268;&#27169;&#22270;&#24211;&#20013;&#20063;&#33021;&#21462;&#24471;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#38598;&#21512;&#20154;&#33080;&#35782;&#21035;&#25551;&#36848;&#20102;&#22312;&#27979;&#35797;&#26102;&#20986;&#29616;&#26410;&#30693;&#30340;&#20027;&#39064;&#65292;&#32780;&#22312;&#35757;&#32451;&#38454;&#27573;&#26410;&#35265;&#36807;&#12290;&#23427;&#19981;&#20165;&#38656;&#35201;&#20934;&#30830;&#35782;&#21035;&#24863;&#20852;&#36259;&#30340;&#20010;&#20307;&#30340;&#26041;&#27861;&#65292;&#36824;&#38656;&#35201;&#26377;&#25928;&#22788;&#29702;&#38476;&#29983;&#30340;&#38754;&#23380;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#24320;&#25918;&#38598;&#21512;&#20154;&#33080;&#35782;&#21035;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#21253;&#21547;&#25968;&#30334;&#21644;&#25968;&#21315;&#20010;&#20027;&#39064;&#30340;&#22270;&#24211;&#12290;&#23427;&#30001;&#32858;&#31867;&#21644;&#19968;&#32452;&#20108;&#36827;&#21046;&#23398;&#20064;&#31639;&#27861;&#32452;&#25104;&#65292;&#29992;&#20110;&#20272;&#35745;&#26597;&#35810;&#20154;&#33080;&#26679;&#26412;&#26159;&#21542;&#23646;&#20110;&#20154;&#33080;&#22270;&#24211;&#65292;&#24182;&#26816;&#32034;&#20854;&#27491;&#30830;&#30340;&#36523;&#20221;&#12290;&#35813;&#26041;&#27861;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#22270;&#24211;&#20027;&#39064;&#65292;&#24182;&#20351;&#29992;&#38598;&#25104;&#27169;&#22411;&#25913;&#21892;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#30693;&#21517;&#30340;LFW&#21644;YTF&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#38024;&#23545;&#21487;&#25193;&#23637;&#24615;&#65292;&#20063;&#21487;&#20197;&#36798;&#21040;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-set face recognition describes a scenario where unknown subjects, unseen during the training stage, appear on test time. Not only it requires methods that accurately identify individuals of interest, but also demands approaches that effectively deal with unfamiliar faces. This work details a scalable open-set face identification approach to galleries composed of hundreds and thousands of subjects. It is composed of clustering and an ensemble of binary learning algorithms that estimates when query face samples belong to the face gallery and then retrieves their correct identity. The approach selects the most suitable gallery subjects and uses the ensemble to improve prediction performance. We carry out experiments on well-known LFW and YTF benchmarks. Results show that competitive performance can be achieved even when targeting scalability.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InstructGLM&#30340;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25506;&#32034;&#26159;&#21542;&#21487;&#20197;&#29992;&#35821;&#35328;&#27169;&#22411;&#21462;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.07134</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#26159;&#22270;&#34920;&#25152;&#38656;&#35201;&#30340;&#20840;&#37096;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Natural Language is All a Graph Needs. (arXiv:2308.07134v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InstructGLM&#30340;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25506;&#32034;&#26159;&#21542;&#21487;&#20197;&#29992;&#35821;&#35328;&#27169;&#22411;&#21462;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#22914;ChatGPT&#65292;&#24050;&#32463;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#21508;&#20010;&#30740;&#31350;&#39046;&#22495;&#20013;&#24341;&#36215;&#20102;&#38761;&#21629;&#12290;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36880;&#28176;&#21462;&#20195;&#20102;CNN&#21644;RNN&#65292;&#23558;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#32479;&#19968;&#36215;&#26469;&#12290;&#19982;&#30456;&#23545;&#29420;&#31435;&#23384;&#22312;&#30340;&#25968;&#25454;&#65288;&#22914;&#22270;&#20687;&#12289;&#35270;&#39057;&#25110;&#25991;&#26412;&#65289;&#30456;&#27604;&#65292;&#22270;&#34920;&#26159;&#19968;&#31181;&#21253;&#21547;&#20016;&#23500;&#32467;&#26500;&#21644;&#20851;&#31995;&#20449;&#24687;&#30340;&#25968;&#25454;&#31867;&#22411;&#12290;&#21516;&#26102;&#65292;&#20316;&#20026;&#26368;&#20855;&#34920;&#29616;&#21147;&#30340;&#23186;&#20171;&#20043;&#19968;&#65292;&#33258;&#28982;&#35821;&#35328;&#22312;&#25551;&#36848;&#22797;&#26434;&#32467;&#26500;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23558;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#32435;&#20837;&#29983;&#25104;&#24335;&#35821;&#35328;&#24314;&#27169;&#26694;&#26550;&#30340;&#29616;&#26377;&#24037;&#20316;&#20173;&#28982;&#38750;&#24120;&#26377;&#38480;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#19981;&#26029;&#22686;&#38271;&#65292;&#25506;&#32034;LLMs&#26159;&#21542;&#20063;&#21487;&#20197;&#26367;&#20195;GNNs&#25104;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InstructGLM&#65288;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#65289;&#31639;&#27861;&#65292;&#31995;&#32479;&#22320;&#35774;&#35745;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#27169;&#22411;&#26469;&#22788;&#29702;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformers-based large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model), systematically design highly scal
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#27604;&#36739;&#20102;&#20849;&#21516;&#23545;&#27604;&#23398;&#20064;&#21644;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#22312;CT&#25195;&#25551;&#21367;&#31215;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06534</link><description>&lt;p&gt;
&#35299;&#20915;&#21307;&#23398;&#24433;&#20687;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#23567;&#22411;&#27880;&#37322;&#25968;&#25454;&#38598;&#38382;&#39064;&#65306;&#23545;&#27604;&#20849;&#21516;&#23545;&#27604;&#23398;&#20064;&#21644;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#22312;CT&#25195;&#25551;&#21367;&#31215;&#27169;&#22411;&#20013;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Dealing with Small Annotated Datasets for Deep Learning in Medical Imaging: An Evaluation of Self-Supervised Pre-Training on CT Scans Comparing Contrastive and Masked Autoencoder Methods for Convolutional Models. (arXiv:2308.06534v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#27604;&#36739;&#20102;&#20849;&#21516;&#23545;&#27604;&#23398;&#20064;&#21644;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#22312;CT&#25195;&#25551;&#21367;&#31215;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#26377;&#28508;&#21147;&#20943;&#23569;&#35786;&#26029;&#38169;&#35823;&#30340;&#39118;&#38505;&#12289;&#20943;&#36731;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#24037;&#20316;&#36127;&#25285;&#24182;&#21152;&#36895;&#30830;&#35786;&#12290;&#35757;&#32451;&#36825;&#26679;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#22411;&#19988;&#20934;&#30830;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#38656;&#35201;&#20026;&#25152;&#26377;&#35757;&#32451;&#26679;&#26412;&#25552;&#20379;&#27880;&#37322;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#65292;&#30001;&#20110;&#27880;&#37322;&#30340;&#39640;&#22797;&#26434;&#24615;&#12289;&#21463;&#38480;&#30340;&#33719;&#21462;&#26041;&#24335;&#25110;&#30142;&#30149;&#30340;&#32597;&#35265;&#24615;&#65292;&#29305;&#23450;&#20219;&#21153;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#36890;&#24120;&#24456;&#23567;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#39046;&#22495;&#30340;&#26041;&#27861;&#65292;&#22312;&#27809;&#26377;&#27880;&#37322;&#30340;&#22823;&#22411;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#22312;&#39044;&#35757;&#32451;&#20043;&#21518;&#65292;&#23567;&#22411;&#30340;&#24050;&#27880;&#37322;&#25968;&#25454;&#38598;&#23601;&#36275;&#20197;&#23545;&#27169;&#22411;&#36827;&#34892;&#29305;&#23450;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#21363;&#25152;&#35859;&#30340;&#8220;&#19979;&#28216;&#20219;&#21153;&#8221;&#12290;&#21307;&#23398;&#24433;&#20687;&#20013;&#26368;&#27969;&#34892;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#22522;&#20110;&#20849;&#21516;&#23545;&#27604;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#33258;&#28982;&#22270;&#20687;&#22788;&#29702;&#30740;&#31350;&#34920;&#26126;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20108;&#32773;&#22312;CT&#25195;&#25551;&#21367;&#31215;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning in medical imaging has the potential to minimize the risk of diagnostic errors, reduce radiologist workload, and accelerate diagnosis. Training such deep learning models requires large and accurate datasets, with annotations for all training samples. However, in the medical imaging domain, annotated datasets for specific tasks are often small due to the high complexity of annotations, limited access, or the rarity of diseases. To address this challenge, deep learning models can be pre-trained on large image datasets without annotations using methods from the field of self-supervised learning. After pre-training, small annotated datasets are sufficient to fine-tune the models for a specific task, the so-called ``downstream task". The most popular self-supervised pre-training approaches in medical imaging are based on contrastive learning. However, recent studies in natural image processing indicate a strong potential for masked autoencoder approaches. Our work compares sta
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21463;&#38480;&#39057;&#29575;&#30340;&#36523;&#20221;&#19981;&#21487;&#30693;&#25915;&#20987;&#36827;&#34892;&#20154;&#33080;&#21152;&#23494;&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;&#22806;&#37096;&#25200;&#21160;&#21152;&#23494;&#20154;&#33080;&#22270;&#20687;&#30340;&#38544;&#31169;&#20445;&#25252;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#40657;&#30418;&#22330;&#26223;&#19979;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.05983</link><description>&lt;p&gt;
&#36890;&#36807;&#21463;&#38480;&#39057;&#29575;&#30340;&#36523;&#20221;&#19981;&#21487;&#30693;&#25915;&#20987;&#36827;&#34892;&#20154;&#33080;&#21152;&#23494;
&lt;/p&gt;
&lt;p&gt;
Face Encryption via Frequency-Restricted Identity-Agnostic Attacks. (arXiv:2308.05983v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05983
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21463;&#38480;&#39057;&#29575;&#30340;&#36523;&#20221;&#19981;&#21487;&#30693;&#25915;&#20987;&#36827;&#34892;&#20154;&#33080;&#21152;&#23494;&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;&#22806;&#37096;&#25200;&#21160;&#21152;&#23494;&#20154;&#33080;&#22270;&#20687;&#30340;&#38544;&#31169;&#20445;&#25252;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#40657;&#30418;&#22330;&#26223;&#19979;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#22825;&#26377;&#25968;&#21313;&#20159;&#20154;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#20998;&#20139;&#20182;&#20204;&#30340;&#26085;&#24120;&#29031;&#29255;&#12290;&#28982;&#32780;&#65292;&#24694;&#24847;&#37319;&#38598;&#32773;&#21033;&#29992;&#28145;&#24230;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#36731;&#26494;&#22320;&#20174;&#36825;&#20123;&#22270;&#29255;&#20013;&#31363;&#21462;&#20182;&#20204;&#30340;&#29983;&#29289;&#29305;&#24449;&#20449;&#24687;&#65288;&#20363;&#22914;&#20154;&#33080;&#65289;&#12290;&#19968;&#20123;&#30740;&#31350;&#27491;&#22312;&#36827;&#34892;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#38590;&#20197;&#23519;&#35273;&#30340;&#25200;&#21160;&#26469;&#29983;&#25104;&#21152;&#23494;&#20154;&#33080;&#29031;&#29255;&#65292;&#20197;&#20943;&#23569;&#20154;&#33080;&#20449;&#24687;&#27844;&#28431;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#38656;&#35201;&#26356;&#24378;&#30340;&#40657;&#30418;&#22330;&#26223;&#21487;&#34892;&#24615;&#21644;&#26356;&#33258;&#28982;&#30340;&#35270;&#35273;&#22806;&#35266;&#65292;&#36825;&#23545;&#38544;&#31169;&#20445;&#25252;&#30340;&#21487;&#34892;&#24615;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#38480;&#39057;&#29575;&#30340;&#36523;&#20221;&#19981;&#21487;&#30693;&#65288;FRIA&#65289;&#26694;&#26550;&#65292;&#20197;&#20174;&#26410;&#32463;&#25480;&#26435;&#30340;&#20154;&#33080;&#35782;&#21035;&#20013;&#21152;&#23494;&#20154;&#33080;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#20010;&#20154;&#20449;&#24687;&#12290;&#23545;&#20110;&#24369;&#40657;&#30418;&#22330;&#26223;&#30340;&#21487;&#34892;&#24615;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22810;&#20010;&#20154;&#33080;&#35782;&#21035;&#27169;&#22411;&#20013;&#30340;&#24179;&#22343;&#29305;&#24449;&#34920;&#31034;&#30456;&#20284;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#36890;&#36807;&#20114;&#32852;&#32593;&#29228;&#21462;&#30340;&#25968;&#25454;&#38598;&#20013;&#30340;&#24179;&#22343;&#29305;&#24449;&#20316;&#20026;t
&lt;/p&gt;
&lt;p&gt;
Billions of people are sharing their daily live images on social media everyday. However, malicious collectors use deep face recognition systems to easily steal their biometric information (e.g., faces) from these images. Some studies are being conducted to generate encrypted face photos using adversarial attacks by introducing imperceptible perturbations to reduce face information leakage. However, existing studies need stronger black-box scenario feasibility and more natural visual appearances, which challenge the feasibility of privacy protection. To address these problems, we propose a frequency-restricted identity-agnostic (FRIA) framework to encrypt face images from unauthorized face recognition without access to personal information. As for the weak black-box scenario feasibility, we obverse that representations of the average feature in multiple face recognition models are similar, thus we propose to utilize the average feature via the crawled dataset from the Internet as the t
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#30340;&#25361;&#25112;&#65292;&#28982;&#32780;&#65292;&#21457;&#23637;&#33073;&#38772;&#27861;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#20799;&#31461;&#30340;&#33021;&#21147;&#21457;&#23637;&#36807;&#31243;&#65292;&#20026;&#21019;&#24314;&#31283;&#20581;&#21487;&#38752;&#30340;AI&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;</title><link>http://arxiv.org/abs/2308.04586</link><description>&lt;p&gt;
AIs&#30340;&#21457;&#23637;&#33073;&#38772;&#27861;
&lt;/p&gt;
&lt;p&gt;
Developmental Bootstrapping of AIs. (arXiv:2308.04586v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04586
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#30340;&#25361;&#25112;&#65292;&#28982;&#32780;&#65292;&#21457;&#23637;&#33073;&#38772;&#27861;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#20799;&#31461;&#30340;&#33021;&#21147;&#21457;&#23637;&#36807;&#31243;&#65292;&#20026;&#21019;&#24314;&#31283;&#20581;&#21487;&#38752;&#30340;AI&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24403;&#21069;&#19968;&#20123;AI&#22312;&#23553;&#38381;&#30340;&#19990;&#30028;&#65292;&#22914;&#26827;&#30424;&#28216;&#25103;&#20013;&#36229;&#36234;&#20102;&#20154;&#31867;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#28151;&#20081;&#30340;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#34920;&#29616;&#26377;&#38480;&#12290;&#23427;&#20204;&#20250;&#29359;&#22855;&#24618;&#30340;&#38169;&#35823;&#32780;&#19988;&#27809;&#26377;&#24847;&#35782;&#21040;&#12290;&#23427;&#20204;&#24456;&#38590;&#21463;&#21040;&#25351;&#23548;&#65292;&#19981;&#33021;&#36816;&#29992;&#24120;&#35782;&#65292;&#32570;&#20047;&#22909;&#22855;&#24515;&#12290;&#23427;&#20204;&#19981;&#33021;&#25104;&#20026;&#33391;&#22909;&#30340;&#21512;&#20316;&#32773;&#12290;&#20256;&#32479;&#25163;&#21160;&#26500;&#24314;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#26500;&#24314;&#30340;&#31995;&#32479;&#21644;&#20351;&#29992;&#29983;&#25104;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;(&#21253;&#25324;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;)&#26500;&#24314;&#30340;&#31995;&#32479;&#37117;&#26080;&#27861;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#23427;&#20204;&#19981;&#36866;&#21512;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#12290;&#23613;&#31649;&#27492;&#26041;&#27861;&#19981;&#23646;&#20110;&#20027;&#27969;&#30340;AI&#26041;&#27861;&#65292;&#20294;&#21457;&#23637;&#33073;&#38772;&#27861;&#26174;&#31034;&#20986;&#24076;&#26395;&#12290;&#22312;&#21457;&#23637;&#33073;&#38772;&#27861;&#20013;&#65292;AI&#20687;&#20154;&#31867;&#20799;&#31461;&#19968;&#26679;&#21457;&#23637;&#33021;&#21147;&#12290;&#23427;&#20204;&#20174;&#20808;&#22825;&#33021;&#21147;&#24320;&#22987;&#12290;&#20687;&#20154;&#31867;&#19968;&#26679;&#65292;&#23427;&#20204;&#19982;&#29615;&#22659;&#20114;&#21160;&#65292;&#24182;&#20174;&#20114;&#21160;&#20013;&#23398;&#20064;&#12290;&#23427;&#20204;&#36890;&#36807;&#33258;&#25105;&#21457;&#23637;&#30340;&#33021;&#21147;&#36880;&#27493;&#25193;&#23637;&#20808;&#22825;&#33021;&#21147;&#12290;&#23427;&#20204;&#20114;&#21160;&#24182;&#36880;&#28176;&#23558;&#25152;&#23398;&#24212;&#29992;&#20110;&#23454;&#38469;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although some current AIs surpass human abilities especially in closed worlds such as board games, their performance in the messy real world is limited. They make strange mistakes and do not notice them. They cannot be instructed easily, fail to use common sense, and lack curiosity. They do not make good collaborators. Neither systems built using the traditional manually-constructed symbolic AI approach nor systems built using generative and deep learning AI approaches including large language models (LLMs) can meet the challenges. They are not well suited for creating robust and trustworthy AIs. Although it is outside of mainstream AI approaches, developmental bootstrapping shows promise. In developmental bootstrapping, AIs develop competences like human children do. They start with innate competences. Like humans, they interact with the environment and learn from their interactions. They incrementally extend their innate competences with self-developed competences. They interact and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GridMM&#65292;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#30340;&#33258;&#39030;&#21521;&#19979;&#32593;&#26684;&#35760;&#24518;&#22270;&#65292;&#20174;&#20840;&#23616;&#21644;&#23616;&#37096;&#35270;&#35282;&#26377;&#25928;&#22320;&#34920;&#31034;&#21644;&#32467;&#26500;&#21270;&#20808;&#21069;&#35775;&#38382;&#30340;&#29615;&#22659;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.12907</link><description>&lt;p&gt;
GridMM:&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#30340;&#32593;&#26684;&#35760;&#24518;&#22270;
&lt;/p&gt;
&lt;p&gt;
GridMM: Grid Memory Map for Vision-and-Language Navigation. (arXiv:2307.12907v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GridMM&#65292;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#30340;&#33258;&#39030;&#21521;&#19979;&#32593;&#26684;&#35760;&#24518;&#22270;&#65292;&#20174;&#20840;&#23616;&#21644;&#23616;&#37096;&#35270;&#35282;&#26377;&#25928;&#22320;&#34920;&#31034;&#21644;&#32467;&#26500;&#21270;&#20808;&#21069;&#35775;&#38382;&#30340;&#29615;&#22659;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#20351;&#20195;&#29702;&#33021;&#22815;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#22312;3D&#29615;&#22659;&#20013;&#23548;&#33322;&#21040;&#36828;&#31243;&#20301;&#32622;&#12290;&#20026;&#20102;&#34920;&#31034;&#20808;&#21069;&#35775;&#38382;&#30340;&#29615;&#22659;&#65292;VLN&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#20351;&#29992;&#32463;&#24120;&#24615;&#29366;&#24577;&#12289;&#25299;&#25169;&#22320;&#22270;&#25110;&#33258;&#39030;&#21521;&#19979;&#30340;&#35821;&#20041;&#22320;&#22270;&#26469;&#23454;&#29616;&#35760;&#24518;&#12290;&#19982;&#36825;&#20123;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#33258;&#39030;&#21521;&#19979;&#30340;&#20197;&#33258;&#25105;&#20026;&#20013;&#24515;&#24182;&#21160;&#24577;&#22686;&#38271;&#30340;&#32593;&#26684;&#35760;&#24518;&#22270;&#65288;&#21363;GridMM&#65289;&#26469;&#32467;&#26500;&#21270;&#35775;&#38382;&#30340;&#29615;&#22659;&#12290;&#20174;&#20840;&#23616;&#35270;&#35282;&#26469;&#30475;&#65292;&#21382;&#21490;&#35266;&#23519;&#32467;&#26524;&#22312;&#33258;&#19978;&#32780;&#19979;&#30340;&#35270;&#22270;&#20013;&#34987;&#25237;&#24433;&#21040;&#32479;&#19968;&#30340;&#32593;&#26684;&#22320;&#22270;&#20013;&#65292;&#36825;&#21487;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;&#29615;&#22659;&#30340;&#31354;&#38388;&#20851;&#31995;&#12290;&#20174;&#23616;&#37096;&#35270;&#35282;&#26469;&#30475;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#25351;&#20196;&#30456;&#20851;&#24615;&#32858;&#21512;&#26041;&#27861;&#65292;&#20197;&#25429;&#25417;&#27599;&#20010;&#32593;&#26684;&#21306;&#22495;&#20013;&#32454;&#31890;&#24230;&#30340;&#35270;&#35273;&#32447;&#32034;&#12290;&#22312;&#31163;&#25955;&#29615;&#22659;&#20013;&#23545;REVERIE&#12289;R2R&#12289;SOON&#25968;&#25454;&#38598;&#20197;&#21450;&#36830;&#32493;&#29615;&#22659;&#20013;&#30340;R2R-CE&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-and-language navigation (VLN) enables the agent to navigate to a remote location following the natural language instruction in 3D environments. To represent the previously visited environment, most approaches for VLN implement memory using recurrent states, topological maps, or top-down semantic maps. In contrast to these approaches, we build the top-down egocentric and dynamically growing Grid Memory Map (i.e., GridMM) to structure the visited environment. From a global perspective, historical observations are projected into a unified grid map in a top-down view, which can better represent the spatial relations of the environment. From a local perspective, we further propose an instruction relevance aggregation method to capture fine-grained visual clues in each grid region. Extensive experiments are conducted on both the REVERIE, R2R, SOON datasets in the discrete environments, and the R2R-CE dataset in the continuous environments, showing the superiority of our proposed metho
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;OpenPose&#26694;&#26550;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#20998;&#26512;&#32771;&#35797;&#35270;&#39057;&#24182;&#39640;&#25928;&#26377;&#25928;&#22320;&#26816;&#27979;&#21487;&#30097;&#27963;&#21160;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2307.11413</link><description>&lt;p&gt;
&#37319;&#29992;OpenPose&#30340;&#22522;&#20110;&#35270;&#39057;&#30340;&#32771;&#35797;&#21487;&#30097;&#27963;&#21160;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Video-based Detector for Suspicious Activity in Examination with OpenPose. (arXiv:2307.11413v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;OpenPose&#26694;&#26550;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#20998;&#26512;&#32771;&#35797;&#35270;&#39057;&#24182;&#39640;&#25928;&#26377;&#25928;&#22320;&#26816;&#27979;&#21487;&#30097;&#27963;&#21160;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#35797;&#26159;&#23398;&#20064;&#36807;&#31243;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#19968;&#37096;&#20998;&#65292;&#23398;&#26415;&#26426;&#26500;&#25237;&#20837;&#22823;&#37327;&#36164;&#28304;&#32500;&#25252;&#20854;&#23436;&#25972;&#24615;&#65292;&#38450;&#27490;&#23398;&#29983;&#25110;&#30417;&#32771;&#21592;&#20316;&#24330;&#12290;&#28982;&#32780;&#65292;&#20316;&#24330;&#22312;&#32771;&#35797;&#20013;&#21464;&#24471;&#29462;&#29527;&#65292;&#36825;&#25439;&#23475;&#20102;&#32771;&#35797;&#30340;&#23436;&#25972;&#24615;&#12290;&#20381;&#38752;&#30417;&#32771;&#21592;&#30417;&#35270;&#27599;&#20010;&#23398;&#29983;&#30340;&#20256;&#32479;&#26041;&#27861;&#19981;&#20999;&#23454;&#38469;&#19988;&#26080;&#25928;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#38656;&#35201;&#36830;&#32493;&#35760;&#24405;&#32771;&#35797;&#36807;&#31243;&#20197;&#30417;&#25511;&#23398;&#29983;&#30340;&#21487;&#30097;&#27963;&#21160;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24405;&#20687;&#36890;&#24120;&#36807;&#38271;&#20197;&#33267;&#20110;&#30417;&#32771;&#21592;&#26080;&#27861;&#26377;&#25928;&#20998;&#26512;&#65292;&#30130;&#21171;&#21487;&#33021;&#23548;&#33268;&#20182;&#20204;&#38169;&#36807;&#37325;&#35201;&#32454;&#33410;&#12290;&#20026;&#25193;&#22823;&#30417;&#25511;&#33539;&#22260;&#65292;&#30417;&#32771;&#21592;&#21487;&#20197;&#20351;&#29992;&#22266;&#23450;&#26550;&#35774;&#22312;&#39640;&#22788;&#25110;&#20329;&#25140;&#30340;&#25668;&#20687;&#22836;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21033;&#29992;&#33258;&#21160;&#21270;&#20998;&#26512;&#35270;&#39057;&#24182;&#26377;&#25928;&#39640;&#25928;&#22320;&#26816;&#27979;&#32771;&#35797;&#20013;&#21487;&#30097;&#27963;&#21160;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#21033;&#29992;OpenPose&#26694;&#26550;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#35782;&#21035;&#23398;&#29983;&#20132;&#25442;&#30340;
&lt;/p&gt;
&lt;p&gt;
Examinations are a crucial part of the learning process, and academic institutions invest significant resources into maintaining their integrity by preventing cheating from students or facilitators. However, cheating has become rampant in examination setups, compromising their integrity. The traditional method of relying on invigilators to monitor every student is impractical and ineffective. To address this issue, there is a need to continuously record exam sessions to monitor students for suspicious activities. However, these recordings are often too lengthy for invigilators to analyze effectively, and fatigue may cause them to miss significant details. To widen the coverage, invigilators could use fixed overhead or wearable cameras. This paper introduces a framework that uses automation to analyze videos and detect suspicious activities during examinations efficiently and effectively. We utilized the OpenPose framework and Convolutional Neural Network (CNN) to identify students exch
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35270;&#39057;&#28966;&#28857;&#32593;&#32476;&#30340;&#35270;&#39057;&#35782;&#21035;&#26550;&#26500;&#65292;&#36890;&#36807;&#26102;&#31354;&#28966;&#28857;&#35843;&#21046;&#26469;&#27169;&#25311;&#23616;&#37096;&#21644;&#20840;&#23616;&#19978;&#19979;&#25991;&#65292;&#32467;&#21512;&#20102;Transformer&#21644;&#21367;&#31215;&#35774;&#35745;&#30340;&#20248;&#28857;&#65292;&#26082;&#26377;&#25928;&#21448;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2307.06947</link><description>&lt;p&gt;
&#35270;&#39057;&#28966;&#28857;&#32593;&#32476;&#65306;&#29992;&#20110;&#35270;&#39057;&#21160;&#20316;&#35782;&#21035;&#30340;&#26102;&#31354;&#28966;&#28857;&#35843;&#21046;
&lt;/p&gt;
&lt;p&gt;
Video-FocalNets: Spatio-Temporal Focal Modulation for Video Action Recognition. (arXiv:2307.06947v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35270;&#39057;&#28966;&#28857;&#32593;&#32476;&#30340;&#35270;&#39057;&#35782;&#21035;&#26550;&#26500;&#65292;&#36890;&#36807;&#26102;&#31354;&#28966;&#28857;&#35843;&#21046;&#26469;&#27169;&#25311;&#23616;&#37096;&#21644;&#20840;&#23616;&#19978;&#19979;&#25991;&#65292;&#32467;&#21512;&#20102;Transformer&#21644;&#21367;&#31215;&#35774;&#35745;&#30340;&#20248;&#28857;&#65292;&#26082;&#26377;&#25928;&#21448;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35270;&#39057;&#35782;&#21035;&#27169;&#22411;&#21033;&#29992;Transformer&#27169;&#22411;&#36827;&#34892;&#38271;&#36317;&#31163;&#26102;&#31354;&#19978;&#19979;&#25991;&#24314;&#27169;&#12290;&#35270;&#39057;Transformer&#35774;&#35745;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#65292;&#21487;&#20197;&#20197;&#39640;&#35745;&#31639;&#25104;&#26412;&#27169;&#25311;&#20840;&#23616;&#19978;&#19979;&#25991;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#29992;&#20110;&#35270;&#39057;&#30340;&#21367;&#31215;&#35774;&#35745;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20294;&#32570;&#20047;&#38271;&#36317;&#31163;&#20381;&#36182;&#24314;&#27169;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20004;&#31181;&#35774;&#35745;&#30340;&#26368;&#20339;&#25928;&#26524;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35270;&#39057;&#28966;&#28857;&#32593;&#32476;&#65288;Video-FocalNet&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26082;&#26377;&#25928;&#21448;&#39640;&#25928;&#30340;&#35270;&#39057;&#35782;&#21035;&#26550;&#26500;&#65292;&#21487;&#20197;&#27169;&#25311;&#23616;&#37096;&#21644;&#20840;&#23616;&#19978;&#19979;&#25991;&#12290;&#35270;&#39057;&#28966;&#28857;&#32593;&#32476;&#22522;&#20110;&#26102;&#31354;&#28966;&#28857;&#35843;&#21046;&#26550;&#26500;&#65292;&#23545;&#33258;&#27880;&#24847;&#21147;&#30340;&#20132;&#20114;&#21644;&#32858;&#21512;&#27493;&#39588;&#36827;&#34892;&#20102;&#39072;&#20498;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#32858;&#21512;&#27493;&#39588;&#21644;&#20132;&#20114;&#27493;&#39588;&#37117;&#20351;&#29992;&#20102;&#39640;&#25928;&#30340;&#21367;&#31215;&#21644;&#36880;&#20803;&#32032;&#20056;&#27861;&#25805;&#20316;&#26469;&#23454;&#29616;&#65292;&#20854;&#35745;&#31639;&#25104;&#26412;&#27604;&#35270;&#39057;&#34920;&#36798;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#23545;&#24212;&#37096;&#20998;&#35201;&#20302;&#24471;&#22810;&#12290;&#25105;&#20204;&#24191;&#27867;&#25506;&#32034;&#20102;&#28966;&#28857;&#35843;&#21046;&#30340;&#35774;&#35745;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent video recognition models utilize Transformer models for long-range spatio-temporal context modeling. Video transformer designs are based on self-attention that can model global context at a high computational cost. In comparison, convolutional designs for videos offer an efficient alternative but lack long-range dependency modeling. Towards achieving the best of both designs, this work proposes Video-FocalNet, an effective and efficient architecture for video recognition that models both local and global contexts. Video-FocalNet is based on a spatio-temporal focal modulation architecture that reverses the interaction and aggregation steps of self-attention for better efficiency. Further, the aggregation step and the interaction step are both implemented using efficient convolution and element-wise multiplication operations that are computationally less expensive than their self-attention counterparts on video representations. We extensively explore the design space of focal modu
&lt;/p&gt;</description></item><item><title>DoReMi&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#35328;&#27169;&#22411;&#22522;&#30784;&#26550;&#26500;&#65292;&#36890;&#36807;&#26816;&#27979;&#21644;&#20462;&#22797;&#35745;&#21010;&#19982;&#25191;&#34892;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#26469;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#12290;&#35813;&#26550;&#26500;&#21033;&#29992;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#26816;&#26597;&#32422;&#26463;&#26465;&#20214;&#20197;&#21457;&#29616;&#19981;&#19968;&#33268;&#65292;&#24182;&#35843;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#37325;&#26032;&#35268;&#21010;&#20197;&#23454;&#29616;&#24674;&#22797;&#12290;</title><link>http://arxiv.org/abs/2307.00329</link><description>&lt;p&gt;
DoReMi: &#36890;&#36807;&#26816;&#27979;&#21644;&#20462;&#22797;&#35745;&#21010;&#25191;&#34892;&#19981;&#19968;&#33268;&#26469;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
DoReMi: Grounding Language Model by Detecting and Recovering from Plan-Execution Misalignment. (arXiv:2307.00329v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00329
&lt;/p&gt;
&lt;p&gt;
DoReMi&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#35328;&#27169;&#22411;&#22522;&#30784;&#26550;&#26500;&#65292;&#36890;&#36807;&#26816;&#27979;&#21644;&#20462;&#22797;&#35745;&#21010;&#19982;&#25191;&#34892;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#26469;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#12290;&#35813;&#26550;&#26500;&#21033;&#29992;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#26816;&#26597;&#32422;&#26463;&#26465;&#20214;&#20197;&#21457;&#29616;&#19981;&#19968;&#33268;&#65292;&#24182;&#35843;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#37325;&#26032;&#35268;&#21010;&#20197;&#23454;&#29616;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21253;&#21547;&#22823;&#37327;&#30340;&#35821;&#20041;&#30693;&#35782;&#65292;&#24182;&#20855;&#22791;&#20986;&#33394;&#30340;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;&#22914;&#20309;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#26426;&#22120;&#20154;&#20219;&#21153;&#30456;&#32467;&#21512;&#65292;&#20197;&#30830;&#20445;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#24207;&#21015;&#22312;&#36923;&#36753;&#19978;&#27491;&#30830;&#19988;&#21487;&#25191;&#34892;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29615;&#22659;&#25200;&#21160;&#25110;&#25511;&#21046;&#22120;&#35774;&#35745;&#30340;&#19981;&#23436;&#21892;&#65292;&#24213;&#23618;&#25191;&#34892;&#21487;&#33021;&#20250;&#20559;&#31163;&#39640;&#32423;&#35745;&#21010;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DoReMi&#30340;&#26032;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#30784;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#33021;&#22815;&#21450;&#26102;&#26816;&#27979;&#21644;&#20462;&#22797;&#35745;&#21010;&#19982;&#25191;&#34892;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;LLM&#36827;&#34892;&#35268;&#21010;&#65292;&#24182;&#29983;&#25104;&#35745;&#21010;&#27493;&#39588;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#36825;&#20123;&#32422;&#26463;&#26465;&#20214;&#21487;&#20197;&#25351;&#31034;&#35745;&#21010;&#19982;&#25191;&#34892;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#25105;&#20204;&#20351;&#29992;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#27169;&#22411;&#22312;&#20302;&#23618;&#25216;&#33021;&#25191;&#34892;&#36807;&#31243;&#20013;&#26816;&#26597;&#32422;&#26463;&#26465;&#20214;&#12290;&#22914;&#26524;&#21457;&#29983;&#29305;&#23450;&#30340;&#19981;&#19968;&#33268;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#35843;&#29992;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#35268;&#21010;&#20197;&#20174;&#20013;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models encode a vast amount of semantic knowledge and possess remarkable understanding and reasoning capabilities. Previous research has explored how to ground language models in robotic tasks to ensure that the sequences generated by the language model are both logically correct and practically executable. However, low-level execution may deviate from the high-level plan due to environmental perturbations or imperfect controller design. In this paper, we propose DoReMi, a novel language model grounding framework that enables immediate Detection and Recovery from Misalignments between plan and execution. Specifically, LLMs are leveraged for both planning and generating constraints for planned steps. These constraints can indicate plan-execution misalignments and we use a vision question answering (VQA) model to check constraints during low-level skill execution. If certain misalignment occurs, our method will call the language model to re-plan in order to recover from mi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20851;&#38190;&#22240;&#32032;&#65292;&#25552;&#20379;&#20102;&#21307;&#30103;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#20840;&#38754;&#23454;&#35777;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21307;&#23398;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#20316;&#20026;&#39044;&#35757;&#32451;&#25968;&#25454;&#25110;&#26032;&#30340;&#22522;&#20934;&#65292;&#20026;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#21644;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2306.06494</link><description>&lt;p&gt;
&#21307;&#30103;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#19982;&#29983;&#25104;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#65306;&#19968;&#39033;&#26032;&#22522;&#20934;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Pre-training for Medical Vision-language Understanding and Generation: An Empirical Study with A New Benchmark. (arXiv:2306.06494v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20851;&#38190;&#22240;&#32032;&#65292;&#25552;&#20379;&#20102;&#21307;&#30103;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#20840;&#38754;&#23454;&#35777;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21307;&#23398;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#20316;&#20026;&#39044;&#35757;&#32451;&#25968;&#25454;&#25110;&#26032;&#30340;&#22522;&#20934;&#65292;&#20026;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#21644;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#12289;&#20840;&#38754;&#21644;&#36890;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;&#65288;VL&#65289;&#25968;&#25454;&#38598;&#65288;&#22914;MSCOCO&#65289;&#30340;&#21487;&#29992;&#24615;&#65292;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;VLP&#65289;&#24050;&#25104;&#20026;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#24182;&#34987;&#35777;&#26126;&#22312;&#21508;&#31181;VL&#20219;&#21153;&#65288;&#22914;&#35270;&#35273;&#38382;&#31572;&#65289;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22312;&#21307;&#30103;&#39046;&#22495;&#23545;VLP&#30340;&#30740;&#31350;&#36824;&#24456;&#23569;&#12290;&#20026;&#20102;&#20026;&#21307;&#30103;VL&#20219;&#21153;&#25552;&#20379;&#20840;&#38754;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#20840;&#38754;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#30740;&#31350;&#21487;&#33021;&#24433;&#21709;&#32479;&#19968;&#35270;&#35273;&#35821;&#35328;Transformer&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#20026;&#20102;&#33021;&#22815;&#36827;&#34892;&#26126;&#26234;&#21644;&#24555;&#36895;&#30340;&#39044;&#35757;&#32451;&#20915;&#31574;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RadioGraphy Captions&#65288;RGC&#65289;&#65292;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#22810;&#27169;&#24577;&#25918;&#23556;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;18,434&#20010;&#26469;&#33258;&#24320;&#25918;&#33719;&#21462;&#22312;&#32447;&#25968;&#25454;&#24211;MedPix&#30340;&#22270;&#20687;-&#26631;&#39064;&#23545;&#12290;RGC&#21487;&#20197;&#29992;&#20316;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20063;&#21487;&#20197;&#20316;&#20026;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#21644;&#21307;&#23398;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#30340;&#26032;&#22522;&#20934;&#12290;&#36890;&#36807;&#21033;&#29992;RGC&#21644;&#20854;&#20182;&#21487;&#29992;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#26032;&#30340;&#21307;&#30103;VL&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the availability of large-scale, comprehensive, and general-purpose vision-language (VL) datasets such as MSCOCO, vision-language pre-training (VLP) has become an active area of research and proven to be effective for various VL tasks such as visual-question answering. However, studies on VLP in the medical domain have so far been scanty. To provide a comprehensive perspective on VLP for medical VL tasks, we conduct a thorough experimental analysis to study key factors that may affect the performance of VLP with a unified vision-language Transformer. To allow making sound and quick pre-training decisions, we propose RadioGraphy Captions (RGC), a high-quality, multi-modality radiographic dataset containing 18,434 image-caption pairs collected from an open-access online database MedPix. RGC can be used as a pre-training dataset or a new benchmark for medical report generation and medical image-text retrieval. By utilizing RGC and other available datasets for pre-training, we develop
&lt;/p&gt;</description></item><item><title>LANISTR&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26694;&#26550;&#65292;&#21487;&#20174;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#22312;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.16556</link><description>&lt;p&gt;
LANISTR&#65306;&#20174;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#36827;&#34892;&#22810;&#27169;&#24577;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LANISTR: Multimodal Learning from Structured and Unstructured Data. (arXiv:2305.16556v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16556
&lt;/p&gt;
&lt;p&gt;
LANISTR&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26694;&#26550;&#65292;&#21487;&#20174;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#22312;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#24050;&#32463;&#22312;&#22788;&#29702;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#21253;&#25324;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#35270;&#39057;&#65289;&#26041;&#38754;&#23637;&#29616;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#20294;&#26159;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#26368;&#24120;&#35265;&#30340;&#24773;&#20917;&#26159;&#32467;&#26500;&#21270;&#65288;&#21253;&#25324;&#34920;&#26684;&#21644;&#26102;&#38388;&#24207;&#21015;&#65289;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#32467;&#21512;&#65292;&#20294;&#36825;&#19968;&#39046;&#22495;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30340;&#30740;&#31350;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LANISTR&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#35821;&#35328;&#12289;&#22270;&#20687;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#22810;&#27169;&#24577;&#36974;&#32617;&#25439;&#22833;&#65292;&#20351;&#24471;LANISTR&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#23398;&#20064;&#36328;&#27169;&#24577;&#20851;&#31995;&#65292;&#24182;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#22788;&#29702;&#32570;&#22833;&#30340;&#27169;&#24577;&#12290;&#22312;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;MIMIC-IV&#21644;Amazon Product Review&#19978;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#30456;&#27604;&#65292;LANISTR&#20998;&#21035;&#36798;&#21040;&#20102;6.47%&#65288;AUROC&#65289;&#21644;&#39640;&#36798;17.69%&#65288;&#20934;&#30830;&#24230;&#65289;&#30340;&#32477;&#23545;&#25552;&#21319;&#65292;&#24182;&#26174;&#31034;&#20986;&#26356;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal large-scale pretraining has shown impressive performance gains for unstructured data including language, image, audio, and video. Yet, the scenario most prominent in real-world applications is the existence of combination of structured (including tabular and time-series) and unstructured data, and this has so far been understudied. Towards this end, we propose LANISTR, a novel attention-based framework to learn from LANguage, Image, and STRuctured data. We introduce a new multimodal fusion module with a similarity-based multimodal masking loss that enables LANISTR to learn cross-modal relations from large-scale multimodal data with missing modalities during training and test time. On two publicly available challenging datasets, MIMIC-IV and Amazon Product Review, LANISTR achieves absolute improvements of 6.47% (AUROC) and up to 17.69% (accuracy), respectively, compared to the state-of-the-art multimodal models while showing superior generalization capabilities.
&lt;/p&gt;</description></item><item><title>PruMUX&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#32467;&#26500;&#21270;&#21098;&#26525;&#21644;&#25968;&#25454;&#22797;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;BERT-base&#27169;&#22411;&#30340;&#21534;&#21520;&#37327;&#12290;Auto-PruMUX&#21487;&#20197;&#39044;&#27979;&#20462;&#21098;&#21644;&#22797;&#29992;&#30340;&#39640;&#24615;&#33021;&#21442;&#25968;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2305.14706</link><description>&lt;p&gt;
PruMUX&#65306;&#21033;&#29992;&#27169;&#22411;&#21387;&#32553;&#22686;&#24378;&#25968;&#25454;&#22797;&#29992;
&lt;/p&gt;
&lt;p&gt;
PruMUX: Augmenting Data Multiplexing with Model Compression. (arXiv:2305.14706v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14706
&lt;/p&gt;
&lt;p&gt;
PruMUX&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#32467;&#26500;&#21270;&#21098;&#26525;&#21644;&#25968;&#25454;&#22797;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;BERT-base&#27169;&#22411;&#30340;&#21534;&#21520;&#37327;&#12290;Auto-PruMUX&#21487;&#20197;&#39044;&#27979;&#20462;&#21098;&#21644;&#22797;&#29992;&#30340;&#39640;&#24615;&#33021;&#21442;&#25968;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#26029;&#25193;&#22823;&#65292;&#25552;&#39640;&#20854;&#25512;&#29702;&#25928;&#29575;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290; &#20808;&#21069;&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;&#27169;&#22411;&#20462;&#21098;&#65292;&#30693;&#35782;&#33976;&#39311;&#21644;&#25968;&#25454;&#22797;&#29992;&#31561;&#25216;&#26415;&#65292;&#20197;&#22686;&#21152;&#27169;&#22411;&#30340;&#21534;&#21520;&#37327;&#32780;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#32467;&#26500;&#21270;&#21098;&#26525;&#21644;&#25968;&#25454;&#22797;&#29992;&#20004;&#31181;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#22686;&#24378;&#20004;&#31181;&#26041;&#27861;&#33719;&#24471;&#30340;&#21152;&#36895;&#20248;&#21183;&#12290; &#25105;&#20204;&#30340;&#26041;&#27861;PruMUX&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#38408;&#20540;&#20026;80&#65285;&#21040;74&#65285;&#30340;&#24773;&#20917;&#19979;&#65292;&#19982;BERT-base&#27169;&#22411;&#30456;&#27604;&#65292;&#21487;&#33719;&#24471;7.5-29.5&#20493;&#30340;&#21534;&#21520;&#37327;&#25552;&#39640;&#12290; &#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#20004;&#31181;&#25216;&#26415;&#20013;&#19981;&#21516;&#21442;&#25968;&#65288;&#20363;&#22914;&#31232;&#30095;&#24615;&#21644;&#22797;&#29992;&#22240;&#23376;&#65289;&#30340;&#21508;&#31181;&#32452;&#21512;&#65292;&#20197;&#25552;&#20379;&#26377;&#20851;&#20934;&#30830;&#24615;&#21644;&#21534;&#21520;&#37327;&#20043;&#38388;&#26435;&#34913;&#30340;&#32508;&#21512;&#20998;&#26512;&#12290; &#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Auto-PruMUX&#65292;&#36825;&#26159;&#19968;&#31181;&#20803;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#32473;&#23450;&#26399;&#26395;&#30340;&#31934;&#24230;&#25439;&#22833;&#39044;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#39044;&#27979;&#20462;&#21098;&#21644;&#22797;&#29992;&#30340;&#39640;&#24615;&#33021;&#21442;&#25968;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
As language models increase in size by the day, methods for efficient inference are critical to leveraging their capabilities for various applications. Prior work has investigated techniques like model pruning, knowledge distillation, and data multiplexing to increase model throughput without sacrificing accuracy. In this paper, we combine two such methods -structured pruning and data multiplexing -- to compound the speedup gains obtained by either method. Our approach, PruMUX, obtains up to 7.5-29.5X throughput improvement over BERT-base model with accuracy threshold from 80% to 74%. We further study various combinations of parameters (such as sparsity and multiplexing factor) in the two techniques to provide a comprehensive analysis of the tradeoff between accuracy and throughput in the resulting models. We then propose Auto-PruMUX, a meta-level model that can predict the high-performance parameters for pruning and multiplexing given a desired accuracy loss budget, providing a prac
&lt;/p&gt;</description></item><item><title>Structure-CLIP&#20351;&#29992;&#25991;&#26412;&#20013;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20351;&#29992;&#22330;&#26223;&#22270;&#24378;&#21270;&#22810;&#27169;&#24577;&#35821;&#35328;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06152</link><description>&lt;p&gt;
Structure-CLIP: &#32467;&#21512;&#32467;&#26500;&#30693;&#35782;&#20248;&#21270;&#22810;&#27169;&#24577;&#35821;&#35328;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Structure-CLIP: Enhance Multi-modal Language Representations with Structure Knowledge. (arXiv:2305.06152v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06152
&lt;/p&gt;
&lt;p&gt;
Structure-CLIP&#20351;&#29992;&#25991;&#26412;&#20013;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20351;&#29992;&#22330;&#26223;&#22270;&#24378;&#21270;&#22810;&#27169;&#24577;&#35821;&#35328;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#38656;&#35201;&#23545;&#25991;&#26412;&#36827;&#34892;&#35814;&#32454;&#35821;&#20041;&#29702;&#35299;&#30340;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#20219;&#21153;&#19978;&#36890;&#24120;&#34920;&#29616;&#36739;&#24046;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#19968;&#20123;&#30740;&#31350;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#21477;&#23376;&#20013;&#23384;&#22312;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#26469;&#22686;&#24378;&#22810;&#27169;&#24577;&#35821;&#35328;&#34920;&#31034;&#65292;&#23548;&#33268;&#24615;&#33021;&#36739;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#26694;&#26550;Structure-CLIP&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#30340;&#38544;&#24335;&#35814;&#32454;&#35821;&#20041;&#65292;&#20197;&#22686;&#24378;&#31934;&#32454;&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;(1)&#25105;&#20204;&#20351;&#29992;&#22330;&#26223;&#22270;&#26469;&#26356;&#21152;&#20851;&#27880;&#25991;&#26412;&#20013;&#30340;&#35814;&#32454;&#35821;&#20041;&#23398;&#20064;&#65292;&#24182;&#20805;&#20998;&#25506;&#32034;&#32454;&#31890;&#24230;&#35821;&#20041;&#20043;&#38388;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;(2)&#25105;&#20204;&#32467;&#21512;&#22330;&#26223;&#22270;&#30340;&#30693;&#35782;&#24378;&#21270;&#26694;&#26550;&#26469;&#20805;&#20998;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale vision-language pre-training has shown promising advances on various downstream tasks and achieved significant performance in multi-modal understanding and generation tasks. However, existing methods often perform poorly on image-text matching tasks that require a detailed semantics understanding of the text. Although there have been some works on this problem, they do not sufficiently exploit the structural knowledge present in sentences to enhance multi-modal language representations, which leads to poor performance. In this paper, we present an end-to-end framework Structure-CLIP, which integrates latent detailed semantics from the text to enhance fine-grained semantic representations. Specifically, (1) we use scene graphs in order to pay more attention to the detailed semantic learning in the text and fully explore structured knowledge between fine-grained semantics, and (2) we utilize the knowledge-enhanced framework with the help of the scene graph to make full use of
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20197;&#24184;&#31119;&#20026;&#23548;&#21521;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#38754;&#20020;&#30528;&#30693;&#35782;&#21644;&#21160;&#26426;&#20004;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#27010;&#24565;&#21270;&#12289;&#27979;&#37327;&#21644;&#20248;&#21270;&#24184;&#31119;&#65292;&#35774;&#35745;&#36866;&#24403;&#30340;AI&#34892;&#21160;&#65292;&#20197;&#21450;&#28608;&#21169;&#25514;&#26045;&#12289;&#36130;&#21153;&#21644;&#23459;&#20256;&#39118;&#38505;&#30340;&#19981;&#19968;&#33268;&#20197;&#21450;&#25968;&#25454;&#33719;&#21462;&#30340;&#32570;&#20047;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#38656;&#35201;&#22312;&#31185;&#23398;&#29702;&#35299;AI&#31995;&#32479;&#23545;&#24184;&#31119;&#24433;&#21709;&#26041;&#38754;&#36827;&#34892;&#30740;&#31350;&#65292;&#24182;&#25351;&#23548;&#35774;&#35745;&#34892;&#21160;&#12290;</title><link>http://arxiv.org/abs/2304.12241</link><description>&lt;p&gt;
&#27491;&#21521;&#20154;&#24037;&#26234;&#33021;&#65306;&#35774;&#35745;&#20197;&#24184;&#31119;&#20026;&#23548;&#21521;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Positive AI: Key Challenges for Designing Wellbeing-aligned Artificial Intelligence. (arXiv:2304.12241v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12241
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20197;&#24184;&#31119;&#20026;&#23548;&#21521;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#38754;&#20020;&#30528;&#30693;&#35782;&#21644;&#21160;&#26426;&#20004;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#27010;&#24565;&#21270;&#12289;&#27979;&#37327;&#21644;&#20248;&#21270;&#24184;&#31119;&#65292;&#35774;&#35745;&#36866;&#24403;&#30340;AI&#34892;&#21160;&#65292;&#20197;&#21450;&#28608;&#21169;&#25514;&#26045;&#12289;&#36130;&#21153;&#21644;&#23459;&#20256;&#39118;&#38505;&#30340;&#19981;&#19968;&#33268;&#20197;&#21450;&#25968;&#25454;&#33719;&#21462;&#30340;&#32570;&#20047;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#38656;&#35201;&#22312;&#31185;&#23398;&#29702;&#35299;AI&#31995;&#32479;&#23545;&#24184;&#31119;&#24433;&#21709;&#26041;&#38754;&#36827;&#34892;&#30740;&#31350;&#65292;&#24182;&#25351;&#23548;&#35774;&#35745;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27491;&#36805;&#36895;&#25913;&#21464;&#31038;&#20250;&#65292;&#36843;&#20999;&#38656;&#35201;&#30830;&#20445;&#20854;&#31215;&#26497;&#24433;&#21709;&#12290;&#26412;&#25991;&#37319;&#29992;&#31215;&#26497;&#35774;&#35745;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23558;&#20854;&#35270;&#20026;&#35774;&#35745;&#20027;&#21160;&#25903;&#25345;&#20154;&#31867;&#24184;&#31119;&#30340;AI&#31995;&#32479;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#20197;&#24184;&#31119;&#20026;&#23548;&#21521;&#30340;AI&#31995;&#32479;&#26159;&#22256;&#38590;&#30340;&#12290;&#26412;&#25991;&#37319;&#29992;&#25511;&#21046;&#35770;&#30340;&#35270;&#35282;&#65292;&#35782;&#21035;&#20102;&#20004;&#20010;&#31867;&#21035;&#20013;&#30340;&#21313;&#20108;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#30693;&#35782;&#32570;&#20047;&#21644;&#21160;&#26426;&#32570;&#20047;&#12290;&#30693;&#35782;&#38556;&#30861;&#21253;&#25324;&#27010;&#24565;&#21270;&#12289;&#27979;&#37327;&#21644;&#20248;&#21270;&#24184;&#31119;&#65292;&#24182;&#35774;&#35745;&#36866;&#24403;&#30340;AI&#34892;&#21160;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#21160;&#26426;&#38556;&#30861;&#21253;&#25324;&#19981;&#19968;&#33268;&#30340;&#28608;&#21169;&#25514;&#26045;&#12289;&#36130;&#21153;&#21644;&#23459;&#20256;&#39118;&#38505;&#65292;&#20197;&#21450;&#32570;&#20047;&#25968;&#25454;&#33719;&#21462;&#38459;&#27490;&#20102;&#65288;&#31532;&#19977;&#26041;&#65289;&#23545;&#24184;&#31119;&#36827;&#34892;&#30740;&#31350;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30740;&#31350;&#35758;&#31243;&#65292;&#21253;&#25324;&#25512;&#36827;&#23545;AI&#31995;&#32479;&#23545;&#24184;&#31119;&#24433;&#21709;&#30340;&#31185;&#23398;&#29702;&#35299;&#65292;&#24182;&#25351;&#23548;&#22914;&#20309;&#36827;&#34892;AI&#31995;&#32479;&#30340;&#35774;&#35745;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) is rapidly transforming society, creating an urgent need to ensure its positive impact. In this article, we take a positive design approach towards this issue, viewing it as a matter of designing AI systems that actively support human wellbeing. However, designing wellbeing-aligned AI systems is difficult. This article adopts a cybernetic perspective to identify twelve key challenges across two categories: lack of knowledge and lack of motivation. Knowledge barriers include challenges in conceptualizing, measuring, and optimizing for wellbeing, then designing appropriate AI actions. Motivation barriers include misaligned incentives, financial and publicity risks, and a lack of data access preventing (third-party) research on wellbeing. To address these challenges we have captured our key takeaways in a research agenda related to 1) advancing the scientific understanding of the impact of AI systems on wellbeing, and 2) guiding design actions on how AI system
&lt;/p&gt;</description></item><item><title>HyperTab&#26159;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#32467;&#21512;&#20102;&#38543;&#26426;&#26862;&#26519;&#21644;&#31070;&#32463;&#32593;&#32476;&#20248;&#28857;&#30340;&#23567;&#22411;&#34920;&#26684;&#25968;&#25454;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#27599;&#20010;&#29305;&#23450;&#20302;&#32500;&#35270;&#22270;&#22788;&#29702;&#25968;&#25454;&#65292;&#34394;&#25311;&#22686;&#21152;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#65292;&#36991;&#20813;&#36807;&#24230;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2304.03543</link><description>&lt;p&gt;
HyperTab: &#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#23567;&#22411;&#34920;&#26684;&#25968;&#25454;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HyperTab: Hypernetwork Approach for Deep Learning on Small Tabular Datasets. (arXiv:2304.03543v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03543
&lt;/p&gt;
&lt;p&gt;
HyperTab&#26159;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#32467;&#21512;&#20102;&#38543;&#26426;&#26862;&#26519;&#21644;&#31070;&#32463;&#32593;&#32476;&#20248;&#28857;&#30340;&#23567;&#22411;&#34920;&#26684;&#25968;&#25454;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#27599;&#20010;&#29305;&#23450;&#20302;&#32500;&#35270;&#22270;&#22788;&#29702;&#25968;&#25454;&#65292;&#34394;&#25311;&#22686;&#21152;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#65292;&#36991;&#20813;&#36807;&#24230;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#34920;&#29616;&#65292;&#20363;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#20294;&#23427;&#22312;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#30456;&#23545;&#20256;&#32479;&#27973;&#23618;&#26041;&#27861;&#30340;&#20248;&#21183;&#20173;&#28982;&#20540;&#24471;&#21830;&#27063;&#12290;&#22312;&#23567;&#22411;&#25968;&#25454;&#38598;&#65288;&#23567;&#20110;1k&#20010;&#26679;&#26412;&#65289;&#19978;&#36229;&#36807;&#26641;&#29366;&#38598;&#25104;&#65288;&#22914;XGBoost&#25110;&#38543;&#26426;&#26862;&#26519;&#65289;&#30340;&#34920;&#29616;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;HyperTab&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#35299;&#20915;&#34920;&#26684;&#25968;&#25454;&#38598;&#23567;&#26679;&#26412;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#38543;&#26426;&#26862;&#26519;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#65292;HyperTab&#29983;&#25104;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#38598;&#21512;&#65292;&#20854;&#20013;&#27599;&#20010;&#30446;&#26631;&#27169;&#22411;&#19987;&#38376;&#22788;&#29702;&#25968;&#25454;&#30340;&#29305;&#23450;&#20302;&#32500;&#35270;&#22270;&#12290;&#30001;&#20110;&#27599;&#20010;&#35270;&#22270;&#25198;&#28436;&#25968;&#25454;&#22686;&#24378;&#30340;&#35282;&#33394;&#65292;&#25105;&#20204;&#22312;&#20445;&#25345;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#19981;&#21464;&#30340;&#24773;&#20917;&#19979;&#65292;&#34394;&#25311;&#22686;&#21152;&#20102;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#36807;&#24230;&#25311;&#21512;&#12290;&#25105;&#20204;&#23545;40&#22810;&#20010;&#22823;&#23567;&#19981;&#21516;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#23545;HyperTab&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has achieved impressive performance in many domains, such as computer vision and natural language processing, but its advantage over classical shallow methods on tabular datasets remains questionable. It is especially challenging to surpass the performance of tree-like ensembles, such as XGBoost or Random Forests, on small-sized datasets (less than 1k samples). To tackle this challenge, we introduce HyperTab, a hypernetwork-based approach to solving small sample problems on tabular datasets. By combining the advantages of Random Forests and neural networks, HyperTab generates an ensemble of neural networks, where each target model is specialized to process a specific lower-dimensional view of the data. Since each view plays the role of data augmentation, we virtually increase the number of training samples while keeping the number of trainable parameters unchanged, which prevents model overfitting. We evaluated HyperTab on more than 40 tabular datasets of a varying number
&lt;/p&gt;</description></item><item><title>&#27492;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HALO&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#20010;&#23618;&#32423;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#20445;&#30495;&#12289;&#32454;&#31890;&#24230;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#20934;&#30830;&#30340;ML&#27169;&#22411;&#65292;&#19988;&#26080;&#38656;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.02169</link><description>&lt;p&gt;
&#22522;&#20110;&#23618;&#32423;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#21512;&#25104;&#26497;&#39640;&#32500;&#38271;&#26399;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;
&lt;/p&gt;
&lt;p&gt;
Synthesize Extremely High-dimensional Longitudinal Electronic Health Records via Hierarchical Autoregressive Language Model. (arXiv:2304.02169v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02169
&lt;/p&gt;
&lt;p&gt;
&#27492;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HALO&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#20010;&#23618;&#32423;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#20445;&#30495;&#12289;&#32454;&#31890;&#24230;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#20934;&#30830;&#30340;ML&#27169;&#22411;&#65292;&#19988;&#26080;&#38656;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;(EHRs)&#33021;&#22815;&#22312;&#26426;&#22120;&#23398;&#20064;(ML)&#21644;&#32479;&#35745;&#20998;&#26512;&#20013;&#20316;&#20026;&#30495;&#23454;EHRs&#30340;&#26367;&#20195;&#21697;&#65292;&#26082;&#30495;&#23454;&#21448;&#20445;&#25252;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#32500;&#25968;&#25454;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#65292;&#20197;&#20854;&#21407;&#22987;&#39640;&#24230;&#32500;&#24418;&#24335;&#29983;&#25104;&#39640;&#20445;&#30495;&#12289;&#32454;&#31890;&#24230;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;(EHR)&#25968;&#25454;&#23545;&#29616;&#26377;&#26041;&#27861;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Hierarchical Autoregressive Language mOdel (HALO)&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#32437;&#21521;&#39640;&#32500;EHR&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#20445;&#30041;&#20102;&#30495;&#23454;EHR&#30340;&#32479;&#35745;&#29305;&#24615;&#65292;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#20934;&#30830;&#30340;ML&#27169;&#22411;&#32780;&#19981;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;HALO&#26041;&#27861;&#34987;&#35774;&#35745;&#20026;&#19968;&#20010;&#23618;&#32423;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#29983;&#25104;&#19968;&#32452;&#38024;&#23545;&#21307;&#23398;&#20195;&#30721;&#12289;&#20020;&#24202;&#23601;&#35786;&#21644;&#30149;&#20154;&#35760;&#24405;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65292;&#21487;&#20197;&#22312;&#20854;&#21407;&#22987;&#26410;&#32858;&#21512;&#24418;&#24335;&#19979;&#29983;&#25104;&#30495;&#23454;&#30340;EHR&#25968;&#25454;&#65292;&#26080;&#38656;&#36827;&#34892;&#21464;&#37327;&#36873;&#25321;&#25110;&#32858;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#20135;&#29983;&#22823;&#37327;&#30340;&#38543;&#26426;&#26679;&#26412;&#65292;&#20197;&#25552;&#20379;&#22797;&#26434;&#24230;&#36739;&#20302;&#20294;&#20173;&#26377;&#24847;&#20041;&#30340;EHR&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic electronic health records (EHRs) that are both realistic and preserve privacy can serve as an alternative to real EHRs for machine learning (ML) modeling and statistical analysis. However, generating high-fidelity and granular electronic health record (EHR) data in its original, highly-dimensional form poses challenges for existing methods due to the complexities inherent in high-dimensional data. In this paper, we propose Hierarchical Autoregressive Language mOdel (HALO) for generating longitudinal high-dimensional EHR, which preserve the statistical properties of real EHR and can be used to train accurate ML models without privacy concerns. Our HALO method, designed as a hierarchical autoregressive model, generates a probability density function of medical codes, clinical visits, and patient records, allowing for the generation of realistic EHR data in its original, unaggregated form without the need for variable selection or aggregation. Additionally, our model also produc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20803;&#21270;&#32769;&#21270;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#65288;PADA&#65289;&#65292;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#22810;&#26679;&#30340;&#20302;&#23618;&#32769;&#21270;&#32454;&#33410;&#65292;&#24182;&#20351;&#29992;&#27010;&#29575;&#32769;&#21270;&#23884;&#20837;&#65288;PAE&#65289;&#25429;&#25417;&#22810;&#26679;&#30340;&#39640;&#23618;&#32769;&#21270;&#27169;&#24335;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#26356;&#22810;&#26679;&#21644;&#39640;&#36136;&#37327;&#30340;&#32769;&#21270;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.11086</link><description>&lt;p&gt;
&#22810;&#20803;&#21270;&#32769;&#21270;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Pluralistic Aging Diffusion Autoencoder. (arXiv:2303.11086v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20803;&#21270;&#32769;&#21270;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#65288;PADA&#65289;&#65292;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#22810;&#26679;&#30340;&#20302;&#23618;&#32769;&#21270;&#32454;&#33410;&#65292;&#24182;&#20351;&#29992;&#27010;&#29575;&#32769;&#21270;&#23884;&#20837;&#65288;PAE&#65289;&#25429;&#25417;&#22810;&#26679;&#30340;&#39640;&#23618;&#32769;&#21270;&#27169;&#24335;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#26356;&#22810;&#26679;&#21644;&#39640;&#36136;&#37327;&#30340;&#32769;&#21270;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#32769;&#21270;&#26159;&#19968;&#20010;&#27169;&#31946;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#19968;&#20010;&#32473;&#23450;&#30340;&#36755;&#20837;&#21487;&#33021;&#23545;&#24212;&#30528;&#22810;&#31181;&#21512;&#29702;&#30340;&#32769;&#21270;&#27169;&#24335;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#36890;&#24120;&#21482;&#33021;&#20135;&#29983;&#19968;&#20010;&#30830;&#23450;&#24615;&#30340;&#20272;&#35745;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;CLIP&#39537;&#21160;&#30340;&#22810;&#20803;&#21270;&#32769;&#21270;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#65288;PADA&#65289;&#65292;&#20197;&#22686;&#24378;&#32769;&#21270;&#27169;&#24335;&#30340;&#22810;&#26679;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#19968;&#20010;&#39034;&#24207;&#21435;&#22122;&#36870;&#36807;&#31243;&#29983;&#25104;&#22810;&#26679;&#30340;&#20302;&#23618;&#32769;&#21270;&#32454;&#33410;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27010;&#29575;&#32769;&#21270;&#23884;&#20837;&#65288;PAE&#65289;&#65292;&#20197;&#22312;&#24120;&#35265;&#30340;CLIP&#28508;&#22312;&#31354;&#38388;&#20013;&#23558;&#24180;&#40836;&#20449;&#24687;&#34920;&#31034;&#20026;&#27010;&#29575;&#20998;&#24067;&#65292;&#20174;&#32780;&#25429;&#25417;&#22810;&#26679;&#30340;&#39640;&#23618;&#32769;&#21270;&#27169;&#24335;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25991;&#26412;&#24341;&#23548;&#30340;KL&#25955;&#24230;&#25439;&#22833;&#26469;&#25351;&#23548;&#36825;&#20010;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26681;&#25454;&#38754;&#21521;&#24320;&#25918;&#19990;&#30028;&#30340;&#32769;&#21270;&#25991;&#26412;&#21644;&#20219;&#24847;&#26410;&#35265;&#30340;&#38754;&#37096;&#22270;&#20687;&#23454;&#29616;&#22810;&#20803;&#21270;&#30340;&#20154;&#33080;&#32769;&#21270;&#12290;&#23450;&#24615;&#21644;&#23450;&#37327;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#26356;&#22810;&#26679;&#21644;&#39640;&#36136;&#37327;&#30340;&#32769;&#21270;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face aging is an ill-posed problem because multiple plausible aging patterns may correspond to a given input. Most existing methods often produce one deterministic estimation. This paper proposes a novel CLIP-driven Pluralistic Aging Diffusion Autoencoder (PADA) to enhance the diversity of aging patterns. First, we employ diffusion models to generate diverse low-level aging details via a sequential denoising reverse process. Second, we present Probabilistic Aging Embedding (PAE) to capture diverse high-level aging patterns, which represents age information as probabilistic distributions in the common CLIP latent space. A text-guided KL-divergence loss is designed to guide this learning. Our method can achieve pluralistic face aging conditioned on open-world aging texts and arbitrary unseen face images. Qualitative and quantitative experiments demonstrate that our method can generate more diverse and high-quality plausible aging results.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#22312;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#20013;&#25506;&#32034;&#20102;Anderson&#21152;&#36895;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;AA&#65292;&#36798;&#21040;&#20102;&#26174;&#33879;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#21644;&#20943;&#23567;&#35757;&#32451;&#35823;&#24046;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.00347</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#29289;&#20449;&#24687;&#23398;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;Anderson&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
Anderson Acceleration For Bioinformatics-Based Machine Learning. (arXiv:2302.00347v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00347
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#22312;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#20013;&#25506;&#32034;&#20102;Anderson&#21152;&#36895;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;AA&#65292;&#36798;&#21040;&#20102;&#26174;&#33879;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#21644;&#20943;&#23567;&#35757;&#32451;&#35823;&#24046;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Anderson&#21152;&#36895;&#65288;AA&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#21152;&#24555;&#36845;&#20195;&#31639;&#27861;&#25910;&#25947;&#36895;&#24230;&#30340;&#33879;&#21517;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#20248;&#21270;&#31561;&#21508;&#20010;&#39046;&#22495;&#12290;&#23613;&#31649;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;AA&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#20854;&#22312;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#20013;&#30340;&#26377;&#25928;&#24615;&#23578;&#26410;&#24471;&#21040;&#24443;&#24213;&#30740;&#31350;&#12290;&#23588;&#20854;&#26159;&#23545;&#20110;&#34920;&#26684;&#25968;&#25454;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#32780;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#36825;&#20123;&#22330;&#26223;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#21463;&#21040;&#20102;&#26377;&#38480;&#30340;&#20851;&#27880;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#31181;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#20998;&#31867;&#22120;&#21464;&#31181;&#65292;&#32467;&#21512;&#20102;AA&#20197;&#21152;&#36895;&#25910;&#25947;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#22312;&#22810;&#20010;&#29983;&#29289;&#23398;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#21644;&#19981;&#20351;&#29992;Anderson&#21152;&#36895;&#30340;SVM&#30340;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#22312;&#36845;&#20195;&#27425;&#25968;&#22686;&#21152;&#26102;&#65292;&#20351;&#29992;AA&#26174;&#33879;&#25552;&#39640;&#20102;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#20943;&#23567;&#20102;&#35757;&#32451;&#35823;&#24046;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25552;&#20379;&#20102;&#19968;&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anderson acceleration (AA) is a well-known method for accelerating the convergence of iterative algorithms, with applications in various fields including deep learning and optimization. Despite its popularity in these areas, the effectiveness of AA in classical machine learning classifiers has not been thoroughly studied. Tabular data, in particular, presents a unique challenge for deep learning models, and classical machine learning models are known to perform better in these scenarios. However, the convergence analysis of these models has received limited attention. To address this gap in research, we implement a support vector machine (SVM) classifier variant that incorporates AA to speed up convergence. We evaluate the performance of our SVM with and without Anderson acceleration on several datasets from the biology domain and demonstrate that the use of AA significantly improves convergence and reduces the training loss as the number of iterations increases. Our findings provide a
&lt;/p&gt;</description></item><item><title>BallGAN&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;3D&#24863;&#30693;GAN&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#32972;&#26223;&#36817;&#20284;&#20026;&#29699;&#24418;&#34920;&#38754;&#65292;&#24182;&#20351;&#29992;&#29305;&#23450;&#32422;&#26463;&#26469;&#20943;&#23569;&#32972;&#26223;&#33258;&#30001;&#24230;&#65292;&#21487;&#20197;&#20135;&#29983;&#26356;&#21512;&#29702;&#30340;3D&#20960;&#20309;&#12290;</title><link>http://arxiv.org/abs/2301.09091</link><description>&lt;p&gt;
BallGAN: &#24102;&#26377;&#29699;&#24418;&#32972;&#26223;&#30340;3D&#24863;&#30693;&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
BallGAN: 3D-aware Image Synthesis with a Spherical Background. (arXiv:2301.09091v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09091
&lt;/p&gt;
&lt;p&gt;
BallGAN&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;3D&#24863;&#30693;GAN&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#32972;&#26223;&#36817;&#20284;&#20026;&#29699;&#24418;&#34920;&#38754;&#65292;&#24182;&#20351;&#29992;&#29305;&#23450;&#32422;&#26463;&#26469;&#20943;&#23569;&#32972;&#26223;&#33258;&#30001;&#24230;&#65292;&#21487;&#20197;&#20135;&#29983;&#26356;&#21512;&#29702;&#30340;3D&#20960;&#20309;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#24863;&#30693;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26088;&#22312;&#21512;&#25104;&#36924;&#30495;&#30340;3D&#22330;&#26223;&#65292;&#20197;&#20415;&#21487;&#20197;&#20197;&#20219;&#24847;&#35282;&#24230;&#36827;&#34892;&#28210;&#26579;&#20197;&#20135;&#29983;&#22270;&#20687;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#20294;&#23427;&#20204;&#22312;&#35757;&#32451;&#19981;&#31283;&#23450;&#25110;&#23384;&#22312;&#19981;&#33258;&#28982;&#30340;3D&#20960;&#20309;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#25105;&#20204;&#20551;&#35774;3D&#20960;&#20309;&#22312;&#32422;&#26463;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#26159;&#19981;&#30830;&#23450;&#30340;&#65292;&#21363;&#20165;&#23558;&#20854;&#20998;&#31867;&#20026;&#30495;&#23454;&#22270;&#20687;&#23545;&#20110;&#37492;&#21035;&#22120;&#26469;&#35828;&#26159;&#19981;&#22815;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#32972;&#26223;&#36817;&#20284;&#20026;&#29699;&#24418;&#34920;&#38754;&#65292;&#24182;&#23558;&#22330;&#26223;&#34920;&#31034;&#20026;&#25918;&#32622;&#22312;&#29699;&#20307;&#20013;&#30340;&#21069;&#26223;&#21644;&#34180;&#29699;&#24418;&#32972;&#26223;&#30340;&#32852;&#21512;&#12290;&#36825;&#26679;&#21487;&#20197;&#20943;&#23569;&#32972;&#26223;&#22330;&#30340;&#33258;&#30001;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;&#20307;&#28210;&#26579;&#26041;&#31243;&#65292;&#24182;&#21152;&#20837;&#20102;&#19987;&#29992;&#30340;&#32422;&#26463;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;BallGAN&#30340;&#26032;&#22411;3D&#24863;&#30693;GAN&#26694;&#26550;&#12290; BallGAN&#20855;&#26377;&#20197;&#19979;&#22810;&#20010;&#20248;&#28857;&#12290;1&#65289;&#23427;&#20135;&#29983;&#20102;&#26356;&#21512;&#29702;&#30340;3D&#20960;&#20309;&#65307;&#22330;&#26223;&#22312;&#19981;&#21516;&#35270;&#35282;&#19979;&#30340;&#22270;&#20687;&#20855;&#26377;&#26356;&#22909;&#30340;&#20809;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D-aware GANs aim to synthesize realistic 3D scenes such that they can be rendered in arbitrary perspectives to produce images. Although previous methods produce realistic images, they suffer from unstable training or degenerate solutions where the 3D geometry is unnatural. We hypothesize that the 3D geometry is underdetermined due to the insufficient constraint, i.e., being classified as real image to the discriminator is not enough. To solve this problem, we propose to approximate the background as a spherical surface and represent a scene as a union of the foreground placed in the sphere and the thin spherical background. It reduces the degree of freedom in the background field. Accordingly, we modify the volume rendering equation and incorporate dedicated constraints to design a novel 3D-aware GAN framework named BallGAN. BallGAN has multiple advantages as follows. 1) It produces more reasonable 3D geometry; the images of a scene across different viewpoints have better photometric 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#21487;&#24494;&#20998;&#20809;&#32447;&#36861;&#36394;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#22810;&#27169;&#20809;&#23548;&#31649;&#20013;&#23454;&#29616;&#20102;&#23545;Megapixel&#22270;&#20687;&#30340;&#39640;&#25928;&#20256;&#36755;&#65292;&#36890;&#36807;&#20351;&#29992;&#25968;&#23383;&#23402;&#29983;&#21644;U-Net&#32467;&#21512;&#65292;&#20943;&#23569;&#20102;&#35299;&#30721;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#27169;&#24335;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;&#26368;&#32456;&#23454;&#29616;&#20102;&#23545;&#22823;&#35268;&#27169;&#25968;&#23383;&#22270;&#20687;&#30340;&#35299;&#30721;&#21644;&#26816;&#32034;&#65292;&#20026;&#20809;&#23398;&#23384;&#20648;&#24212;&#29992;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#25216;&#26415;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2301.06496</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#24494;&#20998;&#20809;&#32447;&#36861;&#36394;&#21644;&#26426;&#22120;&#23398;&#20064;&#22312;&#22810;&#27169;&#20809;&#23548;&#31649;&#20013;&#39640;&#25928;&#20256;&#36755;Megapixel&#22270;&#20687;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Efficient data transport over multimode light-pipes with Megapixel images using differentiable ray tracing and Machine-learning. (arXiv:2301.06496v3 [physics.optics] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#21487;&#24494;&#20998;&#20809;&#32447;&#36861;&#36394;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#22810;&#27169;&#20809;&#23548;&#31649;&#20013;&#23454;&#29616;&#20102;&#23545;Megapixel&#22270;&#20687;&#30340;&#39640;&#25928;&#20256;&#36755;&#65292;&#36890;&#36807;&#20351;&#29992;&#25968;&#23383;&#23402;&#29983;&#21644;U-Net&#32467;&#21512;&#65292;&#20943;&#23569;&#20102;&#35299;&#30721;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#27169;&#24335;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;&#26368;&#32456;&#23454;&#29616;&#20102;&#23545;&#22823;&#35268;&#27169;&#25968;&#23383;&#22270;&#20687;&#30340;&#35299;&#30721;&#21644;&#26816;&#32034;&#65292;&#20026;&#20809;&#23398;&#23384;&#20648;&#24212;&#29992;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#25216;&#26415;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22810;&#27169;&#20809;&#32420;&#33021;&#22815;&#26377;&#25928;&#22320;&#32422;&#26463;&#21644;&#20256;&#36755;&#20809;&#32447;&#65292;&#22312;&#20256;&#36755;&#36890;&#36807;&#22810;&#27169;&#20809;&#32420;&#30340;&#22270;&#20687;&#26041;&#38754;&#30340;&#30740;&#31350;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#35299;&#30721;&#22823;&#35268;&#27169;&#25968;&#23383;&#22270;&#20687;&#65292;&#20197;&#26368;&#22823;&#21270;&#20809;&#23398;&#23384;&#20648;&#24212;&#29992;&#30340;&#39029;&#38754;&#23481;&#37327;&#12290;&#36890;&#36807;&#20351;&#29992;&#27627;&#31859;&#32423;&#26041;&#24418;&#27178;&#25130;&#38754;&#27874;&#23548;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;8&#20301;&#31354;&#38388;&#20809;&#35843;&#21046;&#22120;&#25104;&#20687;&#65292;&#23558;&#25968;&#25454;&#34920;&#31034;&#20026;&#31526;&#21495;&#30697;&#38453;&#12290;&#36890;&#36807;&#23558;&#31995;&#32479;&#30340;&#25968;&#23383;&#23402;&#29983;&#19982;U-Net&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#21487;&#20197;&#20165;&#20351;&#29992;&#39640;&#25928;&#30340;&#21367;&#31215;&#25805;&#20316;&#26469;&#26816;&#32034;&#39640;&#36798;66 kB&#30340;&#25968;&#25454;&#65292;&#32780;&#36890;&#24120;&#30340;&#35299;&#30721;&#22120;&#22312;&#22788;&#29702;&#31354;&#38388;&#28151;&#20081;&#30340;&#25968;&#25454;&#26102;&#20250;&#23548;&#33268;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;O(n^2)&#12290;&#36890;&#36807;&#27604;&#36739;&#22522;&#20110;&#21487;&#35757;&#32451;&#20809;&#32447;&#36861;&#36394;&#21644;&#22522;&#20110;&#26412;&#24449;&#27169;&#30340;&#25968;&#23383;&#23402;&#29983;&#65292;&#25105;&#20204;&#21457;&#29616;&#21069;&#32773;&#20855;&#26377;&#20248;&#36234;&#24615;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#36890;&#36807;&#35843;&#25972;&#20809;&#23398;&#32570;&#38519;&#26469;&#24357;&#34917;&#27169;&#25311;&#19982;&#23454;&#39564;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#20351;&#29992;&#21487;&#24494;&#20998;&#20114;&#20449;&#24687;&#26041;&#27861;&#23545;&#25972;&#20010;&#27969;&#31243;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieving images transmitted through multi-mode fibers is of growing interest, thanks to their ability to confine and transport light efficiently in a compact system. Here, we demonstrate machine-learning-based decoding of large-scale digital images (pages), maximizing page capacity for optical storage applications. Using a millimeter-sized square cross-section waveguide, we image an 8-bit spatial light modulator, presenting data as a matrix of symbols. Normally, decoders will incur a prohibitive O(n^2) computational scaling to decode n symbols in spatially scrambled data. However, by combining a digital twin of the setup with a U-Net, we can retrieve up to 66 kB using efficient convolutional operations only. We compare trainable ray-tracing-based with eigenmode-based twins and show the former to be superior thanks to its ability to overcome the simulation-to-experiment gap by adjusting to optical imperfections. We train the pipeline end-to-end using a differentiable mutual-informatio
&lt;/p&gt;</description></item><item><title>DexBERT&#26159;&#19968;&#39033;&#30740;&#31350;Android&#23383;&#33410;&#30721;&#30340;&#26377;&#25928;&#12289;&#20219;&#21153;&#26080;&#20851;&#21644;&#31934;&#32454;&#21270;&#34920;&#31034;&#23398;&#20064;&#30340;&#24037;&#20316;&#65292;&#26088;&#22312;&#25429;&#25417;&#20449;&#24687;&#20197;&#36866;&#29992;&#20110;&#21508;&#31181;&#20302;&#32423;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2212.05976</link><description>&lt;p&gt;
DexBERT&#65306;Android&#23383;&#33410;&#30721;&#30340;&#26377;&#25928;&#12289;&#20219;&#21153;&#26080;&#20851;&#21644;&#31934;&#32454;&#21270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DexBERT: Effective, Task-Agnostic and Fine-grained Representation Learning of Android Bytecode. (arXiv:2212.05976v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05976
&lt;/p&gt;
&lt;p&gt;
DexBERT&#26159;&#19968;&#39033;&#30740;&#31350;Android&#23383;&#33410;&#30721;&#30340;&#26377;&#25928;&#12289;&#20219;&#21153;&#26080;&#20851;&#21644;&#31934;&#32454;&#21270;&#34920;&#31034;&#23398;&#20064;&#30340;&#24037;&#20316;&#65292;&#26088;&#22312;&#25429;&#25417;&#20449;&#24687;&#20197;&#36866;&#29992;&#20110;&#21508;&#31181;&#20302;&#32423;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#23637;&#65292;&#22823;&#37327;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#30340;&#33258;&#21160;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;&#22312;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#36719;&#20214;&#24037;&#20214;&#65288;&#22914;&#28304;&#20195;&#30721;&#25110;&#21487;&#25191;&#34892;&#20195;&#30721;&#65289;&#26102;&#65292;&#20851;&#38190;&#26159;&#23558;&#23427;&#20204;&#36716;&#21270;&#20026;&#36866;&#21512;&#23398;&#20064;&#30340;&#24418;&#24335;&#12290;&#20256;&#32479;&#19978;&#65292;&#30740;&#31350;&#20154;&#21592;&#20381;&#36182;&#20110;&#22522;&#20110;&#19987;&#23478;&#30693;&#35782;&#30340;&#25163;&#21160;&#36873;&#25321;&#29305;&#24449;&#65292;&#20294;&#26377;&#26102;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#19981;&#20934;&#30830;&#25110;&#19981;&#23436;&#20840;&#12290;&#34920;&#31034;&#23398;&#20064;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#33021;&#33258;&#21160;&#36873;&#25321;&#21512;&#36866;&#30340;&#34920;&#31034;&#21644;&#30456;&#20851;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19982;Android&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#29616;&#26377;&#27169;&#22411;&#22914;apk2vec&#32858;&#28966;&#20110;&#25972;&#20010;&#24212;&#29992;&#31243;&#24207;&#32423;&#21035;&#65292;&#25110;&#32773;&#19987;&#27880;&#20110;&#29305;&#23450;&#20219;&#21153;&#22914;smali2vec&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26159;&#19968;&#39033;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#26088;&#22312;&#35843;&#26597;Android&#23383;&#33410;&#30721;&#30340;&#26377;&#25928;&#12289;&#20219;&#21153;&#26080;&#20851;&#21644;&#31934;&#32454;&#21270;&#30340;&#36890;&#29992;&#34920;&#31034;&#65292;&#20197;&#20943;&#36731;&#36825;&#20004;&#20010;&#38480;&#21046;&#12290;&#36825;&#20123;&#34920;&#31034;&#26088;&#22312;&#25429;&#25417;&#19982;&#21508;&#31181;&#20302;&#32423;&#19979;&#28216;&#20219;&#21153;&#65288;&#20363;&#22914;&#31867;&#32423;&#21035;&#65289;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The automation of a large number of software engineering tasks is becoming possible thanks to Machine Learning (ML). Central to applying ML to software artifacts (like source or executable code) is converting them into forms suitable for learning. Traditionally, researchers have relied on manually selected features, based on expert knowledge which is sometimes imprecise and generally incomplete. Representation learning has allowed ML to automatically choose suitable representations and relevant features. Yet, for Android-related tasks, existing models like apk2vec focus on whole-app levels, or target specific tasks like smali2vec, which limits their applicability. Our work is part of a new line of research that investigates effective, task-agnostic, and fine-grained universal representations of bytecode to mitigate both of these two limitations. Such representations aim to capture information relevant to various low-level downstream tasks (e.g., at the class-level). We are inspired by 
&lt;/p&gt;</description></item><item><title>&#25968;&#23383;&#24066;&#22330;&#27861;&#35268;&#23450;&#20102;&#23545;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#27431;&#30431;&#35268;&#21017;&#65292;&#20854;&#23545;&#25259;&#38706;&#35201;&#27714;&#12289;&#20154;&#24037;&#26234;&#33021;&#35757;&#32451;&#25968;&#25454;&#30340;&#31649;&#29702;&#12289;&#35775;&#38382;&#35268;&#21017;&#21644;&#20844;&#24179;&#25490;&#21517;&#21046;&#24230;&#20855;&#26377;&#24191;&#27867;&#32780;&#26377;&#25928;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2212.04997</link><description>&lt;p&gt;
&#31649;&#29702;AI&#21644;&#25968;&#25454;&#30340;&#23432;&#38376;&#21592;&#65306;&#22312;DMA&#12289;GDPR&#21450;&#26356;&#22810;&#27861;&#35268;&#19979;&#30340;&#36879;&#26126;&#24230;&#12289;&#35775;&#38382;&#21644;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Regulating Gatekeeper AI and Data: Transparency, Access, and Fairness under the DMA, the GDPR, and beyond. (arXiv:2212.04997v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04997
&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#24066;&#22330;&#27861;&#35268;&#23450;&#20102;&#23545;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#27431;&#30431;&#35268;&#21017;&#65292;&#20854;&#23545;&#25259;&#38706;&#35201;&#27714;&#12289;&#20154;&#24037;&#26234;&#33021;&#35757;&#32451;&#25968;&#25454;&#30340;&#31649;&#29702;&#12289;&#35775;&#38382;&#35268;&#21017;&#21644;&#20844;&#24179;&#25490;&#21517;&#21046;&#24230;&#20855;&#26377;&#24191;&#27867;&#32780;&#26377;&#25928;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19981;&#20165;&#22312;&#21830;&#19994;&#21644;&#34892;&#25919;&#39046;&#22495;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#23545;&#20854;&#35268;&#33539;&#30340;&#31454;&#36187;&#20063;&#27491;&#22312;&#36827;&#34892;&#20013;&#65292;&#27431;&#30431;&#22312;&#36825;&#26041;&#38754;&#22788;&#20110;&#21069;&#27839;&#12290;&#20294;&#19982;&#29616;&#26377;&#25991;&#29486;&#19981;&#21516;&#65292;&#26412;&#25991;&#35748;&#20026;&#65292;&#22312;&#25968;&#23383;&#24066;&#22330;&#27861;&#35758;&#26696;&#20013;&#25152;&#21253;&#21547;&#30340;&#23545;&#25968;&#23383;&#32463;&#27982;&#20013;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#27431;&#30431;&#35268;&#21017;&#19981;&#20165;&#20855;&#26377;&#26368;&#24191;&#27867;&#30340;&#24433;&#21709;&#21147;&#65292;&#32780;&#19988;&#26159;&#26368;&#26377;&#25928;&#30340;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;DMA&#21450;&#30456;&#20851;&#27431;&#30431;&#27861;&#26696;&#23545;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21450;&#20854;&#22522;&#30784;&#25968;&#25454;&#30340;&#22235;&#20010;&#20851;&#38190;&#39046;&#22495;&#30340;&#24433;&#21709;&#65306;&#25259;&#38706;&#35201;&#27714;&#65307;&#20154;&#24037;&#26234;&#33021;&#35757;&#32451;&#25968;&#25454;&#30340;&#31649;&#29702;&#65307;&#35775;&#38382;&#35268;&#21017;&#65307;&#20844;&#24179;&#25490;&#21517;&#21046;&#24230;&#12290;&#26412;&#25991;&#35777;&#26126;&#65292;DMA&#20013;&#30340;&#20844;&#24179;&#24615;&#36229;&#36234;&#20102;&#30446;&#21069;&#20154;&#24037;&#26234;&#33021;&#19982;&#27861;&#24459;&#20132;&#21449;&#23398;&#31185;&#20013;&#20027;&#35201;&#20851;&#27880;&#30340;&#20256;&#32479;&#38750;&#27495;&#35270;&#27861;&#24459;&#25152;&#20445;&#25252;&#30340;&#31867;&#21035;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#31454;&#20105;&#27861;&#21644;&#30693;&#35782;&#20135;&#26435;&#27861;&#20013;&#30340;FRAND&#20934;&#21017;&#26469;&#35299;&#37322;&#21644;&#35780;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence is not only increasingly used in business and administration contexts, but a race for its regulation is also underway, with the EU spearheading the efforts. Contrary to existing literature, this article suggests, however, that the most far-reaching and effective EU rules for AI applications in the digital economy will not be contained in the proposed AI Act - but have just been enacted in the Digital Markets Act. We analyze the impact of the DMA and related EU acts on AI models and their underlying data across four key areas: disclosure requirements; the regulation of AI training data; access rules; and the regime for fair rankings. The paper demonstrates that fairness, in the sense of the DMA, goes beyond traditionally protected categories of non-discrimination law on which scholarship at the intersection of AI and law has so far largely focused on. Rather, we draw on competition law and the FRAND criteria known from intellectual property law to interpret and r
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31070;&#32463;&#20613;&#37324;&#21494;&#28388;&#27874;&#22120;&#32452;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26082;&#22312;&#31354;&#38388;&#19978;&#21448;&#22312;&#39057;&#29575;&#19978;&#20998;&#35299;&#20449;&#21495;&#65292;&#36890;&#36807;&#20351;&#29992;&#20613;&#37324;&#21494;&#32534;&#30721;&#29305;&#23450;&#39057;&#29575;&#26469;&#23384;&#20648;&#27599;&#20010;&#32593;&#26684;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;2D&#22270;&#20687;&#25311;&#21512;&#12289;3D&#24418;&#29366;&#37325;&#24314;&#21644;&#31070;&#32463;&#36752;&#23556;&#22330;&#31561;&#22810;&#20010;&#20219;&#21153;&#19978;&#65292;&#34920;&#29616;&#20986;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#27169;&#22411;&#32039;&#20945;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2212.01735</link><description>&lt;p&gt;
&#31070;&#32463;&#20613;&#37324;&#21494;&#28388;&#27874;&#22120;&#32452;
&lt;/p&gt;
&lt;p&gt;
Neural Fourier Filter Bank. (arXiv:2212.01735v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01735
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31070;&#32463;&#20613;&#37324;&#21494;&#28388;&#27874;&#22120;&#32452;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26082;&#22312;&#31354;&#38388;&#19978;&#21448;&#22312;&#39057;&#29575;&#19978;&#20998;&#35299;&#20449;&#21495;&#65292;&#36890;&#36807;&#20351;&#29992;&#20613;&#37324;&#21494;&#32534;&#30721;&#29305;&#23450;&#39057;&#29575;&#26469;&#23384;&#20648;&#27599;&#20010;&#32593;&#26684;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;2D&#22270;&#20687;&#25311;&#21512;&#12289;3D&#24418;&#29366;&#37325;&#24314;&#21644;&#31070;&#32463;&#36752;&#23556;&#22330;&#31561;&#22810;&#20010;&#20219;&#21153;&#19978;&#65292;&#34920;&#29616;&#20986;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#27169;&#22411;&#32039;&#20945;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#20379;&#39640;&#25928;&#19988;&#39640;&#24230;&#35814;&#32454;&#30340;&#37325;&#26500;&#12290;&#21463;&#23567;&#27874;&#21551;&#21457;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#20010;&#31070;&#32463;&#22330;&#65292;&#26082;&#22312;&#31354;&#38388;&#19978;&#21448;&#22312;&#39057;&#29575;&#19978;&#20998;&#35299;&#20449;&#21495;&#12290;&#25105;&#20204;&#36981;&#24490;&#26368;&#36817;&#30340;&#22522;&#20110;&#32593;&#26684;&#30340;&#31354;&#38388;&#20998;&#35299;&#33539;&#20363;&#65292;&#20294;&#19982;&#29616;&#26377;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#36890;&#36807;&#20613;&#37324;&#21494;&#29305;&#24449;&#32534;&#30721;&#40723;&#21169;&#22312;&#27599;&#20010;&#32593;&#26684;&#20013;&#23384;&#20648;&#29305;&#23450;&#30340;&#39057;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#24102;&#27491;&#24358;&#28608;&#27963;&#20989;&#25968;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65292;&#20197;&#22312;&#36866;&#24403;&#30340;&#23618;&#27425;&#19978;&#25509;&#21463;&#36825;&#20123;&#20613;&#37324;&#21494;&#32534;&#30721;&#30340;&#29305;&#24449;&#65292;&#20197;&#20351;&#39640;&#39057;&#32452;&#20214;&#20381;&#27425;&#32047;&#31215;&#22312;&#20302;&#39057;&#32452;&#20214;&#20043;&#19978;&#65292;&#26368;&#21518;&#23558;&#23427;&#20204;&#30456;&#21152;&#20197;&#24418;&#25104;&#26368;&#32456;&#36755;&#20986;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#65288;2D&#22270;&#20687;&#25311;&#21512;&#65292;3D&#24418;&#29366;&#37325;&#24314;&#21644;&#31070;&#32463;&#36752;&#23556;&#22330;&#65289;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#21253;&#25324;&#27169;&#22411;&#32039;&#20945;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/ubc-vision/NFFB&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel method to provide efficient and highly detailed reconstructions. Inspired by wavelets, we learn a neural field that decompose the signal both spatially and frequency-wise. We follow the recent grid-based paradigm for spatial decomposition, but unlike existing work, encourage specific frequencies to be stored in each grid via Fourier features encodings. We then apply a multi-layer perceptron with sine activations, taking these Fourier encoded features in at appropriate layers so that higher-frequency components are accumulated on top of lower-frequency components sequentially, which we sum up to form the final output. We demonstrate that our method outperforms the state of the art regarding model compactness and convergence speed on multiple tasks: 2D image fitting, 3D shape reconstruction, and neural radiance fields. Our code is available at https://github.com/ubc-vision/NFFB.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#20840;&#22330;&#34394;&#25311;&#36127;&#36733;&#30417;&#27979;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#31163;&#23736;&#39118;&#30005;&#32467;&#26500;&#30417;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#23454;&#38469;&#38480;&#21046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.00642</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#31163;&#23736;&#39118;&#30005;&#32467;&#26500;&#20840;&#22330;&#34394;&#25311;&#36127;&#36733;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
Farm-wide virtual load monitoring for offshore wind structures via Bayesian neural networks. (arXiv:2211.00642v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00642
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#20840;&#22330;&#34394;&#25311;&#36127;&#36733;&#30417;&#27979;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#31163;&#23736;&#39118;&#30005;&#32467;&#26500;&#30417;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#23454;&#38469;&#38480;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#23736;&#39118;&#30005;&#32467;&#26500;&#22312;&#36816;&#34892;&#23551;&#21629;&#20869;&#20250;&#21463;&#21040;&#36864;&#21270;&#26426;&#21046;&#30340;&#24433;&#21709;&#12290;&#21363;&#20351;&#36890;&#36807;&#22522;&#20110;&#29289;&#29702;&#30340;&#36864;&#21270;&#27169;&#22411;&#21487;&#20197;&#20272;&#35745;&#32467;&#26500;&#20803;&#20214;&#30340;&#36864;&#21270;&#28436;&#21464;&#65292;&#20294;&#36807;&#31243;&#20013;&#28041;&#21450;&#30340;&#19981;&#30830;&#23450;&#24615;&#38459;&#30861;&#20102;&#29983;&#21629;&#21608;&#26399;&#31649;&#29702;&#20915;&#31574;&#30340;&#36873;&#25321;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#30417;&#27979;&#31995;&#32479;&#25910;&#38598;&#30456;&#20851;&#20449;&#24687;&#65292;&#21487;&#20197;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#65292;&#26368;&#32456;&#25512;&#21160;&#26356;&#20248;&#21270;&#30340;&#29983;&#21629;&#21608;&#26399;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#22312;&#25972;&#20010;&#39118;&#30005;&#22330;&#30340;&#25152;&#26377;&#39118;&#21147;&#28065;&#36718;&#19978;&#23454;&#26045;&#23436;&#25972;&#30340;&#30417;&#27979;&#20202;&#22120;&#21487;&#33021;&#30001;&#20110;&#23454;&#38469;&#21644;&#32463;&#27982;&#38480;&#21046;&#32780;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#27492;&#22806;&#65292;&#26576;&#20123;&#36127;&#36733;&#30417;&#27979;&#31995;&#32479;&#22312;&#28023;&#27915;&#29615;&#22659;&#26292;&#38706;&#20960;&#24180;&#21518;&#32463;&#24120;&#21457;&#29983;&#25925;&#38556;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#30001;&#19968;&#20010;&#39046;&#23548;&#22411;&#39118;&#21147;&#28065;&#36718;&#25351;&#23548;&#30340;&#20840;&#22330;&#34394;&#25311;&#36127;&#36733;&#30417;&#27979;&#26041;&#26696;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offshore wind structures are subject to deterioration mechanisms throughout their operational lifetime. Even if the deterioration evolution of structural elements can be estimated through physics-based deterioration models, the uncertainties involved in the process hurdle the selection of lifecycle management decisions. In this scenario, the collection of relevant information through an efficient monitoring system enables the reduction of uncertainties, ultimately driving more optimal lifecycle decisions. However, a full monitoring instrumentation implemented on all wind turbines in a farm might become unfeasible due to practical and economical constraints. Besides, certain load monitoring systems often become defective after a few years of marine environment exposure. Addressing the aforementioned concerns, a farm-wide virtual load monitoring scheme directed by a fleet-leader wind turbine offers an attractive solution. Fetched with data retrieved from a fully-instrumented wind turbine
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20381;&#36182;&#22686;&#24378;&#30340;&#33258;&#36866;&#24212;&#34701;&#21512;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#23427;&#23558;&#20381;&#36182;&#20449;&#24687;&#19982;&#21407;&#22987;&#35821;&#20041;&#20449;&#21495;&#33258;&#36866;&#24212;&#34701;&#21512;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#22797;&#26434;&#30340;&#35821;&#20041;&#21305;&#37197;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2210.08471</link><description>&lt;p&gt;
&#36890;&#36807;&#32467;&#26500;&#22686;&#24378;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#33258;&#36866;&#24212;&#34701;&#21512;&#25552;&#39640;&#35821;&#20041;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Improving Semantic Matching through Dependency-Enhanced Pre-trained Model with Adaptive Fusion. (arXiv:2210.08471v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20381;&#36182;&#22686;&#24378;&#30340;&#33258;&#36866;&#24212;&#34701;&#21512;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#23427;&#23558;&#20381;&#36182;&#20449;&#24687;&#19982;&#21407;&#22987;&#35821;&#20041;&#20449;&#21495;&#33258;&#36866;&#24212;&#34701;&#21512;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#22797;&#26434;&#30340;&#35821;&#20041;&#21305;&#37197;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22914;BERT&#65292;&#22312;&#35821;&#20041;&#21477;&#23376;&#21305;&#37197;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#12290;&#21516;&#26102;&#65292;&#20381;&#36182;&#24615;&#20808;&#39564;&#30693;&#35782;&#22312;&#22810;&#20010;NLP&#20219;&#21153;&#20013;&#20063;&#26174;&#31034;&#20986;&#26222;&#36941;&#30340;&#30410;&#22788;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#23558;&#20381;&#36182;&#24615;&#20808;&#39564;&#32467;&#26500;&#26377;&#25928;&#22320;&#38598;&#25104;&#21040;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#22797;&#26434;&#30340;&#35821;&#20041;&#21305;&#37197;&#20851;&#31995;&#65292;&#20173;&#26410;&#30830;&#23450;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DAFA&#30340;&#20381;&#36182;&#22686;&#24378;&#33258;&#36866;&#24212;&#34701;&#21512;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#36825;&#23558;&#20381;&#36182;&#32467;&#26500;&#26126;&#30830;&#22320;&#24341;&#20837;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#21040;&#35821;&#20041;&#20449;&#24687;&#20013;&#12290;&#20855;&#20307;&#22320;&#65292;DAFA&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#25935;&#24863;&#33539;&#24335;&#26469;&#26500;&#24314;&#19968;&#20010;&#20381;&#36182;&#30697;&#38453;&#65292;&#20197;&#26657;&#20934;&#27880;&#24847;&#21147;&#26435;&#37325;&#12290;&#23427;&#37319;&#29992;&#33258;&#36866;&#24212;&#34701;&#21512;&#27169;&#22359;&#26469;&#38598;&#25104;&#33719;&#21462;&#30340;&#20381;&#36182;&#20449;&#24687;&#21644;&#21407;&#22987;&#35821;&#20041;&#20449;&#21495;&#12290;&#27492;&#22806;&#65292;DAFA&#37325;&#26500;&#20102;&#27880;&#24847;&#21147;&#35745;&#31639;&#27969;&#31243;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based pre-trained models like BERT have achieved great progress on Semantic Sentence Matching. Meanwhile, dependency prior knowledge has also shown general benefits in multiple NLP tasks. However, how to efficiently integrate dependency prior structure into pre-trained models to better model complex semantic matching relations is still unsettled. In this paper, we propose the \textbf{D}ependency-Enhanced \textbf{A}daptive \textbf{F}usion \textbf{A}ttention (\textbf{DAFA}), which explicitly introduces dependency structure into pre-trained models and adaptively fuses it with semantic information. Specifically, \textbf{\emph{(i)}} DAFA first proposes a structure-sensitive paradigm to construct a dependency matrix for calibrating attention weights. It adopts an adaptive fusion module to integrate the obtained dependency information and the original semantic signals. Moreover, DAFA reconstructs the attention calculation flow and provides better interpretability. By applying it o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#25351;&#23548;&#30340;&#31574;&#30053;&#26469;&#25552;&#21319;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#22270;&#20687;&#30340;&#31283;&#23450;&#24615;&#21644;&#36136;&#37327;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2210.00939</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#25351;&#23548;&#25552;&#39640;&#25193;&#25955;&#27169;&#22411;&#30340;&#26679;&#26412;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Improving Sample Quality of Diffusion Models Using Self-Attention Guidance. (arXiv:2210.00939v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00939
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#25351;&#23548;&#30340;&#31574;&#30053;&#26469;&#25552;&#21319;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#22270;&#20687;&#30340;&#31283;&#23450;&#24615;&#21644;&#36136;&#37327;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#20197;&#20854;&#20986;&#33394;&#30340;&#29983;&#25104;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#21463;&#21040;&#20851;&#27880;&#12290;&#36825;&#31181;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#22240;&#20110;&#20351;&#29992;&#20998;&#31867;&#25110;&#25991;&#26412;&#26465;&#20214;&#30340;&#25193;&#25955;&#25351;&#23548;&#26041;&#27861;&#65292;&#22914;&#20998;&#31867;&#22120;&#21644;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#20840;&#38754;&#30340;&#35270;&#35282;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#25351;&#23548;&#26041;&#27861;&#12290;&#20174;&#36825;&#20010;&#24191;&#20041;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#26080;&#26465;&#20214;&#21644;&#26080;&#30417;&#30563;&#30340;&#31574;&#30053;&#26469;&#25552;&#39640;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;&#20316;&#20026;&#19968;&#31181;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#27169;&#31946;&#25351;&#23548;&#25913;&#21892;&#20102;&#20013;&#38388;&#26679;&#26412;&#30340;&#36866;&#29992;&#24615;&#65292;&#20351;&#24471;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#20197;&#36866;&#24230;&#30340;&#25351;&#23548;&#23610;&#24230;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#33258;&#27880;&#24847;&#21147;&#25351;&#23548;&#65288;SAG&#65289;&#21033;&#29992;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#20013;&#38388;&#33258;&#27880;&#24847;&#21147;&#26144;&#23556;&#26469;&#22686;&#24378;&#23427;&#20204;&#30340;&#31283;&#23450;&#24615;&#21644;&#25928;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SAG&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#20165;&#23545;&#25193;&#25955;&#27169;&#22411;&#20851;&#27880;&#30340;&#21306;&#22495;&#36827;&#34892;&#23545;&#25239;&#24615;&#27169;&#31946;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion models (DDMs) have attracted attention for their exceptional generation quality and diversity. This success is largely attributed to the use of class- or text-conditional diffusion guidance methods, such as classifier and classifier-free guidance. In this paper, we present a more comprehensive perspective that goes beyond the traditional guidance methods. From this generalized perspective, we introduce novel condition- and training-free strategies to enhance the quality of generated images. As a simple solution, blur guidance improves the suitability of intermediate samples for their fine-scale information and structures, enabling diffusion models to generate higher quality samples with a moderate guidance scale. Improving upon this, Self-Attention Guidance (SAG) uses the intermediate self-attention maps of diffusion models to enhance their stability and efficacy. Specifically, SAG adversarially blurs only the regions that diffusion models attend to at each iteratio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Scene-Rep Transformer&#26469;&#25552;&#21319;&#24378;&#21270;&#23398;&#20064;&#20915;&#31574;&#33021;&#21147;&#65292;&#36890;&#36807;&#25913;&#36827;&#22330;&#26223;&#34920;&#31034;&#32534;&#30721;&#21644;&#39034;&#24207;&#39044;&#27979;&#28508;&#22312;&#33976;&#39311;&#12290;&#37319;&#29992;&#22810;&#38454;&#27573;Transformer&#32534;&#30721;&#22120;&#24314;&#27169;&#20132;&#20114;&#24847;&#35782;&#21644;&#24847;&#22270;&#24847;&#35782;&#65292;&#24182;&#20351;&#29992;&#39034;&#24207;&#28508;&#22312;Transformer&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#21152;&#36895;&#35757;&#32451;&#21644;&#20943;&#23569;&#25506;&#32034;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2208.12263</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#22330;&#26223;&#34920;&#31034;&#23398;&#20064;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Augmenting Reinforcement Learning with Transformer-based Scene Representation Learning for Decision-making of Autonomous Driving. (arXiv:2208.12263v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Scene-Rep Transformer&#26469;&#25552;&#21319;&#24378;&#21270;&#23398;&#20064;&#20915;&#31574;&#33021;&#21147;&#65292;&#36890;&#36807;&#25913;&#36827;&#22330;&#26223;&#34920;&#31034;&#32534;&#30721;&#21644;&#39034;&#24207;&#39044;&#27979;&#28508;&#22312;&#33976;&#39311;&#12290;&#37319;&#29992;&#22810;&#38454;&#27573;Transformer&#32534;&#30721;&#22120;&#24314;&#27169;&#20132;&#20114;&#24847;&#35782;&#21644;&#24847;&#22270;&#24847;&#35782;&#65292;&#24182;&#20351;&#29992;&#39034;&#24207;&#28508;&#22312;Transformer&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#21152;&#36895;&#35757;&#32451;&#21644;&#20943;&#23569;&#25506;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#33258;&#21160;&#39550;&#39542;&#30340;&#20915;&#31574;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#30001;&#20110;&#20132;&#36890;&#21442;&#19982;&#32773;&#30340;&#38543;&#26426;&#24615;&#21644;&#36947;&#36335;&#32467;&#26500;&#30340;&#22797;&#26434;&#24615;&#12290;&#23613;&#31649;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#20915;&#31574;&#26041;&#26696;&#22312;&#22788;&#29702;&#22478;&#24066;&#39550;&#39542;&#22330;&#26223;&#26041;&#38754;&#24456;&#26377;&#21069;&#26223;&#65292;&#20294;&#23427;&#30340;&#37319;&#26679;&#25928;&#29575;&#20302;&#19988;&#36866;&#24212;&#24615;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Scene-Rep Transformer&#26469;&#25913;&#21892;RL&#20915;&#31574;&#33021;&#21147;&#65292;&#36890;&#36807;&#26356;&#22909;&#30340;&#22330;&#26223;&#34920;&#31034;&#32534;&#30721;&#21644;&#39034;&#24207;&#39044;&#27979;&#28508;&#22312;&#33976;&#39311;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#38454;&#27573;Transformer&#65288;MST&#65289;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#24314;&#27169;&#33258;&#36710;&#19982;&#20854;&#37051;&#23621;&#20043;&#38388;&#30340;&#20132;&#20114;&#24847;&#35782;&#20197;&#21450;&#20195;&#29702;&#32773;&#19982;&#20505;&#36873;&#36335;&#24452;&#20043;&#38388;&#30340;&#24847;&#22270;&#24847;&#35782;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#20855;&#26377;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#30340;&#39034;&#24207;&#28508;&#22312;Transformer&#65288;SLT&#65289;&#65292;&#23558;&#26410;&#26469;&#30340;&#39044;&#27979;&#20449;&#24687;&#33976;&#39311;&#21040;&#28508;&#22312;&#30340;&#22330;&#26223;&#34920;&#31034;&#20013;&#65292;&#20197;&#20943;&#23569;&#25506;&#32034;&#31354;&#38388;&#24182;&#21152;&#24555;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-making for urban autonomous driving is challenging due to the stochastic nature of interactive traffic participants and the complexity of road structures. Although reinforcement learning (RL)-based decision-making scheme is promising to handle urban driving scenarios, it suffers from low sample efficiency and poor adaptability. In this paper, we propose Scene-Rep Transformer to improve the RL decision-making capabilities with better scene representation encoding and sequential predictive latent distillation. Specifically, a multi-stage Transformer (MST) encoder is constructed to model not only the interaction awareness between the ego vehicle and its neighbors but also intention awareness between the agents and their candidate routes. A sequential latent Transformer (SLT) with self-supervised learning objectives is employed to distill the future predictive information into the latent scene representation, in order to reduce the exploration space and speed up training. The fina
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#24471;&#21040;&#30340;&#34920;&#31034;&#36866;&#24212;&#21040;&#27979;&#35797;&#26102;&#30340;&#20998;&#24067;&#36716;&#31227;&#12290;&#36890;&#36807;&#21033;&#29992;&#36328;&#27169;&#24577;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#20266;&#26631;&#31614;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25991;&#26723;&#29702;&#35299;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#22312;&#23454;&#20307;&#35782;&#21035;&#12289;&#38190;&#20540;&#25552;&#21462;&#21644;&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#19978;&#20998;&#21035;&#25552;&#39640;&#20102;1.89%&#12289;3.43%&#21644;17.68%&#12290;</title><link>http://arxiv.org/abs/2206.07240</link><description>&lt;p&gt;
&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#35270;&#35273;&#25991;&#26723;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Test-Time Adaptation for Visual Document Understanding. (arXiv:2206.07240v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07240
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#24471;&#21040;&#30340;&#34920;&#31034;&#36866;&#24212;&#21040;&#27979;&#35797;&#26102;&#30340;&#20998;&#24067;&#36716;&#31227;&#12290;&#36890;&#36807;&#21033;&#29992;&#36328;&#27169;&#24577;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#20266;&#26631;&#31614;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25991;&#26723;&#29702;&#35299;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#22312;&#23454;&#20307;&#35782;&#21035;&#12289;&#38190;&#20540;&#25552;&#21462;&#21644;&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#19978;&#20998;&#21035;&#25552;&#39640;&#20102;1.89%&#12289;3.43%&#21644;17.68%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35270;&#35273;&#25991;&#26723;&#29702;&#35299;&#65288;VDU&#65289;&#65292;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#25104;&#21151;&#29983;&#25104;&#20855;&#26377;&#21487;&#20256;&#36882;&#24615;&#30340;&#34920;&#31034;&#65292;&#28982;&#32780;&#65292;&#22312;&#27979;&#35797;&#26102;&#38388;&#23545;&#36825;&#31181;&#34920;&#31034;&#36827;&#34892;&#26377;&#25928;&#30340;&#36866;&#24212;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DocTTA&#65292;&#19968;&#31181;&#29992;&#20110;&#25991;&#26723;&#30340;&#26032;&#22411;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#26080;&#26631;&#31614;&#30340;&#30446;&#26631;&#25991;&#26723;&#25968;&#25454;&#36827;&#34892;&#26080;&#28304;&#22495;&#36866;&#24212;&#12290;DocTTA&#21033;&#29992;&#36328;&#27169;&#24577;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#20266;&#26631;&#31614;&#26041;&#27861;&#65292;&#23558;&#22312;&#8220;&#28304;&#8221;&#22495;&#19978;&#23398;&#21040;&#30340;&#27169;&#22411;&#36866;&#24212;&#21040;&#26410;&#26631;&#35760;&#30340;&#8220;&#30446;&#26631;&#8221;&#22495;&#12290;&#25105;&#20204;&#20351;&#29992;&#29616;&#26377;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#20026;&#21508;&#31181;VDU&#20219;&#21153;&#24341;&#20837;&#20102;&#26032;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;&#23454;&#20307;&#35782;&#21035;&#12289;&#38190;&#20540;&#25552;&#21462;&#21644;&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#12290;&#30456;&#27604;&#20110;&#28304;&#27169;&#22411;&#24615;&#33021;&#65292;DocTTA&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20998;&#21035;&#25552;&#39640;&#20102;1.89%&#65288;F1&#20998;&#25968;&#65289;&#12289;3.43%&#65288;F1&#20998;&#25968;&#65289;&#21644;17.68%&#65288;ANLS&#20998;&#25968;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
For visual document understanding (VDU), self-supervised pretraining has been shown to successfully generate transferable representations, yet, effective adaptation of such representations to distribution shifts at test-time remains to be an unexplored area. We propose DocTTA, a novel test-time adaptation method for documents, that does source-free domain adaptation using unlabeled target document data. DocTTA leverages cross-modality self-supervised learning via masked visual language modeling, as well as pseudo labeling to adapt models learned on a \textit{source} domain to an unlabeled \textit{target} domain at test time. We introduce new benchmarks using existing public datasets for various VDU tasks, including entity recognition, key-value extraction, and document visual question answering. DocTTA shows significant improvements on these compared to the source model performance, up to 1.89\% in (F1 score), 3.43\% (F1 score), and 17.68\% (ANLS score), respectively. Our benchmark dat
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#20840;&#23616;&#20108;&#36827;&#21046;&#25513;&#30721;&#26469;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20013;&#32467;&#26500;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22120;&#23448;&#30340;&#35299;&#21078;&#24418;&#29366;&#21644;&#20301;&#32622;&#20449;&#24687;&#30340;&#19968;&#33268;&#24615;&#26469;&#36827;&#34892;&#20998;&#21106;&#12290;&#30740;&#31350;&#20102;&#20004;&#31181;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;&#65292;&#24182;&#22312;&#33041;&#37096;&#21644;&#24515;&#33039;CT&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2205.09107</link><description>&lt;p&gt;
&#22312;&#21307;&#23398;&#22270;&#20687;&#20013;&#21033;&#29992;&#20840;&#23616;&#20108;&#36827;&#21046;&#25513;&#30721;&#36827;&#34892;&#32467;&#26500;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Leveraging Global Binary Masks for Structure Segmentation in Medical Images. (arXiv:2205.09107v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09107
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#20840;&#23616;&#20108;&#36827;&#21046;&#25513;&#30721;&#26469;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20013;&#32467;&#26500;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22120;&#23448;&#30340;&#35299;&#21078;&#24418;&#29366;&#21644;&#20301;&#32622;&#20449;&#24687;&#30340;&#19968;&#33268;&#24615;&#26469;&#36827;&#34892;&#20998;&#21106;&#12290;&#30740;&#31350;&#20102;&#20004;&#31181;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;&#65292;&#24182;&#22312;&#33041;&#37096;&#21644;&#24515;&#33039;CT&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21463;&#21040;&#36755;&#20837;&#22270;&#20687;&#24378;&#24230;&#21464;&#21270;&#30340;&#24433;&#21709;&#24456;&#22823;&#65292;&#24182;&#19988;&#30001;&#20110;&#20027;&#35201;&#21033;&#29992;&#20687;&#32032;&#24378;&#24230;&#20449;&#24687;&#36827;&#34892;&#25512;&#29702;&#65292;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#12290;&#33719;&#21462;&#36275;&#22815;&#30340;&#35757;&#32451;&#25968;&#25454;&#26159;&#38480;&#21046;&#27169;&#22411;&#24212;&#29992;&#30340;&#21478;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#21307;&#23398;&#22270;&#20687;&#20013;&#22120;&#23448;&#30340;&#35299;&#21078;&#24418;&#29366;&#21644;&#20301;&#32622;&#20449;&#24687;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#20840;&#23616;&#20108;&#36827;&#21046;&#25513;&#30721;&#26469;&#21033;&#29992;&#37325;&#22797;&#20986;&#29616;&#30340;&#35299;&#21078;&#27169;&#24335;&#36827;&#34892;&#22120;&#23448;&#20998;&#21106;&#12290;&#30740;&#31350;&#20102;&#20004;&#31181;&#24773;&#20917;&#65306;1&#65289;&#20840;&#23616;&#20108;&#36827;&#21046;&#25513;&#30721;&#26159;&#27169;&#22411;(U-Net)&#30340;&#21807;&#19968;&#36755;&#20837;&#65292;&#24378;&#21046;&#36827;&#34892;&#22120;&#23448;&#30340;&#23450;&#20301;&#21644;&#24418;&#29366;&#20449;&#24687;&#32534;&#30721;&#36827;&#34892;&#20998;&#21106;&#12290;2&#65289;&#23558;&#20840;&#23616;&#20108;&#36827;&#21046;&#25513;&#30721;&#20316;&#20026;&#38468;&#21152;&#36890;&#36947;&#65292;&#24182;&#29992;&#20316;&#20301;&#32622;&#21644;&#24418;&#29366;&#32447;&#32034;&#20197;&#20943;&#23569;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#12290;&#23558;&#20004;&#20010;&#21253;&#21547;&#33041;&#37096;&#21644;&#24515;&#33039;CT&#22270;&#20687;&#21450;&#20854;&#22320;&#38754;&#23454;&#20917;&#30340;&#25968;&#25454;&#38598;&#20998;&#20026;(26:10:10)&#21644;(12:3:5)&#36827;&#34892;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) models for medical image segmentation are highly influenced by intensity variations of input images and lack generalization due to primarily utilizing pixels' intensity information for inference. Acquiring sufficient training data is another challenge limiting models' applications. We proposed to leverage the consistency of organs' anatomical shape and position information in medical images. We introduced a framework leveraging recurring anatomical patterns through global binary masks for organ segmentation. Two scenarios were studied.1) Global binary masks were the only model's (i.e. U-Net) input, forcing exclusively encoding organs' position and shape information for segmentation/localization.2) Global binary masks were incorporated as an additional channel functioning as position/shape clues to mitigate training data scarcity. Two datasets of the brain and heart CT images with their ground-truth were split into (26:10:10) and (12:3:5) for training, validation, and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MonoDETR&#30340;&#28145;&#24230;&#24341;&#23548;Transformer&#26694;&#26550;&#65292;&#29992;&#20110;&#21333;&#30446;3D&#30446;&#26631;&#26816;&#27979;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#26041;&#27861;&#65292;MonoDETR&#36890;&#36807;&#24341;&#20837;&#28145;&#24230;&#20449;&#24687;&#26469;&#25351;&#23548;&#25972;&#20010;&#26816;&#27979;&#36807;&#31243;&#65292;&#25552;&#39640;&#20102;&#23545;&#22330;&#26223;&#30340;&#29702;&#35299;&#21644;&#30446;&#26631;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2203.13310</link><description>&lt;p&gt;
MonoDETR&#65306;&#28145;&#24230;&#24341;&#23548;&#30340;&#21333;&#30446;3D&#30446;&#26631;&#26816;&#27979;&#30340;Transformer
&lt;/p&gt;
&lt;p&gt;
MonoDETR: Depth-guided Transformer for Monocular 3D Object Detection. (arXiv:2203.13310v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.13310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MonoDETR&#30340;&#28145;&#24230;&#24341;&#23548;Transformer&#26694;&#26550;&#65292;&#29992;&#20110;&#21333;&#30446;3D&#30446;&#26631;&#26816;&#27979;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#26041;&#27861;&#65292;MonoDETR&#36890;&#36807;&#24341;&#20837;&#28145;&#24230;&#20449;&#24687;&#26469;&#25351;&#23548;&#25972;&#20010;&#26816;&#27979;&#36807;&#31243;&#65292;&#25552;&#39640;&#20102;&#23545;&#22330;&#26223;&#30340;&#29702;&#35299;&#21644;&#30446;&#26631;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#30446;&#19977;&#32500;&#30446;&#26631;&#26816;&#27979;&#19968;&#30452;&#26159;&#33258;&#21160;&#39550;&#39542;&#20013;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#26159;&#26681;&#25454;&#20256;&#32479;&#30340;&#20108;&#32500;&#26816;&#27979;&#22120;&#39318;&#20808;&#23450;&#20301;&#30446;&#26631;&#20013;&#24515;&#65292;&#28982;&#21518;&#36890;&#36807;&#37051;&#36817;&#29305;&#24449;&#39044;&#27979;&#19977;&#32500;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#20165;&#20165;&#20351;&#29992;&#23616;&#37096;&#35270;&#35273;&#29305;&#24449;&#26159;&#19981;&#36275;&#20197;&#29702;&#35299;&#22330;&#26223;&#32423;&#21035;&#30340;&#19977;&#32500;&#31354;&#38388;&#32467;&#26500;&#24182;&#24573;&#30053;&#20102;&#36828;&#36317;&#31163;&#30340;&#30446;&#26631;&#28145;&#24230;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#37319;&#29992;&#28145;&#24230;&#24341;&#23548;Transformer&#30340;&#21333;&#30446;&#26816;&#27979;&#26694;&#26550;&#65292;&#31216;&#20026;MonoDETR&#12290;&#25105;&#20204;&#23558;&#22522;&#26412;&#30340;Transformer&#36827;&#34892;&#20102;&#20462;&#25913;&#65292;&#20351;&#20854;&#20855;&#26377;&#28145;&#24230;&#24863;&#30693;&#65292;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#28145;&#24230;&#32447;&#32034;&#26469;&#25351;&#23548;&#25972;&#20010;&#26816;&#27979;&#36807;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#25429;&#25417;&#29289;&#20307;&#22806;&#35266;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#30340;&#21516;&#26102;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39044;&#27979;&#21069;&#26223;&#28145;&#24230;&#22270;&#65292;&#24182;&#19987;&#38376;&#35774;&#35745;&#20102;&#19968;&#20010;&#28145;&#24230;&#32534;&#30721;&#22120;&#26469;&#25552;&#21462;&#38750;&#23616;&#37096;&#28145;&#24230;&#23884;&#20837;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#19977;&#32500;&#30446;&#26631;&#20505;&#36873;&#29289;&#24418;&#24335;&#21270;&#20026;&#21487;&#23398;&#20064;&#30340;&#26597;&#35810;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#24341;&#23548;&#30340;&#35299;&#30721;&#22120;&#26469;&#36827;&#34892;&#30446;&#26631;-&#22330;&#26223;&#28145;&#24230;&#20132;&#20114;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#27599;&#20010;&#30446;&#26631;&#37117;&#21487;&#20197;&#24471;&#21040;&#26356;&#20840;&#38754;&#30340;&#28145;&#24230;&#24863;&#30693;&#21644;&#26356;&#20934;&#30830;&#30340;&#19977;&#32500;&#26816;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monocular 3D object detection has long been a challenging task in autonomous driving. Most existing methods follow conventional 2D detectors to first localize object centers, and then predict 3D attributes by neighboring features. However, only using local visual features is insufficient to understand the scene-level 3D spatial structures and ignores the long-range inter-object depth relations. In this paper, we introduce the first DETR framework for Monocular DEtection with a depth-guided TRansformer, named MonoDETR. We modify the vanilla transformer to be depth-aware and guide the whole detection process by contextual depth cues. Specifically, concurrent to the visual encoder that captures object appearances, we introduce to predict a foreground depth map, and specialize a depth encoder to extract non-local depth embeddings. Then, we formulate 3D object candidates as learnable queries and propose a depth-guided decoder to conduct object-scene depth interactions. In this way, each obj
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#31350;&#20102;&#30456;&#20851;&#29305;&#24449;&#23545;&#29992;&#20110;&#39044;&#27979;&#30707;&#27833;&#20844;&#21496;&#32929;&#31080;&#30340;LSTM&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28155;&#21152;&#19982;&#30707;&#27833;&#32929;&#31080;&#30456;&#20851;&#30340;&#29305;&#24449;&#24182;&#19981;&#20250;&#25552;&#39640;LSTM&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#27492;&#24212;&#35880;&#24910;&#20381;&#38752;LSTM&#27169;&#22411;&#36827;&#34892;&#32929;&#31080;&#24066;&#22330;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2201.00350</link><description>&lt;p&gt;
LSTM&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#30707;&#27833;&#20844;&#21496;&#32929;&#31080;&#30340;&#21487;&#35299;&#37322;&#24615;&#65306;&#30456;&#20851;&#29305;&#24449;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Interpretability of LSTM Models for Predicting Oil Company Stocks: impacts of correlated features. (arXiv:2201.00350v3 [q-fin.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.00350
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#31350;&#20102;&#30456;&#20851;&#29305;&#24449;&#23545;&#29992;&#20110;&#39044;&#27979;&#30707;&#27833;&#20844;&#21496;&#32929;&#31080;&#30340;LSTM&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28155;&#21152;&#19982;&#30707;&#27833;&#32929;&#31080;&#30456;&#20851;&#30340;&#29305;&#24449;&#24182;&#19981;&#20250;&#25552;&#39640;LSTM&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#27492;&#24212;&#35880;&#24910;&#20381;&#38752;LSTM&#27169;&#22411;&#36827;&#34892;&#32929;&#31080;&#24066;&#22330;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30707;&#27833;&#20844;&#21496;&#26159;&#20840;&#29699;&#26368;&#22823;&#30340;&#20844;&#21496;&#20043;&#19968;&#65292;&#30001;&#20110;&#19982;&#40644;&#37329;&#12289;&#21407;&#27833;&#21644;&#32654;&#20803;&#30456;&#20851;&#65292;&#20854;&#32463;&#27982;&#25351;&#26631;&#23545;&#20840;&#29699;&#32463;&#27982;&#21644;&#24066;&#22330;&#26377;&#30528;&#24040;&#22823;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#30456;&#20851;&#29305;&#24449;&#23545;&#29992;&#20110;&#39044;&#27979;&#30707;&#27833;&#20844;&#21496;&#32929;&#31080;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;(LSTM)&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26631;&#20934;&#30340;LSTM&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#30456;&#20851;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#32771;&#34385;&#24433;&#21709;&#24066;&#22330;&#30340;&#22810;&#20010;&#22240;&#32032;&#65292;&#22914;&#21407;&#27833;&#20215;&#26684;&#12289;&#40644;&#37329;&#20215;&#26684;&#21644;&#32654;&#20803;&#65292;&#26469;&#25552;&#39640;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28155;&#21152;&#19982;&#30707;&#27833;&#32929;&#31080;&#30456;&#20851;&#30340;&#29305;&#24449;&#24182;&#19981;&#20250;&#25552;&#39640;LSTM&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;&#34429;&#28982;LSTM&#27169;&#22411;&#22312;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#26041;&#38754;&#21487;&#33021;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#20854;&#21487;&#35299;&#37322;&#24615;&#21487;&#33021;&#26377;&#38480;&#12290;&#22312;&#20165;&#20381;&#38752;LSTM&#27169;&#22411;&#36827;&#34892;&#32929;&#31080;&#24066;&#22330;&#20915;&#31574;&#26102;&#24212;&#26684;&#22806;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;
Oil companies are among the largest companies in the world whose economic indicators in the global stock market have a great impact on the world economy and market due to their relation to gold, crude oil, and the dollar. This study investigates the impact of correlated features on the interpretability of Long Short-Term Memory (LSTM) models for predicting oil company stocks. To achieve this, we designed a Standard Long Short-Term Memory (LSTM) network and trained it using various correlated datasets. Our approach aims to improve the accuracy of stock price prediction by considering the multiple factors affecting the market, such as crude oil prices, gold prices, and the US dollar. The results demonstrate that adding a feature correlated with oil stocks does not improve the interpretability of LSTM models. These findings suggest that while LSTM models may be effective in predicting stock prices, their interpretability may be limited. Caution should be exercised when relying solely on L
&lt;/p&gt;</description></item></channel></rss>