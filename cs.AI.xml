<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>FormalGeo&#26159;&#19968;&#31181;&#23436;&#25972;&#19988;&#20860;&#23481;&#30340;&#27491;&#24335;&#24179;&#38754;&#20960;&#20309;&#31995;&#32479;&#65292;&#33021;&#22815;&#21033;&#29992;&#29616;&#20195;AI&#27169;&#22411;&#25552;&#20379;&#28436;&#32462;&#25512;&#29702;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;AI&#33021;&#22815;&#20687;&#22788;&#29702;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#19968;&#26679;&#35299;&#20915;IMO&#32423;&#24179;&#38754;&#20960;&#20309;&#38382;&#39064;&#65292;&#35777;&#26126;&#21487;&#35835;&#12289;&#36861;&#28335;&#21644;&#21487;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.18021</link><description>&lt;p&gt;
FormalGeo&#65306;&#36808;&#21521;&#20154;&#31867;&#32423;IMO&#27700;&#24179;&#20960;&#20309;&#33258;&#21160;&#25512;&#29702;&#30340;&#31532;&#19968;&#27493;
&lt;/p&gt;
&lt;p&gt;
FormalGeo: The First Step Toward Human-like IMO-level Geometric Automated Reasoning. (arXiv:2310.18021v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18021
&lt;/p&gt;
&lt;p&gt;
FormalGeo&#26159;&#19968;&#31181;&#23436;&#25972;&#19988;&#20860;&#23481;&#30340;&#27491;&#24335;&#24179;&#38754;&#20960;&#20309;&#31995;&#32479;&#65292;&#33021;&#22815;&#21033;&#29992;&#29616;&#20195;AI&#27169;&#22411;&#25552;&#20379;&#28436;&#32462;&#25512;&#29702;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;AI&#33021;&#22815;&#20687;&#22788;&#29702;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#19968;&#26679;&#35299;&#20915;IMO&#32423;&#24179;&#38754;&#20960;&#20309;&#38382;&#39064;&#65292;&#35777;&#26126;&#21487;&#35835;&#12289;&#36861;&#28335;&#21644;&#21487;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#25105;&#20204;&#36807;&#21435;&#21313;&#24180;&#24037;&#20316;&#30340;&#31532;&#19968;&#31687;&#25991;&#31456;&#12290;&#22312;&#36825;&#19968;&#31995;&#21015;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#23436;&#25972;&#19988;&#20860;&#23481;&#30340;&#27491;&#24335;&#24179;&#38754;&#20960;&#20309;&#31995;&#32479;&#12290;&#36825;&#23558;&#20316;&#20026;IMO&#32423;&#24179;&#38754;&#20960;&#20309;&#25361;&#25112;&#19982;&#21487;&#35835;&#30340;AI&#33258;&#21160;&#25512;&#29702;&#20043;&#38388;&#30340;&#20851;&#38190;&#26725;&#26753;&#12290;&#26377;&#20102;&#36825;&#20010;&#27491;&#24335;&#31995;&#32479;&#65292;&#25105;&#20204;&#33021;&#22815;&#26080;&#32541;&#22320;&#23558;&#29616;&#20195;AI&#27169;&#22411;&#19982;&#25105;&#20204;&#30340;&#27491;&#24335;&#31995;&#32479;&#38598;&#25104;&#22312;&#19968;&#36215;&#12290;&#22312;&#36825;&#20010;&#27491;&#24335;&#26694;&#26550;&#20869;&#65292;AI&#29616;&#22312;&#33021;&#22815;&#20687;&#22788;&#29702;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#19968;&#26679;&#23545;IMO&#32423;&#24179;&#38754;&#20960;&#20309;&#38382;&#39064;&#25552;&#20379;&#28436;&#32462;&#25512;&#29702;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#36825;&#20123;&#35777;&#26126;&#26159;&#21487;&#35835;&#30340;&#12289;&#21487;&#36861;&#28335;&#30340;&#21644;&#21487;&#39564;&#35777;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20309;&#24418;&#24335;&#21270;&#29702;&#35770;&#65288;GFT&#65289;&#26469;&#25351;&#23548;&#20960;&#20309;&#24418;&#24335;&#31995;&#32479;&#30340;&#21457;&#23637;&#12290;&#22522;&#20110;GFT&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;FormalGeo&#65292;&#21253;&#25324;88&#20010;&#20960;&#20309;&#35859;&#35789;&#21644;196&#20010;&#23450;&#29702;&#12290;&#23427;&#33021;&#22815;&#34920;&#31034;&#12289;&#39564;&#35777;&#21644;&#35299;&#20915;IMO&#32423;&#20960;&#20309;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;Python&#24320;&#21457;&#20102;FGPS&#65288;&#27491;&#24335;&#20960;&#20309;&#38382;&#39064;&#27714;&#35299;&#22120;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This is the first article of our work over the past decade. In this series of papers, we have constructed a complete and compatible formal plane geometry system. This will serve as a crucial bridge between IMO-level plane geometry challenges and readable AI automated reasoning. With this formal system in place, we have been able to seamlessly integrate modern AI models with our formal system. Within this formal framework, AI is now capable of providing deductive reasoning solutions to IMO-level plane geometry problems, just like handling other natural languages, and these proofs are readable, traceable, and verifiable. We propose the geometry formalization theory (GFT) to guide the development of the geometry formal system. Based on the GFT, we have established the FormalGeo, which consists of 88 geometric predicates and 196 theorems. It can represent, validate, and solve IMO-level geometry problems. we also have crafted the FGPS (formal geometry problem solver) in Python. It serves as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#32771;&#34385;&#20102;&#24403;&#21069;&#36890;&#36947;&#29420;&#31435;&#31574;&#30053;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#26159;&#21542;&#26159;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CSC&#30340;&#36890;&#36947;&#33258;&#32858;&#31867;&#31574;&#30053;&#26469;&#22686;&#24378;&#24615;&#33021;&#24182;&#20943;&#23567;&#21442;&#25968;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2310.17658</link><description>&lt;p&gt;
&#36890;&#36947;&#29420;&#31435;&#31574;&#30053;&#26159;&#21542;&#26159;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26368;&#20339;&#35299;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Channel Independent strategy optimal for Time Series Forecasting?. (arXiv:2310.17658v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#32771;&#34385;&#20102;&#24403;&#21069;&#36890;&#36947;&#29420;&#31435;&#31574;&#30053;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#26159;&#21542;&#26159;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CSC&#30340;&#36890;&#36947;&#33258;&#32858;&#31867;&#31574;&#30053;&#26469;&#22686;&#24378;&#24615;&#33021;&#24182;&#20943;&#23567;&#21442;&#25968;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#20986;&#29616;&#20102;&#35768;&#22810;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#21333;&#19968;&#32447;&#24615;&#23618;&#30340;&#36890;&#36947;&#30456;&#20851;(CD)&#25110;&#36890;&#36947;&#29420;&#31435;(CI)&#24314;&#27169;&#65292;&#29978;&#33267;&#21487;&#20197;&#36229;&#36807;&#35768;&#22810;&#22797;&#26434;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#23558;CD&#21644;CI&#35270;&#20026;&#20004;&#31181;&#20114;&#34917;&#20294;&#20114;&#26021;&#30340;&#26041;&#27861;&#65292;&#26080;&#27861;&#21516;&#26102;&#21033;&#29992;&#36825;&#20004;&#20010;&#26497;&#31471;&#12290;&#32780;&#19988;&#65292;CD&#21644;CI&#37117;&#26159;&#38745;&#24577;&#31574;&#30053;&#65292;&#26080;&#27861;&#22312;&#27809;&#26377;&#22823;&#37327;&#23454;&#39564;&#30340;&#24773;&#20917;&#19979;&#30830;&#23450;&#26159;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;&#26368;&#20339;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20102;&#24403;&#21069;CI&#31574;&#30053;&#26159;&#21542;&#26159;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#31216;&#20026;CSC&#65288;&#36890;&#36947;&#33258;&#32858;&#31867;&#31574;&#30053;&#65289;&#65292;&#29992;&#20110;&#32447;&#24615;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#36890;&#36947;&#33258;&#32858;&#31867;&#31574;&#30053;&#22686;&#24378;&#20102;CI&#31574;&#30053;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#20943;&#23567;&#20102;&#21442;&#25968;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been an emergence of various models for long-term time series forecasting. Recent studies have demonstrated that a single linear layer, using Channel Dependent (CD) or Channel Independent (CI) modeling, can even outperform a large number of sophisticated models. However, current research primarily considers CD and CI as two complementary yet mutually exclusive approaches, unable to harness these two extremes simultaneously. And it is also a challenging issue that both CD and CI are static strategies that cannot be determined to be optimal for a specific dataset without extensive experiments. In this paper, we reconsider whether the current CI strategy is the best solution for time series forecasting. First, we propose a simple yet effective strategy called CSC, which stands for $\mathbf{C}$hannel $\mathbf{S}$elf-$\mathbf{C}$lustering strategy, for linear models. Our Channel Self-Clustering (CSC) enhances CI strategy's performance improvements while reducing parameter size, fo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;GraphGPT&#26694;&#26550;&#65292;&#23427;&#26159;&#19968;&#31181;&#38754;&#21521;&#22270;&#32467;&#26500;&#30693;&#35782;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#25351;&#20196;&#35843;&#20248;&#23454;&#29616;&#39640;&#24230;&#27867;&#21270;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#19979;&#28216;&#22270;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#21462;&#24471;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13023</link><description>&lt;p&gt;
GraphGPT: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
GraphGPT: Graph Instruction Tuning for Large Language Models. (arXiv:2310.13023v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;GraphGPT&#26694;&#26550;&#65292;&#23427;&#26159;&#19968;&#31181;&#38754;&#21521;&#22270;&#32467;&#26500;&#30693;&#35782;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#25351;&#20196;&#35843;&#20248;&#23454;&#29616;&#39640;&#24230;&#27867;&#21270;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#19979;&#28216;&#22270;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#21462;&#24471;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22270;&#33410;&#28857;&#20043;&#38388;&#30340;&#36882;&#24402;&#20449;&#24687;&#20132;&#25442;&#21644;&#32858;&#21512;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#29702;&#35299;&#22270;&#32467;&#26500;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#20026;&#20102;&#25552;&#39640;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#29983;&#25104;&#39044;&#35757;&#32451;&#22270;&#23884;&#20837;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#26631;&#31614;&#36827;&#34892;&#24494;&#35843;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#25110;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#30340;&#21487;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#25552;&#21319;&#22270;&#27169;&#22411;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#31181;&#38754;&#21521;&#22270;&#32467;&#26500;&#30693;&#35782;&#30340;LLM&#65292;&#21363;&#20351;&#27809;&#26377;&#26469;&#33258;&#19979;&#28216;&#22270;&#25968;&#25454;&#30340;&#20219;&#20309;&#20449;&#24687;&#65292;&#20063;&#33021;&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#23454;&#29616;&#39640;&#24230;&#27867;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GraphGPT&#26694;&#26550;&#65292;&#36890;&#36807;&#22270;&#25351;&#20196;&#35843;&#20248;&#23558;LLM&#19982;&#22270;&#32467;&#26500;&#30693;&#35782;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have advanced graph structure understanding via recursive information exchange and aggregation among graph nodes. To improve model robustness, self-supervised learning (SSL) has emerged as a promising approach for data augmentation. However, existing methods for generating pre-trained graph embeddings often rely on fine-tuning with specific downstream task labels, which limits their usability in scenarios where labeled data is scarce or unavailable. To address this, our research focuses on advancing the generalization capabilities of graph models in challenging zero-shot learning scenarios. Inspired by the success of large language models (LLMs), we aim to develop a graph-oriented LLM that can achieve high generalization across diverse downstream datasets and tasks, even without any information available from the downstream graph data. In this work, we present the GraphGPT framework that aligns LLMs with graph structural knowledge with a graph instruction t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31895;&#31890;&#24230;&#24341;&#23548;&#26862;&#26519;&#21644;&#22810;&#20013;&#24515;&#25439;&#22833;&#30340;&#38271;&#23614;&#20998;&#31867;&#26694;&#26550;&#65292;&#21517;&#20026;Cognisance&#12290;&#35813;&#26694;&#26550;&#33268;&#21147;&#20110;&#35299;&#20915;&#38271;&#23614;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#31867;&#38388;&#21644;&#31867;&#20869;&#19981;&#24179;&#34913;&#65292;&#24182;&#36890;&#36807;&#19981;&#21464;&#29305;&#24449;&#23398;&#20064;&#26500;&#24314;&#22810;&#31890;&#24230;&#32852;&#21512;&#35299;&#20915;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.08206</link><description>&lt;p&gt;
&#22522;&#20110;&#31895;&#31890;&#24230;&#24341;&#23548;&#26862;&#26519;&#21644;&#22810;&#20013;&#24515;&#25439;&#22833;&#30340;&#38271;&#23614;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Long-Tailed Classification Based on Coarse-Grained Leading Forest and Multi-Center Loss. (arXiv:2310.08206v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31895;&#31890;&#24230;&#24341;&#23548;&#26862;&#26519;&#21644;&#22810;&#20013;&#24515;&#25439;&#22833;&#30340;&#38271;&#23614;&#20998;&#31867;&#26694;&#26550;&#65292;&#21517;&#20026;Cognisance&#12290;&#35813;&#26694;&#26550;&#33268;&#21147;&#20110;&#35299;&#20915;&#38271;&#23614;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#31867;&#38388;&#21644;&#31867;&#20869;&#19981;&#24179;&#34913;&#65292;&#24182;&#36890;&#36807;&#19981;&#21464;&#29305;&#24449;&#23398;&#20064;&#26500;&#24314;&#22810;&#31890;&#24230;&#32852;&#21512;&#35299;&#20915;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#23614;&#20998;&#31867;&#26159;&#29616;&#23454;&#19990;&#30028;&#20013;&#19981;&#21487;&#36991;&#20813;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#38271;&#23614;&#20998;&#31867;&#26041;&#27861;&#20165;&#20851;&#27880;&#35299;&#20915;&#31867;&#38388;&#19981;&#24179;&#34913;&#65292;&#21363;&#22836;&#37096;&#31867;&#21035;&#30340;&#26679;&#26412;&#27604;&#23614;&#37096;&#31867;&#21035;&#30340;&#26679;&#26412;&#22810;&#65292;&#32780;&#24573;&#30053;&#20102;&#31867;&#20869;&#19981;&#24179;&#34913;&#65292;&#21363;&#21516;&#19968;&#31867;&#21035;&#20013;&#22836;&#37096;&#23646;&#24615;&#26679;&#26412;&#25968;&#37327;&#36828;&#22823;&#20110;&#23614;&#37096;&#23646;&#24615;&#26679;&#26412;&#25968;&#37327;&#12290;&#27169;&#22411;&#30340;&#20559;&#24046;&#26159;&#30001;&#36825;&#20004;&#20010;&#22240;&#32032;&#24341;&#36215;&#30340;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#20013;&#30340;&#23646;&#24615;&#26159;&#38544;&#21547;&#30340;&#19988;&#23646;&#24615;&#32452;&#21512;&#38750;&#24120;&#22797;&#26434;&#65292;&#22788;&#29702;&#31867;&#20869;&#19981;&#24179;&#34913;&#26356;&#21152;&#22256;&#38590;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31895;&#31890;&#24230;&#24341;&#23548;&#26862;&#26519;&#65288;CLF&#65289;&#21644;&#22810;&#20013;&#24515;&#25439;&#22833;&#65288;MCL&#65289;&#30340;&#38271;&#23614;&#20998;&#31867;&#26694;&#26550;&#65292;&#21517;&#20026;Cognisance&#65292;&#26088;&#22312;&#36890;&#36807;&#19981;&#21464;&#29305;&#24449;&#23398;&#20064;&#26500;&#24314;&#22810;&#31890;&#24230;&#32852;&#21512;&#35299;&#20915;&#27169;&#22411;&#12290;&#22312;&#36825;&#20010;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#21644;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#24179;&#34913;&#19981;&#21516;&#31867;&#21035;&#21644;&#23646;&#24615;&#20043;&#38388;&#30340;&#26679;&#26412;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-tailed(LT) classification is an unavoidable and challenging problem in the real world. Most of the existing long-tailed classification methods focus only on solving the inter-class imbalance in which there are more samples in the head class than in the tail class, while ignoring the intra-lass imbalance in which the number of samples of the head attribute within the same class is much larger than the number of samples of the tail attribute. The deviation in the model is caused by both of these factors, and due to the fact that attributes are implicit in most datasets and the combination of attributes is very complex, the intra-class imbalance is more difficult to handle. For this purpose, we proposed a long-tailed classification framework, known as \textbf{\textsc{Cognisance}}, which is founded on Coarse-Grained Leading Forest (CLF) and Multi-Center Loss (MCL), aiming to build a multi-granularity joint solution model by means of invariant feature learning. In this method, we desig
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#28216;&#25103;&#29702;&#35770;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;AI&#39537;&#21160;&#30340;&#21093;&#22842;&#20013;&#30340;&#19981;&#22242;&#32467;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#21463;&#23475;&#32773;&#38656;&#35201;&#35753;&#26410;&#26469;&#21463;&#23475;&#32773;&#35748;&#35782;&#21040;&#20182;&#20204;&#30340;&#21033;&#30410;&#21516;&#26679;&#38754;&#20020;&#20005;&#37325;&#21644;&#32039;&#36843;&#30340;&#23041;&#32961;&#65292;&#20197;&#28608;&#21169;&#26410;&#26469;&#21463;&#23475;&#32773;&#20197;&#22242;&#32467;&#25903;&#25345;&#24403;&#21069;&#21463;&#23475;&#32773;&#12290;</title><link>http://arxiv.org/abs/2310.06009</link><description>&lt;p&gt;
AI&#39537;&#21160;&#30340;&#21093;&#22842;&#20013;&#30340;&#20998;&#32780;&#27835;&#20043;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Divide-and-Conquer Dynamics in AI-Driven Disempowerment. (arXiv:2310.06009v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06009
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#28216;&#25103;&#29702;&#35770;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;AI&#39537;&#21160;&#30340;&#21093;&#22842;&#20013;&#30340;&#19981;&#22242;&#32467;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#21463;&#23475;&#32773;&#38656;&#35201;&#35753;&#26410;&#26469;&#21463;&#23475;&#32773;&#35748;&#35782;&#21040;&#20182;&#20204;&#30340;&#21033;&#30410;&#21516;&#26679;&#38754;&#20020;&#20005;&#37325;&#21644;&#32039;&#36843;&#30340;&#23041;&#32961;&#65292;&#20197;&#28608;&#21169;&#26410;&#26469;&#21463;&#23475;&#32773;&#20197;&#22242;&#32467;&#25903;&#25345;&#24403;&#21069;&#21463;&#23475;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#20844;&#21496;&#35797;&#22270;&#21019;&#36896;&#20986;&#22312;&#22823;&#37096;&#20998;&#32463;&#27982;&#20215;&#20540;&#24037;&#20316;&#19978;&#36229;&#36234;&#20154;&#31867;&#30340;AI&#31995;&#32479;&#12290;&#24403;&#21069;&#30340;AI&#27169;&#22411;&#24050;&#32463;&#33258;&#21160;&#21270;&#21066;&#24369;&#20102;&#19968;&#20123;&#33402;&#26415;&#23478;&#12289;&#28436;&#21592;&#21644;&#20316;&#23478;&#30340;&#29983;&#35745;&#12290;&#20294;&#26159;&#22312;&#37027;&#20123;&#20248;&#20808;&#32771;&#34385;&#24403;&#21069;&#21361;&#23475;&#21644;&#26410;&#26469;&#21361;&#23475;&#20043;&#38388;&#23384;&#22312;&#30528;&#20869;&#35751;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21338;&#24328;&#35770;&#27169;&#22411;&#26469;&#30740;&#31350;&#36825;&#31181;&#19981;&#22242;&#32467;&#30340;&#21407;&#22240;&#21644;&#21518;&#26524;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#26377;&#21161;&#20110;&#35299;&#37322;&#20026;&#20160;&#20040;&#22312;&#21382;&#21490;&#19978;&#65292;&#38754;&#20020;&#20849;&#21516;&#23041;&#32961;&#30340;&#21033;&#30410;&#30456;&#20851;&#26041;&#21457;&#29616;&#32852;&#21512;&#36215;&#26469;&#23545;&#25239;&#35813;&#23041;&#32961;&#26159;&#26377;&#21033;&#30340;&#65292;&#32780;&#35813;&#20849;&#21516;&#23041;&#32961;&#21448;&#21457;&#29616;&#20998;&#32780;&#27835;&#20043;&#26159;&#26377;&#21033;&#30340;&#12290;&#22312;&#29616;&#23454;&#21442;&#25968;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25552;&#20986;&#20102;&#20960;&#20010;&#39044;&#27979;&#65292;&#22312;&#21382;&#21490;&#32463;&#39564;&#35760;&#24405;&#20013;&#24471;&#21040;&#20102;&#21021;&#27493;&#30340;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI companies are attempting to create AI systems that outperform humans at most economically valuable work. Current AI models are already automating away the livelihoods of some artists, actors, and writers. But there is infighting between those who prioritize current harms and future harms. We construct a game-theoretic model of conflict to study the causes and consequences of this disunity. Our model also helps explain why throughout history, stakeholders sharing a common threat have found it advantageous to unite against it, and why the common threat has in turn found it advantageous to divide and conquer.  Under realistic parameter assumptions, our model makes several predictions that find preliminary corroboration in the historical-empirical record. First, current victims of AI-driven disempowerment need the future victims to realize that their interests are also under serious and imminent threat, so that future victims are incentivized to support current victims in solidarity. Se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#20010;&#24615;&#21270;&#32534;&#31243;&#21453;&#39304;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32467;&#21512;GPT-4&#20316;&#20026;&#8220;&#23548;&#24072;&#8221;&#27169;&#22411;&#29983;&#25104;&#25552;&#31034;&#65292;&#21033;&#29992;&#22833;&#36133;&#30340;&#27979;&#35797;&#29992;&#20363;&#30340;&#20449;&#24687;&#21644;&#20462;&#22797;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#29983;&#25104;&#25552;&#31034;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#36739;&#24369;&#30340;GPT-3.5&#27169;&#22411;&#20316;&#20026;&#8220;&#23398;&#29983;&#8221;&#27169;&#22411;&#36827;&#19968;&#27493;&#39564;&#35777;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03780</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#20154;&#24037;&#23548;&#24072;&#24335;&#32534;&#31243;&#21453;&#39304;: &#21033;&#29992;GPT-4&#23548;&#24072;&#27169;&#22411;&#29983;&#25104;&#25552;&#31034;&#21644;GPT-3.5&#23398;&#29983;&#27169;&#22411;&#36827;&#34892;&#25552;&#31034;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Automating Human Tutor-Style Programming Feedback: Leveraging GPT-4 Tutor Model for Hint Generation and GPT-3.5 Student Model for Hint Validation. (arXiv:2310.03780v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#20010;&#24615;&#21270;&#32534;&#31243;&#21453;&#39304;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32467;&#21512;GPT-4&#20316;&#20026;&#8220;&#23548;&#24072;&#8221;&#27169;&#22411;&#29983;&#25104;&#25552;&#31034;&#65292;&#21033;&#29992;&#22833;&#36133;&#30340;&#27979;&#35797;&#29992;&#20363;&#30340;&#20449;&#24687;&#21644;&#20462;&#22797;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#29983;&#25104;&#25552;&#31034;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#36739;&#24369;&#30340;GPT-3.5&#27169;&#22411;&#20316;&#20026;&#8220;&#23398;&#29983;&#8221;&#27169;&#22411;&#36827;&#19968;&#27493;&#39564;&#35777;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#20379;&#20010;&#24615;&#21270;&#32534;&#31243;&#21453;&#39304;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#25552;&#20379;&#20154;&#24037;&#23548;&#24072;&#24335;&#32534;&#31243;&#25552;&#31034;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#20197;&#24110;&#21161;&#23398;&#29983;&#35299;&#20915;&#31243;&#24207;&#20013;&#30340;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#26368;&#26032;&#30340;&#30740;&#31350;&#24037;&#20316;&#34429;&#28982;&#23545;&#21508;&#31181;&#21453;&#39304;&#29983;&#25104;&#22330;&#26223;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20294;&#20854;&#25972;&#20307;&#36136;&#37327;&#20173;&#36828;&#19981;&#21450;&#20154;&#24037;&#23548;&#24072;&#65292;&#24182;&#19988;&#36824;&#27809;&#26377;&#20934;&#22791;&#22909;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#25237;&#20837;&#20351;&#29992;&#12290;&#20026;&#20102;&#25552;&#39640;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#25552;&#20379;&#39640;&#36136;&#37327;&#32534;&#31243;&#25552;&#31034;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#65292;&#21517;&#20026;GPT4Hints-GPT3.5Val&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#21033;&#29992;GPT-4&#20316;&#20026;&#8220;&#23548;&#24072;&#8221;&#27169;&#22411;&#29983;&#25104;&#25552;&#31034;&#65292;&#36890;&#36807;&#20351;&#29992;&#22833;&#36133;&#30340;&#27979;&#35797;&#29992;&#20363;&#30340;&#31526;&#21495;&#20449;&#24687;&#21644;&#25552;&#31034;&#20013;&#30340;&#20462;&#22797;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#36136;&#37327;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#21033;&#29992;&#36739;&#24369;&#30340;GPT-3.5&#27169;&#22411;&#20316;&#20026;&#8220;&#23398;&#29983;&#8221;&#27169;&#22411;&#36827;&#19968;&#27493;&#39564;&#35777;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI and large language models hold great promise in enhancing programming education by automatically generating individualized feedback for students. We investigate the role of generative AI models in providing human tutor-style programming hints to help students resolve errors in their buggy programs. Recent works have benchmarked state-of-the-art models for various feedback generation scenarios; however, their overall quality is still inferior to human tutors and not yet ready for real-world deployment. In this paper, we seek to push the limits of generative AI models toward providing high-quality programming hints and develop a novel technique, GPT4Hints-GPT3.5Val. As a first step, our technique leverages GPT-4 as a ``tutor'' model to generate hints -- it boosts the generative quality by using symbolic information of failing test cases and fixes in prompts. As a next step, our technique leverages GPT-3.5, a weaker model, as a ``student'' model to further validate the hint 
&lt;/p&gt;</description></item><item><title>LLMR&#26159;&#19968;&#20010;&#29992;&#20110;&#23454;&#26102;&#21019;&#24314;&#21644;&#20462;&#25913;&#20132;&#20114;&#24335;&#28151;&#21512;&#29616;&#23454;&#20307;&#39564;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#23427;&#33021;&#22815;&#35299;&#20915;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#21644;&#35774;&#35745;&#30446;&#26631;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#26631;&#20934;&#30340;GPT-4&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLMR&#30340;&#36328;&#24179;&#21488;&#20114;&#25805;&#20316;&#24615;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#21644;&#29992;&#25143;&#30740;&#31350;&#35777;&#26126;&#20102;&#20854;&#23545;&#20110;&#29983;&#25104;&#21644;&#32534;&#36753;&#21508;&#31181;&#23545;&#35937;&#12289;&#24037;&#20855;&#21644;&#22330;&#26223;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.12276</link><description>&lt;p&gt;
LLMR&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#26102;&#25552;&#31034;&#20132;&#20114;&#24335;&#19990;&#30028;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
LLMR: Real-time Prompting of Interactive Worlds using Large Language Models. (arXiv:2309.12276v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12276
&lt;/p&gt;
&lt;p&gt;
LLMR&#26159;&#19968;&#20010;&#29992;&#20110;&#23454;&#26102;&#21019;&#24314;&#21644;&#20462;&#25913;&#20132;&#20114;&#24335;&#28151;&#21512;&#29616;&#23454;&#20307;&#39564;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#23427;&#33021;&#22815;&#35299;&#20915;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#21644;&#35774;&#35745;&#30446;&#26631;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#26631;&#20934;&#30340;GPT-4&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLMR&#30340;&#36328;&#24179;&#21488;&#20114;&#25805;&#20316;&#24615;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#21644;&#29992;&#25143;&#30740;&#31350;&#35777;&#26126;&#20102;&#20854;&#23545;&#20110;&#29983;&#25104;&#21644;&#32534;&#36753;&#21508;&#31181;&#23545;&#35937;&#12289;&#24037;&#20855;&#21644;&#22330;&#26223;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#28151;&#21512;&#29616;&#23454;&#22330;&#26223;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMR)&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#26102;&#21019;&#24314;&#21644;&#20462;&#25913;&#20132;&#20114;&#24335;&#28151;&#21512;&#29616;&#23454;&#20307;&#39564;&#12290;LLMR&#21033;&#29992;&#20102;&#26032;&#39062;&#30340;&#31574;&#30053;&#26469;&#35299;&#20915;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#25110;&#35774;&#35745;&#30446;&#26631;&#38656;&#35201;&#21512;&#25104;&#20869;&#37096;&#21160;&#24577;&#12289;&#30452;&#35266;&#20998;&#26512;&#25110;&#39640;&#32423;&#20132;&#20114;&#30340;&#22256;&#38590;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20381;&#36182;&#20110;&#25991;&#26412;&#20132;&#20114;&#21644;Unity&#28216;&#25103;&#24341;&#25806;&#12290;&#36890;&#36807;&#34701;&#21512;&#22330;&#26223;&#29702;&#35299;&#12289;&#20219;&#21153;&#35268;&#21010;&#12289;&#33258;&#25105;&#35843;&#35797;&#21644;&#20869;&#23384;&#31649;&#29702;&#25216;&#26415;&#65292;LLMR&#22312;&#24179;&#22343;&#38169;&#35823;&#29575;&#19978;&#27604;&#26631;&#20934;&#30340;GPT-4&#25552;&#39640;&#20102;4&#20493;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLMR&#19982;&#20960;&#20010;&#31034;&#20363;&#19990;&#30028;&#30340;&#36328;&#24179;&#21488;&#20114;&#25805;&#20316;&#24615;&#65292;&#24182;&#36890;&#36807;&#22810;&#20010;&#21019;&#24314;&#21644;&#20462;&#25913;&#20219;&#21153;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20197;&#23637;&#31034;&#23427;&#33021;&#22815;&#29983;&#25104;&#21644;&#32534;&#36753;&#21508;&#31181;&#23545;&#35937;&#12289;&#24037;&#20855;&#21644;&#22330;&#26223;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#26377;&#22810;&#26679;&#24615;&#30340;&#21487;&#29992;&#24615;&#30740;&#31350;&#65288;N=11&#65289;&#65292;&#25581;&#31034;&#20102;&#21442;&#19982;&#32773;&#23545;&#35813;&#31995;&#32479;&#26377;&#31215;&#26497;&#30340;&#20307;&#39564;&#65292;&#24182;&#24895;&#24847;&#20877;&#27425;&#20351;&#29992;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Large Language Model for Mixed Reality (LLMR), a framework for the real-time creation and modification of interactive Mixed Reality experiences using LLMs. LLMR leverages novel strategies to tackle difficult cases where ideal training data is scarce, or where the design goal requires the synthesis of internal dynamics, intuitive analysis, or advanced interactivity. Our framework relies on text interaction and the Unity game engine. By incorporating techniques for scene understanding, task planning, self-debugging, and memory management, LLMR outperforms the standard GPT-4 by 4x in average error rate. We demonstrate LLMR's cross-platform interoperability with several example worlds, and evaluate it on a variety of creation and modification tasks to show that it can produce and edit diverse objects, tools, and scenes. Finally, we conducted a usability study (N=11) with a diverse set that revealed participants had positive experiences with the system and would use it again.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#24341;&#20837;&#22122;&#22768;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#29305;&#23450;&#22122;&#22768;&#21487;&#20197;&#22312;&#38477;&#20302;&#20219;&#21153;&#22797;&#26434;&#24615;&#30340;&#26465;&#20214;&#19979;&#25552;&#21319;&#28145;&#24230;&#26550;&#26500;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.10625</link><description>&lt;p&gt;
&#25506;&#32034;&#23398;&#20064;&#31995;&#32479;&#20013;&#20449;&#24687;&#29109;&#21464;&#21270;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring the Influence of Information Entropy Change in Learning Systems. (arXiv:2309.10625v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#24341;&#20837;&#22122;&#22768;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#29305;&#23450;&#22122;&#22768;&#21487;&#20197;&#22312;&#38477;&#20302;&#20219;&#21153;&#22797;&#26434;&#24615;&#30340;&#26465;&#20214;&#19979;&#25552;&#21319;&#28145;&#24230;&#26550;&#26500;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21521;&#36755;&#20837;/&#38544;&#21547;&#29305;&#24449;&#28155;&#21152;&#22122;&#22768;&#26469;&#25506;&#32034;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#29109;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#30340;&#24212;&#29992;&#37325;&#28857;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#65292;&#20294;&#25152;&#25552;&#20986;&#30340;&#29702;&#35770;&#21487;&#20197;&#36827;&#19968;&#27493;&#24212;&#29992;&#20110;&#20854;&#20182;&#39046;&#22495;&#12290;&#22122;&#22768;&#36890;&#24120;&#34987;&#35270;&#20026;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65288;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#35270;&#35273;&#21464;&#25442;&#22120;&#65289;&#20197;&#21450;&#22270;&#20687;&#20998;&#31867;&#21644;&#36801;&#31227;&#23398;&#20064;&#31561;&#19981;&#21516;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#26377;&#23475;&#25200;&#21160;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#26088;&#22312;&#37325;&#26032;&#24605;&#32771;&#20256;&#32479;&#21629;&#39064;&#26159;&#21542;&#24635;&#26159;&#25104;&#31435;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#29305;&#23450;&#22122;&#22768;&#21487;&#20197;&#25552;&#21319;&#21508;&#31181;&#28145;&#24230;&#26550;&#26500;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#20449;&#24687;&#29109;&#23450;&#20041;&#30340;&#20219;&#21153;&#22797;&#26434;&#24615;&#20943;&#23569;&#26041;&#38754;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#27491;&#22122;&#22768;&#30340;&#22686;&#24378;&#25928;&#26524;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#25968;&#25454;&#38598;&#65288;&#22914;ImageNet&#65289;&#20013;&#23454;&#39564;&#35777;&#26126;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we explore the influence of entropy change in deep learning systems by adding noise to the inputs/latent features. The applications in this paper focus on deep learning tasks within computer vision, but the proposed theory can be further applied to other fields. Noise is conventionally viewed as a harmful perturbation in various deep learning architectures, such as convolutional neural networks (CNNs) and vision transformers (ViTs), as well as different learning tasks like image classification and transfer learning. However, this paper aims to rethink whether the conventional proposition always holds. We demonstrate that specific noise can boost the performance of various deep architectures under certain conditions. We theoretically prove the enhancement gained from positive noise by reducing the task complexity defined by information entropy and experimentally show the significant performance gain in large image datasets, such as the ImageNet. Herein, we use the informat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#33268;&#26102;&#38388;&#36923;&#36753;&#35268;&#21010;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#20010;&#39640;&#32423;&#23376;&#20219;&#21153;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#12290;&#20854;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#22914;&#20309;&#20197;&#27491;&#30830;&#24615;&#30340;&#35282;&#24230;&#25512;&#29702;&#26426;&#22120;&#20154;&#35745;&#21010;&#19982;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#36923;&#36753;&#20219;&#21153;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.10092</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#33268;&#26102;&#38388;&#36923;&#36753;&#35268;&#21010;&#65306;&#30693;&#36947;&#20309;&#26102;&#20570;&#20160;&#20040;&#21644;&#20309;&#26102;&#23547;&#27714;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformal Temporal Logic Planning using Large Language Models: Knowing When to Do What and When to Ask for Help. (arXiv:2309.10092v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#33268;&#26102;&#38388;&#36923;&#36753;&#35268;&#21010;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#20010;&#39640;&#32423;&#23376;&#20219;&#21153;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#12290;&#20854;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#22914;&#20309;&#20197;&#27491;&#30830;&#24615;&#30340;&#35282;&#24230;&#25512;&#29702;&#26426;&#22120;&#20154;&#35745;&#21010;&#19982;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#36923;&#36753;&#20219;&#21153;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#19968;&#20010;&#26032;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#65292;&#20219;&#21153;&#26159;&#20197;&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#34920;&#36798;&#24182;&#20197;&#26102;&#38388;&#21644;&#36923;&#36753;&#39034;&#24207;&#23436;&#25104;&#22810;&#20010;&#39640;&#32423;&#23376;&#20219;&#21153;&#12290;&#20026;&#20102;&#27491;&#24335;&#23450;&#20041;&#36825;&#26679;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;NL&#30340;&#21407;&#23376;&#35859;&#35789;&#22312;LTL&#19978;&#23450;&#20041;&#20102;&#27169;&#22411;&#12290;&#36825;&#19982;&#30456;&#20851;&#30340;&#35268;&#21010;&#26041;&#27861;&#24418;&#25104;&#23545;&#27604;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#21407;&#23376;&#35859;&#35789;&#19978;&#23450;&#20041;&#20102;&#25429;&#25417;&#25152;&#38656;&#20302;&#32423;&#31995;&#32479;&#37197;&#32622;&#30340;LTL&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#26426;&#22120;&#20154;&#35745;&#21010;&#65292;&#28385;&#36275;&#22522;&#20110;NL&#30340;&#21407;&#23376;&#21629;&#39064;&#23450;&#20041;&#30340;LTL&#20219;&#21153;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#20986;&#29616;&#30340;&#19968;&#20010;&#26032;&#30340;&#25216;&#26415;&#25361;&#25112;&#22312;&#20110;&#25512;&#29702;&#26426;&#22120;&#20154;&#35745;&#21010;&#30340;&#27491;&#30830;&#24615;&#19982;&#36825;&#20123;LTL&#32534;&#30721;&#30340;&#20219;&#21153;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HERACLEs&#65292;&#19968;&#20010;&#20998;&#23618;&#19968;&#33268;&#30340;&#33258;&#28982;&#35821;&#35328;&#35268;&#21010;&#22120;&#65292;&#23427;&#20381;&#36182;&#20110;&#29616;&#26377;&#24037;&#20855;&#30340;&#26032;&#22411;&#25972;&#21512;&#65292;&#21253;&#25324;&#65288;i&#65289;&#33258;&#21160;&#26426;&#29702;&#35770;&#65292;&#20197;&#30830;&#23450;&#26426;&#22120;&#20154;&#24212;&#35813;&#23436;&#25104;&#30340;NL&#25351;&#23450;&#30340;&#23376;&#20219;&#21153;&#20197;&#25512;&#36827;&#20219;&#21153;&#36827;&#23637;&#65307;
&lt;/p&gt;
&lt;p&gt;
This paper addresses a new motion planning problem for mobile robots tasked with accomplishing multiple high-level sub-tasks, expressed using natural language (NL), in a temporal and logical order. To formally define such missions, we leverage LTL defined over NL-based atomic predicates modeling the considered NL-based sub-tasks. This is contrast to related planning approaches that define LTL tasks over atomic predicates capturing desired low-level system configurations. Our goal is to design robot plans that satisfy LTL tasks defined over NL-based atomic propositions. A novel technical challenge arising in this setup lies in reasoning about correctness of a robot plan with respect to such LTL-encoded tasks. To address this problem, we propose HERACLEs, a hierarchical conformal natural language planner, that relies on a novel integration of existing tools that include (i) automata theory to determine the NL-specified sub-task the robot should accomplish next to make mission progress; (
&lt;/p&gt;</description></item><item><title>SeaEval&#26159;&#19968;&#20010;&#35780;&#20272;&#22810;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#30740;&#31350;&#20102;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#20197;&#21450;&#23545;&#25991;&#21270;&#23454;&#36341;&#12289;&#32454;&#24494;&#24046;&#21035;&#21644;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;&#37325;&#35201;&#21457;&#29616;&#21253;&#25324;&#27169;&#22411;&#22312;&#32473;&#20986;&#25913;&#20889;&#25351;&#20196;&#26102;&#34892;&#20026;&#21508;&#24322;&#65292;&#21463;&#21040;&#26292;&#38706;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#23545;&#20110;&#35821;&#20041;&#31561;&#20215;&#30340;&#22810;&#35821;&#35328;&#26597;&#35810;&#30340;&#22238;&#31572;&#19981;&#19968;&#33268;&#65292;&#20197;&#21450;&#27169;&#22411;&#22312;&#24773;&#24863;&#30456;&#20851;&#38382;&#39064;&#19978;&#30340;&#19968;&#33268;&#24615;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2309.04766</link><description>&lt;p&gt;
SeaEval&#22810;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65306;&#20174;&#36328;&#35821;&#35328;&#23545;&#40784;&#21040;&#25991;&#21270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment to Cultural Reasoning. (arXiv:2309.04766v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04766
&lt;/p&gt;
&lt;p&gt;
SeaEval&#26159;&#19968;&#20010;&#35780;&#20272;&#22810;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#30740;&#31350;&#20102;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#20197;&#21450;&#23545;&#25991;&#21270;&#23454;&#36341;&#12289;&#32454;&#24494;&#24046;&#21035;&#21644;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;&#37325;&#35201;&#21457;&#29616;&#21253;&#25324;&#27169;&#22411;&#22312;&#32473;&#20986;&#25913;&#20889;&#25351;&#20196;&#26102;&#34892;&#20026;&#21508;&#24322;&#65292;&#21463;&#21040;&#26292;&#38706;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#23545;&#20110;&#35821;&#20041;&#31561;&#20215;&#30340;&#22810;&#35821;&#35328;&#26597;&#35810;&#30340;&#22238;&#31572;&#19981;&#19968;&#33268;&#65292;&#20197;&#21450;&#27169;&#22411;&#22312;&#24773;&#24863;&#30456;&#20851;&#38382;&#39064;&#19978;&#30340;&#19968;&#33268;&#24615;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;SeaEval&#22522;&#20934;&#27979;&#35797;&#12290;&#38500;&#20102;&#34920;&#24449;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#29702;&#35299;&#21644;&#25512;&#29702;&#33258;&#28982;&#35821;&#35328;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#23427;&#20204;&#23545;&#25991;&#21270;&#23454;&#36341;&#12289;&#32454;&#24494;&#24046;&#21035;&#21644;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#38500;&#20102;&#26631;&#20934;&#30340;&#20934;&#30830;&#24230;&#25351;&#26631;&#65292;&#25105;&#20204;&#36824;&#35843;&#26597;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#35821;&#20041;&#21644;&#22810;&#35821;&#35328;&#24615;&#32500;&#24230;&#19978;&#30340;&#33030;&#24369;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;&#24320;&#28304;&#21644;&#38381;&#28304;&#27169;&#22411;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#22312;&#32463;&#20856;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12289;&#25512;&#29702;&#21644;&#25991;&#21270;&#29702;&#35299;&#26041;&#38754;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;&#37325;&#35201;&#21457;&#29616;&#21253;&#25324;&#65306;&#65288;1&#65289;&#22823;&#22810;&#25968;&#27169;&#22411;&#22312;&#32473;&#20986;&#25913;&#20889;&#25351;&#20196;&#26102;&#30340;&#34892;&#20026;&#21508;&#24322;&#65307;&#65288;2&#65289;&#35768;&#22810;&#27169;&#22411;&#20173;&#28982;&#21463;&#21040;&#26292;&#38706;&#20559;&#24046;&#30340;&#24433;&#21709;&#65288;&#22914;&#20301;&#32622;&#20559;&#24046;&#12289;&#22823;&#22810;&#25968;&#26631;&#31614;&#20559;&#24046;&#65289;&#65307;&#65288;3&#65289;&#23545;&#20110;&#26681;&#28304;&#20110;&#20107;&#23454;&#12289;&#31185;&#23398;&#21644;&#24120;&#35782;&#30693;&#35782;&#30340;&#38382;&#39064;&#65292;&#39044;&#26399;&#22312;&#35821;&#20041;&#19978;&#31561;&#20215;&#30340;&#22810;&#35821;&#35328;&#26597;&#35810;&#24212;&#35813;&#24471;&#21040;&#19968;&#33268;&#30340;&#22238;&#31572;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#22312;&#36825;&#20123;&#26597;&#35810;&#19978;&#34920;&#29616;&#20986;&#20196;&#20154;&#24847;&#22806;&#30340;&#19981;&#19968;&#33268;&#24615;&#65307;&#65288;4&#65289;&#22810;&#35821;&#35328;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#23545;&#20110;&#24773;&#24863;&#30456;&#20851;&#30340;&#38382;&#39064;&#34920;&#29616;&#20986;&#19981;&#21516;&#31243;&#24230;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SeaEval, a benchmark for multilingual foundation models. In addition to characterizing how these models understand and reason with natural language, we also investigate how well they comprehend cultural practices, nuances, and values. Alongside standard accuracy metrics, we investigate the brittleness of foundation models in the dimensions of semantics and multilinguality. Our analyses span both open-sourced and closed models, leading to empirical results across classic NLP tasks, reasoning, and cultural comprehension. Key findings indicate (1) Most models exhibit varied behavior when given paraphrased instructions. (2) Many models still suffer from exposure bias (e.g., positional bias, majority label bias). (3) For questions rooted in factual, scientific, and commonsense knowledge, consistent responses are expected across multilingual queries that are semantically equivalent. Yet, most models surprisingly demonstrate inconsistent performance on these queries. (4) Multilingu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36328;&#27169;&#22411;&#19968;&#33268;&#24615;&#36827;&#34892;&#26631;&#31614;&#21435;&#22122;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35266;&#23519;&#21457;&#29616;&#65292;&#19981;&#21516;&#27169;&#22411;&#22312;&#24178;&#20928;&#31034;&#20363;&#19978;&#30340;&#39044;&#27979;&#30456;&#23545;&#30456;&#20284;&#65292;&#32780;&#22312;&#26377;&#22122;&#22768;&#31034;&#20363;&#19978;&#30340;&#39044;&#27979;&#22312;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#21464;&#21270;&#26356;&#22823;&#12290;&#22312;&#36825;&#31181;&#35266;&#23519;&#30340;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#36328;&#27169;&#22411;&#19968;&#33268;&#24615;&#36827;&#34892;&#21435;&#22122;&#30340;&#26041;&#27861;&#65288;DeCA&#65289;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#20004;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#21270;&#30340;&#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#25968;&#25454;&#35266;&#27979;&#30340;&#20284;&#28982;&#12290;</title><link>http://arxiv.org/abs/2308.13976</link><description>&lt;p&gt;
&#36890;&#36807;&#36328;&#27169;&#22411;&#19968;&#33268;&#24615;&#36827;&#34892;&#26631;&#31614;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
Label Denoising through Cross-Model Agreement. (arXiv:2308.13976v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36328;&#27169;&#22411;&#19968;&#33268;&#24615;&#36827;&#34892;&#26631;&#31614;&#21435;&#22122;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35266;&#23519;&#21457;&#29616;&#65292;&#19981;&#21516;&#27169;&#22411;&#22312;&#24178;&#20928;&#31034;&#20363;&#19978;&#30340;&#39044;&#27979;&#30456;&#23545;&#30456;&#20284;&#65292;&#32780;&#22312;&#26377;&#22122;&#22768;&#31034;&#20363;&#19978;&#30340;&#39044;&#27979;&#22312;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#21464;&#21270;&#26356;&#22823;&#12290;&#22312;&#36825;&#31181;&#35266;&#23519;&#30340;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#36328;&#27169;&#22411;&#19968;&#33268;&#24615;&#36827;&#34892;&#21435;&#22122;&#30340;&#26041;&#27861;&#65288;DeCA&#65289;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#20004;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#21270;&#30340;&#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#25968;&#25454;&#35266;&#27979;&#30340;&#20284;&#28982;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#20174;&#26377;&#22122;&#22768;&#30340;&#26631;&#31614;&#23398;&#20064;&#26159;&#38750;&#24120;&#24120;&#35265;&#30340;&#12290;&#35760;&#24518;&#36825;&#20123;&#26377;&#22122;&#22768;&#30340;&#26631;&#31614;&#21487;&#33021;&#20250;&#24433;&#21709;&#27169;&#22411;&#30340;&#23398;&#20064;&#65292;&#20174;&#32780;&#23548;&#33268;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#26377;&#22122;&#22768;&#26631;&#31614;&#20013;&#23398;&#20064;&#40065;&#26834;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#27169;&#22411;&#22312;&#24178;&#20928;&#31034;&#20363;&#19978;&#30340;&#39044;&#27979;&#30456;&#23545;&#30456;&#20284;&#65292;&#32780;&#22312;&#26377;&#22122;&#22768;&#31034;&#20363;&#19978;&#30340;&#39044;&#27979;&#22312;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#21464;&#21270;&#26356;&#22823;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#36328;&#27169;&#22411;&#19968;&#33268;&#24615;&#36827;&#34892;&#21435;&#22122;&#65288;DeCA&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#26368;&#23567;&#21270;&#30001;&#20004;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#21270;&#30340;&#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#25968;&#25454;&#35266;&#27979;&#30340;&#20284;&#28982;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;DeCA&#26041;&#27861;&#24212;&#29992;&#20110;&#20108;&#36827;&#21046;&#26631;&#31614;&#24773;&#26223;&#21644;&#22810;&#26631;&#31614;&#24773;&#26223;&#12290;&#23545;&#20110;&#20108;&#36827;&#21046;&#26631;&#31614;&#24773;&#26223;&#65292;&#25105;&#20204;&#36873;&#25321;&#38544;&#24335;&#21453;&#39304;&#25512;&#33616;&#20316;&#20026;&#19979;&#28216;&#20219;&#21153;&#65292;&#24182;&#36827;&#34892;&#20102;&#22235;&#31181;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from corrupted labels is very common in real-world machine-learning applications. Memorizing such noisy labels could affect the learning of the model, leading to sub-optimal performances. In this work, we propose a novel framework to learn robust machine-learning models from noisy labels. Through an empirical study, we find that different models make relatively similar predictions on clean examples, while the predictions on noisy examples vary much more across different models. Motivated by this observation, we propose \em denoising with cross-model agreement \em (DeCA) which aims to minimize the KL-divergence between the true label distributions parameterized by two machine learning models while maximizing the likelihood of data observation. We employ the proposed DeCA on both the binary label scenario and the multiple label scenario. For the binary label scenario, we select implicit feedback recommendation as the downstream task and conduct experiments with four state-of-the
&lt;/p&gt;</description></item><item><title>LR-XFL&#26159;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36923;&#36753;&#35268;&#21017;&#21644;&#27169;&#22411;&#26356;&#26032;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#20102;&#23545;FL&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#25552;&#21319;&#21644;&#21152;&#26435;&#32858;&#21512;&#65292;&#24182;&#22312;&#30456;&#20851;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.12681</link><description>&lt;p&gt;
LR-XFL: &#22522;&#20110;&#36923;&#36753;&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LR-XFL: Logical Reasoning-based Explainable Federated Learning. (arXiv:2308.12681v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12681
&lt;/p&gt;
&lt;p&gt;
LR-XFL&#26159;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36923;&#36753;&#35268;&#21017;&#21644;&#27169;&#22411;&#26356;&#26032;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#20102;&#23545;FL&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#25552;&#21319;&#21644;&#21152;&#26435;&#32858;&#21512;&#65292;&#24182;&#22312;&#30456;&#20851;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064; (FL) &#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21327;&#20316;&#35757;&#32451;&#26041;&#27861;&#65292;&#33021;&#22815;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#38544;&#31169;&#20445;&#25252;&#30340;&#38656;&#27714;&#20351;&#24471;FL&#27169;&#22411;&#24456;&#38590;&#23454;&#29616;&#20840;&#23616;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36923;&#36753;&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#32852;&#37030;&#23398;&#20064; (LR-XFL) &#26041;&#27861;&#65292;&#23558;&#36923;&#36753;&#25512;&#29702;&#34701;&#20837;FL&#20013;&#12290;&#22312;LR-XFL&#20013;&#65292;FL&#23458;&#25143;&#31471;&#26681;&#25454;&#20854;&#26412;&#22320;&#25968;&#25454;&#21019;&#24314;&#26412;&#22320;&#36923;&#36753;&#35268;&#21017;&#65292;&#24182;&#23558;&#20854;&#19982;&#27169;&#22411;&#26356;&#26032;&#19968;&#36215;&#21457;&#36865;&#21040;FL&#26381;&#21153;&#22120;&#12290;FL&#26381;&#21153;&#22120;&#36890;&#36807;&#36866;&#24403;&#30340;&#36923;&#36753;&#36830;&#25509;&#31526;&#23558;&#26412;&#22320;&#36923;&#36753;&#35268;&#21017;&#36830;&#25509;&#36215;&#26469;&#65292;&#35813;&#36830;&#25509;&#31526;&#22522;&#20110;&#23458;&#25143;&#31471;&#25968;&#25454;&#30340;&#23646;&#24615;&#36827;&#34892;&#25512;&#23548;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#21407;&#22987;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#26381;&#21153;&#22120;&#36824;&#26681;&#25454;&#23458;&#25143;&#31471;&#19978;&#20256;&#30340;&#36923;&#36753;&#35268;&#21017;&#21453;&#26144;&#30340;&#26412;&#22320;&#25968;&#25454;&#30340;&#36136;&#37327;&#65292;&#20351;&#29992;&#26435;&#37325;&#20540;&#23545;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#36827;&#34892;&#32858;&#21512;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;LR-XFL&#22312;&#26368;&#30456;&#20851;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;1.19&#65285;&#65292;5.81&#65285;&#21644;5.41&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is an emerging approach for training machine learning models collaboratively while preserving data privacy. The need for privacy protection makes it difficult for FL models to achieve global transparency and explainability. To address this limitation, we incorporate logic-based explanations into FL by proposing the Logical Reasoning-based eXplainable Federated Learning (LR-XFL) approach. Under LR-XFL, FL clients create local logic rules based on their local data and send them, along with model updates, to the FL server. The FL server connects the local logic rules through a proper logical connector that is derived based on properties of client data, without requiring access to the raw data. In addition, the server also aggregates the local model updates with weight values determined by the quality of the clients' local data as reflected by their uploaded logic rules. The results show that LR-XFL outperforms the most relevant baseline by 1.19%, 5.81% and 5.41% in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26377;&#38480;&#20803;&#31639;&#23376;&#32593;&#32476;&#65288;FEONet&#65289;&#35299;&#20915;&#21442;&#25968;PDE&#12290;&#23427;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#25968;&#20540;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#36755;&#20837;-&#36755;&#20986;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#21442;&#25968;PDE&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#20934;&#30830;&#24230;&#12289;&#27867;&#21270;&#24615;&#21644;&#35745;&#31639;&#28789;&#27963;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.04690</link><description>&lt;p&gt;
&#29992;&#20110;&#35299;&#20915;&#21442;&#25968;PDE&#30340;&#26377;&#38480;&#20803;&#31639;&#23376;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Finite Element Operator Network for Solving Parametric PDEs. (arXiv:2308.04690v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26377;&#38480;&#20803;&#31639;&#23376;&#32593;&#32476;&#65288;FEONet&#65289;&#35299;&#20915;&#21442;&#25968;PDE&#12290;&#23427;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#25968;&#20540;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#36755;&#20837;-&#36755;&#20986;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#21442;&#25968;PDE&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#20934;&#30830;&#24230;&#12289;&#27867;&#21270;&#24615;&#21644;&#35745;&#31639;&#28789;&#27963;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#26159;&#25105;&#20204;&#29702;&#35299;&#21644;&#39044;&#27979;&#29289;&#29702;&#12289;&#24037;&#31243;&#21644;&#37329;&#34701;&#31561;&#20247;&#22810;&#39046;&#22495;&#33258;&#28982;&#29616;&#35937;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#35299;&#20915;&#21442;&#25968;PDE&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#39640;&#25928;&#30340;&#25968;&#20540;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26377;&#38480;&#20803;&#31639;&#23376;&#32593;&#32476;&#65288;FEONet&#65289;&#35299;&#20915;&#21442;&#25968;PDE&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#25968;&#20540;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#26377;&#38480;&#20803;&#27861;&#65292;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#37197;&#23545;&#30340;&#36755;&#20837;-&#36755;&#20986;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#21442;&#25968;PDE&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#34920;&#26126;&#23427;&#22312;&#20934;&#30830;&#24230;&#12289;&#27867;&#21270;&#24615;&#21644;&#35745;&#31639;&#28789;&#27963;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;FEONet&#26694;&#26550;&#22312;&#27169;&#25311;&#20855;&#26377;&#19981;&#21516;&#36793;&#30028;&#26465;&#20214;&#21644;&#22797;&#26434;&#22495;&#30340;&#21508;&#31181;&#39046;&#22495;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partial differential equations (PDEs) underlie our understanding and prediction of natural phenomena across numerous fields, including physics, engineering, and finance. However, solving parametric PDEs is a complex task that necessitates efficient numerical methods. In this paper, we propose a novel approach for solving parametric PDEs using a Finite Element Operator Network (FEONet). Our proposed method leverages the power of deep learning in conjunction with traditional numerical methods, specifically the finite element method, to solve parametric PDEs in the absence of any paired input-output training data. We demonstrate the effectiveness of our approach on several benchmark problems and show that it outperforms existing state-of-the-art methods in terms of accuracy, generalization, and computational flexibility. Our FEONet framework shows potential for application in various fields where PDEs play a crucial role in modeling complex domains with diverse boundary conditions and sin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21311;&#21517;&#21270;&#35821;&#38899;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#35780;&#20272;&#20102;&#21311;&#21517;&#21270;&#30340;&#31243;&#24230;&#65292;&#20197;&#24212;&#23545;&#35821;&#38899;&#25968;&#25454;&#25910;&#38598;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#21644;&#24694;&#24847;&#20351;&#29992;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2308.04455</link><description>&lt;p&gt;
&#21311;&#21517;&#21270;&#35821;&#38899;&#65306;&#35780;&#20272;&#21644;&#35774;&#35745;&#35828;&#35805;&#20154;&#21311;&#21517;&#21270;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Anonymizing Speech: Evaluating and Designing Speaker Anonymization Techniques. (arXiv:2308.04455v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21311;&#21517;&#21270;&#35821;&#38899;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#35780;&#20272;&#20102;&#21311;&#21517;&#21270;&#30340;&#31243;&#24230;&#65292;&#20197;&#24212;&#23545;&#35821;&#38899;&#25968;&#25454;&#25910;&#38598;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#21644;&#24694;&#24847;&#20351;&#29992;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#38899;&#29992;&#25143;&#30028;&#38754;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#35821;&#38899;&#25968;&#25454;&#30340;&#25910;&#38598;&#21644;&#23384;&#20648;&#20063;&#22823;&#22823;&#22686;&#21152;&#12290;&#34429;&#28982;&#25968;&#25454;&#25910;&#38598;&#21487;&#20197;&#20026;&#22823;&#22810;&#25968;&#35821;&#38899;&#26381;&#21153;&#25552;&#20379;&#39640;&#25928;&#30340;&#24037;&#20855;&#65292;&#20294;&#23427;&#20063;&#32473;&#29992;&#25143;&#30340;&#38544;&#31169;&#36896;&#25104;&#20005;&#37325;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#38598;&#20013;&#23384;&#20648;&#20351;&#20010;&#20154;&#30340;&#35821;&#38899;&#25968;&#25454;&#23481;&#26131;&#21463;&#21040;&#32593;&#32476;&#23041;&#32961;&#30340;&#20405;&#23475;&#12290;&#38543;&#30528;&#20122;&#39532;&#36874;&#30340;Alexa&#65292;&#35895;&#27468;&#30340;Home&#21644;&#33529;&#26524;&#30340;Siri&#31561;&#22522;&#20110;&#35821;&#38899;&#30340;&#25968;&#23383;&#21161;&#25163;&#30340;&#20351;&#29992;&#22686;&#21152;&#65292;&#20197;&#21450;&#20010;&#20154;&#35821;&#38899;&#25968;&#25454;&#25910;&#38598;&#21464;&#24471;&#36234;&#26469;&#36234;&#23481;&#26131;&#65292;&#22768;&#38899;&#20811;&#38534;&#21644;&#35828;&#35805;&#20154;/&#24615;&#21035;/&#30149;&#29702;&#31561;&#35782;&#21035;&#30340;&#24694;&#24847;&#20351;&#29992;&#30340;&#39118;&#38505;&#20063;&#22686;&#21152;&#20102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21311;&#21517;&#21270;&#35821;&#38899;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#35780;&#20272;&#21311;&#21517;&#21270;&#30340;&#31243;&#24230;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#21311;&#21517;&#21270;&#26159;&#25351;&#20351;&#20010;&#20154;&#35821;&#38899;&#25968;&#25454;&#19982;&#36523;&#20221;&#26080;&#27861;&#20851;&#32852;&#65292;&#21516;&#26102;&#20445;&#25345;&#35821;&#38899;&#20449;&#21495;&#30340;&#23454;&#29992;&#24615;&#65288;&#20363;&#22914;&#65292;&#35775;&#38382;&#35821;&#35328;&#20869;&#23481;&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#20960;&#20010;&#35780;&#20272;&#21327;&#35758;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing use of voice user interfaces has led to a surge in the collection and storage of speech data. While data collection allows for the development of efficient tools powering most speech services, it also poses serious privacy issues for users as centralized storage makes private personal speech data vulnerable to cyber threats. With the increasing use of voice-based digital assistants like Amazon's Alexa, Google's Home, and Apple's Siri, and with the increasing ease with which personal speech data can be collected, the risk of malicious use of voice-cloning and speaker/gender/pathological/etc. recognition has increased.  This thesis proposes solutions for anonymizing speech and evaluating the degree of the anonymization. In this work, anonymization refers to making personal speech data unlinkable to an identity while maintaining the usefulness (utility) of the speech signal (e.g., access to linguistic content). We start by identifying several challenges that evaluation protoco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24314;&#31435;&#22238;&#25253;&#24046;&#36317;&#19978;&#30028;&#65292;&#23558;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#38382;&#39064;&#36716;&#21270;&#20026;&#31163;&#25955;&#28040;&#24687;&#30340;&#22312;&#32447;&#32858;&#31867;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#37327;&#21270;&#20445;&#35777;&#65292;&#24182;&#19988;&#20855;&#26377;&#36890;&#20449;&#24320;&#38144;&#20302;&#12289;&#21487;&#35299;&#37322;&#24615;&#22909;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.03358</link><description>&lt;p&gt;
&#20998;&#25955;&#24335;POMDP&#20013;&#22522;&#20110;&#22312;&#32447;&#32858;&#31867;&#26631;&#31614;&#30340;&#31163;&#25955;&#28040;&#24687;&#20256;&#36882;
&lt;/p&gt;
&lt;p&gt;
Discrete Message via Online Clustering Labels in Decentralized POMDP. (arXiv:2308.03358v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24314;&#31435;&#22238;&#25253;&#24046;&#36317;&#19978;&#30028;&#65292;&#23558;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#38382;&#39064;&#36716;&#21270;&#20026;&#31163;&#25955;&#28040;&#24687;&#30340;&#22312;&#32447;&#32858;&#31867;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#37327;&#21270;&#20445;&#35777;&#65292;&#24182;&#19988;&#20855;&#26377;&#36890;&#20449;&#24320;&#38144;&#20302;&#12289;&#21487;&#35299;&#37322;&#24615;&#22909;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#36890;&#20449;&#23545;&#20110;&#35299;&#20915;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#24037;&#20316;&#36890;&#24120;&#20381;&#36182;&#20110;&#40657;&#30418;&#26041;&#27861;&#65292;&#23558;&#26412;&#22320;&#20449;&#24687;/&#29305;&#24449;&#32534;&#30721;&#25104;&#19982;&#20854;&#20182;&#26234;&#33021;&#20307;&#20849;&#20139;&#30340;&#28040;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#40657;&#30418;&#26041;&#27861;&#26080;&#27861;&#23545;&#26399;&#26395;&#22238;&#25253;&#25552;&#20379;&#20219;&#20309;&#37327;&#21270;&#20445;&#35777;&#65292;&#24120;&#24120;&#23548;&#33268;&#29983;&#25104;&#36890;&#20449;&#24320;&#38144;&#39640;&#12289;&#21487;&#35299;&#37322;&#24615;&#24046;&#30340;&#36830;&#32493;&#28040;&#24687;&#12290;&#26412;&#25991;&#22312;&#29702;&#24819;&#31574;&#30053;&#19982;&#26368;&#20248;&#37096;&#20998;&#21487;&#35266;&#23519;&#31574;&#30053;&#20043;&#38388;&#24314;&#31435;&#20102;&#22238;&#25253;&#24046;&#36317;&#30340;&#19978;&#30028;&#12290;&#35813;&#32467;&#26524;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#37325;&#26032;&#23450;&#20041;&#20026;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#26412;&#22320;&#35266;&#23519;&#20013;&#30340;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#32858;&#31867;&#38382;&#39064;&#65292;&#20854;&#20013;&#28040;&#24687;&#20316;&#20026;&#32858;&#31867;&#26631;&#31614;&#65292;&#24182;&#19988;&#22238;&#25253;&#24046;&#36317;&#30340;&#19978;&#30028;&#20316;&#20026;&#32858;&#31867;&#25439;&#22833;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;&#19978;&#30028;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#22320;&#31616;&#21333;&#30340;&#28040;&#24687;&#29983;&#25104;&#20989;&#25968;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication is crucial for solving cooperative Multi-Agent Reinforcement Learning tasks in Partially-Observable Markov Decision Processes. Existing works often rely on black-box methods to encode local information/features into messages shared with other agents. However, such black-box approaches are unable to provide any quantitative guarantees on the expected return and often lead to the generation of continuous messages with high communication overhead and poor interpretability. In this paper, we establish an upper bound on the return gap between an ideal policy with full observability and an optimal partially-observable policy with discrete communication. This result enables us to recast multi-agent communication into a novel online clustering problem over the local observations at each agent, with messages as cluster labels and the upper bound on the return gap as clustering loss. By minimizing the upper bound, we propose a surprisingly simple design of message generation functi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#20351;&#29992;&#26377;&#38480;&#35775;&#38382;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL&#65289;&#30340;&#23376;&#38598;&#25351;&#23450;&#30340;&#30446;&#26631;&#36716;&#21270;&#20026;&#34892;&#20026;&#26641;&#65288;BT&#65289;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#19968;&#20123;&#35268;&#21010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.06399</link><description>&lt;p&gt;
&#20174;&#38754;&#21521;&#30446;&#26631;&#30340;LTLf&#20844;&#24335;&#35774;&#35745;&#34892;&#20026;&#26641;
&lt;/p&gt;
&lt;p&gt;
Designing Behavior Trees from Goal-Oriented LTLf Formulas. (arXiv:2307.06399v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#20351;&#29992;&#26377;&#38480;&#35775;&#38382;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL&#65289;&#30340;&#23376;&#38598;&#25351;&#23450;&#30340;&#30446;&#26631;&#36716;&#21270;&#20026;&#34892;&#20026;&#26641;&#65288;BT&#65289;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#19968;&#20123;&#35268;&#21010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#36923;&#36753;&#21487;&#20197;&#29992;&#20110;&#24418;&#24335;&#21270;&#25351;&#23450;&#33258;&#20027;&#20195;&#29702;&#30446;&#26631;&#65292;&#20294;&#21512;&#25104;&#33021;&#22815;&#20445;&#35777;&#30446;&#26631;&#28385;&#36275;&#30340;&#35745;&#21010;&#32773;&#21487;&#33021;&#20855;&#26377;&#35745;&#31639;&#19978;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20351;&#29992;&#26377;&#38480;&#35775;&#38382;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL&#65289;&#30340;&#23376;&#38598;&#25351;&#23450;&#30340;&#30446;&#26631;&#36716;&#21270;&#20026;&#20445;&#35777;&#25104;&#21151;&#36335;&#24452;&#28385;&#36275;LTL&#30446;&#26631;&#30340;&#34892;&#20026;&#26641;&#65288;BT&#65289;&#12290;&#21487;&#20197;&#20351;&#29992;&#20197;&#23454;&#29616;&#30446;&#26631;&#20026;&#23548;&#21521;&#30340;&#20219;&#21153;&#20219;&#21153;&#35821;&#27861;&#26469;&#25552;&#21462;&#26377;&#29992;&#30340;LTL&#20844;&#24335;&#65292;&#20854;&#20013;&#30001;LTL&#36816;&#31639;&#31526;&#32452;&#21512;&#30340;&#20219;&#21153;&#26500;&#25104;&#20219;&#21153;&#12290;&#36890;&#36807;LTL&#20844;&#24335;&#26500;&#24314;BT&#23548;&#33268;&#20102;&#19968;&#31181;&#25918;&#26494;&#30340;&#34892;&#20026;&#21512;&#25104;&#38382;&#39064;&#65292;&#22312;&#35813;&#38382;&#39064;&#20013;&#65292;&#21508;&#31181;&#35745;&#21010;&#32773;&#21487;&#20197;&#23454;&#29616;BT&#20013;&#30340;&#21160;&#20316;&#33410;&#28857;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#35745;&#21010;&#32773;&#23548;&#33268;&#30340;&#20219;&#20309;&#25104;&#21151;&#36335;&#24452;&#37117;&#28385;&#36275;&#30456;&#24212;&#30340;LTL&#20844;&#24335;&#12290;&#35813;&#26041;&#27861;&#30340;&#26377;&#29992;&#24615;&#36890;&#36807;&#20004;&#31181;&#26041;&#24335;&#36827;&#34892;&#20102;&#28436;&#31034;&#65306;a) &#25506;&#32034;&#20004;&#20010;&#35745;&#21010;&#32773;&#21644;LTL&#30446;&#26631;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;b) &#35299;&#20915;Fetch&#26426;&#22120;&#20154;&#30340;&#39034;&#24207;&#38376;&#38145;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal logic can be used to formally specify autonomous agent goals, but synthesizing planners that guarantee goal satisfaction can be computationally prohibitive. This paper shows how to turn goals specified using a subset of finite trace Linear Temporal Logic (LTL) into a behavior tree (BT) that guarantees that successful traces satisfy the LTL goal. Useful LTL formulas for achievement goals can be derived using achievement-oriented task mission grammars, leading to missions made up of tasks combined using LTL operators. Constructing BTs from LTL formulas leads to a relaxed behavior synthesis problem in which a wide range of planners can implement the action nodes in the BT. Importantly, any successful trace induced by the planners satisfies the corresponding LTL formula. The usefulness of the approach is demonstrated in two ways: a) exploring the alignment between two planners and LTL goals, and b) solving a sequential key-door problem for a Fetch robot.
&lt;/p&gt;</description></item><item><title>HypLL&#26159;&#19968;&#20010;&#20351;&#29992;&#24076;&#20122;&#31354;&#38388;&#30340;&#28145;&#24230;&#23398;&#20064;&#24211;&#65292;&#22522;&#20110;PyTorch&#65292;&#26088;&#22312;&#20351;&#20854;&#26131;&#20110;&#20351;&#29992;&#65292;&#25645;&#24314;&#24076;&#20122;&#32593;&#32476;&#27169;&#22359;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22788;&#29702;&#23618;&#27425;&#21270;&#25968;&#25454;&#21644;&#20351;&#29992;&#23569;&#37327;&#23884;&#20837;&#32500;&#24230;&#65292;&#26159;&#19968;&#31181;&#26032;&#30340;&#12289;&#24320;&#25918;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.06154</link><description>&lt;p&gt;
HypLL: &#24076;&#20122;&#31354;&#38388;&#28145;&#24230;&#23398;&#20064;&#24211;
&lt;/p&gt;
&lt;p&gt;
HypLL: The Hyperbolic Learning Library. (arXiv:2306.06154v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06154
&lt;/p&gt;
&lt;p&gt;
HypLL&#26159;&#19968;&#20010;&#20351;&#29992;&#24076;&#20122;&#31354;&#38388;&#30340;&#28145;&#24230;&#23398;&#20064;&#24211;&#65292;&#22522;&#20110;PyTorch&#65292;&#26088;&#22312;&#20351;&#20854;&#26131;&#20110;&#20351;&#29992;&#65292;&#25645;&#24314;&#24076;&#20122;&#32593;&#32476;&#27169;&#22359;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22788;&#29702;&#23618;&#27425;&#21270;&#25968;&#25454;&#21644;&#20351;&#29992;&#23569;&#37327;&#23884;&#20837;&#32500;&#24230;&#65292;&#26159;&#19968;&#31181;&#26032;&#30340;&#12289;&#24320;&#25918;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#12289;&#22810;&#23186;&#20307;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#65292;&#24076;&#20122;&#31354;&#38388;&#28145;&#24230;&#23398;&#20064;&#27491;&#36805;&#36895;&#24341;&#36215;&#20851;&#27880;&#12290;&#28145;&#24230;&#32593;&#32476;&#36890;&#24120;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#36816;&#34892;&#65292;&#38544;&#21547;&#22320;&#20551;&#35774;&#25968;&#25454;&#22312;&#35268;&#21017;&#32593;&#26684;&#19978;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#22788;&#29702;&#23618;&#27425;&#21270;&#25968;&#25454;&#21644;&#20351;&#29992;&#23569;&#37327;&#23884;&#20837;&#32500;&#24230;&#26102;&#65292;&#24076;&#20122;&#20960;&#20309;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#21487;&#35775;&#38382;&#30340;&#24320;&#28304;&#24211;&#29992;&#20110;&#26500;&#24314;&#31867;&#20284;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#28145;&#24230;&#23398;&#20064;&#24211;&#30340;&#24076;&#20122;&#32593;&#32476;&#27169;&#22359;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;HypLL, &#21363;&#24076;&#20122;&#31354;&#38388;&#28145;&#24230;&#23398;&#20064;&#24211;&#65292;&#20197;&#23558;&#24076;&#20122;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#23637;&#32858;&#38598;&#22312;&#19968;&#36215;&#12290;HypLL&#24314;&#31435;&#22312;PyTorch&#20043;&#19978;&#65292;&#29305;&#21035;&#24378;&#35843;&#20854;&#26131;&#29992;&#24615;&#35774;&#35745;&#65292;&#20197;&#21560;&#24341;&#24191;&#27867;&#30340;&#21463;&#20247;&#20851;&#27880;&#36825;&#20010;&#26032;&#30340;&#21644;&#24320;&#25918;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#20195;&#30721;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#25214;&#21040;&#65306;https://github.com/maxvanspengler/hyperbolic_learning_library&#12290;&#21387;&#32553;&#25991;&#20214;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#25214;&#21040;&#65306;https://d
&lt;/p&gt;
&lt;p&gt;
Deep learning in hyperbolic space is quickly gaining traction in the fields of machine learning, multimedia, and computer vision. Deep networks commonly operate in Euclidean space, implicitly assuming that data lies on regular grids. Recent advances have shown that hyperbolic geometry provides a viable alternative foundation for deep learning, especially when data is hierarchical in nature and when working with few embedding dimensions. Currently however, no accessible open-source library exists to build hyperbolic network modules akin to well-known deep learning libraries. We present HypLL, the Hyperbolic Learning Library to bring the progress on hyperbolic deep learning together. HypLL is built on top of PyTorch, with an emphasis in its design for easy-of-use, in order to attract a broad audience towards this new and open-ended research direction. The code is available at: https://github.com/maxvanspengler/hyperbolic_learning_library. The compressed archive is available at: https://d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#30495;&#23454;&#19990;&#30028;&#23637;&#31034;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#34892;&#21160;&#30417;&#30563;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#21046;&#21644;&#25512;&#24191;&#20043;&#38388;&#24179;&#34913;&#30340; RL &#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.13030</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#30495;&#23454;&#19990;&#30028;&#23637;&#31034;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#34892;&#21160;&#30417;&#30563;
&lt;/p&gt;
&lt;p&gt;
Adaptive action supervision in reinforcement learning from real-world multi-agent demonstrations. (arXiv:2305.13030v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#30495;&#23454;&#19990;&#30028;&#23637;&#31034;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#34892;&#21160;&#30417;&#30563;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#21046;&#21644;&#25512;&#24191;&#20043;&#38388;&#24179;&#34913;&#30340; RL &#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#20013;&#65292;&#23545;&#30495;&#23454;&#19990;&#30028;&#29983;&#29289;&#22810;&#26234;&#33021;&#20307;&#36827;&#34892;&#24314;&#27169;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26159;&#22312;&#32593;&#32476;&#31354;&#38388;&#20013;&#29983;&#25104;&#28789;&#27963;&#21644;&#22810;&#26679;&#21270;&#34892;&#20026;&#30340;&#24378;&#22823;&#26694;&#26550;&#65307;&#28982;&#32780;&#65292;&#22312;&#24314;&#27169;&#30495;&#23454;&#19990;&#30028;&#29983;&#29289;&#22810;&#26234;&#33021;&#20307;&#26102;&#65292;&#22312;&#28304;&#65288;&#21363;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#65289;&#21644;&#30446;&#26631;&#65288;&#21363; RL &#30340;&#32593;&#32476;&#31354;&#38388;&#65289;&#20043;&#38388;&#23384;&#22312;&#22495;&#24046;&#24322;&#65292;&#24182;&#19988;&#28304;&#29615;&#22659;&#21442;&#25968;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#30340;&#30495;&#23454;&#19990;&#30028;&#23637;&#31034;&#20013;&#36827;&#34892; RL &#30340;&#33258;&#36866;&#24212;&#34892;&#21160;&#30417;&#30563;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#32467;&#21512; RL &#21644;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#22522;&#20110;&#21160;&#24577;&#26102;&#38388;&#25197;&#26354;&#30340;&#28436;&#31034;&#21160;&#20316;&#26469;&#22312; RL &#20013;&#21033;&#29992;&#26410;&#30693;&#28304;&#21160;&#24577;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#35768;&#22810;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#20026;&#25105;&#20204;&#25552;&#20379;&#19968;&#20010;&#22312;&#22797;&#21046;&#21644;&#25512;&#24191;&#20043;&#38388;&#24179;&#34913;&#30340; RL &#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling of real-world biological multi-agents is a fundamental problem in various scientific and engineering fields. Reinforcement learning (RL) is a powerful framework to generate flexible and diverse behaviors in cyberspace; however, when modeling real-world biological multi-agents, there is a domain gap between behaviors in the source (i.e., real-world data) and the target (i.e., cyberspace for RL), and the source environment parameters are usually unknown. In this paper, we propose a method for adaptive action supervision in RL from real-world demonstrations in multi-agent scenarios. We adopt an approach that combines RL and supervised learning by selecting actions of demonstrations in RL based on the minimum distance of dynamic time warping for utilizing the information of the unknown source dynamics. This approach can be easily applied to many existing neural network architectures and provide us with an RL model balanced between reproducibility as imitation and generalization ab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#33258;&#28982;&#22270;&#20687;&#35757;&#32451;&#30340;&#27169;&#22411;&#21019;&#24314;&#32472;&#30011;&#39118;&#26684;&#65292;&#32780;&#19981;&#38656;&#35201;&#33402;&#26415;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#33021;&#20026;&#33402;&#26415;&#20013;&#30340;&#29983;&#25104;AI&#21512;&#27861;&#20351;&#29992;&#38138;&#24179;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2305.12015</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#28982;&#28789;&#24863;&#21457;&#26126;&#32472;&#30011;&#39118;&#26684;
&lt;/p&gt;
&lt;p&gt;
Inventing painting styles through natural inspiration. (arXiv:2305.12015v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#33258;&#28982;&#22270;&#20687;&#35757;&#32451;&#30340;&#27169;&#22411;&#21019;&#24314;&#32472;&#30011;&#39118;&#26684;&#65292;&#32780;&#19981;&#38656;&#35201;&#33402;&#26415;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#33021;&#20026;&#33402;&#26415;&#20013;&#30340;&#29983;&#25104;AI&#21512;&#27861;&#20351;&#29992;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#36807;&#20165;&#20351;&#29992;&#33258;&#28982;&#22270;&#20687;&#35757;&#32451;&#30340;&#27169;&#22411;&#21019;&#24314;&#32472;&#30011;&#39118;&#26684;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#23458;&#35266;&#35777;&#25454;&#35777;&#26126;&#27169;&#22411;&#27809;&#26377;&#21117;&#31363;&#20154;&#31867;&#33402;&#26415;&#39118;&#26684;&#12290;&#22312;&#31532;&#19968;&#31181;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#33402;&#26415;&#23186;&#20171;&#30340;&#24402;&#32435;&#20559;&#32622;&#26469;&#23454;&#29616;&#21019;&#36896;&#24615;&#30340;&#34920;&#36798;&#12290;&#36890;&#36807;&#20351;&#29992;&#37325;&#24314;&#25439;&#22833;&#26469;&#23454;&#29616;&#25277;&#35937;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#20351;&#29992;&#39069;&#22806;&#30340;&#33258;&#28982;&#22270;&#20687;&#20316;&#20026;&#28789;&#24863;&#26469;&#21019;&#24314;&#26032;&#30340;&#39118;&#26684;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#20351;&#24471;&#21487;&#20197;&#21457;&#26126;&#26032;&#30340;&#32472;&#30011;&#39118;&#26684;&#65292;&#32780;&#26080;&#38656;&#33402;&#26415;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#20026;&#33402;&#26415;&#20013;&#29983;&#25104;AI&#30340;&#21512;&#27861;&#20351;&#29992;&#38138;&#24179;&#36947;&#36335;&#65292;&#32780;&#19981;&#20405;&#29359;&#20154;&#31867;&#21019;&#36896;&#32773;&#30340;&#29420;&#21019;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose two procedures to create painting styles using models trained only on natural images, providing objective proof that the model is not plagiarizing human art styles. In the first procedure we use the inductive bias from the artistic medium to achieve creative expression. Abstraction is achieved by using a reconstruction loss. The second procedure uses an additional natural image as inspiration to create a new style. These two procedures make it possible to invent new painting styles with no artistic training data. We believe that our approach can help pave the way for the ethical employment of generative AI in art, without infringing upon the originality of human creators.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27604;&#36739;&#22235;&#31181;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#21160;&#24577;&#22240;&#23376;&#27169;&#22411;&#23545;&#32654;&#22269;GDP&#23395;&#24230;&#22686;&#38271;&#30340;&#39044;&#27979;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#24179;&#34913;&#32463;&#27982;&#22686;&#38271;&#26399;&#38388;&#65292;&#26356;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#33021;&#22815;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#20294;&#26159;&#36825;&#31181;&#25928;&#26524;&#20250;&#22312;&#19981;&#21040;&#20004;&#24180;&#30340;&#26102;&#38388;&#20869;&#28040;&#22833;&#12290;&#22312;&#32463;&#27982;&#21160;&#33633;&#26102;&#26399;&#65292;&#38271;&#26399;&#35760;&#24518;&#30340;&#25928;&#26524;&#21464;&#24471;&#26126;&#26174;&#12290;</title><link>http://arxiv.org/abs/2304.05805</link><description>&lt;p&gt;
&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#22269;&#20869;&#29983;&#20135;&#24635;&#20540;&#65306;&#38271;&#26399;&#35760;&#24518;&#26377;&#22810;&#22823;&#30340;&#20316;&#29992;&#65311;
&lt;/p&gt;
&lt;p&gt;
GDP nowcasting with artificial neural networks: How much does long-term memory matter?. (arXiv:2304.05805v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05805
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27604;&#36739;&#22235;&#31181;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#21160;&#24577;&#22240;&#23376;&#27169;&#22411;&#23545;&#32654;&#22269;GDP&#23395;&#24230;&#22686;&#38271;&#30340;&#39044;&#27979;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#24179;&#34913;&#32463;&#27982;&#22686;&#38271;&#26399;&#38388;&#65292;&#26356;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#33021;&#22815;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#20294;&#26159;&#36825;&#31181;&#25928;&#26524;&#20250;&#22312;&#19981;&#21040;&#20004;&#24180;&#30340;&#26102;&#38388;&#20869;&#28040;&#22833;&#12290;&#22312;&#32463;&#27982;&#21160;&#33633;&#26102;&#26399;&#65292;&#38271;&#26399;&#35760;&#24518;&#30340;&#25928;&#26524;&#21464;&#24471;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#19981;&#21516;&#30340;&#32479;&#35745;&#27169;&#22411;&#24212;&#29992;&#20110;&#32654;&#22269;&#32463;&#27982;&#23395;&#24230;&#22269;&#20869;&#29983;&#20135;&#24635;&#20540;&#65288;GDP&#65289;&#22686;&#38271;&#39044;&#27979;&#12290;&#20351;&#29992;&#27599;&#26376;&#30340;FRED-MD&#25968;&#25454;&#24211;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#21160;&#24577;&#22240;&#23376;&#27169;&#22411;&#65288;DFM&#65289;&#21644;&#22235;&#20010;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#30340;&#39044;&#27979;&#34920;&#29616;&#65306;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#12289;&#19968;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;1D CNN&#65289;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#21644;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#12290;&#23454;&#35777;&#20998;&#26512;&#21576;&#29616;&#20102;&#20004;&#20010;&#19981;&#21516;&#35780;&#20272;&#21608;&#26399;&#30340;&#32467;&#26524;&#12290;&#31532;&#19968;&#20010;&#21608;&#26399;&#65288;2010&#24180;&#31532;1&#23395;&#24230;&#33267;2019&#24180;&#31532;4&#23395;&#24230;&#65289;&#20855;&#26377;&#24179;&#34913;&#30340;&#32463;&#27982;&#22686;&#38271;&#65292;&#32780;&#31532;&#20108;&#20010;&#21608;&#26399;&#65288;2010&#24180;&#31532;1&#23395;&#24230;&#33267;2022&#24180;&#31532;3&#23395;&#24230;&#65289;&#36824;&#21253;&#25324;COVID-19&#34928;&#36864;&#26399;&#38388;&#30340;&#26102;&#38388;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#26356;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#22312;&#24179;&#34913;&#32463;&#27982;&#22686;&#38271;&#26399;&#38388;&#33021;&#22815;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#22312;&#19968;&#20010;&#30456;&#23545;&#36739;&#20302;&#30340;&#38408;&#20540;&#20540;&#65288;&#32422;&#20845;&#20010;&#23395;&#24230;&#25110;&#21313;&#20843;&#20010;&#26376;&#65289;&#20197;&#21518;&#65292;&#36825;&#31181;&#25928;&#24212;&#20250;&#28040;&#22833;&#12290;&#22312;&#32463;&#27982;&#21160;&#33633;&#26399;&#65288;&#22914;COVID-19&#34928;&#36864;&#26399;&#38388;&#65289;&#65292;&#38271;&#26399;&#35760;&#24518;&#30340;&#25928;&#26524;&#20250;&#21464;&#24471;&#36739;&#20026;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;
In our study, we apply different statistical models to nowcast quarterly GDP growth for the US economy. Using the monthly FRED-MD database, we compare the nowcasting performance of the dynamic factor model (DFM) and four artificial neural networks (ANNs): the multilayer perceptron (MLP), the one-dimensional convolutional neural network (1D CNN), the long short-term memory network (LSTM), and the gated recurrent unit (GRU). The empirical analysis presents the results from two distinctively different evaluation periods. The first (2010:Q1 -- 2019:Q4) is characterized by balanced economic growth, while the second (2010:Q1 -- 2022:Q3) also includes periods of the COVID-19 recession. According to our results, longer input sequences result in more accurate nowcasts in periods of balanced economic growth. However, this effect ceases above a relatively low threshold value of around six quarters (eighteen months). During periods of economic turbulence (e.g., during the COVID-19 recession), long
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Supervision Interpolation &#30340;&#26032;&#27010;&#24565;&#26694;&#26550;&#65292;&#36890;&#36807;&#25918;&#26494;&#21644;&#25512;&#24191; Mixup &#25552;&#20379;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#25554;&#20540;&#22686;&#24378;&#35270;&#35282;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; LossMix &#30340;&#31616;&#21333;&#32780;&#22810;&#21151;&#33021;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22686;&#24378;&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#65292;&#25110;&#32773;&#35828;LossMix &#22312;&#30446;&#26631;&#26816;&#27979;&#21644;&#20854;&#20182;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2303.10343</link><description>&lt;p&gt;
LossMix&#65306;&#31616;&#21270;&#21644;&#24191;&#27867;&#24212;&#29992; Mixup &#20110;&#30446;&#26631;&#26816;&#27979;&#21644;&#26356;&#22810;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
LossMix: Simplify and Generalize Mixup for Object Detection and Beyond. (arXiv:2303.10343v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Supervision Interpolation &#30340;&#26032;&#27010;&#24565;&#26694;&#26550;&#65292;&#36890;&#36807;&#25918;&#26494;&#21644;&#25512;&#24191; Mixup &#25552;&#20379;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#25554;&#20540;&#22686;&#24378;&#35270;&#35282;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; LossMix &#30340;&#31616;&#21333;&#32780;&#22810;&#21151;&#33021;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22686;&#24378;&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#65292;&#25110;&#32773;&#35828;LossMix &#22312;&#30446;&#26631;&#26816;&#27979;&#21644;&#20854;&#20182;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#28151;&#21512;&#22686;&#24378;&#24191;&#27867;&#24212;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#20294;&#30001;&#20110;&#31354;&#38388;&#38169;&#20301;&#12289;&#21069;&#26223;/&#32972;&#26223;&#21306;&#20998;&#20197;&#21450;&#22810;&#20010;&#23454;&#20363;&#30340;&#25361;&#25112;&#65292;&#36825;&#20123;&#25216;&#26415;&#19981;&#26131;&#24212;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;&#30417;&#30563;&#25554;&#20540;&#30340;&#26032;&#27010;&#24565;&#26694;&#26550;&#65292;&#36890;&#36807;&#25918;&#26494;&#21644;&#25512;&#24191; Mixup &#25552;&#20379;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#25554;&#20540;&#22686;&#24378;&#35270;&#35282;&#65292;&#28982;&#21518;&#22312;&#36825;&#20010;&#26694;&#26550;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102; LossMix&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#22810;&#21151;&#33021;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22686;&#24378;&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;insight&#26159;&#65292;&#36890;&#36807;&#25554;&#20540;&#25439;&#22833;&#35823;&#24046;&#26469;&#35843;&#25972;&#35757;&#32451;&#21487;&#20197;&#26377;&#25928;&#35268;&#33539;&#28151;&#21512;&#25968;&#25454;&#30340;&#35757;&#32451;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;ground truth&#26631;&#31614;&#12290;&#22312;PASCAL VOC&#21644;MS COCO&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;LossMix&#22987;&#32456;&#20248;&#20110;&#24403;&#21069;&#27969;&#34892;&#30340;&#28151;&#21512;&#31574;&#30053;&#65292;&#24182;&#19988;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#39046;&#22495;m...
&lt;/p&gt;
&lt;p&gt;
The success of data mixing augmentations in image classification tasks has been well-received. However, these techniques cannot be readily applied to object detection due to challenges such as spatial misalignment, foreground/background distinction, and plurality of instances. To tackle these issues, we first introduce a novel conceptual framework called Supervision Interpolation, which offers a fresh perspective on interpolation-based augmentations by relaxing and generalizing Mixup. Building on this framework, we propose LossMix, a simple yet versatile and effective regularization that enhances the performance and robustness of object detectors and more. Our key insight is that we can effectively regularize the training on mixed data by interpolating their loss errors instead of ground truth labels. Empirical results on the PASCAL VOC and MS COCO datasets demonstrate that LossMix consistently outperforms currently popular mixing strategies. Furthermore, we design a two-stage domain m
&lt;/p&gt;</description></item><item><title>GPT-4&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#21487;&#20197;&#25509;&#25910;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#24182;&#20135;&#29983;&#25991;&#26412;&#36755;&#20986;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#19987;&#19994;&#21644;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#36890;&#36807;&#27169;&#25311;&#30340;&#24459;&#24072;&#32771;&#35797;&#12290;&#35813;&#39033;&#30446;&#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#24320;&#21457;&#22522;&#30784;&#35774;&#26045;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#22312;&#24191;&#27867;&#30340;&#35268;&#27169;&#33539;&#22260;&#20869;&#34920;&#29616;&#39044;&#27979;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.08774</link><description>&lt;p&gt;
GPT-4&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
GPT-4 Technical Report. (arXiv:2303.08774v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08774
&lt;/p&gt;
&lt;p&gt;
GPT-4&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#21487;&#20197;&#25509;&#25910;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#24182;&#20135;&#29983;&#25991;&#26412;&#36755;&#20986;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#19987;&#19994;&#21644;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#36890;&#36807;&#27169;&#25311;&#30340;&#24459;&#24072;&#32771;&#35797;&#12290;&#35813;&#39033;&#30446;&#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#24320;&#21457;&#22522;&#30784;&#35774;&#26045;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#22312;&#24191;&#27867;&#30340;&#35268;&#27169;&#33539;&#22260;&#20869;&#34920;&#29616;&#39044;&#27979;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25253;&#21578;&#20102;GPT-4&#30340;&#24320;&#21457;&#65292;&#23427;&#26159;&#19968;&#20010;&#21487;&#20197;&#25509;&#21463;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#24182;&#20135;&#29983;&#25991;&#26412;&#36755;&#20986;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#34429;&#28982;&#22312;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#20013;&#19981;&#22914;&#20154;&#31867;&#65292;&#20294;GPT-4&#22312;&#21508;&#31181;&#19987;&#19994;&#21644;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#36890;&#36807;&#27169;&#25311;&#30340;&#24459;&#24072;&#32771;&#35797;&#65292;&#25104;&#32489;&#25490;&#21517;&#22312;&#21069;10&#65285;&#24038;&#21491;&#12290;GPT-4&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#39044;&#35757;&#32451;&#29992;&#20110;&#39044;&#27979;&#25991;&#26723;&#20013;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#21518;&#35757;&#32451;&#23545;&#40784;&#36807;&#31243;&#25552;&#39640;&#20102;&#20107;&#23454;&#24615;&#21644;&#31526;&#21512;&#26399;&#26395;&#34892;&#20026;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;&#39033;&#30446;&#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#24320;&#21457;&#22522;&#30784;&#35774;&#26045;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#22312;&#24191;&#27867;&#30340;&#35268;&#27169;&#33539;&#22260;&#20869;&#34920;&#29616;&#39044;&#27979;&#24615;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;GPT-4&#30340;&#26576;&#20123;&#24615;&#33021;&#26041;&#38754;&#65292;&#32780;&#36825;&#20123;&#24615;&#33021;&#26159;&#22522;&#20110;&#20351;&#29992;&#19981;&#36229;&#36807;GPT-4&#35745;&#31639;&#33021;&#21147;&#30340;1/1,000&#30340;&#27169;&#22411;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20266;&#23545;&#27604;&#23398;&#20064;&#30340;&#21322;&#30417;&#30563;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#21487;&#38752;&#30340;&#36127;&#26679;&#26412;&#23545;&#26469;&#25913;&#36827;&#20266;&#26631;&#31614;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2302.09532</link><description>&lt;p&gt;
&#22522;&#20110;&#20266;&#23545;&#27604;&#23398;&#20064;&#30340;&#22522;&#20110;&#22270;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Pseudo Contrastive Learning for Graph-based Semi-supervised Learning. (arXiv:2302.09532v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20266;&#23545;&#27604;&#23398;&#20064;&#30340;&#21322;&#30417;&#30563;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#21487;&#38752;&#30340;&#36127;&#26679;&#26412;&#23545;&#26469;&#25913;&#36827;&#20266;&#26631;&#31614;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20266;&#26631;&#31614;&#26159;&#19968;&#31181;&#29992;&#20110;&#25913;&#36827;&#21322;&#30417;&#30563;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#24615;&#33021;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#26681;&#25454;&#33258;&#20449;&#30340;&#39044;&#27979;&#29983;&#25104;&#38468;&#21152;&#30340;&#20266;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20998;&#31867;&#30446;&#26631;&#23545;&#32473;&#23450;&#26631;&#31614;&#30340;&#25935;&#24863;&#24615;&#65292;&#29983;&#25104;&#30340;&#20266;&#26631;&#31614;&#36136;&#37327;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#36991;&#20813;&#19981;&#21487;&#38752;&#30340;&#20998;&#31867;&#30417;&#30563;&#8220;&#19968;&#20010;&#33410;&#28857;&#23646;&#20110;&#29305;&#23450;&#31867;&#8221;&#65292;&#25105;&#20204;&#26356;&#21916;&#27426;&#23481;&#38169;&#24615;&#23545;&#27604;&#30417;&#30563;&#8220;&#20004;&#20010;&#33410;&#28857;&#19981;&#23646;&#20110;&#21516;&#19968;&#31867;&#8221;&#12290;&#22240;&#27492;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#25918;&#26494;&#30340;&#29256;&#26412;&#65292;&#21363;&#35782;&#21035;&#21487;&#38752;&#30340;&#36127;&#26679;&#26412;&#23545;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;GNNs&#26694;&#26550;&#65292;&#31216;&#20043;&#20026;&#20266;&#23545;&#27604;&#23398;&#20064;(PCL)&#12290;&#23427;&#23558;&#30446;&#26631;&#20026;&#30456;&#21516;&#31867;&#30340;&#27491;&#20266;&#26631;&#31614;&#21644;&#36127;&#20266;&#26631;&#31614;&#30340;&#20004;&#20010;&#33410;&#28857;&#20998;&#24320;&#12290;&#20026;&#20102;&#23558;&#25299;&#25169;&#30693;&#35782;&#32435;&#20837;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#25299;&#25169;&#21152;&#26435;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Pseudo Labeling is a technique used to improve the performance of semi-supervised Graph Neural Networks (GNNs) by generating additional pseudo-labels based on confident predictions. However, the quality of generated pseudo-labels has been a longstanding concern due to the sensitivity of the classification objective with respect to the given labels. To avoid the untrustworthy classification supervision indicating ``a node belongs to a specific class,'' we favor the fault-tolerant contrasting supervision demonstrating ``two nodes do not belong to the same class.'' Thus, the problem of generating high-quality pseudo-labels is then transformed into a relaxed version, i.e., identifying reliable negative pairs. To achieve this, we propose a general framework for GNNs, termed Pseudo Contrastive Learning (PCL). It separates two nodes whose positive and negative pseudo-labels target the same class. To incorporate topological knowledge into learning, we devise a topologically weighted contrastiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24212;&#29992;&#33539;&#30068;&#35770;&#30340;&#26041;&#27861;&#20026;&#36817;&#20284;&#25512;&#26029;&#25552;&#20379;&#20102;&#20989;&#23376;&#35821;&#20041;&#65292;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#36125;&#21494;&#26031;&#36879;&#38236;&#30340;&#27010;&#24565;&#21644;&#32479;&#35745;&#21338;&#24328;&#30340;&#32420;&#32500;&#65292;&#23545;&#32479;&#35745;&#25512;&#26029;&#38382;&#39064;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2212.12538</link><description>&lt;p&gt;
&#19968;&#31181;&#36125;&#21494;&#26031;&#22823;&#33041;&#30340;&#32452;&#21512;&#24615;&#35299;&#37322;&#30340;&#25968;&#23398;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Mathematical Foundations for a Compositional Account of the Bayesian Brain. (arXiv:2212.12538v2 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24212;&#29992;&#33539;&#30068;&#35770;&#30340;&#26041;&#27861;&#20026;&#36817;&#20284;&#25512;&#26029;&#25552;&#20379;&#20102;&#20989;&#23376;&#35821;&#20041;&#65292;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#36125;&#21494;&#26031;&#36879;&#38236;&#30340;&#27010;&#24565;&#21644;&#32479;&#35745;&#21338;&#24328;&#30340;&#32420;&#32500;&#65292;&#23545;&#32479;&#35745;&#25512;&#26029;&#38382;&#39064;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25253;&#21578;&#20102;&#20851;&#20110;&#20027;&#21160;&#25512;&#26029;&#21644;&#36125;&#21494;&#26031;&#22823;&#33041;&#32452;&#21512;&#24615;&#35299;&#37322;&#30340;&#19968;&#20123;&#21021;&#27493;&#30740;&#31350;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#24403;&#20195;&#24212;&#29992;&#33539;&#30068;&#35770;&#30340;&#24037;&#20855;&#20026;&#36817;&#20284;&#25512;&#26029;&#25552;&#20379;&#20102;&#20989;&#23376;&#35821;&#20041;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#8220;&#35821;&#27861;&#8221;&#26041;&#38754;&#23450;&#20041;&#20102;&#26032;&#30340;&#36125;&#21494;&#26031;&#36879;&#38236;&#30340;&#27010;&#24565;&#65292;&#24182;&#23637;&#31034;&#20102;&#36125;&#21494;&#26031;&#26356;&#26032;&#36981;&#24490;&#32452;&#21512;&#36879;&#38236;&#27169;&#24335;&#12290;&#21033;&#29992;&#36125;&#21494;&#26031;&#36879;&#38236;&#65292;&#24182;&#21463;&#32452;&#21512;&#21338;&#24328;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#32479;&#35745;&#21338;&#24328;&#30340;&#32420;&#32500;&#21644;&#23558;&#21508;&#31181;&#32479;&#35745;&#25512;&#26029;&#38382;&#39064;&#20998;&#31867;&#20026;&#23545;&#24212;&#30340;&#37096;&#20998;&#65306;&#30456;&#23545;&#29109;&#30340;&#38142;&#24335;&#35268;&#21017;&#34987;&#24418;&#24335;&#21270;&#20026;&#20005;&#26684;&#37096;&#20998;&#65292;&#32780;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#21644;&#33258;&#30001;&#33021;&#21017;&#32473;&#20986;&#20102;&#26494;&#24347;&#37096;&#20998;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#22797;&#21046;-&#32452;&#21512;&#8221;&#30340;&#27010;&#24565;&#12290;&#22312;&#8220;&#35821;&#20041;&#8221;&#26041;&#38754;&#65292;&#25105;&#20204;&#23558;&#19968;&#33324;&#30340;&#24320;&#25918;&#21160;&#21147;&#31995;&#32479;&#65288;&#23588;&#20854;&#26159;&#30830;&#23450;&#24615;&#12289;&#38543;&#26426;&#24615;&#21644;&#38543;&#26426;&#24615;&#65292;&#20197;&#21450;&#31163;&#25955;&#21644;&#36830;&#32493;&#26102;&#38388;&#65289;&#20316;&#20026;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
This dissertation reports some first steps towards a compositional account of active inference and the Bayesian brain. Specifically, we use the tools of contemporary applied category theory to supply functorial semantics for approximate inference. To do so, we define on the `syntactic' side the new notion of Bayesian lens and show that Bayesian updating composes according to the compositional lens pattern. Using Bayesian lenses, and inspired by compositional game theory, we define fibrations of statistical games and classify various problems of statistical inference as corresponding sections: the chain rule of the relative entropy is formalized as a strict section, while maximum likelihood estimation and the free energy give lax sections. In the process, we introduce a new notion of `copy-composition'.  On the `semantic' side, we present a new formalization of general open dynamical systems (particularly: deterministic, stochastic, and random; and discrete- and continuous-time) as cert
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#25351;&#25968;&#21028;&#25454;&#26469;&#25552;&#39640;&#20854;&#31995;&#32479;&#25239;&#24178;&#25200;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#20316;&#32773;&#36827;&#34892;&#20102;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#34920;&#26126;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#25191;&#34892;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.09010</link><description>&lt;p&gt;
&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65306;&#25351;&#25968;&#26631;&#20934;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Risk-Sensitive Reinforcement Learning with Exponential Criteria. (arXiv:2212.09010v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#25351;&#25968;&#21028;&#25454;&#26469;&#25552;&#39640;&#20854;&#31995;&#32479;&#25239;&#24178;&#25200;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#20316;&#32773;&#36827;&#34892;&#20102;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#34920;&#26126;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#25191;&#34892;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39118;&#38505;&#20013;&#24615;&#30340;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#22312;&#24456;&#22810;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#23454;&#39564;&#25104;&#21151;&#65292;&#20294;&#26159;&#36825;&#31181;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#22122;&#22768;&#21644;&#31995;&#32479;&#21442;&#25968;&#25200;&#21160;&#30340;&#24433;&#21709;&#32780;&#19981;&#22815;&#31283;&#20581;&#12290;&#22240;&#27492;,&#23545;&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20197;&#25552;&#39640;&#20854;&#31995;&#32479;&#25239;&#24178;&#25200;&#24615;&#65292;&#26679;&#26412;&#25928;&#29575;&#21644;&#23454;&#29992;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#26080;&#27169;&#22411;&#39118;&#38505;&#25935;&#24863;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#24191;&#27867;&#20351;&#29992;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#36827;&#34892;&#21464;&#20307;&#65292;&#20854;&#23454;&#29616;&#36807;&#31243;&#31867;&#20284;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#25351;&#25968;&#26631;&#20934;&#23545;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#31574;&#30053;&#39118;&#38505;&#25935;&#24863;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#24320;&#21457;&#20102;&#33945;&#29305;&#21345;&#32599;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#21644;&#22312;&#32447;(&#26102;&#38388;&#24046;&#20998;)&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#30340;&#21464;&#20307;&#12290;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;&#25351;&#25968;&#26631;&#20934;&#30340;&#20351;&#29992;&#33021;&#22815;&#25512;&#24191;&#24120;&#29992;&#30340;&#29305;&#23450;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;&#20316;&#32773;&#22312;&#25670;&#21160;&#26438;&#21644;&#25670;&#25670;&#26438;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#23454;&#29616;&#24615;&#33021;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While risk-neutral reinforcement learning has shown experimental success in a number of applications, it is well-known to be non-robust with respect to noise and perturbations in the parameters of the system. For this reason, risk-sensitive reinforcement learning algorithms have been studied to introduce robustness and sample efficiency, and lead to better real-life performance. In this work, we introduce new model-free risk-sensitive reinforcement learning algorithms as variations of widely-used Policy Gradient algorithms with similar implementation properties. In particular, we study the effect of exponential criteria on the risk-sensitivity of the policy of a reinforcement learning agent, and develop variants of the Monte Carlo Policy Gradient algorithm and the online (temporal-difference) Actor-Critic algorithm. Analytical results showcase that the use of exponential criteria generalize commonly used ad-hoc regularization approaches. The implementation, performance, and robustness 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36827;&#34892;&#34394;&#20551;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20165;&#20351;&#29992;100&#20010;&#26631;&#35760;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;91\%&#12290;</title><link>http://arxiv.org/abs/2212.01071</link><description>&lt;p&gt;
&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#34394;&#20551;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Fake detection in imbalance dataset by Semi-supervised learning with GAN. (arXiv:2212.01071v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36827;&#34892;&#34394;&#20551;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20165;&#20351;&#29992;100&#20010;&#26631;&#35760;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;91\%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#39578;&#25200;&#34892;&#20026;&#21464;&#24471;&#26356;&#21152;&#26222;&#36941;&#65292;&#36825;&#23548;&#33268;&#20102;&#34394;&#20551;&#26816;&#27979;&#25104;&#20026;&#30740;&#31350;&#20154;&#21592;&#20013;&#24341;&#20154;&#27880;&#30446;&#30340;&#39046;&#22495;&#12290;&#25968;&#25454;&#30340;&#22270;&#24418;&#29305;&#24615;&#20197;&#21450;&#22823;&#37327;&#33410;&#28857;&#23548;&#33268;&#20102;&#35768;&#22810;&#38556;&#30861;&#65292;&#21253;&#25324;&#30697;&#38453;&#20013;&#22823;&#37327;&#26080;&#20851;&#29305;&#24449;&#30340;&#39640;&#31163;&#25955;&#24230;&#21644;&#19981;&#24179;&#34913;&#31867;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#37319;&#29992;&#20102;&#33258;&#32534;&#30721;&#22120;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#19982;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#31639;&#27861;&#30340;&#32452;&#21512;&#65292;&#21363;SGAN&#12290;&#26412;&#25991;&#23558;&#23569;&#37327;&#26631;&#31614;&#24212;&#29992;&#20110;SGAN&#20316;&#20026;&#20998;&#31867;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20165;&#20351;&#29992;100&#20010;&#26631;&#35760;&#26679;&#26412;&#65292;&#35813;&#26041;&#27861;&#22312;&#26816;&#27979;&#34394;&#20551;&#36134;&#25143;&#26041;&#38754;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;91\%&#12290;
&lt;/p&gt;
&lt;p&gt;
As social media grows faster, harassment becomes more prevalent which leads to considered fake detection a fascinating field among researchers. The graph nature of data with the large number of nodes caused different obstacles including a considerable amount of unrelated features in matrices as high dispersion and imbalance classes in the dataset. To deal with these issues Auto-encoders and a combination of semi-supervised learning and the GAN algorithm which is called SGAN were used. This paper is deploying a smaller number of labels and applying SGAN as a classifier. The result of this test showed that the accuracy had reached 91\% in detecting fake accounts using only 100 labeled samples.
&lt;/p&gt;</description></item><item><title>&#23545;&#27169;&#22411;&#35299;&#37322;&#30340;&#29992;&#25143;&#30740;&#31350;&#32508;&#36848;&#21457;&#29616;&#65292;&#21487;&#35299;&#37322;&#22411;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#27491;&#22312;&#26576;&#20123;&#24212;&#29992;&#39046;&#22495;&#24555;&#36895;&#25193;&#25955;&#65292;&#20294;&#29992;&#25143;&#35780;&#20272;&#20173;&#28982;&#31232;&#32570;&#19988;&#20960;&#20046;&#19981;&#28041;&#21450;&#35748;&#30693;&#25110;&#31038;&#20250;&#31185;&#23398;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2210.11584</link><description>&lt;p&gt;
&#26397;&#30528;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#21487;&#35299;&#37322;&#22411;&#20154;&#24037;&#26234;&#33021;&#65306;&#23545;&#27169;&#22411;&#35299;&#37322;&#30340;&#29992;&#25143;&#30740;&#31350;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Towards Human-centered Explainable AI: A Survey of User Studies for Model Explanations. (arXiv:2210.11584v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11584
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27169;&#22411;&#35299;&#37322;&#30340;&#29992;&#25143;&#30740;&#31350;&#32508;&#36848;&#21457;&#29616;&#65292;&#21487;&#35299;&#37322;&#22411;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#27491;&#22312;&#26576;&#20123;&#24212;&#29992;&#39046;&#22495;&#24555;&#36895;&#25193;&#25955;&#65292;&#20294;&#29992;&#25143;&#35780;&#20272;&#20173;&#28982;&#31232;&#32570;&#19988;&#20960;&#20046;&#19981;&#28041;&#21450;&#35748;&#30693;&#25110;&#31038;&#20250;&#31185;&#23398;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#22411;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#19981;&#26029;&#25193;&#23637;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#24517;&#38656;&#26465;&#20214;&#12290;&#23545;XAI&#29992;&#25143;&#38656;&#27714;&#30340;&#26356;&#22909;&#29702;&#35299;&#20197;&#21450;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#20154;&#26412;&#35780;&#20272;&#26082;&#26159;&#24517;&#35201;&#24615;&#20063;&#26159;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#30740;&#31350;&#20102;HCI&#21644;AI&#30740;&#31350;&#20154;&#21592;&#22914;&#20309;&#36827;&#34892;XAI&#24212;&#29992;&#30340;&#29992;&#25143;&#30740;&#31350;&#12290;&#36890;&#36807;&#23545;&#36807;&#21435;&#20116;&#24180;&#20013;&#22522;&#20110;&#20154;&#31867;&#30340;XAI&#35780;&#20272;&#30340;97&#31687;&#26680;&#24515;&#35770;&#25991;&#36827;&#34892;&#35782;&#21035;&#21644;&#28145;&#20837;&#20998;&#26512;&#65292;&#25105;&#20204;&#23558;&#20854;&#25353;&#29031;&#35299;&#37322;&#26041;&#27861;&#30340;&#27979;&#37327;&#29305;&#24449;&#65288;&#20449;&#20219;&#12289;&#29702;&#35299;&#12289;&#21487;&#29992;&#24615;&#21644;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#21512;&#20316;&#34920;&#29616;&#65289;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;XAI&#22312;&#26576;&#20123;&#24212;&#29992;&#39046;&#22495;&#65288;&#22914;&#25512;&#33616;&#31995;&#32479;&#65289;&#25193;&#25955;&#26356;&#36805;&#36895;&#65292;&#20294;&#29992;&#25143;&#35780;&#20272;&#20173;&#30456;&#24403;&#31232;&#32570;&#65292;&#24182;&#19988;&#20960;&#20046;&#27809;&#26377;&#34701;&#20837;&#35748;&#30693;&#25110;&#31038;&#20250;&#31185;&#23398;&#30340;&#20219;&#20309;&#35265;&#35299;&#12290;&#22522;&#20110;&#32508;&#21512;&#35752;&#35770;&#30340;&#26368;&#20339;&#23454;&#36341;&#65292;&#21363;&#24120;&#35265;&#27169;&#22411;&#12289;&#35774;&#35745;&#36873;&#25321;&#21644;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable AI (XAI) is widely viewed as a sine qua non for ever-expanding AI research. A better understanding of the needs of XAI users, as well as human-centered evaluations of explainable models are both a necessity and a challenge. In this paper, we explore how HCI and AI researchers conduct user studies in XAI applications based on a systematic literature review. After identifying and thoroughly analyzing 97core papers with human-based XAI evaluations over the past five years, we categorize them along the measured characteristics of explanatory methods, namely trust, understanding, usability, and human-AI collaboration performance. Our research shows that XAI is spreading more rapidly in certain application domains, such as recommender systems than in others, but that user evaluations are still rather sparse and incorporate hardly any insights from cognitive or social sciences. Based on a comprehensive discussion of best practices, i.e., common models, design choices, and measures
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22686;&#24378;&#24863;&#30693;&#30340;&#33258;&#30417;&#30563;&#21028;&#21035;&#22120;&#29992;&#20110;&#23545;&#29983;&#25104;&#25968;&#25454;&#21450;&#20854;&#22686;&#24378;&#21442;&#25968;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#25552;&#39640;&#21028;&#21035;&#22120;&#30340;&#34920;&#29616;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#26377;&#25928;&#30340; GAN &#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2205.15677</link><description>&lt;p&gt;
&#25968;&#25454;&#26377;&#25928;&#30340; GAN &#35757;&#32451;&#20013;&#30340;&#33258;&#25105;&#30417;&#30563;&#22686;&#24378;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Augmentation-Aware Self-Supervision for Data-Efficient GAN Training. (arXiv:2205.15677v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22686;&#24378;&#24863;&#30693;&#30340;&#33258;&#30417;&#30563;&#21028;&#21035;&#22120;&#29992;&#20110;&#23545;&#29983;&#25104;&#25968;&#25454;&#21450;&#20854;&#22686;&#24378;&#21442;&#25968;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#25552;&#39640;&#21028;&#21035;&#22120;&#30340;&#34920;&#29616;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#26377;&#25928;&#30340; GAN &#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#35757;&#32451;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#21028;&#21035;&#22120;&#23481;&#26131;&#36807;&#25311;&#21512;&#12290;&#20808;&#21069;&#25552;&#20986;&#30340;&#21487;&#24494;&#22686;&#24378;&#25216;&#26415;&#25913;&#21892;&#20102;GAN&#35757;&#32451;&#30340;&#25968;&#25454;&#25928;&#29575;&#12290;&#20294;&#26159;&#65292;&#22686;&#24378;&#25216;&#26415;&#38544;&#24335;&#22320;&#24341;&#20837;&#20102;&#19981;&#33391;&#19981;&#21464;&#24615;&#22240;&#32032;&#65292;&#22240;&#20026;&#23427;&#24573;&#30053;&#20102;&#30001;&#25968;&#25454;&#36716;&#25442;&#24341;&#36215;&#30340;&#26631;&#31614;&#31354;&#38388;&#35821;&#20041;&#21464;&#21270;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#21028;&#21035;&#22120;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#26368;&#32456;&#24433;&#21709;&#29983;&#25104;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#20943;&#36731;&#19981;&#21464;&#24615;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#21516;&#26102;&#32487;&#25215;&#25968;&#25454;&#22686;&#24378;&#30340;&#22909;&#22788;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#24863;&#30693;&#30340;&#33258;&#30417;&#30563;&#21028;&#21035;&#22120;&#65292;&#35813;&#21028;&#21035;&#22120;&#21487;&#20197;&#39044;&#27979;&#22686;&#24378;&#25968;&#25454;&#30340;&#21442;&#25968;&#12290;&#29305;&#21035;&#22320;&#65292;&#30495;&#23454;&#25968;&#25454;&#21644;&#29983;&#25104;&#25968;&#25454;&#30340;&#39044;&#27979;&#30446;&#26631;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38656;&#35201;&#21306;&#21035;&#24320;&#26469;&#12290;&#25105;&#20204;&#36824;&#40723;&#21169;&#29983;&#25104;&#22120;&#23545;&#25239;&#22320;&#29983;&#25104;&#20854;&#22686;&#24378;&#21442;&#25968;&#21487;&#20197;&#34987;&#21028;&#21035;&#22120;&#20934;&#30830;&#39044;&#27979;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22810;&#20449;&#24687;&#37327;&#21644;&#26356;&#39640;&#25928;&#30340;&#21028;&#21035;&#22120;&#65292;&#25552;&#39640;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#26377;&#25928;&#30340; GAN &#35757;&#32451;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training generative adversarial networks (GANs) with limited data is challenging because discriminator is prone to overfitting. Previously proposed differentiable augmentation demonstrates improved data efficiency of training GANs. However, the augmentation implicitly introduces undesired invariance to augmentation for the discriminator since it ignores the change of semantics in the label space caused by data transformation, which may limit the representation learning ability of the discriminator and ultimately affect the generative modeling performance of the generator. To mitigate the negative impact of invariance while inheriting the benefits of data augmentation, we propose a novel augmentation-aware self-supervised discriminator that predicts the augmentation parameter of the augmented data. Particularly, the prediction targets of real data and generated data are required to be distinguished since they are different during training. We further encourage the generator to adversari
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20004;&#31181;&#31639;&#27861;&#22312;&#36817;&#20284;&#22810;&#20154;&#21338;&#24328;Nash&#22343;&#34913;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#21457;&#29616;Fictitious Play&#27604;Counterfactual Regret Minimization&#26356;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2001.11165</link><description>&lt;p&gt;
Fictitious Play&#20248;&#20110;Counterfactual Regret Minimization
&lt;/p&gt;
&lt;p&gt;
Fictitious Play Outperforms Counterfactual Regret Minimization. (arXiv:2001.11165v7 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2001.11165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20004;&#31181;&#31639;&#27861;&#22312;&#36817;&#20284;&#22810;&#20154;&#21338;&#24328;Nash&#22343;&#34913;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#21457;&#29616;Fictitious Play&#27604;Counterfactual Regret Minimization&#26356;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#20004;&#31181;&#24191;&#21463;&#27426;&#36814;&#30340;&#31639;&#27861;&#8212;&#8212;Fictitious Play&#21644;Counterfactual Regret Minimization&#22312;&#36817;&#20284;&#22810;&#20154;&#21338;&#24328;Nash&#22343;&#34913;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#34429;&#28982;Counterfactual Regret Minimization&#22312;&#22810;&#20154;&#25169;&#20811;&#20013;&#21462;&#24471;&#20102;&#36739;&#22823;&#25104;&#21151;&#24182;&#34987;&#35748;&#20026;&#26159;&#26356;&#20248;&#31168;&#30340;&#31639;&#27861;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;Fictitious Play&#22312;&#21508;&#31181;&#31867;&#21035;&#21644;&#35268;&#27169;&#30340;&#28216;&#25103;&#20013;&#37117;&#21487;&#20197;&#24102;&#26469;&#26356;&#22909;&#30340;Nash&#22343;&#34913;&#36817;&#20284;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We compare the performance of two popular algorithms, fictitious play and counterfactual regret minimization, in approximating Nash equilibrium in multiplayer games. Despite recent success of counterfactual regret minimization in multiplayer poker and conjectures of its superiority, we show that fictitious play leads to improved Nash equilibrium approximation over a variety of game classes and sizes.
&lt;/p&gt;</description></item></channel></rss>