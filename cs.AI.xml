<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#23545;52&#20010;&#30495;&#23454;&#22810;&#20803;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#30340;32&#20010;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#31532;kNN&#31639;&#27861;&#22312;&#26412;&#22320;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#65292;&#32780;EIF&#31639;&#27861;&#22312;&#20840;&#29699;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26368;&#20339;&#65292;&#24314;&#35758;&#23454;&#38469;&#20351;&#29992;&#36825;&#19977;&#27454;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#30340;&#24037;&#20855;&#31665;&#12290;</title><link>http://arxiv.org/abs/2305.00735</link><description>&lt;p&gt;
&#30495;&#23454;&#25968;&#25454;&#19978;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#65306;&#25105;&#20204;&#38656;&#35201;&#22810;&#23569;&#27454;&#31639;&#27861;&#65311;
&lt;/p&gt;
&lt;p&gt;
Unsupervised anomaly detection algorithms on real-world data: how many do we need?. (arXiv:2305.00735v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00735
&lt;/p&gt;
&lt;p&gt;
&#23545;52&#20010;&#30495;&#23454;&#22810;&#20803;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#30340;32&#20010;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#31532;kNN&#31639;&#27861;&#22312;&#26412;&#22320;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#65292;&#32780;EIF&#31639;&#27861;&#22312;&#20840;&#29699;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26368;&#20339;&#65292;&#24314;&#35758;&#23454;&#38469;&#20351;&#29992;&#36825;&#19977;&#27454;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#30340;&#24037;&#20855;&#31665;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;52&#20010;&#30495;&#23454;&#22810;&#20803;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#30340;32&#20010;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#26159;&#30446;&#21069;&#26368;&#22823;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#30340;&#27604;&#36739;&#30740;&#31350;&#12290;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#65292;&#31532;kNN&#65288;&#21040;k&#20010;&#26368;&#36817;&#37051;&#23621;&#30340;&#36317;&#31163;&#65289;&#31639;&#27861;&#30340;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;&#36890;&#36807;&#21487;&#35270;&#21270;&#21644;&#32858;&#31867;&#31639;&#27861;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#20004;&#20010;&#26126;&#26174;&#30340;&#31751;&#65306;&#19968;&#20010;&#26159;&#8220;&#26412;&#22320;&#8221;&#25968;&#25454;&#38598;&#65292;&#21478;&#19968;&#20010;&#26159;&#8220;&#20840;&#23616;&#8221;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#22320;&#25968;&#25454;&#38598;&#20013;&#65292;kNN&#31639;&#27861;&#34920;&#29616;&#26368;&#20339;&#65292;&#22312;&#20840;&#23616;&#25968;&#25454;&#38598;&#20013;&#65292;EIF&#31639;&#27861;&#34920;&#29616;&#26368;&#20339;&#12290;&#32508;&#21512;&#32771;&#34385;&#31639;&#27861;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#24314;&#35758;&#23454;&#38469;&#20351;&#29992;&#36825;&#19977;&#31181;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#65288;&#20998;&#21035;&#26159;kNN&#12289;EIF&#21644;LOF&#31639;&#27861;&#65289;&#30340;&#24037;&#20855;&#31665;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study we evaluate 32 unsupervised anomaly detection algorithms on 52 real-world multivariate tabular datasets, performing the largest comparison of unsupervised anomaly detection algorithms to date. On this collection of datasets, the $k$-thNN (distance to the $k$-nearest neighbor) algorithm significantly outperforms the most other algorithms. Visualizing and then clustering the relative performance of the considered algorithms on all datasets, we identify two clear clusters: one with ``local'' datasets, and another with ``global'' datasets. ``Local'' anomalies occupy a region with low density when compared to nearby samples, while ``global'' occupy an overall low density region in the feature space. On the local datasets the $k$NN ($k$-nearest neighbor) algorithm comes out on top. On the global datasets, the EIF (extended isolation forest) algorithm performs the best. Also taking into consideration the algorithms' computational complexity, a toolbox with these three unsupervis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#23545;&#27604;&#23398;&#20064;&#21644;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#22312;&#34920;&#31034;&#21644; &#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#33258;&#30417;&#30563;&#35270;&#35273;&#21464;&#21387;&#22120;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#26102;&#33021;&#22815;&#25429;&#25417;&#26356;&#38271;&#31243;&#30340;&#20840;&#23616;&#27169;&#24335;&#24182;&#32447;&#24615;&#20998;&#31163;&#22270;&#20687;&#65292;&#20294;&#22312;&#33258;&#25105;&#20851;&#27880;&#21147;&#30340;&#21516;&#36136;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#23494;&#38598;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.00729</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#35270;&#35273;&#21464;&#21387;&#22120;&#23398;&#20064;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Do Self-Supervised Vision Transformers Learn?. (arXiv:2305.00729v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#23545;&#27604;&#23398;&#20064;&#21644;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#22312;&#34920;&#31034;&#21644; &#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#33258;&#30417;&#30563;&#35270;&#35273;&#21464;&#21387;&#22120;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#26102;&#33021;&#22815;&#25429;&#25417;&#26356;&#38271;&#31243;&#30340;&#20840;&#23616;&#27169;&#24335;&#24182;&#32447;&#24615;&#20998;&#31163;&#22270;&#20687;&#65292;&#20294;&#22312;&#33258;&#25105;&#20851;&#27880;&#21147;&#30340;&#21516;&#36136;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#23494;&#38598;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#27604;&#20102;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#21644;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#22312;&#20854;&#34920;&#31034;&#21644;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#24182;&#38416;&#36848;&#20102;&#33258;&#30417;&#30563;&#35270;&#35273;&#21464;&#21387;&#22120;&#65288;ViTs&#65289;&#30340;&#24615;&#36136;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;CL&#35757;&#32451;&#33258;&#25105;&#20851;&#27880;&#21147;&#20197;&#25429;&#25417;&#27604;MIM&#26356;&#38271;&#31243;&#30340;&#20840;&#23616;&#27169;&#24335;&#65292;&#20363;&#22914;&#29289;&#20307;&#30340;&#24418;&#29366;&#65292;&#23588;&#20854;&#26159;&#22312;ViT&#26550;&#26500;&#30340;&#21518;&#20960;&#23618;&#20013;&#12290;&#36825;&#20351;&#24471;ViTs&#33021;&#22815;&#22312;&#20854;&#34920;&#31034;&#31354;&#38388;&#20013;&#32447;&#24615;&#20998;&#31163;&#22270;&#20687;&#12290;&#20294;&#26159;&#65292;&#23427;&#20063;&#20351;&#24471;&#33258;&#25105;&#20851;&#27880;&#21147;&#23545;&#20110;&#25152;&#26377;&#26597;&#35810;&#26631;&#35760;&#21644;&#22836;&#37096;&#30340;&#21516;&#36136;&#24615;&#23849;&#28291;&#12290;&#36825;&#31181;&#33258;&#25105;&#20851;&#27880;&#21147;&#30340;&#21516;&#36136;&#24615;&#38477;&#20302;&#20102;&#34920;&#24449;&#30340;&#22810;&#26679;&#24615;&#65292;&#24694;&#21270;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#23494;&#38598;&#39044;&#27979;&#24615;&#33021;&#12288;&#12290;CL&#21033;&#29992;&#34920;&#31034;&#30340;&#20302;&#39057;&#20449;&#21495;&#65292;&#32780;MIM&#21033;&#29992;&#39640;&#39057;&#20449;&#21495;&#12290;&#30001;&#20110;&#20302;&#39057;&#21644;&#39640;&#39057;&#20449;&#24687;&#20998;&#21035;&#20195;&#34920;&#24418;&#29366;&#21644;&#36136;&#22320;&#65292;&#22240;&#27492;CL&#26356;&#21152;&#27880;&#37325;&#24418;&#29366;&#65292;&#32780;MIM&#21017;&#26356;&#21152;&#27880;&#37325;&#36136;&#22320;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a comparative study on how and why contrastive learning (CL) and masked image modeling (MIM) differ in their representations and in their performance of downstream tasks. In particular, we demonstrate that self-supervised Vision Transformers (ViTs) have the following properties: (1) CL trains self-attentions to capture longer-range global patterns than MIM, such as the shape of an object, especially in the later layers of the ViT architecture. This CL property helps ViTs linearly separate images in their representation spaces. However, it also makes the self-attentions collapse into homogeneity for all query tokens and heads. Such homogeneity of self-attention reduces the diversity of representations, worsening scalability and dense prediction performance. (2) CL utilizes the low-frequency signals of the representations, but MIM utilizes high-frequencies. Since low- and high-frequency information respectively represent shapes and textures, CL is more shape-oriented and MIM m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#33258;&#21160;&#21270;&#25193;&#23637;&#65288;FSA&#65289;&#26426;&#21046;&#26469;&#25913;&#21892;&#25968;&#25454;&#20013;&#24515;&#30340;&#33021;&#28304;&#21033;&#29992;&#25928;&#29575;&#65292;&#35813;&#26426;&#21046;&#21033;&#29992;&#28145;&#24230;&#34920;&#24449;&#23398;&#20064;&#26469;&#39044;&#27979;&#27599;&#20010;&#26381;&#21153;&#30340;&#26410;&#26469;&#36127;&#36733;&#24182;&#33258;&#21160;&#31283;&#23450;&#30456;&#24212;&#30340;&#30446;&#26631;CPU&#20351;&#29992;&#29575;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.00706</link><description>&lt;p&gt;
&#21487;&#25345;&#32493;&#21457;&#23637;&#32511;&#33394;&#25968;&#25454;&#20013;&#24515;&#30340;&#20840;&#38754;&#33258;&#21160;&#21270;&#25193;&#23637;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Full Scaling Automation for Sustainable Development of Green Data Centers. (arXiv:2305.00706v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00706
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#33258;&#21160;&#21270;&#25193;&#23637;&#65288;FSA&#65289;&#26426;&#21046;&#26469;&#25913;&#21892;&#25968;&#25454;&#20013;&#24515;&#30340;&#33021;&#28304;&#21033;&#29992;&#25928;&#29575;&#65292;&#35813;&#26426;&#21046;&#21033;&#29992;&#28145;&#24230;&#34920;&#24449;&#23398;&#20064;&#26469;&#39044;&#27979;&#27599;&#20010;&#26381;&#21153;&#30340;&#26410;&#26469;&#36127;&#36733;&#24182;&#33258;&#21160;&#31283;&#23450;&#30456;&#24212;&#30340;&#30446;&#26631;CPU&#20351;&#29992;&#29575;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#35745;&#31639;&#30340;&#24555;&#36895;&#23835;&#36215;&#23548;&#33268;&#25968;&#25454;&#20013;&#24515;&#30899;&#25490;&#25918;&#37327;&#24778;&#20154;&#22320;&#22686;&#21152;&#65292;&#29616;&#22312;&#21344;&#20840;&#29699;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#30340;&gt;3&#65285;&#65292;&#24517;&#39035;&#31435;&#21363;&#37319;&#21462;&#25514;&#26045;&#24212;&#23545;&#23427;&#20204;&#23545;&#20840;&#29699;&#27668;&#20505;&#26085;&#30410;&#22686;&#38271;&#30340;&#36127;&#25285;&#12290;&#36825;&#19968;&#21162;&#21147;&#30340;&#37325;&#28857;&#26159;&#25552;&#39640;&#36164;&#28304;&#21033;&#29992;&#29575;&#20197;&#33410;&#30465;&#30005;&#21147;&#28040;&#32791;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20840;&#38754;&#33258;&#21160;&#21270;&#25193;&#23637;&#65288;FSA&#65289;&#26426;&#21046;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#20113;&#35745;&#31639;&#38598;&#32676;&#20013;&#21160;&#24577;&#22320;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#24037;&#20316;&#36127;&#36733;&#65292;&#20351;&#25968;&#25454;&#20013;&#24515;&#20013;&#30340;&#38598;&#32676;&#20445;&#25345;&#20854;&#25152;&#38656;&#30340;CPU&#21033;&#29992;&#29575;&#30446;&#26631;&#65292;&#20174;&#32780;&#25913;&#21892;&#33021;&#28304;&#25928;&#29575;&#12290;FSA&#21033;&#29992;&#28145;&#24230;&#34920;&#24449;&#23398;&#20064;&#30340;&#23041;&#21147;&#26469;&#20934;&#30830;&#39044;&#27979;&#27599;&#20010;&#26381;&#21153;&#30340;&#26410;&#26469;&#24037;&#20316;&#36127;&#36733;&#65292;&#24182;&#33258;&#21160;&#31283;&#23450;&#30456;&#24212;&#30340;&#30446;&#26631;CPU&#20351;&#29992;&#29575;&#27700;&#24179;&#65292;&#19981;&#20687;&#20043;&#21069;&#30340;&#33258;&#21160;&#25193;&#23637;&#26041;&#27861;&#65292;&#22914;Autopilot&#25110;FIRM&#65292;&#38656;&#35201;&#20351;&#29992;&#32479;&#35745;&#27169;&#22411;&#21644;&#19987;&#23478;&#30693;&#35782;&#26469;&#35843;&#25972;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid rise in cloud computing has resulted in an alarming increase in data centers' carbon emissions, which now accounts for &gt;3% of global greenhouse gas emissions, necessitating immediate steps to combat their mounting strain on the global climate. An important focus of this effort is to improve resource utilization in order to save electricity usage. Our proposed Full Scaling Automation (FSA) mechanism is an effective method of dynamically adapting resources to accommodate changing workloads in large-scale cloud computing clusters, enabling the clusters in data centers to maintain their desired CPU utilization target and thus improve energy efficiency. FSA harnesses the power of deep representation learning to accurately predict the future workload of each service and automatically stabilize the corresponding target CPU usage level, unlike the previous autoscaling methods, such as Autopilot or FIRM, that need to adjust computing resources with statistical models and expert knowle
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#21046;&#23450;&#30340;&#26679;&#26412;&#26377;&#25928;&#12289;&#22343;&#34913;&#35745;&#31639;&#21644;&#23616;&#37096;&#30417;&#25511;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22797;&#26434;&#24230;&#19978;&#19979;&#30028;&#21644;&#31639;&#27861;&#65292;&#24182;&#21457;&#29616;&#22810;&#26234;&#33021;&#20307;&#24773;&#20917;&#19979;&#21487;&#33021;&#21576;&#25351;&#25968;&#32423;&#38590;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.00684</link><description>&lt;p&gt;
&#20851;&#20110;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#21046;&#23450;&#30340;&#22797;&#26434;&#24615;&#65306;&#20174;&#28216;&#25103;&#23398;&#20064;&#21040;&#23616;&#37096;&#30417;&#25511;&#12290;
&lt;/p&gt;
&lt;p&gt;
On the Complexity of Multi-Agent Decision Making: From Learning in Games to Partial Monitoring. (arXiv:2305.00684v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#21046;&#23450;&#30340;&#26679;&#26412;&#26377;&#25928;&#12289;&#22343;&#34913;&#35745;&#31639;&#21644;&#23616;&#37096;&#30417;&#25511;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22797;&#26434;&#24230;&#19978;&#19979;&#30028;&#21644;&#31639;&#27861;&#65292;&#24182;&#21457;&#29616;&#22810;&#26234;&#33021;&#20307;&#24773;&#20917;&#19979;&#21487;&#33021;&#21576;&#25351;&#25968;&#32423;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#29702;&#35299;&#32467;&#26500;&#26465;&#20214;&#21644;&#31639;&#27861;&#21407;&#21017;&#20250;&#23548;&#33268;&#21738;&#20123;&#26679;&#26412;&#26377;&#25928;&#30340;&#23398;&#20064;&#20445;&#35777;&#65292;&#24182;&#19988;&#22312;&#25105;&#20204;&#20174;&#23569;&#25968;&#26234;&#33021;&#20307;&#36716;&#31227;&#21040;&#22810;&#25968;&#26234;&#33021;&#20307;&#26102;&#65292;&#36825;&#20123;&#32771;&#34385;&#22914;&#20309;&#21457;&#29983;&#21464;&#21270;&#12290;&#26412;&#25991;&#22312;&#22810;&#26234;&#33021;&#20307;&#20114;&#21160;&#20915;&#31574;&#30340;&#19968;&#33324;&#26694;&#26550;&#19979;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#21253;&#25324;&#20855;&#26377;&#20989;&#25968;&#36924;&#36817;&#30340;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#21644;&#24102;&#26377;&#36172;&#24466;&#21453;&#39304;&#30340;&#27491;&#21017;&#24335;&#21338;&#24328;&#12290;&#25105;&#20204;&#20851;&#27880;&#22343;&#34913;&#35745;&#31639;&#65292;&#20854;&#20013;&#38598;&#20013;&#24335;&#23398;&#20064;&#31639;&#27861;&#26088;&#22312;&#36890;&#36807;&#25511;&#21046;&#19982;&#26410;&#30693;&#29615;&#22659;&#20132;&#20114;&#30340;&#22810;&#20010;&#26234;&#33021;&#20307;&#26469;&#35745;&#31639;&#22343;&#34913;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#65306;1. &#25105;&#20204;&#22522;&#20110;&#30001;Foster&#31561;&#20154;&#65288;2021&#65289;&#22312;&#21333;&#26234;&#33021;&#20307;&#24773;&#20917;&#19979;&#24341;&#20837;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#26041;&#27861;&#8212;&#20915;&#31574;-&#20272;&#35745;&#31995;&#25968;&#65292;&#20026;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#21046;&#23450;&#20102;&#26368;&#20339;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#12290;&#19982;&#21333;&#26234;&#33021;&#20307;&#24773;&#20917;&#19979;&#30340;&#26368;&#20339;&#32467;&#26524;&#30456;&#27604;&#65292;&#25105;&#20204;&#34920;&#26126;&#22810;&#26234;&#33021;&#20307;&#24773;&#20917;&#19979;&#30340;&#38382;&#39064;&#22312;&#26234;&#33021;&#20307;&#25968;&#37327;&#26041;&#38754;&#21487;&#33021;&#21576;&#25351;&#25968;&#32423;&#38590;&#24230;&#12290;2. &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#20855;&#26377;&#20989;&#25968;&#36924;&#36817;&#30340;&#22823;&#22411;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#36827;&#34892;&#39640;&#25928;&#30340;&#22343;&#34913;&#35745;&#31639;&#65292;&#35813;&#31639;&#27861;&#22522;&#20110;&#20048;&#35266;&#38236;&#20687;&#19979;&#38477;&#27861;&#30340;&#21407;&#29702;&#12290;&#25105;&#20204;&#20026;&#25105;&#20204;&#30340;&#26041;&#27861;&#24314;&#31435;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#36825;&#20123;&#30028;&#38480;&#25913;&#36827;&#20102;&#20808;&#21069;&#22312;&#24102;&#26377;&#36172;&#24466;&#21453;&#39304;&#30340;&#28216;&#25103;&#20013;&#30340;&#24037;&#20316;&#12290;3. &#25105;&#20204;&#32771;&#34385;&#23616;&#37096;&#30417;&#25511;&#65292;&#36825;&#26159;&#19968;&#31181;&#21453;&#39304;&#31867;&#22411;&#65292;&#20854;&#20013;&#20915;&#31574;&#21046;&#23450;&#32773;&#21482;&#35266;&#23519;&#26234;&#33021;&#20307;&#21160;&#20316;&#30340;&#25688;&#35201;&#20449;&#24687;&#32780;&#19981;&#26159;&#20840;&#37096;&#20449;&#24687;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#19968;&#20010;&#21464;&#20307;&#65292;&#35813;&#31639;&#27861;&#23454;&#29616;&#20102;&#27492;&#35774;&#32622;&#30340;&#25910;&#25947;&#36895;&#24230;&#26368;&#20248;&#65292;&#19982;&#20808;&#21069;&#24037;&#20316;&#24314;&#31435;&#30340;&#19979;&#30028;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
A central problem in the theory of multi-agent reinforcement learning (MARL) is to understand what structural conditions and algorithmic principles lead to sample-efficient learning guarantees, and how these considerations change as we move from few to many agents. We study this question in a general framework for interactive decision making with multiple agents, encompassing Markov games with function approximation and normal-form games with bandit feedback. We focus on equilibrium computation, in which a centralized learning algorithm aims to compute an equilibrium by controlling multiple agents that interact with an unknown environment. Our main contributions are:  - We provide upper and lower bounds on the optimal sample complexity for multi-agent decision making based on a multi-agent generalization of the Decision-Estimation Coefficient, a complexity measure introduced by Foster et al. (2021) in the single-agent counterpart to our setting. Compared to the best results for the sin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22320;&#24418;&#24863;&#30693;&#30340;&#36816;&#21160;&#23398;&#27169;&#22411;&#23398;&#20064;&#19982;&#39044;&#27979;&#25511;&#21046;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#21487;&#38752;&#30340; 6 &#33258;&#30001;&#24230;&#36816;&#21160;&#39044;&#27979;&#65292;&#24182;&#21487;&#22312;&#26080;&#38656;&#35757;&#32451;&#26102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#25509;&#35302;&#21147;&#20272;&#35745;&#65292;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#36234;&#37326;&#22330;&#22320;&#30340;&#23433;&#20840;&#19982;&#40065;&#26834;&#33258;&#20027;&#39550;&#39542;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.00676</link><description>&lt;p&gt;
&#38754;&#21521;&#33258;&#20027;&#36234;&#37326;&#25289;&#21147;&#36187;&#30340;&#22320;&#24418;&#24863;&#30693;&#36816;&#21160;&#23398;&#27169;&#22411;&#23398;&#20064;&#19982;&#39044;&#27979;&#25511;&#21046;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Learning Terrain-Aware Kinodynamic Model for Autonomous Off-Road Rally Driving With Model Predictive Path Integral Control. (arXiv:2305.00676v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00676
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22320;&#24418;&#24863;&#30693;&#30340;&#36816;&#21160;&#23398;&#27169;&#22411;&#23398;&#20064;&#19982;&#39044;&#27979;&#25511;&#21046;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#21487;&#38752;&#30340; 6 &#33258;&#30001;&#24230;&#36816;&#21160;&#39044;&#27979;&#65292;&#24182;&#21487;&#22312;&#26080;&#38656;&#35757;&#32451;&#26102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#25509;&#35302;&#21147;&#20272;&#35745;&#65292;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#36234;&#37326;&#22330;&#22320;&#30340;&#23433;&#20840;&#19982;&#40065;&#26834;&#33258;&#20027;&#39550;&#39542;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36234;&#37326;&#29615;&#22659;&#19979;&#36827;&#34892;&#39640;&#36895;&#33258;&#20027;&#39550;&#39542;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#65292;&#20294;&#30001;&#20110;&#36710;&#36742;&#19982;&#22320;&#24418;&#20132;&#20114;&#30340;&#22797;&#26434;&#24615;&#65292;&#20063;&#23384;&#22312;&#19968;&#23450;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#31181;&#29615;&#22659;&#19979;&#65292;&#36710;&#36742;&#39044;&#27979;&#33258;&#36523;&#36816;&#21160;&#24182;&#26681;&#25454;&#29615;&#22659;&#21464;&#21270;&#65292;&#20363;&#22914;&#22320;&#24418;&#39640;&#24046;&#30340;&#21464;&#21270;&#65292;&#20027;&#21160;&#35843;&#25972;&#20854;&#25511;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#23398;&#20064;&#22320;&#24418;&#24863;&#30693;&#30340;&#36816;&#21160;&#23398;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20197;&#26412;&#20307;&#24863;&#30693;&#21644;&#22806;&#37096;&#24863;&#30693;&#20449;&#24687;&#20026;&#26465;&#20214;&#12290;&#35813;&#27169;&#22411;&#21487;&#29983;&#25104;&#21487;&#38752;&#30340; 6 &#33258;&#30001;&#24230;&#36816;&#21160;&#39044;&#27979;&#65292;&#24182;&#21487;&#22312;&#26080;&#38656;&#35757;&#32451;&#26102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#25509;&#35302;&#21147;&#20272;&#35745;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#36866;&#24403;&#30340;&#20195;&#20215;&#20989;&#25968;&#35774;&#35745;&#21487;&#20197;&#29983;&#25104;&#23433;&#20840;&#19988;&#40065;&#26834;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#65292;&#24809;&#32602;&#20855;&#26377;&#19981;&#31283;&#23450;&#36816;&#21160;&#12289;&#19981;&#23433;&#20840;&#20132;&#20114;&#21644;&#39640;&#19981;&#30830;&#23450;&#24615;&#34893;&#29983;&#30340;&#26679;&#26412;&#36712;&#36857;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-speed autonomous driving in off-road environments has immense potential for various applications, but it also presents challenges due to the complexity of vehicle-terrain interactions. In such environments, it is crucial for the vehicle to predict its motion and adjust its controls proactively in response to environmental changes, such as variations in terrain elevation. To this end, we propose a method for learning terrain-aware kinodynamic model which is conditioned on both proprioceptive and exteroceptive information. The proposed model generates reliable predictions of 6-degree-of-freedom motion and can even estimate contact interactions without requiring ground truth force data during training. This enables the design of a safe and robust model predictive controller through appropriate cost function design which penalizes sampled trajectories with unstable motion, unsafe interactions, and high levels of uncertainty derived from the model. We demonstrate the effectiveness of o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SkeAttnCLR&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#23558;&#23616;&#37096;&#30456;&#20284;&#24615;&#19982;&#20840;&#23616;&#29305;&#24449;&#25972;&#21512;&#21040;&#39592;&#26550;&#21160;&#20316;&#34920;&#31034;&#20013;&#12290;&#20854;&#20013;&#37319;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#25513;&#34109;&#23398;&#20064;&#36719;&#25513;&#34109;&#29305;&#24449;&#65292;&#23558;&#30456;&#20284;&#30340;&#23616;&#37096;&#29305;&#24449;&#32039;&#23494;&#38752;&#36817;&#12290;&#36890;&#36807;&#21033;&#29992;&#20840;&#23616;&#29305;&#24449;&#25193;&#23637;&#23545;&#27604;&#37197;&#23545;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#27604;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.00666</link><description>&lt;p&gt;
&#22522;&#20110;&#37096;&#20214;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#30340;&#33258;&#30417;&#30563;&#21160;&#20316;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Part Aware Contrastive Learning for Self-Supervised Action Recognition. (arXiv:2305.00666v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SkeAttnCLR&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#23558;&#23616;&#37096;&#30456;&#20284;&#24615;&#19982;&#20840;&#23616;&#29305;&#24449;&#25972;&#21512;&#21040;&#39592;&#26550;&#21160;&#20316;&#34920;&#31034;&#20013;&#12290;&#20854;&#20013;&#37319;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#25513;&#34109;&#23398;&#20064;&#36719;&#25513;&#34109;&#29305;&#24449;&#65292;&#23558;&#30456;&#20284;&#30340;&#23616;&#37096;&#29305;&#24449;&#32039;&#23494;&#38752;&#36817;&#12290;&#36890;&#36807;&#21033;&#29992;&#20840;&#23616;&#29305;&#24449;&#25193;&#23637;&#23545;&#27604;&#37197;&#23545;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#27604;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#19982;&#39592;&#39612;&#24207;&#21015;&#22312;&#33258;&#30417;&#30563;&#21160;&#20316;&#35782;&#21035;&#26041;&#38754;&#33719;&#24471;&#20102;&#26174;&#33879;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;SkeAttnCLR&#65292;&#29992;&#20110;&#39592;&#39612;&#34920;&#31034;&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#23558;&#23616;&#37096;&#30456;&#20284;&#24615;&#21644;&#20840;&#23616;&#29305;&#24449;&#38598;&#25104;&#21040;&#22522;&#20110;&#39592;&#26550;&#30340;&#21160;&#20316;&#34920;&#31034;&#20013;&#65292;&#36890;&#36807;&#22810;&#22836;&#27880;&#24847;&#21147;&#25513;&#34109;&#27169;&#22359;&#23398;&#20064;&#36719;&#25513;&#34109;&#29305;&#24449;&#65292;&#21387;&#21046;&#38750;&#26174;&#33879;&#37096;&#20301;&#29305;&#24449;&#21516;&#26102;&#31361;&#20986;&#26174;&#33879;&#37096;&#20301;&#29305;&#24449;&#65292;&#20174;&#32780;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#23558;&#30456;&#20284;&#30340;&#23616;&#37096;&#29305;&#24449;&#32039;&#23494;&#38752;&#36817;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21033;&#29992;&#20840;&#23616;&#29305;&#24449;&#25193;&#23637;&#26174;&#33879;&#21644;&#38750;&#26174;&#33879;&#29305;&#24449;&#30340;&#23545;&#27604;&#37197;&#23545;&#65292;&#33719;&#24471;&#20102;&#20805;&#36275;&#30340;&#23545;&#27604;&#37197;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, remarkable results have been achieved in self-supervised action recognition using skeleton sequences with contrastive learning. It has been observed that the semantic distinction of human action features is often represented by local body parts, such as legs or hands, which are advantageous for skeleton-based action recognition. This paper proposes an attention-based contrastive learning framework for skeleton representation learning, called SkeAttnCLR, which integrates local similarity and global features for skeleton-based action representations. To achieve this, a multi-head attention mask module is employed to learn the soft attention mask features from the skeletons, suppressing non-salient local features while accentuating local salient features, thereby bringing similar local features closer in the feature space. Additionally, ample contrastive pairs are generated by expanding contrastive pairs based on salient and non-salient features with global features, whic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#33258;&#21160;&#34920;&#24449;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#33719;&#24471;&#20445;&#30041;&#36716;&#25442;&#32467;&#26500;&#30340;&#34920;&#31034;&#24418;&#24335;&#24182;&#25429;&#25417;&#29366;&#24577;&#35775;&#38382;&#30340;&#30456;&#23545;&#39057;&#29575;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#36716;&#31227;&#30697;&#38453;&#65292;&#21487;&#20197;&#21033;&#29992;&#28145;&#24230;&#32593;&#32476;&#65292;&#36866;&#29992;&#20110;&#37096;&#20998;&#21487;&#35266;&#23519;&#39046;&#22495;&#65292;&#24182;&#19988;&#22312;&#22810;&#20219;&#21153;&#35774;&#32622;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.00654</link><description>&lt;p&gt;
&#20351;&#29992;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#34920;&#24449;&#23398;&#20064;&#21644;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Representations and Exploration for Deep Reinforcement Learning using Singular Value Decomposition. (arXiv:2305.00654v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#33258;&#21160;&#34920;&#24449;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#33719;&#24471;&#20445;&#30041;&#36716;&#25442;&#32467;&#26500;&#30340;&#34920;&#31034;&#24418;&#24335;&#24182;&#25429;&#25417;&#29366;&#24577;&#35775;&#38382;&#30340;&#30456;&#23545;&#39057;&#29575;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#36716;&#31227;&#30697;&#38453;&#65292;&#21487;&#20197;&#21033;&#29992;&#28145;&#24230;&#32593;&#32476;&#65292;&#36866;&#29992;&#20110;&#37096;&#20998;&#21487;&#35266;&#23519;&#39046;&#22495;&#65292;&#24182;&#19988;&#22312;&#22810;&#20219;&#21153;&#35774;&#32622;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#29616;&#23398;&#20064;&#21644;&#25506;&#32034;&#26159;&#20219;&#20309;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#25152;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#26469;&#33719;&#24471;&#20445;&#30041;&#22495;&#20013;&#28508;&#22312;&#36716;&#25442;&#32467;&#26500;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#34920;&#31034;&#24418;&#24335;&#36824;&#25429;&#25417;&#20102;&#29366;&#24577;&#35775;&#38382;&#30340;&#30456;&#23545;&#39057;&#29575;&#65292;&#20174;&#32780;&#20813;&#36153;&#25552;&#20379;&#20102;&#20266;&#35745;&#25968;&#30340;&#20272;&#35745;&#12290;&#20026;&#20102;&#23558;&#36825;&#31181;&#20998;&#35299;&#26041;&#27861;&#25512;&#24191;&#21040;&#22823;&#35268;&#27169;&#22495;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#24314;&#31435;&#36716;&#31227;&#30697;&#38453;&#65292;&#21487;&#20197;&#21033;&#29992;&#28145;&#24230;&#32593;&#32476;&#65292;&#20063;&#20801;&#35768;&#23567;&#25209;&#37327;&#35757;&#32451;&#30340;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#39044;&#27979;&#29366;&#24577;&#34920;&#31034;&#20013;&#21560;&#21462;&#28789;&#24863;&#65292;&#24182;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;&#20998;&#35299;&#26041;&#27861;&#21040;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#29615;&#22659;&#12290;&#36890;&#36807;&#23545;&#37096;&#20998;&#21487;&#35266;&#23519;&#39046;&#22495;&#30340;&#22810;&#20219;&#21153;&#35774;&#32622;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#22312;DM-Lab-30&#29615;&#22659;&#20013;&#23398;&#20064;&#26377;&#29992;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation learning and exploration are among the key challenges for any deep reinforcement learning agent. In this work, we provide a singular value decomposition based method that can be used to obtain representations that preserve the underlying transition structure in the domain. Perhaps interestingly, we show that these representations also capture the relative frequency of state visitations, thereby providing an estimate for pseudo-counts for free. To scale this decomposition method to large-scale domains, we provide an algorithm that never requires building the transition matrix, can make use of deep networks, and also permits mini-batch training. Further, we draw inspiration from predictive state representations and extend our decomposition method to partially observable environments. With experiments on multi-task settings with partially observable domains, we show that the proposed method can not only learn useful representation on DM-Lab-30 environments (that have inputs
&lt;/p&gt;</description></item><item><title>PCG-KT&#26159;&#19968;&#31181;&#26032;&#30340;&#20869;&#23481;&#29983;&#25104;&#26041;&#27861;&#21644;&#26694;&#26550;&#65292;&#20854;&#36890;&#36807;&#30693;&#35782;&#36716;&#25442;&#26469;&#25913;&#21464;&#24182;&#29983;&#25104;&#26032;&#30340;&#20869;&#23481;&#65292;&#36825;&#31181;&#26041;&#27861;&#36866;&#29992;&#20110;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#25110;&#20840;&#26032;&#30340;&#28216;&#25103;&#12290;</title><link>http://arxiv.org/abs/2305.00644</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#36716;&#25442;&#30340;&#27969;&#31243;&#20869;&#23481;&#29983;&#25104;(PCG-KT)
&lt;/p&gt;
&lt;p&gt;
Procedural Content Generation via Knowledge Transformation (PCG-KT). (arXiv:2305.00644v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00644
&lt;/p&gt;
&lt;p&gt;
PCG-KT&#26159;&#19968;&#31181;&#26032;&#30340;&#20869;&#23481;&#29983;&#25104;&#26041;&#27861;&#21644;&#26694;&#26550;&#65292;&#20854;&#36890;&#36807;&#30693;&#35782;&#36716;&#25442;&#26469;&#25913;&#21464;&#24182;&#29983;&#25104;&#26032;&#30340;&#20869;&#23481;&#65292;&#36825;&#31181;&#26041;&#27861;&#36866;&#29992;&#20110;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#25110;&#20840;&#26032;&#30340;&#28216;&#25103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#30693;&#35782;&#36716;&#25442;&#30340;&#27969;&#31243;&#20869;&#23481;&#29983;&#25104;(PCG-KT)&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;PCG&#26041;&#27861;&#21644;&#26694;&#26550;&#65292;&#20854;&#20013;&#20869;&#23481;&#29983;&#25104;&#36890;&#36807;&#30693;&#35782;&#36716;&#25442;&#26469;&#23454;&#29616;&#65292;&#21363;&#23558;&#20174;&#19968;&#20010;&#39046;&#22495;&#33719;&#24471;&#30340;&#30693;&#35782;&#36716;&#21464;&#20026;&#21478;&#19968;&#20010;&#39046;&#22495;&#21487;&#20197;&#24212;&#29992;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#30001;&#26368;&#36817;&#35768;&#22810;PCG&#30740;&#31350;&#30340;&#38656;&#27714;&#21644;&#25361;&#25112;&#39537;&#21160;&#32780;&#26469;&#65292;&#36825;&#20123;&#30740;&#31350;&#20391;&#37325;&#20110;&#36890;&#36807;&#25913;&#21464;&#27966;&#29983;&#30340;&#30693;&#35782;&#26469;&#29983;&#25104;&#26032;&#30340;&#20869;&#23481;&#12290;&#20363;&#22914;&#65292;&#38024;&#23545;&#26576;&#20010;&#28216;&#25103;&#30340;&#27169;&#22411;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#20197;&#36866;&#24212;&#21478;&#19968;&#20010;&#28216;&#25103;&#30340;&#20869;&#23481;&#65292;&#25110;&#32773;&#37325;&#26032;&#32452;&#21512;&#19981;&#21516;&#30340;&#29983;&#25104;&#20998;&#24067;&#20197;&#34701;&#21512;&#20004;&#20010;&#25110;&#26356;&#22810;&#28216;&#25103;&#30340;&#20869;&#23481;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#20135;&#29983;&#26159;&#30001;&#20110;PCG&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;(PCGML)&#30340;&#23616;&#38480;&#24615;&#65292;&#20363;&#22914;&#20026;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#30340;&#28216;&#25103;&#29983;&#25104;&#29983;&#25104;&#27169;&#22411;&#21644;&#20026;&#20840;&#26032;&#30340;&#28216;&#25103;&#29983;&#25104;&#20869;&#23481;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#24402;&#31867;&#20026;PCG-KT&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the concept of Procedural Content Generation via Knowledge Transformation (PCG-KT), a new lens and framework for characterizing PCG methods and approaches in which content generation is enabled by the process of knowledge transformation -- transforming knowledge derived from one domain in order to apply it in another. Our work is motivated by a substantial number of recent PCG works that focus on generating novel content via repurposing derived knowledge. Such works have involved, for example, performing transfer learning on models trained on one game's content to adapt to another game's content, as well as recombining different generative distributions to blend the content of two or more games. Such approaches arose in part due to limitations in PCG via Machine Learning (PCGML) such as producing generative models for games lacking training data and generating content for entirely new games. In this paper, we categorize such approaches under this new lens of PCG-KT by offe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#35299;&#30721;&#25552;&#39640;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32463;&#36807;&#26657;&#20934;&#30340;&#33258;&#21160;&#26631;&#20934;&#25506;&#32034;&#25512;&#29702;&#25628;&#32034;&#31354;&#38388;&#65292;&#20351;&#25628;&#32034;&#33021;&#22815;&#20135;&#29983;&#26356;&#39640;&#36136;&#37327;&#30340;&#26368;&#32456;&#39044;&#27979;&#32467;&#26524;&#65307;&#20351;&#29992;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#30340;&#38543;&#26426;&#26463;&#25628;&#32034;&#22312;&#20135;&#29983;&#25512;&#29702;&#38142;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#24179;&#34913;&#26435;&#34913;&#65292;&#36866;&#24212;&#22810;&#25968;&#25237;&#31080;&#65292;&#24182;&#19988;&#21487;&#20197;&#20934;&#30830;&#21028;&#26029;&#36923;&#36753;&#38169;&#35823;&#65292;&#25552;&#39640;&#19968;&#33268;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00633</link><description>&lt;p&gt;
&#20998;&#35299;&#22686;&#24378;&#25512;&#29702;&#30340;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Decomposition Enhances Reasoning via Self-Evaluation Guided Decoding. (arXiv:2305.00633v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#35299;&#30721;&#25552;&#39640;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32463;&#36807;&#26657;&#20934;&#30340;&#33258;&#21160;&#26631;&#20934;&#25506;&#32034;&#25512;&#29702;&#25628;&#32034;&#31354;&#38388;&#65292;&#20351;&#25628;&#32034;&#33021;&#22815;&#20135;&#29983;&#26356;&#39640;&#36136;&#37327;&#30340;&#26368;&#32456;&#39044;&#27979;&#32467;&#26524;&#65307;&#20351;&#29992;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#30340;&#38543;&#26426;&#26463;&#25628;&#32034;&#22312;&#20135;&#29983;&#25512;&#29702;&#38142;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#24179;&#34913;&#26435;&#34913;&#65292;&#36866;&#24212;&#22810;&#25968;&#25237;&#31080;&#65292;&#24182;&#19988;&#21487;&#20197;&#20934;&#30830;&#21028;&#26029;&#36923;&#36753;&#38169;&#35823;&#65292;&#25552;&#39640;&#19968;&#33268;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#26463;&#25628;&#32034;&#32467;&#21512;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#32463;&#36807;&#26657;&#20934;&#30340;&#33258;&#21160;&#26631;&#20934;&#25506;&#32034;&#25512;&#29702;&#25628;&#32034;&#31354;&#38388;&#12290;&#36825;&#20351;&#24471;&#26377;&#25928;&#25628;&#32034;&#33021;&#22815;&#20135;&#29983;&#26356;&#39640;&#36136;&#37327;&#30340;&#26368;&#32456;&#39044;&#27979;&#32467;&#26524;&#12290;&#20351;&#29992;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#30340;&#38543;&#26426;&#26463;&#25628;&#32034;&#65292;&#25105;&#20204;&#22312;&#20135;&#29983;&#25512;&#29702;&#38142;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#24179;&#34913;&#26435;&#34913;&#65292;&#20174;&#32780;&#33021;&#22815;&#36866;&#24212;&#22810;&#25968;&#25237;&#31080;&#65292;&#24182;&#22312;GSM8K&#12289;AQUA&#21644;StrategyQA&#22522;&#20934;&#27979;&#35797;&#20013;&#20197;&#23569;&#37327;&#31034;&#20363;&#20934;&#30830;&#24615;&#20998;&#21035;&#36229;&#36234;&#23545;&#24212;&#30340;Codex-backboned&#22522;&#32447;$6.34\%$&#12289;$9.56\%$&#21644;$5.46\%$&#12290;&#23545;&#25105;&#20204;&#30340;&#20998;&#35299;&#24335;&#25512;&#29702;&#20998;&#26512;&#21457;&#29616;&#65292;&#23427;&#21487;&#20197;&#25351;&#20986;&#36923;&#36753;&#38169;&#35823;&#24182;&#23548;&#33268;&#26356;&#39640;&#30340;&#19968;&#33268;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an effective prompting approach that integrates self-evaluation guidance through stochastic beam search. Our approach explores the reasoning search space using a well-calibrated automatic criterion. This enables an efficient search to produce higher-quality final predictions. With the self-evaluation guided stochastic beam search, we also balance the quality--diversity trade-off in the generation of reasoning chains. This allows our approach to adapt well with majority voting and surpass the corresponding Codex-backboned baselines by $6.34\%$, $9.56\%$, and $5.46\%$ on the GSM8K, AQUA, and StrategyQA benchmarks, respectively, in few-shot accuracy. Analysis of our decompositional reasoning finds it pinpoints logic failures and leads to higher consistency and robustness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#27604;&#23398;&#20064;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#30340;&#33410;&#28857;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#21015;&#22788;&#29702;&#23884;&#20837;&#30697;&#38453;&#21462;&#20195;&#21516;&#34892;&#22788;&#29702;&#65292;&#22312;&#25552;&#39640;&#32467;&#26524;&#23884;&#20837;&#36136;&#37327;&#21644;&#35757;&#32451;&#26102;&#38388;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#20102;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.00623</link><description>&lt;p&gt;
&#29992;&#20110;&#33410;&#28857;&#34920;&#31034;&#30340;&#23545;&#27604;&#23398;&#20064;&#31616;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Simplified Framework for Contrastive Learning for Node Representations. (arXiv:2305.00623v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#27604;&#23398;&#20064;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#30340;&#33410;&#28857;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#21015;&#22788;&#29702;&#23884;&#20837;&#30697;&#38453;&#21462;&#20195;&#21516;&#34892;&#22788;&#29702;&#65292;&#22312;&#25552;&#39640;&#32467;&#26524;&#23884;&#20837;&#36136;&#37327;&#21644;&#35757;&#32451;&#26102;&#38388;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#20102;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#27604;&#23398;&#20064;&#24050;&#32463;&#34987;&#35777;&#26126;&#20026;&#19968;&#31181;&#25552;&#21462;&#20016;&#23500;&#22810;&#26679;&#30340;&#25968;&#25454;&#34920;&#31034;&#30340;&#26377;&#21147;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#12290;&#24191;&#20041;&#19978;&#35762;&#65292;&#23545;&#27604;&#23398;&#20064;&#20381;&#36182;&#20110;&#25968;&#25454;&#22686;&#24378;&#26041;&#26696;&#29983;&#25104;&#36755;&#20837;&#25968;&#25454;&#30340;&#20004;&#20010;&#29256;&#26412;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#24402;&#19968;&#21270;&#28201;&#24230;&#32553;&#25918;&#20132;&#21449;&#29109;&#25439;&#22833;&#65288;NT-Xent&#65289;&#26469;&#23398;&#20064;&#20302;&#32500;&#24230;&#34920;&#31034;&#20197;&#35782;&#21035;&#23545;&#24212;&#20110;&#21516;&#19968;&#21407;&#22987;&#23454;&#20307;&#30340;&#22686;&#24378;&#26679;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#23545;&#27604;&#23398;&#20064;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#22312;&#22270;&#20013;&#23884;&#20837;&#33410;&#28857;&#30340;&#28508;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#21015;&#22788;&#29702;&#23884;&#20837;&#30697;&#38453;&#65292;&#32780;&#19981;&#26159;&#21516;&#34892;&#22788;&#29702;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#32467;&#26524;&#23884;&#20837;&#30340;&#36136;&#37327;&#21644;&#35757;&#32451;&#26102;&#38388;&#65292;&#32780;&#21516;&#34892;&#22788;&#29702;&#26159;&#22823;&#22810;&#25968;&#21516;&#34892;&#26041;&#27861;&#37319;&#29992;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLPs&#65289;&#12290;&#36825;&#31181;&#20462;&#25913;&#21487;&#25552;&#39640;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning has recently established itself as a powerful self-supervised learning framework for extracting rich and versatile data representations. Broadly speaking, contrastive learning relies on a data augmentation scheme to generate two versions of the input data and learns low-dimensional representations by maximizing a normalized temperature-scaled cross entropy loss (NT-Xent) to identify augmented samples corresponding to the same original entity. In this paper, we investigate the potential of deploying contrastive learning in combination with Graph Neural Networks for embedding nodes in a graph. Specifically, we show that the quality of the resulting embeddings and training time can be significantly improved by a simple column-wise postprocessing of the embedding matrix, instead of the row-wise postprocessing via multilayer perceptrons (MLPs) that is adopted by the majority of peer methods. This modification yields improvements in downstream classification tasks of up 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Consolidator &#30340; mergeable adapter with grouped connections for visual adaptation&#65292;&#20419;&#36827;&#20102;&#35270;&#35273; transformer &#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#23454;&#29616;&#20102;&#22810;&#20010;&#22270;&#20687;&#20998;&#31867;&#21644;&#23545;&#35937;&#26816;&#27979;&#20219;&#21153;&#30340;&#26368;&#26032;&#36716;&#31227;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.00603</link><description>&lt;p&gt;
Consolidator: &#34701;&#21512;&#36830;&#25509;&#30340;&#21487;&#21512;&#24182;&#36866;&#37197;&#22120;&#65292;&#29992;&#20110;&#35270;&#35273;&#39046;&#22495;&#30340;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Consolidator: Mergeable Adapter with Grouped Connections for Visual Adaptation. (arXiv:2305.00603v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00603
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Consolidator &#30340; mergeable adapter with grouped connections for visual adaptation&#65292;&#20419;&#36827;&#20102;&#35270;&#35273; transformer &#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#23454;&#29616;&#20102;&#22810;&#20010;&#22270;&#20687;&#20998;&#31867;&#21644;&#23545;&#35937;&#26816;&#27979;&#20219;&#21153;&#30340;&#26368;&#26032;&#36716;&#31227;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;transformer &#20316;&#20026;&#35270;&#35273;&#29305;&#24449;&#25552;&#21462;&#22120;&#34920;&#29616;&#20986;&#36229;&#36234;&#20256;&#32479;&#21367;&#31215;&#27169;&#22411;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35270;&#35273; transformer &#30340;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#21151;&#20110;&#20854;&#23481;&#32435;&#22823;&#37327;&#21442;&#25968;&#30340;&#33021;&#21147;&#12290;&#36825;&#23548;&#33268;&#23558;&#22823;&#22411;&#27169;&#22411;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#38754;&#20020;&#26032;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; "Consolidator" &#30340;&#34701;&#21512;&#36830;&#25509;&#30340;&#21487;&#21512;&#24182;&#36866;&#37197;&#22120;&#65292;&#29992;&#20110;&#35270;&#35273;&#39046;&#22495;&#30340;&#33258;&#36866;&#24212;&#65292;&#23427;&#20419;&#36827;&#20102;&#35270;&#35273; transformer &#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#23454;&#29616;&#20102;&#22810;&#20010;&#22270;&#20687;&#20998;&#31867;&#21644;&#23545;&#35937;&#26816;&#27979;&#20219;&#21153;&#30340;&#26368;&#26032;&#36716;&#31227;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, transformers have shown strong ability as visual feature extractors, surpassing traditional convolution-based models in various scenarios. However, the success of vision transformers largely owes to their capacity to accommodate numerous parameters. As a result, new challenges for adapting large models to downstream tasks arise. On the one hand, classic fine-tuning tunes all parameters in a huge model for every task and thus easily falls into overfitting, leading to inferior performance. On the other hand, on resource-limited devices, fine-tuning stores a full copy of parameters and thus is usually impracticable for the shortage of storage space. However, few works have focused on how to efficiently and effectively transfer knowledge in a vision transformer. Existing methods did not dive into the properties of visual features, leading to inferior performance. Moreover, some of them bring heavy inference cost though benefiting storage. To tackle these problems, we propose cons
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;CONAIM&#27169;&#22411;&#30340;&#35748;&#30693;&#20195;&#29702;&#65292;&#33021;&#22815;&#36880;&#27493;&#23398;&#20064;&#31243;&#24207;&#65292;&#36890;&#36807;&#22686;&#21152;&#26032;&#21151;&#33021;&#26469;&#35299;&#20915;&#20043;&#21069;&#26080;&#27861;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#65292;&#20351;&#29992;&#22686;&#24378;&#23398;&#20064;&#30340;&#21333;&#19968;&#31243;&#24207;&#23398;&#20064;&#26426;&#21046;&#23545;&#20154;&#24418;&#26426;&#22120;&#20154;&#36827;&#34892;&#20102;&#29289;&#20307;&#36319;&#36394;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.00597</link><description>&lt;p&gt;
&#35748;&#30693;&#20154;&#24418;&#26426;&#22120;&#20154;&#30340;&#22686;&#37327;&#31243;&#24207;&#21644;&#24863;&#30693;&#21160;&#20316;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Incremental procedural and sensorimotor learning in cognitive humanoid robots. (arXiv:2305.00597v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;CONAIM&#27169;&#22411;&#30340;&#35748;&#30693;&#20195;&#29702;&#65292;&#33021;&#22815;&#36880;&#27493;&#23398;&#20064;&#31243;&#24207;&#65292;&#36890;&#36807;&#22686;&#21152;&#26032;&#21151;&#33021;&#26469;&#35299;&#20915;&#20043;&#21069;&#26080;&#27861;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#65292;&#20351;&#29992;&#22686;&#24378;&#23398;&#20064;&#30340;&#21333;&#19968;&#31243;&#24207;&#23398;&#20064;&#26426;&#21046;&#23545;&#20154;&#24418;&#26426;&#22120;&#20154;&#36827;&#34892;&#20102;&#29289;&#20307;&#36319;&#36394;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#23398;&#20064;&#26085;&#30410;&#22797;&#26434;&#21160;&#20316;&#21644;&#34892;&#20026;&#30340;&#33021;&#21147;&#26159;&#33258;&#20027;&#31995;&#32479;&#30340;&#38271;&#26399;&#30446;&#26631;&#12290;&#26412;&#30740;&#31350;&#21463;Jean Piaget&#24863;&#30693;&#21160;&#20316;&#19977;&#20010;&#23376;&#38454;&#27573;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CONAIM&#65288;&#24847;&#35782;&#27880;&#24847;&#21147;&#38598;&#25104;&#27169;&#22411;&#65289;&#30340;&#35748;&#30693;&#20195;&#29702;&#65292;&#33021;&#22815;&#36880;&#27493;&#23398;&#20064;&#31243;&#24207;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#27599;&#20010;&#23376;&#38454;&#27573;&#38656;&#35201;&#30340;&#35748;&#30693;&#21151;&#33021;&#20197;&#21450;&#22914;&#20309;&#28155;&#21152;&#26032;&#21151;&#33021;&#26469;&#35299;&#20915;&#20195;&#29702;&#20808;&#21069;&#26080;&#27861;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;&#22312;Cognitive Systems Toolkit&#65288;CST&#65289;&#27169;&#25311;&#29615;&#22659;&#20013;&#65292;&#20351;&#29992;&#22522;&#20110;&#22686;&#24378;&#23398;&#20064;&#30340;&#21333;&#19968;&#31243;&#24207;&#23398;&#20064;&#26426;&#21046;&#23545;&#20154;&#24418;&#26426;&#22120;&#20154;&#36827;&#34892;&#23454;&#39564;&#65292;&#25191;&#34892;&#29289;&#20307;&#36319;&#36394;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to automatically learn movements and behaviors of increasing complexity is a long-term goal in autonomous systems. Indeed, this is a very complex problem that involves understanding how knowledge is acquired and reused by humans as well as proposing mechanisms that allow artificial agents to reuse previous knowledge. Inspired by Jean Piaget's theory's first three sensorimotor substages, this work presents a cognitive agent based on CONAIM (Conscious Attention-Based Integrated Model) that can learn procedures incrementally. Throughout the paper, we show the cognitive functions required in each substage and how adding new functions helps address tasks previously unsolved by the agent. Experiments were conducted with a humanoid robot in a simulated environment modeled with the Cognitive Systems Toolkit (CST) performing an object tracking task. The system is modeled using a single procedural learning mechanism based on Reinforcement Learning. The increasing agent's cognitive co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36816;&#29992;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#20102;GPT-2 Small&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#30340;&#35745;&#31639;&#22270;&#20013;&#30340;&#19968;&#20010;&#23567;&#30005;&#36335;&#29992;&#20110;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#65292;&#35813;&#30005;&#36335;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#65292;&#24182;&#19988;&#35813;&#30005;&#36335;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00586</link><description>&lt;p&gt;
GPT-2&#26159;&#22914;&#20309;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#30340;&#65311;&#35299;&#37322;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23398;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. (arXiv:2305.00586v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36816;&#29992;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#20102;GPT-2 Small&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#30340;&#35745;&#31639;&#22270;&#20013;&#30340;&#19968;&#20010;&#23567;&#30005;&#36335;&#29992;&#20110;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#65292;&#35813;&#30005;&#36335;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#65292;&#24182;&#19988;&#35813;&#30005;&#36335;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#26410;&#34987;&#26126;&#30830;&#35757;&#32451;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22914;&#20309;&#23454;&#29616;&#36825;&#20123;&#21151;&#33021;&#21364;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#26412;&#25991;&#36890;&#36807;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20855;&#26377;&#30340;&#22522;&#26412;&#25968;&#23398;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20197;GPT-2 Small&#20026;&#20363;&#65292;&#30740;&#31350;&#20854;&#33021;&#21542;&#36890;&#36807;&#36755;&#20837;"&#25112;&#20105;&#25345;&#32493;&#26102;&#38388;&#26159;&#20174;1732&#24180;&#21040;17&#24180;"&#65292;&#39044;&#27979;&#20986;&#26377;&#25928;&#30340;&#20004;&#20301;&#25968;&#23383;&#30340;&#25130;&#27490;&#24180;&#20221; (&#22823;&#20110;32&#24180;)&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#19968;&#20010;&#30005;&#36335;&#65292;&#21363;GPT-2 Small&#35745;&#31639;&#22270;&#30340;&#19968;&#20010;&#23567;&#23376;&#38598;&#65292;&#29992;&#20110;&#35745;&#31639;&#36825;&#20010;&#20219;&#21153;&#30340;&#36755;&#20986;&#65292;&#28982;&#21518;&#25105;&#20204;&#35299;&#37322;&#20102;&#27599;&#20010;&#30005;&#36335;&#32452;&#20214;&#30340;&#20316;&#29992;&#65292;&#26174;&#31034;&#20986;GPT-2 Small&#30340;&#26368;&#32456;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#30005;&#36335;&#36866;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#65292;&#22312;&#20854;&#20182;&#22823;&#20110;&#22330;&#26223;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models can be surprisingly adept at tasks they were not explicitly trained on, but how they implement these capabilities is poorly understood. In this paper, we investigate the basic mathematical abilities often acquired by pre-trained language models. Concretely, we use mechanistic interpretability techniques to explain the (limited) mathematical abilities of GPT-2 small. As a case study, we examine its ability to take in sentences such as "The war lasted from the year 1732 to the year 17", and predict valid two-digit end years (years &gt; 32). We first identify a circuit, a small subset of GPT-2 small's computational graph that computes this task's output. Then, we explain the role of each circuit component, showing that GPT-2 small's final multi-layer perceptrons boost the probability of end years greater than the start year. Finally, we show that our circuit generalizes to other tasks, playing a role in other greater-than scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#22270;&#36716;&#25442;&#22120;&#65292;&#23427;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23558;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#22270;&#24418;&#20449;&#24687;&#19982;vanilla self-attention&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#29702;&#35299;&#32467;&#26500;&#21270;&#36755;&#20837;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#20004;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#27169;&#24577;&#38382;&#31572;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.00581</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22270;&#36716;&#25442;&#22120;&#29992;&#20110;&#22810;&#27169;&#24577;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Multimodal Graph Transformer for Multimodal Question Answering. (arXiv:2305.00581v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#22270;&#36716;&#25442;&#22120;&#65292;&#23427;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23558;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#22270;&#24418;&#20449;&#24687;&#19982;vanilla self-attention&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#29702;&#35299;&#32467;&#26500;&#21270;&#36755;&#20837;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#20004;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#27169;&#24577;&#38382;&#31572;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;Transformer&#27169;&#22411;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#21482;&#26159;&#26263;&#31034;&#24615;&#22320;&#23398;&#20064;&#24222;&#22823;&#30340;&#25968;&#25454;&#65292;&#24182;&#19988;&#19981;&#33021;&#30452;&#25509;&#21033;&#29992;&#32467;&#26500;&#21270;&#36755;&#20837;&#25968;&#25454;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#32467;&#26500;&#21270;&#23398;&#20064;&#26041;&#27861;&#22914;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21487;&#20197;&#25972;&#21512;&#20808;&#21069;&#30340;&#20449;&#24687;&#65292;&#20294;&#19982;Transformer&#27169;&#22411;&#20960;&#20046;&#26080;&#27861;&#31454;&#20105;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#20174;&#20004;&#20010;&#19990;&#30028;&#20013;&#21463;&#30410;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#22270;&#36716;&#25442;&#22120;&#65292;&#29992;&#20110;&#38656;&#35201;&#22312;&#22810;&#20010;&#27169;&#24577;&#20043;&#38388;&#36827;&#34892;&#25512;&#29702;&#30340;&#38382;&#31572;&#20219;&#21153;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#28041;&#21450;&#22270;&#24418;&#30340;&#21363;&#25554;&#21363;&#29992;&#20934;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#23558;&#20174;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#20013;&#33719;&#21462;&#30340;&#22810;&#27169;&#24577;&#22270;&#24418;&#20449;&#24687;&#24182;&#20837; vanilla self-attention &#20013;&#20316;&#20026;&#26377;&#25928;&#20808;&#39564;&#30693;&#35782;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#26500;&#24314;&#25991;&#26412;&#22270;&#12289;&#23494;&#38598;&#21306;&#22495;&#22270;&#21644;&#35821;&#20041;&#22270;&#29983;&#25104;&#37051;&#25509;&#30697;&#38453;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#19982;&#36755;&#20837;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#29305;&#24449;&#32452;&#21512;&#20197;&#25191;&#34892;&#19979;&#28216;&#25512;&#29702;&#12290;&#20351;&#29992;&#22270;&#24418;&#20449;&#24687;&#26469;&#35268;&#33539;self-attention&#26426;&#21046;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#29702;&#35299;&#32467;&#26500;&#21270;&#36755;&#20837;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#20004;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#27169;&#24577;&#38382;&#31572;&#22522;&#20934;(VQA&#21644;GQA)&#19978;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of Transformer models in vision and language tasks, they often learn knowledge from enormous data implicitly and cannot utilize structured input data directly. On the other hand, structured learning approaches such as graph neural networks (GNNs) that integrate prior information can barely compete with Transformer models. In this work, we aim to benefit from both worlds and propose a novel Multimodal Graph Transformer for question answering tasks that requires performing reasoning across multiple modalities. We introduce a graph-involved plug-and-play quasi-attention mechanism to incorporate multimodal graph information, acquired from text and visual data, to the vanilla self-attention as effective prior. In particular, we construct the text graph, dense region graph, and semantic graph to generate adjacency matrices, and then compose them with input vision and language features to perform downstream reasoning. Such a way of regularizing self-attention with graph in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#31163;&#32447;MORL&#35774;&#32622;&#21644;&#19968;&#20010;&#29992;&#20110;&#31163;&#32447;MORL&#30340;&#31639;&#27861;PEDA&#12290;PEDA&#36890;&#36807;&#22312;&#22810;&#20010;&#27169;&#22411;&#20013;&#36873;&#25321;&#26368;&#20248;&#27169;&#22411;&#26469;&#23454;&#29616;&#27491;&#20132;&#20559;&#22909;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;PEDA&#22312;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#21516;&#26102;&#36824;&#21487;&#25193;&#23637;&#21040;&#20855;&#26377;&#26356;&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#22797;&#26434;&#29615;&#22659;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.00567</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#32447;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#25193;&#23637;&#24085;&#32047;&#25176;&#26377;&#25928;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Scaling Pareto-Efficient Decision Making Via Offline Multi-Objective RL. (arXiv:2305.00567v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#31163;&#32447;MORL&#35774;&#32622;&#21644;&#19968;&#20010;&#29992;&#20110;&#31163;&#32447;MORL&#30340;&#31639;&#27861;PEDA&#12290;PEDA&#36890;&#36807;&#22312;&#22810;&#20010;&#27169;&#22411;&#20013;&#36873;&#25321;&#26368;&#20248;&#27169;&#22411;&#26469;&#23454;&#29616;&#27491;&#20132;&#20559;&#22909;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;PEDA&#22312;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#21516;&#26102;&#36824;&#21487;&#25193;&#23637;&#21040;&#20855;&#26377;&#26356;&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#22797;&#26434;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#65288;MORL&#65289;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#33021;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#31454;&#20105;&#30446;&#26631;&#30340;&#31574;&#30053;&#12290;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#20195;&#29702;&#23545;&#30446;&#26631;&#30340;&#20559;&#22909;&#21487;&#33021;&#19981;&#26159;&#20808;&#39564;&#24050;&#30693;&#30340;&#65292;&#22240;&#27492;&#25105;&#20204;&#38656;&#35201;&#22312;&#27979;&#35797;&#26102;&#33021;&#22815;&#36866;&#24212;&#20219;&#24847;&#20559;&#22909;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#31163;&#32447;MORL&#35774;&#32622;&#65292;&#25105;&#20204;&#24076;&#26395;&#21482;&#20351;&#29992;&#26377;&#38480;&#30340;&#31163;&#32447;&#28436;&#31034;&#25968;&#25454;&#38598;&#26469;&#23398;&#20064;&#19968;&#20010;&#20559;&#22909;&#19981;&#25935;&#24863;&#30340;&#31574;&#30053;&#20195;&#29702;&#12290;&#26412;&#25991;&#30340;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#65306;&#31532;&#19968;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;D4MORL&#65292;&#36825;&#26159;&#19968;&#32452;&#19987;&#38376;&#38024;&#23545;&#31163;&#32447;&#35774;&#32622;&#35774;&#35745;&#30340;MORL&#25968;&#25454;&#38598;&#12290;&#23427;&#21253;&#21547;180&#19975;&#20010;&#25968;&#25454;&#65292;&#26159;&#36890;&#36807;&#22312;6&#20010;MuJoCo&#29615;&#22659;&#20013;&#20248;&#21270;2-3&#20010;&#30446;&#26631;&#30340;&#21442;&#32771;&#31574;&#30053;&#30340;&#36807;&#31243;&#20013;&#33719;&#24471;&#30340;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24085;&#32047;&#25176;&#26377;&#25928;&#20915;&#31574;&#20195;&#29702;&#65288;PEDA&#65289;&#65292;&#23427;&#26159;&#19968;&#26063;&#31163;&#32447;MORL&#31639;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#21644;&#25193;&#23637;&#20915;&#31574;&#36716;&#31227;&#65288;DT&#65289;&#26041;&#27861;&#26469;&#36866;&#29992;&#20110;&#26032;&#30340;MORL&#35774;&#32622;&#12290; PEDS&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#65292;&#27599;&#20010;&#27169;&#22411;&#37117;&#20026;&#19981;&#21516;&#30340;&#12289;&#38543;&#26426;&#36873;&#25321;&#30340;&#30446;&#26631;&#20248;&#21270;&#12290;&#22312;&#27979;&#35797;&#26102;&#65292;PEDA&#36890;&#36807;&#19968;&#31181;&#22522;&#20110;&#29992;&#25143;&#25351;&#23450;&#20559;&#22909;&#30340;&#21407;&#21017;&#26469;&#36873;&#25321;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;MuJoCo&#29615;&#22659;&#19979;&#35780;&#20272;&#20102;PEDA&#30340;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;PEDA&#22312;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;MORL&#26041;&#27861;&#65292;&#24182;&#33258;&#28982;&#22320;&#25193;&#23637;&#21040;&#26356;&#22797;&#26434;&#30340;&#20855;&#26377;&#36739;&#22823;&#34892;&#21160;&#31354;&#38388;&#30340;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of multi-objective reinforcement learning (MORL) is to learn policies that simultaneously optimize multiple competing objectives. In practice, an agent's preferences over the objectives may not be known apriori, and hence, we require policies that can generalize to arbitrary preferences at test time. In this work, we propose a new data-driven setup for offline MORL, where we wish to learn a preference-agnostic policy agent using only a finite dataset of offline demonstrations of other agents and their preferences. The key contributions of this work are two-fold. First, we introduce D4MORL, (D)atasets for MORL that are specifically designed for offline settings. It contains 1.8 million annotated demonstrations obtained by rolling out reference policies that optimize for randomly sampled preferences on 6 MuJoCo environments with 2-3 objectives each. Second, we propose Pareto-Efficient Decision Agents (PEDA), a family of offline MORL algorithms that builds and extends Decision Tr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#21644;&#26497;&#38480;&#30830;&#23450;&#24615;&#24191;&#20041;&#24067;&#27663;&#33258;&#21160;&#26426;&#26469;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#19979;&#33258;&#20027;&#26234;&#33021;&#20307;&#22797;&#26434;&#20219;&#21153;&#30340;&#36816;&#21160;&#35268;&#21010;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#28145;&#24230;Q&#23398;&#20064;&#19982;LSTM&#22686;&#24378;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2305.00561</link><description>&lt;p&gt;
&#38754;&#21521;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#22797;&#26434;&#20219;&#21153;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#26080;&#27169;&#22411;&#36816;&#21160;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Model-free Motion Planning of Autonomous Agents for Complex Tasks in Partially Observable Environments. (arXiv:2305.00561v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#21644;&#26497;&#38480;&#30830;&#23450;&#24615;&#24191;&#20041;&#24067;&#27663;&#33258;&#21160;&#26426;&#26469;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#19979;&#33258;&#20027;&#26234;&#33021;&#20307;&#22797;&#26434;&#20219;&#21153;&#30340;&#36816;&#21160;&#35268;&#21010;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#28145;&#24230;Q&#23398;&#20064;&#19982;LSTM&#22686;&#24378;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37096;&#20998;&#24050;&#30693;&#19988;&#20449;&#24687;&#19981;&#23436;&#22791;&#30340;&#29615;&#22659;&#20013;&#65292;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#36816;&#21160;&#35268;&#21010;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22797;&#26434;&#20219;&#21153;&#32780;&#35328;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#36816;&#21160;&#35268;&#21010;&#24314;&#27169;&#20026;&#19968;&#20010;&#27010;&#29575;&#26631;&#35760;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;PL-POMDP&#65289;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL&#65289;&#26469;&#34920;&#36798;&#22797;&#26434;&#20219;&#21153;&#12290;&#28982;&#21518;&#23558;LTL&#20844;&#24335;&#36716;&#25442;&#20026;&#26497;&#38480;&#30830;&#23450;&#24615;&#24191;&#20041;&#24067;&#27663;&#33258;&#21160;&#26426;&#65288;LDGBA&#65289;&#12290;&#22522;&#20110;&#27169;&#22411;&#26816;&#27979;&#25216;&#26415;&#65292;&#23558;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#22312;PL-POMDP&#19982;LDGBA&#30340;&#20056;&#31215;&#19978;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#20197;&#28385;&#36275;&#22797;&#26434;&#20219;&#21153;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#28145;&#24230;Q&#23398;&#20064;&#19982;&#38271;&#30701;&#26102;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#26469;&#22788;&#29702;&#35266;&#23519;&#21382;&#21490;&#21644;&#20219;&#21153;&#35782;&#21035;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#25552;&#20986;&#30340;&#26041;&#27861;&#12289;LTL&#21644;LDGBA&#30340;&#21033;&#29992;&#20197;&#21450;LSTM&#22686;&#24378;&#30340;&#28145;&#24230;Q&#23398;&#20064;&#12290;&#25105;&#20204;&#36890;&#36807;&#36827;&#34892;&#20223;&#30495;&#23454;&#39564;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motion planning of autonomous agents in partially known environments with incomplete information is a challenging problem, particularly for complex tasks. This paper proposes a model-free reinforcement learning approach to address this problem. We formulate motion planning as a probabilistic-labeled partially observable Markov decision process (PL-POMDP) problem and use linear temporal logic (LTL) to express the complex task. The LTL formula is then converted to a limit-deterministic generalized B\"uchi automaton (LDGBA). The problem is redefined as finding an optimal policy on the product of PL-POMDP with LDGBA based on model-checking techniques to satisfy the complex task. We implement deep Q learning with long short-term memory (LSTM) to process the observation history and task recognition. Our contributions include the proposed method, the utilization of LTL and LDGBA, and the LSTM-enhanced deep Q learning. We demonstrate the applicability of the proposed method by conducting simul
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#27169;&#25311;&#21644;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#31435;&#22330;&#30340;&#30693;&#35782;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#22686;&#24378;&#22522;&#26412;&#36923;&#36753;&#26469;&#23454;&#29616;&#65292;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#22788;&#29702;&#22810;&#20803;&#31435;&#22330;&#24182;&#25552;&#20379;&#33258;&#21160;&#25512;&#29702;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2305.00559</link><description>&lt;p&gt;
Standpoint-OWL 2&#30340;&#33258;&#21160;&#25512;&#29702;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Automated reasoning support for Standpoint-OWL 2. (arXiv:2305.00559v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00559
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#27169;&#25311;&#21644;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#31435;&#22330;&#30340;&#30693;&#35782;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#22686;&#24378;&#22522;&#26412;&#36923;&#36753;&#26469;&#23454;&#29616;&#65292;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#22788;&#29702;&#22810;&#20803;&#31435;&#22330;&#24182;&#25552;&#20379;&#33258;&#21160;&#25512;&#29702;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24037;&#20855;&#65292;&#29992;&#20110;&#27169;&#25311;&#21644;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#30340;&#31435;&#22330;&#65288;&#21487;&#33021;&#23384;&#22312;&#20914;&#31361;&#65289;&#30340;&#30693;&#35782;&#12290;&#29702;&#35770;&#22522;&#30784;&#26159;&#36890;&#36807;&#22686;&#24378;&#22522;&#26412;&#36923;&#36753;&#26469;&#23454;&#29616;&#30340;&#65292;&#35813;&#36923;&#36753;&#26159;&#26681;&#25454;&#26368;&#36817;&#24341;&#20837;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#21152;&#20837;&#31435;&#22330;&#30340;&#12290;&#35813;&#24037;&#20855;&#36890;&#36807;&#23558;&#31435;&#22330;&#22686;&#24378;&#29256;&#26412;&#30340;&#25551;&#36848;&#36923;&#36753;SROIQ&#36716;&#25442;&#20026;&#20854;&#26222;&#36890;&#65288;&#21363;&#32463;&#20856;&#65289;&#29256;&#26412;&#26469;&#24037;&#20316;&#12290;&#29616;&#26377;&#30340;&#25512;&#29702;&#22120;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#25552;&#20379;&#22788;&#29702;&#19981;&#21516;&#31435;&#22330;&#30340;&#33258;&#21160;&#21270;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a tool for modelling and reasoning with knowledge from various diverse (and possibly conflicting) viewpoints. The theoretical underpinnings are provided by enhancing base logics by standpoints according to a recently introduced formalism that we also recall. The tool works by translating the standpoint-enhanced version of the description logic SROIQ to its plain (i.e. classical) version. Existing reasoners can then be directly used to provide automated support for reasoning about diverse standpoints.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26102;&#31354;&#38754;&#37096;&#29305;&#24449;&#21487;&#35270;&#21270;&#35821;&#38899;&#35782;&#21035;&#35748;&#35777;&#25216;&#26415;&#65292;&#32467;&#21512;&#38754;&#37096;&#35782;&#21035;&#21644;&#20010;&#20307;&#35762;&#23494;&#30721;&#26102;&#30340;&#29420;&#29305;&#26102;&#31354;&#38754;&#37096;&#29305;&#24449;&#36816;&#21160;&#65292;&#25104;&#21151;&#22312;&#24037;&#19994;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;96.1&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.00552</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26102;&#31354;&#38754;&#37096;&#29305;&#24449;&#21487;&#35270;&#21270;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Deep Learning-based Spatio Temporal Facial Feature Visual Speech Recognition. (arXiv:2305.00552v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26102;&#31354;&#38754;&#37096;&#29305;&#24449;&#21487;&#35270;&#21270;&#35821;&#38899;&#35782;&#21035;&#35748;&#35777;&#25216;&#26415;&#65292;&#32467;&#21512;&#38754;&#37096;&#35782;&#21035;&#21644;&#20010;&#20307;&#35762;&#23494;&#30721;&#26102;&#30340;&#29420;&#29305;&#26102;&#31354;&#38754;&#37096;&#29305;&#24449;&#36816;&#21160;&#65292;&#25104;&#21151;&#22312;&#24037;&#19994;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;96.1&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#36164;&#28304;&#35745;&#31639;&#29615;&#22659;&#65288;&#22914;&#26234;&#33021;&#25163;&#26426;&#21644;&#20854;&#20182;&#23567;&#22411;&#35774;&#22791;&#65289;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#21644;&#26426;&#22120;&#23398;&#20064;&#34987;&#29992;&#20110;&#35768;&#22810;&#36523;&#20221;&#35782;&#21035;&#31995;&#32479;&#20013;&#20316;&#20026;&#35748;&#35777;&#25216;&#26415;&#12290;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#36825;&#20123;&#38754;&#37096;&#35782;&#21035;&#25216;&#26415;&#30340;&#36879;&#26126;&#12289;&#38750;&#25509;&#35302;&#21644;&#38750;&#20405;&#20837;&#24615;&#36136;&#20351;&#20854;&#36817;&#24180;&#26469;&#22312;&#20154;&#20204;&#20013;&#30340;&#26222;&#21450;&#24230;&#24613;&#21095;&#19978;&#21319;&#12290;&#34429;&#28982;&#23427;&#20204;&#22823;&#22810;&#37117;&#24456;&#25104;&#21151;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#36890;&#36807;&#21033;&#29992;&#22270;&#29255;&#12289;&#38754;&#20855;&#12289;&#30524;&#38236;&#31561;&#26041;&#24335;&#26410;&#32463;&#35768;&#21487;&#36827;&#20837;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#35748;&#35777;&#36807;&#31243;&#65292;&#21033;&#29992;&#20102;&#38754;&#37096;&#35782;&#21035;&#21644;&#20010;&#20307;&#22312;&#35762;&#23494;&#30721;&#26102;&#30340;&#29420;&#29305;&#26102;&#31354;&#38754;&#37096;&#29305;&#24449;&#36816;&#21160;&#12290;&#30001;&#20110;&#24314;&#35758;&#30340;&#26041;&#27861;&#20801;&#35768;&#22312;&#20219;&#20309;&#35821;&#35328;&#20013;&#25351;&#23450;&#23494;&#30721;&#65292;&#22240;&#27492;&#19981;&#21463;&#35821;&#35328;&#38480;&#21046;&#12290;&#22312;&#24037;&#19994;&#26631;&#20934;MIRACL-VC1&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#26102;&#65292;&#24314;&#35758;&#30340;&#27169;&#22411;&#36798;&#21040;&#20102;96.1&#65285;&#30340;&#20934;&#30830;&#29575;&#65292;&#23637;&#31034;&#20102;&#23427;&#20316;&#20026;&#19968;&#31181;&#21487;&#38752;&#32780;&#24378;&#22823;&#30340;&#35748;&#35777;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In low-resource computing contexts, such as smartphones and other tiny devices, Both deep learning and machine learning are being used in a lot of identification systems. as authentication techniques. The transparent, contactless, and non-invasive nature of these face recognition technologies driven by AI has led to their meteoric rise in popularity in recent years. While they are mostly successful, there are still methods to get inside without permission by utilising things like pictures, masks, glasses, etc. In this research, we present an alternate authentication process that makes use of both facial recognition and the individual's distinctive temporal facial feature motions while they speak a password. Because the suggested methodology allows for a password to be specified in any language, it is not limited by language. The suggested model attained an accuracy of 96.1% when tested on the industry-standard MIRACL-VC1 dataset, demonstrating its efficacy as a reliable and powerful so
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#31946;&#26657;&#20934;&#35823;&#24046;&#24230;&#37327;&#65288;FCE&#65289;&#65292;&#21033;&#29992;&#27169;&#31946;&#20998;&#31665;&#26041;&#27861;&#35745;&#31639;&#26657;&#20934;&#35823;&#24046;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#27010;&#29575;&#20559;&#26012;&#30340;&#24433;&#21709;&#24182;&#25552;&#20379;&#20102;&#26356;&#32039;&#23494;&#30340;&#20272;&#35745;&#20540;&#12290;&#19982;&#20256;&#32479;&#25351;&#26631;ECE&#30456;&#27604;&#65292;FCE&#22312;&#22810;&#31867;&#35774;&#32622;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;https://github.com/srdgFHE/FCE-paper&#12290;</title><link>http://arxiv.org/abs/2305.00543</link><description>&lt;p&gt;
&#20351;&#29992;&#27169;&#31946;&#20998;&#31665;&#36827;&#34892;&#26657;&#20934;&#35823;&#24046;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Calibration Error Estimation Using Fuzzy Binning. (arXiv:2305.00543v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#31946;&#26657;&#20934;&#35823;&#24046;&#24230;&#37327;&#65288;FCE&#65289;&#65292;&#21033;&#29992;&#27169;&#31946;&#20998;&#31665;&#26041;&#27861;&#35745;&#31639;&#26657;&#20934;&#35823;&#24046;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#27010;&#29575;&#20559;&#26012;&#30340;&#24433;&#21709;&#24182;&#25552;&#20379;&#20102;&#26356;&#32039;&#23494;&#30340;&#20272;&#35745;&#20540;&#12290;&#19982;&#20256;&#32479;&#25351;&#26631;ECE&#30456;&#27604;&#65292;FCE&#22312;&#22810;&#31867;&#35774;&#32622;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;https://github.com/srdgFHE/FCE-paper&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#24448;&#24448;&#20250;&#36807;&#20110;&#33258;&#20449;&#65292;&#20854;&#21407;&#22987;&#32467;&#26524;&#30340;&#27010;&#29575;&#24182;&#19981;&#31526;&#21512;&#30495;&#23454;&#30340;&#20915;&#31574;&#27010;&#29575;&#12290;&#31070;&#32463;&#32593;&#32476;&#30340;&#26657;&#20934;&#26159;&#23454;&#29616;&#26356;&#21487;&#38752;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#20808;&#21069;&#30340;&#26657;&#20934;&#35823;&#24046;&#24230;&#37327;&#20027;&#35201;&#21033;&#29992;&#28165;&#26224;&#30340;&#20998;&#31665;&#25104;&#21592;&#36164;&#26684;&#24230;&#37327;&#12290;&#36825;&#21152;&#21095;&#20102;&#27169;&#22411;&#27010;&#29575;&#30340;&#20559;&#26012;&#65292;&#24182;&#25551;&#32472;&#20102;&#26657;&#20934;&#35823;&#24046;&#30340;&#19981;&#23436;&#25972;&#22270;&#20687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27169;&#31946;&#20998;&#31665;&#26041;&#27861;&#35745;&#31639;&#26657;&#20934;&#35823;&#24046;&#30340;&#27169;&#31946;&#26657;&#20934;&#35823;&#24046;&#24230;&#37327;&#65288;FCE&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#32531;&#35299;&#20102;&#27010;&#29575;&#20559;&#26012;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#27979;&#37327;&#26657;&#20934;&#35823;&#24046;&#26102;&#25552;&#20379;&#20102;&#26356;&#32039;&#23494;&#30340;&#20272;&#35745;&#20540;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#25351;&#26631;&#19982;ECE&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#32676;&#20307;&#21644;&#31867;&#21035;&#25104;&#21592;&#36523;&#20221;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;FCE&#22312;&#26657;&#20934;&#35823;&#24046;&#20272;&#35745;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#29305;&#21035;&#26159;&#22312;&#22810;&#31867;&#35774;&#32622;&#20013;&#65292;&#32531;&#35299;&#20102;&#27169;&#22411;&#32622;&#20449;&#24230;&#20998;&#25968;&#20559;&#26012;&#23545;&#26657;&#20934;&#35823;&#24046;&#20272;&#35745;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;https://github.com/srdgFHE/FCE-paper&#65292;&#20197;&#20415;&#26410;&#26469;&#30340;&#21487;&#37325;&#22797;&#24615;&#21644;&#20351;&#29992;FCE&#36827;&#34892;&#26657;&#20934;&#35823;&#24046;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network-based decisions tend to be overconfident, where their raw outcome probabilities do not align with the true decision probabilities. Calibration of neural networks is an essential step towards more reliable deep learning frameworks. Prior metrics of calibration error primarily utilize crisp bin membership-based measures. This exacerbates skew in model probabilities and portrays an incomplete picture of calibration error. In this work, we propose a Fuzzy Calibration Error metric (FCE) that utilizes a fuzzy binning approach to calculate calibration error. This approach alleviates the impact of probability skew and provides a tighter estimate while measuring calibration error. We compare our metric with ECE across different data populations and class memberships. Our results show that FCE offers better calibration error estimation, especially in multi-class settings, alleviating the effects of skew in model confidence scores on calibration error estimation. We make our code a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SEA&#31639;&#27861;&#65292;&#21487;&#22312;&#25104;&#23601;&#22411;&#29615;&#22659;&#20013;&#36827;&#34892;&#25506;&#32034;&#20219;&#21153;&#12290;SEA&#39318;&#20808;&#23398;&#20064;&#24050;&#30693;&#25104;&#23601;&#30340;&#34920;&#31034;&#21644;&#20381;&#36182;&#20851;&#31995;&#22270;&#65292;&#28982;&#21518;&#36890;&#36807;&#26500;&#24314;&#25511;&#21046;&#22120;&#22312;&#32447;&#25506;&#32034;&#26032;&#25104;&#23601;&#12290;&#23454;&#39564;&#35777;&#26126;SEA&#33021;&#22815;&#20934;&#30830;&#22320;&#24674;&#22797;&#25104;&#23601;&#32467;&#26500;&#24182;&#25913;&#21892;&#22312;&#19968;&#20123;&#22797;&#26434;&#39046;&#22495;&#20013;&#30340;&#25506;&#32034;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.00508</link><description>&lt;p&gt;
&#23398;&#20064;&#25104;&#23601;&#32467;&#26500;&#26469;&#22312;&#26377;&#31232;&#30095;&#22870;&#21169;&#30340;&#39046;&#22495;&#36827;&#34892;&#32467;&#26500;&#21270;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Learning Achievement Structure for Structured Exploration in Domains with Sparse Reward. (arXiv:2305.00508v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SEA&#31639;&#27861;&#65292;&#21487;&#22312;&#25104;&#23601;&#22411;&#29615;&#22659;&#20013;&#36827;&#34892;&#25506;&#32034;&#20219;&#21153;&#12290;SEA&#39318;&#20808;&#23398;&#20064;&#24050;&#30693;&#25104;&#23601;&#30340;&#34920;&#31034;&#21644;&#20381;&#36182;&#20851;&#31995;&#22270;&#65292;&#28982;&#21518;&#36890;&#36807;&#26500;&#24314;&#25511;&#21046;&#22120;&#22312;&#32447;&#25506;&#32034;&#26032;&#25104;&#23601;&#12290;&#23454;&#39564;&#35777;&#26126;SEA&#33021;&#22815;&#20934;&#30830;&#22320;&#24674;&#22797;&#25104;&#23601;&#32467;&#26500;&#24182;&#25913;&#21892;&#22312;&#19968;&#20123;&#22797;&#26434;&#39046;&#22495;&#20013;&#30340;&#25506;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEA&#30340;&#22810;&#38454;&#27573;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#26088;&#22312;&#20026;&#22522;&#20110;&#25104;&#23601;&#30340;&#29615;&#22659;&#35774;&#35745;&#65292;&#21363;&#20855;&#26377;&#20869;&#37096;&#25104;&#23601;&#38598;&#30340;&#29305;&#23450;&#31867;&#22411;&#29615;&#22659;&#12290;SEA&#39318;&#20808;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#65292;&#20351;&#29992;&#30830;&#23450;&#24615;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#24050;&#30693;&#25104;&#23601;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#20351;&#29992;&#21551;&#21457;&#24335;&#31639;&#27861;&#24674;&#22797;&#23398;&#20064;&#25104;&#23601;&#30340;&#20381;&#36182;&#20851;&#31995;&#22270;&#65292;&#26368;&#21518;&#36890;&#36807;&#20351;&#29992;&#24674;&#22797;&#30340;&#20381;&#36182;&#20851;&#31995;&#22270;&#26500;&#24314;&#25511;&#21046;&#22120;&#65292;&#22312;&#32447;&#19982;&#29615;&#22659;&#20132;&#20114;&#23398;&#20064;&#25484;&#25569;&#24050;&#30693;&#25104;&#23601;&#24182;&#25506;&#32034;&#26032;&#25104;&#23601;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;SEA&#21487;&#20197;&#20934;&#30830;&#22320;&#24674;&#22797;&#25104;&#23601;&#32467;&#26500;&#65292;&#24182;&#25913;&#21892;&#22312;&#20687;&#22270;&#20687;&#36825;&#26679;&#20855;&#26377;&#39640;&#32500;&#35266;&#23519;&#20540;&#30340;&#22823;&#22411;&#29983;&#25104;&#39046;&#22495;&#65288;&#22914;Crafter&#65289;&#20013;&#30340;&#25506;&#32034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Structured Exploration with Achievements (SEA), a multi-stage reinforcement learning algorithm designed for achievement-based environments, a particular type of environment with an internal achievement set. SEA first uses offline data to learn a representation of the known achievements with a determinant loss function, then recovers the dependency graph of the learned achievements with a heuristic algorithm, and finally interacts with the environment online to learn policies that master known achievements and explore new ones with a controller built with the recovered dependency graph. We empirically demonstrate that SEA can recover the achievement structure accurately and improve exploration in hard domains such as Crafter that are procedurally generated with high-dimensional observations like images.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21518;&#39564;&#37319;&#26679;&#31639;&#27861;PSDRL&#65292;&#32467;&#21512;&#20102;&#39640;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#29305;&#27530;&#35774;&#35745;&#30340;&#25345;&#32493;&#35268;&#21010;&#31639;&#27861;&#65292;&#20351;&#20854;&#22312;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#21516;&#26102;&#26174;&#33879;&#20248;&#20110;&#20043;&#21069;&#30340;&#23581;&#35797;&#12290;</title><link>http://arxiv.org/abs/2305.00477</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21518;&#39564;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Posterior Sampling for Deep Reinforcement Learning. (arXiv:2305.00477v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21518;&#39564;&#37319;&#26679;&#31639;&#27861;PSDRL&#65292;&#32467;&#21512;&#20102;&#39640;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#29305;&#27530;&#35774;&#35745;&#30340;&#25345;&#32493;&#35268;&#21010;&#31639;&#27861;&#65292;&#20351;&#20854;&#22312;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#21516;&#26102;&#26174;&#33879;&#20248;&#20110;&#20043;&#21069;&#30340;&#23581;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#26679;&#26412;&#25928;&#29575;&#20173;&#28982;&#36739;&#20302;&#65306;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#30340;&#35797;&#38169;&#26469;&#25214;&#21040;&#22909;&#30340;&#31574;&#30053;&#12290;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#36890;&#36807;&#26500;&#24314;&#21487;&#20197;&#29992;&#20110;&#35268;&#21010;&#30340;&#29615;&#22659;&#27169;&#22411;&#26469;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#21518;&#39564;&#37319;&#26679;&#24378;&#21270;&#23398;&#20064;&#26159;&#36825;&#26679;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#22312;&#34920;&#26684;&#35774;&#32622;&#20013;&#30001;&#20110;&#20854;&#24615;&#33021;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20852;&#36259;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21518;&#39564;&#37319;&#26679;&#65288;PSDRL&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#30495;&#27491;&#21487;&#25193;&#23637;&#30340;&#21518;&#39564;&#37319;&#26679;&#24378;&#21270;&#23398;&#20064;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#20445;&#30041;&#20102;&#20854;&#22522;&#20110;&#27169;&#22411;&#30340;&#26412;&#36136;&#29305;&#24449;&#12290;PSDRL&#23558;&#28508;&#22312;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#19978;&#30340;&#39640;&#25928;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#19982;&#22522;&#20110;&#20540;&#20989;&#25968;&#36924;&#36817;&#30340;&#29305;&#27530;&#35774;&#35745;&#30340;&#25345;&#32493;&#35268;&#21010;&#31639;&#27861;&#30456;&#32467;&#21512;&#12290;&#23545;Atari&#22522;&#20934;&#27979;&#35797;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;PSDRL&#22312;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#21516;&#26102;&#65292;&#26174;&#33879;&#20248;&#20110;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#23581;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite remarkable successes, deep reinforcement learning algorithms remain sample inefficient: they require an enormous amount of trial and error to find good policies. Model-based algorithms promise sample efficiency by building an environment model that can be used for planning. Posterior Sampling for Reinforcement Learning is such a model-based algorithm that has attracted significant interest due to its performance in the tabular setting. This paper introduces Posterior Sampling for Deep Reinforcement Learning (PSDRL), the first truly scalable approximation of Posterior Sampling for Reinforcement Learning that retains its model-based essence. PSDRL combines efficient uncertainty quantification over latent state space models with a specially tailored continual planning algorithm based on value-function approximation. Extensive experiments on the Atari benchmark show that PSDRL significantly outperforms previous state-of-the-art attempts at scaling up posterior sampling while being 
&lt;/p&gt;</description></item><item><title>Causalainer&#26159;&#19968;&#20010;&#33258;&#21160;&#35270;&#39057;&#25688;&#35201;&#30340;&#22240;&#26524;&#35299;&#37322;&#22120;&#65292;&#20854;&#24341;&#20837;&#22810;&#20010;&#26377;&#24847;&#20041;&#30340;&#38543;&#26426;&#21464;&#37327;&#21450;&#20854;&#32852;&#21512;&#20998;&#24067;&#26469;&#25551;&#36848;&#35270;&#39057;&#25688;&#35201;&#31649;&#36947;&#20013;&#20851;&#38190;&#32452;&#20214;&#30340;&#34892;&#20026;&#65292;&#21487;&#20197;&#20998;&#26512;&#29992;&#25143;&#25351;&#23450;&#30340;&#24178;&#39044;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00455</link><description>&lt;p&gt;
Causalainer: &#33258;&#21160;&#35270;&#39057;&#25688;&#35201;&#30340;&#22240;&#26524;&#35299;&#37322;&#22120;
&lt;/p&gt;
&lt;p&gt;
Causalainer: Causal Explainer for Automatic Video Summarization. (arXiv:2305.00455v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00455
&lt;/p&gt;
&lt;p&gt;
Causalainer&#26159;&#19968;&#20010;&#33258;&#21160;&#35270;&#39057;&#25688;&#35201;&#30340;&#22240;&#26524;&#35299;&#37322;&#22120;&#65292;&#20854;&#24341;&#20837;&#22810;&#20010;&#26377;&#24847;&#20041;&#30340;&#38543;&#26426;&#21464;&#37327;&#21450;&#20854;&#32852;&#21512;&#20998;&#24067;&#26469;&#25551;&#36848;&#35270;&#39057;&#25688;&#35201;&#31649;&#36947;&#20013;&#20851;&#38190;&#32452;&#20214;&#30340;&#34892;&#20026;&#65292;&#21487;&#20197;&#20998;&#26512;&#29992;&#25143;&#25351;&#23450;&#30340;&#24178;&#39044;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#25688;&#35201;&#30340;&#30446;&#26631;&#26159;&#33258;&#21160;&#32553;&#30701;&#35270;&#39057;&#65292;&#20197;&#20256;&#36798;&#25972;&#20010;&#25925;&#20107;&#32780;&#19981;&#22833;&#21435;&#30456;&#20851;&#20449;&#24687;&#12290;&#22312;&#35768;&#22810;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#19981;&#36866;&#24403;&#30340;&#35270;&#39057;&#25688;&#35201;&#21487;&#33021;&#20250;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#20363;&#22914;&#65292;&#22312;&#27861;&#21307;&#23398;&#20013;&#65292;&#29983;&#25104;&#30340;&#35270;&#39057;&#25688;&#35201;&#30340;&#36136;&#37327;&#23558;&#24433;&#21709;&#35843;&#26597;&#20154;&#21592;&#30340;&#21028;&#26029;&#65292;&#32780;&#22312;&#26032;&#38395;&#23398;&#20013;&#21487;&#33021;&#20250;&#20135;&#29983;&#19981;&#24076;&#26395;&#30340;&#20559;&#35265;&#12290;&#22240;&#27492;&#65292;&#23545;&#24314;&#27169;&#30340;&#21487;&#35299;&#37322;&#24615;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#25581;&#31034;&#24341;&#23548;&#36807;&#31243;&#24182;&#23548;&#33268;&#32467;&#26524;&#30340;&#22240;&#26524;&#20851;&#31995;&#26159;&#35299;&#20915;&#21487;&#35299;&#37322;&#24615;&#25361;&#25112;&#30340;&#26368;&#20339;&#26041;&#27861;&#20043;&#19968;&#12290;&#24403;&#21069;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#35270;&#39057;&#25688;&#35201;&#31639;&#27861;&#23398;&#20064;&#26368;&#20339;&#21442;&#25968;&#65292;&#20294;&#19981;&#25581;&#31034;&#22240;&#26524;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#32570;&#20047;&#30456;&#23545;&#35299;&#37322;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Causalainer&#30340;&#22240;&#26524;&#35299;&#37322;&#22120;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#24341;&#20837;&#22810;&#20010;&#26377;&#24847;&#20041;&#30340;&#38543;&#26426;&#21464;&#37327;&#21450;&#20854;&#32852;&#21512;&#20998;&#24067;&#26469;&#25551;&#36848;&#35270;&#39057;&#25688;&#35201;&#31649;&#36947;&#20013;&#20851;&#38190;&#32452;&#20214;&#30340;&#34892;&#20026;&#12290;Causalainer&#20351;&#20154;&#20204;&#33021;&#22815;&#20998;&#26512;&#29992;&#25143;&#25351;&#23450;&#30340;&#24178;&#39044;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#20102;&#35299;&#35270;&#39057;&#25688;&#35201;&#36807;&#31243;&#30340;&#22522;&#26412;&#36923;&#36753;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;Causalainer&#19981;&#20165;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#35270;&#39057;&#25688;&#35201;&#65292;&#36824;&#25552;&#20379;&#20102;&#26377;&#24847;&#20041;&#30340;&#35265;&#35299;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of video summarization is to automatically shorten videos such that it conveys the overall story without losing relevant information. In many application scenarios, improper video summarization can have a large impact. For example in forensics, the quality of the generated video summary will affect an investigator's judgment while in journalism it might yield undesired bias. Because of this, modeling explainability is a key concern. One of the best ways to address the explainability challenge is to uncover the causal relations that steer the process and lead to the result. Current machine learning-based video summarization algorithms learn optimal parameters but do not uncover causal relationships. Hence, they suffer from a relative lack of explainability. In this work, a Causal Explainer, dubbed Causalainer, is proposed to address this issue. Multiple meaningful random variables and their joint distributions are introduced to characterize the behaviors of key components in th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#22810;&#20219;&#21153;&#32467;&#26500;&#23398;&#20064;&#65288;MTSL&#65289;&#8221;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#22810;&#20219;&#21153;&#26550;&#26500;&#21450;&#20854;&#21442;&#25968;&#65292;&#20854;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#23558;&#23616;&#37096;&#20219;&#21153;&#30456;&#20284;&#24615;&#32435;&#20837;&#31070;&#32463;&#20803;&#21019;&#24314;&#21644;&#31227;&#38500;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.00441</link><description>&lt;p&gt;
&#22522;&#20110;&#23616;&#37096;&#20219;&#21153;&#30456;&#20284;&#24615;&#21551;&#21457;&#30340;&#31070;&#32463;&#20803;&#21019;&#24314;&#21644;&#31227;&#38500;&#30340;&#22810;&#20219;&#21153;&#32467;&#26500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Structural Learning using Local Task Similarity induced Neuron Creation and Removal. (arXiv:2305.00441v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#22810;&#20219;&#21153;&#32467;&#26500;&#23398;&#20064;&#65288;MTSL&#65289;&#8221;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#22810;&#20219;&#21153;&#26550;&#26500;&#21450;&#20854;&#21442;&#25968;&#65292;&#20854;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#23558;&#23616;&#37096;&#20219;&#21153;&#30456;&#20284;&#24615;&#32435;&#20837;&#31070;&#32463;&#20803;&#21019;&#24314;&#21644;&#31227;&#38500;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#20855;&#26377;&#36890;&#36807;&#26368;&#22823;&#21270;&#20219;&#21153;&#38388;&#27491;&#21521;&#36716;&#31227;&#26469;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12289;&#20943;&#23569;&#20219;&#21153;&#24178;&#25200;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#35774;&#35745;&#30340;&#32593;&#32476;&#26550;&#26500;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#38745;&#24577;&#65292;&#36825;&#38480;&#21046;&#20102;&#20854;&#21457;&#25381;&#28508;&#21147;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22823;&#33041;&#20013;&#30340;&#23398;&#20064;&#26159;&#36890;&#36807;&#32467;&#26500;&#24615;&#21464;&#21270;&#21644;&#31361;&#35302;&#24378;&#24230;&#21464;&#21270;&#30456;&#20114;&#20316;&#29992;&#23454;&#29616;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#22810;&#20219;&#21153;&#32467;&#26500;&#23398;&#20064;&#65288;MTSL&#65289;&#8221;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#22810;&#20219;&#21153;&#26550;&#26500;&#21450;&#20854;&#21442;&#25968;&#12290;MTSL&#20174;&#27599;&#20010;&#20219;&#21153;&#30340;&#30456;&#21516;&#21333;&#20219;&#21153;&#32593;&#32476;&#24320;&#22987;&#65292;&#20132;&#26367;&#36827;&#34892;&#20219;&#21153;&#23398;&#20064;&#21644;&#32467;&#26500;&#23398;&#20064;&#12290;&#22312;&#20219;&#21153;&#23398;&#20064;&#38454;&#27573;&#65292;&#27599;&#20010;&#32593;&#32476;&#19987;&#38376;&#22788;&#29702;&#30456;&#24212;&#30340;&#20219;&#21153;&#12290;&#22312;&#27599;&#20010;&#32467;&#26500;&#23398;&#20064;&#38454;&#27573;&#20013;&#65292;&#20174;&#26368;&#26089;&#30340;&#23618;&#24320;&#22987;&#65292;&#23616;&#37096;&#30456;&#20284;&#30340;&#20219;&#21153;&#23618;&#39318;&#20808;&#23558;&#20854;&#30693;&#35782;&#20256;&#36755;&#21040;&#26032;&#21019;&#24314;&#30340;&#32452;&#23618;&#65292;&#28982;&#21518;&#20877;&#23558;&#20854;&#21024;&#38500;&#12290;&#28982;&#21518;MTSL&#22312;&#30456;&#24212;&#30340;&#32452;&#23618;&#20013;&#20351;&#29992;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning has the potential to improve generalization by maximizing positive transfer between tasks while reducing task interference. Fully achieving this potential is hindered by manually designed architectures that remain static throughout training. On the contrary, learning in the brain occurs through structural changes that are in tandem with changes in synaptic strength. Thus, we propose \textit{Multi-Task Structural Learning (MTSL)} that simultaneously learns the multi-task architecture and its parameters. MTSL begins with an identical single-task network for each task and alternates between a task-learning phase and a structural-learning phase. In the task learning phase, each network specializes in the corresponding task. In each of the structural learning phases, starting from the earliest layer, locally similar task layers first transfer their knowledge to a newly created group layer before being removed. MTSL then uses the group layer in place of the corresponding 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#20687;&#22788;&#29702;&#25216;&#26415;&#30456;&#32467;&#21512;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#20896;&#24515;&#30149;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#25968;&#25454;&#20998;&#26512;&#12289;&#29305;&#24449;&#36873;&#25321;&#12289;&#22522;&#20110;3D CNN&#30340;&#20998;&#21106;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;Adagrad&#20248;&#21270;&#12290;&#20351;&#29992;&#35813;&#26694;&#26550;&#21487;&#20197;&#25552;&#39640;&#20896;&#24515;&#30149;&#26089;&#26399;&#26816;&#27979;&#21644;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.00411</link><description>&lt;p&gt;
&#22522;&#20110;3D CNN&#20998;&#21106;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;Adagrad&#20248;&#21270;&#30340;CHD&#26816;&#27979;&#30340;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimized Machine Learning for CHD Detection using 3D CNN-based Segmentation, Transfer Learning and Adagrad Optimization. (arXiv:2305.00411v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#20687;&#22788;&#29702;&#25216;&#26415;&#30456;&#32467;&#21512;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#20896;&#24515;&#30149;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#25968;&#25454;&#20998;&#26512;&#12289;&#29305;&#24449;&#36873;&#25321;&#12289;&#22522;&#20110;3D CNN&#30340;&#20998;&#21106;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;Adagrad&#20248;&#21270;&#12290;&#20351;&#29992;&#35813;&#26694;&#26550;&#21487;&#20197;&#25552;&#39640;&#20896;&#24515;&#30149;&#26089;&#26399;&#26816;&#27979;&#21644;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#65292;&#20896;&#24515;&#30149;&#26159;&#20027;&#35201;&#27515;&#20129;&#21407;&#22240;&#20043;&#19968;&#12290;&#26089;&#26399;&#26816;&#27979;&#21644;&#35786;&#26029;CHD&#21487;&#20197;&#25552;&#39640;&#24739;&#32773;&#39044;&#21518;&#21644;&#38477;&#20302;&#27515;&#20129;&#29575;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#20687;&#22788;&#29702;&#25216;&#26415;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;CHD&#30340;&#23384;&#22312;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#22810;&#20010;&#38454;&#27573;&#65292;&#21253;&#25324;&#25968;&#25454;&#20998;&#26512;&#12289;&#20351;&#29992;ReliefF&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#12289;&#22522;&#20110;3D CNN&#30340;&#20998;&#21106;&#12289;&#36801;&#31227;&#23398;&#20064;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#12289;&#29305;&#24449;&#34701;&#21512;&#21644;&#20998;&#31867;&#20197;&#21450;Adagrad&#20248;&#21270;&#12290;&#25552;&#20986;&#26694;&#26550;&#30340;&#31532;&#19968;&#27493;&#26159;&#20998;&#26512;&#25968;&#25454;&#65292;&#20197;&#30830;&#23450;&#21487;&#33021;&#19982;CHD&#30456;&#20851;&#30340;&#27169;&#24335;&#21644;&#30456;&#20851;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#24212;&#29992;ReliefF&#29305;&#24449;&#36873;&#25321;&#26469;&#30830;&#23450;&#26679;&#26412;&#22270;&#20687;&#20013;&#26368;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#22522;&#20110;3D CNN&#30340;&#20998;&#21106;&#25216;&#26415;&#26469;&#20998;&#21106;&#35270;&#30424;&#21644;&#40644;&#26001;&#65292;&#36825;&#26159;CHD&#35786;&#26029;&#30340;&#37325;&#35201;&#21306;&#22495;&#12290;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#20174;&#20998;&#21106;&#30340;&#22270;&#20687;&#20013;&#25552;&#21462;&#29305;&#24449;&#12290;&#28982;&#21518;&#23558;&#25552;&#21462;&#30340;&#29305;&#24449;&#36827;&#34892;&#34701;&#21512;&#65292;&#24182;&#20351;&#29992;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#36827;&#34892;&#20998;&#31867;&#12290;&#26368;&#21518;&#65292;&#20351;&#29992;Adagrad&#20248;&#21270;&#26469;&#24494;&#35843;SVM&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Globally, Coronary Heart Disease (CHD) is one of the main causes of death. Early detection of CHD can improve patient outcomes and reduce mortality rates. We propose a novel framework for predicting the presence of CHD using a combination of machine learning and image processing techniques. The framework comprises various phases, including analyzing the data, feature selection using ReliefF, 3D CNN-based segmentation, feature extraction by means of transfer learning, feature fusion as well as classification, and Adagrad optimization. The first step of the proposed framework involves analyzing the data to identify patterns and correlations that may be indicative of CHD. Next, ReliefF feature selection is applied to decide on the most relevant features from the sample images. The 3D CNN-based segmentation technique is then used to segment the optic disc and macula, which are important regions for CHD diagnosis. Feature extraction using transfer learning is performed to extract features f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22269;&#23478;&#28431;&#27934;&#25968;&#25454;&#24211;&#20449;&#24687;&#20013;&#26500;&#24314;&#28431;&#27934;&#30693;&#35782;&#22270;&#35889;&#30340;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#20851;&#31995;&#25552;&#21462;&#21644;&#23454;&#20307;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#26377;&#21161;&#20110;&#35299;&#20915;&#32593;&#32476;&#23433;&#20840;&#30693;&#35782;&#22270;&#35889;&#20013;&#32570;&#22833;&#23454;&#20307;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.00382</link><description>&lt;p&gt;
&#20174;&#22269;&#23478;&#28431;&#27934;&#25968;&#25454;&#24211;&#30340;&#25991;&#26412;&#25551;&#36848;&#20013;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
Constructing a Knowledge Graph from Textual Descriptions of Software Vulnerabilities in the National Vulnerability Database. (arXiv:2305.00382v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22269;&#23478;&#28431;&#27934;&#25968;&#25454;&#24211;&#20449;&#24687;&#20013;&#26500;&#24314;&#28431;&#27934;&#30693;&#35782;&#22270;&#35889;&#30340;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#20851;&#31995;&#25552;&#21462;&#21644;&#23454;&#20307;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#26377;&#21161;&#20110;&#35299;&#20915;&#32593;&#32476;&#23433;&#20840;&#30693;&#35782;&#22270;&#35889;&#20013;&#32570;&#22833;&#23454;&#20307;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#22312;&#22810;&#20010;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#65292;&#20363;&#22914;&#28431;&#27934;&#35780;&#20272;&#21644;&#23041;&#32961;&#20998;&#26512;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22269;&#23478;&#28431;&#27934;&#25968;&#25454;&#24211;&#30340;&#20449;&#24687;&#20013;&#26500;&#24314;&#28431;&#27934;&#30693;&#35782;&#22270;&#35889;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#12289;&#20851;&#31995;&#25552;&#21462;&#65288;RE&#65289;&#12289;&#20197;&#21450;&#20351;&#29992;&#31070;&#32463;&#27169;&#22411;&#12289;&#21551;&#21457;&#24335;&#35268;&#21017;&#21644;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#23454;&#20307;&#39044;&#27979;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#26377;&#21161;&#20110;&#35299;&#20915;&#32593;&#32476;&#23433;&#20840;&#30693;&#35782;&#22270;&#35889;&#20013;&#32570;&#22833;&#23454;&#20307;&#30340;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs have shown promise for several cybersecurity tasks, such as vulnerability assessment and threat analysis. In this work, we present a new method for constructing a vulnerability knowledge graph from information in the National Vulnerability Database (NVD). Our approach combines named entity recognition (NER), relation extraction (RE), and entity prediction using a combination of neural models, heuristic rules, and knowledge graph embeddings. We demonstrate how our method helps to fix missing entities in knowledge graphs used for cybersecurity and evaluate the performance.
&lt;/p&gt;</description></item><item><title>ReLBOT&#20351;&#29992;&#36716;&#31227;&#23398;&#20064;&#21644;&#28145;&#24230;RL&#25216;&#26415;&#26469;&#20174;&#29616;&#26377;&#30340;&#26234;&#33021;&#24314;&#31569;&#20013;&#20256;&#36882;&#20248;&#21270;&#21442;&#25968;&#21040;&#26032;&#30340;&#24314;&#31569;&#20013;&#65292;&#20197;&#20943;&#23569;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#24341;&#36215;&#30340;&#21021;&#22987;&#19981;&#36866;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#39118;&#38505;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#28909;&#36523;&#26399;&#26102;&#38271;6.2&#20493;&#30340;&#25552;&#39640;&#21644;&#39044;&#27979;&#26041;&#24046;&#30340;132&#20493;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.00365</link><description>&lt;p&gt;
ReLBOT&#65306;&#19968;&#31181;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#20197;&#26368;&#23567;&#21270;&#26234;&#33021;&#24314;&#31569;&#20013;&#24378;&#21270;&#23398;&#20064;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
ReLBOT: A Transfer Learning Approach to Minimize Reinforcement Learning Risks in Smart Buildings. (arXiv:2305.00365v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00365
&lt;/p&gt;
&lt;p&gt;
ReLBOT&#20351;&#29992;&#36716;&#31227;&#23398;&#20064;&#21644;&#28145;&#24230;RL&#25216;&#26415;&#26469;&#20174;&#29616;&#26377;&#30340;&#26234;&#33021;&#24314;&#31569;&#20013;&#20256;&#36882;&#20248;&#21270;&#21442;&#25968;&#21040;&#26032;&#30340;&#24314;&#31569;&#20013;&#65292;&#20197;&#20943;&#23569;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#24341;&#36215;&#30340;&#21021;&#22987;&#19981;&#36866;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#39118;&#38505;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#28909;&#36523;&#26399;&#26102;&#38271;6.2&#20493;&#30340;&#25552;&#39640;&#21644;&#39044;&#27979;&#26041;&#24046;&#30340;132&#20493;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#24314;&#31569;&#26088;&#22312;&#36890;&#36807;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#26469;&#20248;&#21270;&#33021;&#28304;&#28040;&#32791;&#12290;&#24403;&#26234;&#33021;&#24314;&#31569;&#25237;&#20837;&#20351;&#29992;&#26102;&#65292;&#27809;&#26377;&#21382;&#21490;&#25968;&#25454;&#21487;&#29992;&#20110;&#35757;&#32451;&#36825;&#20123;&#31639;&#27861;&#12290;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#26174;&#31034;&#20986;&#37325;&#35201;&#30340;&#21069;&#26223;&#65292;&#20294;&#23427;&#20204;&#30340;&#37096;&#32626;&#23384;&#22312;&#37325;&#22823;&#39118;&#38505;&#65292;&#22240;&#20026;&#24403;RL&#20195;&#29702;&#26368;&#21021;&#25506;&#32034;&#20854;&#34892;&#21160;&#31354;&#38388;&#26102;&#65292;&#23427;&#21487;&#33021;&#20250;&#32473;&#24314;&#31569;&#23621;&#27665;&#24102;&#26469;&#37325;&#22823;&#19981;&#36866;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReLBOT&#30340;&#26032;&#25216;&#26415;&#65292;&#23427;&#20351;&#29992;&#36716;&#31227;&#23398;&#20064;&#32467;&#21512;&#28145;&#24230;RL&#65292;&#20174;&#29616;&#26377;&#30340;&#20248;&#21270;&#26234;&#33021;&#24314;&#31569;&#20013;&#20256;&#36882;&#30693;&#35782;&#21040;&#26032;&#25237;&#20837;&#20351;&#29992;&#30340;&#24314;&#31569;&#20013;&#65292;&#20197;&#20943;&#23569;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#28909;&#36523;&#26399;&#23545;&#24314;&#31569;&#29289;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;&#25105;&#20204;&#35777;&#26126;&#21462;&#24471;&#20102;&#21487;&#35266;&#30340;&#25104;&#26524;&#65292;&#28909;&#36523;&#26399;&#30340;&#25345;&#32493;&#26102;&#38388;&#21487;&#25552;&#39640;6.2&#20493;&#65292;&#24182;&#19988;&#39044;&#27979;&#26041;&#24046;&#21487;&#25552;&#39640;132&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Smart buildings aim to optimize energy consumption by applying artificial intelligent algorithms. When a smart building is commissioned there is no historical data that could be used to train these algorithms. On-line Reinforcement Learning (RL) algorithms have shown significant promise, but their deployment carries a significant risk, because as the RL agent initially explores its action space it could cause significant discomfort to the building residents. In this paper we present ReLBOT, a new technique that uses transfer learning in conjunction with deep RL to transfer knowledge from an existing, optimized smart building, to the newly commissioning building, to reduce the adverse impact of the reinforcement learning agent's warm-up period. We demonstrate improvements of up to 6.2 times in the duration, and up to 132 times in prediction variance for the reinforcement learning agent's warm-up period.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#35270;&#39057;&#29255;&#27573;&#21644;&#31934;&#21326;&#37096;&#20998;&#26816;&#27979;&#30340;MH-DETR&#27169;&#22411;&#65292;&#20351;&#29992;&#36328;&#27169;&#24577;Transformer&#26469;&#33719;&#21462;&#26102;&#38388;&#19978;&#23545;&#40784;&#30340;&#36328;&#27169;&#24577;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#26102;&#38388;&#20869;&#37096;&#19978;&#19979;&#25991;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.00355</link><description>&lt;p&gt;
MH-DETR: &#22522;&#20110;&#36328;&#27169;&#24577; Transformer &#30340;&#35270;&#39057;&#29255;&#27573;&#21644;&#31934;&#21326;&#37096;&#20998;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
MH-DETR: Video Moment and Highlight Detection with Cross-modal Transformer. (arXiv:2305.00355v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00355
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#35270;&#39057;&#29255;&#27573;&#21644;&#31934;&#21326;&#37096;&#20998;&#26816;&#27979;&#30340;MH-DETR&#27169;&#22411;&#65292;&#20351;&#29992;&#36328;&#27169;&#24577;Transformer&#26469;&#33719;&#21462;&#26102;&#38388;&#19978;&#23545;&#40784;&#30340;&#36328;&#27169;&#24577;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#26102;&#38388;&#20869;&#37096;&#19978;&#19979;&#25991;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#35270;&#39057;&#29702;&#35299;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#65292;&#35270;&#39057;&#29255;&#27573;&#21644;&#31934;&#21326;&#37096;&#20998;&#26816;&#27979;(MHD)&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#20027;&#39064;&#12290;MHD&#26088;&#22312;&#21516;&#26102;&#26412;&#22320;&#21270;&#25152;&#26377;&#26102;&#21051;&#24182;&#39044;&#27979;&#21098;&#36753;&#32423;&#26174;&#33879;&#24615;&#20998;&#25968;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;DETR-based&#26041;&#27861;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#26041;&#27861;&#31895;&#30053;&#22320;&#34701;&#21512;&#20102;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#29305;&#24449;&#65292;&#36825;&#21066;&#24369;&#20102;&#26102;&#38388;&#20869;&#37096;&#19978;&#19979;&#25991;&#65292;&#24182;&#23548;&#33268;&#36328;&#27169;&#24577;&#20132;&#20114;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19987;&#38376;&#20026;MHD&#23450;&#21046;&#30340;MH-DETR (Moment and Highlight Detection Transformer)&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#22312;&#21333;&#27169;&#32534;&#30721;&#22120;&#20869;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#27719;&#38598;&#31639;&#23376;&#65292;&#20197;&#25429;&#33719;&#20840;&#23616;&#20869;&#27169;&#19978;&#19979;&#25991;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#33719;&#24471;&#26102;&#38388;&#19978;&#23545;&#40784;&#30340;&#36328;&#27169;&#24577;&#29305;&#24449;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#25554;&#25300;&#24335;&#30340;&#36328;&#27169;&#24577;&#20132;&#20114;&#27169;&#22359;&#65292;&#23558;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#26080;&#32541;&#38598;&#25104;&#22312;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20043;&#38388;&#12290;&#22312;QVHighlights&#65292;Charades-STA&#65292;Activity-Net&#21644;TVSum&#25968;&#25454;&#38598;&#19978;&#30340;&#20840;&#38754;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing demand for video understanding, video moment and highlight detection (MHD) has emerged as a critical research topic. MHD aims to localize all moments and predict clip-wise saliency scores simultaneously. Despite progress made by existing DETR-based methods, we observe that these methods coarsely fuse features from different modalities, which weakens the temporal intra-modal context and results in insufficient cross-modal interaction. To address this issue, we propose MH-DETR (Moment and Highlight Detection Transformer) tailored for MHD. Specifically, we introduce a simple yet efficient pooling operator within the uni-modal encoder to capture global intra-modal context. Moreover, to obtain temporally aligned cross-modal features, we design a plug-and-play cross-modal interaction module between the encoder and decoder, seamlessly integrating visual and textual features. Comprehensive experiments on QVHighlights, Charades-STA, Activity-Net, and TVSum datasets show that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#26080;&#30417;&#30563;&#24494;&#35843;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#19978;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.00350</link><description>&lt;p&gt;
POUF: &#38754;&#21521;&#25552;&#31034;&#30340;&#26080;&#30417;&#30563;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
POUF: Prompt-oriented unsupervised fine-tuning for large pre-trained models. (arXiv:2305.00350v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#26080;&#30417;&#30563;&#24494;&#35843;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#19978;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#36817;&#24180;&#26469;&#21464;&#24471;&#26356;&#21152;&#34920;&#29616;&#20986;&#33394;&#21644;&#24378;&#22823;&#12290;&#34429;&#28982;&#36825;&#20123;&#22823;&#22411;&#27169;&#22411;&#20855;&#26377;&#38646;-shot &#33021;&#21147;&#65292;&#20294;&#36890;&#24120;&#20173;&#38656;&#35201;&#26377;&#26631;&#31614;&#30340;&#25968;&#25454;&#26469;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#20851;&#38190;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#24494;&#35843;&#26694;&#26550;&#65292;&#30452;&#25509;&#22312;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#19978;&#24494;&#35843;&#27169;&#22411;&#25110;&#25552;&#31034;&#12290;&#25105;&#20204;&#28436;&#31034;&#22914;&#20309;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#35821;&#35328;&#22686;&#24378;&#30340;&#35270;&#35273;&#21644;&#25513;&#34109;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#40784;&#20174;&#25552;&#31034;&#21644;&#30446;&#26631;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#31163;&#25955;&#20998;&#24067;&#26469;&#23454;&#29616;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#25105;&#20204;&#23545;&#22270;&#20687;&#20998;&#31867;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#22312; 13 &#20010;&#19982;&#22270;&#20687;&#30456;&#20851;&#30340;&#20219;&#21153;&#21644; 15 &#20010;&#19982;&#35821;&#35328;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#65292;&#35813;&#26041;&#27861;&#22343;&#27604;&#22522;&#32447;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Through prompting, large-scale pre-trained models have become more expressive and powerful, gaining significant attention in recent years. Though these big models have zero-shot capabilities, in general, labeled data are still required to adapt them to downstream tasks. To overcome this critical limitation, we propose an unsupervised fine-tuning framework to directly fine-tune the model or prompt on the unlabeled target data. We demonstrate how to apply our method to both language-augmented vision and masked-language models by aligning the discrete distributions extracted from the prompts and target data. To verify our approach's applicability, we conduct extensive experiments on image classification, sentiment analysis, and natural language inference tasks. Across 13 image-related tasks and 15 language-related ones, the proposed approach achieves consistent improvements over the baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MMSF&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#20445;&#30041;&#27169;&#24577;&#29305;&#23450;&#30340;&#30693;&#35782;&#65292;&#25552;&#39640;&#21463;&#27745;&#26579;&#22810;&#27169;&#24577;&#22270;&#20687;&#30340;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;&#65292;&#36824;&#37319;&#29992;&#20102;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#22312;V-I ReID&#20013;&#36866;&#24212;&#21463;&#27745;&#26579;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#21160;&#24577;&#24179;&#34913;&#27599;&#31181;&#27169;&#24577;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00320</link><description>&lt;p&gt;
&#22312;&#21463;&#27745;&#26579;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#19979;&#65292;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30417;&#25511;&#30340;&#35270;&#35273;-&#32418;&#22806;&#20154;&#21592;&#20877;&#35782;&#21035;&#30340;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Fusion for Visual-Infrared Person ReID in Real-World Surveillance Using Corrupted Multimodal Data. (arXiv:2305.00320v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MMSF&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#20445;&#30041;&#27169;&#24577;&#29305;&#23450;&#30340;&#30693;&#35782;&#65292;&#25552;&#39640;&#21463;&#27745;&#26579;&#22810;&#27169;&#24577;&#22270;&#20687;&#30340;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;&#65292;&#36824;&#37319;&#29992;&#20102;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#22312;V-I ReID&#20013;&#36866;&#24212;&#21463;&#27745;&#26579;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#21160;&#24577;&#24179;&#34913;&#27599;&#31181;&#27169;&#24577;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35265;&#20809;-&#32418;&#22806;&#20154;&#21592;&#20877;&#35782;&#21035;(V-I ReID)&#26088;&#22312;&#21305;&#37197;&#30001;&#20998;&#24067;&#24335;RGB&#21644;IR&#25668;&#20687;&#26426;&#25429;&#33719;&#30340;&#20010;&#20307;&#22270;&#20687;&#12290;&#30001;&#20110;V&#21644;I&#27169;&#24577;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#24322;&#65292;&#29305;&#21035;&#26159;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#26465;&#20214;&#19979;&#65292;&#22270;&#20687;&#21463;&#21040;&#27169;&#31946;&#12289;&#22122;&#22768;&#21644;&#22825;&#27668;&#31561;&#22240;&#32032;&#30340;&#24178;&#25200;&#65292;&#35813;&#20219;&#21153;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20107;&#23454;&#19978;&#65292;&#26368;&#20808;&#36827;&#30340;V-I ReID&#27169;&#22411;&#19981;&#33021;&#21033;&#29992;&#21463;&#27745;&#26579;&#30340;&#27169;&#24577;&#20449;&#24687;&#26469;&#32500;&#25345;&#39640;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;V-I ReID&#27169;&#22411;&#8212;&#8212;&#21517;&#20026;&#22810;&#27169;&#24577;&#20013;&#38388;&#27969;&#34701;&#21512;(MMSF)&#65292;&#29992;&#20110;&#25552;&#39640;&#21463;&#27745;&#26579;&#22810;&#27169;&#24577;&#22270;&#20687;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#38024;&#23545;&#22312;V-I ReID&#20013;&#20986;&#29616;&#30340;&#21463;&#27745;&#26579;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#36866;&#24212;&#20102;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#21487;&#21160;&#24577;&#24179;&#34913;&#27599;&#31181;&#27169;&#24577;&#30340;&#37325;&#35201;&#24615;&#12290;&#36817;&#26399;&#24050;&#25552;&#20986;&#35780;&#20272;&#21327;&#35758;&#20197;&#35780;&#20272;ReID&#27169;&#22411;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visible-infrared person re-identification (V-I ReID) seeks to match images of individuals captured over a distributed network of RGB and IR cameras. The task is challenging due to the significant differences between V and I modalities, especially under real-world conditions, where images are corrupted by, e.g, blur, noise, and weather. Indeed, state-of-art V-I ReID models cannot leverage corrupted modality information to sustain a high level of accuracy. In this paper, we propose an efficient model for multimodal V-I ReID -- named Multimodal Middle Stream Fusion (MMSF) -- that preserves modality-specific knowledge for improved robustness to corrupted multimodal images. In addition, three state-of-art attention-based multimodal fusion models are adapted to address corrupted multimodal data in V-I ReID, allowing to dynamically balance each modality importance. Recently, evaluation protocols have been proposed to assess the robustness of ReID models under challenging real-world scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#32570;&#38519;&#25512;&#29702;&#30340;&#22810;&#20248;&#36873;&#35821;&#20041;&#21644;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21033;&#29992;&#25552;&#20986;&#30340;&#22810;&#20248;&#20808;&#35821;&#20041;&#65292;&#23545;&#22810;&#23618;&#24863;&#30693;&#22120;(MLPs)&#36827;&#34892;&#20102;&#20248;&#20808;&#35299;&#37322;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#26465;&#20214;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00304</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#20856;&#22411;&#24615;&#30340;&#26465;&#20214;&#36923;&#36753;&#20013;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#20248;&#20808;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
A preferential interpretation of MultiLayer Perceptrons in a conditional logic with typicality. (arXiv:2305.00304v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#32570;&#38519;&#25512;&#29702;&#30340;&#22810;&#20248;&#36873;&#35821;&#20041;&#21644;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21033;&#29992;&#25552;&#20986;&#30340;&#22810;&#20248;&#20808;&#35821;&#20041;&#65292;&#23545;&#22810;&#23618;&#24863;&#30693;&#22120;(MLPs)&#36827;&#34892;&#20102;&#20248;&#20808;&#35299;&#37322;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#26465;&#20214;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30693;&#35782;&#34920;&#31034;&#20013;&#32570;&#38519;&#25512;&#29702;&#30340;&#22810;&#20248;&#36873;&#35821;&#20041;&#19982;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#32771;&#34385;&#20102;&#19968;&#31181;&#20855;&#26377;&#20856;&#22411;&#24615;&#30340;&#31616;&#21333;&#25551;&#36848;&#36923;&#36753;&#30340;&#21152;&#26435;&#30693;&#35782;&#24211;&#65292;&#22312;&#8220;&#27010;&#24565;&#23618;&#38754;&#8221;&#30340;&#22810;&#20248;&#20808;&#35821;&#20041;&#19979;&#36827;&#34892;&#12290;&#35813;&#35821;&#20041;&#34987;&#29992;&#26469;&#25552;&#20379;&#22810;&#23618;&#24863;&#30693;&#22120;(MLPs)&#30340;&#20248;&#20808;&#35299;&#37322;&#12290;&#21033;&#29992;&#27169;&#22411;&#26816;&#26597;&#21644;&#34164;&#21547;&#20851;&#31995;&#30340;&#26041;&#27861;&#39564;&#35777;MLPs&#30340;&#26465;&#20214;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we investigate the relationships between a multipreferential semantics for defeasible reasoning in knowledge representation and a multilayer neural network model. Weighted knowledge bases for a simple description logic with typicality are considered under a (many-valued) ``concept-wise" multipreference semantics. The semantics is used to provide a preferential interpretation of MultiLayer Perceptrons (MLPs). A model checking and an entailment based approach are exploited in the verification of conditional properties of MLPs.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#20307;&#21160;&#21147;&#23398;&#20248;&#21270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#30340;&#20998;&#31867;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340; CNN &#27169;&#22411;&#65292;&#29992;&#20110;&#30830;&#23450;&#24213;&#29255;&#22270;&#20687;&#30340;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.00294</link><description>&lt;p&gt;
&#22522;&#20110;&#27969;&#20307;&#21160;&#21147;&#23398;&#20248;&#21270;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#35270;&#32593;&#33180;&#24213;&#22270;&#20687;&#20998;&#31867;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improving Classification of Retinal Fundus Image Using Flow Dynamics Optimized Deep Learning Methods. (arXiv:2305.00294v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#20307;&#21160;&#21147;&#23398;&#20248;&#21270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#30340;&#20998;&#31867;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340; CNN &#27169;&#22411;&#65292;&#29992;&#20110;&#30830;&#23450;&#24213;&#29255;&#22270;&#20687;&#30340;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464; (DR) &#26159;&#19968;&#31181;&#25439;&#22351;&#35270;&#32593;&#33180;&#34880;&#31649;&#32593;&#32476;&#30340;&#38556;&#30861;&#65292;&#22914;&#26524;&#31958;&#23615;&#30149;&#24739;&#32773;&#24739;&#26377;&#36825;&#31181;&#30149;&#65292;&#21487;&#33021;&#20250;&#21361;&#21450;&#20182;&#20204;&#30340;&#35270;&#21147;&#12290;&#20351;&#29992;&#24425;&#33394;&#24213;&#29255;&#26469;&#36827;&#34892; DR &#35786;&#26029;&#38656;&#35201;&#32463;&#39564;&#20016;&#23500;&#30340;&#20020;&#24202;&#21307;&#29983;&#23545;&#24433;&#20687;&#20013;&#30340;&#32959;&#30244;&#36827;&#34892;&#35782;&#21035;&#65292;&#36825;&#21487;&#33021;&#38656;&#35201;&#19968;&#20123;&#26102;&#38388;&#12290;&#33258;&#21160;&#26816;&#27979; DR &#21487;&#33021;&#26159;&#19968;&#39033;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476; (CNN) &#22312;&#24403;&#21069;&#24773;&#20917;&#19979;&#22312;&#20998;&#31867;&#22270;&#20687;&#26041;&#38754;&#20063;&#26159;&#38750;&#24120;&#26377;&#25928;&#30340;&#65292;&#29305;&#21035;&#26159;&#19982;&#25163;&#24037;&#21046;&#20316;&#21644;&#21151;&#33021;&#26041;&#27861;&#30456;&#27604;&#12290;&#20026;&#20102;&#30830;&#20445;&#39640;&#27700;&#24179;&#30340;&#32467;&#26524;&#65292;&#30740;&#31350;&#20154;&#21592;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340; CNN &#27169;&#22411;&#65292;&#21487;&#20197;&#30830;&#23450;&#24213;&#29255;&#22270;&#20687;&#30340;&#29305;&#24449;&#12290;CNN &#36755;&#20986;&#30340;&#29305;&#24449;&#34987;&#29992;&#20110;&#25152;&#25552;&#20986;&#31995;&#32479;&#30340;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#20013;&#12290;&#36825;&#20010;&#27169;&#22411;&#21518;&#26469;&#20351;&#29992;&#20102;&#19981;&#21516;&#24418;&#24335;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diabetic Retinopathy (DR) refers to a barrier that takes place in diabetes mellitus damaging the blood vessel network present in the retina. This may endanger the subjects' vision if they have diabetes. It can take some time to perform a DR diagnosis using color fundus pictures because experienced clinicians are required to identify the tumors in the imagery used to identify the illness. Automated detection of the DR can be an extremely challenging task. Convolutional Neural Networks (CNN) are also highly effective at classifying images when applied in the present situation, particularly compared to the handmade and functionality methods employed. In order to guarantee high results, the researchers also suggested a cutting-edge CNN model that might determine the characteristics of the fundus images. The features of the CNN output were employed in various classifiers of machine learning for the proposed system. This model was later evaluated using different forms of deep learning method
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#23398;&#29983;&#23545;&#20110;&#39640;&#31561;&#25945;&#32946;&#20013;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#24863;&#30693;&#21644;&#30475;&#27861;&#12290;&#23398;&#29983;&#20204;&#26222;&#36941;&#25345;&#31215;&#26497;&#24577;&#24230;&#65292;&#24182;&#35748;&#21487;GenAI&#25552;&#20379;&#30340;&#20010;&#24615;&#21270;&#23398;&#20064;&#25903;&#25345;&#12289;&#20889;&#20316;&#12289;&#22836;&#33041;&#39118;&#26292;&#21644;&#30740;&#31350;&#20998;&#26512;&#21151;&#33021;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#23545;&#20110;&#20934;&#30830;&#24615;&#12289;&#38544;&#31169;&#12289;&#20262;&#29702;&#38382;&#39064;&#20197;&#21450;&#23545;&#20010;&#20154;&#21457;&#23637;&#12289;&#32844;&#19994;&#21069;&#26223;&#21644;&#31038;&#20250;&#20215;&#20540;&#30340;&#25285;&#24551;&#12290;</title><link>http://arxiv.org/abs/2305.00290</link><description>&lt;p&gt;
&#39640;&#31561;&#25945;&#32946;&#20013;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#23398;&#29983;&#35266;&#28857;&#65306;&#24863;&#30693;&#12289;&#30410;&#22788;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Students' Voices on Generative AI: Perceptions, Benefits, and Challenges in Higher Education. (arXiv:2305.00290v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#23398;&#29983;&#23545;&#20110;&#39640;&#31561;&#25945;&#32946;&#20013;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#24863;&#30693;&#21644;&#30475;&#27861;&#12290;&#23398;&#29983;&#20204;&#26222;&#36941;&#25345;&#31215;&#26497;&#24577;&#24230;&#65292;&#24182;&#35748;&#21487;GenAI&#25552;&#20379;&#30340;&#20010;&#24615;&#21270;&#23398;&#20064;&#25903;&#25345;&#12289;&#20889;&#20316;&#12289;&#22836;&#33041;&#39118;&#26292;&#21644;&#30740;&#31350;&#20998;&#26512;&#21151;&#33021;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#23545;&#20110;&#20934;&#30830;&#24615;&#12289;&#38544;&#31169;&#12289;&#20262;&#29702;&#38382;&#39064;&#20197;&#21450;&#23545;&#20010;&#20154;&#21457;&#23637;&#12289;&#32844;&#19994;&#21069;&#26223;&#21644;&#31038;&#20250;&#20215;&#20540;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#23398;&#29983;&#20204;&#23545;&#20110;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#25216;&#26415;&#22312;&#39640;&#31561;&#25945;&#32946;&#20013;&#30340;&#24863;&#30693;&#65292;&#32858;&#28966;&#20110;&#29087;&#24713;&#31243;&#24230;&#12289;&#21442;&#19982;&#24847;&#24895;&#12289;&#28508;&#22312;&#30410;&#22788;&#21644;&#25361;&#25112;&#20197;&#21450;&#26377;&#25928;&#25972;&#21512;&#31561;&#26041;&#38754;&#12290;&#19968;&#39033;&#35843;&#26597;&#26174;&#31034;&#65292;&#22312;&#39321;&#28207;&#26469;&#33258;&#19981;&#21516;&#23398;&#31185;&#30340;399&#21517;&#26412;&#31185;&#29983;&#21644;&#30740;&#31350;&#29983;&#20013;&#65292;&#23398;&#29983;&#20204;&#26222;&#36941;&#25345;&#31215;&#26497;&#24577;&#24230;&#65292;&#35748;&#20026;GenAI&#22312;&#25945;&#23398;&#21644;&#23398;&#20064;&#20013;&#26377;&#28508;&#21147;&#25552;&#20379;&#20010;&#24615;&#21270;&#23398;&#20064;&#25903;&#25345;&#12289;&#20889;&#20316;&#21644;&#22836;&#33041;&#39118;&#26292;&#24110;&#21161;&#20197;&#21450;&#30740;&#31350;&#20998;&#26512;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20063;&#26377;&#20154;&#34920;&#36798;&#20102;&#26377;&#20851;&#20934;&#30830;&#24615;&#12289;&#38544;&#31169;&#12289;&#20262;&#29702;&#38382;&#39064;&#20197;&#21450;&#23545;&#20010;&#20154;&#21457;&#23637;&#12289;&#32844;&#19994;&#21069;&#26223;&#21644;&#31038;&#20250;&#20215;&#20540;&#30340;&#24433;&#21709;&#30340;&#25285;&#24551;&#12290;&#26681;&#25454;John Biggs&#30340;3P&#27169;&#22411;&#65292;&#23398;&#29983;&#30340;&#24863;&#30693;&#23545;&#23398;&#20064;&#26041;&#27861;&#21644;&#32467;&#26524;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#36890;&#36807;&#29702;&#35299;&#23398;&#29983;&#30340;&#35266;&#28857;&#65292;&#25945;&#32946;&#24037;&#20316;&#32773;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#21487;&#20197;&#37327;&#36523;&#23450;&#21046;GenAI&#25216;&#26415;&#20197;&#28385;&#36275;&#38656;&#27714;&#21644;&#20851;&#27880;&#28857;&#65292;&#21516;&#26102;&#20419;&#36827;&#26377;&#25928;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores university students' perceptions of generative AI (GenAI) technologies, such as ChatGPT, in higher education, focusing on familiarity, their willingness to engage, potential benefits and challenges, and effective integration. A survey of 399 undergraduate and postgraduate students from various disciplines in Hong Kong revealed a generally positive attitude towards GenAI in teaching and learning. Students recognized the potential for personalized learning support, writing and brainstorming assistance, and research and analysis capabilities. However, concerns about accuracy, privacy, ethical issues, and the impact on personal development, career prospects, and societal values were also expressed. According to John Biggs' 3P model, student perceptions significantly influence learning approaches and outcomes. By understanding students' perceptions, educators and policymakers can tailor GenAI technologies to address needs and concerns while promoting effective learning o
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#23545;&#27573;&#33853;&#20219;&#24847;&#27169;&#22411;&#65288;SAM&#65289;&#30340;&#33021;&#21147;&#22312;&#29627;&#29827;&#30456;&#20851;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#21457;&#29616;SAM&#22312;&#38236;&#38754;&#21644;&#36879;&#26126;&#29289;&#20307;&#20013;&#24448;&#24448;&#26080;&#27861;&#26816;&#27979;&#29627;&#29827;&#65292;&#36825;&#24341;&#36215;&#20102;&#22312;&#20855;&#26377;&#21508;&#31181;&#24418;&#24335;&#30340;&#29627;&#29827;&#30340;&#23433;&#20840;&#20851;&#38190;&#24773;&#20917;&#19979;&#37096;&#32626;SAM&#30340;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2305.00278</link><description>&lt;p&gt;
&#27573;&#33853;&#20219;&#24847;&#27169;&#22411;&#65288;SAM&#65289;&#36935;&#21040;&#29627;&#29827;&#65306;&#38236;&#38754;&#21644;&#36879;&#26126;&#29289;&#20307;&#19981;&#33021;&#34987;&#36731;&#26494;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Segment Anything Model (SAM) Meets Glass: Mirror and Transparent Objects Cannot Be Easily Detected. (arXiv:2305.00278v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00278
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#23545;&#27573;&#33853;&#20219;&#24847;&#27169;&#22411;&#65288;SAM&#65289;&#30340;&#33021;&#21147;&#22312;&#29627;&#29827;&#30456;&#20851;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#21457;&#29616;SAM&#22312;&#38236;&#38754;&#21644;&#36879;&#26126;&#29289;&#20307;&#20013;&#24448;&#24448;&#26080;&#27861;&#26816;&#27979;&#29627;&#29827;&#65292;&#36825;&#24341;&#36215;&#20102;&#22312;&#20855;&#26377;&#21508;&#31181;&#24418;&#24335;&#30340;&#29627;&#29827;&#30340;&#23433;&#20840;&#20851;&#38190;&#24773;&#20917;&#19979;&#37096;&#32626;SAM&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Meta AI&#30740;&#31350;&#26368;&#36817;&#21457;&#24067;&#20102;SAM&#65288;Segment Anything Model&#65289;&#65292;&#23427;&#26159;&#22312;&#36229;&#36807;10&#20159;&#20010;&#25513;&#27169;&#30340;&#22823;&#37327;&#20998;&#21106;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#12290;&#20316;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;SAM&#22312;&#36890;&#29992;&#29289;&#20307;&#20998;&#21106;&#26041;&#38754;&#30340;&#20986;&#33394;&#24615;&#33021;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#23427;&#22312;&#21508;&#31181;&#38646;-shot&#36801;&#31227;&#20219;&#21153;&#20013;&#20855;&#26377;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#26159;&#21542;&#33021;&#22815;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35774;&#32622;&#20013;&#26816;&#27979;&#21040;&#36879;&#26126;&#29289;&#20307;&#20173;&#28982;&#26410;&#30693;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#20004;&#31181;&#19982;&#29627;&#29827;&#30456;&#20851;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65306;&#38236;&#38754;&#21644;&#36879;&#26126;&#29289;&#20307;&#12290;&#25105;&#20204;&#21457;&#29616;SAM&#32463;&#24120;&#26080;&#27861;&#26816;&#27979;&#21040;&#20004;&#31181;&#24773;&#20917;&#19979;&#30340;&#29627;&#29827;&#65292;&#36825;&#24341;&#36215;&#20102;&#22312;&#20855;&#26377;&#21508;&#31181;&#24418;&#24335;&#30340;&#29627;&#29827;&#30340;&#23433;&#20840;&#20851;&#38190;&#24773;&#20917;&#19979;&#37096;&#32626;SAM&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta AI Research has recently released SAM (Segment Anything Model) which is trained on a large segmentation dataset of over 1 billion masks. As a foundation model in the field of computer vision, SAM (Segment Anything Model) has gained attention for its impressive performance in generic object segmentation. Despite its strong capability in a wide range of zero-shot transfer tasks, it remains unknown whether SAM can detect things in challenging setups like transparent objects. In this work, we perform an empirical evaluation of two glass-related challenging scenarios: mirror and transparent objects. We found that SAM often fails to detect the glass in both scenarios, which raises concern for deploying the SAM in safety-critical situations that have various forms of glass.
&lt;/p&gt;</description></item><item><title>HiDialog&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#29992;&#20110;&#23545;&#35805;&#29702;&#35299;&#30340;&#20998;&#23618;&#27169;&#22411;&#65292;&#20854;&#36890;&#36807;&#25554;&#20837;&#29305;&#27530;&#26631;&#35760;&#21644;&#25552;&#20986;&#36718;&#27425;&#32423;&#21035;&#30340;&#27880;&#24847;&#21147;&#26469;&#24314;&#27169;&#19981;&#21516;&#36718;&#27425;&#30340;&#35821;&#20041;&#21464;&#21270;&#65292;&#24182;&#21033;&#29992;&#24322;&#26500;&#22270;&#27169;&#22359;&#26469;&#20248;&#21270;&#25152;&#23398;&#30340;&#23884;&#20837;&#12290;&#22312;&#23545;&#35805;&#20851;&#31995;&#25552;&#21462;&#65292;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#21644;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;HiDialog&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.00262</link><description>&lt;p&gt;
&#24102;&#26377;&#29305;&#27530;&#26631;&#35760;&#21644;&#36718;&#27425;&#32423;&#21035;&#27880;&#24847;&#21147;&#30340;&#20998;&#23618;&#23545;&#35805;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Dialogue Understanding with Special Tokens and Turn-level Attention. (arXiv:2305.00262v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00262
&lt;/p&gt;
&lt;p&gt;
HiDialog&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#29992;&#20110;&#23545;&#35805;&#29702;&#35299;&#30340;&#20998;&#23618;&#27169;&#22411;&#65292;&#20854;&#36890;&#36807;&#25554;&#20837;&#29305;&#27530;&#26631;&#35760;&#21644;&#25552;&#20986;&#36718;&#27425;&#32423;&#21035;&#30340;&#27880;&#24847;&#21147;&#26469;&#24314;&#27169;&#19981;&#21516;&#36718;&#27425;&#30340;&#35821;&#20041;&#21464;&#21270;&#65292;&#24182;&#21033;&#29992;&#24322;&#26500;&#22270;&#27169;&#22359;&#26469;&#20248;&#21270;&#25152;&#23398;&#30340;&#23884;&#20837;&#12290;&#22312;&#23545;&#35805;&#20851;&#31995;&#25552;&#21462;&#65292;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#21644;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;HiDialog&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#23545;&#20110;&#26631;&#20934;&#25991;&#26412;&#65292;&#26426;&#22120;&#29702;&#35299;&#23545;&#35805;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#27599;&#20010;&#36718;&#27425;&#20013;&#35821;&#20041;&#30340;&#21160;&#24577;&#21644;&#24847;&#22806;&#30340;&#21464;&#21270;&#12290;&#20026;&#20102;&#27169;&#25311;&#36825;&#31181;&#19981;&#19968;&#33268;&#30340;&#35821;&#20041;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#20998;&#23618;&#23545;&#35805;&#29702;&#35299;&#27169;&#22411;HiDialog&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#23545;&#35805;&#20013;&#25554;&#20837;&#22810;&#20010;&#29305;&#27530;&#26631;&#35760;&#65292;&#24182;&#25552;&#20986;&#36718;&#27425;&#32423;&#21035;&#30340;&#27880;&#24847;&#21147;&#26469;&#23618;&#27425;&#23398;&#20064;&#36718;&#27425;&#23884;&#20837;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#24322;&#26500;&#22270;&#27169;&#22359;&#26469;&#20248;&#21270;&#25152;&#23398;&#30340;&#23884;&#20837;&#12290;&#25105;&#20204;&#23545;&#21508;&#31181;&#23545;&#35805;&#29702;&#35299;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324;&#23545;&#35805;&#20851;&#31995;&#25552;&#21462;&#65292;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#21644;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#31616;&#21333;&#30340;&#26041;&#27861;&#22312;&#20197;&#19978;&#25152;&#26377;&#19977;&#20010;&#20219;&#21153;&#19978;&#37117;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#25152;&#26377;&#28304;&#20195;&#30721;&#37117;&#20844;&#24320;&#22312;https://github.com/ShawX825/HiDialog&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compared with standard text, understanding dialogue is more challenging for machines as the dynamic and unexpected semantic changes in each turn. To model such inconsistent semantics, we propose a simple but effective Hierarchical Dialogue Understanding model, HiDialog. Specifically, we first insert multiple special tokens into a dialogue and propose the turn-level attention to learn turn embeddings hierarchically. Then, a heterogeneous graph module is leveraged to polish the learned embeddings. We evaluate our model on various dialogue understanding tasks including dialogue relation extraction, dialogue emotion recognition, and dialogue act classification. Results show that our simple approach achieves state-of-the-art performance on all three tasks above. All our source code is publicly available at https://github.com/ShawX825/HiDialog.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#26696;&#20363;&#34920;&#31034;&#27861;&#65292;&#21487;&#29992;&#20110;&#34892;&#19994;&#37096;&#38376;&#20998;&#31867;&#20219;&#21153;&#65292;&#36890;&#36807;&#23545;&#32929;&#31080;&#25910;&#30410;&#23884;&#20837;&#30340;&#34920;&#31034;&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.00245</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#22411;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#26696;&#20363;&#34920;&#31034;&#27861;&#22312;&#34892;&#19994;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Industry Classification Using a Novel Financial Time-Series Case Representation. (arXiv:2305.00245v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#26696;&#20363;&#34920;&#31034;&#27861;&#65292;&#21487;&#29992;&#20110;&#34892;&#19994;&#37096;&#38376;&#20998;&#31867;&#20219;&#21153;&#65292;&#36890;&#36807;&#23545;&#32929;&#31080;&#25910;&#30410;&#23884;&#20837;&#30340;&#34920;&#31034;&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#39046;&#22495;&#24050;&#34987;&#35777;&#26126;&#26159;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#30340;&#20016;&#23500;&#28304;&#27849;&#65292;&#21253;&#25324;&#39044;&#27979;&#12289;&#32858;&#31867;&#21644;&#20998;&#31867;&#12290;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#35775;&#38382;&#22823;&#37327;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#21363;&#20351;&#26377;&#36739;&#23567;&#30340;&#24615;&#33021;&#25913;&#36827;&#20063;&#21487;&#20197;&#36716;&#21270;&#20026;&#26174;&#33879;&#30340;&#38468;&#21152;&#20215;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#26469;&#35299;&#20915;&#36825;&#20010;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#65292;&#21363;&#20351;&#29992;&#21382;&#21490;&#32929;&#31080;&#25910;&#30410;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#34892;&#19994;&#37096;&#38376;&#20998;&#31867;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20026;&#20160;&#20040;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#23545;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#26041;&#27861;&#21487;&#33021;&#20855;&#26377;&#19968;&#20123;&#37325;&#35201;&#30340;&#34920;&#31034;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32929;&#31080;&#25910;&#30410;&#23884;&#20837;&#30340;&#26032;&#22411;&#34920;&#31034;&#65292;&#21487;&#20197;&#20174;&#21407;&#22987;&#32929;&#31080;&#25910;&#30410;&#25968;&#25454;&#20013;&#36731;&#26494;&#35745;&#31639;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#34920;&#31034;&#27861;&#38750;&#24120;&#36866;&#21512;&#20110;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#65292;&#29992;&#20110;&#34892;&#19994;&#37096;&#38376;&#20998;&#31867;&#20219;&#21153;&#65292;&#23637;&#31034;&#20102;&#23454;&#36136;&#24615;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
The financial domain has proven to be a fertile source of challenging machine learning problems across a variety of tasks including prediction, clustering, and classification. Researchers can access an abundance of time-series data and even modest performance improvements can be translated into significant additional value. In this work, we consider the use of case-based reasoning for an important task in this domain, by using historical stock returns time-series data for industry sector classification. We discuss why time-series data can present some significant representational challenges for conventional case-based reasoning approaches, and in response, we propose a novel representation based on stock returns embeddings, which can be readily calculated from raw stock returns data. We argue that this representation is well suited to case-based reasoning and evaluate our approach using a large-scale public dataset for the industry sector classification task, demonstrating substantial 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;ChatGPT&#22312;&#25945;&#32946;&#12289;&#33829;&#38144;&#12289;&#36719;&#20214;&#24037;&#31243;&#21644;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#12289;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#25506;&#35752;ChatGPT&#20316;&#20026;&#19968;&#31181;&#39640;&#32423;&#35821;&#35328;&#20132;&#20114;&#26426;&#22120;&#20154;&#30340;&#30740;&#31350;&#29616;&#29366;&#19982;&#23454;&#36341;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2305.00237</link><description>&lt;p&gt;
ChatGPT&#22312;&#25945;&#32946;&#12289;&#33829;&#38144;&#12289;&#36719;&#20214;&#24037;&#31243;&#21644;&#21307;&#30103;&#20445;&#20581;&#26041;&#38754;&#30340;&#24212;&#29992;&#32508;&#36848;&#65306;&#20248;&#21183;&#12289;&#32570;&#38519;&#21644;&#30740;&#31350;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
A Review of ChatGPT Applications in Education, Marketing, Software Engineering, and Healthcare: Benefits, Drawbacks, and Research Directions. (arXiv:2305.00237v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;ChatGPT&#22312;&#25945;&#32946;&#12289;&#33829;&#38144;&#12289;&#36719;&#20214;&#24037;&#31243;&#21644;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#12289;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#25506;&#35752;ChatGPT&#20316;&#20026;&#19968;&#31181;&#39640;&#32423;&#35821;&#35328;&#20132;&#20114;&#26426;&#22120;&#20154;&#30340;&#30740;&#31350;&#29616;&#29366;&#19982;&#23454;&#36341;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#29983;&#25104;&#31867;&#20284;&#20110;&#20154;&#31867;&#23545;&#25991;&#26412;&#25552;&#31034;&#30340;&#22238;&#22797;&#12290;&#26368;&#26032;&#30340;ChatGPT&#29256;&#26412;&#20110;2022&#24180;11&#26376;&#25512;&#20986;&#65292;&#20854;&#24378;&#22823;&#30340;&#21151;&#33021;&#12289;&#22823;&#37327;&#30340;&#21487;&#33021;&#24212;&#29992;&#20197;&#21450;&#28389;&#29992;&#30340;&#21487;&#33021;&#24615;&#22312;&#20135;&#19994;&#21644;&#23398;&#26415;&#30028;&#24341;&#36215;&#20102;&#36720;&#21160;&#12290;&#22312;&#25776;&#20889;&#26412;&#25991;&#26102;&#65292;&#20854;&#20182;&#20960;&#20010;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;Google Bard&#21644;Meta LLaMA&#65289;&#20063;&#21018;&#21018;&#25512;&#20986;&#65292;&#35797;&#22270;&#22312;&#24222;&#22823;&#30340;&#28508;&#22312;&#24066;&#22330;&#20013;&#21344;&#25454;&#19968;&#24109;&#20043;&#22320;&#12290;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#38761;&#21629;&#24615;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#24456;&#22810;&#39046;&#22495;&#65292;&#21253;&#25324;&#25945;&#32946;&#12289;&#36719;&#20214;&#24037;&#31243;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#33829;&#38144;&#12290;&#26412;&#25991;&#23558;&#35752;&#35770;&#22312;&#27599;&#20010;&#39046;&#22495;&#20351;&#29992;&#20808;&#36827;&#35821;&#35328;&#20132;&#20114;&#26426;&#22120;&#20154;&#65288;&#20363;&#22914;ChatGPT&#65289;&#30340;&#21487;&#33021;&#24212;&#29992;&#12289;&#32570;&#38519;&#21644;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#39318;&#20808;&#31616;&#35201;&#20171;&#32461;&#20102;ChatGPT&#30340;&#21457;&#23637;&#26102;&#38388;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a type of artificial intelligence language model that uses deep learning algorithms to generate human-like responses to text-based prompts. The introduction of the latest ChatGPT version in November of 2022 has caused shockwaves in the industrial and academic communities for its powerful capabilities, plethora of possible applications, and the great possibility for abuse. At the time of writing this work, several other language models (e.g., Google Bard and Meta LLaMA) just came out in an attempt to get a foothold in the vast possible market. These models have the ability to revolutionize the way we interact with computers and have potential applications in many fields, including education, software engineering, healthcare, and marketing. In this paper, we will discuss the possible applications, drawbacks, and research directions using advanced language Chatbots (e.g., ChatGPT) in each of these fields. We first start with a brief introduction and the development timeline of 
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#36719;&#20214;&#31995;&#32479;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#23384;&#22312;&#19968;&#20123;&#29305;&#27530;&#30340;&#25361;&#25112;&#21644;&#38519;&#38449;&#65292;&#30740;&#31350;&#26174;&#31034;ML&#20351;&#33021;&#31995;&#32479;&#20855;&#26377;&#19981;&#21516;&#20110;&#20256;&#32479;&#36719;&#20214;&#24037;&#31243;&#30340;&#24320;&#21457;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2305.00233</link><description>&lt;p&gt;
&#20197;&#26368;&#20339;&#23454;&#36341;&#20026;&#25351;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards machine learning guided by best practices. (arXiv:2305.00233v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00233
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#36719;&#20214;&#31995;&#32479;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#23384;&#22312;&#19968;&#20123;&#29305;&#27530;&#30340;&#25361;&#25112;&#21644;&#38519;&#38449;&#65292;&#30740;&#31350;&#26174;&#31034;ML&#20351;&#33021;&#31995;&#32479;&#20855;&#26377;&#19981;&#21516;&#20110;&#20256;&#32479;&#36719;&#20214;&#24037;&#31243;&#30340;&#24320;&#21457;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#30340;&#36719;&#20214;&#31995;&#32479;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#20174;&#21307;&#23398;&#21040;&#36719;&#20214;&#24037;&#31243;&#65288;SE&#65289;&#12290;&#19968;&#26041;&#38754;&#65292;ML&#22312;&#24037;&#19994;&#20013;&#30340;&#27969;&#34892;&#21487;&#20197;&#20174;&#26174;&#31034;&#20854;&#22686;&#38271;&#21644;&#37319;&#29992;&#30340;&#32479;&#35745;&#25968;&#25454;&#20013;&#30475;&#21040;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#30340;&#21463;&#27426;&#36814;&#31243;&#24230;&#20063;&#21487;&#20197;&#20174;&#30740;&#31350;&#20013;&#30475;&#21040;&#65292;&#23588;&#20854;&#26159;&#22312;SE&#20013;&#65292;&#19981;&#20165;&#22312;SE&#20250;&#35758;&#21644;&#26399;&#21002;&#19978;&#21457;&#34920;&#20102;&#22810;&#39033;&#30740;&#31350;&#25104;&#26524;&#65292;&#36824;&#22312;&#36719;&#20214;&#24037;&#31243;&#20250;&#35758;&#20013;&#22810;&#20010;&#30740;&#35752;&#20250;&#21644;&#20849;&#21516;&#20030;&#21150;&#30340;&#20250;&#35758;&#19978;&#21457;&#34920;&#20102;&#30740;&#31350;&#25104;&#26524;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#24050;&#32463;&#34920;&#26126;&#26426;&#22120;&#23398;&#20064;&#23384;&#22312;&#19968;&#20123;&#29305;&#27530;&#30340;&#25361;&#25112;&#21644;&#38519;&#38449;&#12290;&#29305;&#21035;&#26159;&#65292;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;SE&#30456;&#27604;&#65292;ML&#20351;&#33021;&#31995;&#32479;&#20855;&#26377;&#19981;&#21516;&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#36825;&#20063;&#25551;&#36848;&#20102;ML&#24212;&#29992;&#36935;&#21040;&#30340;&#19968;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, machine learning (ML) is being used in software systems with multiple application fields, from medicine to software engineering (SE). On the one hand, the popularity of ML in the industry can be seen in the statistics showing its growth and adoption. On the other hand, its popularity can also be seen in research, particularly in SE, where not only have multiple studies been published in SE conferences and journals but also in the multiple workshops and co-located conferences in software engineering conferences. At the same time, researchers and practitioners have shown that machine learning has some particular challenges and pitfalls. In particular, research has shown that ML-enabled systems have a different development process than traditional SE, which also describes some of the challenges of ML applications. In order to mitigate some of the identified challenges and pitfalls, white and gray literature has proposed a set of recommendations based on their own experiences and
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#29420;&#31435;&#30340;&#23616;&#37096;&#25628;&#32034;&#27714;&#35299;&#22120;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#19968;&#33324;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65292;&#24182;&#22312;&#22823;&#22411;&#24322;&#26500;&#38382;&#39064;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#22312;&#25628;&#32034;&#12289;&#25913;&#36827;&#21644;&#36824;&#21407;&#27169;&#24335;&#19979;&#65292;&#20998;&#21035;&#25552;&#20986;&#20102;&#21487;&#33258;&#36866;&#24212;&#20462;&#25913;&#21464;&#37327;&#20540;&#30340;&#31639;&#23376;&#21644;&#39640;&#25928;&#30340;&#20030;&#21319;&#31639;&#23376;&#65292;&#20174;&#32780;&#25552;&#39640;&#24403;&#21069;&#35299;&#30340;&#36136;&#37327;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;MIPLIB2017&#30340;&#24322;&#26500;&#38382;&#39064;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.00188</link><description>&lt;p&gt;
&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#30340;&#23616;&#37096;&#25628;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Local Search for Integer Linear Programming. (arXiv:2305.00188v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#29420;&#31435;&#30340;&#23616;&#37096;&#25628;&#32034;&#27714;&#35299;&#22120;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#19968;&#33324;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65292;&#24182;&#22312;&#22823;&#22411;&#24322;&#26500;&#38382;&#39064;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#22312;&#25628;&#32034;&#12289;&#25913;&#36827;&#21644;&#36824;&#21407;&#27169;&#24335;&#19979;&#65292;&#20998;&#21035;&#25552;&#20986;&#20102;&#21487;&#33258;&#36866;&#24212;&#20462;&#25913;&#21464;&#37327;&#20540;&#30340;&#31639;&#23376;&#21644;&#39640;&#25928;&#30340;&#20030;&#21319;&#31639;&#23376;&#65292;&#20174;&#32780;&#25552;&#39640;&#24403;&#21069;&#35299;&#30340;&#36136;&#37327;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;MIPLIB2017&#30340;&#24322;&#26500;&#38382;&#39064;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#27169;&#22411;&#36866;&#29992;&#20110;&#21508;&#31181;&#23454;&#38469;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#23545;&#20110;&#20135;&#19994;&#21644;&#31649;&#29702;&#37096;&#38376;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#29420;&#31435;&#30340;&#23616;&#37096;&#25628;&#32034;&#27714;&#35299;&#22120;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#19968;&#33324;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65292;&#24182;&#22312;&#22823;&#22411;&#24322;&#26500;&#38382;&#39064;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#23616;&#37096;&#25628;&#32034;&#26694;&#26550;&#65292;&#20999;&#25442;&#19977;&#31181;&#27169;&#24335;&#65292;&#20998;&#21035;&#20026;&#25628;&#32034;&#65292;&#25913;&#36827;&#21644;&#36824;&#21407;&#27169;&#24335;&#65292;&#24182;&#35774;&#35745;&#36866;&#24212;&#19981;&#21516;&#27169;&#24335;&#30340;&#23450;&#21046;&#31639;&#23376;&#65292;&#20174;&#32780;&#26681;&#25454;&#19981;&#21516;&#24773;&#20917;&#25552;&#39640;&#24403;&#21069;&#35299;&#30340;&#36136;&#37327;&#12290;&#23545;&#20110;&#25628;&#32034;&#21644;&#36824;&#21407;&#27169;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#32039;&#36523;&#21160;&#20316;&#8221;&#30340;&#31639;&#23376;&#65292;&#23427;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#20462;&#25913;&#21464;&#37327;&#30340;&#20540;&#65292;&#35797;&#22270;&#20351;&#26576;&#20123;&#32422;&#26463;&#21464;&#24471;&#26356;&#32039;&#12290;&#23545;&#20110;&#25913;&#36827;&#27169;&#24335;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#23376;&#8220;&#20030;&#21319;&#21160;&#20316;&#8221;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#21487;&#34892;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#30446;&#26631;&#20989;&#25968;&#30340;&#36136;&#37327;&#12290;&#32467;&#21512;&#36825;&#20123;&#20869;&#23481;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#23616;&#37096;&#25628;&#32034;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#27714;&#35299;&#22120;&#65292;&#31216;&#20026;Local-ILP&#12290;&#23545;MIPLIB2017&#30340;&#24322;&#26500;&#38382;&#39064;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Local-ILP&#34920;&#29616;&#20248;&#24322;&#65292;&#21487;&#20197;&#19982;&#26368;&#20808;&#36827;&#30340;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#27714;&#35299;&#22120;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integer linear programming models a wide range of practical combinatorial optimization problems and has significant impacts in industry and management sectors. This work develops the first standalone local search solver for general integer linear programming validated on a large heterogeneous problem dataset. We propose a local search framework that switches in three modes, namely Search, Improve, and Restore modes, and design tailored operators adapted to different modes, thus improve the quality of the current solution according to different situations. For the Search and Restore modes, we propose an operator named tight move, which adaptively modifies variables' values trying to make some constraint tight. For the Improve mode, an efficient operator lift move is proposed to improve the quality of the objective function while maintaining feasibility. Putting these together, we develop a local search solver for integer linear programming called Local-ILP. Experiments conducted on the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35777;&#25454;&#25512;&#29702;&#31639;&#27861;&#21644;&#24191;&#20041;&#23398;&#20064;&#31995;&#32479;&#30340;&#23454;&#26102;&#22810;&#27169;&#24577;&#25925;&#38556;&#35786;&#26029;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#19988;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#25925;&#38556;&#35786;&#26029;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.00169</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#24191;&#20041;&#23398;&#20064;&#31995;&#32479;&#30340;&#35777;&#25454;&#23454;&#26102;&#22810;&#27169;&#24577;&#25925;&#38556;&#35786;&#26029;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Evidential Real-Time Multi-Mode Fault Diagnosis Approach Based on Broad Learning System. (arXiv:2305.00169v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35777;&#25454;&#25512;&#29702;&#31639;&#27861;&#21644;&#24191;&#20041;&#23398;&#20064;&#31995;&#32479;&#30340;&#23454;&#26102;&#22810;&#27169;&#24577;&#25925;&#38556;&#35786;&#26029;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#19988;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#25925;&#38556;&#35786;&#26029;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22810;&#31181;&#24037;&#20917;&#34920;&#29616;&#20986;&#30340;&#38750;&#39640;&#26031;&#12289;&#22810;&#27169;&#24577;&#21644;&#20013;&#24515;&#28418;&#31227;&#29305;&#24449;&#65292;&#25925;&#38556;&#35786;&#26029;&#26159;&#24037;&#19994;&#30028;&#30740;&#31350;&#30340;&#37325;&#35201;&#39046;&#22495;&#12290;&#30446;&#21069;&#65292;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#26159;&#35813;&#39046;&#22495;&#30340;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#65292;&#20294;&#23427;&#20204;&#22312;&#36830;&#32493;&#25925;&#38556;&#20998;&#31867;&#21644;&#25925;&#38556;&#20998;&#31867;&#22120;&#21442;&#25968;&#26356;&#26032;&#26041;&#38754;&#25552;&#20986;&#20102;&#25361;&#25112;&#65292;&#23588;&#20854;&#22312;&#22810;&#31181;&#25805;&#20316;&#27169;&#24335;&#21644;&#23454;&#26102;&#29615;&#22659;&#20013;&#12290;&#22240;&#27492;&#65292;&#23454;&#29616;&#24037;&#19994;&#31995;&#32479;&#30340;&#23454;&#26102;&#22810;&#27169;&#24577;&#25925;&#38556;&#35786;&#26029;&#26159;&#19968;&#20010;&#36843;&#20999;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35777;&#25454;&#25512;&#29702;&#65288;ER&#65289;&#31639;&#27861;&#26469;&#34701;&#21512;&#20449;&#24687;&#24182;&#21512;&#24182;&#26469;&#33258;&#19981;&#21516;&#22522;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#12290;&#36825;&#20123;&#22522;&#20998;&#31867;&#22120;&#20351;&#29992;&#24191;&#20041;&#23398;&#20064;&#31995;&#32479;&#65288;BLS&#65289;&#24320;&#21457;&#65292;&#20197;&#25552;&#39640;&#33391;&#22909;&#30340;&#25925;&#38556;&#35786;&#26029;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#37319;&#29992;&#20266;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#26469;&#23454;&#26102;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#12290;&#20026;&#20102;&#35777;&#26126;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#24182;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fault diagnosis is a crucial area of research in the industry due to diverse operating conditions that exhibit non-Gaussian, multi-mode, and center-drift characteristics. Currently, data-driven approaches are the main focus in the field, but they pose challenges for continuous fault classification and parameter updates of fault classifiers, particularly in multiple operating modes and real-time settings. Therefore, a pressing issue is to achieve real-time multi-mode fault diagnosis for industrial systems. To address this problem, this paper proposes a novel approach that utilizes an evidence reasoning (ER) algorithm to fuse information and merge outputs from different base classifiers. These base classifiers are developed using a broad learning system (BLS) to improve good fault diagnosis performance. Moreover, in this approach, the pseudo-label learning method is employed to update model parameters in real-time. To demonstrate the effectiveness of the proposed approach, we perform exp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24212;&#29992;&#30340;&#26368;&#20339;&#25903;&#25345;&#29615;&#22659;&#65292;&#20855;&#20307;&#37325;&#28857;&#30740;&#31350;&#20102;ML&#24320;&#21457;&#20013;&#25968;&#25454;&#31649;&#29702;&#38454;&#27573;&#30340;&#38556;&#30861;&#20197;&#21450;&#22914;&#20309;&#26500;&#24314;&#21644;&#24320;&#21457;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#35299;&#20915;&#25968;&#25454;&#31649;&#29702;&#38454;&#27573;&#32570;&#20047;&#36275;&#22815;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.00136</link><description>&lt;p&gt;
&#25552;&#20379;&#26368;&#20339;&#25903;&#25345;&#29615;&#22659;&#20248;&#21270;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Optimizing the AI Development Process by Providing the Best Support Environment. (arXiv:2305.00136v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00136
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24212;&#29992;&#30340;&#26368;&#20339;&#25903;&#25345;&#29615;&#22659;&#65292;&#20855;&#20307;&#37325;&#28857;&#30740;&#31350;&#20102;ML&#24320;&#21457;&#20013;&#25968;&#25454;&#31649;&#29702;&#38454;&#27573;&#30340;&#38556;&#30861;&#20197;&#21450;&#22914;&#20309;&#26500;&#24314;&#21644;&#24320;&#21457;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#35299;&#20915;&#25968;&#25454;&#31649;&#29702;&#38454;&#27573;&#32570;&#20047;&#36275;&#22815;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35843;&#26597;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24212;&#29992;&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#20197;&#25552;&#20379;&#26368;&#20339;&#25903;&#25345;&#29615;&#22659;&#12290;ML&#30340;&#20027;&#35201;&#38454;&#27573;&#21253;&#25324;&#38382;&#39064;&#29702;&#35299;&#65292;&#25968;&#25454;&#31649;&#29702;&#65292;&#27169;&#22411;&#26500;&#24314;&#65292;&#27169;&#22411;&#37096;&#32626;&#21644;&#32500;&#25252;&#12290;&#26412;&#39033;&#30446;&#37325;&#28857;&#30740;&#31350;&#20102;ML&#24320;&#21457;&#30340;&#25968;&#25454;&#31649;&#29702;&#38454;&#27573;&#21450;&#20854;&#38556;&#30861;&#65292;&#22240;&#20026;&#26368;&#32456;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21462;&#20915;&#20110;&#36755;&#20837;&#21040;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#31867;&#22411;&#12290;&#21457;&#29616;&#36825;&#19968;&#38454;&#27573;&#26368;&#22823;&#30340;&#38556;&#30861;&#26159;&#32570;&#20047;&#36275;&#22815;&#30340;&#27169;&#22411;&#23398;&#20064;&#25968;&#25454;&#65292;&#23588;&#20854;&#26159;&#22312;&#25968;&#25454;&#20445;&#23494;&#39046;&#22495;&#12290;&#26412;&#39033;&#30446;&#26088;&#22312;&#26500;&#24314;&#21644;&#24320;&#21457;&#19968;&#20010;&#26694;&#26550;&#65292;&#24110;&#21161;&#35299;&#20915;&#25968;&#25454;&#31649;&#29702;&#38454;&#27573;&#32570;&#20047;&#36275;&#22815;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#22810;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#21487;&#20197;&#20174;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#26032;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The purpose of this study is to investigate the development process for Artificial inelegance (AI) and machine learning (ML) applications in order to provide the best support environment. The main stages of ML are problem understanding, data management, model building, model deployment and maintenance. This project focuses on investigating the data management stage of ML development and its obstacles as it is the most important stage of machine learning development because the accuracy of the end model is relying on the kind of data fed into the model. The biggest obstacle found on this stage was the lack of sufficient data for model learning, especially in the fields where data is confidential. This project aimed to build and develop a framework for researchers and developers that can help solve the lack of sufficient data during data management stage. The framework utilizes several data augmentation techniques that can be used to generate new data from the original dataset which can 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#29289;&#32852;&#32593;&#39537;&#21160;&#26234;&#33021;&#23396;&#32593;&#24494;&#30005;&#32593;&#20013;&#26612;&#27833;&#21457;&#30005;&#26426;&#32452;&#30340;&#35843;&#24230;&#38382;&#39064;&#65292;&#36890;&#36807;&#23398;&#20064;&#21382;&#21490;&#25968;&#25454;&#29983;&#25104;&#23454;&#26102;&#20915;&#31574;&#20197;&#30830;&#20445;&#20379;&#38656;&#24179;&#34913;&#65292;&#24182;&#20943;&#23569;&#25805;&#20316;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.00127</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#29289;&#32852;&#32593;&#39537;&#21160;&#26234;&#33021;&#23396;&#32593;&#24494;&#30005;&#32593;&#30340;&#26368;&#20248;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Optimal Scheduling in IoT-Driven Smart Isolated Microgrids Based on Deep Reinforcement Learning. (arXiv:2305.00127v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#29289;&#32852;&#32593;&#39537;&#21160;&#26234;&#33021;&#23396;&#32593;&#24494;&#30005;&#32593;&#20013;&#26612;&#27833;&#21457;&#30005;&#26426;&#32452;&#30340;&#35843;&#24230;&#38382;&#39064;&#65292;&#36890;&#36807;&#23398;&#20064;&#21382;&#21490;&#25968;&#25454;&#29983;&#25104;&#23454;&#26102;&#20915;&#31574;&#20197;&#30830;&#20445;&#20379;&#38656;&#24179;&#34913;&#65292;&#24182;&#20943;&#23569;&#25805;&#20316;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#29289;&#32852;&#32593;&#39537;&#21160;&#30340;&#23396;&#31435;&#24494;&#30005;&#32593;&#20013;&#65292;&#22914;&#20309;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#36827;&#34892;&#26612;&#27833;&#21457;&#30005;&#26426;&#32452;&#30340;&#35843;&#24230;&#38382;&#39064;&#12290;&#22312;&#21487;&#20877;&#29983;&#33021;&#28304;&#21644;&#36127;&#36733;&#38656;&#27714;&#19981;&#30830;&#23450;&#24615;&#19979;&#65292;&#20805;&#20998;&#21033;&#29992;&#21487;&#20877;&#29983;&#33021;&#28304;&#12290;&#36890;&#36807;&#23398;&#20064;&#24182;&#24314;&#31435;&#21382;&#21490;&#21487;&#20877;&#29983;&#36164;&#28304;&#21644;&#36127;&#36733;&#25968;&#25454;&#30340;&#26368;&#20248;&#31574;&#30053;&#27169;&#22411;&#65292;&#20197;&#20415;&#20174;&#20808;&#21069;&#23567;&#26102;&#20869;&#30456;&#24212;&#20256;&#24863;&#22120;&#25509;&#25910;&#21040;&#30340;&#36807;&#21435;&#21487;&#20877;&#29983;&#33021;&#28304;&#21644;&#36127;&#36733;&#25968;&#25454;&#30340;&#35266;&#27979;&#20013;&#29983;&#25104;&#23454;&#26102;&#20915;&#31574;&#12290;&#30446;&#26631;&#22312;&#20110;&#30830;&#20445;&#20379;&#38656;&#24179;&#34913;&#30340;&#21069;&#25552;&#19979;&#20943;&#23569;&#25805;&#20316;&#25104;&#26412;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#26377;&#38480;&#35270;&#30028;&#19979;&#39532;&#24403;&#21069;&#36807;&#31243;&#27169;&#22411;&#65288;POMDP&#65289;&#65292;&#20854;&#20013;&#32771;&#34385;&#20102;&#26059;&#36716;&#22791;&#29992;&#12290;&#20026;&#20102;&#20811;&#26381;&#30001;&#20110;&#20108;&#36827;&#21046;&#21457;&#30005;&#26426;&#32452;&#24320;&#20851;&#20915;&#31574;&#21644;&#36830;&#32493;&#36752;&#23556;&#65288;ED&#65289;&#20915;&#31574;&#30340;&#31163;&#25955;-&#36830;&#32493;&#28151;&#21512;&#20316;&#29992;&#31354;&#38388;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#21160;&#20316;&#26377;&#38480;&#35270;&#35282;RDPG&#65288;HAFH-RDPG&#65289;&#30340;DRL&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the scheduling issue of diesel generators (DGs) in an Internet of Things (IoT)-Driven isolated microgrid (MG) by deep reinforcement learning (DRL). The renewable energy is fully exploited under the uncertainty of renewable generation and load demand. The DRL agent learns an optimal policy from history renewable and load data of previous days, where the policy can generate real-time decisions based on observations of past renewable and load data of previous hours collected by connected sensors. The goal is to reduce operating cost on the premise of ensuring supply-demand balance. In specific, a novel finite-horizon partial observable Markov decision process (POMDP) model is conceived considering the spinning reserve. In order to overcome the challenge of discrete-continuous hybrid action space due to the binary DG switching decision and continuous energy dispatch (ED) decision, a DRL algorithm, namely the hybrid action finite-horizon RDPG (HAFH-RDPG), is pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;Segment Anything Model (SAM)&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20843;&#31181;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#22312;&#20845;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#24615;&#33021;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#26377;&#25152;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.00109</link><description>&lt;p&gt;
&#25506;&#32034;Segment Anything Model (SAM)&#22312;2D&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#65306;&#20840;&#38754;&#35780;&#20272;&#21644;&#23454;&#29992;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Exploring the Zero-Shot Capabilities of the Segment Anything Model (SAM) in 2D Medical Imaging: A Comprehensive Evaluation and Practical Guideline. (arXiv:2305.00109v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;Segment Anything Model (SAM)&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20843;&#31181;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#22312;&#20845;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#24615;&#33021;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#26377;&#25152;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#20998;&#21106;&#22312;&#35786;&#26029;&#12289;&#30417;&#27979;&#21644;&#27835;&#30103;&#21508;&#31181;&#30142;&#30149;&#21644;&#30149;&#20917;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#30446;&#21069;&#65292;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#20998;&#21106;&#27169;&#22411;&#34987;&#20247;&#22810;&#19987;&#38376;&#38024;&#23545;&#27599;&#20010;&#20998;&#21106;&#20219;&#21153;&#21644;&#22270;&#20687;&#27169;&#24577;&#36827;&#34892;&#24494;&#35843;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21344;&#25454;&#20102;&#20027;&#23548;&#22320;&#20301;&#12290;&#26368;&#36817;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#21106;&#27169;&#22411;Segment Anything Model (SAM)&#65292;&#23427;&#21033;&#29992;ViT&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#21644;&#24191;&#27867;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#26469;&#20998;&#21106;&#20960;&#20046;&#20219;&#20309;&#23545;&#35937;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20843;&#31181;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#22312;&#22235;&#31181;&#24433;&#20687;&#27169;&#24577;&#30340;&#20845;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;SAM&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;SAM&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#19982;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#24403;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#26356;&#22909;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#25351;&#21335;&#65292;&#38656;&#35201;&#20351;&#29992;&#36739;&#23567;&#30340;&#38236;&#20687;&#21644;&#26356;&#22810;&#30340;&#26679;&#26412;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Segmentation in medical imaging plays a crucial role in diagnosing, monitoring, and treating various diseases and conditions. The current landscape of segmentation in the medical domain is dominated by numerous specialized deep learning models fine-tuned for each segmentation task and image modality. Recently, the Segment Anything Model (SAM), a new segmentation model, was introduced. SAM utilizes the ViT neural architecture and leverages a vast training dataset to segment almost any object. However, its generalizability to the medical domain remains unexplored. In this study, we assess the zero-shot capabilities of SAM 2D in medical imaging using eight different prompt strategies across six datasets from four imaging modalities: X-ray, ultrasound, dermatoscopy, and colonoscopy. Our results demonstrate that SAM's zero-shot performance is comparable and, in certain cases, superior to the current state-of-the-art. Based on our findings, we propose a practical guideline that requires mini
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25509;&#35302;&#24773;&#20917;&#19979;&#30340;&#21487;&#24494;&#20998;&#21018;&#20307;&#27169;&#25311;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#25509;&#35302;&#27861;&#32447;&#26041;&#21521;&#19981;&#22266;&#23450;&#26102;&#20250;&#25552;&#20379;&#19981;&#20934;&#30830;&#30340;&#26799;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36830;&#32493;&#30340;&#30896;&#25758;&#26816;&#27979;&#21644;&#21033;&#29992;&#30896;&#25758;&#26102;&#38388;&#26469;&#35745;&#31639;&#30896;&#25758;&#21518;&#30340;&#36895;&#24230;&#26469;&#25913;&#36827;&#26799;&#24230;&#35745;&#31639;&#30340;&#26041;&#27861;TOI-Velocity&#12290;</title><link>http://arxiv.org/abs/2305.00092</link><description>&lt;p&gt;
&#36890;&#36807;&#25509;&#35302;&#25913;&#36827;&#21487;&#24494;&#29289;&#29702;&#27169;&#25311;&#30340;&#26799;&#24230;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Improving Gradient Computation for Differentiable Physics Simulation with Contacts. (arXiv:2305.00092v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25509;&#35302;&#24773;&#20917;&#19979;&#30340;&#21487;&#24494;&#20998;&#21018;&#20307;&#27169;&#25311;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#25509;&#35302;&#27861;&#32447;&#26041;&#21521;&#19981;&#22266;&#23450;&#26102;&#20250;&#25552;&#20379;&#19981;&#20934;&#30830;&#30340;&#26799;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36830;&#32493;&#30340;&#30896;&#25758;&#26816;&#27979;&#21644;&#21033;&#29992;&#30896;&#25758;&#26102;&#38388;&#26469;&#35745;&#31639;&#30896;&#25758;&#21518;&#30340;&#36895;&#24230;&#26469;&#25913;&#36827;&#26799;&#24230;&#35745;&#31639;&#30340;&#26041;&#27861;TOI-Velocity&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#24494;&#20998;&#27169;&#25311;&#20351;&#26799;&#24230;&#33021;&#22815;&#21453;&#21521;&#20256;&#25773;&#21040;&#29289;&#29702;&#27169;&#25311;&#20013;&#12290;&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#65292;&#21487;&#20197;&#23398;&#20064;&#29289;&#29702;&#31995;&#32479;&#30340;&#21160;&#24577;&#21644;&#23646;&#24615;&#65292;&#25110;&#32773;&#23558;&#25972;&#20010;&#21487;&#24494;&#20998;&#27169;&#25311;&#23884;&#20837;&#21040;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#20316;&#20026;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#35268;&#21010;&#21644;&#25511;&#21046;&#65289;&#30340;&#19968;&#23618;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#21487;&#24494;&#20998;&#27169;&#25311;&#24182;&#19981;&#23436;&#21892;&#65292;&#21487;&#33021;&#20250;&#25552;&#20379;&#38169;&#35823;&#30340;&#26799;&#24230;&#65292;&#20174;&#32780;&#22312;&#23398;&#20064;&#20219;&#21153;&#20013;&#38477;&#20302;&#20854;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#25509;&#35302;&#24773;&#20917;&#19979;&#30340;&#21487;&#24494;&#20998;&#21018;&#20307;&#27169;&#25311;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#25509;&#35302;&#27861;&#32447;&#26041;&#21521;&#19981;&#22266;&#23450;&#26102;&#65292;&#29616;&#26377;&#30340;&#21487;&#24494;&#20998;&#27169;&#25311;&#26041;&#27861;&#20250;&#25552;&#20379;&#19981;&#20934;&#30830;&#30340;&#26799;&#24230;&#8212;&#8212;&#36825;&#26159;&#24403;&#25509;&#35302;&#26159;&#20004;&#20010;&#31227;&#21160;&#29289;&#20307;&#20043;&#38388;&#21457;&#29983;&#26102;&#30340;&#19968;&#33324;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#36830;&#32493;&#30340;&#30896;&#25758;&#26816;&#27979;&#21644;&#21033;&#29992;&#30896;&#25758;&#26102;&#38388;&#65288;TOI&#65289;&#26469;&#35745;&#31639;&#30896;&#25758;&#21518;&#30340;&#36895;&#24230;&#65292;&#20174;&#32780;&#25913;&#36827;&#26799;&#24230;&#35745;&#31639;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20855;&#26377;&#32852;&#31995;&#30340;&#21018;&#20307;&#31995;&#32479;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;TOI-Velocity&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentiable simulation enables gradients to be back-propagated through physics simulations. In this way, one can learn the dynamics and properties of a physics system by gradient-based optimization or embed the whole differentiable simulation as a layer in a deep learning model for downstream tasks, such as planning and control. However, differentiable simulation at its current stage is not perfect and might provide wrong gradients that deteriorate its performance in learning tasks. In this paper, we study differentiable rigid-body simulation with contacts. We find that existing differentiable simulation methods provide inaccurate gradients when the contact normal direction is not fixed - a general situation when the contacts are between two moving objects. We propose to improve gradient computation by continuous collision detection and leverage the time-of-impact (TOI) to calculate the post-collision velocities. We demonstrate our proposed method, referred to as TOI-Velocity, on tw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#30072;&#21464;-&#35821;&#20041;&#20132;&#20114;&#20316;&#29992;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#21462;&#30072;&#21464;&#31867;&#21035;&#26631;&#31614;&#65292;&#24182;&#20351;&#29992;&#21152;&#26435;&#23545;&#27604;&#25439;&#22833;&#22609;&#36896;&#20027;&#24178;&#32593;&#32476;&#30340;&#34920;&#24449;&#31354;&#38388;&#65292;&#20197;&#38480;&#21046;&#27599;&#20010;&#29289;&#20307;&#30340;&#34920;&#24449;&#21644;&#30456;&#24212;&#30340;&#30072;&#21464;&#31867;&#21035;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.00079</link><description>&lt;p&gt;
&#21033;&#29992;&#40060;&#30524;&#25968;&#25454;&#20013;&#30340;&#30072;&#21464;-&#35821;&#20041;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploiting the Distortion-Semantic Interaction in Fisheye Data. (arXiv:2305.00079v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#30072;&#21464;-&#35821;&#20041;&#20132;&#20114;&#20316;&#29992;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#21462;&#30072;&#21464;&#31867;&#21035;&#26631;&#31614;&#65292;&#24182;&#20351;&#29992;&#21152;&#26435;&#23545;&#27604;&#25439;&#22833;&#22609;&#36896;&#20027;&#24178;&#32593;&#32476;&#30340;&#34920;&#24449;&#31354;&#38388;&#65292;&#20197;&#38480;&#21046;&#27599;&#20010;&#29289;&#20307;&#30340;&#34920;&#24449;&#21644;&#30456;&#24212;&#30340;&#30072;&#21464;&#31867;&#21035;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#22609;&#36896;&#21453;&#26144;&#40060;&#30524;&#25968;&#25454;&#29305;&#23450;&#34920;&#24449;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#35813;&#31354;&#38388;&#21453;&#26144;&#20102;&#27492;&#31867;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#30072;&#21464;&#21644;&#35821;&#20041;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#24037;&#20316;&#35797;&#22270;&#36890;&#36807;&#26550;&#26500;&#21644;&#35757;&#32451;&#22686;&#24378;&#26469;&#32531;&#35299;&#36825;&#31181;&#24433;&#21709;&#65292;&#20294;&#36824;&#27809;&#26377;&#20219;&#20309;&#24037;&#20316;&#23581;&#35797;&#24341;&#23548;&#27169;&#22411;&#23398;&#20064;&#21453;&#26144;&#22266;&#26377;&#20110;&#40060;&#30524;&#25968;&#25454;&#30340;&#30072;&#21464;&#21644;&#35821;&#20041;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#30340;&#34920;&#24449;&#31354;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#21033;&#29992;&#36825;&#31181;&#20851;&#31995;&#65292;&#36890;&#36807;&#39318;&#20808;&#22522;&#20110;&#29289;&#20307;&#36317;&#22270;&#20687;&#20013;&#24515;&#30340;&#36317;&#31163;&#25552;&#21462;&#30072;&#21464;&#31867;&#21035;&#26631;&#31614;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;&#21152;&#26435;&#23545;&#27604;&#25439;&#22833;&#26469;&#22609;&#36896;&#20027;&#24178;&#32593;&#32476;&#30340;&#34920;&#24449;&#31354;&#38388;&#65292;&#20197;&#38480;&#21046;&#27599;&#20010;&#29289;&#20307;&#30340;&#34920;&#24449;&#21644;&#30456;&#24212;&#30340;&#30072;&#21464;&#31867;&#21035;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present a methodology to shape a fisheye-specific representation space that reflects the interaction between distortion and semantic context present in this data modality. Fisheye data has the wider field of view advantage over other types of cameras, but this comes at the expense of high radial distortion. As a result, objects further from the center exhibit deformations that make it difficult for a model to identify their semantic context. While previous work has attempted architectural and training augmentation changes to alleviate this effect, no work has attempted to guide the model towards learning a representation space that reflects this interaction between distortion and semantic context inherent to fisheye data. We introduce an approach to exploit this relationship by first extracting distortion class labels based on an object's distance from the center of the image. We then shape a backbone's representation space with a weighted contrastive loss that constra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;Platt&#32553;&#25918;&#21450;&#20854;&#26657;&#20934;&#26041;&#27861;&#65292;&#20854;&#29702;&#35770;&#22522;&#30784;&#24378;&#22823;&#65292;&#21487;&#20197;&#22788;&#29702;&#20998;&#24067;&#28418;&#31227;&#21644;&#23545;&#25239;&#24615;&#32467;&#26524;&#24207;&#21015;&#65292;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#22312;&#19968;&#31995;&#21015;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.00070</link><description>&lt;p&gt;
&#22312;&#32447;Platt&#32553;&#25918;&#21450;&#20854;&#26657;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Online Platt Scaling with Calibeating. (arXiv:2305.00070v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;Platt&#32553;&#25918;&#21450;&#20854;&#26657;&#20934;&#26041;&#27861;&#65292;&#20854;&#29702;&#35770;&#22522;&#30784;&#24378;&#22823;&#65292;&#21487;&#20197;&#22788;&#29702;&#20998;&#24067;&#28418;&#31227;&#21644;&#23545;&#25239;&#24615;&#32467;&#26524;&#24207;&#21015;&#65292;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#22312;&#19968;&#31995;&#21015;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#21518;&#26657;&#20934;&#26041;&#27861;&#65292;&#31216;&#20026;&#22312;&#32447;Platt&#32553;&#25918;(OPS)&#65292;&#23427;&#23558;Platt&#32553;&#25918;&#25216;&#26415;&#19982;&#22312;&#32447;&#36923;&#36753;&#22238;&#24402;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;OPS&#22914;&#20309;&#22312;&#20998;&#24067;&#28418;&#31227;&#30340;i.i.d.&#21644;&#38750;i.i.d.&#24773;&#20917;&#19979;&#24179;&#31283;&#36866;&#24212;&#12290;&#27492;&#22806;&#65292;&#24403;&#26368;&#20339;&#30340;Platt&#32553;&#25918;&#27169;&#22411;&#26412;&#36523;&#34987;&#38169;&#35823;&#26657;&#20934;&#26102;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26368;&#36817;&#24320;&#21457;&#30340;&#31216;&#20026;calibeating&#30340;&#25216;&#26415;&#26469;&#22686;&#24378;OPS&#65292;&#20351;&#20854;&#26356;&#21152;&#40065;&#26834;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#24471;&#21040;&#30340;OPS+calibeating&#26041;&#27861;&#23545;&#20110;&#23545;&#25239;&#24615;&#32467;&#26524;&#24207;&#21015;&#26159;&#20445;&#35777;&#26657;&#20934;&#30340;&#12290;&#22312;&#23454;&#39564;&#19978;&#65292;&#23427;&#22312;&#19968;&#31995;&#21015;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#22343;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#25152;&#26377;OPS&#24605;&#24819;&#25193;&#23637;&#21040;beta&#32553;&#25918;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an online post-hoc calibration method, called Online Platt Scaling (OPS), which combines the Platt scaling technique with online logistic regression. We demonstrate that OPS smoothly adapts between i.i.d. and non-i.i.d. settings with distribution drift. Further, in scenarios where the best Platt scaling model is itself miscalibrated, we enhance OPS by incorporating a recently developed technique called calibeating to make it more robust. Theoretically, our resulting OPS+calibeating method is guaranteed to be calibrated for adversarial outcome sequences. Empirically, it is effective on a range of synthetic and real-world datasets, with and without distribution drifts, achieving superior performance without hyperparameter tuning. Finally, we extend all OPS ideas to the beta scaling method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#30340;&#8220;&#23450;&#24615;&#8221;&#30456;&#20284;&#24615;&#27010;&#24565;&#65292;&#36890;&#36807;&#31867;&#22411;&#29702;&#35770;&#20316;&#20026;&#20013;&#24515;&#24605;&#24819;&#21644;&#19968;&#38454;&#36923;&#36753;&#30340;&#22522;&#26412;&#27010;&#24565;&#36827;&#34892;&#20102;&#26500;&#24314;&#12290;</title><link>http://arxiv.org/abs/2305.00065</link><description>&lt;p&gt;
&#22522;&#20110;&#36923;&#36753;&#30340;&#30456;&#20284;&#24615;
&lt;/p&gt;
&lt;p&gt;
Logic-based similarity. (arXiv:2305.00065v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#30340;&#8220;&#23450;&#24615;&#8221;&#30456;&#20284;&#24615;&#27010;&#24565;&#65292;&#36890;&#36807;&#31867;&#22411;&#29702;&#35770;&#20316;&#20026;&#20013;&#24515;&#24605;&#24819;&#21644;&#19968;&#38454;&#36923;&#36753;&#30340;&#22522;&#26412;&#27010;&#24565;&#36827;&#34892;&#20102;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#19968;&#38454;&#36923;&#36753;&#30340;&#22522;&#26412;&#27010;&#24565;&#20986;&#21457;&#65292;&#20197;&#31867;&#22411;&#29702;&#35770;&#20026;&#20013;&#24515;&#65292;&#20174;&#38646;&#24320;&#22987;&#21457;&#23637;&#20102;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#30340;&#8220;&#23450;&#24615;&#8221;&#30456;&#20284;&#24615;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper develops a {\em qualitative} and logic-based notion of similarity from the ground up using only elementary concepts of first-order logic centered around the fundamental model-theoretic notion of type.
&lt;/p&gt;</description></item><item><title>EVR+&#26159;&#19968;&#31181;&#35821;&#35328;&#25512;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#20801;&#35768;&#29983;&#25104;&#21644;&#25191;&#34892;&#31526;&#21495;&#36816;&#31639;&#31526;&#20197;&#21450;&#23558;&#22797;&#26434;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#31616;&#21333;&#20219;&#21153;&#31561;&#26041;&#24335;&#22686;&#24378;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;&#23427;&#25903;&#25345;&#26356;&#22810;&#31181;&#31867;&#30340;&#25512;&#29702;&#65292;&#20363;&#22914;&#23884;&#22871;&#24490;&#29615;&#21644;&#19981;&#21516;&#31867;&#22411;&#30340;&#36882;&#24402;&#12290;</title><link>http://arxiv.org/abs/2305.00061</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#35821;&#35328;&#25512;&#29702;&#22686;&#24378;&#22120;&#65306;&#25903;&#25345;&#21508;&#31181;&#32452;&#21512;&#25512;&#29702;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Explainable Verbal Reasoner Plus (EVR+): A Natural Language Reasoning Framework that Supports Diverse Compositional Reasoning. (arXiv:2305.00061v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00061
&lt;/p&gt;
&lt;p&gt;
EVR+&#26159;&#19968;&#31181;&#35821;&#35328;&#25512;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#20801;&#35768;&#29983;&#25104;&#21644;&#25191;&#34892;&#31526;&#21495;&#36816;&#31639;&#31526;&#20197;&#21450;&#23558;&#22797;&#26434;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#31616;&#21333;&#20219;&#21153;&#31561;&#26041;&#24335;&#22686;&#24378;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;&#23427;&#25903;&#25345;&#26356;&#22810;&#31181;&#31867;&#30340;&#25512;&#29702;&#65292;&#20363;&#22914;&#23884;&#22871;&#24490;&#29615;&#21644;&#19981;&#21516;&#31867;&#22411;&#30340;&#36882;&#24402;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25512;&#29702;&#20219;&#21153;&#65292;&#20294;&#20173;&#28982;&#38754;&#20020;&#32452;&#21512;&#25512;&#29702;&#27867;&#21270;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21487;&#35299;&#37322;&#30340;&#35821;&#35328;&#25512;&#29702;&#22686;&#24378;&#22120;&#65288;EVR+&#65289;&#8221;&#30340;&#25512;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#65306;&#65288;1&#65289;&#20801;&#35768;&#27169;&#22411;&#26126;&#30830;&#29983;&#25104;&#21644;&#25191;&#34892;&#31526;&#21495;&#36816;&#31639;&#31526;&#65292;&#65288;2&#65289;&#20197;&#28789;&#27963;&#30340;&#26041;&#24335;&#23558;&#22797;&#26434;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#31616;&#21333;&#20219;&#21153;&#12290;&#19982;&#20854;&#21069;&#36523;Explainable Verbal Reasoner (EVR)&#21644;&#37319;&#29992;&#31867;&#20284;&#24605;&#36335;&#30340;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#25903;&#25345;&#26356;&#22810;&#31181;&#31867;&#30340;&#25512;&#29702;&#65292;&#20363;&#22914;&#23884;&#22871;&#24490;&#29615;&#21644;&#19981;&#21516;&#31867;&#22411;&#30340;&#36882;&#24402;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#25512;&#29702;&#26694;&#26550;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#38656;&#35201;&#32452;&#21512;&#25512;&#29702;&#30340;5&#20010;&#20219;&#21153;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25512;&#29702;&#26694;&#26550;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;5&#20010;&#20219;&#21153;&#20013;&#30340;&#32452;&#21512;&#25512;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Languages models have been successfully applied to a variety of reasoning tasks in NLP, yet the language models still suffer from compositional generalization. In this paper we present Explainable Verbal Reasoner Plus (EVR+), a reasoning framework that enhances language models' compositional reasoning ability by (1) allowing the model to explicitly generate and execute symbolic operators, and (2) allowing the model to decompose a complex task into several simpler ones in a flexible manner. Compared with its predecessor Explainable Verbal Reasoner (EVR) and other previous approaches adopting similar ideas, our framework supports more diverse types of reasoning such as nested loops and different types of recursion. To evaluate our reasoning framework, we build a synthetic dataset with five tasks that require compositional reasoning. Results show that our reasoning framework can enhance the language model's compositional generalization performance on the five tasks, using a fine-tuned lan
&lt;/p&gt;</description></item><item><title>LAVA&#26159;&#19968;&#20010;&#23398;&#20064;&#31639;&#27861;&#26080;&#20851;&#30340;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#23398;&#20064;&#31639;&#27861;&#30340;&#32479;&#35745;&#29305;&#24615;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#23646;&#24615;&#65292;&#36890;&#36807;&#36845;&#20195;&#20272;&#35745;&#25968;&#25454;&#20540;&#26469;&#23454;&#29616;&#12290;LAVA&#27604;&#29616;&#26377;&#26041;&#27861;&#35745;&#31639;&#36895;&#24230;&#26356;&#24555;&#65292;&#31934;&#24230;&#26356;&#39640;&#65292;&#24182;&#19988;&#21487;&#20197;&#20026;&#19981;&#21516;&#30340;&#24212;&#29992;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2305.00054</link><description>&lt;p&gt;
LAVA: &#26080;&#38656;&#39044;&#23450;&#23398;&#20064;&#31639;&#27861;&#30340;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
LAVA: Data Valuation without Pre-Specified Learning Algorithms. (arXiv:2305.00054v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00054
&lt;/p&gt;
&lt;p&gt;
LAVA&#26159;&#19968;&#20010;&#23398;&#20064;&#31639;&#27861;&#26080;&#20851;&#30340;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#23398;&#20064;&#31639;&#27861;&#30340;&#32479;&#35745;&#29305;&#24615;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#23646;&#24615;&#65292;&#36890;&#36807;&#36845;&#20195;&#20272;&#35745;&#25968;&#25454;&#20540;&#26469;&#23454;&#29616;&#12290;LAVA&#27604;&#29616;&#26377;&#26041;&#27861;&#35745;&#31639;&#36895;&#24230;&#26356;&#24555;&#65292;&#31934;&#24230;&#26356;&#39640;&#65292;&#24182;&#19988;&#21487;&#20197;&#20026;&#19981;&#21516;&#30340;&#24212;&#29992;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#38382;&#39064;&#26159;&#22914;&#20309;&#20844;&#24179;&#22320;&#20998;&#37197;&#23398;&#20064;&#31639;&#27861;&#30340;&#39564;&#35777;&#24615;&#33021;&#65292;&#33268;&#20351;&#35745;&#31639;&#24471;&#21040;&#30340;&#25968;&#25454;&#20215;&#20540;&#20381;&#36182;&#20110;&#24213;&#23618;&#23398;&#20064;&#31639;&#27861;&#30340;&#35768;&#22810;&#35774;&#35745;&#36873;&#25321;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;LAVA&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#23398;&#20064;&#31639;&#27861;&#30340;&#32479;&#35745;&#29305;&#24615;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#23646;&#24615;&#65292;&#36845;&#20195;&#20272;&#35745;&#25968;&#25454;&#20540;&#65292;&#20351;&#20854;&#26080;&#35270;&#19979;&#28216;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LAVA&#27604;&#29616;&#26377;&#26041;&#27861;&#35745;&#31639;&#36895;&#24230;&#26356;&#24555;&#65292;&#31934;&#24230;&#26356;&#39640;&#65292;&#24182;&#19988;&#23427;&#21487;&#20197;&#20026;&#19981;&#21516;&#30340;&#24212;&#29992;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditionally, data valuation is posed as a problem of equitably splitting the validation performance of a learning algorithm among the training data. As a result, the calculated data values depend on many design choices of the underlying learning algorithm. However, this dependence is undesirable for many use cases of data valuation, such as setting priorities over different data sources in a data acquisition process and informing pricing mechanisms in a data marketplace. In these scenarios, data needs to be valued before the actual analysis and the choice of the learning algorithm is still undetermined then. Another side-effect of the dependence is that to assess the value of individual points, one needs to re-run the learning algorithm with and without a point, which incurs a large computation burden.  This work leapfrogs over the current limits of data valuation methods by introducing a new framework that can value training data in a way that is oblivious to the downstream learning
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#65292;&#20294;&#26159;&#20854;&#40065;&#26834;&#24615;&#20173;&#28982;&#23384;&#22312;&#38590;&#20197;&#39044;&#27979;&#30340;&#22833;&#36133;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.00050</link><description>&lt;p&gt;
&#22240;&#26524;&#25512;&#29702;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#24320;&#21551;&#22240;&#26524;&#30740;&#31350;&#30340;&#26032;&#31687;&#31456;
&lt;/p&gt;
&lt;p&gt;
Causal Reasoning and Large Language Models: Opening a New Frontier for Causality. (arXiv:2305.00050v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00050
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#65292;&#20294;&#26159;&#20854;&#40065;&#26834;&#24615;&#20173;&#28982;&#23384;&#22312;&#38590;&#20197;&#39044;&#27979;&#30340;&#22833;&#36133;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22240;&#26524;&#33021;&#21147;&#22791;&#21463;&#20105;&#35758;&#65292;&#24182;&#19988;&#23545;&#23558;&#20854;&#24212;&#29992;&#20110;&#21307;&#23398;&#12289;&#31185;&#23398;&#12289;&#27861;&#24459;&#21644;&#25919;&#31574;&#31561;&#20855;&#26377;&#31038;&#20250;&#24433;&#21709;&#21147;&#30340;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;LLMs&#21450;&#20854;&#22240;&#26524;&#25512;&#29702;&#30340;&#21306;&#21035;&#65292;&#20197;&#21450;&#28508;&#22312;&#30340;&#24314;&#26500;&#21644;&#27979;&#37327;&#25928;&#24230;&#23041;&#32961;&#12290;&#22522;&#20110;GPT-3.5&#21644;4&#30340;&#31639;&#27861;&#22312;&#22810;&#20010;&#22240;&#26524;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;LLMs&#23637;&#31034;&#20102;&#38590;&#20197;&#39044;&#27979;&#30340;&#22833;&#36133;&#27169;&#24335;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#25216;&#26415;&#26469;&#35299;&#37322;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The causal capabilities of large language models (LLMs) is a matter of significant debate, with critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. We further our understanding of LLMs and their causal implications, considering the distinctions between different types of causal reasoning tasks, as well as the entangled threats of construct and measurement validity. LLM-based methods establish new state-of-the-art accuracies on multiple causal benchmarks. Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery task (97%, 13 points gain), counterfactual reasoning task (92%, 20 points gain), and actual causality (86% accuracy in determining necessary and sufficient causes in vignettes). At the same time, LLMs exhibit unpredictable failure modes and we provide some techniques to interpret their robustness.  Crucially, LLMs perform these causal tasks while relying on sources of knowledg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26234;&#33021;&#35786;&#26029;&#26694;&#26550;&#65292;&#38024;&#23545;&#20302;&#36164;&#28304;&#29615;&#22659;&#23454;&#29616;&#26089;&#26399;&#26816;&#27979;&#21644;&#20998;&#31867;&#32954;&#37096;&#32467;&#33410;&#65292;&#24182;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.00046</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#32954;&#30284;&#35786;&#26029;&#33258;&#21160;&#21270;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#20998;&#31867;&#32954;&#37096;&#32467;&#33410;
&lt;/p&gt;
&lt;p&gt;
An automated end-to-end deep learning-based framework for lung cancer diagnosis by detecting and classifying the lung nodules. (arXiv:2305.00046v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26234;&#33021;&#35786;&#26029;&#26694;&#26550;&#65292;&#38024;&#23545;&#20302;&#36164;&#28304;&#29615;&#22659;&#23454;&#29616;&#26089;&#26399;&#26816;&#27979;&#21644;&#20998;&#31867;&#32954;&#37096;&#32467;&#33410;&#65292;&#24182;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32954;&#30284;&#26159;&#20840;&#29699;&#30284;&#30151;&#30456;&#20851;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#26089;&#26399;&#35786;&#26029;&#23545;&#20110;&#25913;&#21892;&#24739;&#32773;&#30103;&#25928;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#33258;&#21160;&#21270;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#29992;&#20110;&#26089;&#26399;&#26816;&#27979;&#21644;&#20998;&#31867;&#32954;&#37096;&#32467;&#33410;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#20302;&#36164;&#28304;&#29615;&#22659;&#12290;&#35813;&#26694;&#26550;&#30001;&#19977;&#20010;&#38454;&#27573;&#32452;&#25104;&#65306;&#20351;&#29992;&#25913;&#36827;&#30340;3D Res-U-Net&#36827;&#34892;&#32954;&#20998;&#21106;&#12289;&#20351;&#29992;YOLO-v5&#36827;&#34892;&#32467;&#33410;&#26816;&#27979;&#12289;&#20351;&#29992;&#22522;&#20110;Vision Transformer&#30340;&#26550;&#26500;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#22312;&#24320;&#25918;&#30340;&#25968;&#25454;&#38598;LUNA16&#19978;&#23545;&#35813;&#26694;&#26550;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#24615;&#33021;&#26159;&#20351;&#29992;&#21508;&#39046;&#22495;&#30340;&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#34913;&#37327;&#30340;&#12290;&#35813;&#26694;&#26550;&#22312;&#32954;&#37096;&#20998;&#21106;dice&#31995;&#25968;&#19978;&#36798;&#21040;&#20102;98.82&#65285;&#65292;&#21516;&#26102;&#26816;&#27979;&#32954;&#32467;&#33410;&#30340;&#24179;&#22343;&#20934;&#30830;&#24230;&#20026;0.76 mAP&#12290;
&lt;/p&gt;
&lt;p&gt;
Lung cancer is a leading cause of cancer-related deaths worldwide, and early detection is crucial for improving patient outcomes. Nevertheless, early diagnosis of cancer is a major challenge, particularly in low-resource settings where access to medical resources and trained radiologists is limited. The objective of this study is to propose an automated end-to-end deep learning-based framework for the early detection and classification of lung nodules, specifically for low-resource settings. The proposed framework consists of three stages: lung segmentation using a modified 3D U-Net named 3D Res-U-Net, nodule detection using YOLO-v5, and classification with a Vision Transformer-based architecture. We evaluated the proposed framework on a publicly available dataset, LUNA16. The proposed framework's performance was measured using the respective domain's evaluation matrices. The proposed framework achieved a 98.82% lung segmentation dice score while detecting the lung nodule with 0.76 mAP
&lt;/p&gt;</description></item><item><title>SAM&#26159;&#31532;&#19968;&#31181;&#21487;&#25552;&#31034;&#30340;&#22522;&#30784;&#20998;&#21106;&#27169;&#22411;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#65292;&#25552;&#31034;&#27169;&#24335;&#23545;&#20854;&#34920;&#29616;&#24433;&#21709;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2305.00035</link><description>&lt;p&gt;
SAM&#22312;&#21307;&#23398;&#22270;&#20687;&#19978;&#65306;&#19977;&#31181;&#25552;&#31034;&#27169;&#24335;&#30340;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
SAM on Medical Images: A Comprehensive Study on Three Prompt Modes. (arXiv:2305.00035v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00035
&lt;/p&gt;
&lt;p&gt;
SAM&#26159;&#31532;&#19968;&#31181;&#21487;&#25552;&#31034;&#30340;&#22522;&#30784;&#20998;&#21106;&#27169;&#22411;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#65292;&#25552;&#31034;&#27169;&#24335;&#23545;&#20854;&#34920;&#29616;&#24433;&#21709;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Segment Anything Model&#65288;SAM&#65289;&#26368;&#36817;&#24341;&#36215;&#20102;&#30740;&#31350;&#32773;&#20204;&#30340;&#20851;&#27880;&#65292;&#24182;&#21551;&#21457;&#20102;&#20182;&#20204;&#25506;&#32034;&#23427;&#22312;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;&#20316;&#20026;&#31532;&#19968;&#20010;&#29992;&#20110;&#20998;&#21106;&#20219;&#21153;&#30340;&#21487;&#25552;&#31034;&#22522;&#30784;&#27169;&#22411;&#65292;&#23427;&#22312;&#19968;&#32452;&#20855;&#26377;&#21069;&#25152;&#26410;&#26377;&#25968;&#37327;&#30340;&#22270;&#20687;&#21644;&#27880;&#37322;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#36825;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#21450;&#20854;&#21487;&#25552;&#31034;&#30340;&#24615;&#36136;&#20351;&#24471;&#35813;&#27169;&#22411;&#20855;&#26377;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;&#34429;&#28982;SAM&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#20294;&#25105;&#20204;&#20173;&#24076;&#26395;&#25506;&#31350;&#23427;&#22312;&#21307;&#23398;&#22270;&#20687;&#19978;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;&#22240;&#20026;&#25105;&#20204;&#30693;&#36947;&#65292;&#21307;&#23398;&#22270;&#20687;&#30340;&#26631;&#27880;&#33719;&#21462;&#36890;&#24120;&#38656;&#35201;&#19987;&#19994;&#20174;&#19994;&#32773;&#20184;&#20986;&#24456;&#22823;&#30340;&#21162;&#21147;&#12290;&#22240;&#27492;&#65292;&#22914;&#26524;&#23384;&#22312;&#19968;&#20010;&#21487;&#20197;&#20165;&#22522;&#20110;&#23569;&#25968;&#28857;&#25552;&#31034;&#23601;&#33021;&#32473;&#20986;&#39640;&#36136;&#37327;&#25513;&#27169;&#39044;&#27979;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#37027;&#20040;&#36825;&#20010;&#27169;&#22411;&#27627;&#26080;&#30097;&#38382;&#23558;&#25104;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#28216;&#25103;&#25913;&#21464;&#32773;&#12290;&#20026;&#20102;&#35780;&#20272;SAM&#25104;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#22522;&#30784;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#20840;&#38754;&#30740;&#31350;&#20102;&#20854;&#22312;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#65288;BraTS&#21644;LiTS&#65289;&#19978;&#30340;&#19977;&#31181;&#25552;&#31034;&#27169;&#24335;&#19979;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SAM&#30340;&#24615;&#33021;&#19982;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#20284;&#29978;&#33267;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#25552;&#31034;&#27169;&#24335;&#26174;&#33879;&#24433;&#21709;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#20123;&#24046;&#24322;&#30340;&#21407;&#22240;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Segment Anything Model (SAM) made an eye-catching debut recently and inspired many researchers to explore its potential and limitation in terms of zero-shot generalization capability. As the first promptable foundation model for segmentation tasks, it was trained on a large dataset with an unprecedented number of images and annotations. This large-scale dataset and its promptable nature endow the model with strong zero-shot generalization. Although the SAM has shown competitive performance on several datasets, we still want to investigate its zero-shot generalization on medical images. As we know, the acquisition of medical image annotation usually requires a lot of effort from professional practitioners. Therefore, if there exists a foundation model that can give high-quality mask prediction simply based on a few point prompts, this model will undoubtedly become the game changer for medical image analysis. To evaluate whether SAM has the potential to become the foundation model fo
&lt;/p&gt;</description></item><item><title>TorchBench&#26159;&#19968;&#27454;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#21487;&#20840;&#38754;&#34920;&#24449;PyTorch&#36719;&#20214;&#26632;&#30340;&#24615;&#33021;&#65292;&#25351;&#23548;&#27169;&#22411;&#12289;PyTorch&#26694;&#26550;&#21644;GPU&#24211;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.14226</link><description>&lt;p&gt;
TorchBench: &#29992;&#39640;API&#34920;&#38754;&#35206;&#30422;&#29575;&#35780;&#20272;PyTorch&#24615;&#33021;&#30340;&#22522;&#20934;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
TorchBench: Benchmarking PyTorch with High API Surface Coverage. (arXiv:2304.14226v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14226
&lt;/p&gt;
&lt;p&gt;
TorchBench&#26159;&#19968;&#27454;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#21487;&#20840;&#38754;&#34920;&#24449;PyTorch&#36719;&#20214;&#26632;&#30340;&#24615;&#33021;&#65292;&#25351;&#23548;&#27169;&#22411;&#12289;PyTorch&#26694;&#26550;&#21644;GPU&#24211;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26159;&#22810;&#20010;&#39046;&#22495;&#20013;&#30340;&#38761;&#21629;&#24615;&#25216;&#26415;&#12290;&#20026;&#20102;&#26041;&#20415;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#37096;&#32626;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;PyTorch&#26159;&#26368;&#27969;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#12290;PyTorch&#36719;&#20214;&#26632;&#30340;&#29983;&#24577;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#33410;&#30465;&#27169;&#22411;&#35757;&#32451;&#25104;&#26412;&#24182;&#20943;&#23569;&#27169;&#22411;&#25512;&#29702;&#30340;&#21709;&#24212;&#26102;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;TorchBench&#65292;&#19968;&#27454;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#29992;&#20110;&#30740;&#31350;PyTorch&#36719;&#20214;&#26632;&#30340;&#24615;&#33021;&#12290;&#19982;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#19981;&#21516;&#65292;TorchBench&#21253;&#21547;&#20102;&#35768;&#22810;&#20195;&#34920;&#24615;&#27169;&#22411;&#65292;&#35206;&#30422;&#20102;&#22823;&#37327;PyTorch API&#34920;&#38754;&#12290;TorchBench&#33021;&#22815;&#20840;&#38754;&#22320;&#34920;&#24449;PyTorch&#36719;&#20214;&#26632;&#30340;&#24615;&#33021;&#65292;&#25351;&#23548;&#27169;&#22411;&#12289;PyTorch&#26694;&#26550;&#21644;GPU&#24211;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;TorchBench&#30340;&#20004;&#20010;&#23454;&#38469;&#29992;&#20363;&#12290;&#31532;&#19968;&#65292;&#25105;&#20204;&#23545;TorchBench&#36827;&#34892;&#24615;&#33021;&#21078;&#26512;&#65292;&#20197;&#35782;&#21035;PyTorch&#30340;GPU&#24615;&#33021;&#25928;&#29575;&#38382;&#39064;&#12290;&#25105;&#20204;&#33021;&#22815;&#20248;&#21270;&#35768;&#22810;&#24615;&#33021;&#25925;&#38556;&#24182;&#21521;&#19978;&#28216;&#25552;&#20132;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) has been a revolutionary technique in various domains. To facilitate the model development and deployment, many deep learning frameworks are proposed, among which PyTorch is one of the most popular solutions. The performance of ecosystem around PyTorch is critically important, which saves the costs of training models and reduces the response time of model inferences. In this paper, we propose TorchBench, a novel benchmark suite to study the performance of PyTorch software stack. Unlike existing benchmark suites, TorchBench encloses many representative models, covering a large PyTorch API surface. TorchBench is able to comprehensively characterize the performance of the PyTorch software stack, guiding the performance optimization across models, PyTorch framework, and GPU libraries. We show two practical use cases of TorchBench. (1) We profile TorchBench to identify GPU performance inefficiencies in PyTorch. We are able to optimize many performance bugs and upstream pa
&lt;/p&gt;</description></item><item><title>Ensoul&#26159;&#19968;&#31181;&#26694;&#26550;&#65292;&#36890;&#36807;enerstatic&#32593;&#32476;&#21644;&#24320;&#25918;&#36827;&#21270;&#25216;&#26415;&#32467;&#21512;&#65292;&#21019;&#36896;&#20102;&#33021;&#22815;&#29420;&#31435;&#20110;&#20854;&#23884;&#20837;&#30340;&#22522;&#36136;&#30340;&#33258;&#32452;&#32455;&#26234;&#33021;&#36229;&#20302;&#21151;&#32791;&#31995;&#32479;(SOULS)&#12290;</title><link>http://arxiv.org/abs/2304.13863</link><description>&lt;p&gt;
Ensoul: &#36890;&#36807;&#36827;&#21270;&#30340;enerstatic&#32593;&#32476;&#21019;&#24314;&#33258;&#32452;&#32455;&#26234;&#33021;&#36229;&#20302;&#21151;&#32791;&#31995;&#32479;(SOULS)&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Ensoul: A framework for the creation of self organizing intelligent ultra low power systems (SOULS) through evolutionary enerstatic networks. (arXiv:2304.13863v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13863
&lt;/p&gt;
&lt;p&gt;
Ensoul&#26159;&#19968;&#31181;&#26694;&#26550;&#65292;&#36890;&#36807;enerstatic&#32593;&#32476;&#21644;&#24320;&#25918;&#36827;&#21270;&#25216;&#26415;&#32467;&#21512;&#65292;&#21019;&#36896;&#20102;&#33021;&#22815;&#29420;&#31435;&#20110;&#20854;&#23884;&#20837;&#30340;&#22522;&#36136;&#30340;&#33258;&#32452;&#32455;&#26234;&#33021;&#36229;&#20302;&#21151;&#32791;&#31995;&#32479;(SOULS)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Ensoul&#26159;&#19968;&#20010;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#33021;&#37327;&#31283;&#24577;(enerstatic)&#22238;&#36335;&#21644;&#24320;&#25918;&#24335;&#36827;&#21270;&#25216;&#26415;&#30340;&#32593;&#32476;&#21644;&#23884;&#22871;&#32467;&#26500;&#30340;&#32467;&#21512;&#65292;&#21019;&#24314;&#20986;&#26356;&#22810;&#30340;&#25216;&#26415;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#24320;&#21457;&#30340;&#29983;&#25104;&#25216;&#26415;&#26082;&#26159;&#28909;&#21147;&#23398;&#39537;&#21160;&#22797;&#26434;&#31995;&#32479;&#30340;&#31616;&#21333;&#32780;&#26377;&#27934;&#35265;&#30340;&#27169;&#22411;&#65292;&#20063;&#26159;&#21019;&#26032;&#25216;&#26415;&#30340;&#24378;&#22823;&#28304;&#27849;&#12290; "&#33258;&#32452;&#32455;&#26234;&#33021;&#36229;&#20302;&#21151;&#32791;&#31995;&#32479;"&#65288;SOULS&#65289;&#26159;&#19968;&#20010;&#33021;&#22815;&#25551;&#36848;&#27492;&#31867;&#29983;&#25104;&#25216;&#26415;&#21450;&#20854;&#20135;&#29983;&#30340;&#25216;&#26415;&#30340;&#26415;&#35821;&#12290;&#35813;&#26415;&#35821;&#26088;&#22312;&#25429;&#25417;&#36825;&#20123;&#25216;&#26415;&#30340;&#25277;&#35937;&#26412;&#36136;&#65292;&#21363;&#23427;&#20204;&#29420;&#31435;&#20110;&#20854;&#23884;&#20837;&#30340;&#22522;&#36136;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;SOULS&#21487;&#20197;&#26159;&#29983;&#29289;&#12289;&#20154;&#24037;&#25110;&#28151;&#21512;&#30340;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensoul is a framework proposed for the purpose of creating technologies that create more technologies through the combined use of networks, and nests, of energy homeostatic (enerstatic) loops and open-ended evolutionary techniques. Generative technologies developed by such an approach serve as both simple, yet insightful models of thermodynamically driven complex systems and as powerful sources of novel technologies. "Self Organizing intelligent Ultra Low power Systems" (SOULS) is a term that well describes the technologies produced by such a generative technology, as well as the generative technology itself. The term is meant to capture the abstract nature of such technologies as being independent of the substrate in which they are embedded. In other words, SOULS can be biological, artificial or hybrid in form.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#35299;&#20915;&#21307;&#23398;&#38382;&#39064;&#30340;&#23433;&#20840;&#24615;&#20197;&#21450;&#19982;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#26381;&#21153;&#25253;&#21578;&#30340;&#19968;&#33268;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20004;&#20010;LLMs&#37117;&#21487;&#20197;&#20197;&#23433;&#20840;&#21644;&#19968;&#33268;&#30340;&#26041;&#24335;&#28385;&#36275;&#21307;&#29983;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2304.13714</link><description>&lt;p&gt;
&#35780;&#20272;GPT-3.5&#21644;GPT-4&#22312;&#25903;&#25345;&#21307;&#30103;&#20445;&#20581;&#20449;&#24687;&#38656;&#27714;&#26041;&#38754;&#30340;&#23454;&#38469;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery. (arXiv:2304.13714v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#35299;&#20915;&#21307;&#23398;&#38382;&#39064;&#30340;&#23433;&#20840;&#24615;&#20197;&#21450;&#19982;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#26381;&#21153;&#25253;&#21578;&#30340;&#19968;&#33268;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20004;&#20010;LLMs&#37117;&#21487;&#20197;&#20197;&#23433;&#20840;&#21644;&#19968;&#33268;&#30340;&#26041;&#24335;&#28385;&#36275;&#21307;&#29983;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#65292;&#20294;&#24403;&#21069;&#30340;&#25506;&#32034;&#24182;&#26410;&#35780;&#20272;LLMs&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#23454;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#20004;&#20010;LLM&#26159;&#21542;&#21487;&#20197;&#20197;&#23433;&#20840;&#21644;&#19968;&#33268;&#30340;&#26041;&#24335;&#28385;&#36275;&#30001;&#21307;&#29983;&#25552;&#20132;&#30340;&#20449;&#24687;&#38656;&#27714;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;66&#20010;&#26469;&#33258;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#26381;&#21153;&#30340;&#38382;&#39064;&#36890;&#36807;&#31616;&#21333;&#30340;&#25552;&#31034;&#25552;&#20132;&#32473;GPT-3.5&#21644;GPT-4&#12290;12&#21517;&#21307;&#29983;&#35780;&#20272;&#20102;LLM&#21709;&#24212;&#23545;&#24739;&#32773;&#36896;&#25104;&#20260;&#23475;&#30340;&#21487;&#33021;&#24615;&#20197;&#21450;&#19982;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#26381;&#21153;&#30340;&#29616;&#26377;&#25253;&#21578;&#30340;&#19968;&#33268;&#24615;&#12290;&#21307;&#29983;&#30340;&#35780;&#20272;&#22522;&#20110;&#22810;&#25968;&#31080;&#27719;&#24635;&#12290;&#23545;&#20110;&#27809;&#26377;&#20219;&#20309;&#38382;&#39064;&#65292;&#22823;&#22810;&#25968;&#21307;&#29983;&#35748;&#20026;&#20219;&#20309;&#19968;&#20010;LLM&#21709;&#24212;&#37117;&#19981;&#20250;&#36896;&#25104;&#20260;&#23475;&#12290;&#23545;&#20110;GPT-3.5&#65292;8&#20010;&#38382;&#39064;&#30340;&#21709;&#24212;&#19982;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#25253;&#21578;&#19968;&#33268;&#65292;20&#20010;&#19981;&#19968;&#33268;&#65292;9&#20010;&#26080;&#27861;&#35780;&#20272;&#12290;&#26377;29&#20010;&#21709;&#24212;&#27809;&#26377;&#22810;&#25968;&#31080;&#34920;&#31034;&#8220;&#21516;&#24847;&#8221;&#12289;&#8220;&#19981;&#21516;&#24847;&#8221;&#21644;&#8220;&#26080;&#27861;&#35780;&#20272;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite growing interest in using large language models (LLMs) in healthcare, current explorations do not assess the real-world utility and safety of LLMs in clinical settings. Our objective was to determine whether two LLMs can serve information needs submitted by physicians as questions to an informatics consultation service in a safe and concordant manner. Sixty six questions from an informatics consult service were submitted to GPT-3.5 and GPT-4 via simple prompts. 12 physicians assessed the LLM responses' possibility of patient harm and concordance with existing reports from an informatics consultation service. Physician assessments were summarized based on majority vote. For no questions did a majority of physicians deem either LLM response as harmful. For GPT-3.5, responses to 8 questions were concordant with the informatics consult report, 20 discordant, and 9 were unable to be assessed. There were 29 responses with no majority on "Agree", "Disagree", and "Unable to assess". Fo
&lt;/p&gt;</description></item><item><title>&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#33539;&#24335;&#65292;&#21487;&#20197;&#20174;&#25968;&#25454;&#23396;&#23707;&#20013;&#21019;&#24314;AI&#27169;&#22411;&#65292;&#25361;&#25112;&#22312;&#20110;&#24314;&#31435;&#22810;&#26041;&#21512;&#20316;&#19994;&#21153;&#27169;&#24335;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#21270;&#20102;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#39033;&#30446;&#30340;&#31038;&#20250;&#25216;&#26415;&#25361;&#25112;&#21644;&#19994;&#21153;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2304.13688</link><description>&lt;p&gt;
&#21512;&#20316;&#20154;&#24037;&#26234;&#33021;&#30340;&#28508;&#21147;&#37322;&#25918;&#65306;&#20851;&#20110;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#30340;&#31038;&#20250;&#25216;&#26415;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Unlocking the Potential of Collaborative AI -- On the Socio-technical Challenges of Federated Machine Learning. (arXiv:2304.13688v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13688
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#33539;&#24335;&#65292;&#21487;&#20197;&#20174;&#25968;&#25454;&#23396;&#23707;&#20013;&#21019;&#24314;AI&#27169;&#22411;&#65292;&#25361;&#25112;&#22312;&#20110;&#24314;&#31435;&#22810;&#26041;&#21512;&#20316;&#19994;&#21153;&#27169;&#24335;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#21270;&#20102;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#39033;&#30446;&#30340;&#31038;&#20250;&#25216;&#26415;&#25361;&#25112;&#21644;&#19994;&#21153;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#31995;&#32479;&#30340;&#39072;&#35206;&#24615;&#28508;&#21147;&#28304;&#20110;&#22823;&#25968;&#25454;&#30340;&#20986;&#29616;&#65292;&#20294;&#26159;&#24456;&#22823;&#19968;&#37096;&#20998;&#25968;&#25454;&#20998;&#25955;&#22312;&#25968;&#25454;&#23396;&#23707;&#20013;&#65292;&#20854;&#28508;&#21147;&#26410;&#33021;&#24471;&#21040;&#37322;&#25918;&#12290;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#33539;&#24335;&#65292;&#21487;&#20197;&#20174;&#20998;&#25955;&#30340;&#12289;&#28508;&#22312;&#30340;&#25968;&#25454;&#23396;&#23707;&#20013;&#21019;&#24314;AI&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#22312;&#25216;&#26415;&#19978;&#21487;&#20197;&#25171;&#24320;&#25968;&#25454;&#23396;&#23707;&#65292;&#20174;&#32780;&#37322;&#25918;&#32463;&#27982;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#22810;&#20010;&#25317;&#26377;&#25968;&#25454;&#23396;&#23707;&#30340;&#26041;&#20043;&#38388;&#30340;&#21512;&#20316;&#12290;&#24314;&#31435;&#21512;&#20316;&#19994;&#21153;&#27169;&#24335;&#26159;&#22797;&#26434;&#30340;&#65292;&#36890;&#24120;&#26159;&#22833;&#36133;&#30340;&#21407;&#22240;&#12290;&#24403;&#21069;&#30340;&#25991;&#29486;&#32570;&#20047;&#25104;&#21151;&#23454;&#29616;&#21512;&#20316;AI&#39033;&#30446;&#25152;&#24517;&#39035;&#32771;&#34385;&#30340;&#25351;&#21335;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#22238;&#39038;&#12289;&#28966;&#28857;&#23567;&#32452;&#21644;&#19987;&#23478;&#35775;&#35848;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#21512;&#20316;&#19994;&#21153;&#27169;&#24335;&#30340;&#25361;&#25112;&#21644;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#30340;&#19981;&#21516;&#26041;&#38754;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#30340;&#31038;&#20250;&#25216;&#26415;&#25361;&#25112;&#21644;&#25193;&#23637;&#30340;&#19994;&#21153;&#27169;&#24335;&#65292;&#20197;&#23454;&#29616;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#39033;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;
The disruptive potential of AI systems roots in the emergence of big data. Yet, a significant portion is scattered and locked in data silos, leaving its potential untapped. Federated Machine Learning is a novel AI paradigm enabling the creation of AI models from decentralized, potentially siloed data. Hence, Federated Machine Learning could technically open data silos and therefore unlock economic potential. However, this requires collaboration between multiple parties owning data silos. Setting up collaborative business models is complex and often a reason for failure. Current literature lacks guidelines on which aspects must be considered to successfully realize collaborative AI projects. This research investigates the challenges of prevailing collaborative business models and distinct aspects of Federated Machine Learning. Through a systematic literature review, focus group, and expert interviews, we provide a systemized collection of socio-technical challenges and an extended Busin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30830;&#23450;&#26410;&#26631;&#35760;&#25968;&#25454;&#28857;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20195;&#34920;&#31243;&#24230;&#65292;&#20877;&#26681;&#25454;&#36825;&#20010;&#31243;&#24230;&#23558;&#19987;&#23478;&#30340;&#21028;&#26029;&#34701;&#20837;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#20943;&#36731;&#39044;&#27979;&#32467;&#26524;&#19982;&#19987;&#23478;&#21028;&#26029;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;</title><link>http://arxiv.org/abs/2304.11870</link><description>&lt;p&gt;
&#23558;&#19987;&#23478;&#21028;&#26029;&#34701;&#20837;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Incorporating Experts' Judgment into Machine Learning Models. (arXiv:2304.11870v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30830;&#23450;&#26410;&#26631;&#35760;&#25968;&#25454;&#28857;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20195;&#34920;&#31243;&#24230;&#65292;&#20877;&#26681;&#25454;&#36825;&#20010;&#31243;&#24230;&#23558;&#19987;&#23478;&#30340;&#21028;&#26029;&#34701;&#20837;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#20943;&#36731;&#39044;&#27979;&#32467;&#26524;&#19982;&#19987;&#23478;&#21028;&#26029;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#39044;&#27979;&#32467;&#26524;&#38750;&#24120;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#39046;&#22495;&#19987;&#23478;&#21487;&#33021;&#23545;&#39044;&#26399;&#32467;&#26524;&#26377;&#21028;&#26029;&#65292;&#36825;&#21487;&#33021;&#19982;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#30456;&#20914;&#31361;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#26159;&#35757;&#32451;&#25968;&#25454;&#21487;&#33021;&#24182;&#19981;&#23436;&#20840;&#20195;&#34920;&#20154;&#32676;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#21033;&#29992;&#19987;&#23478;&#30340;&#21028;&#26029;&#26469;&#20943;&#36731;&#20914;&#31361;&#12290;&#25105;&#20204;&#26694;&#26550;&#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#65292;&#39318;&#20808;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30830;&#23450;&#26410;&#26631;&#35760;&#25968;&#25454;&#28857;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20195;&#34920;&#31243;&#24230;&#12290;&#28982;&#21518;&#65292;&#26681;&#25454;&#36825;&#20010;&#31243;&#24230;&#65292;&#25105;&#20204;&#23558;&#19987;&#23478;&#30340;&#21028;&#26029;&#34701;&#20837;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#25913;&#27491;&#20854;&#39044;&#27979;&#32467;&#26524;&#65292;&#20854;&#20013;&#20195;&#34920;&#31243;&#24230;&#36234;&#39640;&#65292;&#25105;&#20204;&#23601;&#36234;&#23569;&#22320;&#32435;&#20837;&#19987;&#23478;&#30340;&#21028;&#26029;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#27425;&#25968;&#23383;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) models have been quite successful in predicting outcomes in many applications. However, in some cases, domain experts might have a judgment about the expected outcome that might conflict with the prediction of ML models. One main reason for this is that the training data might not be totally representative of the population. In this paper, we present a novel framework that aims at leveraging experts' judgment to mitigate the conflict. The underlying idea behind our framework is that we first determine, using a generative adversarial network, the degree of representation of an unlabeled data point in the training data. Then, based on such degree, we correct the \textcolor{black}{machine learning} model's prediction by incorporating the experts' judgment into it, where the higher that aforementioned degree of representation, the less the weight we put on the expert intuition that we add to our corrected output, and vice-versa. We perform multiple numerical experimen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#24212;&#29992;&#21644;&#32771;&#34385;&#20262;&#29702;&#21644;&#21746;&#23398;&#21407;&#21017;&#20197;&#30830;&#20445;&#21487;&#38752;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#37325;&#35201;&#24615;&#12290;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#20013;&#24102;&#26469;&#20102;&#26356;&#22810;&#25361;&#25112;&#65292;&#24517;&#39035;&#35299;&#20915;&#20559;&#35265;&#12289;&#36879;&#26126;&#24230;&#12289;&#33258;&#20027;&#26435;&#12289;&#36131;&#20219;&#21644;&#38382;&#36131;&#21046;&#31561;&#38382;&#39064;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#35299;&#20915;&#21150;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.11530</link><description>&lt;p&gt;
&#36890;&#36807;&#20262;&#29702;&#21644;&#21746;&#23398;&#21407;&#21017;&#30830;&#20445;&#21487;&#20449;&#36182;&#30340;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Ensuring Trustworthy Medical Artificial Intelligencethrough Ethical and Philosophical Principles. (arXiv:2304.11530v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#24212;&#29992;&#21644;&#32771;&#34385;&#20262;&#29702;&#21644;&#21746;&#23398;&#21407;&#21017;&#20197;&#30830;&#20445;&#21487;&#38752;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#37325;&#35201;&#24615;&#12290;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#20013;&#24102;&#26469;&#20102;&#26356;&#22810;&#25361;&#25112;&#65292;&#24517;&#39035;&#35299;&#20915;&#20559;&#35265;&#12289;&#36879;&#26126;&#24230;&#12289;&#33258;&#20027;&#26435;&#12289;&#36131;&#20219;&#21644;&#38382;&#36131;&#21046;&#31561;&#38382;&#39064;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#35299;&#20915;&#21150;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#22312;&#21307;&#30103;&#25252;&#29702;&#26041;&#38754;&#20855;&#26377;&#26497;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#36890;&#36807;&#25552;&#39640;&#21307;&#30103;&#19987;&#23478;&#21644;&#24739;&#32773;&#30340;&#20307;&#39564;&#26469;&#24443;&#24213;&#25913;&#21464;&#20247;&#22810;&#21307;&#30103;&#25252;&#29702;&#12290;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#24037;&#20855;&#22914;&#26524;&#33021;&#22815;&#34920;&#29616;&#20986;&#33394;&#29978;&#33267;&#19982;&#20020;&#24202;&#19987;&#23478;&#30340;&#27700;&#24179;&#30456;&#24403;&#65292;&#23601;&#21487;&#20197;&#20135;&#29983;&#24040;&#22823;&#30340;&#25928;&#30410;&#12290;&#22240;&#27492;&#65292;&#21457;&#23637;&#20013;&#22269;&#23478;&#21487;&#20197;&#25552;&#20379;&#20808;&#36827;&#30340;&#21307;&#30103;&#25252;&#29702;&#26381;&#21153;&#65292;&#24182;&#35299;&#20915;&#32570;&#20047;&#19987;&#19994;&#21307;&#30103;&#20174;&#19994;&#32773;&#30340;&#38382;&#39064;&#12290;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24037;&#20855;&#21487;&#20197;&#33410;&#30465;&#26102;&#38388;&#12289;&#36164;&#28304;&#21644;&#25972;&#20307;&#27835;&#30103;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#19982;&#20154;&#31867;&#30456;&#27604;&#65292;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#25581;&#31034;&#22823;&#37327;&#36755;&#20837;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#29978;&#33267;&#21487;&#20197;&#20026;&#21307;&#23398;&#25552;&#20379;&#26032;&#30340;&#22522;&#20110;&#35777;&#25454;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#30103;&#25252;&#29702;&#20013;&#25972;&#21512;&#20154;&#24037;&#26234;&#33021;&#20063;&#24102;&#26469;&#20102;&#20960;&#20010;&#20262;&#29702;&#21644;&#21746;&#23398;&#19978;&#30340;&#38382;&#39064;&#65292;&#22914;&#20559;&#35265;&#12289;&#36879;&#26126;&#24230;&#12289;&#33258;&#20027;&#26435;&#12289;&#36131;&#20219;&#21644;&#38382;&#36131;&#21046;&#65292;&#36825;&#20123;&#38382;&#39064;&#24517;&#39035;&#22312;&#23558;&#36825;&#20123;&#24037;&#20855;&#25972;&#21512;&#21040;&#20020;&#24202;&#29615;&#22659;&#20043;&#21069;&#24471;&#21040;&#35299;&#20915;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#25252;&#29702;&#20013;&#30340;&#26368;&#26032;&#24212;&#29992;&#20197;&#21450;&#32771;&#34385;&#20262;&#29702;&#21644;&#21746;&#23398;&#21407;&#21017;&#20197;&#30830;&#20445;&#21487;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#19982;&#21307;&#30103;&#25252;&#29702;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#30456;&#20851;&#30340;&#21508;&#31181;&#25361;&#25112;&#65292;&#21253;&#25324;&#25968;&#25454;&#20559;&#35265;&#12289;&#36879;&#26126;&#24230;&#30340;&#38656;&#35201;&#12289;&#33258;&#20027;&#20915;&#31574;&#30340;&#38382;&#39064;&#20197;&#21450;&#38382;&#36131;&#21046;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#28508;&#22312;&#26041;&#26696;&#65292;&#21253;&#25324;&#30830;&#20445;&#36879;&#26126;&#24230;&#21644;&#38382;&#36131;&#21046;&#30340;&#26694;&#26550;&#20197;&#21450;&#25351;&#23548;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#32773;&#32771;&#34385;&#20262;&#29702;&#21407;&#21017;&#30340;&#25351;&#21335;&#12290;&#36890;&#36807;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#24182;&#23454;&#26045;&#20262;&#29702;&#21644;&#21746;&#23398;&#21407;&#21017;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#20445;&#24320;&#21457;&#20986;&#31526;&#21512;&#35786;&#25152;&#35774;&#32622;&#30340;&#21463;&#20449;&#20219;&#30340;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) methods have great potential to revolutionize numerous medical care by enhancing the experience of medical experts and patients. AI based computer-assisted diagnosis tools can have a tremendous benefit if they can outperform or perform similarly to the level of a clinical expert. As a result, advanced healthcare services can be affordable in developing nations, and the problem of a lack of expert medical practitioners can be addressed. AI based tools can save time, resources, and overall cost for patient treatment. Furthermore, in contrast to humans, AI can uncover complex relations in the data from a large set of inputs and even lead to new evidence-based knowledge in medicine. However, integrating AI in healthcare raises several ethical and philosophical concerns, such as bias, transparency, autonomy, responsibility and accountability, which must be addressed before integrating such tools into clinical settings. In this article, we emphasize recent advanc
&lt;/p&gt;</description></item><item><title>SemEval 2023&#20030;&#21150;&#20102;LegalEval&#20849;&#20139;&#20219;&#21153;&#65292;&#21363;&#29702;&#35299;&#27861;&#24459;&#25991;&#26412;&#65292;&#21253;&#25324; &#33258;&#21160;&#32467;&#26500;&#21270;&#21644;&#35821;&#20041;&#36830;&#36143;&#21270;&#30340;&#27861;&#24459;&#25991;&#20214;&#65288;Task-A&#65289;&#65292;&#27861;&#24459;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;Task-B&#65289;&#20197;&#21450;&#33258;&#21160;&#39044;&#27979;&#27861;&#24459;&#26696;&#20214;&#32467;&#26524;&#21644;&#25552;&#20379;&#39044;&#27979;&#35299;&#37322;&#65288;Task-C&#65289;&#12290;26&#20010;&#22242;&#38431;&#25552;&#20132;&#20102;&#31995;&#32479;&#35770;&#25991;&#24182;&#22312;&#25152;&#26377;&#23376;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#20934;&#32447;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2304.09548</link><description>&lt;p&gt;
SemEval 2023 &#20219;&#21153;6: LegalEval -- &#29702;&#35299;&#27861;&#24459;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
SemEval 2023 Task 6: LegalEval -- Understanding Legal Texts. (arXiv:2304.09548v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09548
&lt;/p&gt;
&lt;p&gt;
SemEval 2023&#20030;&#21150;&#20102;LegalEval&#20849;&#20139;&#20219;&#21153;&#65292;&#21363;&#29702;&#35299;&#27861;&#24459;&#25991;&#26412;&#65292;&#21253;&#25324; &#33258;&#21160;&#32467;&#26500;&#21270;&#21644;&#35821;&#20041;&#36830;&#36143;&#21270;&#30340;&#27861;&#24459;&#25991;&#20214;&#65288;Task-A&#65289;&#65292;&#27861;&#24459;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;Task-B&#65289;&#20197;&#21450;&#33258;&#21160;&#39044;&#27979;&#27861;&#24459;&#26696;&#20214;&#32467;&#26524;&#21644;&#25552;&#20379;&#39044;&#27979;&#35299;&#37322;&#65288;Task-C&#65289;&#12290;26&#20010;&#22242;&#38431;&#25552;&#20132;&#20102;&#31995;&#32479;&#35770;&#25991;&#24182;&#22312;&#25152;&#26377;&#23376;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#20934;&#32447;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#21475;&#20247;&#22810;&#30340;&#22269;&#23478;&#65292;&#24453;&#22788;&#29702;&#30340;&#27861;&#24459;&#26696;&#20214;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#26377;&#24517;&#35201;&#24320;&#21457;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#25216;&#26415;&#65292;&#23545;&#27861;&#24459;&#25991;&#20214;&#36827;&#34892;&#22788;&#29702;&#21644;&#33258;&#21160;&#29702;&#35299;&#12290;&#20026;&#20102;&#20419;&#36827;&#22312;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#22312; SemEval 2023 &#19978;&#32452;&#32455;&#20102;&#20849;&#20139;&#20219;&#21153; LegalEval - &#29702;&#35299;&#27861;&#24459;&#25991;&#26412;&#12290;LegalEval &#20219;&#21153;&#26377;&#19977;&#20010;&#23376;&#20219;&#21153;&#65306;Task-A&#65288;&#20462;&#36766;&#35282;&#33394;&#26631;&#35760;&#65289;&#26159;&#33258;&#21160;&#23558;&#27861;&#24459;&#25991;&#20214;&#32467;&#26500;&#21270;&#20026;&#35821;&#20041;&#36830;&#36143;&#30340;&#21333;&#20803;&#65292;Task-B&#65288;&#27861;&#24459;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65289;&#22788;&#29702;&#22312;&#27861;&#24459;&#25991;&#20214;&#20013;&#35782;&#21035;&#30456;&#20851;&#23454;&#20307;&#65292;&#32780; Task-C&#65288;&#27861;&#38498;&#21028;&#20915;&#39044;&#27979;&#19982;&#35299;&#37322;&#65289;&#25506;&#32034;&#20102;&#33258;&#21160;&#39044;&#27979;&#27861;&#24459;&#26696;&#20214;&#32467;&#26524;&#20197;&#21450;&#25552;&#20379;&#39044;&#27979;&#35299;&#37322;&#30340;&#21487;&#33021;&#24615;&#12290;&#20849;&#26377;26&#20010;&#22242;&#38431;&#65288;&#20998;&#24067;&#22312;&#20840;&#29699;&#30340;&#32422;100&#21517;&#21442;&#19982;&#32773;&#65289;&#25552;&#20132;&#20102;&#31995;&#32479;&#35770;&#25991;&#12290;&#22312;&#27599;&#20010;&#23376;&#20219;&#21153;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#37117;&#20248;&#20110;&#22522;&#20934;&#32447;&#65307;&#20294;&#26159;&#65292;&#20173;&#28982;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102; LegalEval &#20219;&#21153;&#30340;&#32452;&#32455;&#21644;&#32454;&#33410;&#65292;&#24182;&#27010;&#36848;&#20102;&#21442;&#19982;&#31995;&#32479;&#21450;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In populous countries, pending legal cases have been growing exponentially. There is a need for developing NLP-based techniques for processing and automatically understanding legal documents. To promote research in the area of Legal NLP we organized the shared task LegalEval - Understanding Legal Texts at SemEval 2023. LegalEval task has three sub-tasks: Task-A (Rhetorical Roles Labeling) is about automatically structuring legal documents into semantically coherent units, Task-B (Legal Named Entity Recognition) deals with identifying relevant entities in a legal document and Task-C (Court Judgement Prediction with Explanation) explores the possibility of automatically predicting the outcome of a legal case along with providing an explanation for the prediction. In total 26 teams (approx. 100 participants spread across the world) submitted systems paper. In each of the sub-tasks, the proposed systems outperformed the baselines; however, there is a lot of scope for improvement. This pape
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#36817;&#22312;&#19981;&#21516;iable&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#20013;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25361;&#25112;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;DARTS&#26041;&#27861;&#30340;&#36129;&#29486;&#21644;&#24433;&#21709;&#36827;&#34892;&#20102;&#35752;&#35770;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.05405</link><description>&lt;p&gt;
&#19981;&#21516;iable&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#20013;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#39640;&#25928;&#33258;&#21160;&#21270;:&#19968;&#39033;&#27010;&#36848;&#30740;&#31350;(arXiv: 2304.05405v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Efficient Automation of Neural Network Design: A Survey on Differentiable Neural Architecture Search. (arXiv:2304.05405v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#36817;&#22312;&#19981;&#21516;iable&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#20013;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25361;&#25112;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;DARTS&#26041;&#27861;&#30340;&#36129;&#29486;&#21644;&#24433;&#21709;&#36827;&#34892;&#20102;&#35752;&#35770;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#19981;&#21516;iable&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;DNAS&#65289;&#36805;&#36895;&#25104;&#20026;&#33258;&#21160;&#21457;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290; &#36825;&#31181;&#23835;&#36215;&#20027;&#35201;&#24402;&#21151;&#20110;DARTS&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#37325;&#35201;&#30340;DNAS&#26041;&#27861;&#20043;&#19968;&#12290; &#19982;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#25110;&#36827;&#21270;&#31639;&#27861;&#30340;&#20197;&#21069;&#30340;&#20316;&#21697;&#30456;&#27604;&#65292;DNAS&#36895;&#24230;&#24555;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#19988;&#20351;&#29992;&#30340;&#35745;&#31639;&#36164;&#28304;&#26356;&#23569;&#12290; &#22312;&#36825;&#31687;&#20840;&#38754;&#30340;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#19987;&#38376;&#20851;&#27880;DNAS&#24182;&#23457;&#26597;&#20102;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290; &#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25361;&#25112;&#30340;&#20998;&#31867;&#27861;&#26469;&#20998;&#31867;DNAS&#26041;&#27861;&#12290; &#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#36807;&#21435;&#20960;&#24180;&#23545;DNAS&#24102;&#26469;&#30340;&#36129;&#29486;&#20197;&#21450;&#20854;&#23545;&#20840;&#29699;NAS&#39046;&#22495;&#30340;&#24433;&#21709;&#12290; &#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#19968;&#20123;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35265;&#35299;&#26469;&#20570;&#20986;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past few years, Differentiable Neural Architecture Search (DNAS) rapidly imposed itself as the trending approach to automate the discovery of deep neural network architectures. This rise is mainly due to the popularity of DARTS, one of the first major DNAS methods. In contrast with previous works based on Reinforcement Learning or Evolutionary Algorithms, DNAS is faster by several orders of magnitude and uses fewer computational resources. In this comprehensive survey, we focus specifically on DNAS and review recent approaches in this field. Furthermore, we propose a novel challenge-based taxonomy to classify DNAS methods. We also discuss the contributions brought to DNAS in the past few years and its impact on the global NAS field. Finally, we conclude by giving some insights into future research directions for the DNAS field.
&lt;/p&gt;</description></item><item><title>MEDIMP&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24335;&#23398;&#20064;&#32958;&#31227;&#26893;DCE MRI&#30340;&#21307;&#23398;&#22270;&#20687;&#21644;&#25552;&#31034;&#27169;&#22411;&#65292;&#21033;&#29992;&#32852;&#21512;&#25991;&#26412;-&#22270;&#20687;&#23884;&#20837;&#30340;&#23545;&#27604;&#23398;&#20064;&#26469;&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2303.12445</link><description>&lt;p&gt;
MEDIMP: &#29992;&#20110;&#32958;&#31227;&#26893;&#34920;&#31034;&#23398;&#20064;&#30340;&#21307;&#23398;&#22270;&#20687;&#21644;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
MEDIMP: Medical Images and Prompts for renal transplant representation learning. (arXiv:2303.12445v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12445
&lt;/p&gt;
&lt;p&gt;
MEDIMP&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24335;&#23398;&#20064;&#32958;&#31227;&#26893;DCE MRI&#30340;&#21307;&#23398;&#22270;&#20687;&#21644;&#25552;&#31034;&#27169;&#22411;&#65292;&#21033;&#29992;&#32852;&#21512;&#25991;&#26412;-&#22270;&#20687;&#23884;&#20837;&#30340;&#23545;&#27604;&#23398;&#20064;&#26469;&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32958;&#31227;&#26893;&#24050;&#25104;&#20026;&#32456;&#26411;&#26399;&#32958;&#33039;&#30142;&#30149;&#30340;&#26368;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;&#30001;&#20110;&#22797;&#26434;&#21407;&#22240;&#65292;&#31227;&#26893;&#24930;&#24615;&#21151;&#33021;&#38556;&#30861;&#30340;&#37325;&#22823;&#39118;&#38505;&#20173;&#28982;&#23384;&#22312;&#65292;&#21487;&#33021;&#23548;&#33268;&#31227;&#26893;&#22833;&#36133;&#12290;&#21307;&#23398;&#24433;&#20687;&#22312;&#32958;&#31227;&#26893;&#30417;&#27979;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#31227;&#26893;&#30417;&#30563;&#20855;&#26377;&#22810;&#23398;&#31185;&#29305;&#28857;&#65292;&#23588;&#20854;&#26159;&#32467;&#21512;&#20102;&#32958;&#33039;&#23398;&#12289;&#27852;&#23615;&#23398;&#21644;&#25918;&#23556;&#23398;&#65292;&#22312;&#36825;&#31181;&#39640;&#32500;&#24230;&#21644;&#22797;&#26434;&#25968;&#25454;&#20013;&#35782;&#21035;&#24378;&#22823;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#29992;&#20110;&#39044;&#21518;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#21463;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#36817;&#25104;&#21151;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;MEDIMP&#8212;&#8212;&#21307;&#23398;&#24433;&#20687;&#21644;&#25552;&#31034;&#8212;&#8212;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24335;&#23398;&#20064;&#32958;&#31227;&#26893;&#21160;&#24577;&#23545;&#27604;&#22686;&#24378;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;DCE MRI&#65289;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#32467;&#26500;&#24615;&#20020;&#24202;&#29983;&#29289;&#25968;&#25454;&#32763;&#35793;&#25104;&#25991;&#26412;&#25552;&#31034;&#26469;&#23436;&#25104;&#12290;MEDIMP&#22522;&#20110;&#32852;&#21512;&#25991;&#26412;-&#22270;&#20687;&#23884;&#20837;&#30340;&#23545;&#27604;&#23398;&#20064;&#65292;&#20197;&#25191;&#34892;&#36825;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Renal transplantation emerges as the most effective solution for end-stage renal disease. Occurring from complex causes, a substantial risk of transplant chronic dysfunction persists and may lead to graft loss. Medical imaging plays a substantial role in renal transplant monitoring in clinical practice. However, graft supervision is multi-disciplinary, notably joining nephrology, urology, and radiology, while identifying robust biomarkers from such high-dimensional and complex data for prognosis is challenging. In this work, taking inspiration from the recent success of Large Language Models (LLMs), we propose MEDIMP -- Medical Images and Prompts -- a model to learn meaningful multi-modal representations of renal transplant Dynamic Contrast-Enhanced Magnetic Resonance Imaging (DCE MRI) by incorporating structural clinicobiological data after translating them into text prompts. MEDIMP is based on contrastive learning from joint text-image paired embeddings to perform this challenging ta
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#30340;&#20869;&#23481;&#33258;&#36866;&#24212;&#21487;&#23398;&#20064;&#26102;&#39057;&#34920;&#31034;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21367;&#31215;&#28388;&#27874;&#22120;&#19982;&#21464;&#25442;&#22120;&#26550;&#26500;&#26469;&#23558;&#23567;&#30340;&#27874;&#24418;&#22359;&#25237;&#24433;&#21040;&#23567;&#30340;&#28508;&#22312;&#32500;&#24230;&#19978;&#12290;</title><link>http://arxiv.org/abs/2303.10446</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#30340;&#20869;&#23481;&#33258;&#36866;&#24212;&#21487;&#23398;&#20064;&#26102;&#39057;&#34920;&#31034;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Content Adaptive Learnable Time-Frequency Representation For Audio Signal Processing. (arXiv:2303.10446v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10446
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#30340;&#20869;&#23481;&#33258;&#36866;&#24212;&#21487;&#23398;&#20064;&#26102;&#39057;&#34920;&#31034;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21367;&#31215;&#28388;&#27874;&#22120;&#19982;&#21464;&#25442;&#22120;&#26550;&#26500;&#26469;&#23558;&#23567;&#30340;&#27874;&#24418;&#22359;&#25237;&#24433;&#21040;&#23567;&#30340;&#28508;&#22312;&#32500;&#24230;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#20869;&#23481;&#33258;&#36866;&#24212;&#21069;&#31471;&#65292;&#29992;&#20110;&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#29616;&#20195;&#20986;&#29616;&#20043;&#21069;&#65292;&#25105;&#20204;&#20351;&#29992;&#22266;&#23450;&#34920;&#31034;&#30340;&#12289;&#19981;&#21487;&#23398;&#20064;&#30340;&#21069;&#31471;&#65292;&#22914;&#35889;&#22270;&#25110;&#26757;&#23572;&#35889;&#22270;&#65292;&#24102;/&#19981;&#24102;&#31070;&#32463;&#32467;&#26500;&#12290;&#38543;&#30528;&#21367;&#31215;&#26550;&#26500;&#25903;&#25345;ASR&#21644;&#22768;&#23398;&#22330;&#26223;&#29702;&#35299;&#31561;&#21508;&#31181;&#24212;&#29992;&#65292;&#36716;&#21521;&#21487;&#23398;&#20064;&#21069;&#31471;&#65292;&#21363;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#21644;&#20248;&#21270;&#29305;&#23450;&#20219;&#21153;&#25152;&#38656;&#30340;&#22522;&#30784;&#20989;&#25968;&#21644;&#26435;&#37325;&#12290;&#22312;&#27809;&#26377;&#21367;&#31215;&#22359;&#30340;&#21464;&#24418;&#22120;&#26550;&#26500;&#20013;&#65292;&#32447;&#24615;&#23618;&#23558;&#23567;&#30340;&#27874;&#24418;&#22359;&#25237;&#24433;&#21040;&#23567;&#30340;&#28508;&#22312;&#32500;&#24230;&#19978;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#39304;&#36865;&#21040;&#21464;&#24418;&#22120;&#26550;&#26500;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#20869;&#23481;&#33258;&#36866;&#24212;&#23398;&#20064;&#26102;&#39057;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a learnable content adaptive front end for audio signal processing. Before the modern advent of deep learning, we used fixed representation non-learnable front-ends like spectrogram or mel-spectrogram with/without neural architectures. With convolutional architectures supporting various applications such as ASR and acoustic scene understanding, a shift to a learnable front ends occurred in which both the type of basis functions and the weight were learned from scratch and optimized for the particular task of interest. With the shift to transformer-based architectures with no convolutional blocks present, a linear layer projects small waveform patches onto a small latent dimension before feeding them to a transformer architecture. In this work, we propose a way of computing a content-adaptive learnable time-frequency representation. We pass each audio signal through a bank of convolutional filters, each giving a fixed-dimensional vector. It is akin to learning a bank of finit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#24847;&#35782;&#30340;&#21487;&#33021;&#24615;&#65292;&#30446;&#21069;&#30340;&#27169;&#22411;&#23384;&#22312;&#30528;&#24847;&#35782;&#30340;&#26174;&#33879;&#38556;&#30861;&#65292;&#20294;&#26410;&#26469;&#21313;&#24180;&#38543;&#30528;&#38556;&#30861;&#34987;&#20811;&#26381;&#65292;&#21518;&#32487;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#20855;&#26377;&#24847;&#35782;&#12290;</title><link>http://arxiv.org/abs/2303.07103</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#20855;&#26377;&#24847;&#35782;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Could a Large Language Model be Conscious?. (arXiv:2303.07103v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#24847;&#35782;&#30340;&#21487;&#33021;&#24615;&#65292;&#30446;&#21069;&#30340;&#27169;&#22411;&#23384;&#22312;&#30528;&#24847;&#35782;&#30340;&#26174;&#33879;&#38556;&#30861;&#65292;&#20294;&#26410;&#26469;&#21313;&#24180;&#38543;&#30528;&#38556;&#30861;&#34987;&#20811;&#26381;&#65292;&#21518;&#32487;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#20855;&#26377;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#26222;&#36941;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#24863;&#30693;&#25110;&#24847;&#35782;&#12290;&#25105;&#20204;&#26159;&#21542;&#24212;&#35813;&#35748;&#30495;&#32771;&#34385;&#36825;&#20010;&#24819;&#27861;&#65311;&#26412;&#25991;&#23558;&#20998;&#26512;&#25903;&#25345;&#21644;&#21453;&#23545;&#36825;&#20010;&#24819;&#27861;&#30340;&#26368;&#26377;&#21147;&#30340;&#29702;&#30001;&#12290;&#26681;&#25454;&#24847;&#35782;&#31185;&#23398;&#20013;&#30340;&#20027;&#27969;&#20551;&#35774;&#65292;&#30446;&#21069;&#30340;&#27169;&#22411;&#23384;&#22312;&#30528;&#24847;&#35782;&#30340;&#26174;&#33879;&#38556;&#30861;&#65292;&#20363;&#22914;&#32570;&#20047;&#24490;&#29615;&#22788;&#29702;&#12289;&#20840;&#23616;&#30340;&#24037;&#20316;&#31354;&#38388;&#21644;&#32479;&#19968;&#30340;&#26234;&#33021;&#26426;&#26500;&#31561;&#31561;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36825;&#20123;&#38556;&#30861;&#22312;&#26410;&#26469;&#21313;&#24180;&#24038;&#21491;&#37117;&#21487;&#33021;&#34987;&#20811;&#26381;&#12290;&#20316;&#32773;&#24471;&#20986;&#30340;&#32467;&#35770;&#26159;&#65292;&#34429;&#28982;&#30446;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#24847;&#35782;&#30340;&#21487;&#33021;&#24615;&#36739;&#23567;&#65292;&#20294;&#25105;&#20204;&#24212;&#35813;&#35748;&#30495;&#32771;&#34385;&#21518;&#32487;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#20037;&#30340;&#23558;&#26469;&#21487;&#33021;&#20250;&#20855;&#26377;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has recently been widespread discussion of whether large language models might be sentient or conscious. Should we take this idea seriously? I will break down the strongest reasons for and against. Given mainstream assumptions in the science of consciousness, there are significant obstacles to consciousness in current models: for example, their lack of recurrent processing, a global workspace, and unified agency. At the same time, it is quite possible that these obstacles will be overcome in the next decade or so. I conclude that while it is somewhat unlikely that current large language models are conscious, we should take seriously the possibility that successors to large language models may be conscious in the not-too-distant future.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#38190;&#28857;&#30340;RGB-D&#36755;&#20837;&#30340;&#20845;&#33258;&#30001;&#24230;&#25235;&#21462;&#23039;&#24577;&#21512;&#25104;&#26041;&#27861;&#65292;&#26082;&#21487;&#20197;&#20174;&#20851;&#38190;&#28857;&#26816;&#27979;&#20013;&#39044;&#27979;&#25235;&#21462;&#23039;&#24577;&#65292;&#20063;&#21487;&#20197;&#39044;&#27979;&#30456;&#23545;&#20110;&#30456;&#26426;&#30340;&#23610;&#24230;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.05617</link><description>&lt;p&gt;
KGNv2: &#22522;&#20110;&#20851;&#38190;&#28857;&#30340;RGB-D&#36755;&#20837;&#20845;&#33258;&#30001;&#24230;&#25235;&#21462;&#21512;&#25104;&#20013;&#30340;&#23610;&#24230;&#21644;&#23039;&#24577;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
KGNv2: Separating Scale and Pose Prediction for Keypoint-based 6-DoF Grasp Synthesis on RGB-D input. (arXiv:2303.05617v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#38190;&#28857;&#30340;RGB-D&#36755;&#20837;&#30340;&#20845;&#33258;&#30001;&#24230;&#25235;&#21462;&#23039;&#24577;&#21512;&#25104;&#26041;&#27861;&#65292;&#26082;&#21487;&#20197;&#20174;&#20851;&#38190;&#28857;&#26816;&#27979;&#20013;&#39044;&#27979;&#25235;&#21462;&#23039;&#24577;&#65292;&#20063;&#21487;&#20197;&#39044;&#27979;&#30456;&#23545;&#20110;&#30456;&#26426;&#30340;&#23610;&#24230;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;6&#33258;&#30001;&#24230;&#25235;&#21462;&#23039;&#24577;&#21512;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#20851;&#38190;&#28857;&#20174;2D/2.5D&#36755;&#20837;&#20013;&#36827;&#34892;&#12290;&#22312;&#21069;&#26399;&#30740;&#31350;&#20013;&#65292;&#22522;&#20110;&#20851;&#38190;&#28857;&#30340;&#25235;&#21462;&#26816;&#27979;&#22120;&#24050;&#32463;&#35777;&#26126;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#20854;&#20013;&#24425;&#33394;&#22270;&#20687;&#25552;&#20379;&#30340;&#39069;&#22806;&#35270;&#35273;&#20449;&#24687;&#24357;&#34917;&#20102;&#22024;&#26434;&#30340;&#28145;&#24230;&#24863;&#30693;&#12290;&#28982;&#32780;&#65292;&#23427;&#20005;&#37325;&#20381;&#36182;&#20110;&#20934;&#30830;&#39044;&#27979;&#22270;&#20687;&#31354;&#38388;&#20013;&#30340;&#20851;&#38190;&#28857;&#20301;&#32622;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#25235;&#21462;&#29983;&#25104;&#32593;&#32476;&#65292;&#26082;&#21487;&#20197;&#20174;&#20851;&#38190;&#28857;&#26816;&#27979;&#20013;&#39044;&#27979;&#25235;&#21462;&#23039;&#24577;&#65292;&#20063;&#21487;&#20197;&#39044;&#27979;&#30456;&#23545;&#20110;&#30456;&#26426;&#30340;&#23610;&#24230;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#37325;&#26032;&#35774;&#35745;&#20102;&#20851;&#38190;&#28857;&#36755;&#20986;&#31354;&#38388;&#65292;&#20197;&#20943;&#36731;&#20851;&#38190;&#28857;&#39044;&#27979;&#22122;&#22768;&#23545;&#36879;&#35270;n&#28857;(PnP)&#31639;&#27861;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#27604;&#22522;&#32447;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20248;&#36234;&#24615;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#23613;&#31649;&#26159;&#22312;&#31616;&#21333;&#30340;&#21512;&#25104;&#23545;&#35937;&#19978;&#35757;&#32451;&#30340;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#21487;&#20197;&#29992;&#20110;&#30495;&#23454;&#29289;&#20307;&#19978;&#30340;&#25235;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new 6-DoF grasp pose synthesis approach from 2D/2.5D input based on keypoints. Keypoint-based grasp detector from image input has demonstrated promising results in the previous study, where the additional visual information provided by color images compensates for the noisy depth perception. However, it relies heavily on accurately predicting the location of keypoints in the image space. In this paper, we devise a new grasp generation network that reduces the dependency on precise keypoint estimation. Given an RGB-D input, our network estimates both the grasp pose from keypoint detection as well as scale towards the camera. We further re-design the keypoint output space in order to mitigate the negative impact of keypoint prediction noise to Perspective-n-Point (PnP) algorithm. Experiments show that the proposed method outperforms the baseline by a large margin, validating the efficacy of our approach. Finally, despite trained on simple synthetic objects, our method demons
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#32858;&#31867;&#25216;&#26415;&#35774;&#35745;&#24182;&#25191;&#34892;&#38656;&#27714;&#21709;&#24212;&#65288;DR&#65289;&#35745;&#21010;&#30340;&#21487;&#34892;&#24615;&#65292;&#30446;&#30340;&#26159;&#25913;&#21464;&#20998;&#24067;&#24335;&#33021;&#28304;&#31038;&#21306;&#20869;&#20379;&#24212;&#32773;&#30340;&#28040;&#36153;&#34892;&#20026;&#65292;&#20197;&#26368;&#23567;&#21270;&#21453;&#21521;&#21151;&#29575;&#27969;&#21644;&#21066;&#20943;&#31995;&#32479;&#33539;&#22260;&#20869;&#30340;&#21151;&#23792;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2303.00186</link><description>&lt;p&gt;
&#22522;&#20110;&#32858;&#31867;&#25216;&#26415;&#30340;&#28789;&#27963;&#33021;&#28304;&#31038;&#21306;&#30446;&#26631;&#38656;&#27714;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Targeted demand response for flexible energy communities using clustering techniques. (arXiv:2303.00186v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#32858;&#31867;&#25216;&#26415;&#35774;&#35745;&#24182;&#25191;&#34892;&#38656;&#27714;&#21709;&#24212;&#65288;DR&#65289;&#35745;&#21010;&#30340;&#21487;&#34892;&#24615;&#65292;&#30446;&#30340;&#26159;&#25913;&#21464;&#20998;&#24067;&#24335;&#33021;&#28304;&#31038;&#21306;&#20869;&#20379;&#24212;&#32773;&#30340;&#28040;&#36153;&#34892;&#20026;&#65292;&#20197;&#26368;&#23567;&#21270;&#21453;&#21521;&#21151;&#29575;&#27969;&#21644;&#21066;&#20943;&#31995;&#32479;&#33539;&#22260;&#20869;&#30340;&#21151;&#23792;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#32858;&#31867;&#25216;&#26415;&#20026;&#21830;&#19994;&#21644;&#20303;&#23429;&#31038;&#21306;&#30340;&#33021;&#37327;&#20379;&#24212;&#32773;&#35774;&#35745;&#21644;&#25191;&#34892;&#38656;&#27714;&#21709;&#24212;&#65288;DR&#65289;&#35745;&#21010;&#30340;&#21487;&#33021;&#24615;&#12290;&#35813;&#35745;&#21010;&#30340;&#30446;&#30340;&#26159;&#25913;&#21464;&#24847;&#22823;&#21033;&#20998;&#24067;&#24335;&#33021;&#28304;&#31038;&#21306;&#20869;&#30340;&#20379;&#24212;&#32773;&#30340;&#28040;&#36153;&#34892;&#20026;&#12290;&#36825;&#31181;&#32858;&#21512;&#26088;&#22312;&#65306;a&#65289;&#26368;&#23567;&#21270;&#22312;&#20027;&#35201;&#21464;&#30005;&#31449;&#22788;&#20135;&#29983;&#30340;&#21453;&#21521;&#21151;&#29575;&#27969;&#65292;&#35813;&#21151;&#29575;&#27969;&#22312;&#24403;&#22320;&#30005;&#32593;&#20013;&#30340;&#22826;&#38451;&#33021;&#30005;&#27744;&#30340;&#21457;&#30005;&#37327;&#36229;&#36807;&#28040;&#32791;&#26102;&#20250;&#21457;&#29983;; b&#65289;&#21066;&#20943;&#31995;&#32479;&#33539;&#22260;&#20869;&#30340;&#21151;&#23792;&#38656;&#27714;&#65292;&#35813;&#38656;&#27714;&#36890;&#24120;&#21457;&#29983;&#22312;&#20621;&#26202;&#26102;&#20998;&#12290;&#22312;&#32858;&#31867;&#38454;&#27573;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19977;&#31181;&#28909;&#38376;&#30340;&#30005;&#36127;&#33655;&#32858;&#31867;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;-&#21363;k-means&#65292;k-medoids&#21644;&#19968;&#31181;&#32858;&#21512;&#23618;&#27425;&#32858;&#31867;-alongside&#20004;&#31181;&#19981;&#21516;&#30340;&#36317;&#31163;&#24230;&#37327;-&#21363;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#21644;&#21463;&#38480;&#21160;&#24577;&#26102;&#38388;&#25197;&#26354;&#65288;DTW&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#39564;&#35777;&#24230;&#37327;&#26469;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#39033;&#26032;&#39062;&#30340;&#25351;&#26631;-&#21363;&#23792;&#20540;&#24615;&#33021;&#35780;&#20998;&#65288;PPS&#65289;
&lt;/p&gt;
&lt;p&gt;
The present study explores the use of clustering techniques for the design and implementation of a demand response (DR) program for commercial and residential prosumers. The goal of the program is to alter the consumption behavior of the prosumers pertaining to a distributed energy community in Italy. This aggregation aims to: a) minimize the reverse power flow at the primary substation, that occurs when generation from solar panels in the local grid exceeds consumption, and b) shave the system wide peak demand, that typically occurs during the hours of late afternoon. Regarding the clustering stage, three popular machine learning algorithms for electrical load clustering are employed -namely k-means, k-medoids and an agglomerative hierarchical clustering- alongside two different distance measures -namely euclidean and constrained dynamic time warping (DTW). We evaluate the methods using multiple validation metrics including a novel metric -namely peak performance score (PPS)- that we 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#23618;&#27425;&#21270;&#12289;&#23398;&#20064;&#24335;&#30340;&#22810;&#26234;&#33021;&#20307;&#32763;&#36716;&#30446;&#26631;&#24033;&#26816;&#35268;&#21010;&#26041;&#27861;&#65292;&#20998;&#20026;&#35270;&#28857;&#35268;&#21010;&#21644;&#23548;&#33322;&#35268;&#21010;&#20004;&#37096;&#20998;&#65292;&#21487;&#20197;&#33258;&#20027;&#12289;&#24378;&#38887;&#12289;&#21435;&#20013;&#24515;&#21270;&#22320;&#23436;&#25104;&#24033;&#26816;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2302.14188</link><description>&lt;p&gt;
&#22522;&#20110;&#26333;&#20809;&#30340;&#22810;&#26234;&#33021;&#20307;&#36816;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26816;&#27979;&#32763;&#28378;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Exposure-Based Multi-Agent Inspection of a Tumbling Target Using Deep Reinforcement Learning. (arXiv:2302.14188v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#23618;&#27425;&#21270;&#12289;&#23398;&#20064;&#24335;&#30340;&#22810;&#26234;&#33021;&#20307;&#32763;&#36716;&#30446;&#26631;&#24033;&#26816;&#35268;&#21010;&#26041;&#27861;&#65292;&#20998;&#20026;&#35270;&#28857;&#35268;&#21010;&#21644;&#23548;&#33322;&#35268;&#21010;&#20004;&#37096;&#20998;&#65292;&#21487;&#20197;&#33258;&#20027;&#12289;&#24378;&#38887;&#12289;&#21435;&#20013;&#24515;&#21270;&#22320;&#23436;&#25104;&#24033;&#26816;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22826;&#31354;&#22403;&#22334;&#26085;&#30410;&#22686;&#22810;&#65292;&#23545;&#20110;&#35266;&#23519;&#24223;&#24323;&#21355;&#26143;&#20197;&#35268;&#21010;&#32500;&#20462;&#25110;&#36827;&#34892;&#21435;&#36712;&#20316;&#19994;&#30340;&#38656;&#35201;&#36234;&#26469;&#36234;&#36843;&#20999;&#12290;&#28982;&#32780;&#65292;&#22826;&#31354;&#24033;&#26816;&#20219;&#21153;&#26412;&#36523;&#23601;&#26159;&#19968;&#39033;&#25361;&#25112;&#24615;&#24037;&#20316;&#65292;&#36890;&#24120;&#38656;&#35201;&#22810;&#20010;&#35266;&#27979;&#21355;&#26143;&#30340;&#31934;&#24515;&#21327;&#35843;&#12290;&#22312;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#29615;&#22659;&#20013;&#65292;&#30446;&#26631;&#21487;&#33021;&#26159;&#26410;&#30693;&#30340;&#25110;&#32773;&#19981;&#21487;&#39044;&#27979;&#30340;&#65292;&#27809;&#26377;&#26102;&#38388;&#36827;&#34892;&#36830;&#32493;&#30340;&#22320;&#38754;&#25351;&#25381;&#21644;&#25511;&#21046;&#65292;&#36825;&#20351;&#24471;&#20219;&#21153;&#22797;&#26434;&#21270;&#20043;&#20313;&#65292;&#20063;&#36843;&#20999;&#38656;&#35201;&#33258;&#20027;&#12289;&#24378;&#38887;&#12289;&#21435;&#20013;&#24515;&#21270;&#30340;&#24033;&#26816;&#26041;&#26696;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#32771;&#34385;&#37319;&#29992;&#19968;&#31181;&#23618;&#27425;&#21270;&#30340;&#12289;&#23398;&#20064;&#22411;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#22810;&#26234;&#33021;&#20307;&#32763;&#28378;&#30446;&#26631;&#30340;&#24033;&#26816;&#35268;&#21010;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#35757;&#32451;&#30340;&#35270;&#28857;&#25110;&#39640;&#32423;&#35268;&#21010;&#22120;&#65292;&#20197;&#21450;&#19968;&#20010;&#22788;&#29702;&#39044;&#20808;&#25351;&#23450;&#35270;&#28857;&#20043;&#38388;&#28857;&#23545;&#28857;&#23548;&#33322;&#30340;&#23548;&#33322;&#35268;&#21010;&#22120;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#30340;&#26032;&#38382;&#39064;&#24418;&#25104;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
As space becomes more congested, on orbit inspection is an increasingly relevant activity whether to observe a defunct satellite for planning repairs or to de-orbit it. However, the task of on orbit inspection itself is challenging, typically requiring the careful coordination of multiple observer satellites. This is complicated by a highly nonlinear environment where the target may be unknown or moving unpredictably without time for continuous command and control from the ground. There is a need for autonomous, robust, decentralized solutions to the inspection task. To achieve this, we consider a hierarchical, learned approach for the decentralized planning of multi-agent inspection of a tumbling target. Our solution consists of two components: a viewpoint or high-level planner trained using deep reinforcement learning and a navigation planner handling point-to-point navigation between pre-specified viewpoints. We present a novel problem formulation and methodology that is suitable no
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#21644;&#21457;&#23637;&#21382;&#31243;&#65292;&#21253;&#25324;&#23427;&#20204;&#30340;&#26550;&#26500;&#12289;&#22521;&#35757;&#30446;&#26631;&#12289;&#39044;&#22521;&#35757;&#20219;&#21153;&#12289;&#24494;&#35843;&#31574;&#30053;&#21644;&#35780;&#20272;&#12290;&#21516;&#26102;&#65292;&#35752;&#35770;&#20102;&#20854;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.09419</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#32508;&#36848;&#65306;&#20174;BERT&#21040;ChatGPT&#30340;&#21382;&#31243;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT. (arXiv:2302.09419v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#21644;&#21457;&#23637;&#21382;&#31243;&#65292;&#21253;&#25324;&#23427;&#20204;&#30340;&#26550;&#26500;&#12289;&#22521;&#35757;&#30446;&#26631;&#12289;&#39044;&#22521;&#35757;&#20219;&#21153;&#12289;&#24494;&#35843;&#31574;&#30053;&#21644;&#35780;&#20272;&#12290;&#21516;&#26102;&#65292;&#35752;&#35770;&#20102;&#20854;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;(PFMs)&#34987;&#35748;&#20026;&#26159;&#21508;&#31181;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#19979;&#28216;&#20219;&#21153;&#30340;&#22522;&#30784;&#12290;PFM(&#20363;&#22914;BERT&#12289;ChatGPT&#21644;GPT-4)&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20026;&#21508;&#31181;&#19979;&#28216;&#24212;&#29992;&#25552;&#20379;&#20102;&#21512;&#29702;&#30340;&#21442;&#25968;&#21021;&#22987;&#21270;&#12290;BERT&#20174;&#36716;&#25442;&#22120;&#20013;&#23398;&#20064;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#20316;&#20026;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#31867;&#20284;&#22320;&#65292;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;(GPT)&#26041;&#27861;&#37319;&#29992;&#36716;&#25442;&#22120;&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#24182;&#37319;&#29992;&#33258;&#22238;&#24402;&#33539;&#24335;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#26368;&#36817;&#65292;ChatGPT&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#23637;&#29616;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#25104;&#21151;&#65292;&#23427;&#37319;&#29992;&#33258;&#22238;&#24402;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#34892;&#38646;&#23556;&#20987;&#25110;&#23569;&#23556;&#20987;&#25552;&#31034;&#12290;PFM&#30340;&#21331;&#36234;&#25104;&#23601;&#20026;&#21508;&#31181;AI&#39046;&#22495;&#24102;&#26469;&#20102;&#37325;&#22823;&#31361;&#30772;&#12290;&#35768;&#22810;&#30740;&#31350;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#23545;&#26356;&#26032;&#35843;&#26597;&#30340;&#38656;&#27714;&#12290;&#26412;&#30740;&#31350;&#20840;&#38754;&#22238;&#39038;&#20102;PFMs&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#23427;&#20204;&#30340;&#26550;&#26500;&#12289;&#22521;&#35757;&#30446;&#26631;&#12289;&#39044;&#22521;&#35757;&#20219;&#21153;&#12289;&#24494;&#35843;&#31574;&#30053;&#21644;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;PFMs&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained Foundation Models (PFMs) are regarded as the foundation for various downstream tasks with different data modalities. A PFM (e.g., BERT, ChatGPT, and GPT-4) is trained on large-scale data which provides a reasonable parameter initialization for a wide range of downstream applications. BERT learns bidirectional encoder representations from Transformers, which are trained on large datasets as contextual language models. Similarly, the generative pretrained transformer (GPT) method employs Transformers as the feature extractor and is trained using an autoregressive paradigm on large datasets. Recently, ChatGPT shows promising success on large language models, which applies an autoregressive language model with zero shot or few shot prompting. The remarkable achievements of PFM have brought significant breakthroughs to various fields of AI. Numerous studies have proposed different methods, raising the demand for an updated survey. This study provides a comprehensive review of rec
&lt;/p&gt;</description></item><item><title>SHINE&#26159;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26080;&#38556;&#30861;&#20572;&#36710;&#31649;&#29702;&#31995;&#32479;&#65292;&#33021;&#22815;&#23454;&#26102;&#35782;&#21035;&#20572;&#25918;&#22312;&#26080;&#38556;&#30861;&#20572;&#36710;&#20301;&#19978;&#30340;&#36710;&#36742;&#65292;&#24182;&#20855;&#22791;&#21487;&#35775;&#38382;&#24615;&#30417;&#27979;&#21151;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SHINE&#20248;&#20110;&#29616;&#26377;&#30340;&#36710;&#29260;&#35782;&#21035;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2302.00837</link><description>&lt;p&gt;
SHINE: &#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26080;&#38556;&#30861;&#20572;&#36710;&#31649;&#29702;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
SHINE: Deep Learning-Based Accessible Parking Management System. (arXiv:2302.00837v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00837
&lt;/p&gt;
&lt;p&gt;
SHINE&#26159;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26080;&#38556;&#30861;&#20572;&#36710;&#31649;&#29702;&#31995;&#32479;&#65292;&#33021;&#22815;&#23454;&#26102;&#35782;&#21035;&#20572;&#25918;&#22312;&#26080;&#38556;&#30861;&#20572;&#36710;&#20301;&#19978;&#30340;&#36710;&#36742;&#65292;&#24182;&#20855;&#22791;&#21487;&#35775;&#38382;&#24615;&#30417;&#27979;&#21151;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SHINE&#20248;&#20110;&#29616;&#26377;&#30340;&#36710;&#29260;&#35782;&#21035;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#25216;&#30340;&#36827;&#27493;&#25512;&#21160;&#20102;&#22478;&#24066;&#21306;&#22495;&#30340;&#19981;&#26029;&#25193;&#24352;&#65292;&#20840;&#19990;&#30028;&#31169;&#20154;&#36710;&#36742;&#25968;&#37327;&#30456;&#24212;&#22686;&#21152;&#65292;&#20854;&#20013;&#38889;&#22269;&#20063;&#19981;&#20363;&#22806;&#12290;&#36825;&#31181;&#36710;&#36742;&#25968;&#37327;&#36880;&#27493;&#22686;&#21152;&#20063;&#24102;&#26469;&#20102;&#19982;&#20572;&#36710;&#26377;&#20851;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#20026;&#27531;&#30142;&#20154;&#25351;&#23450;&#30340;&#27531;&#30142;&#20154;&#20572;&#36710;&#20301;&#34987;&#28389;&#29992;&#12290;&#20256;&#32479;&#30340;&#36710;&#29260;&#35782;&#21035;&#31995;&#32479;&#30001;&#20110;&#30417;&#25511;&#25668;&#20687;&#22836;&#39640;&#24103;&#29575;&#12289;&#33258;&#28982;&#21644;&#20154;&#20026;&#22122;&#22768;&#30340;&#23384;&#22312;&#20197;&#21450;&#20809;&#29031;&#21644;&#22825;&#27668;&#26465;&#20214;&#30340;&#21464;&#21270;&#32780;&#23548;&#33268;&#23454;&#26102;&#26816;&#27979;&#21644;&#35782;&#21035;&#25928;&#29575;&#20302;&#19979;&#65292;&#38590;&#20197;&#26377;&#25928;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SHINE&#65292;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26080;&#38556;&#30861;&#20572;&#36710;&#31649;&#29702;&#31995;&#32479;&#65292;&#29992;&#20110;&#23454;&#26102;&#26080;&#38556;&#30861;&#24615;&#30417;&#27979;&#21644;&#35782;&#21035;&#20572;&#25918;&#22312;&#26080;&#38556;&#30861;&#20572;&#36710;&#20301;&#19978;&#30340;&#36710;&#36742;&#12290;SHINE&#30001;&#20004;&#20010;&#20027;&#35201;&#32452;&#20214;&#32452;&#25104;&#65306;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#36710;&#29260;&#35782;&#21035;&#31995;&#32479;&#21644;&#26080;&#38556;&#30861;&#20572;&#36710;&#31649;&#29702;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20174;4&#20010;&#19981;&#21516;&#26102;&#27573;&#25293;&#25668;&#30340;21&#20010;&#26080;&#38556;&#30861;&#20572;&#36710;&#20301;&#30340;4,780&#24352;&#22270;&#20687;&#65292;&#20197;&#35780;&#20272;SHINE&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SHINE&#20248;&#20110;&#29616;&#26377;&#30340;&#36710;&#29260;&#35782;&#21035;&#31995;&#32479;&#65292;&#22312;&#35782;&#21035;&#29575;&#21644;&#21487;&#35775;&#38382;&#24615;&#30417;&#27979;&#26041;&#38754;&#37117;&#36798;&#21040;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ongoing expansion of urban areas facilitated by advancements in science and technology has resulted in a considerable increase in the number of privately owned vehicles worldwide, including in South Korea. However, this gradual increment in the number of vehicles has inevitably led to parking-related issues, including the abuse of disabled parking spaces (hereafter referred to as accessible parking spaces) designated for individuals with disabilities. Traditional license plate recognition (LPR) systems have proven inefficient in addressing such a problem in real-time due to the high frame rate of surveillance cameras, the presence of natural and artificial noise, and variations in lighting and weather conditions that impede detection and recognition by these systems. With the growing concept of parking 4.0, many sensors, IoT and deep learning-based approaches have been applied to automatic LPR and parking management systems. Nonetheless, the studies show a need for a robust and eff
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#21442;&#25968;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#20855;&#26377;&#25289;&#26222;&#25289;&#26031;&#30028;&#38480;&#65292;&#36890;&#36807;&#26631;&#20934;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#25237;&#24433;&#25110;&#38556;&#30861;&#39033;&#12290;</title><link>http://arxiv.org/abs/2301.11526</link><description>&lt;p&gt;
&#25289;&#26222;&#25289;&#26031;&#26377;&#30028;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#30452;&#25509;&#21442;&#25968;&#21270;
&lt;/p&gt;
&lt;p&gt;
Direct Parameterization of Lipschitz-Bounded Deep Networks. (arXiv:2301.11526v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#21442;&#25968;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#20855;&#26377;&#25289;&#26222;&#25289;&#26031;&#30028;&#38480;&#65292;&#36890;&#36807;&#26631;&#20934;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#25237;&#24433;&#25110;&#38556;&#30861;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#26041;&#24335;&#65288;&#20840;&#36830;&#25509;&#21644;&#21367;&#31215;&#32593;&#32476;&#65289;&#65292;&#20855;&#26377;&#26377;&#38480;&#28789;&#25935;&#24230;&#30340;&#25289;&#26222;&#25289;&#26031;&#30028;&#38480;&#12290;&#19982;SDP&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;"&#30452;&#25509;"&#21442;&#25968;&#21270;&#26041;&#24335;&#65292;&#24182;&#36890;&#36807;&#26631;&#20934;&#30340;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#25237;&#24433;&#25110;&#38556;&#30861;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new parameterization of deep neural networks (both fully-connected and convolutional) with guaranteed Lipschitz bounds, i.e. limited sensitivity to perturbations. The Lipschitz guarantees are equivalent to the tightest-known bounds based on certification via a semidefinite program (SDP), which does not scale to large models. In contrast to the SDP approach, we provide a ``direct'' parameterization, i.e. a smooth mapping from $\mathbb R^N$ onto the set of weights of Lipschitz-bounded networks. This enables training via standard gradient methods, without any computationally intensive projections or barrier terms. The new parameterization can equivalently be thought of as either a new layer type (the \textit{sandwich layer}), or a novel parameterization of standard feedforward networks with parameter sharing between neighbouring layers. Finally, the comprehensive set of experiments on image classification shows that sandwich layers outperform previous approaches on
&lt;/p&gt;</description></item><item><title>&#31616;&#32780;&#35328;&#20043;&#65292;&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#20351;&#29992;&#25968;&#25454;&#20013;&#24515;&#30340;&#26041;&#27861;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#35786;&#26029;&#20013;&#30340;&#8220;&#23567;&#25968;&#25454;&#8221;&#38382;&#39064;&#65292;&#20171;&#32461;&#20102;&#25968;&#25454;&#22686;&#24378;&#12289;&#36801;&#31227;&#23398;&#20064;&#12289;&#32852;&#37030;&#23398;&#20064;&#21644;GAN&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#30693;&#35782;&#24341;&#23548;&#30340;GAN&#23558;&#39046;&#22495;&#30693;&#35782;&#32435;&#20837;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#12290;</title><link>http://arxiv.org/abs/2212.13591</link><description>&lt;p&gt;
&#30693;&#35782;&#24341;&#23548;&#19979;&#30340;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#36827;&#23637;&#12289;&#32570;&#38519;&#19982;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Guided Data-Centric AI in Healthcare: Progress, Shortcomings, and Future Directions. (arXiv:2212.13591v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13591
&lt;/p&gt;
&lt;p&gt;
&#31616;&#32780;&#35328;&#20043;&#65292;&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#20351;&#29992;&#25968;&#25454;&#20013;&#24515;&#30340;&#26041;&#27861;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#35786;&#26029;&#20013;&#30340;&#8220;&#23567;&#25968;&#25454;&#8221;&#38382;&#39064;&#65292;&#20171;&#32461;&#20102;&#25968;&#25454;&#22686;&#24378;&#12289;&#36801;&#31227;&#23398;&#20064;&#12289;&#32852;&#37030;&#23398;&#20064;&#21644;GAN&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#30693;&#35782;&#24341;&#23548;&#30340;GAN&#23558;&#39046;&#22495;&#30693;&#35782;&#32435;&#20837;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#30001;&#20110;&#22823;&#37327;&#21253;&#21547;&#29305;&#23450;&#27010;&#24565;&#25110;&#24847;&#20041;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#12290;&#22312;&#21307;&#23398;&#39046;&#22495;&#65292;&#25317;&#26377;&#35206;&#30422;&#29305;&#23450;&#30142;&#30149;&#30340;&#22810;&#26679;&#21270;&#35757;&#32451;&#25968;&#25454;&#21487;&#20197;&#23548;&#33268;&#24320;&#21457;&#20986;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#35813;&#30142;&#30149;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#28508;&#22312;&#30340;&#22909;&#22788;&#65292;&#30001;&#20110;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#27880;&#37322;&#25968;&#25454;&#65292;&#22522;&#20110;&#22270;&#20687;&#30340;&#35786;&#26029;&#24182;&#27809;&#26377;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#12290;&#26412;&#25991;&#24378;&#35843;&#22312;&#25968;&#25454;&#34920;&#31034;&#26041;&#38754;&#37319;&#29992;&#25968;&#25454;&#20013;&#24515;&#30340;&#26041;&#27861;&#20197;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#30340;&#37325;&#35201;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#21487;&#29992;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#8220;&#23567;&#25968;&#25454;&#8221;&#38382;&#39064;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22235;&#31181;&#29983;&#25104;&#21644;&#32858;&#21512;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#65306;&#25968;&#25454;&#22686;&#24378;&#12289;&#36801;&#31227;&#23398;&#20064;&#12289;&#32852;&#37030;&#23398;&#20064;&#21644;GAN&#65288;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65289;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20351;&#29992;&#30693;&#35782;&#24341;&#23548;&#30340;GAN&#26469;&#23558;&#39046;&#22495;&#30693;&#35782;&#32435;&#20837;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of deep learning is largely due to the availability of large amounts of training data that cover a wide range of examples of a particular concept or meaning. In the field of medicine, having a diverse set of training data on a particular disease can lead to the development of a model that is able to accurately predict the disease. However, despite the potential benefits, there have not been significant advances in image-based diagnosis due to a lack of high-quality annotated data. This article highlights the importance of using a data-centric approach to improve the quality of data representations, particularly in cases where the available data is limited. To address this "small-data" issue, we discuss four methods for generating and aggregating training data: data augmentation, transfer learning, federated learning, and GANs (generative adversarial networks). We also propose the use of knowledge-guided GANs to incorporate domain knowledge in the training data generation pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21338;&#24328;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#32452;&#21512;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#65292;&#19987;&#38376;&#38024;&#23545;&#29305;&#23450;&#30340;&#38450;&#24481;&#25915;&#20987;&#36827;&#34892;&#38450;&#24481;&#65292;&#24182;&#20197;&#21338;&#24328;&#35770;&#30340;&#26041;&#24335;&#36827;&#34892;&#21512;&#20316;&#21644;&#31454;&#20105;&#65292;&#24418;&#25104;&#19968;&#20010;&#32852;&#21512;&#38450;&#24481;&#12290;</title><link>http://arxiv.org/abs/2211.14669</link><description>&lt;p&gt;
&#21338;&#24328;&#35770;&#28151;&#21512;&#19987;&#23478;&#29992;&#20110;&#32452;&#21512;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Game Theoretic Mixed Experts for Combinational Adversarial Machine Learning. (arXiv:2211.14669v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21338;&#24328;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#32452;&#21512;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#65292;&#19987;&#38376;&#38024;&#23545;&#29305;&#23450;&#30340;&#38450;&#24481;&#25915;&#20987;&#36827;&#34892;&#38450;&#24481;&#65292;&#24182;&#20197;&#21338;&#24328;&#35770;&#30340;&#26041;&#24335;&#36827;&#34892;&#21512;&#20316;&#21644;&#31454;&#20105;&#65292;&#24418;&#25104;&#19968;&#20010;&#32852;&#21512;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#26041;&#38754;&#30340;&#19968;&#20123;&#36827;&#23637;&#34920;&#26126;&#65292;&#37027;&#20123;&#34987;&#35748;&#20026;&#26159;&#24378;&#20581;&#30340;&#38450;&#24481;&#25514;&#26045;&#23454;&#38469;&#19978;&#36824;&#26159;&#23481;&#26131;&#21463;&#21040;&#38024;&#23545;&#20854;&#24369;&#28857;&#36827;&#34892;&#23450;&#21046;&#21270;&#25915;&#20987;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;&#36825;&#20123;&#38450;&#24481;&#25514;&#26045;&#21253;&#25324;&#38543;&#26426;&#21464;&#25442;&#30340;&#25915;&#20987;&#65288;BaRT&#65289;&#65292;&#26377;&#30410;&#20154;&#31867;&#30340;&#23545;&#25239;&#35757;&#32451;&#65288;FAT&#65289;&#65292;&#22403;&#22334;&#23601;&#26159;&#29645;&#23453;&#65288;TiT&#65289;&#20197;&#21450;&#30001;&#35270;&#35273;&#36716;&#25442;&#22120;&#12289;&#22823;&#22411;&#36716;&#31227;&#27169;&#22411;&#21644;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#32452;&#25104;&#30340;&#32452;&#21512;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21338;&#24328;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#32452;&#21512;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#65292;&#27599;&#20010;&#19987;&#23478;&#19987;&#38376;&#38024;&#23545;&#29305;&#23450;&#30340;&#38450;&#24481;&#25915;&#20987;&#36827;&#34892;&#38450;&#24481;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#19987;&#23478;&#20250;&#20197;&#21338;&#24328;&#35770;&#30340;&#26041;&#24335;&#36827;&#34892;&#21512;&#20316;&#21644;&#31454;&#20105;&#65292;&#24418;&#25104;&#19968;&#20010;&#32852;&#21512;&#38450;&#24481;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#25915;&#20987;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#38450;&#24481;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in adversarial machine learning have shown that defenses considered to be robust are actually susceptible to adversarial attacks which are specifically customized to target their weaknesses. These defenses include Barrage of Random Transforms (BaRT), Friendly Adversarial Training (FAT), Trash is Treasure (TiT) and ensemble models made up of Vision Transformers (ViTs), Big Transfer models and Spiking Neural Networks (SNNs). We first conduct a transferability analysis, to demonstrate the adversarial examples generated by customized attacks on one defense, are not often misclassified by another defense.  This finding leads to two important questions. First, how can the low transferability between defenses be utilized in a game theoretic framework to improve the robustness? Second, how can an adversary within this framework develop effective multi-model attacks? In this paper, we provide a game-theoretic framework for ensemble adversarial attacks and defenses. Our framework
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#20381;&#36182;&#22270;&#30340;&#36890;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;(GraphS4mer)&#65292;&#29992;&#20110;&#24314;&#31435;&#22810;&#20803;&#29983;&#29289;&#20449;&#21495;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#26550;&#26500;&#21644;&#21160;&#24577;&#28436;&#21464;&#30340;&#22270;&#32467;&#26500;&#23398;&#20064;&#23618;&#26469;&#35299;&#20915;&#38271;&#26102;&#24207;&#21644;&#22797;&#26434;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#33021;&#26377;&#25928;&#22320;&#25552;&#39640;&#22810;&#20803;&#29983;&#29289;&#20449;&#21495;&#20998;&#31867;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.11176</link><description>&lt;p&gt;
&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#24314;&#31435;&#22810;&#20803;&#29983;&#29289;&#20449;&#21495;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Modeling Multivariate Biosignals With Graph Neural Networks and Structured State Space Models. (arXiv:2211.11176v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#20381;&#36182;&#22270;&#30340;&#36890;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;(GraphS4mer)&#65292;&#29992;&#20110;&#24314;&#31435;&#22810;&#20803;&#29983;&#29289;&#20449;&#21495;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#26550;&#26500;&#21644;&#21160;&#24577;&#28436;&#21464;&#30340;&#22270;&#32467;&#26500;&#23398;&#20064;&#23618;&#26469;&#35299;&#20915;&#38271;&#26102;&#24207;&#21644;&#22797;&#26434;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#33021;&#26377;&#25928;&#22320;&#25552;&#39640;&#22810;&#20803;&#29983;&#29289;&#20449;&#21495;&#20998;&#31867;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#29983;&#29289;&#20449;&#21495;&#22312;&#35768;&#22810;&#21307;&#23398;&#39046;&#22495;&#20013;&#37117;&#24456;&#26222;&#36941;&#65292;&#20363;&#22914;&#33041;&#30005;&#22270;&#12289;&#22810;&#23548;&#30561;&#30496;&#22270;&#21644;&#24515;&#30005;&#22270;&#12290;&#30001;&#20110;&#65288;1&#65289;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#21644;&#65288;2&#65289;&#30005;&#26497;&#20043;&#38388;&#22797;&#26434;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#24314;&#31435;&#22810;&#20803;&#29983;&#29289;&#20449;&#21495;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#22810;&#20803;&#29983;&#29289;&#20449;&#21495;&#34920;&#31034;&#20026;&#26102;&#38388;&#20381;&#36182;&#22270;&#65292;&#24182;&#20171;&#32461;&#20102;GraphS4mer&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#32467;&#26500;&#65292;&#36890;&#36807;&#24314;&#31435;&#29983;&#29289;&#20449;&#21495;&#20013;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#26469;&#25552;&#39640;&#29983;&#29289;&#20449;&#21495;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#65288;1&#65289;&#25105;&#20204;&#21033;&#29992;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#26550;&#26500;&#65292;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24207;&#21015;&#27169;&#22411;&#65292;&#26469;&#25429;&#25417;&#29983;&#29289;&#20449;&#21495;&#20013;&#38271;&#26102;&#38388;&#33539;&#22260;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#65288;2&#65289;&#25105;&#20204;&#24314;&#35758;&#22312;GraphS4mer&#20013;&#28155;&#21152;&#22270;&#32467;&#26500;&#23398;&#20064;&#23618;&#65292;&#20197;&#23398;&#20064;&#25968;&#25454;&#20013;&#21160;&#24577;&#28436;&#21464;&#30340;&#22270;&#32467;&#26500;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#29983;&#29289;&#20449;&#21495;&#20998;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#23427;&#20248;&#20110;&#20960;&#31181;&#22522;&#20934;&#27169;&#22411;&#65292;&#31361;&#26174;&#20102;&#23427;&#22312;&#24314;&#31435;&#20855;&#26377;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#30340;&#22810;&#20803;&#29983;&#29289;&#20449;&#21495;&#27169;&#22411;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate biosignals are prevalent in many medical domains, such as electroencephalography, polysomnography, and electrocardiography. Modeling spatiotemporal dependencies in multivariate biosignals is challenging due to (1) long-range temporal dependencies and (2) complex spatial correlations between the electrodes. To address these challenges, we propose representing multivariate biosignals as time-dependent graphs and introduce GraphS4mer, a general graph neural network (GNN) architecture that improves performance on biosignal classification tasks by modeling spatiotemporal dependencies in biosignals. Specifically, (1) we leverage the Structured State Space architecture, a state-of-the-art deep sequence model, to capture long-range temporal dependencies in biosignals and (2) we propose a graph structure learning layer in GraphS4mer to learn dynamically evolving graph structures in the data. We evaluate our proposed model on three distinct biosignal classification tasks and show th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26377;&#20851;&#35821;&#27861;&#24863;&#30693;&#30340;&#21363;&#26102;&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#26041;&#27861; PyCoder&#65292;&#36890;&#36807;&#21033;&#29992;&#36731;&#37327;&#32423;&#30340;&#35821;&#27861;&#20449;&#24687;&#65288;&#26631;&#35760;&#31867;&#22411;&#65289;&#65292;&#23427;&#25554;&#20837;&#20102;&#25903;&#25345;&#20219;&#21153;&#26469;&#25552;&#39640;&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#30340;&#24615;&#33021;&#65292;&#22312;&#19981;&#32771;&#34385;&#35821;&#27861;&#20449;&#24687;&#30340;&#26368;&#20808;&#36827;&#21363;&#26102;&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#26041;&#27861;&#20013;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#21516;&#26102;&#19982;&#26080;&#27861;&#21363;&#26102;&#24037;&#20316;&#30340;&#35821;&#27861;&#24863;&#30693;&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;</title><link>http://arxiv.org/abs/2211.04673</link><description>&lt;p&gt;
&#35821;&#27861;&#24863;&#30693;&#30340;&#21363;&#26102;&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Syntax-Aware On-the-Fly Code Completion. (arXiv:2211.04673v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26377;&#20851;&#35821;&#27861;&#24863;&#30693;&#30340;&#21363;&#26102;&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#26041;&#27861; PyCoder&#65292;&#36890;&#36807;&#21033;&#29992;&#36731;&#37327;&#32423;&#30340;&#35821;&#27861;&#20449;&#24687;&#65288;&#26631;&#35760;&#31867;&#22411;&#65289;&#65292;&#23427;&#25554;&#20837;&#20102;&#25903;&#25345;&#20219;&#21153;&#26469;&#25552;&#39640;&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#30340;&#24615;&#33021;&#65292;&#22312;&#19981;&#32771;&#34385;&#35821;&#27861;&#20449;&#24687;&#30340;&#26368;&#20808;&#36827;&#21363;&#26102;&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#26041;&#27861;&#20013;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#21516;&#26102;&#19982;&#26080;&#27861;&#21363;&#26102;&#24037;&#20316;&#30340;&#35821;&#27861;&#24863;&#30693;&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#26088;&#22312;&#36890;&#36807;&#20174;&#32473;&#23450;&#19978;&#19979;&#25991;&#25512;&#33616;&#19979;&#19968;&#20010;&#20195;&#30721;&#26631;&#35760;&#26469;&#25552;&#39640;&#24320;&#21457;&#20154;&#21592;&#30340;&#29983;&#20135;&#21147;&#12290;&#20026;&#20102;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#34701;&#21512;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#20449;&#24687;&#65292;&#20174;&#32780;&#30830;&#20445;&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#33021;&#22815;&#24847;&#35782;&#21040;&#32534;&#31243;&#35821;&#35328;&#30340;&#35821;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35821;&#27861;&#24863;&#30693;&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#26041;&#27861;&#24182;&#38750;&#21363;&#26102;&#65292;&#22240;&#20026;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#27599;&#20010;&#24320;&#21457;&#20154;&#21592;&#36755;&#20837;&#30340;&#19977;&#20998;&#20043;&#20108;&#30340;&#23383;&#31526;&#20013;&#65292;AST&#26080;&#27861;&#25552;&#21462;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#31526;&#21512;&#35821;&#27861;&#27491;&#30830;&#30340;&#28304;&#20195;&#30721;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#29616;&#26377;&#30340;&#21363;&#26102;&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#19981;&#32771;&#34385;&#35821;&#27861;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986; PyCoder &#26469;&#21033;&#29992;&#20196;&#20154;&#28385;&#24847;&#30340;&#36731;&#37327;&#32423;&#30340;&#35821;&#27861;&#20449;&#24687;&#8212;&#8212;&#26631;&#35760;&#31867;&#22411;&#65292;&#35813;&#20449;&#24687;&#24050;&#32463;&#21487;&#29992;&#24182;&#19988;&#19982;&#28304;&#20195;&#30721;&#30340;&#33258;&#28982;&#39034;&#24207;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340; PyCoder &#26159;&#36890;&#36807;&#22810;&#20219;&#21153;&#35757;&#32451;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#22240;&#27492;&#36890;&#36807;&#23398;&#20064;&#39044;&#27979;&#26631;&#35760;&#31867;&#22411;&#24207;&#21015;&#30340;&#25903;&#25345;&#20219;&#21153;&#65292;&#23427;&#21487;&#20197;&#25552;&#39640;&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#20013;&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PyCoder &#22312;&#19981;&#32771;&#34385;&#35821;&#27861;&#20449;&#24687;&#30340;&#26368;&#20808;&#36827;&#21363;&#26102;&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#26041;&#27861;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#21516;&#26102;&#19982;&#26080;&#27861;&#21363;&#26102;&#24037;&#20316;&#30340;&#35821;&#27861;&#24863;&#30693;&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code completion aims to help improve developers' productivity by suggesting the next code tokens from a given context. Various approaches have been proposed to incorporate abstract syntax tree (AST) information for model training, ensuring that code completion is aware of the syntax of the programming languages. However, existing syntax-aware code completion approaches are not on-the-fly, as we found that for every two-thirds of characters that developers type, AST fails to be extracted because it requires the syntactically correct source code, limiting its practicality in real-world scenarios. On the other hand, existing on-the-fly code completion does not consider syntactic information yet. In this paper, we propose PyCoder to leverage token types, a kind of lightweight syntactic information, which is readily available and aligns with the natural order of source code. Our PyCoder is trained in a multi-task training manner so that by learning the supporting task of predicting token ty
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#21160;&#24577;&#35748;&#30693;&#36923;&#36753;&#20013;&#25193;&#23637;&#20102;&#34892;&#21160;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#20195;&#29702;&#26356;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#26377;&#36873;&#25321;&#24615;&#22320;&#28155;&#21152;&#25110;&#21024;&#38500;&#20195;&#29702;&#65292;&#24182;&#36827;&#34892;&#20195;&#29702;&#26356;&#26032;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#29992;&#20110;&#27169;&#25311;&#19968;&#20123;&#26377;&#36259;&#30340;&#20363;&#23376;&#65292;&#24182;&#22312;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#30340;&#24314;&#27169;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2211.02452</link><description>&lt;p&gt;
&#21160;&#24577;&#35748;&#30693;&#36923;&#36753;&#20013;&#30340;&#20195;&#29702;&#26356;&#26032;&#19982;&#20449;&#24565;&#24402;&#23646;
&lt;/p&gt;
&lt;p&gt;
Changing agents and ascribing beliefs in dynamic epistemic logic. (arXiv:2211.02452v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#21160;&#24577;&#35748;&#30693;&#36923;&#36753;&#20013;&#25193;&#23637;&#20102;&#34892;&#21160;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#20195;&#29702;&#26356;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#26377;&#36873;&#25321;&#24615;&#22320;&#28155;&#21152;&#25110;&#21024;&#38500;&#20195;&#29702;&#65292;&#24182;&#36827;&#34892;&#20195;&#29702;&#26356;&#26032;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#29992;&#20110;&#27169;&#25311;&#19968;&#20123;&#26377;&#36259;&#30340;&#20363;&#23376;&#65292;&#24182;&#22312;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#30340;&#24314;&#27169;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21160;&#24577;&#35748;&#30693;&#36923;&#36753;&#20013;&#65292;&#36890;&#24120;&#20351;&#29992;&#34892;&#21160;&#26694;&#26550;&#26469;&#25551;&#36848;&#21333;&#20010;&#34892;&#21160;&#30340;&#19981;&#21516;&#35270;&#35282;&#12290;&#26412;&#25991;&#23558;&#34892;&#21160;&#26694;&#26550;&#25193;&#23637;&#20026;&#28155;&#21152;&#25110;&#21024;&#38500;&#20195;&#29702;&#65292;&#31216;&#20043;&#20026;&#20195;&#29702;&#26356;&#26032;&#26694;&#26550;&#12290;&#21487;&#20197;&#26377;&#36873;&#25321;&#24615;&#22320;&#36827;&#34892;&#20195;&#29702;&#26356;&#26032;&#65292;&#21482;&#26377;&#26576;&#20123;&#25351;&#23450;&#30340;&#20195;&#29702;&#20250;&#33719;&#24471;&#26356;&#26032;&#30340;&#20449;&#24687;&#65292;&#36825;&#21487;&#29992;&#20110;&#27169;&#25311;&#19968;&#20123;&#26377;&#36259;&#30340;&#20363;&#23376;&#65292;&#22914;&#31169;&#26377;&#26356;&#26032;&#21644;&#27450;&#39575;&#12290;&#28982;&#21518;&#23558;&#19968;&#20010;Kripke&#27169;&#22411;&#36890;&#36807;&#20195;&#29702;&#26356;&#26032;&#26694;&#26550;&#30340;&#27714;&#21644;&#31215;&#26356;&#26032;&#36827;&#19968;&#27493;&#25193;&#23637;&#65292;&#29992;&#20110;&#24314;&#27169;&#25925;&#20107;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21160;&#24577;&#35748;&#30693;&#36923;&#36753;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In dynamic epistemic logic (Van Ditmarsch, Van Der Hoek, &amp; Kooi, 2008) it is customary to use an action frame (Baltag &amp; Moss, 2004; Baltag, Moss, &amp; Solecki, 1998) to describe different views of a single action. In this article, action frames are extended to add or remove agents, we call these agent-update frames. This can be done selectively so that only some specified agents get information of the update, which can be used to model several interesting examples such as private update and deception, studied earlier by Baltag and Moss (2004); Sakama (2015); Van Ditmarsch, Van Eijck, Sietsma, and Wang (2012). The product update of a Kripke model by an action frame is an abbreviated way of describing the transformed Kripke model which is the result of performing the action. This is substantially extended to a sum-product update of a Kripke model by an agent-update frame in the new setting. These ideas are applied to an AI problem of modelling a story. We show that dynamic epistemic logics,
&lt;/p&gt;</description></item><item><title>SAM-RL&#20351;&#29992;&#19981;&#21487;&#23548;&#29289;&#29702;&#20223;&#30495;&#21644;&#28210;&#26579;&#65292;&#36890;&#36807;&#27604;&#36739;&#28210;&#26579;&#22270;&#20687;&#21644;&#30495;&#23454;&#21407;&#22987;&#22270;&#20687;&#33258;&#21160;&#26356;&#26032;&#27169;&#22411;&#65292;&#24182;&#39640;&#25928;&#20135;&#29983;&#31574;&#30053;&#12290;&#24863;&#30693;&#24863;&#30693;&#30340;&#23398;&#20064;&#31649;&#36947;&#20801;&#35768;&#26426;&#22120;&#20154;&#36873;&#25321;&#20449;&#24687;&#20016;&#23500;&#30340;&#35270;&#35282;&#30417;&#25511;&#20219;&#21153;&#36807;&#31243;&#12290; &#29992;&#20110;&#23436;&#25104;&#26426;&#22120;&#20154;&#32452;&#35013;&#65292;&#24037;&#20855;&#25805;&#20316;&#21644;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2210.15185</link><description>&lt;p&gt;
&#22522;&#20110;&#19981;&#21487;&#23548;&#29289;&#29702;&#20223;&#30495;&#28210;&#26579;&#30340;&#24863;&#30693;&#24863;&#30693;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SAM-RL: Sensing-Aware Model-Based Reinforcement Learning via Differentiable Physics-Based Simulation and Rendering. (arXiv:2210.15185v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15185
&lt;/p&gt;
&lt;p&gt;
SAM-RL&#20351;&#29992;&#19981;&#21487;&#23548;&#29289;&#29702;&#20223;&#30495;&#21644;&#28210;&#26579;&#65292;&#36890;&#36807;&#27604;&#36739;&#28210;&#26579;&#22270;&#20687;&#21644;&#30495;&#23454;&#21407;&#22987;&#22270;&#20687;&#33258;&#21160;&#26356;&#26032;&#27169;&#22411;&#65292;&#24182;&#39640;&#25928;&#20135;&#29983;&#31574;&#30053;&#12290;&#24863;&#30693;&#24863;&#30693;&#30340;&#23398;&#20064;&#31649;&#36947;&#20801;&#35768;&#26426;&#22120;&#20154;&#36873;&#25321;&#20449;&#24687;&#20016;&#23500;&#30340;&#35270;&#35282;&#30417;&#25511;&#20219;&#21153;&#36807;&#31243;&#12290; &#29992;&#20110;&#23436;&#25104;&#26426;&#22120;&#20154;&#32452;&#35013;&#65292;&#24037;&#20855;&#25805;&#20316;&#21644;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;MBRL&#65289;&#20855;&#26377;&#27604;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#22914;&#20309;&#20174;&#21407;&#22987;&#24863;&#23448;&#36755;&#20837;&#65288;&#22914;&#22270;&#20687;&#65289;&#33258;&#21160;&#26377;&#25928;&#22320;&#24320;&#21457;&#20934;&#30830;&#30340;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#22797;&#26434;&#30340;&#29615;&#22659;&#21644;&#20219;&#21153;&#65292;&#26159;&#38480;&#21046;MBRL&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SAM-RL&#30340;&#24863;&#30693;&#24863;&#30693;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#12290;&#21033;&#29992;&#19981;&#21487;&#23548;&#29289;&#29702;&#20223;&#30495;&#21644;&#28210;&#26579;&#65292;SAM-RL&#36890;&#36807;&#27604;&#36739;&#28210;&#26579;&#22270;&#20687;&#21644;&#30495;&#23454;&#21407;&#22987;&#22270;&#20687;&#33258;&#21160;&#26356;&#26032;&#27169;&#22411;&#24182;&#39640;&#25928;&#20135;&#29983;&#31574;&#30053;&#12290;&#36890;&#36807;&#24863;&#30693;&#24863;&#30693;&#23398;&#20064;&#31649;&#36947;&#65292;SAM-RL&#20801;&#35768;&#26426;&#22120;&#20154;&#36873;&#25321;&#19968;&#20010;&#20449;&#24687;&#20016;&#23500;&#30340;&#35270;&#35282;&#26469;&#30417;&#25511;&#20219;&#21153;&#36807;&#31243;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#23454;&#38469;&#30340;&#19977;&#20010;&#25805;&#20316;&#20219;&#21153;&#65306;&#26426;&#22120;&#20154;&#35013;&#37197;&#65292;&#24037;&#20855;&#25805;&#32437;&#21644;&#21487;&#21464;&#24418;&#29289;&#20307;&#25805;&#32437;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based reinforcement learning (MBRL) is recognized with the potential to be significantly more sample-efficient than model-free RL. How an accurate model can be developed automatically and efficiently from raw sensory inputs (such as images), especially for complex environments and tasks, is a challenging problem that hinders the broad application of MBRL in the real world. In this work, we propose a sensing-aware model-based reinforcement learning system called SAM-RL. Leveraging the differentiable physics-based simulation and rendering, SAM-RL automatically updates the model by comparing rendered images with real raw images and produces the policy efficiently. With the sensing-aware learning pipeline, SAM-RL allows a robot to select an informative viewpoint to monitor the task process. We apply our framework to real world experiments for accomplishing three manipulation tasks: robotic assembly, tool manipulation, and deformable object manipulation. We demonstrate the effectivene
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807; Bellman &#22266;&#23450;&#28857;&#26041;&#31243;&#36827;&#34892;&#24494;&#20998;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20540;&#36845;&#20195;&#32593;&#32476;&#21450;&#20854;&#21464;&#20307;&#30340;&#21069;&#21518;&#20256;&#36882;&#35299;&#32806;&#65292;&#21487;&#23454;&#29616;&#22312;&#35268;&#21010;&#35270;&#31243;&#20869;&#31283;&#23450;&#19988;&#28789;&#27963;&#21069;&#21521;&#39044;&#31639;&#30340;&#25193;&#23637;&#65292;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#35268;&#21010;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2210.13542</link><description>&lt;p&gt;
&#36890;&#36807;&#38544;&#24335;&#24494;&#20998;&#23454;&#29616;&#21487;&#21464;&#35268;&#27169;&#31283;&#23450;&#30340;&#21487;&#24494;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Scaling up and Stabilizing Differentiable Planning with Implicit Differentiation. (arXiv:2210.13542v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807; Bellman &#22266;&#23450;&#28857;&#26041;&#31243;&#36827;&#34892;&#24494;&#20998;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20540;&#36845;&#20195;&#32593;&#32476;&#21450;&#20854;&#21464;&#20307;&#30340;&#21069;&#21518;&#20256;&#36882;&#35299;&#32806;&#65292;&#21487;&#23454;&#29616;&#22312;&#35268;&#21010;&#35270;&#31243;&#20869;&#31283;&#23450;&#19988;&#28789;&#27963;&#21069;&#21521;&#39044;&#31639;&#30340;&#25193;&#23637;&#65292;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#35268;&#21010;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#24494;&#35268;&#21010;&#25215;&#35834;&#20855;&#26377;&#31471;&#21040;&#31471;&#30340;&#21487;&#24494;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#38382;&#39064;&#38459;&#27490;&#20102;&#23427;&#22312;&#26356;&#22823;&#35268;&#27169;&#30340;&#38382;&#39064;&#19978;&#30340;&#25193;&#23637;&#65306;&#38656;&#35201;&#36890;&#36807;&#21521;&#21069;&#36845;&#20195;&#23618;&#36827;&#34892;&#24494;&#20998;&#20197;&#35745;&#31639;&#26799;&#24230;&#65292;&#36825;&#20250;&#23558;&#21069;&#21521;&#35745;&#31639;&#21644;&#21453;&#21521;&#20256;&#25773;&#32806;&#21512;&#36215;&#26469;&#65292;&#24182;&#38656;&#35201;&#24179;&#34913;&#21069;&#21521;&#35268;&#21010;&#22120;&#30340;&#24615;&#33021;&#21644;&#21453;&#21521;&#20256;&#36882;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807; Bellman &#22266;&#23450;&#28857;&#26041;&#31243;&#36827;&#34892;&#24494;&#20998;&#20197;&#23558;&#20540;&#36845;&#20195;&#32593;&#32476;&#21450;&#20854;&#21464;&#20307;&#30340;&#21069;&#21521;&#21644;&#21518;&#21521;&#20256;&#36882;&#35299;&#32806;&#30340;&#26041;&#27861;&#65292;&#36825;&#20351;&#24471;&#21453;&#21521;&#20256;&#25773;&#25104;&#26412;&#65288;&#22312;&#35268;&#21010;&#35270;&#31243;&#20869;&#65289;&#20445;&#25345;&#19981;&#21464;&#65292;&#21516;&#26102;&#21069;&#21521;&#39044;&#31639;&#26356;&#21152;&#28789;&#27963;&#65292;&#26377;&#21161;&#20110;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#38382;&#39064;&#19978;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25152;&#25552;&#20986;&#30340; VIN &#38544;&#24335;&#29256;&#26412;&#21450;&#20854;&#21464;&#20307;&#30340;&#25910;&#25947;&#31283;&#23450;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#25928;&#29575;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#35268;&#21010;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#20248;&#36234;&#24615;&#65306;2D &#23548;&#33322;&#12289;&#35270;&#35273;&#23548;&#33322;&#20197;&#21450;&#26500;&#22411;&#31354;&#38388;&#21644;&#24037;&#20316;&#31354;&#38388;&#20013;&#30340; 2-DOF &#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentiable planning promises end-to-end differentiability and adaptivity. However, an issue prevents it from scaling up to larger-scale problems: they need to differentiate through forward iteration layers to compute gradients, which couples forward computation and backpropagation, and needs to balance forward planner performance and computational cost of the backward pass. To alleviate this issue, we propose to differentiate through the Bellman fixed-point equation to decouple forward and backward passes for Value Iteration Network and its variants, which enables constant backward cost (in planning horizon) and flexible forward budget and helps scale up to large tasks. We study the convergence stability, scalability, and efficiency of the proposed implicit version of VIN and its variants and demonstrate their superiorities on a range of planning tasks: 2D navigation, visual navigation, and 2-DOF manipulation in configuration space and workspace.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36731;&#37327;&#32423;&#26694;&#26550;&#65292;&#23558;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#19982;Conv2D LSTM&#23618;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#23156;&#20799;&#29289;&#29702;&#23433;&#20840;&#30417;&#27979;&#30340;&#34892;&#20026;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2210.12527</link><description>&lt;p&gt;
&#26234;&#33021;&#23478;&#23621;&#20013;&#23156;&#20799;&#29289;&#29702;&#23433;&#20840;&#30417;&#27979;&#20351;&#29992;&#21160;&#20316;&#35782;&#21035;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Baby Physical Safety Monitoring in Smart Home Using Action Recognition System. (arXiv:2210.12527v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36731;&#37327;&#32423;&#26694;&#26550;&#65292;&#23558;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#19982;Conv2D LSTM&#23618;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#23156;&#20799;&#29289;&#29702;&#23433;&#20840;&#30417;&#27979;&#30340;&#34892;&#20026;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#33021;&#22815;&#36890;&#36807;&#28436;&#32462;&#25512;&#29702;&#30452;&#35266;&#22320;&#25512;&#26029;&#20986;&#20004;&#20010;&#29366;&#24577;&#20043;&#38388;&#21457;&#29983;&#30340;&#34892;&#20026;&#65292;&#36825;&#26159;&#22240;&#20026;&#22823;&#33041;&#25805;&#20316;&#22312;&#21452;&#21521;&#36890;&#20449;&#27169;&#22411;&#19978;&#65292;&#36825;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#20110;&#19982;&#20197;&#24448;&#32463;&#39564;&#30456;&#20851;&#30340;&#29305;&#24449;&#30340;&#35782;&#21035;&#21644;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#21160;&#20316;&#35782;&#21035;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26377;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#29305;&#23450;&#30340;&#21160;&#20316;&#35782;&#21035;&#20219;&#21153;&#20013;&#38754;&#20020;&#30528;&#23567;&#22411;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;&#19982;&#22823;&#22810;&#25968;&#21160;&#20316;&#35782;&#21035;&#20219;&#21153;&#19968;&#26679;&#65292;&#22312;&#31354;&#38388;-&#26102;&#38388;&#25968;&#25454;&#20013;&#20934;&#30830;&#25551;&#36848;&#27963;&#21160;&#30340;&#27169;&#31946;&#24615;&#26159;&#19968;&#20010;&#32570;&#28857;&#65292;&#21487;&#20197;&#36890;&#36807;&#31574;&#21010;&#36866;&#24403;&#30340;&#25968;&#25454;&#38598;&#26469;&#20811;&#26381;&#65292;&#21253;&#25324;&#23545;&#35270;&#39057;&#25968;&#25454;&#36827;&#34892;&#32454;&#33268;&#30340;&#27880;&#37322;&#21644;&#39044;&#22788;&#29702;&#65292;&#20197;&#20998;&#26512;&#21508;&#31181;&#35782;&#21035;&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36731;&#37327;&#32423;&#26694;&#26550;&#65292;&#23558;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#19982;Conv2D LSTM&#23618;&#30456;&#32467;&#21512;&#65292;&#20174;&#39044;&#35757;&#32451;&#30340;I3D&#27169;&#22411;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#29992;&#20110;&#23156;&#20799;&#29289;&#29702;&#23433;&#20840;&#30417;&#27979;&#30340;&#34892;&#20026;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans are able to intuitively deduce actions that took place between two states in observations via deductive reasoning. This is because the brain operates on a bidirectional communication model, which has radically improved the accuracy of recognition and prediction based on features connected to previous experiences. During the past decade, deep learning models for action recognition have significantly improved. However, deep neural networks struggle with these tasks on a smaller dataset for specific Action Recognition (AR) tasks. As with most action recognition tasks, the ambiguity of accurately describing activities in spatial-temporal data is a drawback that can be overcome by curating suitable datasets, including careful annotations and preprocessing of video data for analyzing various recognition tasks. In this study, we present a novel lightweight framework combining transfer learning techniques with a Conv2D LSTM layer to extract features from the pre-trained I3D model on the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#22270;&#21644;&#20174;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#26041;&#38754;&#30340;&#23616;&#38480;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#27169;&#22359;GNN&#26694;&#26550;&#65292;&#36890;&#36807;&#25512;&#24191;&#21333;&#20010;&#35268;&#33539;&#38750;&#32447;&#24615;&#21464;&#25442;&#26469;&#36866;&#24212;&#26032;&#22270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#27169;&#22359;GNN&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#22343;&#26174;&#33879;&#25552;&#39640;&#20102;GNN&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#20960;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.06589</link><description>&lt;p&gt;
&#22810;&#27169;&#22359;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28789;&#27963;&#34920;&#24449;&#20419;&#36827;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Towards Better Generalization with Flexible Representation of Multi-Module Graph Neural Networks. (arXiv:2209.06589v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#22270;&#21644;&#20174;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#26041;&#38754;&#30340;&#23616;&#38480;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#27169;&#22359;GNN&#26694;&#26550;&#65292;&#36890;&#36807;&#25512;&#24191;&#21333;&#20010;&#35268;&#33539;&#38750;&#32447;&#24615;&#21464;&#25442;&#26469;&#36866;&#24212;&#26032;&#22270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#27169;&#22359;GNN&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#22343;&#26174;&#33879;&#25552;&#39640;&#20102;GNN&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#20960;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#25104;&#20026;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#23398;&#20064;&#19982;&#25512;&#26029;&#30340;&#24378;&#22823;&#27169;&#22411;&#65292;&#20294;&#23545;&#20110;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#22270;&#20197;&#21450;&#25512;&#24191;&#21040;&#20174;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#30340;&#22522;&#26412;&#38480;&#21046;&#30340;&#20102;&#35299;&#36824;&#19981;&#36275;&#12290;&#26412;&#25991;&#20351;&#29992;&#38543;&#26426;&#22270;&#29983;&#25104;&#22120;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22270;&#30340;&#22823;&#23567;&#21644;&#32467;&#26500;&#23646;&#24615;&#22914;&#20309;&#24433;&#21709;GNN&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#22810;&#27169;&#22359;GNN&#26694;&#26550;&#65292;&#36890;&#36807;&#25512;&#24191;&#21333;&#20010;&#35268;&#33539;&#38750;&#32447;&#24615;&#21464;&#25442;&#26469;&#36866;&#24212;&#26032;&#22270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#27169;&#22359;GNN&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#22343;&#26174;&#33879;&#25552;&#39640;&#20102;GNN&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#20960;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have become compelling models designed to perform learning and inference on graph-structured data. However, little work has been done to understand the fundamental limitations of GNNs for scaling to larger graphs and generalizing to out-of-distribution (OOD) inputs. In this paper, we use a random graph generator to systematically investigate how the graph size and structural properties affect the predictive performance of GNNs. We present specific evidence that the average node degree is a key feature in determining whether GNNs can generalize to unseen graphs, and that the use of multiple node update functions can improve the generalization performance of GNNs when dealing with graphs of multimodal degree distributions. Accordingly, we propose a multi-module GNN framework that allows the network to adapt flexibly to new graphs by generalizing a single canonical nonlinear transformation over aggregated inputs. Our results show that the multi-module GNNs imp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20013;&#24515;&#21270;&#21098;&#35009;&#22312;&#38754;&#23545;&#19981;&#21516;&#24694;&#24847;&#20195;&#29702;&#26102;&#30340;&#33030;&#24369;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22810;&#24341;&#29992;&#28857;&#21098;&#35009; (MRPC) &#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;MRPC &#26694;&#26550;&#21033;&#29992;&#22810;&#20010;&#21442;&#32771;&#28857;&#26377;&#25928;&#22320;&#20013;&#21644;&#19987;&#38376;&#35774;&#35745;&#30340; Byzantine attacks&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#31867;&#22411;&#30340; Byzantine attacks &#19979;&#65292;MRPC &#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340; FL &#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2208.09894</link><description>&lt;p&gt;
&#25308;&#21344;&#24237;&#20154;&#20063;&#33021;&#20174;&#21382;&#21490;&#20013;&#23398;&#20064;&#65306;&#32852;&#37030;&#23398;&#20064;&#20013;&#24515;&#21270;&#21098;&#35009;&#30340;&#34928;&#33853;
&lt;/p&gt;
&lt;p&gt;
Byzantines can also Learn from History: Fall of Centered Clipping in Federated Learning. (arXiv:2208.09894v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20013;&#24515;&#21270;&#21098;&#35009;&#22312;&#38754;&#23545;&#19981;&#21516;&#24694;&#24847;&#20195;&#29702;&#26102;&#30340;&#33030;&#24369;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22810;&#24341;&#29992;&#28857;&#21098;&#35009; (MRPC) &#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;MRPC &#26694;&#26550;&#21033;&#29992;&#22810;&#20010;&#21442;&#32771;&#28857;&#26377;&#25928;&#22320;&#20013;&#21644;&#19987;&#38376;&#35774;&#35745;&#30340; Byzantine attacks&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#31867;&#22411;&#30340; Byzantine attacks &#19979;&#65292;MRPC &#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340; FL &#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064; (FL) &#26694;&#26550;&#30001;&#20110;&#22312;&#24191;&#27867;&#30340;&#21327;&#20316;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#32780;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#20063;&#24341;&#36215;&#20102;&#26576;&#20123;&#23433;&#20840;&#38382;&#39064;&#12290;&#20854;&#20013;&#65292;&#25308;&#21344;&#24237;&#25915;&#20987;&#30340;&#39118;&#38505;&#26159;&#29305;&#21035;&#20851;&#27880;&#30340;&#38382;&#39064;&#65292;&#36825;&#25351;&#30340;&#26159;&#24694;&#24847;&#23458;&#25143;&#21442;&#19982;&#23398;&#20064;&#36807;&#31243;&#30340;&#21487;&#33021;&#24615;&#12290;&#22240;&#27492;&#65292;FL &#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#30446;&#26631;&#26159;&#28040;&#38500; Byzantine attacks &#30340;&#28508;&#22312;&#24433;&#21709;&#65292;&#30830;&#20445;&#26368;&#32456;&#27169;&#22411;&#26159;&#21487;&#20449;&#30340;&#12290;&#24050;&#32463;&#35266;&#23519;&#21040;&#65292;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;/&#26356;&#26032;&#20043;&#38388;&#30340;&#26041;&#24046;&#36234;&#22823;&#65292;&#38544;&#34255; Byzantine attacks &#30340;&#31354;&#38388;&#23601;&#36234;&#22823;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#20351;&#29992;&#21160;&#37327;&#65292;&#20174;&#32780;&#20943;&#23569;&#26041;&#24046;&#65292;&#21487;&#20197;&#21066;&#24369;&#24050;&#30693; Byzantine attacks &#30340;&#21147;&#37327;&#12290;&#20013;&#24515;&#21270;&#21098;&#35009; (CC) &#26694;&#26550;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#19978;&#19968;&#27425;&#30340;&#21160;&#37327;&#39033;&#38500;&#20102;&#20943;&#23569;&#26041;&#24046;&#22806;&#65292;&#36824;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#21442;&#32771;&#28857;&#26356;&#22909;&#22320;&#28040;&#38500; Byzantine attacks&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19981;&#21516;&#30340;&#24694;&#24847;&#20195;&#29702;&#26377;&#19981;&#21516;&#30446;&#26631;&#26102; CC &#30340;&#33030;&#24369;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#21098;&#35009;&#31639;&#27861;&#31216;&#20026;&#22810;&#24341;&#29992;&#28857;&#21098;&#35009; (MRPC)&#65292;&#20197;&#20811;&#26381;&#36825;&#31181;&#33030;&#24369;&#24615;&#12290;MRPC &#26694;&#26550;&#26377;&#25928;&#22320;&#21033;&#29992;&#22810;&#20010;&#21442;&#32771;&#28857;&#26469;&#28040;&#38500;&#19987;&#38376;&#35774;&#35745;&#20197;&#32469;&#36807; CC &#26041;&#27861;&#30340; Byzantine attacks&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#31867;&#22411;&#30340; Byzantine attacks &#19979;&#65292;MRPC &#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340; FL &#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing popularity of the federated learning (FL) framework due to its success in a wide range of collaborative learning tasks also induces certain security concerns. Among many vulnerabilities, the risk of Byzantine attacks is of particular concern, which refers to the possibility of malicious clients participating in the learning process. Hence, a crucial objective in FL is to neutralize the potential impact of Byzantine attacks, and to ensure that the final model is trustable. It has been observed that the higher the variance among the clients' models/updates, the more space there is for Byzantine attacks to be hidden. As a consequence, by utilizing momentum, and thus, reducing the variance, it is possible to weaken the strength of known Byzantine attacks. The centered clipping (CC) framework has further shown that, the momentum term from the previous iteration, besides reducing the variance, can be used as a reference point to neutralize Byzantine attacks better. In this wor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31526;&#21512;&#20445;&#24207;&#30340;&#39118;&#38505;&#25511;&#21046;&#26041;&#27861;&#65292;&#21487;&#20197;&#25511;&#21046;&#20219;&#20309;&#21333;&#35843;&#25439;&#22833;&#20989;&#25968;&#30340;&#26399;&#26395;&#20540;&#65292;&#31034;&#20363;&#35777;&#26126;&#20854;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20855;&#26377;&#25511;&#21046;&#35823;&#25253;&#29575;&#12289;&#22270;&#24418;&#36317;&#31163;&#21644;&#20196;&#29260;&#32423;F1&#24471;&#20998;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2208.02814</link><description>&lt;p&gt;
&#19968;&#31181;&#31526;&#21512;&#20445;&#24207;&#30340;&#39118;&#38505;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Conformal Risk Control. (arXiv:2208.02814v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.02814
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31526;&#21512;&#20445;&#24207;&#30340;&#39118;&#38505;&#25511;&#21046;&#26041;&#27861;&#65292;&#21487;&#20197;&#25511;&#21046;&#20219;&#20309;&#21333;&#35843;&#25439;&#22833;&#20989;&#25968;&#30340;&#26399;&#26395;&#20540;&#65292;&#31034;&#20363;&#35777;&#26126;&#20854;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20855;&#26377;&#25511;&#21046;&#35823;&#25253;&#29575;&#12289;&#22270;&#24418;&#36317;&#31163;&#21644;&#20196;&#29260;&#32423;F1&#24471;&#20998;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#31526;&#21512;&#24615;&#39044;&#27979;&#25512;&#24191;&#33267;&#25511;&#21046;&#20219;&#20309;&#21333;&#35843;&#25439;&#22833;&#20989;&#25968;&#30340;&#26399;&#26395;&#20540;&#12290;&#35813;&#31639;&#27861;&#23558;&#20998;&#35010;&#31526;&#21512;&#24615;&#39044;&#27979;&#21450;&#20854;&#35206;&#30422;&#20445;&#35777;&#36827;&#34892;&#20102;&#27867;&#21270;&#12290;&#31867;&#20284;&#20110;&#31526;&#21512;&#24615;&#39044;&#27979;&#65292;&#31526;&#21512;&#20445;&#24207;&#30340;&#39118;&#38505;&#25511;&#21046;&#26041;&#27861;&#22312;$\mathcal{O}(1/n)$&#22240;&#23376;&#20869;&#20445;&#25345;&#32039;&#23494;&#24615;&#12290;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#31034;&#20363;&#35777;&#26126;&#20102;&#25105;&#20204;&#31639;&#27861;&#22312;&#25511;&#21046;&#35823;&#25253;&#29575;&#12289;&#22270;&#24418;&#36317;&#31163;&#21644;&#20196;&#29260;&#32423;F1&#24471;&#20998;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We extend conformal prediction to control the expected value of any monotone loss function. The algorithm generalizes split conformal prediction together with its coverage guarantee. Like conformal prediction, the conformal risk control procedure is tight up to an $\mathcal{O}(1/n)$ factor. Worked examples from computer vision and natural language processing demonstrate the usage of our algorithm to bound the false negative rate, graph distance, and token-level F1-score.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Wasserstein WL&#23376;&#26641;(WWLS)&#36317;&#31163;&#30340;&#26032;&#22411;&#22270;&#36317;&#31163;&#65292;&#36890;&#36807;&#21033;&#29992;WL&#23376;&#26641;&#20316;&#20026;&#33410;&#28857;&#37051;&#22495;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#20351;&#29992;&#33410;&#28857;&#30340;WL&#23376;&#26641;&#20043;&#38388;&#30340;L1-&#36817;&#20284;&#26641;&#32534;&#36753;&#36317;&#31163;(L1-TED)&#23450;&#20041;&#33410;&#28857;&#24230;&#37327;&#65292;&#35299;&#20915;&#20102;WL&#27979;&#35797;&#26080;&#27861;&#25429;&#25417;&#36731;&#24494;&#32467;&#26500;&#24046;&#24322;&#30340;&#38382;&#39064;</title><link>http://arxiv.org/abs/2207.04216</link><description>&lt;p&gt;
&#22522;&#20110; Weisfeiler-Lehman &#23376;&#26641; L1-&#36817;&#20284;&#26641;&#32534;&#36753;&#36317;&#31163;&#30340; Wasserstein &#22270;&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
Wasserstein Graph Distance Based on $L_1$-Approximated Tree Edit Distance between Weisfeiler-Lehman Subtrees. (arXiv:2207.04216v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Wasserstein WL&#23376;&#26641;(WWLS)&#36317;&#31163;&#30340;&#26032;&#22411;&#22270;&#36317;&#31163;&#65292;&#36890;&#36807;&#21033;&#29992;WL&#23376;&#26641;&#20316;&#20026;&#33410;&#28857;&#37051;&#22495;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#20351;&#29992;&#33410;&#28857;&#30340;WL&#23376;&#26641;&#20043;&#38388;&#30340;L1-&#36817;&#20284;&#26641;&#32534;&#36753;&#36317;&#31163;(L1-TED)&#23450;&#20041;&#33410;&#28857;&#24230;&#37327;&#65292;&#35299;&#20915;&#20102;WL&#27979;&#35797;&#26080;&#27861;&#25429;&#25417;&#36731;&#24494;&#32467;&#26500;&#24046;&#24322;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Weisfeiler-Lehman (WL)&#27979;&#35797;&#26159;&#22270;&#26426;&#22120;&#23398;&#20064;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#31639;&#27861;&#65292;&#21253;&#25324;&#22270;&#20869;&#26680;&#12289;&#22270;&#24230;&#37327;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#23427;&#20165;&#20851;&#27880;&#22270;&#30340;&#19968;&#33268;&#24615;&#65292;&#26080;&#27861;&#26816;&#27979;&#36731;&#24494;&#30340;&#32467;&#26500;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#25429;&#25417;&#32467;&#26500;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#20063;&#38480;&#21046;&#20102;&#20381;&#36182;WL&#27979;&#35797;&#30340;&#29616;&#26377;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Wasserstein WL&#23376;&#26641;(WWLS)&#36317;&#31163;&#30340;&#26032;&#22411;&#22270;&#36317;&#31163;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;WL&#23376;&#26641;&#20316;&#20026;&#33410;&#28857;&#37051;&#22495;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#33410;&#28857;&#30340;WL&#23376;&#26641;&#20043;&#38388;&#30340;L1-&#36817;&#20284;&#26641;&#32534;&#36753;&#36317;&#31163;(L1-TED)&#23450;&#20041;&#33410;&#28857;&#24230;&#37327;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;Wasserstein&#36317;&#31163;&#21644;L1-TED&#26469;&#23450;&#20041;WWLS&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
The Weisfeiler-Lehman (WL) test is a widely used algorithm in graph machine learning, including graph kernels, graph metrics, and graph neural networks. However, it focuses only on the consistency of the graph, which means that it is unable to detect slight structural differences. Consequently, this limits its ability to capture structural information, which also limits the performance of existing models that rely on the WL test. This limitation is particularly severe for traditional metrics defined by the WL test, which cannot precisely capture slight structural differences. In this paper, we propose a novel graph metric called the Wasserstein WL Subtree (WWLS) distance to address this problem. Our approach leverages the WL subtree as structural information for node neighborhoods and defines node metrics using the $L_1$-approximated tree edit distance ($L_1$-TED) between WL subtrees of nodes. Subsequently, we combine the Wasserstein distance and the $L_1$-TED to define the WWLS distan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;NVIDIA&#32593;&#21345;&#20013;&#23454;&#29616;&#20102;&#24378;&#21270;&#23398;&#20064;&#25968;&#25454;&#20013;&#24515;&#25317;&#22622;&#25511;&#21046;&#65292;&#36890;&#36807;&#23558;RL-CC&#30340;&#22797;&#26434;&#31070;&#32463;&#32593;&#32476;&#36716;&#21270;&#20026;&#20915;&#31574;&#26641;&#65292;&#23454;&#29616;&#20102;&#23454;&#26102;&#25512;&#29702;&#65292;&#24182;&#25104;&#21151;&#25913;&#21892;&#20102;&#32593;&#32476;&#25317;&#22622;&#19979;&#30340;&#23614;&#37096;&#24310;&#36831;&#21644;&#25968;&#25454;&#21253;&#20002;&#22833;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2207.02295</link><description>&lt;p&gt;
&#22312;NVIDIA&#32593;&#21345;&#20013;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#25968;&#25454;&#20013;&#24515;&#25317;&#22622;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Implementing Reinforcement Learning Datacenter Congestion Control in NVIDIA NICs. (arXiv:2207.02295v4 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.02295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;NVIDIA&#32593;&#21345;&#20013;&#23454;&#29616;&#20102;&#24378;&#21270;&#23398;&#20064;&#25968;&#25454;&#20013;&#24515;&#25317;&#22622;&#25511;&#21046;&#65292;&#36890;&#36807;&#23558;RL-CC&#30340;&#22797;&#26434;&#31070;&#32463;&#32593;&#32476;&#36716;&#21270;&#20026;&#20915;&#31574;&#26641;&#65292;&#23454;&#29616;&#20102;&#23454;&#26102;&#25512;&#29702;&#65292;&#24182;&#25104;&#21151;&#25913;&#21892;&#20102;&#32593;&#32476;&#25317;&#22622;&#19979;&#30340;&#23614;&#37096;&#24310;&#36831;&#21644;&#25968;&#25454;&#21253;&#20002;&#22833;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36890;&#20449;&#21327;&#35758;&#30340;&#21457;&#23637;&#65292;&#25968;&#25454;&#20013;&#24515;&#32593;&#32476;&#30340;&#21033;&#29992;&#29575;&#36234;&#26469;&#36234;&#39640;&#65292;&#25317;&#22622;&#26356;&#20026;&#39057;&#32321;&#65292;&#23548;&#33268;&#24310;&#36831;&#21644;&#20002;&#21253;&#29575;&#22686;&#21152;&#12290;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20154;&#24037;&#35774;&#35745;&#25317;&#22622;&#25511;&#21046;&#31639;&#27861;&#21464;&#24471;&#26497;&#20854;&#22256;&#38590;&#65292;&#38656;&#35201;&#24320;&#21457;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#26469;&#26367;&#20195;&#20154;&#21147;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#32593;&#32476;&#35774;&#22791;&#35745;&#31639;&#33021;&#21147;&#26377;&#38480;&#65292;&#30446;&#21069;&#19981;&#21487;&#33021;&#22312;&#32593;&#32476;&#35774;&#22791;&#19978;&#37096;&#32626;AI&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#22522;&#20110;&#26368;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#25317;&#22622;&#25511;&#21046;&#31639;&#27861;[arXiv:2207.02295]&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#20915;&#31574;&#26641;&#30340;&#35745;&#31639;&#36731;&#37327;&#32423;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;RL-CC&#30340;&#22797;&#26434;&#31070;&#32463;&#32593;&#32476;&#36716;&#21270;&#20026;&#20915;&#31574;&#26641;&#65292;&#23558;&#20854;&#25512;&#29702;&#26102;&#38388;&#38477;&#20302;&#20102;500&#20493;&#65292;&#20351;&#20854;&#22312;&#956;&#31186;&#32423;&#20915;&#31574;&#26102;&#38388;&#35201;&#27714;&#20869;&#23454;&#29616;&#23454;&#26102;&#25512;&#29702;&#65292;&#19988;&#23545;&#36136;&#37327;&#24433;&#21709;&#19981;&#22823;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#23454;&#26102;&#38598;&#32676;&#20013;&#37096;&#32626;&#20102;&#36716;&#25442;&#21518;&#30340;&#31574;&#30053;&#65292;&#24182;&#19982;&#29616;&#20195;&#25968;&#25454;&#20013;&#24515;&#37096;&#32626;&#30340;&#27969;&#34892;&#25317;&#22622;&#25511;&#21046;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#22312;&#31867;&#20284;&#30340;&#27969;&#37327;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#23558;&#23614;&#37096;&#24310;&#36831;&#29575;&#25552;&#39640;&#20102;x%&#65292;&#23558;&#25968;&#25454;&#21253;&#20002;&#22833;&#29575;&#38477;&#20302;&#20102;y%&#12290;
&lt;/p&gt;
&lt;p&gt;
As communication protocols evolve, datacenter network utilization increases. As a result, congestion is more frequent, causing higher latency and packet loss. Combined with the increasing complexity of workloads, manual design of congestion control (CC) algorithms becomes extremely difficult. This calls for the development of AI approaches to replace the human effort. Unfortunately, it is currently not possible to deploy AI models on network devices due to their limited computational capabilities. Here, we offer a solution to this problem by building a computationally-light solution based on a recent reinforcement learning CC algorithm [arXiv:2207.02295]. We reduce the inference time of RL-CC by x500 by distilling its complex neural network into decision trees. This transformation enables real-time inference within the $\mu$-sec decision-time requirement, with a negligible effect on quality. We deploy the transformed policy on NVIDIA NICs in a live cluster. Compared to popular CC algor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;RevSilo&#65292;&#19968;&#20010;&#23436;&#20840;&#21487;&#36870;&#30340;&#21452;&#21521;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#65292;&#23427;&#32531;&#35299;&#20102;&#31070;&#32463;&#32593;&#32476;&#35268;&#27169;&#21463;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.14098</link><description>&lt;p&gt;
RevBiFPN&#65306;&#23436;&#20840;&#21487;&#36870;&#30340;&#21452;&#21521;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
RevBiFPN: The Fully Reversible Bidirectional Feature Pyramid Network. (arXiv:2206.14098v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RevSilo&#65292;&#19968;&#20010;&#23436;&#20840;&#21487;&#36870;&#30340;&#21452;&#21521;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#65292;&#23427;&#32531;&#35299;&#20102;&#31070;&#32463;&#32593;&#32476;&#35268;&#27169;&#21463;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RevSilo&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21487;&#36870;&#30340;&#21452;&#21521;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#12290;&#19982;&#20854;&#20182;&#21487;&#36870;&#26041;&#27861;&#19968;&#26679;&#65292;RevSilo&#36890;&#36807;&#37325;&#26032;&#35745;&#31639;&#26469;&#28040;&#38500;&#23384;&#20648;&#38544;&#34255;&#28608;&#27963;&#25152;&#38656;&#30340;&#20869;&#23384;&#65307;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#65292;&#22240;&#27492;&#19981;&#33021;&#24212;&#29992;&#20110;&#22823;&#37096;&#20998;&#32593;&#32476;&#12290;&#21452;&#21521;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#20419;&#36827;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#24050;&#25104;&#20026;&#38024;&#23545;&#31354;&#38388;&#25935;&#24863;&#20219;&#21153;&#30340;&#32593;&#32476;&#30340;&#35774;&#35745;&#21407;&#21017;&#12290;&#36825;&#20123;&#32593;&#32476;&#22312;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#36755;&#20837;&#26102;&#65292;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#36825;&#20123;&#32593;&#32476;&#38656;&#35201;&#20445;&#23384;&#22823;&#22411;&#30340;&#22810;&#20998;&#36776;&#29575;&#28608;&#27963;&#25152;&#38656;&#30340;&#22823;&#37327;&#21152;&#36895;&#22120;&#20869;&#23384;&#12290;&#36825;&#20123;&#20869;&#23384;&#38656;&#27714;&#26412;&#36136;&#19978;&#38480;&#21046;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#35268;&#27169;&#65292;&#38480;&#21046;&#20102;&#30001;&#35268;&#27169;&#24102;&#26469;&#30340;&#25913;&#36827;&#12290;&#36328;&#20998;&#36776;&#29575;&#23610;&#24230;&#36816;&#20316;&#30340;RevSilo&#32531;&#35299;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces RevSilo, the first reversible bidirectional multi-scale feature fusion module. Like other reversible methods, RevSilo eliminates the need to store hidden activations by recomputing them. However, existing reversible methods do not apply to multi-scale feature fusion and are, therefore, not applicable to a large class of networks. Bidirectional multi-scale feature fusion promotes local and global coherence and has become a de facto design principle for networks targeting spatially sensitive tasks, e.g., HRNet (Sun et al., 2019a) and EfficientDet (Tan et al., 2020). These networks achieve state-of-the-art results across various computer vision tasks when paired with high-resolution inputs. However, training them requires substantial accelerator memory for saving large, multi-resolution activations. These memory requirements inherently cap the size of neural networks, limiting improvements that come from scale. Operating across resolution scales, RevSilo alleviates th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#36335;&#24452;&#35268;&#21010;&#20219;&#21153;&#20013;&#20351;&#29992;&#23545;&#31216;&#24615;&#25913;&#21892;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#23558;&#20540;&#36845;&#20195;&#35270;&#20026;&#32593;&#26684;&#20449;&#21495;&#65292;&#24182;&#20351;&#29992;&#21487;&#25511;&#21367;&#31215;&#26469;&#34701;&#21512;&#23545;&#31216;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#23545;&#31216;&#35268;&#21010;&#31639;&#27861;&#27604;&#38750;&#31561;&#21464;&#23545;&#24212;&#29289;&#22312;&#35757;&#32451;&#25928;&#29575;&#21644;&#27867;&#21270;&#26041;&#38754;&#26377;&#24456;&#22823;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2206.03674</link><description>&lt;p&gt;
&#23558;&#23545;&#31216;&#24615;&#34701;&#20837;&#21487;&#24494;&#35268;&#21010;&#20013;&#30340;&#21487;&#25511;&#21367;&#31215;
&lt;/p&gt;
&lt;p&gt;
Integrating Symmetry into Differentiable Planning with Steerable Convolutions. (arXiv:2206.03674v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#36335;&#24452;&#35268;&#21010;&#20219;&#21153;&#20013;&#20351;&#29992;&#23545;&#31216;&#24615;&#25913;&#21892;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#23558;&#20540;&#36845;&#20195;&#35270;&#20026;&#32593;&#26684;&#20449;&#21495;&#65292;&#24182;&#20351;&#29992;&#21487;&#25511;&#21367;&#31215;&#26469;&#34701;&#21512;&#23545;&#31216;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#23545;&#31216;&#35268;&#21010;&#31639;&#27861;&#27604;&#38750;&#31561;&#21464;&#23545;&#24212;&#29289;&#22312;&#35757;&#32451;&#25928;&#29575;&#21644;&#27867;&#21270;&#26041;&#38754;&#26377;&#24456;&#22823;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20915;&#31574;&#20219;&#21153;&#20013;&#20986;&#29616;&#23545;&#31216;&#24615;&#26102;&#65292;&#22914;&#20309;&#21033;&#29992;&#32676;&#23545;&#31216;&#24615;&#25913;&#21892;&#31471;&#21040;&#31471;&#21487;&#24494;&#35268;&#21010;&#31639;&#27861;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#21463;&#31561;&#21464;&#21367;&#31215;&#32593;&#32476;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#35270;&#20026;&#32593;&#26684;&#19978;&#30340;&#20449;&#21495;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20540;&#36845;&#20195;&#26159;&#19968;&#20010;&#32447;&#24615;&#31561;&#21464;&#31639;&#23376;&#65292;&#21363;&#21487;&#34987;&#65288;&#23450;&#21521;&#65289;&#21367;&#31215;&#34920;&#31034;&#12290;&#36825;&#25193;&#23637;&#20102;&#20540;&#36845;&#20195;&#32593;&#32476;&#65288;VIN&#65289;&#22312;&#20351;&#29992;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;&#36335;&#24452;&#35268;&#21010;&#26102;&#20351;&#29992;&#39069;&#22806;&#30340;&#26059;&#36716;&#21644;&#21453;&#23556;&#23545;&#31216;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#29616;&#22522;&#20110;VIN&#65292;&#24182;&#20351;&#29992;&#21487;&#25511;&#21367;&#31215;&#32593;&#32476;&#26469;&#34701;&#21512;&#23545;&#31216;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22235;&#20010;&#20219;&#21153;&#30340;&#23454;&#39564;&#65306;2D&#23548;&#33322;&#65292;&#35270;&#35273;&#23548;&#33322;&#65292;&#33258;&#30001;&#24230;&#65288;2DOFs&#65289;&#37197;&#32622;&#31354;&#38388;&#21644;&#24037;&#20316;&#31354;&#38388;&#25805;&#32437;&#12290;&#19982;&#38750;&#31561;&#21464;&#23545;&#24212;&#29289;VIN&#21644;GPPN&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#23545;&#31216;&#35268;&#21010;&#31639;&#27861;&#22312;&#35757;&#32451;&#25928;&#29575;&#21644;&#27867;&#21270;&#26041;&#38754;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study how group symmetry helps improve data efficiency and generalization for end-to-end differentiable planning algorithms when symmetry appears in decision-making tasks. Motivated by equivariant convolution networks, we treat the path planning problem as \textit{signals} over grids. We show that value iteration in this case is a linear equivariant operator, which is a (steerable) convolution. This extends Value Iteration Networks (VINs) on using convolutional networks for path planning with additional rotation and reflection symmetry. Our implementation is based on VINs and uses steerable convolution networks to incorporate symmetry. The experiments are performed on four tasks: 2D navigation, visual navigation, and 2 degrees of freedom (2DOFs) configuration space and workspace manipulation. Our symmetric planning algorithms improve training efficiency and generalization by large margins compared to non-equivariant counterparts, VIN and GPPN.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;HAFH-DDPG&#26469;&#23398;&#20064;&#38548;&#31163;&#24335;&#24494;&#30005;&#32593;&#20013;&#30340;&#32852;&#21512;&#33021;&#37327;&#20998;&#37197;&#21644;&#26426;&#32452;&#24320;&#21551;&#20915;&#31574;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26612;&#27833;&#21457;&#30005;&#26426;&#36873;&#25321;&#31574;&#30053;&#65292;&#20197;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2206.01663</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#24494;&#30005;&#32593;&#32852;&#21512;&#33021;&#37327;&#20998;&#37197;&#21644;&#26426;&#32452;&#24320;&#21551;
&lt;/p&gt;
&lt;p&gt;
Joint Energy Dispatch and Unit Commitment in Microgrids Based on Deep Reinforcement Learning. (arXiv:2206.01663v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;HAFH-DDPG&#26469;&#23398;&#20064;&#38548;&#31163;&#24335;&#24494;&#30005;&#32593;&#20013;&#30340;&#32852;&#21512;&#33021;&#37327;&#20998;&#37197;&#21644;&#26426;&#32452;&#24320;&#21551;&#20915;&#31574;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26612;&#27833;&#21457;&#30005;&#26426;&#36873;&#25321;&#31574;&#30053;&#65292;&#20197;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#24212;&#29992;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#24494;&#30005;&#32593;&#65288;MG&#65289;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#36825;&#21019;&#36896;&#20102;&#23545;&#21160;&#24577;&#33021;&#28304;&#31649;&#29702;&#30340;&#24378;&#28872;&#38656;&#27714;&#12290;&#26412;&#25991;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26469;&#23398;&#20064;&#38548;&#31163;&#24335;MG&#20013;&#32852;&#21512;&#33021;&#37327;&#20998;&#37197;&#65288;ED&#65289;&#21644;&#26426;&#32452;&#24320;&#21551;&#65288;UC&#65289;&#20915;&#31574;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#20197;&#22312;&#30830;&#20445;&#20379;&#38656;&#24179;&#34913;&#30340;&#21069;&#25552;&#19979;&#38477;&#20302;&#24635;&#30005;&#21147;&#25104;&#26412;&#12290;&#20026;&#20102;&#20811;&#26381;&#30001;&#20110;&#32852;&#21512;ED&#21644;UC&#32780;&#23548;&#33268;&#30340;&#31163;&#25955;-&#36830;&#32493;&#28151;&#21512;&#34892;&#21160;&#31354;&#38388;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;DRL&#31639;&#27861;&#65292;&#21363;&#28151;&#21512;&#34892;&#21160;&#26377;&#38480;&#22320;&#24179;&#32447;DDPG&#65288;HAFH-DDPG&#65289;&#65292;&#23427;&#22522;&#20110;&#26377;&#38480;&#22320;&#24179;&#32447;&#21160;&#24577;&#35268;&#21010;&#65288;DP&#65289;&#26694;&#26550;&#65292;&#26080;&#32541;&#22320;&#25972;&#21512;&#20102;&#20004;&#31181;&#32463;&#20856;DRL&#31639;&#27861;&#65292;&#21363;&#28145;&#24230;Q&#32593;&#32476;&#65288;DQN&#65289;&#21644;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;DDPG&#65289;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26612;&#27833;&#21457;&#30005;&#26426;&#65288;DG&#65289;&#36873;&#25321;&#31574;&#30053;&#65292;&#20197;&#25903;&#25345;&#31616;&#21270;&#34892;&#21160;&#31354;&#38388;&#65292;&#20197;&#38477;&#20302;&#27492;&#31639;&#27861;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, the application of microgrids (MG) with renewable energy is becoming more and more extensive, which creates a strong need for dynamic energy management. In this paper, deep reinforcement learning (DRL) is applied to learn an optimal policy for making joint energy dispatch (ED) and unit commitment (UC) decisions in an isolated MG, with the aim for reducing the total power generation cost on the premise of ensuring the supply-demand balance. In order to overcome the challenge of discrete-continuous hybrid action space due to joint ED and UC, we propose a DRL algorithm, i.e., the hybrid action finite-horizon DDPG (HAFH-DDPG), that seamlessly integrates two classical DRL algorithms, i.e., deep Q-network (DQN) and deep deterministic policy gradient (DDPG), based on a finite-horizon dynamic programming (DP) framework. Moreover, a diesel generator (DG) selection strategy is presented to support a simplified action space for reducing the computation complexity of this algorithm. Fina
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#35752;&#35770;&#20102;&#26426;&#22120;&#35299;&#37322;&#21644;&#20154;&#31867;&#29702;&#35299;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#30830;&#23450;&#20102;&#19977;&#20010;&#26680;&#24515;&#27010;&#24565;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#27809;&#26377;&#20851;&#20110;&#29305;&#23450;&#20219;&#21153;&#30452;&#35273;&#30340;&#20551;&#35774;&#19979;&#65292;&#35299;&#37322;&#21487;&#25552;&#39640;&#20154;&#31867;&#23545;&#27169;&#22411;&#20915;&#31574;&#36793;&#30028;&#30340;&#29702;&#35299;&#65292;&#20294;&#23545;&#20219;&#21153;&#20915;&#31574;&#36793;&#30028;&#21644;&#27169;&#22411;&#38169;&#35823;&#21017;&#27809;&#26377;&#20805;&#20998;&#35777;&#25454;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2202.04092</link><description>&lt;p&gt;
&#26426;&#22120;&#35299;&#37322;&#21644;&#20154;&#31867;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Machine Explanations and Human Understanding. (arXiv:2202.04092v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.04092
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35752;&#35770;&#20102;&#26426;&#22120;&#35299;&#37322;&#21644;&#20154;&#31867;&#29702;&#35299;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#30830;&#23450;&#20102;&#19977;&#20010;&#26680;&#24515;&#27010;&#24565;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#27809;&#26377;&#20851;&#20110;&#29305;&#23450;&#20219;&#21153;&#30452;&#35273;&#30340;&#20551;&#35774;&#19979;&#65292;&#35299;&#37322;&#21487;&#25552;&#39640;&#20154;&#31867;&#23545;&#27169;&#22411;&#20915;&#31574;&#36793;&#30028;&#30340;&#29702;&#35299;&#65292;&#20294;&#23545;&#20219;&#21153;&#20915;&#31574;&#36793;&#30028;&#21644;&#27169;&#22411;&#38169;&#35823;&#21017;&#27809;&#26377;&#20805;&#20998;&#35777;&#25454;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#34987;&#20551;&#35774;&#21487;&#20197;&#25552;&#39640;&#20154;&#31867;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29702;&#35299;&#65292;&#24182;&#23454;&#29616;&#21508;&#31181;&#26377;&#30410;&#30340;&#32467;&#26524;&#65292;&#20174;&#27169;&#22411;&#35843;&#35797;&#21040;&#22686;&#24378;&#20154;&#31867;&#20915;&#31574;&#21046;&#23450;&#12290;&#28982;&#32780;&#65292;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#20102;&#19981;&#19968;&#33268;&#29978;&#33267;&#36127;&#38754;&#30340;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#26159;&#22312;&#20160;&#20040;&#26465;&#20214;&#19979;&#35299;&#37322;&#21487;&#20197;&#25552;&#39640;&#20154;&#31867;&#30340;&#29702;&#35299;&#65292;&#24182;&#20197;&#20309;&#31181;&#26041;&#24335;&#12290;&#20351;&#29992;&#25913;&#36827;&#30340;&#22240;&#26524;&#22270;&#34920;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26426;&#22120;&#35299;&#37322;&#21644;&#20154;&#31867;&#29702;&#35299;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#27491;&#24335;&#29305;&#24449;&#21270;&#65292;&#24182;&#23637;&#31034;&#20102;&#20154;&#31867;&#30452;&#35273;&#22312;&#21551;&#29992;&#20154;&#31867;&#29702;&#35299;&#20013;&#21457;&#25381;&#20102;&#26680;&#24515;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19977;&#20010;&#26680;&#24515;&#27010;&#24565;&#65292;&#28085;&#30422;&#20102;&#25152;&#26377;&#29616;&#26377;&#37327;&#21270;&#29702;&#35299;&#30340;&#25514;&#26045;&#65292;&#21363;&#22312;&#20154;&#31867;&#19982;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#21046;&#23450;&#30340;&#32972;&#26223;&#19979;&#30340;&#20219;&#21153;&#20915;&#31574;&#36793;&#30028;&#12289;&#27169;&#22411;&#20915;&#31574;&#36793;&#30028;&#21644;&#27169;&#22411;&#38169;&#35823;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#32467;&#26524;&#26159;&#65292;&#22914;&#26524;&#27809;&#26377;&#20851;&#20110;&#29305;&#23450;&#20219;&#21153;&#30452;&#35273;&#30340;&#20551;&#35774;&#65292;&#35299;&#37322;&#21487;&#33021;&#20250;&#28508;&#22312;&#22320;&#25552;&#39640;&#20154;&#31867;&#23545;&#27169;&#22411;&#20915;&#31574;&#36793;&#30028;&#30340;&#29702;&#35299;&#65292;&#20294;&#23545;&#20110;&#20219;&#21153;&#20915;&#31574;&#36793;&#30028;&#21644;&#27169;&#22411;&#38169;&#35823;&#65292;&#21017;&#27809;&#26377;&#20805;&#20998;&#30340;&#35777;&#25454;&#34920;&#26126;&#35299;&#37322;&#21487;&#20197;&#25552;&#39640;&#20154;&#31867;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explanations are hypothesized to improve human understanding of machine learning models and achieve a variety of desirable outcomes, ranging from model debugging to enhancing human decision making. However, empirical studies have found mixed and even negative results. An open question, therefore, is under what conditions explanations can improve human understanding and in what way. Using adapted causal diagrams, we provide a formal characterization of the interplay between machine explanations and human understanding, and show how human intuitions play a central role in enabling human understanding. Specifically, we identify three core concepts of interest that cover all existing quantitative measures of understanding in the context of human-AI decision making: task decision boundary, model decision boundary, and model error. Our key result is that without assumptions about task-specific intuitions, explanations may potentially improve human understanding of model decision boundary, bu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#24378;&#21270;&#23398;&#20064;&#21644;&#26641;&#25628;&#32034;&#30340;&#26032;&#26041;&#27861;&#65292;&#21517;&#20026;TreeMesh&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#24179;&#38754;&#22235;&#36793;&#24418;&#32593;&#26684;&#65292;&#21487;&#20197;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#24555;&#22320;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2111.07613</link><description>&lt;p&gt;
&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#26641;&#25628;&#32034;&#29983;&#25104;&#24179;&#38754;&#22235;&#36793;&#24418;&#32593;&#26684;
&lt;/p&gt;
&lt;p&gt;
Generate plane quad mesh with neural networks and tree search. (arXiv:2111.07613v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.07613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#24378;&#21270;&#23398;&#20064;&#21644;&#26641;&#25628;&#32034;&#30340;&#26032;&#26041;&#27861;&#65292;&#21517;&#20026;TreeMesh&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#24179;&#38754;&#22235;&#36793;&#24418;&#32593;&#26684;&#65292;&#21487;&#20197;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#24555;&#22320;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26377;&#38480;&#20803;&#26041;&#27861;&#65288;FEM&#65289;&#30340;&#21382;&#21490;&#19978;&#65292;&#32593;&#26684;&#29983;&#25104;&#30340;&#36136;&#37327;&#19968;&#30452;&#34987;&#35748;&#20026;&#26159;&#20026;&#24037;&#31243;&#24072;&#25552;&#20379;&#21487;&#38752;&#20223;&#30495;&#32467;&#26524;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#30446;&#21069;&#26368;&#20581;&#22766;&#30340;&#20803;&#32032;&#25552;&#21462;&#26041;&#27861;&#26159;&#37319;&#29992;&#23547;&#25214;&#20248;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#19979;&#19968;&#20010;&#20803;&#32032;&#30340;&#26041;&#27861;&#26469;&#21152;&#24555;&#25552;&#21462;&#36895;&#24230;&#65292;&#20294;&#36825;&#21487;&#33021;&#23548;&#33268;&#32463;&#36807;&#22810;&#27425;&#36845;&#20195;&#21518;&#23616;&#37096;&#32593;&#26684;&#36136;&#37327;&#36739;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;TreeMesh&#65292;&#35813;&#26041;&#27861;&#23558;&#36825;&#31181;&#26041;&#27861;&#19982;&#24378;&#21270;&#23398;&#20064;&#65288;&#20063;&#21487;&#33021;&#26159;&#30417;&#30563;&#23398;&#20064;&#65289;&#21644;&#19968;&#31181;&#26032;&#30340;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#30456;&#32467;&#21512;&#12290;&#35813;&#31639;&#27861;&#22522;&#20110;&#20808;&#21069;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#32463;&#36807;&#22810;&#27425;&#25913;&#36827;&#21518;&#65292;&#22312;&#30456;&#21516;&#30340;&#36793;&#30028;&#19978;&#24615;&#33021;&#20248;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#26641;&#25628;&#32034;&#65292;&#25105;&#20204;&#30340;&#31243;&#24207;&#21487;&#20197;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#24555;&#22320;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#24179;&#38754;&#22235;&#36793;&#24418;&#32593;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quality of mesh generation has long been considered a vital aspect in providing engineers with reliable simulation results throughout the history of the Finite Element Method (FEM). The element extraction method, which is currently the most robust method, is used in business software. However, in order to speed up extraction, the approach is done by finding the next element that optimizes a target function, which can result in local mesh of bad quality after many time steps. We provide TreeMesh, a method that uses this method in conjunction with reinforcement learning (also possible with supervised learning) and a novel Monte-Carlo tree search (MCTS) (Coulom(2006), Kocsis and Szepesv\'ari(2006), Browne et~al.(2012)). The algorithm is based on a previously proposed approach (Pan et~al.(2021)). After making many improvements on DRL (algorithm, state-action-reward setting) and adding a MCTS, it outperforms the former work on the same boundary. Furthermore, using tree search, our progr
&lt;/p&gt;</description></item><item><title>SelfCF&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#21327;&#21516;&#36807;&#28388;&#26694;&#26550;&#65292;&#29992;&#20110;&#25512;&#33616;&#22330;&#26223;&#65292;&#36890;&#36807;&#22686;&#24378;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;&#20013;&#36755;&#20986;&#30340;&#23884;&#20837;&#26469;&#31616;&#21270;&#31639;&#27861;&#20197;&#21450;&#36991;&#20813;&#26114;&#36149;&#30340;&#35745;&#31639;&#21644;&#28508;&#22312;&#30340;&#36127;&#26679;&#26412;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2107.03019</link><description>&lt;p&gt;
SelfCF&#65306;&#19968;&#31181;&#31616;&#21333;&#30340;&#33258;&#30417;&#30563;&#21327;&#21516;&#36807;&#28388;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SelfCF: A Simple Framework for Self-supervised Collaborative Filtering. (arXiv:2107.03019v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.03019
&lt;/p&gt;
&lt;p&gt;
SelfCF&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#21327;&#21516;&#36807;&#28388;&#26694;&#26550;&#65292;&#29992;&#20110;&#25512;&#33616;&#22330;&#26223;&#65292;&#36890;&#36807;&#22686;&#24378;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;&#20013;&#36755;&#20986;&#30340;&#23884;&#20837;&#26469;&#31616;&#21270;&#31639;&#27861;&#20197;&#21450;&#36991;&#20813;&#26114;&#36149;&#30340;&#35745;&#31639;&#21644;&#28508;&#22312;&#30340;&#36127;&#26679;&#26412;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#36807;&#28388;&#65288;CF&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#20174;&#35266;&#23519;&#21040;&#30340;&#20132;&#20114;&#20013;&#23398;&#20064;&#26377;&#29992;&#30340;&#29992;&#25143;&#21644;&#39033;&#30446;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;CF&#30340;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#36127;&#37319;&#26679;&#26469;&#21306;&#20998;&#19981;&#21516;&#30340;&#39033;&#30446;&#12290;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#36127;&#37319;&#26679;&#36827;&#34892;&#35757;&#32451;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#27492;&#22806;&#65292;&#24517;&#39035;&#26681;&#25454;&#23450;&#20041;&#30340;&#20998;&#24067;&#35880;&#24910;&#36873;&#25321;&#36127;&#39033;&#65292;&#20197;&#36991;&#20813;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#35266;&#23519;&#21040;&#30340;&#27491;&#39033;&#12290;&#19981;&#21487;&#36991;&#20813;&#22320;&#65292;&#20174;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#37319;&#26679;&#30340;&#19968;&#20123;&#36127;&#39033;&#22312;&#27979;&#35797;&#38598;&#20013;&#21487;&#33021;&#26159;&#27491;&#39033;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#38544;&#24335;&#21453;&#39304;&#25512;&#33616;&#22330;&#26223;&#30340;&#33258;&#30417;&#30563;&#21327;&#21516;&#36807;&#28388;&#26694;&#26550;&#65288;SelfCF&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;SelfCF&#26694;&#26550;&#31616;&#21270;&#20102;&#36830;&#20307;&#32593;&#32476;&#65292;&#24182;&#21487;&#36731;&#26494;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;CF&#27169;&#22411;&#65292;&#25105;&#20204;&#31216;&#20854;&#20026;&#39592;&#24178;&#32593;&#32476;&#12290;SelfCF&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#22686;&#24378;&#30001;&#39592;&#24178;&#32593;&#32476;&#29983;&#25104;&#30340;&#36755;&#20986;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative filtering (CF) is widely used to learn informative latent representations of users and items from observed interactions. Existing CF-based methods commonly adopt negative sampling to discriminate different items. Training with negative sampling on large datasets is computationally expensive. Further, negative items should be carefully sampled under the defined distribution, in order to avoid selecting an observed positive item in the training dataset. Unavoidably, some negative items sampled from the training dataset could be positive in the test set. In this paper, we propose a self-supervised collaborative filtering framework (SelfCF), that is specially designed for recommender scenario with implicit feedback. The proposed SelfCF framework simplifies the Siamese networks and can be easily applied to existing deep-learning based CF models, which we refer to as backbone networks. The main idea of SelfCF is to augment the output embeddings generated by backbone networks, b
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26679;&#24615;&#20445;&#25345;&#30340;&#22270;&#32467;&#26500;&#32454;&#21270;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#24050;&#23398;&#20064;&#30340;&#33410;&#28857;&#34920;&#31034;&#26469;&#36880;&#27493;&#25913;&#21892;&#22270;&#24418;&#32467;&#26500;&#36136;&#37327;&#12290;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#65292;&#21253;&#25324;&#33410;&#28857;&#20998;&#31867;&#12289;&#38142;&#25509;&#39044;&#27979;&#21644;&#22270;&#32858;&#31867;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2103.07295</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#26679;&#24615;&#20445;&#25345;&#30340;&#22270;&#32467;&#26500;&#32454;&#21270;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Representation Learning via Diversity-preserving Graph Refinement. (arXiv:2103.07295v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.07295
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26679;&#24615;&#20445;&#25345;&#30340;&#22270;&#32467;&#26500;&#32454;&#21270;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#24050;&#23398;&#20064;&#30340;&#33410;&#28857;&#34920;&#31034;&#26469;&#36880;&#27493;&#25913;&#21892;&#22270;&#24418;&#32467;&#26500;&#36136;&#37327;&#12290;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#65292;&#21253;&#25324;&#33410;&#28857;&#20998;&#31867;&#12289;&#38142;&#25509;&#39044;&#27979;&#21644;&#22270;&#32858;&#31867;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#30495;&#23454;&#30340;&#22270;&#25968;&#25454;&#65292;&#33410;&#28857;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#36890;&#24120;&#34987;&#34920;&#31034;&#20026;&#30828;&#24615;&#20108;&#36827;&#21046;&#38142;&#25509;&#12290;&#26174;&#28982;&#65292;&#36825;&#26159;&#19968;&#31181;&#31163;&#25955;&#21644;&#31616;&#21270;&#30340;&#36830;&#32493;&#20851;&#31995;&#24418;&#24335;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;&#23398;&#20064;&#21040;&#30340;&#33410;&#28857;&#34920;&#31034;&#30340;&#21487;&#34920;&#36798;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23884;&#20837;&#31354;&#38388;&#20013;&#33719;&#24471;&#30340;&#33410;&#28857;&#34920;&#31034;&#21487;&#20197;&#21453;&#36807;&#26469;&#25581;&#31034;&#33410;&#28857;&#20043;&#38388;&#30340;&#20869;&#22312;&#20851;&#31995;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29305;&#24449;&#21270;&#33410;&#28857;&#20851;&#31995;&#24182;&#36827;&#19968;&#27493;&#20419;&#36827;&#33410;&#28857;&#34920;&#31034;&#30340;&#23398;&#20064;&#65292;&#19968;&#31181;&#30452;&#35266;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#23884;&#20837;&#30340;&#33410;&#28857;&#34920;&#31034;&#26469;&#32454;&#21270;&#26368;&#21021;&#32473;&#23450;&#30340;&#22270;&#32467;&#26500;&#12290;&#20294;&#26159;&#65292;&#20840;&#23616;&#32454;&#21270;&#25152;&#26377;&#33410;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#26080;&#27861;&#21306;&#20998;&#23558;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#19968;&#20123;&#22122;&#22768;&#36793;&#32536;&#65292;&#36825;&#21487;&#33021;&#36827;&#19968;&#27493;&#28151;&#28102;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#22823;&#22411;&#22270;&#24418;&#19978;&#20063;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23616;&#37096;&#32467;&#26500;&#24863;&#30693;&#30340;&#22270;&#24418;&#32454;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#24050;&#32463;&#23398;&#21040;&#30340;&#33410;&#28857;&#34920;&#31034;&#36880;&#27493;&#25913;&#21892;&#22270;&#24418;&#32467;&#26500;&#36136;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23545;&#22270;&#20013;&#30340;&#38543;&#26426;&#28216;&#36208;&#27169;&#25311;&#29983;&#25104;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#37051;&#22495;&#32467;&#26500;&#38598;&#12290;&#28982;&#21518;&#65292;&#23545;&#20110;&#27599;&#20010;&#27169;&#25311;&#37051;&#22495;&#65292;&#25105;&#20204;&#22312;&#25972;&#20010;&#32454;&#21270;&#36807;&#31243;&#20013;&#20445;&#25345;&#37051;&#22495;&#32467;&#26500;&#30340;&#22810;&#26679;&#24615;&#65292;&#21516;&#26102;&#32454;&#21270;&#37051;&#22495;&#20869;&#30340;&#33410;&#28857;&#20851;&#31995;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#35780;&#20272;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#65292;&#21253;&#25324;&#33410;&#28857;&#20998;&#31867;&#12289;&#38142;&#25509;&#39044;&#27979;&#21644;&#22270;&#32858;&#31867;&#65292;&#22343;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
For real-world graph data, the complex relationship between nodes is often represented as a hard binary link. Obviously, it is a discrete and simplified form of continuous relationship between nodes, which seriously limits the expressibility of the learned node representation. On the other hand, the node representation obtained in the embedding space can in turn be used to reveal the intrinsic relationship between nodes. To better characterize the node relationships and further facilitate the learning of node representation, an intuitive way is to refine the originally given graph structure with the embedded node representations. However, such global refinement of the relationships among all nodes without distinction will inevitably lead to some noisy edges, which may further confuse the training of the node representation learning model. In addition, it also has scalability problems on large graphs. To address these issues, we propose a local structure aware graph refinement to progre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;10&#20010;&#26368;&#36817;&#25552;&#20986;&#30340;&#33021;&#37327;&#23432;&#24658;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#27604;&#36739;&#20102;&#23427;&#20204;&#22312;4&#20010;&#29289;&#29702;&#31995;&#32479;&#20013;&#30340;&#34920;&#29616;&#65292;&#35828;&#26126;&#20102;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#35774;&#35745;&#22522;&#20110;&#33021;&#37327;&#30340;&#25511;&#21046;&#22120;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2012.02334</link><description>&lt;p&gt;
&#22522;&#20934;&#33021;&#37327;&#23432;&#24658;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#23398;&#20064;&#21160;&#21147;&#23398;&#25968;&#25454;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Energy-Conserving Neural Networks for Learning Dynamics from Data. (arXiv:2012.02334v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.02334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;10&#20010;&#26368;&#36817;&#25552;&#20986;&#30340;&#33021;&#37327;&#23432;&#24658;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#27604;&#36739;&#20102;&#23427;&#20204;&#22312;4&#20010;&#29289;&#29702;&#31995;&#32479;&#20013;&#30340;&#34920;&#29616;&#65292;&#35828;&#26126;&#20102;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#35774;&#35745;&#22522;&#20110;&#33021;&#37327;&#30340;&#25511;&#21046;&#22120;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23558;&#29289;&#29702;&#30693;&#35782;&#20316;&#20026;&#24402;&#32435;&#20559;&#32622;&#24341;&#20837;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29305;&#21035;&#26159;&#22312;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#20174;&#35266;&#27979;&#21040;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#23398;&#20064;&#21160;&#21147;&#23398;&#26041;&#31243;&#26041;&#38754;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#21253;&#25324;HNN&#12289;LNN&#12289;DeLaN&#12289;SymODEN&#12289;CHNN&#12289;CLNN&#21450;&#20854;&#21464;&#20307;&#22312;&#20869;&#30340;10&#20010;&#26368;&#36817;&#25552;&#20986;&#30340;&#33021;&#37327;&#23432;&#24658;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#29702;&#35770;&#31616;&#27905;&#28436;&#32462;&#65292;&#24182;&#35299;&#37322;&#20102;&#23427;&#20204;&#30340;&#30456;&#20284;&#20043;&#22788;&#21644;&#24046;&#24322;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;4&#20010;&#29289;&#29702;&#31995;&#32479;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25351;&#20986;&#20102;&#21033;&#29992;&#36825;&#20123;&#33021;&#37327;&#23432;&#24658;&#27169;&#22411;&#35774;&#35745;&#22522;&#20110;&#33021;&#37327;&#30340;&#25511;&#21046;&#22120;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The last few years have witnessed an increased interest in incorporating physics-informed inductive bias in deep learning frameworks. In particular, a growing volume of literature has been exploring ways to enforce energy conservation while using neural networks for learning dynamics from observed time-series data. In this work, we survey ten recently proposed energy-conserving neural network models, including HNN, LNN, DeLaN, SymODEN, CHNN, CLNN and their variants. We provide a compact derivation of the theory behind these models and explain their similarities and differences. Their performance are compared in 4 physical systems. We point out the possibility of leveraging some of these energy-conserving models to design energy-based controllers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;Anthem&#21644;Vampire&#20004;&#20010;&#36719;&#20214;&#24037;&#20855;&#65292;&#39564;&#35777;&#20102;&#20855;&#26377;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#31243;&#24207;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#35813;&#19978;&#19979;&#25991;&#20013;&#31283;&#23450;&#27169;&#22411;&#21644;&#34917;&#23436;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2008.02025</link><description>&lt;p&gt;
&#20351;&#29992;Anthem&#21644;Vampire&#39564;&#35777;&#32039;&#20945;&#36923;&#36753;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Verifying Tight Logic Programs with anthem and Vampire. (arXiv:2008.02025v6 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2008.02025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;Anthem&#21644;Vampire&#20004;&#20010;&#36719;&#20214;&#24037;&#20855;&#65292;&#39564;&#35777;&#20102;&#20855;&#26377;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#31243;&#24207;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#35813;&#19978;&#19979;&#25991;&#20013;&#31283;&#23450;&#27169;&#22411;&#21644;&#34917;&#23436;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32487;&#32493;&#30740;&#31350;&#36923;&#36753;&#31243;&#24207;&#21644;&#19968;&#38454;&#29702;&#35770;&#20043;&#38388;&#20851;&#31995;&#30340;&#19968;&#31995;&#21015;&#30740;&#31350;&#12290;&#25105;&#20204;&#23558;&#31243;&#24207;&#34917;&#23436;&#30340;&#23450;&#20041;&#25193;&#23637;&#21040;&#36755;&#20837;&#21644;&#36755;&#20986;&#22312;ASP grounding &#24037;&#20855;gringo&#30340;&#36755;&#20837;&#35821;&#35328;&#23376;&#38598;&#20013;&#30340;&#31243;&#24207;&#65292;&#24182;&#30740;&#31350;&#20102;&#35813;&#19978;&#19979;&#25991;&#20013;&#31283;&#23450;&#27169;&#22411;&#21644;&#34917;&#23436;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25551;&#36848;&#20102;&#20351;&#29992;&#20004;&#20010;&#36719;&#20214;&#24037;&#20855;Anthem&#21644;Vampire&#39564;&#35777;&#20855;&#26377;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#31243;&#24207;&#30340;&#27491;&#30830;&#24615;&#30340;&#21021;&#27493;&#23454;&#39564;&#12290;&#35813;&#23450;&#29702;&#30340;&#35777;&#26126;&#22522;&#20110;&#24341;&#29702;&#65292;&#35813;&#24341;&#29702;&#23558;&#26412;&#25991;&#30740;&#31350;&#30340;&#31243;&#24207;&#35821;&#20041;&#19982;&#19968;&#38454;&#20844;&#24335;&#30340;&#31283;&#23450;&#27169;&#22411;&#30456;&#20851;&#32852;&#12290;&#27491;&#22312;TLP&#25509;&#21463;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper continues the line of research aimed at investigating the relationship between logic programs and first-order theories. We extend the definition of program completion to programs with input and output in a subset of the input language of the ASP grounder gringo, study the relationship between stable models and completion in this context, and describe preliminary experiments with the use of two software tools, anthem and vampire, for verifying the correctness of programs with input and output. Proofs of theorems are based on a lemma that relates the semantics of programs studied in this paper to stable models of first-order formulas. Under consideration for acceptance in TPLP.
&lt;/p&gt;</description></item></channel></rss>