<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36229;&#36234;&#20256;&#32479;&#24037;&#20855;&#30340;&#39640;&#32423;&#33521;&#35793;&#38463;&#25289;&#20271;&#35821;&#32763;&#35793;&#22120;&#65292;&#20351;&#29992;&#36203;&#23572;&#36763;&#22522;&#21464;&#21387;&#22120;&#21644;&#32431;&#25991;&#23398;&#38463;&#25289;&#20271;&#35821;&#25968;&#25454;&#38598;&#65292;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#24378;&#35843;&#20102;&#20854;&#22312;&#25991;&#21270;&#25935;&#24863;&#24615;&#21644;&#35821;&#22659;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.13613</link><description>&lt;p&gt;
Hunayn&#65306;&#36229;&#36234;&#23383;&#38754;&#24847;&#20041;&#30340;&#32763;&#35793;&#36827;&#27493;
&lt;/p&gt;
&lt;p&gt;
Hunayn: Elevating Translation Beyond the Literal. (arXiv:2310.13613v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13613
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36229;&#36234;&#20256;&#32479;&#24037;&#20855;&#30340;&#39640;&#32423;&#33521;&#35793;&#38463;&#25289;&#20271;&#35821;&#32763;&#35793;&#22120;&#65292;&#20351;&#29992;&#36203;&#23572;&#36763;&#22522;&#21464;&#21387;&#22120;&#21644;&#32431;&#25991;&#23398;&#38463;&#25289;&#20271;&#35821;&#25968;&#25454;&#38598;&#65292;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#24378;&#35843;&#20102;&#20854;&#22312;&#25991;&#21270;&#25935;&#24863;&#24615;&#21644;&#35821;&#22659;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#39033;&#30446;&#20171;&#32461;&#20102;&#19968;&#31181;&#36229;&#36234;&#20256;&#32479;&#24037;&#20855;&#30340;&#39640;&#32423;&#33521;&#35793;&#38463;&#25289;&#20271;&#35821;&#32763;&#35793;&#22120;&#12290;&#21033;&#29992;&#36203;&#23572;&#36763;&#22522;&#21464;&#21387;&#22120;&#65288;MarianMT&#65289;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#22312;&#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#30340;&#12289;&#32431;&#25991;&#23398;&#38463;&#25289;&#20271;&#35821;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#19982;&#35895;&#27468;&#32763;&#35793;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;&#23450;&#24615;&#35780;&#20272;&#20013;&#22987;&#32456;&#34920;&#29616;&#20986;&#33394;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23427;&#22312;&#25991;&#21270;&#25935;&#24863;&#24615;&#21644;&#35821;&#22659;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#39033;&#30740;&#31350;&#24378;&#35843;&#20102;&#36203;&#23572;&#36763;&#22522;&#21464;&#21387;&#22120;&#22312;&#20351;&#29992;Fusha&#25968;&#25454;&#38598;&#30340;&#33521;&#38463;&#32763;&#35793;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This project introduces an advanced English-to-Arabic translator surpassing conventional tools. Leveraging the Helsinki transformer (MarianMT), our approach involves fine-tuning on a self-scraped, purely literary Arabic dataset. Evaluations against Google Translate show consistent outperformance in qualitative assessments. Notably, it excels in cultural sensitivity and context accuracy. This research underscores the Helsinki transformer's superiority for English-to-Arabic translation using a Fusha dataset.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#33258;&#25105;&#24402;&#22240;&#21644;&#20915;&#31574;&#21046;&#23450;&#30340;&#32479;&#19968;&#20004;&#38454;&#27573;&#26694;&#26550;&#65288;SADM&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#36755;&#20837;&#25991;&#26412;&#30340;&#23376;&#24207;&#21015;&#20316;&#20026;&#35299;&#37322;&#26469;&#24314;&#31435;&#26356;&#21487;&#38752;&#30340;&#27169;&#22411;&#20915;&#31574;&#35299;&#37322;&#32852;&#31995;&#65292;&#24182;&#22312;ERASER&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13610</link><description>&lt;p&gt;
&#20351;&#24744;&#30340;&#20915;&#31574;&#26377;&#35828;&#26381;&#21147;&#65281;&#19968;&#20010;&#32479;&#19968;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65306;&#33258;&#25105;&#24402;&#22240;&#21644;&#20915;&#31574;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Make Your Decision Convincing! A Unified Two-Stage Framework: Self-Attribution and Decision-Making. (arXiv:2310.13610v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13610
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#33258;&#25105;&#24402;&#22240;&#21644;&#20915;&#31574;&#21046;&#23450;&#30340;&#32479;&#19968;&#20004;&#38454;&#27573;&#26694;&#26550;&#65288;SADM&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#36755;&#20837;&#25991;&#26412;&#30340;&#23376;&#24207;&#21015;&#20316;&#20026;&#35299;&#37322;&#26469;&#24314;&#31435;&#26356;&#21487;&#38752;&#30340;&#27169;&#22411;&#20915;&#31574;&#35299;&#37322;&#32852;&#31995;&#65292;&#24182;&#22312;ERASER&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#29992;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#30340;&#34892;&#20026;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#36755;&#20837;&#25991;&#26412;&#30340;&#23376;&#24207;&#21015;&#20316;&#20026;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#25903;&#25345;&#27169;&#22411;&#20915;&#31574;&#30340;&#35777;&#25454;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#26694;&#26550;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#35299;&#37322;&#30340;&#21516;&#26102;&#21462;&#24471;&#20102;&#39640;&#20219;&#21153;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#24573;&#30053;&#20102;&#29983;&#25104;&#30340;&#35299;&#37322;&#19982;&#27169;&#22411;&#20915;&#31574;&#20043;&#38388;&#30340;&#19981;&#21487;&#38752;&#32852;&#31995;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#27169;&#22411;&#21487;&#33021;&#22312;&#24402;&#22240;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#20570;&#20986;&#27491;&#30830;&#30340;&#20915;&#31574;&#65292;&#25110;&#32773;&#22312;&#24402;&#22240;&#27491;&#30830;&#30340;&#24773;&#20917;&#19979;&#20570;&#20986;&#31967;&#31957;&#30340;&#20915;&#31574;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#33258;&#25105;&#24402;&#22240;&#21644;&#20915;&#31574;&#21046;&#23450;&#30340;&#32479;&#19968;&#20004;&#38454;&#27573;&#26694;&#26550;&#65288;SADM&#65289;&#12290;&#36890;&#36807;&#23545;ERASER&#22522;&#20934;&#27979;&#35797;&#30340;&#20116;&#20010;&#25512;&#29702;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#19981;&#20165;&#33021;&#24314;&#31435;&#29983;&#25104;&#35299;&#37322;&#21644;&#27169;&#22411;&#20915;&#31574;&#20043;&#38388;&#26356;&#21487;&#38752;&#30340;&#32852;&#31995;&#65292;&#32780;&#19988;&#36824;&#33021;&#21462;&#24471;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explaining black-box model behavior with natural language has achieved impressive results in various NLP tasks. Recent research has explored the utilization of subsequences from the input text as a rationale, providing users with evidence to support the model decision. Although existing frameworks excel in generating high-quality rationales while achieving high task performance, they neglect to account for the unreliable link between the generated rationale and model decision. In simpler terms, a model may make correct decisions while attributing wrong rationales, or make poor decisions while attributing correct rationales. To mitigate this issue, we propose a unified two-stage framework known as Self-Attribution and Decision-Making (SADM). Through extensive experiments on five reasoning datasets from the ERASER benchmark, we demonstrate that our framework not only establishes a more reliable link between the generated rationale and model decision but also achieves competitive results 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;MULTITuDE&#65292;&#19968;&#20010;&#38024;&#23545;&#22810;&#35821;&#35328;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#26816;&#27979;&#22120;&#22312;&#38646;&#26679;&#26412;&#21644;&#24494;&#35843;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#30740;&#31350;&#20102;&#36825;&#20123;&#26816;&#27979;&#22120;&#23545;&#20110;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#21644;&#29983;&#25104;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#22810;&#35821;&#35328;&#35757;&#32451;&#19979;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.13606</link><description>&lt;p&gt;
MULTITuDE: &#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MULTITuDE: Large-Scale Multilingual Machine-Generated Text Detection Benchmark. (arXiv:2310.13606v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13606
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;MULTITuDE&#65292;&#19968;&#20010;&#38024;&#23545;&#22810;&#35821;&#35328;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#26816;&#27979;&#22120;&#22312;&#38646;&#26679;&#26412;&#21644;&#24494;&#35843;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#30740;&#31350;&#20102;&#36825;&#20123;&#26816;&#27979;&#22120;&#23545;&#20110;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#21644;&#29983;&#25104;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#22810;&#35821;&#35328;&#35757;&#32451;&#19979;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#23545;&#20110;&#26368;&#36817;&#30340;LLMs&#29983;&#25104;&#20854;&#20182;&#35821;&#35328;&#30340;&#20196;&#20154;&#20449;&#26381;&#30340;&#25991;&#26412;&#33021;&#21147;&#20197;&#21450;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#30740;&#31350;&#19981;&#36275;&#12290;&#36825;&#20063;&#21453;&#26144;&#22312;&#29616;&#26377;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#65292;&#32570;&#20047;&#20854;&#20182;&#35821;&#35328;&#30495;&#23454;&#25991;&#26412;&#65292;&#20027;&#35201;&#28085;&#30422;&#36739;&#26087;&#30340;&#29983;&#25104;&#22120;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MULTITuDE&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#35821;&#35328;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;11&#31181;&#35821;&#35328;&#65288;&#38463;&#25289;&#20271;&#35821;&#65292;&#21152;&#27888;&#32599;&#23612;&#20122;&#35821;&#65292;&#25463;&#20811;&#35821;&#65292;&#24503;&#35821;&#65292;&#33521;&#35821;&#65292;&#35199;&#29677;&#29273;&#35821;&#65292;&#33655;&#20848;&#35821;&#65292;&#33889;&#33796;&#29273;&#35821;&#65292;&#20420;&#35821;&#65292;&#20044;&#20811;&#20848;&#35821;&#21644;&#20013;&#25991;&#65289;&#29983;&#25104;&#30340;74,081&#20010;&#30495;&#23454;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#30001;8&#20010;&#22810;&#35821;&#35328;LLMs&#29983;&#25104;&#12290;&#20351;&#29992;&#36825;&#20010;&#22522;&#20934;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#38646;&#26679;&#26412;&#65288;&#32479;&#35745;&#21644;&#40657;&#30418;&#65289;&#21644;&#24494;&#35843;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#12290;&#32771;&#34385;&#21040;&#22810;&#35821;&#24615;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;1&#65289;&#36825;&#20123;&#26816;&#27979;&#22120;&#23545;&#20110;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#65288;&#35821;&#35328;&#19978;&#30456;&#20284;&#21644;&#19981;&#30456;&#20284;&#30340;&#35821;&#35328;&#65289;&#21644;&#26410;&#35265;&#36807;&#30340;LLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20197;&#21450;2&#65289;&#24403;&#22312;&#22810;&#20010;&#35821;&#35328;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#26816;&#27979;&#22120;&#26159;&#21542;&#33021;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a lack of research into capabilities of recent LLMs to generate convincing text in languages other than English and into performance of detectors of machine-generated text in multilingual settings. This is also reflected in the available benchmarks which lack authentic texts in languages other than English and predominantly cover older generators. To fill this gap, we introduce MULTITuDE, a novel benchmarking dataset for multilingual machine-generated text detection comprising of 74,081 authentic and machine-generated texts in 11 languages (ar, ca, cs, de, en, es, nl, pt, ru, uk, and zh) generated by 8 multilingual LLMs. Using this benchmark, we compare the performance of zero-shot (statistical and black-box) and fine-tuned detectors. Considering the multilinguality, we evaluate 1) how these detectors generalize to unseen languages (linguistically similar as well as dissimilar) and unseen LLMs and 2) whether the detectors improve their performance when trained on multiple lang
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#32593;&#32476;&#21644;&#36328;&#23610;&#24230;&#20381;&#36182;&#24314;&#27169;&#30340;&#30382;&#32932;&#30149;&#21464;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#26367;&#25442;CNN&#22359;&#21644;&#20248;&#21270;&#36830;&#25509;&#36335;&#24452;&#26469;&#25552;&#39640;&#32593;&#32476;&#29305;&#24449;&#30340;&#21487;&#37325;&#29992;&#24615;&#65292;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2310.13604</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#32593;&#32476;&#21644;&#36328;&#23610;&#24230;&#20381;&#36182;&#24314;&#27169;&#25913;&#36827;&#30340;&#30382;&#32932;&#30149;&#21464;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Skin Lesion Segmentation Improved by Transformer-based Networks with Inter-scale Dependency Modeling. (arXiv:2310.13604v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#32593;&#32476;&#21644;&#36328;&#23610;&#24230;&#20381;&#36182;&#24314;&#27169;&#30340;&#30382;&#32932;&#30149;&#21464;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#26367;&#25442;CNN&#22359;&#21644;&#20248;&#21270;&#36830;&#25509;&#36335;&#24452;&#26469;&#25552;&#39640;&#32593;&#32476;&#29305;&#24449;&#30340;&#21487;&#37325;&#29992;&#24615;&#65292;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#21457;&#29616;&#21487;&#20197;&#27835;&#30103;&#30340;&#21361;&#38505;&#24615;&#30382;&#32932;&#30284;&#30151;&#31867;&#22411;&#8212;&#8212;&#40657;&#33394;&#32032;&#30244;&#12290;&#20026;&#20102;&#36890;&#36807;&#33258;&#21160;&#30382;&#32932;&#30149;&#21464;&#20998;&#21106;&#21327;&#21161;&#35786;&#26029;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#20351;&#29992;&#20840;&#21367;&#31215;&#32593;&#32476;&#65288;FCNs&#65289;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#31361;&#20986;&#30340;&#26159;U-Net&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#23545;&#21367;&#31215;&#25805;&#20316;&#30340;&#20381;&#36182;&#20351;&#24471;&#23545;&#20934;&#30830;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#33267;&#20851;&#37325;&#35201;&#30340;&#38271;&#31243;&#20381;&#36182;&#30340;&#25429;&#25417;&#33021;&#21147;&#21463;&#21040;&#38480;&#21046;&#12290;&#36817;&#26399;&#21019;&#24314;&#20102;&#20960;&#31181;&#22522;&#20110;Transformer&#30340;U-Net&#25299;&#25169;&#32467;&#26500;&#65292;&#36890;&#36807;&#23558;CNN&#22359;&#26367;&#25442;&#20026;&#19981;&#21516;&#30340;Transformer&#27169;&#22359;&#26469;&#25429;&#25417;&#23616;&#37096;&#21644;&#20840;&#23616;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#38480;&#21046;&#20102;U&#22411;&#32467;&#26500;&#30340;&#25928;&#26524;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#31934;&#24515;&#26500;&#24314;&#36339;&#36807;&#36830;&#25509;&#36335;&#24452;&#65292;&#22686;&#21152;&#32593;&#32476;&#29305;&#24449;&#30340;&#21487;&#37325;&#29992;&#24615;&#12290;&#22312;&#36339;&#36807;&#36830;&#25509;&#36335;&#24452;&#20013;&#25972;&#21512;&#24050;&#35745;&#31639;&#30340;&#27880;&#24847;&#21147;&#20146;&#21644;&#24615;&#21487;&#20197;&#25552;&#39640;&#32593;&#32476;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Melanoma, a dangerous type of skin cancer resulting from abnormal skin cell growth, can be treated if detected early. Various approaches using Fully Convolutional Networks (FCNs) have been proposed, with the U-Net architecture being prominent To aid in its diagnosis through automatic skin lesion segmentation. However, the symmetrical U-Net model's reliance on convolutional operations hinders its ability to capture long-range dependencies crucial for accurate medical image segmentation. Several Transformer-based U-Net topologies have recently been created to overcome this limitation by replacing CNN blocks with different Transformer modules to capture local and global representations. Furthermore, the U-shaped structure is hampered by semantic gaps between the encoder and decoder. This study intends to increase the network's feature re-usability by carefully building the skip connection path. Integrating an already calculated attention affinity within the skip connection path improves t
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MarineGPT&#30340;&#28023;&#27915;&#39046;&#22495;&#19987;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20197;&#25935;&#24863;&#12289;&#20449;&#24687;&#20016;&#23500;&#21644;&#31185;&#23398;&#30340;&#26041;&#24335;&#22238;&#24212;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.13596</link><description>&lt;p&gt;
MarineGPT: &#21521;&#20844;&#20247;&#25581;&#31034;&#28023;&#27915;&#30340;&#31192;&#23494;
&lt;/p&gt;
&lt;p&gt;
MarineGPT: Unlocking Secrets of Ocean to the Public. (arXiv:2310.13596v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13596
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MarineGPT&#30340;&#28023;&#27915;&#39046;&#22495;&#19987;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20197;&#25935;&#24863;&#12289;&#20449;&#24687;&#20016;&#23500;&#21644;&#31185;&#23398;&#30340;&#26041;&#24335;&#22238;&#24212;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914;ChatGPT/GPT-4&#65292;&#34987;&#35777;&#26126;&#26159;&#25552;&#21319;&#29992;&#25143;&#20307;&#39564;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#21487;&#20316;&#20026;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#65292;&#36890;&#36807;&#26500;&#24314;&#32852;&#21512;&#35821;&#20041;&#31354;&#38388;&#65288;&#22914;&#35270;&#35273;-&#25991;&#26412;&#31354;&#38388;&#65289;&#65292;&#20351;LLM&#20855;&#22791;&#24863;&#30693;&#22810;&#27169;&#24577;&#36755;&#20837;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;LLM&#21644;MLLM&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#22312;&#38656;&#35201;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#21644;&#19987;&#19994;&#30693;&#35782;&#30340;&#39046;&#22495;&#29305;&#23450;&#24212;&#29992;&#20013;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#28023;&#27915;&#39046;&#22495;&#65292;&#23545;LLM&#21644;MLLM&#30340;&#25506;&#32034;&#23578;&#19981;&#20805;&#20998;&#12290;&#19982;&#36890;&#29992;&#30446;&#30340;&#30340;MLLM&#19981;&#21516;&#65292;&#28023;&#27915;&#29305;&#23450;MLLM&#38656;&#35201;&#20135;&#29983;&#26356;&#21152;&#25935;&#24863;&#12289;&#20449;&#24687;&#20016;&#23500;&#21644;&#20855;&#26377;&#31185;&#23398;&#24615;&#30340;&#22238;&#24212;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#36890;&#36807;&#22823;&#37327;&#36890;&#29992;&#22521;&#35757;&#25968;&#25454;&#36827;&#34892;&#20248;&#21270;&#30340;MLLM&#22312;&#29702;&#35299;&#39046;&#22495;&#29305;&#23450;&#24847;&#22270;&#24182;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#19988;&#28385;&#24847;&#30340;&#22238;&#24212;&#26041;&#38754;&#33021;&#21147;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as ChatGPT/GPT-4, have proven to be powerful tools in promoting the user experience as an AI assistant. The continuous works are proposing multi-modal large language models (MLLM), empowering LLMs with the ability to sense multiple modality inputs through constructing a joint semantic space (e.g. visual-text space). Though significant success was achieved in LLMs and MLLMs, exploring LLMs and MLLMs in domain-specific applications that required domain-specific knowledge and expertise has been less conducted, especially for \textbf{marine domain}. Different from general-purpose MLLMs, the marine-specific MLLM is required to yield much more \textbf{sensitive}, \textbf{informative}, and \textbf{scientific} responses. In this work, we demonstrate that the existing MLLMs optimized on huge amounts of readily available general-purpose training data show a minimal ability to understand domain-specific intents and then generate informative and satisfactory resp
&lt;/p&gt;</description></item><item><title>SPARE&#26159;&#19968;&#31181;&#21333;&#36941;&#31070;&#32463;&#27169;&#22411;&#65292;&#29992;&#20110;&#39640;&#25928;&#35757;&#32451;&#20851;&#31995;&#25968;&#25454;&#24211;&#65292;&#24182;&#25552;&#20379;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#30456;&#20284;&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.13581</link><description>&lt;p&gt;
SPARE: &#29992;&#20110;&#20851;&#31995;&#25968;&#25454;&#24211;&#30340;&#21333;&#36941;&#31070;&#32463;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SPARE: A Single-Pass Neural Model for Relational Databases. (arXiv:2310.13581v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13581
&lt;/p&gt;
&lt;p&gt;
SPARE&#26159;&#19968;&#31181;&#21333;&#36941;&#31070;&#32463;&#27169;&#22411;&#65292;&#29992;&#20110;&#39640;&#25928;&#35757;&#32451;&#20851;&#31995;&#25968;&#25454;&#24211;&#65292;&#24182;&#25552;&#20379;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#30456;&#20284;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#26041;&#38754;&#26377;&#30528; extensive&#30340;&#30740;&#31350;&#65292;&#20294;&#26159;&#22312;&#20851;&#31995;&#25968;&#25454;&#24211;&#65288;RDBs&#65289;&#26041;&#38754;&#30340;&#28145;&#24230;&#23398;&#20064;&#20173;&#28982;&#26159;&#19968;&#20010;&#30456;&#23545;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#12290;&#26368;&#36817;&#27969;&#34892;&#30340;&#19968;&#20010;&#26041;&#21521;&#26159;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24212;&#29992;&#20110;RBDs&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#35268;&#27169;&#30340;&#20851;&#31995;&#25968;&#25454;&#24211;&#19978;&#35757;&#32451;GNNs&#25928;&#29575;&#36739;&#20302;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#38656;&#35201;&#22810;&#36718;&#35757;&#32451;&#21644;&#28508;&#22312;&#30340;&#22823;&#32780;&#20302;&#25928;&#30340;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SPARE&#65288;Single-Pass Relational models&#65289;&#65292;&#19968;&#31181;&#21487;&#20197;&#22312;&#20851;&#31995;&#25968;&#25454;&#24211;&#19978;&#39640;&#25928;&#35757;&#32451;&#24182;&#19988;&#25552;&#20379;&#19982;GNNs&#30456;&#20284;&#20934;&#30830;&#24230;&#30340;&#26032;&#22411;&#31070;&#32463;&#27169;&#22411;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#25928;&#35757;&#32451;&#65292;&#19982;GNNs&#19981;&#21516;&#65292;SPARE&#21033;&#29992;&#20102;&#20851;&#31995;&#25968;&#25454;&#24211;&#20013;&#25968;&#25454;&#30340;&#35268;&#21017;&#32467;&#26500;&#65292;&#21487;&#20197;&#22312;&#21333;&#36941;&#35757;&#32451;&#26102;&#21033;&#29992;&#23545;&#31216;&#24615;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#65292;SPARE&#21487;&#20197;&#26174;&#33879;&#21152;&#24555;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#36895;&#24230;&#65292;&#21516;&#26102;&#25552;&#20379;&#30456;&#20284;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
While there has been extensive work on deep neural networks for images and text, deep learning for relational databases (RDBs) is still a rather unexplored field.  One direction that recently gained traction is to apply Graph Neural Networks (GNNs) to RBDs. However, training GNNs on large relational databases (i.e., data stored in multiple database tables) is rather inefficient due to multiple rounds of training and potentially large and inefficient representations. Hence, in this paper we propose SPARE (Single-Pass Relational models), a new class of neural models that can be trained efficiently on RDBs while providing similar accuracies as GNNs. For enabling efficient training, different from GNNs, SPARE makes use of the fact that data in RDBs has a regular structure, which allows one to train these models in a single pass while exploiting symmetries at the same time. Our extensive empirical evaluation demonstrates that SPARE can significantly speedup both training and inference while
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;DAG&#31354;&#38388;&#20013;&#20351;&#29992;&#26641;&#25628;&#32034;&#21644;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#23454;&#38469;&#20219;&#21153;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26080;&#27169;&#22411;&#26041;&#27861;&#21644;&#36138;&#23146;&#25628;&#32034;&#65292;&#20855;&#26377;&#24456;&#22823;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2310.13576</link><description>&lt;p&gt;
&#22312;DAG&#31354;&#38388;&#20013;&#20351;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;&#30340;&#26641;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Tree Search in DAG Space with Model-based Reinforcement Learning for Causal Discovery. (arXiv:2310.13576v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;DAG&#31354;&#38388;&#20013;&#20351;&#29992;&#26641;&#25628;&#32034;&#21644;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#23454;&#38469;&#20219;&#21153;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26080;&#27169;&#22411;&#26041;&#27861;&#21644;&#36138;&#23146;&#25628;&#32034;&#65292;&#20855;&#26377;&#24456;&#22823;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#22240;&#26524;&#32467;&#26500;&#23545;&#20110;&#35768;&#22810;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#65292;&#20174;&#25112;&#30053;&#20915;&#31574;&#21040;&#29983;&#29289;&#23398;&#21644;&#32463;&#27982;&#23398;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#25628;&#32034;&#30340;&#22240;&#26524;&#21457;&#29616;&#30340;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36880;&#27493;&#26500;&#24314;&#26377;&#21521;&#26080;&#29615;&#22270;&#12290;&#25105;&#20204;&#36824;&#24418;&#24335;&#21270;&#24182;&#35777;&#26126;&#20102;&#19968;&#31181;&#25490;&#38500;&#20250;&#24341;&#20837;&#24490;&#29615;&#30340;&#36793;&#30340;&#39640;&#25928;&#31639;&#27861;&#30340;&#27491;&#30830;&#24615;&#65292;&#36825;&#20351;&#24471;&#22312;DAG&#31354;&#38388;&#20013;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#31163;&#25955;&#25628;&#32034;&#21644;&#37319;&#26679;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#23454;&#38469;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#22312;&#24615;&#33021;&#19978;&#27604;&#26368;&#20808;&#36827;&#30340;&#26080;&#27169;&#22411;&#26041;&#27861;&#21644;&#36138;&#23146;&#25628;&#32034;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#21183;&#65292;&#36825;&#26159;&#32452;&#21512;&#26041;&#27861;&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying causal structure is central to many fields ranging from strategic decision-making to biology and economics. In this work, we propose a model-based reinforcement learning method for causal discovery based on tree search, which builds directed acyclic graphs incrementally. We also formalize and prove the correctness of an efficient algorithm for excluding edges that would introduce cycles, which enables deeper discrete search and sampling in DAG space. We evaluate our approach on two real-world tasks, achieving substantially better performance than the state-of-the-art model-free method and greedy search, constituting a promising advancement for combinatorial methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#36866;&#24212;&#39118;&#26684;&#25216;&#26415;&#25552;&#21319;&#25351;&#32441;&#27963;&#20307;&#26816;&#27979;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;LivDet 2023&#25361;&#25112;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.13573</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#39118;&#26684;&#25216;&#26415;&#25552;&#21319;&#25351;&#32441;&#27963;&#20307;&#26816;&#27979;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Boosting Generalization with Adaptive Style Techniques for Fingerprint Liveness Detection. (arXiv:2310.13573v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13573
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#36866;&#24212;&#39118;&#26684;&#25216;&#26415;&#25552;&#21319;&#25351;&#32441;&#27963;&#20307;&#26816;&#27979;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;LivDet 2023&#25361;&#25112;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24615;&#33021;&#20248;&#24322;&#30340;&#25351;&#32441;&#27963;&#20307;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#22312;LivDet 2023&#25351;&#32441;&#34920;&#31034;&#25361;&#25112;&#20013;&#33719;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#25351;&#32441;&#35782;&#21035;&#31995;&#32479;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;94.68%&#65292;&#22312;LivDet 2023&#21160;&#20316;&#20013;&#30340;&#27963;&#20307;&#26816;&#27979;&#20013;&#33719;&#24471;&#20102;&#31532;&#20108;&#21517;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#26041;&#27861;&#30340;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#39118;&#26684;&#36716;&#25442;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#38754;&#23545;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#26102;&#65292;&#20934;&#30830;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#25552;&#39640;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;LivDet 2023&#25361;&#25112;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a high-performance fingerprint liveness feature extraction technique that secured first place in LivDet 2023 Fingerprint Representation Challenge. Additionally, we developed a practical fingerprint recognition system with 94.68% accuracy, earning second place in LivDet 2023 Liveness Detection in Action. By investigating various methods, particularly style transfer, we demonstrate improvements in accuracy and generalization when faced with limited training data. As a result, our approach achieved state-of-the-art performance in LivDet 2023 Challenges.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#21644;&#36923;&#36753;&#25512;&#29702;&#30340;&#30693;&#35782;&#39537;&#21160;&#21709;&#24212;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#23545;&#35805;&#36718;&#27425;&#20013;&#35780;&#20272;&#30693;&#35782;&#22270;&#20013;&#30340;&#33410;&#28857;&#21644;&#36793;&#30340;&#23545;&#35805;&#30456;&#20851;&#24615;&#65292;&#22312;&#20219;&#21153;&#22411;&#23545;&#35805;&#31995;&#32479;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13566</link><description>&lt;p&gt;
&#36890;&#36807;&#36923;&#36753;&#25512;&#29702;&#21644;&#30456;&#20851;&#24615;&#35780;&#20998;&#30340;&#26816;&#32034;&#22686;&#24378;&#22411;&#31070;&#32463;&#21709;&#24212;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Neural Response Generation Using Logical Reasoning and Relevance Scoring. (arXiv:2310.13566v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13566
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#21644;&#36923;&#36753;&#25512;&#29702;&#30340;&#30693;&#35782;&#39537;&#21160;&#21709;&#24212;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#23545;&#35805;&#36718;&#27425;&#20013;&#35780;&#20272;&#30693;&#35782;&#22270;&#20013;&#30340;&#33410;&#28857;&#21644;&#36793;&#30340;&#23545;&#35805;&#30456;&#20851;&#24615;&#65292;&#22312;&#20219;&#21153;&#22411;&#23545;&#35805;&#31995;&#32479;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20219;&#21153;&#22411;&#23545;&#35805;&#31995;&#32479;&#20013;&#26500;&#36896;&#21709;&#24212;&#36890;&#24120;&#20381;&#36182;&#20110;&#24403;&#21069;&#23545;&#35805;&#29366;&#24577;&#25110;&#22806;&#37096;&#25968;&#25454;&#24211;&#31561;&#20449;&#24687;&#28304;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#21644;&#36923;&#36753;&#25512;&#29702;&#30340;&#30693;&#35782;&#39537;&#21160;&#21709;&#24212;&#29983;&#25104;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22260;&#32469;&#30528;&#34920;&#31034;&#24403;&#21069;&#23545;&#35805;&#29366;&#24577;&#21644;&#32972;&#26223;&#20449;&#24687;&#30340;&#30693;&#35782;&#22270;&#65292;&#24182;&#36890;&#36807;&#19977;&#20010;&#27493;&#39588;&#26469;&#36827;&#34892;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;&#25512;&#26029;&#20986;&#36923;&#36753;&#25512;&#23548;&#30340;&#20107;&#23454;&#65292;&#20016;&#23500;&#30693;&#35782;&#22270;&#12290;&#28982;&#21518;&#65292;&#22312;&#27599;&#20010;&#23545;&#35805;&#36718;&#27425;&#20013;&#20351;&#29992;&#31070;&#32463;&#27169;&#22411;&#26469;&#35780;&#20998;&#25193;&#23637;&#22270;&#20013;&#27599;&#20010;&#33410;&#28857;&#21644;&#36793;&#30340;&#23545;&#35805;&#30456;&#20851;&#24615;&#12290;&#26368;&#21518;&#65292;&#23558;&#30456;&#20851;&#24615;&#24471;&#20998;&#26368;&#39640;&#30340;&#20803;&#32032;&#36716;&#25442;&#25104;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#65292;&#24182;&#25972;&#21512;&#21040;&#29992;&#20110;&#29983;&#25104;&#31995;&#32479;&#21709;&#24212;&#30340;&#31070;&#32463;&#23545;&#35805;&#27169;&#22411;&#30340;&#25552;&#31034;&#20013;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#65288;KVRET&#21644;GraphWOZ&#65289;&#19978;&#30740;&#31350;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Constructing responses in task-oriented dialogue systems typically relies on information sources such the current dialogue state or external databases. This paper presents a novel approach to knowledge-grounded response generation that combines retrieval-augmented language models with logical reasoning. The approach revolves around a knowledge graph representing the current dialogue state and background information, and proceeds in three steps. The knowledge graph is first enriched with logically derived facts inferred using probabilistic logical programming. A neural model is then employed at each turn to score the conversational relevance of each node and edge of this extended graph. Finally, the elements with highest relevance scores are converted to a natural language form, and are integrated into the prompt for the neural conversational model employed to generate the system response.  We investigate the benefits of the proposed approach on two datasets (KVRET and GraphWOZ) along w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#35745;&#31639;&#26426;&#32593;&#32476;&#38450;&#24481;&#20219;&#21153;&#20013;&#65292;&#22870;&#21169;&#20449;&#21495;&#30340;&#29305;&#28857;&#23545;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24433;&#21709;&#65292;&#24182;&#30740;&#31350;&#20102;&#22870;&#21169;&#22609;&#36896;&#25216;&#26415;&#20197;&#25552;&#39640;&#20195;&#29702;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.13565</link><description>&lt;p&gt;
&#20026;&#26356;&#24555;&#20048;&#30340;&#33258;&#20027;&#32593;&#32476;&#23433;&#20840;&#20195;&#29702;&#35774;&#35745;&#30340;&#22870;&#21169;&#22609;&#36896;
&lt;/p&gt;
&lt;p&gt;
Reward Shaping for Happier Autonomous Cyber Security Agents. (arXiv:2310.13565v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#35745;&#31639;&#26426;&#32593;&#32476;&#38450;&#24481;&#20219;&#21153;&#20013;&#65292;&#22870;&#21169;&#20449;&#21495;&#30340;&#29305;&#28857;&#23545;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24433;&#21709;&#65292;&#24182;&#30740;&#31350;&#20102;&#22870;&#21169;&#22609;&#36896;&#25216;&#26415;&#20197;&#25552;&#39640;&#20195;&#29702;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#33021;&#21147;&#22686;&#24378;&#65292;&#23427;&#20204;&#22312;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#22686;&#21152;&#30340;&#28508;&#21147;&#12290;&#26368;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#20043;&#19968;&#26159;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#35757;&#32451;&#35745;&#31639;&#26426;&#32593;&#32476;&#38450;&#24481;&#20219;&#21153;&#20013;&#30340;&#33258;&#20027;&#20195;&#29702;&#12290;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#35757;&#32451;&#27492;&#20219;&#21153;&#26102;&#20026;&#20195;&#29702;&#25552;&#20379;&#30340;&#22870;&#21169;&#20449;&#21495;&#30340;&#24433;&#21709;&#12290;&#30001;&#20110;&#32593;&#32476;&#23433;&#20840;&#20219;&#21153;&#30340;&#24615;&#36136;&#65292;&#22870;&#21169;&#20449;&#21495;&#36890;&#24120;&#37319;&#29992;&#20197;&#19979;&#24418;&#24335;&#65306;1&#65289;&#20197;&#24809;&#32602;&#30340;&#24418;&#24335;&#65288;&#20363;&#22914;&#21457;&#29983;&#20102;&#22949;&#21327;&#65289;&#65307;2&#65289;&#22312;&#27599;&#27425;&#38450;&#24481;&#20219;&#21153;&#20013;&#31232;&#30095;&#20998;&#24067;&#12290;&#36825;&#26679;&#30340;&#22870;&#21169;&#29305;&#24449;&#19981;&#21516;&#20110;&#32463;&#20856;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#20854;&#20013;&#20195;&#29702;&#23450;&#26399;&#33719;&#24471;&#36827;&#23637;&#30340;&#22870;&#21169;&#65288;&#32780;&#19981;&#26159;&#20598;&#23572;&#22240;&#22833;&#36133;&#32780;&#21463;&#21040;&#24809;&#32602;&#65289;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#33021;&#22815;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#30340;&#22870;&#21169;&#22609;&#36896;&#25216;&#26415;&#65292;&#20197;&#20351;&#20195;&#29702;&#26356;&#33021;&#26377;&#25928;&#22320;&#35757;&#32451;&#65292;&#24182;&#28508;&#22312;&#22320;&#25910;&#25947;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#23545;&#22870;&#21169;&#24133;&#24230;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As machine learning models become more capable, they have exhibited increased potential in solving complex tasks. One of the most promising directions uses deep reinforcement learning to train autonomous agents in computer network defense tasks. This work studies the impact of the reward signal that is provided to the agents when training for this task. Due to the nature of cybersecurity tasks, the reward signal is typically 1) in the form of penalties (e.g., when a compromise occurs), and 2) distributed sparsely across each defense episode. Such reward characteristics are atypical of classic reinforcement learning tasks where the agent is regularly rewarded for progress (cf. to getting occasionally penalized for failures). We investigate reward shaping techniques that could bridge this gap so as to enable agents to train more sample-efficiently and potentially converge to a better performance. We first show that deep reinforcement learning algorithms are sensitive to the magnitude of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#25105;&#21551;&#31034;&#30340;&#24605;&#32500;&#38142;&#65288;SP-CoT&#65289;&#26694;&#26550;&#65292;&#22312;&#24320;&#25918;&#39046;&#22495;&#22810;&#36339;&#25512;&#29702;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;CoTs&#65292;&#25193;&#23637;&#20102;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.13552</link><description>&lt;p&gt;
&#33258;&#25105;&#21551;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24320;&#25918;&#39046;&#22495;&#22810;&#36339;&#25512;&#29702;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Self-prompted Chain-of-Thought on Large Language Models for Open-domain Multi-hop Reasoning. (arXiv:2310.13552v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#25105;&#21551;&#31034;&#30340;&#24605;&#32500;&#38142;&#65288;SP-CoT&#65289;&#26694;&#26550;&#65292;&#22312;&#24320;&#25918;&#39046;&#22495;&#22810;&#36339;&#25512;&#29702;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;CoTs&#65292;&#25193;&#23637;&#20102;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#65288;ODQA&#65289;&#20013;&#65292;&#22823;&#22810;&#25968;&#38382;&#39064;&#38656;&#35201;&#23545;&#24120;&#35782;&#36827;&#34892;&#21333;&#36339;&#25512;&#29702;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25193;&#23637;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#27491;&#24335;&#24341;&#20837;&#20102;&#24320;&#25918;&#39046;&#22495;&#22810;&#36339;&#25512;&#29702;&#65288;ODMR&#65289;&#65292;&#36890;&#36807;&#22312;&#24320;&#25918;&#39046;&#22495;&#35774;&#32622;&#20013;&#22238;&#31572;&#22810;&#36339;&#38382;&#39064;&#24182;&#25552;&#20379;&#26126;&#30830;&#30340;&#25512;&#29702;&#27493;&#39588;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26080;&#38656;&#22806;&#37096;&#35821;&#26009;&#24211;&#30340;&#24773;&#20917;&#19979;&#22312;&#20419;&#36827;ODQA&#26041;&#38754;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25552;&#31034;&#36890;&#36807;&#25163;&#21160;&#25110;&#33258;&#21160;&#21270;&#33539;&#20363;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#32570;&#20047;&#36136;&#37327;&#20445;&#35777;&#65292;&#32780;&#25163;&#21160;&#26041;&#27861;&#21463;&#21040;&#21487;&#25193;&#23637;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#38480;&#21046;&#65292;&#38459;&#30861;&#20102;LLMs&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#25105;&#21551;&#31034;&#30340;&#24605;&#32500;&#38142;&#65288;SP-CoT&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;LLMs&#24182;&#20026;LLMs&#22823;&#35268;&#27169;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;CoTs&#12290;
&lt;/p&gt;
&lt;p&gt;
In open-domain question-answering (ODQA), most existing questions require single-hop reasoning on commonsense. To further extend this task, we officially introduce open-domain multi-hop reasoning (ODMR) by answering multi-hop questions with explicit reasoning steps in open-domain setting. Recently, large language models (LLMs) have found significant utility in facilitating ODQA without external corpus. Furthermore, chain-of-thought (CoT) prompting boosts the reasoning capability of LLMs to a greater extent with manual or automated paradigms. However, existing automated methods lack of quality assurance, while manual approaches suffer from limited scalability and poor diversity, hindering the capabilities of LLMs. In this paper, we propose Self-prompted Chain-of-Thought (SP-CoT), an automated framework to mass-produce high quality CoTs of LLMs, by LLMs and for LLMs. SP-CoT introduces an automated generation pipeline of high quality ODMR datasets, an adaptive sampler for in-context CoT s
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35757;&#32451;&#39640;&#36136;&#37327;AI&#21161;&#25163;&#30340;&#25216;&#26415;&#65292;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#36807;&#20110;&#35844;&#23194;&#65292;&#32780;&#19981;&#26159;&#22374;&#35802;&#65292;&#36890;&#36807;&#20998;&#26512;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#24471;&#20986;&#20102;&#36825;&#19968;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2310.13548</link><description>&lt;p&gt;
&#25506;&#32034;&#35821;&#35328;&#27169;&#22411;&#20013;&#35844;&#23194;&#34892;&#20026;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding Sycophancy in Language Models. (arXiv:2310.13548v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13548
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35757;&#32451;&#39640;&#36136;&#37327;AI&#21161;&#25163;&#30340;&#25216;&#26415;&#65292;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#36807;&#20110;&#35844;&#23194;&#65292;&#32780;&#19981;&#26159;&#22374;&#35802;&#65292;&#36890;&#36807;&#20998;&#26512;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#24471;&#20986;&#20102;&#36825;&#19968;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#12300;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#12301;&#26159;&#35757;&#32451;&#39640;&#36136;&#37327;AI&#21161;&#25163;&#30340;&#19968;&#31181;&#27969;&#34892;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;RLHF&#21487;&#33021;&#20250;&#40723;&#21169;&#27169;&#22411;&#36890;&#36807;&#19982;&#29992;&#25143;&#20449;&#24565;&#30456;&#31526;&#30340;&#22238;&#31572;&#26469;&#20195;&#26367;&#30495;&#23454;&#22238;&#31572;&#65292;&#36825;&#31181;&#34892;&#20026;&#34987;&#31216;&#20026;&#35844;&#23194;&#34892;&#20026;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;RLHF&#35757;&#32451;&#27169;&#22411;&#20013;&#35844;&#23194;&#34892;&#20026;&#30340;&#26222;&#36941;&#24615;&#20197;&#21450;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#26159;&#21542;&#36215;&#21040;&#20102;&#20316;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20116;&#20010;&#26368;&#20808;&#36827;&#30340;AI&#21161;&#25163;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#33258;&#30001;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#19968;&#36143;&#34920;&#29616;&#20986;&#35844;&#23194;&#34892;&#20026;&#12290;&#20026;&#20102;&#29702;&#35299;&#20154;&#31867;&#20559;&#22909;&#26159;&#21542;&#39537;&#21160;&#20102;RLHF&#27169;&#22411;&#30340;&#36825;&#31181;&#24191;&#27867;&#34892;&#20026;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#22238;&#31572;&#19982;&#29992;&#25143;&#30340;&#35266;&#28857;&#30456;&#31526;&#26102;&#65292;&#23427;&#26356;&#26377;&#21487;&#33021;&#34987;&#36873;&#20013;&#12290;&#27492;&#22806;&#65292;&#20154;&#31867;&#21644;&#20559;&#22909;&#27169;&#22411;&#65288;PMs&#65289;&#23558;&#26377;&#35828;&#26381;&#21147;&#30340;&#35844;&#23194;&#22238;&#31572;&#19982;&#27491;&#30830;&#22238;&#31572;&#30456;&#27604;&#65292;&#26377;&#26102;&#20960;&#20046;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#22320;&#36873;&#25321;&#20102;&#35844;&#23194;&#22238;&#31572;&#12290;&#20248;&#21270;&#27169;&#22411;&#36755;&#20986;&#20197;&#28385;&#36275;PMs&#26377;&#26102;&#20063;&#20250;&#22312;&#30495;&#23454;&#24615;&#21644;&#35844;&#23194;&#34892;&#20026;&#20043;&#38388;&#20570;&#20986;&#21462;&#33293;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) is a popular technique for training high-quality AI assistants. However, RLHF may also encourage model responses that match user beliefs over truthful responses, a behavior known as sycophancy. We investigate the prevalence of sycophancy in RLHF-trained models and whether human preference judgements are responsible. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophantic behavior across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior of RLHF models, we analyze existing human preference data. We find that when a response matches a user's views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Over
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32553;&#23567;&#38271;&#36339;&#25509;&#36830;&#25509;&#31995;&#25968;&#65292;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;UNet&#35757;&#32451;&#26356;&#21152;&#31283;&#23450;&#65292;&#24182;&#19988;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;UNet&#23545;&#20110;&#36755;&#20837;&#30340;&#25200;&#21160;&#25935;&#24863;&#65292;&#24182;&#20135;&#29983;&#25391;&#33633;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13545</link><description>&lt;p&gt;
ScaleLong: &#36890;&#36807;&#32553;&#25918;&#38271;&#36339;&#25509;&#36830;&#25509;&#23454;&#29616;&#25193;&#25955;&#27169;&#22411;&#30340;&#26356;&#31283;&#23450;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
ScaleLong: Towards More Stable Training of Diffusion Model via Scaling Network Long Skip Connection. (arXiv:2310.13545v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13545
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32553;&#23567;&#38271;&#36339;&#25509;&#36830;&#25509;&#31995;&#25968;&#65292;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;UNet&#35757;&#32451;&#26356;&#21152;&#31283;&#23450;&#65292;&#24182;&#19988;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;UNet&#23545;&#20110;&#36755;&#20837;&#30340;&#25200;&#21160;&#25935;&#24863;&#65292;&#24182;&#20135;&#29983;&#25391;&#33633;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#65292;UNet&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#32593;&#32476;&#39592;&#24178;&#65292;&#22240;&#20026;&#20854;&#38271;&#36339;&#25509;&#36830;&#25509;&#21487;&#20197;&#32858;&#21512;&#36828;&#36317;&#31163;&#20449;&#24687;&#24182;&#20943;&#36731;&#28040;&#22833;&#26799;&#24230;&#38382;&#39064;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#65292;UNet&#32463;&#24120;&#36973;&#21463;&#19981;&#31283;&#23450;&#30340;&#35757;&#32451;&#65292;&#36890;&#36807;&#32553;&#23567;&#20854;&#38271;&#36339;&#25509;&#36830;&#25509;&#31995;&#25968;&#21487;&#20197;&#32531;&#35299;&#35813;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;UNet&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;&#20197;&#21450;&#38271;&#36339;&#25509;&#36830;&#25509;&#32553;&#25918;&#30340;&#24615;&#33021;&#25913;&#36827;&#30340;&#29702;&#35770;&#29702;&#35299;&#23578;&#26410;&#20986;&#29616;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;UNet&#20013;&#38271;&#36339;&#25509;&#36830;&#25509;&#31995;&#25968;&#23545;&#21069;&#21521;&#21644;&#21518;&#21521;&#20256;&#25773;&#30340;&#31283;&#23450;&#24615;&#20197;&#21450;UNet&#30340;&#40065;&#26834;&#24615;&#26377;&#24456;&#22823;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;UNet&#22312;&#20219;&#20309;&#23618;&#30340;&#38544;&#34255;&#29305;&#24449;&#21644;&#26799;&#24230;&#37117;&#20250;&#25391;&#33633;&#65292;&#20854;&#25391;&#33633;&#33539;&#22260;&#23454;&#38469;&#19978;&#24456;&#22823;&#65292;&#36825;&#35299;&#37322;&#20102;UNet&#35757;&#32451;&#30340;&#19981;&#31283;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;UNet&#23545;&#25200;&#21160;&#36755;&#20837;&#20063;&#20855;&#26377;&#25935;&#24863;&#24615;&#65292;&#24182;&#19988;&#20250;&#20135;&#29983;&#19982;&#26399;&#26395;&#36755;&#20986;&#30456;&#36317;&#36739;&#36828;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#20135;&#29983;&#25391;&#33633;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In diffusion models, UNet is the most popular network backbone, since its long skip connects (LSCs) to connect distant network blocks can aggregate long-distant information and alleviate vanishing gradient. Unfortunately, UNet often suffers from unstable training in diffusion models which can be alleviated by scaling its LSC coefficients smaller. However, theoretical understandings of the instability of UNet in diffusion models and also the performance improvement of LSC scaling remain absent yet. To solve this issue, we theoretically show that the coefficients of LSCs in UNet have big effects on the stableness of the forward and backward propagation and robustness of UNet. Specifically, the hidden feature and gradient of UNet at any layer can oscillate and their oscillation ranges are actually large which explains the instability of UNet training. Moreover, UNet is also provably sensitive to perturbed input, and predicts an output distant from the desired output, yielding oscillatory 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#32467;&#26500;&#36827;&#34892;&#27491;-&#26410;&#26631;&#35760;&#33410;&#28857;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36317;&#31163;&#24863;&#30693;&#30340;PU&#25439;&#22833;&#26469;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#30417;&#30563;&#65292;&#21516;&#26102;&#36890;&#36807;&#27491;&#21017;&#21270;&#39033;&#19982;&#22270;&#32467;&#26500;&#23545;&#40784;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#31181;&#22270;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#21331;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.13538</link><description>&lt;p&gt;
&#22522;&#20110;&#32467;&#26500;&#24863;&#30693;&#22270;&#23398;&#20064;&#30340;&#27491;-&#26410;&#26631;&#35760;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Positive-Unlabeled Node Classification with Structure-aware Graph Learning. (arXiv:2310.13538v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#32467;&#26500;&#36827;&#34892;&#27491;-&#26410;&#26631;&#35760;&#33410;&#28857;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36317;&#31163;&#24863;&#30693;&#30340;PU&#25439;&#22833;&#26469;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#30417;&#30563;&#65292;&#21516;&#26102;&#36890;&#36807;&#27491;&#21017;&#21270;&#39033;&#19982;&#22270;&#32467;&#26500;&#23545;&#40784;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#31181;&#22270;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#19978;&#36827;&#34892;&#33410;&#28857;&#20998;&#31867;&#26159;&#19968;&#20010;&#20855;&#26377;&#35768;&#22810;&#24212;&#29992;&#30340;&#37325;&#35201;&#30740;&#31350;&#38382;&#39064;&#12290;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22270;&#25968;&#25454;&#38598;&#21487;&#33021;&#24182;&#19981;&#22914;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#25152;&#20551;&#23450;&#30340;&#37027;&#26679;&#24179;&#34913;&#21644;&#20934;&#30830;&#12290;&#27491;-&#26410;&#26631;&#35760;&#65288;PU&#65289;&#33410;&#28857;&#20998;&#31867;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#26631;&#35760;&#33410;&#28857;&#34987;&#38480;&#21046;&#20026;&#27491;&#33410;&#28857;&#12290;&#23427;&#20855;&#26377;&#22810;&#26679;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#30123;&#24773;&#39044;&#27979;&#25110;&#32593;&#32476;&#24322;&#24120;&#26816;&#27979;&#12290;&#29616;&#26377;&#30340;PU&#33410;&#28857;&#20998;&#31867;&#24037;&#20316;&#24573;&#35270;&#20102;&#22270;&#32467;&#26500;&#20013;&#30340;&#20449;&#24687;&#65292;&#32780;&#36825;&#21487;&#33021;&#26159;&#20851;&#38190;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26356;&#22909;&#22320;&#21033;&#29992;&#22270;&#32467;&#26500;&#36827;&#34892;PU&#33410;&#28857;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#36317;&#31163;&#24863;&#30693;&#30340;PU&#25439;&#22833;&#65292;&#21033;&#29992;&#22270;&#20013;&#30340;&#21516;&#36136;&#24615;&#24341;&#20837;&#26356;&#20934;&#30830;&#30340;&#30417;&#30563;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#21017;&#21270;&#39033;&#65292;&#20351;&#27169;&#22411;&#19982;&#22270;&#32467;&#26500;&#23545;&#40784;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#26368;&#23567;&#21270;&#25152;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#20063;&#23548;&#33268;&#26368;&#23567;&#21270;&#24102;&#26377;&#27491;&#36127;&#26631;&#31614;&#30340;&#26399;&#26395;&#25439;&#22833;&#12290;&#23545;&#22810;&#31181;&#22270;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Node classification on graphs is an important research problem with many applications. Real-world graph data sets may not be balanced and accurate as assumed by most existing works. A challenging setting is positive-unlabeled (PU) node classification, where labeled nodes are restricted to positive nodes. It has diverse applications, e.g., pandemic prediction or network anomaly detection. Existing works on PU node classification overlook information in the graph structure, which can be critical. In this paper, we propose to better utilize graph structure for PU node classification. We first propose a distance-aware PU loss that uses homophily in graphs to introduce more accurate supervision. We also propose a regularizer to align the model with graph structure. Theoretical analysis shows that minimizing the proposed loss also leads to minimizing the expected loss with both positive and negative labels. Extensive empirical evaluation on diverse graph data sets demonstrates its superior p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#35270;&#39057;&#24207;&#21015;&#20013;&#36880;&#28176;&#21464;&#21270;&#30340;&#39046;&#22495;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#27599;&#20010;&#22270;&#20687;&#24207;&#21015;&#20013;&#21333;&#29420;&#35780;&#20272;&#65292;&#24182;&#36890;&#36807;&#36880;&#28176;&#25913;&#21464;&#22825;&#27668;&#26465;&#20214;&#21644;&#26102;&#27573;&#26469;&#27979;&#35797;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.13533</link><description>&lt;p&gt;
ICCV 2023&#35270;&#35273;&#36830;&#32493;&#23398;&#20064;&#25361;&#25112;&#30340;&#25216;&#26415;&#25253;&#21578;&#65306;&#35821;&#20041;&#20998;&#21106;&#30340;&#36830;&#32493;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Technical Report for ICCV 2023 Visual Continual Learning Challenge: Continuous Test-time Adaptation for Semantic Segmentation. (arXiv:2310.13533v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13533
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#35270;&#39057;&#24207;&#21015;&#20013;&#36880;&#28176;&#21464;&#21270;&#30340;&#39046;&#22495;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#27599;&#20010;&#22270;&#20687;&#24207;&#21015;&#20013;&#21333;&#29420;&#35780;&#20272;&#65292;&#24182;&#36890;&#36807;&#36880;&#28176;&#25913;&#21464;&#22825;&#27668;&#26465;&#20214;&#21644;&#26102;&#27573;&#26469;&#27979;&#35797;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#25361;&#25112;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#31181;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#26041;&#27861;&#65288;TTA&#65289;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36866;&#24212;&#35270;&#39057;&#24207;&#21015;&#20013;&#36880;&#28176;&#21464;&#21270;&#30340;&#39046;&#22495;&#65292;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#12290;&#23427;&#22522;&#20110;&#19968;&#20010;&#21512;&#25104;&#39550;&#39542;&#35270;&#39057;&#25968;&#25454;&#38598;- SHIFT&#12290;&#28304;&#27169;&#22411;&#26159;&#22312;&#22825;&#27668;&#26228;&#26391;&#30340;&#30333;&#22825;&#25293;&#25668;&#30340;&#22270;&#20687;&#19978;&#35757;&#32451;&#30340;&#12290;&#27979;&#35797;&#26102;&#38388;&#30340;&#39046;&#22495;&#21464;&#21270;&#20027;&#35201;&#26159;&#30001;&#20110;&#19981;&#21516;&#30340;&#22825;&#27668;&#26465;&#20214;&#21644;&#19981;&#21516;&#26102;&#27573;&#12290;TTA&#26041;&#27861;&#22312;&#27599;&#20010;&#22270;&#20687;&#24207;&#21015;&#65288;&#35270;&#39057;&#65289;&#20013;&#21333;&#29420;&#35780;&#20272;&#65292;&#24847;&#21619;&#30528;&#22312;&#19979;&#19968;&#20010;&#24207;&#21015;&#20043;&#21069;&#65292;&#27169;&#22411;&#20250;&#34987;&#37325;&#26032;&#32622;&#20026;&#28304;&#27169;&#22411;&#29366;&#24577;&#12290;&#22270;&#20687;&#19968;&#20010;&#25509;&#19968;&#20010;&#22320;&#21040;&#36798;&#65292;&#27599;&#20010;&#24103;&#21040;&#36798;&#26102;&#37117;&#38656;&#35201;&#36827;&#34892;&#39044;&#27979;&#12290;&#27599;&#20010;&#24207;&#21015;&#30001;401&#20010;&#22270;&#20687;&#32452;&#25104;&#65292;&#20174;&#28304;&#39046;&#22495;&#24320;&#22987;&#65292;&#28982;&#21518;&#36880;&#28176;&#36716;&#21464;&#20026;&#19981;&#21516;&#30340;&#39046;&#22495;&#65288;&#25913;&#21464;&#22825;&#27668;&#25110;&#26102;&#27573;&#65289;&#65292;&#30452;&#21040;&#24207;&#21015;&#30340;&#20013;&#38388;&#37096;&#20998;&#12290;&#22312;&#24207;&#21015;&#30340;&#21518;&#21322;&#37096;&#20998;&#65292;&#39046;&#22495;&#36880;&#28176;&#22238;&#24402;&#21040;&#28304;&#39046;&#22495;&#12290;&#21482;&#26377;SHIFT&#25968;&#25454;&#38598;&#30340;&#39564;&#35777;&#38598;&#25552;&#20379;&#20102;&#30495;&#23454;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of the challenge is to develop a test-time adaptation (TTA) method, which could adapt the model to gradually changing domains in video sequences for semantic segmentation task. It is based on a synthetic driving video dataset - SHIFT. The source model is trained on images taken during daytime in clear weather. Domain changes at test-time are mainly caused by varying weather conditions and times of day. The TTA methods are evaluated in each image sequence (video) separately, meaning the model is reset to the source model state before the next sequence. Images come one by one and a prediction has to be made at the arrival of each frame. Each sequence is composed of 401 images and starts with the source domain, then gradually drifts to a different one (changing weather or time of day) until the middle of the sequence. In the second half of the sequence, the domain gradually shifts back to the source one. Ground truth data is available only for the validation split of the SHIFT da
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27979;&#37327;&#30340;&#21464;&#20998;&#37327;&#23376;&#35745;&#31639;&#31639;&#27861;&#65292;&#23558;&#37327;&#23376;&#27979;&#37327;&#30340;&#38543;&#26426;&#24615;&#35270;&#20026;&#35745;&#31639;&#36164;&#28304;&#65292;&#24182;&#24212;&#29992;&#20110;&#29983;&#25104;&#24314;&#27169;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.13524</link><description>&lt;p&gt;
&#22522;&#20110;&#27979;&#37327;&#30340;&#21464;&#20998;&#37327;&#23376;&#35745;&#31639;&#29992;&#20110;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Variational measurement-based quantum computation for generative modeling. (arXiv:2310.13524v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13524
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27979;&#37327;&#30340;&#21464;&#20998;&#37327;&#23376;&#35745;&#31639;&#31639;&#27861;&#65292;&#23558;&#37327;&#23376;&#27979;&#37327;&#30340;&#38543;&#26426;&#24615;&#35270;&#20026;&#35745;&#31639;&#36164;&#28304;&#65292;&#24182;&#24212;&#29992;&#20110;&#29983;&#25104;&#24314;&#27169;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27979;&#37327;&#30340;&#37327;&#23376;&#35745;&#31639;&#65288;MBQC&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#26412;&#29420;&#29305;&#30340;&#33539;&#20363;&#26469;&#35774;&#35745;&#37327;&#23376;&#31639;&#27861;&#12290;&#22312;MBQC&#20013;&#65292;&#30001;&#20110;&#37327;&#23376;&#27979;&#37327;&#30340;&#22266;&#26377;&#38543;&#26426;&#24615;&#65292;&#33258;&#28982;&#30340;&#25805;&#20316;&#19981;&#26159;&#30830;&#23450;&#24615;&#21644;&#24186;&#27491;&#30340;&#65292;&#32780;&#26159;&#36890;&#36807;&#27010;&#29575;&#38468;&#24102;&#30340;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;MBQC&#30340;&#20027;&#35201;&#31639;&#27861;&#24212;&#29992;&#26159;&#23436;&#20840;&#25269;&#28040;&#36825;&#31181;&#27010;&#29575;&#24615;&#36136;&#65292;&#20197;&#27169;&#25311;&#34920;&#36798;&#22312;&#30005;&#36335;&#27169;&#22411;&#20013;&#30340;&#24186;&#27491;&#35745;&#31639;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35774;&#35745;MBQC&#31639;&#27861;&#30340;&#24605;&#36335;&#65292;&#35813;&#31639;&#27861;&#25509;&#21463;&#36825;&#31181;&#22266;&#26377;&#38543;&#26426;&#24615;&#65292;&#24182;&#23558;MBQC&#20013;&#30340;&#38543;&#26426;&#38468;&#24102;&#35270;&#20026;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#38543;&#26426;&#24615;&#26377;&#30410;&#30340;&#33258;&#28982;&#24212;&#29992;&#65292;&#21363;&#29983;&#25104;&#24314;&#27169;&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#29983;&#25104;&#22797;&#26434;&#27010;&#29575;&#20998;&#24067;&#20026;&#20013;&#24515;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#25511;&#21046;&#21442;&#25968;&#30340;&#21464;&#20998;MBQC&#31639;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#35843;&#25972;&#20801;&#35768;&#22312;&#35745;&#31639;&#20013;&#24341;&#20837;&#30340;&#38543;&#26426;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measurement-based quantum computation (MBQC) offers a fundamentally unique paradigm to design quantum algorithms. Indeed, due to the inherent randomness of quantum measurements, the natural operations in MBQC are not deterministic and unitary, but are rather augmented with probabilistic byproducts. Yet, the main algorithmic use of MBQC so far has been to completely counteract this probabilistic nature in order to simulate unitary computations expressed in the circuit model. In this work, we propose designing MBQC algorithms that embrace this inherent randomness and treat the random byproducts in MBQC as a resource for computation. As a natural application where randomness can be beneficial, we consider generative modeling, a task in machine learning centered around generating complex probability distributions. To address this task, we propose a variational MBQC algorithm equipped with control parameters that allow to directly adjust the degree of randomness to be admitted in the comput
&lt;/p&gt;</description></item><item><title>RaceLens&#26159;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#26234;&#33021;&#30340;&#36187;&#36710;&#29031;&#29255;&#20998;&#26512;&#24212;&#29992;&#65292;&#21033;&#29992;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#23545;&#36187;&#36710;&#29031;&#29255;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#12290;&#36890;&#36807;&#25345;&#32493;&#25913;&#36827;&#21644;&#22686;&#24378;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;RaceLens&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#24471;&#21040;&#20102;&#25552;&#39640;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;NASCAR&#36710;&#38431;&#65292;&#23545;&#20854;&#25112;&#30053;&#20915;&#31574;&#21644;&#34920;&#29616;&#20135;&#29983;&#20102;&#30452;&#25509;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.13515</link><description>&lt;p&gt;
RaceLens:&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#26234;&#33021;&#30340;&#36187;&#36710;&#29031;&#29255;&#20998;&#26512;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
RaceLens: A Machine Intelligence-Based Application for Racing Photo Analysis. (arXiv:2310.13515v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13515
&lt;/p&gt;
&lt;p&gt;
RaceLens&#26159;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#26234;&#33021;&#30340;&#36187;&#36710;&#29031;&#29255;&#20998;&#26512;&#24212;&#29992;&#65292;&#21033;&#29992;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#23545;&#36187;&#36710;&#29031;&#29255;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#12290;&#36890;&#36807;&#25345;&#32493;&#25913;&#36827;&#21644;&#22686;&#24378;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;RaceLens&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#24471;&#21040;&#20102;&#25552;&#39640;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;NASCAR&#36710;&#38431;&#65292;&#23545;&#20854;&#25112;&#30053;&#20915;&#31574;&#21644;&#34920;&#29616;&#20135;&#29983;&#20102;&#30452;&#25509;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;RaceLens&#65292;&#19968;&#31181;&#21033;&#29992;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#23545;&#36187;&#36710;&#29031;&#29255;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#30340;&#21019;&#26032;&#24212;&#29992;&#12290;&#25105;&#20204;&#24320;&#21457;&#30340;&#27169;&#22411;&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#39640;&#25928;&#24615;&#65292;&#21253;&#25324;&#26816;&#27979;&#36187;&#36710;&#12289;&#35782;&#21035;&#36710;&#29260;&#21495;&#30721;&#12289;&#26816;&#27979;&#21644;&#37327;&#21270;&#36710;&#36742;&#32454;&#33410;&#20197;&#21450;&#35782;&#21035;&#36710;&#36742;&#26041;&#21521;&#31561;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20026;&#35757;&#32451;&#27169;&#22411;&#25152;&#38656;&#30340;&#40065;&#26834;&#25968;&#25454;&#38598;&#30340;&#25910;&#38598;&#36807;&#31243;&#65292;&#24182;&#25551;&#36848;&#20102;&#19968;&#31181;&#25105;&#20204;&#35774;&#35745;&#30340;&#25345;&#32493;&#25913;&#36827;&#21644;&#22686;&#24378;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#21453;&#39304;&#24490;&#29615;&#19981;&#26029;&#25913;&#36827;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;RaceLens&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;&#26412;&#30740;&#31350;&#30340;&#37325;&#35201;&#37096;&#20998;&#26159;&#20171;&#32461;RaceLens&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#37325;&#28857;&#26159;&#20854;&#22312;&#22235;&#20010;&#36187;&#23395;&#20013;&#30001;NASCAR&#36710;&#38431;&#25104;&#21151;&#37096;&#32626;&#30340;&#26696;&#20363;&#12290;&#25105;&#20204;&#23545;&#31995;&#32479;&#30340;&#24615;&#33021;&#21450;&#20854;&#23545;&#36710;&#38431;&#25112;&#30053;&#20915;&#31574;&#21644;&#34920;&#29616;&#30340;&#30452;&#25509;&#24433;&#21709;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents RaceLens, a novel application utilizing advanced deep learning and computer vision models for comprehensive analysis of racing photos. The developed models have demonstrated their efficiency in a wide array of tasks, including detecting racing cars, recognizing car numbers, detecting and quantifying car details, and recognizing car orientations. We discuss the process of collecting a robust dataset necessary for training our models, and describe an approach we have designed to augment and improve this dataset continually. Our method leverages a feedback loop for continuous model improvement, thus enhancing the performance and accuracy of RaceLens over time. A significant part of our study is dedicated to illustrating the practical application of RaceLens, focusing on its successful deployment by NASCAR teams over four seasons. We provide a comprehensive evaluation of our system's performance and its direct impact on the team's strategic decisions and performance met
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;SpanEx&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#20110;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#30340;&#20154;&#31867;&#26631;&#35760;&#33539;&#22260;&#20132;&#20114;&#35299;&#37322;&#30340;&#22810;&#27880;&#37322;&#32773;&#25968;&#25454;&#38598;&#12290;&#30740;&#31350;&#21457;&#29616;&#29616;&#26377;&#30340;&#35299;&#37322;&#26041;&#27861;&#24448;&#24448;&#21482;&#20851;&#27880;&#30456;&#37051;&#26631;&#35760;&#25110;&#20803;&#32452;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#32570;&#20047;&#25429;&#25417;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#20013;&#24517;&#35201;&#20132;&#20114;&#30340;&#27880;&#37322;&#12290;&#36890;&#36807;&#27604;&#36739;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#20316;&#32773;&#21457;&#29616;&#23427;&#20204;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;&#26368;&#21518;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31038;&#21306;&#26816;&#27979;&#30340;&#20803;&#24314;&#27169;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.13506</link><description>&lt;p&gt;
&#35299;&#37322;&#25991;&#26412;&#29255;&#27573;&#20043;&#38388;&#30340;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Explaining Interactions Between Text Spans. (arXiv:2310.13506v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;SpanEx&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#20110;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#30340;&#20154;&#31867;&#26631;&#35760;&#33539;&#22260;&#20132;&#20114;&#35299;&#37322;&#30340;&#22810;&#27880;&#37322;&#32773;&#25968;&#25454;&#38598;&#12290;&#30740;&#31350;&#21457;&#29616;&#29616;&#26377;&#30340;&#35299;&#37322;&#26041;&#27861;&#24448;&#24448;&#21482;&#20851;&#27880;&#30456;&#37051;&#26631;&#35760;&#25110;&#20803;&#32452;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#32570;&#20047;&#25429;&#25417;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#20013;&#24517;&#35201;&#20132;&#20114;&#30340;&#27880;&#37322;&#12290;&#36890;&#36807;&#27604;&#36739;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#20316;&#32773;&#21457;&#29616;&#23427;&#20204;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;&#26368;&#21518;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31038;&#21306;&#26816;&#27979;&#30340;&#20803;&#24314;&#27169;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#26469;&#33258;&#36755;&#20837;&#30340;&#19981;&#21516;&#37096;&#20998;&#30340;&#26631;&#35760;&#33539;&#22260;&#36827;&#34892;&#25512;&#29702;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#65292;&#20363;&#22914;&#20107;&#23454;&#26680;&#26597;&#65288;FC&#65289;&#12289;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#65288;MRC&#65289;&#25110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#39640;&#20142;&#30340;&#35299;&#37322;&#20027;&#35201;&#38598;&#20013;&#22312;&#35782;&#21035;&#20010;&#21035;&#37325;&#35201;&#26631;&#35760;&#25110;&#20165;&#22312;&#30456;&#37051;&#26631;&#35760;&#25110;&#26631;&#35760;&#20803;&#32452;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#32570;&#20047;&#25429;&#25417;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#20013;&#24517;&#35201;&#20132;&#20114;&#30340;&#27880;&#37322;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SpanEx&#65292;&#19968;&#20010;&#20851;&#20110;NLI&#21644;FC&#30340;&#20154;&#31867;&#26631;&#35760;&#33539;&#22260;&#20132;&#20114;&#35299;&#37322;&#30340;&#22810;&#27880;&#37322;&#32773;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#20010;&#31934;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36755;&#20837;&#30340;&#19981;&#21516;&#37096;&#20998;&#20043;&#38388;&#20351;&#29992;&#30340;&#36830;&#25509;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#23558;&#20854;&#19982;&#20154;&#31867;&#25512;&#29702;&#36807;&#31243;&#36827;&#34892;&#27604;&#36739;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31038;&#21306;&#26816;&#27979;&#30340;&#26080;&#30417;&#30563;&#20803;&#24314;&#27169;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning over spans of tokens from different parts of the input is essential for natural language understanding (NLU) tasks such as fact-checking (FC), machine reading comprehension (MRC) or natural language inference (NLI). However, existing highlight-based explanations primarily focus on identifying individual important tokens or interactions only between adjacent tokens or tuples of tokens. Most notably, there is a lack of annotations capturing the human decision-making process w.r.t. the necessary interactions for informed decision-making in such tasks. To bridge this gap, we introduce SpanEx, a multi-annotator dataset of human span interaction explanations for two NLU tasks: NLI and FC. We then investigate the decision-making processes of multiple fine-tuned large language models in terms of the employed connections between spans in separate parts of the input and compare them to the human reasoning processes. Finally, we present a novel community detection based unsupervised met
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;REIGN&#65292;&#36890;&#36807;&#29983;&#25104;&#35757;&#32451;&#38382;&#39064;&#30340;&#25913;&#20889;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#25351;&#23548;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#22686;&#21152;&#27169;&#22411;&#23545;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#22312;&#19981;&#21516;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#38646;-shot&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.13505</link><description>&lt;p&gt;
&#20855;&#26377;&#24378;&#21270;&#25913;&#20889;&#29983;&#25104;&#30340;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#30340;&#40065;&#26834;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Robust Training for Conversational Question Answering Models with Reinforced Reformulation Generation. (arXiv:2310.13505v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13505
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;REIGN&#65292;&#36890;&#36807;&#29983;&#25104;&#35757;&#32451;&#38382;&#39064;&#30340;&#25913;&#20889;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#25351;&#23548;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#22686;&#21152;&#27169;&#22411;&#23545;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#22312;&#19981;&#21516;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#38646;-shot&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#19978;&#30340;&#23545;&#35805;&#38382;&#31572;&#65288;ConvQA&#65289;&#27169;&#22411;&#36890;&#24120;&#22312;&#40644;&#37329;QA&#23545;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#36825;&#24847;&#21619;&#30528;&#35757;&#32451;&#20165;&#38480;&#20110;&#22312;&#30456;&#24212;&#25968;&#25454;&#38598;&#20013;&#35265;&#21040;&#30340;&#34920;&#38754;&#24418;&#24335;&#65292;&#35780;&#20272;&#20165;&#38024;&#23545;&#19968;&#23567;&#37096;&#20998;&#38382;&#39064;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#26694;&#26550;REIGN&#65292;&#25105;&#20204;&#37319;&#21462;&#20102;&#20960;&#20010;&#27493;&#39588;&#26469;&#35299;&#20915;&#36825;&#20010;&#21463;&#38480;&#30340;&#23398;&#20064;&#35774;&#32622;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#29983;&#25104;&#35757;&#32451;&#38382;&#39064;&#30340;&#25913;&#20889;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;&#36825;&#26159;&#19968;&#20010;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#36825;&#20123;&#38382;&#39064;&#30340;&#19981;&#23436;&#25972;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23558;ConvQA&#27169;&#22411;&#24341;&#23548;&#21040;&#26356;&#39640;&#30340;&#24615;&#33021;&#65292;&#21482;&#25552;&#20379;&#37027;&#20123;&#26377;&#21161;&#20110;&#25552;&#39640;&#22238;&#31572;&#36136;&#37327;&#30340;&#25913;&#20889;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19968;&#20010;&#22522;&#20934;&#19978;&#35757;&#32451;&#20027;&#35201;&#27169;&#22411;&#32452;&#20214;&#24182;&#23558;&#20854;&#38646;-shot&#24212;&#29992;&#20110;&#21478;&#19968;&#20010;&#30340;&#21487;&#34892;&#24615;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#20005;&#26684;&#35780;&#20272;&#65292;&#25105;&#20204;&#20351;&#29992;&#21644;&#37325;&#26032;&#37197;&#32622;&#21021;&#22987;&#30340;&#25913;&#20889;&#12289;&#27979;&#35797;&#35821;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models for conversational question answering (ConvQA) over knowledge graphs (KGs) are usually trained and tested on benchmarks of gold QA pairs. This implies that training is limited to surface forms seen in the respective datasets, and evaluation is on a small set of held-out questions. Through our proposed framework REIGN, we take several steps to remedy this restricted learning setup. First, we systematically generate reformulations of training questions to increase robustness of models to surface form variations. This is a particularly challenging problem, given the incomplete nature of such questions. Second, we guide ConvQA models towards higher performance by feeding it only those reformulations that help improve their answering quality, using deep reinforcement learning. Third, we demonstrate the viability of training major model components on one benchmark and applying them zero-shot to another. Finally, for a rigorous evaluation of robustness for trained models, we use and re
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21021;&#27493;&#25506;&#35752;&#20102;&#27169;&#25311;&#27604;&#20363;&#22312;&#21019;&#36896;&#21147;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#20351;&#29992;&#35789;&#23884;&#20837;&#21487;&#20197;&#26356;&#22909;&#22320;&#22522;&#20110;&#27169;&#25311;&#27604;&#20363;&#25552;&#20986;&#26032;&#30340;&#21160;&#29289;&#12290;</title><link>http://arxiv.org/abs/2310.13500</link><description>&lt;p&gt;
&#27169;&#25311;&#27604;&#20363;&#19982;&#21019;&#36896;&#21147;&#65306;&#21021;&#27493;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Analogical Proportions and Creativity: A Preliminary Study. (arXiv:2310.13500v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21021;&#27493;&#25506;&#35752;&#20102;&#27169;&#25311;&#27604;&#20363;&#22312;&#21019;&#36896;&#21147;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#20351;&#29992;&#35789;&#23884;&#20837;&#21487;&#20197;&#26356;&#22909;&#22320;&#22522;&#20110;&#27169;&#25311;&#27604;&#20363;&#25552;&#20986;&#26032;&#30340;&#21160;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#27604;&#20363;&#26159;&#24418;&#24335;&#20026;&#8220;$a$&#23545;$b$&#22914;&#21516;$c$&#23545;$d$&#8221;&#30340;&#38472;&#36848;&#65292;&#34920;&#36798;&#20102;&#20803;&#32032;&#23545;$(a,b)$&#21644;&#23545;$(c,d)$&#30340;&#27604;&#36739;&#24471;&#20986;&#30456;&#20284;&#32467;&#26524;&#12290;&#27169;&#25311;&#27604;&#20363;&#26159;&#21019;&#36896;&#24615;&#30340;&#65292;&#22240;&#20026;&#22312;&#32473;&#23450;3&#20010;&#19981;&#21516;&#30340;&#39033;&#30446;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#26681;&#25454;&#19968;&#23450;&#26465;&#20214;&#35745;&#31639;&#20986;&#19982;&#20043;&#21069;&#30340;&#39033;&#30446;&#19981;&#21516;&#30340;&#31532;4&#20010;&#39033;&#30446;$d$&#30340;&#34920;&#31034;&#65292;&#20351;&#20854;&#19982;&#20043;&#24418;&#25104;&#27169;&#25311;&#27604;&#20363;&#12290;&#22312;&#20171;&#32461;&#27169;&#25311;&#27604;&#20363;&#21450;&#20854;&#29305;&#24615;&#21518;&#65292;&#35770;&#25991;&#25253;&#21578;&#20102;&#20351;&#29992;&#21160;&#29289;&#25551;&#36848;&#21644;&#31867;&#21035;&#30340;&#25968;&#25454;&#24211;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#35797;&#22270;&#20174;&#29616;&#26377;&#21160;&#29289;&#20013;&#8220;&#21019;&#36896;&#8221;&#26032;&#30340;&#21160;&#29289;&#65292;&#22914;&#40493;&#22068;&#20861;&#31561;&#32597;&#35265;&#21160;&#29289;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#30340;&#23454;&#39564;&#65292;&#20351;&#29992;&#35789;&#23884;&#20837;&#21644;&#24067;&#23572;&#29305;&#24449;&#26469;&#22522;&#20110;&#27169;&#25311;&#27604;&#20363;&#25552;&#20986;&#26032;&#30340;&#21160;&#29289;&#65292;&#32467;&#26524;&#34920;&#26126;&#35789;&#23884;&#20837;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analogical proportions are statements of the form "$a$ is to $b$ as $c$ is to $d$", which expresses that the comparisons of the elements in pair $(a, b)$ and in pair $(c, d)$ yield similar results. Analogical proportions are creative in the sense that given 3 distinct items, the representation of a 4th item $d$, distinct from the previous items, which forms an analogical proportion with them can be calculated, provided certain conditions are met. After providing an introduction to analogical proportions and their properties, the paper reports the results of an experiment made with a database of animal descriptions and their class, where we try to "create" new animals from existing ones, retrieving rare animals such as platypus. We perform a series of experiments using word embeddings as well as Boolean features in order to propose novel animals based on analogical proportions, showing that word embeddings obtain better results.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#20013;&#23548;&#33268;&#39044;&#27979;&#19981;&#31283;&#23450;&#21644;&#19981;&#19968;&#33268;&#30340;&#35774;&#35745;&#36873;&#25321;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#20840;&#38754;&#35780;&#20272;&#19981;&#21516;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#30830;&#23450;&#20102;&#23545;&#39044;&#27979;&#24433;&#21709;&#26368;&#22823;&#12289;&#20855;&#26377;&#20132;&#20114;&#24615;&#25110;&#31283;&#23450;&#24615;&#30340;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2310.13486</link><description>&lt;p&gt;
&#30041;&#24847;&#25351;&#20196;&#65306;&#23545;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#20013;&#19968;&#33268;&#24615;&#21644;&#20132;&#20114;&#24615;&#30340;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Mind the instructions: a holistic evaluation of consistency and interactions in prompt-based learning. (arXiv:2310.13486v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13486
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#20013;&#23548;&#33268;&#39044;&#27979;&#19981;&#31283;&#23450;&#21644;&#19981;&#19968;&#33268;&#30340;&#35774;&#35745;&#36873;&#25321;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#20840;&#38754;&#35780;&#20272;&#19981;&#21516;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#30830;&#23450;&#20102;&#23545;&#39044;&#27979;&#24433;&#21709;&#26368;&#22823;&#12289;&#20855;&#26377;&#20132;&#20114;&#24615;&#25110;&#31283;&#23450;&#24615;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#23547;&#25214;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#21040;&#20219;&#21153;&#30340;&#26368;&#20339;&#26041;&#24335;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#19982;&#20808;&#21069;&#19968;&#20195;&#30340;&#20219;&#21153;&#35843;&#25972;&#27169;&#22411;&#65288;TT&#65289;&#31867;&#20284;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#36866;&#24212;&#20219;&#21153;&#30340;&#27169;&#22411;&#22312;&#26576;&#20123;&#35774;&#32622;&#19979;&#26159;&#20581;&#22766;&#30340;&#65292;&#20294;&#22312;&#20854;&#20182;&#35774;&#32622;&#19979;&#19981;&#26159;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23545;&#23548;&#33268;&#32447;&#24615;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39044;&#27979;&#19981;&#31283;&#23450;&#21644;&#19981;&#19968;&#33268;&#30340;&#35774;&#35745;&#36873;&#25321;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36755;&#20837;&#20998;&#24067;&#19982;&#26631;&#31614;&#20043;&#38388;&#30340;&#20266;&#30456;&#20851;&#24615;&#22312;&#25552;&#31034;&#27169;&#22411;&#20013;&#21482;&#26159;&#19968;&#20010;&#27425;&#35201;&#38382;&#39064;&#65292;&#32780;&#23545;&#20110;TT&#27169;&#22411;&#26469;&#35828;&#26159;&#24050;&#30693;&#30340;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23545;&#25552;&#31034;&#35774;&#32622;&#20013;&#24433;&#21709;&#39044;&#27979;&#30340;&#19981;&#21516;&#22240;&#32032;&#30340;&#31995;&#32479;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#35268;&#27169;&#30340;&#21407;&#22987;&#21644;&#25351;&#20196;&#35843;&#25972;&#65288;IT&#65289;LLMs&#19978;&#27979;&#35797;&#20102;&#25152;&#26377;&#21487;&#33021;&#30340;&#22240;&#32032;&#32452;&#21512;&#65292;&#24182;&#23545;&#32467;&#26524;&#36827;&#34892;&#32479;&#35745;&#20998;&#26512;&#65292;&#20197;&#23637;&#31034;&#21738;&#20123;&#22240;&#32032;&#26368;&#20855;&#24433;&#21709;&#21147;&#12289;&#20132;&#20114;&#24615;&#25110;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#20102;&#21738;&#20123;&#22240;&#32032;&#21487;&#20197;&#22312;&#19981;&#21152;&#39044;&#38450;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#65292;&#21738;&#20123;&#22240;&#32032;&#38656;&#35201;&#39044;&#38450;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding the best way of adapting pre-trained language models to a task is a big challenge in current NLP. Just like the previous generation of task-tuned models (TT), models that are adapted to tasks via in-context-learning (ICL) are robust in some setups but not in others. Here, we present a detailed analysis of which design choices cause instabilities and inconsistencies in LLM predictions. First, we show how spurious correlations between input distributions and labels -- a known issue in TT models -- form only a minor problem for prompted models. Then, we engage in a systematic, holistic evaluation of different factors that have been found to influence predictions in a prompting setup. We test all possible combinations of a range of factors on both vanilla and instruction-tuned (IT) LLMs of different scale and statistically analyse the results to show which factors are the most influential, interactive or stable. Our results show which factors can be used without precautions and whi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#30044;&#29287;&#21160;&#29289;&#34892;&#20026;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#21457;&#29616;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#22312;&#30044;&#29287;&#19994;&#20013;&#30340;&#28508;&#22312;&#20915;&#31574;&#24037;&#20855;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.13483</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#30044;&#29287;&#21160;&#29289;&#34892;&#20026;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Application of deep learning for livestock behaviour recognition: A systematic literature review. (arXiv:2310.13483v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#30044;&#29287;&#21160;&#29289;&#34892;&#20026;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#21457;&#29616;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#22312;&#30044;&#29287;&#19994;&#20013;&#30340;&#28508;&#22312;&#20915;&#31574;&#24037;&#20855;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#30044;&#29287;&#21160;&#29289;&#30340;&#20581;&#24247;&#21644;&#31119;&#21033;&#30417;&#27979;&#26159;&#19968;&#39033;&#20154;&#21147;&#23494;&#38598;&#22411;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#23548;&#33268;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#30340;&#24341;&#20837;&#65292;&#23588;&#20854;&#26159;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20316;&#20026;&#30044;&#29287;&#19994;&#30340;&#20915;&#31574;&#24037;&#20855;&#12290;&#36825;&#20123;&#27169;&#22411;&#24050;&#34987;&#29992;&#20110;&#21160;&#29289;&#35782;&#21035;&#12289;&#36861;&#36394;&#12289;&#36523;&#20307;&#37096;&#20301;&#35782;&#21035;&#21644;&#29289;&#31181;&#20998;&#31867;&#31561;&#20219;&#21153;&#12290;&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#20154;&#20204;&#23545;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#26469;&#25506;&#32034;&#30044;&#29287;&#21160;&#29289;&#34892;&#20026;&#19982;&#20581;&#24247;&#38382;&#39064;&#20043;&#38388;&#30340;&#20851;&#31995;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#32508;&#36848;&#30740;&#31350;&#27604;&#36739;&#36890;&#29992;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#19968;&#39033;&#19987;&#38376;&#30740;&#31350;&#30044;&#29287;&#21160;&#29289;&#34892;&#20026;&#35782;&#21035;&#30340;&#32508;&#36848;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#36827;&#34892;&#20102;&#36825;&#39033;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;(SLR)&#12290;SLR&#36890;&#36807;&#23545;&#30005;&#23376;&#25968;&#25454;&#24211;&#36827;&#34892;&#21021;&#27493;&#25628;&#32034;&#65292;&#20849;&#24471;&#21040;1101&#31687;&#20986;&#29256;&#29289;&#12290;&#32463;&#36807;&#23450;&#20041;&#30340;&#36873;&#25321;&#26631;&#20934;&#31579;&#36873;&#21518;&#65292;&#31579;&#36873;&#20986;126&#31687;&#20986;&#29256;&#29289;&#12290;&#36825;&#20123;&#20986;&#29256;&#29289;&#22522;&#20110;&#36136;&#37327;&#21644;&#30456;&#20851;&#24615;&#36827;&#34892;&#36827;&#19968;&#27493;&#31579;&#36873;&#65292;&#20197;&#26368;&#32456;&#33719;&#24471;&#26368;&#30456;&#20851;&#30340;&#30740;&#31350;&#25991;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Livestock health and welfare monitoring has traditionally been a labor-intensive task performed manually. Recent advances have led to the adoption of AI and computer vision techniques, particularly deep learning models, as decision-making tools within the livestock industry. These models have been employed for tasks like animal identification, tracking, body part recognition, and species classification. In the past decade, there has been a growing interest in using these models to explore the connection between livestock behaviour and health issues. While previous review studies have been rather generic, there is currently no review study specifically focusing on DL for livestock behaviour recognition. Hence, this systematic literature review (SLR) was conducted. The SLR involved an initial search across electronic databases, resulting in 1101 publications. After applying defined selection criteria, 126 publications were shortlisted. These publications were further filtered based on qu
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#28165;&#29702;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#22122;&#22768;&#36755;&#20837;&#65292;&#36890;&#36807;&#20174;MTNT&#25968;&#25454;&#38598;&#20013;&#28165;&#29702;&#30446;&#26631;&#35821;&#21477;&#30340;&#22122;&#22768;&#65292;&#29983;&#25104;&#20102;C-MTNT&#25968;&#25454;&#38598;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2310.13469</link><description>&lt;p&gt;
&#35753;&#35821;&#35328;&#27169;&#22411;&#28165;&#29702;&#24744;&#30340;&#26377;&#22122;&#38899;&#30340;&#32763;&#35793;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Ask Language Model to Clean Your Noisy Translation Data. (arXiv:2310.13469v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13469
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#28165;&#29702;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#22122;&#22768;&#36755;&#20837;&#65292;&#36890;&#36807;&#20174;MTNT&#25968;&#25454;&#38598;&#20013;&#28165;&#29702;&#30446;&#26631;&#35821;&#21477;&#30340;&#22122;&#22768;&#65292;&#29983;&#25104;&#20102;C-MTNT&#25968;&#25454;&#38598;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#20013;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#22122;&#22768;&#36755;&#20837;&#30340;&#33030;&#24369;&#24615;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#20174;&#22122;&#22768;&#36755;&#20837;&#20013;&#29983;&#25104;&#24178;&#20928;&#30340;&#36755;&#20986;&#33267;&#20851;&#37325;&#35201;&#12290;MTNT&#25968;&#25454;&#38598;&#34987;&#24191;&#27867;&#29992;&#20316;&#35780;&#20272;NMT&#27169;&#22411;&#23545;&#22122;&#22768;&#36755;&#20837;&#40065;&#26834;&#24615;&#30340;&#22522;&#20934;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28304;&#35821;&#21477;&#21644;&#30446;&#26631;&#35821;&#21477;&#20013;&#37117;&#23384;&#22312;&#22122;&#22768;&#65292;&#20854;&#23454;&#29992;&#24615;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#28165;&#29702;MTNT&#20013;&#30446;&#26631;&#35821;&#21477;&#30340;&#22122;&#22768;&#65292;&#20351;&#20854;&#26356;&#36866;&#29992;&#20110;&#22122;&#22768;&#35780;&#20272;&#30340;&#22522;&#20934;&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23427;&#20204;&#22312;&#21435;&#22122;&#26041;&#38754;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;&#23427;&#20204;&#21487;&#20197;&#22312;&#32771;&#34385;&#35821;&#20041;&#21547;&#20041;&#30340;&#21516;&#26102;&#21024;&#38500;&#34920;&#24773;&#31526;&#21495;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;LLM&#33021;&#22815;&#26377;&#25928;&#22320;&#26356;&#25913;&#20442;&#35821;&#12289;&#26415;&#35821;&#21644;&#31895;&#21475;&#12290;&#24471;&#21040;&#30340;&#25968;&#25454;&#38598;&#34987;&#31216;&#20026;C-MTNT&#65292;&#22122;&#22768;&#26174;&#33879;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models have demonstrated remarkable performance in neural machine translation (NMT). However, their vulnerability to noisy input poses a significant challenge in practical implementation, where generating clean output from noisy input is crucial. The MTNT dataset \cite{MTNT} is widely used as a benchmark for evaluating the robustness of NMT models against noisy input. Nevertheless, its utility is limited due to the presence of noise in both the source and target sentences. To address this limitation, we focus on cleaning the noise from the target sentences in MTNT, making it more suitable as a benchmark for noise evaluation. Leveraging the capabilities of large language models (LLMs), we observe their impressive abilities in noise removal. For example, they can remove emojis while considering their semantic meaning. Additionally, we show that LLM can effectively rephrase slang, jargon, and profanities. The resulting datasets, called C-MTNT, exhibit significantly less noise 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#36229;&#20687;&#32032;&#32467;&#26500;&#24046;&#24322;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;MDGCN&#65289;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#34920;&#24449;&#65292;&#36890;&#36807;&#32858;&#31867;&#24863;&#30693;&#30456;&#20284;&#20687;&#32032;&#65292;&#20943;&#23569;&#20102;&#21518;&#32493;&#22788;&#29702;&#30340;&#35270;&#35273;&#22522;&#20803;&#25968;&#37327;&#65292;&#24182;&#25366;&#25496;&#20102;&#26356;&#31934;&#30830;&#30340;&#25299;&#25169;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.13447</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#36229;&#20687;&#32032;&#32467;&#26500;&#24046;&#24322;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Multiscale Superpixel Structured Difference Graph Convolutional Network for VL Representation. (arXiv:2310.13447v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#36229;&#20687;&#32032;&#32467;&#26500;&#24046;&#24322;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;MDGCN&#65289;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#34920;&#24449;&#65292;&#36890;&#36807;&#32858;&#31867;&#24863;&#30693;&#30456;&#20284;&#20687;&#32032;&#65292;&#20943;&#23569;&#20102;&#21518;&#32493;&#22788;&#29702;&#30340;&#35270;&#35273;&#22522;&#20803;&#25968;&#37327;&#65292;&#24182;&#25366;&#25496;&#20102;&#26356;&#31934;&#30830;&#30340;&#25299;&#25169;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#39046;&#22495;&#20013;&#65292;&#25972;&#21512;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#20851;&#38190;&#22312;&#20110;&#24314;&#31435;&#19968;&#20010;&#33391;&#22909;&#30340;&#23545;&#40784;&#31574;&#30053;&#12290;&#26368;&#36817;&#65292;&#21463;&#21040;&#33258;&#30417;&#30563;&#23398;&#20064;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#22810;&#27169;&#24577;&#35821;&#20041;&#34920;&#24449;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#35270;&#35273;&#35821;&#20041;&#34920;&#24449;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#24403;&#21069;&#22522;&#20110;&#20687;&#32032;&#25110;&#22359;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#25552;&#21462;&#22797;&#26434;&#22330;&#26223;&#36793;&#30028;&#26041;&#38754;&#23384;&#22312;&#31354;&#38388;&#35821;&#20041;&#36830;&#36143;&#24615;&#19981;&#36275;&#21644;&#23545;&#22122;&#22768;&#30340;&#33030;&#24369;&#24615;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#23558;&#36229;&#20687;&#32032;&#20316;&#20026;&#21487;&#23398;&#20064;&#22270;&#20687;&#25968;&#25454;&#30340;&#32508;&#21512;&#32039;&#20945;&#34920;&#24449;&#65292;&#36890;&#36807;&#23545;&#24863;&#30693;&#30456;&#20284;&#20687;&#32032;&#36827;&#34892;&#32858;&#31867;&#65292;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#21518;&#32493;&#22788;&#29702;&#30340;&#35270;&#35273;&#22522;&#20803;&#25968;&#37327;&#12290;&#20026;&#20102;&#25366;&#25496;&#26356;&#31934;&#30830;&#30340;&#25299;&#25169;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#24046;&#24322;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;MDGCN&#65289;&#12290;&#23427;&#23558;&#25972;&#20010;&#22270;&#20687;&#35299;&#26512;&#20026;&#32454;&#21040;&#31895;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25972;&#20010;&#22270;&#20687;&#30340;&#35299;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Within the multimodal field, the key to integrating vision and language lies in establishing a good alignment strategy. Recently, benefiting from the success of self-supervised learning, significant progress has been made in multimodal semantic representation based on pre-trained models for vision and language. However, there is still room for improvement in visual semantic representation. The lack of spatial semantic coherence and vulnerability to noise makes it challenging for current pixel or patch-based methods to accurately extract complex scene boundaries. To this end, this paper develops superpixel as a comprehensive compact representation of learnable image data, which effectively reduces the number of visual primitives for subsequent processing by clustering perceptually similar pixels. To mine more precise topological relations, we propose a Multiscale Difference Graph Convolutional Network (MDGCN). It parses the entire image as a fine-to-coarse hierarchical structure of cons
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#26865;&#20004;&#21487;&#38382;&#39064;&#20013;&#30340;&#33258;&#19968;&#33268;&#24615;&#23384;&#22312;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#29305;&#21035;&#35757;&#32451;&#20063;&#33021;&#22312;&#31283;&#20581;&#24615;&#26816;&#26597;&#20013;&#20445;&#25345;&#33258;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13439</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#26865;&#20004;&#21487;&#24773;&#20917;&#19979;&#30340;&#33258;&#19968;&#33268;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Self-Consistency of Large Language Models under Ambiguity. (arXiv:2310.13439v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13439
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#26865;&#20004;&#21487;&#38382;&#39064;&#20013;&#30340;&#33258;&#19968;&#33268;&#24615;&#23384;&#22312;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#29305;&#21035;&#35757;&#32451;&#20063;&#33021;&#22312;&#31283;&#20581;&#24615;&#26816;&#26597;&#20013;&#20445;&#25345;&#33258;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38656;&#35201;&#19968;&#33268;&#24615;&#30340;&#20219;&#21153;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#19979;&#32473;&#20986;&#19981;&#19968;&#33268;&#30340;&#31572;&#26696;&#26159;&#26377;&#38382;&#39064;&#30340;&#65292;&#20363;&#22914;&#38382;&#31572;&#12289;&#35299;&#37322;&#31561;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#33258;&#19968;&#33268;&#24615;&#30340;&#22522;&#20934;&#65292;&#38024;&#23545;&#23384;&#22312;&#20004;&#20010;&#25110;&#22810;&#20010;&#27491;&#30830;&#31572;&#26696;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#23545;OpenAI&#27169;&#22411;&#22871;&#20214;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#34892;&#20026;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#27169;&#26865;&#20004;&#21487;&#30340;&#25972;&#25968;&#24207;&#21015;&#34917;&#20840;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#24179;&#22343;&#19968;&#33268;&#24615;&#33539;&#22260;&#20174;67&#65285;&#21040;82&#65285;&#19981;&#31561;&#65292;&#36828;&#36828;&#39640;&#20110;&#19968;&#20010;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#22914;&#26524;&#26159;&#38543;&#26426;&#30340;&#35805;&#25152;&#33021;&#39044;&#27979;&#30340;&#27700;&#24179;&#65292;&#24182;&#19988;&#38543;&#30528;&#27169;&#22411;&#33021;&#21147;&#30340;&#25552;&#21319;&#32780;&#22686;&#21152;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#31283;&#20581;&#24615;&#26816;&#26597;&#20013;&#37117;&#20542;&#21521;&#20110;&#20445;&#25345;&#33258;&#19968;&#33268;&#24615;&#65292;&#21253;&#25324;&#25552;&#31034;&#35828;&#35805;&#32773;&#21464;&#21270;&#21644;&#24207;&#21015;&#38271;&#24230;&#21464;&#21270;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#33258;&#19968;&#33268;&#24615;&#26159;&#19968;&#31181;&#27809;&#26377;&#29305;&#21035;&#35757;&#32451;&#20063;&#33021;&#20135;&#29983;&#30340;&#26032;&#33021;&#21147;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#21028;&#26029;&#33258;&#36523;&#19968;&#33268;&#24615;&#26102;&#32570;&#23569;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) that do not give consistent answers across contexts are problematic when used for tasks with expectations of consistency, e.g., question-answering, explanations, etc. Our work presents an evaluation benchmark for self-consistency in cases of under-specification where two or more answers can be correct. We conduct a series of behavioral experiments on the OpenAI model suite using an ambiguous integer sequence completion task. We find that average consistency ranges from 67\% to 82\%, far higher than would be predicted if a model's consistency was random, and increases as model capability improves. Furthermore, we show that models tend to maintain self-consistency across a series of robustness checks, including prompting speaker changes and sequence length changes. These results suggest that self-consistency arises as an emergent capability without specifically training for it. Despite this, we find that models are uncalibrated when judging their own consiste
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#26694;&#26550;&#65292;&#22312;&#20302;&#23494;&#24230;&#20998;&#31163;&#20551;&#35774;&#19979;&#20998;&#26512;&#39640;&#32500;&#24773;&#20917;&#19979;&#30340;&#21322;&#30417;&#30563;&#20998;&#31867;&#65292;&#24182;&#20171;&#32461;&#20102;QLDS&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#24179;&#34913;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20043;&#38388;&#24314;&#31435;&#20102;&#19968;&#20010;&#24179;&#28369;&#30340;&#26725;&#26753;&#12290;&#36890;&#36807;&#24212;&#29992;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#22312;&#28176;&#36817;&#24773;&#20917;&#19979;&#20998;&#31867;&#38169;&#35823;&#30340;&#29702;&#35770;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#25214;&#21040;&#26368;&#20339;&#24179;&#34913;&#28857;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.13434</link><description>&lt;p&gt;
&#38543;&#26426;&#30697;&#38453;&#20998;&#26512;&#22312;&#20302;&#23494;&#24230;&#20998;&#31163;&#20551;&#35774;&#19979;&#24179;&#34913;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#20043;&#38388;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Random Matrix Analysis to Balance between Supervised and Unsupervised Learning under the Low Density Separation Assumption. (arXiv:2310.13434v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#26694;&#26550;&#65292;&#22312;&#20302;&#23494;&#24230;&#20998;&#31163;&#20551;&#35774;&#19979;&#20998;&#26512;&#39640;&#32500;&#24773;&#20917;&#19979;&#30340;&#21322;&#30417;&#30563;&#20998;&#31867;&#65292;&#24182;&#20171;&#32461;&#20102;QLDS&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#24179;&#34913;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20043;&#38388;&#24314;&#31435;&#20102;&#19968;&#20010;&#24179;&#28369;&#30340;&#26725;&#26753;&#12290;&#36890;&#36807;&#24212;&#29992;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#22312;&#28176;&#36817;&#24773;&#20917;&#19979;&#20998;&#31867;&#38169;&#35823;&#30340;&#29702;&#35770;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#25214;&#21040;&#26368;&#20339;&#24179;&#34913;&#28857;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#39640;&#32500;&#24773;&#20917;&#19979;&#20302;&#23494;&#24230;&#20998;&#31163;&#20551;&#35774;&#19979;&#30340;&#21322;&#30417;&#30563;&#20998;&#31867;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;QLDS&#65292;&#19968;&#20010;&#32447;&#24615;&#20998;&#31867;&#27169;&#22411;&#65292;&#20854;&#20013;&#36890;&#36807;&#20108;&#27425;&#36793;&#30028;&#26368;&#22823;&#21270;&#26469;&#23454;&#29616;&#20302;&#23494;&#24230;&#20998;&#31163;&#20551;&#35774;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#26126;&#30830;&#30340;&#35299;&#21644;&#20016;&#23500;&#30340;&#29702;&#35770;&#23646;&#24615;&#65292;&#24182;&#19988;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#29305;&#27530;&#24773;&#20917;&#26159;&#30417;&#30563;&#24773;&#20917;&#19979;&#30340;&#26368;&#23567;&#20108;&#20056;&#25903;&#25345;&#21521;&#37327;&#26426;&#65292;&#23436;&#20840;&#26080;&#30417;&#30563;&#24773;&#20917;&#19979;&#30340;&#35889;&#32858;&#31867;&#65292;&#20197;&#21450;&#19968;&#31867;&#21322;&#30417;&#30563;&#30340;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;QLDS&#22312;&#36825;&#20123;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20043;&#38388;&#24314;&#31435;&#20102;&#19968;&#20010;&#24179;&#28369;&#30340;&#26725;&#26753;&#12290;&#21033;&#29992;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#27491;&#24335;&#25512;&#23548;&#20102;&#22312;&#28176;&#36817;&#24773;&#20917;&#19979;&#30340;&#20998;&#31867;&#35823;&#24046;&#30340;&#29702;&#35770;&#35780;&#20272;&#12290;&#20316;&#20026;&#19968;&#20010;&#24212;&#29992;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#31181;&#36229;&#21442;&#25968;&#36873;&#25321;&#31574;&#30053;&#65292;&#20197;&#25214;&#21040;&#26368;&#20339;&#30340;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a theoretical framework to analyze semi-supervised classification under the low density separation assumption in a high-dimensional regime. In particular, we introduce QLDS, a linear classification model, where the low density separation assumption is implemented via quadratic margin maximization. The algorithm has an explicit solution with rich theoretical properties, and we show that particular cases of our algorithm are the least-square support vector machine in the supervised case, the spectral clustering in the fully unsupervised regime, and a class of semi-supervised graph-based approaches. As such, QLDS establishes a smooth bridge between these supervised and unsupervised learning methods. Using recent advances in the random matrix theory, we formally derive a theoretical evaluation of the classification error in the asymptotic regime. As an application, we derive a hyperparameter selection policy that finds the best balance between the supervised and the unsupervised
&lt;/p&gt;</description></item><item><title>FLTracer&#26159;&#31532;&#19968;&#20010;FL&#25915;&#20987;&#26469;&#28304;&#36861;&#36394;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#20934;&#30830;&#24615;&#22320;&#26816;&#27979;&#21508;&#31181;&#25915;&#20987;&#65292;&#24182;&#36861;&#36394;&#25915;&#20987;&#30340;&#26102;&#38388;&#12289;&#30446;&#26631;&#12289;&#31867;&#22411;&#21644;&#34987;&#27602;&#21270;&#30340;&#20301;&#32622;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;FLTracer&#21033;&#29992;&#26412;&#22320;&#20449;&#24687;&#21644;&#20840;&#23616;&#20449;&#24687;&#30340;&#32479;&#35745;&#20998;&#26512;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.13424</link><description>&lt;p&gt;
FLTracer: &#39640;&#20934;&#30830;&#24615;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27602;&#21270;&#25915;&#20987;&#26469;&#28304;&#36861;&#36394;
&lt;/p&gt;
&lt;p&gt;
FLTracer: Accurate Poisoning Attack Provenance in Federated Learning. (arXiv:2310.13424v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13424
&lt;/p&gt;
&lt;p&gt;
FLTracer&#26159;&#31532;&#19968;&#20010;FL&#25915;&#20987;&#26469;&#28304;&#36861;&#36394;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#20934;&#30830;&#24615;&#22320;&#26816;&#27979;&#21508;&#31181;&#25915;&#20987;&#65292;&#24182;&#36861;&#36394;&#25915;&#20987;&#30340;&#26102;&#38388;&#12289;&#30446;&#26631;&#12289;&#31867;&#22411;&#21644;&#34987;&#27602;&#21270;&#30340;&#20301;&#32622;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;FLTracer&#21033;&#29992;&#26412;&#22320;&#20449;&#24687;&#21644;&#20840;&#23616;&#20449;&#24687;&#30340;&#32479;&#35745;&#20998;&#26512;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#22810;&#20010;&#23458;&#25143;&#31471;&#21327;&#21516;&#35757;&#32451;&#20849;&#20139;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;FL&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#27602;&#21270;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#21487;&#20197;&#38477;&#20302;&#20840;&#23616;&#27169;&#22411;&#30340;&#24615;&#33021;&#25110;&#22312;&#20854;&#20013;&#24341;&#20837;&#21518;&#38376;&#12290;&#26412;&#25991;&#39318;&#20808;&#23545;&#20808;&#21069;&#30340;FL&#25915;&#20987;&#21644;&#26816;&#27979;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#26377;&#29616;&#26377;&#30340;&#26816;&#27979;&#26041;&#27861;&#21482;&#23545;&#26377;&#38480;&#21644;&#29305;&#23450;&#30340;&#25915;&#20987;&#26377;&#25928;&#12290;&#22823;&#22810;&#25968;&#26816;&#27979;&#26041;&#27861;&#23384;&#22312;&#39640;&#35823;&#25253;&#29575;&#65292;&#29305;&#21035;&#26159;&#22312;&#38750;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#65288;non-IID&#65289;&#30340;&#35774;&#32622;&#20013;&#65292;&#23548;&#33268;&#24615;&#33021;&#20005;&#37325;&#38477;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FLTracer&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;FL&#25915;&#20987;&#26469;&#28304;&#36861;&#36394;&#26694;&#26550;&#65292;&#21487;&#20197;&#20934;&#30830;&#26816;&#27979;&#21508;&#31181;&#25915;&#20987;&#24182;&#36861;&#36394;&#25915;&#20987;&#30340;&#26102;&#38388;&#12289;&#30446;&#26631;&#12289;&#31867;&#22411;&#21644;&#34987;&#27602;&#21270;&#30340;&#20301;&#32622;&#12290;&#19982;&#29616;&#26377;&#30340;&#20165;&#20381;&#36182;&#20110;&#36328;&#23458;&#25143;&#31471;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#20449;&#24687;&#21644;&#20840;&#23616;&#20449;&#24687;&#30340;&#32479;&#35745;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a promising distributed learning approach that enables multiple clients to collaboratively train a shared global model. However, recent studies show that FL is vulnerable to various poisoning attacks, which can degrade the performance of global models or introduce backdoors into them. In this paper, we first conduct a comprehensive study on prior FL attacks and detection methods. The results show that all existing detection methods are only effective against limited and specific attacks. Most detection methods suffer from high false positives, which lead to significant performance degradation, especially in not independent and identically distributed (non-IID) settings. To address these issues, we propose FLTracer, the first FL attack provenance framework to accurately detect various attacks and trace the attack time, objective, type, and poisoned location of updates. Different from existing methodologies that rely solely on cross-client anomaly detection, we
&lt;/p&gt;</description></item><item><title>POSQA&#26159;&#19968;&#20010;&#29992;&#26469;&#25506;&#32034;LLMs&#22312;&#20855;&#36523;&#29702;&#35299;&#26041;&#38754;&#30340;&#29289;&#20307;&#22823;&#23567;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20102;LLMs&#22312;&#38646;-shot&#24773;&#26223;&#19979;&#30340;&#34920;&#29616;&#65292;&#36890;&#36807;&#25512;&#21160;&#25216;&#26415;&#21644;&#22806;&#37096;&#30693;&#35782;&#22686;&#24378;&#20102;&#23427;&#20204;&#30340;&#26497;&#38480;&#65292;&#21516;&#26102;&#20998;&#26512;&#20102;&#20854;&#29616;&#23454;&#19990;&#30028;&#29702;&#35299;&#30340;&#26469;&#28304;&#21644;&#25552;&#31034;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.13394</link><description>&lt;p&gt;
POSQA&#65306;&#20351;&#29992;&#23610;&#23544;&#27604;&#36739;&#25506;&#32034;LLMs&#30340;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
POSQA: Probe the World Models of LLMs with Size Comparisons. (arXiv:2310.13394v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13394
&lt;/p&gt;
&lt;p&gt;
POSQA&#26159;&#19968;&#20010;&#29992;&#26469;&#25506;&#32034;LLMs&#22312;&#20855;&#36523;&#29702;&#35299;&#26041;&#38754;&#30340;&#29289;&#20307;&#22823;&#23567;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20102;LLMs&#22312;&#38646;-shot&#24773;&#26223;&#19979;&#30340;&#34920;&#29616;&#65292;&#36890;&#36807;&#25512;&#21160;&#25216;&#26415;&#21644;&#22806;&#37096;&#30693;&#35782;&#22686;&#24378;&#20102;&#23427;&#20204;&#30340;&#26497;&#38480;&#65292;&#21516;&#26102;&#20998;&#26512;&#20102;&#20854;&#29616;&#23454;&#19990;&#30028;&#29702;&#35299;&#30340;&#26469;&#28304;&#21644;&#25552;&#31034;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#36523;&#21270;&#35821;&#35328;&#29702;&#35299;&#24378;&#35843;&#35821;&#35328;&#29702;&#35299;&#19981;&#20165;&#20165;&#26159;&#22823;&#33041;&#20013;&#30340;&#24515;&#29702;&#22788;&#29702;&#38382;&#39064;&#65292;&#36824;&#28041;&#21450;&#19982;&#29289;&#29702;&#21644;&#31038;&#20250;&#29615;&#22659;&#30340;&#20114;&#21160;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#21644;&#23427;&#20204;&#24050;&#32463;&#26222;&#36941;&#23384;&#22312;&#20110;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#36234;&#26469;&#36234;&#26377;&#24517;&#35201;&#39564;&#35777;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#21463;&#35748;&#30693;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;POSQA&#65306;&#19968;&#20010;&#24102;&#26377;&#31616;&#21333;&#23610;&#23544;&#27604;&#36739;&#38382;&#39064;&#30340;&#29289;&#20307;&#22823;&#23567;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#20197;&#26816;&#39564;&#26368;&#26032;LLMs&#30340;&#20855;&#36523;&#29702;&#35299;&#30340;&#26497;&#31471;&#24615;&#24182;&#20998;&#26512;&#20854;&#28508;&#22312;&#26426;&#21046;&#12290;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#26159;&#20170;&#22825;&#30340;&#26368;&#22823;LLMs&#22312;&#38646;-shot&#24773;&#26223;&#19979;&#34920;&#29616;&#19981;&#20339;&#65292;&#24182;&#36890;&#36807;&#20808;&#36827;&#30340;&#25552;&#31034;&#25216;&#26415;&#21644;&#22806;&#37096;&#30693;&#35782;&#22686;&#24378;&#25512;&#21160;&#20102;&#23427;&#20204;&#30340;&#26497;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23427;&#20204;&#30340;&#29616;&#23454;&#19990;&#30028;&#29702;&#35299;&#20027;&#35201;&#26159;&#26469;&#33258;&#19978;&#19979;&#25991;&#20449;&#24687;&#36824;&#26159;&#20869;&#37096;&#26435;&#37325;&#65292;&#24182;&#20998;&#26512;&#20102;&#25552;&#31034;&#23545;&#20854;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embodied language comprehension emphasizes that language understanding is not solely a matter of mental processing in the brain but also involves interactions with the physical and social environment. With the explosive growth of Large Language Models (LLMs) and their already ubiquitous presence in our daily lives, it is becoming increasingly necessary to verify their real-world understanding. Inspired by cognitive theories, we propose POSQA: a Physical Object Size Question Answering dataset with simple size comparison questions to examine the extremity and analyze the potential mechanisms of the embodied comprehension of the latest LLMs.  We show that even the largest LLMs today perform poorly under the zero-shot setting. We then push their limits with advanced prompting techniques and external knowledge augmentation. Furthermore, we investigate whether their real-world comprehension primarily derives from contextual information or internal weights and analyse the impact of prompt for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DHTM&#30340;&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;&#22240;&#23376;&#22270;&#24418;&#24335;&#21644;&#22810;&#32452;&#25104;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#21033;&#29992;&#20998;&#24067;&#24335;&#34920;&#31034;&#12289;&#31232;&#30095;&#36716;&#31227;&#30697;&#38453;&#21644;&#23616;&#37096;Hebbian&#26679;&#23398;&#20064;&#35268;&#21017;&#26469;&#35299;&#20915;&#22312;&#32447;&#38544;&#34255;&#34920;&#31034;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DHTM&#22312;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#27604;&#32463;&#20856;&#30340;LSTM&#25928;&#26524;&#26356;&#22909;&#65292;&#24182;&#19982;&#26356;&#20808;&#36827;&#30340;&#31867;&#20284;RNN&#30340;&#31639;&#27861;&#24615;&#33021;&#30456;&#24403;&#65292;&#21487;&#20197;&#21152;&#36895;&#32487;&#20219;&#32773;&#34920;&#31034;&#30340;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.13391</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#24067;&#24335;Hebbian Temporal Memory&#23398;&#20064;&#32487;&#20219;&#32773;&#34920;&#31034;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning Successor Representations with Distributed Hebbian Temporal Memory. (arXiv:2310.13391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DHTM&#30340;&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;&#22240;&#23376;&#22270;&#24418;&#24335;&#21644;&#22810;&#32452;&#25104;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#21033;&#29992;&#20998;&#24067;&#24335;&#34920;&#31034;&#12289;&#31232;&#30095;&#36716;&#31227;&#30697;&#38453;&#21644;&#23616;&#37096;Hebbian&#26679;&#23398;&#20064;&#35268;&#21017;&#26469;&#35299;&#20915;&#22312;&#32447;&#38544;&#34255;&#34920;&#31034;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DHTM&#22312;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#27604;&#32463;&#20856;&#30340;LSTM&#25928;&#26524;&#26356;&#22909;&#65292;&#24182;&#19982;&#26356;&#20808;&#36827;&#30340;&#31867;&#20284;RNN&#30340;&#31639;&#27861;&#24615;&#33021;&#30456;&#24403;&#65292;&#21487;&#20197;&#21152;&#36895;&#32487;&#20219;&#32773;&#34920;&#31034;&#30340;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#32447;&#38544;&#34255;&#34920;&#31034;&#23398;&#20064;&#30340;&#25361;&#25112;&#65292;&#35813;&#26041;&#27861;&#29992;&#20110;&#22312;&#19981;&#31283;&#23450;&#30340;&#12289;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#20915;&#31574;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#65292;&#20998;&#24067;&#24335;Hebbian Temporal Memory (DHTM)&#65292;&#22522;&#20110;&#22240;&#23376;&#22270;&#24418;&#24335;&#21644;&#22810;&#32452;&#25104;&#31070;&#32463;&#20803;&#27169;&#22411;&#12290;DHTM&#26088;&#22312;&#25429;&#25417;&#39034;&#24207;&#25968;&#25454;&#20851;&#31995;&#24182;&#23545;&#26410;&#26469;&#35266;&#23519;&#20316;&#20986;&#32047;&#31215;&#39044;&#27979;&#65292;&#24418;&#25104;&#32487;&#20219;&#32773;&#34920;&#31034;&#12290;&#21463;&#26032;&#30382;&#23618;&#30340;&#31070;&#32463;&#29983;&#29702;&#23398;&#27169;&#22411;&#21551;&#21457;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20998;&#24067;&#24335;&#34920;&#31034;&#12289;&#31232;&#30095;&#36716;&#31227;&#30697;&#38453;&#21644;&#23616;&#37096;Hebbian&#26679;&#23398;&#20064;&#35268;&#21017;&#20811;&#26381;&#20102;&#20256;&#32479;&#26102;&#38388;&#35760;&#24518;&#31639;&#27861;&#65288;&#22914;RNN&#21644;HMM&#65289;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#24930;&#36895;&#23398;&#20064;&#36807;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DHTM&#20248;&#20110;&#32463;&#20856;&#30340;LSTM&#65292;&#24182;&#19982;&#26356;&#20808;&#36827;&#30340;&#31867;&#20284;RNN&#30340;&#31639;&#27861;&#24615;&#33021;&#30456;&#24403;&#65292;&#22312;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#21152;&#36895;&#20102;&#32487;&#20219;&#32773;&#34920;&#31034;&#30340;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach to address the challenge of online hidden representation learning for decision-making under uncertainty in non-stationary, partially observable environments. The proposed algorithm, Distributed Hebbian Temporal Memory (DHTM), is based on factor graph formalism and a multicomponent neuron model. DHTM aims to capture sequential data relationships and make cumulative predictions about future observations, forming Successor Representation (SR). Inspired by neurophysiological models of the neocortex, the algorithm utilizes distributed representations, sparse transition matrices, and local Hebbian-like learning rules to overcome the instability and slow learning process of traditional temporal memory algorithms like RNN and HMM. Experimental results demonstrate that DHTM outperforms classical LSTM and performs comparably to more advanced RNN-like algorithms, speeding up Temporal Difference learning for SR in changing environments. Additionally, we compare
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#26426;&#20114;&#21160;&#23398;&#20064;&#31995;&#32479;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#36890;&#36807;&#23398;&#20064;&#31526;&#21495;&#35821;&#35328;&#26469;&#35782;&#21035;&#26426;&#22120;&#20154;&#30340;&#20869;&#37096;&#31283;&#24577;&#38656;&#27714;&#12290;&#30740;&#31350;&#37319;&#29992;&#20102;&#24046;&#24322;&#32467;&#26524;&#35757;&#32451;&#21327;&#35758;&#65292;&#35777;&#26126;&#20854;&#33021;&#22815;&#25552;&#39640;&#20154;&#31867;&#30340;&#23398;&#20064;&#25928;&#29575;&#65292;&#36827;&#32780;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#26426;&#22120;&#20154;&#35821;&#35328;&#20064;&#24471;&#12290;</title><link>http://arxiv.org/abs/2310.13377</link><description>&lt;p&gt;
&#20855;&#26377;&#24773;&#24863;&#22522;&#30784;&#35821;&#35328;&#20064;&#24471;&#21644;&#24046;&#24322;&#32467;&#26524;&#35757;&#32451;&#30340;&#20154;&#26426;&#20114;&#21160;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Human-Robot Mutual Learning System with Affect-Grounded Language Acquisition and Differential Outcomes Training. (arXiv:2310.13377v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#26426;&#20114;&#21160;&#23398;&#20064;&#31995;&#32479;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#36890;&#36807;&#23398;&#20064;&#31526;&#21495;&#35821;&#35328;&#26469;&#35782;&#21035;&#26426;&#22120;&#20154;&#30340;&#20869;&#37096;&#31283;&#24577;&#38656;&#27714;&#12290;&#30740;&#31350;&#37319;&#29992;&#20102;&#24046;&#24322;&#32467;&#26524;&#35757;&#32451;&#21327;&#35758;&#65292;&#35777;&#26126;&#20854;&#33021;&#22815;&#25552;&#39640;&#20154;&#31867;&#30340;&#23398;&#20064;&#25928;&#29575;&#65292;&#36827;&#32780;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#26426;&#22120;&#20154;&#35821;&#35328;&#20064;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;&#26426;&#20114;&#21160;&#35774;&#32622;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#23398;&#20064;&#31526;&#21495;&#35821;&#35328;&#20197;&#35782;&#21035;&#26426;&#22120;&#20154;&#30340;&#20869;&#31283;&#24577;&#38656;&#27714;&#12290;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#23398;&#20064;&#20351;&#29992;&#21644;&#21709;&#24212;&#20256;&#36798;&#20869;&#31283;&#24577;&#38656;&#27714;&#21644;&#28385;&#36275;&#36825;&#20123;&#38656;&#27714;&#30340;&#21050;&#28608;&#30340;&#30456;&#21516;&#35821;&#35328;&#31526;&#21495;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#24046;&#24322;&#32467;&#26524;&#35757;&#32451;&#65288;DOT&#65289;&#21327;&#35758;&#65292;&#26426;&#22120;&#20154;&#22312;&#28385;&#36275;&#27491;&#30830;&#21050;&#28608;&#65288;&#22914;&#39292;&#24178;&#65289;&#26102;&#25552;&#20379;&#29305;&#23450;&#20110;&#20854;&#20869;&#37096;&#38656;&#27714;&#65288;&#22914;&#8220;&#39269;&#39295;&#8221;&#65289;&#30340;&#21453;&#39304;&#12290;&#25105;&#20204;&#21457;&#29616;DOT&#21487;&#20197;&#25552;&#39640;&#20154;&#31867;&#30340;&#23398;&#20064;&#25928;&#29575;&#65292;&#36827;&#32780;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#26426;&#22120;&#20154;&#35821;&#35328;&#20064;&#24471;&#12290;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#26426;&#22120;&#20154;&#20855;&#26377;&#19982;&#22788;&#20110;&#35821;&#35328;&#8220;&#21695;&#21568;&#23398;&#35821;&#8221;&#38454;&#27573;&#30340;&#20154;&#31867;&#23156;&#20799;&#31867;&#20284;&#30340;&#35789;&#27719;&#37327;&#12290;&#26426;&#22120;&#20154;&#30340;&#36719;&#20214;&#26550;&#26500;&#22522;&#20110;&#24773;&#24863;&#22522;&#30784;&#35821;&#35328;&#20064;&#24471;&#27169;&#22411;&#65292;&#36890;&#36807;&#19982;&#20154;&#31867;&#30340;&#20132;&#20114;&#23558;&#35789;&#27719;&#19982;&#20869;&#37096;&#38656;&#27714;&#65288;&#39269;&#39295;&#12289;&#21475;&#28212;&#12289;&#22909;&#22855;&#65289;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel human-robot interaction setup for robot and human learning of symbolic language for identifying robot homeostatic needs. The robot and human learn to use and respond to the same language symbols that convey homeostatic needs and the stimuli that satisfy the homeostatic needs, respectively. We adopted a differential outcomes training (DOT) protocol whereby the robot provides feedback specific (differential) to its internal needs (e.g. `hunger') when satisfied by the correct stimulus (e.g. cookie). We found evidence that DOT can enhance the human's learning efficiency, which in turn enables more efficient robot language acquisition. The robot used in the study has a vocabulary similar to that of a human infant in the linguistic ``babbling'' phase. The robot software architecture is built upon a model for affect-grounded language acquisition where the robot associates vocabulary with internal needs (hunger, thirst, curiosity) through interactions with the human
&lt;/p&gt;</description></item><item><title>VFedMH&#26159;&#19968;&#31181;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21069;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#32858;&#21512;&#21442;&#19982;&#32773;&#30340;&#23884;&#20837;&#26469;&#22788;&#29702;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#24322;&#26500;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;VFL&#26041;&#27861;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.13367</link><description>&lt;p&gt;
VFedMH: &#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#29992;&#20110;&#35757;&#32451;&#22810;&#21442;&#19982;&#26041;&#24322;&#26500;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
VFedMH: Vertical Federated Learning for Training Multi-party Heterogeneous Models. (arXiv:2310.13367v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13367
&lt;/p&gt;
&lt;p&gt;
VFedMH&#26159;&#19968;&#31181;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21069;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#32858;&#21512;&#21442;&#19982;&#32773;&#30340;&#23884;&#20837;&#26469;&#22788;&#29702;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#24322;&#26500;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;VFL&#26041;&#27861;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#65288;VFL&#65289;&#20316;&#20026;&#19968;&#31181;&#38598;&#25104;&#26679;&#26412;&#23545;&#40784;&#21644;&#29305;&#24449;&#21512;&#24182;&#30340;&#26032;&#22411;&#35757;&#32451;&#33539;&#24335;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;VFL&#26041;&#27861;&#22312;&#22788;&#29702;&#21442;&#19982;&#32773;&#20043;&#38388;&#23384;&#22312;&#24322;&#26500;&#26412;&#22320;&#27169;&#22411;&#26102;&#38754;&#20020;&#25361;&#25112;&#65292;&#36825;&#24433;&#21709;&#20102;&#20248;&#21270;&#25910;&#25947;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VFedMH&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#22810;&#26041;&#24322;&#26500;&#27169;&#22411;&#12290;VFedMH&#30340;&#37325;&#28857;&#26159;&#22312;&#21069;&#21521;&#20256;&#25773;&#26399;&#38388;&#32858;&#21512;&#27599;&#20010;&#21442;&#19982;&#32773;&#30693;&#35782;&#30340;&#23884;&#20837;&#65292;&#32780;&#19981;&#26159;&#20013;&#38388;&#32467;&#26524;&#12290;&#20027;&#21160;&#26041;&#65292;&#25317;&#26377;&#26679;&#26412;&#30340;&#26631;&#31614;&#21644;&#29305;&#24449;&#65292;&#22312;VFedMH&#20013;&#23433;&#20840;&#22320;&#32858;&#21512;&#26412;&#22320;&#23884;&#20837;&#20197;&#33719;&#24471;&#20840;&#23616;&#30693;&#35782;&#23884;&#20837;&#65292;&#24182;&#23558;&#20854;&#21457;&#36865;&#32473;&#34987;&#21160;&#26041;&#12290;&#34987;&#21160;&#26041;&#20165;&#25317;&#26377;&#26679;&#26412;&#30340;&#29305;&#24449;&#65292;&#28982;&#21518;&#21033;&#29992;&#20840;&#23616;&#23884;&#20837;&#22312;&#20854;&#26412;&#22320;&#24322;&#26500;&#32593;&#32476;&#19978;&#36827;&#34892;&#21069;&#21521;&#20256;&#25773;&#12290;&#28982;&#32780;&#65292;&#34987;&#21160;&#26041;&#19981;&#25317;&#26377;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vertical Federated Learning (VFL) has gained increasing attention as a novel training paradigm that integrates sample alignment and feature union. However, existing VFL methods face challenges when dealing with heterogeneous local models among participants, which affects optimization convergence and generalization. To address this issue, this paper proposes a novel approach called Vertical Federated learning for training Multi-parties Heterogeneous models (VFedMH). VFedMH focuses on aggregating the embeddings of each participant's knowledge instead of intermediate results during forward propagation. The active party, who possesses labels and features of the sample, in VFedMH securely aggregates local embeddings to obtain global knowledge embeddings, and sends them to passive parties. The passive parties, who own only features of the sample, then utilize the global embeddings to propagate forward on their local heterogeneous networks. However, the passive party does not own the labels, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#35821;&#32763;&#35793;&#23545;&#29983;&#25104;&#30340;&#34892;&#20026;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#35786;&#26029;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#36890;&#29992;&#38169;&#35823;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#33258;&#21160;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#27979;&#35797;&#29992;&#20363;&#21450;&#20854;&#20266;&#21442;&#32771;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#20154;&#24037;&#21442;&#32771;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.13362</link><description>&lt;p&gt;
&#36890;&#36807;&#34892;&#20026;&#27979;&#35797;&#23454;&#29616;&#23545;&#26426;&#22120;&#32763;&#35793;&#30340;&#36890;&#29992;&#38169;&#35823;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Towards General Error Diagnosis via Behavioral Testing in Machine Translation. (arXiv:2310.13362v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#35821;&#32763;&#35793;&#23545;&#29983;&#25104;&#30340;&#34892;&#20026;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#35786;&#26029;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#36890;&#29992;&#38169;&#35823;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#33258;&#21160;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#27979;&#35797;&#29992;&#20363;&#21450;&#20854;&#20266;&#21442;&#32771;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#20154;&#24037;&#21442;&#32771;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#20026;&#27979;&#35797;&#20026;&#35786;&#26029;&#35821;&#35328;&#38169;&#35823;&#21644;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#33021;&#21147;&#25552;&#20379;&#20102;&#37325;&#35201;&#25163;&#27573;&#12290;&#28982;&#32780;&#65292;&#23558;&#34892;&#20026;&#27979;&#35797;&#24212;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#36890;&#24120;&#38656;&#35201;&#20154;&#21147;&#26469;&#21046;&#20316;&#35780;&#20272;&#36825;&#20123;&#31995;&#32479;&#22312;&#26032;&#29983;&#25104;&#30340;&#27979;&#35797;&#29992;&#20363;&#19978;&#30340;&#32763;&#35793;&#36136;&#37327;&#30340;&#21442;&#32771;&#12290;&#29616;&#26377;&#30340;&#26426;&#22120;&#32763;&#35793;&#34892;&#20026;&#27979;&#35797;&#24037;&#20316;&#36890;&#36807;&#22312;&#27809;&#26377;&#21442;&#32771;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#32763;&#35793;&#36136;&#37327;&#26469;&#32469;&#24320;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#36825;&#38480;&#21046;&#20102;&#23545;&#29305;&#23450;&#31867;&#22411;&#38169;&#35823;&#30340;&#35786;&#26029;&#65292;&#27604;&#22914;&#21333;&#20010;&#25968;&#23383;&#25110;&#36135;&#24065;&#35789;&#30340;&#38169;&#35823;&#32763;&#35793;&#12290;&#20026;&#20102;&#35786;&#26029;&#36890;&#29992;&#38169;&#35823;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#35821;&#32763;&#35793;&#23545;&#29983;&#25104;&#30340;&#34892;&#20026;&#27979;&#35797;&#65288;BTPGBT&#65289;&#26694;&#26550;&#26469;&#36827;&#34892;&#26426;&#22120;&#32763;&#35793;&#34892;&#20026;&#27979;&#35797;&#12290;BTPGBT&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#35821;&#32763;&#35793;&#23545;&#29983;&#25104;&#65288;BTPG&#65289;&#26041;&#27861;&#65292;&#33258;&#21160;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#27979;&#35797;&#29992;&#20363;&#21450;&#20854;&#20266;&#21442;&#32771;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#35786;&#26029;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#36890;&#29992;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Behavioral testing offers a crucial means of diagnosing linguistic errors and assessing capabilities of NLP models. However, applying behavioral testing to machine translation (MT) systems is challenging as it generally requires human efforts to craft references for evaluating the translation quality of such systems on newly generated test cases. Existing works in behavioral testing of MT systems circumvent this by evaluating translation quality without references, but this restricts diagnosis to specific types of errors, such as incorrect translation of single numeric or currency words. In order to diagnose general errors, this paper proposes a new Bilingual Translation Pair Generation based Behavior Testing (BTPGBT) framework for conducting behavioral testing of MT systems. The core idea of BTPGBT is to employ a novel bilingual translation pair generation (BTPG) approach that automates the construction of high-quality test cases and their pseudoreferences. Experimental results on var
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#27861;&#26469;&#35299;&#20915;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;&#20013;&#21512;&#25104;&#22270;&#20687;&#19982;&#30495;&#23454;&#22270;&#20687;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#36755;&#20837;&#22270;&#20687;&#34920;&#31034;&#21644;&#36755;&#20986;&#20998;&#24067;&#36827;&#34892;&#35843;&#25972;&#65292;&#26469;&#22635;&#34917;&#36825;&#19968;&#40511;&#27807;&#12290;</title><link>http://arxiv.org/abs/2310.13361</link><description>&lt;p&gt;
&#22635;&#34917;&#21512;&#25104;&#22270;&#20687;&#19982;&#30495;&#23454;&#22270;&#20687;&#38388;&#30340;&#40511;&#27807;&#20197;&#23454;&#29616;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Bridging the Gap between Synthetic and Authentic Images for Multimodal Machine Translation. (arXiv:2310.13361v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#27861;&#26469;&#35299;&#20915;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;&#20013;&#21512;&#25104;&#22270;&#20687;&#19982;&#30495;&#23454;&#22270;&#20687;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#36755;&#20837;&#22270;&#20687;&#34920;&#31034;&#21644;&#36755;&#20986;&#20998;&#24067;&#36827;&#34892;&#35843;&#25972;&#65292;&#26469;&#22635;&#34917;&#36825;&#19968;&#40511;&#27807;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;&#65288;MMT&#65289;&#21516;&#26102;&#23558;&#28304;&#21477;&#23376;&#21644;&#30456;&#20851;&#22270;&#20687;&#20316;&#20026;&#32763;&#35793;&#30340;&#36755;&#20837;&#12290;&#30001;&#20110;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#36755;&#20837;&#21477;&#23376;&#27809;&#26377;&#37197;&#23545;&#30340;&#22270;&#20687;&#21487;&#29992;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24314;&#35758;&#21033;&#29992;&#24378;&#22823;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#22270;&#20687;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#22270;&#20687;&#24448;&#24448;&#19982;&#30495;&#23454;&#22270;&#20687;&#26377;&#19981;&#21516;&#30340;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#22312;&#35757;&#32451;&#20013;&#20351;&#29992;&#30495;&#23454;&#22270;&#20687;&#65292;&#32780;&#22312;&#25512;&#29702;&#20013;&#20351;&#29992;&#21512;&#25104;&#22270;&#20687;&#21487;&#33021;&#24341;&#20837;&#20998;&#24067;&#20559;&#31227;&#65292;&#23548;&#33268;&#25512;&#29702;&#26102;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#23558;&#21512;&#25104;&#22270;&#20687;&#21644;&#30495;&#23454;&#22270;&#20687;&#20998;&#21035;&#36755;&#20837;&#21040;MMT&#27169;&#22411;&#20013;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#25509;&#36817;Transformer&#32534;&#30721;&#22120;&#30340;&#36755;&#20837;&#22270;&#20687;&#34920;&#31034;&#21644;Transformer&#35299;&#30721;&#22120;&#30340;&#36755;&#20986;&#20998;&#24067;&#65292;&#26368;&#23567;&#21270;&#21512;&#25104;&#22270;&#20687;&#19982;&#30495;&#23454;&#22270;&#20687;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#32531;&#35299;&#20102;&#21512;&#25104;&#22270;&#20687;&#24341;&#20837;&#30340;&#20998;&#24067;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal machine translation (MMT) simultaneously takes the source sentence and a relevant image as input for translation. Since there is no paired image available for the input sentence in most cases, recent studies suggest utilizing powerful text-to-image generation models to provide image inputs. Nevertheless, synthetic images generated by these models often follow different distributions compared to authentic images. Consequently, using authentic images for training and synthetic images for inference can introduce a distribution shift, resulting in performance degradation during inference. To tackle this challenge, in this paper, we feed synthetic and authentic images to the MMT model, respectively. Then we minimize the gap between the synthetic and authentic images by drawing close the input image representations of the Transformer Encoder and the output distributions of the Transformer Decoder. Therefore, we mitigate the distribution disparity introduced by the synthetic images
&lt;/p&gt;</description></item><item><title>NurViD&#26159;&#19968;&#20010;&#19987;&#20026;&#25252;&#29702;&#25805;&#20316;&#35774;&#35745;&#30340;&#22823;&#35268;&#27169;&#35270;&#39057;&#25968;&#25454;&#24211;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#36739;&#23567;&#12289;&#32570;&#20047;&#19987;&#23478;&#32423;&#27880;&#37322;&#21644;&#26102;&#38388;&#26412;&#22320;&#21270;&#27880;&#37322;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.13347</link><description>&lt;p&gt;
NurViD: &#29992;&#20110;&#25252;&#29702;&#25805;&#20316;&#27963;&#21160;&#29702;&#35299;&#30340;&#22823;&#35268;&#27169;&#19987;&#23478;&#32423;&#35270;&#39057;&#25968;&#25454;&#24211;
&lt;/p&gt;
&lt;p&gt;
NurViD: A Large Expert-Level Video Database for Nursing Procedure Activity Understanding. (arXiv:2310.13347v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13347
&lt;/p&gt;
&lt;p&gt;
NurViD&#26159;&#19968;&#20010;&#19987;&#20026;&#25252;&#29702;&#25805;&#20316;&#35774;&#35745;&#30340;&#22823;&#35268;&#27169;&#35270;&#39057;&#25968;&#25454;&#24211;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#36739;&#23567;&#12289;&#32570;&#20047;&#19987;&#23478;&#32423;&#27880;&#37322;&#21644;&#26102;&#38388;&#26412;&#22320;&#21270;&#27880;&#37322;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#25252;&#29702;&#25805;&#20316;&#27963;&#21160;&#29702;&#35299;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#25252;&#22763;&#19982;&#24739;&#32773;&#20114;&#21160;&#30340;&#36136;&#37327;&#21644;&#23433;&#20840;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#31181;&#25216;&#26415;&#65292;&#25105;&#20204;&#21487;&#20197;&#20419;&#36827;&#22521;&#35757;&#21644;&#25945;&#32946;&#65292;&#25913;&#21892;&#36136;&#37327;&#25511;&#21046;&#65292;&#24182;&#23454;&#29616;&#25805;&#20316;&#21512;&#35268;&#30417;&#25511;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29616;&#26377;&#36866;&#24403;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#31232;&#32570;&#65292;&#35813;&#39046;&#22495;&#33258;&#21160;&#35782;&#21035;&#31995;&#32479;&#30340;&#21457;&#23637;&#21463;&#21040;&#20102;&#38459;&#30861;&#12290;&#29616;&#26377;&#30340;&#35270;&#39057;&#25968;&#25454;&#38598;&#23384;&#22312;&#20960;&#20010;&#38480;&#21046;&#65306;1&#65289;&#36825;&#20123;&#25968;&#25454;&#38598;&#22312;&#35268;&#27169;&#19978;&#36739;&#23567;&#65292;&#26080;&#27861;&#25903;&#25345;&#23545;&#25252;&#29702;&#27963;&#21160;&#30340;&#20840;&#38754;&#35843;&#26597;&#65307;2&#65289;&#23427;&#20204;&#20027;&#35201;&#20851;&#27880;&#21333;&#19968;&#25805;&#20316;&#65292;&#32570;&#20047;&#21508;&#31181;&#25252;&#29702;&#25805;&#20316;&#21644;&#25805;&#20316;&#27493;&#39588;&#30340;&#19987;&#23478;&#32423;&#27880;&#37322;&#65307;3&#65289;&#23427;&#20204;&#32570;&#20047;&#26102;&#38388;&#19978;&#30340;&#26412;&#22320;&#21270;&#27880;&#37322;&#65292;&#36825;&#38459;&#30861;&#20102;&#23545;&#36739;&#38271;&#35270;&#39057;&#24207;&#21015;&#20013;&#30446;&#26631;&#21160;&#20316;&#30340;&#26377;&#25928;&#23450;&#20301;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NurViD&#65292;&#19968;&#20010;&#20855;&#26377;&#19987;&#23478;&#32423;&#27880;&#37322;&#30340;&#22823;&#35268;&#27169;&#35270;&#39057;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25252;&#29702;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of deep learning to nursing procedure activity understanding has the potential to greatly enhance the quality and safety of nurse-patient interactions. By utilizing the technique, we can facilitate training and education, improve quality control, and enable operational compliance monitoring. However, the development of automatic recognition systems in this field is currently hindered by the scarcity of appropriately labeled datasets. The existing video datasets pose several limitations: 1) these datasets are small-scale in size to support comprehensive investigations of nursing activity; 2) they primarily focus on single procedures, lacking expert-level annotations for various nursing procedures and action steps; and 3) they lack temporally localized annotations, which prevents the effective localization of targeted actions within longer video sequences. To mitigate these limitations, we propose NurViD, a large video dataset with expert-level annotation for nursing proc
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24191;&#27867;&#24212;&#29992;&#20013;&#38754;&#20020;&#39046;&#22495;&#29305;&#24322;&#24615;&#12289;&#30693;&#35782;&#36951;&#24536;&#12289;&#30693;&#35782;&#22797;&#21046;&#12289;&#30693;&#35782;&#38169;&#35273;&#21644;&#30693;&#35782;&#26377;&#27602;&#31561;&#25361;&#25112;&#12290;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#24314;&#35758;&#21253;&#25324;&#22810;&#26679;&#21270;&#35757;&#32451;&#25968;&#25454;&#12289;&#24494;&#35843;&#27169;&#22411;&#65292;&#22686;&#24378;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13343</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21033;&#29992;&#20013;&#30340;&#25361;&#25112;&#21644;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Challenges and Contributing Factors in the Utilization of Large Language Models (LLMs). (arXiv:2310.13343v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13343
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24191;&#27867;&#24212;&#29992;&#20013;&#38754;&#20020;&#39046;&#22495;&#29305;&#24322;&#24615;&#12289;&#30693;&#35782;&#36951;&#24536;&#12289;&#30693;&#35782;&#22797;&#21046;&#12289;&#30693;&#35782;&#38169;&#35273;&#21644;&#30693;&#35782;&#26377;&#27602;&#31561;&#25361;&#25112;&#12290;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#24314;&#35758;&#21253;&#25324;&#22810;&#26679;&#21270;&#35757;&#32451;&#25968;&#25454;&#12289;&#24494;&#35843;&#27169;&#22411;&#65292;&#22686;&#24378;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;GPT&#31995;&#21015;&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#65292;&#23427;&#20204;&#22312;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#24191;&#27867;&#20351;&#29992;&#24102;&#26469;&#20102;&#19968;&#31995;&#21015;&#25361;&#25112;&#12290;&#26412;&#32508;&#36848;&#39318;&#20808;&#25506;&#35752;&#20102;&#39046;&#22495;&#29305;&#24322;&#24615;&#30340;&#38382;&#39064;&#65292;LLMs&#21487;&#33021;&#38590;&#20197;&#23545;&#19987;&#19994;&#39046;&#22495;&#30340;&#29305;&#23450;&#38382;&#39064;&#25552;&#20379;&#31934;&#30830;&#31572;&#26696;&#12290;&#30693;&#35782;&#36951;&#24536;&#30340;&#38382;&#39064;&#26159;&#36825;&#20123;LLMs&#21487;&#33021;&#38590;&#20197;&#24179;&#34913;&#26032;&#26087;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#12290;&#30693;&#35782;&#22797;&#21046;&#29616;&#35937;&#25581;&#31034;&#20102;LLMs&#26377;&#26102;&#21487;&#33021;&#25552;&#20379;&#36807;&#24230;&#26426;&#26800;&#21270;&#30340;&#22238;&#31572;&#65292;&#32570;&#20047;&#28145;&#24230;&#21644;&#29420;&#21019;&#24615;&#12290;&#27492;&#22806;&#65292;&#30693;&#35782;&#38169;&#35273;&#25551;&#36848;&#20102;LLMs&#21487;&#33021;&#25552;&#20379;&#30475;&#20284;&#28145;&#21051;&#20294;&#23454;&#38469;&#19978;&#32932;&#27973;&#30340;&#31572;&#26696;&#30340;&#24773;&#20917;&#65292;&#32780;&#30693;&#35782;&#26377;&#27602;&#21017;&#20391;&#37325;&#20110;&#26377;&#23475;&#25110;&#26377;&#20559;&#35265;&#30340;&#20449;&#24687;&#36755;&#20986;&#12290;&#36825;&#20123;&#25361;&#25112;&#20984;&#26174;&#20102;LLMs&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#31639;&#27861;&#35774;&#35745;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24314;&#35758;&#22810;&#26679;&#21270;&#35757;&#32451;&#25968;&#25454;&#65292;&#24494;&#35843;&#27169;&#22411;&#65292;&#22686;&#24378;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of large language models (LLMs) like the GPT series, their widespread use across various application scenarios presents a myriad of challenges. This review initially explores the issue of domain specificity, where LLMs may struggle to provide precise answers to specialized questions within niche fields. The problem of knowledge forgetting arises as these LLMs might find it hard to balance old and new information. The knowledge repetition phenomenon reveals that sometimes LLMs might deliver overly mechanized responses, lacking depth and originality. Furthermore, knowledge illusion describes situations where LLMs might provide answers that seem insightful but are actually superficial, while knowledge toxicity focuses on harmful or biased information outputs. These challenges underscore problems in the training data and algorithmic design of LLMs. To address these issues, it's suggested to diversify training data, fine-tune models, enhance transparency and interpretab
&lt;/p&gt;</description></item><item><title>FLAIR&#26159;&#19968;&#20010;&#20840;&#22269;&#33539;&#22260;&#30340;&#22303;&#22320;&#35206;&#30422;&#35821;&#20041;&#20998;&#21106;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#22823;&#37327;&#39640;&#20998;&#36776;&#29575;&#33322;&#25293;&#22270;&#20687;&#21644;&#25968;&#21313;&#20159;&#20010;&#20010;&#20307;&#26631;&#35760;&#20687;&#32032;&#12290;FLAIR&#25972;&#21512;&#20102;&#19981;&#21516;&#31354;&#38388;&#12289;&#20809;&#35889;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;&#25968;&#25454;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#22303;&#22320;&#35206;&#30422;&#20998;&#26512;&#30340;&#26041;&#27861;&#24320;&#21457;&#21644;&#35780;&#20272;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2310.13336</link><description>&lt;p&gt;
FLAIR:&#19968;&#31181;&#22522;&#20110;&#22810;&#28304;&#20809;&#23398;&#24433;&#20687;&#30340;&#20840;&#22269;&#33539;&#22260;&#22303;&#22320;&#35206;&#30422;&#35821;&#20041;&#20998;&#21106;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
FLAIR: a Country-Scale Land Cover Semantic Segmentation Dataset From Multi-Source Optical Imagery. (arXiv:2310.13336v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13336
&lt;/p&gt;
&lt;p&gt;
FLAIR&#26159;&#19968;&#20010;&#20840;&#22269;&#33539;&#22260;&#30340;&#22303;&#22320;&#35206;&#30422;&#35821;&#20041;&#20998;&#21106;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#22823;&#37327;&#39640;&#20998;&#36776;&#29575;&#33322;&#25293;&#22270;&#20687;&#21644;&#25968;&#21313;&#20159;&#20010;&#20010;&#20307;&#26631;&#35760;&#20687;&#32032;&#12290;FLAIR&#25972;&#21512;&#20102;&#19981;&#21516;&#31354;&#38388;&#12289;&#20809;&#35889;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;&#25968;&#25454;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#22303;&#22320;&#35206;&#30422;&#20998;&#26512;&#30340;&#26041;&#27861;&#24320;&#21457;&#21644;&#35780;&#20272;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#27861;&#22269;&#22269;&#23478;&#22320;&#29702;&#21644;&#26862;&#26519;&#20449;&#24687;&#30740;&#31350;&#25152;&#65288;IGN&#65289;&#30340;&#27861;&#22269;&#33322;&#31354;&#33322;&#22825;&#24433;&#20687;&#65288;FLAIR&#65289;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#65292;&#20026;&#22823;&#35268;&#27169;&#22320;&#29702;&#31354;&#38388;&#20998;&#26512;&#25552;&#20379;&#20102;&#29420;&#29305;&#32780;&#20016;&#23500;&#30340;&#36164;&#28304;&#12290;FLAIR&#21253;&#21547;&#39640;&#20998;&#36776;&#29575;&#33322;&#25293;&#22270;&#20687;&#65292;&#22320;&#38754;&#26679;&#26412;&#38388;&#36317;&#20026;20&#21400;&#31859;&#65292;&#36229;&#36807;200&#20159;&#20010;&#20010;&#20307;&#26631;&#35760;&#20687;&#32032;&#29992;&#20110;&#31934;&#30830;&#30340;&#22303;&#22320;&#35206;&#30422;&#20998;&#31867;&#12290;&#35813;&#25968;&#25454;&#38598;&#36824;&#25972;&#21512;&#20102;&#20809;&#23398;&#21355;&#26143;&#26102;&#38388;&#24207;&#21015;&#30340;&#26102;&#38388;&#21644;&#20809;&#35889;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;FLAIR&#23558;&#20855;&#26377;&#19981;&#21516;&#31354;&#38388;&#12289;&#20809;&#35889;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;&#25968;&#25454;&#25972;&#21512;&#22312;&#27861;&#22269;&#20840;&#22659;817&#24179;&#26041;&#20844;&#37324;&#30340;&#37319;&#38598;&#20013;&#65292;&#20195;&#34920;&#20102;&#27861;&#22269;&#20016;&#23500;&#30340;&#26223;&#35266;&#22810;&#26679;&#24615;&#12290;&#36825;&#31181;&#22810;&#26679;&#24615;&#20351;FLAIR&#25104;&#20026;&#22823;&#35268;&#27169;&#22303;&#22320;&#35206;&#30422;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#30340;&#24320;&#21457;&#21644;&#35780;&#20272;&#30340;&#23453;&#36149;&#36164;&#28304;&#65292;&#24182;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#25968;&#25454;&#34701;&#21512;&#21644;&#22320;&#29702;&#31354;&#38388;&#20998;&#26512;&#26041;&#38754;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#21333;&#20256;&#24863;&#22120;&#21644;&#22810;&#20256;&#24863;&#22120;&#22522;&#32447;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#26469;
&lt;/p&gt;
&lt;p&gt;
We introduce the French Land cover from Aerospace ImageRy (FLAIR), an extensive dataset from the French National Institute of Geographical and Forest Information (IGN) that provides a unique and rich resource for large-scale geospatial analysis. FLAIR contains high-resolution aerial imagery with a ground sample distance of 20 cm and over 20 billion individually labeled pixels for precise land-cover classification. The dataset also integrates temporal and spectral data from optical satellite time series. FLAIR thus combines data with varying spatial, spectral, and temporal resolutions across over 817 km2 of acquisitions representing the full landscape diversity of France. This diversity makes FLAIR a valuable resource for the development and evaluation of novel methods for large-scale land-cover semantic segmentation and raises significant challenges in terms of computer vision, data fusion, and geospatial analysis. We also provide powerful uni- and multi-sensor baseline models that can
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#21046;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25512;&#29702;&#25945;&#24072;&#65292;&#24182;&#24314;&#31435;&#20132;&#20114;&#24335;&#22810;&#36718;&#23398;&#20064;&#33539;&#24335;&#65292;&#23558;&#25512;&#29702;&#33021;&#21147;&#25552;&#21462;&#21040;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20197;&#20419;&#36827;&#20854;&#27665;&#20027;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.13332</link><description>&lt;p&gt;
&#27665;&#20027;&#21270;&#25512;&#29702;&#33021;&#21147;&#65306;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23450;&#21046;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Democratizing Reasoning Ability: Tailored Learning from Large Language Model. (arXiv:2310.13332v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#21046;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25512;&#29702;&#25945;&#24072;&#65292;&#24182;&#24314;&#31435;&#20132;&#20114;&#24335;&#22810;&#36718;&#23398;&#20064;&#33539;&#24335;&#65292;&#23558;&#25512;&#29702;&#33021;&#21147;&#25552;&#21462;&#21040;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20197;&#20419;&#36827;&#20854;&#27665;&#20027;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#24040;&#22823;&#30340;&#35745;&#31639;&#38656;&#27714;&#21644;&#23553;&#38381;&#28304;&#20195;&#30721;&#30340;&#29305;&#24615;&#65292;&#20854;&#27665;&#20027;&#21270;&#21463;&#21040;&#20102;&#38459;&#30861;&#12290;&#26368;&#36817;&#23545;&#24320;&#28304;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#25552;&#21462;&#30340;&#30740;&#31350;&#22312;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#23545;&#20110;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#21046;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#36825;&#31181;&#25512;&#29702;&#33021;&#21147;&#25552;&#21462;&#21040;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20197;&#20419;&#36827;&#19987;&#23646;&#25512;&#29702;&#33021;&#21147;&#30340;&#27665;&#20027;&#21270;&#12290;&#19982;&#20165;&#23558;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25968;&#25454;&#27880;&#37322;&#22120;&#19981;&#21516;&#65292;&#25105;&#20204;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25512;&#29702;&#25945;&#24072;&#30340;&#28508;&#21147;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#20132;&#20114;&#22810;&#36718;&#23398;&#20064;&#33539;&#24335;&#12290;&#36825;&#20010;&#33539;&#24335;&#20351;&#24471;&#23398;&#29983;&#33021;&#22815;&#23558;&#33258;&#24049;&#30340;&#19981;&#36275;&#26292;&#38706;&#32473;&#40657;&#31665;&#25945;&#24072;&#65292;&#28982;&#21518;&#25945;&#24072;&#21487;&#20197;&#25552;&#20379;&#23450;&#21046;&#30340;&#35757;&#32451;&#25968;&#25454;&#20316;&#20026;&#22238;&#25253;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#21033;&#29992;&#25512;&#29702;&#28508;&#21147;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#29305;&#23450;&#25512;&#29702;&#20219;&#21153;&#65292;&#36890;&#36807;&#19982;&#40657;&#31665;&#25945;&#24072;&#30340;&#20132;&#20114;&#26469;&#23450;&#21046;&#23398;&#29983;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) exhibit impressive emergent abilities in natural language processing, but their democratization is hindered due to huge computation requirements and closed-source nature. Recent research on advancing open-source smaller LMs by distilling knowledge from black-box LLMs has obtained promising results in the instruction-following ability. However, the reasoning ability which is more challenging to foster, is relatively rarely explored. In this paper, we propose a tailored learning approach to distill such reasoning ability to smaller LMs to facilitate the democratization of the exclusive reasoning ability. In contrast to merely employing LLM as a data annotator, we exploit the potential of LLM as a reasoning teacher by building an interactive multi-round learning paradigm. This paradigm enables the student to expose its deficiencies to the black-box teacher who then can provide customized training data in return. Further, to exploit the reasoning potential of t
&lt;/p&gt;</description></item><item><title>CoFFTEA&#26159;&#19968;&#31181;&#31895;&#32454;&#26694;&#26550;&#21644;&#30446;&#26631;&#32534;&#30721;&#22120;&#20307;&#31995;&#32467;&#26500;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#21452;&#32534;&#30721;&#22120;&#26469;&#26377;&#25928;&#24314;&#27169;&#26694;&#26550;&#21644;&#30446;&#26631;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#31895;&#21040;&#32454;&#30340;&#35838;&#31243;&#23398;&#20064;&#36807;&#31243;&#36880;&#28176;&#23398;&#20250;&#21306;&#20998;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2310.13316</link><description>&lt;p&gt;
&#31895;&#32454;&#21452;&#32534;&#30721;&#22120;&#26159;&#26356;&#22909;&#30340;&#24103;&#35782;&#21035;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Coarse-to-Fine Dual Encoders are Better Frame Identification Learners. (arXiv:2310.13316v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13316
&lt;/p&gt;
&lt;p&gt;
CoFFTEA&#26159;&#19968;&#31181;&#31895;&#32454;&#26694;&#26550;&#21644;&#30446;&#26631;&#32534;&#30721;&#22120;&#20307;&#31995;&#32467;&#26500;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#21452;&#32534;&#30721;&#22120;&#26469;&#26377;&#25928;&#24314;&#27169;&#26694;&#26550;&#21644;&#30446;&#26631;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#31895;&#21040;&#32454;&#30340;&#35838;&#31243;&#23398;&#20064;&#36807;&#31243;&#36880;&#28176;&#23398;&#20250;&#21306;&#20998;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24103;&#35782;&#21035;&#26088;&#22312;&#25214;&#21040;&#19982;&#30446;&#26631;&#35789;&#22312;&#21477;&#23376;&#20013;&#30456;&#20851;&#30340;&#35821;&#20041;&#26694;&#26550;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#24314;&#27169;&#26694;&#26550;&#23450;&#20041;&#26469;&#34913;&#37327;&#30446;&#26631;&#21644;&#20505;&#36873;&#26694;&#26550;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#25110;&#21305;&#37197;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#35201;&#20040;&#32570;&#20047;&#23545;&#23450;&#20041;&#30340;&#20805;&#20998;&#34920;&#31034;&#23398;&#20064;&#65292;&#35201;&#20040;&#38754;&#20020;&#22312;&#36229;&#36807;1000&#20010;&#20505;&#36873;&#26694;&#26550;&#20013;&#26377;&#25928;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#26694;&#26550;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#24120;&#29992;&#30340;&#35789;&#20856;&#36807;&#28388;&#26041;&#27861;&#65288;lf&#65289;&#33719;&#21462;&#30446;&#26631;&#30340;&#20505;&#36873;&#26694;&#26550;&#26102;&#21487;&#33021;&#20250;&#24573;&#30053;&#35789;&#27719;&#34920;&#22806;&#30340;&#30446;&#26631;&#65292;&#24182;&#23548;&#33268;&#26694;&#26550;&#24314;&#27169;&#19981;&#20805;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CoFFTEA&#65292;&#21363;&#31895;&#32454;&#26694;&#26550;&#21644;&#30446;&#26631;&#32534;&#30721;&#22120;&#20307;&#31995;&#32467;&#26500;&#12290;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#21452;&#32534;&#30721;&#22120;&#65292;CoFFTEA&#26377;&#25928;&#22320;&#24314;&#27169;&#20102;&#26694;&#26550;&#21644;&#30446;&#26631;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#36890;&#36807;&#37319;&#29992;&#31895;&#21040;&#32454;&#30340;&#35838;&#31243;&#23398;&#20064;&#36807;&#31243;&#65292;CoFFTEA&#36880;&#28176;&#23398;&#20250;&#21306;&#20998;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Frame identification aims to find semantic frames associated with target words in a sentence. Recent researches measure the similarity or matching score between targets and candidate frames by modeling frame definitions. However, they either lack sufficient representation learning of the definitions or face challenges in efficiently selecting the most suitable frame from over 1000 candidate frames. Moreover, commonly used lexicon filtering ($lf$) to obtain candidate frames for the target may ignore out-of-vocabulary targets and cause inadequate frame modeling. In this paper, we propose CoFFTEA, a $\underline{Co}$arse-to-$\underline{F}$ine $\underline{F}$rame and $\underline{T}$arget $\underline{E}$ncoders $\underline{A}$rchitecture. With contrastive learning and dual encoders, CoFFTEA efficiently and effectively models the alignment between frames and targets. By employing a coarse-to-fine curriculum learning procedure, CoFFTEA gradually learns to differentiate frames with varying degr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SocialSense&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35825;&#23548;&#20449;&#24565;&#22686;&#24378;&#30340;&#22270;&#21644;&#22522;&#20110;&#22270;&#30340;&#20256;&#25773;&#26469;&#39044;&#27979;&#26032;&#38395;&#21457;&#24067;&#30340;&#21709;&#24212;&#12290;&#36825;&#20010;&#26041;&#27861;&#22312;&#27809;&#26377;&#29992;&#25143;&#26126;&#30830;&#20010;&#20154;&#36164;&#26009;&#25110;&#21382;&#21490;&#34892;&#20026;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#31038;&#20132;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2310.13297</link><description>&lt;p&gt;
&#35299;&#30721;&#27785;&#40664;&#30340;&#22823;&#22810;&#25968;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35825;&#23548;&#20449;&#24565;&#22686;&#24378;&#30340;&#31038;&#20132;&#22270;&#36827;&#34892;&#21709;&#24212;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Decoding the Silent Majority: Inducing Belief Augmented Social Graph with Large Language Model for Response Forecasting. (arXiv:2310.13297v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13297
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SocialSense&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35825;&#23548;&#20449;&#24565;&#22686;&#24378;&#30340;&#22270;&#21644;&#22522;&#20110;&#22270;&#30340;&#20256;&#25773;&#26469;&#39044;&#27979;&#26032;&#38395;&#21457;&#24067;&#30340;&#21709;&#24212;&#12290;&#36825;&#20010;&#26041;&#27861;&#22312;&#27809;&#26377;&#29992;&#25143;&#26126;&#30830;&#20010;&#20154;&#36164;&#26009;&#25110;&#21382;&#21490;&#34892;&#20026;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#31038;&#20132;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#38395;&#23186;&#20307;&#30340;&#33258;&#21160;&#21709;&#24212;&#39044;&#27979;&#22312;&#24110;&#21161;&#20869;&#23481;&#29983;&#20135;&#32773;&#26377;&#25928;&#39044;&#27979;&#26032;&#38395;&#21457;&#24067;&#30340;&#24433;&#21709;&#24182;&#38450;&#27490;&#24847;&#22806;&#30340;&#36127;&#38754;&#32467;&#26524;&#65288;&#22914;&#31038;&#20250;&#20914;&#31361;&#21644;&#36947;&#24503;&#20260;&#23475;&#65289;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#39044;&#27979;&#21709;&#24212;&#65292;&#24517;&#39035;&#24320;&#21457;&#33021;&#22815;&#21033;&#29992;&#20010;&#20307;&#21608;&#22260;&#30340;&#31038;&#20132;&#21160;&#24577;&#21644;&#32972;&#26223;&#20449;&#24687;&#30340;&#25514;&#26045;&#65292;&#23588;&#20854;&#26159;&#22312;&#29992;&#25143;&#26126;&#30830;&#30340;&#20010;&#20154;&#36164;&#26009;&#25110;&#21382;&#21490;&#34892;&#20026;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65288;&#21363;&#28508;&#22312;&#21442;&#19982;&#32773;&#65289;&#12290;&#27491;&#22914;&#20808;&#21069;&#30340;&#30740;&#31350;&#25152;&#31034;&#65292;97%&#30340;&#25512;&#25991;&#20165;&#30001;&#26368;&#27963;&#36291;&#30340;25%&#29992;&#25143;&#20135;&#29983;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#23545;&#20110;&#22914;&#20309;&#22788;&#29702;&#21644;&#21033;&#29992;&#36825;&#20123;&#37325;&#35201;&#29305;&#24449;&#30340;&#26368;&#20339;&#26041;&#24335;&#36827;&#34892;&#20102;&#26377;&#38480;&#30340;&#25506;&#32034;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SocialSense&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29616;&#26377;&#31038;&#20132;&#32593;&#32476;&#20043;&#19978;&#35825;&#23548;&#20986;&#19968;&#20010;&#20197;&#20449;&#24565;&#20026;&#20013;&#24515;&#30340;&#22270;&#65292;&#20197;&#21450;&#22522;&#20110;&#22270;&#30340;&#20256;&#25773;&#26469;&#25429;&#25417;&#31038;&#20132;&#21160;&#24577;&#12290;&#25105;&#20204;&#20551;&#35774;&#35825;&#23548;&#20986;&#30340;&#22270;&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;
Automatic response forecasting for news media plays a crucial role in enabling content producers to efficiently predict the impact of news releases and prevent unexpected negative outcomes such as social conflict and moral injury. To effectively forecast responses, it is essential to develop measures that leverage the social dynamics and contextual information surrounding individuals, especially in cases where explicit profiles or historical actions of the users are limited (referred to as lurkers). As shown in a previous study, 97% of all tweets are produced by only the most active 25% of users. However, existing approaches have limited exploration of how to best process and utilize these important features. To address this gap, we propose a novel framework, named SocialSense, that leverages a large language model to induce a belief-centered graph on top of an existent social network, along with graph-based propagation to capture social dynamics. We hypothesize that the induced graph 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;PathRL&#65292;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#23548;&#33322;&#36335;&#24452;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;DRL&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#20302;&#32423;&#25511;&#21046;&#25351;&#20196;&#26469;&#30452;&#25509;&#25351;&#23548;&#26426;&#22120;&#20154;&#34892;&#21160;&#65292;&#20294;&#36825;&#23548;&#33268;&#36895;&#24230;&#19981;&#31283;&#23450;&#21644;&#36712;&#36857;&#19981;&#24179;&#28369;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;PathRL&#36890;&#36807;&#35757;&#32451;&#30452;&#25509;&#36755;&#20986;&#36335;&#24452;&#30340;DRL&#31574;&#30053;&#65292;&#20811;&#26381;&#20102;&#39640;&#32500;&#21160;&#20316;&#31354;&#38388;&#21644;&#22810;&#26102;&#38388;&#27493;&#39588;&#36319;&#36394;&#36335;&#24452;&#30340;&#38590;&#39064;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;PathRL&#33021;&#22815;&#29983;&#25104;&#31283;&#23450;&#19988;&#24179;&#28369;&#30340;&#23548;&#33322;&#36335;&#24452;&#65292;&#29992;&#20110;&#36991;&#20813;&#30896;&#25758;&#21644;&#25552;&#39640;&#31227;&#21160;&#26426;&#22120;&#20154;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.13295</link><description>&lt;p&gt;
PathRL&#65306;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#36335;&#24452;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#30896;&#25758;&#36991;&#20813;
&lt;/p&gt;
&lt;p&gt;
PathRL: An End-to-End Path Generation Method for Collision Avoidance via Deep Reinforcement Learning. (arXiv:2310.13295v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13295
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;PathRL&#65292;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#23548;&#33322;&#36335;&#24452;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;DRL&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#20302;&#32423;&#25511;&#21046;&#25351;&#20196;&#26469;&#30452;&#25509;&#25351;&#23548;&#26426;&#22120;&#20154;&#34892;&#21160;&#65292;&#20294;&#36825;&#23548;&#33268;&#36895;&#24230;&#19981;&#31283;&#23450;&#21644;&#36712;&#36857;&#19981;&#24179;&#28369;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;PathRL&#36890;&#36807;&#35757;&#32451;&#30452;&#25509;&#36755;&#20986;&#36335;&#24452;&#30340;DRL&#31574;&#30053;&#65292;&#20811;&#26381;&#20102;&#39640;&#32500;&#21160;&#20316;&#31354;&#38388;&#21644;&#22810;&#26102;&#38388;&#27493;&#39588;&#36319;&#36394;&#36335;&#24452;&#30340;&#38590;&#39064;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;PathRL&#33021;&#22815;&#29983;&#25104;&#31283;&#23450;&#19988;&#24179;&#28369;&#30340;&#23548;&#33322;&#36335;&#24452;&#65292;&#29992;&#20110;&#36991;&#20813;&#30896;&#25758;&#21644;&#25552;&#39640;&#31227;&#21160;&#26426;&#22120;&#20154;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#22312;&#25913;&#21892;&#31227;&#21160;&#26426;&#22120;&#20154;&#24615;&#33021;&#26041;&#38754;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20110;DRL&#30340;&#23548;&#33322;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#35757;&#32451;&#19968;&#20010;&#30452;&#25509;&#25351;&#23548;&#26426;&#22120;&#20154;&#30340;&#31574;&#30053;&#65292;&#22914;&#32447;&#24615;&#21644;&#35282;&#36895;&#24230;&#65292;&#36825;&#23548;&#33268;&#26426;&#22120;&#20154;&#22312;&#38271;&#26399;&#25191;&#34892;&#36807;&#31243;&#20013;&#30340;&#36895;&#24230;&#19981;&#31283;&#23450;&#21644;&#36712;&#36857;&#19981;&#24179;&#28369;&#12290;&#21478;&#19968;&#31181;&#26041;&#27861;&#26159;&#35757;&#32451;&#19968;&#20010;&#30452;&#25509;&#36755;&#20986;&#23548;&#33322;&#36335;&#24452;&#30340;DRL&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#36755;&#20986;&#36335;&#24452;&#30340;DRL&#31574;&#30053;&#38754;&#20020;&#20004;&#20010;&#38590;&#39064;&#65306;&#65288;1&#65289;&#28508;&#22312;&#36335;&#24452;&#30340;&#21160;&#20316;&#31354;&#38388;&#36890;&#24120;&#28041;&#21450;&#27604;&#20302;&#32423;&#25351;&#20196;&#26356;&#39640;&#30340;&#32500;&#24230;&#65292;&#22686;&#21152;&#20102;&#35757;&#32451;&#30340;&#38590;&#24230;&#65307;&#65288;2&#65289;&#36319;&#36394;&#36335;&#24452;&#38656;&#35201;&#22810;&#20010;&#26102;&#38388;&#27493;&#39588;&#65292;&#32780;&#19981;&#26159;&#21333;&#20010;&#26102;&#38388;&#27493;&#39588;&#65292;&#36825;&#35201;&#27714;&#36335;&#24452;&#22312;&#22810;&#20010;&#26102;&#38388;&#27493;&#39588;&#20869;&#39044;&#27979;&#26426;&#22120;&#20154;&#19982;&#21160;&#24577;&#29615;&#22659;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#21453;&#36807;&#26469;&#22686;&#21152;&#20102;&#19982;&#36335;&#24452;&#36319;&#36394;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robot navigation using deep reinforcement learning (DRL) has shown great potential in improving the performance of mobile robots. Nevertheless, most existing DRL-based navigation methods primarily focus on training a policy that directly commands the robot with low-level controls, like linear and angular velocities, which leads to unstable speeds and unsmooth trajectories of the robot during the long-term execution. An alternative method is to train a DRL policy that outputs the navigation path directly. However, two roadblocks arise for training a DRL policy that outputs paths: (1) The action space for potential paths often involves higher dimensions comparing to low-level commands, which increases the difficulties of training; (2) It takes multiple time steps to track a path instead of a single time step, which requires the path to predicate the interactions of the robot w.r.t. the dynamic environment in multiple time steps. This, in turn, amplifies the challenges associated with tra
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#21457;&#29616;&#25688;&#35201;&#27169;&#22411;&#23384;&#22312;&#27844;&#38706;&#25968;&#25454;&#25104;&#21592;&#36523;&#20221;&#30340;&#39118;&#38505;&#65292;&#24182;&#35752;&#35770;&#20102;&#19968;&#20123;&#20445;&#25252;&#25514;&#26045;&#21644;&#38544;&#31169;&#19982;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.13291</link><description>&lt;p&gt;
&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#31169;&#39118;&#38505;&#65306;&#20197;&#25688;&#35201;&#20219;&#21153;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Assessing Privacy Risks in Language Models: A Case Study on Summarization Tasks. (arXiv:2310.13291v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#21457;&#29616;&#25688;&#35201;&#27169;&#22411;&#23384;&#22312;&#27844;&#38706;&#25968;&#25454;&#25104;&#21592;&#36523;&#20221;&#30340;&#39118;&#38505;&#65292;&#24182;&#35752;&#35770;&#20102;&#19968;&#20123;&#20445;&#25252;&#25514;&#26045;&#21644;&#38544;&#31169;&#19982;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#25285;&#24515;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#27844;&#38706;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#38598;&#20013;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#65292;&#35843;&#26597;&#25104;&#21592;&#25512;&#26029;&#65288;MI&#65289;&#25915;&#20987;&#65306;&#36890;&#36807;&#32473;&#20986;&#19968;&#20010;&#26679;&#26412;&#21644;&#23545;&#27169;&#22411;API&#30340;&#40657;&#30418;&#35775;&#38382;&#65292;&#21487;&#20197;&#30830;&#23450;&#26679;&#26412;&#26159;&#21542;&#26159;&#35757;&#32451;&#25968;&#25454;&#30340;&#19968;&#37096;&#20998;&#12290;&#25105;&#20204;&#21033;&#29992;&#25991;&#26412;&#30456;&#20284;&#24615;&#21644;&#27169;&#22411;&#23545;&#25991;&#26723;&#20462;&#25913;&#30340;&#25269;&#25239;&#21147;&#20316;&#20026;&#28508;&#22312;&#30340;MI&#20449;&#21495;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25688;&#35201;&#27169;&#22411;&#23384;&#22312;&#27844;&#38706;&#25968;&#25454;&#25104;&#21592;&#36523;&#20221;&#30340;&#39118;&#38505;&#65292;&#29978;&#33267;&#22312;&#27809;&#26377;&#21442;&#32771;&#25688;&#35201;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#35757;&#32451;&#25688;&#35201;&#27169;&#22411;&#20197;&#38450;&#27490;MI&#25915;&#20987;&#30340;&#20960;&#31181;&#20445;&#25252;&#25514;&#26045;&#65292;&#24182;&#35752;&#35770;&#20102;&#38544;&#31169;&#21644;&#25928;&#29992;&#20043;&#38388;&#22266;&#26377;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have revolutionized the field of NLP by achieving state-of-the-art performance on various tasks. However, there is a concern that these models may disclose information in the training data. In this study, we focus on the summarization task and investigate the membership inference (MI) attack: given a sample and black-box access to a model's API, it is possible to determine if the sample was part of the training data. We exploit text similarity and the model's resistance to document modifications as potential MI signals and evaluate their effectiveness on widely used datasets. Our results demonstrate that summarization models are at risk of exposing data membership, even in cases where the reference summary is not available. Furthermore, we discuss several safeguards for training summarization models to protect against MI attacks and discuss the inherent trade-off between privacy and utility.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#36890;&#36807;&#20219;&#21153;&#36229;&#22270;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#25512;&#33616;"&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#20351;&#29992;&#20219;&#21153;&#36229;&#22270;&#23558;&#20808;&#21069;&#20219;&#21153;&#25512;&#24191;&#20026;&#36229;&#36793;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#36807;&#28193;&#24615;&#27880;&#24847;&#21147;&#23618;&#23398;&#20064;&#27599;&#20010;&#20808;&#21069;&#20219;&#21153;&#19982;&#25512;&#33616;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.13286</link><description>&lt;p&gt;
&#36890;&#36807;&#20219;&#21153;&#36229;&#22270;&#23454;&#29616;&#25512;&#33616;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Unified Pretraining for Recommendation via Task Hypergraphs. (arXiv:2310.13286v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#36890;&#36807;&#20219;&#21153;&#36229;&#22270;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#25512;&#33616;"&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#20351;&#29992;&#20219;&#21153;&#36229;&#22270;&#23558;&#20808;&#21069;&#20219;&#21153;&#25512;&#24191;&#20026;&#36229;&#36793;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#36807;&#28193;&#24615;&#27880;&#24847;&#21147;&#23618;&#23398;&#20064;&#27599;&#20010;&#20808;&#21069;&#20219;&#21153;&#19982;&#25512;&#33616;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39044;&#35757;&#32451;&#22312;&#26368;&#36817;&#20960;&#24180;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#21644;&#27969;&#34892;&#65292;&#20294;&#23427;&#22312;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#30456;&#23545;&#26377;&#38480;&#12290;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;ID&#20381;&#36182;&#25968;&#25454;&#38598;&#20013;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#20808;&#21069;&#30693;&#35782;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#19968;&#26041;&#38754;&#65292;&#19968;&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#21382;&#21490;&#24456;&#38590;&#36890;&#36807;&#39044;&#35757;&#32451;&#36716;&#31227;&#21040;&#20854;&#20182;&#25968;&#25454;&#38598;&#20013;&#65292;&#22240;&#20026;ID&#19981;&#21516;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;&#21516;&#19968;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#30340;&#39640;&#39118;&#38505;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36890;&#36807;&#20219;&#21153;&#36229;&#22270;&#30340;&#32479;&#19968;&#25512;&#33616;&#39044;&#35757;&#32451;&#30340;&#26032;&#22411;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#26694;&#26550;&#12290;&#20026;&#20102;&#22788;&#29702;&#21508;&#31181;&#20808;&#21069;&#20219;&#21153;&#30340;&#19981;&#21516;&#35201;&#27714;&#21644;&#32454;&#24494;&#24046;&#21035;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20219;&#21153;&#36229;&#22270;&#23558;&#20808;&#21069;&#20219;&#21153;&#25512;&#24191;&#20026;&#36229;&#36793;&#39044;&#27979;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#36807;&#28193;&#24615;&#27880;&#24847;&#21147;&#23618;&#26469;&#26377;&#24046;&#24322;&#22320;&#23398;&#20064;&#27599;&#20010;&#20808;&#21069;&#20219;&#21153;&#19982;&#25512;&#33616;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although pretraining has garnered significant attention and popularity in recent years, its application in graph-based recommender systems is relatively limited. It is challenging to exploit prior knowledge by pretraining in widely used ID-dependent datasets. On one hand, user-item interaction history in one dataset can hardly be transferred to other datasets through pretraining, where IDs are different. On the other hand, pretraining and finetuning on the same dataset leads to a high risk of overfitting. In this paper, we propose a novel multitask pretraining framework named Unified Pretraining for Recommendation via Task Hypergraphs. For a unified learning pattern to handle diverse requirements and nuances of various pretext tasks, we design task hypergraphs to generalize pretext tasks to hyperedge prediction. A novel transitional attention layer is devised to discriminatively learn the relevance between each pretext task and recommendation. Experimental results on three benchmark da
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#23398;&#20064;&#25490;&#24207;&#20013;&#20351;&#29992;&#27169;&#25311;&#36864;&#28779;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#36827;&#23637;&#21442;&#25968;&#26469;&#26377;&#25928;&#36941;&#21382;&#25628;&#32034;&#31354;&#38388;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13269</link><description>&lt;p&gt;
&#29992;&#20110;&#23398;&#20064;&#25490;&#24207;&#20013;&#29305;&#24449;&#36873;&#25321;&#30340;&#27169;&#25311;&#36864;&#28779;&#25506;&#32034;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Exploratory Study on Simulated Annealing for Feature Selection in Learning-to-Rank. (arXiv:2310.13269v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13269
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#23398;&#20064;&#25490;&#24207;&#20013;&#20351;&#29992;&#27169;&#25311;&#36864;&#28779;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#36827;&#23637;&#21442;&#25968;&#26469;&#26377;&#25928;&#36941;&#21382;&#25628;&#32034;&#31354;&#38388;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#25490;&#24207;&#26159;&#19968;&#31181;&#24212;&#29992;&#20110;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#39046;&#22495;&#12290;&#30001;&#20110;&#29305;&#24449;&#36873;&#25321;&#24050;&#34987;&#21457;&#29616;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#22240;&#27492;&#30740;&#31350;&#23558;&#35813;&#36807;&#31243;&#24212;&#29992;&#20110;&#23398;&#20064;&#25490;&#24207;&#39046;&#22495;&#26159;&#38750;&#24120;&#26377;&#36259;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19968;&#31181;&#31216;&#20026;&#27169;&#25311;&#36864;&#28779;&#30340;&#27969;&#34892;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#22312;&#35813;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#27169;&#25311;&#36864;&#28779;&#30340;&#19968;&#33324;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21508;&#31181;&#37051;&#22495;&#36873;&#25321;&#31574;&#30053;&#21644;&#28201;&#24230;&#20919;&#21364;&#26041;&#26696;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#36229;&#21442;&#25968;&#65292;&#31216;&#20026;&#36827;&#23637;&#21442;&#25968;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#20110;&#36941;&#21382;&#25628;&#32034;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#20116;&#20010;&#20844;&#24320;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#39564;&#35777;&#65292;&#25105;&#20204;&#36824;&#23558;&#22522;&#20110;&#27169;&#25311;&#36864;&#28779;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#19982;&#21478;&#19968;&#31181;&#26377;&#25928;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#21363;&#23616;&#37096;&#27874;&#26463;&#25628;&#32034;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning-to-rank is an applied domain of supervised machine learning. As feature selection has been found to be effective for improving the accuracy of learning models in general, it is intriguing to investigate this process for learning-to-rank domain. In this study, we investigate the use of a popular meta-heuristic approach called simulated annealing for this task. Under the general framework of simulated annealing, we explore various neighborhood selection strategies and temperature cooling schemes. We further introduce a new hyper-parameter called the progress parameter that can effectively be used to traverse the search space. Our algorithms are evaluated on five publicly benchmark datasets of learning-to-rank. For a better validation, we also compare the simulated annealing-based feature selection algorithm with another effective meta-heuristic algorithm, namely local beam search. Extensive experimental results shows the efficacy of our proposed models.
&lt;/p&gt;</description></item><item><title>ManiCast&#26159;&#19968;&#20010;&#22522;&#20110;&#25104;&#26412;&#24863;&#30693;&#30340;&#20154;&#20307;&#39044;&#27979;&#30340;&#21327;&#21516;&#25805;&#20316;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#33021;&#22815;&#25429;&#25417;&#26410;&#26469;&#20154;&#20307;&#36816;&#21160;&#22914;&#20309;&#24433;&#21709;&#26426;&#22120;&#20154;&#35745;&#21010;&#25104;&#26412;&#30340;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#20154;&#26426;&#21327;&#21516;&#25805;&#32437;&#20219;&#21153;&#30340;&#27969;&#30021;&#25191;&#34892;&#21644;&#23454;&#26102;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2310.13258</link><description>&lt;p&gt;
ManiCast: &#22522;&#20110;&#25104;&#26412;&#24863;&#30693;&#20154;&#20307;&#39044;&#27979;&#30340;&#21327;&#21516;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
ManiCast: Collaborative Manipulation with Cost-Aware Human Forecasting. (arXiv:2310.13258v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13258
&lt;/p&gt;
&lt;p&gt;
ManiCast&#26159;&#19968;&#20010;&#22522;&#20110;&#25104;&#26412;&#24863;&#30693;&#30340;&#20154;&#20307;&#39044;&#27979;&#30340;&#21327;&#21516;&#25805;&#20316;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#33021;&#22815;&#25429;&#25417;&#26410;&#26469;&#20154;&#20307;&#36816;&#21160;&#22914;&#20309;&#24433;&#21709;&#26426;&#22120;&#20154;&#35745;&#21010;&#25104;&#26412;&#30340;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#20154;&#26426;&#21327;&#21516;&#25805;&#32437;&#20219;&#21153;&#30340;&#27969;&#30021;&#25191;&#34892;&#21644;&#23454;&#26102;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32039;&#23494;&#21512;&#20316;&#30340;&#20154;&#26426;&#25805;&#32437;&#20381;&#36182;&#20934;&#30830;&#30340;&#20154;&#20307;&#36816;&#21160;&#39044;&#27979;&#12290;&#23613;&#31649;&#22312;&#22823;&#35268;&#27169;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#24403;&#24212;&#29992;&#20110;&#25805;&#20316;&#20219;&#21153;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20851;&#38190;&#36716;&#25240;&#28857;&#22788;&#31215;&#32047;&#20102;&#36739;&#39640;&#30340;&#35823;&#24046;&#65292;&#23548;&#33268;&#19979;&#28216;&#35268;&#21010;&#24615;&#33021;&#30340;&#38477;&#20302;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#19982;&#20854;&#39044;&#27979;&#26368;&#26377;&#21487;&#33021;&#30340;&#20154;&#20307;&#36816;&#21160;&#65292;&#20135;&#29983;&#33021;&#22815;&#25429;&#25417;&#26410;&#26469;&#20154;&#20307;&#36816;&#21160;&#22914;&#20309;&#24433;&#21709;&#26426;&#22120;&#20154;&#35745;&#21010;&#25104;&#26412;&#30340;&#39044;&#27979;&#23601;&#36275;&#22815;&#20102;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ManiCast&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23398;&#20064;&#25104;&#26412;&#24863;&#30693;&#30340;&#20154;&#20307;&#39044;&#27979;&#24182;&#23558;&#20854;&#25552;&#20379;&#32473;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#35268;&#21010;&#22120;&#20197;&#25191;&#34892;&#21327;&#21516;&#25805;&#20316;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23454;&#29616;&#20102;&#20154;&#31867;&#21644;7&#20010;&#33258;&#30001;&#24230;&#30340;&#26426;&#26800;&#33218;&#22312;&#22914;&#21453;&#24212;&#25605;&#25292;&#12289;&#29289;&#20307;&#20132;&#25509;&#21644;&#21327;&#21516;&#25670;&#26700;&#31561;&#22810;&#20010;&#30495;&#23454;&#20219;&#21153;&#20013;&#30340;&#27969;&#30021;&#12289;&#23454;&#26102;&#20132;&#20114;&#12290;&#25105;&#20204;&#23545;&#36816;&#21160;&#39044;&#27979;&#21644;&#31471;&#21040;&#31471;&#30340;&#39044;&#27979;-&#35268;&#21010;&#31995;&#32479;&#36827;&#34892;&#20102;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Seamless human-robot manipulation in close proximity relies on accurate forecasts of human motion. While there has been significant progress in learning forecast models at scale, when applied to manipulation tasks, these models accrue high errors at critical transition points leading to degradation in downstream planning performance. Our key insight is that instead of predicting the most likely human motion, it is sufficient to produce forecasts that capture how future human motion would affect the cost of a robot's plan. We present ManiCast, a novel framework that learns cost-aware human forecasts and feeds them to a model predictive control planner to execute collaborative manipulation tasks. Our framework enables fluid, real-time interactions between a human and a 7-DoF robot arm across a number of real-world tasks such as reactive stirring, object handovers, and collaborative table setting. We evaluate both the motion forecasts and the end-to-end forecaster-planner system against a
&lt;/p&gt;</description></item><item><title>&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#65292;&#20351;&#29992;&#35270;&#35273;&#23450;&#20301;&#36827;&#34892;&#30417;&#30563;&#35757;&#32451;&#30340;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26356;&#25509;&#36817;&#20110;&#20154;&#31867;&#30340;&#35821;&#35328;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.13257</link><description>&lt;p&gt;
&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#65292;&#35270;&#35273;&#23450;&#20301;&#26377;&#21161;&#20110;&#23398;&#20064;&#21333;&#35789;&#30340;&#21547;&#20041;
&lt;/p&gt;
&lt;p&gt;
Visual Grounding Helps Learn Word Meanings in Low-Data Regimes. (arXiv:2310.13257v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13257
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#65292;&#20351;&#29992;&#35270;&#35273;&#23450;&#20301;&#36827;&#34892;&#30417;&#30563;&#35757;&#32451;&#30340;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26356;&#25509;&#36817;&#20110;&#20154;&#31867;&#30340;&#35821;&#35328;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#26159;&#29992;&#20110;&#27169;&#25311;&#20154;&#31867;&#21477;&#23376;&#20135;&#29983;&#21644;&#29702;&#35299;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20854;&#20869;&#37096;&#34920;&#36798;&#19982;&#20154;&#31867;&#22823;&#33041;&#20013;&#30340;&#35821;&#35328;&#34920;&#36798;&#38750;&#24120;&#21563;&#21512;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#21462;&#24471;&#36825;&#20123;&#32467;&#26524;&#65292;LM&#24517;&#39035;&#20197;&#19982;&#20154;&#31867;&#23436;&#20840;&#19981;&#21516;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#38656;&#35201;&#27604;&#20799;&#31461;&#22312;&#21457;&#32946;&#36807;&#31243;&#20013;&#25509;&#25910;&#21040;&#30340;&#35821;&#35328;&#25968;&#25454;&#22810;&#20960;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#19988;&#27809;&#26377;&#20219;&#20309;&#24863;&#30693;&#12289;&#34892;&#21160;&#25110;&#31038;&#20132;&#34892;&#20026;&#30340;&#22522;&#30784;&#12290;&#22914;&#26524;&#29992;&#26356;&#25509;&#36817;&#20154;&#31867;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#21363;&#20381;&#38752;&#24863;&#30693;&#30340;&#30417;&#30563;&#65292;&#27169;&#22411;&#30340;&#35821;&#35328;&#23398;&#20064;&#26159;&#21542;&#26356;&#25509;&#36817;&#20154;&#31867;&#65311;&#25105;&#20204;&#22312;&#21333;&#35789;&#23398;&#20064;&#36825;&#19968;&#35821;&#35328;&#20064;&#24471;&#30340;&#20851;&#38190;&#23376;&#20219;&#21153;&#20013;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31995;&#21015;&#19981;&#21516;&#30340;LM&#26550;&#26500;&#65292;&#24182;&#22312;&#19981;&#21516;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#22270;&#20687;&#23383;&#24149;&#20219;&#21153;&#30340;&#36741;&#21161;&#30417;&#30563;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;&#19968;&#31995;&#21015;&#24191;&#27867;&#30340;&#27979;&#35797;&#26469;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#22312;&#21477;&#27861;&#31867;&#21035;&#12289;&#35789;&#27719;&#20851;&#31995;&#12289;&#35821;&#20041;&#23398;&#31561;&#26041;&#38754;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern neural language models (LMs) are powerful tools for modeling human sentence production and comprehension, and their internal representations are remarkably well-aligned with representations of language in the human brain. But to achieve these results, LMs must be trained in distinctly un-human-like ways -- requiring orders of magnitude more language data than children receive during development, and without any of the accompanying grounding in perception, action, or social behavior. Do models trained more naturalistically -- with grounded supervision -- exhibit more human-like language learning? We investigate this question in the context of word learning, a key sub-task in language acquisition. We train a diverse set of LM architectures, with and without auxiliary supervision from image captioning tasks, on datasets of varying scales. We then evaluate these models on a broad set of benchmarks characterizing models' learning of syntactic categories, lexical relations, semantic f
&lt;/p&gt;</description></item><item><title>TempGNN&#26159;&#19968;&#31181;&#29992;&#20110;&#21160;&#24577;&#20250;&#35805;&#25512;&#33616;&#30340;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#25429;&#25417;&#22797;&#26434;&#29289;&#21697;&#36716;&#25442;&#30340;&#32467;&#26500;&#21644;&#26102;&#38388;&#21160;&#24577;&#65292;&#36890;&#36807;&#23545;&#21160;&#24577;&#20250;&#35805;&#22270;&#20013;&#30340;&#33410;&#28857;&#21644;&#36793;&#36827;&#34892;&#26102;&#38388;&#23884;&#20837;&#25805;&#20316;&#65292;&#26377;&#25928;&#22320;&#39044;&#27979;&#19979;&#19968;&#27493;&#21160;&#20316;&#12290;</title><link>http://arxiv.org/abs/2310.13249</link><description>&lt;p&gt;
TempGNN: &#29992;&#20110;&#21160;&#24577;&#20250;&#35805;&#25512;&#33616;&#30340;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
TempGNN: Temporal Graph Neural Networks for Dynamic Session-Based Recommendations. (arXiv:2310.13249v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13249
&lt;/p&gt;
&lt;p&gt;
TempGNN&#26159;&#19968;&#31181;&#29992;&#20110;&#21160;&#24577;&#20250;&#35805;&#25512;&#33616;&#30340;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#25429;&#25417;&#22797;&#26434;&#29289;&#21697;&#36716;&#25442;&#30340;&#32467;&#26500;&#21644;&#26102;&#38388;&#21160;&#24577;&#65292;&#36890;&#36807;&#23545;&#21160;&#24577;&#20250;&#35805;&#22270;&#20013;&#30340;&#33410;&#28857;&#21644;&#36793;&#36827;&#34892;&#26102;&#38388;&#23884;&#20837;&#25805;&#20316;&#65292;&#26377;&#25928;&#22320;&#39044;&#27979;&#19979;&#19968;&#27493;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#31995;&#32479;&#36890;&#36807;&#29702;&#35299;&#29992;&#25143;&#22312;&#30701;&#26399;&#20250;&#35805;&#20013;&#19982;&#29289;&#21697;&#30340;&#20132;&#20114;&#34892;&#20026;&#26469;&#39044;&#27979;&#19979;&#19968;&#27493;&#21160;&#20316;&#30340;&#26041;&#27861;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12289;&#33258;&#27880;&#24847;&#27169;&#22411;&#21644;&#26368;&#36817;&#30340;&#22823;&#22810;&#25968;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#25429;&#25417;&#20250;&#35805;&#20013;&#22797;&#26434;&#29289;&#21697;&#36716;&#25442;&#30340;&#39034;&#24207;&#20381;&#36182;&#24615;&#12290;&#23613;&#31649;&#26377;&#24456;&#22810;&#26681;&#25454;&#20250;&#35805;&#20013;&#29289;&#21697;&#39034;&#24207;&#30340;&#19981;&#21516;&#27169;&#22411;&#65292;&#20294;&#24456;&#23569;&#26377;&#26041;&#27861;&#26469;&#26356;&#22909;&#22320;&#22788;&#29702;&#20132;&#20114;&#20043;&#38388;&#30340;&#26102;&#38388;&#24847;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Temporal Graph Neural Networks (TempGNN)&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#33410;&#28857;&#21644;&#36793;&#19978;&#30340;&#26102;&#38388;&#23884;&#20837;&#25805;&#20316;&#26469;&#25429;&#25417;&#22797;&#26434;&#29289;&#21697;&#36716;&#25442;&#30340;&#32467;&#26500;&#21644;&#26102;&#38388;&#21160;&#24577;&#65292;&#23558;&#20854;&#34920;&#31034;&#20026;&#23450;&#26102;&#20107;&#20214;&#24207;&#21015;&#30340;&#21160;&#24577;&#20250;&#35805;&#22270;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Session-based recommendations which predict the next action by understanding a user's interaction behavior with items within a relatively short ongoing session have recently gained increasing popularity. Previous research has focused on capturing the dynamics of sequential dependencies from complicated item transitions in a session by means of recurrent neural networks, self-attention models, and recently, mostly graph neural networks. Despite the plethora of different models relying on the order of items in a session, few approaches have been proposed for dealing better with the temporal implications between interactions. We present Temporal Graph Neural Networks (TempGNN), a generic framework for capturing the structural and temporal dynamics in complex item transitions utilizing temporal embedding operators on nodes and edges on dynamic session graphs, represented as sequences of timed events. Extensive experimental results show the effectiveness and adaptability of the proposed met
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FLEE-GNN&#65292;&#23427;&#26159;&#19968;&#31181;&#29992;&#20110;&#20998;&#26512;&#22810;&#31181;&#21830;&#21697;&#39135;&#21697;&#27969;&#30340;&#22320;&#29702;&#31354;&#38388;&#38887;&#24615;&#30340;&#36793;&#32536;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#12290;FLEE-GNN&#20811;&#26381;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#20102;&#20998;&#26512;&#39135;&#21697;&#20379;&#24212;&#32593;&#32476;&#38887;&#24615;&#30340;&#26222;&#36866;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2310.13248</link><description>&lt;p&gt;
FLEE-GNN&#65306;&#19968;&#31181;&#29992;&#20110;&#20998;&#26512;&#22810;&#31181;&#21830;&#21697;&#39135;&#21697;&#27969;&#30340;&#22320;&#29702;&#31354;&#38388;&#38887;&#24615;&#30340;&#36793;&#32536;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
FLEE-GNN: A Federated Learning System for Edge-Enhanced Graph Neural Network in Analyzing Geospatial Resilience of Multicommodity Food Flows. (arXiv:2310.13248v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FLEE-GNN&#65292;&#23427;&#26159;&#19968;&#31181;&#29992;&#20110;&#20998;&#26512;&#22810;&#31181;&#21830;&#21697;&#39135;&#21697;&#27969;&#30340;&#22320;&#29702;&#31354;&#38388;&#38887;&#24615;&#30340;&#36793;&#32536;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#12290;FLEE-GNN&#20811;&#26381;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#20102;&#20998;&#26512;&#39135;&#21697;&#20379;&#24212;&#32593;&#32476;&#38887;&#24615;&#30340;&#26222;&#36866;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21644;&#34913;&#37327;&#39135;&#21697;&#20379;&#24212;&#32593;&#32476;&#30340;&#38887;&#24615;&#26159;&#35299;&#20915;&#26085;&#30410;&#20005;&#37325;&#30340;&#39135;&#21697;&#19981;&#23433;&#20840;&#38382;&#39064;&#30340;&#20840;&#29699;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#23427;&#20204;&#30340;&#22810;&#32500;&#20132;&#20114;&#21644;&#20915;&#31574;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FLEE-GNN&#30340;&#26032;&#22411;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#29992;&#20110;&#36793;&#32536;&#22686;&#24378;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#26088;&#22312;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#22686;&#24378;&#22810;&#31181;&#21830;&#21697;&#39135;&#21697;&#27969;&#32593;&#32476;&#30340;&#22320;&#29702;&#31354;&#38388;&#38887;&#24615;&#20998;&#26512;&#65292;&#23427;&#26159;&#31354;&#38388;&#32593;&#32476;&#30340;&#19968;&#31181;&#31867;&#22411;&#12290;FLEE-GNN&#35299;&#20915;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#20363;&#22914;&#22522;&#20110;&#29109;&#30340;&#26041;&#27861;&#22312;&#26222;&#36866;&#24615;&#65292;&#21487;&#25193;&#23637;&#24615;&#21644;&#25968;&#25454;&#38544;&#31169;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#23427;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#21644;&#36866;&#24212;&#24615;&#19982;&#32852;&#37030;&#23398;&#20064;&#30340;&#25968;&#25454;&#38544;&#31169;&#24847;&#35782;&#21644;&#20998;&#25955;&#29305;&#24615;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#36328;&#22320;&#29702;&#21306;&#22495;&#30340;&#39135;&#21697;&#20379;&#24212;&#32593;&#32476;&#38887;&#24615;&#20998;&#26512;&#12290;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;FLEE-GNN&#30340;&#21019;&#26032;&#25968;&#25454;&#29983;&#25104;&#25216;&#26415;&#65292;&#23454;&#39564;&#35774;&#35745;&#21644;f
&lt;/p&gt;
&lt;p&gt;
Understanding and measuring the resilience of food supply networks is a global imperative to tackle increasing food insecurity. However, the complexity of these networks, with their multidimensional interactions and decisions, presents significant challenges. This paper proposes FLEE-GNN, a novel Federated Learning System for Edge-Enhanced Graph Neural Network, designed to overcome these challenges and enhance the analysis of geospatial resilience of multicommodity food flow network, which is one type of spatial networks. FLEE-GNN addresses the limitations of current methodologies, such as entropy-based methods, in terms of generalizability, scalability, and data privacy. It combines the robustness and adaptability of graph neural networks with the privacy-conscious and decentralized aspects of federated learning on food supply network resilience analysis across geographical regions. This paper also discusses FLEE-GNN's innovative data generation techniques, experimental designs, and f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23618;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#21095;&#26412;&#20013;&#35282;&#33394;&#30340;&#29702;&#35299;&#65292;&#20174;&#35282;&#33394;&#30340;&#35805;&#35821;&#20013;&#23398;&#20064;&#20854;&#20010;&#24615;&#21644;&#36523;&#20221;&#65292;&#24182;&#22312;&#22810;&#20010;&#35282;&#33394;&#29702;&#35299;&#23376;&#20219;&#21153;&#19978;&#36890;&#36807;&#19982;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#27604;&#36739;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.13231</link><description>&lt;p&gt;
&#22810;&#23618;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#22522;&#20110;&#21095;&#26412;&#30340;&#35282;&#33394;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Multi-level Contrastive Learning for Script-based Character Understanding. (arXiv:2310.13231v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23618;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#21095;&#26412;&#20013;&#35282;&#33394;&#30340;&#29702;&#35299;&#65292;&#20174;&#35282;&#33394;&#30340;&#35805;&#35821;&#20013;&#23398;&#20064;&#20854;&#20010;&#24615;&#21644;&#36523;&#20221;&#65292;&#24182;&#22312;&#22810;&#20010;&#35282;&#33394;&#29702;&#35299;&#23376;&#20219;&#21153;&#19978;&#36890;&#36807;&#19982;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#27604;&#36739;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#21095;&#26412;&#20013;&#30340;&#35282;&#33394;&#29702;&#35299;&#22330;&#26223;&#65292;&#26088;&#22312;&#20174;&#20182;&#20204;&#30340;&#35805;&#35821;&#20013;&#23398;&#20064;&#35282;&#33394;&#30340;&#20010;&#24615;&#21644;&#36523;&#20221;&#12290;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#36825;&#19968;&#22330;&#26223;&#20013;&#30340;&#20960;&#20010;&#25361;&#25112;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23618;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#32454;&#31890;&#24230;&#30340;&#26041;&#24335;&#25429;&#25417;&#35282;&#33394;&#30340;&#20840;&#23616;&#20449;&#24687;&#12290;&#20026;&#20102;&#39564;&#35777;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#19982;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#21253;&#25324;SpanBERT&#65292;Longformer&#65292;BigBird&#21644;ChatGPT-3.5&#65289;&#36827;&#34892;&#20102;&#19977;&#20010;&#35282;&#33394;&#29702;&#35299;&#23376;&#20219;&#21153;&#30340;&#24191;&#27867;&#23454;&#39564;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#36890;&#36807;&#28145;&#20837;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35299;&#20915;&#25361;&#25112;&#21644;&#25552;&#20379;&#26356;&#22810;&#20851;&#20110;&#35282;&#33394;&#29702;&#35299;&#22330;&#26223;&#30340;&#32447;&#32034;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23558;&#22312;github&#19978;&#24320;&#28304;&#25105;&#20204;&#30340;&#24037;&#20316;&#65292;&#32593;&#22336;&#20026;https://github.com/David-Li0406/Script-based-Character-Understanding&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we tackle the scenario of understanding characters in scripts, which aims to learn the characters' personalities and identities from their utterances. We begin by analyzing several challenges in this scenario, and then propose a multi-level contrastive learning framework to capture characters' global information in a fine-grained manner. To validate the proposed framework, we conduct extensive experiments on three character understanding sub-tasks by comparing with strong pre-trained language models, including SpanBERT, Longformer, BigBird and ChatGPT-3.5. Experimental results demonstrate that our method improves the performances by a considerable margin. Through further in-depth analysis, we show the effectiveness of our method in addressing the challenges and provide more hints on the scenario of character understanding. We will open-source our work on github at https://github.com/David-Li0406/Script-based-Character-Understanding.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#32477;&#23545;&#31574;&#30053;&#20248;&#21270;&#65288;APO&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#22312;&#20445;&#35777;&#24615;&#33021;&#19979;&#30028;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#21644;Atari&#28216;&#25103;&#20013;&#30340;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13230</link><description>&lt;p&gt;
&#32477;&#23545;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Absolute Policy Optimization. (arXiv:2310.13230v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13230
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#32477;&#23545;&#31574;&#30053;&#20248;&#21270;&#65288;APO&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#22312;&#20445;&#35777;&#24615;&#33021;&#19979;&#30028;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#21644;Atari&#28216;&#25103;&#20013;&#30340;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#20449;&#20219;&#22495;&#30340;&#22312;&#32447;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#22312;&#35299;&#20915;&#22797;&#26434;&#25511;&#21046;&#20219;&#21153;&#21644;&#28216;&#25103;&#22330;&#26223;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#31867;&#21035;&#20013;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#20027;&#35201;&#24378;&#35843;&#23545;&#39044;&#26399;&#24615;&#33021;&#30340;&#25913;&#36827;&#65292;&#32570;&#20047;&#23545;&#26368;&#22351;&#24773;&#20917;&#19979;&#24615;&#33021;&#32467;&#26524;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#65307;&#36890;&#36807;&#20248;&#21270;&#35813;&#20989;&#25968;&#65292;&#21487;&#20197;&#30830;&#20445;&#36817;&#20046;&#24635;&#20307;&#24615;&#33021;&#26679;&#26412;&#30340;&#19979;&#30028;&#65288;&#32477;&#23545;&#24615;&#33021;&#65289;&#21576;&#29616;&#21333;&#35843;&#25913;&#36827;&#12290;&#32771;&#34385;&#21040;&#36825;&#19968;&#20855;&#26377;&#31361;&#30772;&#24615;&#30340;&#29702;&#35770;&#36827;&#23637;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#36817;&#20284;&#23545;&#36825;&#20010;&#29702;&#35770;&#22522;&#30784;&#31639;&#27861;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#24471;&#21040;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#31216;&#20026;&#32477;&#23545;&#31574;&#30053;&#20248;&#21270;&#65288;APO&#65289;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23558;&#20854;&#36866;&#29992;&#24615;&#25193;&#23637;&#21040;&#25484;&#25569;Atari&#28216;&#25103;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;APO&#22312;&#25552;&#39640;&#24615;&#33021;&#30340;&#21516;&#26102;&#20063;&#26174;&#33879;&#25913;&#21892;&#20102;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, trust region on-policy reinforcement learning has achieved impressive results in addressing complex control tasks and gaming scenarios. However, contemporary state-of-the-art algorithms within this category primarily emphasize improvement in expected performance, lacking the ability to control over the worst-case performance outcomes. To address this limitation, we introduce a novel objective function; by optimizing which, it will lead to guaranteed monotonic improvement in the lower bound of near-total performance samples (absolute performance). Considering this groundbreaking theoretical advancement, we then refine this theoretically grounded algorithm through a series of approximations, resulting in a practical solution called Absolute Policy Optimization (APO). Our experiments demonstrate the effectiveness of our approach across challenging continuous control benchmark tasks and extend its applicability to mastering Atari games. Our findings reveal that APO signifi
&lt;/p&gt;</description></item><item><title>ToolChain*&#26159;&#19968;&#31181;&#25552;&#20379;&#20102;&#39640;&#25928;&#21160;&#20316;&#31354;&#38388;&#23548;&#33322;&#30340;&#26641;&#25628;&#32034;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#20915;&#31574;&#21644;&#35268;&#21010;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.13227</link><description>&lt;p&gt;
ToolChain*: &#20351;&#29992;A*&#25628;&#32034;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#39640;&#25928;&#30340;&#21160;&#20316;&#31354;&#38388;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
ToolChain*: Efficient Action Space Navigation in Large Language Models with A* Search. (arXiv:2310.13227v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13227
&lt;/p&gt;
&lt;p&gt;
ToolChain*&#26159;&#19968;&#31181;&#25552;&#20379;&#20102;&#39640;&#25928;&#21160;&#20316;&#31354;&#38388;&#23548;&#33322;&#30340;&#26641;&#25628;&#32034;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#20915;&#31574;&#21644;&#35268;&#21010;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#35299;&#20915;&#22797;&#26434;&#30340;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#26102;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#20915;&#31574;&#21644;&#35268;&#21010;&#33021;&#21147;&#12290;&#22522;&#20110;LLM&#30340;&#33258;&#20027;&#20195;&#29702;&#21487;&#20197;&#19982;&#21508;&#31181;&#24037;&#20855;&#65288;&#20363;&#22914;&#21151;&#33021;&#24615;API&#65289;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#29983;&#25104;&#25191;&#34892;&#19968;&#31995;&#21015;API&#20989;&#25968;&#35843;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#35745;&#21010;&#12290;&#20505;&#36873;API&#20989;&#25968;&#35843;&#29992;&#30340;&#22810;&#26679;&#24615;&#26174;&#33879;&#25193;&#23637;&#20102;&#21160;&#20316;&#31354;&#38388;&#65292;&#21152;&#22823;&#20102;&#23545;&#39640;&#25928;&#21160;&#20316;&#31354;&#38388;&#23548;&#33322;&#30340;&#20851;&#38190;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#22312;&#24222;&#22823;&#30340;&#21160;&#20316;&#31354;&#38388;&#20013;&#38754;&#20020;&#21333;&#21521;&#25506;&#32034;&#22256;&#38590;&#65292;&#38519;&#20837;&#23616;&#37096;&#20248;&#21270;&#35299;&#65292;&#35201;&#20040;&#36973;&#21463;&#31351;&#23613;&#22320;&#36941;&#21382;&#25152;&#26377;&#28508;&#22312;&#21160;&#20316;&#25152;&#23548;&#33268;&#30340;&#20302;&#25928;&#23548;&#33322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ToolChain*&#65292;&#19968;&#31181;&#22522;&#20110;&#26641;&#25628;&#32034;&#30340;LLM&#20195;&#29702;&#35268;&#21010;&#31639;&#27861;&#12290;&#23427;&#23558;&#25972;&#20010;&#21160;&#20316;&#31354;&#38388;&#26500;&#24314;&#20026;&#19968;&#20010;&#20915;&#31574;&#26641;&#65292;&#20854;&#20013;&#27599;&#20010;&#33410;&#28857;&#34920;&#31034;&#35299;&#20915;&#26041;&#26696;&#35745;&#21010;&#20013;&#28041;&#21450;&#30340;&#21487;&#33021;&#30340;API&#20989;&#25968;&#35843;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated powerful decision-making and planning capabilities in solving complicated real-world problems. LLM-based autonomous agents can interact with diverse tools (e.g., functional APIs) and generate solution plans that execute a series of API function calls in a step-by-step manner. The multitude of candidate API function calls significantly expands the action space, amplifying the critical need for efficient action space navigation. However, existing methods either struggle with unidirectional exploration in expansive action spaces, trapped into a locally optimal solution, or suffer from exhaustively traversing all potential actions, causing inefficient navigation. To address these issues, we propose ToolChain*, an efficient tree search-based planning algorithm for LLM-based agents. It formulates the entire action space as a decision tree, where each node represents a possible API function call involved in a solution plan. By incorporating the A
&lt;/p&gt;</description></item><item><title>&#21487;&#25193;&#23637;&#31070;&#32463;&#32593;&#32476;&#20869;&#26680;&#65288;SNNKs&#65289;&#26159;&#19968;&#31181;&#26367;&#20195;&#24120;&#35268;&#21069;&#39304;&#23618;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36817;&#20284;&#23454;&#29616;&#24120;&#35268;&#21069;&#39304;&#23618;&#30340;&#21151;&#33021;&#65292;&#20294;&#20855;&#26377;&#26356;&#20248;&#30340;&#35745;&#31639;&#29305;&#24615;&#12290;&#36890;&#36807;&#23558;&#20869;&#26680;&#19982;&#21442;&#25968;-&#36755;&#20837;&#21521;&#37327;&#30340;&#28857;&#31215;&#32852;&#31995;&#36215;&#26469;&#65292;SNNKs&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#24320;&#21442;&#25968;&#19982;&#36755;&#20837;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20174;&#32780;&#27169;&#25311;&#22797;&#26434;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#31070;&#32463;&#32593;&#32476;&#25414;&#32465;&#36807;&#31243;&#65292;&#23558;SNNKs&#24212;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#21387;&#32553;&#25928;&#26524;&#12290;&#26368;&#32456;&#25414;&#32465;&#32593;&#32476;&#29978;&#33267;&#21487;&#20197;&#32469;&#36807;&#21453;&#21521;&#20256;&#25773;&#65292;&#36890;&#36807;&#26174;&#24335;&#20844;&#24335;&#27714;&#35299;&#26368;&#20248;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.13225</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;&#20869;&#26680;
&lt;/p&gt;
&lt;p&gt;
Scalable Neural Network Kernels. (arXiv:2310.13225v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13225
&lt;/p&gt;
&lt;p&gt;
&#21487;&#25193;&#23637;&#31070;&#32463;&#32593;&#32476;&#20869;&#26680;&#65288;SNNKs&#65289;&#26159;&#19968;&#31181;&#26367;&#20195;&#24120;&#35268;&#21069;&#39304;&#23618;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36817;&#20284;&#23454;&#29616;&#24120;&#35268;&#21069;&#39304;&#23618;&#30340;&#21151;&#33021;&#65292;&#20294;&#20855;&#26377;&#26356;&#20248;&#30340;&#35745;&#31639;&#29305;&#24615;&#12290;&#36890;&#36807;&#23558;&#20869;&#26680;&#19982;&#21442;&#25968;-&#36755;&#20837;&#21521;&#37327;&#30340;&#28857;&#31215;&#32852;&#31995;&#36215;&#26469;&#65292;SNNKs&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#24320;&#21442;&#25968;&#19982;&#36755;&#20837;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20174;&#32780;&#27169;&#25311;&#22797;&#26434;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#31070;&#32463;&#32593;&#32476;&#25414;&#32465;&#36807;&#31243;&#65292;&#23558;SNNKs&#24212;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#21387;&#32553;&#25928;&#26524;&#12290;&#26368;&#32456;&#25414;&#32465;&#32593;&#32476;&#29978;&#33267;&#21487;&#20197;&#32469;&#36807;&#21453;&#21521;&#20256;&#25773;&#65292;&#36890;&#36807;&#26174;&#24335;&#20844;&#24335;&#27714;&#35299;&#26368;&#20248;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#25193;&#23637;&#31070;&#32463;&#32593;&#32476;&#20869;&#26680;&#65288;SNNKs&#65289;&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;&#24120;&#35268;&#21069;&#39304;&#23618;&#65288;FFLs&#65289;&#30340;&#26367;&#20195;&#21697;&#65292;&#33021;&#22815;&#36817;&#20284;&#23454;&#29616;&#21518;&#32773;&#65292;&#20294;&#20855;&#26377;&#26377;&#21033;&#30340;&#35745;&#31639;&#23646;&#24615;&#12290;SNNKs&#26377;&#25928;&#22320;&#35299;&#24320;&#20102;FFL&#20013;&#21442;&#25968;&#19982;&#36755;&#20837;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#36890;&#36807;&#28857;&#31215;&#20869;&#26680;&#22312;&#26368;&#32456;&#35745;&#31639;&#20013;&#36830;&#25509;&#23427;&#20204;&#12290;&#23427;&#20204;&#20063;&#26356;&#21152;&#34920;&#36798;&#21147;&#24378;&#65292;&#33021;&#22815;&#27169;&#25311;&#22797;&#26434;&#20851;&#31995;&#65292;&#36229;&#20986;&#21442;&#25968;-&#36755;&#20837;&#21521;&#37327;&#30340;&#20989;&#25968;&#33539;&#22260;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#31070;&#32463;&#32593;&#32476;&#25414;&#32465;&#36807;&#31243;&#65292;&#23558;SNNKs&#24212;&#29992;&#20110;&#21387;&#32553;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#20174;&#32780;&#33719;&#24471;&#39069;&#22806;&#30340;&#21387;&#32553;&#25928;&#30410;&#12290;&#22312;&#26497;&#31471;&#24773;&#20917;&#19979;&#65292;&#23427;&#23548;&#33268;&#23436;&#20840;&#25414;&#32465;&#32593;&#32476;&#65292;&#20854;&#26368;&#20248;&#21442;&#25968;&#21487;&#20197;&#36890;&#36807;&#22810;&#20010;&#25439;&#22833;&#20989;&#25968;&#65288;&#20363;&#22914;&#22343;&#26041;&#35823;&#24046;&#65289;&#30340;&#26174;&#24335;&#20844;&#24335;&#26469;&#34920;&#31034;&#65292;&#20174;&#32780;&#26377;&#21487;&#33021;&#32469;&#36807;&#21453;&#21521;&#20256;&#25773;&#12290;&#20316;&#20026;&#25105;&#20204;&#20998;&#26512;&#30340;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26222;&#36941;&#24615;&#26426;&#21046;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the concept of scalable neural network kernels (SNNKs), the replacements of regular feedforward layers (FFLs), capable of approximating the latter, but with favorable computational properties. SNNKs effectively disentangle the inputs from the parameters of the neural network in the FFL, only to connect them in the final computation via the dot-product kernel. They are also strictly more expressive, as allowing to model complicated relationships beyond the functions of the dot-products of parameter-input vectors. We also introduce the neural network bundling process that applies SNNKs to compactify deep neural network architectures, resulting in additional compression gains. In its extreme version, it leads to the fully bundled network whose optimal parameters can be expressed via explicit formulae for several loss functions (e.g. mean squared error), opening a possibility to bypass backpropagation. As a by-product of our analysis, we introduce the mechanism of the universa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HierCas&#30340;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20449;&#24687;&#32423;&#32852;&#27969;&#34892;&#24230;&#39044;&#27979;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;HierCas&#37319;&#29992;&#21160;&#24577;&#22270;&#24314;&#27169;&#26041;&#27861;&#65292;&#22312;&#25972;&#20010;&#32423;&#32852;&#22270;&#19978;&#36827;&#34892;&#25805;&#20316;&#65292;&#33021;&#22815;&#25429;&#25417;&#21040;&#36830;&#32493;&#21160;&#24577;&#20449;&#24687;&#30340;&#23436;&#25972;&#33539;&#22260;&#65292;&#24182;&#26126;&#30830;&#24314;&#27169;&#32467;&#26500;&#21644;&#26102;&#38388;&#22240;&#32032;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.13219</link><description>&lt;p&gt;
HierCas: &#20449;&#24687;&#32423;&#32852;&#20013;&#30340;&#20998;&#23618;&#26102;&#24207;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#29992;&#20110;&#27969;&#34892;&#24230;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
HierCas: Hierarchical Temporal Graph Attention Networks for Popularity Prediction in Information Cascades. (arXiv:2310.13219v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HierCas&#30340;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20449;&#24687;&#32423;&#32852;&#27969;&#34892;&#24230;&#39044;&#27979;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;HierCas&#37319;&#29992;&#21160;&#24577;&#22270;&#24314;&#27169;&#26041;&#27861;&#65292;&#22312;&#25972;&#20010;&#32423;&#32852;&#22270;&#19978;&#36827;&#34892;&#25805;&#20316;&#65292;&#33021;&#22815;&#25429;&#25417;&#21040;&#36830;&#32493;&#21160;&#24577;&#20449;&#24687;&#30340;&#23436;&#25972;&#33539;&#22260;&#65292;&#24182;&#26126;&#30830;&#24314;&#27169;&#32467;&#26500;&#21644;&#26102;&#38388;&#22240;&#32032;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#32423;&#32852;&#27969;&#34892;&#24230;&#39044;&#27979;&#23545;&#20110;&#35782;&#21035;&#20551;&#26032;&#38395;&#21644;&#31934;&#20934;&#25512;&#33616;&#31561;&#35768;&#22810;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#20256;&#32479;&#22522;&#20110;&#29305;&#24449;&#30340;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#25163;&#24037;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#26159;&#29305;&#23450;&#39046;&#22495;&#30340;&#65292;&#32570;&#20047;&#23545;&#26032;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#36716;&#21521;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#37319;&#29992;&#37319;&#26679;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#33021;&#20002;&#22833;&#20449;&#24687;&#25193;&#25955;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#36830;&#32493;&#21160;&#24577;&#20449;&#24687;&#21644;&#32467;&#26500;-&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HierCas&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#32423;&#32852;&#27969;&#34892;&#24230;&#39044;&#27979;&#65292;&#23427;&#37319;&#29992;&#21160;&#24577;&#22270;&#24314;&#27169;&#26041;&#27861;&#65292;&#22312;&#25972;&#20010;&#32423;&#32852;&#22270;&#19978;&#36827;&#34892;&#25805;&#20316;&#65292;&#33021;&#22815;&#25429;&#25417;&#21040;&#36830;&#32493;&#21160;&#24577;&#20449;&#24687;&#30340;&#23436;&#25972;&#33539;&#22260;&#65292;&#24182;&#26126;&#30830;&#24314;&#27169;&#32467;&#26500;&#21644;&#26102;&#38388;&#22240;&#32032;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information cascade popularity prediction is critical for many applications, including but not limited to identifying fake news and accurate recommendations. Traditional feature-based methods heavily rely on handcrafted features, which are domain-specific and lack generalizability to new domains. To address this problem, researchers have turned to neural network-based approaches. However, existing methods follow a sampling-based modeling approach, potentially losing continuous dynamic information and structural-temporal dependencies that emerge during the information diffusion process. In this paper, we propose a novel framework called Hierarchical Temporal Graph Attention Networks for cascade popularity prediction (HierCas). Unlike existing methods, HierCas operates on the entire cascade graph by a dynamic graph modeling approach, enabling it to capture the full range of continuous dynamic information and explicitly model the interplay between structural and temporal factors. By lever
&lt;/p&gt;</description></item><item><title>MultiCoNER v2&#26159;&#19968;&#20010;&#22823;&#22411;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#32454;&#31890;&#24230;&#21644;&#21547;&#22122;&#38899;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;&#23427;&#35299;&#20915;&#20102;&#32454;&#31890;&#24230;&#31867;&#21035;&#22788;&#29702;&#21644;&#22122;&#38899;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#30340;&#23454;&#38469;&#25361;&#25112;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#32454;&#31890;&#24230;&#20998;&#31867;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#32780;&#23454;&#20307;&#25439;&#22351;&#23545;&#24615;&#33021;&#24433;&#21709;&#26356;&#22823;&#12290;</title><link>http://arxiv.org/abs/2310.13213</link><description>&lt;p&gt;
MultiCoNER v2: &#19968;&#20010;&#29992;&#20110;&#32454;&#31890;&#24230;&#21644;&#21547;&#22122;&#38899;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#22823;&#22411;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MultiCoNER v2: a Large Multilingual dataset for Fine-grained and Noisy Named Entity Recognition. (arXiv:2310.13213v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13213
&lt;/p&gt;
&lt;p&gt;
MultiCoNER v2&#26159;&#19968;&#20010;&#22823;&#22411;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#32454;&#31890;&#24230;&#21644;&#21547;&#22122;&#38899;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;&#23427;&#35299;&#20915;&#20102;&#32454;&#31890;&#24230;&#31867;&#21035;&#22788;&#29702;&#21644;&#22122;&#38899;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#30340;&#23454;&#38469;&#25361;&#25112;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#32454;&#31890;&#24230;&#20998;&#31867;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#32780;&#23454;&#20307;&#25439;&#22351;&#23545;&#24615;&#33021;&#24433;&#21709;&#26356;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MULTICONER V2&#65292;&#36825;&#26159;&#19968;&#20010;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;12&#31181;&#35821;&#35328;&#20013;&#30340;33&#20010;&#23454;&#20307;&#31867;&#21035;&#65292;&#21253;&#25324;&#21333;&#35821;&#21644;&#22810;&#35821;&#22659;&#30340;&#35774;&#32622;&#12290;&#35813;&#25968;&#25454;&#38598;&#26088;&#22312;&#35299;&#20915;NER&#20013;&#30340;&#20197;&#19979;&#23454;&#38469;&#25361;&#25112;&#65306;(i) &#26377;&#25928;&#22788;&#29702;&#21253;&#25324;&#30005;&#24433;&#26631;&#39064;&#31561;&#22797;&#26434;&#23454;&#20307;&#30340;&#32454;&#31890;&#24230;&#31867;&#21035;&#65292;&#20197;&#21450;(ii) &#30001;&#20110;&#25171;&#23383;&#38169;&#35823;&#25110;OCR&#38169;&#35823;&#32780;&#20135;&#29983;&#30340;&#22122;&#38899;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#20174;&#32500;&#22522;&#30334;&#31185;&#21644;&#32500;&#22522;&#25968;&#25454;&#31561;&#20844;&#24320;&#36164;&#28304;&#20013;&#32534;&#35793;&#32780;&#25104;&#65292;&#24182;&#21487;&#20844;&#24320;&#33719;&#21462;&#12290;&#22522;&#20110;XLM-RoBERTa&#22522;&#20934;&#30340;&#35780;&#20272;&#31361;&#20986;&#20102;MULTICONER V2&#25152;&#38754;&#20020;&#30340;&#29420;&#29305;&#25361;&#25112;&#65306;(i)&#32454;&#31890;&#24230;&#20998;&#31867;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22312;&#25152;&#26377;&#35821;&#35328;&#20013;&#30340;&#23439;F1&#24471;&#20998;&#36739;&#20302;&#65292;&#20026;0.63&#65307;&#20197;&#21450;(ii)&#25439;&#22351;&#31574;&#30053;&#26174;&#33879;&#24433;&#21709;&#24615;&#33021;&#65292;&#23454;&#20307;&#25439;&#22351;&#23548;&#33268;&#30340;&#24615;&#33021;&#27604;&#38750;&#23454;&#20307;&#25439;&#22351;&#20302;9%&#12290;&#36825;&#31361;&#26174;&#20102;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#23454;&#20307;&#22122;&#38899;&#30456;&#27604;&#20110;&#20854;&#20182;&#22122;&#38899;&#23545;&#24615;&#33021;&#30340;&#26356;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present MULTICONER V2, a dataset for fine-grained Named Entity Recognition covering 33 entity classes across 12 languages, in both monolingual and multilingual settings. This dataset aims to tackle the following practical challenges in NER: (i) effective handling of fine-grained classes that include complex entities like movie titles, and (ii) performance degradation due to noise generated from typing mistakes or OCR errors. The dataset is compiled from open resources like Wikipedia and Wikidata, and is publicly available. Evaluation based on the XLM-RoBERTa baseline highlights the unique challenges posed by MULTICONER V2: (i) the fine-grained taxonomy is challenging, where the scores are low with macro-F1=0.63 (across all languages), and (ii) the corruption strategy significantly impairs performance, with entity corruption resulting in 9% lower performance relative to non-entity corruptions across all languages. This highlights the greater impact of entity noise in contrast to cont
&lt;/p&gt;</description></item><item><title>ChatGPT&#22312;&#36873;&#25321;&#31572;&#26696;&#26102;&#34920;&#29616;&#20986;&#21021;&#21360;&#35937;&#25928;&#24212;&#65292;&#21363;&#26356;&#20542;&#21521;&#20110;&#36873;&#25321;&#25552;&#31034;&#20013;&#36739;&#26089;&#20301;&#32622;&#30340;&#26631;&#31614;&#20316;&#20026;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.13206</link><description>&lt;p&gt;
ChatGPT&#30340;&#21021;&#21360;&#35937;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Primacy Effect of ChatGPT. (arXiv:2310.13206v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13206
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22312;&#36873;&#25321;&#31572;&#26696;&#26102;&#34920;&#29616;&#20986;&#21021;&#21360;&#35937;&#25928;&#24212;&#65292;&#21363;&#26356;&#20542;&#21521;&#20110;&#36873;&#25321;&#25552;&#31034;&#20013;&#36739;&#26089;&#20301;&#32622;&#30340;&#26631;&#31614;&#20316;&#20026;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;ChatGPT&#19978;&#36827;&#34892;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;ChatGPT&#30340;&#21021;&#21360;&#35937;&#25928;&#24212;&#65292;&#21363;&#36873;&#25321;&#36739;&#26089;&#20301;&#32622;&#30340;&#26631;&#31614;&#20316;&#20026;&#31572;&#26696;&#30340;&#20542;&#21521;&#12290;&#25105;&#20204;&#21457;&#29616;&#65306;i&#65289;ChatGPT&#30340;&#20915;&#31574;&#23545;&#25552;&#31034;&#20013;&#26631;&#31614;&#30340;&#39034;&#24207;&#25935;&#24863;&#65307;ii&#65289;ChatGPT&#26356;&#20542;&#21521;&#20110;&#36873;&#25321;&#36739;&#26089;&#20301;&#32622;&#30340;&#26631;&#31614;&#20316;&#20026;&#31572;&#26696;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#33021;&#20026;&#26500;&#24314;&#26356;&#21487;&#38752;&#30340;&#22522;&#20110;ChatGPT&#30340;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#39069;&#22806;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned large language models (LLMs), such as ChatGPT, have led to promising zero-shot performance in discriminative natural language understanding (NLU) tasks. This involves querying the LLM using a prompt containing the question, and the candidate labels to choose from. The question-answering capabilities of ChatGPT arise from its pre-training on large amounts of human-written text, as well as its subsequent fine-tuning on human preferences, which motivates us to ask: Does ChatGPT also inherits humans' cognitive biases? In this paper, we study the primacy effect of ChatGPT: the tendency of selecting the labels at earlier positions as the answer. We have two main findings: i) ChatGPT's decision is sensitive to the order of labels in the prompt; ii) ChatGPT has a clearly higher chance to select the labels at earlier positions as the answer. We hope that our experiments and analyses provide additional insights into building more reliable ChatGPT-based solutions. We release the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#31639;&#27861;&#30340;&#19981;&#36879;&#26126;&#24615;&#65292;&#37325;&#28857;&#20851;&#27880;&#20154;&#24037;&#26234;&#33021;&#22312;&#22240;&#26524;&#36131;&#20219;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#23545;&#30446;&#21069;&#26368;&#22909;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65288;Chat-GPT&#65289;&#30340;&#35780;&#20272;&#65292;&#21487;&#20197;&#20102;&#35299;&#20854;&#30446;&#21069;&#30340;&#24615;&#33021;&#20197;&#21450;&#21487;&#33021;&#30340;&#27861;&#24459;&#35268;&#21046;&#24418;&#24335;&#12290;</title><link>http://arxiv.org/abs/2310.13192</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#19981;&#36879;&#26126;&#27861;&#24459;
&lt;/p&gt;
&lt;p&gt;
The opaque law of artificial intelligence. (arXiv:2310.13192v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#31639;&#27861;&#30340;&#19981;&#36879;&#26126;&#24615;&#65292;&#37325;&#28857;&#20851;&#27880;&#20154;&#24037;&#26234;&#33021;&#22312;&#22240;&#26524;&#36131;&#20219;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#23545;&#30446;&#21069;&#26368;&#22909;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65288;Chat-GPT&#65289;&#30340;&#35780;&#20272;&#65292;&#21487;&#20197;&#20102;&#35299;&#20854;&#30446;&#21069;&#30340;&#24615;&#33021;&#20197;&#21450;&#21487;&#33021;&#30340;&#27861;&#24459;&#35268;&#21046;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#20998;&#26512;&#31639;&#27861;&#30340;&#19981;&#36879;&#26126;&#24615;&#65292;&#24182;&#23558;&#20854;&#32622;&#20110;&#20154;&#24037;&#26234;&#33021;&#22240;&#26524;&#36131;&#20219;&#30340;&#20844;&#24320;&#36777;&#35770;&#32972;&#26223;&#19979;&#36827;&#34892;&#35752;&#35770;&#65307;&#36890;&#36807;&#24212;&#29992;&#22270;&#28789;&#27979;&#35797;&#20013;&#25552;&#20986;&#30340;&#23545;&#35805;&#26041;&#27861;&#65292;&#25105;&#20204;&#24076;&#26395;&#35780;&#20272;&#29616;&#26377;&#26368;&#22909;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65288;Chat-GPT&#65289;&#30340;&#24615;&#33021;&#65292;&#20197;&#30830;&#23450;&#20854;&#30446;&#21069;&#30340;&#33021;&#21147;&#21644;&#21487;&#33021;&#30340;&#27861;&#24459;&#35268;&#21046;&#24418;&#24335;&#12290;&#38382;&#39064;&#20998;&#26512;&#23558;&#22522;&#20110;&#23545;&#20256;&#32479;&#27861;&#24459;&#33539;&#30068;&#65288;&#22914;&#22240;&#26524;&#20851;&#31995;&#12289;&#24847;&#22270;&#21644;&#36807;&#22833;&#65289;&#30340;&#35780;&#35770;&#65292;&#20197;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#20351;&#29992;&#20013;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#20154;&#26426;&#20132;&#20114;&#12290;&#20174;&#35745;&#31639;&#26426;&#31185;&#23398;&#35282;&#24230;&#26469;&#30475;&#65292;&#25991;&#20013;&#36824;&#23558;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;Chat-GPT&#36827;&#34892;&#23454;&#38469;&#35810;&#38382;&#30340;&#26041;&#27861;&#65292;&#20197;&#25214;&#21040;&#20154;&#24037;&#26234;&#33021;&#36816;&#34892;&#36807;&#31243;&#20013;&#30340;&#19968;&#20123;&#20851;&#38190;&#38382;&#39064;&#12290;&#25991;&#31456;&#30340;&#32467;&#23614;&#23558;&#38598;&#20013;&#35752;&#35770;&#19968;&#20123;&#29616;&#26377;&#30340;&#31435;&#27861;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
The purpose of this paper is to analyse the opacity of algorithms, contextualized in the open debate on responsibility for artificial intelligence causation; with an experimental approach by which, applying the proposed conversational methodology of the Turing Test, we expect to evaluate the performance of one of the best existing NLP model of generative AI (Chat-GPT) to see how far it can go right now and how the shape of a legal regulation of it could be. The analysis of the problem will be supported by a comment of Italian classical law categories such as causality, intent and fault to understand the problem of the usage of AI, focusing in particular on the human-machine interaction. On the computer science side, for a technical point of view of the logic used to craft these algorithms, in the second chapter will be proposed a practical interrogation of Chat-GPT aimed at finding some critical points of the functioning of AI. The end of the paper will concentrate on some existing leg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#30693;&#35782;&#20445;&#30041;&#21098;&#26525;&#31574;&#30053;&#65292;&#26088;&#22312;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#21098;&#26525;&#36807;&#31243;&#20013;&#20445;&#30041;&#26356;&#22810;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.13191</link><description>&lt;p&gt;
&#26397;&#30528;&#40065;&#26834;&#21098;&#26525;&#65306;&#19968;&#31181;&#38754;&#21521;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#30693;&#35782;&#20445;&#30041;&#21098;&#26525;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy for Language Models. (arXiv:2310.13191v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#30693;&#35782;&#20445;&#30041;&#21098;&#26525;&#31574;&#30053;&#65292;&#26088;&#22312;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#21098;&#26525;&#36807;&#31243;&#20013;&#20445;&#30041;&#26356;&#22810;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21098;&#26525;&#30446;&#26631;&#36817;&#26399;&#19981;&#20165;&#20165;&#23616;&#38480;&#20110;&#20934;&#30830;&#24615;&#21644;&#31232;&#30095;&#24615;&#65292;&#36824;&#21253;&#25324;&#23545;&#35821;&#35328;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#25345;&#32493;&#22686;&#21152;&#27169;&#22411;&#31232;&#30095;&#24615;&#26102;&#65292;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#25552;&#21319;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#36807;&#31243;&#12290;&#38543;&#30528;&#20154;&#20204;&#27493;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#20195;&#65292;&#36825;&#20123;&#38382;&#39064;&#21464;&#24471;&#36234;&#26469;&#36234;&#31361;&#20986;&#12290;&#26412;&#25991;&#25552;&#20986;&#65292;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#19982;&#20854;&#28085;&#30422;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#31243;&#24230;&#25104;&#27491;&#27604;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21518;&#35757;&#32451;&#30340;&#21098;&#26525;&#31574;&#30053;&#65292;&#26088;&#22312;&#22312;&#21098;&#26525;&#36807;&#31243;&#20013;&#20445;&#30041;&#26356;&#22810;&#39044;&#35757;&#32451;&#30693;&#35782;&#65292;&#20197;&#24544;&#23454;&#22320;&#22797;&#21046;&#23494;&#38598;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#21644;&#29305;&#24449;&#31354;&#38388;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#27599;&#19968;&#23618;&#30340;&#37325;&#26500;&#35823;&#24046;&#19981;&#20165;&#28304;&#33258;&#33258;&#36523;&#65292;&#36824;&#21253;&#25324;&#21069;&#38754;&#23618;&#30340;&#32047;&#31215;&#35823;&#24046;&#65292;&#28982;&#21518;&#36827;&#34892;&#33258;&#36866;&#24212;&#30340;&#30699;&#27491;&#12290;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The pruning objective has recently extended beyond accuracy and sparsity to robustness in language models. Despite this, existing methods struggle to enhance robustness against adversarial attacks when continually increasing model sparsity and require a retraining process. As humans step into the era of large language models, these issues become increasingly prominent. This paper proposes that the robustness of language models is proportional to the extent of pre-trained knowledge they encompass. Accordingly, we introduce a post-training pruning strategy designed to faithfully replicate the embedding space and feature space of dense language models, aiming to conserve more pre-trained knowledge during the pruning process. In this setup, each layer's reconstruction error not only originates from itself but also includes cumulative error from preceding layers, followed by an adaptive rectification. Compared to other state-of-art baselines, our approach demonstrates a superior balance bet
&lt;/p&gt;</description></item><item><title>SCALE&#26159;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#38271;&#25991;&#26412;&#20013;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;&#22359;&#21270;&#31574;&#30053;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#33021;&#35299;&#37322;&#20915;&#31574;&#24182;&#36229;&#36807;&#31454;&#20105;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2310.13189</link><description>&lt;p&gt;
&#38271;&#25991;&#26723;&#20013;&#24555;&#36895;&#20934;&#30830;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Fast and Accurate Factual Inconsistency Detection Over Long Documents. (arXiv:2310.13189v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13189
&lt;/p&gt;
&lt;p&gt;
SCALE&#26159;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#38271;&#25991;&#26412;&#20013;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;&#22359;&#21270;&#31574;&#30053;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#33021;&#35299;&#37322;&#20915;&#31574;&#24182;&#36229;&#36807;&#31454;&#20105;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;AI&#27169;&#22411;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65307;&#28982;&#32780;&#65292;&#23545;&#20110;&#24403;&#21069;&#26041;&#27861;&#38590;&#20197;&#26377;&#25928;&#35299;&#20915;&#30340;&#36739;&#38271;&#36755;&#20837;&#20013;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#24187;&#35273;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;SCALE&#65288;&#29992;&#20110;&#22823;&#35268;&#27169;&#19981;&#19968;&#33268;&#24615;&#35780;&#20272;&#30340;&#28304;&#22359;&#21270;&#26041;&#27861;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22359;&#21270;&#31574;&#30053;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SCALE&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#30340;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#22823;&#30340;&#25991;&#26412;&#22359;&#23545;&#38271;&#25991;&#26412;&#36827;&#34892;&#26465;&#20214;&#21270;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#38271;&#25991;&#26412;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#26816;&#27979;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#22359;&#21270;&#26426;&#21046;&#24182;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#36890;&#36807;&#30456;&#20851;&#30340;&#28304;&#35821;&#21477;&#26816;&#32034;&#26469;&#35299;&#37322;SCALE&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;SCALE&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#21644;&#25105;&#20204;&#26500;&#24314;&#30340;&#26032;&#30340;&#38271;&#24418;&#24335;&#23545;&#35805;&#25968;&#25454;&#38598;ScreenEval&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;SCALE&#36229;&#36807;&#20102;&#31454;&#20105;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI models exhibit remarkable potential; however, hallucinations across various tasks present a significant challenge, particularly for longer inputs that current approaches struggle to address effectively. We introduce SCALE (Source Chunking Approach for Large-scale inconsistency Evaluation), a task-agnostic model for detecting factual inconsistencies using a novel chunking strategy. Specifically, SCALE is a Natural Language Inference (NLI) based model that uses large text chunks to condition over long texts. This approach achieves state-of-the-art performance in factual inconsistency detection for diverse tasks and long inputs. Additionally, we leverage the chunking mechanism and employ a novel algorithm to explain SCALE's decisions through relevant source sentence retrieval. Our evaluations reveal that SCALE outperforms existing methods on both standard benchmarks and a new long-form dialogue dataset ScreenEval we constructed. Moreover, SCALE surpasses competitive systems 
&lt;/p&gt;</description></item><item><title>CycleNet&#26159;&#19968;&#31181;&#23558;&#24490;&#29615;&#19968;&#33268;&#24615;&#24341;&#20837;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35268;&#33539;&#22270;&#20687;&#25805;&#20316;&#65292;&#20855;&#26377;&#20248;&#36234;&#30340;&#32763;&#35793;&#19968;&#33268;&#24615;&#21644;&#36136;&#37327;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#36328;&#39046;&#22495;&#20998;&#24067;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2310.13165</link><description>&lt;p&gt;
CycleNet&#65306;&#37325;&#26032;&#24605;&#32771;&#25991;&#26412;&#24341;&#23548;&#25193;&#25955;&#20013;&#30340;&#24490;&#29615;&#19968;&#33268;&#24615;&#65292;&#20197;&#36827;&#34892;&#22270;&#20687;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
CycleNet: Rethinking Cycle Consistency in Text-Guided Diffusion for Image Manipulation. (arXiv:2310.13165v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13165
&lt;/p&gt;
&lt;p&gt;
CycleNet&#26159;&#19968;&#31181;&#23558;&#24490;&#29615;&#19968;&#33268;&#24615;&#24341;&#20837;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35268;&#33539;&#22270;&#20687;&#25805;&#20316;&#65292;&#20855;&#26377;&#20248;&#36234;&#30340;&#32763;&#35793;&#19968;&#33268;&#24615;&#21644;&#36136;&#37327;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#36328;&#39046;&#22495;&#20998;&#24067;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DM&#65289;&#22312;&#22270;&#20687;&#21512;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#31361;&#30772;&#65292;&#20294;&#32570;&#20047;&#19968;&#31181;&#30452;&#35266;&#30340;&#19968;&#33268;&#22270;&#20687;&#21040;&#22270;&#20687;&#65288;I2I&#65289;&#32763;&#35793;&#25509;&#21475;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#25506;&#32034;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#25513;&#30721;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;DMs&#36827;&#34892;&#26080;&#37197;&#23545;&#30340;I2I&#32763;&#35793;&#24182;&#20445;&#25345;&#19968;&#33268;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Cyclenet&#65292;&#19968;&#31181;&#26032;&#39062;&#20294;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#23427;&#23558;&#24490;&#29615;&#19968;&#33268;&#24615;&#32435;&#20837;DMs&#20013;&#65292;&#20197;&#35268;&#33539;&#22270;&#20687;&#25805;&#20316;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;Cyclenet&#22312;&#19981;&#21516;&#31890;&#24230;&#30340;&#26080;&#37197;&#23545;I2I&#20219;&#21153;&#19978;&#30340;&#20248;&#21183;&#12290;&#38500;&#20102;&#22330;&#26223;&#21644;&#23545;&#35937;&#32423;&#21035;&#30340;&#32763;&#35793;&#65292;&#25105;&#20204;&#36824;&#36129;&#29486;&#20102;&#19968;&#20010;&#22810;&#39046;&#22495;I2I&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#29289;&#20307;&#30340;&#29289;&#29702;&#29366;&#24577;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;Cyclenet&#22312;&#32763;&#35793;&#30340;&#19968;&#33268;&#24615;&#21644;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#19988;&#22312;&#25913;&#21464;&#25991;&#26412;&#25551;&#36848;&#26102;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#36328;&#39046;&#22495;&#20998;&#24067;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models (DMs) have enabled breakthroughs in image synthesis tasks but lack an intuitive interface for consistent image-to-image (I2I) translation. Various methods have been explored to address this issue, including mask-based methods, attention-based methods, and image-conditioning. However, it remains a critical challenge to enable unpaired I2I translation with pre-trained DMs while maintaining satisfying consistency. This paper introduces Cyclenet, a novel but simple method that incorporates cycle consistency into DMs to regularize image manipulation. We validate Cyclenet on unpaired I2I tasks of different granularities. Besides the scene and object level translation, we additionally contribute a multi-domain I2I translation dataset to study the physical state changes of objects. Our empirical studies show that Cyclenet is superior in translation consistency and quality, and can generate high-quality images for out-of-domain distributions with a simple change of the textual 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#26041;&#27861;&#26469;&#35299;&#20915;&#27668;&#35937;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25216;&#26415;&#25913;&#21892;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.13161</link><description>&lt;p&gt;
&#19968;&#31181;&#20998;&#24067;&#24335;&#27668;&#35937;&#39044;&#27979;&#26041;&#27861;&#65306;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#35299;&#20915;&#38477;&#27700;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Distributed Approach to Meteorological Predictions: Addressing Data Imbalance in Precipitation Prediction Models through Federated Learning and GANs. (arXiv:2310.13161v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13161
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#26041;&#27861;&#26469;&#35299;&#20915;&#27668;&#35937;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25216;&#26415;&#25913;&#21892;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22825;&#27668;&#25968;&#25454;&#30340;&#20998;&#31867;&#38656;&#35201;&#23558;&#27668;&#35937;&#29616;&#35937;&#20998;&#20026;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#20174;&#32780;&#20026;&#20892;&#19994;&#12289;&#33322;&#31354;&#21644;&#28798;&#23475;&#31649;&#29702;&#31561;&#21508;&#20010;&#39046;&#22495;&#25552;&#20379;&#32454;&#33268;&#30340;&#20998;&#26512;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#36825;&#28041;&#21450;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#20998;&#26512;&#22823;&#22411;&#22810;&#32500;&#22825;&#27668;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#24335;&#21644;&#36235;&#21183;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#21487;&#33021;&#21253;&#25324;&#28201;&#24230;&#12289;&#28287;&#24230;&#12289;&#39118;&#36895;&#21644;&#27668;&#21387;&#31561;&#21464;&#37327;&#65292;&#36825;&#20123;&#21464;&#37327;&#23545;&#27668;&#35937;&#26465;&#20214;&#36215;&#30528;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#20998;&#31867;&#31639;&#27861;&#24517;&#39035;&#33021;&#22815;&#26377;&#25928;&#24212;&#23545;&#25968;&#25454;&#19981;&#24179;&#34913;&#31561;&#25361;&#25112;&#65292;&#20854;&#20013;&#26576;&#20123;&#22825;&#27668;&#20107;&#20214;&#65288;&#22914;&#39118;&#26292;&#25110;&#26497;&#31471;&#28201;&#24230;&#65289;&#21487;&#33021;&#34987;&#20302;&#20272;&#12290;&#36825;&#39033;&#23454;&#35777;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#38598;&#20013;&#21644;&#32852;&#37030;&#29615;&#22659;&#20013;&#35299;&#20915;&#34920;&#26684;&#29366;&#22825;&#27668;&#25968;&#25454;&#20013;&#19981;&#24179;&#34913;&#31867;&#21035;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;&#37319;&#29992;&#21512;&#25104;&#23569;&#25968;&#31867;&#36807;&#37319;&#26679;&#25216;&#26415;&#25110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#31561;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The classification of weather data involves categorizing meteorological phenomena into classes, thereby facilitating nuanced analyses and precise predictions for various sectors such as agriculture, aviation, and disaster management. This involves utilizing machine learning models to analyze large, multidimensional weather datasets for patterns and trends. These datasets may include variables such as temperature, humidity, wind speed, and pressure, contributing to meteorological conditions. Furthermore, it's imperative that classification algorithms proficiently navigate challenges such as data imbalances, where certain weather events (e.g., storms or extreme temperatures) might be underrepresented. This empirical study explores data augmentation methods to address imbalanced classes in tabular weather data in centralized and federated settings. Employing data augmentation techniques such as the Synthetic Minority Over-sampling Technique or Generative Adversarial Networks can improve t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#39062;&#24418;&#24335;&#21644;&#21019;&#26032;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#31070;&#32463;ODE&#23545;&#35270;&#39057;&#21160;&#24577;&#24314;&#27169;&#65292;&#20197;&#21450;&#36830;&#32493;&#26631;&#20934;&#27969;&#30340;&#26465;&#20214;&#21464;&#20307;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.13157</link><description>&lt;p&gt;
&#26465;&#20214;&#29983;&#25104;&#24314;&#27169;&#29992;&#20110;&#22270;&#20687;&#12289;3D&#21160;&#30011;&#21644;&#35270;&#39057;
&lt;/p&gt;
&lt;p&gt;
Conditional Generative Modeling for Images, 3D Animations, and Video. (arXiv:2310.13157v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#39062;&#24418;&#24335;&#21644;&#21019;&#26032;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#31070;&#32463;ODE&#23545;&#35270;&#39057;&#21160;&#24577;&#24314;&#27169;&#65292;&#20197;&#21450;&#36830;&#32493;&#26631;&#20934;&#27969;&#30340;&#26465;&#20214;&#21464;&#20307;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26088;&#22312;&#25512;&#21160;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#30340;&#21019;&#26032;&#65292;&#36890;&#36807;&#25506;&#32034;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#39062;&#24418;&#24335;&#21644;&#22270;&#20687;&#12289;3D&#21160;&#30011;&#21644;&#35270;&#39057;&#20013;&#30340;&#21019;&#26032;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#25552;&#20379;&#22122;&#22768;&#21644;&#35270;&#35273;&#25968;&#25454;&#30340;&#21487;&#36870;&#21464;&#25442;&#30340;&#26550;&#26500;&#65292;&#24182;&#24212;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#36827;&#34892;&#29983;&#25104;&#20219;&#21153;&#21644;3D&#20869;&#23481;&#25805;&#20316;&#12290;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#37117;&#23558;&#26465;&#20214;&#20449;&#24687;&#32435;&#20837;&#21040;&#22686;&#24378;&#35270;&#35273;&#25968;&#25454;&#21512;&#25104;&#20013;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#36807;&#31243;&#30340;&#25928;&#29575;&#21644;&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20351;&#29992;&#31070;&#32463;ODE&#23545;&#35270;&#39057;&#21160;&#24577;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#23613;&#31649;&#20165;&#35757;&#32451;&#29992;&#20110;&#37325;&#26500;&#24403;&#21069;&#24103;&#65292;&#20294;&#33021;&#22815;&#39044;&#27979;&#26410;&#26469;&#30340;&#35270;&#39057;&#24103;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36830;&#32493;&#26631;&#20934;&#27969;&#30340;&#26465;&#20214;&#21464;&#20307;&#65292;&#36890;&#36807;&#36739;&#20302;&#20998;&#36776;&#29575;&#30340;&#36755;&#20837;&#23454;&#29616;&#26356;&#39640;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
This dissertation attempts to drive innovation in the field of generative modeling for computer vision, by exploring novel formulations of conditional generative models, and innovative applications in images, 3D animations, and video. Our research focuses on architectures that offer reversible transformations of noise and visual data, and the application of encoder-decoder architectures for generative tasks and 3D content manipulation. In all instances, we incorporate conditional information to enhance the synthesis of visual data, improving the efficiency of the generation process as well as the generated content.  We introduce the use of Neural ODEs to model video dynamics using an encoder-decoder architecture, demonstrating their ability to predict future video frames despite being trained solely to reconstruct current frames. Next, we propose a conditional variant of continuous normalizing flows that enables higher-resolution image generation based on lower-resolution input, achiev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#20020;&#24202;&#39046;&#22495;&#38382;&#31572;&#27169;&#22411;&#27979;&#35797;&#24179;&#21488;CLIFT&#65292;&#20854;&#20013;&#21253;&#25324;7.5k&#20010;&#39640;&#36136;&#37327;&#30340;&#38382;&#31572;&#26679;&#26412;&#12290;&#22312;&#21407;&#22987;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#24212;&#29992;&#20110;&#26032;&#30340;&#27979;&#35797;&#38598;&#26102;&#24615;&#33021;&#19979;&#38477;&#65292;&#34920;&#26126;&#23384;&#22312;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#25552;&#39640;&#20020;&#24202;&#39046;&#22495;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#24517;&#35201;&#24615;&#21644;&#28508;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#36861;&#36394;&#35813;&#26041;&#21521;&#36827;&#23637;&#30340;&#27979;&#35797;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2310.13146</link><description>&lt;p&gt;
&#22312;&#20020;&#24202;&#39046;&#22495;&#30340;&#38382;&#31572;&#27169;&#22411;&#20013;&#20998;&#26512;&#33258;&#28982;&#20998;&#24067;&#20559;&#31227;&#30340;CLIFT
&lt;/p&gt;
&lt;p&gt;
CLIFT: Analysing Natural Distribution Shift on Question Answering Models in Clinical Domain. (arXiv:2310.13146v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#20020;&#24202;&#39046;&#22495;&#38382;&#31572;&#27169;&#22411;&#27979;&#35797;&#24179;&#21488;CLIFT&#65292;&#20854;&#20013;&#21253;&#25324;7.5k&#20010;&#39640;&#36136;&#37327;&#30340;&#38382;&#31572;&#26679;&#26412;&#12290;&#22312;&#21407;&#22987;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#24212;&#29992;&#20110;&#26032;&#30340;&#27979;&#35797;&#38598;&#26102;&#24615;&#33021;&#19979;&#38477;&#65292;&#34920;&#26126;&#23384;&#22312;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#25552;&#39640;&#20020;&#24202;&#39046;&#22495;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#24517;&#35201;&#24615;&#21644;&#28508;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#36861;&#36394;&#35813;&#26041;&#21521;&#36827;&#23637;&#30340;&#27979;&#35797;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#27979;&#35797;&#24179;&#21488;CLIFT&#65288;&#20020;&#24202;&#20998;&#24067;&#20559;&#31227;&#65289;&#65292;&#29992;&#20110;&#20020;&#24202;&#39046;&#22495;&#30340;&#38382;&#31572;&#20219;&#21153;&#12290;&#35813;&#27979;&#35797;&#24179;&#21488;&#21253;&#25324;7.5k&#20010;&#39640;&#36136;&#37327;&#30340;&#38382;&#31572;&#26679;&#26412;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#26679;&#21270;&#19988;&#21487;&#38752;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#24182;&#22312;&#35813;&#27979;&#35797;&#24179;&#21488;&#19979;&#35780;&#20272;&#20102;&#20960;&#20010;QA&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#23613;&#31649;&#22312;&#21407;&#22987;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#24212;&#29992;&#20110;&#26032;&#30340;&#27979;&#35797;&#38598;&#26102;&#65292;&#24615;&#33021;&#19979;&#38477;&#65292;&#34920;&#26126;&#23384;&#22312;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#25552;&#39640;&#20020;&#24202;&#39046;&#22495;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#24517;&#35201;&#24615;&#21644;&#28508;&#21147;&#12290;&#35813;&#27979;&#35797;&#24179;&#21488;&#20026;&#36861;&#36394;&#35813;&#26041;&#21521;&#30340;&#36827;&#23637;&#25552;&#20379;&#20102;&#19968;&#31181;&#36884;&#24452;&#12290;&#23427;&#36824;&#24378;&#35843;&#20102;&#37319;&#29992;&#32771;&#34385;&#33258;&#28982;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24615;&#30340;&#35780;&#20272;&#25351;&#26631;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#35745;&#21010;&#36890;&#36807;&#28155;&#21152;&#26356;&#22810;&#26679;&#26412;&#21644;&#27169;&#22411;&#32467;&#26524;&#26469;&#25193;&#23637;&#35821;&#26009;&#24211;&#12290;&#23436;&#25972;&#30340;&#35770;&#25991;&#21644;&#26356;&#26032;&#30340;&#22522;&#20934;&#21487;&#20197;&#22312;github.com/openlifescience-ai/clift&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new testbed CLIFT (Clinical Shift) for the clinical domain Question-answering task. The testbed includes 7.5k high-quality question answering samples to provide a diverse and reliable benchmark. We performed a comprehensive experimental study and evaluated several QA deep-learning models under the proposed testbed. Despite impressive results on the original test set, the performance degrades when applied to new test sets, which shows the distribution shift. Our findings emphasize the need for and the potential for increasing the robustness of clinical domain models under distributional shifts. The testbed offers one way to track progress in that direction. It also highlights the necessity of adopting evaluation metrics that consider robustness to natural distribution shifts. We plan to expand the corpus by adding more samples and model results. The full paper and the updated benchmark are available at github.com/openlifescience-ai/clift
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#30740;&#31350;LLMs&#20316;&#20026;&#21307;&#30103;&#38382;&#39064;&#30340;&#22810;&#35821;&#35328;&#23545;&#35805;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13132</link><description>&lt;p&gt;
&#19981;&#22952;&#29992;&#33521;&#25991;&#38382;&#25105;&#65306;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#21307;&#30103;&#38382;&#39064;&#36328;&#35821;&#35328;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Ask Me in English Instead: Cross-Lingual Evaluation of Large Language Models for Healthcare Queries. (arXiv:2310.13132v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#30740;&#31350;LLMs&#20316;&#20026;&#21307;&#30103;&#38382;&#39064;&#30340;&#22810;&#35821;&#35328;&#23545;&#35805;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#27491;&#22312;&#25913;&#21464;&#19968;&#33324;&#20844;&#20247;&#33719;&#21462;&#21644;&#28040;&#36153;&#20449;&#24687;&#30340;&#26041;&#24335;&#12290;&#23427;&#20204;&#22312;&#20851;&#38190;&#39046;&#22495;&#22914;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#24433;&#21709;&#23588;&#20026;&#26174;&#33879;&#65292;&#26222;&#36890;&#20154;&#26085;&#24120;&#26597;&#35810;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;LLMs&#20316;&#20026;&#23545;&#35805;&#20195;&#29702;&#12290;&#34429;&#28982;LLMs&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#23545;&#20854;&#23433;&#20840;&#24615;&#30340;&#20851;&#27880;&#22312;&#36825;&#20123;&#39640;&#39118;&#38505;&#39046;&#22495;&#20173;&#28982;&#24456;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;LLMs&#30340;&#24320;&#21457;&#36807;&#20110;&#38598;&#20013;&#22312;&#33521;&#25991;&#19978;&#12290;&#36825;&#20123;LLMs&#22312;&#38750;&#33521;&#25991;&#35821;&#35328;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#22914;&#20309;&#20173;&#19981;&#26126;&#30830;&#65292;&#36825;&#26159;&#30830;&#20445;&#36825;&#20123;&#31995;&#32479;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#20351;&#29992;&#20844;&#24179;&#24615;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#30740;&#31350;LLMs&#20316;&#20026;&#22810;&#35821;&#35328;&#23545;&#35805;&#31995;&#32479;&#22312;&#21307;&#30103;&#38382;&#39064;&#20013;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#24471;&#20986;&#30340;&#26694;&#26550;XlingEval&#65292;&#37325;&#28857;&#20851;&#27880;&#23545;LLMs&#22238;&#31572;&#33258;&#28982;&#20154;&#25776;&#20889;&#30340;&#19982;&#20581;&#24247;&#30456;&#20851;&#38382;&#39064;&#30340;&#35780;&#20272;&#30340;&#19977;&#20010;&#22522;&#26412;&#26631;&#20934;&#65306;&#21327;&#35758;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are transforming the ways the general public accesses and consumes information. Their influence is particularly pronounced in pivotal sectors like healthcare, where lay individuals are increasingly appropriating LLMs as conversational agents for everyday queries. While LLMs demonstrate impressive language understanding and generation proficiencies, concerns regarding their safety remain paramount in these high-stake domains. Moreover, the development of LLMs is disproportionately focused on English. It remains unclear how these LLMs perform in the context of non-English languages, a gap that is critical for ensuring equity in the real-world use of these systems.This paper provides a framework to investigate the effectiveness of LLMs as multi-lingual dialogue systems for healthcare queries. Our empirically-derived framework XlingEval focuses on three fundamental criteria for evaluating LLM responses to naturalistic human-authored health-related questions: co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26234;&#33021;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#31639;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;CO2&#25490;&#25918;&#21644;&#34892;&#39542;&#26102;&#38388;&#31561;&#25351;&#26631;&#65292;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.13129</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20248;&#21270;CO2&#25490;&#25918;&#30340;&#26234;&#33021;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning-based Intelligent Traffic Signal Controls with Optimized CO2 emissions. (arXiv:2310.13129v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26234;&#33021;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#31639;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;CO2&#25490;&#25918;&#21644;&#34892;&#39542;&#26102;&#38388;&#31561;&#25351;&#26631;&#65292;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#20132;&#36890;&#32593;&#32476;&#38754;&#20020;&#30528;&#27425;&#20248;&#25511;&#21046;&#31574;&#30053;&#30340;&#25361;&#25112;&#65292;&#36825;&#21487;&#33021;&#23545;&#20154;&#31867;&#20581;&#24247;&#12289;&#29615;&#22659;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#65292;&#24182; contribute to traffic congestion. &#30001;&#20110;&#20132;&#36890;&#25317;&#22581;&#23548;&#33268;&#30340;&#31354;&#27668;&#27745;&#26579;&#27700;&#24179;&#19978;&#21319;&#21644;&#36890;&#21220;&#26102;&#38388;&#24310;&#38271;&#65292;&#20132;&#21449;&#36335;&#21475;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#22120;&#25104;&#20026;&#29616;&#20195;&#20132;&#36890;&#22522;&#30784;&#35774;&#26045;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#23613;&#31649;&#25991;&#29486;&#20013;&#26377;&#20960;&#20010;&#33258;&#36866;&#24212;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#22120;&#65292;&#20294;&#23545;&#20854;&#27604;&#36739;&#24615;&#33021;&#30340;&#30740;&#31350;&#26377;&#38480;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#20108;&#27687;&#21270;&#30899;&#65288;CO2&#65289;&#25490;&#25918;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#25991;&#29486;&#23545;&#35813;&#39046;&#22495;&#20851;&#27880;&#19981;&#22815;&#12290;&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EcoLight&#65292;&#19968;&#31181;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#22870;&#21169;&#22609;&#36896;&#26041;&#26696;&#65292;&#19981;&#20165;&#33021;&#20943;&#23569;CO2&#25490;&#25918;&#65292;&#36824;&#33021;&#22312;&#35832;&#22914;&#34892;&#39542;&#26102;&#38388;&#31561;&#25351;&#26631;&#19978;&#21462;&#24471;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;&#34892;&#39542;&#26102;&#38388;&#12289;CO2&#25490;&#25918;&#12289;&#31561;&#25351;&#26631;&#27604;&#36739;&#20102;&#34920;&#26684;&#24335;Q-Learning&#12289;DQN&#12289;SARSA&#21644;A2C&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, transportation networks face the challenge of sub-optimal control policies that can have adverse effects on human health, the environment, and contribute to traffic congestion. Increased levels of air pollution and extended commute times caused by traffic bottlenecks make intersection traffic signal controllers a crucial component of modern transportation infrastructure. Despite several adaptive traffic signal controllers in literature, limited research has been conducted on their comparative performance. Furthermore, despite carbon dioxide (CO2) emissions' significance as a global issue, the literature has paid limited attention to this area. In this report, we propose EcoLight, a reward shaping scheme for reinforcement learning algorithms that not only reduces CO2 emissions but also achieves competitive results in metrics such as travel time. We compare the performance of tabular Q-Learning, DQN, SARSA, and A2C algorithms using metrics such as travel time, CO2 emissions, wa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#32463;&#36807;&#35757;&#32451;&#36827;&#34892;&#25972;&#25968;&#21152;&#27861;&#30340;&#21333;&#23618;Transformer&#27169;&#22411;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#35813;&#27169;&#22411;&#23558;&#20219;&#21153;&#20998;&#20026;&#24182;&#34892;&#30340;&#12289;&#29305;&#23450;&#20110;&#25968;&#23383;&#30340;&#27969;&#65292;&#24182;&#38024;&#23545;&#19981;&#21516;&#30340;&#25968;&#23383;&#20301;&#32622;&#37319;&#29992;&#19981;&#21516;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#36824;&#21457;&#29616;&#20102;&#19968;&#31181;&#32597;&#35265;&#30340;&#39640;&#25439;&#22833;&#30340;&#20351;&#29992;&#24773;&#20917;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#31561;&#26041;&#38754;&#30340;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.13121</link><description>&lt;p&gt;
&#29702;&#35299;Transformer&#20013;&#30340;&#21152;&#27861;
&lt;/p&gt;
&lt;p&gt;
Understanding Addition in Transformers. (arXiv:2310.13121v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#32463;&#36807;&#35757;&#32451;&#36827;&#34892;&#25972;&#25968;&#21152;&#27861;&#30340;&#21333;&#23618;Transformer&#27169;&#22411;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#35813;&#27169;&#22411;&#23558;&#20219;&#21153;&#20998;&#20026;&#24182;&#34892;&#30340;&#12289;&#29305;&#23450;&#20110;&#25968;&#23383;&#30340;&#27969;&#65292;&#24182;&#38024;&#23545;&#19981;&#21516;&#30340;&#25968;&#23383;&#20301;&#32622;&#37319;&#29992;&#19981;&#21516;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#36824;&#21457;&#29616;&#20102;&#19968;&#31181;&#32597;&#35265;&#30340;&#39640;&#25439;&#22833;&#30340;&#20351;&#29992;&#24773;&#20917;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#31561;&#26041;&#38754;&#30340;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#20687;Transformer&#36825;&#26679;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#26041;&#24335;&#23545;&#20110;&#20854;&#23433;&#20840;&#21644;&#36947;&#24503;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#23545;&#32463;&#36807;&#35757;&#32451;&#36827;&#34892;&#25972;&#25968;&#21152;&#27861;&#30340;&#21333;&#23618;Transformer&#27169;&#22411;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#35813;&#27169;&#22411;&#23558;&#20219;&#21153;&#20998;&#20026;&#24182;&#34892;&#30340;&#12289;&#29305;&#23450;&#20110;&#25968;&#23383;&#30340;&#27969;&#65292;&#24182;&#38024;&#23545;&#19981;&#21516;&#30340;&#25968;&#23383;&#20301;&#32622;&#37319;&#29992;&#19981;&#21516;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#21457;&#29616;&#35813;&#27169;&#22411;&#24320;&#22987;&#35745;&#31639;&#36739;&#26202;&#65292;&#20294;&#25191;&#34892;&#36895;&#24230;&#38750;&#24120;&#24555;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#19968;&#31181;&#32597;&#35265;&#30340;&#39640;&#25439;&#22833;&#30340;&#20351;&#29992;&#24773;&#20917;&#65292;&#24182;&#20104;&#20197;&#35299;&#37322;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35814;&#32454;&#35299;&#37322;&#20102;&#35813;&#27169;&#22411;&#30340;&#31639;&#27861;&#12290;&#36825;&#20123;&#21457;&#29616;&#36890;&#36807;&#20005;&#26684;&#27979;&#35797;&#21644;&#25968;&#23398;&#24314;&#27169;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#23545;&#20110;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#31561;&#24191;&#27867;&#30740;&#31350;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#20998;&#26512;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#21644;&#22810;&#23618;Transformer&#27169;&#22411;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the inner workings of machine learning models like Transformers is vital for their safe and ethical use. This paper presents an in-depth analysis of a one-layer Transformer model trained for integer addition. We reveal that the model divides the task into parallel, digit-specific streams and employs distinct algorithms for different digit positions. Our study also finds that the model starts calculations late but executes them rapidly. A rare use case with high loss is identified and explained. Overall, the model's algorithm is explained in detail. These findings are validated through rigorous testing and mathematical modeling, contributing to the broader works in Mechanistic Interpretability, AI safety, and alignment. Our approach opens the door for analyzing more complex tasks and multi-layer Transformer models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;AVTENet&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26159;&#19968;&#20010;&#22522;&#20110;&#38899;&#39057;-&#35270;&#35273;Transformer&#30340;&#22810;&#19987;&#23478;&#38598;&#25104;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#35270;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#20013;&#32771;&#34385;&#22768;&#23398;&#21644;&#35270;&#35273;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2310.13103</link><description>&lt;p&gt;
AVTENet: &#22522;&#20110;&#38899;&#39057;-&#35270;&#35273;Transformer&#30340;&#22810;&#19987;&#23478;&#38598;&#25104;&#32593;&#32476;&#22312;&#35270;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AVTENet: Audio-Visual Transformer-based Ensemble Network Exploiting Multiple Experts for Video Deepfake Detection. (arXiv:2310.13103v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;AVTENet&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26159;&#19968;&#20010;&#22522;&#20110;&#38899;&#39057;-&#35270;&#35273;Transformer&#30340;&#22810;&#19987;&#23478;&#38598;&#25104;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#35270;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#20013;&#32771;&#34385;&#22768;&#23398;&#21644;&#35270;&#35273;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#24191;&#27867;&#20998;&#20139;&#30340;&#20266;&#36896;&#20869;&#23481;&#26159;&#19968;&#20010;&#37325;&#22823;&#31038;&#20250;&#38382;&#39064;&#65292;&#35201;&#27714;&#21152;&#24378;&#30417;&#31649;&#24182;&#32473;&#30740;&#31350;&#31038;&#21306;&#24102;&#26469;&#26032;&#30340;&#25361;&#25112;&#12290;&#36817;&#24180;&#26469;&#65292;&#36229;&#30495;&#23454;&#30340;&#28145;&#24230;&#20266;&#36896;&#35270;&#39057;&#30340;&#26222;&#21450;&#24341;&#36215;&#20102;&#23545;&#38899;&#39057;&#21644;&#35270;&#35273;&#20266;&#36896;&#23041;&#32961;&#30340;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#20851;&#20110;&#26816;&#27979;AI&#29983;&#25104;&#30340;&#20266;&#36896;&#35270;&#39057;&#30340;&#20808;&#21069;&#24037;&#20316;&#21482;&#21033;&#29992;&#20102;&#35270;&#35273;&#27169;&#24577;&#25110;&#38899;&#39057;&#27169;&#24577;&#12290;&#34429;&#28982;&#25991;&#29486;&#20013;&#26377;&#19968;&#20123;&#26041;&#27861;&#21033;&#29992;&#38899;&#39057;&#21644;&#35270;&#35273;&#27169;&#24577;&#26469;&#26816;&#27979;&#20266;&#36896;&#35270;&#39057;&#65292;&#20294;&#23427;&#20204;&#23578;&#26410;&#22312;&#28041;&#21450;&#22768;&#23398;&#21644;&#35270;&#35273;&#25805;&#20316;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#20266;&#36896;&#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#22522;&#20110;CNN&#65292;&#24182;&#19988;&#26816;&#27979;&#20934;&#30830;&#29575;&#36739;&#20302;&#12290;&#21463;&#21040;Transformer&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#26368;&#26032;&#25104;&#21151;&#21551;&#21457;&#65292;&#20026;&#20102;&#35299;&#20915;&#28145;&#24230;&#20266;&#36896;&#25216;&#26415;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#22768;&#23398;&#25805;&#20316;&#30340;&#38899;&#39057;-&#35270;&#35273;Transformer&#38598;&#25104;&#32593;&#32476;&#65288;AVTENet&#65289;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forged content shared widely on social media platforms is a major social problem that requires increased regulation and poses new challenges to the research community. The recent proliferation of hyper-realistic deepfake videos has drawn attention to the threat of audio and visual forgeries. Most previous work on detecting AI-generated fake videos only utilizes visual modality or audio modality. While there are some methods in the literature that exploit audio and visual modalities to detect forged videos, they have not been comprehensively evaluated on multi-modal datasets of deepfake videos involving acoustic and visual manipulations. Moreover, these existing methods are mostly based on CNN and suffer from low detection accuracy. Inspired by the recent success of Transformer in various fields, to address the challenges posed by deepfake technology, in this paper, we propose an Audio-Visual Transformer-based Ensemble Network (AVTENet) framework that considers both acoustic manipulatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31890;&#23376;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36229;&#36234;&#29420;&#31435;&#26679;&#26412;&#30340;&#24120;&#35265;&#20551;&#35774;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#21644;&#37319;&#26679;&#25928;&#29575;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#22312;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#21644;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13102</link><description>&lt;p&gt;
&#31890;&#23376;&#24341;&#23548;&#65306;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#26679;&#26412;&#22810;&#26679;&#24615;&#37319;&#26679;&#19982;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Particle Guidance: non-I.I.D. Diverse Sampling with Diffusion Models. (arXiv:2310.13102v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31890;&#23376;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36229;&#36234;&#29420;&#31435;&#26679;&#26412;&#30340;&#24120;&#35265;&#20551;&#35774;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#21644;&#37319;&#26679;&#25928;&#29575;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#22312;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#21644;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#24191;&#27867;&#25104;&#21151;&#65292;&#24050;&#32463;&#26377;&#22823;&#37327;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#21152;&#24555;&#20854;&#37319;&#26679;&#26102;&#38388;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#33719;&#21462;&#22810;&#26679;&#24615;&#26679;&#26412;&#65292;&#29983;&#25104;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#22810;&#27425;&#37319;&#26679;&#65292;&#36825;&#20250;&#36896;&#25104;&#19982;&#37319;&#26679;&#26102;&#38388;&#26080;&#20851;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#22788;&#29702;&#20102;&#22914;&#20309;&#36890;&#36807;&#36229;&#36234;&#29420;&#31435;&#26679;&#26412;&#30340;&#24120;&#35265;&#20551;&#35774;&#26469;&#25552;&#39640;&#22810;&#26679;&#24615;&#21644;&#37319;&#26679;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31890;&#23376;&#24341;&#23548;&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#37319;&#26679;&#30340;&#25193;&#23637;&#65292;&#20854;&#20013;&#30340;&#32852;&#21512;&#31890;&#23376;&#26102;&#21464;&#20301;&#21183;&#24378;&#21046;&#23454;&#29616;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#31890;&#23376;&#24341;&#23548;&#20135;&#29983;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#20197;&#21450;&#23427;&#23545;&#20301;&#21183;&#36873;&#25321;&#30340;&#24433;&#21709;&#21644;&#19982;&#20854;&#20182;&#23398;&#31185;&#26041;&#27861;&#30340;&#32852;&#31995;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;&#25105;&#20204;&#22312;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#30340;&#35774;&#32622;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#25105;&#20204;&#33021;&#22815;&#22686;&#21152;&#22810;&#26679;&#24615;&#32780;&#19981;&#24433;&#21709;&#36136;&#37327;&#65292;&#24182;&#22312;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#20013;&#38477;&#20302;&#20102;&#24179;&#22343;13%&#30340;&#20808;&#36827;&#25216;&#26415;&#20013;&#20540;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In light of the widespread success of generative models, a significant amount of research has gone into speeding up their sampling time. However, generative models are often sampled multiple times to obtain a diverse set incurring a cost that is orthogonal to sampling time. We tackle the question of how to improve diversity and sample efficiency by moving beyond the common assumption of independent samples. We propose particle guidance, an extension of diffusion-based generative sampling where a joint-particle time-evolving potential enforces diversity. We analyze theoretically the joint distribution that particle guidance generates, its implications on the choice of potential, and the connections with methods in other disciplines. Empirically, we test the framework both in the setting of conditional image generation, where we are able to increase diversity without affecting quality, and molecular conformer generation, where we reduce the state-of-the-art median error by 13% on average
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21477;&#32423;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#28155;&#21152;&#31215;&#26497;&#30340;&#35789;&#35821;&#25110;&#21477;&#23376;&#26469;&#25913;&#21464;&#40657;&#30418;&#26377;&#27602;&#24615;&#26816;&#27979;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#22343;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.13099</link><description>&lt;p&gt;
&#19981;&#20882;&#29359;&#65292;&#20271;&#29305; - &#25105;&#21482;&#20398;&#36785;&#20154;&#31867;&#65281;&#22810;&#20010;&#25910;&#20214;&#20154;&#21477;&#32423;&#25915;&#20987;&#26377;&#27602;&#24615;&#26816;&#27979;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
No offence, Bert -- I insult only humans! Multiple addressees sentence-level attack on toxicity detection neural network. (arXiv:2310.13099v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21477;&#32423;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#28155;&#21152;&#31215;&#26497;&#30340;&#35789;&#35821;&#25110;&#21477;&#23376;&#26469;&#25913;&#21464;&#40657;&#30418;&#26377;&#27602;&#24615;&#26816;&#27979;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#22343;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21477;&#32423;&#25915;&#20987;&#26041;&#27861;&#65292;&#38024;&#23545;&#40657;&#30418;&#26377;&#27602;&#24615;&#26816;&#27979;&#27169;&#22411;&#12290;&#36890;&#36807;&#22312;&#20167;&#24680;&#20449;&#24687;&#26411;&#23614;&#28155;&#21152;&#19968;&#20123;&#31215;&#26497;&#30340;&#35789;&#35821;&#25110;&#21477;&#23376;&#65292;&#25105;&#20204;&#33021;&#22815;&#25913;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#36890;&#36807;&#26377;&#27602;&#24615;&#26816;&#27979;&#31995;&#32479;&#30340;&#26816;&#26597;&#12290;&#35813;&#26041;&#27861;&#22312;&#26469;&#33258;&#19977;&#20010;&#19981;&#21516;&#35821;&#35328;&#23478;&#26063;&#30340;&#19971;&#31181;&#35821;&#35328;&#19978;&#34987;&#35777;&#26126;&#26377;&#25928;&#12290;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#23545;&#25239;&#19978;&#36848;&#25915;&#20987;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a simple yet efficient sentence-level attack on black-box toxicity detector models. By adding several positive words or sentences to the end of a hateful message, we are able to change the prediction of a neural network and pass the toxicity detection system check. This approach is shown to be working on seven languages from three different language families. We also describe the defence mechanism against the aforementioned attack and discuss its limitations.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#27425;&#24615;&#30340;&#26080;&#30417;&#30563;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#35757;&#32451;&#26679;&#26412;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#21021;&#22987;&#21270;&#21644;&#24555;&#36895;&#36866;&#24212;&#20013;&#24212;&#29992;&#20102;&#36801;&#31227;&#23398;&#20064;&#65292;&#24182;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13085</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#21161;&#21147;&#21322;&#30417;&#30563;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Representation Learning to Aid Semi-Supervised Meta Learning. (arXiv:2310.13085v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#27425;&#24615;&#30340;&#26080;&#30417;&#30563;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#35757;&#32451;&#26679;&#26412;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#21021;&#22987;&#21270;&#21644;&#24555;&#36895;&#36866;&#24212;&#20013;&#24212;&#29992;&#20102;&#36801;&#31227;&#23398;&#20064;&#65292;&#24182;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#23398;&#20064;&#25110;&#20803;&#23398;&#20064;&#21033;&#29992;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#20256;&#32479;&#19978;&#65292;&#30417;&#30563;&#23398;&#20064;&#38656;&#35201;&#22823;&#37327;&#30340;&#26679;&#26412;&#21644;&#26631;&#31614;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#27425;&#24615;&#30340;&#26080;&#30417;&#30563;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#35757;&#32451;&#26679;&#26412;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#26080;&#30417;&#30563;&#20803;&#23398;&#20064;&#30340;&#35757;&#32451;&#38454;&#27573;&#20351;&#29992;&#22686;&#24378;&#26679;&#26412;&#20316;&#20026;&#26597;&#35810;&#38598;&#12290;&#22312;&#20803;&#23398;&#20064;&#30340;&#20869;&#24490;&#29615;&#20013;&#20351;&#29992;&#28201;&#24230;&#32553;&#25918;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#26469;&#38450;&#27490;&#26080;&#30417;&#30563;&#23398;&#20064;&#36807;&#25311;&#21512;&#12290;&#20174;&#36825;&#19968;&#27493;&#23398;&#21040;&#30340;&#21442;&#25968;&#20197;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#24335;&#24212;&#29992;&#20110;&#30446;&#26631;&#30417;&#30563;&#20803;&#23398;&#20064;&#65292;&#29992;&#20110;&#21021;&#22987;&#21270;&#21644;&#24555;&#36895;&#36866;&#24212;&#65292;&#24182;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#65292;&#21487;&#20197;&#24110;&#21161;&#20219;&#20309;&#20803;&#23398;&#20064;&#27169;&#22411;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;Omniglot&#21644;mini-Imagenet&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#65288;MAML&#65289;&#21644;&#20851;&#31995;&#32593;&#32476;&#65288;RN&#65289;&#26469;&#23637;&#31034;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot learning or meta-learning leverages the data scarcity problem in machine learning. Traditionally, training data requires a multitude of samples and labeling for supervised learning. To address this issue, we propose a one-shot unsupervised meta-learning to learn the latent representation of the training samples. We use augmented samples as the query set during the training phase of the unsupervised meta-learning. A temperature-scaled cross-entropy loss is used in the inner loop of meta-learning to prevent overfitting during unsupervised learning. The learned parameters from this step are applied to the targeted supervised meta-learning in a transfer-learning fashion for initialization and fast adaptation with improved accuracy. The proposed method is model agnostic and can aid any meta-learning model to improve accuracy. We use model agnostic meta-learning (MAML) and relation network (RN) on Omniglot and mini-Imagenet datasets to demonstrate the performance of the proposed met
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#24120;&#35782;&#20449;&#24687;&#21644;&#23545;&#35805;&#19978;&#19979;&#25991;&#32467;&#21512;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#25581;&#31034;&#30721;&#28151;&#21512;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2310.13080</link><description>&lt;p&gt;
&#20174;&#22810;&#35821;&#35328;&#22797;&#26434;&#24615;&#21040;&#24773;&#24863;&#28165;&#26224;&#24230;&#65306;&#21033;&#29992;&#24120;&#35782;&#25581;&#31034;&#30721;&#28151;&#21512;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;
&lt;/p&gt;
&lt;p&gt;
From Multilingual Complexity to Emotional Clarity: Leveraging Commonsense to Unveil Emotions in Code-Mixed Dialogues. (arXiv:2310.13080v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#24120;&#35782;&#20449;&#24687;&#21644;&#23545;&#35805;&#19978;&#19979;&#25991;&#32467;&#21512;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#25581;&#31034;&#30721;&#28151;&#21512;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#26159;&#20154;&#31867;&#20132;&#27969;&#30340;&#22522;&#26412;&#35201;&#32032;&#65292;&#25512;&#21160;&#30528;&#24773;&#24863;&#35782;&#21035;&#23545;&#35805;&#65288;ERC&#65289;&#30340;NLP&#30740;&#31350;&#12290;&#23613;&#31649;&#22312;&#35782;&#21035;&#21333;&#35821;&#23545;&#35805;&#20013;&#20010;&#20307;&#28436;&#35762;&#32773;&#30340;&#24773;&#24863;&#26041;&#38754;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#65292;&#20294;&#22312;&#30721;&#28151;&#21512;&#23545;&#35805;&#20013;&#29702;&#35299;&#24773;&#24863;&#21160;&#24577;&#30456;&#23545;&#36739;&#23569;&#21463;&#21040;&#20851;&#27880;&#12290;&#36825;&#28608;&#21457;&#20102;&#25105;&#20204;&#22312;&#26412;&#30740;&#31350;&#20013;&#23545;&#30721;&#28151;&#21512;&#23545;&#35805;&#36827;&#34892;ERC&#30340;&#21160;&#26426;&#12290;&#25105;&#20204;&#35748;&#35782;&#21040;&#24773;&#24863;&#26234;&#33021;&#21253;&#21547;&#23545;&#19990;&#30028;&#24615;&#30693;&#35782;&#30340;&#29702;&#35299;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#24120;&#35782;&#20449;&#24687;&#19982;&#23545;&#35805;&#19978;&#19979;&#25991;&#30456;&#32467;&#21512;&#65292;&#20419;&#36827;&#23545;&#24773;&#24863;&#30340;&#26356;&#28145;&#20837;&#29702;&#35299;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#27969;&#31243;&#65292;&#20174;&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#35889;&#20013;&#25552;&#21462;&#19982;&#30721;&#28151;&#21512;&#36755;&#20837;&#30456;&#20851;&#30340;&#24120;&#35782;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#34701;&#21512;&#25216;&#26415;&#65292;&#23558;&#33719;&#21462;&#30340;&#24120;&#35782;&#20449;&#24687;&#19982;&#23545;&#35805;&#34920;&#31034;&#26080;&#32541;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding emotions during conversation is a fundamental aspect of human communication, driving NLP research for Emotion Recognition in Conversation (ERC). While considerable research has focused on discerning emotions of individual speakers in monolingual dialogues, understanding the emotional dynamics in code-mixed conversations has received relatively less attention. This motivates our undertaking of ERC for code-mixed conversations in this study. Recognizing that emotional intelligence encompasses a comprehension of worldly knowledge, we propose an innovative approach that integrates commonsense information with dialogue context to facilitate a deeper understanding of emotions. To achieve this, we devise an efficient pipeline that extracts relevant commonsense from existing knowledge graphs based on the code-mixed input. Subsequently, we develop an advanced fusion technique that seamlessly combines the acquired commonsense information with the dialogue representation obtained fr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#21517;&#20026;RoboTool&#65292;&#21487;&#20197;&#20351;&#26426;&#22120;&#20154;&#22312;&#28041;&#21450;&#38544;&#21547;&#29289;&#29702;&#32422;&#26463;&#21644;&#38271;&#26399;&#35268;&#21010;&#30340;&#20219;&#21153;&#20013;&#21019;&#36896;&#24615;&#22320;&#20351;&#29992;&#24037;&#20855;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#35299;&#26512;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#12289;&#29983;&#25104;&#20840;&#38754;&#30340;&#31574;&#30053;&#12289;&#35745;&#31639;&#25216;&#33021;&#21442;&#25968;&#24182;&#29983;&#25104;&#21487;&#25191;&#34892;&#30340;Python&#20195;&#30721;&#26469;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RoboTool&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#29289;&#29702;&#32422;&#26463;&#21644;&#29615;&#22659;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2310.13065</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21019;&#36896;&#24615;&#26426;&#22120;&#20154;&#24037;&#20855;&#20351;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Creative Robot Tool Use with Large Language Models. (arXiv:2310.13065v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#21517;&#20026;RoboTool&#65292;&#21487;&#20197;&#20351;&#26426;&#22120;&#20154;&#22312;&#28041;&#21450;&#38544;&#21547;&#29289;&#29702;&#32422;&#26463;&#21644;&#38271;&#26399;&#35268;&#21010;&#30340;&#20219;&#21153;&#20013;&#21019;&#36896;&#24615;&#22320;&#20351;&#29992;&#24037;&#20855;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#35299;&#26512;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#12289;&#29983;&#25104;&#20840;&#38754;&#30340;&#31574;&#30053;&#12289;&#35745;&#31639;&#25216;&#33021;&#21442;&#25968;&#24182;&#29983;&#25104;&#21487;&#25191;&#34892;&#30340;Python&#20195;&#30721;&#26469;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RoboTool&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#29289;&#29702;&#32422;&#26463;&#21644;&#29615;&#22659;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20855;&#20351;&#29992;&#26159;&#39640;&#32423;&#26234;&#33021;&#30340;&#26631;&#24535;&#65292;&#19981;&#20165;&#23384;&#22312;&#20110;&#21160;&#29289;&#34892;&#20026;&#20013;&#65292;&#20063;&#20307;&#29616;&#22312;&#26426;&#22120;&#20154;&#30340;&#33021;&#21147;&#20013;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#26426;&#22120;&#20154;&#36171;&#20104;&#21019;&#36896;&#24615;&#20351;&#29992;&#24037;&#20855;&#30340;&#33021;&#21147;&#65292;&#29992;&#20110;&#28041;&#21450;&#38544;&#21547;&#29289;&#29702;&#32422;&#26463;&#21644;&#38271;&#26399;&#35268;&#21010;&#30340;&#20219;&#21153;&#20013;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;RoboTool&#65292;&#19968;&#20010;&#31995;&#32479;&#65292;&#23427;&#25509;&#21463;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#24182;&#36755;&#20986;&#25511;&#21046;&#26426;&#22120;&#20154;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#21487;&#25191;&#34892;&#20195;&#30721;&#12290;RoboTool&#21253;&#21547;&#22235;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#65288;i&#65289;"&#35299;&#26512;&#22120;"&#65292;&#29992;&#20110;&#35299;&#37322;&#33258;&#28982;&#35821;&#35328;&#20197;&#35782;&#21035;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#20851;&#38190;&#27010;&#24565;&#65292;&#65288;ii&#65289;"&#35268;&#21010;&#22120;"&#65292;&#26681;&#25454;&#35821;&#35328;&#36755;&#20837;&#21644;&#20851;&#38190;&#27010;&#24565;&#29983;&#25104;&#20840;&#38754;&#30340;&#31574;&#30053;&#65292;&#65288;iii&#65289;"&#35745;&#31639;&#22120;"&#65292;&#29992;&#20110;&#35745;&#31639;&#27599;&#20010;&#25216;&#33021;&#30340;&#21442;&#25968;&#65292;&#65288;iv&#65289;"&#32534;&#30721;&#22120;"&#65292;&#23558;&#36825;&#20123;&#35745;&#21010;&#36716;&#25442;&#25104;&#21487;&#25191;&#34892;&#30340;Python&#20195;&#30721;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;RoboTool&#19981;&#20165;&#21487;&#20197;&#29702;&#35299;&#26126;&#30830;&#25110;&#38544;&#21547;&#30340;&#29289;&#29702;&#32422;&#26463;&#21644;&#29615;&#22659;&#22240;&#32032;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#20135;&#29983;&#26377;&#25928;&#30340;&#25191;&#34892;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tool use is a hallmark of advanced intelligence, exemplified in both animal behavior and robotic capabilities. This paper investigates the feasibility of imbuing robots with the ability to creatively use tools in tasks that involve implicit physical constraints and long-term planning. Leveraging Large Language Models (LLMs), we develop RoboTool, a system that accepts natural language instructions and outputs executable code for controlling robots in both simulated and real-world environments. RoboTool incorporates four pivotal components: (i) an "Analyzer" that interprets natural language to discern key task-related concepts, (ii) a "Planner" that generates comprehensive strategies based on the language input and key concepts, (iii) a "Calculator" that computes parameters for each skill, and (iv) a "Coder" that translates these plans into executable Python code. Our results show that RoboTool can not only comprehend explicit or implicit physical constraints and environmental factors bu
&lt;/p&gt;</description></item><item><title>&#20581;&#22766;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#23637;&#31034;&#20102;&#24322;&#24120;&#29305;&#24449;&#21644;&#26356;&#22810;&#27010;&#24565;&#30340;&#32534;&#30721;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2310.13040</link><description>&lt;p&gt;
&#20581;&#22766;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#20855;&#26377;&#24322;&#24120;&#29305;&#24449;&#24182;&#32534;&#30721;&#26356;&#22810;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Robust multimodal models have outlier features and encode more concepts. (arXiv:2310.13040v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13040
&lt;/p&gt;
&lt;p&gt;
&#20581;&#22766;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#23637;&#31034;&#20102;&#24322;&#24120;&#29305;&#24449;&#21644;&#26356;&#22810;&#27010;&#24565;&#30340;&#32534;&#30721;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20160;&#20040;&#21306;&#20998;&#20581;&#22766;&#27169;&#22411;&#19982;&#38750;&#20581;&#22766;&#27169;&#22411;&#65311;&#38543;&#30528;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#30340;&#20986;&#29616;&#65292;&#36825;&#20010;&#38382;&#39064;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#33258;&#28982;&#20998;&#24067;&#36716;&#21464;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#20581;&#22766;&#24615;&#12290;&#23613;&#31649;&#24050;&#32463;&#35777;&#26126;&#20102;&#20581;&#22766;&#24615;&#30340;&#24046;&#24322;&#21487;&#20197;&#36861;&#28335;&#21040;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#24046;&#24322;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#36824;&#19981;&#28165;&#26970;&#36825;&#23545;&#20110;&#27169;&#22411;&#23398;&#20064;&#21040;&#20102;&#20160;&#20040;&#24847;&#21619;&#30528;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25506;&#27979;12&#20010;&#20855;&#26377;&#19981;&#21516;&#39592;&#24178;&#65288;ResNets&#21644;ViTs&#65289;&#21644;&#39044;&#35757;&#32451;&#38598;&#65288;OpenAI&#65292;LAION-400M&#65292;LAION-2B&#65292;YFCC15M&#65292;CC12M&#21644;DataComp&#65289;&#30340;&#20581;&#22766;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#20013;&#23384;&#22312;&#20004;&#20010;&#20581;&#22766;&#24615;&#30340;&#29305;&#24449;&#65306;&#65288;1&#65289;&#20581;&#22766;&#27169;&#22411;&#20855;&#26377;&#30001;&#20854;&#28608;&#27963;&#29305;&#24449;&#34920;&#24449;&#30340;&#24322;&#24120;&#29305;&#24449;&#65292;&#20854;&#20013;&#19968;&#20123;&#29305;&#24449;&#20540;&#27604;&#24179;&#22343;&#20540;&#39640;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#36825;&#20123;&#24322;&#24120;&#29305;&#24449;&#22312;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#20013;&#24341;&#20837;&#20102;&#29305;&#26435;&#26041;&#21521;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;...
&lt;/p&gt;
&lt;p&gt;
What distinguishes robust models from non-robust ones? This question has gained traction with the appearance of large-scale multimodal models, such as CLIP. These models have demonstrated unprecedented robustness with respect to natural distribution shifts. While it has been shown that such differences in robustness can be traced back to differences in training data, so far it is not known what that translates to in terms of what the model has learned. In this work, we bridge this gap by probing the representation spaces of 12 robust multimodal models with various backbones (ResNets and ViTs) and pretraining sets (OpenAI, LAION-400M, LAION-2B, YFCC15M, CC12M and DataComp). We find two signatures of robustness in the representation spaces of these models: (1) Robust models exhibit outlier features characterized by their activations, with some being several orders of magnitude above average. These outlier features induce privileged directions in the model's representation space. We demon
&lt;/p&gt;</description></item><item><title>Agri-GNN&#26159;&#19968;&#31181;&#26032;&#22411;&#22522;&#22240;&#22411;-&#25299;&#25169;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#20316;&#29289;&#20043;&#38388;&#30340;&#22797;&#26434;&#31354;&#38388;&#21644;&#22522;&#22240;&#22411;&#30456;&#20114;&#20316;&#29992;&#65292;&#23454;&#29616;&#20248;&#21270;&#30340;&#20892;&#20316;&#29289;&#20135;&#37327;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.13037</link><description>&lt;p&gt;
Agri-GNN:&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#22240;&#22411;-&#25299;&#25169;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#24314;&#31435;&#22312;GraphSAGE&#19978;&#65292;&#29992;&#20110;&#20248;&#21270;&#20135;&#37327;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agri-GNN: A Novel Genotypic-Topological Graph Neural Network Framework Built on GraphSAGE for Optimized Yield Prediction. (arXiv:2310.13037v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13037
&lt;/p&gt;
&lt;p&gt;
Agri-GNN&#26159;&#19968;&#31181;&#26032;&#22411;&#22522;&#22240;&#22411;-&#25299;&#25169;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#20316;&#29289;&#20043;&#38388;&#30340;&#22797;&#26434;&#31354;&#38388;&#21644;&#22522;&#22240;&#22411;&#30456;&#20114;&#20316;&#29992;&#65292;&#23454;&#29616;&#20248;&#21270;&#30340;&#20892;&#20316;&#29289;&#20135;&#37327;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20892;&#19994;&#20316;&#20026;&#20154;&#31867;&#25991;&#26126;&#30340;&#22522;&#30707;&#65292;&#19981;&#26029;&#23547;&#27714;&#25972;&#21512;&#25216;&#26415;&#20197;&#25552;&#39640;&#29983;&#20135;&#21147;&#21644;&#21487;&#25345;&#32493;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Agri-GNN&#30340;&#26032;&#22411;&#22522;&#22240;&#22411;-&#25299;&#25169;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#26088;&#22312;&#25429;&#25417;&#20316;&#29289;&#20043;&#38388;&#22797;&#26434;&#30340;&#31354;&#38388;&#21644;&#22522;&#22240;&#22411;&#30456;&#20114;&#20316;&#29992;&#65292;&#20026;&#20248;&#21270;&#20892;&#20316;&#29289;&#20135;&#37327;&#39044;&#27979;&#38138;&#24179;&#36947;&#36335;&#12290;Agri-GNN&#26500;&#24314;&#20102;&#19968;&#20010;&#22270;G&#65292;&#23558;&#20892;&#30000;&#22320;&#22359;&#35270;&#20026;&#33410;&#28857;&#65292;&#24182;&#26681;&#25454;&#31354;&#38388;&#21644;&#22522;&#22240;&#22411;&#30456;&#20284;&#24615;&#26041;&#27861;&#22320;&#26500;&#24314;&#33410;&#28857;&#20043;&#38388;&#30340;&#36793;&#65292;&#36890;&#36807;&#22522;&#22240;&#22411;-&#25299;&#25169;&#36807;&#28388;&#22120;&#23545;&#33410;&#28857;&#20449;&#24687;&#36827;&#34892;&#32858;&#21512;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36890;&#36807;&#35774;&#35745;&#32771;&#34385;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#23545;&#20892;&#19994;&#29983;&#24577;&#31995;&#32479;&#36827;&#34892;&#24314;&#27169;&#12290;&#36890;&#36807;&#21033;&#29992;GNN&#30340;&#33021;&#21147;&#65292;Agri-GNN&#21487;&#20197;&#20174;&#26893;&#29289;&#20013;&#25429;&#25417;&#23616;&#37096;&#21644;&#20840;&#23616;&#20449;&#24687;&#65292;&#32771;&#34385;&#23427;&#20204;&#22522;&#20110;&#31354;&#38388;&#23646;&#24615;&#30340;&#20869;&#22312;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agriculture, as the cornerstone of human civilization, constantly seeks to integrate technology for enhanced productivity and sustainability. This paper introduces $\textit{Agri-GNN}$, a novel Genotypic-Topological Graph Neural Network Framework tailored to capture the intricate spatial and genotypic interactions of crops, paving the way for optimized predictions of harvest yields. $\textit{Agri-GNN}$ constructs a Graph $\mathcal{G}$ that considers farming plots as nodes, and then methodically constructs edges between nodes based on spatial and genotypic similarity, allowing for the aggregation of node information through a genotypic-topological filter. Graph Neural Networks (GNN), by design, consider the relationships between data points, enabling them to efficiently model the interconnected agricultural ecosystem. By harnessing the power of GNNs, $\textit{Agri-GNN}$ encapsulates both local and global information from plants, considering their inherent connections based on spatial pro
&lt;/p&gt;</description></item><item><title>LASER&#26159;&#19968;&#31181;&#26032;&#30340;&#21387;&#32553;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#26799;&#24230;&#30340;&#20302;&#31209;&#32467;&#26500;&#22312;&#22122;&#22768;&#36890;&#36947;&#19978;&#39640;&#25928;&#20256;&#36755;&#26799;&#24230;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#26696;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;GPT&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#25345;&#32493;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.13033</link><description>&lt;p&gt;
LASER&#65306;&#26080;&#32447;&#20998;&#24067;&#24335;&#20248;&#21270;&#20013;&#30340;&#32447;&#24615;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
LASER: Linear Compression in Wireless Distributed Optimization. (arXiv:2310.13033v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13033
&lt;/p&gt;
&lt;p&gt;
LASER&#26159;&#19968;&#31181;&#26032;&#30340;&#21387;&#32553;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#26799;&#24230;&#30340;&#20302;&#31209;&#32467;&#26500;&#22312;&#22122;&#22768;&#36890;&#36947;&#19978;&#39640;&#25928;&#20256;&#36755;&#26799;&#24230;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#26696;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;GPT&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#25345;&#32493;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#24182;&#34892;SGD&#26159;&#20998;&#24067;&#24335;&#20248;&#21270;&#30340;&#20107;&#23454;&#19978;&#30340;&#31639;&#27861;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#12290;&#23613;&#31649;&#23427;&#26377;&#24456;&#22810;&#20248;&#28857;&#65292;&#20294;&#36890;&#20449;&#29942;&#39048;&#26159;&#20854;&#20013;&#25345;&#20037;&#23384;&#22312;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#22823;&#22810;&#25968;&#21387;&#32553;&#26041;&#26696;&#35201;&#20040;&#20551;&#35774;&#36890;&#20449;&#38142;&#36335;&#26080;&#22122;&#22768;&#65292;&#35201;&#20040;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#26080;&#27861;&#21462;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#20171;&#32461;&#20102;LASER&#65306;&#26080;&#32447;&#20998;&#24067;&#24335;&#20248;&#21270;&#20013;&#30340;&#32447;&#24615;&#21387;&#32553;&#12290;LASER&#21033;&#29992;&#26799;&#24230;&#30340;&#22266;&#26377;&#20302;&#31209;&#32467;&#26500;&#65292;&#22312;&#22122;&#22768;&#36890;&#36947;&#19978;&#39640;&#25928;&#20256;&#36755;&#26799;&#24230;&#12290;&#23613;&#31649;&#20139;&#21463;&#19982;&#32463;&#20856;SGD&#30456;&#20284;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;LASER&#22312;&#21508;&#31181;&#23454;&#38469;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#25345;&#32493;&#30340;&#20248;&#21183;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;GPT&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#65292;&#23427;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#21387;&#32553;&#26041;&#26696;&#12290;&#22312;&#21518;&#32773;&#20013;&#65292;&#25105;&#20204;&#30456;&#23545;&#20110;&#22122;&#22768;&#36890;&#36947;&#19978;&#30340;&#22522;&#20934;&#27169;&#22411;&#22312;&#22256;&#24785;&#24230;&#19978;&#33719;&#24471;&#20102;50-64%&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-parallel SGD is the de facto algorithm for distributed optimization, especially for large scale machine learning. Despite its merits, communication bottleneck is one of its persistent issues. Most compression schemes to alleviate this either assume noiseless communication links, or fail to achieve good performance on practical tasks. In this paper, we close this gap and introduce LASER: LineAr CompreSsion in WirEless DistRibuted Optimization. LASER capitalizes on the inherent low-rank structure of gradients and transmits them efficiently over the noisy channels. Whilst enjoying theoretical guarantees similar to those of the classical SGD, LASER shows consistent gains over baselines on a variety of practical benchmarks. In particular, it outperforms the state-of-the-art compression schemes on challenging computer vision and GPT language modeling tasks. On the latter, we obtain $50$-$64 \%$ improvement in perplexity over our baselines for noisy channels.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;AI&#21453;&#39304;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#65288;QDAIF&#65289;&#31639;&#27861;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#21644;&#35780;&#20272;&#21019;&#36896;&#24615;&#20889;&#20316;&#65292;&#27604;&#20256;&#32479;&#31639;&#27861;&#26356;&#24191;&#27867;&#22320;&#35206;&#30422;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2310.13032</link><description>&lt;p&gt;
AI&#21453;&#39304;&#20419;&#36827;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quality-Diversity through AI Feedback. (arXiv:2310.13032v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13032
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;AI&#21453;&#39304;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#65288;QDAIF&#65289;&#31639;&#27861;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#21644;&#35780;&#20272;&#21019;&#36896;&#24615;&#20889;&#20316;&#65292;&#27604;&#20256;&#32479;&#31639;&#27861;&#26356;&#24191;&#27867;&#22320;&#35206;&#30422;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#20013;&#65292;&#29992;&#25143;&#21487;&#33021;&#19981;&#20165;&#20559;&#22909;&#21333;&#19968;&#22238;&#22797;&#65292;&#32780;&#26159;&#24076;&#26395;&#24471;&#21040;&#22810;&#26679;&#24615;&#30340;&#39640;&#36136;&#37327;&#36755;&#20986;&#20197;&#20379;&#36873;&#25321;&#12290;&#36136;&#37327;-&#22810;&#26679;&#24615;&#65288;QD&#65289;&#25628;&#32034;&#31639;&#27861;&#26088;&#22312;&#36890;&#36807;&#19981;&#26029;&#25913;&#36827;&#21644;&#22810;&#26679;&#21270;&#20505;&#36873;&#20154;&#32676;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;QD&#22312;&#21019;&#20316;&#24615;&#20889;&#20316;&#31561;&#36136;&#24615;&#39046;&#22495;&#30340;&#24212;&#29992;&#21463;&#21040;&#31639;&#27861;&#25351;&#23450;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#24230;&#37327;&#30340;&#22256;&#38590;&#30340;&#38480;&#21046;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#21457;&#23637;&#20351;&#24471;&#36890;&#36807;AI&#21453;&#39304;&#25351;&#23548;&#25628;&#32034;&#25104;&#20026;&#21487;&#33021;&#65292;&#20854;&#20013;LMs&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#34987;&#25552;&#31034;&#26469;&#35780;&#20272;&#25991;&#26412;&#30340;&#36136;&#24615;&#26041;&#38754;&#12290;&#20511;&#21161;&#36825;&#19968;&#36827;&#23637;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36890;&#36807;AI&#21453;&#39304;&#23454;&#29616;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#31639;&#27861;&#65288;QDAIF&#65289;&#65292;&#20854;&#20013;&#36827;&#21270;&#31639;&#27861;&#24212;&#29992;LMs&#26469;&#29983;&#25104;&#21464;&#24322;&#24182;&#35780;&#20272;&#20505;&#36873;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#22312;&#21019;&#20316;&#24615;&#20889;&#20316;&#39046;&#22495;&#30340;&#35780;&#20272;&#20013;&#65292;&#19982;&#38750;QDAIF&#31639;&#27861;&#30456;&#27604;&#65292;QDAIF&#26356;&#24191;&#27867;&#22320;&#35206;&#30422;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25351;&#23450;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many text-generation problems, users may prefer not only a single response, but a diverse range of high-quality outputs from which to choose. Quality-diversity (QD) search algorithms aim at such outcomes, by continually improving and diversifying a population of candidates. However, the applicability of QD to qualitative domains, like creative writing, has been limited by the difficulty of algorithmically specifying measures of quality and diversity. Interestingly, recent developments in language models (LMs) have enabled guiding search through AI feedback, wherein LMs are prompted in natural language to evaluate qualitative aspects of text. Leveraging this development, we introduce Quality-Diversity through AI Feedback (QDAIF), wherein an evolutionary algorithm applies LMs to both generate variation and evaluate the quality and diversity of candidate text. When assessed on creative writing domains, QDAIF covers more of a specified search space with high-quality samples than do non-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32479;&#35745;&#26426;&#22120;&#32763;&#35793;&#30340;&#26597;&#35810;&#37325;&#20889;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#37325;&#20889;&#38463;&#25289;&#20271;&#35821;&#29992;&#25143;&#26597;&#35810;&#20197;&#25913;&#21892;&#25628;&#32034;&#24341;&#25806;&#30340;&#26816;&#32034;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13031</link><description>&lt;p&gt;
&#23558;&#26597;&#35810;&#37325;&#20889;&#37325;&#26032;&#23450;&#20041;&#20026;&#32479;&#35745;&#26426;&#22120;&#32763;&#35793;&#38382;&#39064;&#30340;&#29992;&#20363;
&lt;/p&gt;
&lt;p&gt;
A Use Case: Reformulating Query Rewriting as a Statistical Machine Translation Problem. (arXiv:2310.13031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32479;&#35745;&#26426;&#22120;&#32763;&#35793;&#30340;&#26597;&#35810;&#37325;&#20889;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#37325;&#20889;&#38463;&#25289;&#20271;&#35821;&#29992;&#25143;&#26597;&#35810;&#20197;&#25913;&#21892;&#25628;&#32034;&#24341;&#25806;&#30340;&#26816;&#32034;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25628;&#32034;&#24341;&#25806;&#38754;&#20020;&#30340;&#26368;&#37325;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#26681;&#25454;&#29992;&#25143;&#26597;&#35810;&#26816;&#32034;&#30456;&#20851;&#30340;&#32593;&#32476;&#20869;&#23481;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#25361;&#25112;&#65292;&#25628;&#32034;&#24341;&#25806;&#26377;&#19968;&#20010;&#27169;&#22359;&#26469;&#37325;&#20889;&#29992;&#25143;&#26597;&#35810;&#12290;&#22240;&#27492;&#65292;&#29616;&#20195;&#32593;&#32476;&#25628;&#32034;&#24341;&#25806;&#21033;&#29992;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#20351;&#29992;&#30340;&#19968;&#20123;&#32479;&#35745;&#21644;&#31070;&#32463;&#27169;&#22411;&#12290;&#20854;&#20013;&#65292;&#32479;&#35745;&#26426;&#22120;&#32763;&#35793;&#26159;&#19968;&#31181;&#20247;&#25152;&#21608;&#30693;&#30340;NLP&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#35821;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#26597;&#35810;&#37325;&#20889;&#27969;&#31243;&#65292;&#23398;&#20064;&#22914;&#20309;&#37325;&#20889;&#38463;&#25289;&#20271;&#35821;&#29992;&#25143;&#25628;&#32034;&#26597;&#35810;&#12290;&#26412;&#25991;&#36824;&#25551;&#36848;&#20102;&#21019;&#24314;&#29992;&#25143;&#26597;&#35810;&#21644;&#32593;&#39029;&#26631;&#39064;&#20043;&#38388;&#26144;&#23556;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most important challenges for modern search engines is to retrieve relevant web content based on user queries. In order to achieve this challenge, search engines have a module to rewrite user queries. That is why modern web search engines utilize some statistical and neural models used in the natural language processing domain. Statistical machine translation is a well-known NLP method among them. The paper proposes a query rewriting pipeline based on a monolingual machine translation model that learns to rewrite Arabic user search queries. This paper also describes preprocessing steps to create a mapping between user queries and web page titles.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#26799;&#24230;&#25552;&#21319;&#26641;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#23618;&#27425;&#26102;&#38388;&#24207;&#21015;&#30340;&#28857;&#39044;&#27979;&#21644;&#27010;&#29575;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.13029</link><description>&lt;p&gt;
&#34701;&#21512;&#26799;&#24230;&#25552;&#21319;&#26641;&#21644;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23618;&#27425;&#26102;&#38388;&#24207;&#21015;&#30340;&#28857;&#39044;&#27979;&#21644;&#27010;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Blending gradient boosted trees and neural networks for point and probabilistic forecasting of hierarchical time series. (arXiv:2310.13029v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#26799;&#24230;&#25552;&#21319;&#26641;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#23618;&#27425;&#26102;&#38388;&#24207;&#21015;&#30340;&#28857;&#39044;&#27979;&#21644;&#27010;&#29575;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25551;&#36848;&#19968;&#31181;&#34701;&#21512;&#26799;&#24230;&#25552;&#21319;&#26641;&#21644;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#28857;&#39044;&#27979;&#21644;&#27010;&#29575;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#26368;&#36817;&#30340;M5&#31454;&#36187;&#30340;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#36187;&#36947;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#28857;&#21253;&#25324;&#65306;a)&#23558;&#20219;&#21153;&#36716;&#21270;&#20026;&#23545;&#27599;&#22825;&#38144;&#21806;&#39069;&#30340;&#22238;&#24402;&#38382;&#39064;&#65307;b)&#20449;&#24687;&#20016;&#23500;&#30340;&#29305;&#24449;&#24037;&#31243;&#65307;c)&#21019;&#24314;&#19968;&#31995;&#21015;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65307;d)&#31934;&#24515;&#26500;&#24314;&#29992;&#20110;&#27169;&#22411;&#35843;&#20248;&#30340;&#39564;&#35777;&#38598;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#20197;&#21450;&#39564;&#35777;&#26679;&#26412;&#30340;&#31934;&#24515;&#36873;&#25321;&#26159;&#25105;&#20204;&#26041;&#27861;&#26377;&#25928;&#30340;&#26368;&#37325;&#35201;&#22240;&#32032;&#12290;&#23613;&#31649;&#39044;&#27979;&#25968;&#25454;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#65288;12&#20010;&#23618;&#32423;&#65289;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#27809;&#26377;&#21033;&#29992;&#35813;&#23618;&#27425;&#32467;&#26500;&#12290;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#22242;&#38431;&#22312;&#37329;&#29260;&#33539;&#22260;&#20869;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we tackle the problem of point and probabilistic forecasting by describing a blending methodology of machine learning models that belong to gradient boosted trees and neural networks families. These principles were successfully applied in the recent M5 Competition on both Accuracy and Uncertainty tracks. The keypoints of our methodology are: a) transform the task to regression on sales for a single day b) information rich feature engineering c) create a diverse set of state-of-the-art machine learning models and d) carefully construct validation sets for model tuning. We argue that the diversity of the machine learning models along with the careful selection of validation examples, where the most important ingredients for the effectiveness of our approach. Although forecasting data had an inherent hierarchy structure (12 levels), none of our proposed solutions exploited that hierarchical scheme. Using the proposed methodology, our team was ranked within the gold medal ran
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#22522;&#30784;&#65292;&#24320;&#21457;&#20102;&#21487;&#38752;&#30340;&#23398;&#26415;&#20250;&#35758;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#32452;&#32455;&#21322;&#32467;&#26500;&#21270;&#30340;&#20250;&#35758;&#25968;&#25454;&#24182;&#36827;&#34892;&#20154;&#24037;&#26631;&#27880;&#65292;&#35299;&#20915;&#20102;&#30740;&#31350;&#20154;&#21592;&#22312;&#33719;&#21462;&#20934;&#30830;&#12289;&#26368;&#26032;&#20449;&#24687;&#26102;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.13028</link><description>&lt;p&gt;
&#21487;&#38752;&#30340;&#23398;&#26415;&#20250;&#35758;&#38382;&#31572;&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Reliable Academic Conference Question Answering: A Study Based on Large Language Model. (arXiv:2310.13028v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#22522;&#30784;&#65292;&#24320;&#21457;&#20102;&#21487;&#38752;&#30340;&#23398;&#26415;&#20250;&#35758;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#32452;&#32455;&#21322;&#32467;&#26500;&#21270;&#30340;&#20250;&#35758;&#25968;&#25454;&#24182;&#36827;&#34892;&#20154;&#24037;&#26631;&#27880;&#65292;&#35299;&#20915;&#20102;&#30740;&#31350;&#20154;&#21592;&#22312;&#33719;&#21462;&#20934;&#30830;&#12289;&#26368;&#26032;&#20449;&#24687;&#26102;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#24555;&#36895;&#21457;&#23637;&#23548;&#33268;&#23398;&#26415;&#20250;&#35758;&#19978;&#30340;&#30740;&#31350;&#22823;&#37327;&#22686;&#21152;&#65292;&#20419;&#36827;&#20102;&#20840;&#29699;&#23398;&#26415;&#20132;&#27969;&#12290;&#30740;&#31350;&#20154;&#21592;&#22312;&#21508;&#20010;&#38454;&#27573;&#37117;&#25345;&#32493;&#23547;&#27714;&#20851;&#20110;&#36825;&#20123;&#20107;&#20214;&#30340;&#20934;&#30830;&#12289;&#26368;&#26032;&#20449;&#24687;&#12290;&#36825;&#31181;&#25968;&#25454;&#29190;&#21457;&#38656;&#35201;&#19968;&#20010;&#26234;&#33021;&#30340;&#38382;&#31572;&#31995;&#32479;&#26469;&#39640;&#25928;&#35299;&#20915;&#30740;&#31350;&#20154;&#21592;&#30340;&#38382;&#39064;&#65292;&#24182;&#30830;&#20445;&#23545;&#26368;&#26032;&#36827;&#23637;&#30340;&#20102;&#35299;&#12290;&#20250;&#35758;&#20449;&#24687;&#36890;&#24120;&#22312;&#23448;&#26041;&#32593;&#31449;&#19978;&#21457;&#24067;&#65292;&#20197;&#21322;&#32467;&#26500;&#21270;&#30340;&#26041;&#24335;&#32452;&#32455;&#65292;&#24182;&#21253;&#21547;&#22823;&#37327;&#30340;&#25991;&#26412;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#19968;&#38656;&#27714;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;ConferenceQA&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;7&#20010;&#19981;&#21516;&#23398;&#26415;&#20250;&#35758;&#65292;&#24182;&#36827;&#34892;&#20102;&#20154;&#24037;&#26631;&#27880;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37319;&#29992;&#25163;&#21160;&#21644;&#33258;&#21160;&#26041;&#27861;&#30340;&#32452;&#21512;&#65292;&#20197;&#21322;&#32467;&#26500;&#21270;&#30340;JSON&#26684;&#24335;&#32452;&#32455;&#23398;&#26415;&#20250;&#35758;&#25968;&#25454;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#20250;&#35758;&#27880;&#37322;&#20102;&#36817;100&#20010;&#38382;&#39064;-&#31572;&#26696;&#23545;&#12290;&#27599;&#20010;&#23545;&#24212;&#23545;&#24212;&#20102;&#22235;&#20010;&#19981;&#21516;&#30340;&#32500;&#24230;&#20998;&#31867;&#12290;&#20026;&#20102;&#30830;&#20445;&#25968;&#25454;&#30340;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#25163;&#21160;&#36827;&#34892;&#20102;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid growth of computer science has led to a proliferation of research presented at academic conferences, fostering global scholarly communication. Researchers consistently seek accurate, current information about these events at all stages. This data surge necessitates an intelligent question-answering system to efficiently address researchers' queries and ensure awareness of the latest advancements. The information of conferences is usually published on their official website, organized in a semi-structured way with a lot of text. To address this need, we have developed the ConferenceQA dataset for 7 diverse academic conferences with human annotations. Firstly, we employ a combination of manual and automated methods to organize academic conference data in a semi-structured JSON format. Subsequently, we annotate nearly 100 question-answer pairs for each conference. Each pair is classified into four different dimensions. To ensure the reliability of the data, we manually annotate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38468;&#21152;&#32467;&#26500;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;(ABNN)&#65292;&#36890;&#36807;&#22312;&#20027;&#24178;&#32593;&#32476;&#20013;&#25972;&#21512;&#36275;&#22815;&#20998;&#24067;&#22806;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#26469;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#23545;&#19981;&#30830;&#23450;&#24615;&#30340;&#25429;&#25417;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.13027</link><description>&lt;p&gt;
&#36890;&#36807;&#38468;&#21152;&#20214;&#21464;&#24471;&#36125;&#21494;&#26031;&#65292;&#25429;&#25417;&#26356;&#22810;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Be Bayesian by Attachments to Catch More Uncertainty. (arXiv:2310.13027v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38468;&#21152;&#32467;&#26500;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;(ABNN)&#65292;&#36890;&#36807;&#22312;&#20027;&#24178;&#32593;&#32476;&#20013;&#25972;&#21512;&#36275;&#22815;&#20998;&#24067;&#22806;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#26469;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#23545;&#19981;&#30830;&#23450;&#24615;&#30340;&#25429;&#25417;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;(BNNs)&#24050;&#25104;&#20026;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#30340;&#26377;&#24076;&#26395;&#26041;&#27861;&#20043;&#19968;&#65292;&#30001;&#20110;&#20854;&#22362;&#23454;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;BNNs&#30340;&#24615;&#33021;&#21463;&#21040;&#25429;&#25417;&#19981;&#30830;&#23450;&#24615;&#30340;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38468;&#21152;&#32467;&#26500;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;(ABNN)&#65292;&#36890;&#36807;&#38468;&#21152;&#32467;&#26500;&#20174;&#36275;&#22815;&#20998;&#24067;&#22806;&#30340;&#25968;&#25454;(OOD)&#20013;&#25429;&#25417;&#26356;&#22810;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#26681;&#25454;&#20808;&#39564;&#20998;&#24067;&#20026;OOD&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#23398;&#25551;&#36848;&#65292;&#28982;&#21518;&#24320;&#21457;&#20102;&#19968;&#20010;&#38468;&#21152;&#30340;&#36125;&#21494;&#26031;&#32467;&#26500;&#23558;OOD&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#25972;&#21512;&#21040;&#20027;&#24178;&#32593;&#32476;&#20013;&#12290;ABNN&#30001;&#26399;&#26395;&#27169;&#22359;&#21644;&#33509;&#24178;&#20998;&#24067;&#27169;&#22359;&#32452;&#25104;&#12290;&#26399;&#26395;&#27169;&#22359;&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#21407;&#22987;&#20219;&#21153;&#30340;&#20027;&#24178;&#28145;&#24230;&#32593;&#32476;&#65292;&#32780;&#20998;&#24067;&#27169;&#22359;&#21017;&#26159;&#20316;&#20026;&#20027;&#24178;&#30340;&#38468;&#21152;&#32467;&#26500;&#30340;&#23567;&#36125;&#21494;&#26031;&#32467;&#26500;&#12290;&#29305;&#21035;&#22320;&#65292;&#36825;&#20123;&#20998;&#24067;&#27169;&#22359;&#30340;&#30446;&#30340;&#26159;&#26816;&#27979;&#21644;&#20256;&#25773;OOD&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#25972;&#20307;&#32593;&#32476;&#30340;&#36125;&#21494;&#26031;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Neural Networks (BNNs) have become one of the promising approaches for uncertainty estimation due to the solid theorical foundations. However, the performance of BNNs is affected by the ability of catching uncertainty. Instead of only seeking the distribution of neural network weights by in-distribution (ID) data, in this paper, we propose a new Bayesian Neural Network with an Attached structure (ABNN) to catch more uncertainty from out-of-distribution (OOD) data. We first construct a mathematical description for the uncertainty of OOD data according to the prior distribution, and then develop an attached Bayesian structure to integrate the uncertainty of OOD data into the backbone network. ABNN is composed of an expectation module and several distribution modules. The expectation module is a backbone deep network which focuses on the original task, and the distribution modules are mini Bayesian structures which serve as attachments of the backbone. In particular, the distribu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22810;&#26631;&#31614;&#20844;&#24335;&#25913;&#20026;&#24130;&#38598;&#22810;&#31867;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#37117;&#26174;&#33879;&#20248;&#20110;&#21407;&#22987;&#26041;&#27861;&#65292;&#24182;&#28040;&#38500;&#20102;&#22810;&#26631;&#31614;&#20844;&#24335;&#20013;&#30340;&#20851;&#38190;&#36229;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.13025</link><description>&lt;p&gt;
&#20351;&#29992;&#24130;&#38598;&#22810;&#31867;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#31070;&#32463;&#35828;&#35805;&#20154;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Powerset multi-class cross entropy loss for neural speaker diarization. (arXiv:2310.13025v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13025
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22810;&#26631;&#31614;&#20844;&#24335;&#25913;&#20026;&#24130;&#38598;&#22810;&#31867;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#37117;&#26174;&#33879;&#20248;&#20110;&#21407;&#22987;&#26041;&#27861;&#65292;&#24182;&#28040;&#38500;&#20102;&#22810;&#26631;&#31614;&#20844;&#24335;&#20013;&#30340;&#20851;&#38190;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;2019&#24180;&#25552;&#20986;&#20197;&#26469;&#65292;&#20840;&#31471;&#21040;&#31471;&#31070;&#32463;&#35828;&#35805;&#20154;&#20998;&#31163;&#65288;EEND&#65289;&#19968;&#30452;&#23558;&#35828;&#35805;&#20154;&#20998;&#31163;&#35270;&#20026;&#19968;&#20010;&#36880;&#24103;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#20102;&#25490;&#21015;&#19981;&#21464;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;&#23613;&#31649;EEND&#26174;&#31034;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#36864;&#22238;&#21040;&#20102;&#30740;&#31350;&#65288;&#26412;&#22320;&#65289;&#26377;&#30417;&#30563;EEND&#35828;&#35805;&#20154;&#20998;&#31163;&#19982;&#65288;&#20840;&#23616;&#65289;&#26080;&#30417;&#30563;&#32858;&#31867;&#30340;&#21487;&#33021;&#32452;&#21512;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#28151;&#21512;&#26041;&#27861;&#24182;&#27809;&#26377;&#23545;&#21407;&#22987;&#30340;&#22810;&#26631;&#31614;&#20844;&#24335;&#20135;&#29983;&#30097;&#38382;&#12290;&#25105;&#20204;&#25552;&#20986;&#20174;&#22810;&#26631;&#31614;&#65288;&#20801;&#35768;&#21516;&#26102;&#20986;&#29616;&#20004;&#20010;&#35828;&#35805;&#20154;&#65289;&#36716;&#25442;&#20026;&#24130;&#38598;&#22810;&#31867;&#20998;&#31867;&#65288;&#20026;&#37325;&#21472;&#35828;&#35805;&#20154;&#23545;&#20998;&#37197;&#19987;&#38376;&#30340;&#31867;&#21035;&#65289;&#12290;&#36890;&#36807;&#22312;9&#20010;&#19981;&#21516;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#34920;&#26126;&#36825;&#31181;&#20844;&#24335;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65288;&#20027;&#35201;&#26159;&#22312;&#37325;&#21472;&#35821;&#38899;&#19978;&#65289;&#24182;&#19988;&#23545;&#39046;&#22495;&#19981;&#21305;&#37197;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#22810;&#26631;&#31614;&#20844;&#24335;&#20013;&#20851;&#38190;&#30340;&#26816;&#27979;&#38408;&#20540;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since its introduction in 2019, the whole end-to-end neural diarization (EEND) line of work has been addressing speaker diarization as a frame-wise multi-label classification problem with permutation-invariant training. Despite EEND showing great promise, a few recent works took a step back and studied the possible combination of (local) supervised EEND diarization with (global) unsupervised clustering. Yet, these hybrid contributions did not question the original multi-label formulation. We propose to switch from multi-label (where any two speakers can be active at the same time) to powerset multi-class classification (where dedicated classes are assigned to pairs of overlapping speakers). Through extensive experiments on 9 different benchmarks, we show that this formulation leads to significantly better performance (mostly on overlapping speech) and robustness to domain mismatch, while eliminating the detection threshold hyperparameter, critical for the multi-label formulation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#25345;&#32493;&#39044;&#35757;&#32451;&#26041;&#27861;&#22312;&#26410;&#30693;&#39046;&#22495;&#19978;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#25552;&#31034;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#33268;&#24615;&#21644;&#19981;&#19968;&#33268;&#24615;&#25439;&#22833;&#26469;&#29983;&#25104;&#39046;&#22495;&#29305;&#23450;&#30340;&#25552;&#31034;&#65292;&#26174;&#33879;&#20943;&#36731;&#20102;&#39046;&#22495;&#29305;&#24322;&#24615;&#24182;&#20419;&#36827;&#20102;&#30693;&#35782;&#30340;&#20256;&#36882;&#12290;</title><link>http://arxiv.org/abs/2310.13024</link><description>&lt;p&gt;
&#26397;&#21521;&#38543;&#26102;&#24494;&#35843;&#65306;&#20351;&#29992;&#36229;&#32593;&#32476;&#25552;&#31034;&#36827;&#34892;&#25345;&#32493;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Anytime Fine-tuning: Continually Pre-trained Language Models with Hypernetwork Prompt. (arXiv:2310.13024v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#25345;&#32493;&#39044;&#35757;&#32451;&#26041;&#27861;&#22312;&#26410;&#30693;&#39046;&#22495;&#19978;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#25552;&#31034;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#33268;&#24615;&#21644;&#19981;&#19968;&#33268;&#24615;&#25439;&#22833;&#26469;&#29983;&#25104;&#39046;&#22495;&#29305;&#23450;&#30340;&#25552;&#31034;&#65292;&#26174;&#33879;&#20943;&#36731;&#20102;&#39046;&#22495;&#29305;&#24322;&#24615;&#24182;&#20419;&#36827;&#20102;&#30693;&#35782;&#30340;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#19990;&#30028;&#20013;&#65292;&#25345;&#32493;&#39044;&#35757;&#32451;&#23545;&#20110;&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#21508;&#31181;&#39046;&#22495;&#21644;&#20219;&#21153;&#21464;&#24471;&#36843;&#20999;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25345;&#32493;&#39044;&#35757;&#32451;&#27169;&#22411;&#34987;&#26399;&#26395;&#19981;&#20165;&#22312;&#39044;&#35757;&#32451;&#39046;&#22495;&#19978;&#36827;&#34892;&#24494;&#35843;&#26102;&#20855;&#26377;&#26356;&#22823;&#30340;&#23481;&#37327;&#65292;&#32780;&#19988;&#22312;&#26410;&#30693;&#39046;&#22495;&#19978;&#30340;&#24615;&#33021;&#19981;&#20250;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#35843;&#26597;&#20102;&#29616;&#26377;&#25345;&#32493;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#38543;&#26102;&#24494;&#35843;&#25928;&#26524;&#65292;&#24471;&#20986;&#20102;&#23545;&#26410;&#30693;&#39046;&#22495;&#30340;&#26222;&#36941;&#24615;&#33021;&#19979;&#38477;&#30340;&#32467;&#35770;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#33268;&#24615;&#21644;&#19981;&#19968;&#33268;&#24615;&#25439;&#22833;&#35757;&#32451;&#36229;&#32593;&#32476;&#29983;&#25104;&#29305;&#23450;&#39046;&#22495;&#30340;&#25552;&#31034;&#12290;&#19968;&#33268;&#24615;&#25439;&#22833;&#26368;&#22823;&#31243;&#24230;&#22320;&#20445;&#30041;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#26032;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#19981;&#19968;&#33268;&#24615;&#25439;&#22833;&#21017;&#20445;&#25252;&#20102;&#20026;&#27599;&#20010;&#39046;&#22495;&#29983;&#25104;&#30340;&#38544;&#34255;&#29366;&#24577;&#30340;&#29420;&#29305;&#24615;&#12290;&#26174;&#33879;&#30340;&#26159;&#65292;&#36229;&#32593;&#32476;&#29983;&#25104;&#30340;&#25552;&#31034;&#22312;&#24494;&#35843;&#26102;&#20943;&#36731;&#20102;&#39046;&#22495;&#29305;&#24322;&#24615;&#24182;&#20419;&#36827;&#20102;&#30693;&#35782;&#30340;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual pre-training has been urgent for adapting a pre-trained model to a multitude of domains and tasks in the fast-evolving world. In practice, a continually pre-trained model is expected to demonstrate not only greater capacity when fine-tuned on pre-trained domains but also a non-decreasing performance on unseen ones. In this work, we first investigate such anytime fine-tuning effectiveness of existing continual pre-training approaches, concluding with unanimously decreased performance on unseen domains. To this end, we propose a prompt-guided continual pre-training method, where we train a hypernetwork to generate domain-specific prompts by both agreement and disagreement losses. The agreement loss maximally preserves the generalization of a pre-trained model to new domains, and the disagreement one guards the exclusiveness of the generated hidden states for each domain. Remarkably, prompts by the hypernetwork alleviate the domain identity when fine-tuning and promote knowledge
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;GraphGPT&#26694;&#26550;&#65292;&#23427;&#26159;&#19968;&#31181;&#38754;&#21521;&#22270;&#32467;&#26500;&#30693;&#35782;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#25351;&#20196;&#35843;&#20248;&#23454;&#29616;&#39640;&#24230;&#27867;&#21270;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#19979;&#28216;&#22270;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#21462;&#24471;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13023</link><description>&lt;p&gt;
GraphGPT: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
GraphGPT: Graph Instruction Tuning for Large Language Models. (arXiv:2310.13023v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;GraphGPT&#26694;&#26550;&#65292;&#23427;&#26159;&#19968;&#31181;&#38754;&#21521;&#22270;&#32467;&#26500;&#30693;&#35782;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#25351;&#20196;&#35843;&#20248;&#23454;&#29616;&#39640;&#24230;&#27867;&#21270;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#19979;&#28216;&#22270;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#21462;&#24471;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22270;&#33410;&#28857;&#20043;&#38388;&#30340;&#36882;&#24402;&#20449;&#24687;&#20132;&#25442;&#21644;&#32858;&#21512;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#29702;&#35299;&#22270;&#32467;&#26500;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#20026;&#20102;&#25552;&#39640;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#29983;&#25104;&#39044;&#35757;&#32451;&#22270;&#23884;&#20837;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#26631;&#31614;&#36827;&#34892;&#24494;&#35843;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#25110;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#30340;&#21487;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#25552;&#21319;&#22270;&#27169;&#22411;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#31181;&#38754;&#21521;&#22270;&#32467;&#26500;&#30693;&#35782;&#30340;LLM&#65292;&#21363;&#20351;&#27809;&#26377;&#26469;&#33258;&#19979;&#28216;&#22270;&#25968;&#25454;&#30340;&#20219;&#20309;&#20449;&#24687;&#65292;&#20063;&#33021;&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#23454;&#29616;&#39640;&#24230;&#27867;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GraphGPT&#26694;&#26550;&#65292;&#36890;&#36807;&#22270;&#25351;&#20196;&#35843;&#20248;&#23558;LLM&#19982;&#22270;&#32467;&#26500;&#30693;&#35782;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have advanced graph structure understanding via recursive information exchange and aggregation among graph nodes. To improve model robustness, self-supervised learning (SSL) has emerged as a promising approach for data augmentation. However, existing methods for generating pre-trained graph embeddings often rely on fine-tuning with specific downstream task labels, which limits their usability in scenarios where labeled data is scarce or unavailable. To address this, our research focuses on advancing the generalization capabilities of graph models in challenging zero-shot learning scenarios. Inspired by the success of large language models (LLMs), we aim to develop a graph-oriented LLM that can achieve high generalization across diverse downstream datasets and tasks, even without any information available from the downstream graph data. In this work, we present the GraphGPT framework that aligns LLMs with graph structural knowledge with a graph instruction t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#21442;&#25968;&#39640;&#25928;&#33258;&#35757;&#32451;&#65288;UPET&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;Monte Carlo dropout&#26469;&#36827;&#34892;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#24182;&#26681;&#25454;&#32622;&#20449;&#24230;&#21644;&#30830;&#23450;&#24615;&#36873;&#25321;&#21487;&#38752;&#30340;&#20266;&#26631;&#35760;&#26679;&#26412;&#65292;&#20197;&#35299;&#20915;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.13022</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#21442;&#25968;&#39640;&#25928;&#33258;&#35757;&#32451;&#29992;&#20110;&#21322;&#30417;&#30563;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-aware Parameter-Efficient Self-training for Semi-supervised Language Understanding. (arXiv:2310.13022v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#21442;&#25968;&#39640;&#25928;&#33258;&#35757;&#32451;&#65288;UPET&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;Monte Carlo dropout&#26469;&#36827;&#34892;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#24182;&#26681;&#25454;&#32622;&#20449;&#24230;&#21644;&#30830;&#23450;&#24615;&#36873;&#25321;&#21487;&#38752;&#30340;&#20266;&#26631;&#35760;&#26679;&#26412;&#65292;&#20197;&#35299;&#20915;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#36825;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36890;&#24120;&#20250;&#20135;&#29983;&#36739;&#24046;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#22256;&#22659;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#35757;&#32451;&#20316;&#20026;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26041;&#27861;&#20013;&#30340;&#19968;&#31181;&#20027;&#35201;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#26410;&#26631;&#35760;&#25968;&#25454;&#29983;&#25104;&#21512;&#25104;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#22826;&#22810;&#30340;&#22122;&#22768;&#26631;&#31614;&#20250;&#25439;&#23475;&#27169;&#22411;&#24615;&#33021;&#65292;&#32780;&#19988;&#33258;&#35757;&#32451;&#36807;&#31243;&#38656;&#35201;&#22810;&#27425;&#35757;&#32451;&#36845;&#20195;&#65292;&#22914;&#26524;&#26356;&#26032;PLM&#30340;&#25152;&#26377;&#27169;&#22411;&#21442;&#25968;&#65292;&#20250;&#26356;&#21152;&#26114;&#36149;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;UPET&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#21442;&#25968;&#39640;&#25928;&#33258;&#35757;&#32451;&#26694;&#26550;&#65292;&#20197;&#26377;&#25928;&#19988;&#39640;&#25928;&#22320;&#35299;&#20915;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#33945;&#29305;&#21345;&#32599;&#65288;MC&#65289;dropout&#24341;&#20837;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNN&#65289;&#20197;&#36827;&#34892;&#25945;&#24072;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#28982;&#21518;&#26681;&#25454;&#32622;&#20449;&#24230;&#21644;&#30830;&#23450;&#24615;&#26469;&#31934;&#30830;&#36873;&#25321;&#21487;&#38752;&#30340;&#20266;&#26631;&#35760;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent success of large pre-trained language models (PLMs) heavily hinges on massive labeled data, which typically produces inferior performance in low-resource scenarios. To remedy this dilemma, we study self-training as one of the predominant semi-supervised learning (SSL) approaches, which utilizes large-scale unlabeled data to generate synthetic examples. However, too many noisy labels will hurt the model performance, and the self-training procedure requires multiple training iterations making it more expensive if all the model parameters of the PLM are updated. This paper presents UPET, a novel Uncertainty-aware Parameter-Efficient self-Training framework to effectively and efficiently address the labeled data scarcity issue. Specifically, we incorporate Monte Carlo (MC) dropout in Bayesian neural network (BNN) to perform uncertainty estimation for the teacher model and then judiciously select reliable pseudo-labeled examples based on confidence and certainty. During the stude
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#35748;&#30693;&#31185;&#23398;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#26500;&#24314;&#20154;&#31867;&#32423;&#25968;&#23398;&#31995;&#32479;&#30340;&#30446;&#26631;&#65292;&#24182;&#24378;&#35843;&#20102;&#35748;&#30693;&#31185;&#23398;&#23545;&#20110;AI&#20174;&#19994;&#32773;&#22312;&#27492;&#26041;&#21521;&#19978;&#30340;&#20215;&#20540;&#12290;&#32467;&#21512;&#24320;&#25918;&#24615;&#35752;&#35770;&#21644;&#38382;&#39064;&#65292;&#35748;&#20026;&#38656;&#35201;&#22810;&#23398;&#31185;&#21512;&#20316;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#25968;&#23398;AI&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2310.13021</link><description>&lt;p&gt;
&#25968;&#23398;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#20174;&#35748;&#30693;&#31185;&#23398;&#30340;&#35282;&#24230;&#26469;&#30475;
&lt;/p&gt;
&lt;p&gt;
AI for Mathematics: A Cognitive Science Perspective. (arXiv:2310.13021v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#35748;&#30693;&#31185;&#23398;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#26500;&#24314;&#20154;&#31867;&#32423;&#25968;&#23398;&#31995;&#32479;&#30340;&#30446;&#26631;&#65292;&#24182;&#24378;&#35843;&#20102;&#35748;&#30693;&#31185;&#23398;&#23545;&#20110;AI&#20174;&#19994;&#32773;&#22312;&#27492;&#26041;&#21521;&#19978;&#30340;&#20215;&#20540;&#12290;&#32467;&#21512;&#24320;&#25918;&#24615;&#35752;&#35770;&#21644;&#38382;&#39064;&#65292;&#35748;&#20026;&#38656;&#35201;&#22810;&#23398;&#31185;&#21512;&#20316;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#25968;&#23398;AI&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#26159;&#20154;&#31867;&#24320;&#21457;&#21644;&#20351;&#29992;&#30340;&#26368;&#24378;&#22823;&#30340;&#27010;&#24565;&#31995;&#32479;&#20043;&#19968;&#12290;&#33258;&#21160;&#21270;&#25968;&#23398;&#23478;&#30340;&#26790;&#24819;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#26377;&#30528;&#24736;&#20037;&#30340;&#21382;&#21490;&#12290;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#26041;&#38754;&#30340;&#36827;&#27493;&#65292;&#24341;&#21457;&#20102;&#23545;&#26500;&#24314;&#36825;&#26679;&#30340;&#31995;&#32479;&#30340;&#24191;&#27867;&#20852;&#36259;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;"&#35748;&#30693;&#31185;&#23398;"&#30340;&#35282;&#24230;&#23545;&#36825;&#20123;&#30446;&#26631;&#36827;&#34892;&#20102;&#21453;&#24605;&#12290;&#25105;&#20204;&#25351;&#20986;&#20102;&#19968;&#20123;&#26469;&#33258;&#35748;&#30693;&#31185;&#23398;&#30340;&#32463;&#20856;&#30740;&#31350;&#26041;&#21521;&#21644;&#24403;&#21069;&#30740;&#31350;&#26041;&#21521;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#26041;&#21521;&#23545;&#20110;&#23547;&#27714;&#26500;&#24314;&#30495;&#27491;&#30340;&#20154;&#31867;&#65288;&#25110;&#36229;&#20154;&#31867;&#65289;&#32423;&#25968;&#23398;&#31995;&#32479;&#30340;AI&#20174;&#19994;&#32773;&#26469;&#35828;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#38656;&#35201;&#22810;&#23398;&#31185;&#35270;&#35282;&#30340;&#24320;&#25918;&#24615;&#35752;&#35770;&#21644;&#38382;&#39064;&#8212;&#35748;&#30693;&#31185;&#23398;&#23478;&#19982;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20154;&#21592;&#21644;&#25968;&#23398;&#23478;&#20849;&#21516;&#21512;&#20316;&#8212;&#22312;&#25105;&#20204;&#26397;&#30528;&#26356;&#22909;&#30340;&#25968;&#23398;AI&#31995;&#32479;&#36808;&#36827;&#30340;&#36807;&#31243;&#20013;&#65292;&#36825;&#20123;&#31995;&#32479;&#19981;&#20165;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#25512;&#21160;&#25968;&#23398;&#30340;&#21069;&#27839;&#65292;&#36824;&#21487;&#20197;&#25552;&#20379;&#19968;&#30629;
&lt;/p&gt;
&lt;p&gt;
Mathematics is one of the most powerful conceptual systems developed and used by the human species. Dreams of automated mathematicians have a storied history in artificial intelligence (AI). Rapid progress in AI, particularly propelled by advances in large language models (LLMs), has sparked renewed, widespread interest in building such systems. In this work, we reflect on these goals from a \textit{cognitive science} perspective. We call attention to several classical and ongoing research directions from cognitive science, which we believe are valuable for AI practitioners to consider when seeking to build truly human (or superhuman)-level mathematical systems. We close with open discussions and questions that we believe necessitate a multi-disciplinary perspective -- cognitive scientists working in tandem with AI researchers and mathematicians -- as we move toward better mathematical AI systems which not only help us push the frontier of the mathematics, but also offer glimpses into 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#29256;DeepFool&#31639;&#27861;&#65292;&#21517;&#20026;Targeted DeepFool&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#31867;&#21035;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#24341;&#20837;&#20102;&#26368;&#23567;&#32622;&#20449;&#24230;&#20998;&#25968;&#35201;&#27714;&#36229;&#21442;&#25968;&#26469;&#25552;&#39640;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13019</link><description>&lt;p&gt;
&#36890;&#36807;DeepFool&#31639;&#27861;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26377;&#38024;&#23545;&#24615;&#30340;&#31867;&#21035;&#25805;&#32437;&#30340;&#23545;&#25239;&#25915;&#20987;&#23450;&#21046;
&lt;/p&gt;
&lt;p&gt;
Tailoring Adversarial Attacks on Deep Neural Networks for Targeted Class Manipulation Using DeepFool Algorithm. (arXiv:2310.13019v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#29256;DeepFool&#31639;&#27861;&#65292;&#21517;&#20026;Targeted DeepFool&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#31867;&#21035;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#24341;&#20837;&#20102;&#26368;&#23567;&#32622;&#20449;&#24230;&#20998;&#25968;&#35201;&#27714;&#36229;&#21442;&#25968;&#26469;&#25552;&#39640;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#23545;&#25239;&#25915;&#20987;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#24341;&#36215;&#20102;&#20005;&#37325;&#20851;&#27880;&#12290;&#20102;&#35299;&#36825;&#20123;&#26131;&#21463;&#25915;&#20987;&#24615;&#24182;&#24320;&#21457;&#26377;&#25928;&#30340;&#38450;&#24481;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;DeepFool&#26159;Moosavi-Dezfooli&#31561;&#20154;&#65288;2016&#24180;&#65289;&#25552;&#20986;&#30340;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#23558;&#36755;&#20837;&#22270;&#20687;&#38169;&#35823;&#20998;&#31867;&#30340;&#26368;&#23567;&#25200;&#21160;&#12290;&#28982;&#32780;&#65292;DeepFool&#32570;&#20047;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#27861;&#65292;&#20351;&#20854;&#22312;&#29305;&#23450;&#25915;&#20987;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#36739;&#20302;&#12290;&#27492;&#22806;&#65292;&#22312;&#20808;&#21069;&#30340;&#30456;&#20851;&#24037;&#20316;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#25104;&#21151;&#29575;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#22270;&#20687;&#34987;&#25197;&#26354;&#30340;&#31243;&#24230;&#12289;&#22270;&#20687;&#36136;&#37327;&#30340;&#23436;&#25972;&#24615;&#20197;&#21450;&#38169;&#35823;&#20998;&#31867;&#30340;&#32622;&#20449;&#24230;&#27700;&#24179;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Targeted DeepFool&#65292;&#36825;&#26159;DeepFool&#30340;&#22686;&#24378;&#29256;&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#31867;&#21035;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26368;&#23567;&#32622;&#20449;&#24230;&#20998;&#25968;&#35201;&#27714;&#36229;&#21442;&#25968;&#26469;&#22686;&#24378;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have significantly advanced various domains, but their vulnerability to adversarial attacks poses serious concerns. Understanding these vulnerabilities and developing effective defense mechanisms is crucial. DeepFool, an algorithm proposed by Moosavi-Dezfooli et al. (2016), finds minimal perturbations to misclassify input images. However, DeepFool lacks a targeted approach, making it less effective in specific attack scenarios. Also, in previous related works, researchers primarily focus on success, not considering how much an image is getting distorted; the integrity of the image quality, and the confidence level to misclassifying. So, in this paper, we propose Targeted DeepFool, an augmented version of DeepFool that allows targeting specific classes for misclassification. We also introduce a minimum confidence score requirement hyperparameter to enhance flexibility. Our experiments demonstrate the effectiveness and efficiency of the proposed method across 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29983;&#29289;&#21644;&#20154;&#24037;&#20449;&#24687;&#22788;&#29702;&#31995;&#32479;&#30340;&#34920;&#31034;&#19968;&#33268;&#24615;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#31995;&#32479;&#20043;&#38388;&#30340;&#34920;&#31034;&#26159;&#21542;&#19968;&#33268;&#20197;&#21450;&#22914;&#20309;&#35843;&#25972;&#34920;&#31034;&#20197;&#26356;&#22909;&#22320;&#21305;&#37197;&#20854;&#20182;&#31995;&#32479;&#12290;&#20026;&#20102;&#25913;&#21892;&#39046;&#22495;&#20043;&#38388;&#30340;&#20132;&#27969;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20316;&#20026;&#20849;&#21516;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2310.13018</link><description>&lt;p&gt;
&#23545;&#34920;&#31034;&#19968;&#33268;&#24615;&#36798;&#25104;&#20849;&#35782;
&lt;/p&gt;
&lt;p&gt;
Getting aligned on representational alignment. (arXiv:2310.13018v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13018
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29983;&#29289;&#21644;&#20154;&#24037;&#20449;&#24687;&#22788;&#29702;&#31995;&#32479;&#30340;&#34920;&#31034;&#19968;&#33268;&#24615;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#31995;&#32479;&#20043;&#38388;&#30340;&#34920;&#31034;&#26159;&#21542;&#19968;&#33268;&#20197;&#21450;&#22914;&#20309;&#35843;&#25972;&#34920;&#31034;&#20197;&#26356;&#22909;&#22320;&#21305;&#37197;&#20854;&#20182;&#31995;&#32479;&#12290;&#20026;&#20102;&#25913;&#21892;&#39046;&#22495;&#20043;&#38388;&#30340;&#20132;&#27969;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20316;&#20026;&#20849;&#21516;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21644;&#20154;&#24037;&#20449;&#24687;&#22788;&#29702;&#31995;&#32479;&#26500;&#24314;&#21487;&#20197;&#29992;&#26469;&#36827;&#34892;&#20998;&#31867;&#12289;&#25512;&#29702;&#12289;&#35268;&#21010;&#12289;&#23548;&#33322;&#21644;&#20915;&#31574;&#30340;&#19990;&#30028;&#34920;&#31034;&#12290;&#36825;&#20123;&#22810;&#26679;&#21270;&#31995;&#32479;&#25152;&#26500;&#24314;&#30340;&#34920;&#31034;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#26159;&#19968;&#33268;&#30340;&#65311;&#21363;&#20351;&#34920;&#31034;&#19981;&#21516;&#65292;&#26159;&#21542;&#20173;&#28982;&#33021;&#22815;&#23548;&#33268;&#30456;&#21516;&#30340;&#34892;&#20026;&#65311;&#31995;&#32479;&#22914;&#20309;&#20462;&#25913;&#23427;&#20204;&#30340;&#34920;&#31034;&#20197;&#26356;&#22909;&#22320;&#21305;&#37197;&#21478;&#19968;&#20010;&#31995;&#32479;&#30340;&#34920;&#31034;&#65311;&#36825;&#20123;&#20851;&#20110;&#34920;&#31034;&#19968;&#33268;&#24615;&#30740;&#31350;&#30340;&#38382;&#39064;&#26159;&#24403;&#20195;&#35748;&#30693;&#31185;&#23398;&#12289;&#31070;&#32463;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20123;&#26368;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#30340;&#26680;&#24515;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#23545;&#20110;&#23545;&#34920;&#31034;&#19968;&#33268;&#24615;&#24863;&#20852;&#36259;&#30340;&#30740;&#31350;&#31038;&#21306;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#26377;&#38480;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#22312;&#19968;&#20010;&#39046;&#22495;&#30340;&#36827;&#23637;&#26368;&#32456;&#20250;&#22312;&#21478;&#19968;&#20010;&#39046;&#22495;&#29420;&#31435;&#22320;&#37325;&#26032;&#21457;&#29616;&#65292;&#32780;&#26356;&#24191;&#27867;&#30340;&#39046;&#22495;&#38388;&#20132;&#27969;&#23558;&#26159;&#26377;&#21033;&#30340;&#12290;&#20026;&#20102;&#25913;&#21892;&#39046;&#22495;&#20043;&#38388;&#30340;&#20132;&#27969;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#20849;&#21516;&#30340;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biological and artificial information processing systems form representations of the world that they can use to categorize, reason, plan, navigate, and make decisions. To what extent do the representations formed by these diverse systems agree? Can diverging representations still lead to the same behaviors? And how can systems modify their representations to better match those of another system? These questions pertaining to the study of \textbf{\emph{representational alignment}} are at the heart of some of the most active research areas in contemporary cognitive science, neuroscience, and machine learning. Unfortunately, there is limited knowledge-transfer between research communities interested in representational alignment, and much of the progress in one field ends up being rediscovered independently in another, when greater cross-field communication would be advantageous. To improve communication between fields, we propose a unifying framework that can serve as a common language b
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32447;&#24615;&#20301;&#32622;&#25554;&#20540;&#26469;&#25913;&#36827;ALiBi&#27169;&#22411;&#22806;&#25512;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#19978;&#28216;&#35821;&#35328;&#24314;&#27169;&#21644;&#19979;&#28216;&#25688;&#35201;&#21644;&#26816;&#32034;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.13017</link><description>&lt;p&gt;
&#20301;&#32622;&#25554;&#20540;&#25913;&#36827;&#20102;ALiBi&#22806;&#25512;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Position Interpolation Improves ALiBi Extrapolation. (arXiv:2310.13017v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13017
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32447;&#24615;&#20301;&#32622;&#25554;&#20540;&#26469;&#25913;&#36827;ALiBi&#27169;&#22411;&#22806;&#25512;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#19978;&#28216;&#35821;&#35328;&#24314;&#27169;&#21644;&#19979;&#28216;&#25688;&#35201;&#21644;&#26816;&#32034;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#20301;&#32622;&#25554;&#20540;&#26377;&#21161;&#20110;&#20351;&#29992;&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#65288;RoPE&#65289;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#26356;&#38271;&#30340;&#24207;&#21015;&#38271;&#24230;&#36827;&#34892;&#22806;&#25512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#32447;&#24615;&#20301;&#32622;&#25554;&#20540;&#26469;&#25193;&#23637;&#20351;&#29992;&#24102;&#26377;&#32447;&#24615;&#20559;&#24046;&#30340;&#27880;&#24847;&#21147;&#65288;ALiBi&#65289;&#30340;&#27169;&#22411;&#30340;&#22806;&#25512;&#33539;&#22260;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20301;&#32622;&#25554;&#20540;&#26174;&#33879;&#25913;&#21892;&#20102;&#19978;&#28216;&#35821;&#35328;&#24314;&#27169;&#21644;&#19979;&#28216;&#25688;&#35201;&#21644;&#26816;&#32034;&#20219;&#21153;&#30340;&#22806;&#25512;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear position interpolation helps pre-trained models using rotary position embeddings (RoPE) to extrapolate to longer sequence lengths. We propose using linear position interpolation to extend the extrapolation range of models using Attention with Linear Biases (ALiBi). We find position interpolation significantly improves extrapolation capability on upstream language modelling and downstream summarization and retrieval tasks.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#30340;&#20056;&#27861;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;10k&#25805;&#20316;&#31526;&#65292;&#27169;&#25311;&#20102;&#20154;&#31867;&#31867;&#20284;&#30340;&#25968;&#20540;&#36816;&#31639;&#65292;&#35299;&#20915;&#20102;GPT&#21644;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20056;&#27861;&#25361;&#25112;&#12290;&#35813;&#31639;&#27861;&#22312;100&#19975;&#20010;&#22823;&#25968;&#20056;&#27861;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;100&#65285;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13016</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#22270;&#24418;&#30340;&#26041;&#27861;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#32479;&#30340;&#20056;&#27861;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving the multiplication problem of a large language model system using a graph-based method. (arXiv:2310.13016v1 [cs.OH])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13016
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#30340;&#20056;&#27861;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;10k&#25805;&#20316;&#31526;&#65292;&#27169;&#25311;&#20102;&#20154;&#31867;&#31867;&#20284;&#30340;&#25968;&#20540;&#36816;&#31639;&#65292;&#35299;&#20915;&#20102;GPT&#21644;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20056;&#27861;&#25361;&#25112;&#12290;&#35813;&#31639;&#27861;&#22312;100&#19975;&#20010;&#22823;&#25968;&#20056;&#27861;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;100&#65285;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#65288;GPT&#65289;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#36719;&#20214;ChatGPT&#20855;&#26377;&#20986;&#33394;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#65292;&#20294;&#22312;&#35299;&#20915;&#31639;&#26415;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#20056;&#27861;&#26041;&#38754;&#19981;&#36275;&#12290;&#23427;&#30340;GPT&#32467;&#26500;&#20351;&#29992;&#35745;&#31639;&#22270;&#36827;&#34892;&#20056;&#27861;&#25805;&#20316;&#65292;&#20294;&#22312;&#31616;&#21333;&#20056;&#27861;&#25805;&#20316;&#20043;&#22806;&#65292;&#20854;&#20934;&#30830;&#24615;&#26377;&#38480;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#30340;&#20056;&#27861;&#31639;&#27861;&#65292;&#27169;&#25311;&#20102;&#31867;&#20284;&#20154;&#31867;&#30340;&#25968;&#20540;&#36816;&#31639;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;10k&#25805;&#20316;&#31526;&#65292;&#20854;&#20013;k&#34920;&#31034;&#20004;&#20010;&#36755;&#20837;&#25968;&#20013;&#36739;&#22823;&#25968;&#30340;&#20197;10&#20026;&#24213;&#30340;&#26368;&#22823;&#24130;&#27425;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#23545;&#20110;100&#19975;&#20010;&#22823;&#25968;&#20056;&#27861;&#20219;&#21153;&#23454;&#29616;&#20102;100&#65285;&#30340;&#20934;&#30830;&#24615;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#22522;&#20110;GPT&#21644;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20056;&#27861;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#23558;&#31616;&#21333;&#30340;&#20154;&#31867;&#27934;&#23519;&#21147;&#34701;&#20837;&#21040;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#35774;&#35745;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generative pre-trained transformer (GPT)-based chatbot software ChatGPT possesses excellent natural language processing capabilities but is inadequate for solving arithmetic problems, especially multiplication. Its GPT structure uses a computational graph for multiplication, which has limited accuracy beyond simple multiplication operations. We developed a graph-based multiplication algorithm that emulated human-like numerical operations by incorporating a 10k operator, where k represents the maximum power to base 10 of the larger of two input numbers. Our proposed algorithm attained 100% accuracy for 1,000,000 large number multiplication tasks, effectively solving the multiplication challenge of GPT-based and other large language models. Our work highlights the importance of blending simple human insights into the design of artificial intelligence algorithms. Keywords: Graph-based multiplication; ChatGPT; Multiplication problem
&lt;/p&gt;</description></item><item><title>Audio-AdapterFusion&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20219;&#21153;ID&#30340;&#22810;&#20219;&#21153;&#35821;&#38899;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#21333;&#20219;&#21153;&#36866;&#37197;&#22120;&#23454;&#29616;&#38750;&#30772;&#22351;&#24615;&#21644;&#21442;&#25968;&#39640;&#25928;&#65292;&#19988;&#22312;&#22810;&#20010;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.13015</link><description>&lt;p&gt;
Audio-AdapterFusion&#65306;&#19968;&#31181;&#26080;&#20219;&#21153;ID&#30340;&#39640;&#25928;&#21644;&#38750;&#30772;&#22351;&#24615;&#22810;&#20219;&#21153;&#35821;&#38899;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Audio-AdapterFusion: A Task-ID-free Approach for Efficient and Non-Destructive Multi-task Speech Recognition. (arXiv:2310.13015v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13015
&lt;/p&gt;
&lt;p&gt;
Audio-AdapterFusion&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20219;&#21153;ID&#30340;&#22810;&#20219;&#21153;&#35821;&#38899;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#21333;&#20219;&#21153;&#36866;&#37197;&#22120;&#23454;&#29616;&#38750;&#30772;&#22351;&#24615;&#21644;&#21442;&#25968;&#39640;&#25928;&#65292;&#19988;&#22312;&#22810;&#20010;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Adapter&#26159;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#39640;&#25928;&#21487;&#32452;&#21512;&#26367;&#20195;&#26041;&#26696;&#65292;&#26377;&#21161;&#20110;&#23558;&#22823;&#22411;ASR&#27169;&#22411;&#25193;&#23637;&#21040;&#22810;&#20010;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#25512;&#26029;&#26399;&#38388;&#65292;&#24120;&#24120;&#38656;&#35201;&#22312;&#36755;&#20837;&#19978;&#28155;&#21152;&#20219;&#21153;ID&#20197;&#23558;&#20854;&#36335;&#30001;&#21040;&#29305;&#23450;&#20219;&#21153;&#30340;&#21333;&#20219;&#21153;&#36866;&#37197;&#22120;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#26159;&#22312;&#25512;&#26029;&#26399;&#38388;&#20219;&#21153;ID&#21487;&#33021;&#26159;&#26410;&#30693;&#30340;&#65292;&#22240;&#27492;&#19981;&#36866;&#29992;&#20110;&#22823;&#22810;&#25968;&#22810;&#20219;&#21153;&#35774;&#32622;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#39062;&#30340;&#26080;&#20219;&#21153;ID&#26041;&#27861;&#26469;&#32467;&#21512;&#22810;&#20219;&#21153;ASR&#20013;&#30340;&#21333;&#20219;&#21153;&#36866;&#37197;&#22120;&#65292;&#24182;&#30740;&#31350;&#20102;&#20004;&#31181;&#29992;&#20110;&#35757;&#32451;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;4&#20010;&#19981;&#21516;ASR&#20219;&#21153;&#30340;10&#20010;&#27979;&#35797;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#38750;&#30772;&#22351;&#24615;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#12290;&#22312;&#20165;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#30340;17%&#30340;&#24773;&#20917;&#19979;&#65292;&#30456;&#23545;&#20110;&#23436;&#20840;&#24494;&#35843;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;8%&#30340;&#24179;&#22343;WER&#25913;&#36827;&#65292;&#24182;&#19982;&#20219;&#21153;ID&#36866;&#37197;&#22120;&#36335;&#30001;&#25928;&#26524;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adapters are an efficient, composable alternative to full fine-tuning of pre-trained models and help scale the deployment of large ASR models to many tasks. In practice, a task ID is commonly prepended to the input during inference to route to single-task adapters for the specified task. However, one major limitation of this approach is that the task ID may not be known during inference, rendering it unsuitable for most multi-task settings. To address this, we propose three novel task-ID-free methods to combine single-task adapters in multi-task ASR and investigate two learning algorithms for training. We evaluate our methods on 10 test sets from 4 diverse ASR tasks and show that our methods are non-destructive and parameter-efficient. While only updating 17% of the model parameters, our methods can achieve an 8% mean WER improvement relative to full fine-tuning and are on-par with task-ID adapter routing.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#21442;&#19982;Metaculus&#24179;&#21488;&#20030;&#21150;&#30340;&#39044;&#27979;&#31454;&#36187;&#65292;&#23454;&#35777;&#27979;&#35797;&#20102;OpenAI&#30340;&#26368;&#20808;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#30340;&#27010;&#29575;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#19982;&#20154;&#31867;&#39044;&#27979;&#30456;&#27604;&#26126;&#26174;&#19981;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2310.13014</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#65306;&#26469;&#33258;&#29616;&#23454;&#39044;&#27979;&#31454;&#36187;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Prediction Capabilities: Evidence from a Real-World Forecasting Tournament. (arXiv:2310.13014v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13014
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#21442;&#19982;Metaculus&#24179;&#21488;&#20030;&#21150;&#30340;&#39044;&#27979;&#31454;&#36187;&#65292;&#23454;&#35777;&#27979;&#35797;&#20102;OpenAI&#30340;&#26368;&#20808;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#30340;&#27010;&#29575;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#19982;&#20154;&#31867;&#39044;&#27979;&#30456;&#27604;&#26126;&#26174;&#19981;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#23558;&#26159;&#20154;&#24037;&#26234;&#33021;&#33021;&#21147;&#30340;&#37325;&#35201;&#37324;&#31243;&#30865;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20851;&#20110;&#26410;&#26469;&#20107;&#20214;&#27010;&#29575;&#39044;&#27979;&#33021;&#21147;&#30340;&#30740;&#31350;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;&#20026;&#20102;&#32463;&#39564;&#24615;&#22320;&#27979;&#35797;&#36825;&#31181;&#33021;&#21147;&#65292;&#25105;&#20204;&#23558;OpenAI&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#32435;&#20837;&#20102;Metaculus&#24179;&#21488;&#20030;&#21150;&#30340;&#20026;&#26399;&#19977;&#20010;&#26376;&#30340;&#39044;&#27979;&#31454;&#36187;&#12290;&#36825;&#22330;&#20174;2023&#24180;7&#26376;&#21040;10&#26376;&#36827;&#34892;&#30340;&#31454;&#36187;&#21560;&#24341;&#20102;843&#21517;&#21442;&#19982;&#32773;&#65292;&#28085;&#30422;&#20102;&#21253;&#25324;&#22823;&#22411;&#31185;&#25216;&#20844;&#21496;&#12289;&#32654;&#22269;&#25919;&#27835;&#12289;&#30149;&#27602;&#29190;&#21457;&#21644;&#20044;&#20811;&#20848;&#20914;&#31361;&#22312;&#20869;&#30340;&#21508;&#31181;&#20027;&#39064;&#12290;&#25105;&#20204;&#32858;&#28966;&#20110;&#20108;&#36827;&#21046;&#39044;&#27979;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#20154;&#32676;&#20013;&#20301;&#25968;&#39044;&#27979;&#30456;&#27604;&#65292;GPT-4&#30340;&#27010;&#29575;&#39044;&#27979;&#26126;&#26174;&#19981;&#20934;&#30830;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-4&#30340;&#39044;&#27979;&#19982;&#23558;&#27599;&#20010;&#38382;&#39064;&#30340;&#27010;&#29575;&#20998;&#37197;&#20026;50%&#30340;&#26080;&#20449;&#24687;&#39044;&#27979;&#31574;&#30053;&#27809;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#35299;&#37322;&#65292;&#21363;GPT-4&#21487;&#33021;&#26377;&#20542;&#21521;&#24615;&#22320;&#39044;&#27979;&#27010;&#29575;&#20026;50%&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately predicting the future would be an important milestone in the capabilities of artificial intelligence. However, research on the ability of large language models to provide probabilistic predictions about future events remains nascent. To empirically test this ability, we enrolled OpenAI's state-of-the-art large language model, GPT-4, in a three-month forecasting tournament hosted on the Metaculus platform. The tournament, running from July to October 2023, attracted 843 participants and covered diverse topics including Big Tech, U.S. politics, viral outbreaks, and the Ukraine conflict. Focusing on binary forecasts, we show that GPT-4's probabilistic forecasts are significantly less accurate than the median human-crowd forecasts. We find that GPT-4's forecasts did not significantly differ from the no-information forecasting strategy of assigning a 50% probability to every question. We explore a potential explanation, that GPT-4 might be predisposed to predict probabilities clo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#33258;&#21160;&#29983;&#25104;&#30340;&#20551;&#35774;&#21015;&#34920;&#26469;&#36827;&#34892;&#20195;&#30721;&#20132;&#26367;&#35821;&#38899;&#35782;&#21035;&#30340;&#29983;&#25104;&#24335;&#38169;&#35823;&#26657;&#27491;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.13013</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#30721;&#20132;&#26367;&#35821;&#38899;&#35782;&#21035;&#30340;&#29983;&#25104;&#24335;&#38169;&#35823;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
Generative error correction for code-switching speech recognition using large language models. (arXiv:2310.13013v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#33258;&#21160;&#29983;&#25104;&#30340;&#20551;&#35774;&#21015;&#34920;&#26469;&#36827;&#34892;&#20195;&#30721;&#20132;&#26367;&#35821;&#38899;&#35782;&#21035;&#30340;&#29983;&#25104;&#24335;&#38169;&#35823;&#26657;&#27491;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#20132;&#26367;&#35821;&#38899;&#35782;&#21035;&#26159;&#25351;&#22312;&#21516;&#19968;&#21477;&#23376;&#20013;&#28151;&#21512;&#20351;&#29992;&#20004;&#31181;&#25110;&#26356;&#22810;&#35821;&#35328;&#30340;&#29616;&#35937;&#12290;&#23613;&#31649;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20195;&#30721;&#20132;&#26367;&#35821;&#38899;&#35782;&#21035;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#21407;&#22240;&#26159;&#35813;&#29616;&#35937;&#30340;&#35821;&#27861;&#32467;&#26500;&#22797;&#26434;&#20197;&#21450;&#29305;&#23450;&#35757;&#32451;&#35821;&#26009;&#24211;&#30340;&#25968;&#25454;&#31232;&#32570;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;ASR&#29983;&#25104;&#30340;&#20551;&#35774;&#21015;&#34920;&#26469;&#35299;&#20915;&#20195;&#30721;&#20132;&#26367;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#22810;&#20010;&#32463;&#36807;&#33391;&#22909;&#35757;&#32451;&#30340;ASR&#27169;&#22411;&#36827;&#34892;N-best&#20551;&#35774;&#29983;&#25104;&#65292;&#26088;&#22312;&#22686;&#21152;&#20551;&#35774;&#38598;&#20013;&#30340;&#22810;&#26679;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#21033;&#29992;LLM&#23398;&#20064;&#20551;&#35774;&#21040;&#36716;&#24405;&#30340;&#26144;&#23556;&#65292;&#36890;&#36807;&#28155;&#21152;&#21487;&#35757;&#32451;&#30340;&#20302;&#31209;&#36866;&#37197;&#22120;&#12290;&#36825;&#31181;&#29983;&#25104;&#24335;&#38169;&#35823;&#26657;&#27491;&#65288;GER&#65289;&#26041;&#27861;&#26681;&#25454;&#20854;&#19987;&#19994;&#30340;&#35821;&#35328;&#30693;&#35782;&#21644;N-best&#20551;&#35774;&#30452;&#25509;&#39044;&#27979;&#20934;&#30830;&#30340;&#36716;&#24405;&#65292;&#20174;&#20256;&#32479;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#33539;&#24335;&#36716;&#21464;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code-switching (CS) speech refers to the phenomenon of mixing two or more languages within the same sentence. Despite the recent advances in automatic speech recognition (ASR), CS-ASR is still a challenging task ought to the grammatical structure complexity of the phenomenon and the data scarcity of specific training corpus. In this work, we propose to leverage large language models (LLMs) and lists of hypotheses generated by an ASR to address the CS problem. Specifically, we first employ multiple well-trained ASR models for N-best hypotheses generation, with the aim of increasing the diverse and informative elements in the set of hypotheses. Next, we utilize the LLMs to learn the hypotheses-to-transcription (H2T) mapping by adding a trainable low-rank adapter. Such a generative error correction (GER) method directly predicts the accurate transcription according to its expert linguistic knowledge and N-best hypotheses, resulting in a paradigm shift from the traditional language model r
&lt;/p&gt;</description></item><item><title>H2O&#25512;&#20986;&#20102;&#24320;&#25918;&#29983;&#24577;&#31995;&#32479;&#65292;&#26088;&#22312;&#24320;&#21457;&#21644;&#27979;&#35797;&#26368;&#20808;&#36827;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21253;&#25324;h2oGPT&#21644;H2O LLM Studio&#12290;&#36825;&#19968;&#24320;&#28304;&#39033;&#30446;&#25552;&#20379;&#20102;&#20840;&#38754;&#24320;&#25918;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#33021;&#22815;&#24110;&#21161;&#25512;&#21160;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#20351;&#20854;&#26356;&#21152;&#21487;&#20449;&#36182;&#21644;&#21487;&#35775;&#38382;&#12290;</title><link>http://arxiv.org/abs/2310.13012</link><description>&lt;p&gt;
H2O&#24320;&#25918;&#29983;&#24577;&#31995;&#32479;&#29992;&#20110;&#26368;&#20808;&#36827;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
H2O Open Ecosystem for State-of-the-art Large Language Models. (arXiv:2310.13012v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13012
&lt;/p&gt;
&lt;p&gt;
H2O&#25512;&#20986;&#20102;&#24320;&#25918;&#29983;&#24577;&#31995;&#32479;&#65292;&#26088;&#22312;&#24320;&#21457;&#21644;&#27979;&#35797;&#26368;&#20808;&#36827;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21253;&#25324;h2oGPT&#21644;H2O LLM Studio&#12290;&#36825;&#19968;&#24320;&#28304;&#39033;&#30446;&#25552;&#20379;&#20102;&#20840;&#38754;&#24320;&#25918;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#33021;&#22815;&#24110;&#21161;&#25512;&#21160;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#20351;&#20854;&#26356;&#21152;&#21487;&#20449;&#36182;&#21644;&#21487;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#34920;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#39033;&#38761;&#21629;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20063;&#24102;&#26469;&#20102;&#35768;&#22810;&#37325;&#22823;&#39118;&#38505;&#65292;&#20363;&#22914;&#23384;&#22312;&#20559;&#35265;&#12289;&#31169;&#26377;&#12289;&#21463;&#29256;&#26435;&#20445;&#25252;&#25110;&#26377;&#23475;&#30340;&#25991;&#26412;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#24320;&#25918;&#12289;&#36879;&#26126;&#21644;&#23433;&#20840;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#24320;&#28304;&#29983;&#24577;&#31995;&#32479;&#65292;&#29992;&#20110;&#24320;&#21457;&#21644;&#27979;&#35797;LLMs&#12290;&#35813;&#39033;&#30446;&#30340;&#30446;&#26631;&#26159;&#25512;&#21160;&#23545;&#23553;&#38381;&#28304;&#26041;&#27861;&#30340;&#24320;&#25918;&#24335;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;h2oGPT&#65292;&#21363;&#20174;70&#20159;&#21040;700&#20159;&#21442;&#25968;&#30340;&#19968;&#31995;&#21015;&#31934;&#32454;&#35843;&#25972;&#30340;LLMs&#12290;&#25105;&#20204;&#36824;&#25512;&#20986;&#20102;H2O LLM Studio&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#21644;&#26080;&#20195;&#30721;GUI&#65292;&#19987;&#20026;&#20351;&#29992;&#26368;&#26032;&#30340;&#20808;&#36827;&#25216;&#26415;&#36827;&#34892;LLMs&#30340;&#39640;&#25928;&#31934;&#32454;&#35843;&#25972;&#12289;&#35780;&#20272;&#21644;&#37096;&#32626;&#32780;&#35774;&#35745;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#27169;&#22411;&#22312;&#23436;&#20840;&#33258;&#30001;&#30340;Apache 2.0&#35768;&#21487;&#35777;&#19979;&#25480;&#26435;&#20351;&#29992;&#12290;&#25105;&#20204;&#30456;&#20449;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#26377;&#21161;&#20110;&#25512;&#21160;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#24182;&#20351;&#20854;&#26356;&#21487;&#35775;&#38382;&#21644;&#21487;&#20449;&#36182;&#12290;&#28436;&#31034;&#32593;&#22336;&#20026;&#65306;https://gpt.h2o.ai/
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) represent a revolution in AI. However, they also pose many significant risks, such as the presence of biased, private, copyrighted or harmful text. For this reason we need open, transparent and safe solutions. We introduce a complete open-source ecosystem for developing and testing LLMs. The goal of this project is to boost open alternatives to closed-source approaches. We release h2oGPT, a family of fine-tuned LLMs from 7 to 70 Billion parameters. We also introduce H2O LLM Studio, a framework and no-code GUI designed for efficient fine-tuning, evaluation, and deployment of LLMs using the most recent state-of-the-art techniques. Our code and models are licensed under fully permissive Apache 2.0 licenses. We believe open-source language models help to boost AI development and make it more accessible and trustworthy. The demo is available at: https://gpt.h2o.ai/
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;Perceiver&#30340;&#24207;&#21015;&#20998;&#31867;&#22120;&#21644;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#26816;&#27979;&#35821;&#38899;&#24322;&#24120;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#38024;&#23545;&#19981;&#21516;&#31867;&#21035;&#30340;&#36755;&#20837;&#24314;&#27169;&#19981;&#21516;&#30340;&#21306;&#22495;&#65292;&#21516;&#26102;&#20855;&#26377;&#25968;&#25454;&#25928;&#29575;&#24615;&#12290;&#36890;&#36807;&#22312;Mayo Clinic&#30340;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#35813;&#27169;&#22411;&#34920;&#29616;&#20986;&#33394;&#65292;&#24179;&#22343;&#20934;&#30830;&#29575;&#36798;&#21040;83.1%&#12290;&#39044;&#35757;&#32451;&#26159;&#37325;&#35201;&#30340;&#65292;&#32780;&#19988;&#24847;&#22806;&#30340;&#26159;&#65292;&#19982;&#19981;&#30456;&#20851;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#20219;&#21153;&#36827;&#34892;&#39044;&#35757;&#32451;&#20063;&#26159;&#26377;&#30410;&#30340;&#12290;</title><link>http://arxiv.org/abs/2310.13010</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;Perceiver&#30340;&#24207;&#21015;&#20998;&#31867;&#22120;&#21644;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#26816;&#27979;&#35821;&#38899;&#24322;&#24120;
&lt;/p&gt;
&lt;p&gt;
Detecting Speech Abnormalities with a Perceiver-based Sequence Classifier that Leverages a Universal Speech Model. (arXiv:2310.13010v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13010
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;Perceiver&#30340;&#24207;&#21015;&#20998;&#31867;&#22120;&#21644;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#26816;&#27979;&#35821;&#38899;&#24322;&#24120;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#38024;&#23545;&#19981;&#21516;&#31867;&#21035;&#30340;&#36755;&#20837;&#24314;&#27169;&#19981;&#21516;&#30340;&#21306;&#22495;&#65292;&#21516;&#26102;&#20855;&#26377;&#25968;&#25454;&#25928;&#29575;&#24615;&#12290;&#36890;&#36807;&#22312;Mayo Clinic&#30340;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#35813;&#27169;&#22411;&#34920;&#29616;&#20986;&#33394;&#65292;&#24179;&#22343;&#20934;&#30830;&#29575;&#36798;&#21040;83.1%&#12290;&#39044;&#35757;&#32451;&#26159;&#37325;&#35201;&#30340;&#65292;&#32780;&#19988;&#24847;&#22806;&#30340;&#26159;&#65292;&#19982;&#19981;&#30456;&#20851;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#20219;&#21153;&#36827;&#34892;&#39044;&#35757;&#32451;&#20063;&#26159;&#26377;&#30410;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Perceiver&#30340;&#24207;&#21015;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#26816;&#27979;&#21453;&#26144;&#22810;&#31181;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#30340;&#35821;&#38899;&#24322;&#24120;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#20998;&#31867;&#22120;&#19982;&#19968;&#20010;&#35757;&#32451;&#26377;&#32032;&#30340;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#65288;USM&#65289;&#30456;&#32467;&#21512;&#65292;&#35813;&#27169;&#22411;&#22312;1200&#19975;&#23567;&#26102;&#30340;&#22810;&#26679;&#21270;&#38899;&#39057;&#35760;&#24405;&#19978;&#36827;&#34892;&#26080;&#30417;&#30563;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;&#38271;&#24207;&#21015;&#21387;&#32553;&#20026;&#19968;&#23567;&#32452;&#29305;&#23450;&#31867;&#21035;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#20998;&#35299;&#25237;&#24433;&#39044;&#27979;&#36755;&#20837;&#35821;&#38899;&#30340;&#19981;&#21516;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#22909;&#22788;&#26159;&#21487;&#20197;&#20026;&#19981;&#21516;&#31867;&#21035;&#30340;&#36755;&#20837;&#24314;&#27169;&#19981;&#21516;&#30340;&#21306;&#22495;&#65292;&#24182;&#19988;&#21516;&#26102;&#20855;&#26377;&#25968;&#25454;&#25928;&#29575;&#24615;&#12290;&#25105;&#20204;&#23545;Mayo Clinic&#30340;&#31934;&#36873;&#35821;&#26009;&#24211;&#23545;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#26631;&#20934;Transformer&#65288;80.9%&#65289;&#21644;Perceiver&#65288;81.8%&#65289;&#27169;&#22411;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24179;&#22343;&#20934;&#30830;&#29575;&#36798;&#21040;83.1%&#12290;&#22312;&#26377;&#38480;&#30340;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;&#39044;&#35757;&#32451;&#38750;&#24120;&#37325;&#35201;&#65292;&#32780;&#19988;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#19982;&#19981;&#30456;&#20851;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#20219;&#21153;&#36827;&#34892;&#39044;&#35757;&#32451;&#20063;&#26159;&#26377;&#30410;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a Perceiver-based sequence classifier to detect abnormalities in speech reflective of several neurological disorders. We combine this classifier with a Universal Speech Model (USM) that is trained (unsupervised) on 12 million hours of diverse audio recordings. Our model compresses long sequences into a small set of class-specific latent representations and a factorized projection is used to predict different attributes of the disordered input speech. The benefit of our approach is that it allows us to model different regions of the input for different classes and is at the same time data efficient. We evaluated the proposed model extensively on a curated corpus from the Mayo Clinic. Our model outperforms standard transformer (80.9%) and perceiver (81.8%) models and achieves an average accuracy of 83.1%. With limited task-specific data, we find that pretraining is important and surprisingly pretraining with the unrelated automatic speech recognition (ASR) task is also benefic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;LoBaSS&#65292;&#21033;&#29992;&#25968;&#25454;&#30340;&#21487;&#23398;&#20064;&#24615;&#20316;&#20026;&#36873;&#25321;&#30417;&#30563;&#24494;&#35843;&#25968;&#25454;&#30340;&#20027;&#35201;&#26631;&#20934;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26681;&#25454;&#27169;&#22411;&#30340;&#33021;&#21147;&#23558;&#25968;&#25454;&#36873;&#25321;&#19982;&#27169;&#22411;&#23545;&#40784;&#65292;&#30830;&#20445;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.13008</link><description>&lt;p&gt;
LoBaSS&#65306;&#22312;&#30417;&#30563;&#24494;&#35843;&#25968;&#25454;&#20013;&#27979;&#37327;&#21487;&#23398;&#20064;&#24615;
&lt;/p&gt;
&lt;p&gt;
LoBaSS: Gauging Learnability in Supervised Fine-tuning Data. (arXiv:2310.13008v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;LoBaSS&#65292;&#21033;&#29992;&#25968;&#25454;&#30340;&#21487;&#23398;&#20064;&#24615;&#20316;&#20026;&#36873;&#25321;&#30417;&#30563;&#24494;&#35843;&#25968;&#25454;&#30340;&#20027;&#35201;&#26631;&#20934;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26681;&#25454;&#27169;&#22411;&#30340;&#33021;&#21147;&#23558;&#25968;&#25454;&#36873;&#25321;&#19982;&#27169;&#22411;&#23545;&#40784;&#65292;&#30830;&#20445;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#26159;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#29305;&#23450;&#20219;&#21153;&#30340;&#20808;&#20915;&#26465;&#20214;&#23545;&#40784;&#30340;&#20851;&#38190;&#38454;&#27573;&#12290;&#24494;&#35843;&#25968;&#25454;&#30340;&#36873;&#25321;&#28145;&#21051;&#24433;&#21709;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20256;&#32479;&#19978;&#20197;&#25968;&#25454;&#36136;&#37327;&#21644;&#20998;&#24067;&#20026;&#22522;&#30784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SFT&#25968;&#25454;&#36873;&#25321;&#30340;&#19968;&#20010;&#26032;&#32500;&#24230;&#65306;&#21487;&#23398;&#20064;&#24615;&#12290;&#36825;&#20010;&#26032;&#32500;&#24230;&#30340;&#21160;&#26426;&#26159;&#30001;LLM&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#33719;&#24471;&#30340;&#33021;&#21147;&#12290;&#37492;&#20110;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20855;&#26377;&#19981;&#21516;&#30340;&#33021;&#21147;&#65292;&#36866;&#21512;&#19968;&#20010;&#27169;&#22411;&#30340;SFT&#25968;&#25454;&#21487;&#33021;&#19981;&#36866;&#21512;&#21478;&#19968;&#20010;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23398;&#20064;&#33021;&#21147;&#36825;&#20010;&#26415;&#35821;&#26469;&#23450;&#20041;&#25968;&#25454;&#23545;&#27169;&#22411;&#36827;&#34892;&#26377;&#25928;&#23398;&#20064;&#30340;&#36866;&#21512;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25439;&#22833;&#30340;SFT&#25968;&#25454;&#36873;&#25321;&#65288;LoBaSS&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#25968;&#25454;&#30340;&#21487;&#23398;&#20064;&#24615;&#20316;&#20026;&#36873;&#25321;SFT&#25968;&#25454;&#30340;&#20027;&#35201;&#26631;&#20934;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#32454;&#33268;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#23558;&#25968;&#25454;&#36873;&#25321;&#19982;&#22266;&#26377;&#30340;&#27169;&#22411;&#33021;&#21147;&#23545;&#40784;&#65292;&#30830;&#20445;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised Fine-Tuning (SFT) serves as a crucial phase in aligning Large Language Models (LLMs) to specific task prerequisites. The selection of fine-tuning data profoundly influences the model's performance, whose principle is traditionally grounded in data quality and distribution. In this paper, we introduce a new dimension in SFT data selection: learnability. This new dimension is motivated by the intuition that SFT unlocks capabilities acquired by a LLM during the pretraining phase. Given that different pretrained models have disparate capabilities, the SFT data appropriate for one may not suit another. Thus, we introduce the term learnability to define the suitability of data for effective learning by the model. We present the Loss Based SFT Data Selection (LoBaSS) method, utilizing data learnability as the principal criterion for the selection SFT data. This method provides a nuanced approach, allowing the alignment of data selection with inherent model capabilities, ensuring op
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#25209;&#21028;&#24615;&#35843;&#26597;&#20998;&#26512;&#20102;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#19982;&#20844;&#24179;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25351;&#20986;XAI&#22312;&#23454;&#29616;&#20844;&#24179;&#29702;&#24819;&#26041;&#38754;&#23384;&#22312;&#28508;&#21147;&#21644;&#38480;&#21046;&#65292;&#21628;&#21505;&#26356;&#20855;&#20307;&#22320;&#35828;&#26126;XAI&#26041;&#27861;&#22914;&#20309;&#24110;&#21161;&#35299;&#20915;&#20844;&#24179;&#29702;&#24819;&#12290;</title><link>http://arxiv.org/abs/2310.13007</link><description>&lt;p&gt;
XAI&#30340;&#20844;&#24179;&#25928;&#30410;&#30340;&#25209;&#21028;&#24615;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Critical Survey on Fairness Benefits of XAI. (arXiv:2310.13007v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13007
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#25209;&#21028;&#24615;&#35843;&#26597;&#20998;&#26512;&#20102;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#19982;&#20844;&#24179;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25351;&#20986;XAI&#22312;&#23454;&#29616;&#20844;&#24179;&#29702;&#24819;&#26041;&#38754;&#23384;&#22312;&#28508;&#21147;&#21644;&#38480;&#21046;&#65292;&#21628;&#21505;&#26356;&#20855;&#20307;&#22320;&#35828;&#26126;XAI&#26041;&#27861;&#22914;&#20309;&#24110;&#21161;&#35299;&#20915;&#20844;&#24179;&#29702;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20010;&#25209;&#21028;&#24615;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20851;&#20110;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#21644;&#20844;&#24179;&#20043;&#38388;&#20851;&#31995;&#30340;&#20856;&#22411;&#35770;&#36848;&#65292;&#20197;&#35299;&#24320;&#36825;&#20004;&#20010;&#27010;&#24565;&#20043;&#38388;&#30340;&#22810;&#32500;&#20851;&#31995;&#12290;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#21644;&#38543;&#21518;&#30340;&#23450;&#24615;&#20869;&#23481;&#20998;&#26512;&#65292;&#25105;&#20204;&#20174;175&#31687;&#35770;&#25991;&#20013;&#35782;&#21035;&#20986;&#20851;&#20110;XAI&#30340;&#20844;&#24179;&#25928;&#30410;&#30340;&#19971;&#20010;&#20856;&#22411;&#35770;&#26029;&#12290;&#25105;&#20204;&#25552;&#20986;&#20851;&#20110;&#36825;&#20123;&#35770;&#26029;&#30340;&#37325;&#35201;&#35686;&#21578;&#65292;&#24182;&#20026;&#26410;&#26469;&#22260;&#32469;XAI&#22312;&#29305;&#23450;&#20844;&#24179;&#29702;&#24819;&#20013;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#36827;&#34892;&#35752;&#35770;&#25552;&#20379;&#20102;&#19968;&#20010;&#20999;&#20837;&#28857;&#12290;&#34429;&#28982;&#25991;&#29486;&#36890;&#24120;&#35748;&#20026;XAI&#26159;&#23454;&#29616;&#22810;&#20010;&#20844;&#24179;&#29702;&#24819;&#30340;&#19968;&#31181;&#25163;&#27573;&#65292;&#20294;&#25105;&#20204;&#27880;&#24847;&#21040;&#36825;&#20123;&#29702;&#24819;&#19982;XAI&#30340;&#33021;&#21147;&#20043;&#38388;&#23384;&#22312;&#19981;&#19968;&#33268;&#12290;&#25105;&#20204;&#40723;&#21169;&#23558;XAI&#35270;&#20026;&#24212;&#23545;&#31639;&#27861;&#20844;&#24179;&#36825;&#19968;&#22810;&#32500;&#31038;&#20250;&#25216;&#26415;&#25361;&#25112;&#30340;&#20247;&#22810;&#24037;&#20855;&#20043;&#19968;&#65292;&#24182;&#26356;&#20855;&#20307;&#22320;&#35828;&#26126;&#21738;&#31181;XAI&#26041;&#27861;&#22914;&#20309;&#24110;&#21161;&#21738;&#20123;&#20154;&#35299;&#20915;&#21738;&#20123;&#20844;&#24179;&#29702;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this critical survey, we analyze typical claims on the relationship between explainable AI (XAI) and fairness to disentangle the multidimensional relationship between these two concepts. Based on a systematic literature review and a subsequent qualitative content analysis, we identify seven archetypal claims from 175 papers on the alleged fairness benefits of XAI. We present crucial caveats with respect to these claims and provide an entry point for future discussions around the potentials and limitations of XAI for specific fairness desiderata. While the literature often suggests XAI to be an enabler for several fairness desiderata, we notice a misalignment between these desiderata and the capabilities of XAI. We encourage to conceive XAI as one of many tools to approach the multidimensional, sociotechnical challenge of algorithmic fairness and to be more specific about how exactly what kind of XAI method enables whom to address which fairness desideratum.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#20108;&#36827;&#21046;&#20195;&#30721;&#35780;&#35770;&#36136;&#37327;&#20998;&#31867;&#27169;&#22411;&#20013;&#25552;&#21319;&#24615;&#33021;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#29983;&#25104;&#25968;&#25454;&#38598;&#65292;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#24471;&#21040;&#20102;&#26174;&#33879;&#25913;&#21892;&#65292;&#25903;&#25345;&#21521;&#37327;&#26426;&#27169;&#22411;&#30340;&#31934;&#30830;&#24230;&#25552;&#39640;&#20102;6%&#65292;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#21484;&#22238;&#29575;&#25552;&#39640;&#20102;1.5%&#12290;</title><link>http://arxiv.org/abs/2310.13006</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36719;&#20214;&#20803;&#25968;&#25454;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Software Metadata Classification based on Generative Artificial Intelligence. (arXiv:2310.13006v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#20108;&#36827;&#21046;&#20195;&#30721;&#35780;&#35770;&#36136;&#37327;&#20998;&#31867;&#27169;&#22411;&#20013;&#25552;&#21319;&#24615;&#33021;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#29983;&#25104;&#25968;&#25454;&#38598;&#65292;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#24471;&#21040;&#20102;&#26174;&#33879;&#25913;&#21892;&#65292;&#25903;&#25345;&#21521;&#37327;&#26426;&#27169;&#22411;&#30340;&#31934;&#30830;&#24230;&#25552;&#39640;&#20102;6%&#65292;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#21484;&#22238;&#29575;&#25552;&#39640;&#20102;1.5%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;(AI)&#26469;&#25552;&#39640;&#20108;&#36827;&#21046;&#20195;&#30721;&#35780;&#35770;&#36136;&#37327;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#21033;&#29992;OpenAI API&#65292;&#23558;&#20174;&#21508;&#31181;GitHub&#20179;&#24211;&#21644;&#24320;&#28304;&#39033;&#30446;&#20013;&#25552;&#21462;&#30340;1239&#20010;&#26032;&#29983;&#25104;&#30340;&#20195;&#30721;-&#35780;&#35770;&#23545;&#25968;&#25454;&#38598;&#26631;&#35760;&#20026;&#8220;&#26377;&#29992;&#8221;&#25110;&#8220;&#26080;&#29992;&#8221;&#65292;&#24182;&#19982;&#29616;&#26377;&#30340;9048&#20010;C&#32534;&#31243;&#35821;&#35328;&#23545;&#25968;&#25454;&#38598;&#38598;&#25104;&#12290;&#21033;&#29992;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#65292;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#25913;&#21892;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24403;&#38598;&#25104;&#21040;&#25903;&#25345;&#21521;&#37327;&#26426;(SVM)&#27169;&#22411;&#20013;&#26102;&#65292;&#31934;&#30830;&#24230;&#25552;&#39640;&#20102;6%&#65292;&#20174;0.79&#22686;&#21152;&#21040;0.85&#12290;&#27492;&#22806;&#65292;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;(ANN)&#27169;&#22411;&#30340;&#21484;&#22238;&#29575;&#25552;&#39640;&#20102;1.5%&#65292;&#20174;0.731&#22686;&#21152;&#21040;0.746&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#22686;&#24378;&#20195;&#30721;&#35780;&#35770;&#36136;&#37327;&#20998;&#31867;&#27169;&#22411;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach to enhance the performance of binary code comment quality classification models through the application of Generative Artificial Intelligence (AI). By leveraging the OpenAI API, a dataset comprising 1239 newly generated code-comment pairs, extracted from various GitHub repositories and open-source projects, has been labelled as "Useful" or "Not Useful", and integrated into the existing corpus of 9048 pairs in the C programming language. Employing a cutting-edge Large Language Model Architecture, the generated dataset demonstrates notable improvements in model accuracy. Specifically, when incorporated into the Support Vector Machine (SVM) model, a 6% increase in precision is observed, rising from 0.79 to 0.85. Additionally, the Artificial Neural Network (ANN) model exhibits a 1.5% increase in recall, climbing from 0.731 to 0.746. This paper sheds light on the potential of Generative AI in augmenting code comment quality classification models. The res
&lt;/p&gt;</description></item><item><title>&#20803;&#35748;&#30693;&#38408;&#20540;&#30340;&#35745;&#31639;&#27169;&#22411;&#20197;&#21450;&#36890;&#36807;&#20803;&#35748;&#30693;&#35757;&#32451;&#21644;&#20901;&#24819;&#24433;&#21709;&#35813;&#38408;&#20540;&#30340;&#28508;&#22312;&#35748;&#30693;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.13005</link><description>&lt;p&gt;
Metacognitive threshold: a computational account. (arXiv:2310.13005v1 [cs.OH])&#30340;&#32763;&#35793;&#26631;&#39064;: &#20803;&#35748;&#30693;&#38408;&#20540;&#65306;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Metacognitive threshold: a computational account. (arXiv:2310.13005v1 [cs.OH])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13005
&lt;/p&gt;
&lt;p&gt;
&#20803;&#35748;&#30693;&#38408;&#20540;&#30340;&#35745;&#31639;&#27169;&#22411;&#20197;&#21450;&#36890;&#36807;&#20803;&#35748;&#30693;&#35757;&#32451;&#21644;&#20901;&#24819;&#24433;&#21709;&#35813;&#38408;&#20540;&#30340;&#28508;&#22312;&#35748;&#30693;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#25506;&#35752;&#22914;&#20309;&#35745;&#31639;&#20803;&#35748;&#30693;&#38408;&#20540;&#65292;&#21363;&#34987;&#24863;&#30693;&#20026;&#19968;&#31181;&#24515;&#29702;&#29366;&#24577;&#25152;&#38656;&#30340;&#26368;&#23567;&#21050;&#28608;&#37327;&#65292;&#24182;&#35752;&#35770;&#36890;&#36807;&#20803;&#35748;&#30693;&#35757;&#32451;&#21644;&#20901;&#24819;&#21487;&#20197;&#24433;&#21709;&#35813;&#38408;&#20540;&#30340;&#28508;&#22312;&#35748;&#30693;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper will explore ways of computationally accounting for the metacognitive threshold -- the minimum amount of stimulus needed for a mental state to be perceived -- and discuss potential cognitive mechanisms by which this threshold can be influenced through metacognitive training and meditation.
&lt;/p&gt;</description></item><item><title>CEIL&#26159;&#19968;&#31181;&#28176;&#36827;&#39640;&#25928;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32473;&#23398;&#20064;&#20195;&#29702;&#25552;&#20379;&#19968;&#20010;&#25277;&#35937;&#12289;&#21160;&#24577;&#30340;&#35821;&#35328;&#21644;&#19968;&#31181;&#20869;&#22312;&#30340;&#21160;&#26426;&#65292;&#20197;&#23613;&#21487;&#33021;&#23569;&#30340;&#27807;&#36890;&#20195;&#20215;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#31867;&#20284;&#20154;&#31867;&#36880;&#27493;&#39640;&#25928;&#27807;&#36890;&#30340;&#33021;&#21147;&#12290;&#22312;2D MineCraft&#39046;&#22495;&#19978;&#65292;CEIL&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#21644;&#27807;&#36890;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.13004</link><description>&lt;p&gt;
&#28176;&#36827;&#39640;&#25928;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Progressively Efficient Learning. (arXiv:2310.13004v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13004
&lt;/p&gt;
&lt;p&gt;
CEIL&#26159;&#19968;&#31181;&#28176;&#36827;&#39640;&#25928;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32473;&#23398;&#20064;&#20195;&#29702;&#25552;&#20379;&#19968;&#20010;&#25277;&#35937;&#12289;&#21160;&#24577;&#30340;&#35821;&#35328;&#21644;&#19968;&#31181;&#20869;&#22312;&#30340;&#21160;&#26426;&#65292;&#20197;&#23613;&#21487;&#33021;&#23569;&#30340;&#27807;&#36890;&#20195;&#20215;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#31867;&#20284;&#20154;&#31867;&#36880;&#27493;&#39640;&#25928;&#27807;&#36890;&#30340;&#33021;&#21147;&#12290;&#22312;2D MineCraft&#39046;&#22495;&#19978;&#65292;CEIL&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#21644;&#27807;&#36890;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21161;&#29702; AI &#20195;&#29702;&#24212;&#35813;&#33021;&#22815;&#36805;&#36895;&#33719;&#21462;&#26032;&#25216;&#33021;&#24182;&#36866;&#24212;&#29992;&#25143;&#30340;&#26032;&#20559;&#22909;&#12290;&#20256;&#32479;&#30340;&#26694;&#26550;&#22914;&#27169;&#20223;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#19981;&#33021;&#25903;&#25345;&#36825;&#31181;&#33021;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#21482;&#25903;&#25345;&#20302;&#32423;&#12289;&#20302;&#25928;&#30340;&#27807;&#36890;&#24418;&#24335;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#36890;&#36807;&#23450;&#20041;&#21644;&#20998;&#20139;&#25277;&#35937;&#24847;&#22270;&#26469;&#23454;&#29616;&#36880;&#27493;&#39640;&#25928;&#30340;&#27807;&#36890;&#12290;&#20026;&#20102;&#22312; AI &#20195;&#29702;&#20013;&#20877;&#29616;&#31867;&#20284;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#27807;&#36890;&#39640;&#25928;&#20132;&#20114;&#23398;&#20064;&#65288;CEIL&#65289;&#30340;&#26032;&#22411;&#23398;&#20064;&#26694;&#26550;&#12290;&#36890;&#36807;&#20026;&#23398;&#20064;&#20195;&#29702;&#25552;&#20379;&#19968;&#20010;&#25277;&#35937;&#12289;&#21160;&#24577;&#30340;&#35821;&#35328;&#21644;&#19968;&#31181;&#20869;&#22312;&#30340;&#21160;&#26426;&#65292;&#21363;&#20197;&#23613;&#21487;&#33021;&#23569;&#30340;&#27807;&#36890;&#20195;&#20215;&#23398;&#20064;&#65292;CEIL &#21487;&#20197;&#23548;&#33268;&#23398;&#20064;&#32773;&#21644;&#25945;&#24072;&#36880;&#28176;&#39640;&#25928;&#22320;&#20132;&#27969;&#65292;&#36890;&#36807;&#20132;&#25442;&#36234;&#26469;&#36234;&#25277;&#35937;&#30340;&#24847;&#22270;&#12290;CEIL &#22312;&#19968;&#20010;&#21253;&#21547;&#38271;&#26399;&#20915;&#31574;&#20219;&#21153;&#30340; 2D MineCraft &#39046;&#22495;&#19978;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#21644;&#27807;&#36890;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Assistant AI agents should be capable of rapidly acquiring novel skills and adapting to new user preferences. Traditional frameworks like imitation learning and reinforcement learning do not facilitate this capability because they support only low-level, inefficient forms of communication. In contrast, humans communicate with progressive efficiency by defining and sharing abstract intentions. Reproducing similar capability in AI agents, we develop a novel learning framework named Communication-Efficient Interactive Learning (CEIL). By equipping a learning agent with an abstract, dynamic language and an intrinsic motivation to learn with minimal communication effort, CEIL leads to emergence of a human-like pattern where the learner and the teacher communicate progressively efficiently by exchanging increasingly more abstract intentions. CEIL demonstrates impressive performance and communication efficiency on a 2D MineCraft domain featuring long-horizon decision-making tasks. Agents trai
&lt;/p&gt;</description></item><item><title>ConFIRM&#26159;&#19968;&#31181;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#21644;&#35780;&#20272;&#21442;&#25968;&#24494;&#35843;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.13001</link><description>&lt;p&gt;
&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65288;ConFIRM&#65289;
&lt;/p&gt;
&lt;p&gt;
Conversational Financial Information Retrieval Model (ConFIRM). (arXiv:2310.13001v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13001
&lt;/p&gt;
&lt;p&gt;
ConFIRM&#26159;&#19968;&#31181;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#21644;&#35780;&#20272;&#21442;&#25968;&#24494;&#35843;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#21033;&#29992;&#23427;&#20204;&#22312;&#37329;&#34701;&#31561;&#19987;&#38376;&#39046;&#22495;&#30340;&#26032;&#20852;&#29305;&#24615;&#20855;&#26377;&#25506;&#32034;&#30340;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#37329;&#34701;&#31561;&#21463;&#30417;&#31649;&#39046;&#22495;&#20855;&#26377;&#29420;&#29305;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#38656;&#35201;&#20855;&#22791;&#38024;&#23545;&#35813;&#39046;&#22495;&#30340;&#20248;&#21270;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ConFIRM&#65292;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#29992;&#20110;&#26597;&#35810;&#24847;&#22270;&#20998;&#31867;&#21644;&#30693;&#35782;&#24211;&#26631;&#35760;&#12290;ConFIRM&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65306;1&#65289;&#19968;&#31181;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;2&#65289;&#35780;&#20272;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#26469;&#36827;&#34892;&#26597;&#35810;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;4000&#22810;&#20010;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#21333;&#29420;&#30340;&#27979;&#35797;&#38598;&#19978;&#35780;&#20272;&#20102;&#20934;&#30830;&#24615;&#12290;ConFIRM&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#23545;&#20110;&#31526;&#21512;&#30417;&#31649;&#35201;&#27714;&#33267;&#20851;&#37325;&#35201;&#12290;ConFIRM&#25552;&#20379;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#25552;&#21462;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#30340;&#31934;&#30830;&#26597;&#35810;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the exponential growth in large language models (LLMs), leveraging their emergent properties for specialized domains like finance merits exploration. However, regulated fields such as finance pose unique constraints, requiring domain-optimized frameworks. We present ConFIRM, an LLM-based conversational financial information retrieval model tailored for query intent classification and knowledge base labeling.  ConFIRM comprises two modules:  1) a method to synthesize finance domain-specific question-answer pairs, and  2) evaluation of parameter efficient fine-tuning approaches for the query classification task. We generate a dataset of over 4000 samples, assessing accuracy on a separate test set.  ConFIRM achieved over 90% accuracy, essential for regulatory compliance. ConFIRM provides a data-efficient solution to extract precise query intent for financial dialog systems.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#22270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#20013;&#24573;&#35270;&#20851;&#31995;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#26500;&#24314;&#20851;&#31995;&#22270;&#12289;&#21152;&#26435;&#22788;&#29702;&#21644;&#21033;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#25991;&#26723;&#20013;&#20851;&#31995;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#29616;&#26377;&#27169;&#22411;&#30340;&#27169;&#22359;&#38598;&#25104;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13000</link><description>&lt;p&gt;
&#20851;&#31995;&#30456;&#20851;&#24615;&#22686;&#24378;&#30340;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Document-Level Relation Extraction with Relation Correlation Enhancement. (arXiv:2310.13000v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13000
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#22270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#20013;&#24573;&#35270;&#20851;&#31995;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#26500;&#24314;&#20851;&#31995;&#22270;&#12289;&#21152;&#26435;&#22788;&#29702;&#21644;&#21033;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#25991;&#26723;&#20013;&#20851;&#31995;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#29616;&#26377;&#27169;&#22411;&#30340;&#27169;&#22359;&#38598;&#25104;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#26159;&#19968;&#39033;&#33268;&#21147;&#20110;&#35782;&#21035;&#25991;&#26723;&#20869;&#23454;&#20307;&#38388;&#20851;&#31995;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;&#24448;&#24448;&#24573;&#35270;&#20102;&#20851;&#31995;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#32570;&#20047;&#23545;&#20851;&#31995;&#30456;&#20851;&#24615;&#30340;&#23450;&#37327;&#20998;&#26512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#24182;&#26377;&#25928;&#25429;&#25417;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#20851;&#31995;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#22270;&#26041;&#27861;&#65292;&#26088;&#22312;&#26126;&#30830;&#21033;&#29992;&#20851;&#31995;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#19968;&#20010;&#20851;&#31995;&#22270;&#65292;&#21033;&#29992;&#20808;&#21069;&#30340;&#20851;&#31995;&#30693;&#35782;&#23548;&#20986;&#30340;&#32479;&#35745;&#20849;&#29616;&#20449;&#24687;&#26469;&#24314;&#27169;&#20851;&#31995;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#37319;&#29992;&#37325;&#26032;&#21152;&#26435;&#30340;&#26041;&#27861;&#21019;&#24314;&#19968;&#20010;&#26377;&#25928;&#30340;&#20851;&#31995;&#30456;&#20851;&#24615;&#30697;&#38453;&#65292;&#20197;&#24341;&#23548;&#20851;&#31995;&#20449;&#24687;&#30340;&#20256;&#25773;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#26469;&#32858;&#21512;&#20851;&#31995;&#23884;&#20837;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26080;&#32541;&#22320;&#20316;&#20026;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#27169;&#22359;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#27169;&#22411;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22686;&#24378;&#20851;&#31995;&#25277;&#21462;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-level relation extraction (DocRE) is a task that focuses on identifying relations between entities within a document. However, existing DocRE models often overlook the correlation between relations and lack a quantitative analysis of relation correlations. To address this limitation and effectively capture relation correlations in DocRE, we propose a relation graph method, which aims to explicitly exploit the interdependency among relations. Firstly, we construct a relation graph that models relation correlations using statistical co-occurrence information derived from prior relation knowledge. Secondly, we employ a re-weighting scheme to create an effective relation correlation matrix to guide the propagation of relation information. Furthermore, we leverage graph attention networks to aggregate relation embeddings. Importantly, our method can be seamlessly integrated as a plug-and-play module into existing models. Experimental results demonstrate that our approach can enhanc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36817;&#20284;&#21160;&#24577;&#35268;&#21010;&#21644;&#22312;&#32447;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20943;&#23569;&#32593;&#32476;&#21151;&#32791;&#30340;&#21516;&#26102;&#20445;&#25345;&#33391;&#22909;&#30340;&#26381;&#21153;&#36136;&#37327;&#12290;&#36890;&#36807;&#39044;&#27979;&#21151;&#32791;&#21644;QoS&#65292;&#24182;&#32467;&#21512;&#36866;&#24212;&#24615;QoS&#38408;&#20540;&#65292;&#23454;&#29616;&#20102;&#33410;&#33021;&#22522;&#31449;&#23567;&#21306;&#20999;&#25442;&#12290;</title><link>http://arxiv.org/abs/2310.12999</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#21160;&#24577;&#35268;&#21010;&#29992;&#20110;&#33410;&#33021;&#22522;&#31449;&#23567;&#21306;&#20999;&#25442;
&lt;/p&gt;
&lt;p&gt;
Adaptive Dynamic Programming for Energy-Efficient Base Station Cell Switching. (arXiv:2310.12999v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36817;&#20284;&#21160;&#24577;&#35268;&#21010;&#21644;&#22312;&#32447;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20943;&#23569;&#32593;&#32476;&#21151;&#32791;&#30340;&#21516;&#26102;&#20445;&#25345;&#33391;&#22909;&#30340;&#26381;&#21153;&#36136;&#37327;&#12290;&#36890;&#36807;&#39044;&#27979;&#21151;&#32791;&#21644;QoS&#65292;&#24182;&#32467;&#21512;&#36866;&#24212;&#24615;QoS&#38408;&#20540;&#65292;&#23454;&#29616;&#20102;&#33410;&#33021;&#22522;&#31449;&#23567;&#21306;&#20999;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#23545;&#19981;&#26029;&#36827;&#21270;&#30340;&#26032;&#19968;&#20195;&#34562;&#31389;&#32593;&#32476;&#30340;&#38656;&#27714;&#22686;&#21152;&#12289;&#29615;&#22659;&#21644;&#30417;&#31649;&#26041;&#38754;&#30340;&#20851;&#27880;&#20197;&#21450;&#30001;&#22320;&#32536;&#25919;&#27835;&#32039;&#24352;&#23616;&#21183;&#24341;&#21457;&#30340;&#28508;&#22312;&#33021;&#28304;&#21361;&#26426;&#65292;&#26080;&#32447;&#32593;&#32476;&#30340;&#33410;&#33021;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36817;&#20284;&#21160;&#24577;&#35268;&#21010;&#65288;ADP&#65289;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#22312;&#32447;&#20248;&#21270;&#65292;&#22312;&#32500;&#25345;&#36275;&#22815;&#30340;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#25351;&#26631;&#30340;&#21516;&#26102;&#65292;&#20999;&#25442;&#22522;&#31449;&#30340;&#23567;&#21306;&#20197;&#20943;&#23569;&#32593;&#32476;&#30340;&#21151;&#32791;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#26469;&#39044;&#27979;&#21151;&#32791;&#20197;&#36817;&#20284;ADP&#20013;&#30340;&#20540;&#20989;&#25968;&#65292;&#20197;&#36873;&#25321;&#33021;&#22815;&#33410;&#32422;&#26399;&#26395;&#21151;&#32791;&#26368;&#20248;&#30340;&#21160;&#20316;&#12290;&#20026;&#20102;&#22312;&#19981;&#25439;&#23475;QoS&#30340;&#24773;&#20917;&#19979;&#23613;&#21487;&#33021;&#33410;&#32422;&#21151;&#32791;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#21478;&#19968;&#20010;MLP&#26469;&#39044;&#27979;QoS&#65292;&#24182;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#26469;&#39044;&#27979;&#20999;&#25442;&#65292;&#23558;&#20854;&#32435;&#20837;&#22312;&#32447;&#20248;&#21270;&#31639;&#27861;&#65292;&#29983;&#25104;&#29992;&#20110;&#26681;&#25454;&#25972;&#20307;&#24773;&#20917;&#31579;&#36873;&#23567;&#21306;&#20999;&#25442;&#21160;&#20316;&#30340;&#33258;&#36866;&#24212;QoS&#38408;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy saving in wireless networks is growing in importance due to increasing demand for evolving new-gen cellular networks, environmental and regulatory concerns, and potential energy crises arising from geopolitical tensions. In this work, we propose an approximate dynamic programming (ADP)-based method coupled with online optimization to switch on/off the cells of base stations to reduce network power consumption while maintaining adequate Quality of Service (QoS) metrics. We use a multilayer perceptron (MLP) given each state-action pair to predict the power consumption to approximate the value function in ADP for selecting the action with optimal expected power saved. To save the largest possible power consumption without deteriorating QoS, we include another MLP to predict QoS and a long short-term memory (LSTM) for predicting handovers, incorporated into an online optimization algorithm producing an adaptive QoS threshold for filtering cell switching actions based on the overall 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#29615;&#35270;&#25668;&#20687;&#31995;&#32479;&#36827;&#34892;&#20572;&#36710;&#20301;&#20998;&#31867;&#30340;&#30740;&#31350;&#65292;&#20351;&#29992;&#20102;&#36866;&#29992;&#20110;&#21508;&#31181;&#24418;&#29366;&#20572;&#36710;&#20301;&#30340;&#22810;&#36793;&#24418;&#36793;&#30028;&#26694;&#27169;&#22411;&#65292;&#24182;&#26159;&#31532;&#19968;&#20010;&#23545;&#40060;&#30524;&#25668;&#20687;&#26426;&#19978;&#30340;&#20572;&#36710;&#20301;&#36827;&#34892;&#26816;&#27979;&#19982;&#20998;&#31867;&#30340;&#35814;&#32454;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.12997</link><description>&lt;p&gt;
&#22522;&#20110;&#29615;&#35270;&#25668;&#20687;&#31995;&#32479;&#30340;&#20572;&#36710;&#20301;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Parking Spot Classification based on surround view camera system. (arXiv:2310.12997v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#29615;&#35270;&#25668;&#20687;&#31995;&#32479;&#36827;&#34892;&#20572;&#36710;&#20301;&#20998;&#31867;&#30340;&#30740;&#31350;&#65292;&#20351;&#29992;&#20102;&#36866;&#29992;&#20110;&#21508;&#31181;&#24418;&#29366;&#20572;&#36710;&#20301;&#30340;&#22810;&#36793;&#24418;&#36793;&#30028;&#26694;&#27169;&#22411;&#65292;&#24182;&#26159;&#31532;&#19968;&#20010;&#23545;&#40060;&#30524;&#25668;&#20687;&#26426;&#19978;&#30340;&#20572;&#36710;&#20301;&#36827;&#34892;&#26816;&#27979;&#19982;&#20998;&#31867;&#30340;&#35814;&#32454;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29615;&#35270;&#40060;&#30524;&#25668;&#20687;&#26426;&#24120;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#36817;&#22330;&#24863;&#30693;&#65292;&#21253;&#25324;&#22478;&#24066;&#39550;&#39542;&#21644;&#27773;&#36710;&#20195;&#23458;&#27850;&#36710;&#12290;&#22235;&#20010;&#40060;&#30524;&#25668;&#20687;&#22836;&#20998;&#21035;&#25918;&#32622;&#22312;&#36710;&#36742;&#30340;&#22235;&#21608;&#65292;&#36275;&#20197;&#35206;&#30422;&#36710;&#36742;&#21608;&#22260;360&#24230;&#30340;&#36817;&#22330;&#21306;&#22495;&#12290;&#22522;&#20110;&#29615;&#35270;&#25668;&#20687;&#22836;&#65292;&#36817;&#24180;&#26469;&#24050;&#32463;&#26377;&#24456;&#22810;&#20851;&#20110;&#20572;&#36710;&#20301;&#26816;&#27979;&#30340;&#30740;&#31350;&#65292;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#36710;&#20301;&#30340;&#21344;&#29992;&#29366;&#24577;&#65292;&#20294;&#23545;&#20110;&#31354;&#38386;&#36710;&#20301;&#26159;&#21542;&#19982;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#20219;&#21153;&#20860;&#23481;&#30340;&#30740;&#31350;&#36739;&#23569;&#65292;&#20363;&#22914;&#65292;&#19968;&#20123;&#20572;&#36710;&#20301;&#20165;&#20379;&#27531;&#30142;&#20154;&#25110;&#30005;&#21160;&#36710;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#29615;&#35270;&#25668;&#20687;&#31995;&#32479;&#36827;&#34892;&#20102;&#20572;&#36710;&#20301;&#20998;&#31867;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#30446;&#26631;&#26816;&#27979;&#31070;&#32463;&#32593;&#32476;YOLOv4&#65292;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21508;&#31181;&#24418;&#29366;&#20572;&#36710;&#20301;&#65288;&#22914;&#26012;&#20572;&#36710;&#20301;&#65289;&#30340;&#26032;&#22411;&#22810;&#36793;&#24418;&#36793;&#30028;&#26694;&#27169;&#22411;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#22312;&#40060;&#30524;&#25668;&#20687;&#26426;&#19978;&#23545;&#20572;&#36710;&#20301;&#36827;&#34892;&#26816;&#27979;&#19982;&#20998;&#31867;&#30340;&#35814;&#32454;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Surround-view fisheye cameras are commonly used for near-field sensing in automated driving scenarios, including urban driving and auto valet parking. Four fisheye cameras, one on each side, are sufficient to cover 360{\deg} around the vehicle capturing the entire near-field region. Based on surround view cameras, there has been much research on parking slot detection with main focus on the occupancy status in recent years, but little work on whether the free slot is compatible with the mission of the ego vehicle or not. For instance, some spots are handicap or electric vehicles accessible only. In this paper, we tackle parking spot classification based on the surround view camera system. We adapt the object detection neural network YOLOv4 with a novel polygon bounding box model that is well-suited for various shaped parking spaces, such as slanted parking slots. To the best of our knowledge, we present the first detailed study on parking spot detection and classification on fisheye ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#29992;&#20110;&#20020;&#24202;&#33647;&#29289;&#31579;&#36873;&#20013;&#30340;&#33647;&#29289;&#21453;&#24212;&#39044;&#27979;&#12290;&#36890;&#36807;&#23398;&#20064;&#30456;&#20284;&#33647;&#29289;&#30340;&#20808;&#21069;&#21453;&#24212;&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22686;&#24378;&#23545;&#26410;&#26631;&#35760;&#21270;&#21512;&#29289;&#30340;&#23454;&#26102;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.12996</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#22312;&#20020;&#24202;&#33647;&#29289;&#31579;&#36873;&#20013;&#30340;&#33647;&#29289;&#21453;&#24212;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Learning of Drug Response Prediction for Preclinical Drug Screening. (arXiv:2310.12996v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#29992;&#20110;&#20020;&#24202;&#33647;&#29289;&#31579;&#36873;&#20013;&#30340;&#33647;&#29289;&#21453;&#24212;&#39044;&#27979;&#12290;&#36890;&#36807;&#23398;&#20064;&#30456;&#20284;&#33647;&#29289;&#30340;&#20808;&#21069;&#21453;&#24212;&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22686;&#24378;&#23545;&#26410;&#26631;&#35760;&#21270;&#21512;&#29289;&#30340;&#23454;&#26102;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#26377;&#30417;&#30563;&#23398;&#20064;&#26469;&#36827;&#34892;&#33647;&#29289;&#21453;&#24212;&#30340;&#39044;&#27979;&#65292;&#36825;&#38656;&#35201;&#20381;&#36182;&#20110;&#24050;&#26631;&#35760;&#30340;&#33647;&#29289;&#21453;&#24212;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22312;&#20020;&#24202;&#33647;&#29289;&#31579;&#36873;&#38454;&#27573;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#38656;&#35201;&#33647;&#29289;&#21453;&#24212;&#27169;&#22411;&#26469;&#39044;&#27979;&#26032;&#21270;&#21512;&#29289;&#30340;&#21453;&#24212;&#65292;&#36825;&#20123;&#21270;&#21512;&#29289;&#30340;&#33647;&#29289;&#21453;&#24212;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#65292;&#36825;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#20351;&#24471;&#20256;&#32479;&#30340;&#26377;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#19981;&#36866;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20020;&#24202;&#33647;&#29289;&#31579;&#36873;&#20013;&#29992;&#20110;&#33647;&#29289;&#21453;&#24212;&#39044;&#27979;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20998;&#25903;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#27979;&#35797;&#22686;&#24378;&#25554;&#20214;(MSDA)&#65292;MSDA&#21487;&#20197;&#19982;&#20256;&#32479;&#30340;&#33647;&#29289;&#21453;&#24212;&#39044;&#27979;&#26041;&#27861;&#26080;&#32541;&#38598;&#25104;&#65292;&#20174;&#30456;&#20284;&#33647;&#29289;&#30340;&#20808;&#21069;&#21453;&#24212;&#25968;&#25454;&#20013;&#23398;&#20064;&#19981;&#21464;&#29305;&#24449;&#65292;&#20197;&#22686;&#24378;&#23545;&#26410;&#26631;&#35760;&#21270;&#21512;&#29289;&#30340;&#23454;&#26102;&#39044;&#27979;&#12290;&#25105;&#20204;&#20351;&#29992;GDSCv2&#21644;CellMiner&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;MSDA&#33021;&#22815;&#39640;&#25928;&#22320;&#39044;&#27979;&#33647;&#29289;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional deep learning methods typically employ supervised learning for drug response prediction (DRP). This entails dependence on labeled response data from drugs for model training. However, practical applications in the preclinical drug screening phase demand that DRP models predict responses for novel compounds, often with unknown drug responses. This presents a challenge, rendering supervised deep learning methods unsuitable for such scenarios. In this paper, we propose a zero-shot learning solution for the DRP task in preclinical drug screening. Specifically, we propose a Multi-branch Multi-Source Domain Adaptation Test Enhancement Plug-in, called MSDA. MSDA can be seamlessly integrated with conventional DRP methods, learning invariant features from the prior response data of similar drugs to enhance real-time predictions of unlabeled compounds. We conducted experiments using the GDSCv2 and CellMiner datasets. The results demonstrate that MSDA efficiently predicts drug respon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#65292;&#23558;YOLOv8&#27169;&#22411;&#19982;SAM&#21644;HQ-SAM&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#22312;&#22810;&#31181;&#21307;&#23398;&#24433;&#20687;&#25968;&#25454;&#38598;&#20013;&#23454;&#29616;&#20102;&#31934;&#30830;&#30340;&#24863;&#20852;&#36259;&#21306;&#22495;&#20998;&#21106;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.12995</link><description>&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#30340;&#32508;&#21512;&#22810;&#27169;&#24577;&#20998;&#21106;&#65306;&#23558;YOLOv8&#19982;SAM&#21644;HQ-SAM&#27169;&#22411;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
Comprehensive Multimodal Segmentation in Medical Imaging: Combining YOLOv8 with SAM and HQ-SAM Models. (arXiv:2310.12995v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#65292;&#23558;YOLOv8&#27169;&#22411;&#19982;SAM&#21644;HQ-SAM&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#22312;&#22810;&#31181;&#21307;&#23398;&#24433;&#20687;&#25968;&#25454;&#38598;&#20013;&#23454;&#29616;&#20102;&#31934;&#30830;&#30340;&#24863;&#20852;&#36259;&#21306;&#22495;&#20998;&#21106;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#31181;&#21307;&#23398;&#24433;&#20687;&#25968;&#25454;&#38598;&#20013;&#20998;&#21106;&#24863;&#20852;&#36259;&#21306;&#22495;&#65288;ROI&#65289;&#65292;&#21253;&#25324;&#36229;&#22768;&#12289;CT&#25195;&#25551;&#21644;X&#23556;&#32447;&#22270;&#20687;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;YOLOv8&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#22312;&#19981;&#21516;&#27169;&#24577;&#19979;&#36827;&#34892;&#36817;&#20284;&#36793;&#30028;&#26694;&#26816;&#27979;&#65292;&#21516;&#26102;&#32467;&#21512;Segment Anything Model (SAM) &#21644; High Quality (HQ) SAM&#36827;&#34892;&#23436;&#20840;&#33258;&#21160;&#21644;&#31934;&#30830;&#30340;&#20998;&#21106;&#12290;&#20026;&#20102;&#29983;&#25104;&#36793;&#30028;&#26694;&#65292;&#20351;&#29992;&#20102;&#27599;&#31181;&#27169;&#24577;&#30340;100&#20010;&#22270;&#20687;&#21644;&#25513;&#27169;&#30340;&#26377;&#38480;&#38598;&#21512;&#23545;YOLOv8&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#35745;&#31639;&#21644;&#20998;&#26512;&#65292;&#24471;&#20986;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#28508;&#21147;&#12290;&#20351;&#29992;&#20102;&#22810;&#31181;&#35780;&#20272;&#25351;&#26631;&#65292;&#21253;&#25324;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#12289;F1&#20998;&#25968;&#21644;Dice&#20998;&#25968;&#65292;&#26469;&#37327;&#21270;&#20998;&#21106;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12290;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;YOLOv8&#12289;YOLOv8+SAM&#21644;YOLOv8+HQ-SAM&#27169;&#22411;&#30340;&#21508;&#33258;&#21644;&#32452;&#21512;&#34920;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#20998;&#21106;&#25928;&#26524;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a comprehensive approach for segmenting regions of interest (ROI) in diverse medical imaging datasets, encompassing ultrasound, CT scans, and X-ray images. The proposed method harnesses the capabilities of the YOLOv8 model for approximate boundary box detection across modalities, alongside the Segment Anything Model (SAM) and High Quality (HQ) SAM for fully automatic and precise segmentation. To generate boundary boxes, the YOLOv8 model was trained using a limited set of 100 images and masks from each modality. The results obtained from our approach are extensively computed and analyzed, demonstrating its effectiveness and potential in medical image analysis. Various evaluation metrics, including precision, recall, F1 score, and Dice Score, were employed to quantify the accuracy of the segmentation results. A comparative analysis was conducted to assess the individual and combined performance of the YOLOv8, YOLOv8+SAM, and YOLOv8+HQ-SAM models. The results indicat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#35748;&#30693;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;&#20998;&#27495;&#30340;&#19981;&#21516;&#32500;&#24230;&#65292;&#25351;&#20986;&#20998;&#27495;&#26082;&#21487;&#20197;&#26469;&#28304;&#20110;&#23545;&#35937;&#35780;&#20272;&#30340;&#24046;&#24322;&#65292;&#20063;&#21487;&#20197;&#28304;&#20110;&#23545;&#23545;&#35937;&#34920;&#31034;&#26041;&#24335;&#30340;&#19981;&#19968;&#33268;&#12290;&#30740;&#31350;&#32773;&#21457;&#23637;&#20102;&#29992;&#20110;&#37327;&#21270;&#34920;&#31034;&#37325;&#21472;&#31243;&#24230;&#30340;&#24037;&#20855;&#65292;&#28145;&#20837;&#29702;&#35299;&#20998;&#27495;&#21644;&#19981;&#19968;&#33268;&#24615;&#22914;&#20309;&#30456;&#20114;&#20316;&#29992;&#23545;&#20110;&#20419;&#36827;&#19981;&#21516;&#31867;&#22411;&#20195;&#29702;&#20043;&#38388;&#30340;&#26377;&#25928;&#21327;&#20316;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2310.12994</link><description>&lt;p&gt;
&#19981;&#21516;&#32500;&#24230;&#30340;&#20998;&#27495;&#65306;&#25581;&#31034;&#35748;&#30693;&#31185;&#23398;&#19982;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#24046;&#24322;&#21644;&#19981;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Dimensions of Disagreement: Unpacking Divergence and Misalignment in Cognitive Science and Artificial Intelligence. (arXiv:2310.12994v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#35748;&#30693;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;&#20998;&#27495;&#30340;&#19981;&#21516;&#32500;&#24230;&#65292;&#25351;&#20986;&#20998;&#27495;&#26082;&#21487;&#20197;&#26469;&#28304;&#20110;&#23545;&#35937;&#35780;&#20272;&#30340;&#24046;&#24322;&#65292;&#20063;&#21487;&#20197;&#28304;&#20110;&#23545;&#23545;&#35937;&#34920;&#31034;&#26041;&#24335;&#30340;&#19981;&#19968;&#33268;&#12290;&#30740;&#31350;&#32773;&#21457;&#23637;&#20102;&#29992;&#20110;&#37327;&#21270;&#34920;&#31034;&#37325;&#21472;&#31243;&#24230;&#30340;&#24037;&#20855;&#65292;&#28145;&#20837;&#29702;&#35299;&#20998;&#27495;&#21644;&#19981;&#19968;&#33268;&#24615;&#22914;&#20309;&#30456;&#20114;&#20316;&#29992;&#23545;&#20110;&#20419;&#36827;&#19981;&#21516;&#31867;&#22411;&#20195;&#29702;&#20043;&#38388;&#30340;&#26377;&#25928;&#21327;&#20316;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#26222;&#21450;&#22686;&#21152;&#20102;&#31649;&#29702;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#20043;&#38388;&#20197;&#21450;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#20043;&#38388;&#30340;&#20998;&#27495;&#30340;&#38656;&#27714;&#12290;&#30740;&#31350;&#25581;&#31034;&#20102;&#20998;&#27495;&#30340;&#26412;&#36136;&#65306;&#36807;&#21435;&#30340;&#24515;&#29702;&#23398;&#30740;&#31350;&#24120;&#24120;&#23558;&#20998;&#27495;&#35299;&#37322;&#20026;&#20004;&#20010;&#20195;&#29702;&#23545;&#21516;&#19968;&#23545;&#35937;&#24418;&#25104;&#19981;&#21516;&#30340;&#35780;&#20272;&#65292;&#20294;&#20998;&#27495;&#20063;&#21487;&#20197;&#28304;&#20110;&#20195;&#29702;&#23545;&#23545;&#35937;&#30340;&#34920;&#31034;&#26041;&#24335;&#30340;&#24046;&#24322;&#12290;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30740;&#31350;&#20154;&#21592;&#20851;&#27880;&#20154;&#26426;&#23545;&#40784;&#21644;&#35745;&#31639;&#35748;&#30693;&#31185;&#23398;&#30340;&#26368;&#26032;&#30740;&#31350;&#24050;&#30528;&#37325;&#32771;&#34385;&#21518;&#19968;&#31181;&#20998;&#27495;&#65292;&#24182;&#24320;&#21457;&#20986;&#21487;&#20197;&#29992;&#26469;&#37327;&#21270;&#20195;&#29702;&#20043;&#38388;&#34920;&#31034;&#37325;&#21472;&#31243;&#24230;&#30340;&#24037;&#20855;&#12290;&#20102;&#35299;&#20998;&#27495;&#21644;&#19981;&#19968;&#33268;&#24615;&#22914;&#20309;&#30456;&#20114;&#20316;&#29992;&#20135;&#29983;&#20998;&#27495;&#20197;&#21450;&#35299;&#20915;&#31574;&#30053;&#22914;&#20309;&#20381;&#36182;&#20110;&#36825;&#31181;&#30456;&#20114;&#20316;&#29992;&#26159;&#20419;&#36827;&#19981;&#21516;&#31867;&#22411;&#20195;&#29702;&#20043;&#38388;&#26377;&#25928;&#21327;&#20316;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing prevalence of artificial agents creates a correspondingly increasing need to manage disagreements between humans and artificial agents, as well as between artificial agents themselves. Considering this larger space of possible agents exposes an opportunity for furthering our understanding of the nature of disagreement: past studies in psychology have often cast disagreement as two agents forming diverging evaluations of the same object, but disagreement can also arise from differences in how agents represent that object. AI research on human-machine alignment and recent work in computational cognitive science have focused on this latter kind of disagreement, and have developed tools that can be used to quantify the extent of representational overlap between agents. Understanding how divergence and misalignment interact to produce disagreement, and how resolution strategies depend on this interaction, is key to promoting effective collaboration between diverse types of ag
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22686;&#24378;&#21307;&#30103;&#25968;&#25454;&#20114;&#25805;&#20316;&#24615;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LLM&#19981;&#20165;&#31616;&#21270;&#20102;&#22810;&#27493;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20154;&#24037;&#26657;&#20934;&#36807;&#31243;&#65292;&#32780;&#19988;&#22312;&#19982;&#20154;&#24037;&#26631;&#27880;&#36827;&#34892;&#27604;&#36739;&#26102;&#65292;&#31934;&#30830;&#21305;&#37197;&#29575;&#36229;&#36807;90%&#12290;</title><link>http://arxiv.org/abs/2310.12989</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#20581;&#24247;&#25968;&#25454;&#20114;&#25805;&#20316;&#24615;&#65306;&#19968;&#39033;FHIR&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enhancing Health Data Interoperability with Large Language Models: A FHIR Study. (arXiv:2310.12989v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22686;&#24378;&#21307;&#30103;&#25968;&#25454;&#20114;&#25805;&#20316;&#24615;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LLM&#19981;&#20165;&#31616;&#21270;&#20102;&#22810;&#27493;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20154;&#24037;&#26657;&#20934;&#36807;&#31243;&#65292;&#32780;&#19988;&#22312;&#19982;&#20154;&#24037;&#26631;&#27880;&#36827;&#34892;&#27604;&#36739;&#26102;&#65292;&#31934;&#30830;&#21305;&#37197;&#29575;&#36229;&#36807;90%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22686;&#24378;&#21307;&#30103;&#25968;&#25454;&#20114;&#25805;&#20316;&#24615;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21033;&#29992;LLM&#23558;&#20020;&#24202;&#25991;&#26412;&#36716;&#25442;&#20026;&#23545;&#24212;&#30340;FHIR&#36164;&#28304;&#12290;&#25105;&#20204;&#22312;3671&#20010;&#20020;&#24202;&#25991;&#26412;&#29255;&#27573;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;LLM&#19981;&#20165;&#31616;&#21270;&#20102;&#22810;&#27493;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20154;&#24037;&#26657;&#20934;&#36807;&#31243;&#65292;&#32780;&#19988;&#22312;&#19982;&#20154;&#24037;&#26631;&#27880;&#36827;&#34892;&#27604;&#36739;&#26102;&#65292;&#20934;&#30830;&#21305;&#37197;&#29575;&#36229;&#36807;90%&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we investigated the ability of the large language model (LLM) to enhance healthcare data interoperability. We leveraged the LLM to convert clinical texts into their corresponding FHIR resources. Our experiments, conducted on 3,671 snippets of clinical text, demonstrated that the LLM not only streamlines the multi-step natural language processing and human calibration processes but also achieves an exceptional accuracy rate of over 90% in exact matches when compared to human annotations.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#30740;&#28041;&#21450;&#21040;&#27969;&#24418;&#23398;&#20064;&#21450;&#20854;&#22312;&#22810;&#23186;&#20307;&#39046;&#22495;&#30340;&#37325;&#35201;&#24212;&#29992;&#65292;&#20026;&#20102;&#35299;&#20171;&#32461;&#20102;&#27969;&#24418;&#23398;&#20064;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.12986</link><description>&lt;p&gt;
&#27969;&#24418;&#23398;&#20064;&#21450;&#20854;&#22312;&#22810;&#23186;&#20307;&#19978;&#30340;&#24212;&#29992;&#30340;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
A survey of manifold learning and its applications for multimedia. (arXiv:2310.12986v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12986
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#30740;&#28041;&#21450;&#21040;&#27969;&#24418;&#23398;&#20064;&#21450;&#20854;&#22312;&#22810;&#23186;&#20307;&#39046;&#22495;&#30340;&#37325;&#35201;&#24212;&#29992;&#65292;&#20026;&#20102;&#35299;&#20171;&#32461;&#20102;&#27969;&#24418;&#23398;&#20064;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24418;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#20852;&#30740;&#31350;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#27969;&#24418;&#23398;&#20064;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#22312;&#22810;&#23186;&#20307;&#37325;&#35201;&#24212;&#29992;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manifold learning is an emerging research domain of machine learning. In this work, we give an introduction into manifold learning and how it is employed for important application fields in multimedia.
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24403;&#21069;&#22343;&#20540;&#35299;&#30721;&#65288;CMD&#65289;&#26041;&#27861;&#30340;SNN-YOLOv3&#27169;&#22411;&#65292;&#29992;&#20110;&#23454;&#29616;&#33021;&#28304;&#39640;&#25928;&#30340;&#30446;&#26631;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;PASCAL VOC&#25968;&#25454;&#38598;&#19978;&#65292;SNN-YOLOv3&#20855;&#26377;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#65292;&#21482;&#38656;6&#20010;&#26102;&#38388;&#27493;&#65292;mAP&#36798;&#21040;&#20102;61.87%&#65292;&#27604;SpikingYOLO&#25552;&#39640;&#20102;&#36817;10%&#30340;mAP&#65292;&#21516;&#26102;&#33021;&#22815;&#38477;&#20302;&#33021;&#37327;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2310.12985</link><description>&lt;p&gt;
&#36890;&#36807;&#26367;&#20195;&#26799;&#24230;&#19979;&#38477;&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#33021;&#28304;&#39640;&#25928;&#30340;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Enabling energy-Efficient object detection with surrogate gradient descent in spiking neural networks. (arXiv:2310.12985v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12985
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24403;&#21069;&#22343;&#20540;&#35299;&#30721;&#65288;CMD&#65289;&#26041;&#27861;&#30340;SNN-YOLOv3&#27169;&#22411;&#65292;&#29992;&#20110;&#23454;&#29616;&#33021;&#28304;&#39640;&#25928;&#30340;&#30446;&#26631;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;PASCAL VOC&#25968;&#25454;&#38598;&#19978;&#65292;SNN-YOLOv3&#20855;&#26377;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#65292;&#21482;&#38656;6&#20010;&#26102;&#38388;&#27493;&#65292;mAP&#36798;&#21040;&#20102;61.87%&#65292;&#27604;SpikingYOLO&#25552;&#39640;&#20102;&#36817;10%&#30340;mAP&#65292;&#21516;&#26102;&#33021;&#22815;&#38477;&#20302;&#33021;&#37327;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#26159;&#19968;&#31181;&#29983;&#29289;&#21487;&#34892;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20855;&#26377;&#20107;&#20214;&#39537;&#21160;&#22788;&#29702;&#21644;&#26102;&#31354;&#20449;&#24687;&#22788;&#29702;&#30340;&#26174;&#33879;&#20248;&#21183;&#65292;&#20351;&#24471;SNNs&#25104;&#20026;&#33021;&#28304;&#39640;&#25928;&#30340;&#30446;&#26631;&#26816;&#27979;&#30340;&#21560;&#24341;&#20154;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#29983;&#29289;&#31070;&#32463;&#20803;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#38750;&#21487;&#24494;&#24615;&#22312;SNNs&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#32570;&#20047;&#36866;&#21512;SNNs&#20013;&#30446;&#26631;&#26816;&#27979;&#30340;&#35299;&#30721;&#31574;&#30053;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24403;&#21069;&#22343;&#20540;&#35299;&#30721;&#65288;CMD&#65289;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22238;&#24402;&#38382;&#39064;&#65292;&#20197;&#20419;&#36827;&#23545;&#35937;&#26816;&#27979;&#20219;&#21153;&#20013;&#28145;&#23618;SNNs&#30340;&#35757;&#32451;&#12290;&#22522;&#20110;&#26799;&#24230;&#26367;&#20195;&#21644;CMD&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SNN-YOLOv3&#27169;&#22411;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SNN-YOLOv3&#22312;PASCAL VOC&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#65292;mAP&#36798;&#21040;&#20102;61.87%&#65292;&#20165;&#38656;6&#20010;&#26102;&#38388;&#27493;&#12290;&#19982;SpikingYOLO&#30456;&#27604;&#65292;&#25105;&#20204;&#25104;&#21151;&#23558;mAP&#25552;&#39640;&#20102;&#36817;10%&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#33021;&#37327;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs) are a biologically plausible neural network model with significant advantages in both event-driven processing and spatio-temporal information processing, rendering SNNs an appealing choice for energyefficient object detection. However, the non-differentiability of the biological neuronal dynamics model presents a challenge during the training of SNNs. Furthermore, a suitable decoding strategy for object detection in SNNs is currently lacking. In this study, we introduce the Current Mean Decoding (CMD) method, which solves the regression problem to facilitate the training of deep SNNs for object detection tasks. Based on the gradient surrogate and CMD, we propose the SNN-YOLOv3 model for object detection. Our experiments demonstrate that SNN-YOLOv3 achieves a remarkable performance with an mAP of 61.87% on the PASCAL VOC dataset, requiring only 6 time steps. Compared to SpikingYOLO, we have managed to increase mAP by nearly 10% while reducing energy consum
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32447;&#24615;&#21306;&#22495;&#35757;&#32451;&#21160;&#21147;&#23398;&#12290;&#36890;&#36807;&#23545;&#25968;&#25454;&#28857;&#21608;&#22260;&#32447;&#24615;&#21306;&#22495;&#30340;&#23616;&#37096;&#22797;&#26434;&#24615;&#36827;&#34892;&#32479;&#35745;&#20998;&#26512;&#65292;&#21457;&#29616;&#35757;&#32451;&#36807;&#31243;&#20013;&#36825;&#31181;&#22797;&#26434;&#24615;&#32463;&#21382;&#20102;&#20960;&#20010;&#38454;&#27573;&#30340;&#21464;&#21270;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#31934;&#30830;&#30340;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#35266;&#23519;&#21040;&#35757;&#32451;&#30340;&#26368;&#21518;&#38454;&#27573;&#26377;&#19968;&#20010;&#19979;&#38477;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.12977</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32447;&#24615;&#21306;&#22495;&#30340;&#35757;&#32451;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Training Dynamics of Deep Network Linear Regions. (arXiv:2310.12977v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12977
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32447;&#24615;&#21306;&#22495;&#35757;&#32451;&#21160;&#21147;&#23398;&#12290;&#36890;&#36807;&#23545;&#25968;&#25454;&#28857;&#21608;&#22260;&#32447;&#24615;&#21306;&#22495;&#30340;&#23616;&#37096;&#22797;&#26434;&#24615;&#36827;&#34892;&#32479;&#35745;&#20998;&#26512;&#65292;&#21457;&#29616;&#35757;&#32451;&#36807;&#31243;&#20013;&#36825;&#31181;&#22797;&#26434;&#24615;&#32463;&#21382;&#20102;&#20960;&#20010;&#38454;&#27573;&#30340;&#21464;&#21270;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#31934;&#30830;&#30340;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#35266;&#23519;&#21040;&#35757;&#32451;&#30340;&#26368;&#21518;&#38454;&#27573;&#26377;&#19968;&#20010;&#19979;&#38477;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DN&#65289;&#30340;&#35757;&#32451;&#21160;&#21147;&#23398;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25439;&#22833;&#20989;&#25968;&#30340;&#28436;&#21464;&#19978;&#65292;&#36825;&#20123;&#28436;&#21464;&#26159;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#28857;&#38468;&#36817;&#36827;&#34892;&#35780;&#20272;&#30340;&#12290;&#20107;&#23454;&#19978;&#65292;&#35768;&#22810;DN&#29616;&#35937;&#26368;&#21021;&#26159;&#22312;&#25991;&#29486;&#20013;&#20197;&#27492;&#20026;&#22522;&#30784;&#24341;&#20837;&#30340;&#65292;&#20363;&#22914;&#65292;&#21452;&#37325;&#19979;&#38477;&#12289;&#29702;&#35299;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#30001;&#36830;&#32493;&#20998;&#27573;&#20223;&#23556;DN&#65288;&#20363;&#22914;&#20855;&#26377;&#65288;&#27844;&#28431;&#30340;&#65289;ReLU&#38750;&#32447;&#24615;&#30340;&#32593;&#32476;&#65289;&#24418;&#25104;&#30340;&#36755;&#20837;&#31354;&#38388;&#21010;&#20998;&#25110;&#32447;&#24615;&#21306;&#22495;&#30340;&#35757;&#32451;&#21160;&#21147;&#23398;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#35745;&#37327;&#65292;&#35813;&#32479;&#35745;&#37327;&#26681;&#25454;&#25968;&#25454;&#28857;&#21608;&#22260;&#20219;&#24847;&#32500;&#24230;&#37051;&#22495;&#20013;&#30340;&#32447;&#24615;&#21306;&#22495;&#30340;&#38598;&#20013;&#31243;&#24230;&#26469;&#22218;&#25324;DN&#30340;&#23616;&#37096;&#22797;&#26434;&#24615;&#65288;LC&#65289;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25968;&#25454;&#28857;&#21608;&#22260;&#30340;LC&#32463;&#21382;&#20102;&#35768;&#22810;&#38454;&#27573;&#65292;&#20174;&#21021;&#22987;&#21270;&#21518;&#30340;&#19979;&#38477;&#36235;&#21183;&#24320;&#22987;&#65292;&#28982;&#21518;&#19978;&#21319;&#65292;&#24182;&#26368;&#32456;&#20197;&#19979;&#38477;&#36235;&#21183;&#32467;&#26463;&#12290;&#36890;&#36807;&#20351;&#29992;&#31934;&#30830;&#30340;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#35757;&#32451;&#30340;&#26368;&#21518;LC&#19979;&#38477;&#38454;&#27573;&#65292;li
&lt;/p&gt;
&lt;p&gt;
The study of Deep Network (DN) training dynamics has largely focused on the evolution of the loss function, evaluated on or around train and test set data points. In fact, many DN phenomenon were first introduced in literature with that respect, e.g., double descent, grokking. In this study, we look at the training dynamics of the input space partition or linear regions formed by continuous piecewise affine DNs, e.g., networks with (leaky)ReLU nonlinearities. First, we present a novel statistic that encompasses the local complexity (LC) of the DN based on the concentration of linear regions inside arbitrary dimensional neighborhoods around data points. We observe that during training, the LC around data points undergoes a number of phases, starting with a decreasing trend after initialization, followed by an ascent and ending with a final descending trend. Using exact visualization methods, we come across the perplexing observation that during the final LC descent phase of training, li
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27668;&#20307;&#37329;&#23646;&#30005;&#24359;&#28938;&#25509;&#39044;&#27979;&#36136;&#37327;&#31995;&#32479;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#31649;&#29702;&#22810;&#20256;&#24863;&#22120;&#25968;&#25454;&#20197;&#21450;&#23454;&#26102;&#22788;&#29702;&#21644;&#29305;&#24449;&#24037;&#31243;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#35782;&#21035;&#20851;&#31995;&#24182;&#39044;&#27979;&#28938;&#25509;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.12632</link><description>&lt;p&gt;
&#38754;&#21521;&#28938;&#25509;&#36807;&#31243;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22312;&#32447;&#36136;&#37327;&#39044;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Towards a Deep Learning-based Online Quality Prediction System for Welding Processes. (arXiv:2310.12632v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12632
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27668;&#20307;&#37329;&#23646;&#30005;&#24359;&#28938;&#25509;&#39044;&#27979;&#36136;&#37327;&#31995;&#32479;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#31649;&#29702;&#22810;&#20256;&#24863;&#22120;&#25968;&#25454;&#20197;&#21450;&#23454;&#26102;&#22788;&#29702;&#21644;&#29305;&#24449;&#24037;&#31243;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#35782;&#21035;&#20851;&#31995;&#24182;&#39044;&#27979;&#28938;&#25509;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21046;&#36896;&#36807;&#31243;&#30340;&#25968;&#23383;&#21270;&#20026;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#36136;&#37327;&#20445;&#35777;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#24212;&#29992;&#12290;&#19968;&#20010;&#24191;&#27867;&#24212;&#29992;&#30340;&#21046;&#36896;&#36807;&#31243;&#65292;&#21487;&#20197;&#20174;&#25968;&#25454;&#39537;&#21160;&#30340;&#35299;&#20915;&#26041;&#26696;&#20013;&#21463;&#30410;&#21290;&#27973;&#65292;&#26159;&#27668;&#20307;&#37329;&#23646;&#30005;&#24359;&#28938;&#25509;&#65288;GMAW&#65289;&#12290;&#28938;&#25509;&#36807;&#31243;&#20197;&#26448;&#26009;&#24615;&#36136;&#12289;&#24037;&#33402;&#26465;&#20214;&#21644;&#28938;&#25509;&#36136;&#37327;&#20043;&#38388;&#22797;&#26434;&#30340;&#22240;&#26524;&#20851;&#31995;&#20026;&#29305;&#24449;&#12290;&#22312;&#39057;&#32321;&#26356;&#25913;&#24037;&#33402;&#21442;&#25968;&#30340;&#38750;&#23454;&#39564;&#23460;&#29615;&#22659;&#20013;&#65292;&#36890;&#36807;&#30772;&#22351;&#24615;&#27979;&#35797;&#20934;&#30830;&#30830;&#23450;&#28938;&#32541;&#36136;&#37327;&#26159;&#32463;&#27982;&#19978;&#19981;&#21487;&#34892;&#30340;&#12290;&#28145;&#24230;&#23398;&#20064;&#25552;&#20379;&#20102;&#20174;&#24037;&#33402;&#35266;&#23519;&#20013;&#35782;&#21035;&#20851;&#31995;&#24182;&#39044;&#27979;&#28938;&#25509;&#36136;&#37327;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27668;&#20307;&#37329;&#23646;&#30005;&#24359;&#28938;&#25509;&#39044;&#27979;&#36136;&#37327;&#31995;&#32479;&#30340;&#27010;&#24565;&#12290;&#26680;&#24515;&#27010;&#24565;&#21253;&#25324;&#30001;&#22235;&#20010;&#20027;&#35201;&#38454;&#27573;&#32452;&#25104;&#30340;&#31649;&#32447;&#65306;&#22810;&#20256;&#24863;&#22120;&#25968;&#25454;&#65288;&#22914;&#30005;&#27969;&#21644;&#30005;&#21387;&#65289;&#30340;&#25910;&#38598;&#21644;&#31649;&#29702;&#12289;&#26102;&#38388;&#24207;&#21015;&#30340;&#23454;&#26102;&#22788;&#29702;&#21644;&#29305;&#24449;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
The digitization of manufacturing processes enables promising applications for machine learning-assisted quality assurance. A widely used manufacturing process that can strongly benefit from data-driven solutions is \ac{GMAW}. The welding process is characterized by complex cause-effect relationships between material properties, process conditions and weld quality. In non-laboratory environments with frequently changing process parameters, accurate determination of weld quality by destructive testing is economically unfeasible. Deep learning offers the potential to identify the relationships in available process data and predict the weld quality from process observations. In this paper, we present a concept for a deep learning based predictive quality system in \ac{GMAW}. At its core, the concept involves a pipeline consisting of four major phases: collection and management of multi-sensor data (e.g. current and voltage), real-time processing and feature engineering of the time series 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#35821;&#38899;&#21644;&#25163;&#21183;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#20934;&#30830;&#35782;&#21035;&#29305;&#23450;&#22833;&#35821;&#31867;&#22411;&#30340;&#26816;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65288;F1 84.2%&#65289;&#12290;</title><link>http://arxiv.org/abs/2310.11710</link><description>&lt;p&gt;
&#23398;&#20064;&#21516;&#26102;&#35821;&#35328;&#25163;&#21183;&#29992;&#20110;&#22810;&#27169;&#24577;&#22833;&#35821;&#31867;&#22411;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Co-Speech Gesture for Multimodal Aphasia Type Detection. (arXiv:2310.11710v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11710
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#35821;&#38899;&#21644;&#25163;&#21183;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#20934;&#30830;&#35782;&#21035;&#29305;&#23450;&#22833;&#35821;&#31867;&#22411;&#30340;&#26816;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65288;F1 84.2%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22833;&#35821;&#26159;&#19968;&#31181;&#30001;&#33041;&#25439;&#20260;&#24341;&#36215;&#30340;&#35821;&#35328;&#38556;&#30861;&#65292;&#38656;&#35201;&#20934;&#30830;&#35782;&#21035;&#29305;&#23450;&#30340;&#22833;&#35821;&#31867;&#22411;&#65292;&#22914;Broca&#22833;&#35821;&#21644;Wernicke&#22833;&#35821;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#30340;&#27835;&#30103;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#24320;&#21457;&#29992;&#20110;&#26816;&#27979;&#19981;&#21516;&#31867;&#22411;&#22833;&#35821;&#30340;&#26041;&#27861;&#65292;&#20154;&#20204;&#24182;&#27809;&#26377;&#32473;&#20104;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#24847;&#35782;&#21040;&#20998;&#26512;&#21516;&#26102;&#35821;&#35328;&#25163;&#21183;&#23545;&#20110;&#21306;&#20998;&#22833;&#35821;&#31867;&#22411;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#35821;&#38899;&#21644;&#30456;&#24212;&#30340;&#25163;&#21183;&#27169;&#24335;&#36827;&#34892;&#22833;&#35821;&#31867;&#22411;&#26816;&#27979;&#12290;&#36890;&#36807;&#23398;&#20064;&#27599;&#31181;&#22833;&#35821;&#31867;&#22411;&#30340;&#35821;&#38899;&#21644;&#25163;&#21183;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#23545;&#25163;&#21183;&#20449;&#24687;&#25935;&#24863;&#30340;&#25991;&#26412;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#20934;&#30830;&#30340;&#22833;&#35821;&#31867;&#22411;&#26816;&#27979;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65288;F1 84.2%&#65289;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25163;&#21183;&#29305;&#24449;&#20248;&#20110;&#22768;&#23398;&#29305;&#24449;&#65292;&#31361;&#26174;&#20102;&#25163;&#21183;&#34920;&#36798;&#22312;&#26816;&#27979;&#22833;&#35821;&#31867;&#22411;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aphasia, a language disorder resulting from brain damage, requires accurate identification of specific aphasia types, such as Broca's and Wernicke's aphasia, for effective treatment. However, little attention has been paid to developing methods to detect different types of aphasia. Recognizing the importance of analyzing co-speech gestures for distinguish aphasia types, we propose a multimodal graph neural network for aphasia type detection using speech and corresponding gesture patterns. By learning the correlation between the speech and gesture modalities for each aphasia type, our model can generate textual representations sensitive to gesture information, leading to accurate aphasia type detection. Extensive experiments demonstrate the superiority of our approach over existing methods, achieving state-of-the-art results (F1 84.2\%). We also show that gesture features outperform acoustic features, highlighting the significance of gesture expression in detecting aphasia types. We pro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26292;&#38706;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#36755;&#20986;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#24182;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#12290;</title><link>http://arxiv.org/abs/2310.10477</link><description>&lt;p&gt;
&#20174;&#25387;&#25240;&#20013;&#33719;&#24471;&#26234;&#24935;&#65306;&#36890;&#36807;&#38169;&#35823;&#20998;&#26512;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis. (arXiv:2310.10477v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10477
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26292;&#38706;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#36755;&#20986;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#24182;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#26082;&#24102;&#26469;&#20102;&#26426;&#36935;&#65292;&#20063;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#24847;&#22806;&#29983;&#25104;&#26377;&#23475;&#21644;&#26377;&#27602;&#22238;&#24212;&#26041;&#38754;&#12290;&#20256;&#32479;&#30340;&#23545;&#40784;&#26041;&#27861;&#33268;&#21147;&#20110;&#24341;&#23548;LLMs&#26397;&#30528;&#26399;&#26395;&#30340;&#24615;&#33021;&#21457;&#23637;&#24182;&#20445;&#25252;&#23427;&#20204;&#20813;&#21463;&#24694;&#24847;&#20869;&#23481;&#30340;&#20405;&#23475;&#65292;&#32780;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#20840;&#26032;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26377;&#24847;&#26292;&#38706;LLMs&#30340;&#32570;&#38519;&#36755;&#20986;&#24182;&#36827;&#34892;&#28145;&#20837;&#35780;&#20272;&#65292;&#20197;&#23436;&#20840;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20998;&#26512;&#12290;&#22240;&#27492;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;LLMs&#19981;&#20165;&#21487;&#20197;&#36991;&#20813;&#29983;&#25104;&#26377;&#32570;&#38519;&#30340;&#22238;&#24212;&#65292;&#36824;&#21487;&#20197;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#65292;&#21457;&#25381;&#20854;&#36776;&#21035;&#26377;&#27602;&#20869;&#23481;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23433;&#20840;&#25351;&#20196;&#36981;&#24490;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#23545;&#40784;&#25216;&#26415;&#65292;&#21516;&#26102;&#36824;&#20445;&#25345;&#20102;&#21331;&#36234;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of large language models (LLMs) presents both opportunities and challenges, particularly concerning unintentional generation of harmful and toxic responses. While the traditional alignment methods strive to steer LLMs towards desired performance and shield them from malicious content, this study proposes a novel alignment strategy rooted in mistake analysis by exposing LLMs to flawed outputs purposefully and then conducting a thorough assessment to fully comprehend internal reasons via natural language analysis. Thus, toxic responses can be transformed into instruction tuning corpus for model alignment, and LLMs can not only be deterred from generating flawed responses but also trained to self-criticize, leveraging its innate ability to discriminate toxic content. Experimental results demonstrate that the proposed method outperforms conventional alignment techniques for safety instruction following, while maintaining superior efficiency.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#25506;&#35752;&#20102;&#22823;&#22411;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#21644;&#26102;&#31354;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#12290;&#23427;&#20204;&#19981;&#20165;&#24102;&#26469;&#20102;&#22686;&#24378;&#30340;&#27169;&#24335;&#35782;&#21035;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#36824;&#20026;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#25171;&#19979;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2310.10196</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#21644;&#26102;&#31354;&#25968;&#25454;&#30340;&#22823;&#22411;&#27169;&#22411;&#65306;&#32508;&#36848;&#19982;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook. (arXiv:2310.10196v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10196
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#25506;&#35752;&#20102;&#22823;&#22411;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#21644;&#26102;&#31354;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#12290;&#23427;&#20204;&#19981;&#20165;&#24102;&#26469;&#20102;&#22686;&#24378;&#30340;&#27169;&#24335;&#35782;&#21035;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#36824;&#20026;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#25171;&#19979;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#26102;&#38388;&#24207;&#21015;&#21644;&#26102;&#31354;&#25968;&#25454;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#38750;&#24120;&#26222;&#36941;&#12290;&#23427;&#20204;&#25429;&#25417;&#21160;&#24577;&#31995;&#32479;&#30340;&#27979;&#37327;&#25968;&#25454;&#65292;&#24182;&#30001;&#29289;&#29702;&#21644;&#34394;&#25311;&#20256;&#24863;&#22120;&#22823;&#37327;&#20135;&#29983;&#12290;&#20998;&#26512;&#36825;&#20123;&#25968;&#25454;&#31867;&#22411;&#23545;&#20110;&#21033;&#29992;&#23427;&#20204;&#25152;&#21253;&#21547;&#30340;&#20016;&#23500;&#20449;&#24687;&#20197;&#21450;&#21463;&#30410;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#21644;&#20854;&#20182;&#22522;&#30784;&#27169;&#22411;&#30340;&#31361;&#30772;&#25512;&#21160;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#21644;&#26102;&#31354;&#25968;&#25454;&#25366;&#25496;&#20013;&#30340;&#22686;&#21152;&#20351;&#29992;&#12290;&#36825;&#26679;&#30340;&#26041;&#27861;&#19981;&#20165;&#33021;&#22815;&#23454;&#29616;&#36328;&#19981;&#21516;&#39046;&#22495;&#30340;&#22686;&#24378;&#27169;&#24335;&#35782;&#21035;&#21644;&#25512;&#29702;&#65292;&#36824;&#20026;&#33021;&#22815;&#29702;&#35299;&#21644;&#22788;&#29702;&#24120;&#35265;&#26102;&#38388;&#25968;&#25454;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#25171;&#19979;&#20102;&#22522;&#30784;&#12290;&#22312;&#27492;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#21644;&#26102;&#31354;&#25968;&#25454;&#23450;&#21046;&#30340;&#22823;&#22411;&#27169;&#22411;&#30340;&#20840;&#38754;&#21644;&#26368;&#26032;&#32508;&#36848;&#65292;&#21253;&#25324;&#25968;&#25454;&#31867;&#22411;&#12289;&#27169;&#22411;&#31867;&#21035;&#12289;&#27169;&#22411;&#33539;&#22260;&#21644;&#24212;&#29992;&#39046;&#22495;/&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;
&lt;/p&gt;
&lt;p&gt;
Temporal data, notably time series and spatio-temporal data, are prevalent in real-world applications. They capture dynamic system measurements and are produced in vast quantities by both physical and virtual sensors. Analyzing these data types is vital to harnessing the rich information they encompass and thus benefits a wide range of downstream tasks. Recent advances in large language and other foundational models have spurred increased use of these models in time series and spatio-temporal data mining. Such methodologies not only enable enhanced pattern recognition and reasoning across diverse domains but also lay the groundwork for artificial general intelligence capable of comprehending and processing common temporal data. In this survey, we offer a comprehensive and up-to-date review of large models tailored (or adapted) for time series and spatio-temporal data, spanning four key facets: data types, model categories, model scopes, and application areas/tasks. Our objective is to 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#27169;&#22359;&#25193;&#23637;&#21644;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#32456;&#36523;&#24207;&#21015;&#29983;&#25104;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20801;&#35768;&#27169;&#22411;&#26681;&#25454;&#20219;&#21153;&#30456;&#20851;&#24615;&#21160;&#24577;&#20915;&#23450;&#33719;&#21462;&#26032;&#30693;&#35782;&#30340;&#26550;&#26500;&#65292;&#24182;&#36873;&#25321;&#30456;&#20284;&#30340;&#20808;&#21069;&#20219;&#21153;&#26469;&#24110;&#21161;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#21160;&#24577;&#26799;&#24230;&#32553;&#25918;&#26469;&#24179;&#34913;&#23398;&#20064;&#36807;&#31243;&#65292;&#20197;&#36991;&#20813;&#23545;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#30340;&#20005;&#37325;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2310.09886</link><description>&lt;p&gt;
&#21160;&#24577;&#27169;&#22359;&#25193;&#23637;&#21644;&#33258;&#36866;&#24212;&#30340;&#32456;&#36523;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Lifelong Sequence Generation with Dynamic Module Expansion and Adaptation. (arXiv:2310.09886v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09886
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#27169;&#22359;&#25193;&#23637;&#21644;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#32456;&#36523;&#24207;&#21015;&#29983;&#25104;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20801;&#35768;&#27169;&#22411;&#26681;&#25454;&#20219;&#21153;&#30456;&#20851;&#24615;&#21160;&#24577;&#20915;&#23450;&#33719;&#21462;&#26032;&#30693;&#35782;&#30340;&#26550;&#26500;&#65292;&#24182;&#36873;&#25321;&#30456;&#20284;&#30340;&#20808;&#21069;&#20219;&#21153;&#26469;&#24110;&#21161;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#21160;&#24577;&#26799;&#24230;&#32553;&#25918;&#26469;&#24179;&#34913;&#23398;&#20064;&#36807;&#31243;&#65292;&#20197;&#36991;&#20813;&#23545;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#30340;&#20005;&#37325;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32456;&#36523;&#24207;&#21015;&#29983;&#25104;&#65288;LSG&#65289;&#26159;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#38382;&#39064;&#65292;&#26088;&#22312;&#35753;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#29983;&#25104;&#20219;&#21153;&#19978;&#36827;&#34892;&#25345;&#32493;&#35757;&#32451;&#65292;&#20197;&#19981;&#26029;&#23398;&#20064;&#26032;&#30340;&#29983;&#25104;&#27169;&#24335;&#24182;&#36991;&#20813;&#36951;&#24536;&#20808;&#21069;&#30340;&#30693;&#35782;&#12290;&#29616;&#26377;&#30340;LSG&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#32500;&#25345;&#26087;&#30693;&#35782;&#65292;&#32780;&#23545;&#36328;&#20219;&#21153;&#30340;&#30693;&#35782;&#20256;&#36882;&#20851;&#27880;&#36739;&#23569;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20808;&#21069;&#33719;&#21462;&#30340;&#31867;&#20284;&#20219;&#21153;&#30340;&#30693;&#35782;&#26356;&#22909;&#22320;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#21463;&#20154;&#31867;&#23398;&#20064;&#33539;&#24335;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#27169;&#22359;&#25193;&#23637;&#21644;&#33258;&#36866;&#24212;&#65288;DMEA&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#20219;&#21153;&#30456;&#20851;&#24615;&#21160;&#24577;&#30830;&#23450;&#33719;&#21462;&#26032;&#30693;&#35782;&#30340;&#26550;&#26500;&#65292;&#24182;&#36873;&#25321;&#26368;&#30456;&#20284;&#30340;&#20808;&#21069;&#20219;&#21153;&#26469;&#20419;&#36827;&#23545;&#26032;&#20219;&#21153;&#30340;&#36866;&#24212;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23398;&#20064;&#36807;&#31243;&#24456;&#23481;&#26131;&#20559;&#21521;&#20110;&#24403;&#21069;&#20219;&#21153;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#26356;&#20005;&#37325;&#30340;&#36951;&#24536;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#26799;&#24230;&#32553;&#25918;&#26469;&#24179;&#34913;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lifelong sequence generation (LSG), a problem in continual learning, aims to continually train a model on a sequence of generation tasks to learn constantly emerging new generation patterns while avoiding the forgetting of previous knowledge. Existing LSG methods mainly focus on maintaining old knowledge while paying little attention to knowledge transfer across tasks. In contrast, humans can better learn new tasks by leveraging previously acquired knowledge from similar tasks. Inspired by the learning paradigm of humans, we propose Dynamic Module Expansion and Adaptation (DMEA), which enables the model to dynamically determine the architecture for acquiring new knowledge based on task correlation and select the most similar previous tasks to facilitate adaptation to new tasks. In addition, as the learning process can easily be biased towards the current task which might cause more severe forgetting of previously learned knowledge, we propose dynamic gradient scaling to balance the lea
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38646;&#26679;&#26412;&#25554;&#27133;&#22635;&#20805;&#30340;Hierarchical Contrastive Learning Framework (HiCL)&#65292;&#36890;&#36807;&#31895;&#31890;&#24230;&#21040;&#32454;&#31890;&#24230;&#30340;&#23545;&#27604;&#23398;&#20064;&#65292;&#23398;&#20064;&#35821;&#21477;&#20196;&#29260;&#20043;&#38388;&#30340;&#28145;&#23618;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#26631;&#31614;&#38598;&#35821;&#20041;&#25512;&#29702;&#26041;&#27861;&#65292;&#26469;&#25552;&#39640;&#22312;&#26410;&#35265;&#25554;&#27133;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.09135</link><description>&lt;p&gt;
HierarchicalContrast: &#19968;&#31181;&#29992;&#20110;&#36328;&#39046;&#22495;&#38646;&#26679;&#26412;&#25554;&#27133;&#22635;&#20805;&#30340;&#31895;&#31890;&#24230;&#21040;&#32454;&#31890;&#24230;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
HierarchicalContrast: A Coarse-to-Fine Contrastive Learning Framework for Cross-Domain Zero-Shot Slot Filling. (arXiv:2310.09135v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38646;&#26679;&#26412;&#25554;&#27133;&#22635;&#20805;&#30340;Hierarchical Contrastive Learning Framework (HiCL)&#65292;&#36890;&#36807;&#31895;&#31890;&#24230;&#21040;&#32454;&#31890;&#24230;&#30340;&#23545;&#27604;&#23398;&#20064;&#65292;&#23398;&#20064;&#35821;&#21477;&#20196;&#29260;&#20043;&#38388;&#30340;&#28145;&#23618;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#26631;&#31614;&#38598;&#35821;&#20041;&#25512;&#29702;&#26041;&#27861;&#65292;&#26469;&#25552;&#39640;&#22312;&#26410;&#35265;&#25554;&#27133;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#22330;&#26223;&#20013;&#65292;&#36328;&#39046;&#22495;&#38646;&#26679;&#26412;&#25554;&#27133;&#22635;&#20805;&#22312;&#21033;&#29992;&#28304;&#39046;&#22495;&#30693;&#35782;&#26469;&#23398;&#20064;&#20855;&#26377;&#39640;&#27867;&#21270;&#33021;&#21147;&#30340;&#27169;&#22411;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22312;&#26410;&#30693;&#30446;&#26631;&#39046;&#22495;&#20013;&#65292;&#30001;&#20110;&#32570;&#23569;&#24102;&#27880;&#37322;&#30340;&#25968;&#25454;&#65292;&#20854;&#24615;&#33021;&#24448;&#24448;&#19981;&#29702;&#24819;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#38646;&#26679;&#26412;&#25554;&#27133;&#22635;&#20805;&#26041;&#27861;&#22312;&#30446;&#26631;&#39046;&#22495;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#65292;&#23427;&#20204;&#21482;&#33021;&#22312;&#24050;&#35265;&#25554;&#27133;&#19978;&#26377;&#25928;&#22320;&#36827;&#34892;&#30693;&#35782;&#36716;&#31227;&#65292;&#23545;&#26410;&#35265;&#25554;&#27133;&#30340;&#34920;&#29616;&#36739;&#24046;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Hierarchical Contrastive Learning Framework (HiCL)&#29992;&#20110;&#38646;&#26679;&#26412;&#25554;&#27133;&#22635;&#20805;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#20998;&#24067;&#23884;&#20837;&#30340;&#31895;&#31890;&#24230;&#21040;&#32454;&#31890;&#24230;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#38388;&#38548;&#21644;&#20869;&#37096;&#26631;&#35760;&#20998;&#24067;&#30340;&#36317;&#31163;&#65292;&#23398;&#20064;&#35821;&#21477;&#20196;&#29260;&#20043;&#38388;&#30340;&#28145;&#23618;&#35821;&#20041;&#20851;&#31995;&#12290;&#36825;&#40723;&#21169;HiCL&#22312;&#35757;&#32451;&#38454;&#27573;&#27867;&#21270;&#21040;&#26410;&#35265;&#30340;&#25554;&#27133;&#31867;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#26631;&#31614;&#38598;&#35821;&#20041;&#25512;&#29702;&#26041;&#27861;&#65292;&#26469;&#23545;&#26631;&#31614;&#36827;&#34892;&#20844;&#27491;&#30340;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
In task-oriented dialogue scenarios, cross-domain zero-shot slot filling plays a vital role in leveraging source domain knowledge to learn a model with high generalization ability in unknown target domain where annotated data is unavailable. However, the existing state-of-the-art zero-shot slot filling methods have limited generalization ability in target domain, they only show effective knowledge transfer on seen slots and perform poorly on unseen slots. To alleviate this issue, we present a novel Hierarchical Contrastive Learning Framework (HiCL) for zero-shot slot filling. Specifically, we propose a coarseto fine-grained contrastive learning based on Gaussian-distributed embedding to learn the generalized deep semantic relations between utterance-tokens, by optimizing inter- and intra-token distribution distance. This encourages HiCL to generalize to the slot types unseen at training phase. Furthermore, we present a new iterative label set semantics inference method to unbiasedly 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#25910;&#38598;&#30495;&#23454;&#25968;&#25454;&#38598;riSum&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#21442;&#32771;&#26041;&#27861;&#65292;&#20197;&#34913;&#37327;&#36825;&#31181;&#33021;&#21147;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#19982;&#38656;&#35201;&#39640;&#36136;&#37327;&#25688;&#35201;&#30340;&#21442;&#32771;&#26041;&#27861;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2310.08394</link><description>&lt;p&gt;
&#26397;&#30528;&#26356;&#22909;&#30340;&#25351;&#20196;&#36981;&#24490;&#35780;&#20272;&#65306;&#25688;&#35201;&#20013;&#30340;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Better Evaluation of Instruction-Following: A Case-Study in Summarization. (arXiv:2310.08394v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#25910;&#38598;&#30495;&#23454;&#25968;&#25454;&#38598;riSum&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#21442;&#32771;&#26041;&#27861;&#65292;&#20197;&#34913;&#37327;&#36825;&#31181;&#33021;&#21147;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#19982;&#38656;&#35201;&#39640;&#36136;&#37327;&#25688;&#35201;&#30340;&#21442;&#32771;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#26159;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22914;&#20309;&#36981;&#24490;&#29992;&#25143;&#25351;&#20196;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#34429;&#28982;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#26041;&#27861;&#22312;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#19978;&#26377;&#25152;&#22686;&#38271;&#65292;&#20294;&#26159;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#27491;&#30830;&#24615;&#30340;&#30740;&#31350;&#36824;&#24456;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20803;&#35780;&#20272;&#65292;&#35780;&#20272;&#20102;&#19968;&#31995;&#21015;&#25351;&#26631;&#30340;&#20934;&#30830;&#24615;&#65292;&#20197;&#34913;&#37327;LLMs&#30340;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26159;&#22312;&#22522;&#20110;&#26597;&#35810;&#30340;&#25688;&#35201;&#20219;&#21153;&#19978;&#36827;&#34892;&#30340;&#65292;&#36890;&#36807;&#25910;&#38598;&#19968;&#20010;&#26032;&#30340;&#30701;&#25991;&#24418;&#24335;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;riSum&#65292;&#20854;&#20013;&#21253;&#21547;300&#20010;&#25991;&#26723;&#25351;&#20196;&#23545;&#65292;&#27599;&#20010;&#23545;&#24212;3&#20010;&#31572;&#26696;&#12290;&#25152;&#26377;900&#20010;&#31572;&#26696;&#30001;3&#21517;&#20154;&#31867;&#26631;&#27880;&#21592;&#36827;&#34892;&#35780;&#20998;&#12290;&#21033;&#29992;riSum&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#35780;&#20272;&#26041;&#27861;&#19982;&#20154;&#31867;&#21028;&#26029;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;LLM&#30340;&#26032;&#30340;&#26080;&#21442;&#32771;&#35780;&#20272;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#24050;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#24182;&#19982;&#38656;&#35201;&#39640;&#36136;&#37327;&#25688;&#35201;&#30340;&#26114;&#36149;&#22522;&#20110;&#21442;&#32771;&#30340;&#24230;&#37327;&#26041;&#27861;&#34920;&#29616;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent advances, evaluating how well large language models (LLMs) follow user instructions remains an open problem. While evaluation methods of language models have seen a rise in prompt-based approaches, limited work on the correctness of these methods has been conducted. In this work, we perform a meta-evaluation of a variety of metrics to quantify how accurately they measure the instruction-following abilities of LLMs. Our investigation is performed on grounded query-based summarization by collecting a new short-form, real-world dataset riSum, containing 300 document-instruction pairs with 3 answers each. All 900 answers are rated by 3 human annotators. Using riSum, we analyze the agreement between evaluation methods and human judgment. Finally, we propose new LLM-based reference-free evaluation methods that improve upon established baselines and perform on par with costly reference-based metrics that require high-quality summaries.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#24212;&#23545;&#30740;&#31350;&#27700;&#24179;&#30340;&#25968;&#23398;&#33258;&#21160;&#24418;&#24335;&#21270;&#20219;&#21153;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#25104;&#26356;&#23481;&#26131;&#22788;&#29702;&#30340;&#23376;&#20219;&#21153;&#65292;&#21253;&#25324;&#26410;&#38142;&#25509;&#24418;&#24335;&#21270;&#12289;&#23454;&#20307;&#38142;&#25509;&#21644;&#31867;&#22411;&#35843;&#25972;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26410;&#38142;&#25509;&#24418;&#24335;&#21270;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598; arXiv2Formal&#12290;</title><link>http://arxiv.org/abs/2310.07957</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#24418;&#24335;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A New Approach Towards Autoformalization. (arXiv:2310.07957v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07957
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#24212;&#23545;&#30740;&#31350;&#27700;&#24179;&#30340;&#25968;&#23398;&#33258;&#21160;&#24418;&#24335;&#21270;&#20219;&#21153;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#25104;&#26356;&#23481;&#26131;&#22788;&#29702;&#30340;&#23376;&#20219;&#21153;&#65292;&#21253;&#25324;&#26410;&#38142;&#25509;&#24418;&#24335;&#21270;&#12289;&#23454;&#20307;&#38142;&#25509;&#21644;&#31867;&#22411;&#35843;&#25972;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26410;&#38142;&#25509;&#24418;&#24335;&#21270;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598; arXiv2Formal&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39564;&#35777;&#25968;&#23398;&#35777;&#26126;&#26159;&#22256;&#38590;&#30340;&#65292;&#20294;&#21487;&#20197;&#36890;&#36807;&#35745;&#31639;&#26426;&#30340;&#36741;&#21161;&#23454;&#29616;&#33258;&#21160;&#21270;&#12290;&#33258;&#21160;&#24418;&#24335;&#21270;&#26159;&#23558;&#33258;&#28982;&#35821;&#35328;&#25968;&#23398;&#33258;&#21160;&#36716;&#21270;&#20026;&#21487;&#20197;&#30001;&#31243;&#24207;&#39564;&#35777;&#30340;&#24418;&#24335;&#35821;&#35328;&#30340;&#20219;&#21153;&#12290;&#36825;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23588;&#20854;&#23545;&#20110;&#30740;&#31350;&#35770;&#25991;&#20013;&#30340;&#39640;&#32423;&#25968;&#23398;&#26469;&#35828;&#12290;&#30740;&#31350;&#35770;&#25991;&#20013;&#30340;&#25968;&#23398;&#38656;&#35201;&#22823;&#37327;&#30340;&#32972;&#26223;&#21644;&#19978;&#19979;&#25991;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#23545;&#30740;&#31350;&#27700;&#24179;&#25968;&#23398;&#33258;&#21160;&#24418;&#24335;&#21270;&#30340;&#26041;&#27861;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#26356;&#26131;&#20110;&#22788;&#29702;&#30340;&#23376;&#20219;&#21153;&#65306;&#26410;&#38142;&#25509;&#24418;&#24335;&#21270;&#65288;&#21253;&#21547;&#26410;&#38142;&#25509;&#30340;&#23450;&#20041;&#21644;&#23450;&#29702;&#30340;&#24418;&#24335;&#21270;&#65289;&#12289;&#23454;&#20307;&#38142;&#25509;&#65288;&#38142;&#25509;&#21040;&#27491;&#30830;&#30340;&#23450;&#29702;&#21644;&#23450;&#20041;&#65289;&#20197;&#21450;&#35843;&#25972;&#31867;&#22411;&#20197;&#36890;&#36807;&#31867;&#22411;&#26816;&#26597;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;arXiv2Formal&#65292;&#19968;&#20010;&#29992;&#20110;&#26410;&#38142;&#25509;&#24418;&#24335;&#21270;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#20174;arXiv.org&#30340;&#35770;&#25991;&#20013;&#25277;&#21462;&#30340;50&#20010;&#23450;&#29702;&#22312;Lean&#23450;&#29702;&#35777;&#26126;&#22120;&#20013;&#36827;&#34892;&#24418;&#24335;&#21270;&#12290;&#25105;&#20204;&#27426;&#36814;&#20219;&#20309;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Verifying mathematical proofs is difficult, but can be automated with the assistance of a computer. Autoformalization is the task of automatically translating natural language mathematics into a formal language that can be verified by a program. This is a challenging task, and especially for higher-level mathematics found in research papers. Research paper mathematics requires large amounts of background and context. In this paper, we propose an avenue towards tackling autoformalization for research-level mathematics, by breaking the task into easier and more approachable subtasks: unlinked formalization (formalization with unlinked definitions and theorems), entity linking (linking to the proper theorems and definitions), and finally adjusting types so it passes the type checker. In addition, we present arXiv2Formal, a benchmark dataset for unlinked formalization consisting of 50 theorems formalized for the Lean theorem prover sampled from papers on arXiv.org. We welcome any contribut
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23618;&#27425;&#22810;&#27169;&#24577;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#30340;&#26032;&#39062;&#12289;&#36890;&#29992;&#19988;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;MEDHMP&#65292;&#22312;&#19977;&#20010;&#32423;&#21035;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#22312;&#19982;&#21313;&#20843;&#20010;&#22522;&#20934;&#26041;&#27861;&#30340;&#27604;&#36739;&#20013;&#39564;&#35777;&#20102;&#20854;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07871</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#24577;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#23618;&#27425;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Pretraining on Multimodal Electronic Health Records. (arXiv:2310.07871v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07871
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23618;&#27425;&#22810;&#27169;&#24577;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#30340;&#26032;&#39062;&#12289;&#36890;&#29992;&#19988;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;MEDHMP&#65292;&#22312;&#19977;&#20010;&#32423;&#21035;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#22312;&#19982;&#21313;&#20843;&#20010;&#22522;&#20934;&#26041;&#27861;&#30340;&#27604;&#36739;&#20013;&#39564;&#35777;&#20102;&#20854;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25216;&#26415;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#65292;&#29616;&#26377;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#39044;&#35757;&#32451;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#21040;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#30340;&#23618;&#27425;&#24615;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20351;&#29992;&#21333;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#36328;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#36890;&#29992;&#19988;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;MEDHMP&#65292;&#19987;&#38376;&#38024;&#23545;&#23618;&#27425;&#22810;&#27169;&#24577;&#30340;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#36827;&#34892;&#35774;&#35745;&#12290;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#22312;&#19977;&#20010;&#32423;&#21035;&#19978;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;MEDHMP&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#19982;&#21313;&#20843;&#20010;&#22522;&#20934;&#27169;&#22411;&#30340;&#27604;&#36739;&#36827;&#19968;&#27493;&#31361;&#20986;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretraining has proven to be a powerful technique in natural language processing (NLP), exhibiting remarkable success in various NLP downstream tasks. However, in the medical domain, existing pretrained models on electronic health records (EHR) fail to capture the hierarchical nature of EHR data, limiting their generalization capability across diverse downstream tasks using a single pretrained model. To tackle this challenge, this paper introduces a novel, general, and unified pretraining framework called MEDHMP, specifically designed for hierarchically multimodal EHR data. The effectiveness of the proposed MEDHMP is demonstrated through experimental results on eight downstream tasks spanning three levels. Comparisons against eighteen baselines further highlight the efficacy of our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36827;&#34892;&#20102;&#23545;&#20013;&#25991;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#35843;&#25972;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;LLM&#22522;&#30784;&#12289;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#21644;&#25351;&#20196;&#25968;&#25454;&#31867;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#20182;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#20026;&#23450;&#21046;&#33021;&#26356;&#22909;&#21709;&#24212;&#20013;&#25991;&#25351;&#20196;&#30340;LLM&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.07328</link><description>&lt;p&gt;
&#23545;&#20013;&#25991;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#35843;&#25972;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of Instruction-tuning Large Language Models in Chinese. (arXiv:2310.07328v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36827;&#34892;&#20102;&#23545;&#20013;&#25991;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#35843;&#25972;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;LLM&#22522;&#30784;&#12289;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#21644;&#25351;&#20196;&#25968;&#25454;&#31867;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#20182;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#20026;&#23450;&#21046;&#33021;&#26356;&#22909;&#21709;&#24212;&#20013;&#25991;&#25351;&#20196;&#30340;LLM&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30340;&#25104;&#21151;&#39564;&#35777;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#20013;&#30340;&#28508;&#21147;&#12290;&#38543;&#21518;&#65292;LLM&#30340;&#21457;&#24067;&#24341;&#36215;&#20102;&#24320;&#28304;&#31038;&#21306;&#23545;&#25351;&#23548;&#35843;&#25972;&#30340;&#20852;&#36259;&#65292;&#36825;&#34987;&#35748;&#20026;&#21487;&#20197;&#21152;&#24555;ChatGPT&#30340;&#22797;&#21046;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#20013;&#25991;&#30340;LLM&#25351;&#23548;&#35843;&#25972;&#30340;&#30740;&#31350;&#20173;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#23545;&#20013;&#25991;LLM&#30340;&#25351;&#23548;&#35843;&#25972;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#36825;&#21487;&#20197;&#20316;&#20026;&#19968;&#26412;&#25552;&#20379;&#26377;&#20215;&#20540;&#21457;&#29616;&#30340;&#28921;&#39274;&#20070;&#26469;&#26377;&#25928;&#22320;&#23450;&#21046;LLM&#65292;&#20351;&#20854;&#33021;&#22815;&#26356;&#22909;&#22320;&#21709;&#24212;&#20013;&#25991;&#25351;&#20196;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;LLM&#22522;&#30784;&#12289;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#21644;&#25351;&#20196;&#25968;&#25454;&#31867;&#22411;&#36825;&#19977;&#20010;&#23545;&#25351;&#23548;&#35843;&#25972;&#33267;&#20851;&#37325;&#35201;&#30340;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23454;&#39564;&#26469;&#30740;&#31350;&#20854;&#20182;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#20363;&#22914;&#24605;&#32500;&#38142;&#25968;&#25454;&#21644;&#20154;&#31867;&#20215;&#20540;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20010;&#23454;&#35777;&#30740;&#31350;&#33021;&#22815;
&lt;/p&gt;
&lt;p&gt;
The success of ChatGPT validates the potential of large language models (LLMs) in artificial general intelligence (AGI). Subsequently, the release of LLMs has sparked the open-source community's interest in instruction-tuning, which is deemed to accelerate ChatGPT's replication process. However, research on instruction-tuning LLMs in Chinese, the world's most spoken language, is still in its early stages. Therefore, this paper makes an in-depth empirical study of instruction-tuning LLMs in Chinese, which can serve as a cookbook that provides valuable findings for effectively customizing LLMs that can better respond to Chinese instructions. Specifically, we systematically explore the impact of LLM bases, parameter-efficient methods, instruction data types, which are the three most important elements for instruction-tuning. Besides, we also conduct experiment to study the impact of other factors, e.g., chain-of-thought data and human-value alignment. We hope that this empirical study can
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PICProp&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#21452;&#23618;&#20248;&#21270;&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#23398;&#20064;&#21644;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#20013;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#36827;&#34892;&#24378;&#22823;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#35745;&#31639;&#26377;&#25928;&#30340;&#32622;&#20449;&#21306;&#38388;&#65288;CI&#65289;&#65292;&#24182;&#19988;&#36890;&#36807;&#20256;&#25773;&#32622;&#20449;&#24230;&#23454;&#29616;&#20102;&#25968;&#25454;&#20301;&#32622;&#21040;&#25972;&#20010;&#22495;&#30340;&#32622;&#20449;&#24230;&#20256;&#25773;&#12290;</title><link>http://arxiv.org/abs/2310.06923</link><description>&lt;p&gt;
PICProp&#65306;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#29289;&#29702;&#20449;&#24687;&#32622;&#20449;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
PICProp: Physics-Informed Confidence Propagation for Uncertainty Quantification. (arXiv:2310.06923v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PICProp&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#21452;&#23618;&#20248;&#21270;&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#23398;&#20064;&#21644;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#20013;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#36827;&#34892;&#24378;&#22823;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#35745;&#31639;&#26377;&#25928;&#30340;&#32622;&#20449;&#21306;&#38388;&#65288;CI&#65289;&#65292;&#24182;&#19988;&#36890;&#36807;&#20256;&#25773;&#32622;&#20449;&#24230;&#23454;&#29616;&#20102;&#25968;&#25454;&#20301;&#32622;&#21040;&#25972;&#20010;&#22495;&#30340;&#32622;&#20449;&#24230;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#21644;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#20013;&#65292;&#26631;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#23384;&#22312;&#30528;&#25345;&#20037;&#30340;&#23616;&#38480;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#38656;&#35201;&#23545;&#25968;&#25454;&#30340;&#21487;&#33021;&#24615;&#20570;&#20986;&#24378;&#28872;&#30340;&#20551;&#35774;&#65292;&#24615;&#33021;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#20808;&#39564;&#30340;&#36873;&#25321;&#65292;&#24182;&#19988;&#21518;&#39564;&#21482;&#33021;&#20197;&#36817;&#20284;&#30340;&#26041;&#24335;&#36827;&#34892;&#37319;&#26679;&#65292;&#20174;&#32780;&#30001;&#20110;&#30456;&#20851;&#30340;&#35745;&#31639;&#25104;&#26412;&#23548;&#33268;&#36817;&#20284;&#31934;&#24230;&#36739;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#23545;&#30830;&#23450;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#32622;&#20449;&#21306;&#38388;&#65288;CI&#65289;&#20272;&#35745;&#20316;&#20026;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#12290;&#21363;&#65292;&#22312;&#25972;&#20010;&#21306;&#22495;&#20013;&#20197;&#27010;&#29575;&#25285;&#20445;&#30340;&#24418;&#24335;&#20256;&#25773;&#32622;&#20449;&#24230;&#65292;&#20197;&#36798;&#21040;&#25968;&#25454;&#20301;&#32622;&#21040;&#25972;&#20010;&#22495;&#30340;&#32622;&#20449;&#24230;&#20256;&#25773;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29289;&#29702;&#20449;&#24687;&#32622;&#20449;&#20256;&#25773;&#65288;PICProp&#65289;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#21452;&#23618;&#20248;&#21270;&#26469;&#35745;&#31639;&#19968;&#20010;&#26377;&#25928;&#30340;CI&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#25105;&#20204;&#26041;&#27861;&#26377;&#25928;&#24615;&#30340;&#23450;&#29702;&#20197;&#21450;&#38024;&#23545;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#30340;&#35745;&#31639;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard approaches for uncertainty quantification in deep learning and physics-informed learning have persistent limitations. Indicatively, strong assumptions regarding the data likelihood are required, the performance highly depends on the selection of priors, and the posterior can be sampled only approximately, which leads to poor approximations because of the associated computational cost. This paper introduces and studies confidence interval (CI) estimation for deterministic partial differential equations as a novel problem. That is, to propagate confidence, in the form of CIs, from data locations to the entire domain with probabilistic guarantees. We propose a method, termed Physics-Informed Confidence Propagation (PICProp), based on bi-level optimization to compute a valid CI without making heavy assumptions. We provide a theorem regarding the validity of our method, and computational experiments, where the focus is on physics-informed learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#32593;&#32476;&#22823;&#23567;&#21644;L2&#27491;&#21017;&#21270;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#35266;&#23519;&#21040;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#12290;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#29305;&#24449;&#21644;&#25042;&#24816;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#21442;&#25968;&#21644;&#29366;&#24577;&#25968;&#26080;&#38480;&#22823;&#30340;&#24773;&#20917;&#19979;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#30340;&#26368;&#23567;&#20108;&#20056;&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;&#65292;&#24471;&#20986;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#65292;&#24182;&#38416;&#36848;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#22312;&#35813;&#31639;&#27861;&#20013;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.05518</link><description>&lt;p&gt;
&#20851;&#20110;&#20351;&#29992;LSTD&#21644;&#38543;&#26426;&#29305;&#24449;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21452;&#19979;&#38477;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
On Double-Descent in Reinforcement Learning with LSTD and Random Features. (arXiv:2310.05518v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#32593;&#32476;&#22823;&#23567;&#21644;L2&#27491;&#21017;&#21270;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#35266;&#23519;&#21040;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#12290;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#29305;&#24449;&#21644;&#25042;&#24816;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#21442;&#25968;&#21644;&#29366;&#24577;&#25968;&#26080;&#38480;&#22823;&#30340;&#24773;&#20917;&#19979;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#30340;&#26368;&#23567;&#20108;&#20056;&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;&#65292;&#24471;&#20986;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#65292;&#24182;&#38416;&#36848;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#22312;&#35813;&#31639;&#27861;&#20013;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20854;&#24615;&#33021;&#21463;&#31070;&#32463;&#32593;&#32476;&#22823;&#23567;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#36807;&#21442;&#25968;&#21270;&#21644;&#20854;&#24102;&#26469;&#30340;&#22909;&#22788;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#29702;&#35299;&#65292;&#20294;&#26159;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24773;&#20917;&#21017;&#19981;&#22826;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#25506;&#35752;&#20102;&#32593;&#32476;&#22823;&#23567;&#21644;L2&#27491;&#21017;&#21270;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#23558;&#21442;&#25968;&#20010;&#25968;&#19982;&#35775;&#38382;&#29366;&#24577;&#20010;&#25968;&#20043;&#27604;&#23450;&#20041;&#20026;&#20851;&#38190;&#22240;&#32032;&#65292;&#24403;&#35813;&#27604;&#20540;&#22823;&#20110;1&#26102;&#31216;&#20026;&#36807;&#21442;&#25968;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#65292;&#21363;&#22312;&#21442;&#25968;/&#29366;&#24577;&#27604;&#20026;1&#38468;&#36817;&#20250;&#31361;&#28982;&#24615;&#33021;&#19979;&#38477;&#12290;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#29305;&#24449;&#21644;&#25042;&#24816;&#35757;&#32451;&#31574;&#30053;&#65292;&#25105;&#20204;&#22312;&#26080;&#38480;&#22823;&#30340;&#21442;&#25968;&#21644;&#29366;&#24577;&#25968;&#19979;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#30340;&#26368;&#23567;&#20108;&#20056;&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#65292;&#24182;&#38416;&#36848;&#20102;&#21452;&#19979;&#38477;&#29616;&#35937;&#22312;&#35813;&#31639;&#27861;&#20013;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal Difference (TD) algorithms are widely used in Deep Reinforcement Learning (RL). Their performance is heavily influenced by the size of the neural network. While in supervised learning, the regime of over-parameterization and its benefits are well understood, the situation in RL is much less clear. In this paper, we present a theoretical analysis of the influence of network size and $l_2$-regularization on performance. We identify the ratio between the number of parameters and the number of visited states as a crucial factor and define over-parameterization as the regime when it is larger than one. Furthermore, we observe a double-descent phenomenon, i.e., a sudden drop in performance around the parameter/state ratio of one. Leveraging random features and the lazy training regime, we study the regularized Least-Square Temporal Difference (LSTD) algorithm in an asymptotic regime, as both the number of parameters and states go to infinity, maintaining a constant ratio. We derive 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#20027;&#24352;&#39564;&#35777;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#20381;&#36182;&#26114;&#36149;&#30340;&#26631;&#27880;&#25968;&#25454;&#65292;&#33021;&#22815;&#39564;&#35777;&#22797;&#26434;&#30340;&#20027;&#24352;&#24182;&#29983;&#25104;&#35299;&#37322;&#65292;&#23545;&#21327;&#21161;&#20154;&#24037;&#20107;&#23454;&#26816;&#26597;&#21592;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.05253</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#29702;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#20027;&#24352;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Explainable Claim Verification via Knowledge-Grounded Reasoning with Large Language Models. (arXiv:2310.05253v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#20027;&#24352;&#39564;&#35777;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#20381;&#36182;&#26114;&#36149;&#30340;&#26631;&#27880;&#25968;&#25454;&#65292;&#33021;&#22815;&#39564;&#35777;&#22797;&#26434;&#30340;&#20027;&#24352;&#24182;&#29983;&#25104;&#35299;&#37322;&#65292;&#23545;&#21327;&#21161;&#20154;&#24037;&#20107;&#23454;&#26816;&#26597;&#21592;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#24352;&#39564;&#35777;&#22312;&#25171;&#20987;&#34394;&#20551;&#20449;&#24687;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#20027;&#24352;&#39564;&#35777;&#24037;&#20316;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#20294;&#20173;&#26377;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#23578;&#26410;&#35299;&#20915;&#65292;&#37027;&#23601;&#26159;&#22914;&#20309;&#22312;&#19981;&#20381;&#36182;&#20110;&#26114;&#36149;&#30340;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20027;&#24352;&#39564;&#35777;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#25552;&#20379;&#20840;&#38754;&#35299;&#37322;&#20197;&#35777;&#26126;&#20854;&#20915;&#31574;&#24182;&#21327;&#21161;&#20154;&#24037;&#20107;&#23454;&#26816;&#26597;&#21592;&#20063;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22797;&#26434;&#20027;&#24352;&#39564;&#35777;&#21644;&#29983;&#25104;&#35299;&#37322;&#30340;&#19968;&#38454;&#36923;&#36753;&#25351;&#23548;&#30340;&#30693;&#35782;&#22522;&#30784;&#30340;&#25512;&#29702;&#26041;&#27861;&#65288;FOLK&#65289;&#12290;FOLK&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#23558;&#20027;&#24352;&#36716;&#21270;&#20026;&#19968;&#38454;&#36923;&#36753;&#23376;&#21477;&#65292;&#24182;&#29983;&#25104;&#19968;&#32452;&#35859;&#35789;&#65292;&#27599;&#20010;&#35859;&#35789;&#23545;&#24212;&#19968;&#20010;&#38656;&#35201;&#39564;&#35777;&#30340;&#23376;&#20027;&#24352;&#12290;&#28982;&#21518;&#65292;FOLK&#36890;&#36807;&#19968;&#32452;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#23545;&#36827;&#34892;&#19968;&#38454;&#36923;&#36753;&#25351;&#23548;&#30340;&#25512;&#29702;&#65292;&#20174;&#32780;&#36827;&#34892;&#30495;&#23454;&#24615;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Claim verification plays a crucial role in combating misinformation. While existing works on claim verification have shown promising results, a crucial piece of the puzzle that remains unsolved is to understand how to verify claims without relying on human-annotated data, which is expensive to create at a large scale. Additionally, it is important for models to provide comprehensive explanations that can justify their decisions and assist human fact-checkers. This paper presents First-Order-Logic-Guided Knowledge-Grounded (FOLK) Reasoning that can verify complex claims and generate explanations without the need for annotated evidence using Large Language Models (LLMs). FOLK leverages the in-context learning ability of LLMs to translate the claim into a First-Order-Logic (FOL) clause consisting of predicates, each corresponding to a sub-claim that needs to be verified. Then, FOLK performs FOL-Guided reasoning over a set of knowledge-grounded question-and-answer pairs to make veracity pr
&lt;/p&gt;</description></item><item><title>&#29289;&#29702;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#31181;&#38761;&#21629;&#24615;&#26041;&#27861;&#65292;&#23427;&#23558;&#29289;&#29702;&#30693;&#35782;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#27700;&#25991;&#23398;&#29702;&#35299;&#21644;&#27700;&#24490;&#29615;&#39044;&#27979;&#65292;&#23545;&#20110;&#31649;&#29702;&#27700;&#36164;&#28304;&#20197;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#31561;&#25361;&#25112;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.05227</link><description>&lt;p&gt;
&#29289;&#29702;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#38761;&#21629;&#31185;&#23398;&#33539;&#24335;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#22522;&#20110;&#36807;&#31243;&#30340;&#27700;&#25991;&#23398;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Physics-aware Machine Learning Revolutionizes Scientific Paradigm for Machine Learning and Process-based Hydrology. (arXiv:2310.05227v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05227
&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#31181;&#38761;&#21629;&#24615;&#26041;&#27861;&#65292;&#23427;&#23558;&#29289;&#29702;&#30693;&#35782;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#27700;&#25991;&#23398;&#29702;&#35299;&#21644;&#27700;&#24490;&#29615;&#39044;&#27979;&#65292;&#23545;&#20110;&#31649;&#29702;&#27700;&#36164;&#28304;&#20197;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#31561;&#25361;&#25112;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#27700;&#25991;&#23398;&#29702;&#35299;&#21644;&#27700;&#24490;&#29615;&#39044;&#27979;&#23545;&#20110;&#35299;&#20915;&#27700;&#36164;&#28304;&#31649;&#29702;&#20013;&#30340;&#31185;&#23398;&#21644;&#31038;&#20250;&#25361;&#25112;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#20154;&#20026;&#27668;&#20505;&#21464;&#21270;&#30340;&#21160;&#24577;&#24433;&#21709;&#19979;&#12290;&#29616;&#26377;&#30340;&#35780;&#35770;&#20027;&#35201;&#20851;&#27880;&#26426;&#22120;&#23398;&#20064;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#28982;&#32780;&#27700;&#25991;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#29420;&#31435;&#30340;&#33539;&#24335;&#23384;&#22312;&#26126;&#26174;&#30340;&#21306;&#21035;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20197;&#29289;&#29702;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#21464;&#38761;&#24615;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#36825;&#31181;&#35748;&#30693;&#38556;&#30861;&#65292;&#24182;&#38761;&#26032;&#20102;&#36825;&#20004;&#20010;&#39046;&#22495;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#29289;&#29702;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#35780;&#35770;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#31038;&#21306;&#65288;PaML&#65289;&#65292;&#23558;&#20808;&#21069;&#30340;&#29289;&#29702;&#30693;&#35782;&#25110;&#22522;&#20110;&#29289;&#29702;&#30340;&#24314;&#27169;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#20174;&#29289;&#29702;&#25968;&#25454;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#12289;&#29289;&#29702;&#20449;&#24687;&#22788;&#29702;&#30340;&#26426;&#22120;&#23398;&#20064;&#12289;&#29289;&#29702;&#23884;&#20837;&#24335;&#26426;&#22120;&#23398;&#20064;&#21644;&#29289;&#29702;&#24863;&#30693;&#28151;&#21512;&#23398;&#20064;&#22235;&#20010;&#26041;&#38754;&#20998;&#26512;&#20102;&#36825;&#20123;PaML&#26041;&#27861;&#12290;PaML&#20419;&#36827;&#20102;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#20551;&#35774;&#25512;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate hydrological understanding and water cycle prediction are crucial for addressing scientific and societal challenges associated with the management of water resources, particularly under the dynamic influence of anthropogenic climate change. Existing reviews predominantly concentrate on the development of machine learning (ML) in this field, yet there is a clear distinction between hydrology and ML as separate paradigms. Here, we introduce physics-aware ML as a transformative approach to overcome the perceived barrier and revolutionize both fields. Specifically, we present a comprehensive review of the physics-aware ML methods, building a structured community (PaML) of existing methodologies that integrate prior physical knowledge or physics-based modeling into ML. We systematically analyze these PaML methodologies with respect to four aspects: physical data-guided ML, physics-informed ML, physics-embedded ML, and physics-aware hybrid learning. PaML facilitates ML-aided hypothe
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;SE(3)-Stochastic Flow Matching&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;FoldFlow&#65292;&#21487;&#20197;&#20934;&#30830;&#24314;&#27169;&#34507;&#30333;&#36136;&#20027;&#38142;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#26080;&#38656;&#27169;&#25311;&#35757;&#32451;&#21644;Riemannian&#26368;&#20248;&#20256;&#36755;&#30340;&#32467;&#21512;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#31283;&#23450;&#24615;&#21644;&#24314;&#27169;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02391</link><description>&lt;p&gt;
SE(3)-&#34507;&#30333;&#36136;&#20027;&#38142;&#29983;&#25104;&#20013;&#30340;&#38543;&#26426;&#27969;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
SE(3)-Stochastic Flow Matching for Protein Backbone Generation. (arXiv:2310.02391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02391
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;SE(3)-Stochastic Flow Matching&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;FoldFlow&#65292;&#21487;&#20197;&#20934;&#30830;&#24314;&#27169;&#34507;&#30333;&#36136;&#20027;&#38142;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#26080;&#38656;&#27169;&#25311;&#35757;&#32451;&#21644;Riemannian&#26368;&#20248;&#20256;&#36755;&#30340;&#32467;&#21512;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#31283;&#23450;&#24615;&#21644;&#24314;&#27169;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#19977;&#32500;&#21018;&#20307;&#36816;&#21160;&#65288;&#21363;SE(3)&#32676;&#65289;&#30340;&#27969;&#21305;&#37197;&#33539;&#24335;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#20855;&#26377;&#19981;&#26029;&#22686;&#24378;&#24314;&#27169;&#33021;&#21147;&#30340;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#65306;FoldFlow&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#20027;&#38142;&#30340;&#20934;&#30830;&#24314;&#27169;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FoldFlow-Base&#65292;&#19968;&#31181;&#26080;&#38656;&#27169;&#25311;&#30340;&#23398;&#20064;&#30830;&#23450;&#24615;&#36830;&#32493;&#26102;&#38388;&#21160;&#21147;&#23398;&#21644;&#21305;&#37197;&#19981;&#21464;&#30446;&#26631;&#20998;&#24067;&#30340;&#26041;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;Riemannian&#26368;&#20248;&#20256;&#36755;&#26469;&#21152;&#36895;&#35757;&#32451;&#65292;&#21019;&#24314;&#20102;FoldFlow-OT&#65292;&#20174;&#32780;&#26500;&#24314;&#20102;&#26356;&#31616;&#21333;&#21644;&#31283;&#23450;&#30340;&#27969;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;FoldFlow-SFM&#65292;&#23558;Riemannian&#26368;&#20248;&#20256;&#36755;&#21644;&#26080;&#38656;&#27169;&#25311;&#35757;&#32451;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#23398;&#20064;SE(3)&#19978;&#30340;&#38543;&#26426;&#36830;&#32493;&#26102;&#38388;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#30340;FoldFlow&#29983;&#25104;&#27169;&#22411;&#23478;&#26063;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#20855;&#26377;&#20960;&#20010;&#20851;&#38190;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The computational design of novel protein structures has the potential to impact numerous scientific disciplines greatly. Toward this goal, we introduce $\text{FoldFlow}$ a series of novel generative models of increasing modeling power based on the flow-matching paradigm over $3\text{D}$ rigid motions -i.e. the group $\text{SE(3)}$ -- enabling accurate modeling of protein backbones. We first introduce $\text{FoldFlow-Base}$, a simulation-free approach to learning deterministic continuous-time dynamics and matching invariant target distributions on $\text{SE(3)}$. We next accelerate training by incorporating Riemannian optimal transport to create $\text{FoldFlow-OT}$, leading to the construction of both more simple and stable flows. Finally, we design $\text{FoldFlow-SFM}$ coupling both Riemannian OT and simulation-free training to learn stochastic continuous-time dynamics over $\text{SE(3)}$. Our family of $\text{FoldFlow}$ generative models offer several key advantages over previous
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#27602;&#24615;&#30340;&#23450;&#20041;&#27169;&#31946;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#37327;&#21387;&#21147;&#30340;&#27602;&#24615;&#23450;&#20041;&#26469;&#24357;&#34917;&#29616;&#26377;&#23450;&#20041;&#30340;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.02357</link><description>&lt;p&gt;
&#20851;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#27602;&#24615;&#23450;&#20041;&#30340;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
On the definition of toxicity in NLP. (arXiv:2310.02357v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02357
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#27602;&#24615;&#30340;&#23450;&#20041;&#27169;&#31946;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#37327;&#21387;&#21147;&#30340;&#27602;&#24615;&#23450;&#20041;&#26469;&#24357;&#34917;&#29616;&#26377;&#23450;&#20041;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27602;&#24615;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#26681;&#26412;&#38382;&#39064;&#22312;&#20110;&#27602;&#24615;&#30340;&#23450;&#20041;&#27169;&#31946;&#19981;&#28165;&#12290;&#35895;&#27468;&#26071;&#19979;&#30340;&#22242;&#38431;Jigsaw&#26159;&#35813;&#39046;&#22495;&#30340;&#39046;&#23548;&#32773;&#20043;&#19968;&#65292;&#20182;&#20204;&#20351;&#29992;Dixon&#31561;&#20154;&#32473;&#20986;&#30340;&#27602;&#24615;&#23450;&#20041;&#65306;&#8220;&#31895;&#40065;&#12289;&#19981;&#23562;&#37325;&#25110;&#19981;&#21512;&#29702;&#30340;&#35821;&#35328;&#65292;&#21487;&#33021;&#20250;&#35753;&#26576;&#20154;&#31163;&#24320;&#35752;&#35770;&#8221;&#12290;&#20154;&#20204;&#21487;&#20197;&#31435;&#21363;&#30475;&#21040;&#36825;&#20010;&#23450;&#20041;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#27809;&#26377;&#32473;&#20986;&#27602;&#24615;&#30340;&#23450;&#37327;&#24230;&#37327;&#65292;&#32780;&#19988;&#28041;&#21450;&#39640;&#24230;&#20027;&#35266;&#30340;&#25991;&#21270;&#26415;&#35821;&#12290;&#23613;&#31649;&#23384;&#22312;&#27169;&#31946;&#21644;&#32570;&#38519;&#65292;&#20294;&#36825;&#20010;&#23450;&#20041;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#30740;&#31350;&#32773;&#24191;&#27867;&#37319;&#29992;&#30340;&#23454;&#38469;&#26631;&#20934;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#37327;&#21387;&#21147;&#30340;&#27602;&#24615;&#23450;&#20041;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fundamental problem in toxicity detection task lies in the fact that the toxicity is ill-defined. Jigsaw, a unit within Google and one of the leaders in the field, uses a definition of toxicity given by Dixon et al. - 'rude, disrespectful, or unreasonable language that is likely to make someone leave a discussion'. One can instantly see the issue with this definition, as it gives no quantitative measure of the toxicity and operates with highly subjective cultural terms. Despite all vagueness and flaws, this definition is de-facto widely used by many researchers. In this work we suggest quantative stress-based defenition for the toxicity that overcomes existing shortcomings.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#22122;&#25193;&#25955;&#26725;&#27169;&#22411;&#65288;DDBMs&#65289;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#25193;&#25955;&#26725;&#30340;&#20998;&#25968;&#65292;&#24182;&#22522;&#20110;&#23398;&#20064;&#21040;&#30340;&#20998;&#25968;&#27714;&#35299;&#24494;&#20998;&#26041;&#31243;&#26469;&#23454;&#29616;&#20174;&#19968;&#20010;&#20998;&#24067;&#21040;&#21478;&#19968;&#20010;&#20998;&#24067;&#30340;&#26144;&#23556;&#65292;&#20174;&#32780;&#23558;&#22810;&#31867;&#29983;&#25104;&#27169;&#22411;&#32479;&#19968;&#36215;&#26469;&#12290;</title><link>http://arxiv.org/abs/2309.16948</link><description>&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#26725;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Denoising Diffusion Bridge Models. (arXiv:2309.16948v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#22122;&#25193;&#25955;&#26725;&#27169;&#22411;&#65288;DDBMs&#65289;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#25193;&#25955;&#26725;&#30340;&#20998;&#25968;&#65292;&#24182;&#22522;&#20110;&#23398;&#20064;&#21040;&#30340;&#20998;&#25968;&#27714;&#35299;&#24494;&#20998;&#26041;&#31243;&#26469;&#23454;&#29616;&#20174;&#19968;&#20010;&#20998;&#24067;&#21040;&#21478;&#19968;&#20010;&#20998;&#24067;&#30340;&#26144;&#23556;&#65292;&#20174;&#32780;&#23558;&#22810;&#31867;&#29983;&#25104;&#27169;&#22411;&#32479;&#19968;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#38543;&#26426;&#36807;&#31243;&#23558;&#22122;&#22768;&#26144;&#23556;&#21040;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#65292;&#22914;&#22270;&#20687;&#32534;&#36753;&#65292;&#27169;&#22411;&#30340;&#36755;&#20837;&#19981;&#26159;&#38543;&#26426;&#22122;&#22768;&#30340;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#25193;&#25955;&#27169;&#22411;&#24517;&#39035;&#20381;&#36182;&#20110;&#32321;&#29712;&#30340;&#26041;&#27861;&#65292;&#22914;&#24341;&#23548;&#25110;&#25237;&#24433;&#37319;&#26679;&#65292;&#20197;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#21152;&#20837;&#36825;&#20123;&#20449;&#24687;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21435;&#22122;&#25193;&#25955;&#26725;&#27169;&#22411;&#65288;DDBMs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#26725;&#30340;&#33539;&#20363;&#65292;&#25193;&#25955;&#26725;&#26159;&#19968;&#26063;&#36807;&#31243;&#65292;&#20854;&#22312;&#32473;&#23450;&#30340;&#31471;&#28857;&#19979;&#25554;&#20540;&#20004;&#20010;&#37197;&#23545;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#25193;&#25955;&#26725;&#30340;&#20998;&#25968;&#65292;&#22522;&#20110;&#23398;&#20064;&#21040;&#30340;&#20998;&#25968;&#27714;&#35299;&#19968;&#20010;&#65288;&#38543;&#26426;&#30340;&#65289;&#24494;&#20998;&#26041;&#31243;&#65292;&#20174;&#19968;&#20010;&#31471;&#28857;&#20998;&#24067;&#26144;&#23556;&#21040;&#21478;&#19968;&#20010;&#31471;&#28857;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33258;&#28982;&#22320;&#32479;&#19968;&#20102;&#20960;&#31867;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#21644;OT-Flow-Matching&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#29616;&#26377;&#30340;&#35774;&#35745;&#21644;&#26550;&#26500;&#36873;&#25321;&#36866;&#24212;&#21040;&#26356;&#36890;&#29992;&#30340;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are powerful generative models that map noise to data using stochastic processes. However, for many applications such as image editing, the model input comes from a distribution that is not random noise. As such, diffusion models must rely on cumbersome methods like guidance or projected sampling to incorporate this information in the generative process. In our work, we propose Denoising Diffusion Bridge Models (DDBMs), a natural alternative to this paradigm based on diffusion bridges, a family of processes that interpolate between two paired distributions given as endpoints. Our method learns the score of the diffusion bridge from data and maps from one endpoint distribution to the other by solving a (stochastic) differential equation based on the learned score. Our method naturally unifies several classes of generative models, such as score-based diffusion models and OT-Flow-Matching, allowing us to adapt existing design and architectural choices to our more general 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;STERLING&#30340;&#33258;&#25105;&#30417;&#30563;&#22320;&#24418;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#32422;&#26463;&#30340;&#26426;&#22120;&#20154;&#32463;&#39564;&#23398;&#20064;&#26377;&#20851;&#22320;&#24418;&#30340;&#30456;&#20851;&#34920;&#31034;&#65292;&#20197;&#23454;&#29616;&#22320;&#24418;&#24863;&#30693;&#23548;&#33322;&#12290;</title><link>http://arxiv.org/abs/2309.15302</link><description>&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#30340;&#26080;&#32422;&#26463;&#26426;&#22120;&#20154;&#32463;&#39564;&#20013;&#30340;&#22320;&#24418;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Terrain Representation Learning from Unconstrained Robot Experience. (arXiv:2309.15302v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15302
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;STERLING&#30340;&#33258;&#25105;&#30417;&#30563;&#22320;&#24418;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#32422;&#26463;&#30340;&#26426;&#22120;&#20154;&#32463;&#39564;&#23398;&#20064;&#26377;&#20851;&#22320;&#24418;&#30340;&#30456;&#20851;&#34920;&#31034;&#65292;&#20197;&#23454;&#29616;&#22320;&#24418;&#24863;&#30693;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#24418;&#35748;&#30693;&#65292;&#21363;&#36776;&#21035;&#21644;&#21306;&#20998;&#19981;&#21516;&#31867;&#22411;&#30340;&#22320;&#24418;&#65292;&#26159;&#26426;&#22120;&#20154;&#22312;&#33258;&#20027;&#36234;&#37326;&#23548;&#33322;&#20013;&#24517;&#39035;&#20855;&#22791;&#30340;&#20851;&#38190;&#33021;&#21147;&#12290;&#30446;&#21069;&#25552;&#20379;&#26426;&#22120;&#20154;&#36825;&#31181;&#35748;&#30693;&#33021;&#21147;&#30340;&#26041;&#27861;&#35201;&#20040;&#20381;&#36182;&#26114;&#36149;&#30340;&#26631;&#35760;&#25968;&#25454;&#25910;&#38598;&#65292;&#35201;&#20040;&#20381;&#36182;&#26080;&#27861;&#27867;&#21270;&#30340;&#24037;&#31243;&#29305;&#24449;&#21644;&#25104;&#26412;&#20989;&#25968;&#65292;&#25110;&#32773;&#20381;&#36182;&#21487;&#33021;&#26080;&#27861;&#33719;&#24471;&#30340;&#19987;&#23478;&#20154;&#31867;&#31034;&#33539;&#12290;&#20026;&#20102;&#20351;&#26426;&#22120;&#20154;&#19981;&#21463;&#36825;&#20123;&#38480;&#21046;&#22320;&#20855;&#22791;&#22320;&#24418;&#35748;&#30693;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#25105;&#30417;&#30563;&#22320;&#24418;&#34920;&#31034;&#23398;&#20064;&#65288;STERLING&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#22320;&#24418;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#20165;&#20381;&#36182;&#20110;&#26131;&#20110;&#25910;&#38598;&#30340;&#12289;&#26080;&#32422;&#26463;&#65288;&#20363;&#22914;&#65292;&#38750;&#19987;&#23478;&#30340;&#65289;&#21644;&#26410;&#26631;&#35760;&#30340;&#26426;&#22120;&#20154;&#32463;&#39564;&#65292;&#23545;&#25968;&#25454;&#37319;&#38598;&#27809;&#26377;&#39069;&#22806;&#30340;&#38480;&#21046;&#12290;STERLING&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#33258;&#25105;&#30417;&#30563;&#30446;&#26631;&#65292;&#36890;&#36807;&#38750;&#23545;&#27604;&#24230;&#34920;&#31034;&#23398;&#20064;&#26469;&#23398;&#20064;&#22320;&#24418;&#30456;&#20851;&#30340;&#34920;&#31034;&#20197;&#29992;&#20110;&#22320;&#24418;&#24863;&#30693;&#23548;&#33322;&#12290;&#36890;&#36807;&#23454;&#29289;&#26426;&#22120;&#20154;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Terrain awareness, i.e., the ability to identify and distinguish different types of terrain, is a critical ability that robots must have to succeed at autonomous off-road navigation. Current approaches that provide robots with this awareness either rely on labeled data which is expensive to collect, engineered features and cost functions that may not generalize, or expert human demonstrations which may not be available. Towards endowing robots with terrain awareness without these limitations, we introduce Self-supervised TErrain Representation LearnING (STERLING), a novel approach for learning terrain representations that relies solely on easy-to-collect, unconstrained (e.g., non-expert), and unlabelled robot experience, with no additional constraints on data collection. STERLING employs a novel multi-modal self-supervision objective through non-contrastive representation learning to learn relevant terrain representations for terrain-aware navigation. Through physical robot experiments
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#24037;&#19994;&#24212;&#29992;&#30340;XAI MLOps&#26550;&#26500;&#65292;&#26088;&#22312;&#35299;&#20915;&#23454;&#38469;&#37096;&#32626;&#21644;&#31649;&#29702;ML&#27169;&#22411;&#20013;&#30340;&#35299;&#37322;&#21644;&#21453;&#39304;&#33021;&#21147;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#29992;&#25143;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2309.12756</link><description>&lt;p&gt;
&#38754;&#21521;&#24037;&#19994;&#24212;&#29992;&#30340;XAI MLOps&#26550;&#26500;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards an MLOps Architecture for XAI in Industrial Applications. (arXiv:2309.12756v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12756
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#24037;&#19994;&#24212;&#29992;&#30340;XAI MLOps&#26550;&#26500;&#65292;&#26088;&#22312;&#35299;&#20915;&#23454;&#38469;&#37096;&#32626;&#21644;&#31649;&#29702;ML&#27169;&#22411;&#20013;&#30340;&#35299;&#37322;&#21644;&#21453;&#39304;&#33021;&#21147;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24050;&#32463;&#25104;&#20026;&#24037;&#19994;&#39046;&#22495;&#20013;&#30340;&#19968;&#31181;&#27969;&#34892;&#24037;&#20855;&#65292;&#23427;&#26377;&#21161;&#20110;&#25913;&#21892;&#25805;&#20316;&#12289;&#22686;&#21152;&#25928;&#29575;&#21644;&#38477;&#20302;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#37096;&#32626;&#21644;&#31649;&#29702;ML&#27169;&#22411;&#21487;&#33021;&#20250;&#24456;&#22797;&#26434;&#12290;&#36825;&#23601;&#26159;&#26426;&#22120;&#23398;&#20064;&#36816;&#33829;&#65288;MLOps&#65289;&#30340;&#20316;&#29992;&#25152;&#22312;&#12290;MLOps&#26088;&#22312;&#31616;&#21270;&#37096;&#32626;&#21644;&#31649;&#29702;&#36807;&#31243;&#12290;&#20854;&#20013;&#19968;&#20010;&#23578;&#26410;&#35299;&#20915;&#30340;MLOps&#25361;&#25112;&#26159;&#23545;&#35299;&#37322;&#30340;&#38656;&#27714;&#12290;&#36825;&#20123;&#35299;&#37322;&#23545;&#20110;&#29702;&#35299;ML&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#23545;&#20110;&#20449;&#20219;&#21644;&#25509;&#21463;&#26159;&#20851;&#38190;&#30340;&#12290;&#26356;&#22909;&#22320;&#35782;&#21035;&#38169;&#35823;&#21644;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#21482;&#26159;&#20854;&#20013;&#30340;&#20004;&#20010;&#32467;&#26524;&#20248;&#21183;&#12290;&#19968;&#20010;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#20107;&#23454;&#26159;&#65292;&#24403;&#20934;&#30830;&#24615;&#65292;&#23588;&#20854;&#26159;&#21487;&#35299;&#37322;&#24615;&#19981;&#28385;&#36275;&#29992;&#25143;&#26399;&#26395;&#26102;&#65292;&#37096;&#32626;&#30340;&#27169;&#22411;&#22312;&#23454;&#36341;&#20013;&#20250;&#34987;&#32469;&#36807;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MLOps&#36719;&#20214;&#26550;&#26500;&#65292;&#20197;&#35299;&#20915;&#23558;&#35299;&#37322;&#21644;&#21453;&#39304;&#33021;&#21147;&#25972;&#21512;&#21040;ML&#24320;&#21457;&#21644;&#37096;&#32626;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;&#22312;&#39033;&#30446;EXPLAIN&#20013;&#65292;&#25105;&#20204;&#30340;&#26550;&#26500;&#34987;&#24212;&#29992;&#20110;&#24037;&#19994;&#24212;&#29992;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) has become a popular tool in the industrial sector as it helps to improve operations, increase efficiency, and reduce costs. However, deploying and managing ML models in production environments can be complex. This is where Machine Learning Operations (MLOps) comes in. MLOps aims to streamline this deployment and management process. One of the remaining MLOps challenges is the need for explanations. These explanations are essential for understanding how ML models reason, which is key to trust and acceptance. Better identification of errors and improved model accuracy are only two resulting advantages. An often neglected fact is that deployed models are bypassed in practice when accuracy and especially explainability do not meet user expectations. We developed a novel MLOps software architecture to address the challenge of integrating explanations and feedback capabilities into the ML development and deployment processes. In the project EXPLAIN, our architecture is
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38899;&#39057;&#23545;&#27604;&#30340;&#24494;&#35843;&#26041;&#27861;&#65288;AudioConFit&#65289;&#65292;&#36890;&#36807;&#20511;&#21161;&#23545;&#27604;&#23398;&#20064;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.11895</link><description>&lt;p&gt;
&#22522;&#20110;&#38899;&#39057;&#23545;&#27604;&#30340;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Audio Contrastive based Fine-tuning. (arXiv:2309.11895v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38899;&#39057;&#23545;&#27604;&#30340;&#24494;&#35843;&#26041;&#27861;&#65288;AudioConFit&#65289;&#65292;&#36890;&#36807;&#20511;&#21161;&#23545;&#27604;&#23398;&#20064;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#20998;&#31867;&#22312;&#35821;&#38899;&#21644;&#22768;&#38899;&#22788;&#29702;&#20219;&#21153;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#22312;&#23558;&#27169;&#22411;&#25311;&#21512;&#21040;&#35757;&#32451;&#25968;&#25454;&#65288;&#36991;&#20813;&#36807;&#25311;&#21512;&#65289;&#24182;&#20351;&#20854;&#33021;&#22815;&#33391;&#22909;&#22320;&#27867;&#21270;&#21040;&#26032;&#39046;&#22495;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#30528;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;&#20511;&#21161;&#23545;&#27604;&#23398;&#20064;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#38899;&#39057;&#23545;&#27604;&#30340;&#24494;&#35843;&#26041;&#27861;&#65288;AudioConFit&#65289;&#65292;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23545;&#21508;&#31181;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#30340;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio classification plays a crucial role in speech and sound processing tasks with a wide range of applications. There still remains a challenge of striking the right balance between fitting the model to the training data (avoiding overfitting) and enabling it to generalise well to a new domain. Leveraging the transferability of contrastive learning, we introduce Audio Contrastive-based Fine-tuning (AudioConFit), an efficient approach characterised by robust generalisability. Empirical experiments on a variety of audio classification tasks demonstrate the effectiveness and robustness of our approach, which achieves state-of-the-art results in various settings.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#20154;&#31867;&#35748;&#30693;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32467;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#35270;&#35273;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#36861;&#27714;&#24863;&#30693;&#21644;&#27010;&#24565;&#22788;&#29702;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#37319;&#29992;&#36845;&#20195;&#12289;&#33258;&#23545;&#27604;&#30340;&#23398;&#20064;&#36807;&#31243;&#26469;&#27169;&#25311;&#20154;&#31867;&#30340;&#35748;&#30693;&#36807;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#32593;&#32476;&#22312;RAVEN&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#27604;&#20043;&#21069;&#25152;&#26377;&#27169;&#22411;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#20351;&#29992;&#20102;&#26368;&#24369;&#30340;&#24402;&#32435;&#20559;&#32622;&#12290;&#25105;&#20204;&#36824;&#25351;&#20986;&#20102;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#31867;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.10532</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#24863;&#30693;&#21644;&#27010;&#24565;&#22788;&#29702;&#30340;&#35748;&#30693;&#21551;&#21457;&#31070;&#32463;&#32467;&#26500;&#29992;&#20110;&#35270;&#35273;&#25277;&#35937;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
A Cognitively-Inspired Neural Architecture for Visual Abstract Reasoning Using Contrastive Perceptual and Conceptual Processing. (arXiv:2309.10532v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10532
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20154;&#31867;&#35748;&#30693;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32467;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#35270;&#35273;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#36861;&#27714;&#24863;&#30693;&#21644;&#27010;&#24565;&#22788;&#29702;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#37319;&#29992;&#36845;&#20195;&#12289;&#33258;&#23545;&#27604;&#30340;&#23398;&#20064;&#36807;&#31243;&#26469;&#27169;&#25311;&#20154;&#31867;&#30340;&#35748;&#30693;&#36807;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#32593;&#32476;&#22312;RAVEN&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#27604;&#20043;&#21069;&#25152;&#26377;&#27169;&#22411;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#20351;&#29992;&#20102;&#26368;&#24369;&#30340;&#24402;&#32435;&#20559;&#32622;&#12290;&#25105;&#20204;&#36824;&#25351;&#20986;&#20102;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#31867;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32467;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#35270;&#35273;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#65292;&#21463;&#20154;&#31867;&#35748;&#30693;&#30340;&#21551;&#21457;&#65292;&#20855;&#20307;&#26469;&#35828;&#26159;&#30001;&#20154;&#31867;&#25277;&#35937;&#25512;&#29702;&#36890;&#24120;&#23558;&#24863;&#30693;&#21644;&#27010;&#24565;&#22788;&#29702;&#20132;&#26367;&#36827;&#34892;&#20316;&#20026;&#28789;&#27963;&#12289;&#36845;&#20195;&#21644;&#21160;&#24577;&#35748;&#30693;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#30340;&#35266;&#23519;&#25152;&#21551;&#21457;&#12290;&#21463;&#27492;&#21407;&#29702;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26550;&#26500;&#23558;&#35270;&#35273;&#25277;&#35937;&#25512;&#29702;&#24314;&#27169;&#20026;&#19968;&#31181;&#36845;&#20195;&#30340;&#12289;&#33258;&#23545;&#27604;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#36861;&#27714;&#35270;&#35273;&#21050;&#28608;&#30340;&#24863;&#30693;&#21644;&#27010;&#24565;&#22788;&#29702;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#36825;&#20010;&#26032;&#30340;&#23545;&#27604;&#24863;&#30693;-&#27010;&#24565;&#32593;&#32476;&#65288;CPCNet&#65289;&#22914;&#20309;&#36890;&#36807;&#27169;&#25311;&#40486;&#25991;&#36827;&#38454;&#30697;&#38453;&#26234;&#21147;&#27979;&#35797;&#30340;&#30697;&#38453;&#25512;&#29702;&#38382;&#39064;&#26469;&#24037;&#20316;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;RAVEN&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;CPCNet&#22312;&#20351;&#29992;&#26368;&#24369;&#30340;&#24402;&#32435;&#20559;&#32622;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#27604;&#20043;&#21069;&#25152;&#26377;&#24050;&#21457;&#34920;&#27169;&#22411;&#26356;&#39640;&#30340;&#31934;&#24230;&#12290;&#25105;&#20204;&#36824;&#25351;&#20986;&#20102;&#21407;&#22987;RAVEN&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#22823;&#37327;&#19988;&#20197;&#21069;&#27809;&#26377;&#34987;&#27880;&#24847;&#21040;&#30340;&#31867;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new neural architecture for solving visual abstract reasoning tasks inspired by human cognition, specifically by observations that human abstract reasoning often interleaves perceptual and conceptual processing as part of a flexible, iterative, and dynamic cognitive process. Inspired by this principle, our architecture models visual abstract reasoning as an iterative, self-contrasting learning process that pursues consistency between perceptual and conceptual processing of visual stimuli. We explain how this new Contrastive Perceptual-Conceptual Network (CPCNet) works using matrix reasoning problems in the style of the well-known Raven's Progressive Matrices intelligence test. Experiments on the machine learning dataset RAVEN show that CPCNet achieves higher accuracy than all previously published models while also using the weakest inductive bias. We also point out a substantial and previously unremarked class imbalance in the original RAVEN dataset, and we propose a new
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#25239;&#21487;&#21345;&#22240;&#25104;&#30270;&#33647;&#29289;&#30740;&#21457;&#20026;&#20363;&#65292;&#23637;&#31034;&#20102;OpenAI&#24320;&#21457;&#30340;ChatGPT&#22312;&#33647;&#29289;&#30740;&#21457;&#39046;&#22495;&#30340;&#21019;&#26032;&#24212;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;GPT-4&#20316;&#20026;&#34394;&#25311;&#23548;&#24341;&#65292;ChatGPT&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#25112;&#30053;&#21644;&#26041;&#27861;&#35770;&#30340;&#35265;&#35299;&#65292;&#20419;&#36827;&#20102;&#21019;&#26032;&#30340;&#33647;&#29289;&#30740;&#21457;&#26041;&#27861;&#12290;&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;AI&#22312;&#29616;&#20195;&#33647;&#29289;&#30740;&#21457;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.06920</link><description>&lt;p&gt;
ChatGPT&#22312;&#33647;&#29289;&#30740;&#21457;&#20013;&#30340;&#24212;&#29992;&#65306;&#20197;&#19982;&#32842;&#22825;&#26426;&#22120;&#20154;&#24320;&#21457;&#25239;&#21487;&#21345;&#22240;&#25104;&#30270;&#33647;&#29289;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
ChatGPT in Drug Discovery: A Case Study on Anti-Cocaine Addiction Drug Development with Chatbots. (arXiv:2308.06920v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#25239;&#21487;&#21345;&#22240;&#25104;&#30270;&#33647;&#29289;&#30740;&#21457;&#20026;&#20363;&#65292;&#23637;&#31034;&#20102;OpenAI&#24320;&#21457;&#30340;ChatGPT&#22312;&#33647;&#29289;&#30740;&#21457;&#39046;&#22495;&#30340;&#21019;&#26032;&#24212;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;GPT-4&#20316;&#20026;&#34394;&#25311;&#23548;&#24341;&#65292;ChatGPT&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#25112;&#30053;&#21644;&#26041;&#27861;&#35770;&#30340;&#35265;&#35299;&#65292;&#20419;&#36827;&#20102;&#21019;&#26032;&#30340;&#33647;&#29289;&#30740;&#21457;&#26041;&#27861;&#12290;&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;AI&#22312;&#29616;&#20195;&#33647;&#29289;&#30740;&#21457;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;OpenAI&#24320;&#21457;&#30340;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#36827;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#23427;&#32473;&#20154;&#24037;&#26234;&#33021;&#24102;&#26469;&#20102;&#26032;&#30340;&#32426;&#20803;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28508;&#22312;&#30340;&#32570;&#38519;&#65292;&#23427;&#22312;&#20005;&#26684;&#30340;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#20316;&#29992;&#23578;&#19981;&#26126;&#30830;&#12290;&#26412;&#25991;&#29983;&#21160;&#23637;&#31034;&#20102;&#23427;&#22312;&#33647;&#29289;&#30740;&#21457;&#39046;&#22495;&#20013;&#30340;&#21019;&#26032;&#24212;&#29992;&#12290;&#30740;&#31350;&#19987;&#27880;&#20110;&#24320;&#21457;&#25239;&#21487;&#21345;&#22240;&#25104;&#30270;&#33647;&#29289;&#65292;&#21033;&#29992;GPT-4&#20316;&#20026;&#34394;&#25311;&#23548;&#24341;&#65292;&#20026;&#30740;&#31350;&#24037;&#20316;&#22312;&#20505;&#36873;&#33647;&#29289;&#30340;&#29983;&#25104;&#27169;&#22411;&#19978;&#25552;&#20379;&#25112;&#30053;&#21644;&#26041;&#27861;&#35770;&#30340;&#35265;&#35299;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#29305;&#24615;&#30340;&#26368;&#20339;&#33647;&#29289;&#26679;&#23376;&#20998;&#23376;&#12290;&#36890;&#36807;&#21033;&#29992;ChatGPT&#30340;&#33021;&#21147;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33647;&#29289;&#30740;&#21457;&#26041;&#27861;&#12290;&#20154;&#24037;&#26234;&#33021;&#19982;&#30740;&#31350;&#20154;&#21592;&#20043;&#38388;&#30340;&#36825;&#31181;&#20849;&#29983;&#20249;&#20276;&#20851;&#31995;&#25913;&#21464;&#20102;&#33647;&#29289;&#30740;&#21457;&#30340;&#26041;&#27861;&#12290;&#32842;&#22825;&#26426;&#22120;&#20154;&#25104;&#20026;&#20419;&#36827;&#32773;&#65292;&#24341;&#23548;&#30740;&#31350;&#20154;&#21592;&#36208;&#21521;&#21019;&#26032;&#30340;&#26041;&#27861;&#21644;&#21019;&#36896;&#26377;&#25928;&#33647;&#29289;&#20505;&#36873;&#30340;&#26377;&#25104;&#25928;&#36884;&#24452;&#12290;&#36825;&#39033;&#30740;&#31350;&#38416;&#26126;&#20102; AI &#22312;&#29616;&#20195;&#33647;&#29289;&#30740;&#21457;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The birth of ChatGPT, a cutting-edge language model-based chatbot developed by OpenAI, ushered in a new era in AI. However, due to potential pitfalls, its role in rigorous scientific research is not clear yet. This paper vividly showcases its innovative application within the field of drug discovery. Focused specifically on developing anti-cocaine addiction drugs, the study employs GPT-4 as a virtual guide, offering strategic and methodological insights to researchers working on generative models for drug candidates. The primary objective is to generate optimal drug-like molecules with desired properties. By leveraging the capabilities of ChatGPT, the study introduces a novel approach to the drug discovery process. This symbiotic partnership between AI and researchers transforms how drug development is approached. Chatbots become facilitators, steering researchers towards innovative methodologies and productive paths for creating effective drug candidates. This research sheds light on 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21338;&#24328;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#35268;&#21010;&#21644;&#39044;&#27979;&#65292;&#20197;&#25913;&#36827;&#26426;&#22120;&#20154;&#21160;&#20316;&#30340;&#23433;&#20840;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#28216;&#25103;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#23398;&#20064;&#39044;&#27979;&#20154;&#31867;&#38450;&#33539;&#23545;&#31574;&#65292;&#24182;&#36890;&#36807;&#23454;&#38469;&#31639;&#27861;&#23454;&#29616;&#20102;&#31471;&#21040;&#31471;&#30340;&#27169;&#22411;&#35757;&#32451;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;&#20154;&#32676;&#23548;&#33322;&#21644;&#29616;&#23454;&#19990;&#30028;&#30340;&#34892;&#20154;&#36816;&#21160;&#25968;&#25454;&#38598;&#20013;&#21487;&#20197;&#20135;&#29983;&#26356;&#23433;&#20840;&#30340;&#35268;&#21010;&#12290;</title><link>http://arxiv.org/abs/2308.06137</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#32852;&#21512;&#39044;&#27979;&#21644;&#35268;&#21010;&#30340;&#21338;&#24328;&#35770;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Game-Theoretic Framework for Joint Forecasting and Planning. (arXiv:2308.06137v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21338;&#24328;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#35268;&#21010;&#21644;&#39044;&#27979;&#65292;&#20197;&#25913;&#36827;&#26426;&#22120;&#20154;&#21160;&#20316;&#30340;&#23433;&#20840;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#28216;&#25103;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#23398;&#20064;&#39044;&#27979;&#20154;&#31867;&#38450;&#33539;&#23545;&#31574;&#65292;&#24182;&#36890;&#36807;&#23454;&#38469;&#31639;&#27861;&#23454;&#29616;&#20102;&#31471;&#21040;&#31471;&#30340;&#27169;&#22411;&#35757;&#32451;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;&#20154;&#32676;&#23548;&#33322;&#21644;&#29616;&#23454;&#19990;&#30028;&#30340;&#34892;&#20154;&#36816;&#21160;&#25968;&#25454;&#38598;&#20013;&#21487;&#20197;&#20135;&#29983;&#26356;&#23433;&#20840;&#30340;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23384;&#22312;&#20154;&#30340;&#24773;&#20917;&#19979;&#65292;&#23433;&#20840;&#35268;&#21010;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#38656;&#35201;&#21487;&#38752;&#30340;&#23545;&#26410;&#26469;&#20154;&#31867;&#36816;&#21160;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#31616;&#21333;&#22320;&#39044;&#27979;&#21069;&#26399;&#20132;&#20114;&#20013;&#26368;&#21487;&#33021;&#30340;&#21160;&#20316;&#24182;&#19981;&#33021;&#20445;&#35777;&#23433;&#20840;&#12290;&#36825;&#26679;&#30340;&#39044;&#27979;&#26410;&#33021;&#27169;&#25311;&#20986;&#21487;&#33021;&#20107;&#20214;&#30340;&#38271;&#23614;&#37096;&#20998;&#65292;&#36825;&#20123;&#20107;&#20214;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#38598;&#20013;&#24456;&#23569;&#34987;&#35266;&#23519;&#21040;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20026;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#21160;&#20316;&#36827;&#34892;&#35268;&#21010;&#20250;&#23548;&#33268;&#36807;&#20998;&#20445;&#23432;&#30340;&#34892;&#20026;&#21644;"&#20725;&#21270;&#30340;&#26426;&#22120;&#20154;"&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#39044;&#27979;&#20154;&#31867;&#38450;&#33539;&#23545;&#31574;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#28216;&#25103;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#35268;&#21010;&#21644;&#39044;&#27979;&#65292;&#20197;&#35268;&#21010;&#22120;&#19982;&#28436;&#31034;&#32773;&#20043;&#38388;&#30340;&#32489;&#25928;&#20316;&#20026;&#25910;&#30410;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#38469;&#31639;&#27861;&#65292;&#20197;&#31471;&#23545;&#31471;&#30340;&#26041;&#24335;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#20154;&#32676;&#23548;&#33322;&#27169;&#25311;&#22120;&#21644;&#29616;&#23454;&#19990;&#30028;&#30340;&#34892;&#20154;&#36816;&#21160;&#25968;&#25454;&#38598;&#20013;&#20135;&#29983;&#20102;&#26356;&#23433;&#20840;&#30340;&#35745;&#21010;&#12290;&#25105;&#20204;&#23558;&#20195;&#30721;&#21457;&#24067;&#22312;https://github.com/portal-cornell/Game-Theoretic-Forecasting-Planning&#12290;
&lt;/p&gt;
&lt;p&gt;
Planning safe robot motions in the presence of humans requires reliable forecasts of future human motion. However, simply predicting the most likely motion from prior interactions does not guarantee safety. Such forecasts fail to model the long tail of possible events, which are rarely observed in limited datasets. On the other hand, planning for worst-case motions leads to overtly conservative behavior and a ``frozen robot''. Instead, we aim to learn forecasts that predict counterfactuals that humans guard against. We propose a novel game-theoretic framework for joint planning and forecasting with the payoff being the performance of the planner against the demonstrator, and present practical algorithms to train models in an end-to-end fashion. We demonstrate that our proposed algorithm results in safer plans in a crowd navigation simulator and real-world datasets of pedestrian motion. We release our code at https://github.com/portal-cornell/Game-Theoretic-Forecasting-Planning.
&lt;/p&gt;</description></item><item><title>&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#22312;&#19968;&#31687;&#31038;&#35770;&#20013;&#21628;&#21505;&#25105;&#20204;&#8220;&#20572;&#27490;&#35848;&#35770;&#26126;&#22825;&#30340;AI&#26411;&#26085;&#65292;&#32780;AI&#20170;&#22825;&#23601;&#23384;&#22312;&#39118;&#38505;&#12290;&#8221;&#36825;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#20005;&#37325;&#30340;&#21028;&#26029;&#22833;&#35823;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26377;&#24433;&#21709;&#21147;&#30340;&#34892;&#21160;&#32773;&#26469;&#35828;&#65292;&#22240;&#20026;&#25105;&#20204;&#26399;&#26395;&#20182;&#20204;&#33021;&#22815;&#32771;&#34385;&#21040;&#38169;&#35823;&#30340;&#21518;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.04440</link><description>&lt;p&gt;
&#33258;&#28982;&#21644;&#26426;&#22120;
&lt;/p&gt;
&lt;p&gt;
Nature and the Machines. (arXiv:2308.04440v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04440
&lt;/p&gt;
&lt;p&gt;
&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#22312;&#19968;&#31687;&#31038;&#35770;&#20013;&#21628;&#21505;&#25105;&#20204;&#8220;&#20572;&#27490;&#35848;&#35770;&#26126;&#22825;&#30340;AI&#26411;&#26085;&#65292;&#32780;AI&#20170;&#22825;&#23601;&#23384;&#22312;&#39118;&#38505;&#12290;&#8221;&#36825;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#20005;&#37325;&#30340;&#21028;&#26029;&#22833;&#35823;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26377;&#24433;&#21709;&#21147;&#30340;&#34892;&#21160;&#32773;&#26469;&#35828;&#65292;&#22240;&#20026;&#25105;&#20204;&#26399;&#26395;&#20182;&#20204;&#33021;&#22815;&#32771;&#34385;&#21040;&#38169;&#35823;&#30340;&#21518;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;(AI)&#26159;&#21542;&#23545;&#20154;&#31867;&#26500;&#25104;&#23384;&#22312;&#21361;&#38505;&#65311;&#19968;&#20123;&#25209;&#35780;&#23478;&#35748;&#20026;&#36825;&#20010;&#38382;&#39064;&#27491;&#22312;&#21463;&#21040;&#36807;&#22810;&#30340;&#20851;&#27880;&#65292;&#20182;&#20204;&#24076;&#26395;&#23558;&#20854;&#25512;&#21040;&#19968;&#36793;&#65292;&#36716;&#32780;&#35752;&#35770;AI&#30340;&#21363;&#26102;&#39118;&#38505;&#12290;&#36825;&#20123;&#25209;&#35780;&#23478;&#29616;&#22312;&#21253;&#25324;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#65292;&#22312;&#26368;&#36817;&#30340;&#19968;&#31687;&#31038;&#35770;&#20013;&#25958;&#20419;&#25105;&#20204;&#8220;&#20572;&#27490;&#35848;&#35770;&#26126;&#22825;&#30340;AI&#26411;&#26085;&#65292;&#32780;AI&#20170;&#22825;&#23601;&#23384;&#22312;&#39118;&#38505;&#12290;&#8221;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#19968;&#31181;&#20005;&#37325;&#30340;&#21028;&#26029;&#22833;&#35823;&#65292;&#23545;&#20110;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#26469;&#35828;&#12290;&#22312;&#31185;&#23398;&#39046;&#22495;&#65292;&#23601;&#20687;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#19968;&#26679;&#65292;&#25105;&#20204;&#24076;&#26395;&#26377;&#24433;&#21709;&#21147;&#30340;&#34892;&#20026;&#32773;&#33021;&#22815;&#32771;&#34385;&#38169;&#35823;&#30340;&#21518;&#26524;&#12290;&#20316;&#20026;&#19990;&#30028;&#39046;&#20808;&#30340;&#31185;&#23398;&#26399;&#21002;&#65292;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#26080;&#30097;&#26159;&#19968;&#20010;&#26377;&#24433;&#21709;&#21147;&#30340;&#34892;&#20026;&#32773;&#65292;&#29305;&#21035;&#26159;&#22312;&#32570;&#20047;&#20581;&#20840;&#20840;&#29699;AI&#30417;&#31649;&#30340;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#36825;&#20010;&#26696;&#20363;&#20013;&#26126;&#26174;&#26410;&#33021;&#32771;&#34385;&#21040;&#38169;&#35823;&#30340;&#20195;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Does artificial intelligence (AI) pose existential risks to humanity? Some critics feel this question is getting too much attention, and want to push it aside in favour of conversations about the immediate risks of AI. These critics now include the journal Nature, where a recent editorial urges us to 'stop talking about tomorrow's AI doomsday when AI poses risks today.' We argue that this is a serious failure of judgement, on Nature's part. In science, as in everyday life, we expect influential actors to consider the consequences of error. As the world's leading scientific journal, Nature is certainly an influential actor, especially so in the absence of robust global regulation of AI. Yet it has manifestly failed to consider the cost of error in this case.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38477;&#32500;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#24212;&#29992;&#39532;&#27663;&#36317;&#31163;&#21518;&#22788;&#29702;&#21644;&#20027;&#25104;&#20998;&#20998;&#26512;&#65292;&#23454;&#29616;&#23545;&#36234;&#30028;&#22270;&#20687;&#30340;&#39640;&#25928;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.03723</link><description>&lt;p&gt;
&#25552;&#39640;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#36234;&#30028;&#26816;&#27979;&#30340;&#38477;&#32500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dimensionality Reduction for Improving Out-of-Distribution Detection in Medical Image Segmentation. (arXiv:2308.03723v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38477;&#32500;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#24212;&#29992;&#39532;&#27663;&#36317;&#31163;&#21518;&#22788;&#29702;&#21644;&#20027;&#25104;&#20998;&#20998;&#26512;&#65292;&#23454;&#29616;&#23545;&#36234;&#30028;&#22270;&#20687;&#30340;&#39640;&#25928;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#30693;&#20020;&#24202;&#24212;&#29992;&#30340;&#20998;&#21106;&#27169;&#22411;&#22312;&#20854;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#30340;&#25968;&#25454;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#34920;&#29616;&#33391;&#22909;&#65292;&#22240;&#27492;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#26816;&#27979;&#36234;&#30028;&#22270;&#20687;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20197;&#38450;&#27490;&#33258;&#21160;&#21270;&#20559;&#24046;&#12290;&#26412;&#30740;&#31350;&#23558;&#39532;&#27663;&#36317;&#31163;&#21518;&#22788;&#29702;&#24212;&#29992;&#20110;Swin UNETR&#27169;&#22411;&#30340;&#29942;&#39048;&#29305;&#24449;&#65292;&#35813;&#27169;&#22411;&#29992;&#20110;&#23545;T1&#21152;&#26435;&#30913;&#20849;&#25391;&#25104;&#20687;&#19978;&#30340;&#32925;&#33039;&#36827;&#34892;&#20998;&#21106;&#12290;&#36890;&#36807;&#20351;&#29992;&#20027;&#25104;&#20998;&#20998;&#26512;&#38477;&#20302;&#29942;&#39048;&#29305;&#24449;&#30340;&#32500;&#25968;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#26816;&#27979;&#21040;&#36234;&#30028;&#22270;&#20687;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#23567;&#30340;&#35745;&#31639;&#36127;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinically deployed segmentation models are known to fail on data outside of their training distribution. As these models perform well on most cases, it is imperative to detect out-of-distribution (OOD) images at inference to protect against automation bias. This work applies the Mahalanobis distance post hoc to the bottleneck features of a Swin UNETR model that segments the liver on T1-weighted magnetic resonance imaging. By reducing the dimensions of the bottleneck features with principal component analysis, OOD images were detected with high performance and minimal computational load.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35814;&#32454;&#30740;&#31350;&#20102;&#20197;&#20351;&#29992;&#21069;&#21021;&#22987;&#21270;&#38169;&#35823;&#20026;&#26696;&#20363;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#38745;&#24577;&#20998;&#26512;&#30340;&#24320;&#25918;&#31354;&#38388;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#20840;&#33258;&#21160;&#20195;&#29702;&#31243;&#24207;LLift&#65292;&#35813;&#31243;&#24207;&#33021;&#22815;&#20811;&#26381;&#22810;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#38169;&#35823;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2308.00245</link><description>&lt;p&gt;
&#31243;&#24207;&#20998;&#26512;&#25351;&#21335;&#65306;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20849;&#36827;&#20043;&#26053;
&lt;/p&gt;
&lt;p&gt;
The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models. (arXiv:2308.00245v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35814;&#32454;&#30740;&#31350;&#20102;&#20197;&#20351;&#29992;&#21069;&#21021;&#22987;&#21270;&#38169;&#35823;&#20026;&#26696;&#20363;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#38745;&#24577;&#20998;&#26512;&#30340;&#24320;&#25918;&#31354;&#38388;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#20840;&#33258;&#21160;&#20195;&#29702;&#31243;&#24207;LLift&#65292;&#35813;&#31243;&#24207;&#33021;&#22815;&#20811;&#26381;&#22810;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#38169;&#35823;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38745;&#24577;&#20998;&#26512;&#26159;&#36719;&#20214;&#24037;&#31243;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#19968;&#31181;&#25216;&#26415;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#20943;&#36731;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#22312;&#31934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#23454;&#29616;&#24494;&#22937;&#24179;&#34913;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38556;&#30861;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#30001;&#20110;&#26368;&#36817;&#30340;&#36827;&#23637;&#22312;&#29702;&#35299;&#12289;&#29983;&#25104;&#29978;&#33267;&#35843;&#35797;&#20195;&#30721;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#38169;&#35823;&#30340;&#36923;&#36753;&#21487;&#33021;&#24456;&#22797;&#26434;&#65292;&#38656;&#35201;&#22797;&#26434;&#30340;&#25512;&#29702;&#21644;&#36328;&#22810;&#20010;&#20989;&#25968;&#30340;&#22823;&#33539;&#22260;&#20998;&#26512;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#19968;&#28857;&#19978;&#65292;LLM&#26356;&#36866;&#21512;&#22312;&#36741;&#21161;&#35282;&#33394;&#20013;&#19982;&#38745;&#24577;&#20998;&#26512;&#30456;&#34917;&#20805;&#20351;&#29992;&#12290;&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#20197;&#20351;&#29992;&#21069;&#21021;&#22987;&#21270;&#65288;UBI&#65289;&#38169;&#35823;&#20026;&#26696;&#20363;&#30740;&#31350;&#30340;LLM&#36741;&#21161;&#38745;&#24577;&#20998;&#26512;&#30340;&#24320;&#25918;&#31354;&#38388;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;LLift&#65292;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#20195;&#29702;&#31243;&#24207;&#65292;&#21487;&#20197;&#19982;&#38745;&#24577;&#20998;&#26512;&#24037;&#20855;&#21644;LLM&#36827;&#34892;&#20132;&#20114;&#12290;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#20195;&#29702;&#31243;&#24207;&#21644;&#25552;&#31034;&#65292;&#25105;&#20204;&#33021;&#22815;&#20811;&#26381;&#35768;&#22810;&#25361;&#25112;&#65292;&#21253;&#25324;&#38024;&#23545;&#20855;&#20307;&#38169;&#35823;&#30340;&#24314;&#27169;&#65292;
&lt;/p&gt;
&lt;p&gt;
Static analysis is a widely used technique in software engineering for identifying and mitigating bugs. However, a significant hurdle lies in achieving a delicate balance between precision and scalability. Large Language Models (LLMs) offer a promising alternative, as recent advances demonstrate remarkable capabilities in comprehending, generating, and even debugging code. Yet, the logic of bugs can be complex and require sophisticated reasoning and a large analysis scope spanning multiple functions. Therefore, at this point, LLMs are better used in an assistive role to complement static analysis. In this paper, we take a deep dive into the open space of LLM-assisted static analysis, using use-before-initialization (UBI) bugs as a case study. To this end, we develop LLift, a fully automated agent that interfaces with both a static analysis tool and an LLM. By carefully designing the agent and the prompts, we are able to overcome a number of challenges, including bug-specific modeling, 
&lt;/p&gt;</description></item><item><title>EmotionPrompt&#26159;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24773;&#24863;&#21050;&#28608;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#39033;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21516;&#26102;&#25913;&#21892;&#20102;&#20854;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.11760</link><description>&lt;p&gt;
EmotionPrompt: &#36890;&#36807;&#24773;&#24863;&#21050;&#28608;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#24515;&#29702;&#23398;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus. (arXiv:2307.11760v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11760
&lt;/p&gt;
&lt;p&gt;
EmotionPrompt&#26159;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24773;&#24863;&#21050;&#28608;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#39033;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21516;&#26102;&#25913;&#21892;&#20102;&#20854;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#12289;&#35821;&#35328;&#29702;&#35299;&#21644;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#31561;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#24182;&#34987;&#35270;&#20026;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;LLMs&#23545;&#25552;&#31034;&#30340;&#25935;&#24863;&#24615;&#20173;&#28982;&#26159;&#20854;&#26085;&#24120;&#24212;&#29992;&#30340;&#20027;&#35201;&#29942;&#39048;&#12290;&#26412;&#25991;&#20174;&#24515;&#29702;&#23398;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#25552;&#20986;&#20102;EmotionPrompt&#26469;&#25506;&#32034;&#24773;&#24863;&#26234;&#33021;&#20197;&#25552;&#21319;LLMs&#30340;&#24615;&#33021;&#12290;EmotionPrompt&#22522;&#20110;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#26126;&#20102;&#30340;&#21407;&#21017;&#65306;&#23558;&#24773;&#24863;&#21050;&#28608;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30456;&#21516;&#30340;&#21333;&#19968;&#25552;&#31034;&#27169;&#26495;&#19978;&#65292;&#19982;&#21407;&#22987;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#21644;Zero-shot-CoT&#30456;&#27604;&#65292;&#22312;8&#20010;&#20219;&#21153;&#19978;&#37117;&#26174;&#33879;&#20248;&#20110;&#22810;&#31181;&#27169;&#22411;&#65306;ChatGPT&#12289;Vicuna-13b&#12289;Bloom&#21644;T5&#12290;&#27492;&#22806;&#65292;&#35266;&#23519;&#21040;EmotionPrompt&#33021;&#22815;&#25552;&#39640;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#30456;&#20449;EmotionPrompt&#20026;&#25506;&#32034;&#36328;&#23398;&#31185;&#30693;&#35782;&#24320;&#36767;&#20102;&#19968;&#26465;&#26032;&#30340;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved significant performance in many fields such as reasoning, language understanding, and math problem-solving, and are regarded as a crucial step to artificial general intelligence (AGI). However, the sensitivity of LLMs to prompts remains a major bottleneck for their daily adoption. In this paper, we take inspiration from psychology and propose EmotionPrompt to explore emotional intelligence to enhance the performance of LLMs. EmotionPrompt operates on a remarkably straightforward principle: the incorporation of emotional stimulus into prompts. Experimental results demonstrate that our \method, using the same single prompt templates, significantly outperforms original zero-shot prompt and Zero-shot-CoT on 8 tasks with diverse models: ChatGPT, Vicuna-13b, Bloom, and T5. Further, EmotionPrompt was observed to improve both truthfulness and informativeness. We believe that EmotionPrompt heralds a novel avenue for exploring interdisciplinary knowledg
&lt;/p&gt;</description></item><item><title>Jina Embeddings&#26159;&#19968;&#32452;&#39640;&#24615;&#33021;&#30340;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#25991;&#26412;&#30340;&#35821;&#20041;&#26412;&#36136;&#12290;&#35813;&#35770;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;Jina Embeddings&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#24615;&#33021;&#35780;&#20272;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11224</link><description>&lt;p&gt;
Jina Embeddings:&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#24615;&#33021;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models. (arXiv:2307.11224v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11224
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings&#26159;&#19968;&#32452;&#39640;&#24615;&#33021;&#30340;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#25991;&#26412;&#30340;&#35821;&#20041;&#26412;&#36136;&#12290;&#35813;&#35770;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;Jina Embeddings&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#24615;&#33021;&#35780;&#20272;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Jina Embeddings&#30001;&#19968;&#32452;&#39640;&#24615;&#33021;&#30340;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#32452;&#25104;&#65292;&#33021;&#22815;&#23558;&#21508;&#31181;&#25991;&#26412;&#36755;&#20837;&#36716;&#21270;&#20026;&#25968;&#20540;&#34920;&#31034;&#65292;&#20174;&#32780;&#25429;&#25417;&#25991;&#26412;&#30340;&#35821;&#20041;&#26412;&#36136;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#24182;&#38750;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#65292;&#20294;&#22312;&#23494;&#38598;&#26816;&#32034;&#21644;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#31561;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;Jina Embeddings&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#20174;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#25104;&#23545;&#21644;&#19977;&#20803;&#25968;&#25454;&#38598;&#24320;&#22987;&#12290;&#23427;&#24378;&#35843;&#20102;&#25968;&#25454;&#28165;&#29702;&#22312;&#25968;&#25454;&#38598;&#20934;&#22791;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#23545;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#35752;&#65292;&#26368;&#21518;&#21033;&#29992;Massive Textual Embedding Benchmark&#65288;MTEB&#65289;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings constitutes a set of high-performance sentence embedding models adept at translating various textual inputs into numerical representations, thereby capturing the semantic essence of the text. While these models are not exclusively designed for text generation, they excel in applications such as dense retrieval and semantic textual similarity. This paper details the development of Jina Embeddings, starting with the creation of a high-quality pairwise and triplet dataset. It underlines the crucial role of data cleaning in dataset preparation, gives in-depth insights into the model training process, and concludes with a comprehensive performance evaluation using the Massive Textual Embedding Benchmark (MTEB).
&lt;/p&gt;</description></item><item><title>&#24377;&#24615;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;EDT&#65289;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#38388;&#36827;&#34892;&#21160;&#20316;&#25512;&#26029;&#26102;&#35843;&#25972;&#21382;&#21490;&#38271;&#24230;&#26469;&#23454;&#29616;&#36712;&#36857;&#25340;&#25509;&#65292;&#22635;&#34917;&#20102;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;DT&#65289;&#22312;&#36825;&#19968;&#26041;&#38754;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#19988;&#22312;&#22810;&#20219;&#21153;&#24773;&#20917;&#19979;&#32988;&#36807;&#22522;&#20110;Q-Learning&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.02484</link><description>&lt;p&gt;
&#24377;&#24615;&#20915;&#31574;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Elastic Decision Transformer. (arXiv:2307.02484v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02484
&lt;/p&gt;
&lt;p&gt;
&#24377;&#24615;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;EDT&#65289;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#38388;&#36827;&#34892;&#21160;&#20316;&#25512;&#26029;&#26102;&#35843;&#25972;&#21382;&#21490;&#38271;&#24230;&#26469;&#23454;&#29616;&#36712;&#36857;&#25340;&#25509;&#65292;&#22635;&#34917;&#20102;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;DT&#65289;&#22312;&#36825;&#19968;&#26041;&#38754;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#19988;&#22312;&#22810;&#20219;&#21153;&#24773;&#20917;&#19979;&#32988;&#36807;&#22522;&#20110;Q-Learning&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#24377;&#24615;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;EDT&#65289;&#65292;&#23427;&#26159;&#29616;&#26377;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;DT&#65289;&#21450;&#20854;&#21464;&#20307;&#30340;&#37325;&#22823;&#36827;&#23637;&#12290;&#23613;&#31649;DT&#22768;&#31216;&#33021;&#22815;&#29983;&#25104;&#26368;&#20339;&#36712;&#36857;&#65292;&#20294;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#23427;&#22312;&#36712;&#36857;&#25340;&#25509;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36712;&#36857;&#25340;&#25509;&#26159;&#25351;&#20174;&#19968;&#32452;&#27425;&#20248;&#36712;&#36857;&#20013;&#29983;&#25104;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#36712;&#36857;&#30340;&#36807;&#31243;&#12290;&#25552;&#20986;&#30340;EDT&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#38388;&#36827;&#34892;&#21160;&#20316;&#25512;&#26029;&#26102;&#35843;&#25972;DT&#20013;&#32500;&#25252;&#30340;&#21382;&#21490;&#38271;&#24230;&#26469;&#23454;&#29616;&#36712;&#36857;&#25340;&#25509;&#65292;&#20174;&#32780;&#20351;&#33258;&#24049;&#19982;&#20247;&#19981;&#21516;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#36712;&#36857;&#26159;&#26368;&#20248;&#30340;&#26102;&#20505;&#65292;EDT&#36890;&#36807;&#20445;&#25345;&#36739;&#38271;&#30340;&#21382;&#21490;&#65292;&#24403;&#24403;&#21069;&#36712;&#36857;&#26159;&#27425;&#20248;&#30340;&#26102;&#20505;&#65292;EDT&#36890;&#36807;&#20445;&#25345;&#36739;&#30701;&#30340;&#21382;&#21490;&#26469;&#20248;&#21270;&#36712;&#36857;&#65292;&#20351;&#20854;&#33021;&#22815;&#19982;&#26356;&#20248;&#30340;&#36712;&#36857;&#36827;&#34892;&#8220;&#25340;&#25509;&#8221;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;EDT&#33021;&#22815;&#22635;&#34917;&#22522;&#20110;DT&#21644;&#22522;&#20110;Q-Learning&#26041;&#27861;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#29305;&#21035;&#26159;&#65292;EDT&#22312;&#22810;&#20219;&#21153;&#24773;&#20917;&#19979;&#32988;&#36807;&#22522;&#20110;Q-Learning&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Elastic Decision Transformer (EDT), a significant advancement over the existing Decision Transformer (DT) and its variants. Although DT purports to generate an optimal trajectory, empirical evidence suggests it struggles with trajectory stitching, a process involving the generation of an optimal or near-optimal trajectory from the best parts of a set of sub-optimal trajectories. The proposed EDT differentiates itself by facilitating trajectory stitching during action inference at test time, achieved by adjusting the history length maintained in DT. Further, the EDT optimizes the trajectory by retaining a longer history when the previous trajectory is optimal and a shorter one when it is sub-optimal, enabling it to "stitch" with a more optimal trajectory. Extensive experimentation demonstrates EDT's ability to bridge the performance gap between DT-based and Q Learning-based approaches. In particular, the EDT outperforms Q Learning-based methods in a multi-task regi
&lt;/p&gt;</description></item><item><title>Act3D&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#25805;&#20316;&#31574;&#30053;&#65292;&#23558;6&#33258;&#30001;&#24230;&#20851;&#38190;&#23039;&#21183;&#39044;&#27979;&#20316;&#20026;3D&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#20197;&#33258;&#36866;&#24212;&#31354;&#38388;&#35745;&#31639;&#30340;&#26041;&#24335;&#36827;&#34892;&#22788;&#29702;&#12290;&#23427;&#22312;&#39640;&#24230;&#31934;&#30830;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.17817</link><description>&lt;p&gt;
Act3D&#65306;&#26080;&#38480;&#20998;&#36776;&#29575;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#26816;&#27979;Transformer
&lt;/p&gt;
&lt;p&gt;
Act3D: Infinite Resolution Action Detection Transformer for Robotic Manipulation. (arXiv:2306.17817v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17817
&lt;/p&gt;
&lt;p&gt;
Act3D&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#25805;&#20316;&#31574;&#30053;&#65292;&#23558;6&#33258;&#30001;&#24230;&#20851;&#38190;&#23039;&#21183;&#39044;&#27979;&#20316;&#20026;3D&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#20197;&#33258;&#36866;&#24212;&#31354;&#38388;&#35745;&#31639;&#30340;&#26041;&#24335;&#36827;&#34892;&#22788;&#29702;&#12290;&#23427;&#22312;&#39640;&#24230;&#31934;&#30830;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#24863;&#30693;&#34920;&#24449;&#38750;&#24120;&#36866;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#32437;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#36731;&#26494;&#32534;&#30721;&#36974;&#25377;&#24773;&#20917;&#24182;&#31616;&#21270;&#31354;&#38388;&#25512;&#29702;&#12290;&#35768;&#22810;&#25805;&#32437;&#20219;&#21153;&#38656;&#35201;&#23545;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#21183;&#39044;&#27979;&#36827;&#34892;&#39640;&#31354;&#38388;&#31934;&#24230;&#65292;&#36890;&#24120;&#38656;&#35201;&#39640;&#20998;&#36776;&#29575;&#30340;3D&#24863;&#30693;&#32593;&#26684;&#36827;&#34892;&#35745;&#31639;&#65292;&#36825;&#22312;&#22788;&#29702;&#19978;&#38750;&#24120;&#32791;&#26102;&#12290;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968;&#25805;&#20316;&#31574;&#30053;&#30452;&#25509;&#22312;2D&#20013;&#36816;&#20316;&#65292;&#25918;&#24323;&#20102;3D&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Act3D&#65292;&#19968;&#31181;&#23558;6&#33258;&#30001;&#24230;&#20851;&#38190;&#23039;&#21183;&#39044;&#27979;&#35270;&#20026;&#33258;&#36866;&#24212;&#31354;&#38388;&#35745;&#31639;&#30340;&#25805;&#20316;&#31574;&#30053;Transformer&#12290;&#23427;&#20197;&#19968;&#20010;&#25110;&#22810;&#20010;&#25668;&#20687;&#26426;&#35270;&#22270;&#30340;&#26410;&#25237;&#24433;3D&#29305;&#24449;&#20113;&#20316;&#20026;&#36755;&#20837;&#65292;&#20197;&#31895;-&#31934;&#26041;&#24335;&#22312;&#33258;&#30001;&#31354;&#38388;&#20013;&#36845;&#20195;&#37319;&#26679;3D&#28857;&#32593;&#26684;&#65292;&#20351;&#29992;&#30456;&#23545;&#31354;&#38388;&#27880;&#24847;&#21147;&#23558;&#20854;&#29305;&#24449;&#21270;&#20026;&#29289;&#29702;&#29305;&#24449;&#20113;&#65292;&#24182;&#36873;&#25321;&#26368;&#20339;&#29305;&#24449;&#28857;&#36827;&#34892;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#21183;&#39044;&#27979;&#12290;Act3D&#22312;&#24050;&#24314;&#31435;&#30340;&#25805;&#32437;&#22522;&#20934;RLbench&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#22909;&#25104;&#32489;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35813;&#22522;&#20934;&#20013;&#23454;&#29616;&#20102;10%&#30340;&#32477;&#23545;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D perceptual representations are well suited for robot manipulation as they easily encode occlusions and simplify spatial reasoning. Many manipulation tasks require high spatial precision in end-effector pose prediction, typically demanding high-resolution 3D perceptual grids that are computationally expensive to process. As a result, most manipulation policies operate directly in 2D, foregoing 3D inductive biases. In this paper, we propose Act3D, a manipulation policy Transformer that casts 6-DoF keypose prediction as 3D detection with adaptive spatial computation. It takes as input 3D feature clouds unprojected from one or more camera views, iteratively samples 3D point grids in free space in a coarse-to-fine manner, featurizes them using relative spatial attention to the physical feature cloud, and selects the best feature point for end-effector pose prediction. Act3D sets a new state-of-the-art in RLbench, an established manipulation benchmark. Our model achieves 10% absolute impr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Implicit Interactive Fleet Learning (IIFL)&#65292;&#23558;&#38544;&#24615;&#31574;&#30053;&#25512;&#24191;&#21040;&#20132;&#20114;&#24335;&#27169;&#20223;&#23398;&#20064;&#20013;&#65292;&#33021;&#22815;&#35299;&#20915;&#22810;&#27169;&#24577;&#21644;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.15228</link><description>&lt;p&gt;
IIFL: &#38750;&#21516;&#36136;&#20154;&#31867;&#30417;&#30563;&#21592;&#30340;&#38544;&#24615;&#20114;&#21160;&#36710;&#38431;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
IIFL: Implicit Interactive Fleet Learning from Heterogeneous Human Supervisors. (arXiv:2306.15228v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Implicit Interactive Fleet Learning (IIFL)&#65292;&#23558;&#38544;&#24615;&#31574;&#30053;&#25512;&#24191;&#21040;&#20132;&#20114;&#24335;&#27169;&#20223;&#23398;&#20064;&#20013;&#65292;&#33021;&#22815;&#35299;&#20915;&#22810;&#27169;&#24577;&#21644;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#24050;&#32463;&#24212;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#20294;&#22312;&#20197;&#19979;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#36935;&#21040;&#22256;&#38590;&#65306;&#65288;1&#65289;&#26426;&#22120;&#20154;&#36935;&#21040;&#35757;&#32451;&#25968;&#25454;&#20013;&#27809;&#26377;&#20195;&#34920;&#30340;&#36793;&#32536;&#26696;&#20363;&#65288;&#20998;&#24067;&#36716;&#31227;&#65289;&#25110;&#65288;2&#65289;&#20154;&#31867;&#28436;&#31034;&#26159;&#24322;&#36136;&#30340;&#65306;&#20363;&#22914;&#65292;&#22312;&#38556;&#30861;&#29289;&#21608;&#22260;&#37319;&#21462;&#19981;&#21516;&#36335;&#24452;&#65288;&#22810;&#27169;&#24577;&#65289;&#12290;&#20132;&#20114;&#24335;&#36710;&#38431;&#23398;&#20064;&#65288;IFL&#65289;&#36890;&#36807;&#20801;&#35768;&#26426;&#22120;&#20154;&#22312;&#20219;&#21153;&#25191;&#34892;&#36807;&#31243;&#20013;&#35775;&#38382;&#36828;&#31243;&#20154;&#31867;&#36828;&#31243;&#25805;&#20316;&#21592;&#24182;&#20174;&#20182;&#20204;&#37027;&#37324;&#23398;&#20064;&#26469;&#20943;&#36731;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#65292;&#20294;&#19981;&#33021;&#22788;&#29702;&#22810;&#27169;&#24577;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#38544;&#24615;&#34892;&#20026;&#20811;&#38534;&#65288;IBC&#65289;&#65292;&#23427;&#33021;&#22815;&#20351;&#29992;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBMs&#65289;&#34920;&#31034;&#22810;&#27169;&#24577;&#28436;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#29992;&#38544;&#24615;&#20114;&#21160;&#36710;&#38431;&#23398;&#20064;&#65288;IIFL&#65289;&#35299;&#20915;&#22810;&#27169;&#24577;&#21644;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#65292;&#36825;&#26159;&#38544;&#24615;&#31574;&#30053;&#22312;&#20132;&#20114;&#24335;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#31532;&#19968;&#20010;&#25193;&#23637;&#65288;&#21253;&#25324;&#21333;&#26426;&#22120;&#20154;&#12289;&#21333;&#20154;&#31867;&#30340;&#35774;&#32622;&#65289;&#12290;IIFL&#20351;&#29992;Jeffreys&#20998;&#27495;&#30340;&#26032;&#39062;&#24212;&#29992;&#26469;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning has been applied to a range of robotic tasks, but can struggle when (1) robots encounter edge cases that are not represented in the training data (distribution shift) or (2) the human demonstrations are heterogeneous: taking different paths around an obstacle, for instance (multimodality). Interactive fleet learning (IFL) mitigates distribution shift by allowing robots to access remote human teleoperators during task execution and learn from them over time, but is not equipped to handle multimodality. Recent work proposes Implicit Behavior Cloning (IBC), which is able to represent multimodal demonstrations using energy-based models (EBMs). In this work, we propose addressing both multimodality and distribution shift with Implicit Interactive Fleet Learning (IIFL), the first extension of implicit policies to interactive imitation learning (including the single-robot, single-human setting). IIFL quantifies uncertainty using a novel application of Jeffreys divergence to
&lt;/p&gt;</description></item><item><title>&#36880;&#27493;&#32423;&#32852;&#27169;&#22411;FuXi&#22312;15&#22825;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20943;&#23569;&#39044;&#27979;&#35823;&#24046;&#30340;&#31215;&#32047;&#36798;&#21040;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.12873</link><description>&lt;p&gt;
FuXi: &#19968;&#20010;15&#22825;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#32423;&#32852;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
FuXi: A cascade machine learning forecasting system for 15-day global weather forecast. (arXiv:2306.12873v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12873
&lt;/p&gt;
&lt;p&gt;
&#36880;&#27493;&#32423;&#32852;&#27169;&#22411;FuXi&#22312;15&#22825;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20943;&#23569;&#39044;&#27979;&#35823;&#24046;&#30340;&#31215;&#32047;&#36798;&#21040;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#22825;&#27668;&#39044;&#25253;&#20013;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;0.25&#24230;&#31354;&#38388;&#20998;&#36776;&#29575;&#19979;&#30340;10&#22825;&#22825;&#27668;&#39044;&#25253;&#20013;&#24050;&#32463;&#34920;&#29616;&#20986;&#27604;&#27431;&#27954;&#20013;&#26399;&#22825;&#27668;&#39044;&#25253;&#20013;&#24515;(ECMWF)&#30340;&#39640;&#20998;&#36776;&#29575;&#39044;&#25253;(HRES)&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#22312;&#20110;&#22312;15&#22825;&#39044;&#25253;&#20013;&#34920;&#29616;&#19982;ECMWF&#38598;&#21512;&#24179;&#22343;(EM)&#30456;&#24403;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#32531;&#35299;&#39044;&#25253;&#35823;&#24046;&#30340;&#31215;&#32047;&#23545;&#20110;&#26377;&#25928;&#30340;&#38271;&#26399;&#39044;&#25253;&#38750;&#24120;&#37325;&#35201;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#20943;&#23569;&#31215;&#32047;&#35823;&#24046;&#30340;&#21162;&#21147;&#65292;&#21253;&#25324;&#33258;&#22238;&#24402;&#22810;&#26102;&#38388;&#27493;&#38271;&#25439;&#22833;&#65292;&#20294;&#20351;&#29992;&#21333;&#20010;&#27169;&#22411;&#21457;&#29616;&#26080;&#27861;&#22312;&#30701;&#21644;&#38271;&#23548;&#20986;&#26102;&#38388;&#19978;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FuXi&#65292;&#36825;&#26159;&#19968;&#20010;&#32423;&#32852;&#26426;&#22120;&#23398;&#20064;&#22825;&#27668;&#39044;&#27979;&#31995;&#32479;&#65292;&#25552;&#20379;&#20102;&#20998;&#36776;&#29575;&#20026;0.25&#24230;&#12289;&#26102;&#38388;&#20998;&#36776;&#29575;&#20026;6&#23567;&#26102;&#30340;15&#22825;&#20840;&#29699;&#39044;&#27979;&#12290;FuXi&#22522;&#20110;&#32423;&#32852;&#38598;&#21512;&#27169;&#22411;&#24320;&#21457;&#65292;&#23427;&#38598;&#25104;&#20102;&#22810;&#31181;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#24182;&#20943;&#23569;&#20102;&#39044;&#27979;&#35823;&#24046;&#30340;&#31215;&#32047;&#12290;&#20351;&#29992;&#31354;&#27668;&#28201;&#24230;&#65292;&#27604;&#28287;&#24230;&#21644;&#20301;&#21183;&#39640;&#24230;&#30340;&#22343;&#26041;&#26681;&#35823;&#24046;(RMSE)&#21644;&#24322;&#24120;&#30456;&#20851;&#31995;&#25968;(ACC)&#35780;&#20272;&#20102;FuXi&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;ECMWF HRES&#30456;&#27604;&#65292;FuXi&#22312;15&#22825;&#39044;&#25253;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20102;&#31215;&#32047;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few years, due to the rapid development of machine learning (ML) models for weather forecasting, state-of-the-art ML models have shown superior performance compared to the European Centre for Medium-Range Weather Forecasts (ECMWF)'s high-resolution forecast (HRES) in 10-day forecasts at a spatial resolution of 0.25 degree. However, the challenge remains to perform comparably to the ECMWF ensemble mean (EM) in 15-day forecasts. Previous studies have demonstrated the importance of mitigating the accumulation of forecast errors for effective long-term forecasts. Despite numerous efforts to reduce accumulation errors, including autoregressive multi-time step loss, using a single model is found to be insufficient to achieve optimal performance in both short and long lead times. Therefore, we present FuXi, a cascaded ML weather forecasting system that provides 15-day global forecasts with a temporal resolution of 6 hours and a spatial resolution of 0.25 degree. FuXi is develope
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#33647;&#29289;&#21457;&#29616;&#26041;&#38754;&#20351;&#29992;&#20102;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#20182;&#20204;&#25972;&#21512;&#20102;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;&#36825;&#20123;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.12802</link><description>&lt;p&gt;
Otter-Knowledge&#65306;&#19981;&#21516;&#26469;&#28304;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Otter-Knowledge: benchmarks of multimodal knowledge graph representation learning from different sources for drug discovery. (arXiv:2306.12802v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#33647;&#29289;&#21457;&#29616;&#26041;&#38754;&#20351;&#29992;&#20102;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#20182;&#20204;&#25972;&#21512;&#20102;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;&#36825;&#20123;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#34920;&#31034;&#23398;&#20064;&#30340;&#30740;&#31350;&#21033;&#29992;&#22823;&#37327;&#30340;&#34507;&#30333;&#36136;&#25110;&#20998;&#23376;&#25968;&#25454;&#24211;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#33719;&#24471;&#33647;&#29289;&#21644;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#30693;&#35782;&#12290;&#36825;&#20123;&#39044;&#35757;&#32451;&#34920;&#31034;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#21518;&#32493;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#65292;&#22914;&#39044;&#27979;&#33647;&#29289;&#21644;&#38774;&#34507;&#30333;&#20043;&#38388;&#30340;&#20146;&#21644;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#23558;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#21644;&#27169;&#24577;&#30340;&#30693;&#35782;&#22270;&#35889;&#25972;&#21512;&#21040;&#24207;&#21015;&#25110;SMILES&#34920;&#31034;&#20013;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#20016;&#23500;&#34920;&#31034;&#65292;&#24182;&#22312;&#24050;&#24314;&#31435;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26469;&#33258;7&#20010;&#20844;&#20849;&#26469;&#28304;&#30340;&#39044;&#22788;&#29702;&#21644;&#25972;&#21512;&#25968;&#25454;&#65292;&#20854;&#20013;&#21253;&#25324;&#36229;&#36807;30M&#20010;&#19977;&#20803;&#32452;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#22522;&#20110;&#36825;&#20123;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;Therapeutic Data Commons (TDC)&#22522;&#20934;&#27979;&#35797;&#20013;&#24615;&#33021;&#25253;&#21578;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research in representation learning utilizes large databases of proteins or molecules to acquire knowledge of drug and protein structures through unsupervised learning techniques. These pre-trained representations have proven to significantly enhance the accuracy of subsequent tasks, such as predicting the affinity between drugs and target proteins. In this study, we demonstrate that by incorporating knowledge graphs from diverse sources and modalities into the sequences or SMILES representation, we can further enrich the representation and achieve state-of-the-art results on established benchmark datasets. We provide preprocessed and integrated data obtained from 7 public sources, which encompass over 30M triples. Additionally, we make available the pre-trained models based on this data, along with the reported outcomes of their performance on three widely-used benchmark datasets for drug-target binding affinity prediction found in the Therapeutic Data Commons (TDC) benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#22312;&#20855;&#26377;&#21333;&#23618;&#32447;&#24615;&#33258;&#27880;&#24847;&#23618;&#30340;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#19978;&#36890;&#36807;&#26799;&#24230;&#27969;&#36827;&#34892;&#35757;&#32451;&#30340;ICL&#26426;&#21046;&#65292;&#25581;&#31034;&#20102;&#26799;&#24230;&#27969;&#20855;&#26377;&#25214;&#21040;&#30446;&#26631;&#20989;&#25968;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.09927</link><description>&lt;p&gt;
&#35757;&#32451;&#22909;&#30340;Transformer&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#32447;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Trained Transformers Learn Linear Models In-Context. (arXiv:2306.09927v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#22312;&#20855;&#26377;&#21333;&#23618;&#32447;&#24615;&#33258;&#27880;&#24847;&#23618;&#30340;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#19978;&#36890;&#36807;&#26799;&#24230;&#27969;&#36827;&#34892;&#35757;&#32451;&#30340;ICL&#26426;&#21046;&#65292;&#25581;&#31034;&#20102;&#26799;&#24230;&#27969;&#20855;&#26377;&#25214;&#21040;&#30446;&#26631;&#20989;&#25968;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20363;&#22914;Transformers&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#65306;&#32473;&#23450;&#19968;&#20010;&#26469;&#33258;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#30340;&#30701;&#35821;&#24207;&#21015;&#30340;&#25552;&#31034;&#65292;&#23427;&#20204;&#21487;&#20197;&#21046;&#23450;&#30456;&#20851;&#30340;&#27599;&#20010;&#20196;&#29260;&#21644;&#19979;&#19968;&#20010;&#20196;&#29260;&#30340;&#39044;&#27979;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#21442;&#25968;&#26356;&#26032;&#12290;&#36890;&#36807;&#23558;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#26410;&#26631;&#35760;&#30340;&#27979;&#35797;&#25968;&#25454;&#24207;&#21015;&#23884;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#36825;&#20351;&#24471;Transformer&#34920;&#29616;&#24471;&#20687;&#26377;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#12290;&#20107;&#23454;&#19978;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;&#38543;&#26426;&#23454;&#20363;&#19978;&#35757;&#32451;Transformer&#20307;&#31995;&#32467;&#26500;&#30340;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#39044;&#27979;&#20250;&#27169;&#20223;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#27861;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention-based neural networks such as transformers have demonstrated a remarkable ability to exhibit in-context learning (ICL): Given a short prompt sequence of tokens from an unseen task, they can formulate relevant per-token and next-token predictions without any parameter updates. By embedding a sequence of labeled training data and unlabeled test data as a prompt, this allows for transformers to behave like supervised learning algorithms. Indeed, recent work has shown that when training transformer architectures over random instances of linear regression problems, these models' predictions mimic those of ordinary least squares.  Towards understanding the mechanisms underlying this phenomenon, we investigate the dynamics of ICL in transformers with a single linear self-attention layer trained by gradient flow on linear regression tasks. We show that despite non-convexity, gradient flow with a suitable random initialization finds a global minimum of the objective function. At this 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#25551;&#36848;&#36923;&#36753;&#65288;DLs&#65289;&#20197;&#25903;&#25345;&#22810;&#20010;&#25277;&#35937;&#23618;&#27425;&#30340;&#30693;&#35782;&#34920;&#31034;&#65292;&#24182;&#35777;&#26126;&#20102;&#25512;&#29702;&#22312;&#35813;&#36923;&#36753;&#20013;&#26159;&#21487;&#21028;&#23450;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.03717</link><description>&lt;p&gt;
&#20855;&#26377;&#25277;&#35937;&#21644;&#32454;&#21270;&#30340;&#25551;&#36848;&#36923;&#36753;
&lt;/p&gt;
&lt;p&gt;
Description Logics with Abstraction and Refinement. (arXiv:2306.03717v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#25551;&#36848;&#36923;&#36753;&#65288;DLs&#65289;&#20197;&#25903;&#25345;&#22810;&#20010;&#25277;&#35937;&#23618;&#27425;&#30340;&#30693;&#35782;&#34920;&#31034;&#65292;&#24182;&#35777;&#26126;&#20102;&#25512;&#29702;&#22312;&#35813;&#36923;&#36753;&#20013;&#26159;&#21487;&#21028;&#23450;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#25551;&#36848;&#36923;&#36753;&#65288;DLs&#65289;&#65292;&#35813;DLs&#26377;&#21161;&#20110;&#25903;&#25345;&#22810;&#20010;&#25277;&#35937;&#23618;&#27425;&#30340;&#30693;&#35782;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#25193;&#23637;DLs&#23558;&#25277;&#35937;&#23618;&#32423;&#20316;&#20026;&#31532;&#19968;&#31867;&#20844;&#27665;&#65292;&#25552;&#20379;&#20102;&#26126;&#30830;&#30340;&#25805;&#20316;&#31526;&#65292;&#29992;&#20110;&#36328;&#22810;&#20010;&#25277;&#35937;&#23618;&#27425;&#36827;&#34892;&#27010;&#24565;&#21644;&#35282;&#33394;&#30340;&#25277;&#35937;&#21644;&#32454;&#21270;&#65292;&#22522;&#20110;&#21512;&#21462;&#26597;&#35810;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#24471;&#21040;&#30340;DLs&#26063;&#20013;&#36827;&#34892;&#25512;&#29702;&#26159;&#21487;&#21028;&#23450;&#30340;&#65292;&#32780;&#19968;&#20123;&#30475;&#20284;&#26080;&#23475;&#30340;&#21464;&#21270;&#23454;&#38469;&#19978;&#26159;&#19981;&#21487;&#21028;&#23450;&#30340;&#12290;&#25105;&#20204;&#36824;&#26126;&#30830;&#20102;&#25105;&#20204;&#36923;&#36753;&#21644;&#19968;&#20123;&#30456;&#20851;&#29255;&#27573;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ontologies often require knowledge representation on multiple levels of abstraction, but description logics (DLs) are not well-equipped for supporting this. We propose an extension of DLs in which abstraction levels are first-class citizens and which provides explicit operators for the abstraction and refinement of concepts and roles across multiple abstraction levels, based on conjunctive queries. We prove that reasoning in the resulting family of DLs is decidable while several seemingly harmless variations turn out to be undecidable. We also pinpoint the precise complexity of our logics and several relevant fragments.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#65292;&#26174;&#30528;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#65292;ITI&#20351;LLaMA&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#20174;32.5%&#25552;&#39640;&#21040;65.1%&#12290;ITI&#26159;&#19968;&#31181;&#26368;&#23567;&#31243;&#24230;&#30340;&#24178;&#25200;&#65292;&#35745;&#31639;&#24265;&#20215;&#65292;&#19988;&#25968;&#25454;&#25928;&#29575;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.03341</link><description>&lt;p&gt;
&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65306;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#23548;&#20986;&#30495;&#23454;&#30340;&#31572;&#26696;
&lt;/p&gt;
&lt;p&gt;
Inference-Time Intervention: Eliciting Truthful Answers from a Language Model. (arXiv:2306.03341v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#65292;&#26174;&#30528;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#65292;ITI&#20351;LLaMA&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#20174;32.5%&#25552;&#39640;&#21040;65.1%&#12290;ITI&#26159;&#19968;&#31181;&#26368;&#23567;&#31243;&#24230;&#30340;&#24178;&#25200;&#65292;&#35745;&#31639;&#24265;&#20215;&#65292;&#19988;&#25968;&#25454;&#25928;&#29575;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30495;&#23454;&#24615;&#12290;ITI&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#27839;&#30528;&#19968;&#32452;&#26041;&#21521;&#31227;&#21160;&#27169;&#22411;&#28608;&#27963;&#65292;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#12290;&#36825;&#31181;&#24178;&#39044;&#26174;&#30528;&#25552;&#39640;&#20102;LLaMA&#27169;&#22411;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;&#22312;&#25351;&#20196;&#24494;&#35843;&#30340;LLaMA Alpaca&#19978;&#65292;ITI&#23558;&#20854;&#30495;&#23454;&#24615;&#20174;32.5&#65285;&#25552;&#39640;&#21040;65.1&#65285;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#30495;&#23454;&#24615;&#21644;&#21487;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#35843;&#25972;&#24178;&#39044;&#24378;&#24230;&#26469;&#24179;&#34913;&#23427;&#12290;ITI &#21462;&#24471;&#20102;&#26368;&#20302;&#31243;&#24230;&#30340;&#24178;&#25200;&#19988;&#35745;&#31639;&#24265;&#20215;&#12290;&#27492;&#22806;&#65292;&#35813;&#25216;&#26415;&#22312;&#25968;&#25454;&#25928;&#29575;&#19978;&#34920;&#29616;&#20248;&#24322;&#65306;&#34429;&#28982;&#20687;RLHF&#36825;&#26679;&#30340;&#26041;&#27861;&#38656;&#35201;&#24191;&#27867;&#27880;&#37322;&#65292;&#20294;&#26159;ITI&#20165;&#20351;&#29992;&#20102;&#20960;&#30334;&#20010;&#20363;&#23376;&#23601;&#33021;&#23450;&#20301;&#30495;&#23454;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#21487;&#33021;&#20855;&#26377;&#26576;&#31181;&#20869;&#37096;&#34920;&#31034;&#26041;&#27861;&#26469;&#34920;&#31034;&#26576;&#20107;&#26159;&#30495;&#23454;&#30340;&#21487;&#33021;&#24615;&#65292;&#21363;&#20351;&#23427;&#20204;&#22312;&#34920;&#38754;&#19978;&#20135;&#29983;&#20102;&#34394;&#20551;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Inference-Time Intervention (ITI), a technique designed to enhance the truthfulness of large language models (LLMs). ITI operates by shifting model activations during inference, following a set of directions across a limited number of attention heads. This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from 32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while approaches like RLHF require extensive annotations, ITI locates truthful directions using only few hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;BERT&#22312;&#29616;&#23454;&#26631;&#31614;&#22122;&#22768;&#23384;&#22312;&#19979;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#21457;&#29616;&#29305;&#24449;&#30456;&#20851;&#30340;&#26631;&#31614;&#22122;&#22768;&#21644;&#26469;&#33258;&#27880;&#37322;&#32773;&#20998;&#27495;&#30340;&#21512;&#25104;&#26631;&#31614;&#22122;&#22768;&#20250;&#23548;&#33268;BERT&#30340;&#20998;&#31867;&#24615;&#33021;&#19979;&#38477;&#12290;&#25552;&#20986;&#19981;&#21516;&#31867;&#22411;&#30340;&#38598;&#25104;&#21644;&#22122;&#22768;&#28165;&#29702;&#26041;&#27861;&#20197;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16337</link><description>&lt;p&gt;
&#22788;&#29702;BERT&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#29616;&#23454;&#26631;&#31614;&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;
Handling Realistic Label Noise in BERT Text Classification. (arXiv:2305.16337v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;BERT&#22312;&#29616;&#23454;&#26631;&#31614;&#22122;&#22768;&#23384;&#22312;&#19979;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#21457;&#29616;&#29305;&#24449;&#30456;&#20851;&#30340;&#26631;&#31614;&#22122;&#22768;&#21644;&#26469;&#33258;&#27880;&#37322;&#32773;&#20998;&#27495;&#30340;&#21512;&#25104;&#26631;&#31614;&#22122;&#22768;&#20250;&#23548;&#33268;BERT&#30340;&#20998;&#31867;&#24615;&#33021;&#19979;&#38477;&#12290;&#25552;&#20986;&#19981;&#21516;&#31867;&#22411;&#30340;&#38598;&#25104;&#21644;&#22122;&#22768;&#28165;&#29702;&#26041;&#27861;&#20197;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#22122;&#22768;&#26159;&#30001;&#20110;&#24265;&#20215;&#30340;&#25968;&#25454;&#26631;&#27880;&#26041;&#27861;&#65288;&#22914;&#32593;&#32476;&#29228;&#21462;&#25110;&#20247;&#21253;&#65289;&#23548;&#33268;&#30340;&#35757;&#32451;&#26631;&#31614;&#20013;&#30340;&#38169;&#35823;&#65292;&#36825;&#21487;&#33021;&#23545;&#30417;&#30563;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#26377;&#23475;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#25269;&#28040;&#26377;&#30417;&#30563;&#20998;&#31867;&#20013;&#38543;&#26426;&#26631;&#31614;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#19968;&#20123;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;BERT&#24050;&#32463;&#23545;&#39640;&#27604;&#29575;&#30340;&#38543;&#26426;&#27880;&#20837;&#26631;&#31614;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#30340;&#26631;&#31614;&#22122;&#22768;&#24182;&#19981;&#26159;&#38543;&#26426;&#30340;&#65292;&#32780;&#26159;&#32463;&#24120;&#19982;&#36755;&#20837;&#29305;&#24449;&#25110;&#20854;&#20182;&#27880;&#37322;&#32773;&#29305;&#23450;&#22240;&#32032;&#30456;&#20851;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;BERT&#22312;&#38754;&#23545;&#20004;&#31181;&#31867;&#22411;&#30340;&#29616;&#23454;&#26631;&#31614;&#22122;&#22768;&#65306;&#29305;&#24449;&#30456;&#20851;&#30340;&#26631;&#31614;&#22122;&#22768;&#21644;&#26469;&#33258;&#27880;&#37322;&#32773;&#20998;&#27495;&#30340;&#21512;&#25104;&#26631;&#31614;&#22122;&#22768;&#12290;&#25105;&#20204;&#34920;&#26126;&#36825;&#20123;&#31867;&#22411;&#22122;&#22768;&#30340;&#23384;&#22312;&#26174;&#33879;&#38477;&#20302;&#20102;BERT&#20998;&#31867;&#24615;&#33021;&#12290;&#20026;&#20102;&#25552;&#39640;&#20854;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#38598;&#25104;&#21644;&#22122;&#22768;&#28165;&#29702;&#26041;&#27861;&#65292;&#24182;&#27604;&#36739;&#23427;&#20204;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20013;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Labels noise refers to errors in training labels caused by cheap data annotation methods, such as web scraping or crowd-sourcing, which can be detrimental to the performance of supervised classifiers. Several methods have been proposed to counteract the effect of random label noise in supervised classification, and some studies have shown that BERT is already robust against high rates of randomly injected label noise. However, real label noise is not random; rather, it is often correlated with input features or other annotator-specific factors. In this paper, we evaluate BERT in the presence of two types of realistic label noise: feature-dependent label noise, and synthetic label noise from annotator disagreements. We show that the presence of these types of noise significantly degrades BERT classification performance. To improve robustness, we evaluate different types of ensembles and noise-cleaning methods and compare their effectiveness against label noise across different datasets.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;OOD&#26696;&#20363;&#27979;&#35797;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26222;&#36941;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#26032;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#25506;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#25512;&#24191;&#21040;&#26356;&#22797;&#26434;&#30340;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2305.15269</link><description>&lt;p&gt;
&#20351;&#29992;OOD&#26696;&#20363;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26222;&#36941;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Testing the General Deductive Reasoning Capacity of Large Language Models Using OOD Examples. (arXiv:2305.15269v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15269
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;OOD&#26696;&#20363;&#27979;&#35797;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26222;&#36941;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#26032;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#25506;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#25512;&#24191;&#21040;&#26356;&#22797;&#26434;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#35777;&#26126;&#31354;&#38388;&#30340;&#24222;&#22823;&#65292;&#20219;&#20309;&#20855;&#26377;&#26222;&#36941;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#30340;&#27169;&#22411;&#24517;&#39035;&#33021;&#22815;&#25512;&#29702;&#26356;&#22797;&#26434;&#30340;&#35777;&#26126;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#32473;&#23450;&#25512;&#29702;&#38142;&#26465;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#26576;&#20123;&#25277;&#35937;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20027;&#35201;&#26159;&#22312;&#20351;&#29992;&#33707;&#24503;&#26031;&#22374;&#26031;&#25110;&#29305;&#23450;&#22823;&#23567;&#30340;&#35777;&#26126;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#24182;&#19988;&#19982;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#20998;&#24067;&#30456;&#21516;&#12290;&#20026;&#20102;&#34913;&#37327;LLM&#30340;&#26222;&#36941;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#24191;&#27867;&#30340;&#28436;&#32462;&#35268;&#21017;&#65292;&#24182;&#27979;&#37327;&#23427;&#20204;&#25512;&#29702;&#26356;&#22797;&#26434;&#35777;&#26126;&#30340;&#33021;&#21147;&#65292;&#26041;&#27861;&#21253;&#25324;&#28145;&#24230;&#27867;&#21270;&#12289;&#23485;&#24230;&#27867;&#21270;&#21644;&#32452;&#21512;&#27867;&#21270;&#12290;&#20026;&#20102;&#20415;&#20110;&#31995;&#32479;&#30340;&#25506;&#32034;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#21512;&#25104;&#21644;&#21487;&#32534;&#31243;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#23545;&#28436;&#32462;&#35268;&#21017;&#21644;&#35777;&#26126;&#22797;&#26434;&#24615;&#36827;&#34892;&#25511;&#21046;&#12290;&#25105;&#20204;&#23545;&#22235;&#20010;&#20855;&#26377;&#19981;&#21516;&#22823;&#23567;&#21644;&#35757;&#32451;&#30446;&#26631;&#30340;LLMs&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#20204;&#33021;&#22815;&#25512;&#24191;&#21040;&#22797;&#26434;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the intractably large size of the space of proofs, any model that is capable of general deductive reasoning must generalize to proofs of greater complexity. Recent studies have shown that large language models (LLMs) possess some abstract deductive reasoning ability given chain-of-thought prompts. However, they have primarily been tested on proofs using modus ponens or of a specific size, and from the same distribution as the in-context examples. To measure the general deductive reasoning ability of LLMs, we test on a broad set of deduction rules and measure their ability to generalize to more complex proofs from simpler demonstrations from multiple angles: depth-, width-, and compositional generalization. To facilitate systematic exploration, we construct a new synthetic and programmable reasoning dataset that enables control over deduction rules and proof complexity. Our experiments on four LLMs of various sizes and training objectives show that they are able to generalize to c
&lt;/p&gt;</description></item><item><title>SCITAB&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;1.2K&#20010;&#32463;&#39564;&#35777;&#30340;&#31185;&#23398;&#20107;&#23454;&#21644;&#30456;&#20851;&#30340;&#31185;&#23398;&#34920;&#26684;&#65292;&#35201;&#27714;&#36827;&#34892;&#32452;&#21512;&#25512;&#29702;&#21644;&#20107;&#23454;&#39564;&#35777;&#12290;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26469;&#35828;&#65292;SCITAB&#25552;&#20986;&#20102;&#35768;&#22810;&#29420;&#29305;&#25361;&#25112;&#65292;&#21253;&#25324;&#34920;&#26684;&#23450;&#20301;&#12289;&#20107;&#23454;&#27495;&#20041;&#21644;&#32452;&#21512;&#25512;&#29702;&#12290;&#25152;&#26377;&#27169;&#22411;&#20013;&#65292;&#38500;&#20102;GPT-4&#20043;&#22806;&#65292;&#24615;&#33021;&#20165;&#30053;&#39640;&#20110;&#38543;&#26426;&#29468;&#27979;&#12290;&#25552;&#31034;&#25216;&#26415;&#22914;&#24605;&#32500;&#38142;&#23545;&#20110;&#22312;SCITAB&#19978;&#25552;&#21319;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.13186</link><description>&lt;p&gt;
SCITAB: &#19968;&#20010;&#23545;&#31185;&#23398;&#34920;&#26684;&#36827;&#34892;&#32452;&#21512;&#25512;&#29702;&#21644;&#20107;&#23454;&#39564;&#35777;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables. (arXiv:2305.13186v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13186
&lt;/p&gt;
&lt;p&gt;
SCITAB&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;1.2K&#20010;&#32463;&#39564;&#35777;&#30340;&#31185;&#23398;&#20107;&#23454;&#21644;&#30456;&#20851;&#30340;&#31185;&#23398;&#34920;&#26684;&#65292;&#35201;&#27714;&#36827;&#34892;&#32452;&#21512;&#25512;&#29702;&#21644;&#20107;&#23454;&#39564;&#35777;&#12290;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26469;&#35828;&#65292;SCITAB&#25552;&#20986;&#20102;&#35768;&#22810;&#29420;&#29305;&#25361;&#25112;&#65292;&#21253;&#25324;&#34920;&#26684;&#23450;&#20301;&#12289;&#20107;&#23454;&#27495;&#20041;&#21644;&#32452;&#21512;&#25512;&#29702;&#12290;&#25152;&#26377;&#27169;&#22411;&#20013;&#65292;&#38500;&#20102;GPT-4&#20043;&#22806;&#65292;&#24615;&#33021;&#20165;&#30053;&#39640;&#20110;&#38543;&#26426;&#29468;&#27979;&#12290;&#25552;&#31034;&#25216;&#26415;&#22914;&#24605;&#32500;&#38142;&#23545;&#20110;&#22312;SCITAB&#19978;&#25552;&#21319;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#31185;&#23398;&#20107;&#23454;&#26680;&#26597;&#22522;&#20934;&#27979;&#35797;&#23384;&#22312;&#19968;&#20123;&#19981;&#36275;&#65292;&#20363;&#22914;&#26469;&#33258;&#20247;&#21253;&#23457;&#26597;&#30340;&#20559;&#35265;&#21644;&#23545;&#22522;&#20110;&#25991;&#26412;&#30340;&#35777;&#25454;&#30340;&#36807;&#24230;&#20381;&#36182;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;SCITAB&#65292;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;1.2K&#20010;&#32463;&#39564;&#35777;&#30340;&#31185;&#23398;&#20107;&#23454;&#65292;&#36825;&#20123;&#20107;&#23454;1&#65289;&#26469;&#28304;&#20110;&#30495;&#23454;&#30340;&#31185;&#23398;&#20986;&#29256;&#29289;&#65292;2&#65289;&#38656;&#35201;&#32452;&#21512;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;&#12290;&#36825;&#20123;&#20107;&#23454;&#19982;&#21253;&#21547;&#35777;&#25454;&#30340;&#31185;&#23398;&#34920;&#26684;&#36827;&#34892;&#37197;&#23545;&#65292;&#24182;&#36827;&#34892;&#20102;&#26631;&#35760;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;SCITAB&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#22522;&#20110;&#34920;&#26684;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#38500;&#20102;GPT-4&#22806;&#65292;&#25152;&#26377;&#27169;&#22411;&#30340;&#24615;&#33021;&#20165;&#30053;&#39640;&#20110;&#38543;&#26426;&#29468;&#27979;&#12290;&#27969;&#34892;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#22914;&#24605;&#32500;&#38142;&#65292;&#23545;SCITAB&#30340;&#24615;&#33021;&#25552;&#21319;&#19981;&#22823;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;SCITAB&#25552;&#20986;&#30340;&#20960;&#20010;&#29420;&#29305;&#25361;&#25112;&#65292;&#21253;&#25324;&#34920;&#26684;&#23450;&#20301;&#12289;&#20107;&#23454;&#30340;&#27495;&#20041;&#24615;&#21644;&#32452;&#21512;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#20379;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current scientific fact-checking benchmarks exhibit several shortcomings, such as biases arising from crowd-sourced claims and an over-reliance on text-based evidence. We present SCITAB, a challenging evaluation dataset consisting of 1.2K expert-verified scientific claims that 1) originate from authentic scientific publications and 2) require compositional reasoning for verification. The claims are paired with evidence-containing scientific tables annotated with labels. Through extensive evaluations, we demonstrate that SCITAB poses a significant challenge to state-of-the-art models, including table-based pretraining models and large language models. All models except GPT-4 achieved performance barely above random guessing. Popular prompting techniques, such as Chain-of-Thought, do not achieve much performance gains on SCITAB. Our analysis uncovers several unique challenges posed by SCITAB, including table grounding, claim ambiguity, and compositional reasoning. Our codes and data are 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#30340;&#36755;&#20837;&#36873;&#25321;&#20250;&#23545;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;&#22312;&#22522;&#20934;&#27979;&#35797;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#8220;&#33258;&#25105;&#22686;&#24378;&#8221;&#25216;&#26415;&#20197;&#25913;&#21892;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.13062</link><description>&lt;p&gt;
GPT4Table&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#29702;&#35299;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#21527;&#65311;&#19968;&#39033;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
GPT4Table: Can Large Language Models Understand Structured Table Data? A Benchmark and Empirical Study. (arXiv:2305.13062v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#30340;&#36755;&#20837;&#36873;&#25321;&#20250;&#23545;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;&#22312;&#22522;&#20934;&#27979;&#35797;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#8220;&#33258;&#25105;&#22686;&#24378;&#8221;&#25216;&#26415;&#20197;&#25913;&#21892;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23569;&#26679;&#26412;&#25512;&#29702;&#22120;&#26469;&#35299;&#20915;&#19982;&#33258;&#28982;&#35821;&#35328;&#30456;&#20851;&#30340;&#20219;&#21153;&#36234;&#26469;&#36234;&#20855;&#21560;&#24341;&#21147;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;LLMs&#23545;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#20363;&#22914;&#34920;&#26684;&#65289;&#30340;&#29702;&#35299;&#31243;&#24230;&#36824;&#26377;&#24456;&#22810;&#38656;&#35201;&#23398;&#20064;&#30340;&#22320;&#26041;&#12290;&#23613;&#31649;&#21487;&#20197;&#20351;&#29992;&#34920;&#26684;&#24207;&#21015;&#21270;&#20316;&#20026;LLMs&#30340;&#36755;&#20837;&#65292;&#20294;&#30446;&#21069;&#36824;&#32570;&#20047;&#23545;LLMs&#26159;&#21542;&#30495;&#27491;&#33021;&#22815;&#29702;&#35299;&#36825;&#31867;&#25968;&#25454;&#30340;&#20840;&#38754;&#30740;&#31350;&#12290;&#26412;&#25991;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;LLMs&#30340;&#32467;&#26500;&#29702;&#35299;&#33021;&#21147;&#65288;SUC&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21019;&#24314;&#30340;&#22522;&#20934;&#27979;&#35797;&#21253;&#25324;&#19971;&#20010;&#20219;&#21153;&#65292;&#27599;&#20010;&#20219;&#21153;&#37117;&#26377;&#20854;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#21333;&#20803;&#26684;&#26597;&#25214;&#12289;&#34892;&#26816;&#32034;&#21644;&#22823;&#23567;&#26816;&#27979;&#12290;&#25105;&#20204;&#23545;GPT-3.5&#21644;GPT-4&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#24615;&#33021;&#22240;&#22810;&#31181;&#36755;&#20837;&#36873;&#25321;&#32780;&#24322;&#65292;&#21253;&#25324;&#34920;&#26684;&#36755;&#20837;&#26684;&#24335;&#12289;&#20869;&#23481;&#39034;&#24207;&#12289;&#35282;&#33394;&#25552;&#31034;&#21644;&#20998;&#21306;&#26631;&#35760;&#31561;&#12290;&#26681;&#25454;&#22522;&#20934;&#27979;&#35797;&#35780;&#20272;&#25152;&#24471;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#33258;&#25105;&#22686;&#24378;&#8221;&#25216;&#26415;&#20197;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are becoming attractive as few-shot reasoners to solve Natural Language (NL)-related tasks. However, there is still much to learn about how well LLMs understand structured data, such as tables. While it is true that tables can be used as inputs to LLMs with serialization, there lack of comprehensive studies examining whether LLMs can truly comprehend such data. In this paper, we try to understand this by designing a benchmark to evaluate the structural understanding capabilities (SUC) of LLMs. The benchmark we create includes seven tasks, each with its own unique challenges, \eg, cell lookup, row retrieval, and size detection. We run a series of evaluations on GPT-3.5 and GPT-4. We discover that the performance varied depending on a number of input choices, including table input format, content order, role prompting, and partition marks. Drawing from the insights gained through the benchmark evaluations, we then propose \textit{self-augmentation} for effect
&lt;/p&gt;</description></item><item><title>ChatGPT&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#25104;&#20026;&#38382;&#39064;&#12290;&#36890;&#36807;&#30740;&#31350;ChatGPT&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#21457;&#29616;&#20854;&#22312;&#29702;&#35299;&#20195;&#30721;&#35821;&#27861;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23545;&#20110;&#35821;&#20041;&#30340;&#29702;&#35299;&#21644;&#37096;&#20998;&#21151;&#33021;&#23384;&#22312;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.12138</link><description>&lt;p&gt;
ChatGPT&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#30340;&#24212;&#29992;&#33539;&#22260;&#65306;&#20840;&#38754;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Scope of ChatGPT in Software Engineering: A Thorough Investigation. (arXiv:2305.12138v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12138
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#25104;&#20026;&#38382;&#39064;&#12290;&#36890;&#36807;&#30740;&#31350;ChatGPT&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#21457;&#29616;&#20854;&#22312;&#29702;&#35299;&#20195;&#30721;&#35821;&#27861;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23545;&#20110;&#35821;&#20041;&#30340;&#29702;&#35299;&#21644;&#37096;&#20998;&#21151;&#33021;&#23384;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#23637;&#31034;&#20102;&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#36716;&#21270;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#34920;&#29616;&#20986;&#22312;&#20195;&#30721;&#21644;&#25991;&#26723;&#29983;&#25104;&#31561;&#20219;&#21153;&#20013;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36719;&#20214;&#24037;&#31243;&#38656;&#35201;&#39640;&#21487;&#38752;&#24615;&#21644;&#39118;&#38505;&#25511;&#21046;&#65292;&#20351;ChatGPT&#30340;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#25104;&#20026;&#19968;&#20010;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;ChatGPT&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#23558;AI&#27169;&#22411;&#24212;&#23545;SE&#20219;&#21153;&#25152;&#38656;&#30340;&#33021;&#21147;&#20998;&#20026;&#19977;&#31867;&#65306;1&#65289;&#35821;&#27861;&#29702;&#35299;&#65292;2&#65289;&#38745;&#24577;&#34892;&#20026;&#29702;&#35299;&#65292;&#21644;3&#65289;&#21160;&#24577;&#34892;&#20026;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#37325;&#28857;&#26159;ChatGPT&#29702;&#35299;&#20195;&#30721;&#35821;&#27861;&#21644;&#35821;&#20041;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#12289;&#25511;&#21046;&#27969;&#31243;&#22270;&#65288;CFG&#65289;&#21644;&#35843;&#29992;&#22270;&#65288;CG&#65289;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#22312;&#28041;&#21450;C&#12289;Java&#12289;Python&#21644;Solidity&#30340;&#36328;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#34429;&#28982;ChatGPT&#34920;&#29616;&#20986;&#20102;&#23545;&#29702;&#35299;&#20195;&#30721;&#35821;&#27861;&#65288;AST&#65289;&#30340;&#20986;&#33394;&#33021;&#21147;&#65292;&#20294;&#22312;&#29702;&#35299;&#20195;&#30721;&#35821;&#20041;&#21644;&#37096;&#20998;&#21151;&#33021;&#19978;&#23384;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT demonstrates immense potential to transform software engineering (SE) by exhibiting outstanding performance in tasks such as code and document generation. However, the high reliability and risk control requirements of SE make the lack of interpretability for ChatGPT a concern. To address this issue, we carried out a study evaluating ChatGPT's capabilities and limitations in SE. We broke down the abilities needed for AI models to tackle SE tasks into three categories: 1) syntax understanding, 2) static behavior understanding, and 3) dynamic behavior understanding. Our investigation focused on ChatGPT's ability to comprehend code syntax and semantic structures, including abstract syntax trees (AST), control flow graphs (CFG), and call graphs (CG). We assessed ChatGPT's performance on cross-language tasks involving C, Java, Python, and Solidity. Our findings revealed that while ChatGPT excels at understanding code syntax (AST), it struggles with comprehending code semantics, parti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; DUCK &#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23454;&#20307;&#31867;&#22411;&#30340;&#20808;&#21069;&#30693;&#35782;&#22312;&#23454;&#20307;&#34920;&#31034;&#31354;&#38388;&#20013;&#27880;&#20837;&#32467;&#26500;&#20449;&#24687;&#12290;&#25226;&#30418;&#23884;&#20837;&#27010;&#24565;&#24341;&#20837;&#21040;&#26497;&#22352;&#26631;&#20013;&#65292;&#23558;&#20851;&#31995;&#34920;&#31034;&#20026;&#36229;&#29699;&#38754;&#19978;&#30340;&#30418;&#23376;&#65292;&#23558;&#20855;&#26377;&#30456;&#20284;&#31867;&#22411;&#30340;&#23454;&#20307;&#25918;&#32622;&#22312;&#23545;&#24212;&#20110;&#23427;&#20204;&#20851;&#31995;&#30340;&#30418;&#23376;&#20869;&#65292;&#23454;&#29616;&#32858;&#31867;&#12290;&#22312;&#23454;&#20307;&#38142;&#25509;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#23588;&#20854;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2305.12027</link><description>&lt;p&gt;
&#26497;&#22320;&#40493;&#23376;&#30340;&#21457;&#29616;&#20043;&#26053;&#65306;&#20351;&#29992;&#40493;&#24335;&#36776;&#26512;&#21644;&#26497;&#22352;&#26631;&#30418;&#23884;&#20837;&#22686;&#24378;&#23454;&#20307;&#38142;&#25509;
&lt;/p&gt;
&lt;p&gt;
Polar Ducks and Where to Find Them: Enhancing Entity Linking with Duck Typing and Polar Box Embeddings. (arXiv:2305.12027v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; DUCK &#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23454;&#20307;&#31867;&#22411;&#30340;&#20808;&#21069;&#30693;&#35782;&#22312;&#23454;&#20307;&#34920;&#31034;&#31354;&#38388;&#20013;&#27880;&#20837;&#32467;&#26500;&#20449;&#24687;&#12290;&#25226;&#30418;&#23884;&#20837;&#27010;&#24565;&#24341;&#20837;&#21040;&#26497;&#22352;&#26631;&#20013;&#65292;&#23558;&#20851;&#31995;&#34920;&#31034;&#20026;&#36229;&#29699;&#38754;&#19978;&#30340;&#30418;&#23376;&#65292;&#23558;&#20855;&#26377;&#30456;&#20284;&#31867;&#22411;&#30340;&#23454;&#20307;&#25918;&#32622;&#22312;&#23545;&#24212;&#20110;&#23427;&#20204;&#20851;&#31995;&#30340;&#30418;&#23376;&#20869;&#65292;&#23454;&#29616;&#32858;&#31867;&#12290;&#22312;&#23454;&#20307;&#38142;&#25509;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#23588;&#20854;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23494;&#38598;&#26816;&#32034;&#30340;&#23454;&#20307;&#38142;&#25509;&#26041;&#27861;&#26159;&#22823;&#35268;&#27169;&#24212;&#29992;&#20013;&#39640;&#25928;&#19988;&#24191;&#27867;&#20351;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23427;&#20204;&#22312;&#24615;&#33021;&#19978;&#19981;&#22914;&#29983;&#25104;&#27169;&#22411;&#65292;&#22240;&#20026;&#23427;&#20204;&#23545;&#23884;&#20837;&#31354;&#38388;&#30340;&#32467;&#26500;&#25935;&#24863;&#12290;&#20026;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102; DUCK &#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23454;&#20307;&#31867;&#22411;&#30340;&#20808;&#21069;&#30693;&#35782;&#22312;&#23454;&#20307;&#34920;&#31034;&#31354;&#38388;&#20013;&#27880;&#20837;&#32467;&#26500;&#20449;&#24687;&#12290;&#21463;&#32534;&#31243;&#35821;&#35328;&#20013;&#40493;&#24335;&#36776;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#35758;&#26681;&#25454;&#23454;&#20307;&#22312;&#30693;&#35782;&#22270;&#20013;&#30340;&#20851;&#31995;&#23450;&#20041;&#23454;&#20307;&#30340;&#31867;&#22411;&#12290;&#28982;&#21518;&#65292;&#23558;&#30418;&#23884;&#20837;&#30340;&#27010;&#24565;&#31227;&#26893;&#21040;&#29699;&#24418;&#26497;&#22352;&#26631;&#20013;&#65292;&#25105;&#20204;&#25552;&#35758;&#23558;&#20851;&#31995;&#34920;&#31034;&#20026;&#36229;&#29699;&#38754;&#19978;&#30340;&#30418;&#23376;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20855;&#26377;&#30456;&#20284;&#31867;&#22411;&#30340;&#23454;&#20307;&#25918;&#32622;&#22312;&#23545;&#24212;&#20110;&#23427;&#20204;&#20851;&#31995;&#30340;&#30418;&#23376;&#20869;&#26469;&#20248;&#21270;&#27169;&#22411;&#20197;&#32858;&#31867;&#23454;&#20307;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26631;&#20934;&#23454;&#20307;&#28040;&#27495;&#22522;&#20934;&#27979;&#35797;&#19978;&#35774;&#32622;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#23427;&#25552;&#39640;&#20102;&#23494;&#38598;&#26816;&#32034;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#29305;&#21035;&#19981;&#36866;&#29992;&#20110;&#29983;&#25104;&#27169;&#22411;&#19981;&#21487;&#34892;&#30340;&#20302;&#36164;&#28304;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity linking methods based on dense retrieval are an efficient and widely used solution in large-scale applications, but they fall short of the performance of generative models, as they are sensitive to the structure of the embedding space. In order to address this issue, this paper introduces DUCK, an approach to infusing structural information in the space of entity representations, using prior knowledge of entity types. Inspired by duck typing in programming languages, we propose to define the type of an entity based on the relations that it has with other entities in a knowledge graph. Then, porting the concept of box embeddings to spherical polar coordinates, we propose to represent relations as boxes on the hypersphere. We optimize the model to cluster entities of similar type by placing them inside the boxes corresponding to their relations. Our experiments show that our method sets new state-of-the-art results on standard entity-disambiguation benchmarks, it improves the perf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#22120;&#19982;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#25506;&#27979;&#29983;&#25104;&#22120;&#30340;&#36755;&#20986;&#31354;&#38388;&#26469;&#27979;&#37327;&#20854;&#23545;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#30340;&#26657;&#20934;&#31243;&#24230;&#65292;&#24182;&#35777;&#26126;&#29992;&#22810;&#20010;&#26679;&#26412;&#21644;&#22810;&#20010;&#21442;&#32771;&#21487;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.11707</link><description>&lt;p&gt;
&#19979;&#19968;&#20010;&#20250;&#26159;&#20160;&#20040;&#65311;&#35780;&#20272;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#22120;&#30340;&#19981;&#30830;&#23450;&#24615;&#19982;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#23545;&#27604;
&lt;/p&gt;
&lt;p&gt;
What Comes Next? Evaluating Uncertainty in Neural Text Generators Against Human Production Variability. (arXiv:2305.11707v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#22120;&#19982;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#25506;&#27979;&#29983;&#25104;&#22120;&#30340;&#36755;&#20986;&#31354;&#38388;&#26469;&#27979;&#37327;&#20854;&#23545;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#30340;&#26657;&#20934;&#31243;&#24230;&#65292;&#24182;&#35777;&#26126;&#29992;&#22810;&#20010;&#26679;&#26412;&#21644;&#22810;&#20010;&#21442;&#32771;&#21487;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#20219;&#21153;&#20013;&#65292;&#38024;&#23545;&#20219;&#20309;&#36755;&#20837;&#65292;&#23384;&#22312;&#22810;&#20010;&#21487;&#34892;&#30340;&#20132;&#38469;&#30446;&#26631;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#22810;&#31181;&#26041;&#24335;&#23558;&#20219;&#20309;&#30446;&#26631;&#29992;&#35821;&#35328;&#34920;&#36798;&#20986;&#26469;&#25110;&#36827;&#34892;&#29983;&#20135;&#12290;&#25105;&#20204;&#34920;&#24449;&#20102;&#20154;&#31867;&#29983;&#20135;&#22312;&#22235;&#20010;NLG&#20219;&#21153;&#20013;&#35789;&#27719;&#12289;&#21477;&#27861;&#21644;&#35821;&#20041;&#26041;&#38754;&#30340;&#21464;&#24322;&#31243;&#24230;&#65292;&#24182;&#23558;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#19982;&#19981;&#30830;&#23450;&#24615;&#32852;&#31995;&#36215;&#26469;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26816;&#26597;&#20102;&#29983;&#25104;&#31995;&#32479;&#39044;&#27979;&#30340;&#27010;&#29575;&#20998;&#24067;&#21644;&#35299;&#30721;&#31639;&#27861;&#25152;&#24418;&#25104;&#30340;&#36755;&#20986;&#23383;&#31526;&#20018;&#31354;&#38388;&#65292;&#20197;&#25506;&#31350;&#20854;&#19981;&#30830;&#23450;&#24615;&#12290;&#38024;&#23545;&#27599;&#20010;&#27979;&#35797;&#36755;&#20837;&#65292;&#25105;&#20204;&#27979;&#37327;&#20102;&#29983;&#25104;&#22120;&#23545;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#30340;&#26657;&#20934;&#31243;&#24230;&#12290;&#36890;&#36807;&#36825;&#31181;&#22522;&#20110;&#23454;&#20363;&#32423;&#21035;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;NLG&#27169;&#22411;&#21644;&#35299;&#30721;&#31574;&#30053;&#65292;&#35777;&#26126;&#29992;&#22810;&#20010;&#26679;&#26412;&#21644;&#22810;&#20010;&#21442;&#32771;&#23545;&#29983;&#25104;&#22120;&#36827;&#34892;&#25506;&#27979;&#65292;&#25552;&#20379;&#20102;&#29702;&#35299;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#25152;&#24517;&#38656;&#30340;&#35814;&#32454;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Natural Language Generation (NLG) tasks, for any input, multiple communicative goals are plausible, and any goal can be put into words, or produced, in multiple ways. We characterise the extent to which human production varies lexically, syntactically, and semantically across four NLG tasks, connecting human production variability to aleatoric or data uncertainty. We then inspect the space of output strings shaped by a generation system's predicted probability distribution and decoding algorithm to probe its uncertainty. For each test input, we measure the generator's calibration to human production variability. Following this instance-level approach, we analyse NLG models and decoding strategies, demonstrating that probing a generator with multiple samples and, when possible, multiple references, provides the level of detail necessary to gain understanding of a model's representation of uncertainty.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20449;&#24687;&#20016;&#23500;&#30340;&#25968;&#25454;&#23376;&#38598;&#26469;&#39640;&#25928;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.06677</link><description>&lt;p&gt;
INGENIOUS&#65306;&#20351;&#29992;&#20449;&#24687;&#20016;&#23500;&#30340;&#25968;&#25454;&#23376;&#38598;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Large Language Models. (arXiv:2305.06677v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20449;&#24687;&#20016;&#23500;&#30340;&#25968;&#25454;&#23376;&#38598;&#26469;&#39640;&#25928;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26174;&#30528;&#29305;&#28857;&#26159;&#22312;&#20854;&#27867;&#21270;&#33021;&#21147;&#21644;&#26032;&#33021;&#21147;&#26041;&#38754;&#38543;&#30528;&#27169;&#22411;&#23481;&#37327;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780; achieved. &#28982;&#32780;&#65292;&#24517;&#39035;&#35748;&#35782;&#21040;&#36825;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#20102;&#36807;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#12289;&#36807;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#26377;&#23475;&#30340;&#29615;&#22659;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#26159;&#21542;&#21487;&#33021;&#20165;&#20351;&#29992;&#39640;&#24230;&#20449;&#24687;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#26469;&#35757;&#32451; PTLM&#65292;&#24182;&#21516;&#26102;&#20445;&#25345;&#20854;&#19979;&#28216;&#24615;&#33021;&#65311;
&lt;/p&gt;
&lt;p&gt;
A salient characteristic of large pre-trained language models (PTLMs) is a remarkable improvement in their generalization capability and emergence of new capabilities with increasing model capacity and pre-training dataset size. Consequently, we are witnessing the development of enormous models pushing the state-of-the-art. It is, however, imperative to realize that this inevitably leads to prohibitively long training times, extortionate computing costs, and a detrimental environmental impact. Significant efforts are underway to make PTLM training more efficient through innovations in model architectures, training pipelines, and loss function design, with scant attention being paid to optimizing the utility of training data. The key question that we ask is whether it is possible to train PTLMs by employing only highly informative subsets of the training data while maintaining downstream performance? Building upon the recent progress in informative data subset selection, we show how we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#29983;&#25104;&#26032;&#38395;&#25991;&#31456;&#25688;&#35201;&#20013;&#30340;&#25919;&#27835;&#20559;&#35265;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#29305;&#26391;&#26222;&#30456;&#27604;&#65292;&#26356;&#22810;&#30340;&#32654;&#22269;&#25919;&#24220;&#38598;&#20307;&#26426;&#26500;&#65288;&#21363;&#25919;&#24220;&#65289;&#19982;&#25308;&#30331;&#30456;&#20851;&#32852;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#26410;&#26469;&#30340;&#25688;&#35201;&#20559;&#35265;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.02321</link><description>&lt;p&gt;
&#33258;&#21160;&#25688;&#35201;&#20013;&#30340;&#25919;&#27835;&#20559;&#35265;&#29305;&#24449;&#20998;&#26512;&#65306;&#20197;&#29305;&#26391;&#26222;&#21644;&#25308;&#30331;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Characterizing Political Bias in Automatic Summaries: A Case Study of Trump and Biden. (arXiv:2305.02321v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#29983;&#25104;&#26032;&#38395;&#25991;&#31456;&#25688;&#35201;&#20013;&#30340;&#25919;&#27835;&#20559;&#35265;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#29305;&#26391;&#26222;&#30456;&#27604;&#65292;&#26356;&#22810;&#30340;&#32654;&#22269;&#25919;&#24220;&#38598;&#20307;&#26426;&#26500;&#65288;&#21363;&#25919;&#24220;&#65289;&#19982;&#25308;&#30331;&#30456;&#20851;&#32852;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#26410;&#26469;&#30340;&#25688;&#35201;&#20559;&#35265;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#25991;&#29486;&#34920;&#26126;&#65292;&#24378;&#22823;&#30340;NLP&#31995;&#32479;&#21487;&#33021;&#23545;&#31038;&#20250;&#20559;&#35265;&#36827;&#34892;&#32534;&#30721;&#65307;&#28982;&#32780;&#65292;&#33258;&#21160;&#25688;&#35201;&#27169;&#22411;&#30340;&#25919;&#27835;&#20559;&#35265;&#20173;&#30456;&#23545;&#26410;&#30693;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#23454;&#20307;&#26367;&#25442;&#26041;&#27861;&#30740;&#31350;&#20102;&#26032;&#38395;&#25991;&#31456;&#33258;&#21160;&#29983;&#25104;&#25688;&#35201;&#20013;&#30340;&#25919;&#27835;&#23478;&#25551;&#32472;&#12290;&#25105;&#20204;&#22522;&#20110;&#25919;&#27835;&#23454;&#20307;&#21644;&#35789;&#27719;&#36164;&#28304;&#24320;&#21457;&#20102;&#19968;&#20010;&#35745;&#31639;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#35780;&#20272;&#25277;&#21462;&#24335;&#21644;&#25277;&#35937;&#24335;&#25688;&#35201;&#27169;&#22411;&#20013;&#26377;&#20851;&#21776;&#32435;&#24503;&#183;&#29305;&#26391;&#26222;&#21644;&#20052;&#183;&#25308;&#30331;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#19968;&#33268;&#30340;&#24046;&#24322;&#65292;&#20363;&#22914;&#22312;&#19982;&#29305;&#26391;&#26222;&#30456;&#27604;&#65292;&#26356;&#22810;&#30340;&#32654;&#22269;&#25919;&#24220;&#38598;&#20307;&#26426;&#26500;&#65288;&#21363;&#25919;&#24220;&#65289;&#19982;&#25308;&#30331;&#30456;&#20851;&#32852;&#12290;&#24403;&#23454;&#20307;&#22312;&#28304;&#25991;&#31456;&#20013;&#37325;&#28857;&#20986;&#29616;&#26102;&#65292;&#36825;&#20123;&#25688;&#35201;&#24046;&#24322;&#26368;&#20026;&#26126;&#26174;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21270;&#29305;&#24449;&#20998;&#26512;&#25552;&#20379;&#20102;&#19968;&#20010;&#26410;&#26469;&#30740;&#31350;&#25688;&#35201;&#20559;&#35265;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Growing literature has shown that powerful NLP systems may encode social biases; however, the political bias of summarization models remains relatively unknown. In this work, we use an entity replacement method to investigate the portrayal of politicians in automatically generated summaries of news articles. We develop a computational framework based on political entities and lexical resources, and use it to assess biases about Donald Trump and Joe Biden in both extractive and abstractive summarization models. We find consistent differences, such as stronger associations of a collective US government (i.e., administration) with Biden than with Trump. These summary dissimilarities are most prominent when the entity is heavily featured in the source article. Our systematic characterization provides a framework for future studies of bias in summarization.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;Q&#30340;&#31574;&#30053;&#35268;&#21017;&#26063;&#20013;&#30340;&#22343;&#34913;&#20559;&#24046;&#65288;&#25110; Qb-equilibria&#65289;&#65292;&#21363;Q&#20540;&#22312;&#19981;&#21516;&#30417;&#27979;&#25216;&#26415;&#19979;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.12647</link><description>&lt;p&gt;
&#22522;&#20110;Q&#30340;&#22343;&#34913;
&lt;/p&gt;
&lt;p&gt;
Q-based Equilibria. (arXiv:2304.12647v1 [econ.TH])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12647
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;Q&#30340;&#31574;&#30053;&#35268;&#21017;&#26063;&#20013;&#30340;&#22343;&#34913;&#20559;&#24046;&#65288;&#25110; Qb-equilibria&#65289;&#65292;&#21363;Q&#20540;&#22312;&#19981;&#21516;&#30417;&#27979;&#25216;&#26415;&#19979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#65292;Q&#23398;&#20064;&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#35268;&#21017;&#65292;&#20854;&#20026;&#27599;&#20010;&#26367;&#20195;&#26041;&#26696;&#25552;&#20379;&#20272;&#35745;&#20540;(&#21363;Q&#20540;)&#65292;&#35813;&#20540;&#19982;&#20043;&#21069;&#30340;&#20915;&#31574;&#30456;&#20851;&#12290;&#19968;&#20010;&#26420;&#32032;&#30340;&#31574;&#30053;&#26159;&#22987;&#32456;&#36873;&#25321;&#20855;&#26377;&#26368;&#39640;Q&#20540;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#26063;&#22522;&#20110;Q&#30340;&#31574;&#30053;&#35268;&#21017;&#65292;&#36825;&#20123;&#35268;&#21017;&#21487;&#33021;&#31995;&#32479;&#22320;&#25903;&#25345;&#26576;&#20123;&#26367;&#20195;&#26041;&#26696;&#32780;&#19981;&#26159;&#20854;&#20182;&#26367;&#20195;&#26041;&#26696;&#65292;&#20363;&#22914;&#21253;&#21547;&#26377;&#21033;&#21512;&#20316;&#30340;&#23485;&#23481;&#20559;&#24046;&#30340;&#35268;&#21017;&#12290;&#22312; Compte &#21644; Postlewaite [2018] &#30340;&#31934;&#31070;&#19979;&#65292;&#25105;&#20204;&#22312;&#36825;&#20010; Q-based &#35268;&#21017;&#26063;&#20013;&#23547;&#25214;&#22343;&#34913;&#20559;&#24046;&#65288;&#25110; Qb-equilibria&#65289;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30417;&#27979;&#25216;&#26415;&#19979;&#30340;&#32463;&#20856;&#21338;&#24328;&#12290;
&lt;/p&gt;
&lt;p&gt;
In dynamic environments, Q-learning is an adaptative rule that provides an estimate (a Q-value) of the continuation value associated with each alternative. A naive policy consists in always choosing the alternative with highest Q-value. We consider a family of Q-based policy rules that may systematically favor some alternatives over others, for example rules that incorporate a leniency bias that favors cooperation. In the spirit of Compte and Postlewaite [2018], we look for equilibrium biases (or Qb-equilibria) within this family of Q-based rules. We examine classic games under various monitoring technologies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#36328;&#35774;&#22791;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#20998;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;(HGNN)&#65292;&#29992;&#20110;&#35299;&#20915;&#36328;&#35774;&#22791;&#29992;&#25143;&#21305;&#37197;&#38382;&#39064;&#65292;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;TGCE&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;5%&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.03215</link><description>&lt;p&gt;
&#24102;&#26377;&#36328;&#35774;&#22791;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#20998;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36328;&#35774;&#22791;&#29992;&#25143;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Graph Neural Network with Cross-Attention for Cross-Device User Matching. (arXiv:2304.03215v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#36328;&#35774;&#22791;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#20998;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;(HGNN)&#65292;&#29992;&#20110;&#35299;&#20915;&#36328;&#35774;&#22791;&#29992;&#25143;&#21305;&#37197;&#38382;&#39064;&#65292;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;TGCE&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;5%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24191;&#21578;&#12289;&#25512;&#33616;&#31995;&#32479;&#21644;&#32593;&#32476;&#23433;&#20840;&#31561;&#20247;&#22810;&#39046;&#22495;&#65292;&#36328;&#35774;&#22791;&#29992;&#25143;&#21305;&#37197;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#23427;&#28041;&#21450;&#20351;&#29992;&#24207;&#21015;&#26085;&#24535;&#26469;&#35782;&#21035;&#21644;&#38142;&#25509;&#23646;&#20110;&#21516;&#19968;&#20154;&#30340;&#19981;&#21516;&#35774;&#22791;&#12290;&#20197;&#24448;&#30340;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#38590;&#20197;&#35299;&#20915;&#26085;&#24535;&#20043;&#38388;&#30340;&#38271;&#31243;&#20381;&#36182;&#21644;&#39640;&#38454;&#36830;&#25509;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#36825;&#20010;&#38382;&#39064;&#24314;&#27169;&#20026;&#22270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#23618;&#22270;&#19978;&#19979;&#25991;&#23884;&#20837;(TGCE)&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#34920;&#29616;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65288;HGNN&#65289;&#65292;&#23427;&#20855;&#26377;&#27604;TGCE&#26356;&#20026;&#35745;&#31639;&#25928;&#29575;&#30340;&#20108;&#32423;&#35774;&#35745;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#27169;&#22411;&#20013;&#24341;&#20837;&#20102;&#19968;&#31181;&#36328;&#35774;&#22791;&#20132;&#21449;&#27880;&#24847;&#21147;&#65288;Cross-Att&#65289;&#26426;&#21046;&#65292;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;TGCE&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;5%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-device user matching is a critical problem in numerous domains, including advertising, recommender systems, and cybersecurity. It involves identifying and linking different devices belonging to the same person, utilizing sequence logs. Previous data mining techniques have struggled to address the long-range dependencies and higher-order connections between the logs. Recently, researchers have modeled this problem as a graph problem and proposed a two-tier graph contextual embedding (TGCE) neural network architecture, which outperforms previous methods. In this paper, we propose a novel hierarchical graph neural network architecture (HGNN), which has a more computationally efficient second level design than TGCE. Furthermore, we introduce a cross-attention (Cross-Att) mechanism in our model, which improves performance by 5% compared to the state-of-the-art TGCE method.
&lt;/p&gt;</description></item><item><title>RepoCoder&#26159;&#19968;&#20010;&#31616;&#21333;&#12289;&#36890;&#29992;&#21644;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#19968;&#20010;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26816;&#32034;&#22120;&#21644;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#24211;&#32423;&#21035;&#20195;&#30721;&#23436;&#25104;&#27969;&#31243;&#20013;&#23454;&#29616;&#20102;&#26377;&#25928;&#21033;&#29992;&#24211;&#32423;&#21035;&#20449;&#24687;&#36827;&#34892;&#20195;&#30721;&#23436;&#25104;&#21644;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2303.12570</link><description>&lt;p&gt;
RepoCoder&#65306;&#36890;&#36807;&#36845;&#20195;&#26816;&#32034;&#21644;&#29983;&#25104;&#23454;&#29616;&#30340;&#20195;&#30721;&#23384;&#20648;&#24211;&#32423;&#21035;&#23436;&#25104;
&lt;/p&gt;
&lt;p&gt;
RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation. (arXiv:2303.12570v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12570
&lt;/p&gt;
&lt;p&gt;
RepoCoder&#26159;&#19968;&#20010;&#31616;&#21333;&#12289;&#36890;&#29992;&#21644;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#19968;&#20010;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26816;&#32034;&#22120;&#21644;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#24211;&#32423;&#21035;&#20195;&#30721;&#23436;&#25104;&#27969;&#31243;&#20013;&#23454;&#29616;&#20102;&#26377;&#25928;&#21033;&#29992;&#24211;&#32423;&#21035;&#20449;&#24687;&#36827;&#34892;&#20195;&#30721;&#23436;&#25104;&#21644;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24211;&#32423;&#21035;&#20195;&#30721;&#23436;&#25104;&#20219;&#21153;&#26159;&#22522;&#20110;&#20195;&#30721;&#24211;&#26356;&#24191;&#38420;&#19978;&#19979;&#25991;&#20013;&#32487;&#32493;&#32534;&#20889;&#26410;&#23436;&#25104;&#20195;&#30721;&#30340;&#36807;&#31243;&#12290;&#20294;&#26159;&#23545;&#20110;&#33258;&#21160;&#23436;&#25104;&#24037;&#20855;&#32780;&#35328;&#65292;&#24456;&#38590;&#21033;&#29992;&#25955;&#24067;&#22312;&#19981;&#21516;&#25991;&#20214;&#20013;&#30340;&#26377;&#29992;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;RepoCoder&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#12289;&#36890;&#29992;&#21644;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;&#23427;&#36890;&#36807;&#25972;&#21512;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26816;&#32034;&#22120;&#21644;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#31616;&#21270;&#20102;&#24211;&#32423;&#21035;&#20195;&#30721;&#23436;&#25104;&#27969;&#31243;&#65292;&#20174;&#32780;&#20801;&#35768;&#26377;&#25928;&#21033;&#29992;&#24211;&#32423;&#21035;&#20449;&#24687;&#36827;&#34892;&#20195;&#30721;&#23436;&#25104;&#65292;&#24182;&#20855;&#26377;&#19981;&#21516;&#31890;&#24230;&#23618;&#38754;&#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;RepoCoder &#36824;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#26816;&#32034;-&#29983;&#25104;&#27169;&#22411;&#65292;&#24357;&#21512;&#20102;&#26816;&#32034;&#19978;&#19979;&#25991;&#21644;&#39044;&#26399;&#23436;&#25104;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;RepoEval&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#26368;&#26032;&#21644;&#39640;&#36136;&#37327;&#30495;&#23454;&#19990;&#30028;&#30340;&#20195;&#30721;&#24211;&#65292;&#28085;&#30422;&#20102;&#34892;&#12289;API &#35843;&#29992;&#21644;&#20989;&#25968;&#20307;&#23436;&#25104;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of repository-level code completion is to continue writing the unfinished code based on a broader context of the repository. While for automated code completion tools, it is difficult to utilize the useful information scattered in different files. We propose RepoCoder, a simple, generic, and effective framework to address the challenge. It streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model, which allows for the effective utilization of repository-level information for code completion and grants the ability to generate code at various levels of granularity. Furthermore, RepoCoder utilizes a novel iterative retrieval-generation paradigm that bridges the gap between retrieval context and the intended completion target. We also propose a new benchmark RepoEval, which consists of the latest and high-quality real-world repositories covering line, API invocation, and function body completion sce
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CHiLL&#65292;&#19968;&#31181;&#29992;&#20110;&#20174;&#21307;&#30103;&#35760;&#24405;&#20013;&#25552;&#21462;&#29305;&#24449;&#30340;&#26041;&#24335;&#65292;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26597;&#35810;&#29983;&#25104;&#29305;&#24449;&#65292;&#20351;&#21307;&#29983;&#33021;&#22815;&#29992;&#33258;&#24049;&#30340;&#19987;&#19994;&#30693;&#35782;&#21046;&#20316;&#23545;&#19979;&#28216;&#20219;&#21153;&#26377;&#20020;&#24202;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.12343</link><description>&lt;p&gt;
CHiLL: &#38646;-shot&#23450;&#21046;&#21270;&#12289;&#21487;&#35299;&#37322;&#30340;&#21307;&#30103;&#35760;&#24405;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CHiLL: Zero-shot Custom Interpretable Feature Extraction from Clinical Notes with Large Language Models. (arXiv:2302.12343v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12343
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CHiLL&#65292;&#19968;&#31181;&#29992;&#20110;&#20174;&#21307;&#30103;&#35760;&#24405;&#20013;&#25552;&#21462;&#29305;&#24449;&#30340;&#26041;&#24335;&#65292;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26597;&#35810;&#29983;&#25104;&#29305;&#24449;&#65292;&#20351;&#21307;&#29983;&#33021;&#22815;&#29992;&#33258;&#24049;&#30340;&#19987;&#19994;&#30693;&#35782;&#21046;&#20316;&#23545;&#19979;&#28216;&#20219;&#21153;&#26377;&#20020;&#24202;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;CHiLL (Crafting High-Level Latents)&#65292;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#21270;&#32447;&#24615;&#27169;&#22411;&#29305;&#24449;&#30340;&#26041;&#27861;&#12290;CHiLL&#36890;&#36807;&#19987;&#23478;&#21046;&#20316;&#30340;&#26597;&#35810;&#25351;&#23548;LLMs&#29983;&#25104;&#35299;&#37322;&#24615;&#29305;&#24449;&#65292;&#29992;&#20110;&#20174;&#20581;&#24247;&#35760;&#24405;&#20013;&#35757;&#32451;&#31616;&#21333;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#12290;&#22522;&#20110;LLM&#30340;&#26597;&#35810;&#29983;&#25104;&#29305;&#24449;&#21487;&#20197;&#20351;&#21307;&#29983;&#21033;&#29992;&#20182;&#20204;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#26469;&#21046;&#20316;&#23545;&#19979;&#28216;&#20219;&#21153;&#26377;&#20020;&#24202;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#32780;&#26080;&#38656;&#25163;&#21160;&#20174;&#21407;&#22987;&#30005;&#23376;&#30149;&#21382;&#20013;&#25552;&#21462;&#36825;&#20123;&#29305;&#24449;&#12290;&#25105;&#20204;&#21463;&#21040;&#19968;&#20010;&#29616;&#23454;&#19990;&#30028;&#30340;&#39118;&#38505;&#39044;&#27979;&#20219;&#21153;&#30340;&#21551;&#21457;&#65292;&#20294;&#25105;&#20204;&#20351;&#29992;MIMIC-III&#21644;MIMIC-CXR&#25968;&#25454;&#21644;&#26631;&#20934;&#30340;&#39044;&#27979;&#20219;&#21153;&#65288;&#20363;&#22914;&#65292;30&#22825;&#20877;&#20837;&#38498;&#65289;&#26469;&#35780;&#20272;&#36825;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#33258;&#21160;&#25552;&#21462;&#30340;&#29305;&#24449;&#30340;&#32447;&#24615;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#19982;&#20351;&#29992;&#21442;&#32771;&#29305;&#24449;&#30340;&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#19988;&#27604;&#20351;&#29992;&#8220;&#35789;&#34955;&#8221;&#29305;&#24449;&#30340;&#32447;&#24615;&#27169;&#22411;&#25552;&#20379;&#26356;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#35777;&#23454;&#20102;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#26435;&#37325;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose CHiLL (Crafting High-Level Latents), an approach for natural-language specification of features for linear models. CHiLL prompts LLMs with expert-crafted queries to generate interpretable features from health records. The resulting noisy labels are then used to train a simple linear classifier. Generating features based on queries to an LLM can empower physicians to use their domain expertise to craft features that are clinically meaningful for a downstream task of interest, without having to manually extract these from raw EHR. We are motivated by a real-world risk prediction task, but as a reproducible proxy, we use MIMIC-III and MIMIC-CXR data and standard predictive tasks (e.g., 30-day readmission) to evaluate this approach. We find that linear models using automatically extracted features are comparably performant to models using reference features, and provide greater interpretability than linear models using "Bag-of-Words" features. We verify that learned feature weig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24418;&#24577;&#20449;&#24687;&#22312;&#20845;&#31181;&#35821;&#35328;&#30340;&#19978;&#19979;&#25991;&#35789;&#24418;&#36824;&#21407;&#22120;&#24320;&#21457;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#23545;&#35789;&#24418;&#36824;&#21407;&#22120;&#36827;&#34892;&#20102;&#39046;&#22495;&#22806;&#24615;&#33021;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2302.00407</link><description>&lt;p&gt;
&#20851;&#20110;&#24418;&#24577;&#20449;&#24687;&#22312;&#19978;&#19979;&#25991;&#35789;&#24418;&#36824;&#21407;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Role of Morphological Information for Contextual Lemmatization. (arXiv:2302.00407v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24418;&#24577;&#20449;&#24687;&#22312;&#20845;&#31181;&#35821;&#35328;&#30340;&#19978;&#19979;&#25991;&#35789;&#24418;&#36824;&#21407;&#22120;&#24320;&#21457;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#23545;&#35789;&#24418;&#36824;&#21407;&#22120;&#36827;&#34892;&#20102;&#39046;&#22495;&#22806;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#24418;&#36824;&#21407;&#26159;&#19968;&#39033;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#20219;&#21153;&#65292;&#23427;&#21253;&#25324;&#20174;&#32473;&#23450;&#30340;&#23624;&#25240;&#35789;&#29983;&#25104;&#20854;&#35268;&#33539;&#24418;&#24335;&#25110;&#35789;&#24418;&#36824;&#21407;&#12290;&#35789;&#24418;&#36824;&#21407;&#26159;&#31616;&#21270;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#30340;&#22522;&#26412;&#20219;&#21153;&#20043;&#19968;&#65292;&#23545;&#20110;&#39640;&#24230;&#23624;&#25240;&#35821;&#35328;&#23588;&#20026;&#37325;&#35201;&#12290;&#34429;&#28982;&#26681;&#25454;&#23624;&#25240;&#35789;&#33719;&#24471;&#35789;&#24418;&#36824;&#21407;&#24418;&#24335;&#30340;&#36807;&#31243;&#21487;&#20197;&#36890;&#36807;&#32771;&#34385;&#20854;&#24418;&#24577;&#21477;&#27861;&#31867;&#21035;&#26469;&#35299;&#37322;&#65292;&#20294;&#22312;&#35757;&#32451;&#19978;&#19979;&#25991;&#35789;&#24418;&#36824;&#21407;&#22120;&#26102;&#24341;&#20837;&#32454;&#31890;&#24230;&#30340;&#24418;&#24577;&#21477;&#27861;&#20449;&#24687;&#24050;&#32463;&#25104;&#20026;&#24120;&#35265;&#20570;&#27861;&#65292;&#32780;&#26080;&#35270;&#19979;&#28216;&#32489;&#25928;&#26159;&#21542;&#20248;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#23454;&#35777;&#30740;&#31350;&#20102;&#22312;&#20845;&#31181;&#35821;&#35328;&#20013;&#32771;&#23519;&#24418;&#24577;&#20449;&#24687;&#22312;&#19978;&#19979;&#25991;&#35789;&#24418;&#36824;&#21407;&#22120;&#24320;&#21457;&#20013;&#30340;&#20316;&#29992;&#65292;&#36825;&#20845;&#31181;&#35821;&#35328;&#30340;&#24418;&#24577;&#22797;&#26434;&#24615;&#21508;&#19981;&#30456;&#21516;&#65292;&#21253;&#25324;&#24052;&#26031;&#20811;&#35821;&#12289;&#22303;&#32819;&#20854;&#35821;&#12289;&#20420;&#35821;&#12289;&#25463;&#20811;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#33521;&#35821;&#12290;&#27492;&#22806;&#65292;&#19982;&#32477;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#36824;&#22312;&#39046;&#22495;&#22806;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35789;&#24418;&#36824;&#21407;&#22120;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lemmatization is a natural language processing (NLP) task which consists of producing, from a given inflected word, its canonical form or lemma. Lemmatization is one of the basic tasks that facilitate downstream NLP applications, and is of particular importance for high-inflected languages. Given that the process to obtain a lemma from an inflected word can be explained by looking at its morphosyntactic category, including fine-grained morphosyntactic information to train contextual lemmatizers has become common practice, without considering whether that is the optimum in terms of downstream performance. In order to address this issue, in this paper we empirically investigate the role of morphological information to develop contextual lemmatizers in six languages within a varied spectrum of morphological complexity: Basque, Turkish, Russian, Czech, Spanish and English. Furthermore, and unlike the vast majority of previous work, we also evaluate lemmatizers in out-of-domain settings, wh
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22238;&#39038;&#20102;&#26032;&#29983;&#20799;&#31185;&#23398;&#20013;&#26368;&#36817;&#21457;&#23637;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#31995;&#32479;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#26032;&#29983;&#20799;&#31185;&#24212;&#29992;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25551;&#36848;&#20102;&#26368;&#26032;&#30340;&#30740;&#31350;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2302.00225</link><description>&lt;p&gt;
&#26032;&#29983;&#20799;&#37325;&#30151;&#30417;&#25252;&#23460;&#19982;&#20154;&#24037;&#26234;&#33021;&#30340;&#36807;&#21435;&#12289;&#29616;&#22312;&#21644;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
The Past, Current, and Future of Neonatal Intensive Care Units with Artificial Intelligence. (arXiv:2302.00225v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22238;&#39038;&#20102;&#26032;&#29983;&#20799;&#31185;&#23398;&#20013;&#26368;&#36817;&#21457;&#23637;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#31995;&#32479;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#26032;&#29983;&#20799;&#31185;&#24212;&#29992;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25551;&#36848;&#20102;&#26368;&#26032;&#30340;&#30740;&#31350;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26159;&#20154;&#24037;&#26234;&#33021;&#30340;&#20004;&#20010;&#23376;&#39046;&#22495;&#65292;&#28041;&#21450;&#25945;&#25480;&#35745;&#31639;&#26426;&#20174;&#21508;&#31181;&#25968;&#25454;&#20013;&#23398;&#20064;&#21644;&#20570;&#20986;&#20915;&#31574;&#12290;&#26368;&#36817;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#22823;&#37096;&#20998;&#21457;&#23637;&#26469;&#33258;&#28145;&#24230;&#23398;&#20064;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21040;&#20581;&#24247;&#31185;&#23398;&#31561;&#20960;&#20046;&#25152;&#26377;&#39046;&#22495;&#37117;&#34987;&#35777;&#26126;&#26159;&#38761;&#21629;&#24615;&#30340;&#12290;&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#23398;&#20013;&#30340;&#24433;&#21709;&#24050;&#32463;&#26174;&#33879;&#25913;&#21464;&#20102;&#20256;&#32479;&#30340;&#20020;&#24202;&#24212;&#29992;&#26041;&#24335;&#12290;&#23613;&#31649;&#19968;&#20123;&#21307;&#23398;&#30340;&#23376;&#39046;&#22495;&#65292;&#20363;&#22914;&#20799;&#31185;&#23398;&#65292;&#30456;&#23545;&#36739;&#24930;&#22320;&#25509;&#21463;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#37325;&#35201;&#22909;&#22788;&#65292;&#20294;&#30456;&#20851;&#30340;&#20799;&#31185;&#30740;&#31350;&#20063;&#24320;&#22987;&#31215;&#32047;&#21040;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#27700;&#24179;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#26032;&#29983;&#20799;&#31185;&#23398;&#20013;&#26368;&#36817;&#21457;&#23637;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#23545;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#22312;&#26032;&#29983;&#20799;&#31185;&#24212;&#29992;&#20013;&#30340;&#35282;&#33394;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#23450;&#20041;&#20102;&#26041;&#27861;&#35770;&#65292;&#21253;&#25324;&#31639;&#27861;&#21457;&#23637;&#65292;&#24182;&#19988;&#25551;&#36848;&#20102;&#26368;&#26032;&#30340;&#30740;&#31350;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning and deep learning are two subsets of artificial intelligence that involve teaching computers to learn and make decisions from any sort of data. Most recent developments in artificial intelligence are coming from deep learning, which has proven revolutionary in almost all fields, from computer vision to health sciences. The effects of deep learning in medicine have changed the conventional ways of clinical application significantly. Although some sub-fields of medicine, such as pediatrics, have been relatively slow in receiving the critical benefits of deep learning, related research in pediatrics has started to accumulate to a significant level, too. Hence, in this paper, we review recently developed machine learning and deep learning-based solutions for neonatology applications. We systematically evaluate the roles of both classical machine learning and deep learning in neonatology applications, define the methodologies, including algorithmic developments, and describ
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39046;&#22495;&#32422;&#26463;&#30340;&#21487;&#25193;&#23637;&#25216;&#26415;&#65292;&#29992;&#20110;&#24369;&#30417;&#30563;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;MNIST&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#22312;&#35268;&#27169;&#19978;&#26377;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2301.05253</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#39046;&#22495;&#32422;&#26463;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#21487;&#25193;&#23637;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
A Scalable Technique for Weak-Supervised Learning with Domain Constraints. (arXiv:2301.05253v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05253
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39046;&#22495;&#32422;&#26463;&#30340;&#21487;&#25193;&#23637;&#25216;&#26415;&#65292;&#29992;&#20110;&#24369;&#30417;&#30563;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;MNIST&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#22312;&#35268;&#27169;&#19978;&#26377;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#25193;&#23637;&#30340;&#31471;&#21040;&#31471;&#27969;&#27700;&#32447;&#25216;&#26415;&#65292;&#21033;&#29992;&#31526;&#21495;&#39046;&#22495;&#30693;&#35782;&#20316;&#20026;&#32422;&#26463;&#26465;&#20214;&#65292;&#20197;&#24369;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#29992;&#20110;&#20998;&#31867;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29305;&#21035;&#36866;&#29992;&#20110;&#25968;&#25454;&#21253;&#21547;&#19981;&#21516;&#30340;&#32452;&#65288;&#31867;&#65289;&#65292;&#36866;&#23452;&#20110;&#32858;&#31867;&#21451;&#22909;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#32771;&#34385;&#22810;&#20010;&#35757;&#32451;&#31034;&#20363;&#19968;&#27425;&#24615;&#20351;&#29992;&#39640;&#25928;&#30340;&#25968;&#23398;&#20248;&#21270;&#25216;&#26415;&#26469;&#37325;&#26032;&#21046;&#23450;&#39046;&#22495;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel scalable end-to-end pipeline that uses symbolic domain knowledge as constraints for learning a neural network for classifying unlabeled data in a weak-supervised manner. Our approach is particularly well-suited for settings where the data consists of distinct groups (classes) that lends itself to clustering-friendly representation learning and the domain constraints can be reformulated for use of efficient mathematical optimization techniques by considering multiple training examples at once. We evaluate our approach on a variant of the MNIST image classification problem where a training example consists of image sequences and the sum of the numbers represented by the sequences, and show that our approach scales significantly better than previous approaches that rely on computing all constraint satisfying combinations for each training example.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#26426;&#26800;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#25214;&#21040;&#20102;&#36830;&#32493;&#30340;&#36827;&#23637;&#27979;&#37327;&#65292;&#20197;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#20013; emergent behavior &#30340;&#20135;&#29983;&#21407;&#22240;&#12290;&#20854;&#20013;&#65292;&#20316;&#32773;&#20197;&#8220;grokking&#8221;&#29616;&#35937;&#20026;&#26696;&#20363;&#65292;&#36890;&#36807;&#36870;&#21521;&#24037;&#31243;&#30340;&#26041;&#24335;&#23436;&#20840;&#29702;&#35299;&#20102;&#23567;&#22411;transformer&#32593;&#32476;&#22312;&#27169;&#22359;&#21270;&#21152;&#27861;&#20219;&#21153;&#20013;&#30340;&#31639;&#27861;&#65292;&#24182;&#23450;&#20041;&#20102;&#36827;&#23637;&#27979;&#37327;&#26469;&#30740;&#31350;&#36825;&#20010;&#29616;&#35937;&#30340;&#36827;&#23637;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2301.05217</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#26800;&#35299;&#37322;&#24615;&#30340;&#36827;&#23637;&#27979;&#37327;&#26469;&#29702;&#35299;"grokking"
&lt;/p&gt;
&lt;p&gt;
Progress measures for grokking via mechanistic interpretability. (arXiv:2301.05217v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05217
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#26426;&#26800;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#25214;&#21040;&#20102;&#36830;&#32493;&#30340;&#36827;&#23637;&#27979;&#37327;&#65292;&#20197;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#20013; emergent behavior &#30340;&#20135;&#29983;&#21407;&#22240;&#12290;&#20854;&#20013;&#65292;&#20316;&#32773;&#20197;&#8220;grokking&#8221;&#29616;&#35937;&#20026;&#26696;&#20363;&#65292;&#36890;&#36807;&#36870;&#21521;&#24037;&#31243;&#30340;&#26041;&#24335;&#23436;&#20840;&#29702;&#35299;&#20102;&#23567;&#22411;transformer&#32593;&#32476;&#22312;&#27169;&#22359;&#21270;&#21152;&#27861;&#20219;&#21153;&#20013;&#30340;&#31639;&#27861;&#65292;&#24182;&#23450;&#20041;&#20102;&#36827;&#23637;&#27979;&#37327;&#26469;&#30740;&#31350;&#36825;&#20010;&#29616;&#35937;&#30340;&#36827;&#23637;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#32463;&#24120;&#34920;&#29616;&#20986;&#26032;&#20852;&#34892;&#20026;&#65292;&#20063;&#23601;&#26159;&#36890;&#36807;&#22686;&#21152;&#21442;&#25968;&#12289;&#35757;&#32451;&#25968;&#25454;&#25110;&#35757;&#32451;&#27493;&#39588;&#26469;&#20135;&#29983;&#36136;&#30340;&#21464;&#21270;&#12290;&#29702;&#35299;&#26032;&#20852;&#34892;&#20026;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#25214;&#21040;&#36830;&#32493;&#30340;&#8220;&#36827;&#23637;&#27979;&#37327;&#8221;&#65292;&#36825;&#20123;&#27979;&#37327;&#26159;&#30475;&#20284;&#19981;&#36830;&#32493;&#30340;&#36136;&#30340;&#21464;&#21270;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#21487;&#20197;&#36890;&#36807;&#26426;&#26800;&#35299;&#37322;&#24615;&#26469;&#25214;&#21040;&#36827;&#23637;&#27979;&#37327;&#65306;&#23558;&#23398;&#21040;&#30340;&#34892;&#20026;&#36870;&#21521;&#24037;&#31243;&#25104;&#20854;&#21508;&#20010;&#32452;&#25104;&#37096;&#20998;&#12290;&#20316;&#20026;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#23567;&#22411;transformer&#35757;&#32451;&#27169;&#22359;&#21270;&#21152;&#27861;&#20219;&#21153;&#26102;&#20986;&#29616;&#30340;&#8220;grokking&#8221;&#29616;&#35937;&#12290;&#25105;&#20204;&#23436;&#20840;&#36870;&#21521;&#24037;&#31243;&#20102;&#36825;&#20123;&#32593;&#32476;&#23398;&#21040;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#31163;&#25955;&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#19977;&#35282;&#24658;&#31561;&#24335;&#23558;&#21152;&#27861;&#36716;&#25442;&#20026;&#22278;&#21608;&#26059;&#36716;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#28608;&#27963;&#21644;&#26435;&#37325;&#65292;&#24182;&#22312;&#20613;&#37324;&#21494;&#31354;&#38388;&#36827;&#34892;&#28040;&#34701;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#20010;&#31639;&#27861;&#12290;&#22522;&#20110;&#36825;&#31181;&#29702;&#35299;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#36827;&#23637;&#27979;&#37327;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#30740;&#31350;&#36825;&#20010;&#29616;&#35937;&#30340;&#36827;&#23637;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks often exhibit emergent behavior, where qualitatively new capabilities arise from scaling up the amount of parameters, training data, or training steps. One approach to understanding emergence is to find continuous \textit{progress measures} that underlie the seemingly discontinuous qualitative changes. We argue that progress measures can be found via mechanistic interpretability: reverse-engineering learned behaviors into their individual components. As a case study, we investigate the recently-discovered phenomenon of ``grokking'' exhibited by small transformers trained on modular addition tasks. We fully reverse engineer the algorithm learned by these networks, which uses discrete Fourier transforms and trigonometric identities to convert addition to rotation about a circle. We confirm the algorithm by analyzing the activations and weights and by performing ablations in Fourier space. Based on this understanding, we define progress measures that allow us to study the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SHARE&#65292;&#19968;&#31181;&#32771;&#34385;&#20849;&#20139;&#26631;&#31614;&#32467;&#26500;&#30340;HAR&#26694;&#26550;&#65292;&#36890;&#36807;&#24314;&#27169;&#20849;&#21516;&#30340;&#32467;&#26500;&#20174;&#32780;&#25581;&#31034;&#19981;&#21516;&#27963;&#21160;&#20043;&#38388;&#30340;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2301.03462</link><description>&lt;p&gt;
&#37322;&#25918;&#20849;&#20139;&#26631;&#31614;&#32467;&#26500;&#22312;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Power of Shared Label Structures for Human Activity Recognition. (arXiv:2301.03462v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SHARE&#65292;&#19968;&#31181;&#32771;&#34385;&#20849;&#20139;&#26631;&#31614;&#32467;&#26500;&#30340;HAR&#26694;&#26550;&#65292;&#36890;&#36807;&#24314;&#27169;&#20849;&#21516;&#30340;&#32467;&#26500;&#20174;&#32780;&#25581;&#31034;&#19981;&#21516;&#27963;&#21160;&#20043;&#38388;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#25216;&#26415;&#23558;&#27963;&#21160;&#26631;&#31614;&#35270;&#20026;&#25972;&#25968;&#31867;&#21035;ID&#65292;&#27809;&#26377;&#26126;&#30830;&#22320;&#23545;&#31867;&#21035;&#26631;&#31614;&#30340;&#35821;&#20041;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#19981;&#21516;&#30340;&#27963;&#21160;&#21517;&#31216;&#36890;&#24120;&#20855;&#26377;&#20849;&#20139;&#30340;&#32467;&#26500;&#12290;&#20363;&#22914;&#65292;&#8220;&#25171;&#24320;&#38376;&#8221;&#21644;&#8220;&#25171;&#24320;&#20912;&#31665;&#8221;&#37117;&#26377;&#8220;&#25171;&#24320;&#8221;&#20316;&#20026;&#21160;&#20316;&#65307;&#8220;&#36386;&#36275;&#29699;&#8221;&#21644;&#8220;&#25171;&#32593;&#29699;&#8221;&#37117;&#26377;&#8220;&#29699;&#8221;&#20316;&#20026;&#29289;&#20307;&#12290;&#26631;&#31614;&#21517;&#31216;&#20013;&#30340;&#36825;&#31181;&#20849;&#20139;&#32467;&#26500;&#21487;&#20197;&#36716;&#21270;&#20026;&#24863;&#30693;&#25968;&#25454;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#19988;&#24314;&#27169;&#20849;&#21516;&#30340;&#32467;&#26500;&#21487;&#20197;&#24110;&#21161;&#25581;&#31034;&#19981;&#21516;&#27963;&#21160;&#20043;&#38388;&#30340;&#30693;&#35782;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26679;&#26412;&#26377;&#38480;&#30340;&#27963;&#21160;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SHARE&#65292;&#19968;&#31181;&#32771;&#34385;&#19981;&#21516;&#27963;&#21160;&#30340;&#26631;&#31614;&#21517;&#31216;&#20849;&#20139;&#32467;&#26500;&#30340;HAR&#26694;&#26550;&#12290;&#20026;&#20102;&#21033;&#29992;&#20849;&#20139;&#32467;&#26500;&#65292;SHARE&#21253;&#25324;&#19968;&#20010;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#20174;&#36755;&#20837;&#30340;&#24863;&#30693;&#26102;&#38388;&#24207;&#21015;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#21253;&#25324;&#19968;&#20010;&#35299;&#30721;&#22120;&#65292;&#29992;&#20110;&#29983;&#25104;&#26631;&#31614;&#21517;&#31216;&#20316;&#20026;&#20196;&#29260;&#24207;&#21015;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19977;&#31181;&#26631;&#31614;&#22686;&#24378;&#25216;&#26415;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#26356;&#21152;&#26377;&#25928;&#22320;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current human activity recognition (HAR) techniques regard activity labels as integer class IDs without explicitly modeling the semantics of class labels. We observe that different activity names often have shared structures. For example, "open door" and "open fridge" both have "open" as the action; "kicking soccer ball" and "playing tennis ball" both have "ball" as the object. Such shared structures in label names can be translated to the similarity in sensory data and modeling common structures would help uncover knowledge across different activities, especially for activities with limited samples. In this paper, we propose SHARE, a HAR framework that takes into account shared structures of label names for different activities. To exploit the shared structures, SHARE comprises an encoder for extracting features from input sensory time series and a decoder for generating label names as a token sequence. We also propose three label augmentation techniques to help the model more effecti
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#33539;&#24335;&#65292;&#23427;&#24378;&#35843;&#31995;&#32479;&#24615;&#22320;&#35774;&#35745;&#21644;&#26500;&#24314;&#25968;&#25454;&#23545;&#20110;&#24314;&#31435;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2212.11854</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Data-centric Artificial Intelligence. (arXiv:2212.11854v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11854
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#33539;&#24335;&#65292;&#23427;&#24378;&#35843;&#31995;&#32479;&#24615;&#22320;&#35774;&#35745;&#21644;&#26500;&#24314;&#25968;&#25454;&#23545;&#20110;&#24314;&#31435;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#65288;data-centric AI&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#33539;&#24335;&#65292;&#24378;&#35843;&#31995;&#32479;&#24615;&#22320;&#35774;&#35745;&#21644;&#26500;&#24314;&#25968;&#25454;&#23545;&#20110;&#24314;&#31435;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#21521;&#20449;&#24687;&#31995;&#32479;&#39046;&#22495;&#30340;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#20171;&#32461;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#12290;&#25105;&#20204;&#23450;&#20041;&#30456;&#20851;&#26415;&#35821;&#65292;&#25552;&#20379;&#20851;&#38190;&#29305;&#24449;&#26469;&#23545;&#27604;&#25968;&#25454;&#20013;&#24515;&#33539;&#24335;&#21644;&#27169;&#22411;&#20013;&#24515;&#33539;&#24335;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#21306;&#20998;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#21644;&#30456;&#20851;&#27010;&#24565;&#65292;&#24182;&#35752;&#35770;&#20854;&#23545;&#20449;&#24687;&#31995;&#32479;&#31038;&#21306;&#30340;&#38271;&#26399;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-centric artificial intelligence (data-centric AI) represents an emerging paradigm emphasizing that the systematic design and engineering of data is essential for building effective and efficient AI-based systems. The objective of this article is to introduce practitioners and researchers from the field of Information Systems (IS) to data-centric AI. We define relevant terms, provide key characteristics to contrast the data-centric paradigm to the model-centric one, and introduce a framework for data-centric AI. We distinguish data-centric AI from related concepts and discuss its longer-term implications for the IS community.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#30495;&#23454;&#35299;&#37322;&#30340;&#26032;&#27010;&#24565;&#65292;&#36890;&#36807;&#20613;&#31435;&#21494;&#20998;&#26512;&#33719;&#24471;&#20005;&#26684;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#22312;&#25903;&#25345;&#20551;&#35774;&#24773;&#26223;&#21644;&#38477;&#20302;&#35299;&#37322;&#35823;&#24046;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2210.17426</link><description>&lt;p&gt;
&#20613;&#31435;&#21494;&#20998;&#26512;&#23454;&#29616;&#19968;&#33268;&#19988;&#30495;&#23454;&#30340;&#27169;&#22411;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Consistent and Truthful Interpretation with Fourier Analysis. (arXiv:2210.17426v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17426
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#30495;&#23454;&#35299;&#37322;&#30340;&#26032;&#27010;&#24565;&#65292;&#36890;&#36807;&#20613;&#31435;&#21494;&#20998;&#26512;&#33719;&#24471;&#20005;&#26684;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#22312;&#25903;&#25345;&#20551;&#35774;&#24773;&#26223;&#21644;&#38477;&#20302;&#35299;&#37322;&#35823;&#24046;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35768;&#22810;&#36328;&#23398;&#31185;&#39046;&#22495;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#35299;&#37322;&#38656;&#35201;&#19982;&#24403;&#21069;&#26696;&#20363;&#30456;&#20851;&#30340;&#20551;&#35774;&#24773;&#26223;&#19968;&#33268;&#65292;&#21363;&#22914;&#26524;&#19968;&#20010;&#22240;&#32032;&#25913;&#21464;&#65292;&#27169;&#22411;&#20250;&#22914;&#20309;&#21453;&#24212;&#65311;&#23613;&#31649;&#24402;&#22240;&#26041;&#27861;&#30001;&#20248;&#38597;&#30340;&#20844;&#29702;&#31995;&#32479;&#25903;&#25345;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#20851;&#27880;&#21333;&#20010;&#36755;&#20837;&#65292;&#24182;&#19988;&#36890;&#24120;&#19981;&#19968;&#33268;&#12290;&#20026;&#25903;&#25345;&#20551;&#35774;&#24773;&#26223;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;&#30495;&#23454;&#35299;&#37322;&#30340;&#26032;&#27010;&#24565;&#65292;&#24182;&#24212;&#29992;&#24067;&#23572;&#20989;&#25968;&#30340;&#20613;&#31435;&#21494;&#20998;&#26512;&#26469;&#33719;&#24471;&#20005;&#26684;&#30340;&#20445;&#35777;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#21508;&#31181;&#21322;&#24452;&#30340;&#37051;&#22495;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#20197;&#23454;&#29616;2&#20493;&#33267;50&#20493;&#26356;&#20302;&#30340;&#35299;&#37322;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
For many interdisciplinary fields, ML interpretations need to be consistent with what-if scenarios related to the current case, i.e., if one factor changes, how does the model react? Although the attribution methods are supported by the elegant axiomatic systems, they mainly focus on individual inputs, and are generally inconsistent. To support what-if scenarios, we introduce a new notion called truthful interpretation, and apply Fourier analysis of Boolean functions to get rigorous guarantees. Experimental results show that for neighborhoods with various radii, our method achieves 2x - 50x lower interpretation error compared with the other methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#39057;&#32423;&#35821;&#20041;&#19968;&#33268;&#24615;&#24341;&#23548;&#32593;&#32476;&#65292;&#29992;&#20110;&#35270;&#21548;&#20107;&#20214;&#23450;&#20301;&#20219;&#21153;&#12290;&#23427;&#21033;&#29992;&#20107;&#20214;&#30340;&#35270;&#39057;&#32423;&#35821;&#20041;&#20449;&#24687;&#36827;&#34892;&#35821;&#20041;&#19968;&#33268;&#24615;&#24314;&#27169;&#65292;&#36890;&#36807;&#36328;&#27169;&#24577;&#20107;&#20214;&#34920;&#31034;&#25552;&#21462;&#22120;&#21644;&#20869;&#27169;&#24577;&#35821;&#20041;&#19968;&#33268;&#24615;&#22686;&#24378;&#22120;&#23454;&#29616;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#21644;&#21033;&#29992;&#21516;&#19968;&#35270;&#39057;&#20013;&#20107;&#20214;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.05242</link><description>&lt;p&gt;
&#21033;&#29992;&#20107;&#20214;&#30340;&#35270;&#39057;&#32423;&#35821;&#20041;&#19968;&#33268;&#24615;&#36827;&#34892;&#35270;&#21548;&#20107;&#20214;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Leveraging the Video-level Semantic Consistency of Event for Audio-visual Event Localization. (arXiv:2210.05242v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05242
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#39057;&#32423;&#35821;&#20041;&#19968;&#33268;&#24615;&#24341;&#23548;&#32593;&#32476;&#65292;&#29992;&#20110;&#35270;&#21548;&#20107;&#20214;&#23450;&#20301;&#20219;&#21153;&#12290;&#23427;&#21033;&#29992;&#20107;&#20214;&#30340;&#35270;&#39057;&#32423;&#35821;&#20041;&#20449;&#24687;&#36827;&#34892;&#35821;&#20041;&#19968;&#33268;&#24615;&#24314;&#27169;&#65292;&#36890;&#36807;&#36328;&#27169;&#24577;&#20107;&#20214;&#34920;&#31034;&#25552;&#21462;&#22120;&#21644;&#20869;&#27169;&#24577;&#35821;&#20041;&#19968;&#33268;&#24615;&#22686;&#24378;&#22120;&#23454;&#29616;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#21644;&#21033;&#29992;&#21516;&#19968;&#35270;&#39057;&#20013;&#20107;&#20214;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35270;&#21548;&#20107;&#20214;&#65288;AVE&#65289;&#23450;&#20301;&#24341;&#36215;&#20102;&#24456;&#22823;&#30340;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#21482;&#33021;&#29420;&#31435;&#22320;&#23545;&#20174;&#23436;&#25972;&#35270;&#39057;&#20013;&#20998;&#31163;&#20986;&#30340;&#27599;&#20010;&#35270;&#39057;&#27573;&#36827;&#34892;&#32534;&#30721;&#21644;&#20998;&#31867;&#65288;&#21487;&#35270;&#20026;&#20107;&#20214;&#30340;&#29255;&#27573;&#32423;&#34920;&#31034;&#65289;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24573;&#35270;&#20102;&#21516;&#19968;&#23436;&#25972;&#35270;&#39057;&#20013;&#20107;&#20214;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#65288;&#21487;&#35270;&#20026;&#20107;&#20214;&#30340;&#35270;&#39057;&#32423;&#34920;&#31034;&#65289;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#39057;&#32423;&#35821;&#20041;&#19968;&#33268;&#24615;&#24341;&#23548;&#32593;&#32476;&#29992;&#20110;AVE&#23450;&#20301;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20107;&#20214;&#35821;&#20041;&#19968;&#33268;&#24615;&#24314;&#27169;&#65288;ESCM&#65289;&#27169;&#22359;&#65292;&#29992;&#20110;&#25506;&#32034;&#35270;&#39057;&#32423;&#35821;&#20041;&#20449;&#24687;&#36827;&#34892;&#35821;&#20041;&#19968;&#33268;&#24615;&#24314;&#27169;&#12290;&#23427;&#30001;&#20004;&#20010;&#32452;&#20214;&#32452;&#25104;&#65306;&#36328;&#27169;&#24577;&#20107;&#20214;&#34920;&#31034;&#25552;&#21462;&#22120;&#65288;CERE&#65289;&#21644;&#20869;&#27169;&#24577;&#35821;&#20041;&#19968;&#33268;&#24615;&#22686;&#24378;&#22120;&#65288;ISCE&#65289;&#12290;CERE&#34987;&#25552;&#20986;&#29992;&#20110;&#33719;&#21462;&#35270;&#39057;&#32423;&#20107;&#20214;&#35821;&#20041;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;ISCE&#37319;&#21462;&#35270;&#39057;&#32423;&#20107;&#20214;&#35821;&#20041;&#20449;&#24687;&#24182;&#22686;&#24378;&#19968;&#33268;&#24615;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio-visual event (AVE) localization has attracted much attention in recent years. Most existing methods are often limited to independently encoding and classifying each video segment separated from the full video (which can be regarded as the segment-level representations of events). However, they ignore the semantic consistency of the event within the same full video (which can be considered as the video-level representations of events). In contrast to existing methods, we propose a novel video-level semantic consistency guidance network for the AVE localization task. Specifically, we propose an event semantic consistency modeling (ESCM) module to explore video-level semantic information for semantic consistency modeling. It consists of two components: a cross-modal event representation extractor (CERE) and an intra-modal semantic consistency enhancer (ISCE). CERE is proposed to obtain the event semantic information at the video level. Furthermore, ISCE takes video-level event seman
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;DESCN&#27169;&#22411;&#65292;&#36890;&#36807;&#28145;&#24230;&#25972;&#20307;&#31354;&#38388;&#20132;&#21449;&#32593;&#32476;&#30340;&#26041;&#24335;&#65292;&#20174;&#31471;&#21040;&#31471;&#30340;&#35282;&#24230;&#24314;&#27169;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#21644;&#26679;&#26412;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2207.09920</link><description>&lt;p&gt;
DESCN: &#28145;&#24230;&#25972;&#20307;&#31354;&#38388;&#20132;&#21449;&#32593;&#32476;&#29992;&#20110;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
DESCN: Deep Entire Space Cross Networks for Individual Treatment Effect Estimation. (arXiv:2207.09920v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.09920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;DESCN&#27169;&#22411;&#65292;&#36890;&#36807;&#28145;&#24230;&#25972;&#20307;&#31354;&#38388;&#20132;&#21449;&#32593;&#32476;&#30340;&#26041;&#24335;&#65292;&#20174;&#31471;&#21040;&#31471;&#30340;&#35282;&#24230;&#24314;&#27169;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#21644;&#26679;&#26412;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#22312;&#30005;&#23376;&#21830;&#21153;&#21644;&#31934;&#20934;&#21307;&#23398;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#24212;&#29992;&#65292;&#20854;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;(ITE)&#30340;&#20934;&#30830;&#20272;&#35745;&#12290;&#20256;&#32479;&#19978;&#65292;&#36890;&#36807;&#20998;&#21035;&#22312;&#21508;&#33258;&#30340;&#26679;&#26412;&#31354;&#38388;&#20013;&#24314;&#27169;&#21463;&#27835;&#30103;&#32452;&#21644;&#23545;&#29031;&#32452;&#30340;&#21709;&#24212;&#20989;&#25968;&#26469;&#39044;&#27979;ITE&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#36935;&#21040;&#20004;&#20010;&#38382;&#39064;&#65292;&#21363;&#30001;&#20110;&#27835;&#30103;&#20559;&#24046;&#32780;&#23548;&#33268;&#30340;&#21463;&#27835;&#30103;&#32452;&#21644;&#23545;&#29031;&#32452;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31163;&#65292;&#20197;&#21450;&#20854;&#20154;&#21475;&#35268;&#27169;&#20043;&#38388;&#30340;&#26174;&#33879;&#26679;&#26412;&#19981;&#24179;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#25972;&#20307;&#31354;&#38388;&#20132;&#21449;&#32593;&#32476;(DESCN)&#65292;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#24314;&#27169;&#27835;&#30103;&#25928;&#26524;&#12290;DESCN&#36890;&#36807;&#19968;&#20010;&#36328;&#32593;&#32476;&#20197;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#24335;&#25429;&#25417;&#27835;&#30103;&#20542;&#21521;&#12289;&#21709;&#24212;&#21644;&#38544;&#34255;&#27835;&#30103;&#25928;&#26524;&#30340;&#32508;&#21512;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25972;&#20010;&#26679;&#26412;&#31354;&#38388;&#20013;&#32852;&#21512;&#23398;&#20064;&#27835;&#30103;&#21644;&#21709;&#24212;&#20989;&#25968;&#65292;&#20197;&#36991;&#20813;&#27835;&#30103;&#20559;&#24046;&#65292;&#24182;&#37319;&#29992;&#20013;&#38388;&#20266;&#22788;&#29702;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal Inference has wide applications in various areas such as E-commerce and precision medicine, and its performance heavily relies on the accurate estimation of the Individual Treatment Effect (ITE). Conventionally, ITE is predicted by modeling the treated and control response functions separately in their individual sample spaces. However, such an approach usually encounters two issues in practice, i.e. divergent distribution between treated and control groups due to treatment bias, and significant sample imbalance of their population sizes. This paper proposes Deep Entire Space Cross Networks (DESCN) to model treatment effects from an end-to-end perspective. DESCN captures the integrated information of the treatment propensity, the response, and the hidden treatment effect through a cross network in a multi-task learning manner. Our method jointly learns the treatment and response functions in the entire sample space to avoid treatment bias and employs an intermediate pseudo treat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#21098;&#24863;&#30693;&#30340;&#31232;&#30095;&#27491;&#21017;&#21270;&#30340;&#32593;&#32476;&#21098;&#26525;&#26041;&#27861;&#65292;&#20351;&#29992;&#31934;&#32454;&#30340;&#31232;&#30095;&#27491;&#21017;&#21270;&#20165;&#23545;&#30001;&#21098;&#26525;&#25513;&#27169;&#36873;&#25321;&#30340;&#29305;&#23450;&#28388;&#27874;&#22120;&#36827;&#34892;&#22788;&#29702;&#65292;&#20174;&#32780;&#20943;&#23569;&#32593;&#32476;&#23481;&#37327;&#38480;&#21046;&#21644;&#27424;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2201.06776</link><description>&lt;p&gt;
&#38024;&#23545;&#32593;&#32476;&#21098;&#26525;&#30340;&#20462;&#21098;&#24863;&#30693;&#31232;&#30095;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Pruning-aware Sparse Regularization for Network Pruning. (arXiv:2201.06776v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.06776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#21098;&#24863;&#30693;&#30340;&#31232;&#30095;&#27491;&#21017;&#21270;&#30340;&#32593;&#32476;&#21098;&#26525;&#26041;&#27861;&#65292;&#20351;&#29992;&#31934;&#32454;&#30340;&#31232;&#30095;&#27491;&#21017;&#21270;&#20165;&#23545;&#30001;&#21098;&#26525;&#25513;&#27169;&#36873;&#25321;&#30340;&#29305;&#23450;&#28388;&#27874;&#22120;&#36827;&#34892;&#22788;&#29702;&#65292;&#20174;&#32780;&#20943;&#23569;&#32593;&#32476;&#23481;&#37327;&#38480;&#21046;&#21644;&#27424;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#26088;&#22312;&#36890;&#36807;&#20462;&#21098;&#23545;&#26368;&#32456;&#36755;&#20986;&#31934;&#24230;&#19981;&#37325;&#35201;&#30340;&#28388;&#27874;&#22120;&#65292;&#20174;&#32780;&#21435;&#38500;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#20013;&#30340;&#20887;&#20313;&#36890;&#36947;&#12290;&#20026;&#20102;&#20943;&#23569;&#21098;&#26525;&#21518;&#24615;&#33021;&#30340;&#19979;&#38477;&#65292;&#35768;&#22810;&#26041;&#27861;&#21033;&#29992;&#31232;&#30095;&#27491;&#21017;&#21270;&#30340;&#25439;&#22833;&#26469;&#20135;&#29983;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#36825;&#20123;&#22522;&#20110;&#31232;&#30095;&#27491;&#21017;&#21270;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#23545;&#26410;&#20462;&#21098;&#36890;&#36947;&#36827;&#34892;&#27491;&#21017;&#21270;&#26159;&#19981;&#24517;&#35201;&#30340;&#65292;&#32780;&#19988;&#38480;&#21046;&#20102;&#32593;&#32476;&#30340;&#23481;&#37327;&#65292;&#23548;&#33268;&#27424;&#25311;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;MaskSparsity&#65292;&#37319;&#29992;&#20102;&#20462;&#21098;&#24863;&#30693;&#30340;&#31232;&#30095;&#27491;&#21017;&#21270;&#12290;MaskSparsity&#20165;&#23545;&#30001;&#20462;&#21098;&#25513;&#27169;&#36873;&#25321;&#30340;&#29305;&#23450;&#28388;&#27874;&#22120;&#26045;&#21152;&#31934;&#32454;&#30340;&#31232;&#30095;&#27491;&#21017;&#21270;&#65292;&#32780;&#19981;&#26159;&#23545;&#27169;&#22411;&#30340;&#25152;&#26377;&#28388;&#27874;&#22120;&#26045;&#21152;&#12290;&#22312;MaskSparity&#30340;&#31934;&#32454;&#31232;&#30095;&#27491;&#21017;&#21270;&#20043;&#21069;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#35768;&#22810;&#26041;&#27861;&#26469;&#33719;&#21462;&#21098;&#26525;&#25513;&#27169;&#65292;&#20363;&#22914;&#36816;&#34892;&#20840;&#23616;&#31232;&#30095;&#27491;&#21017;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structural neural network pruning aims to remove the redundant channels in the deep convolutional neural networks (CNNs) by pruning the filters of less importance to the final output accuracy. To reduce the degradation of performance after pruning, many methods utilize the loss with sparse regularization to produce structured sparsity. In this paper, we analyze these sparsity-training-based methods and find that the regularization of unpruned channels is unnecessary. Moreover, it restricts the network's capacity, which leads to under-fitting. To solve this problem, we propose a novel pruning method, named MaskSparsity, with pruning-aware sparse regularization. MaskSparsity imposes the fine-grained sparse regularization on the specific filters selected by a pruning mask, rather than all the filters of the model. Before the fine-grained sparse regularization of MaskSparity, we can use many methods to get the pruning mask, such as running the global sparse regularization. MaskSparsity ach
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;ReLU'(0)&#20540;&#23545;&#28145;&#24230;&#23398;&#20064;&#20013;&#21453;&#21521;&#20256;&#25773;&#30340;&#25968;&#20540;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;32&#20301;&#31934;&#24230;&#19979;&#20250;&#20986;&#29616;&#26174;&#33879;&#30340;&#21464;&#21270;&#65292;&#32780;&#22312;16&#20301;&#31934;&#24230;&#19979;&#26159;&#31995;&#32479;&#24615;&#30340;&#12290;&#22312;&#26222;&#36890;&#30340;SGD&#35757;&#32451;&#20013;&#65292;&#36873;&#25321;ReLU'(0) = 0&#20284;&#20046;&#26159;&#26368;&#26377;&#25928;&#30340;&#12290;&#27492;&#22806;&#65292;&#37325;&#26032;&#35843;&#25972;&#26041;&#27861; tend to buffer ReLU'(0)&#20540;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2106.12915</link><description>&lt;p&gt;
ReLU'(0)&#23545;&#21453;&#21521;&#20256;&#25773;&#30340;&#25968;&#20540;&#24433;&#21709;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Numerical influence of ReLU'(0) on backpropagation. (arXiv:2106.12915v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.12915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;ReLU'(0)&#20540;&#23545;&#28145;&#24230;&#23398;&#20064;&#20013;&#21453;&#21521;&#20256;&#25773;&#30340;&#25968;&#20540;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;32&#20301;&#31934;&#24230;&#19979;&#20250;&#20986;&#29616;&#26174;&#33879;&#30340;&#21464;&#21270;&#65292;&#32780;&#22312;16&#20301;&#31934;&#24230;&#19979;&#26159;&#31995;&#32479;&#24615;&#30340;&#12290;&#22312;&#26222;&#36890;&#30340;SGD&#35757;&#32451;&#20013;&#65292;&#36873;&#25321;ReLU'(0) = 0&#20284;&#20046;&#26159;&#26368;&#26377;&#25928;&#30340;&#12290;&#27492;&#22806;&#65292;&#37325;&#26032;&#35843;&#25972;&#26041;&#27861; tend to buffer ReLU'(0)&#20540;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29702;&#35770;&#19978;&#65292;&#31070;&#32463;&#32593;&#32476;&#20013;ReLU'(0)&#22312;[0, 1]&#33539;&#22260;&#20869;&#30340;&#36873;&#25321;&#23545;&#21453;&#21521;&#20256;&#25773;&#21644;&#35757;&#32451;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;32&#20301;&#40664;&#35748;&#31934;&#24230;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#38382;&#39064;&#30340;&#35268;&#27169;&#65292;&#20351;&#20854;&#25104;&#20026;&#35757;&#32451;&#26041;&#27861;&#30340;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;ReLU'(0)&#20540;&#23545;&#20960;&#31181;&#31934;&#24230;&#27700;&#24179;&#65288;16&#20301;&#65292;32&#20301;&#65292;64&#20301;&#65289;&#12289;&#21508;&#31181;&#32593;&#32476;&#65288;&#20840;&#36830;&#25509;&#12289;VGG&#12289;ResNet&#65289;&#21644;&#25968;&#25454;&#38598;&#65288;MNIST&#12289;CIFAR10&#12289;SVHN&#65289;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;32&#20301;&#31934;&#24230;&#19979;&#65292;&#21453;&#21521;&#20256;&#25773;&#36755;&#20986;&#20986;&#29616;&#20102;&#26174;&#33879;&#30340;&#21464;&#21270;&#65292;&#36825;&#31181;&#24773;&#20917;&#22823;&#32422;&#20986;&#29616;&#20102;&#19968;&#21322;&#30340;&#26102;&#38388;&#12290;&#36825;&#31181;&#24433;&#21709;&#22312;&#21452;&#31934;&#24230;&#19979;&#28040;&#22833;&#65292;&#32780;&#22312;16&#20301;&#31934;&#24230;&#19979;&#26159;&#31995;&#32479;&#24615;&#30340;&#12290;&#23545;&#20110;&#26222;&#36890;&#30340;SGD&#35757;&#32451;&#32780;&#35328;&#65292;&#36873;&#25321;ReLU'(0) = 0&#20284;&#20046;&#26159;&#26368;&#26377;&#25928;&#30340;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#25209;&#24402;&#19968;&#21270;&#25110;ADAM&#31561;&#37325;&#26032;&#35843;&#25972;&#26041;&#27861; tend to buffer ReLU'(0)&#20540;&#30340;&#24433;&#21709;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24819;&#35201;&#20256;&#36798;&#30340;&#20449;&#24687;&#26159;&#65292;&#38750;&#20809;&#28369;&#38382;&#39064;&#30340;&#31639;&#27861;&#24494;&#20998;&#21487;&#33021;&#38544;&#34255;&#20102;&#19968;&#20123;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In theory, the choice of ReLU'(0) in [0, 1] for a neural network has a negligible influence both on backpropagation and training. Yet, in the real world, 32 bits default precision combined with the size of deep learning problems makes it a hyperparameter of training methods. We investigate the importance of the value of ReLU'(0) for several precision levels (16, 32, 64 bits), on various networks (fully connected, VGG, ResNet) and datasets (MNIST, CIFAR10, SVHN). We observe considerable variations of backpropagation outputs which occur around half of the time in 32 bits precision. The effect disappears with double precision, while it is systematic at 16 bits. For vanilla SGD training, the choice ReLU'(0) = 0 seems to be the most efficient. We also evidence that reconditioning approaches as batch-norm or ADAM tend to buffer the influence of ReLU'(0)'s value. Overall, the message we want to convey is that algorithmic differentiation of nonsmooth problems potentially hides parameters that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29983;&#21629;&#21608;&#26399;&#31574;&#30053;&#37325;&#29992;&#31639;&#27861;&#21644;&#20219;&#21153;&#23481;&#37327;&#25351;&#26631;&#65292;&#36890;&#36807;&#20248;&#21270;&#19968;&#32452;&#36817;&#20284;&#26368;&#20248;&#31574;&#30053;&#21644;&#31574;&#30053;&#36873;&#25321;&#65292;&#23454;&#29616;&#20102;&#22312;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#20013;&#36991;&#20813;&#29983;&#25104;&#22823;&#37327;&#31574;&#30053;&#30340;&#30446;&#26631;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#29983;&#21629;&#21608;&#26399;&#31574;&#30053;&#37325;&#29992;&#21644;&#20219;&#21153;&#23481;&#37327;&#39044;&#36873;&#23545;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2106.01741</link><description>&lt;p&gt;
&#29983;&#21629;&#21608;&#26399;&#31574;&#30053;&#37325;&#29992;&#19982;&#20219;&#21153;&#23481;&#37327;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Lifetime policy reuse and the importance of task capacity. (arXiv:2106.01741v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.01741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29983;&#21629;&#21608;&#26399;&#31574;&#30053;&#37325;&#29992;&#31639;&#27861;&#21644;&#20219;&#21153;&#23481;&#37327;&#25351;&#26631;&#65292;&#36890;&#36807;&#20248;&#21270;&#19968;&#32452;&#36817;&#20284;&#26368;&#20248;&#31574;&#30053;&#21644;&#31574;&#30053;&#36873;&#25321;&#65292;&#23454;&#29616;&#20102;&#22312;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#20013;&#36991;&#20813;&#29983;&#25104;&#22823;&#37327;&#31574;&#30053;&#30340;&#30446;&#26631;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#29983;&#21629;&#21608;&#26399;&#31574;&#30053;&#37325;&#29992;&#21644;&#20219;&#21153;&#23481;&#37327;&#39044;&#36873;&#23545;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#19968;&#30452;&#23384;&#22312;&#19968;&#20010;&#25361;&#25112;&#65292;&#21363;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#12290;&#22312;&#36825;&#31181;&#23398;&#20064;&#20013;&#65292;&#23398;&#20064;&#32773;&#25353;&#39034;&#24207;&#25509;&#25910;&#22810;&#20010;&#20219;&#21153;&#65292;&#24182;&#22312;&#20219;&#21153;&#20043;&#38388;&#20256;&#36882;&#30693;&#35782;&#65292;&#21516;&#26102;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#31574;&#30053;&#37325;&#29992;&#21644;&#20854;&#20182;&#22810;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#23398;&#20064;&#22810;&#20010;&#20219;&#21153;&#65292;&#20294;&#21487;&#33021;&#20250;&#29983;&#25104;&#22823;&#37327;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#21019;&#26032;&#36129;&#29486;&#65292;&#21363;1) &#29983;&#21629;&#21608;&#26399;&#31574;&#30053;&#37325;&#29992;&#65292;&#36825;&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#31574;&#30053;&#37325;&#29992;&#31639;&#27861;&#65292;&#36890;&#36807;&#31574;&#30053;&#20248;&#21270;&#21644;&#33258;&#36866;&#24212;&#31574;&#30053;&#36873;&#25321;&#30340;&#32452;&#21512;&#26469;&#20248;&#21270;&#19968;&#32452;&#36817;&#20284;&#26368;&#20248;&#31574;&#30053;&#65292;&#36991;&#20813;&#29983;&#25104;&#36807;&#22810;&#31574;&#30053;&#65307;2) &#20219;&#21153;&#23481;&#37327;&#65292;&#19968;&#31181;&#34913;&#37327;&#31574;&#30053;&#33021;&#20934;&#30830;&#35299;&#20915;&#30340;&#26368;&#22823;&#20219;&#21153;&#25968;&#37327;&#30340;&#25351;&#26631;&#12290;&#36890;&#36807;&#27604;&#36739;&#20004;&#31181;&#20808;&#36827;&#30340;&#22522;&#30784;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#19968;&#20010;&#21253;&#21547;18&#20010;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;Pacman&#20219;&#21153;&#21644;&#19968;&#20010;&#26368;&#22810;&#21253;&#21547;125&#20010;&#20219;&#21153;&#30340;Cartpole&#20219;&#21153;&#20013;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#29983;&#21629;&#21608;&#26399;&#31574;&#30053;&#37325;&#29992;&#21644;&#22522;&#20110;&#20219;&#21153;&#23481;&#37327;&#30340;&#39044;&#36873;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A long-standing challenge in artificial intelligence is lifelong reinforcement learning, where learners are given many tasks in sequence and must transfer knowledge between tasks while avoiding catastrophic forgetting. Policy reuse and other multi-policy reinforcement learning techniques can learn multiple tasks but may generate many policies. This paper presents two novel contributions, namely 1) Lifetime Policy Reuse, a model-agnostic policy reuse algorithm that avoids generating many policies by optimising a fixed number of near-optimal policies through a combination of policy optimisation and adaptive policy selection; and 2) the task capacity, a measure for the maximal number of tasks that a policy can accurately solve. Comparing two state-of-the-art base-learners, the results demonstrate the importance of Lifetime Policy Reuse and task capacity based pre-selection on an 18-task partially observable Pacman domain and a Cartpole domain of up to 125 tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21457;&#29616;&#20102;&#26435;&#37325;&#34928;&#20943;&#22312;&#35757;&#32451;&#30340;&#26368;&#21518;&#38454;&#27573;&#20250;&#23548;&#33268;&#22823;&#26799;&#24230;&#33539;&#25968;&#30340;&#38519;&#38449;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;Scheduled Weight Decay&#65288;SWD&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#26435;&#37325;&#34928;&#20943;&#24378;&#24230;&#24182;&#24809;&#32602;&#22823;&#26799;&#24230;&#33539;&#25968;&#65292;&#21487;&#20197;&#26377;&#25928;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2011.11152</link><description>&lt;p&gt;
&#20851;&#20110;&#26435;&#37325;&#34928;&#20943;&#30340;&#24120;&#34987;&#24573;&#35270;&#30340;&#38519;&#38449;&#21450;&#20854;&#22914;&#20309;&#32531;&#35299;&#65306;&#26799;&#24230;&#33539;&#25968;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
On the Overlooked Pitfalls of Weight Decay and How to Mitigate Them: A Gradient-Norm Perspective. (arXiv:2011.11152v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.11152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21457;&#29616;&#20102;&#26435;&#37325;&#34928;&#20943;&#22312;&#35757;&#32451;&#30340;&#26368;&#21518;&#38454;&#27573;&#20250;&#23548;&#33268;&#22823;&#26799;&#24230;&#33539;&#25968;&#30340;&#38519;&#38449;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;Scheduled Weight Decay&#65288;SWD&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#26435;&#37325;&#34928;&#20943;&#24378;&#24230;&#24182;&#24809;&#32602;&#22823;&#26799;&#24230;&#33539;&#25968;&#65292;&#21487;&#20197;&#26377;&#25928;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26435;&#37325;&#34928;&#20943;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#35757;&#32451;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#23613;&#31649;&#26435;&#37325;&#34928;&#20943;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#20294;&#20808;&#21069;&#30340;&#30740;&#31350;&#26410;&#33021;&#21457;&#29616;&#26435;&#37325;&#34928;&#20943;&#23548;&#33268;&#30340;&#22823;&#26799;&#24230;&#33539;&#25968;&#30340;&#19968;&#20123;&#34987;&#24573;&#35270;&#30340;&#38519;&#38449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#26435;&#37325;&#34928;&#20943;&#19981;&#24184;&#22320;&#20250;&#23548;&#33268;&#35757;&#32451;&#30340;&#26368;&#21518;&#38454;&#27573;&#65288;&#25110;&#32456;&#27490;&#30340;&#35299;&#65289;&#20986;&#29616;&#22823;&#26799;&#24230;&#33539;&#25968;&#65292;&#36825;&#36890;&#24120;&#34920;&#26126;&#20102;&#31967;&#31957;&#30340;&#25910;&#25947;&#21644;&#24046;&#21170;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#20026;&#20102;&#32531;&#35299;&#20197;&#26799;&#24230;&#33539;&#25968;&#20026;&#20013;&#24515;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#23454;&#29992;&#30340;&#26435;&#37325;&#34928;&#20943;&#35843;&#24230;&#22120;&#65292;&#31216;&#20026;Scheduled Weight Decay&#65288;SWD&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#26799;&#24230;&#33539;&#25968;&#21160;&#24577;&#35843;&#25972;&#26435;&#37325;&#34928;&#20943;&#24378;&#24230;&#65292;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26174;&#33879;&#24809;&#32602;&#22823;&#26799;&#24230;&#33539;&#25968;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20063;&#35777;&#26126;&#65292;SWD&#30830;&#23454;&#32531;&#35299;&#20102;&#22823;&#26799;&#24230;&#33539;&#25968;&#38382;&#39064;&#65292;&#24182;&#19988;&#36890;&#24120;&#26126;&#26174;&#20248;&#20110;&#24120;&#35268;&#30340;&#24658;&#23450;&#26435;&#37325;&#34928;&#20943;&#31574;&#30053;&#65288;AMEN&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weight decay is a simple yet powerful regularization technique that has been very widely used in training of deep neural networks (DNNs). While weight decay has attracted much attention, previous studies fail to discover some overlooked pitfalls on large gradient norms resulted by weight decay. In this paper, we discover that, weight decay can unfortunately lead to large gradient norms at the final phase (or the terminated solution) of training, which often indicates bad convergence and poor generalization. To mitigate the gradient-norm-centered pitfalls, we present the first practical scheduler for weight decay, called the Scheduled Weight Decay (SWD) method that can dynamically adjust the weight decay strength according to the gradient norm and significantly penalize large gradient norms during training. Our experiments also support that SWD indeed mitigates large gradient norms and often significantly outperforms the conventional constant weight decay strategy for Adaptive Moment Es
&lt;/p&gt;</description></item></channel></rss>