<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23545;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#36827;&#34892;&#31639;&#27861;&#20844;&#27491;&#24615;&#30417;&#25511;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#30417;&#27979;&#20844;&#27491;&#24615;&#23646;&#24615;&#65292;&#22914;&#20154;&#21475;&#24179;&#31561;&#21644;&#31561;&#21516;&#26426;&#20250;&#65292;&#20026;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#19981;&#22522;&#20110;&#25935;&#24863;&#23646;&#24615;&#27495;&#35270;&#20010;&#20154;&#25552;&#20379;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2305.15979</link><description>&lt;p&gt;
&#30417;&#25511;&#31639;&#27861;&#20844;&#27491;&#24615;
&lt;/p&gt;
&lt;p&gt;
Monitoring Algorithmic Fairness. (arXiv:2305.15979v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23545;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#36827;&#34892;&#31639;&#27861;&#20844;&#27491;&#24615;&#30417;&#25511;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#30417;&#27979;&#20844;&#27491;&#24615;&#23646;&#24615;&#65292;&#22914;&#20154;&#21475;&#24179;&#31561;&#21644;&#31561;&#21516;&#26426;&#20250;&#65292;&#20026;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#19981;&#22522;&#20110;&#25935;&#24863;&#23646;&#24615;&#27495;&#35270;&#20010;&#20154;&#25552;&#20379;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#23545;&#20154;&#31867;&#20570;&#20986;&#20915;&#31574;&#65292;&#22240;&#27492;&#30830;&#20445;&#23427;&#20204;&#26159;&#20844;&#27491;&#30340;&#65288;&#21363;&#19981;&#20250;&#22522;&#20110;&#25935;&#24863;&#23646;&#24615;&#27495;&#35270;&#20010;&#20154;&#65289;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#27169;&#22411;&#26410;&#30693;&#20294;&#20551;&#23450;&#20855;&#26377;&#39532;&#23572;&#21487;&#22827;&#38142;&#32467;&#26500;&#30340;&#31995;&#32479;&#30340;&#31639;&#27861;&#20844;&#27491;&#24615;&#30340;&#36816;&#34892;&#26102;&#39564;&#35777;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#35268;&#33539;&#35821;&#35328;&#65292;&#21487;&#20197;&#24314;&#27169;&#35768;&#22810;&#24120;&#35265;&#30340;&#31639;&#27861;&#20844;&#27491;&#24615;&#23646;&#24615;&#65292;&#20363;&#22914;&#20154;&#21475;&#24179;&#31561;&#65292;&#31561;&#21516;&#26426;&#20250;&#21644;&#31038;&#20250;&#36127;&#25285;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#30417;&#35270;&#22120;&#65292;&#35266;&#23519;&#32473;&#23450;&#31995;&#32479;&#29983;&#25104;&#30340;&#19968;&#31995;&#21015;&#20107;&#20214;&#65292;&#24182;&#22312;&#27599;&#27425;&#35266;&#23519;&#20043;&#21518;&#36755;&#20986;&#35813;&#31995;&#32479;&#22312;&#21040;&#30446;&#21069;&#20026;&#27490;&#30340;&#36816;&#34892;&#20013;&#26377;&#22810;&#20844;&#27491;&#25110;&#26377;&#22810;&#20542;&#21521;&#20110;&#20559;&#35265;&#30340;&#23450;&#37327;&#20272;&#35745;&#12290;&#35813;&#20272;&#35745;&#20540;&#22312;&#32473;&#23450;&#30340;&#32622;&#20449;&#27700;&#24179;&#21644;&#21464;&#37327;&#35823;&#24046;&#33539;&#22260;&#19979;&#34987;&#35777;&#26126;&#26159;&#27491;&#30830;&#30340;&#65292;&#20854;&#20013;&#35823;&#24046;&#33539;&#22260;&#38543;&#30528;&#35266;&#23519;&#24207;&#21015;&#30340;&#38271;&#24230;&#32780;&#21464;&#24471;&#26356;&#32039;&#12290;&#25105;&#20204;&#30340;&#30417;&#35270;&#22120;&#26377;&#20004;&#31181;&#31867;&#22411;&#65292;&#20998;&#21035;&#20351;&#29992;&#39057;&#29575;&#27966;&#21644;&#36125;&#21494;&#26031;&#32479;&#35745;&#25512;&#26029;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine-learned systems are in widespread use for making decisions about humans, and it is important that they are fair, i.e., not biased against individuals based on sensitive attributes. We present runtime verification of algorithmic fairness for systems whose models are unknown, but are assumed to have a Markov chain structure. We introduce a specification language that can model many common algorithmic fairness properties, such as demographic parity, equal opportunity, and social burden. We build monitors that observe a long sequence of events as generated by a given system, and output, after each observation, a quantitative estimate of how fair or biased the system was on that run until that point in time. The estimate is proven to be correct modulo a variable error bound and a given confidence level, where the error bound gets tighter as the observed sequence gets longer. Our monitors are of two types, and use, respectively, frequentist and Bayesian statistical inference techniqu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#29992;&#20154;&#24037;&#27169;&#25311;&#30740;&#31350;&#37327;&#21270;&#35299;&#37322;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#22312;&#26377;&#29992;&#24615;&#65292;&#20197;&#35299;&#20915;&#35780;&#20272;&#35299;&#37322;&#36136;&#37327;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#30456;&#20851;&#35299;&#37322;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26679;&#26412;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15961</link><description>&lt;p&gt;
&#29992;&#20154;&#24037;&#27169;&#25311;&#30740;&#31350;&#37327;&#21270;&#35299;&#37322;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#22312;&#26377;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Quantifying the Intrinsic Usefulness of Attributional Explanations for Graph Neural Networks with Artificial Simulatability Studies. (arXiv:2305.15961v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#29992;&#20154;&#24037;&#27169;&#25311;&#30740;&#31350;&#37327;&#21270;&#35299;&#37322;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#22312;&#26377;&#29992;&#24615;&#65292;&#20197;&#35299;&#20915;&#35780;&#20272;&#35299;&#37322;&#36136;&#37327;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#30456;&#20851;&#35299;&#37322;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26679;&#26412;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20294;&#35780;&#20272;&#35299;&#37322;&#36136;&#37327;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22240;&#20026;&#19982;&#20154;&#31867;&#23454;&#39564;&#30456;&#20851;&#30340;&#39640;&#25104;&#26412;&#65292;&#36890;&#24120;&#20351;&#29992;&#21508;&#31181;&#20195;&#29702;&#24230;&#37327;&#26469;&#36817;&#20284;&#37327;&#21270;&#35299;&#37322;&#36136;&#37327;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#20154;&#24037;&#27169;&#25311;&#30740;&#31350;&#25193;&#23637;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#39046;&#22495;&#12290;&#25105;&#20204;&#20351;&#29992;&#25903;&#25345;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27169;&#25311;&#24615;&#30740;&#31350;&#65292;&#37327;&#21270;&#24402;&#22240;&#22270;&#35299;&#37322;&#30340;&#20869;&#22312;&#26377;&#29992;&#24615;&#65292;&#32780;&#19981;&#26159;&#26114;&#36149;&#30340;&#20154;&#31867;&#35797;&#39564;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#28040;&#34701;&#30740;&#31350;&#65292;&#20197;&#35843;&#26597;&#25152;&#25552;&#20986;&#30340;&#20998;&#26512;&#22312;&#21738;&#20123;&#26465;&#20214;&#19979;&#26368;&#26377;&#24847;&#20041;&#12290;&#25105;&#20204;&#36824;&#22312;&#23454;&#38469;&#30340;&#22270;&#20998;&#31867;&#21644;&#22238;&#24402;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#30456;&#20851;&#35299;&#37322;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26679;&#26412;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the increasing relevance of explainable AI, assessing the quality of explanations remains a challenging issue. Due to the high costs associated with human-subject experiments, various proxy metrics are often used to approximately quantify explanation quality. Generally, one possible interpretation of the quality of an explanation is its inherent value for teaching a related concept to a student. In this work, we extend artificial simulatability studies to the domain of graph neural networks. Instead of costly human trials, we use explanation-supervisable graph neural networks to perform simulatability studies to quantify the inherent usefulness of attributional graph explanations. We perform an extensive ablation study to investigate the conditions under which the proposed analyses are most meaningful. We additionally validate our methods applicability on real-world graph classification and regression datasets. We find that relevant explanations can significantly boost the samp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#28436;&#21270;&#31070;&#32463;&#20803;&#20013;&#24515;&#21442;&#25968;&#26469;&#33719;&#24471;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#31070;&#32463;&#20803;&#65292;&#20174;&#32780;&#25191;&#34892;&#22797;&#26434;&#35745;&#31639;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#26174;&#31034;&#65292;&#28436;&#21270;&#31070;&#32463;&#21442;&#25968;&#23601;&#36275;&#20197;&#20351;&#20195;&#29702;&#22312;&#19981;&#20248;&#21270;&#20219;&#20309;&#31361;&#35302;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.15945</link><description>&lt;p&gt;
&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#20013;&#36890;&#36807;&#31070;&#32463;&#20803;&#22810;&#26679;&#24615;&#30340;&#28436;&#21270;&#23398;&#20064;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Learning to Act through Evolution of Neural Diversity in Random Neural Networks. (arXiv:2305.15945v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#28436;&#21270;&#31070;&#32463;&#20803;&#20013;&#24515;&#21442;&#25968;&#26469;&#33719;&#24471;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#31070;&#32463;&#20803;&#65292;&#20174;&#32780;&#25191;&#34892;&#22797;&#26434;&#35745;&#31639;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#26174;&#31034;&#65292;&#28436;&#21270;&#31070;&#32463;&#21442;&#25968;&#23601;&#36275;&#20197;&#20351;&#20195;&#29702;&#22312;&#19981;&#20248;&#21270;&#20219;&#20309;&#31361;&#35302;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#31070;&#32463;&#31995;&#32479;&#30001;&#19981;&#21516;&#31867;&#21035;&#31070;&#32463;&#20803;&#32452;&#25104;&#30340;&#32593;&#32476;&#26500;&#25104;&#65292;&#23427;&#20204;&#26159;&#22810;&#26679;&#21270;&#12289;&#22797;&#26434;&#30340;&#20449;&#24687;&#22788;&#29702;&#22120;&#12290;&#22312;&#22823;&#22810;&#25968;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#31070;&#32463;&#35745;&#31639;&#34987;&#25277;&#35937;&#20026;&#19968;&#20010;&#28608;&#27963;&#20989;&#25968;&#65292;&#36890;&#24120;&#26159;&#22312;&#25972;&#20010;&#32593;&#32476;&#25110;&#21516;&#19968;&#23618;&#20013;&#30340;&#25152;&#26377;&#31070;&#32463;&#20803;&#20043;&#38388;&#20849;&#20139;&#30340;&#65307;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#20027;&#35201;&#20851;&#27880;&#31361;&#35302;&#26435;&#37325;&#30340;&#20248;&#21270;&#12290;&#26412;&#25991;&#25512;&#33616;&#36890;&#36807;&#20248;&#21270;&#31070;&#32463;&#20803;&#20013;&#24515;&#21442;&#25968;&#26469;&#33719;&#24471;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#31070;&#32463;&#20803;&#65292;&#20174;&#32780;&#25191;&#34892;&#22797;&#26434;&#35745;&#31639;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#35777;&#26126;&#20165;&#36890;&#36807;&#28436;&#21270;&#31070;&#32463;&#21442;&#25968;&#21363;&#21487;&#20351;&#20195;&#29702;&#22312;&#19981;&#20248;&#21270;&#20219;&#20309;&#31361;&#35302;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#21508;&#31181;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#12290;&#34429;&#28982;&#19981;&#33268;&#21147;&#20110;&#31934;&#30830;&#30340;&#29983;&#29289;&#27169;&#22411;&#65292;&#20294;&#36890;&#36807;&#27604;&#24403;&#21069;&#24120;&#35265;&#30340;&#20570;&#27861;&#26356;&#22823;&#31243;&#24230;&#22320;&#23545;&#31070;&#32463;&#20803;&#21442;&#25968;&#21270;&#65292;&#25105;&#20204;&#33021;&#22815;&#25506;&#35752;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#20013;&#31070;&#32463;&#22810;&#26679;&#24615;&#25552;&#20379;&#30340;&#35745;&#31639;&#33021;&#21147;&#12290;&#36825;&#20123;&#32467;&#26524;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#26497;&#22823;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biological nervous systems consist of networks of diverse, sophisticated information processors in the form of neurons of different classes. In most artificial neural networks (ANNs), neural computation is abstracted to an activation function that is usually shared between all neurons within a layer or even the whole network; training of ANNs focuses on synaptic optimization. In this paper, we propose the optimization of neuro-centric parameters to attain a set of diverse neurons that can perform complex computations. Demonstrating the promise of the approach, we show that evolving neural parameters alone allows agents to solve various reinforcement learning tasks without optimizing any synaptic weights. While not aiming to be an accurate biological model, parameterizing neurons to a larger degree than the current common practice, allows us to ask questions about the computational abilities afforded by neural diversity in random neural networks. The presented results open up interestin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#30693;&#35782;&#22270;&#23884;&#20837;&#27169;&#22411;&#36716;&#21270;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#31934;&#30830;MLE&#23398;&#20064;&#12289;&#26377;&#25928;&#25277;&#26679;&#26032;&#30340;&#19977;&#20803;&#32452;&#20197;&#21450;&#20445;&#35777;&#36923;&#36753;&#32422;&#26463;&#65292;&#33719;&#24471;&#20102;&#27604;&#21407;&#22987;KGEs&#26356;&#20855;&#20280;&#32553;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15944</link><description>&lt;p&gt;
&#22914;&#20309;&#36890;&#36807;&#27010;&#29575;&#30005;&#36335;&#23558;&#24744;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#36716;&#21270;&#20026;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
How to Turn Your Knowledge Graph Embeddings into Generative Models via Probabilistic Circuits. (arXiv:2305.15944v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#30693;&#35782;&#22270;&#23884;&#20837;&#27169;&#22411;&#36716;&#21270;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#31934;&#30830;MLE&#23398;&#20064;&#12289;&#26377;&#25928;&#25277;&#26679;&#26032;&#30340;&#19977;&#20803;&#32452;&#20197;&#21450;&#20445;&#35777;&#36923;&#36753;&#32422;&#26463;&#65292;&#33719;&#24471;&#20102;&#27604;&#21407;&#22987;KGEs&#26356;&#20855;&#20280;&#32553;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20123;&#25104;&#21151;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#27169;&#22411;&#21487;&#29992;&#20316;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65292;&#32780;&#36825;&#31687;&#35770;&#25991;&#35299;&#37322;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#24471;&#20998;&#20989;&#25968;&#65292;&#23558;&#20854;&#37325;&#26032;&#35299;&#37322;&#25104;&#20026;&#30005;&#36335;&#24418;&#24335;--&#36825;&#26159;&#19968;&#31181;&#20801;&#35768;&#26377;&#25928;&#36793;&#38469;&#21270;&#30340;&#32422;&#26463;&#35745;&#31639;&#22270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#26041;&#27861;&#26469;&#33719;&#24471;&#26377;&#25928;&#30340;&#29983;&#25104;&#30005;&#36335;&#27169;&#22411;&#65292;&#20854;&#20013;&#19968;&#20010;&#26041;&#27861;&#26159;&#23558;&#20854;&#28608;&#27963;&#38480;&#21046;&#20026;&#38750;&#36127;&#25968;&#65292;&#21478;&#19968;&#20010;&#26041;&#27861;&#26159;&#23558;&#20854;&#36755;&#20986;&#24179;&#26041;&#12290;&#25105;&#20204;&#30340;&#35299;&#37322;&#19981;&#20250;&#24433;&#21709;&#21040;&#39044;&#27979;&#33410;&#28857;&#36830;&#36793;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20294;&#30005;&#36335;&#26694;&#26550;&#20351;&#24471;MLE&#30340;&#31934;&#30830;&#23398;&#20064;&#12289;&#26032;&#19977;&#20803;&#32452;&#30340;&#26377;&#25928;&#25277;&#26679;&#20197;&#21450;&#20445;&#35777;&#36923;&#36753;&#32422;&#26463;&#24471;&#20197;&#28385;&#36275;&#25104;&#20026;&#21487;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25317;&#26377;&#25968;&#30334;&#19975;&#20010;&#23454;&#20307;&#30340;&#22270;&#19978;&#27604;&#21407;&#22987;&#30340;KGEs&#26356;&#20855;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Some of the most successful knowledge graph embedding (KGE) models for link prediction -- CP, RESCAL, TuckER, ComplEx -- can be interpreted as energy-based models. Under this perspective they are not amenable for exact maximum-likelihood estimation (MLE), sampling and struggle to integrate logical constraints. This work re-interprets the score functions of these KGEs as circuits -- constrained computational graphs allowing efficient marginalisation. Then, we design two recipes to obtain efficient generative circuit models by either restricting their activations to be non-negative or squaring their outputs. Our interpretation comes with little or no loss of performance for link prediction, while the circuits framework unlocks exact learning by MLE, efficient sampling of new triples, and guarantee that logical constraints are satisfied by design. Furthermore, our models scale more gracefully than the original KGEs on graphs with millions of entities.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#20174;&#20165;&#26377;&#23569;&#37327;&#26681;&#22240;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;DAGs&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#21487;&#35782;&#21035;&#24615;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.15936</link><description>&lt;p&gt;
&#20174;&#23569;&#37327;&#26681;&#22240;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;DAGs
&lt;/p&gt;
&lt;p&gt;
Learning DAGs from Data with Few Root Causes. (arXiv:2305.15936v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15936
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#20174;&#20165;&#26377;&#23569;&#37327;&#26681;&#22240;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;DAGs&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#21487;&#35782;&#21035;&#24615;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35282;&#24230;&#21644;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#32447;&#24615;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;(SEM)&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#26377;&#21521;&#26080;&#29615;&#22270;(DAGs)&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#32447;&#24615;SEM&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#31181;&#32447;&#24615;&#21464;&#25442;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#20351;&#29992;&#19968;&#20010;&#30001;&#19982;&#33410;&#28857;&#20851;&#32852;&#30340;&#38543;&#26426;&#20540;&#26681;&#22240;(&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;)&#30340;&#31264;&#23494;&#36755;&#20837;&#21521;&#37327;&#35745;&#31639;&#25968;&#25454;&#12290;&#25105;&#20204;&#32771;&#34385;&#20165;&#23384;&#22312;&#20960;&#20010;&#26681;&#22240;(&#36817;&#20284;)&#30340;&#24773;&#20917;&#65292;&#24182;&#22312;&#25968;&#25454;&#30340;&#27979;&#37327;&#20013;&#24341;&#20837;&#22122;&#22768;&#12290;&#20174;&#30452;&#35273;&#19978;&#35762;&#65292;&#36825;&#24847;&#21619;&#30528;DAG&#25968;&#25454;&#26159;&#30001;&#23569;&#25968;&#25968;&#25454;&#29983;&#25104;&#20107;&#20214;&#20135;&#29983;&#30340;&#65292;&#20854;&#25928;&#26524;&#36890;&#36807;DAG&#20256;&#25773;&#12290;&#25105;&#20204;&#22312;&#36825;&#31181;&#26032;&#35774;&#32622;&#20013;&#35777;&#26126;&#21487;&#35782;&#21035;&#24615;&#65292;&#24182;&#34920;&#26126;&#30495;&#27491;&#30340;DAG&#26159;&#26681;&#22240;&#21521;&#37327;L0-&#33539;&#25968;&#30340;&#20840;&#23616;&#26368;&#23567;&#21270;&#32773;&#12290;&#23545;&#20110;&#20855;&#26377;&#23569;&#37327;&#26681;&#22240;&#30340;&#25968;&#25454;&#65292;&#26377;&#21644;&#27809;&#26377;&#22122;&#38899;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#34920;&#29616;&#20986;&#27604;&#20197;&#21069;&#30340;DAG&#23398;&#20064;&#26041;&#27861;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel perspective and algorithm for learning directed acyclic graphs (DAGs) from data generated by a linear structural equation model (SEM). First, we show that a linear SEM can be viewed as a linear transform that, in prior work, computes the data from a dense input vector of random valued root causes (as we will call them) associated with the nodes. Instead, we consider the case of (approximately) few root causes and also introduce noise in the measurement of the data. Intuitively, this means that the DAG data is produced by few data-generating events whose effect percolates through the DAG. We prove identifiability in this new setting and show that the true DAG is the global minimizer of the $L^0$-norm of the vector of root causes. For data with few root causes, with and without noise, we show superior performance compared to prior DAG learning methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20135;&#21697;&#35282;&#24230;&#30340;&#35786;&#26029;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#36319;&#36394;&#20135;&#21697;&#22312;&#24452;&#21521;&#32034;&#24341;&#26426;&#20013;&#30340;&#27493;&#39588;&#26469;&#35786;&#26029;&#25925;&#38556;&#12290;&#25991;&#31456;&#20998;&#26512;&#20102;RIM&#30340;&#29305;&#24615;&#20197;&#21450;&#36825;&#20123;&#29305;&#24615;&#22914;&#20309;&#24433;&#21709;&#36825;&#20123;&#26426;&#22120;&#20013;&#30340;&#25925;&#38556;&#35786;&#26029;&#12290;</title><link>http://arxiv.org/abs/2305.15934</link><description>&lt;p&gt;
&#24452;&#21521;&#32034;&#24341;&#26426;&#30340;&#35786;&#26029;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Diagnosis Algorithms for a Rotary Indexing Machine. (arXiv:2305.15934v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20135;&#21697;&#35282;&#24230;&#30340;&#35786;&#26029;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#36319;&#36394;&#20135;&#21697;&#22312;&#24452;&#21521;&#32034;&#24341;&#26426;&#20013;&#30340;&#27493;&#39588;&#26469;&#35786;&#26029;&#25925;&#38556;&#12290;&#25991;&#31456;&#20998;&#26512;&#20102;RIM&#30340;&#29305;&#24615;&#20197;&#21450;&#36825;&#20123;&#29305;&#24615;&#22914;&#20309;&#24433;&#21709;&#36825;&#20123;&#26426;&#22120;&#20013;&#30340;&#25925;&#38556;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24452;&#21521;&#32034;&#24341;&#26426;&#65288;RIM&#65289;&#22240;&#33021;&#22815;&#22312;&#26080;&#38656;&#25163;&#21160;&#37325;&#26032;&#23450;&#20301;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#22810;&#20010;&#29983;&#20135;&#27493;&#39588;&#65292;&#20174;&#32780;&#32553;&#30701;&#29983;&#20135;&#26102;&#38388;&#12289;&#25552;&#39640;&#31934;&#24230;&#21644;&#19968;&#33268;&#24615;&#65292;&#24191;&#27867;&#29992;&#20110;&#21046;&#36896;&#19994;&#12290;&#23613;&#31649;&#20855;&#26377;&#35832;&#22810;&#20248;&#21183;&#65292;&#20294;&#22312;RIM&#20013;&#35786;&#26029;&#25925;&#38556;&#30340;&#30740;&#31350;&#36739;&#23569;&#65292;&#23588;&#20854;&#26159;&#20174;&#36825;&#20123;&#26426;&#22120;&#36827;&#34892;&#30340;&#23454;&#38469;&#29983;&#20135;&#27493;&#39588;&#35282;&#24230;&#32771;&#34385;&#12290;&#25925;&#38556;&#36896;&#25104;&#30340;&#38271;&#26102;&#38388;&#20572;&#26426;&#23545;&#20110;&#38599;&#29992;&#36825;&#20123;&#26426;&#22120;&#30340;&#36739;&#23567;&#20844;&#21496;&#23588;&#20854;&#26159;&#26377;&#38382;&#39064;&#30340;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#22522;&#20110;&#20135;&#21697;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#31181;&#35786;&#26029;&#31639;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#30001; RIM &#25191;&#34892;&#30340;&#20135;&#21697;&#12290;&#35813;&#31639;&#27861;&#36319;&#36394;&#20135;&#21697;&#22312;&#26426;&#22120;&#20013;&#32463;&#36807;&#30340;&#27493;&#39588;&#65292;&#24182;&#33021;&#22815;&#35786;&#26029;&#25925;&#38556;&#30340;&#21487;&#33021;&#21407;&#22240;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;RIM&#30340;&#29305;&#24615;&#20197;&#21450;&#36825;&#20123;&#29305;&#24615;&#22914;&#20309;&#24433;&#21709;&#36825;&#20123;&#26426;&#22120;&#20013;&#30340;&#25925;&#38556;&#35786;&#26029;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;RIM&#29305;&#24615;&#30340;&#20998;&#26512;&#65292;
&lt;/p&gt;
&lt;p&gt;
Rotary Indexing Machines (RIMs) are widely used in manufacturing due to their ability to perform multiple production steps on a single product without manual repositioning, reducing production time and improving accuracy and consistency. Despite their advantages, little research has been done on diagnosing faults in RIMs, especially from the perspective of the actual production steps carried out on these machines. Long downtimes due to failures are problematic, especially for smaller companies employing these machines. To address this gap, we propose a diagnosis algorithm based on the product perspective, which focuses on the product being processed by RIMs. The algorithm traces the steps that a product takes through the machine and is able to diagnose possible causes in case of failure. We also analyze the properties of RIMs and how these influence the diagnosis of faults in these machines. Our contributions are three-fold. Firstly, we provide an analysis of the properties of RIMs and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20116;&#32423;AI&#33021;&#21147;&#35780;&#20272;&#27169;&#22411;&#65288;AI-CAM&#65289;&#21644;&#30456;&#20851;&#30340;AI&#33021;&#21147;&#30697;&#38453;&#65288;AI-CM&#65289;&#65292;&#29992;&#20110;&#24110;&#21161;&#32452;&#32455;&#30340;&#20915;&#31574;&#32773;&#20102;&#35299;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;AI&#25968;&#25454;&#20998;&#26512;&#29992;&#20363;&#12289;&#20351;&#29992;&#35821;&#20041;&#25216;&#26415;&#30340;&#30693;&#35782;&#24037;&#31243;&#21644;&#20195;&#34920;&#25968;&#25454;&#30340;&#30693;&#35782;&#34920;&#31034;&#20197;&#21450;&#26088;&#22312;&#27169;&#20223;&#20154;&#31867;&#25512;&#29702;&#21644;&#20915;&#31574;&#30340;AI&#35299;&#20915;&#26041;&#26696;&#30340;&#26680;&#24515;&#33021;&#21147;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2305.15922</link><description>&lt;p&gt;
&#38754;&#21521;&#32452;&#32455;&#20013;&#23545;&#20154;&#24037;&#26234;&#33021;&#29702;&#35299;&#21644;&#37319;&#29992;&#30340;&#33021;&#21147;&#35780;&#20272;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards a Capability Assessment Model for the Comprehension and Adoption of AI in Organisations. (arXiv:2305.15922v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20116;&#32423;AI&#33021;&#21147;&#35780;&#20272;&#27169;&#22411;&#65288;AI-CAM&#65289;&#21644;&#30456;&#20851;&#30340;AI&#33021;&#21147;&#30697;&#38453;&#65288;AI-CM&#65289;&#65292;&#29992;&#20110;&#24110;&#21161;&#32452;&#32455;&#30340;&#20915;&#31574;&#32773;&#20102;&#35299;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;AI&#25968;&#25454;&#20998;&#26512;&#29992;&#20363;&#12289;&#20351;&#29992;&#35821;&#20041;&#25216;&#26415;&#30340;&#30693;&#35782;&#24037;&#31243;&#21644;&#20195;&#34920;&#25968;&#25454;&#30340;&#30693;&#35782;&#34920;&#31034;&#20197;&#21450;&#26088;&#22312;&#27169;&#20223;&#20154;&#31867;&#25512;&#29702;&#21644;&#20915;&#31574;&#30340;AI&#35299;&#20915;&#26041;&#26696;&#30340;&#26680;&#24515;&#33021;&#21147;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#29702;&#35299;&#21644;&#37319;&#29992;&#38754;&#20020;&#23454;&#38469;&#21644;&#20262;&#29702;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20116;&#32423;AI&#33021;&#21147;&#35780;&#20272;&#27169;&#22411;&#65288;AI-CAM&#65289;&#21644;&#30456;&#20851;&#30340;AI&#33021;&#21147;&#30697;&#38453;&#65288;AI-CM&#65289;&#65292;&#20197;&#24110;&#21161;&#20174;&#19994;&#20154;&#21592;&#29702;&#35299;&#21644;&#37319;&#29992;AI&#12290;&#36825;&#20123;&#23454;&#29992;&#24037;&#20855;&#26159;&#20026;&#21830;&#19994;&#39640;&#31649;&#12289;&#25216;&#26415;&#20154;&#21592;&#21644;&#20854;&#20182;&#32452;&#32455;&#21033;&#30410;&#30456;&#20851;&#32773;&#35774;&#35745;&#30340;&#12290;&#23427;&#20204;&#22522;&#20110;&#23545;AI&#30340;&#20840;&#38754;&#27010;&#24565;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;AI&#37319;&#29992;&#27169;&#22411;&#65292;&#20063;&#26159;&#24320;&#25918;&#28304;&#20195;&#30721;&#12290;&#22240;&#27492;&#65292;AI-CAM&#21644;AI-CM&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20197;&#33719;&#21462;&#30340;&#36164;&#28304;&#65292;&#20197;&#24110;&#21161;&#32452;&#32455;&#30340;&#20915;&#31574;&#32773;&#20102;&#35299;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#22522;&#20110;AI&#30340;&#25968;&#25454;&#20998;&#26512;&#29992;&#20363;&#12289;&#20351;&#29992;&#35821;&#20041;&#25216;&#26415;&#26469;&#24037;&#31243;&#21270;&#21644;&#20195;&#34920;&#25968;&#25454;&#12289;&#20449;&#24687;&#21644;&#30693;&#35782;&#30340;&#30693;&#35782;&#34920;&#31034;&#20197;&#21450;&#26088;&#22312;&#27169;&#20223;&#20154;&#31867;&#25512;&#29702;&#21644;&#20915;&#31574;&#30340;AI&#35299;&#20915;&#26041;&#26696;&#30340;&#26680;&#24515;&#33021;&#21147;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
The comprehension and adoption of Artificial Intelligence (AI) are beset with practical and ethical problems. This article presents a 5-level AI Capability Assessment Model (AI-CAM) and a related AI Capabilities Matrix (AI-CM) to assist practitioners in AI comprehension and adoption. These practical tools were developed with business executives, technologists, and other organisational stakeholders in mind. They are founded on a comprehensive conception of AI compared to those in other AI adoption models and are also open-source artefacts. Thus, the AI-CAM and AI-CM present an accessible resource to help inform organisational decision-makers on the capability requirements for (1) AI-based data analytics use cases based on machine learning technologies; (2) Knowledge representation to engineer and represent data, information and knowledge using semantic technologies; and (3) AI-based solutions that seek to emulate human reasoning and decision-making. The AI-CAM covers the core capability
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20551;&#35774;&#30340;&#35770;&#35777;&#26694;&#26550;(Assumption-based Argumentation, ABA)&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#19982;&#29616;&#26377;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#23398;&#20064;&#19968;&#33324;&#35268;&#21017;&#30340;&#20363;&#22806;&#24773;&#20917;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.15921</link><description>&lt;p&gt;
&#23398;&#20064;&#22522;&#20110;&#20551;&#35774;&#30340;&#35770;&#35777;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Learning Assumption-based Argumentation Frameworks. (arXiv:2305.15921v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15921
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20551;&#35774;&#30340;&#35770;&#35777;&#26694;&#26550;(Assumption-based Argumentation, ABA)&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#19982;&#29616;&#26377;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#23398;&#20064;&#19968;&#33324;&#35268;&#21017;&#30340;&#20363;&#22806;&#24773;&#20917;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#36923;&#36753;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#27491;&#36127;&#20363;&#20013;&#65292;&#20351;&#29992;&#32473;&#23450;&#30340;&#32972;&#26223;&#30693;&#35782;&#29983;&#25104;&#20551;&#35774;&#20026;&#22522;&#30784;&#30340;&#35770;&#35777;&#26694;&#26550;(Assumption-based Argumentation, ABA)&#65292;&#36825;&#20123;ABA&#26694;&#26550;&#21487;&#20197;&#26144;&#23556;&#21040;&#20855;&#26377;&#22833;&#36133;&#21542;&#23450;&#30340;&#36923;&#36753;&#31243;&#24207;&#65292;&#36825;&#20123;&#31243;&#24207;&#20063;&#21487;&#20197;&#26159;&#38750;&#27969;&#32441;&#29702;&#30340;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#35770;&#35777;&#30340;&#26041;&#27861;&#36890;&#36807;&#23558;&#20363;&#22806;&#20107;&#39033;&#35299;&#37322;&#20026;&#21453;&#39539;&#25915;&#20987;&#26469;&#23398;&#20064;&#19968;&#33324;&#35268;&#21017;&#30340;&#20363;&#22806;&#24773;&#20917;&#65292;&#32780;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20854;&#35299;&#37322;&#20026;&#25200;&#21160;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#23398;&#20064;&#25216;&#26415;&#22522;&#20110;&#20351;&#29992;&#36716;&#25442;&#35268;&#21017;&#65292;&#21253;&#25324;&#19968;&#20123;&#25913;&#32534;&#33258;&#36923;&#36753;&#31243;&#24207;&#36716;&#25442;&#35268;&#21017;&#65288;&#29305;&#21035;&#26159;&#25240;&#21472;&#65289;&#20197;&#21450;&#20854;&#20182;&#35268;&#21017;&#65292;&#20363;&#22914;&#36880;&#23383;&#23398;&#20064;&#21644;&#20551;&#35774;&#24341;&#20837;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#31574;&#30053;&#65292;&#25353;&#29031;&#36866;&#24403;&#30340;&#39034;&#24207;&#24212;&#29992;&#36716;&#25442;&#35268;&#21017;&#26469;&#23398;&#20064;&#27969;&#32441;&#29702;&#26694;&#26550;&#65292;&#25105;&#20204;&#20063;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#38750;&#27969;&#32441;&#29702;&#24773;&#20917;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#36890;&#36807;&#35768;&#22810;&#20363;&#23376;&#35828;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#36825;&#20123;&#20363;&#23376;&#34920;&#26126;&#65292;&#25105;&#20204;&#26082;&#33021;&#22815;&#23398;&#20064;&#19982;&#25552;&#20379;&#30340;&#27491;&#36127;&#20363;&#30456;&#23545;&#24212;&#30340;ABA&#26694;&#26550;&#65292;&#21448;&#33021;&#22815;&#27604;&#29616;&#26377;&#26041;&#27861;&#20135;&#29983;&#30340;&#26694;&#26550;&#20855;&#26377;&#26356;&#22909;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel approach to logic-based learning which generates assumption-based argumentation (ABA) frameworks from positive and negative examples, using a given background knowledge. These ABA frameworks can be mapped onto logic programs with negation as failure that may be non-stratified. Whereas existing argumentation-based methods learn exceptions to general rules by interpreting the exceptions as rebuttal attacks, our approach interprets them as undercutting attacks. Our learning technique is based on the use of transformation rules, including some adapted from logic program transformation rules (notably folding) as well as others, such as rote learning and assumption introduction. We present a general strategy that applies the transformation rules in a suitable order to learn stratified frameworks, and we also propose a variant that handles the non-stratified case. We illustrate the benefits of our approach with a number of examples, which show that, on one hand, we are able
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#31639;&#27861;&#26469;&#26816;&#26597;&#37096;&#20998;&#26377;&#24207;&#26102;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#30456;&#27604;&#24050;&#26377;&#31639;&#27861;&#26102;&#38388;&#22797;&#26434;&#24230;&#26356;&#20302;&#65292;&#20026;$O^*((0.26n)^n)$&#65292;&#26159;&#36890;&#36807;&#23545;&#31867;&#20284;&#20110;&#20840;&#24207;&#30340;&#32467;&#26500;&#36827;&#34892;&#31934;&#32454;&#26522;&#20030;&#65292;&#24182;&#36138;&#23146;&#22320;&#25193;&#23637;&#21521;&#35299;&#20915;&#26041;&#26696;&#23454;&#29616;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.15917</link><description>&lt;p&gt;
&#19968;&#31181;&#26816;&#26597;&#37096;&#20998;&#26377;&#24207;&#26102;&#38388;&#19968;&#33268;&#24615;&#30340;&#24555;&#36895;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Fast Algorithm for Consistency Checking Partially Ordered Time. (arXiv:2305.15917v1 [cs.CC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#31639;&#27861;&#26469;&#26816;&#26597;&#37096;&#20998;&#26377;&#24207;&#26102;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#30456;&#27604;&#24050;&#26377;&#31639;&#27861;&#26102;&#38388;&#22797;&#26434;&#24230;&#26356;&#20302;&#65292;&#20026;$O^*((0.26n)^n)$&#65292;&#26159;&#36890;&#36807;&#23545;&#31867;&#20284;&#20110;&#20840;&#24207;&#30340;&#32467;&#26500;&#36827;&#34892;&#31934;&#32454;&#26522;&#20030;&#65292;&#24182;&#36138;&#23146;&#22320;&#25193;&#23637;&#21521;&#35299;&#20915;&#26041;&#26696;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30340;&#37096;&#20998;&#26377;&#24207;&#27169;&#22411;&#22312;&#24212;&#29992;&#20013;&#33258;&#28982;&#32780;&#28982;&#22320;&#20986;&#29616;&#65292;&#36825;&#20123;&#24212;&#29992;&#20013;&#20195;&#29702;&#25110;&#36827;&#31243;&#19981;&#33021;&#23436;&#32654;&#22320;&#24444;&#27492;&#36890;&#20449;&#65292;&#21487;&#20197;&#36861;&#28335;&#21040;Lamport&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21028;&#26029;&#26159;&#21542;&#19968;&#20010;&#65288;&#21487;&#33021;&#19981;&#23436;&#25972;&#30340;&#65289;&#20107;&#20214;&#31995;&#32479;&#30340;&#25551;&#36848;&#26159;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#21363;&#37096;&#20998;&#26377;&#24207;&#26102;&#38388;&#65288;POT&#65289;&#28857;&#20195;&#25968;&#30340;&#32593;&#32476;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;&#34429;&#28982;&#36825;&#20010;&#38382;&#39064;&#30340;&#32463;&#20856;&#22797;&#26434;&#24615;&#24050;&#32463;&#23436;&#20840;&#35299;&#20915;&#65292;&#20294;&#26159;&#20851;&#20110;POT&#30340;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#30340;&#20102;&#35299;&#30456;&#23545;&#36739;&#23569;&#65292;&#38500;&#20102;&#21487;&#20197;&#36890;&#36807;&#26522;&#20030;&#26377;&#24207;&#20998;&#21306;&#20197;$O^*((0.368n)^n)$&#26102;&#38388;&#35299;&#20915;&#12290;&#25105;&#20204;&#26500;&#36896;&#20102;&#19968;&#31181;&#26356;&#24555;&#30340;&#31639;&#27861;&#65292;&#20854;&#36816;&#34892;&#26102;&#38388;&#20165;&#20026;$O^*((0.26n)^n)$&#12290;&#36825;&#26159;&#36890;&#36807;&#23545;&#31867;&#20284;&#20110;&#20840;&#24207;&#30340;&#32467;&#26500;&#36827;&#34892;&#31934;&#32454;&#26522;&#20030;&#65292;&#28982;&#21518;&#21521;&#35299;&#20915;&#26041;&#26696;&#36138;&#23146;&#22320;&#25193;&#23637;&#26469;&#23454;&#29616;&#30340;&#12290;&#34429;&#28982;&#26089;&#26399;&#30340;&#31867;&#20284;&#24819;&#27861;&#24050;&#32463;&#22312;&#30456;&#20851;&#38382;&#39064;&#20013;&#25506;&#32034;&#36807;&#65292;&#20294;&#23454;&#38469;&#19978;POT&#30340;&#20998;&#26512;&#26159;&#38750;&#24179;&#20961;&#30340;&#65292;&#38656;&#35201;&#37325;&#22823;&#30340;&#26032;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partially ordered models of time occur naturally in applications where agents or processes cannot perfectly communicate with each other, and can be traced back to the seminal work of Lamport. In this paper we consider the problem of deciding if a (likely incomplete) description of a system of events is consistent, the network consistency problem for the point algebra of partially ordered time (POT). While the classical complexity of this problem has been fully settled, comparably little is known of the fine-grained complexity of POT except that it can be solved in $O^*((0.368n)^n)$ time by enumerating ordered partitions. We construct a much faster algorithm with a run-time bounded by $O^*((0.26n)^n)$. This is achieved by a sophisticated enumeration of structures similar to total orders, which are then greedily expanded toward a solution. While similar ideas have been explored earlier for related problems it turns out that the analysis for POT is non-trivial and requires significant new
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26694;&#26550;MTCue&#65292;&#23427;&#23558;&#25152;&#26377;&#19978;&#19979;&#25991;&#35299;&#37322;&#20026;&#25991;&#26412;&#65292;&#23454;&#29616;&#20102;&#21487;&#36716;&#31227;&#24615;&#24182;&#23398;&#20250;&#20102;&#20197;&#38646;&#26679;&#26412;&#30340;&#26041;&#24335;&#21033;&#29992;&#39069;&#22806;&#30340;&#25991;&#26412;&#23646;&#24615;&#65288;&#22914;&#31036;&#35980;&#21644;&#23545;&#35805;&#34892;&#20026;&#31561;&#21464;&#37327;&#65289;&#30340;&#25511;&#21046;&#12290;&#22312;&#22235;&#20010;&#35821;&#35328;&#23545;&#30340;&#32763;&#35793;&#26041;&#21521;&#19978;&#65292;MTCue&#30340;&#32763;&#35793;&#36136;&#37327;&#26174;&#30528;&#25552;&#39640;&#65292;BLEU&#65288;+0.88&#65289;&#21644;Comet&#65288;+1.58&#65289;&#12290;</title><link>http://arxiv.org/abs/2305.15904</link><description>&lt;p&gt;
MTCue&#65306;&#21033;&#29992;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#26410;&#32467;&#26500;&#21270;&#19978;&#19979;&#25991;&#23398;&#20064;&#38646;&#26679;&#26412;&#25511;&#21046;&#39069;&#22806;&#25991;&#26412;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
MTCue: Learning Zero-Shot Control of Extra-Textual Attributes by Leveraging Unstructured Context in Neural Machine Translation. (arXiv:2305.15904v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26694;&#26550;MTCue&#65292;&#23427;&#23558;&#25152;&#26377;&#19978;&#19979;&#25991;&#35299;&#37322;&#20026;&#25991;&#26412;&#65292;&#23454;&#29616;&#20102;&#21487;&#36716;&#31227;&#24615;&#24182;&#23398;&#20250;&#20102;&#20197;&#38646;&#26679;&#26412;&#30340;&#26041;&#24335;&#21033;&#29992;&#39069;&#22806;&#30340;&#25991;&#26412;&#23646;&#24615;&#65288;&#22914;&#31036;&#35980;&#21644;&#23545;&#35805;&#34892;&#20026;&#31561;&#21464;&#37327;&#65289;&#30340;&#25511;&#21046;&#12290;&#22312;&#22235;&#20010;&#35821;&#35328;&#23545;&#30340;&#32763;&#35793;&#26041;&#21521;&#19978;&#65292;MTCue&#30340;&#32763;&#35793;&#36136;&#37327;&#26174;&#30528;&#25552;&#39640;&#65292;BLEU&#65288;+0.88&#65289;&#21644;Comet&#65288;+1.58&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22320;&#21033;&#29992;&#25991;&#26412;&#20869;&#21644;&#25991;&#26412;&#22806;&#30340;&#19978;&#19979;&#25991;&#20173;&#26159;&#26426;&#22120;&#21644;&#20154;&#31867;&#32763;&#35793;&#20043;&#38388;&#30340;&#20851;&#38190;&#24046;&#36317;&#20043;&#19968;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22312;&#32763;&#35793;&#20013;&#25552;&#20379;&#20010;&#21035;&#23450;&#20041;&#33391;&#22909;&#31867;&#22411;&#30340;&#19978;&#19979;&#25991;&#65292;&#22914;&#21608;&#22260;&#30340;&#25991;&#26412;&#25110;&#31163;&#25955;&#30340;&#22806;&#37096;&#21464;&#37327;&#65288;&#22914;&#35828;&#35805;&#32773;&#30340;&#24615;&#21035;&#65289;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MTCue&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#26694;&#26550;&#65292;&#23427;&#23558;&#25152;&#26377;&#19978;&#19979;&#25991;&#65288;&#21253;&#25324;&#31163;&#25955;&#21464;&#37327;&#65289;&#35299;&#37322;&#20026;&#25991;&#26412;&#12290; MTCue&#23398;&#20064;&#19978;&#19979;&#25991;&#30340;&#25277;&#35937;&#34920;&#36798;&#65292;&#21363;&#20351;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#35774;&#32622;&#21644;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#65292;&#20063;&#33021;&#23454;&#29616;&#21487;&#36716;&#31227;&#24615;&#24182;&#21033;&#29992;&#31867;&#20284;&#23646;&#24615;&#12290;&#25105;&#20204;&#19981;&#26029;&#35780;&#20272;MTCue&#22312;&#22235;&#20010;&#35821;&#35328;&#23545;&#30340;&#32763;&#35793;&#26041;&#21521;&#19978;&#65292;&#37325;&#28857;&#20851;&#27880;&#20855;&#26377;&#25991;&#26723;&#21644;&#20803;&#25968;&#25454;&#19978;&#19979;&#25991;&#35775;&#38382;&#26435;&#38480;&#30340;&#23545;&#35805;&#39046;&#22495;&#12290;&#19982;&#21442;&#25968;&#21305;&#37197;&#30340;&#38750;&#19978;&#19979;&#25991;&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#32763;&#35793;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#30528;&#30340;&#25552;&#39640;&#65292;BLEU&#65288;+0.88&#65289;&#21644;Comet&#65288;+1.58&#65289;&#12290;&#27492;&#22806;&#65292;MTCue&#25104;&#21151;&#22320;&#23398;&#20250;&#20102;&#20197;&#38646;&#26679;&#26412;&#30340;&#26041;&#24335;&#21033;&#29992;&#39069;&#22806;&#30340;&#25991;&#26412;&#23646;&#24615;&#65292;&#23454;&#29616;&#20102;&#35832;&#22914;&#31036;&#35980;&#21644;&#23545;&#35805;&#34892;&#20026;&#31561;&#21464;&#37327;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient utilisation of both intra- and extra-textual context remains one of the critical gaps between machine and human translation. Existing research has primarily focused on providing individual, well-defined types of context in translation, such as the surrounding text or discrete external variables like the speaker's gender. This work introduces MTCue, a novel neural machine translation (NMT) framework that interprets all context (including discrete variables) as text. MTCue learns an abstract representation of context, enabling transferability across different data settings and leveraging similar attributes in low-resource scenarios. With a focus on a dialogue domain with access to document and metadata context, we extensively evaluate MTCue in four language pairs in both translation directions. Our framework demonstrates significant improvements in translation quality over a parameter-matched non-contextual baseline, as measured by BLEU (+0.88) and Comet (+1.58). Moreover, MTCu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#23398;&#20064;&#20013;&#26368;&#23567;&#21270;&#39118;&#38505;&#30340;&#20498;&#25968;&#20542;&#21521;&#35780;&#20998;(IPS)&#30340;&#24179;&#28369;&#27491;&#21017;&#21270;&#65292;&#25512;&#23548;&#20986;&#20102;&#21487;&#22788;&#29702;&#12289;&#21487;&#25193;&#23637;&#12289;&#21487;&#35299;&#37322;&#30340;&#23398;&#20064;&#35777;&#26126;&#65292;&#24182;&#30830;&#23450;&#20102;&#22312;&#20309;&#31181;&#24773;&#20917;&#19979;&#19981;&#38656;&#35201;&#27491;&#21017;&#21270;IPS&#12290;</title><link>http://arxiv.org/abs/2305.15877</link><description>&lt;p&gt;
&#25351;&#25968;&#24179;&#28369;&#29992;&#20110;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Exponential Smoothing for Off-Policy Learning. (arXiv:2305.15877v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#23398;&#20064;&#20013;&#26368;&#23567;&#21270;&#39118;&#38505;&#30340;&#20498;&#25968;&#20542;&#21521;&#35780;&#20998;(IPS)&#30340;&#24179;&#28369;&#27491;&#21017;&#21270;&#65292;&#25512;&#23548;&#20986;&#20102;&#21487;&#22788;&#29702;&#12289;&#21487;&#25193;&#23637;&#12289;&#21487;&#35299;&#37322;&#30340;&#23398;&#20064;&#35777;&#26126;&#65292;&#24182;&#30830;&#23450;&#20102;&#22312;&#20309;&#31181;&#24773;&#20917;&#19979;&#19981;&#38656;&#35201;&#27491;&#21017;&#21270;IPS&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#26368;&#23567;&#21270;&#39118;&#38505;&#30340;&#20498;&#25968;&#20542;&#21521;&#35780;&#20998;&#65288;IPS&#65289;&#26469;&#23547;&#25214;&#25913;&#36827;&#30340;&#31574;&#30053;&#65292;&#36890;&#24120;&#20351;&#29992;&#35760;&#24405;&#30340;&#36172;&#21338;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;IPS&#30340;&#24179;&#28369;&#27491;&#21017;&#21270;&#65292;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#21452;&#21521;PAC-Bayes&#27867;&#21270;&#30028;&#38480;&#12290;&#35813;&#30028;&#38480;&#26159;&#21487;&#22788;&#29702;&#30340;&#12289;&#21487;&#25193;&#23637;&#30340;&#12289;&#21487;&#35299;&#37322;&#30340;&#24182;&#25552;&#20379;&#20102;&#23398;&#20064;&#35777;&#26126;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#23398;&#20064;&#20219;&#21153;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#30456;&#20851;&#24615;&#21644;&#26377;&#21033;&#30340;&#24615;&#33021;&#12290;&#30001;&#20110;&#25105;&#20204;&#30340;&#30028;&#38480;&#36866;&#29992;&#20110;&#26631;&#20934;IPS&#65292;&#22240;&#27492;&#25105;&#20204;&#33021;&#22815;&#25552;&#20379;&#20851;&#20110;&#20309;&#26102;&#27491;&#21017;&#21270;IPS&#26377;&#29992;&#30340;&#35265;&#35299;&#12290;&#21363;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19981;&#38656;&#35201;&#27491;&#21017;&#21270;&#30340;&#24773;&#20917;&#12290;&#36825;&#19982;&#22312;&#23454;&#36341;&#20013;&#65292;&#21098;&#36753;IPS&#24120;&#24120;&#27604;OPL&#20013;&#30340;&#26631;&#20934;IPS&#34920;&#29616;&#26356;&#22909;&#30340;&#20449;&#24565;&#30456;&#21453;&#12290;
&lt;/p&gt;
&lt;p&gt;
Off-policy learning (OPL) aims at finding improved policies from logged bandit data, often by minimizing the inverse propensity scoring (IPS) estimator of the risk. In this work, we investigate a smooth regularization for IPS, for which we derive a two-sided PAC-Bayes generalization bound. The bound is tractable, scalable, interpretable and provides learning certificates. In particular, it is also valid for standard IPS without making the assumption that the importance weights are bounded. We demonstrate the relevance of our approach and its favorable performance through a set of learning tasks. Since our bound holds for standard IPS, we are able to provide insight into when regularizing IPS is useful. Namely, we identify cases where regularization might not be needed. This goes against the belief that, in practice, clipped IPS often enjoys favorable performance than standard IPS in OPL.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;LLM&#30340;&#19981;&#30495;&#23454;&#22238;&#31572;&#29616;&#35937;&#65292;&#21457;&#29616;GPT-3&#27169;&#22411;&#23545;&#32473;&#23450;&#25552;&#31034;&#30340;&#22238;&#31572;&#22312;&#35821;&#35328;&#29305;&#24615;&#19978;&#24456;&#30456;&#20284;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#27809;&#26377;&#35780;&#20272;&#20869;&#23481;&#26412;&#36523;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#20381;&#36182;&#27169;&#22411;&#22238;&#31572;&#30340;&#39118;&#26684;&#25104;&#20998;&#21363;&#21487;&#20998;&#31867;&#30495;&#23454;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15875</link><description>&lt;p&gt;
&#30495;&#23454;&#22238;&#31572;&#30340;&#35821;&#35328;&#29305;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Linguistic Properties of Truthful Response. (arXiv:2305.15875v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15875
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;LLM&#30340;&#19981;&#30495;&#23454;&#22238;&#31572;&#29616;&#35937;&#65292;&#21457;&#29616;GPT-3&#27169;&#22411;&#23545;&#32473;&#23450;&#25552;&#31034;&#30340;&#22238;&#31572;&#22312;&#35821;&#35328;&#29305;&#24615;&#19978;&#24456;&#30456;&#20284;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#27809;&#26377;&#35780;&#20272;&#20869;&#23481;&#26412;&#36523;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#20381;&#36182;&#27169;&#22411;&#22238;&#31572;&#30340;&#39118;&#26684;&#25104;&#20998;&#21363;&#21487;&#20998;&#31867;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;220&#20010;&#25163;&#24037;&#21046;&#20316;&#30340;&#35821;&#35328;&#29305;&#24615;&#23545;LLM&#19981;&#30495;&#23454;&#22238;&#31572;&#30340;&#29616;&#35937;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;GPT-3&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#22823;&#23567;&#30340;LLM&#23545;&#32473;&#23450;&#25552;&#31034;&#30340;&#22238;&#31572;&#22312;&#35821;&#35328;&#29305;&#24615;&#19978;&#24456;&#30456;&#20284;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#21482;&#20381;&#36182;&#27169;&#22411;&#22238;&#31572;&#30340;&#39118;&#26684;&#25104;&#20998;&#26469;&#20998;&#31867;&#38472;&#36848;&#30495;&#23454;&#24615;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#25193;&#23637;&#20102;&#36825;&#19968;&#21457;&#29616;&#12290;&#34429;&#28982;&#25968;&#25454;&#38598;&#22823;&#23567;&#38480;&#21046;&#20102;&#25105;&#20204;&#30340;&#24403;&#21069;&#30740;&#31350;&#25104;&#26524;&#65292;&#20294;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35777;&#25454;&#65292;&#34920;&#26126;&#21487;&#20197;&#22312;&#19981;&#35780;&#20272;&#20869;&#23481;&#26412;&#36523;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the phenomenon of an LLM's untruthful response using a large set of 220 handcrafted linguistic features. We focus on GPT-3 models and find that the linguistic profiles of responses are similar across model sizes. That is, how varying-sized LLMs respond to given prompts stays similar on the linguistic properties level. We expand upon this finding by training support vector machines that rely only upon the stylistic components of model responses to classify the truthfulness of statements. Though the dataset size limits our current findings, we present promising evidence that truthfulness detection is possible without evaluating the content itself.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Jointprop&#26694;&#26550;&#65292;&#29992;&#20110;&#22522;&#20110;&#24322;&#26500;&#22270;&#20256;&#25773;&#30340;&#32852;&#21512;&#21322;&#30417;&#30563;&#23454;&#20307;&#21644;&#20851;&#31995;&#25552;&#21462;&#65292;&#37319;&#29992;&#19968;&#20010;&#32479;&#19968;&#30340;&#24322;&#26500;&#22270;&#26469;&#20351;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#30340;&#20840;&#23616;&#32467;&#26500;&#20449;&#24687;&#65292;&#25552;&#39640;&#23454;&#20307;&#21644;&#20851;&#31995;&#25552;&#21462;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15872</link><description>&lt;p&gt;
Jointprop&#65306;&#22522;&#20110;&#24322;&#26500;&#22270;&#20256;&#25773;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#32852;&#21512;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Jointprop: Joint Semi-supervised Learning for Entity and Relation Extraction with Heterogeneous Graph-based Propagation. (arXiv:2305.15872v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15872
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Jointprop&#26694;&#26550;&#65292;&#29992;&#20110;&#22522;&#20110;&#24322;&#26500;&#22270;&#20256;&#25773;&#30340;&#32852;&#21512;&#21322;&#30417;&#30563;&#23454;&#20307;&#21644;&#20851;&#31995;&#25552;&#21462;&#65292;&#37319;&#29992;&#19968;&#20010;&#32479;&#19968;&#30340;&#24322;&#26500;&#22270;&#26469;&#20351;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#30340;&#20840;&#23616;&#32467;&#26500;&#20449;&#24687;&#65292;&#25552;&#39640;&#23454;&#20307;&#21644;&#20851;&#31995;&#25552;&#21462;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#26159;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#25552;&#21462;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#37325;&#35201;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#20998;&#21035;&#22788;&#29702;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;&#20004;&#20010;&#20219;&#21153;&#65292;&#24182;&#24573;&#30053;&#23454;&#20307;&#21644;&#20851;&#31995;&#23454;&#20363;&#20043;&#38388;&#30340;&#20132;&#21449;&#30456;&#20851;&#24615;&#20197;&#21450;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#30456;&#20284;&#23454;&#20363;&#30340;&#23384;&#22312;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Jointprop&#65292;&#19968;&#31181;&#22522;&#20110;&#24322;&#26500;&#22270;&#20256;&#25773;&#30340;&#32852;&#21512;&#21322;&#30417;&#30563;&#23454;&#20307;&#21644;&#20851;&#31995;&#25552;&#21462;&#26694;&#26550;&#65292;&#20854;&#25429;&#33719;&#21508;&#20010;&#20219;&#21153;&#20043;&#38388;&#30340;&#20840;&#23616;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#30340;&#20132;&#20114;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20174;&#23454;&#20307;&#21644;&#20851;&#31995;&#20505;&#36873;&#26500;&#24314;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22522;&#20110;&#26631;&#35760;&#30340;&#24322;&#26500;&#22270;&#65292;&#24182;&#26681;&#25454;&#32622;&#20449;&#24230;&#24471;&#20998;&#20256;&#25773;&#31867;&#26631;&#31614;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#20256;&#25773;&#23398;&#20064;&#26041;&#26696;&#26469;&#21033;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#26679;&#26412;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#34920;&#29616;&#20986;&#20196;&#20154;&#28385;&#24847;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#25552;&#21462;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning has been an important approach to address challenges in extracting entities and relations from limited data. However, current semi-supervised works handle the two tasks (i.e., Named Entity Recognition and Relation Extraction) separately and ignore the cross-correlation of entity and relation instances as well as the existence of similar instances across unlabeled data. To alleviate the issues, we propose Jointprop, a Heterogeneous Graph-based Propagation framework for joint semi-supervised entity and relation extraction, which captures the global structure information between individual tasks and exploits interactions within unlabeled data. Specifically, we construct a unified span-based heterogeneous graph from entity and relation candidates and propagate class labels based on confidence scores. We then employ a propagation learning scheme to leverage the affinities between labelled and unlabeled samples. Experiments on benchmark datasets show that our framewo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#25991;&#26412;&#32534;&#30721;&#26041;&#27861;&#65292;&#20351;&#29992;&#23567;&#22411;&#22522;&#20110;&#23383;&#31526;&#30340;&#27169;&#22411;&#37325;&#26500;&#22823;&#22411;&#39044;&#35757;&#32451;&#23884;&#20837;&#30697;&#38453;&#65292;&#20854;&#21487;&#20197;&#22312;&#25216;&#26415;&#39046;&#22495;&#20869;&#36798;&#21040;&#19982;&#21477;&#23376;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#36136;&#37327;&#65292;&#20294;&#22823;&#23567;&#20026;&#21518;&#32773;&#30340;&#20116;&#20998;&#20043;&#19968;&#65292;&#35745;&#31639;&#26102;&#38388;&#33021;&#24555;10&#20493;&#12290;</title><link>http://arxiv.org/abs/2305.15867</link><description>&lt;p&gt;
&#25216;&#26415;&#39046;&#22495;&#26415;&#35821;&#21644;&#30701;&#35821;&#30340;&#25991;&#26412;&#34920;&#24449;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Extracting Text Representations for Terms and Phrases in Technical Domains. (arXiv:2305.15867v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#25991;&#26412;&#32534;&#30721;&#26041;&#27861;&#65292;&#20351;&#29992;&#23567;&#22411;&#22522;&#20110;&#23383;&#31526;&#30340;&#27169;&#22411;&#37325;&#26500;&#22823;&#22411;&#39044;&#35757;&#32451;&#23884;&#20837;&#30697;&#38453;&#65292;&#20854;&#21487;&#20197;&#22312;&#25216;&#26415;&#39046;&#22495;&#20869;&#36798;&#21040;&#19982;&#21477;&#23376;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#36136;&#37327;&#65292;&#20294;&#22823;&#23567;&#20026;&#21518;&#32773;&#30340;&#20116;&#20998;&#20043;&#19968;&#65292;&#35745;&#31639;&#26102;&#38388;&#33021;&#24555;10&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33719;&#21462;&#26415;&#35821;&#21644;&#30701;&#35821;&#30340;&#23494;&#38598;&#34920;&#31034;&#26159;&#38754;&#21521;&#39640;&#24230;&#25216;&#26415;&#39046;&#22495;&#30340;&#30693;&#35782;&#21457;&#29616;&#24179;&#21488;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#24120;&#29992;&#30340;&#26041;&#27861;&#21253;&#25324;&#20351;&#29992;&#33258;&#30417;&#30563;&#35774;&#32622;&#35757;&#32451;&#39046;&#22495;&#29305;&#23450;&#30340;&#23884;&#20837;&#25110;&#20351;&#29992;&#35757;&#32451;&#36807;&#30456;&#20284;&#24615;&#20219;&#21153;&#30340;&#21477;&#23376;&#32534;&#30721;&#22120;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#25991;&#26412;&#32534;&#30721;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#20351;&#29992;&#23567;&#22411;&#22522;&#20110;&#23383;&#31526;&#30340;&#27169;&#22411;&#26469;&#37325;&#26500;&#22823;&#22411;&#39044;&#35757;&#32451;&#23884;&#20837;&#30697;&#38453;&#12290;&#19982;&#38745;&#24577;&#23884;&#20837;&#30456;&#27604;&#65292;&#21477;&#23376;&#32534;&#30721;&#22120;&#19981;&#20250;&#21463;&#21040;&#35789;&#27719;&#22806;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#20294;&#20250;&#24102;&#26469;&#26174;&#33879;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting dense representations for terms and phrases is a task of great importance for knowledge discovery platforms targeting highly-technical fields. Dense representations are used as features for downstream components and have multiple applications ranging from ranking results in search to summarization. Common approaches to create dense representations include training domain-specific embeddings with self-supervised setups or using sentence encoder models trained over similarity tasks. In contrast to static embeddings, sentence encoders do not suffer from the out-of-vocabulary (OOV) problem, but impose significant computational costs. In this paper, we propose a fully unsupervised approach to text encoding that consists of training small character-based models with the objective of reconstructing large pre-trained embedding matrices. Models trained with this approach can not only match the quality of sentence encoders in technical domains, but are 5 times smaller and up to 10 tim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39034;&#24207; Integrated Gradients&#65288;SIG&#65289;&#30340;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#25345;&#20854;&#20182;&#21333;&#35789;&#19981;&#21464;&#65292;&#20165;&#22312;&#22522;&#32447;&#21644;&#24863;&#20852;&#36259;&#30340;&#21333;&#35789;&#20043;&#38388;&#21019;&#24314;&#25554;&#20540;&#26469;&#35745;&#31639;&#21477;&#23376;&#20013;&#27599;&#20010;&#21333;&#35789;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#29992;&#35757;&#32451;&#30340;&#20196;&#29260;&#8220;mask&#8221;&#26367;&#25442;&#22522;&#32447;&#20196;&#29260;&#8220;pad&#8221;&#26469;&#26174;&#30528;&#25913;&#21892;&#35299;&#37322;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.15853</link><description>&lt;p&gt;
&#39034;&#24207;Integrated Gradients&#65306;&#19968;&#31181;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential Integrated Gradients: a simple but effective method for explaining language models. (arXiv:2305.15853v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39034;&#24207; Integrated Gradients&#65288;SIG&#65289;&#30340;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#25345;&#20854;&#20182;&#21333;&#35789;&#19981;&#21464;&#65292;&#20165;&#22312;&#22522;&#32447;&#21644;&#24863;&#20852;&#36259;&#30340;&#21333;&#35789;&#20043;&#38388;&#21019;&#24314;&#25554;&#20540;&#26469;&#35745;&#31639;&#21477;&#23376;&#20013;&#27599;&#20010;&#21333;&#35789;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#29992;&#35757;&#32451;&#30340;&#20196;&#29260;&#8220;mask&#8221;&#26367;&#25442;&#22522;&#32447;&#20196;&#29260;&#8220;pad&#8221;&#26469;&#26174;&#30528;&#25913;&#21892;&#35299;&#37322;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#31181;&#35299;&#37322;&#26041;&#27861;&#65288;&#20363;&#22914;Integrated Gradients&#65288;IG&#65289;&#65289;&#21487;&#20197;&#34987;&#25551;&#36848;&#20026;&#22522;&#20110;&#36335;&#24452;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#25968;&#25454;&#21644;&#26080;&#20449;&#24687;&#22522;&#32447;&#20043;&#38388;&#30340;&#30452;&#32447;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#21516;&#26102;&#20026;&#27599;&#20010;&#21477;&#23376;&#21333;&#35789;&#37327;&#20135;&#29983;&#36335;&#24452;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#20174;&#25554;&#20540;&#35789;&#29983;&#25104;&#30340;&#21477;&#23376;&#27809;&#26377;&#26126;&#30830;&#30340;&#21547;&#20041;&#65292;&#25110;&#32773;&#19982;&#21407;&#22987;&#21477;&#23376;&#30456;&#27604;&#26377;&#26174;&#30528;&#19981;&#21516;&#30340;&#21547;&#20041;&#12290;&#20026;&#20102;&#20351;&#36825;&#20123;&#21477;&#23376;&#30340;&#21547;&#20041;&#23613;&#21487;&#33021;&#25509;&#36817;&#21407;&#22987;&#21477;&#23376;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39034;&#24207;Integrated Gradients&#65288;SIG&#65289;&#65292;&#23427;&#36890;&#36807;&#20445;&#25345;&#20854;&#20182;&#21333;&#35789;&#19981;&#21464;&#65292;&#20165;&#22312;&#22522;&#32447;&#21644;&#24863;&#20852;&#36259;&#30340;&#21333;&#35789;&#20043;&#38388;&#21019;&#24314;&#25554;&#20540;&#26469;&#35745;&#31639;&#21477;&#23376;&#20013;&#27599;&#20010;&#21333;&#35789;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#21463;&#21040;&#20960;&#20010;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36824;&#24314;&#35758;&#29992;&#35757;&#32451;&#30340;&#20196;&#29260;&#8220;mask&#8221;&#26367;&#25442;&#22522;&#32447;&#20196;&#29260;&#8220;pad&#8221;&#12290;&#34429;&#28982;&#36825;&#21482;&#26159;&#23545;&#21407;&#22987;IG&#26041;&#27861;&#30340;&#31616;&#21333;&#25913;&#36827;&#65292;&#20294;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several explanation methods such as Integrated Gradients (IG) can be characterised as path-based methods, as they rely on a straight line between the data and an uninformative baseline. However, when applied to language models, these methods produce a path for each word of a sentence simultaneously, which could lead to creating sentences from interpolated words either having no clear meaning, or having a significantly different meaning compared to the original sentence. In order to keep the meaning of these sentences as close as possible to the original one, we propose Sequential Integrated Gradients (SIG), which computes the importance of each word in a sentence by keeping fixed every other words, only creating interpolations between the baseline and the word of interest. Moreover, inspired by the training procedure of several language models, we also propose to replace the baseline token "pad" with the trained token "mask". While being a simple improvement over the original IG method
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#30456;&#30683;&#30462;&#24187;&#35273;&#36827;&#34892;&#20102;&#35780;&#20272;&#12289;&#26816;&#27979;&#21644;&#32531;&#35299;&#65292;&#25506;&#31350;&#20102;&#36825;&#19968;&#24187;&#35273;&#24418;&#24335;&#30340;&#26222;&#36941;&#23384;&#22312;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#26694;&#26550;&#26377;&#25928;&#35302;&#21457;&#33258;&#30456;&#30683;&#30462;&#65292;&#21457;&#29616;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#20013;&#36825;&#31181;&#29616;&#35937;&#37117;&#39057;&#32321;&#20986;&#29616;&#12290;ChatGPT&#21644;GPT-4&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#33258;&#30456;&#30683;&#30462;&#65292;&#32780;Vicuna-13B&#21017;&#26377;&#20123;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2305.15852</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#30456;&#30683;&#30462;&#24187;&#35273;&#65306;&#35780;&#20272;&#12289;&#26816;&#27979;&#21644;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation. (arXiv:2305.15852v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#30456;&#30683;&#30462;&#24187;&#35273;&#36827;&#34892;&#20102;&#35780;&#20272;&#12289;&#26816;&#27979;&#21644;&#32531;&#35299;&#65292;&#25506;&#31350;&#20102;&#36825;&#19968;&#24187;&#35273;&#24418;&#24335;&#30340;&#26222;&#36941;&#23384;&#22312;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#26694;&#26550;&#26377;&#25928;&#35302;&#21457;&#33258;&#30456;&#30683;&#30462;&#65292;&#21457;&#29616;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#20013;&#36825;&#31181;&#29616;&#35937;&#37117;&#39057;&#32321;&#20986;&#29616;&#12290;ChatGPT&#21644;GPT-4&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#33258;&#30456;&#30683;&#30462;&#65292;&#32780;Vicuna-13B&#21017;&#26377;&#20123;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#20135;&#29983;&#24187;&#24819;&#30340;&#25991;&#26412;&#12290;&#33258;&#30456;&#30683;&#30462;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#24187;&#35273;&#24418;&#24335;&#65292;&#25351;&#30340;&#26159;&#35821;&#35328;&#27169;&#22411;&#22312;&#21516;&#19968;&#35821;&#22659;&#20013;&#29983;&#25104;&#20004;&#20010;&#30683;&#30462;&#30340;&#21477;&#23376;&#12290;&#26412;&#25991;&#38024;&#23545;&#26368;&#20808;&#36827;&#12289;&#32463;&#36807;&#25351;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23545;&#33258;&#30456;&#30683;&#30462;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#12289;&#35780;&#20272;&#12289;&#26816;&#27979;&#21644;&#32531;&#35299;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#26377;&#25928;&#22320;&#35302;&#21457;&#33258;&#30456;&#30683;&#30462;&#65292;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#26080;&#35770;&#26159;&#23545;&#20110;&#33879;&#21517;&#30340;&#36824;&#26159;&#19981;&#22826;&#20986;&#21517;&#30340;&#35805;&#39064;&#65292;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#30456;&#30683;&#30462;&#37117;&#32463;&#24120;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (large LMs) are susceptible to producing text with hallucinated content. Self-contradiction, where the LM generates two contradictory sentences within the same context, is an important form of hallucination. In this work, we present a comprehensive analysis on self-contradiction for state-of-the-art, instruction-tuned LMs, including evaluation, detection, and mitigation. To effectively trigger self-contradictions, we design a framework that constrains LMs to generate appropriate sentence pairs. Our evaluation on these sentence pairs reveals that self-contradictions occur frequently across different LMs for both famous and lesser-known topics. Next, we prompt the LMs to detect self-contradictions. Our results indicate that ChatGPT and GPT-4 are able to accurately identify self-contradictions, while Vicuna-13B struggles to do so. For example, with our best prompting method, ChatGPT achieves 91.0% precision and 80.5% recall on the sentence pairs generated by itself. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#21363;&#22810;&#23610;&#24230; KPPillarsBEV&#65292;&#20197;&#32531;&#35299;&#38647;&#36798;&#30446;&#26631;&#26816;&#27979;&#20013;&#20174;&#28857;&#20113;&#25968;&#25454;&#36716;&#21270;&#20026;&#32593;&#26684;&#32467;&#26500;&#36807;&#31243;&#20013;&#30340;&#20449;&#24687;&#20002;&#22833;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32593;&#26684;&#28210;&#26579;&#26041;&#27861; KPBEV&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.15836</link><description>&lt;p&gt;
&#29992;&#20110;&#38647;&#36798;&#30446;&#26631;&#26816;&#27979;&#30340;&#28857;&#20113;&#22810;&#23610;&#24230;&#32593;&#26684;&#28210;&#26579;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improved Multi-Scale Grid Rendering of Point Clouds for Radar Object Detection Networks. (arXiv:2305.15836v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#21363;&#22810;&#23610;&#24230; KPPillarsBEV&#65292;&#20197;&#32531;&#35299;&#38647;&#36798;&#30446;&#26631;&#26816;&#27979;&#20013;&#20174;&#28857;&#20113;&#25968;&#25454;&#36716;&#21270;&#20026;&#32593;&#26684;&#32467;&#26500;&#36807;&#31243;&#20013;&#30340;&#20449;&#24687;&#20002;&#22833;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32593;&#26684;&#28210;&#26579;&#26041;&#27861; KPBEV&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#28857;&#20113;&#36716;&#25442;&#20026;&#32593;&#26684;&#34920;&#24449;&#65292;&#28982;&#21518;&#24212;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#21487;&#29992;&#20110;&#38647;&#36798;&#30446;&#26631;&#26816;&#27979;&#65292;&#20294;&#20174;&#19981;&#35268;&#21017;&#30340;&#28857;&#20113;&#25968;&#25454;&#36716;&#25442;&#20026;&#23494;&#38598;&#30340;&#32593;&#26684;&#32467;&#26500;&#24120;&#24120;&#20250;&#23548;&#33268;&#20449;&#24687;&#30340;&#20002;&#22833;&#65292;&#36825;&#26159;&#30001;&#20110;&#28857;&#30340;&#31163;&#25955;&#21270;&#21644;&#32858;&#21512;&#36896;&#25104;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#21363;&#22810;&#23610;&#24230; KPPillarsBEV&#65292;&#20197;&#32531;&#35299;&#32593;&#26684;&#28210;&#26579;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32593;&#26684;&#28210;&#26579;&#26041;&#27861; KPBEV&#65292;&#23427;&#21033;&#29992;&#26680;&#28857;&#21367;&#31215;&#30340;&#25551;&#36848;&#33021;&#21147;&#26469;&#25913;&#36827;&#32593;&#26684;&#28210;&#26579;&#36807;&#31243;&#20013;&#23616;&#37096;&#28857;&#20113;&#19978;&#19979;&#25991;&#30340;&#32534;&#30721;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#22810;&#23610;&#24230;&#32593;&#26684;&#28210;&#26579;&#20844;&#24335;&#65292;&#20197;&#20219;&#24847;&#32593;&#26684;&#28210;&#26579;&#26041;&#27861;&#23558;&#22810;&#23610;&#24230;&#29305;&#24449;&#26144;&#23556;&#34701;&#21512;&#21040;&#26816;&#27979;&#32593;&#32476;&#30340;&#21367;&#31215;&#39592;&#24178;&#20013;&#12290;&#25105;&#20204;&#22312; nuScenes &#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#35780;&#20272;&#20102;&#26816;&#27979;&#27773;&#36710;&#12289;&#21345;&#36710;&#21644;&#20844;&#20132;&#36710;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#22810;&#23610;&#24230; KPPillarsBEV &#32467;&#26500;&#21644; KPBEV &#32593;&#26684;&#28210;&#26579;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Architectures that first convert point clouds to a grid representation and then apply convolutional neural networks achieve good performance for radar-based object detection. However, the transfer from irregular point cloud data to a dense grid structure is often associated with a loss of information, due to the discretization and aggregation of points. In this paper, we propose a novel architecture, multi-scale KPPillarsBEV, that aims to mitigate the negative effects of grid rendering. Specifically, we propose a novel grid rendering method, KPBEV, which leverages the descriptive power of kernel point convolutions to improve the encoding of local point cloud contexts during grid rendering. In addition, we propose a general multi-scale grid rendering formulation to incorporate multi-scale feature maps into convolutional backbones of detection networks with arbitrary grid rendering methods. We perform extensive experiments on the nuScenes dataset and evaluate the methods in terms of dete
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PDE+&#65292;&#19968;&#31181;&#36890;&#36807;&#33258;&#36866;&#24212;&#20998;&#24067;&#25193;&#25955;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#26469;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#19981;&#20165;&#33021;&#21462;&#24471;&#26368;&#20808;&#36827;&#25110;&#26377;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#36824;&#26377;&#26356;&#23569;&#30340;&#35757;&#32451;&#36718;&#27425;&#21644;&#32593;&#32476;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.15835</link><description>&lt;p&gt;
PDE+&#65306;&#36890;&#36807;&#33258;&#36866;&#24212;&#20998;&#24067;&#25193;&#25955;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
PDE+: Enhancing Generalization via PDE with Adaptive Distributional Diffusion. (arXiv:2305.15835v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PDE+&#65292;&#19968;&#31181;&#36890;&#36807;&#33258;&#36866;&#24212;&#20998;&#24067;&#25193;&#25955;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#26469;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#19981;&#20165;&#33021;&#21462;&#24471;&#26368;&#20808;&#36827;&#25110;&#26377;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#36824;&#26377;&#26356;&#23569;&#30340;&#35757;&#32451;&#36718;&#27425;&#21644;&#32593;&#32476;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#23545;&#19982;&#35757;&#32451;&#25968;&#25454;&#19981;&#21516;&#30340;&#20998;&#24067;&#26102;&#30340;&#24615;&#33021;&#12290;&#30446;&#21069;&#20027;&#35201;&#37319;&#29992;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#33539;&#24335;&#30340;&#26041;&#24335;&#65292;&#22914;&#25968;&#25454;&#22686;&#24378;&#65292;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#22122;&#22768;&#27880;&#20837;&#31561;&#65292;&#36825;&#20123;&#26041;&#27861;&#30001;&#20110;&#27169;&#22411;&#30340;&#38750;&#24179;&#28369;&#24615;&#21487;&#33021;&#20250;&#23548;&#33268;&#27867;&#21270;&#33021;&#21147;&#21463;&#38480;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#35282;&#24230;&#26469;&#30740;&#31350;&#27867;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#30452;&#25509;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#30784;&#20989;&#25968;&#26469;&#22686;&#24378;&#23427;&#65292;&#32780;&#19981;&#26159;&#32858;&#28966;&#20110;&#35843;&#25972;&#36755;&#20837;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#19982;&#29305;&#23450;PDE&#35299;&#30340;&#24179;&#28369;&#24230;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#21363;&#8220;&#36755;&#36816;&#26041;&#31243;&#8221;&#12290;&#36825;&#26679;&#24314;&#31435;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24635;&#20307;&#26694;&#26550;&#65292;&#23558;&#33258;&#36866;&#24212;&#20998;&#24067;&#25193;&#25955;&#24341;&#20837;&#36755;&#36816;&#26041;&#31243;&#20013;&#65292;&#20197;&#22686;&#24378;&#20854;&#35299;&#30340;&#24179;&#28369;&#24230;&#65292;&#20174;&#32780;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#25554;&#20214;&#27169;&#22359;&#20351;&#29992;&#65292;&#19982;&#20219;&#20309;&#29616;&#26377;&#30340;&#35757;&#32451;&#31639;&#27861;&#26080;&#32541;&#38598;&#25104;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#23569;&#30340;&#32593;&#32476;&#21442;&#25968;&#21644;&#35757;&#32451;&#36718;&#27425;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#25110;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generalization of neural networks is a central challenge in machine learning, especially concerning the performance under distributions that differ from training ones. Current methods, mainly based on the data-driven paradigm such as data augmentation, adversarial training, and noise injection, may encounter limited generalization due to model non-smoothness. In this paper, we propose to investigate generalization from a Partial Differential Equation (PDE) perspective, aiming to enhance it directly through the underlying function of neural networks, rather than focusing on adjusting input data. Specifically, we first establish the connection between neural network generalization and the smoothness of the solution to a specific PDE, namely ``transport equation''. Building upon this, we propose a general framework that introduces adaptive distributional diffusion into transport equation to enhance the smoothness of its solution, thereby improving generalization. In the context of neu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;PLC/DCS&#25511;&#21046;&#36923;&#36753;&#65292;&#24182;&#21019;&#36896;&#20102;&#29992;&#20110;&#27979;&#35797;&#21644;&#27604;&#36739;&#36825;&#20123;&#27169;&#22411;&#25511;&#21046;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#38598;&#21512;&#12290;</title><link>http://arxiv.org/abs/2305.15809</link><description>&lt;p&gt;
ChatGPT&#29992;&#20110;PLC/DCS&#25511;&#21046;&#36923;&#36753;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
ChatGPT for PLC/DCS Control Logic Generation. (arXiv:2305.15809v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;PLC/DCS&#25511;&#21046;&#36923;&#36753;&#65292;&#24182;&#21019;&#36896;&#20102;&#29992;&#20110;&#27979;&#35797;&#21644;&#27604;&#36739;&#36825;&#20123;&#27169;&#22411;&#25511;&#21046;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25552;&#20379;&#29983;&#25104;&#22411;AI&#24050;&#32463;&#27969;&#34892;&#36215;&#26469;&#65292;&#20197;&#25903;&#25345;&#36719;&#20214;&#24037;&#31243;&#24072;&#21019;&#24314;&#12289;&#27719;&#24635;&#12289;&#20248;&#21270;&#21644;&#35760;&#24405;&#28304;&#20195;&#30721;&#12290;&#20294;&#26159;LLMs&#22914;&#20309;&#25903;&#25345;&#25511;&#21046;&#24037;&#31243;&#24072;&#20351;&#29992;&#20856;&#22411;&#30340;&#25511;&#21046;&#32534;&#31243;&#35821;&#35328;&#36827;&#34892;&#32534;&#31243;&#20219;&#21153;&#20173;&#28982;&#26410;&#30693;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#25506;&#35752;&#20102;GitHub CoPilot&#25110;DeepMind AlphaCode&#36827;&#34892;&#28304;&#20195;&#30721;&#29983;&#25104;&#65292;&#20294;&#23578;&#26410;&#25915;&#20811;&#25511;&#21046;&#36923;&#36753;&#32534;&#31243;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#26159;&#19968;&#39033;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;100&#20010;LLM&#25552;&#31034;&#22312;10&#20010;&#20195;&#34920;&#24615;&#31867;&#21035;&#20013;&#20998;&#26512;PLCs&#21644;DCS&#30340;&#25511;&#21046;&#36923;&#36753;&#29983;&#25104;&#30340;&#33258;&#28982;&#35821;&#35328;&#12290;&#25105;&#20204;&#20351;&#29992;ChatGPT&#21644;GPT-4 LLM&#29983;&#25104;&#31572;&#26696;&#26469;&#27979;&#35797;&#36825;&#20123;&#25552;&#31034;&#12290;&#23427;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#29983;&#25104;&#31526;&#21512;&#35821;&#27861;&#35268;&#33539;&#30340;IEC 61131-3&#32467;&#26500;&#21270;&#25991;&#26412;&#20195;&#30721;&#65292;&#24182;&#23637;&#31034;&#20102;&#26377;&#29992;&#30340;&#25512;&#29702;&#25216;&#24039;&#65292;&#36825;&#21487;&#33021;&#20250;&#25552;&#39640;&#25511;&#21046;&#24037;&#31243;&#24072;&#30340;&#29983;&#20135;&#21147;&#12290;&#25105;&#20204;&#30340;&#25552;&#31034;&#38598;&#26159;&#19968;&#20010;&#26356;&#27491;&#24335;&#30340;LLM&#22522;&#20934;&#27979;&#35797;&#30340;&#22522;&#30784;&#65292;&#20197;&#27979;&#35797;&#21644;&#27604;&#36739;&#27492;&#31867;&#27169;&#22411;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) providing generative AI have become popular to support software engineers in creating, summarizing, optimizing, and documenting source code. It is still unknown how LLMs can support control engineers using typical control programming languages in programming tasks. Researchers have explored GitHub CoPilot or DeepMind AlphaCode for source code generation but did not yet tackle control logic programming. The contribution of this paper is an exploratory study, for which we created 100 LLM prompts in 10 representative categories to analyze control logic generation for of PLCs and DCS from natural language. We tested the prompts by generating answers with ChatGPT using the GPT-4 LLM. It generated syntactically correct IEC 61131-3 Structured Text code in many cases and demonstrated useful reasoning skills that could boost control engineer productivity. Our prompt collection is the basis for a more formal LLM benchmark to test and compare such models for control l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340; Lucy-SKG &#23398;&#20064;&#29609;&#8220;&#28779;&#31661;&#32852;&#30431;&#8221;&#28216;&#25103;&#30340;&#26041;&#27861;&#65292;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#35813;&#28216;&#25103;&#20013;&#25490;&#21517;&#26368;&#39640;&#30340;&#20004;&#20010;&#26426;&#22120;&#20154;&#65292;&#24182;&#25104;&#20026;&#20102;&#26368;&#20808;&#36827;&#30340;&#20195;&#29702;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2305.15801</link><description>&lt;p&gt;
Lucy-SKG&#65306;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#39640;&#25928;&#23398;&#20064;&#29609;&#8220;&#28779;&#31661;&#32852;&#30431;&#8221;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Lucy-SKG: Learning to Play Rocket League Efficiently Using Deep Reinforcement Learning. (arXiv:2305.15801v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15801
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340; Lucy-SKG &#23398;&#20064;&#29609;&#8220;&#28779;&#31661;&#32852;&#30431;&#8221;&#28216;&#25103;&#30340;&#26041;&#27861;&#65292;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#35813;&#28216;&#25103;&#20013;&#25490;&#21517;&#26368;&#39640;&#30340;&#20004;&#20010;&#26426;&#22120;&#20154;&#65292;&#24182;&#25104;&#20026;&#20102;&#26368;&#20808;&#36827;&#30340;&#20195;&#29702;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;AI&#39046;&#22495;&#21462;&#24471;&#36827;&#23637;&#30340;&#19968;&#31181;&#25104;&#21151;&#31574;&#30053;&#26159;&#23558;&#28216;&#25103;&#35270;&#20026;&#38382;&#39064;&#65292;&#36825;&#24050;&#34987;&#35777;&#26126;&#20250;&#23548;&#33268;&#21508;&#31181;&#37325;&#22823;&#31361;&#30772;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#36825;&#31181;&#31574;&#30053;&#26469;&#30740;&#31350;&#8220;&#28779;&#31661;&#32852;&#30431;&#8221;&#28216;&#25103;&#65292;&#36825;&#26159;&#19968;&#27454;&#24191;&#21463;&#27426;&#36814;&#20294;&#30456;&#23545;&#26410;&#34987;&#28145;&#20837;&#25506;&#32034;&#30340;3D&#22810;&#20154;&#22312;&#32447;&#28216;&#25103;&#65292;&#20855;&#26377;&#29420;&#29305;&#30340;&#29289;&#29702;&#24341;&#25806;&#21644;&#22797;&#26434;&#30340;&#21160;&#21147;&#23398;&#65292;&#23545;&#20110;&#24320;&#21457;&#39640;&#25928;&#21644;&#39640;&#24615;&#33021;&#30340;&#28216;&#25103;&#20195;&#29702;&#31243;&#24207;&#26469;&#35828;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340; Lucy-SKG&#65292;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#23398;&#20064;&#22914;&#20309;&#29609;&#28779;&#31661;&#32852;&#30431;&#28216;&#25103;&#65292;&#22312;&#24615;&#33021;&#19978;&#26126;&#26174;&#20248;&#20110;&#26412;&#28216;&#25103;&#20013;&#25490;&#21517;&#26368;&#39640;&#30340;&#20004;&#20010;&#26426;&#22120;&#20154; Necto&#65288;2022&#24180;&#26426;&#22120;&#20154;&#20896;&#20891;&#65289;&#21644;&#20854;&#21518;&#32487; Nexto&#65292;&#20174;&#32780;&#25104;&#20026;&#26368;&#20808;&#36827;&#30340;&#20195;&#29702;&#31243;&#24207;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#65306;a&#65289;&#24320;&#21457;&#20102;&#22870;&#21169;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#24211;&#65307;b&#65289;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#32908;&#32905;&#21453;&#39304;&#32452;&#21512;&#65292;&#25429;&#25417;&#22797;&#26434;&#22870;&#21169;&#31867;&#22411;&#30340;&#25928;&#29992;&#30340;&#26032;&#22411;&#21442;&#25968;&#21270;&#22870;&#21169;&#24418;&#29366;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
A successful tactic that is followed by the scientific community for advancing AI is to treat games as problems, which has been proven to lead to various breakthroughs. We adapt this strategy in order to study Rocket League, a widely popular but rather under-explored 3D multiplayer video game with a distinct physics engine and complex dynamics that pose a significant challenge in developing efficient and high-performance game-playing agents. In this paper, we present Lucy-SKG, a Reinforcement Learning-based model that learned how to play Rocket League in a sample-efficient manner, outperforming by a notable margin the two highest-ranking bots in this game, namely Necto (2022 bot champion) and its successor Nexto, thus becoming a state-of-the-art agent. Our contributions include: a) the development of a reward analysis and visualization library, b) novel parameterizable reward shape functions that capture the utility of complex reward types via our proposed Kinesthetic Reward Combinatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;RFMS&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#30340;&#22788;&#29702;&#25968;&#21315;&#20010;&#31867;&#21035;&#30340;&#36229;&#39640;&#32500;&#24230;&#25968;&#25454;&#65292;&#36890;&#36807;&#38182;&#26631;&#36187;&#25490;&#24207;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#36873;&#25321;&#23454;&#29616;&#29305;&#24449;&#36873;&#25321;&#65292;&#20855;&#26377;&#19982;&#34892;&#19994;&#26631;&#20934;&#30340;&#29305;&#24449;&#31579;&#36873;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#21644;&#26356;&#22810;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.15793</link><description>&lt;p&gt;
&#29305;&#24449;&#31354;&#38388;&#38477;&#32500;&#26041;&#27861;&#23545;&#20110;&#36229;&#39640;&#32500;&#24230;&#22810;&#31867;&#25968;&#25454;&#30340;&#30740;&#31350;&#65306;&#22522;&#20110;&#38543;&#26426;&#26862;&#26519;&#30340;&#22810;&#36718;&#31579;&#36873;(RFMS)
&lt;/p&gt;
&lt;p&gt;
Feature space reduction method for ultrahigh-dimensional, multiclass data: Random forest-based multiround screening (RFMS). (arXiv:2305.15793v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;RFMS&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#30340;&#22788;&#29702;&#25968;&#21315;&#20010;&#31867;&#21035;&#30340;&#36229;&#39640;&#32500;&#24230;&#25968;&#25454;&#65292;&#36890;&#36807;&#38182;&#26631;&#36187;&#25490;&#24207;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#36873;&#25321;&#23454;&#29616;&#29305;&#24449;&#36873;&#25321;&#65292;&#20855;&#26377;&#19982;&#34892;&#19994;&#26631;&#20934;&#30340;&#29305;&#24449;&#31579;&#36873;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#21644;&#26356;&#22810;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24050;&#26377;&#22823;&#37327;&#38024;&#23545;&#25317;&#26377;&#25968;&#21313;&#19975;&#29305;&#24449;&#30340;&#36229;&#39640;&#32500;&#24230;&#25968;&#25454;&#30340;&#31579;&#36873;&#26041;&#27861;&#34987;&#21457;&#24067;&#65292;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#25968;&#21315;&#20010;&#31867;&#21035;&#30340;&#25968;&#25454;&#12290;&#22522;&#20110;&#22810;&#36890;&#36947;&#29983;&#29289;&#29305;&#24449;&#25968;&#25454;&#39564;&#35777;&#29992;&#25143;&#30340;&#39044;&#27979;&#27169;&#22411;&#20063;&#38754;&#20020;&#30528;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#38543;&#26426;&#26862;&#26519;&#30340;&#22810;&#36718;&#31579;&#36873;(RFMS)&#65292;&#21487;&#20197;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#24212;&#29992;&#12290;&#35813;&#31639;&#27861;&#23558;&#29305;&#24449;&#31354;&#38388;&#21010;&#20998;&#20026;&#23567;&#30340;&#23376;&#38598;&#65292;&#24182;&#25191;&#34892;&#19968;&#31995;&#21015;&#37096;&#20998;&#27169;&#22411;&#26500;&#24314;&#12290;&#36825;&#20123;&#37096;&#20998;&#27169;&#22411;&#29992;&#20110;&#23454;&#29616;&#22522;&#20110;&#38182;&#26631;&#36187;&#25490;&#24207;&#21644;&#22522;&#20110;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#29305;&#24449;&#36873;&#25321;&#12290;&#20351;&#29992;&#21512;&#25104;&#29983;&#29289;&#29305;&#24449;&#31354;&#38388;&#29983;&#25104;&#22120;BiometricBlender&#26469;&#23545;RFMS&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#26681;&#25454;&#32467;&#26524;&#65292;RFMS&#19982;&#34892;&#19994;&#26631;&#20934;&#30340;&#29305;&#24449;&#31579;&#36873;&#26041;&#27861;&#30456;&#24403;&#65292;&#24182;&#19988;&#21516;&#26102;&#20855;&#26377;&#35768;&#22810;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, numerous screening methods have been published for ultrahigh-dimensional data that contain hundreds of thousands of features; however, most of these features cannot handle data with thousands of classes. Prediction models built to authenticate users based on multichannel biometric data result in this type of problem. In this study, we present a novel method known as random forest-based multiround screening (RFMS) that can be effectively applied under such circumstances. The proposed algorithm divides the feature space into small subsets and executes a series of partial model builds. These partial models are used to implement tournament-based sorting and the selection of features based on their importance. To benchmark RFMS, a synthetic biometric feature space generator known as BiometricBlender is employed. Based on the results, the RFMS is on par with industry-standard feature screening methods while simultaneously possessing many advantages over these methods.
&lt;/p&gt;</description></item><item><title>ChatGPT-like&#31995;&#32479;&#33021;&#22815;&#25903;&#25345;&#33258;&#21160;&#26381;&#21153;&#32452;&#21512;&#39046;&#22495;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.15788</link><description>&lt;p&gt;
&#22312;ChatGPT&#26102;&#20195;&#30340;&#26381;&#21153;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
Service Composition in the ChatGPT Era. (arXiv:2305.15788v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15788
&lt;/p&gt;
&lt;p&gt;
ChatGPT-like&#31995;&#32479;&#33021;&#22815;&#25903;&#25345;&#33258;&#21160;&#26381;&#21153;&#32452;&#21512;&#39046;&#22495;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;ChatGPT&#31561;&#31995;&#32479;&#22914;&#20309;&#25903;&#25345;&#33258;&#21160;&#26381;&#21153;&#32452;&#21512;&#39046;&#22495;&#65292;&#24182;&#30830;&#23450;&#20102;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#20197;&#20415;&#20805;&#20998;&#21033;&#29992;&#36825;&#26679;&#30340;&#24037;&#20855;&#22312;&#38754;&#21521;&#26381;&#21153;&#30340;&#32452;&#21512;&#39046;&#22495;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper speculates about how ChatGPT-like systems can support the field of automated service composition and identifies new research areas to explore in order to take advantage of such tools in the field of service-oriented composition.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35268;&#21010;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;LLMs&#33258;&#20027;&#29983;&#25104;&#21487;&#25191;&#34892;&#35745;&#21010;&#30340;&#33021;&#21147;&#26377;&#38480;&#65292;&#20294;&#22312;&#21551;&#21457;&#24335;&#27169;&#24335;&#19979;&#34920;&#29616;&#26356;&#26377;&#21069;&#36884;&#12290;</title><link>http://arxiv.org/abs/2305.15771</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#21010;&#33021;&#21147;&#8212;&#8212;&#19968;&#39033;&#20851;&#38190;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
On the Planning Abilities of Large Language Models -- A Critical Investigation. (arXiv:2305.15771v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35268;&#21010;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;LLMs&#33258;&#20027;&#29983;&#25104;&#21487;&#25191;&#34892;&#35745;&#21010;&#30340;&#33021;&#21147;&#26377;&#38480;&#65292;&#20294;&#22312;&#21551;&#21457;&#24335;&#27169;&#24335;&#19979;&#34920;&#29616;&#26356;&#26377;&#21069;&#36884;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21463;&#21040;&#35821;&#35328;&#27169;&#22411;&#22312;&#24120;&#35782;&#35268;&#21010;&#20219;&#21153;&#20013;&#20855;&#26377;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#30340;&#22768;&#31216;&#30340;&#21551;&#21457;&#65292;&#26088;&#22312;&#30740;&#31350;&#20854;&#35268;&#21010;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35780;&#20272;LLMs&#22312;&#33258;&#20027;&#29983;&#25104;&#24120;&#35782;&#35268;&#21010;&#20219;&#21153;&#20013;&#30340;&#35745;&#21010;&#25928;&#26524;&#21644;LLMs&#20316;&#20026;&#20854;&#20182;&#26234;&#33021;&#20307;&#65288;AI&#35268;&#21010;&#32773;&#65289;&#22312;&#20854;&#35268;&#21010;&#20219;&#21153;&#20013;&#21551;&#21457;&#24615;&#25351;&#23548;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#29983;&#25104;&#19968;&#22871;&#19982;&#22269;&#38469;&#35745;&#21010;&#31454;&#36187;&#20013;&#20351;&#29992;&#30340;&#30456;&#20284;&#39046;&#22495;&#30340;&#23454;&#20363;&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#65292;&#24182;&#22312;&#20004;&#31181;&#19981;&#21516;&#30340;&#27169;&#24335;&#19979;&#35780;&#20272;LLMs&#65306;&#33258;&#20027;&#21644;&#21551;&#21457;&#24335;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;LLMs&#33258;&#20027;&#29983;&#25104;&#21487;&#25191;&#34892;&#35745;&#21010;&#30340;&#33021;&#21147;&#30456;&#24403;&#26377;&#38480;&#65292;&#26368;&#20339;&#27169;&#22411;&#65288;GPT-4&#65289;&#22312;&#39046;&#22495;&#20013;&#30340;&#24179;&#22343;&#25104;&#21151;&#29575;&#32422;&#20026;12%&#12290;&#28982;&#32780;&#65292;&#21551;&#21457;&#24335;&#27169;&#24335;&#19979;&#30340;&#32467;&#26524;&#26174;&#31034;&#20102;&#26356;&#26377;&#21069;&#36884;&#30340;&#36857;&#35937;&#12290;&#22312;&#21551;&#21457;&#24335;&#27169;&#24335;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;LLM&#29983;&#25104;&#30340;&#35745;&#21010;&#21487;&#20197;&#25913;&#21892;&#22522;&#30784;&#35268;&#21010;&#22120;&#30340;&#25628;&#32034;&#36807;&#31243;&#21644;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intrigued by the claims of emergent reasoning capabilities in LLMs trained on general web corpora, in this paper, we set out to investigate their planning capabilities. We aim to evaluate (1) the effectiveness of LLMs in generating plans autonomously in commonsense planning tasks and (2) the potential of LLMs as a source of heuristic guidance for other agents (AI planners) in their planning tasks. We conduct a systematic study by generating a suite of instances on domains similar to the ones employed in the International Planning Competition and evaluate LLMs in two distinct modes: autonomous and heuristic. Our findings reveal that LLMs' ability to generate executable plans autonomously is rather limited, with the best model (GPT-4) having an average success rate of ~12% across the domains. However, the results in the heuristic mode show more promise. In the heuristic mode, we demonstrate that LLM-generated plans can improve the search process for underlying sound planners and addition
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#25442;&#23398;&#20064;&#30340;&#32593;&#32476;&#26550;&#26500;&#35774;&#35745;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#22312;&#23398;&#20064;&#20013;&#22686;&#24378;&#24863;&#21463;&#37326;&#21644;&#34701;&#21512;&#19981;&#21516;&#23610;&#24230;&#29305;&#24449;&#30340;&#20248;&#28857;&#65292;&#20351;&#29992;&#20102;&#20613;&#37324;&#21494;&#21464;&#25442;&#12289;&#22855;&#24322;&#20540;&#20998;&#35299;&#12289;&#30697;&#38453;&#20056;&#27861;&#21644;&#21367;&#31215;&#22359;&#31561;&#22235;&#31181;&#19981;&#21516;&#30340;&#21464;&#25442;&#26426;&#21046;&#36827;&#34892;&#26500;&#24314;&#65292;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#65292;&#36229;&#36807;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.15770</link><description>&lt;p&gt;
TLNets: &#22522;&#20110;&#21464;&#25442;&#23398;&#20064;&#30340;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
TLNets: Transformation Learning Networks for long-range time-series prediction. (arXiv:2305.15770v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15770
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#25442;&#23398;&#20064;&#30340;&#32593;&#32476;&#26550;&#26500;&#35774;&#35745;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#22312;&#23398;&#20064;&#20013;&#22686;&#24378;&#24863;&#21463;&#37326;&#21644;&#34701;&#21512;&#19981;&#21516;&#23610;&#24230;&#29305;&#24449;&#30340;&#20248;&#28857;&#65292;&#20351;&#29992;&#20102;&#20613;&#37324;&#21494;&#21464;&#25442;&#12289;&#22855;&#24322;&#20540;&#20998;&#35299;&#12289;&#30697;&#38453;&#20056;&#27861;&#21644;&#21367;&#31215;&#22359;&#31561;&#22235;&#31181;&#19981;&#21516;&#30340;&#21464;&#25442;&#26426;&#21046;&#36827;&#34892;&#26500;&#24314;&#65292;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#65292;&#36229;&#36807;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26159;&#35768;&#22810;&#23398;&#31185;&#39046;&#22495;&#37324;&#30340;&#19968;&#20010;&#24191;&#27867;&#38382;&#39064;&#65292;&#22914;&#27668;&#35937;&#23398;&#12289;&#20132;&#36890;&#30417;&#27979;&#12289;&#25237;&#36164;&#12289;&#33021;&#28304;&#29983;&#20135;&#21644;&#28040;&#36153;&#31561;&#12290;&#35768;&#22810;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#31574;&#30053;&#24050;&#32463;&#34987;&#24320;&#21457;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#26159;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#35201;&#20040;&#24403;&#39044;&#27979;&#26102;&#38388;&#33539;&#22260;&#22686;&#21152;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#25442;&#30340;&#32593;&#32476;&#26550;&#26500;&#35774;&#35745;&#26041;&#26696;&#65292;&#20855;&#26377;&#28508;&#21147;&#22312;&#23398;&#20064;&#20013;&#23454;&#29616;&#22686;&#24378;&#30340;&#24863;&#21463;&#37326;&#65292;&#24182;&#22312;&#19981;&#21516;&#23610;&#24230;&#19978;&#34701;&#21512;&#29305;&#24449;&#30340;&#22909;&#22788;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#21464;&#25442;&#26426;&#21046;&#20316;&#20026;&#26500;&#24314;&#23398;&#20064;&#27169;&#22411;&#30340;&#22522;&#30784;&#65292;&#21253;&#25324;&#20613;&#37324;&#21494;&#21464;&#25442;&#65288;FT&#65289;&#12289;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#12289;&#30697;&#38453;&#20056;&#27861;&#21644;&#21367;&#31215;&#22359;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22522;&#20110;&#20197;&#19978;&#26500;&#24314;&#22359;&#24320;&#21457;&#20102;&#22235;&#20010;&#23398;&#20064;&#27169;&#22411;&#65292;&#20998;&#21035;&#20026;FT-Matrix&#12289;FT-SVD&#12289;FT-Conv&#21644;Conv-SVD&#12290;&#38656;&#35201;&#27880;&#24847;&#30340;&#26159;&#65292;FT&#21644;SVD&#27169;&#22411;&#20855;&#26377;&#25552;&#21462;&#20302;&#32500;&#34920;&#31034;&#20197;&#22686;&#24378;&#23398;&#20064;&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#21512;&#25104;&#21644;&#23454;&#38469;&#38271;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#32463;&#39564;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#30456;&#23545;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#20248;&#24322;&#30340;&#38271;&#26399;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series prediction is a prevalent issue across various disciplines, such as meteorology, traffic surveillance, investment, and energy production and consumption. Many statistical and machine-learning strategies have been developed to tackle this problem. However, these approaches either lack explainability or exhibit less satisfactory performance when the prediction horizon increases. To this end, we propose a novel plan for the designing of networks' architecture based on transformations, possessing the potential to achieve an enhanced receptive field in learning which brings benefits to fuse features across scales. In this context, we introduce four different transformation mechanisms as bases to construct the learning model including Fourier Transform (FT), Singular Value Decomposition (SVD), matrix multiplication and Conv block. Hence, we develop four learning models based on the above building blocks, namely, FT-Matrix, FT-SVD, FT-Conv, and Conv-SVD. Note that the FT and SVD b
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;MERGE&#65292;&#19968;&#20010;&#22522;&#20110;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#31169;&#26377;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MERGE&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;26.5&#20493;&#30340;&#21152;&#36895;&#21644;80%&#30340;&#36890;&#20449;&#23383;&#33410;&#25968;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2305.15769</link><description>&lt;p&gt;
MERGE: &#24555;&#36895;&#30340;&#31169;&#26377;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
MERGE: Fast Private Text Generation. (arXiv:2305.15769v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15769
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;MERGE&#65292;&#19968;&#20010;&#22522;&#20110;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#31169;&#26377;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MERGE&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;26.5&#20493;&#30340;&#21152;&#36895;&#21644;80%&#30340;&#36890;&#20449;&#23383;&#33410;&#25968;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;NLP&#26381;&#21153;&#21644;Transformer&#27169;&#22411;&#30340;&#31169;&#26377;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20004;&#26041;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#20165;&#32771;&#34385;NLU&#22330;&#26223;&#65292;&#32780;&#25991;&#26412;&#29983;&#25104;&#30340;&#31169;&#26377;&#25512;&#29702;&#65292;&#22914;&#32763;&#35793;&#12289;&#23545;&#35805;&#21644;&#20195;&#30721;&#34917;&#20840;&#65292;&#20173;&#26410;&#35299;&#20915;&#12290;&#27492;&#22806;&#65292;&#23558;&#29616;&#26377;&#30340;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#36801;&#31227;&#21040;NLG&#27169;&#22411;&#26102;&#65292;&#24615;&#33021;&#34920;&#29616;&#24046;&#65292;&#32780;&#22312;&#35757;&#32451;&#38454;&#27573;&#21463;&#21040;&#25910;&#25947;&#38382;&#39064;&#30340;&#22256;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MERGE&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#31169;&#26377;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MERGE&#37325;&#29992;&#36755;&#20986;&#38544;&#34255;&#29366;&#24577;&#20316;&#20026;&#21333;&#35789;&#23884;&#20837;&#65292;&#20197;&#36339;&#36807;&#23884;&#20837;&#35745;&#31639;&#65292;&#24182;&#37325;&#26032;&#32452;&#32455;Transformer&#27169;&#22359;&#20013;&#30340;&#32447;&#24615;&#25805;&#20316;&#20197;&#21152;&#36895;&#21521;&#21069;&#36807;&#31243;&#12290;&#22522;&#20110;&#36825;&#20004;&#20010;&#20248;&#21270;&#65292;&#22823;&#37327;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#24207;&#21015;&#38271;&#24230;&#20026;512&#26102;&#65292;MERGE&#21487;&#23454;&#29616;26.5&#20493;&#30340;&#21152;&#36895;&#65292;&#24182;&#20943;&#23569;80\%&#30340;&#36890;&#20449;&#23383;&#33410;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen increasing concerns about the private inference of NLP services and Transformer models. However, existing two-party privacy-preserving methods solely consider NLU scenarios, while the private inference of text generation such as translation, dialogue, and code completion remains unsolved. Besides, while migrated to NLG models, existing privacy-preserving methods perform poorly in terms of inference speed, and suffer from the convergence problem during the training stage. To address these issues, we propose MERGE, a fast private text generation framework for Transformer-based language models. Specifically, MERGE reuse the output hidden state as the word embedding to bypass the embedding computation, and reorganize the linear operations in the Transformer module to accelerate the forward procedure. Based on these two optimizations, extensive experiments show that MERGE can achieve a 26.5x speedup under the sequence length 512, and reduce 80\% communication bytes, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35821;&#35328;&#25351;&#23548;&#30340;&#28857;&#20113;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#26041;&#27861;&#65292;&#21253;&#21547;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20196;&#29260;&#34701;&#21512;&#31574;&#30053;&#65292;&#24182;&#23454;&#29616;&#20102;&#19982;&#33258;&#28982;&#35821;&#35328;&#35821;&#20041;&#34701;&#21512;&#30340;&#28608;&#20809;&#38647;&#36798;&#29289;&#20307;&#25506;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36328;&#27169;&#24577;&#23454;&#39564;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#39640;&#25928;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.15765</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#20013;&#22522;&#20110;&#35821;&#35328;&#25351;&#23548;&#30340;&#28857;&#20113;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Language-Guided 3D Object Detection in Point Cloud for Autonomous Driving. (arXiv:2305.15765v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35821;&#35328;&#25351;&#23548;&#30340;&#28857;&#20113;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#26041;&#27861;&#65292;&#21253;&#21547;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20196;&#29260;&#34701;&#21512;&#31574;&#30053;&#65292;&#24182;&#23454;&#29616;&#20102;&#19982;&#33258;&#28982;&#35821;&#35328;&#35821;&#20041;&#34701;&#21512;&#30340;&#28608;&#20809;&#38647;&#36798;&#29289;&#20307;&#25506;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36328;&#27169;&#24577;&#23454;&#39564;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#39640;&#25928;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#19979;&#25509;&#21463;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#28857;&#20113;&#20013;&#30340;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#38382;&#39064;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#36890;&#24120;&#32858;&#28966;&#20110;&#20108;&#32500;&#25110;&#32773;&#19977;&#32500;&#23460;&#20869;&#22330;&#26223;&#65292;&#38590;&#20197;&#20934;&#30830;&#22320;&#39044;&#27979;&#22312;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#20013;&#25152;&#26597;&#35810;&#30340;&#19977;&#32500;&#21306;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#35270;&#35273;&#23450;&#20301;&#20219;&#21153;&#65292;&#31216;&#20026;&#28608;&#20809;&#38647;&#36798;&#35270;&#35273;&#23450;&#20301;&#12290;&#24182;&#19988;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21333;&#31471;&#23450;&#20301;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21253;&#21547;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20196;&#29260;&#34701;&#21512;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#19982;&#33258;&#28982;&#35821;&#35328;&#35821;&#20041;&#34701;&#21512;&#30340;&#28608;&#20809;&#38647;&#36798;&#29289;&#20307;&#25506;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#28789;&#27963;&#22320;&#23545;&#22270;&#20687;&#29305;&#24449;&#36827;&#34892;&#38598;&#25104;&#65292;&#20197;&#25552;&#20379;&#26356;&#20016;&#23500;&#30340;&#32441;&#29702;&#21644;&#39068;&#33394;&#20449;&#24687;&#12290;&#36328;&#27169;&#24577;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#39640;&#25928;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the problem of 3D referring expression comprehension (REC) in autonomous driving scenario, which aims to ground a natural language to the targeted region in LiDAR point clouds. Previous approaches for REC usually focus on the 2D or 3D-indoor domain, which is not suitable for accurately predicting the location of the queried 3D region in an autonomous driving scene. In addition, the upper-bound limitation and the heavy computation cost motivate us to explore a better solution. In this work, we propose a new multi-modal visual grounding task, termed LiDAR Grounding. Then we devise a Multi-modal Single Shot Grounding (MSSG) approach with an effective token fusion strategy. It jointly learns the LiDAR-based object detector with the language features and predicts the targeted region directly from the detector without any post-processing. Moreover, the image feature can be flexibly integrated into our approach to provide rich texture and color information. The cross-moda
&lt;/p&gt;</description></item><item><title>TransWorldNG&#26159;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#21644;&#22270;&#24418;&#35745;&#31639;&#25216;&#26415;&#30340;&#20132;&#36890;&#20223;&#30495;&#22120;&#65292;&#33021;&#22815;&#33258;&#21160;&#23398;&#20064;&#20132;&#36890;&#27169;&#24335;&#24182;&#29983;&#25104;&#26356;&#30495;&#23454;&#30340;&#20132;&#36890;&#27169;&#24335;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15743</link><description>&lt;p&gt;
TransWorldNG: &#36890;&#36807;&#22522;&#30784;&#27169;&#22411;&#23454;&#29616;&#20132;&#36890;&#20223;&#30495;
&lt;/p&gt;
&lt;p&gt;
TransWorldNG: Traffic Simulation via Foundation Model. (arXiv:2305.15743v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15743
&lt;/p&gt;
&lt;p&gt;
TransWorldNG&#26159;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#21644;&#22270;&#24418;&#35745;&#31639;&#25216;&#26415;&#30340;&#20132;&#36890;&#20223;&#30495;&#22120;&#65292;&#33021;&#22815;&#33258;&#21160;&#23398;&#20064;&#20132;&#36890;&#27169;&#24335;&#24182;&#29983;&#25104;&#26356;&#30495;&#23454;&#30340;&#20132;&#36890;&#27169;&#24335;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#20223;&#30495;&#26159;&#20132;&#36890;&#20915;&#31574;&#21644;&#25919;&#31574;&#24320;&#21457;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#38754;&#23545;&#20132;&#36890;&#29615;&#22659;&#30340;&#39640;&#32500;&#24230;&#21644;&#24322;&#36136;&#24615;&#65292;&#23454;&#29616;&#36924;&#30495;&#30340;&#20223;&#30495;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#21644;&#22270;&#24418;&#35745;&#31639;&#25216;&#26415;&#20174;&#30495;&#23454;&#25968;&#25454;&#20013;&#23398;&#20064;&#20132;&#36890;&#21160;&#24577;&#30340;&#20132;&#36890;&#20223;&#30495;&#22120;TransWordNG&#12290;&#20171;&#32461;&#20102;TransWorldNG&#30340;&#21151;&#33021;&#21644;&#32467;&#26500;&#65292;&#23427;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#20132;&#36890;&#31649;&#29702;&#21644;&#25511;&#21046;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#20223;&#30495;&#22120;&#30456;&#27604;&#65292;TransWorldNG&#33021;&#22815;&#29983;&#25104;&#26356;&#30495;&#23454;&#30340;&#20132;&#36890;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;TransWorldNG&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#38543;&#30528;&#22330;&#26223;&#35268;&#27169;&#30340;&#22686;&#22823;&#65292;&#35745;&#31639;&#26102;&#38388;&#21576;&#32447;&#24615;&#22686;&#38271;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#33258;&#21160;&#20174;&#23454;&#38469;&#25968;&#25454;&#20013;&#23398;&#20064;&#20132;&#36890;&#27169;&#24335;&#24182;&#39640;&#25928;&#29983;&#25104;&#20934;&#30830;&#36924;&#30495;&#30340;&#20132;&#36890;&#20223;&#30495;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic simulation is a crucial tool for transportation decision-making and policy development. However, achieving realistic simulations in the face of the high dimensionality and heterogeneity of traffic environments is a longstanding challenge. In this paper, we present TransWordNG, a traffic simulator that uses Data-driven algorithms and Graph Computing techniques to learn traffic dynamics from real data. The functionality and structure of TransWorldNG are introduced, which utilize a foundation model for transportation management and control. The results demonstrate that TransWorldNG can generate more realistic traffic patterns compared to traditional simulators. Additionally, TransWorldNG exhibits better scalability, as it shows linear growth in computation time as the scenario scale increases. To the best of our knowledge, this is the first traffic simulator that can automatically learn traffic patterns from real-world data and efficiently generate accurate and realistic traffic e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MPE4G&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#20849;&#21516;&#35821;&#38899;&#25163;&#21183;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#26469;&#35757;&#32451;&#22522;&#20110;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#32534;&#30721;&#22120;&#20197;&#21253;&#21547;&#27599;&#31181;&#27169;&#24577;&#30340;&#20449;&#24687;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#36924;&#30495;&#30340;&#20849;&#21516;&#35821;&#38899;&#25163;&#21183;&#65292;&#21363;&#20351;&#23384;&#22312;&#32570;&#22833;&#25110;&#22024;&#26434;&#30340;&#36755;&#20837;&#27169;&#24577;&#12290;</title><link>http://arxiv.org/abs/2305.15740</link><description>&lt;p&gt;
MPE4G: &#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#29992;&#20110;&#21516;&#26102;&#35821;&#38899;&#25163;&#21183;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
MPE4G: Multimodal Pretrained Encoder for Co-Speech Gesture Generation. (arXiv:2305.15740v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MPE4G&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#20849;&#21516;&#35821;&#38899;&#25163;&#21183;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#26469;&#35757;&#32451;&#22522;&#20110;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#32534;&#30721;&#22120;&#20197;&#21253;&#21547;&#27599;&#31181;&#27169;&#24577;&#30340;&#20449;&#24687;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#36924;&#30495;&#30340;&#20849;&#21516;&#35821;&#38899;&#25163;&#21183;&#65292;&#21363;&#20351;&#23384;&#22312;&#32570;&#22833;&#25110;&#22024;&#26434;&#30340;&#36755;&#20837;&#27169;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#34394;&#25311;&#20195;&#29702;&#20154;&#19982;&#20154;&#31867;&#36827;&#34892;&#20132;&#20114;&#26102;&#65292;&#25163;&#21183;&#23545;&#20110;&#36890;&#36807;&#35821;&#38899;&#20256;&#36882;&#20854;&#24847;&#22270;&#33267;&#20851;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#22810;&#27169;&#24577;&#21516;&#26102;&#35821;&#38899;&#25163;&#21183;&#29983;&#25104;&#27169;&#22411;&#38656;&#35201;&#23545;&#25152;&#26377;&#27169;&#24577;&#30340;&#32534;&#30721;&#29305;&#24449;&#36827;&#34892;&#32534;&#30721;&#65292;&#25165;&#33021;&#29983;&#25104;&#25163;&#21183;&#12290;&#22914;&#26524;&#26576;&#20123;&#36755;&#20837;&#27169;&#24577;&#34987;&#31227;&#38500;&#25110;&#21253;&#21547;&#22122;&#38899;&#65292;&#21017;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#27491;&#30830;&#29983;&#25104;&#25163;&#21183;&#12290;&#20026;&#20102;&#33719;&#24471;&#31283;&#20581;&#21644;&#24191;&#20041;&#30340;&#32534;&#30721;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#39044;&#20808;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#36827;&#34892;&#20849;&#21516;&#35821;&#38899;&#25163;&#21183;&#29983;&#25104;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;&#22522;&#20110;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#32534;&#30721;&#22120;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#21253;&#21547;&#27599;&#31181;&#27169;&#24577;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25910;&#38598;&#30001;3D&#20851;&#33410;&#26059;&#36716;&#32452;&#25104;&#30340;&#20840;&#36523;&#25163;&#21183;&#20197;&#25913;&#21892;&#21487;&#35270;&#21270;&#65292;&#24182;&#23558;&#25163;&#21183;&#24212;&#29992;&#20110;&#21487;&#25193;&#23637;&#36523;&#20307;&#27169;&#22411;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#23454;&#39564;&#21644;&#20154;&#24037;&#35780;&#20272;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#32473;&#23450;&#25152;&#26377;&#36755;&#20837;&#27169;&#24577;&#20197;&#21450;&#32570;&#23569;&#25110;&#21253;&#21547;&#22024;&#26434;&#27169;&#24577;&#26102;&#65292;&#37117;&#21487;&#20197;&#29983;&#25104;&#36924;&#30495;&#30340;&#20849;&#21516;&#35821;&#38899;&#25163;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
When virtual agents interact with humans, gestures are crucial to delivering their intentions with speech. Previous multimodal co-speech gesture generation models required encoded features of all modalities to generate gestures. If some input modalities are removed or contain noise, the model may not generate the gestures properly. To acquire robust and generalized encodings, we propose a novel framework with a multimodal pre-trained encoder for co-speech gesture generation. In the proposed method, the multi-head-attention-based encoder is trained with self-supervised learning to contain the information on each modality. Moreover, we collect full-body gestures that consist of 3D joint rotations to improve visualization and apply gestures to the extensible body model. Through the series of experiments and human evaluation, the proposed method renders realistic co-speech gestures not only when all input modalities are given but also when the input modalities are missing or noisy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20351;&#29992;&#19977;&#39033;QP&#30340;DMC&#31639;&#27861;&#65292;&#21487;&#20197;&#27604;&#20256;&#32479;&#30340;&#20004;&#39033;DMC&#23454;&#29616;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15735</link><description>&lt;p&gt;
&#19977;&#39033;DMC&#30340;&#20998;&#26512;&#19982;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Analysis and tuning of a three-term DMC. (arXiv:2305.15735v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20351;&#29992;&#19977;&#39033;QP&#30340;DMC&#31639;&#27861;&#65292;&#21487;&#20197;&#27604;&#20256;&#32479;&#30340;&#20004;&#39033;DMC&#23454;&#29616;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22312;&#24037;&#19994;&#20013;&#20351;&#29992;&#21644;&#22312;&#25511;&#21046;&#23398;&#26415;&#20013;&#30740;&#31350;&#30340;MPC&#65288;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65289;&#31639;&#27861;&#20351;&#29992;&#20004;&#39033;QP&#65288;&#20108;&#27425;&#35268;&#21010;&#65289;&#65292;&#20854;&#20013;&#31532;&#19968;&#39033;&#26159;&#36755;&#20986;&#35823;&#24046;&#21152;&#26435;&#33539;&#25968;&#65292;&#31532;&#20108;&#39033;&#26159;&#36755;&#20837;&#22686;&#37327;&#21152;&#26435;&#33539;&#25968;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20351;&#29992;&#19977;&#39033;QP&#30340;DMC&#65288;&#21160;&#24577;&#30697;&#38453;&#25511;&#21046;&#65289;&#31639;&#27861;&#65292;&#20854;&#20013;&#31532;&#19977;&#39033;&#26159;&#36755;&#20986;&#22686;&#37327;&#30340;&#21152;&#26435;&#33539;&#25968;&#12290;&#22312;&#20998;&#26512;&#20013;&#65292;&#24314;&#31435;&#20102;&#19977;&#39033;DMC&#21644;&#20004;&#39033;DMC&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#22522;&#20110;&#27492;&#23548;&#20986;&#20102;&#38381;&#29615;&#21709;&#24212;&#26354;&#32447;&#12290;&#26681;&#25454;&#20998;&#26512;&#65292;&#20026;&#19977;&#39033;DMC&#24320;&#21457;&#20102;&#20004;&#31181;&#25511;&#21046;&#22120;&#35843;&#33410;&#31243;&#24207;&#65292;&#19968;&#31181;&#29992;&#20110;&#38381;&#29615;&#38454;&#36291;&#21709;&#24212;&#65292;&#19968;&#31181;&#29992;&#20110;&#25200;&#21160;&#38477;&#20302;&#12290;&#26368;&#21518;&#65292;&#35777;&#26126;&#19977;&#39033;DMC&#21487;&#20197;&#27604;&#20004;&#39033;DMC&#23454;&#29616;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;&#27169;&#25311;&#30740;&#31350;&#29992;&#20110;&#23637;&#31034;&#30740;&#31350;&#32467;&#26524;&#21644;&#35843;&#20248;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most MPC (Model Predictive Control) algorithms used in industries and studied in the control academia use a two-term QP (quadratic programming), where the first term is the weighted norm of the output errors, and the second term is that of the input increments. In this work, a DMC (Dynamic Matrix Control) algorithm that uses three-term QP is studied, where the third term is the weighted norm of the output increments. In the analysis, a relationship between the three-term DMC and the two-term DMC is established; based on that, the closed-loop response curves are derived. Based on the analysis, two controller tuning procedures are developed for the three-term DMC, one for closed-loop step response and one for disturbance reduction. Finally, it will be proven that the three-term DMC can achieve a higher performance and robustness than the two-term DMC can. Simulation studies are used to demonstrate the findings and the tuning methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#30693;&#35782;&#33976;&#39311;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#24182;&#24402;&#22240;&#20110;&#20174;&#32769;&#24072;&#27169;&#22411;&#20256;&#36882;&#32473;&#23398;&#29983;&#27169;&#22411;&#30340;&#31867;&#30456;&#20284;&#20449;&#24687;&#30340;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2305.15734</link><description>&lt;p&gt;
&#20851;&#20110;&#30693;&#35782;&#33976;&#39311;&#22312;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#19978;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On the Impact of Knowledge Distillation for Model Interpretability. (arXiv:2305.15734v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#30693;&#35782;&#33976;&#39311;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#24182;&#24402;&#22240;&#20110;&#20174;&#32769;&#24072;&#27169;&#22411;&#20256;&#36882;&#32473;&#23398;&#29983;&#27169;&#22411;&#30340;&#31867;&#30456;&#20284;&#20449;&#24687;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30693;&#35782;&#33976;&#39311;(KD)&#20026;&#20309;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#24050;&#34987;&#38416;&#26126;&#65292;&#20294;&#26159;&#24456;&#23569;&#26377;&#20154;&#30740;&#31350;&#38500;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#22806;KD&#30340;&#20854;&#20182;&#20248;&#28857;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35777;&#26126;KD&#19981;&#20165;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#36824;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#32593;&#32476;&#20998;&#35299;&#31639;&#27861;&#20013;&#26816;&#27979;&#30340;&#27010;&#24565;&#26816;&#27979;&#22120;&#25968;&#37327;&#23545;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#36827;&#34892;&#20102;&#23450;&#37327;&#27604;&#36739;&#12290;&#25105;&#20204;&#23558;&#21487;&#35299;&#37322;&#24615;&#30340;&#25552;&#39640;&#24402;&#22240;&#20110;&#20174;&#32769;&#24072;&#27169;&#22411;&#20256;&#36882;&#32473;&#23398;&#29983;&#27169;&#22411;&#30340;&#31867;&#30456;&#20284;&#20449;&#24687;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#36923;&#36753;&#22238;&#24402;&#33976;&#39311;&#30830;&#35748;&#20102;&#31867;&#30456;&#20284;&#20449;&#24687;&#30340;&#20256;&#36882;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#31867;&#30456;&#20284;&#20449;&#24687;&#22312;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#21253;&#25324;&#20854;&#23384;&#22312;&#25110;&#32570;&#22833;&#20197;&#21450;&#30456;&#20284;&#20449;&#24687;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21508;&#31181;&#23450;&#37327;&#21644;&#23450;&#24615;&#23454;&#39564;&#65292;&#24182;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#12289;&#19981;&#21516;&#30340;KD&#26041;&#27861;&#21644;&#19981;&#21516;&#30340;&#27169;&#22411;&#32467;&#26500;&#19978;&#26816;&#26597;&#20102;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several recent studies have elucidated why knowledge distillation (KD) improves model performance. However, few have researched the other advantages of KD in addition to its improving model performance. In this study, we have attempted to show that KD enhances the interpretability as well as the accuracy of models. We measured the number of concept detectors identified in network dissection for a quantitative comparison of model interpretability. We attributed the improvement in interpretability to the class-similarity information transferred from the teacher to student models. First, we confirmed the transfer of class-similarity information from the teacher to student model via logit distillation. Then, we analyzed how class-similarity information affects model interpretability in terms of its presence or absence and degree of similarity information. We conducted various quantitative and qualitative experiments and examined the results on different datasets, different KD methods, and 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MeLoDy&#30340;&#39640;&#25928;&#31070;&#32463;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#22312;&#23454;&#26102;&#12289;&#21333;CPU&#29615;&#22659;&#19979;&#29983;&#25104;&#21508;&#31181;&#39118;&#26684;&#21644;&#38271;&#24230;&#30340;&#22810;&#26679;&#21270;&#12289;&#39640;&#36136;&#37327;&#30340;&#38899;&#20048;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.15719</link><description>&lt;p&gt;
&#39640;&#25928;&#31070;&#32463;&#38899;&#20048;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Efficient Neural Music Generation. (arXiv:2305.15719v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15719
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MeLoDy&#30340;&#39640;&#25928;&#31070;&#32463;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#22312;&#23454;&#26102;&#12289;&#21333;CPU&#29615;&#22659;&#19979;&#29983;&#25104;&#21508;&#31181;&#39118;&#26684;&#21644;&#38271;&#24230;&#30340;&#22810;&#26679;&#21270;&#12289;&#39640;&#36136;&#37327;&#30340;&#38899;&#20048;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#20808;&#36827;&#30340;MusicLM&#65292;&#38899;&#20048;&#29983;&#25104;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;MusicLM&#30001;&#19977;&#20010;LM&#30340;&#23618;&#27425;&#32467;&#26500;&#32452;&#25104;&#65292;&#20998;&#21035;&#29992;&#20110;&#35821;&#20041;&#12289;&#31895;&#30053;&#22768;&#23398;&#21644;&#32454;&#33410;&#22768;&#23398;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;MusicLM&#36827;&#34892;&#37319;&#26679;&#38656;&#35201;&#36880;&#20010;&#22788;&#29702;&#36825;&#20123;LM&#20197;&#33719;&#24471;&#32454;&#31890;&#24230;&#30340;&#22768;&#23398;&#26631;&#35760;&#65292;&#36825;&#20351;&#24471;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#19981;&#36866;&#29992;&#20110;&#23454;&#26102;&#29983;&#25104;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MeLoDy&#65288;M&#20195;&#34920;&#38899;&#20048;&#65307;L&#20195;&#34920;LM&#65307;D&#20195;&#34920;&#25193;&#25955;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;LM&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#29983;&#25104;&#38899;&#20048;&#38899;&#39057;&#65292;&#21516;&#26102;&#22312;&#37319;&#26679;10&#31186;&#25110;30&#31186;&#38899;&#20048;&#26102;&#65292;&#20998;&#21035;&#20943;&#23569;&#20102;MusicLM&#20013;&#30340;95.7%&#25110;99.6%&#30340;&#21069;&#21521;&#20256;&#36882;&#12290;MeLoDy&#32487;&#25215;&#20102;MusicLM&#30340;&#35821;&#20041;&#24314;&#27169;&#30340;&#26368;&#39640;&#32423;&#21035;&#30340;LM&#65292;&#24182;&#24212;&#29992;&#20102;&#26032;&#39062;&#30340;&#21452;&#36335;&#24452;&#25193;&#25955;(DPD)&#27169;&#22411;&#21644;&#38899;&#39057;VAE-GAN&#26469;&#39640;&#25928;&#22320;&#23558;&#26465;&#20214;&#35821;&#20041;&#26631;&#35760;&#35299;&#30721;&#20026;&#27874;&#24418;&#12290;DPD&#26159;&#19968;&#31181;&#19981;&#23545;&#31216;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#27839;&#30528;&#30830;&#23450;&#24615;&#36335;&#24452;&#20256;&#25773;&#39640;&#32423;&#21035;&#35821;&#20041;&#20449;&#24687;&#65292;&#27839;&#30528;&#38543;&#26426;&#36335;&#24452;&#20256;&#25773;&#20302;&#32423;&#22768;&#23398;&#32454;&#33410;&#12290;&#32463;&#39564;&#19978;&#65292;MeLoDy&#21487;&#20197;&#22312;&#21333;&#20010;CPU&#19978;&#23454;&#26102;&#29983;&#25104;&#21508;&#31181;&#39118;&#26684;&#21644;&#38271;&#24230;&#30340;&#22810;&#26679;&#21270;&#12289;&#39640;&#36136;&#37327;&#30340;&#38899;&#20048;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in music generation has been remarkably advanced by the state-of-the-art MusicLM, which comprises a hierarchy of three LMs, respectively, for semantic, coarse acoustic, and fine acoustic modelings. Yet, sampling with the MusicLM requires processing through these LMs one by one to obtain the fine-grained acoustic tokens, making it computationally expensive and prohibitive for a real-time generation. Efficient music generation with a quality on par with MusicLM remains a significant challenge. In this paper, we present MeLoDy (M for music; L for LM; D for diffusion), an LM-guided diffusion model that generates music audios of state-of-the-art quality meanwhile reducing 95.7% or 99.6% forward passes in MusicLM, respectively, for sampling 10s or 30s music. MeLoDy inherits the highest-level LM from MusicLM for semantic modeling, and applies a novel dual-path diffusion (DPD) model and an audio VAE-GAN to efficiently decode the conditioning semantic tokens into waveform. DPD i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;DiffKD&#65292;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21435;&#38500;&#23398;&#29983;&#29305;&#24449;&#20013;&#30340;&#22122;&#22768;&#20449;&#24687;&#65292;&#25552;&#21462;&#28165;&#26224;&#30340;&#26377;&#20215;&#20540;&#20449;&#24687;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#33976;&#39311;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.15712</link><description>&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#20013;&#30340;&#30693;&#35782;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Knowledge Diffusion for Distillation. (arXiv:2305.15712v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;DiffKD&#65292;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21435;&#38500;&#23398;&#29983;&#29305;&#24449;&#20013;&#30340;&#22122;&#22768;&#20449;&#24687;&#65292;&#25552;&#21462;&#28165;&#26224;&#30340;&#26377;&#20215;&#20540;&#20449;&#24687;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#33976;&#39311;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#33976;&#39311;&#20013;&#65292;&#25945;&#24072;&#21644;&#23398;&#29983;&#20043;&#38388;&#30340;&#34920;&#24449;&#24046;&#36317;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#35805;&#39064;&#12290;&#20026;&#20102;&#20943;&#23569;&#36825;&#31181;&#24046;&#36317;&#24182;&#25552;&#39640;&#34920;&#29616;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#22797;&#26434;&#30340;&#35757;&#32451;&#26041;&#26696;&#12289;&#25439;&#22833;&#20989;&#25968;&#21644;&#29305;&#24449;&#23545;&#40784;&#65292;&#36825;&#20123;&#37117;&#26159;&#20219;&#21153;&#29305;&#23450;&#21644;&#29305;&#24449;&#29305;&#23450;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffKD&#30340;&#26032;&#22411;KD&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#26126;&#30830;&#21435;&#22122;&#21644;&#21305;&#37197;&#29305;&#24449;&#65292;&#26469;&#25670;&#33073;&#22122;&#22768;&#20449;&#24687;&#65292;&#25552;&#21462;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#36798;&#21040;&#26356;&#22909;&#30340;&#33976;&#39311;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#36825;&#26679;&#19968;&#20010;&#35266;&#23519;&#65306;&#23398;&#29983;&#30340;&#29305;&#24449;&#36890;&#24120;&#27604;&#25945;&#24072;&#30340;&#29305;&#24449;&#26356;&#22810;&#22122;&#22768;&#65292;&#22240;&#20026;&#23398;&#29983;&#27169;&#22411;&#30340;&#23481;&#37327;&#26356;&#23567;&#12290;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#25945;&#24072;&#29305;&#24449;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#23545;&#23398;&#29983;&#29305;&#24449;&#36827;&#34892;&#21435;&#22122;&#30340;&#26041;&#27861;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#32454;&#21270;&#30340;&#28165;&#27905;&#29305;&#24449;&#21644;&#25945;&#24072;&#29305;&#24449;&#20043;&#38388;&#36827;&#34892;&#26356;&#22909;&#30340;&#33976;&#39311;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#32447;&#24615;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The representation gap between teacher and student is an emerging topic in knowledge distillation (KD). To reduce the gap and improve the performance, current methods often resort to complicated training schemes, loss functions, and feature alignments, which are task-specific and feature-specific. In this paper, we state that the essence of these methods is to discard the noisy information and distill the valuable information in the feature, and propose a novel KD method dubbed DiffKD, to explicitly denoise and match features using diffusion models. Our approach is based on the observation that student features typically contain more noises than teacher features due to the smaller capacity of student model. To address this, we propose to denoise student features using a diffusion model trained by teacher features. This allows us to perform better distillation between the refined clean feature and teacher feature. Additionally, we introduce a light-weight diffusion model with a linear a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23567;&#25439;&#22833;&#36793;&#30028;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20998;&#24067;&#24335;RL&#22909;&#22788;&#30340;&#19968;&#20010;&#35299;&#37322;&#65292;&#35813;&#36793;&#30028;&#19982;&#23454;&#20363;&#30456;&#20851;&#30340;&#26368;&#20248;&#25104;&#26412;&#25104;&#27604;&#20363;&#12290;&#22914;&#26524;&#26368;&#20248;&#25104;&#26412;&#24456;&#23567;&#65292;&#20998;&#24067;&#24335;&#26041;&#27861;&#20248;&#20110;&#38750;&#20998;&#24067;&#24335;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.15703</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#30340;&#22909;&#22788;&#65306;&#23567;&#25439;&#22833;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
The Benefits of Being Distributional: Small-Loss Bounds for Reinforcement Learning. (arXiv:2305.15703v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15703
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23567;&#25439;&#22833;&#36793;&#30028;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20998;&#24067;&#24335;RL&#22909;&#22788;&#30340;&#19968;&#20010;&#35299;&#37322;&#65292;&#35813;&#36793;&#30028;&#19982;&#23454;&#20363;&#30456;&#20851;&#30340;&#26368;&#20248;&#25104;&#26412;&#25104;&#27604;&#20363;&#12290;&#22914;&#26524;&#26368;&#20248;&#25104;&#26412;&#24456;&#23567;&#65292;&#20998;&#24067;&#24335;&#26041;&#27861;&#20248;&#20110;&#38750;&#20998;&#24067;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#21462;&#24471;&#20102;&#23454;&#35777;&#25104;&#26524;&#65292;&#20294;&#20854;&#20309;&#26102;&#20309;&#22320;&#26377;&#30410;&#30340;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#22238;&#31572;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#36890;&#36807;&#23567;&#25439;&#22833;&#36793;&#30028;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20998;&#24067;&#24335;RL&#22909;&#22788;&#30340;&#19968;&#20010;&#35299;&#37322;&#65292;&#35813;&#36793;&#30028;&#19982;&#23454;&#20363;&#30456;&#20851;&#30340;&#26368;&#20248;&#25104;&#26412;&#25104;&#27604;&#20363;&#12290;&#22914;&#26524;&#26368;&#20248;&#25104;&#26412;&#24456;&#23567;&#65292;&#25105;&#20204;&#30340;&#36793;&#30028;&#20250;&#27604;&#38750;&#20998;&#24067;&#24335;&#26041;&#27861;&#26356;&#24378;&#12290;&#20316;&#20026;&#28909;&#36523;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23398;&#20064;&#25104;&#26412;&#20998;&#24067;&#20250;&#22312;&#24773;&#22659;&#23637;&#24320;&#65288;CB&#65289;&#20013;&#23548;&#33268;&#23567;&#25439;&#22833;&#21518;&#24724;&#36793;&#30028;&#65292;&#25105;&#20204;&#21457;&#29616;&#20998;&#24067;&#24335;CB&#22312;&#19977;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#27604;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#22312;&#23454;&#35777;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;&#23545;&#20110;&#22312;&#32447;RL&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#29256;&#26412;&#31354;&#38388;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#22312;&#34920;&#26684;MDP&#20013;&#23454;&#29616;&#20102;&#23567;&#25439;&#22833;&#21518;&#24724;&#65292;&#21516;&#26102;&#22312;&#28508;&#21464;&#37327;&#27169;&#22411;&#20013;&#20139;&#26377;&#23567;&#25439;&#22833;PAC&#36793;&#30028;&#12290;&#20197;&#31867;&#20284;&#30340;&#35265;&#35299;&#20026;&#22522;&#30784;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#31163;&#32447;RL&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
While distributional reinforcement learning (RL) has demonstrated empirical success, the question of when and why it is beneficial has remained unanswered. In this work, we provide one explanation for the benefits of distributional RL through the lens of small-loss bounds, which scale with the instance-dependent optimal cost. If the optimal cost is small, our bounds are stronger than those from non-distributional approaches. As warmup, we show that learning the cost distribution leads to small-loss regret bounds in contextual bandits (CB), and we find that distributional CB empirically outperforms the state-of-the-art on three challenging tasks. For online RL, we propose a distributional version-space algorithm that constructs confidence sets using maximum likelihood estimation, and we prove that it achieves small-loss regret in the tabular MDPs and enjoys small-loss PAC bounds in latent variable models. Building on similar insights, we propose a distributional offline RL algorithm bas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#27979;&#35797;&#30340;&#20845;&#20010;&#20855;&#20307;&#25351;&#26631;&#26469;&#26816;&#27979;&#21644;&#32531;&#35299;&#20854;&#28431;&#27934;&#65292;&#35748;&#20026;&#35768;&#22810;DNN&#27979;&#35797;&#20219;&#21153;&#24212;&#35813;&#26159;&#23450;&#21521;&#27979;&#35797;&#38382;&#39064;&#32780;&#19981;&#26159;&#36890;&#29992;&#27979;&#35797;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.15698</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#28145;&#24230;&#23398;&#20064;&#27979;&#35797;&#20013;&#30340;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rethink Diversity in Deep Learning Testing. (arXiv:2305.15698v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#27979;&#35797;&#30340;&#20845;&#20010;&#20855;&#20307;&#25351;&#26631;&#26469;&#26816;&#27979;&#21644;&#32531;&#35299;&#20854;&#28431;&#27934;&#65292;&#35748;&#20026;&#35768;&#22810;DNN&#27979;&#35797;&#20219;&#21153;&#24212;&#35813;&#26159;&#23450;&#21521;&#27979;&#35797;&#38382;&#39064;&#32780;&#19981;&#26159;&#36890;&#29992;&#27979;&#35797;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#23637;&#29616;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#65292;&#26159;&#29616;&#20195;&#36719;&#20214;&#31995;&#32479;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20063;&#38754;&#20020;&#30528;&#21508;&#31181;&#28431;&#27934;&#65292;&#22914;&#23545;&#25239;&#25915;&#20987;&#21644;&#19981;&#20844;&#24179;&#24615;&#12290;&#22240;&#27492;&#65292;&#27979;&#35797;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#31995;&#32479;&#26159;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#65292;&#20197;&#26816;&#27979;&#21644;&#20943;&#36731;&#36825;&#20123;&#28431;&#27934;&#12290;&#21463;&#20256;&#32479;&#36719;&#20214;&#27979;&#35797;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;DNN&#30340;&#22810;&#26679;&#24615;&#24230;&#37327;&#65292;&#20197;&#24110;&#21161;&#26377;&#25928;&#22320;&#26292;&#38706;DNNs&#30340;buggy&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#35768;&#22810;DNN&#27979;&#35797;&#20219;&#21153;&#24212;&#35813;&#34987;&#35270;&#20026;&#23450;&#21521;&#27979;&#35797;&#38382;&#39064;&#32780;&#19981;&#26159;&#36890;&#29992;&#27979;&#35797;&#20219;&#21153;&#65292;&#22240;&#20026;&#36825;&#20123;&#20219;&#21153;&#26159;&#20855;&#20307;&#21644;&#26126;&#30830;&#23450;&#20041;&#30340;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#25928;&#26524;&#36739;&#24046;&#12290;&#36981;&#24490;&#25105;&#20204;&#30340;&#27979;&#35797;&#30446;&#26631;&#21644;DNN&#35821;&#20041;&#30340;&#35770;&#35777;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;6&#20010;&#25351;&#26631;&#65292;&#21487;&#20197;&#29992;&#20110;DNN&#27979;&#35797;&#65292;&#24182;&#20180;&#32454;&#20998;&#26512;&#23427;&#20204;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;&#25105;&#20204;&#36890;&#36807;&#32463;&#39564;&#24615;&#32467;&#26524;&#26174;&#31034;&#23427;&#20204;&#30340;&#24212;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have demonstrated extraordinary capabilities and are an integral part of modern software systems. However, they also suffer from various vulnerabilities such as adversarial attacks and unfairness. Testing deep learning (DL) systems is therefore an important task, to detect and mitigate those vulnerabilities. Motivated by the success of traditional software testing, which often employs diversity heuristics, various diversity measures on DNNs have been proposed to help efficiently expose the buggy behavior of DNNs. In this work, we argue that many DNN testing tasks should be treated as directed testing problems rather than general-purpose testing tasks, because these tasks are specific and well-defined. Hence, the diversity-based approach is less effective.  Following our argument based on the semantics of DNNs and the testing goal, we derive $6$ metrics that can be used for DNN testing and carefully analyze their application scopes. We empirically show their 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#8212;&#8212;&#38544;&#31169;&#21487;&#20445;&#25252;&#24230;&#65292;&#29992;&#20110;&#34913;&#37327;&#22312;&#32473;&#23450;&#26576;&#31181;&#35270;&#39057;&#20998;&#26512;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#65292;&#35270;&#39057;&#27969;&#21487;&#20197;&#24471;&#21040;&#22810;&#22823;&#31243;&#24230;&#30340;&#20445;&#25252;&#65292;&#24182;&#20351;&#29992;&#20449;&#24687;&#29702;&#35770;&#24037;&#20855;&#23545;&#20854;&#36827;&#34892;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.15697</link><description>&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#65306;&#19968;&#31181;&#20449;&#24687;&#29702;&#35770;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Privacy Protectability: An Information-theoretical Approach. (arXiv:2305.15697v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15697
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#8212;&#8212;&#38544;&#31169;&#21487;&#20445;&#25252;&#24230;&#65292;&#29992;&#20110;&#34913;&#37327;&#22312;&#32473;&#23450;&#26576;&#31181;&#35270;&#39057;&#20998;&#26512;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#65292;&#35270;&#39057;&#27969;&#21487;&#20197;&#24471;&#21040;&#22810;&#22823;&#31243;&#24230;&#30340;&#20445;&#25252;&#65292;&#24182;&#20351;&#29992;&#20449;&#24687;&#29702;&#35770;&#24037;&#20855;&#23545;&#20854;&#36827;&#34892;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25512;&#29702;&#38544;&#31169;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#25512;&#29702;&#38544;&#31169;&#38382;&#39064;&#26368;&#26126;&#26174;&#22320;&#20986;&#29616;&#22312;&#24191;&#27867;&#37096;&#32626;&#30340;&#36793;&#32536;-&#20113;&#35270;&#39057;&#20998;&#26512;&#31995;&#32479;&#20013;&#65292;&#20854;&#20013;&#20113;&#38656;&#35201;&#20174;&#36793;&#32536;&#25429;&#33719;&#35270;&#39057;&#12290;&#35270;&#39057;&#25968;&#25454;&#21487;&#33021;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#65292;&#24182;&#19988;&#22312;&#20256;&#36755;&#21040;&#20113;&#36827;&#34892;&#25512;&#29702;&#26102;&#20250;&#36973;&#21463;&#25915;&#20987;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#38544;&#31169;&#20445;&#25252;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#26041;&#26696;&#30340;&#24615;&#33021;&#38656;&#35201;&#36890;&#36807;&#23454;&#39564;&#30830;&#23450;&#25110;&#36890;&#36807;&#20998;&#26512;&#29305;&#23450;&#26696;&#20363;&#36827;&#34892;&#25512;&#26029;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#31216;&#20026;&#8220;&#38544;&#31169;&#21487;&#20445;&#25252;&#24230;&#8221;&#65292;&#29992;&#20110;&#34920;&#24449;&#22312;&#32473;&#23450;&#26576;&#31181;&#35270;&#39057;&#20998;&#26512;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#65292;&#35270;&#39057;&#27969;&#21487;&#20197;&#24471;&#21040;&#22810;&#22823;&#31243;&#24230;&#30340;&#20445;&#25252;&#12290;&#36825;&#31181;&#24230;&#37327;&#26631;&#20934;&#20855;&#26377;&#24378;&#30340;&#25805;&#20316;&#24847;&#20041;&#12290;&#20363;&#22914;&#65292;&#20302;&#21487;&#20445;&#25252;&#24615;&#24847;&#21619;&#30528;&#21487;&#33021;&#38656;&#35201;&#24314;&#31435;&#19968;&#20010;&#25972;&#20307;&#23433;&#20840;&#30340;&#29615;&#22659;&#12290;&#25105;&#20204;&#36824;&#21487;&#20197;&#35780;&#20272;&#38544;&#31169;&#20445;&#25252;&#26041;&#26696;&#65292;&#20363;&#22914;&#65292;&#20551;&#35774;&#23427;&#28151;&#28102;&#20102;&#35270;&#39057;&#25968;&#25454;&#65292;&#21017;&#22312;&#28151;&#28102;&#25968;&#25454;&#21518;&#35813;&#26041;&#26696;&#23454;&#29616;&#20102;&#22810;&#22823;&#31243;&#24230;&#30340;&#20445;&#25252;&#12290;&#25105;&#20204;&#20351;&#29992;&#20449;&#24687;&#29702;&#35770;&#24037;&#20855;&#24341;&#20837;&#20102;&#23450;&#20041;&#24182;&#37327;&#21270;&#20102;&#38544;&#31169;&#20445;&#25252;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#26816;&#26597;&#19968;&#20123;&#20856;&#22411;&#30340;&#35270;&#39057;&#20998;&#26512;&#20219;&#21153;&#26469;&#35828;&#26126;&#25152;&#25552;&#20986;&#30340;&#24230;&#37327;&#26631;&#20934;&#30340;&#23454;&#38469;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, inference privacy has attracted increasing attention. The inference privacy concern arises most notably in the widely deployed edge-cloud video analytics systems, where the cloud needs the videos captured from the edge. The video data can contain sensitive information and subject to attack when they are transmitted to the cloud for inference. Many privacy protection schemes have been proposed. Yet, the performance of a scheme needs to be determined by experiments or inferred by analyzing the specific case. In this paper, we propose a new metric, \textit{privacy protectability}, to characterize to what degree a video stream can be protected given a certain video analytics task. Such a metric has strong operational meaning. For example, low protectability means that it may be necessary to set up an overall secure environment. We can also evaluate a privacy protection scheme, e.g., assume it obfuscates the video data, what level of protection this scheme has achieved after obfus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#35270;&#39057;&#34892;&#20026;&#35782;&#21035;&#20013;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#12290;&#19982;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20248;&#21183;&#26174;&#33879;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15692</link><description>&lt;p&gt;
&#35270;&#39057;&#20154;&#20307;&#21160;&#20316;&#35782;&#21035;&#20013;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks in Video Human Action Recognition: A Review. (arXiv:2305.15692v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#35270;&#39057;&#34892;&#20026;&#35782;&#21035;&#20013;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#12290;&#19982;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20248;&#21183;&#26174;&#33879;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#35270;&#39057;&#34892;&#20026;&#35782;&#21035;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#26368;&#22522;&#30784;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#20108;&#32500;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#35782;&#21035;&#20687;RGB&#12289;RGB-D&#25110;&#20809;&#27969;&#26684;&#24335;&#36825;&#26679;&#30340;&#20687;&#32032;&#32423;&#20449;&#24687;&#30340;&#22270;&#20687;&#65292;&#38543;&#30528;&#30417;&#25511;&#35270;&#39057;&#30340;&#26085;&#30410;&#26222;&#21450;&#21644;&#26356;&#22810;&#19982;&#20154;&#20307;&#21160;&#20316;&#35782;&#21035;&#30456;&#20851;&#30340;&#20219;&#21153;&#30340;&#20986;&#29616;&#65292;&#38656;&#35201;&#32771;&#34385;&#24103;&#38388;&#20449;&#24687;&#20998;&#26512;&#12290;&#27604;&#36215;&#20687;&#32032;&#32423;&#20449;&#24687;&#65292;&#30740;&#31350;&#20154;&#21592;&#26356;&#22810;&#22320;&#30740;&#31350;&#22522;&#20110;&#35270;&#39057;&#30340;&#35782;&#21035;&#65292;&#20174;&#20960;&#20309;&#20219;&#21153;&#20013;&#25552;&#21462;&#26356;&#22810;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#20010;&#26032;&#39062;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#23545;&#36825;&#20123;&#27966;&#29983;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32780;&#19981;&#26159;&#19982;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#36827;&#34892;&#27604;&#36739;&#12290;&#27604;&#36739;&#21457;&#29983;&#22312;&#29616;&#26377;&#26694;&#26550;&#21644;&#20165;&#20026;&#35270;&#39057;&#26684;&#24335;&#30340;&#25968;&#25454;&#38598;&#20043;&#38388;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#20154;&#20307;&#21160;&#20316;&#20855;&#26377;&#29305;&#27530;&#24615;&#36136;&#65292;&#20854;&#20351;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Currently, video behavior recognition is one of the most foundational tasks of computer vision. The 2D neural networks of deep learning are built for recognizing pixel-level information such as images with RGB, RGB-D, or optical flow formats, with the current increasingly wide usage of surveillance video and more tasks related to human action recognition. There are increasing tasks requiring temporal information for frames dependency analysis. The researchers have widely studied video-based recognition rather than image-based(pixel-based) only to extract more informative elements from geometry tasks. Our current related research addresses multiple novel proposed research works and compares their advantages and disadvantages between the derived deep learning frameworks rather than machine learning frameworks. The comparison happened between existing frameworks and datasets, which are video format data only. Due to the specific properties of human actions and the increasingly wide usage 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#26041;&#27861;&#65292;&#33258;&#21160;&#29983;&#25104;&#22810;&#20010;&#31867;&#20284;&#20110;&#22522;&#30784;&#25552;&#31034;&#30340;&#39640;&#36136;&#37327;&#25552;&#31034;&#65292;&#24182;&#20351;&#29992;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#36827;&#34892;&#25490;&#21517;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#25552;&#31034;&#30340;&#25200;&#21160;&#25935;&#24863;&#24615;&#65292;&#24182;&#22312;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#20013;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15689</link><description>&lt;p&gt;
&#20811;&#26381;&#25552;&#31034;&#25200;&#21160;&#25935;&#24863;&#24615;&#30340;&#38646;&#26679;&#26412;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Approach to Overcome Perturbation Sensitivity of Prompts. (arXiv:2305.15689v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#26041;&#27861;&#65292;&#33258;&#21160;&#29983;&#25104;&#22810;&#20010;&#31867;&#20284;&#20110;&#22522;&#30784;&#25552;&#31034;&#30340;&#39640;&#36136;&#37327;&#25552;&#31034;&#65292;&#24182;&#20351;&#29992;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#36827;&#34892;&#25490;&#21517;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#25552;&#31034;&#30340;&#25200;&#21160;&#25935;&#24863;&#24615;&#65292;&#24182;&#22312;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#20013;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#21487;&#20197;&#24110;&#21161;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#30340;&#30693;&#35782;&#36827;&#34892;&#20108;&#20803;&#21477;&#32423;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#35774;&#32622;&#65292;&#20351;&#29992;&#25163;&#21160;&#25110;&#33258;&#21160;&#29983;&#25104;&#30340;&#25552;&#31034;&#26469;&#24494;&#35843;&#24773;&#24863;&#20998;&#31867;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#23545;&#25152;&#20351;&#29992;&#25552;&#31034;&#30340;&#25200;&#21160;&#25935;&#24863;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#23569;&#37327;&#24102;&#26631;&#31614;&#23454;&#20363;&#36827;&#34892;&#33258;&#21160;&#25552;&#31034;&#29983;&#25104;&#21644;&#25552;&#31034;&#25490;&#24207;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#20013;&#20026;&#25152;&#32473;&#23450;&#30340;&#20219;&#21153;&#25214;&#21040;&#39640;&#36136;&#37327;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#26041;&#27861;&#32473;&#23450;&#19968;&#20010;&#22522;&#30784;&#25552;&#31034;&#65292;&#37319;&#29992;&#20301;&#32622;&#12289;&#25512;&#29702;&#21644;&#37322;&#20041;&#25216;&#26415;&#33258;&#21160;&#29983;&#25104;&#22810;&#20010;&#31867;&#20284;&#20110;&#22522;&#30784;&#25552;&#31034;&#30340;&#25552;&#31034;&#65292;&#28982;&#21518;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#23545;&#25552;&#31034;&#36827;&#34892;&#25490;&#21517;&#12290;&#25105;&#20204;&#20174;&#23454;&#39564;&#19978;&#35777;&#26126;&#65292;&#25490;&#21517;&#38752;&#21069;&#30340;&#25552;&#31034;&#20855;&#26377;&#24456;&#39640;&#30340;&#36136;&#37327;&#65292;&#24182;&#22312;&#25552;&#31034;&#25200;&#21160;&#40065;&#26834;&#24615;&#21644;&#25972;&#20307;&#20934;&#30830;&#24615;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#22522;&#30784;&#25552;&#31034;&#21644;&#20854;&#20182;&#29616;&#26377;&#30340;&#25552;&#31034;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have demonstrated that natural-language prompts can help to leverage the knowledge learned by pre-trained language models for the binary sentence-level sentiment classification task. Specifically, these methods utilize few-shot learning settings to fine-tune the sentiment classification model using manual or automatically generated prompts. However, the performance of these methods is sensitive to the perturbations of the utilized prompts. Furthermore, these methods depend on a few labeled instances for automatic prompt generation and prompt ranking. This study aims to find high-quality prompts for the given task in a zero-shot setting. Given a base prompt, our proposed approach automatically generates multiple prompts similar to the base prompt employing positional, reasoning, and paraphrasing techniques and then ranks the prompts using a novel metric. We empirically demonstrate that the top-ranked prompts are high-quality and significantly outperform the base prompt an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; RewriteLM&#65292;&#19968;&#31181;&#25351;&#20196;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#38271;&#31687;&#25991;&#26412;&#37325;&#20889;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; OpenRewriteEval &#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#21508;&#31181;&#31867;&#22411;&#30340;&#24320;&#25918;&#24335;&#38271;&#31687;&#25991;&#26412;&#37325;&#20889;&#12290;&#25105;&#20204;&#37319;&#29992;&#26032;&#30340;&#31574;&#30053;&#26469;&#20419;&#36827;&#22810;&#26679;&#30340;&#25351;&#20196;&#21644;&#20559;&#22909;&#25968;&#25454;&#29983;&#25104;&#65292;&#20174;&#32780;&#20026;&#38271;&#31687;&#25991;&#26412;&#37325;&#20889;&#25552;&#20379;&#26356;&#22909;&#30340;&#35780;&#20272;&#25163;&#27573;&#12290;</title><link>http://arxiv.org/abs/2305.15685</link><description>&lt;p&gt;
RewriteLM&#65306;&#19968;&#31181;&#38754;&#21521;&#25991;&#26412;&#37325;&#20889;&#30340;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
RewriteLM: An Instruction-Tuned Large Language Model for Text Rewriting. (arXiv:2305.15685v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; RewriteLM&#65292;&#19968;&#31181;&#25351;&#20196;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#38271;&#31687;&#25991;&#26412;&#37325;&#20889;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; OpenRewriteEval &#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#21508;&#31181;&#31867;&#22411;&#30340;&#24320;&#25918;&#24335;&#38271;&#31687;&#25991;&#26412;&#37325;&#20889;&#12290;&#25105;&#20204;&#37319;&#29992;&#26032;&#30340;&#31574;&#30053;&#26469;&#20419;&#36827;&#22810;&#26679;&#30340;&#25351;&#20196;&#21644;&#20559;&#22909;&#25968;&#25454;&#29983;&#25104;&#65292;&#20174;&#32780;&#20026;&#38271;&#31687;&#25991;&#26412;&#37325;&#20889;&#25552;&#20379;&#26356;&#22909;&#30340;&#35780;&#20272;&#25163;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#29616;&#20986;&#22312;&#38271;&#31687;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#34920;&#36798;&#26469;&#30340;&#24778;&#20154;&#30340;&#38646;-shot&#33021;&#21147;&#65292;&#28982;&#32780;&#29992;&#25143;&#23545;&#20110;&#38271;&#31687;&#25991;&#26412;&#37325;&#20889;&#30340;&#26399;&#26395;&#20540;&#24456;&#39640;&#65292;&#27169;&#22411;&#20135;&#29983;&#30340;&#24847;&#22806;&#37325;&#20889;&#65288;&#8220;&#24187;&#35273;&#8221;&#65289;&#20250;&#23545;&#20854;&#25972;&#20307;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#35780;&#20272;&#22522;&#20934;&#20027;&#35201;&#20851;&#27880;&#26377;&#38480;&#30340;&#37325;&#20889;&#39118;&#26684;&#21644;&#21477;&#23376;&#32423;&#37325;&#20889;&#65292;&#32780;&#19981;&#26159;&#38271;&#31687;&#24320;&#25918;&#24335;&#37325;&#20889;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;OpenRewriteEval&#65292;&#23427;&#28085;&#30422;&#20102;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#34920;&#36798;&#30340;&#21508;&#31181;&#37325;&#20889;&#31867;&#22411;&#12290;&#23427;&#29305;&#21035;&#35774;&#35745;&#29992;&#20110;&#20419;&#36827;&#38271;&#31687;&#25991;&#26412;&#24320;&#25918;&#24335;&#37325;&#20889;&#30340;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#22522;&#32447;&#27169;&#22411;RewriteLM&#65292;&#19968;&#20010;&#29992;&#20110;&#38271;&#31687;&#25991;&#26412;&#37325;&#20889;&#30340;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20123;&#26032;&#31574;&#30053;&#65292;&#20197;&#26368;&#23567;&#20154;&#24037;&#24178;&#39044;&#20419;&#36827;&#29983;&#25104;&#22810;&#26679;&#30340;&#25351;&#20196;&#21644;&#20559;&#22909;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated impressive zero-shot capabilities in long-form text generation tasks expressed through natural language instructions. However, user expectations for long-form text rewriting is high, and unintended rewrites (''hallucinations'') produced by the model can negatively impact its overall performance. Existing evaluation benchmarks primarily focus on limited rewriting styles and sentence-level rewriting rather than long-form open-ended rewriting.We introduce OpenRewriteEval, a novel benchmark that covers a wide variety of rewriting types expressed through natural language instructions. It is specifically designed to facilitate the evaluation of open-ended rewriting of long-form texts. In addition, we propose a strong baseline model, RewriteLM, an instruction-tuned large language model for long-form text rewriting. We develop new strategies that facilitate the generation of diverse instructions and preference data with minimal human intervention.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;MultiSim&#22522;&#20934;&#65292;&#23427;&#21253;&#21547;&#20102;27&#20010;&#36164;&#28304;&#12289;12&#31181;&#35821;&#35328;&#36229;&#36807;1.7&#30334;&#19975;&#20010;&#22797;&#26434;-&#31616;&#21333;&#30340;&#21477;&#23376;&#23545;&#12290;&#20351;&#29992;&#35813;&#22522;&#20934;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#38750;&#33521;&#35821;&#29615;&#22659;&#20013;&#24102;&#26469;&#20196;&#20154;&#20852;&#22859;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#19988;&#20420;&#35821;&#22312;&#36328;&#35821;&#35328;&#36716;&#31227;&#26041;&#38754;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15678</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#38750;&#33521;&#35821;&#25991;&#26412;&#31616;&#21270;&#65306;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#35821;&#35328;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Revisiting non-English Text Simplification: A Unified Multilingual Benchmark. (arXiv:2305.15678v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15678
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;MultiSim&#22522;&#20934;&#65292;&#23427;&#21253;&#21547;&#20102;27&#20010;&#36164;&#28304;&#12289;12&#31181;&#35821;&#35328;&#36229;&#36807;1.7&#30334;&#19975;&#20010;&#22797;&#26434;-&#31616;&#21333;&#30340;&#21477;&#23376;&#23545;&#12290;&#20351;&#29992;&#35813;&#22522;&#20934;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#38750;&#33521;&#35821;&#29615;&#22659;&#20013;&#24102;&#26469;&#20196;&#20154;&#20852;&#22859;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#19988;&#20420;&#35821;&#22312;&#36328;&#35821;&#35328;&#36716;&#31227;&#26041;&#38754;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33521;&#35821;&#33258;&#21160;&#25991;&#26412;&#31616;&#21270;&#65288;ATS&#65289;&#30740;&#31350;&#20013;&#39640;&#36136;&#37327;&#12289;&#22823;&#35268;&#27169;&#30340;&#33521;&#35821;&#36164;&#28304;&#30340;&#36827;&#23637;&#23558;&#33521;&#35821;ATS&#30740;&#31350;&#30340;&#21069;&#27839;&#25512;&#21521;&#20102;&#26356;&#39640;&#30340;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#19968;&#20010;&#35206;&#30422;&#22810;&#31181;&#35821;&#35328;&#20013;&#30340;&#22797;&#26434;-&#31616;&#27905;&#21477;&#23376;&#23545;&#30340;&#22810;&#26679;&#21270;&#35780;&#20272;&#22522;&#20934;&#65292;&#23545;&#22810;&#35821;&#35328;&#25991;&#26412;&#31616;&#21270;&#30340;&#30740;&#31350;&#24037;&#20316;&#36739;&#23569;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MultiSim&#22522;&#20934;&#65292;&#36825;&#26159;&#19968;&#20010;&#25910;&#38598;&#20102;12&#31181;&#19981;&#21516;&#35821;&#35328;&#20013;&#36229;&#36807;1.7&#30334;&#19975;&#20010;&#22797;&#26434;-&#31616;&#21333;&#21477;&#23376;&#23545;&#30340;27&#20010;&#36164;&#28304;&#30340;&#38598;&#21512;&#12290;&#36825;&#20010;&#22522;&#20934;&#23558;&#40723;&#21169;&#30740;&#31350;&#24320;&#21457;&#26356;&#26377;&#25928;&#30340;&#22810;&#35821;&#35328;&#25991;&#26412;&#31616;&#21270;&#27169;&#22411;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#20351;&#29992;MultiSim&#19982;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#22312;&#38750;&#33521;&#35821;&#29615;&#22659;&#20013;&#36827;&#34892;&#22810;&#35821;&#35328;&#35757;&#32451;&#21487;&#20197;&#24102;&#26469;&#20196;&#20154;&#20852;&#22859;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20420;&#35821;&#22312;&#38646;-shot&#36328;&#35821;&#35328;&#36716;&#31227;&#21040;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#24378;&#22823;&#34920;&#29616;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20351;&#29992;BLOOM-176b&#30340;&#23569;&#37327;&#25552;&#31034;&#21487;&#20197;&#36798;&#21040;&#21487;&#27604;&#30340;&#21442;&#32771;&#31616;&#21270;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in high-quality, large-scale English resources have pushed the frontier of English Automatic Text Simplification (ATS) research. However, less work has been done on multilingual text simplification due to the lack of a diverse evaluation benchmark that covers complex-simple sentence pairs in many languages. This paper introduces the MultiSim benchmark, a collection of 27 resources in 12 distinct languages containing over 1.7 million complex-simple sentence pairs. This benchmark will encourage research in developing more effective multilingual text simplification models and evaluation metrics. Our experiments using MultiSim with pre-trained multilingual language models reveal exciting performance improvements from multilingual training in non-English settings. We observe strong performance from Russian in zero-shot cross-lingual transfer to low-resource languages. We further show that few-shot prompting with BLOOM-176b achieves comparable quality to reference simplif
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PROTO&#30340;&#26032;&#22411;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#28436;&#21270;&#30340;&#35268;&#33539;&#21270;&#39033;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#27425;&#20248;&#12289;&#36866;&#24212;&#24615;&#26377;&#38480;&#21644;&#35745;&#31639;&#25928;&#29575;&#19981;&#36275;&#31561;&#38382;&#39064;&#12290;PROTO&#21487;&#20197;&#26497;&#22823;&#22320;&#36866;&#24212;&#21508;&#31181;&#26041;&#27861;&#65292;&#19988;&#20165;&#38656;&#26368;&#23567;&#30340;&#39069;&#22806;&#35745;&#31639;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.15669</link><description>&lt;p&gt;
PROTO: &#36845;&#20195;&#31574;&#30053;&#35268;&#33539;&#21270;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PROTO: Iterative Policy Regularized Offline-to-Online Reinforcement Learning. (arXiv:2305.15669v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15669
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PROTO&#30340;&#26032;&#22411;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#28436;&#21270;&#30340;&#35268;&#33539;&#21270;&#39033;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#27425;&#20248;&#12289;&#36866;&#24212;&#24615;&#26377;&#38480;&#21644;&#35745;&#31639;&#25928;&#29575;&#19981;&#36275;&#31561;&#38382;&#39064;&#12290;PROTO&#21487;&#20197;&#26497;&#22823;&#22320;&#36866;&#24212;&#21508;&#31181;&#26041;&#27861;&#65292;&#19988;&#20165;&#38656;&#26368;&#23567;&#30340;&#39069;&#22806;&#35745;&#31639;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#32467;&#21512;&#31163;&#32447;&#39044;&#35757;&#32451;&#21644;&#22312;&#32447;&#24494;&#35843;&#30340;&#20248;&#28857;&#65292;&#25215;&#35834;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#31574;&#30053;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#34429;&#28982;&#26377;&#25928;&#65292;&#20294;&#23384;&#22312;&#24615;&#33021;&#27425;&#20248;&#12289;&#36866;&#24212;&#24615;&#26377;&#38480;&#21644;&#35745;&#31639;&#25928;&#29575;&#19981;&#36275;&#31561;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;PROTO&#65292;&#36890;&#36807;&#23558;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#30446;&#26631;&#19982;&#36845;&#20195;&#28436;&#21270;&#30340;&#35268;&#33539;&#21270;&#39033;&#30456;&#32467;&#21512;&#65292;&#20811;&#26381;&#20102;&#19978;&#36848;&#38480;&#21046;&#12290;PROTO&#36890;&#36807;&#25191;&#34892;&#20449;&#20219;&#21306;&#22495;&#26679;&#24335;&#26356;&#26032;&#65292;&#22312;&#28176;&#36827;&#25918;&#26494;&#32422;&#26463;&#24378;&#24230;&#30340;&#21516;&#26102;&#65292;&#20351;&#21021;&#22987;&#24494;&#35843;&#31283;&#23450;&#12289;&#26368;&#32456;&#24615;&#33021;&#26368;&#20248;&#12290;&#36890;&#36807;&#35843;&#25972;&#21482;&#26377;&#20960;&#34892;&#20195;&#30721;&#65292;PROTO&#21487;&#20197;&#23558;&#20219;&#20309;&#31163;&#32447;&#31574;&#30053;&#39044;&#35757;&#32451;&#21644;&#26631;&#20934;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#26725;&#25509;&#36215;&#26469;&#65292;&#20174;&#32780;&#20135;&#29983;&#23545;&#21508;&#31181;&#26041;&#27861;&#30340;&#26497;&#24378;&#36866;&#24212;&#24615;&#12290;PROTO&#31616;&#21333;&#32780;&#20248;&#38597;&#65292;&#20165;&#24378;&#21152;&#26368;&#23567;&#30340;&#39069;&#22806;&#35745;&#31639;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline-to-online reinforcement learning (RL), by combining the benefits of offline pretraining and online finetuning, promises enhanced sample efficiency and policy performance. However, existing methods, effective as they are, suffer from suboptimal performance, limited adaptability, and unsatisfactory computational efficiency. We propose a novel framework, PROTO, which overcomes the aforementioned limitations by augmenting the standard RL objective with an iteratively evolving regularization term. Performing a trust-region-style update, PROTO yields stable initial finetuning and optimal final performance by gradually evolving the regularization term to relax the constraint strength. By adjusting only a few lines of code, PROTO can bridge any offline policy pretraining and standard off-policy RL finetuning to form a powerful offline-to-online RL pathway, birthing great adaptability to diverse methods. Simple yet elegant, PROTO imposes minimal additional computation and enables highly
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#21457;&#29616;&#24179;&#22374;&#26497;&#23567;&#20540;&#28857;&#30340;&#31639;&#27861;&#38382;&#39064;&#65292;&#22312;&#25903;&#25345;&#25214;&#21040;&#23616;&#37096;&#36817;&#20284;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#28857;&#30340;&#22522;&#30784;&#19978;&#65292;&#35774;&#35745;&#20102;&#20004;&#31181;&#31639;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#65292;&#19968;&#31181;&#22522;&#20110;&#26368;&#23567;&#21270;&#38160;&#24230;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.15659</link><description>&lt;p&gt;
&#22914;&#20309;&#36867;&#31163;&#38160;&#21270;&#30340;&#26497;&#23567;&#20540;&#28857;
&lt;/p&gt;
&lt;p&gt;
How to escape sharp minima. (arXiv:2305.15659v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21457;&#29616;&#24179;&#22374;&#26497;&#23567;&#20540;&#28857;&#30340;&#31639;&#27861;&#38382;&#39064;&#65292;&#22312;&#25903;&#25345;&#25214;&#21040;&#23616;&#37096;&#36817;&#20284;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#28857;&#30340;&#22522;&#30784;&#19978;&#65292;&#35774;&#35745;&#20102;&#20004;&#31181;&#31639;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#65292;&#19968;&#31181;&#22522;&#20110;&#26368;&#23567;&#21270;&#38160;&#24230;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#20248;&#21270;&#31639;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#36825;&#20123;&#31639;&#27861;&#34987;&#35774;&#35745;&#29992;&#26469;&#21457;&#29616;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#31639;&#27861;&#38382;&#39064;&#65292;&#26412;&#25991;&#37319;&#29992;&#25439;&#22833;&#20989;&#25968;&#28023;&#26862;&#30697;&#38453;&#30340;&#36857;&#26469;&#24230;&#37327;&#23427;&#30340;&#24179;&#22374;&#31243;&#24230;&#65292;&#24182;&#24418;&#24335;&#21270;&#23450;&#20041;&#20102;&#36817;&#20284;&#24179;&#22374;&#26497;&#23567;&#20540;&#28857;&#30340;&#27010;&#24565;&#12290;&#22312;&#27492;&#27010;&#24565;&#19979;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26377;&#25928;&#22320;&#25214;&#21040;&#36817;&#20284;&#24179;&#22374;&#26497;&#23567;&#20540;&#28857;&#30340;&#31639;&#27861;&#12290;&#38024;&#23545;&#19968;&#33324;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25214;&#21040;&#36817;&#20284;&#24179;&#22374;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#28857;&#12290;&#31639;&#27861;&#30340;&#20027;&#35201;&#32452;&#20214;&#26159;&#20351;&#29992;&#20174;&#38543;&#26426;&#25200;&#21160;&#36845;&#20195;&#20013;&#35745;&#31639;&#30340;&#26799;&#24230;&#26469;&#20272;&#35745;&#23548;&#33268;&#26356;&#24179;&#22374;&#26497;&#23567;&#20540;&#28857;&#30340;&#26041;&#21521;&#12290;&#23545;&#20110;&#25104;&#26412;&#20989;&#25968;&#26159;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#32463;&#39564;&#39118;&#38505;&#30340;&#35774;&#32622;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#36895;&#30340;&#31639;&#27861;&#65292;&#21463;&#26368;&#36817;&#25552;&#20986;&#30340;&#23454;&#29992;&#31639;&#27861;&#8212;&#8212;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#30340;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern machine learning applications have seen a remarkable success of optimization algorithms that are designed to find flat minima. Motivated by this paradigm, this work formulates and studies the algorithmic question of how to find flat minima. As an initial effort, this work adopts the trace of hessian of the cost function as the measure of flatness, and formally defines the notion of approximate flat minima. Under this notion, we then design algorithms that find approximate flat minima efficiently. For general cost functions, we present a gradient-based algorithm that finds an approximate flat local minimum efficiently. The main component of the algorithm is to use gradients computed from randomly perturbed iterates to estimate a direction that leads to flatter minima. For the setting where the cost function is an empirical risk over training data, we present a faster algorithm that is inspired by a recently proposed practical algorithm called sharpness-aware minimization, support
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;LeMO&#30340;&#22312;&#32447;&#23398;&#20064;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#26412;&#26041;&#27861;&#20351;&#29992;&#23398;&#20064;&#24615;&#35760;&#24518;&#65292;&#21253;&#21547;&#20102;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#24322;&#24120;&#26816;&#27979;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#21516;&#26102;&#20248;&#21270;&#20869;&#23384;&#21644;&#22270;&#20687;&#29305;&#24449;&#65292;&#33021;&#22815;&#36866;&#24212;&#24037;&#19994;&#22312;&#32447;&#27969;&#25968;&#25454;&#38656;&#27714;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#31616;&#21333;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15652</link><description>&lt;p&gt;
&#21040;&#36798;&#24037;&#19994;&#35270;&#35273;&#20013;&#30340;&#24635;&#20307;&#22312;&#32447;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#19982;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Towards Total Online Unsupervised Anomaly Detection and Localization in Industrial Vision. (arXiv:2305.15652v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;LeMO&#30340;&#22312;&#32447;&#23398;&#20064;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#26412;&#26041;&#27861;&#20351;&#29992;&#23398;&#20064;&#24615;&#35760;&#24518;&#65292;&#21253;&#21547;&#20102;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#24322;&#24120;&#26816;&#27979;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#21516;&#26102;&#20248;&#21270;&#20869;&#23384;&#21644;&#22270;&#20687;&#29305;&#24449;&#65292;&#33021;&#22815;&#36866;&#24212;&#24037;&#19994;&#22312;&#32447;&#27969;&#25968;&#25454;&#38656;&#27714;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#31616;&#21333;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#29616;&#26377;&#30340;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#22823;&#22810;&#26159;&#38656;&#35201;&#36807;&#22810;&#39044;&#20808;&#25910;&#38598;&#25968;&#25454;&#30340;&#31163;&#32447;&#23398;&#20064;&#33539;&#24335;&#65292;&#22312;&#38656;&#35201;&#22312;&#32447;&#27969;&#25968;&#25454;&#30340;&#24037;&#19994;&#22330;&#26223;&#20013;&#38480;&#21046;&#20854;&#36866;&#24212;&#24615;&#12290;&#22522;&#20110;&#22312;&#32447;&#23398;&#20064;&#30340;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#26356;&#36866;&#29992;&#20110;&#24037;&#19994;&#22312;&#32447;&#27969;&#25968;&#25454;&#65292;&#20294;&#40092;&#20026;&#20154;&#30693;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#22312;&#32447;&#23398;&#20064;&#30340;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#21517;&#20026;LeMO&#65292;&#21363;&#20026;&#22312;&#32447;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#23398;&#20064;&#35760;&#24518;&#65292;&#20854;&#21033;&#29992;&#30340;&#26159;&#23398;&#20064;&#24615;&#35760;&#24518;&#65292;&#32463;&#21021;&#22987;&#21270;&#20026;&#27491;&#20132;&#38543;&#26426;&#22122;&#22768;&#65292;&#28040;&#38500;&#20102;&#20869;&#23384;&#21021;&#22987;&#21270;&#30340;&#36807;&#24230;&#25968;&#25454;&#38656;&#27714;&#21644;&#31163;&#32447;&#25968;&#25454;&#25910;&#38598;&#30340;&#20302;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23454;&#29616;&#22312;&#32447;&#20869;&#23384;&#21644;&#22270;&#20687;&#30446;&#26631;&#23548;&#21521;&#29305;&#24449;&#30340;&#32852;&#21512;&#20248;&#21270;&#65292;&#36824;&#35774;&#35745;&#20102;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#24322;&#24120;&#26816;&#27979;&#25439;&#22833;&#20989;&#25968;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#31616;&#21333;&#32780;&#39640;&#25928;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although existing image anomaly detection methods yield impressive results, they are mostly an offline learning paradigm that requires excessive data pre-collection, limiting their adaptability in industrial scenarios with online streaming data. Online learning-based image anomaly detection methods are more compatible with industrial online streaming data but are rarely noticed. For the first time, this paper presents a fully online learning image anomaly detection method, namely LeMO, learning memory for online image anomaly detection. LeMO leverages learnable memory initialized with orthogonal random noise, eliminating the need for excessive data in memory initialization and circumventing the inefficiencies of offline data collection. Moreover, a contrastive learning-based loss function for anomaly detection is designed to enable online joint optimization of memory and image target-oriented features. The presented method is simple and highly effective. Extensive experiments demonstra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23569;&#22495;&#36890;&#29992;&#21270;&#26694;&#26550;&#21644;&#20803;&#33258;&#36866;&#24212;&#20219;&#21153;&#37319;&#26679;&#65288;MATS&#65289;&#36807;&#31243;&#65292;&#26088;&#22312;&#21033;&#29992;&#26497;&#23569;&#37327;&#30340;&#26032;&#20219;&#21153;&#22495;&#26469;&#23398;&#20064;&#21487;&#25512;&#24191;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#22522;&#30784;&#20219;&#21153;&#19978;&#33719;&#24471;&#30693;&#35782;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#22495;&#22806;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15644</link><description>&lt;p&gt;
&#20803;&#33258;&#36866;&#24212;&#20219;&#21153;&#37319;&#26679;&#23454;&#29616;&#23569;&#22495;&#36890;&#29992;&#21270;
&lt;/p&gt;
&lt;p&gt;
Meta Adaptive Task Sampling for Few-Domain Generalization. (arXiv:2305.15644v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23569;&#22495;&#36890;&#29992;&#21270;&#26694;&#26550;&#21644;&#20803;&#33258;&#36866;&#24212;&#20219;&#21153;&#37319;&#26679;&#65288;MATS&#65289;&#36807;&#31243;&#65292;&#26088;&#22312;&#21033;&#29992;&#26497;&#23569;&#37327;&#30340;&#26032;&#20219;&#21153;&#22495;&#26469;&#23398;&#20064;&#21487;&#25512;&#24191;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#22522;&#30784;&#20219;&#21153;&#19978;&#33719;&#24471;&#30693;&#35782;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#22495;&#22806;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#30830;&#20445;&#27169;&#22411;&#20855;&#26377;&#36275;&#22815;&#30340;&#22495;&#22806;&#27867;&#21270;&#24615;&#33021;&#65292;&#20256;&#32479;&#30340;&#39046;&#22495;&#36890;&#29992;&#21270;&#26041;&#27861;&#24120;&#24120;&#38656;&#35201;&#22312;&#19981;&#21516;&#24213;&#23618;&#20998;&#24067;&#30340;&#22810;&#20010;&#28304;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#26377;&#22810;&#26679;&#21270;&#30340;&#35757;&#32451;&#20998;&#24067;&#12290;&#20294;&#30001;&#20110;&#39640;&#26114;&#30340;&#36153;&#29992;&#12289;&#38544;&#31169;&#38382;&#39064;&#25110;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#31561;&#21407;&#22240;&#65292;&#36890;&#24120;&#38656;&#35201;&#20184;&#20986;&#24040;&#22823;&#30340;&#21162;&#21147;&#25165;&#33021;&#33719;&#24471;&#36275;&#22815;&#30340;&#24322;&#26500;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#22312;&#24863;&#30693;&#21040;&#30340;&#24322;&#36136;&#24615;&#21463;&#38480;&#26102;&#65292;&#22914;&#20309;&#25552;&#39640;&#27169;&#22411;&#30340;&#22495;&#22806;&#27867;&#21270;&#24615;&#33021;&#26159;&#19968;&#20010;&#26377;&#36259;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#23569;&#22495;&#36890;&#29992;&#21270;&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#20174;&#26497;&#23569;&#37327;&#30340;&#26032;&#20219;&#21153;&#22495;&#20013;&#23398;&#20064;&#21487;&#25512;&#24191;&#30340;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#20808;&#21069;&#22312;&#22522;&#30784;&#20219;&#21153;&#19978;&#33719;&#24471;&#30340;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#33258;&#36866;&#24212;&#20219;&#21153;&#37319;&#26679;&#65288;MATS&#65289;&#36807;&#31243;&#65292;&#26681;&#25454;&#23427;&#20204;&#30340;&#35821;&#20041;&#21644;&#39046;&#22495;&#36716;&#31227;&#30456;&#24322;&#24615;&#26469;&#21306;&#20998;&#22522;&#30784;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
To ensure the out-of-distribution (OOD) generalization performance, traditional domain generalization (DG) methods resort to training on data from multiple sources with different underlying distributions. And the success of those DG methods largely depends on the fact that there are diverse training distributions. However, it usually needs great efforts to obtain enough heterogeneous data due to the high expenses, privacy issues or the scarcity of data. Thus an interesting yet seldom investigated problem arises: how to improve the OOD generalization performance when the perceived heterogeneity is limited. In this paper, we instantiate a new framework called few-domain generalization (FDG), which aims to learn a generalizable model from very few domains of novel tasks with the knowledge acquired from previous learning experiences on base tasks. Moreover, we propose a Meta Adaptive Task Sampling (MATS) procedure to differentiate base tasks according to their semantic and domain-shift sim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#33258;&#21160;&#36719;&#20214;&#20195;&#30721;&#21644;&#37197;&#32622;&#29983;&#25104;&#26041;&#27861;&#65292;&#20854;&#20013;&#36890;&#36807;&#36951;&#20256;&#31639;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#36866;&#24212;&#24230;&#20989;&#25968;&#33258;&#21160;&#21512;&#25104;&#31243;&#24207;&#65292;&#20351;&#29992;&#21327;&#26041;&#24046;&#30697;&#38453;&#36866;&#24212;&#36827;&#21270;&#31574;&#30053;&#23436;&#25104;&#31243;&#24207;&#21512;&#25104;&#65292;&#24182;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#37197;&#32622;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2305.15642</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#36719;&#20214;&#20195;&#30721;&#21644;&#37197;&#32622;&#33258;&#21160;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Learning-Based Automatic Synthesis of Software Code and Configuration. (arXiv:2305.15642v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#33258;&#21160;&#36719;&#20214;&#20195;&#30721;&#21644;&#37197;&#32622;&#29983;&#25104;&#26041;&#27861;&#65292;&#20854;&#20013;&#36890;&#36807;&#36951;&#20256;&#31639;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#36866;&#24212;&#24230;&#20989;&#25968;&#33258;&#21160;&#21512;&#25104;&#31243;&#24207;&#65292;&#20351;&#29992;&#21327;&#26041;&#24046;&#30697;&#38453;&#36866;&#24212;&#36827;&#21270;&#31574;&#30053;&#23436;&#25104;&#31243;&#24207;&#21512;&#25104;&#65292;&#24182;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#37197;&#32622;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#34892;&#19994;&#30340;&#38656;&#27714;&#22686;&#21152;&#21644;&#36719;&#20214;&#24037;&#31243;&#24072;&#30340;&#31232;&#32570;&#24615;&#25512;&#21160;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#33258;&#21160;&#21270;&#36719;&#20214;&#29983;&#25104;&#21644;&#37197;&#32622;&#30340;&#36807;&#31243;&#12290;&#22823;&#35268;&#27169;&#30340;&#33258;&#21160;&#36719;&#20214;&#20195;&#30721;&#29983;&#25104;&#21644;&#37197;&#32622;&#26159;&#19968;&#20010;&#38750;&#24120;&#22797;&#26434;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#23558;&#33258;&#21160;&#36719;&#20214;&#29983;&#25104;&#21644;&#37197;&#32622;&#25286;&#20998;&#20026;&#20004;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#36827;&#34892;&#30740;&#31350;&#12290;&#22312;&#31532;&#19968;&#20010;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#36755;&#20837;&#36755;&#20986;&#35268;&#33539;&#33258;&#21160;&#21512;&#25104;&#36719;&#20214;&#30340;&#26041;&#27861;&#12290;&#36825;&#20010;&#20219;&#21153;&#34987;&#36827;&#19968;&#27493;&#20998;&#20026;&#20004;&#20010;&#23376;&#20219;&#21153;&#12290;&#31532;&#19968;&#20010;&#23376;&#20219;&#21153;&#26159;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#21512;&#25104;&#31243;&#24207;&#65292;&#20854;&#39537;&#21160;&#31070;&#32463;&#32593;&#32476;&#22522;&#20110;&#31243;&#24207;&#36319;&#36394;&#21644;&#35268;&#33539;&#35757;&#32451;&#30340;&#36866;&#24212;&#24230;&#20989;&#25968;&#12290;&#23545;&#20110;&#31532;&#20108;&#20010;&#23376;&#20219;&#21153;&#65292;&#25105;&#20204;&#23558;&#31243;&#24207;&#21512;&#25104;&#24418;&#24335;&#21270;&#20026;&#36830;&#32493;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#21327;&#26041;&#24046;&#30697;&#38453;&#36866;&#24212;&#36827;&#21270;&#31574;&#30053;&#65288;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#36830;&#32493;&#20248;&#21270;&#26041;&#27861;&#65289;&#26469;&#21512;&#25104;&#31243;&#24207;&#12290;&#26368;&#21518;&#65292;&#23545;&#20110;&#37197;&#32622;&#29983;&#25104;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Increasing demands in software industry and scarcity of software engineers motivates researchers and practitioners to automate the process of software generation and configuration. Large scale automatic software generation and configuration is a very complex and challenging task. In this proposal, we set out to investigate this problem by breaking down automatic software generation and configuration into two different tasks. In first task, we propose to synthesize software automatically with input output specifications. This task is further broken down into two sub-tasks. The first sub-task is about synthesizing programs with a genetic algorithm which is driven by a neural network based fitness function trained with program traces and specifications. For the second sub-task, we formulate program synthesis as a continuous optimization problem and synthesize programs with covariance matrix adaption evolutionary strategy (a state-of-the-art continuous optimization method). Finally, for th
&lt;/p&gt;</description></item><item><title>&#32654;&#22269;&#19968;&#23478;&#22823;&#22411;&#21307;&#38498;&#32593;&#32476;&#19982;&#23398;&#26415;&#30028;&#21644;&#39038;&#38382;&#21512;&#20316;&#65292;&#24320;&#21457;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#24739;&#32773;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#32467;&#26524;&#65292;&#21516;&#26102;&#23558;&#39044;&#27979;&#19982;&#21307;&#29983;&#30340;&#39044;&#27979;&#30456;&#32467;&#21512;&#21487;&#20197;&#20351;&#26356;&#22810;&#30340;&#24739;&#32773;&#20986;&#38498;&#24182;&#20943;&#23569;&#20102;&#20877;&#20837;&#38498;&#12290;</title><link>http://arxiv.org/abs/2305.15629</link><description>&lt;p&gt;
&#24739;&#32773;&#32467;&#26524;&#39044;&#27979;&#25913;&#21892;&#20102;&#22823;&#22411;&#21307;&#38498;&#32593;&#32476;&#30340;&#36816;&#33829;
&lt;/p&gt;
&lt;p&gt;
Patient Outcome Predictions Improve Operations at a Large Hospital Network. (arXiv:2305.15629v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15629
&lt;/p&gt;
&lt;p&gt;
&#32654;&#22269;&#19968;&#23478;&#22823;&#22411;&#21307;&#38498;&#32593;&#32476;&#19982;&#23398;&#26415;&#30028;&#21644;&#39038;&#38382;&#21512;&#20316;&#65292;&#24320;&#21457;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#24739;&#32773;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#32467;&#26524;&#65292;&#21516;&#26102;&#23558;&#39044;&#27979;&#19982;&#21307;&#29983;&#30340;&#39044;&#27979;&#30456;&#32467;&#21512;&#21487;&#20197;&#20351;&#26356;&#22810;&#30340;&#24739;&#32773;&#20986;&#38498;&#24182;&#20943;&#23569;&#20102;&#20877;&#20837;&#38498;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#23450;&#20041;&#65306;&#33719;&#24471;&#20934;&#30830;&#30340;&#24739;&#32773;&#32467;&#26524;&#39044;&#27979;&#21487;&#20197;&#22686;&#24378;&#21307;&#21153;&#20154;&#21592;&#30340;&#20915;&#31574;&#33021;&#21147;&#65292;&#26368;&#32456;&#20351;&#21307;&#38498;&#30340;&#25152;&#26377;&#21033;&#30410;&#30456;&#20851;&#32773;&#21463;&#30410;&#12290;&#32654;&#22269;&#19968;&#23478;&#22823;&#22411;&#21307;&#38498;&#32593;&#32476;&#19968;&#30452;&#19982;&#23398;&#26415;&#30028;&#21644;&#39038;&#38382;&#21512;&#20316;&#65292;&#39044;&#27979;&#20854;&#19971;&#23478;&#21307;&#38498;&#25152;&#26377;&#20303;&#38498;&#24739;&#32773;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#32467;&#26524;&#12290;&#26041;&#27861;/&#32467;&#26524;&#65306;&#25105;&#20204;&#24320;&#21457;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#39044;&#27979;&#19979;&#19968;&#20010;24&#23567;&#26102;/48&#23567;&#26102;&#20986;&#38498;&#21644;&#37325;&#30151;&#30417;&#25252;&#23460;&#36716;&#31227;&#65292;&#20986;&#38498;&#27515;&#20129;&#29575;&#21644;&#20986;&#38498;&#23433;&#25490;&#30340;&#27010;&#29575;&#12290;&#25152;&#26377;&#27169;&#22411;&#37117;&#23454;&#29616;&#20102;&#39640;&#30340;&#22806;&#37096;&#26679;&#26412;AUC&#65288;75.7&#65285;-92.5&#65285;&#65289;&#65292;&#24182;&#19988;&#24456;&#22909;&#22320;&#26657;&#20934;&#12290;&#27492;&#22806;&#65292;&#23558;48&#23567;&#26102;&#20986;&#38498;&#39044;&#27979;&#19982;&#21307;&#29983;&#21516;&#26102;&#39044;&#27979;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#20351;&#26356;&#22810;&#30340;&#24739;&#32773;&#20986;&#38498;&#65288;10&#65285;-28.7&#65285;&#65289;&#65292;&#24182;&#20943;&#23569;&#20102;7&#22825;/ 30&#22825;&#30340;&#20877;&#20837;&#38498;&#65288;p&#20540;&#23567;&#20110;0.001&#65289;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#31649;&#36947;&#65292;&#27599;&#22825;&#26089;&#19978;&#25552;&#21462;&#25968;&#25454;&#24182;&#26356;&#26032;&#39044;&#27979;&#65292;&#20197;&#21450;&#29992;&#25143;&#21451;&#22909;&#22411;&#36719;&#20214;&#21644;&#24425;&#33394;&#35686;&#25253;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Problem definition: Access to accurate predictions of patients' outcomes can enhance medical staff's decision-making, which ultimately benefits all stakeholders in the hospitals. A large hospital network in the US has been collaborating with academics and consultants to predict short-term and long-term outcomes for all inpatients across their seven hospitals. Methodology/results: We develop machine learning models that predict the probabilities of next 24-hr/48-hr discharge and intensive care unit transfers, end-of-stay mortality and discharge dispositions. All models achieve high out-of-sample AUC (75.7%-92.5%) and are well calibrated. In addition, combining 48-hr discharge predictions with doctors' predictions simultaneously enables more patient discharges (10%-28.7%) and fewer 7-day/30-day readmissions ($p$-value $&lt;0.001$). We implement an automated pipeline that extracts data and updates predictions every morning, as well as user-friendly software and a color-coded alert system to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36870;&#21521;&#24037;&#31243;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#35757;&#32451;&#34920;&#31034;&#65292;&#21457;&#29616;SSL&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#27491;&#21017;&#21270;&#39033;&#26412;&#36136;&#19978;&#20419;&#36827;&#20102;&#26679;&#26412;&#22522;&#20110;&#35821;&#20041;&#26631;&#31614;&#30340;&#32858;&#31867;&#12290;SSL&#35757;&#32451;&#30340;&#34920;&#31034;&#19982;&#35821;&#20041;&#31867;&#21035;&#26356;&#21152;&#25509;&#36817;&#65292;&#23545;&#40784;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22686;&#21152;&#65292;&#32780;&#19988;&#22312;&#32593;&#32476;&#28145;&#24230;&#21152;&#28145;&#26102;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2305.15614</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#36870;&#21521;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Reverse Engineering Self-Supervised Learning. (arXiv:2305.15614v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36870;&#21521;&#24037;&#31243;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#35757;&#32451;&#34920;&#31034;&#65292;&#21457;&#29616;SSL&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#27491;&#21017;&#21270;&#39033;&#26412;&#36136;&#19978;&#20419;&#36827;&#20102;&#26679;&#26412;&#22522;&#20110;&#35821;&#20041;&#26631;&#31614;&#30340;&#32858;&#31867;&#12290;SSL&#35757;&#32451;&#30340;&#34920;&#31034;&#19982;&#35821;&#20041;&#31867;&#21035;&#26356;&#21152;&#25509;&#36817;&#65292;&#23545;&#40784;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22686;&#21152;&#65292;&#32780;&#19988;&#22312;&#32593;&#32476;&#28145;&#24230;&#21152;&#28145;&#26102;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26377;&#21147;&#30340;&#24037;&#20855;&#65292;&#20294;&#29702;&#35299;&#23398;&#20064;&#34920;&#31034;&#21450;&#20854;&#22522;&#30784;&#26426;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#23545;SSL&#35757;&#32451;&#34920;&#31034;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#21253;&#25324;&#22810;&#31181;&#27169;&#22411;&#12289;&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;SSL&#35757;&#32451;&#36807;&#31243;&#30340;&#19968;&#20010;&#26377;&#36259;&#26041;&#38754;&#65306;&#23427;&#26412;&#36136;&#19978;&#20419;&#36827;&#20102;&#26679;&#26412;&#22522;&#20110;&#35821;&#20041;&#26631;&#31614;&#30340;&#32858;&#31867;&#65292;&#36825;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#36825;&#26159;&#30001;SSL&#30446;&#26631;&#30340;&#27491;&#21017;&#21270;&#39033;&#39537;&#21160;&#30340;&#12290;&#36825;&#31181;&#32858;&#31867;&#36807;&#31243;&#19981;&#20165;&#22686;&#24378;&#20102;&#19979;&#28216;&#20998;&#31867;&#65292;&#32780;&#19988;&#21387;&#32553;&#20102;&#25968;&#25454;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;SSL&#35757;&#32451;&#30340;&#34920;&#31034;&#19982;&#35821;&#20041;&#31867;&#21035;&#26356;&#21152;&#25509;&#36817;&#65292;&#32780;&#19981;&#26159;&#38543;&#26426;&#31867;&#21035;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23398;&#20064;&#34920;&#31034;&#19982;&#21508;&#31181;&#23618;&#27425;&#30340;&#35821;&#20041;&#31867;&#21035;&#23545;&#40784;&#65292;&#24182;&#19988;&#36825;&#31181;&#23545;&#40784;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22686;&#21152;&#65292;&#32780;&#19988;&#22312;&#32593;&#32476;&#28145;&#24230;&#21152;&#28145;&#26102;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) is a powerful tool in machine learning, but understanding the learned representations and their underlying mechanisms remains a challenge. This paper presents an in-depth empirical analysis of SSL-trained representations, encompassing diverse models, architectures, and hyperparameters. Our study reveals an intriguing aspect of the SSL training process: it inherently facilitates the clustering of samples with respect to semantic labels, which is surprisingly driven by the SSL objective's regularization term. This clustering process not only enhances downstream classification but also compresses the data information. Furthermore, we establish that SSL-trained representations align more closely with semantic classes rather than random classes. Remarkably, we show that learned representations align with semantic classes across various hierarchical levels, and this alignment increases during training and when moving deeper into the network. Our findings provid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35889;&#35282;&#24230;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;GNNs&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#30495;&#23454;&#29983;&#29289;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21457;&#29616;GNNs&#22312;&#24230;&#20998;&#24067;&#21644;&#35889;&#20998;&#24067;&#20559;&#31227;&#26102;&#22343;&#34920;&#29616;&#25935;&#24863;&#65292;&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#30340;&#22823;&#22270;&#19978;&#30340;&#24615;&#33021;&#20173;&#28982;&#19979;&#38477;&#65292;&#25581;&#31034;&#20102; GNNs&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15611</link><description>&lt;p&gt;
&#22522;&#20110;&#35889;&#35282;&#24230;&#21078;&#26512;&#29983;&#29289;&#25968;&#25454;&#20013;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#65306;&#35266;&#28857;&#21644;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Size Generalizability of Graph Neural Networks on Biological Data: Insights and Practices from the Spectral Perspective. (arXiv:2305.15611v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35889;&#35282;&#24230;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;GNNs&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#30495;&#23454;&#29983;&#29289;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21457;&#29616;GNNs&#22312;&#24230;&#20998;&#24067;&#21644;&#35889;&#20998;&#24067;&#20559;&#31227;&#26102;&#22343;&#34920;&#29616;&#25935;&#24863;&#65292;&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#30340;&#22823;&#22270;&#19978;&#30340;&#24615;&#33021;&#20173;&#28982;&#19979;&#38477;&#65292;&#25581;&#31034;&#20102; GNNs&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476; (GNNs) &#26159;&#21542;&#20855;&#26377;&#20174;&#23567;&#22270;&#20013;&#23398;&#20064;&#30340;&#30693;&#35782;&#21487;&#25512;&#24191;&#21040;&#21516;&#19968;&#39046;&#22495;&#30340;&#22823;&#22270;&#20013;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#21516;&#22823;&#23567;&#30340;&#22270;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#23588;&#20854;&#26159;&#24230;&#20998;&#24067;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#22270;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#29289;&#25968;&#25454;&#38598;&#20013;&#65292;&#24230;&#25968;&#26159;&#26377;&#30028;&#30340;&#65292;&#22240;&#27492;&#24230;&#20998;&#24067;&#30340;&#20559;&#31227;&#24456;&#23567;&#12290;&#21363;&#20351;&#24230;&#20998;&#24067;&#20559;&#31227;&#24456;&#23567;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;GNNs&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#30340;&#22823;&#22270;&#19978;&#30340;&#24615;&#33021;&#20173;&#28982;&#19979;&#38477;&#65292;&#26263;&#31034;&#26377;&#20854;&#20182;&#21407;&#22240;&#12290;&#20107;&#23454;&#19978;&#65292;&#20197;&#24448;&#23545;&#20110;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#21508;&#31181;&#22270;&#23610;&#23544;&#24341;&#36215;&#30340;&#20998;&#24067;&#20559;&#31227;&#31867;&#22411;&#21644;&#23646;&#24615;&#30340;&#25506;&#32034;&#19981;&#36275;&#12290;&#27492;&#22806;&#65292;&#20197;&#21069;&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#20998;&#26512;&#22823;&#22810;&#38598;&#20013;&#22312;&#31354;&#38388;&#39046;&#22495;&#12290;&#20026;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#37319;&#29992;&#35889;&#35282;&#24230;&#21435;&#30740;&#31350;GNNs&#22312;&#29983;&#29289;&#22270;&#25968;&#25454;&#19978;&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#19968;&#20010;&#26032;&#26694;&#26550;&#26469;&#27169;&#25311;&#21508;&#31181;&#31867;&#22411;&#30340;&#24230;&#20998;&#24067;&#20559;&#31227;&#65292;&#24182;&#21033;&#29992;&#23427;&#26469;&#27979;&#35797;GNNs &#22312;&#30495;&#23454;&#29983;&#29289;&#25968;&#25454;&#38598;&#19978;&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#38500;&#20102;&#24230;&#20998;&#24067;&#20559;&#31227;&#22806;&#65292;GNNs &#36824;&#23545;&#22270;&#22823;&#23567;&#21464;&#21270;&#24341;&#36215;&#30340;&#35889;&#20998;&#24067;&#20559;&#31227;&#24456;&#25935;&#24863;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#19981;&#21516;&#30340;GNN&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#26126;&#65292;&#19968;&#20123;&#27169;&#22411;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#20855;&#26377;&#23610;&#23544;&#27867;&#21270;&#24615;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#20851;&#20110;GNNs&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#30340;&#26032;&#35266;&#28857;&#21644;&#23454;&#36341;&#65292;&#24182;&#20026;&#35813;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#27934;&#23519;&#21644;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the question of whether the knowledge learned by graph neural networks (GNNs) from small graphs is generalizable to large graphs in the same domain. Prior works suggest that the distribution shift, particularly in the degree distribution, between graphs of different sizes can lead to performance degradation in the graph classification task. However, this may not be the case for biological datasets where the degrees are bounded and the distribution shift of degrees is small. Even with little degree distribution shift, our observations show that GNNs' performance on larger graphs from the same datasets still degrades, suggesting other causes. In fact, there has been a lack of exploration in real datasets to understand the types and properties of distribution shifts caused by various graph sizes. Furthermore, previous analyses of size generalizability mostly focus on the spatial domain.  To fill these gaps, we take the spectral perspective and study the size generalizabilit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25511;&#21046;&#19981;&#21464;&#38598;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#23398;&#20064;&#38454;&#27573;&#20197;&#25552;&#39640;&#31283;&#23450;&#24615;&#20445;&#35777;&#21644;&#37319;&#26679;&#25928;&#29575;&#12290;&#20316;&#32773;&#30740;&#31350;&#20102;&#35813;&#26041;&#27861;&#30340;&#20581;&#22766;&#24615;&#65292;&#24182;&#23558;CIS&#32435;&#20837;&#21040;&#22870;&#21169;&#35774;&#35745;&#12289;&#21021;&#22987;&#29366;&#24577;&#37319;&#26679;&#21644;&#29366;&#24577;&#37325;&#32622;&#31243;&#24207;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.15602</link><description>&lt;p&gt;
&#25511;&#21046;&#19981;&#21464;&#38598;&#22686;&#24378;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65306;&#25913;&#36827;&#30340;&#37319;&#26679;&#25928;&#29575;&#12289;&#20445;&#35777;&#30340;&#31283;&#23450;&#24615;&#21644;&#20581;&#22766;&#24615;
&lt;/p&gt;
&lt;p&gt;
Control invariant set enhanced safe reinforcement learning: improved sampling efficiency, guaranteed stability and robustness. (arXiv:2305.15602v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25511;&#21046;&#19981;&#21464;&#38598;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#23398;&#20064;&#38454;&#27573;&#20197;&#25552;&#39640;&#31283;&#23450;&#24615;&#20445;&#35777;&#21644;&#37319;&#26679;&#25928;&#29575;&#12290;&#20316;&#32773;&#30740;&#31350;&#20102;&#35813;&#26041;&#27861;&#30340;&#20581;&#22766;&#24615;&#65292;&#24182;&#23558;CIS&#32435;&#20837;&#21040;&#22870;&#21169;&#35774;&#35745;&#12289;&#21021;&#22987;&#29366;&#24577;&#37319;&#26679;&#21644;&#29366;&#24577;&#37325;&#32622;&#31243;&#24207;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#20010;&#22791;&#21463;&#20851;&#27880;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#22240;&#20854;&#33021;&#22815;&#22788;&#29702;&#30495;&#23454;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#23433;&#20840;&#32422;&#26463;&#32780;&#22791;&#21463;&#30633;&#30446;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25511;&#21046;&#19981;&#21464;&#38598;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#65288;CIS&#65289;&#30340;&#26032;&#22411;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20351;&#29992;CIS&#30340;&#26174;&#24335;&#24418;&#24335;&#26469;&#25552;&#39640;&#31283;&#23450;&#24615;&#20445;&#35777;&#21644;&#37319;&#26679;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#22312;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#20917;&#19979;&#30740;&#31350;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#20581;&#22766;&#24615;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#23398;&#20064;&#38454;&#27573;&#65306;&#31163;&#32447;&#21644;&#22312;&#32447;&#12290;&#22312;&#31163;&#32447;&#38454;&#27573;&#65292;CIS&#34987;&#32435;&#20837;&#21040;&#22870;&#21169;&#35774;&#35745;&#12289;&#21021;&#22987;&#29366;&#24577;&#37319;&#26679;&#21644;&#29366;&#24577;&#37325;&#32622;&#31243;&#24207;&#20013;&#12290;&#36825;&#31181;&#32435;&#20837;CIS&#30340;&#26041;&#24335;&#26377;&#21161;&#20110;&#25552;&#39640;&#31163;&#32447;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#22312;&#22312;&#32447;&#38454;&#27573;&#65292;&#24403;&#39044;&#27979;&#30340;&#19979;&#19968;&#27493;&#29366;&#24577;&#22312;CIS&#20043;&#22806;&#26102;&#65292;&#21363;&#24341;&#20837;&#23433;&#20840;&#20934;&#21017;&#26102;&#65292;RL&#23558;&#20250;&#37325;&#26032;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) is an area of significant research interest, and safe RL in particular is attracting attention due to its ability to handle safety-driven constraints that are crucial for real-world applications. This work proposes a novel approach to RL training, called control invariant set (CIS) enhanced RL, which leverages the advantages of utilizing the explicit form of CIS to improve stability guarantees and sampling efficiency. Furthermore, the robustness of the proposed approach is investigated in the presence of uncertainty. The approach consists of two learning stages: offline and online. In the offline stage, CIS is incorporated into the reward design, initial state sampling, and state reset procedures. This incorporation of CIS facilitates improved sampling efficiency during the offline training process. In the online stage, RL is retrained whenever the predicted next step state is outside of the CIS, which serves as a stability criterion, by introducing a Safety
&lt;/p&gt;</description></item><item><title>TAGREAL&#26159;&#19968;&#31181;&#21487;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#26597;&#35810;&#25552;&#31034;&#20449;&#24687;&#65292;&#20174;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#25903;&#25345;&#20449;&#24687;&#20197;&#20174;PLM&#20013;&#25506;&#27979;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24320;&#25918;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#65292;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#21363;&#20351;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#20855;&#26377;&#31361;&#20986;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15597</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#22686;&#24378;&#24320;&#25918;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Text-Augmented Open Knowledge Graph Completion via Pre-Trained Language Models. (arXiv:2305.15597v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15597
&lt;/p&gt;
&lt;p&gt;
TAGREAL&#26159;&#19968;&#31181;&#21487;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#26597;&#35810;&#25552;&#31034;&#20449;&#24687;&#65292;&#20174;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#25903;&#25345;&#20449;&#24687;&#20197;&#20174;PLM&#20013;&#25506;&#27979;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24320;&#25918;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#65292;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#21363;&#20351;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#20855;&#26377;&#31361;&#20986;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#20219;&#21153;&#26159;&#20174;&#24050;&#30693;&#20107;&#23454;&#20013;&#25552;&#21462;&#26032;&#30340;&#21457;&#29616;&#12290;&#29616;&#26377;&#30340;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#20107;&#23454;&#19977;&#20803;&#32452;&#20197;&#25193;&#22823;&#22270;&#25512;&#29702;&#31354;&#38388;&#65292;&#35201;&#20040;&#38656;&#35201;&#25163;&#21160;&#35774;&#35745;&#25552;&#31034;&#20449;&#24687;&#20197;&#20174;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#36825;&#20123;&#26041;&#27861;&#24615;&#33021;&#26377;&#38480;&#65292;&#38656;&#35201;&#19987;&#23478;&#26114;&#36149;&#30340;&#24037;&#20316;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TAGREAL&#65292;&#23427;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26597;&#35810;&#25552;&#31034;&#20449;&#24687;&#65292;&#24182;&#20174;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#25903;&#25345;&#20449;&#24687;&#20197;&#20174;PLM&#20013;&#25506;&#27979;&#30693;&#35782;&#20197;&#23436;&#25104;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;TAGREAL&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#26159;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;TAGREAL&#30340;&#24615;&#33021;&#20173;&#28982;&#38750;&#24120;&#31361;&#20986;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#22522;&#20110;&#23884;&#20837;&#12289;&#22522;&#20110;&#22270;&#21644;&#22522;&#20110;PLM&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The mission of open knowledge graph (KG) completion is to draw new findings from known facts. Existing works that augment KG completion require either (1) factual triples to enlarge the graph reasoning space or (2) manually designed prompts to extract knowledge from a pre-trained language model (PLM), exhibiting limited performance and requiring expensive efforts from experts. To this end, we propose TAGREAL that automatically generates quality query prompts and retrieves support information from large text corpora to probe knowledge from PLM for KG completion. The results show that TAGREAL achieves state-of-the-art performance on two benchmark datasets. We find that TAGREAL has superb performance even with limited training data, outperforming existing embedding-based, graph-based, and PLM-based methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#32447;&#31574;&#30053;&#65292;&#36890;&#36807;&#20998;&#24067;&#24335;&#30340;&#21327;&#35843;&#31574;&#30053;&#35299;&#20915;&#20102;&#26410;&#24314;&#22270;&#29615;&#22659;&#19979;&#22810;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15596</link><description>&lt;p&gt;
&#26410;&#24314;&#22270;&#29615;&#22659;&#19979;&#22810;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Distributed Online Rollout for Multivehicle Routing in Unmapped Environments. (arXiv:2305.15596v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15596
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#32447;&#31574;&#30053;&#65292;&#36890;&#36807;&#20998;&#24067;&#24335;&#30340;&#21327;&#35843;&#31574;&#30053;&#35299;&#20915;&#20102;&#26410;&#24314;&#22270;&#29615;&#22659;&#19979;&#22810;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#24191;&#27867;&#21270;&#30340;&#22810;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65306;&#32473;&#23450;&#19968;&#20010;&#32593;&#32476;&#12289;&#19968;&#32452;&#21344;&#25454;&#32593;&#32476;&#33410;&#28857;&#23376;&#38598;&#30340;&#26234;&#33021;&#20307;&#21644;&#19968;&#32452;&#20219;&#21153;, &#25105;&#20204;&#23547;&#27714;&#19968;&#20010;&#26368;&#23567;&#25104;&#26412;&#30340;&#31227;&#21160;&#24207;&#21015;&#65292;&#20197;&#28385;&#36275;&#27599;&#20010;&#20219;&#21153;&#33267;&#23569;&#34987;&#19968;&#20010;&#26234;&#33021;&#20307;&#35775;&#38382;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#19982;&#32463;&#20856;&#38382;&#39064;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#20551;&#23450;&#27809;&#26377;&#20013;&#22830;&#26381;&#21153;&#22120;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#37117;&#26159;&#19968;&#20010;&#20010;&#20307;&#22788;&#29702;&#22120;&#65292;&#27809;&#26377;&#20851;&#20110;&#22522;&#30784;&#32593;&#32476;&#65288;&#21253;&#25324;&#20219;&#21153;&#21644;&#26234;&#33021;&#20307;&#20301;&#32622;&#65289;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26234;&#33021;&#20307;&#20855;&#26377;&#20005;&#26684;&#30340;&#26412;&#22320;&#36890;&#20449;&#21644;&#24863;&#30693;&#33021;&#21147;&#65288;&#38480;&#21046;&#22312;&#23427;&#20204;&#21508;&#33258;&#20301;&#32622;&#21608;&#22260;&#30340;&#21322;&#24452;&#33539;&#22260;&#20869;&#65289;&#65292;&#26356;&#25509;&#36817;&#22810;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#31574;&#30053;&#26041;&#27861;&#65292;&#20854;&#20013;&#26234;&#33021;&#20307;&#26681;&#25454;&#38598;&#20013;&#24335;&#27169;&#25311;&#22120;&#35757;&#32451;&#30340;&#23398;&#20064;&#22411;&#31574;&#30053;&#23616;&#37096;&#35268;&#21010;&#20854;&#36712;&#36857;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#23427;&#22312;&#32593;&#32476;&#25299;&#25169;&#21644;&#20219;&#21153;&#20998;&#24067;&#30340;&#21464;&#21270;&#19979;&#20173;&#28982;&#33021;&#22815;&#23454;&#29616;&#36817;&#26368;&#20248;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we consider a generalization of the well-known multivehicle routing problem: given a network, a set of agents occupying a subset of its nodes, and a set of tasks, we seek a minimum cost sequence of movements subject to the constraint that each task is visited by some agent at least once. The classical version of this problem assumes a central computational server that observes the entire state of the system perfectly and directs individual agents according to a centralized control scheme. In contrast, we assume that there is no centralized server and that each agent is an individual processor with no a priori knowledge of the underlying network (including task and agent locations). Moreover, our agents possess strictly local communication and sensing capabilities (restricted to a fixed radius around their respective locations), aligning more closely with several real-world multiagent applications. These restrictions introduce many challenges that are overcome through local
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#20154;&#20204;&#23545;&#25239;&#24615;&#25991;&#26412;&#26679;&#26412;&#30340;&#21487;&#24863;&#30693;&#24615;&#65292;&#24471;&#20986;&#29616;&#26377;&#25991;&#26412;&#25915;&#20987;&#22312;&#20154;&#31867;&#21442;&#19982;&#30340;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#25552;&#20379;&#20102;&#26356;&#20026;&#29616;&#23454;&#30340;&#23545;NLP&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.15587</link><description>&lt;p&gt;
&#20154;&#31867;&#22914;&#20309;&#24863;&#30693;&#23545;&#25239;&#25991;&#26412;&#65311;&#23545;&#22522;&#20110;&#35789;&#35821;&#23545;&#25239;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#21644;&#33258;&#28982;&#24615;&#36827;&#34892;&#29616;&#23454;&#26816;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do humans perceive adversarial text? A reality check on the validity and naturalness of word-based adversarial attacks. (arXiv:2305.15587v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#20154;&#20204;&#23545;&#25239;&#24615;&#25991;&#26412;&#26679;&#26412;&#30340;&#21487;&#24863;&#30693;&#24615;&#65292;&#24471;&#20986;&#29616;&#26377;&#25991;&#26412;&#25915;&#20987;&#22312;&#20154;&#31867;&#21442;&#19982;&#30340;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#25552;&#20379;&#20102;&#26356;&#20026;&#29616;&#23454;&#30340;&#23545;NLP&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#8212;&#8212;&#24694;&#24847;&#31639;&#27861;&#20250;&#24494;&#23567;&#22320;&#20462;&#25913;&#36755;&#20837;&#25991;&#26412;&#65292;&#23548;&#33268;&#27169;&#22411;&#20570;&#20986;&#38169;&#35823;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25915;&#20987;&#30340;&#35780;&#20272;&#24573;&#30053;&#20102;&#19981;&#21487;&#23519;&#35273;&#24615;&#36136;&#25110;&#32773;&#22312;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#30740;&#31350;&#12290;&#36825;&#24847;&#21619;&#30528;&#23545;&#25239;&#25200;&#21160;&#19981;&#20250;&#36890;&#36807;&#20219;&#20309;&#20154;&#31867;&#36136;&#37327;&#27979;&#35797;&#65292;&#20063;&#19981;&#20250;&#23545;&#36890;&#36807;&#20154;&#24037;&#26816;&#26597;&#30340;NLP&#31995;&#32479;&#26500;&#25104;&#30495;&#27491;&#30340;&#23041;&#32961;&#12290;&#20026;&#20102;&#32469;&#36807;&#36825;&#20010;&#38480;&#21046;&#24182;&#23454;&#29616;NLP&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#36866;&#24403;&#35780;&#20272;&#65288;&#20197;&#21450;&#21518;&#26469;&#30340;&#25913;&#36827;&#65289;&#65292;&#25105;&#20204;&#23545;378&#21517;&#20154;&#31867;&#21442;&#19982;&#32773;&#23601;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25915;&#20987;&#26041;&#27861;&#29983;&#20135;&#30340;&#25991;&#26412;&#23545;&#25239;&#26679;&#26412;&#30340;&#21487;&#24863;&#30693;&#24615;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#25915;&#20987;&#22312;&#20154;&#31867;&#21442;&#19982;&#30340;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#36825;&#19982;&#20808;&#21069;&#35268;&#27169;&#36739;&#23567;&#30340;&#20154;&#31867;&#30740;&#31350;&#30456;&#30683;&#30462;&#65292;&#21518;&#32773;&#25253;&#36947;&#20102;&#25915;&#20987;&#25104;&#21151;&#30340;&#36807;&#20110;&#20048;&#35266;&#32467;&#35770;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#24076;&#26395;&#20026;&#24403;&#21069;&#23545;NLP&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#30740;&#31350;&#20570;&#20986;&#31215;&#26497;&#36129;&#29486;&#65292;&#25552;&#20379;&#23545;&#20854;&#28508;&#22312;&#24433;&#21709;&#30340;&#26356;&#29616;&#23454;&#30340;&#30475;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing (NLP) models based on Machine Learning (ML) are susceptible to adversarial attacks -- malicious algorithms that imperceptibly modify input text to force models into making incorrect predictions. However, evaluations of these attacks ignore the property of imperceptibility or study it under limited settings. This entails that adversarial perturbations would not pass any human quality gate and do not represent real threats to human-checked NLP systems. To bypass this limitation and enable proper assessment (and later, improvement) of NLP model robustness, we have surveyed 378 human participants about the perceptibility of text adversarial examples produced by state-of-the-art methods. Our results underline that existing text attacks are impractical in real-world scenarios where humans are involved. This contrasts with previous smaller-scale human studies, which reported overly optimistic conclusions regarding attack success. Through our work, we hope to positi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#21453;&#24179;&#26041;Levy&#27493;&#24577;&#65288;&#31216;&#20026;Cauchy&#27493;&#24577;&#65289;&#22312;&#30446;&#26631;&#23548;&#21521;&#20219;&#21153;&#20013;&#26222;&#36941;&#20986;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.15559</link><description>&lt;p&gt;
&#30446;&#26631;&#23548;&#21521;&#20219;&#21153;&#20013;&#30340;&#21453;&#24179;&#26041;Levy&#27493;&#24577;&#26222;&#36941;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse square Levy walk emerging universally in goal-oriented tasks. (arXiv:2305.15559v1 [cond-mat.stat-mech])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#21453;&#24179;&#26041;Levy&#27493;&#24577;&#65288;&#31216;&#20026;Cauchy&#27493;&#24577;&#65289;&#22312;&#30446;&#26631;&#23548;&#21521;&#20219;&#21153;&#20013;&#26222;&#36941;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Levy&#27493;&#24577;&#20013;&#65292;&#27493;&#38271;&#20986;&#29616;&#39057;&#29575;&#36981;&#24490;&#24130;&#24459;&#20998;&#24067;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#29983;&#29289;&#30340;&#36801;&#31227;&#34892;&#20026;&#20013;&#35266;&#23519;&#21040;&#12290;&#35266;&#23519;&#21040;&#20102;&#25509;&#36817;&#20110;2&#30340;&#24130;&#25351;&#25968;&#30340;Levy&#27493;&#24577;&#65292;&#20294;&#20854;&#21407;&#22240;&#23578;&#19981;&#28165;&#26970;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#26222;&#36941;&#20135;&#29983;&#21453;&#24179;&#26041;Levy&#27493;&#24577;&#65288;&#31216;&#20026;Cauchy&#27493;&#24577;&#65289;&#30340;&#27169;&#22411;&#65292;&#24182;&#30830;&#23450;&#20986;Cauchy&#27493;&#24577;&#20986;&#29616;&#26465;&#20214;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#22312;&#30446;&#26631;&#23548;&#21521;&#30340;&#20219;&#21153;&#20013;&#65292;Cauchy&#27493;&#24577;&#26222;&#36941;&#20986;&#29616;&#12290;&#25105;&#20204;&#20351;&#29992;&#26415;&#35821;&#8220;&#30446;&#26631;&#23548;&#21521;&#8221;&#65292;&#24403;&#30446;&#26631;&#26126;&#30830;&#26102;&#65292;&#20294;&#21487;&#20197;&#36890;&#36807;&#19981;&#21516;&#30340;&#26041;&#24335;&#23454;&#29616;&#65292;&#32780;&#26080;&#27861;&#30830;&#23450;&#21807;&#19968;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#27169;&#25311;&#65292;&#19968;&#20010;&#20195;&#29702;&#35266;&#23519;&#21040;&#22312;&#20108;&#32500;&#31354;&#38388;&#20013;&#20174;&#27010;&#29575;&#20998;&#24067;&#29983;&#25104;&#30340;&#25968;&#25454;&#65292;&#24182;&#36830;&#32493;&#20272;&#35745;&#35813;&#27010;&#29575;&#20998;&#24067;&#30340;&#20013;&#24515;&#22352;&#26631;&#12290;&#20195;&#29702;&#26377;&#19968;&#20010;&#27010;&#29575;&#20998;&#24067;&#27169;&#22411;&#20316;&#20026;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#30340;&#20551;&#35774;&#65292;&#24182;&#21487;&#20197;&#20462;&#25913;&#35813;&#27169;&#22411;&#65292;&#20197;&#20351;&#20854;&#26356;&#31526;&#21512;&#23454;&#38469;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Levy walk in which the frequency of occurrence of step lengths follows a power-law distribution, can be observed in the migratory behavior of organisms at various levels. Levy walks with power exponents close to 2 are observed, and the reasons are unclear. This study aims to propose a model that universally generates inverse square Levy walks (called Cauchy walks) and to identify the conditions under which Cauchy walks appear. We demonstrate that Cauchy walks emerge universally in goal-oriented tasks. We use the term "goal-oriented" when the goal is clear, but this can be achieved in different ways, which cannot be uniquely determined. We performed a simulation in which an agent observed the data generated from a probability distribution in a two-dimensional space and successively estimated the central coordinates of that probability distribution. The agent has a model of probability distribution as a hypothesis for data-generating distribution and can modify the model such that ea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#23558;&#26426;&#22120;&#20154;&#20132;&#20114;&#23398;&#20064;&#30340;&#30740;&#31350;&#37325;&#28857;&#25918;&#22312;&#21327;&#21516;&#26500;&#24314;&#36807;&#31243;&#19978;&#65292;&#20197;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#26085;&#24120;&#24773;&#20917;&#19979;&#20174;&#38750;&#19987;&#23478;&#29992;&#25143;&#37027;&#37324;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.15535</link><description>&lt;p&gt;
&#20174;&#20132;&#20114;&#24335;&#21040;&#21327;&#21516;&#26500;&#24314;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
From Interactive to Co-Constructive Task Learning. (arXiv:2305.15535v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#23558;&#26426;&#22120;&#20154;&#20132;&#20114;&#23398;&#20064;&#30340;&#30740;&#31350;&#37325;&#28857;&#25918;&#22312;&#21327;&#21516;&#26500;&#24314;&#36807;&#31243;&#19978;&#65292;&#20197;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#26085;&#24120;&#24773;&#20917;&#19979;&#20174;&#38750;&#19987;&#23478;&#29992;&#25143;&#37027;&#37324;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#36890;&#36807;&#21033;&#29992;&#25903;&#26550;&#31574;&#30053;&#65292;&#20197;&#24456;&#23569;&#30340;&#20219;&#21153;&#28436;&#31034;&#27425;&#25968;&#65292;&#23558;&#26032;&#20219;&#21153;&#25110;&#36866;&#24212;&#20219;&#21153;&#30340;&#30456;&#20851;&#26041;&#38754;&#25945;&#32473;&#31038;&#20132;&#20249;&#20276;&#65292;&#24182;&#21033;&#29992;&#20808;&#21069;&#30340;&#30693;&#35782;&#21644;&#37325;&#35201;&#30340;&#20808;&#21069;&#32852;&#21512;&#20307;&#39564;&#20135;&#29983;&#20849;&#21516;&#29702;&#35299;&#21644;&#20849;&#21516;&#25191;&#34892;&#25152;&#38656;&#30340;&#27493;&#39588;&#12290;&#36825;&#19968;&#36807;&#31243;&#24050;&#32463;&#22312;&#29238;&#27597;&#24188;&#20799;&#20114;&#21160;&#20013;&#34987;&#21457;&#29616;&#21644;&#20998;&#26512;&#65292;&#24182;&#26500;&#25104;&#8220;&#20849;&#21516;&#24314;&#26500;&#8221;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#25945;&#24072;&#21644;&#23398;&#20064;&#32773;&#20849;&#21516;&#20026;&#20219;&#21153;&#20570;&#20986;&#36129;&#29486;&#12290;&#25105;&#20204;&#24314;&#35758;&#23558;&#26426;&#22120;&#20154;&#20132;&#20114;&#23398;&#20064;&#30340;&#30740;&#31350;&#37325;&#28857;&#25918;&#22312;&#36825;&#31181;&#20849;&#21516;&#24314;&#26500;&#36807;&#31243;&#19978;&#65292;&#20197;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#26085;&#24120;&#24773;&#20917;&#19979;&#20174;&#38750;&#19987;&#23478;&#29992;&#25143;&#37027;&#37324;&#23398;&#20064;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23558;&#23457;&#26597;&#24403;&#21069;&#30340;&#20132;&#20114;&#24335;&#20219;&#21153;&#23398;&#20064;&#25552;&#26696;&#65292;&#24182;&#35752;&#35770;&#23427;&#20204;&#22312;&#28041;&#21450;&#20114;&#21160;&#26041;&#38754;&#30340;&#20027;&#35201;&#36129;&#29486;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#35752;&#35770;&#25105;&#20204;&#20851;&#20110;&#21327;&#21516;&#26500;&#24314;&#30340;&#27010;&#24565;&#65292;&#24182;&#24635;&#32467;&#26469;&#33258;&#25104;&#24180;&#20154;-&#23401;&#23376;&#21644;&#20154;&#26426;&#20132;&#20114;&#30340;&#30740;&#31350;&#27934;&#35265;&#65292;&#20197;&#38416;&#26126;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans have developed the capability to teach relevant aspects of new or adapted tasks to a social peer with very few task demonstrations by making use of scaffolding strategies that leverage prior knowledge and importantly prior joint experience to yield a joint understanding and a joint execution of the required steps to solve the task. This process has been discovered and analyzed in parent-infant interaction and constitutes a ``co-construction'' as it allows both, the teacher and the learner, to jointly contribute to the task. We propose to focus research in robot interactive learning on this co-construction process to enable robots to learn from non-expert users in everyday situations. In the following, we will review current proposals for interactive task learning and discuss their main contributions with respect to the entailing interaction. We then discuss our notion of co-construction and summarize research insights from adult-child and human-robot interactions to elucidate it
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;Python&#26631;&#35782;&#31526;&#20132;&#25442;&#30340;&#35782;&#21035;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#36870;&#27604;&#20363;&#32553;&#25918;&#29616;&#35937;&#24433;&#21709;&#19979;&#34920;&#29616;&#26356;&#20026;&#26174;&#33879;&#12290;&#36825;&#34920;&#26126;LLM&#32570;&#20047;&#28145;&#21051;&#12289;&#25277;&#35937;&#30340;&#29702;&#35299;&#65292;&#26080;&#27861;&#32988;&#20219;&#19982;&#35757;&#32451;&#20559;&#24046;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.15507</link><description>&lt;p&gt;
&#36234;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#38169;&#35823;&#36234;&#38590;&#20197;&#25417;&#25720;&#65306;Python&#20013;&#30340;&#26631;&#35782;&#31526;&#20132;&#25442;&#19981;&#34987;&#35821;&#35328;&#27169;&#22411;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
The Larger They Are, the Harder They Fail: Language Models do not Recognize Identifier Swaps in Python. (arXiv:2305.15507v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15507
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;Python&#26631;&#35782;&#31526;&#20132;&#25442;&#30340;&#35782;&#21035;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#36870;&#27604;&#20363;&#32553;&#25918;&#29616;&#35937;&#24433;&#21709;&#19979;&#34920;&#29616;&#26356;&#20026;&#26174;&#33879;&#12290;&#36825;&#34920;&#26126;LLM&#32570;&#20047;&#28145;&#21051;&#12289;&#25277;&#35937;&#30340;&#29702;&#35299;&#65292;&#26080;&#27861;&#32988;&#20219;&#19982;&#35757;&#32451;&#20559;&#24046;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#36825;&#20123;&#27169;&#22411;&#23545;&#32534;&#31243;&#30340;&#29702;&#35299;&#31243;&#24230;&#22914;&#20309;&#12290;&#20256;&#32479;&#30340;&#32534;&#31243;&#35821;&#35328;&#20855;&#26377;&#19981;&#21464;&#24615;&#21644;&#31561;&#21464;&#24615;&#65292;&#20154;&#31867;&#31243;&#24207;&#21592;&#33021;&#30452;&#35266;&#22320;&#29702;&#35299;&#21644;&#21033;&#29992;&#36825;&#20123;&#24615;&#36136;&#65292;&#22914;&#26631;&#35782;&#31526;&#37325;&#21629;&#21517;&#65288;&#36817;&#20284;&#65289;&#19981;&#21464;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;LLM&#22312;&#40664;&#35748;&#20989;&#25968;&#21517;&#31216;&#20132;&#25442;&#26102;&#19981;&#20165;&#26080;&#27861;&#27491;&#30830;&#29983;&#25104;Python&#20195;&#30721;&#65292;&#26377;&#20123;&#27169;&#22411;&#22312;&#27169;&#22411;&#22823;&#23567;&#22686;&#21152;&#26102;&#29978;&#33267;&#21464;&#24471;&#26356;&#21152;&#33258;&#20449;&#22320;&#36827;&#34892;&#38169;&#35823;&#39044;&#27979;&#65292;&#36825;&#26159;&#26368;&#36817;&#21457;&#29616;&#30340;&#36870;&#27604;&#20363;&#32553;&#25918;&#29616;&#35937;&#30340;&#23454;&#20363;&#65292;&#19982;&#36890;&#24120;&#35266;&#23519;&#21040;&#30340;&#27169;&#22411;&#22823;&#23567;&#22686;&#21152;&#20250;&#25552;&#39640;&#39044;&#27979;&#36136;&#37327;&#30340;&#36235;&#21183;&#30456;&#21453;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#23613;&#31649;LLM&#30340;&#20856;&#22411;&#24773;&#20917;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#32570;&#20047;&#19968;&#20010;&#28145;&#21051;&#30340;&#12289;&#25277;&#35937;&#30340;&#29702;&#35299;&#25454;&#20197;&#25805;&#32437;&#20869;&#23481;&#65292;&#20351;&#23427;&#20204;&#26080;&#27861;&#32988;&#20219;&#19982;&#35757;&#32451;&#20559;&#24046;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have successfully been applied to code generation tasks, raising the question of how well these models understand programming. Typical programming languages have invariances and equivariances in their semantics that human programmers intuitively understand and exploit, such as the (near) invariance to the renaming of identifiers. We show that LLMs not only fail to properly generate correct Python code when default function names are swapped, but some of them even become more confident in their incorrect predictions as the model size increases, an instance of the recently discovered phenomenon of Inverse Scaling, which runs contrary to the commonly observed trend of increasing prediction quality with increasing model size. Our findings indicate that, despite their astonishing typical-case performance, LLMs still lack a deep, abstract understanding of the content they manipulate, making them unsuitable for tasks that statistically deviate from their training 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23545;&#29992;&#25143;&#20852;&#36259;&#36827;&#34892;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23450;&#20041;&#20852;&#36259;&#26053;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#26088;&#22312;&#25552;&#39640;&#25512;&#33616;&#30340;&#36136;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#21644;&#26032;&#39062;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15498</link><description>&lt;p&gt;
&#29992;&#25143;&#20852;&#36259;&#26053;&#31243;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for User Interest Journeys. (arXiv:2305.15498v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15498
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23545;&#29992;&#25143;&#20852;&#36259;&#36827;&#34892;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23450;&#20041;&#20852;&#36259;&#26053;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#26088;&#22312;&#25552;&#39640;&#25512;&#33616;&#30340;&#36136;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#21644;&#26032;&#39062;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#30340;&#20196;&#20154;&#30633;&#30446;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#29992;&#25143;&#21644;&#25913;&#21892;&#20010;&#24615;&#21270;&#25512;&#33616;&#24179;&#21488;&#20307;&#39564;&#26041;&#38754;&#30340;&#28508;&#21147;&#36824;&#36828;&#26410;&#34987;&#21457;&#25381;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;LLMs&#23545;&#29992;&#25143;&#20852;&#36259;&#36827;&#34892;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#24182;&#23450;&#20041;&#20102;&#20852;&#36259;&#26053;&#31243;&#20316;&#20026;&#29992;&#25143;&#22522;&#20110;&#20182;&#20204;&#30340;&#27963;&#21160;&#32780;&#36941;&#21382;&#36807;&#30340;&#20852;&#36259;&#29366;&#24577;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#29992;&#25143;&#34920;&#31034;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#25512;&#33616;&#30340;&#36136;&#37327;&#65292;&#24182;&#19988;&#29983;&#25104;&#30340;&#20852;&#36259;&#26053;&#31243;&#20026;&#25512;&#33616;&#36807;&#31243;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#21644;&#26032;&#39062;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown impressive capabilities in natural language understanding and generation. Their potential for deeper user understanding and improved personalized user experience on recommendation platforms is, however, largely untapped. This paper aims to address this gap. Recommender systems today capture users' interests through encoding their historical activities on the platforms. The generated user representations are hard to examine or interpret. On the other hand, if we were to ask people about interests they pursue in their life, they might talk about their hobbies, like I just started learning the ukulele, or their relaxation routines, e.g., I like to watch Saturday Night Live, or I want to plant a vertical garden. We argue, and demonstrate through extensive experiments, that LLMs as foundation models can reason through user activities, and describe their interests in nuanced and interesting ways, similar to how a human would.  We define interest journe
&lt;/p&gt;</description></item><item><title>SPRING&#26159;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#24320;&#25918;&#19990;&#30028;&#28216;&#25103;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23427;&#36890;&#36807;&#38405;&#35835;&#28216;&#25103;&#30340;&#21407;&#22987;&#23398;&#26415;&#35770;&#25991;&#24182;&#20351;&#29992;&#25152;&#23398;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#24182;&#29609;&#28216;&#25103;&#12290;&#12290;</title><link>http://arxiv.org/abs/2305.15486</link><description>&lt;p&gt;
SPRING: GPT-4&#36890;&#36807;&#23398;&#20064;&#35770;&#25991;&#21644;&#25512;&#29702;&#22312;&#28216;&#25103;&#20013;&#34920;&#29616;&#36229;&#36807;RL&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
SPRING: GPT-4 Out-performs RL Algorithms by Studying Papers and Reasoning. (arXiv:2305.15486v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15486
&lt;/p&gt;
&lt;p&gt;
SPRING&#26159;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#24320;&#25918;&#19990;&#30028;&#28216;&#25103;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23427;&#36890;&#36807;&#38405;&#35835;&#28216;&#25103;&#30340;&#21407;&#22987;&#23398;&#26415;&#35770;&#25991;&#24182;&#20351;&#29992;&#25152;&#23398;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#24182;&#29609;&#28216;&#25103;&#12290;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#28216;&#25103;&#30001;&#20110;&#20854;&#22810;&#20219;&#21153;&#12289;&#28145;&#24230;&#25506;&#32034;&#21644;&#30446;&#26631;&#20248;&#20808;&#32423;&#35201;&#27714;&#65292;&#23545;AI&#31639;&#27861;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#35299;&#20915;&#28216;&#25103;&#26041;&#38754;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#20854;&#39640;&#26679;&#26412;&#22797;&#26434;&#24615;&#38480;&#21046;&#20102;&#23427;&#22312;&#20687;Crafter&#25110;Minecraft&#36825;&#26679;&#22797;&#26434;&#30340;&#24320;&#25918;&#19990;&#30028;&#28216;&#25103;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;SPRING&#65292;&#36890;&#36807;&#38405;&#35835;&#28216;&#25103;&#30340;&#21407;&#22987;&#23398;&#26415;&#35770;&#25991;&#24182;&#20351;&#29992;&#25152;&#23398;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#24182;&#29609;&#28216;&#25103;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#32473;&#23450;LaTeX&#28304;&#20316;&#20026;&#28216;&#25103;&#35821;&#22659;&#21644;&#20195;&#29702;&#24403;&#21069;&#35266;&#23519;&#30340;&#25551;&#36848;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;SPRING&#26694;&#26550;&#21033;&#29992;&#20855;&#26377;&#28216;&#25103;&#30456;&#20851;&#38382;&#39064;&#30340;&#23450;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#20316;&#20026;&#33410;&#28857;&#21644;&#20381;&#36182;&#20851;&#31995;&#20316;&#20026;&#36793;&#12290;&#36890;&#36807;&#25353;&#25299;&#25169;&#39034;&#24207;&#36941;&#21382;DAG&#24182;&#35745;&#31639;&#27599;&#20010;&#33410;&#28857;&#30340;LLM&#21709;&#24212;&#26469;&#30830;&#23450;&#22312;&#29615;&#22659;&#20013;&#37319;&#21462;&#30340;&#26368;&#20248;&#34892;&#21160;&#65292;LLM&#23545;&#26368;&#32456;&#33410;&#28857;&#30340;&#31572;&#26696;&#30452;&#25509;&#36716;&#21270;&#20026;&#29615;&#22659;&#34892;&#21160;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;
&lt;/p&gt;
&lt;p&gt;
Open-world survival games pose significant challenges for AI algorithms due to their multi-tasking, deep exploration, and goal prioritization requirements. Despite reinforcement learning (RL) being popular for solving games, its high sample complexity limits its effectiveness in complex open-world games like Crafter or Minecraft. We propose a novel approach, SPRING, to read the game's original academic paper and use the knowledge learned to reason and play the game through a large language model (LLM). Prompted with the LaTeX source as game context and a description of the agent's current observation, our SPRING framework employs a directed acyclic graph (DAG) with game-related questions as nodes and dependencies as edges. We identify the optimal action to take in the environment by traversing the DAG and calculating LLM responses for each node in topological order, with the LLM's answer to final node directly translating to environment actions. In our experiments, we study the quality
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#33521;&#22269;&#35199;&#31859;&#24503;&#20848;&#22320;&#21306;85&#23478;&#20013;&#23567;&#20225;&#19994;&#20026;&#20363;&#65292;&#25506;&#35752;&#20102;&#20013;&#23567;&#20225;&#19994;&#22312;&#36827;&#34892;&#26377;&#25928;&#25968;&#25454;&#39537;&#21160;&#20915;&#31574;&#26041;&#38754;&#38754;&#20020;&#30340;&#36235;&#21183;&#21644;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#30001;&#20110;&#26377;&#38480;&#30340;&#36164;&#28304;&#21644;&#34701;&#36164;&#28192;&#36947;&#21463;&#38480;&#31561;&#21407;&#22240;&#65292;&#22914;&#20309;&#22312;&#25216;&#33021;&#21644;IT&#25237;&#36164;&#26041;&#38754;&#23547;&#25214;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.15454</link><description>&lt;p&gt;
&#33521;&#22269;&#20013;&#23567;&#20225;&#19994;&#26377;&#25928;&#25968;&#25454;&#39537;&#21160;&#20915;&#31574;&#30340;&#36235;&#21183;&#19982;&#25361;&#25112;&#65306;&#22522;&#20110;&#23545;85&#23478;&#20013;&#23567;&#20225;&#19994;&#26696;&#20363;&#30740;&#31350;&#21644;&#32463;&#39564;&#25945;&#35757;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Trends and Challenges Towards an Effective Data-Driven Decision Making in UK SMEs: Case Studies and Lessons Learnt from the Analysis of 85 SMEs. (arXiv:2305.15454v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#33521;&#22269;&#35199;&#31859;&#24503;&#20848;&#22320;&#21306;85&#23478;&#20013;&#23567;&#20225;&#19994;&#20026;&#20363;&#65292;&#25506;&#35752;&#20102;&#20013;&#23567;&#20225;&#19994;&#22312;&#36827;&#34892;&#26377;&#25928;&#25968;&#25454;&#39537;&#21160;&#20915;&#31574;&#26041;&#38754;&#38754;&#20020;&#30340;&#36235;&#21183;&#21644;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#30001;&#20110;&#26377;&#38480;&#30340;&#36164;&#28304;&#21644;&#34701;&#36164;&#28192;&#36947;&#21463;&#38480;&#31561;&#21407;&#22240;&#65292;&#22914;&#20309;&#22312;&#25216;&#33021;&#21644;IT&#25237;&#36164;&#26041;&#38754;&#23547;&#25214;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31185;&#23398;&#30340;&#37319;&#29992;&#20026;&#20013;&#23567;&#22411;&#20225;&#19994;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#22909;&#22788;&#65292;&#21253;&#25324;&#25552;&#39640;&#19994;&#21153;&#29983;&#20135;&#21147;&#12289;&#20419;&#36827;&#32463;&#27982;&#22686;&#38271;&#12289;&#21019;&#26032;&#21644;&#21019;&#36896;&#23601;&#19994;&#26426;&#20250;&#12290;&#25968;&#25454;&#31185;&#23398;&#21487;&#20197;&#25903;&#25345;&#20013;&#23567;&#20225;&#19994;&#20248;&#21270;&#29983;&#20135;&#27969;&#31243;&#65292;&#39044;&#27979;&#23458;&#25143;&#38656;&#27714;&#65292;&#39044;&#27979;&#26426;&#22120;&#25925;&#38556;&#24182;&#25552;&#20379;&#39640;&#25928;&#30340;&#26234;&#33021;&#26381;&#21153;&#12290;&#20225;&#19994;&#36824;&#21487;&#20197;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#25968;&#25454;&#30340;&#21147;&#37327;&#20197;&#21450;&#26234;&#33021;&#20351;&#29992;&#25968;&#23383;&#25216;&#26415;&#26469;&#25552;&#39640;&#29983;&#20135;&#21147;&#21644;&#19994;&#21153;&#34920;&#29616;&#65292;&#20026;&#21019;&#26032;&#38138;&#24179;&#36947;&#36335;&#12290;&#28982;&#32780;&#65292;&#23558;&#25968;&#25454;&#31185;&#23398;&#20915;&#31574;&#25972;&#21512;&#21040;&#20013;&#23567;&#20225;&#19994;&#20013;&#38656;&#35201;&#25216;&#33021;&#21644;IT&#25237;&#36164;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#36164;&#28304;&#26377;&#38480;&#21644;&#34701;&#36164;&#28192;&#36947;&#21463;&#38480;&#65292;&#36825;&#20123;&#36153;&#29992;&#36229;&#20986;&#20102;&#20013;&#23567;&#20225;&#19994;&#30340;&#36127;&#25285;&#33539;&#22260;&#12290;&#26412;&#25991;&#20197;&#33521;&#22269;&#35199;&#31859;&#24503;&#20848;&#22320;&#21306;&#20026;&#20027;&#35201;&#26696;&#20363;&#30740;&#31350;85&#23478;&#20013;&#23567;&#20225;&#19994;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#32452;&#32455;&#26377;&#25928;&#25968;&#25454;&#39537;&#21160;&#20915;&#31574;&#30340;&#36235;&#21183;&#21644;&#25361;&#25112;&#12290;&#35813;&#24037;&#20316;&#26159;&#20316;&#20026;3&#24180;ERDF&#65288;&#27431;&#27954;&#21306;&#22495;&#21457;&#23637;&#22522;&#37329;&#65289;&#35745;&#21010;&#30340;&#19968;&#37096;&#20998;&#36164;&#21161;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The adoption of data science brings vast benefits to Small and Medium-sized Enterprises (SMEs) including business productivity, economic growth, innovation and jobs creation. Data Science can support SMEs to optimise production processes, anticipate customers' needs, predict machinery failures and deliver efficient smart services. Businesses can also harness the power of Artificial Intelligence (AI) and Big Data and the smart use of digital technologies to enhance productivity and performance, paving the way for innovation. However, integrating data science decisions into an SME requires both skills and IT investments. In most cases, such expenses are beyond the means of SMEs due to limited resources and restricted access to financing. This paper presents trends and challenges towards an effective data-driven decision making for organisations based on a case study of 85 SMEs, mostly from the West Midlands region of England. The work is supported as part of a 3 years ERDF (European Regi
&lt;/p&gt;</description></item><item><title>PromptNER&#26159;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31639;&#27861;&#65292;&#21033;&#29992;LLM&#29983;&#25104;&#28508;&#22312;&#23454;&#20307;&#21015;&#34920;&#24182;&#25552;&#20379;&#35299;&#37322;&#65292;&#22312;&#23569;&#26679;&#26412;NER&#21644;&#36328;&#39046;&#22495;NER&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15444</link><description>&lt;p&gt;
PromptNER: &#22522;&#20110;&#25552;&#31034;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
PromptNER: Prompting For Named Entity Recognition. (arXiv:2305.15444v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15444
&lt;/p&gt;
&lt;p&gt;
PromptNER&#26159;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31639;&#27861;&#65292;&#21033;&#29992;LLM&#29983;&#25104;&#28508;&#22312;&#23454;&#20307;&#21015;&#34920;&#24182;&#25552;&#20379;&#35299;&#37322;&#65292;&#22312;&#23569;&#26679;&#26412;NER&#21644;&#36328;&#39046;&#22495;NER&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#36234;&#26469;&#36234;&#22810;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#29616;&#22312;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#29616;&#25104;&#26041;&#27861;&#65292;&#20026;&#21508;&#31181;&#32463;&#20856;&#30340;NLP&#38382;&#39064;&#25552;&#20379;&#20102;&#23569;&#37327;&#26679;&#26412;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#30528;&#20196;&#20154;&#26399;&#24453;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#20294;&#36825;&#20123;&#22522;&#20110;LLM&#30340;&#23569;&#26679;&#26412;&#26041;&#27861;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#26041;&#38754;&#20173;&#36828;&#26410;&#36798;&#21040;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#21253;&#25324;&#36890;&#36807;&#31471;&#21040;&#31471;&#32467;&#26500;&#29702;&#35299;&#23398;&#20064;&#34920;&#31034;&#65292;&#24182;&#22312;&#26631;&#20934;&#26631;&#35760;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PromptNER&#65292;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#23569;&#26679;&#26412;&#21644;&#36328;&#39046;&#22495;NER&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;&#20026;&#20102;&#36866;&#24212;&#20219;&#20309;&#26032;&#30340;NER&#20219;&#21153;&#65292;PromptNER&#38656;&#35201;&#25552;&#20379;&#19968;&#32452;&#23454;&#20307;&#23450;&#20041;&#65292;&#38500;&#22522;&#26412;&#30340;&#23569;&#26679;&#26412;&#26679;&#20363;&#20197;&#22806;&#12290;&#32473;&#23450;&#36755;&#20837;&#21477;&#23376;&#65292;PromptNER&#25552;&#31034;LLM&#29983;&#25104;&#19968;&#20010;&#28508;&#22312;&#23454;&#20307;&#21015;&#34920;&#65292;&#24182;&#25552;&#20379;&#30456;&#24212;&#30340;&#35299;&#37322;&#65292;&#35777;&#26126;&#23427;&#20204;&#19982;&#25552;&#20379;&#30340;&#23454;&#20307;&#31867;&#22411;&#23450;&#20041;&#30340;&#20860;&#23481;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;PromptNER&#22312;&#23569;&#26679;&#26412;NER&#20219;&#21153;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;WikiAnn&#25968;&#25454;&#38598;&#19978;&#20026;&#36328;&#39046;&#22495;NER&#35774;&#23450;&#20102;&#26032;&#30340;SOTA&#12290;
&lt;/p&gt;
&lt;p&gt;
In a surprising turn, Large Language Models (LLMs) together with a growing arsenal of prompt-based heuristics now offer powerful off-the-shelf approaches providing few-shot solutions to myriad classic NLP problems. However, despite promising early results, these LLM-based few-shot methods remain far from the state of the art in Named Entity Recognition (NER), where prevailing methods include learning representations via end-to-end structural understanding and fine-tuning on standard labeled corpora. In this paper, we introduce PromptNER, a new state-of-the-art algorithm for few-Shot and cross-domain NER. To adapt to any new NER task PromptNER requires a set of entity definitions in addition to the standard few-shot examples. Given a sentence, PromptNER prompts an LLM to produce a list of potential entities along with corresponding explanations justifying their compatibility with the provided entity type definitions. Remarkably, PromptNER achieves state-of-the-art performance on few-sho
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#23545;&#29356;&#30340;ECG&#20449;&#21495;&#36827;&#34892;&#33258;&#21160;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#21644;&#36830;&#32493;&#23567;&#27874;&#21464;&#25442;&#65292;&#20998;&#31867;&#31934;&#24230;&#24471;&#21040;&#20102;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.15424</link><description>&lt;p&gt;
PulseNet: &#20351;&#29992;&#38543;&#26426;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#21644;&#36830;&#32493;&#23567;&#27874;&#21464;&#25442;&#36827;&#34892;&#29356;ECG&#20449;&#21495;&#20998;&#31867;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
PulseNet: Deep Learning ECG-signal classification using random augmentation policy and continous wavelet transform for canines. (arXiv:2305.15424v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15424
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#23545;&#29356;&#30340;ECG&#20449;&#21495;&#36827;&#34892;&#33258;&#21160;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#21644;&#36830;&#32493;&#23567;&#27874;&#21464;&#25442;&#65292;&#20998;&#31867;&#31934;&#24230;&#24471;&#21040;&#20102;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#29356;&#30340;&#24515;&#30005;&#22270;(ECG)&#38656;&#35201;&#29087;&#32451;&#30340;&#20861;&#21307;&#65292;&#20294;&#30446;&#21069;&#21487;&#29992;&#30340;&#20861;&#21307;&#24515;&#33039;&#30149;&#19987;&#23478;&#29992;&#20110;ECG&#35299;&#35835;&#21644;&#35786;&#26029;&#25903;&#25345;&#30340;&#25968;&#37327;&#26377;&#38480;&#12290;&#24320;&#21457;&#33258;&#21160;&#35780;&#20272;ECG&#24207;&#21015;&#30340;&#24037;&#20855;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#20020;&#24202;&#21307;&#29983;&#23454;&#26102;&#32467;&#26524;&#21644;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#26469;&#25913;&#21892;&#20861;&#21307;&#25252;&#29702;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#26469;&#23558;&#29356;&#30340;&#24515;&#30005;&#22270;&#24207;&#21015;&#20998;&#31867;&#20026;&#27491;&#24120;&#25110;&#24322;&#24120;&#12290;&#23558;ECG&#35760;&#24405;&#36716;&#25442;&#20026;8&#31186;&#30340;&#31532;&#20108;&#23548;&#32852;&#24207;&#21015;&#65292;&#26681;&#25454;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#25110;&#22810;&#31181;&#24515;&#33039;&#24322;&#24120;&#23558;&#20854;&#20998;&#31867;&#20026;&#27491;&#24120;&#25110;&#24322;&#24120;&#12290;&#35757;&#32451;ECG&#24207;&#21015;&#20351;&#29992;RandomAugmentECG&#36827;&#34892;&#38543;&#26426;&#25968;&#25454;&#22686;&#24378;&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#35813;&#39033;&#30446;&#23454;&#29616;&#30340;&#26032;&#22686;&#24378;&#24211;&#12290;&#28982;&#21518;&#65292;&#27599;&#20010;&#22359;&#20351;&#29992;&#36830;&#32493;&#23567;&#27874;&#21464;&#25442;&#36716;&#25442;&#25104;2D scalogram&#12290;2D scalogram&#20351;&#29992;&#20108;&#20803;CNN&#20998;&#31867;&#22120;&#20998;&#31867;&#25104;&#27491;&#24120;&#25110;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating canine electrocardiograms (ECG) require skilled veterinarians, but current availability of veterinary cardiologists for ECG interpretation and diagnostic support is limited. Developing tools for automated assessment of ECG sequences can improve veterinary care by providing clinicians real-time results and decision support tools. We implement a deep convolutional neural network (CNN) approach for classifying canine electrocardiogram sequences as either normal or abnormal. ECG records are converted into 8 second Lead II sequences and classified as either normal (no evidence of cardiac abnormalities) or abnormal (presence of one or more cardiac abnormalities). For training ECG sequences are randomly augmented using RandomAugmentECG, a new augmentation library implemented specifically for this project. Each chunk is then is converted using a continuous wavelet transform into a 2D scalogram. The 2D scalogram are then classified as either normal or abnormal by a binary CNN classif
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#35821;&#20041;&#20960;&#20309;&#26041;&#27861;&#30340;&#24314;&#31569;&#28857;&#20113;&#26080;&#26434;&#20081;&#29289;&#24179;&#38754;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#22788;&#29702;&#36755;&#20837;&#28857;&#20113;&#12289;&#20351;&#29992;PointNet++&#32593;&#32476;&#26631;&#35760;&#28857;&#12289;&#26368;&#21518;&#36890;&#36807;&#20960;&#20309;&#26041;&#27861;&#29983;&#25104;&#26080;&#26434;&#20081;&#29289;&#30340;&#24179;&#38754;&#12290;</title><link>http://arxiv.org/abs/2305.15420</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;&#35821;&#20041;&#20960;&#20309;&#26041;&#27861;&#30340;&#24314;&#31569;&#28857;&#20113;&#26080;&#26434;&#20081;&#29289;&#24179;&#38754;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
A Hybrid Semantic-Geometric Approach for Clutter-Resistant Floorplan Generation from Building Point Clouds. (arXiv:2305.15420v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#35821;&#20041;&#20960;&#20309;&#26041;&#27861;&#30340;&#24314;&#31569;&#28857;&#20113;&#26080;&#26434;&#20081;&#29289;&#24179;&#38754;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#22788;&#29702;&#36755;&#20837;&#28857;&#20113;&#12289;&#20351;&#29992;PointNet++&#32593;&#32476;&#26631;&#35760;&#28857;&#12289;&#26368;&#21518;&#36890;&#36807;&#20960;&#20309;&#26041;&#27861;&#29983;&#25104;&#26080;&#26434;&#20081;&#29289;&#30340;&#24179;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31569;&#20449;&#24687;&#27169;&#25311;&#65288;BIM&#65289;&#25216;&#26415;&#26159;&#29616;&#20195;&#24314;&#31569;&#24037;&#31243;&#21644;&#39033;&#30446;&#31649;&#29702;&#24037;&#20316;&#27969;&#31243;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#23637;&#31034;&#39033;&#30446;&#29616;&#22330;&#31354;&#38388;&#23454;&#38469;&#24773;&#20917;&#30340;&#29616;&#29366;BIM&#27169;&#22411;&#21487;&#20197;&#20026;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#20851;&#38190;&#20449;&#24687;&#65292;&#29992;&#20110;&#24314;&#35774;&#36827;&#24230;&#30417;&#25511;&#12289;&#38169;&#35823;&#26816;&#26597;&#21644;&#24314;&#31569;&#32500;&#25252;&#12290;&#33258;&#21160;&#23558;&#25195;&#25551;&#21407;&#22987;&#25968;&#25454;&#36716;&#25442;&#20026;BIM&#27169;&#22411;&#65288;&#25195;&#25551;&#21040;BIM&#65289;&#30340;&#20960;&#20309;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#21033;&#29992;&#25968;&#25454;&#20013;&#26356;&#39640;&#32423;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#32780;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#20165;&#22312;&#28857;&#32423;&#21035;&#19978;&#36755;&#20986;&#26631;&#31614;&#65292;&#32780;&#19981;&#21019;&#24314;&#23545;&#35937;&#32423;&#27169;&#22411;&#65292;&#36825;&#26159;BIM&#25152;&#24517;&#38656;&#30340;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#35821;&#20041;&#20960;&#20309;&#26041;&#27861;&#30340;&#24314;&#31569;&#28857;&#20113;&#26080;&#26434;&#20081;&#29289;&#24179;&#38754;&#29983;&#25104;&#26041;&#27861;&#12290;&#39318;&#20808;&#36890;&#36807;&#24402;&#19968;&#21270;&#22352;&#26631;&#31995;&#21644;&#21435;&#38500;&#24322;&#24120;&#20540;&#26469;&#39044;&#22788;&#29702;&#36755;&#20837;&#28857;&#20113;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;PointNet++&#30340;&#35821;&#20041;&#20998;&#21106;&#32593;&#32476;&#29992;&#20110;&#23545;&#28857;&#36827;&#34892;&#26631;&#35760;&#65292;&#26368;&#21518;&#36890;&#36807;&#20960;&#20309;&#26041;&#27861;&#29983;&#25104;&#26080;&#26434;&#20081;&#29289;&#30340;&#24179;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building Information Modeling (BIM) technology is a key component of modern construction engineering and project management workflows. As-is BIM models that represent the spatial reality of a project site can offer crucial information to stakeholders for construction progress monitoring, error checking, and building maintenance purposes. Geometric methods for automatically converting raw scan data into BIM models (Scan-to-BIM) often fail to make use of higher-level semantic information in the data. Whereas, semantic segmentation methods only output labels at the point level without creating object level models that is necessary for BIM. To address these issues, this research proposes a hybrid semantic-geometric approach for clutter-resistant floorplan generation from laser-scanned building point clouds. The input point clouds are first pre-processed by normalizing the coordinate system and removing outliers. Then, a semantic segmentation network based on PointNet++ is used to label eac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#36807;&#28193;&#37329;&#23646;&#21450;&#20854;&#27687;&#21270;&#29289;&#30340;X&#23556;&#32447;&#34893;&#23556;&#25968;&#25454;&#20013;&#39044;&#27979;&#26230;&#20307;&#32467;&#26500;&#30456;&#65292;&#25552;&#20986;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#33021;&#65292;&#34920;&#26126;&#20102;&#26426;&#22120;&#23398;&#20064;&#23545;X&#23556;&#32447;&#34893;&#23556;&#21644;&#26230;&#20307;&#32467;&#26500;&#30830;&#23450;&#39046;&#22495;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.15410</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#36807;&#28193;&#37329;&#23646;X&#23556;&#32447;&#34893;&#23556;&#30456;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Machine learning-assisted close-set X-ray diffraction phase identification of transition metals. (arXiv:2305.15410v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#36807;&#28193;&#37329;&#23646;&#21450;&#20854;&#27687;&#21270;&#29289;&#30340;X&#23556;&#32447;&#34893;&#23556;&#25968;&#25454;&#20013;&#39044;&#27979;&#26230;&#20307;&#32467;&#26500;&#30456;&#65292;&#25552;&#20986;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#33021;&#65292;&#34920;&#26126;&#20102;&#26426;&#22120;&#23398;&#20064;&#23545;X&#23556;&#32447;&#34893;&#23556;&#21644;&#26230;&#20307;&#32467;&#26500;&#30830;&#23450;&#39046;&#22495;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#34987;&#24212;&#29992;&#20110;X&#23556;&#32447;&#34893;&#23556;&#30456;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#36807;&#28193;&#37329;&#23646;&#21450;&#20854;&#27687;&#21270;&#29289;&#30340;X&#23556;&#32447;&#34893;&#23556;&#25968;&#25454;&#20013;&#39044;&#27979;&#26230;&#20307;&#32467;&#26500;&#30456;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#27604;&#36739;&#20102;&#20854;&#22810;&#31181;&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;&#36825;&#35777;&#26126;&#20102;&#26426;&#22120;&#23398;&#20064;&#23545;X&#23556;&#32447;&#34893;&#23556;&#21644;&#26230;&#20307;&#32467;&#26500;&#30830;&#23450;&#39046;&#22495;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;&#24320;&#28304;&#23454;&#29616;&#65306;https://github.com/maxnygma/NeuralXRD.
&lt;/p&gt;
&lt;p&gt;
Machine learning has been applied to the problem of X-ray diffraction phase prediction with promising results. In this paper, we describe a method for using machine learning to predict crystal structure phases from X-ray diffraction data of transition metals and their oxides. We evaluate the performance of our method and compare the variety of its settings. Our results demonstrate that the proposed machine learning framework achieves competitive performance. This demonstrates the potential for machine learning to significantly impact the field of X-ray diffraction and crystal structure determination. Open-source implementation: https://github.com/maxnygma/NeuralXRD.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#27169;&#22359;&#26469;&#25552;&#28860;&#20302;&#36136;&#37327;&#27531;&#22522;&#65292;&#24341;&#20837;&#35760;&#24518;&#26816;&#32034;&#26426;&#21046;&#23454;&#29616;&#20102;&#36229;&#36807;50%&#30340;&#35757;&#32451;&#26102;&#38388;&#33410;&#30465;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#26159;&#34507;&#30333;&#36136;&#35774;&#35745;&#39046;&#22495;&#30340;&#19968;&#27425;&#21019;&#26032;&#12290;</title><link>http://arxiv.org/abs/2305.15151</link><description>&lt;p&gt;
&#30693;&#35782;&#35774;&#35745;&#65306;&#36890;&#36807;&#30693;&#35782;&#25552;&#28860;&#25512;&#21160;&#34507;&#30333;&#36136;&#35774;&#35745;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Design: Pushing the Limit of Protein Deign via Knowledge Refinement. (arXiv:2305.15151v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#27169;&#22359;&#26469;&#25552;&#28860;&#20302;&#36136;&#37327;&#27531;&#22522;&#65292;&#24341;&#20837;&#35760;&#24518;&#26816;&#32034;&#26426;&#21046;&#23454;&#29616;&#20102;&#36229;&#36807;50%&#30340;&#35757;&#32451;&#26102;&#38388;&#33410;&#30465;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#26159;&#34507;&#30333;&#36136;&#35774;&#35745;&#39046;&#22495;&#30340;&#19968;&#27425;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#34507;&#30333;&#36136;&#35774;&#35745;&#20013;&#65292;&#23547;&#25214;&#25240;&#21472;&#20026;&#25152;&#26399;&#26395;&#32467;&#26500;&#30340;&#27688;&#22522;&#37240;&#24207;&#21015;&#24050;&#32463;&#21462;&#24471;&#20102;&#31454;&#20105;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#24573;&#30053;&#20102;&#39044;&#27979;&#32622;&#20449;&#24230;&#30340;&#37325;&#35201;&#24615;&#65292;&#26410;&#33021;&#35206;&#30422;&#24191;&#27867;&#30340;&#34507;&#30333;&#36136;&#31354;&#38388;&#65292;&#24182;&#19988;&#27809;&#26377;&#34701;&#20837;&#24120;&#35265;&#30340;&#34507;&#30333;&#36136;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#27169;&#22359;&#26469;&#25552;&#28860;&#20302;&#36136;&#37327;&#27531;&#22522;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#35760;&#24518;&#26816;&#32034;&#26426;&#21046;&#26469;&#33410;&#30465;&#36229;&#36807;50%&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#25105;&#20204;&#22312;CATH&#12289;TS50&#21644;TS500&#25968;&#25454;&#38598;&#19978;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#30693;&#35782;&#35774;&#35745;&#26041;&#27861;&#22312;CATH&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#20808;&#21069;&#30340;PiFold&#26041;&#27861;&#32422;9&#65285;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#30693;&#35782;&#35774;&#35745;&#26159;&#31532;&#19968;&#20010;&#23454;&#29616;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Recent studies have shown competitive performance in protein design that aims to find the amino acid sequence folding into the desired structure. However, most of them disregard the importance of predictive confidence, fail to cover the vast protein space, and do not incorporate common protein knowledge. After witnessing the great success of pretrained models on diverse protein-related tasks and the fact that recovery is highly correlated with confidence, we wonder whether this knowledge can push the limits of protein design further. As a solution, we propose a knowledge-aware module that refines low-quality residues. We also introduce a memory-retrieval mechanism to save more than 50\% of the training time. We extensively evaluate our proposed method on the CATH, TS50, and TS500 datasets and our results show that our Knowledge-Design method outperforms the previous PiFold method by approximately 9\% on the CATH dataset. Specifically, Knowledge-Design is the first method that achieves 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26550;&#26500;&#20462;&#25913;&#21644;&#26032;&#39062;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#22823;&#22823;&#25913;&#36827;&#20102;&#21516;&#27493;&#24615;&#22522;&#27169;&#22411;&#65292;&#39318;&#27425;&#33719;&#24471;&#20102;&#19968;&#31867;&#21516;&#27493;&#24615;&#22522;&#27169;&#22411;&#65292;&#22312;&#22810;&#29289;&#20307;&#24425;&#33394;&#25968;&#25454;&#38598;&#20013;&#26080;&#30417;&#30563;&#22320;&#21457;&#29616;&#29289;&#20307;&#12290;</title><link>http://arxiv.org/abs/2305.15001</link><description>&lt;p&gt;
&#22797;&#25968;&#20540;&#33258;&#32534;&#30721;&#22120;&#23545;&#29289;&#20307;&#21457;&#29616;&#30340;&#23545;&#27604;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Contrastive Training of Complex-Valued Autoencoders for Object Discovery. (arXiv:2305.15001v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15001
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26550;&#26500;&#20462;&#25913;&#21644;&#26032;&#39062;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#22823;&#22823;&#25913;&#36827;&#20102;&#21516;&#27493;&#24615;&#22522;&#27169;&#22411;&#65292;&#39318;&#27425;&#33719;&#24471;&#20102;&#19968;&#31867;&#21516;&#27493;&#24615;&#22522;&#27169;&#22411;&#65292;&#22312;&#22810;&#29289;&#20307;&#24425;&#33394;&#25968;&#25454;&#38598;&#20013;&#26080;&#30417;&#30563;&#22320;&#21457;&#29616;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#29289;&#20307;&#20013;&#24515;&#27169;&#22411;&#20351;&#29992;&#25554;&#27133;&#21644;&#27880;&#24847;&#21147;&#36335;&#30001;&#36827;&#34892;&#32465;&#23450;&#12290;&#28982;&#32780;&#65292;&#36825;&#31867;&#27169;&#22411;&#26377;&#20960;&#20010;&#27010;&#24565;&#24615;&#30340;&#23616;&#38480;&#24615;&#65306;&#25554;&#27133;&#30340;&#25968;&#37327;&#26159;&#30828;&#32534;&#30721;&#30340;&#65307;&#25152;&#26377;&#25554;&#27133;&#30340;&#23481;&#37327;&#30456;&#31561;&#65307;&#35757;&#32451;&#25104;&#26412;&#39640;&#26114;&#65307;&#25554;&#27133;&#20869;&#27809;&#26377;&#30446;&#26631;&#32423;&#21035;&#30340;&#20851;&#31995;&#22240;&#32032;&#12290;&#21407;&#21017;&#19978;&#65292;&#22522;&#20110;&#21516;&#27493;&#24615;&#30340;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#22797;&#25968;&#20540;&#28608;&#27963;&#22312;&#20854;&#30456;&#20301;&#20998;&#37327;&#20013;&#23384;&#20648;&#32465;&#23450;&#20449;&#24687;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#36825;&#31867;&#22522;&#20110;&#21516;&#27493;&#24615;&#30340;&#27169;&#22411;&#30340;&#24037;&#20316;&#31034;&#20363;&#21482;&#26159;&#26368;&#36817;&#25165;&#26377;&#65292;&#32780;&#19988;&#23454;&#38469;&#19978;&#20173;&#28982;&#38480;&#20110;&#29609;&#20855;&#28784;&#24230;&#25968;&#25454;&#38598;&#30340;&#21516;&#26102;&#23384;&#20648;&#19981;&#21040;&#19977;&#20010;&#29289;&#20307;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#26550;&#26500;&#20462;&#25913;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#26497;&#22823;&#22320;&#25913;&#36827;&#20102;&#26368;&#20808;&#36827;&#30340;&#21516;&#27493;&#24615;&#22522;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#27425;&#33719;&#24471;&#20102;&#19968;&#31867;&#21516;&#27493;&#24615;&#22522;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#22810;&#29289;&#20307;&#24425;&#33394;&#25968;&#25454;&#38598;&#20013;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#21457;&#29616;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current state-of-the-art object-centric models use slots and attention-based routing for binding. However, this class of models has several conceptual limitations: the number of slots is hardwired; all slots have equal capacity; training has high computational cost; there are no object-level relational factors within slots. Synchrony-based models in principle can address these limitations by using complex-valued activations which store binding information in their phase components. However, working examples of such synchrony-based models have been developed only very recently, and are still limited to toy grayscale datasets and simultaneous storage of less than three objects in practice. Here we introduce architectural modifications and a novel contrastive learning method that greatly improve the state-of-the-art synchrony-based model. For the first time, we obtain a class of synchrony-based models capable of discovering objects in an unsupervised manner in multi-object color datasets 
&lt;/p&gt;</description></item><item><title>CMOT&#26159;&#19968;&#31181;&#29992;&#20110;&#36328;&#27169;&#24577;&#35821;&#38899;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#25214;&#21040;&#35821;&#38899;&#21644;&#25991;&#26412;&#24207;&#21015;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#24182;&#22312;&#26631;&#35760;&#32423;&#21035;&#19978;&#28151;&#21512;&#19981;&#21516;&#27169;&#24577;&#30340;&#24207;&#21015;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#38480;&#25968;&#25454;&#19979;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.14635</link><description>&lt;p&gt;
CMOT: &#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#36328;&#27169;&#24577;Mixup&#65292;&#29992;&#20110;&#35821;&#38899;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
CMOT: Cross-modal Mixup via Optimal Transport for Speech Translation. (arXiv:2305.14635v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14635
&lt;/p&gt;
&lt;p&gt;
CMOT&#26159;&#19968;&#31181;&#29992;&#20110;&#36328;&#27169;&#24577;&#35821;&#38899;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#25214;&#21040;&#35821;&#38899;&#21644;&#25991;&#26412;&#24207;&#21015;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#24182;&#22312;&#26631;&#35760;&#32423;&#21035;&#19978;&#28151;&#21512;&#19981;&#21516;&#27169;&#24577;&#30340;&#24207;&#21015;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#38480;&#25968;&#25454;&#19979;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#26159;&#23558;&#28304;&#35821;&#35328;&#20013;&#30340;&#35821;&#38899;&#20449;&#21495;&#32763;&#35793;&#25104;&#30446;&#26631;&#35821;&#35328;&#25991;&#26412;&#30340;&#20219;&#21153;&#12290;&#20316;&#20026;&#19968;&#39033;&#36328;&#27169;&#24577;&#20219;&#21153;&#65292;&#31471;&#21040;&#31471;ST&#22312;&#26377;&#38480;&#25968;&#25454;&#19979;&#36827;&#34892;&#35757;&#32451;&#38750;&#24120;&#22256;&#38590;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#23581;&#35797;&#20174;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#20013;&#36716;&#31227;&#30693;&#35782;&#65292;&#20294;&#20854;&#24615;&#33021;&#30001;&#20110;&#35821;&#38899;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#27169;&#24577;&#24046;&#36317;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Cross-modal Mixup via Optimal Transport&#65288;CMOT&#65289;&#26469;&#20811;&#26381;&#27169;&#24577;&#24046;&#36317;&#12290;&#25105;&#20204;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#25214;&#21040;&#35821;&#38899;&#21644;&#25991;&#26412;&#24207;&#21015;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#28982;&#21518;&#20351;&#29992;&#23545;&#40784;&#22312;&#26631;&#35760;&#32423;&#21035;&#19978;&#28151;&#21512;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#24207;&#21015;&#12290;&#22312;MuST-C ST&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CMOT&#22312;8&#20010;&#32763;&#35793;&#26041;&#21521;&#19978;&#23454;&#29616;&#20102;&#24179;&#22343;BLEU&#20540;&#20026;30.0&#65292;&#36229;&#36807;&#20102;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;CMOT&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#25214;&#21040;&#27169;&#24577;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#26377;&#21161;&#20110;&#32531;&#35299;&#35821;&#38899;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#27169;&#24577;&#24046;&#36317;&#12290;&#20195;&#30721;&#20844;&#24320;&#21487;&#29992;&#20110; https://github.com/ic
&lt;/p&gt;
&lt;p&gt;
End-to-end speech translation (ST) is the task of translating speech signals in the source language into text in the target language. As a cross-modal task, end-to-end ST is difficult to train with limited data. Existing methods often try to transfer knowledge from machine translation (MT), but their performances are restricted by the modality gap between speech and text. In this paper, we propose Cross-modal Mixup via Optimal Transport CMOT to overcome the modality gap. We find the alignment between speech and text sequences via optimal transport and then mix up the sequences from different modalities at a token level using the alignment. Experiments on the MuST-C ST benchmark demonstrate that CMOT achieves an average BLEU of 30.0 in 8 translation directions, outperforming previous methods. Further analysis shows CMOT can adaptively find the alignment between modalities, which helps alleviate the modality gap between speech and text. Code is publicly available at https://github.com/ic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DISCS&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26080;&#38480;&#23398;&#20064;&#19981;&#21516;&#30340;&#36830;&#32493;&#25216;&#33021;&#65292;&#22312;MuJoCo Ant&#26426;&#22120;&#20154;&#25511;&#21046;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#20854;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#20855;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14377</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#22320;&#22312;&#29699;&#38754;&#19978;&#23398;&#20064;&#36830;&#32493;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Discovery of Continuous Skills on a Sphere. (arXiv:2305.14377v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DISCS&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26080;&#38480;&#23398;&#20064;&#19981;&#21516;&#30340;&#36830;&#32493;&#25216;&#33021;&#65292;&#22312;MuJoCo Ant&#26426;&#22120;&#20154;&#25511;&#21046;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#20854;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#20855;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30740;&#31350;&#20102;&#35768;&#22810;&#31181;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#19981;&#21516;&#30340;&#25216;&#33021;&#24110;&#21161;&#26426;&#22120;&#20154;&#20135;&#29983;&#22810;&#26679;&#21270;&#30340;&#34892;&#20026;&#65292;&#32780;&#19981;&#38656;&#35201;&#22806;&#37096;&#22870;&#21169;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#23398;&#20064;&#30340;&#26159;&#26377;&#38480;&#25968;&#37327;&#30340;&#31163;&#25955;&#25216;&#33021;&#65292;&#22240;&#27492;&#23427;&#20204;&#23637;&#29616;&#20986;&#30340;&#34892;&#20026;&#22810;&#26679;&#24615;&#20063;&#24456;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#8220;&#22312;&#29699;&#38754;&#19978;&#23398;&#20064;&#36830;&#32493;&#25216;&#33021;&#30340;&#21457;&#29616;&#8221;&#65292;&#21487;&#20197;&#23398;&#20064;&#26080;&#38480;&#31181;&#19981;&#21516;&#30340;&#25216;&#33021;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#25216;&#33021;&#26159;&#36890;&#36807;&#26368;&#22823;&#21270;&#25216;&#33021;&#21644;&#29366;&#24577;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#23398;&#20064;&#30340;&#65292;&#32780;&#27599;&#31181;&#25216;&#33021;&#37117;&#23545;&#24212;&#20110;&#29699;&#38754;&#19978;&#30340;&#19968;&#20010;&#36830;&#32493;&#20540;&#12290;&#30001;&#20110;&#25216;&#33021;&#22312;DISCS&#20013;&#30340;&#34920;&#31034;&#26159;&#36830;&#32493;&#30340;&#65292;&#25152;&#20197;&#21487;&#20197;&#23398;&#20064;&#26080;&#38480;&#22810;&#31181;&#19981;&#21516;&#30340;&#25216;&#33021;&#12290;&#25105;&#20204;&#23558;&#29616;&#26377;&#30340;&#26041;&#27861;&#21644;DISCS&#24212;&#29992;&#20110;MuJoCo Ant&#26426;&#22120;&#20154;&#25511;&#21046;&#29615;&#22659;&#20013;&#65292;&#24182;&#23637;&#31034;&#20102;DISCS&#21487;&#20197;&#27604;&#20854;&#20182;&#26041;&#27861;&#23398;&#20064;&#21040;&#26356;&#22810;&#30340;&#22810;&#26679;&#21270;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, methods for learning diverse skills to generate various behaviors without external rewards have been actively studied as a form of unsupervised reinforcement learning. However, most of the existing methods learn a finite number of discrete skills, and thus the variety of behaviors that can be exhibited with the learned skills is limited. In this paper, we propose a novel method for learning potentially an infinite number of different skills, which is named discovery of continuous skills on a sphere (DISCS). In DISCS, skills are learned by maximizing mutual information between skills and states, and each skill corresponds to a continuous value on a sphere. Because the representations of skills in DISCS are continuous, infinitely diverse skills could be learned. We examine existing methods and DISCS in the MuJoCo Ant robot control environments and show that DISCS can learn much more diverse skills than the other methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;WSAUC&#65292;&#19968;&#31181;&#35299;&#20915;&#24369;&#30417;&#30563;&#19979;AUC&#20248;&#21270;&#38382;&#39064;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#23427;&#21253;&#25324;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#12289;&#27491;-&#26080;&#26631;&#31614;&#23398;&#20064;&#12289;&#22810;&#23454;&#20363;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#22330;&#26223;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#37096;&#20998;AUC&#8212;&#8212;&#21453;&#36716;&#37096;&#20998;AUC&#65288;rpAUC&#65289;&#65292;&#20316;&#20026;&#40065;&#26834;&#30340;AUC&#26368;&#22823;&#21270;&#35757;&#32451;&#30446;&#26631;&#65292;&#20026;&#21508;&#31181;&#24369;&#30417;&#30563;&#22330;&#26223;&#19979;&#30340;AUC&#20248;&#21270;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.14258</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#19979;AUC&#20248;&#21270;&#65306;&#32479;&#19968;&#30340;&#37096;&#20998;AUC&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised AUC Optimization: A Unified Partial AUC Approach. (arXiv:2305.14258v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;WSAUC&#65292;&#19968;&#31181;&#35299;&#20915;&#24369;&#30417;&#30563;&#19979;AUC&#20248;&#21270;&#38382;&#39064;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#23427;&#21253;&#25324;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#12289;&#27491;-&#26080;&#26631;&#31614;&#23398;&#20064;&#12289;&#22810;&#23454;&#20363;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#22330;&#26223;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#37096;&#20998;AUC&#8212;&#8212;&#21453;&#36716;&#37096;&#20998;AUC&#65288;rpAUC&#65289;&#65292;&#20316;&#20026;&#40065;&#26834;&#30340;AUC&#26368;&#22823;&#21270;&#35757;&#32451;&#30446;&#26631;&#65292;&#20026;&#21508;&#31181;&#24369;&#30417;&#30563;&#22330;&#26223;&#19979;&#30340;AUC&#20248;&#21270;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33719;&#21462;&#23436;&#32654;&#30340;&#30417;&#30563;&#36890;&#24120;&#24456;&#22256;&#38590;&#65292;&#29616;&#23454;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#36890;&#24120;&#38754;&#20020;&#19981;&#20934;&#30830;&#12289;&#19981;&#23436;&#25972;&#25110;&#19981;&#31934;&#30830;&#30340;&#30417;&#30563;&#65292;&#32479;&#31216;&#20026;&#24369;&#30417;&#30563;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;WSAUC&#65292;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#24369;&#30417;&#30563;&#19979;AUC&#20248;&#21270;&#38382;&#39064;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#23427;&#28085;&#30422;&#20102;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#12289;&#27491;-&#26080;&#26631;&#31614;&#23398;&#20064;&#12289;&#22810;&#23454;&#20363;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#22330;&#26223;&#12290;&#22312;WSAUC&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#21508;&#31181;&#24369;&#30417;&#30563;&#22330;&#26223;&#19979;&#30340;AUC&#20248;&#21270;&#38382;&#39064;&#26694;&#26550;&#21270;&#20026;&#26368;&#23567;&#21270;&#21463;&#27745;&#26579;&#38598;&#21512;&#19978;AUC&#39118;&#38505;&#30340;&#24120;&#35265;&#24418;&#24335;&#65292;&#24182;&#35777;&#26126;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#19982;&#30495;&#23454;AUC&#19968;&#33268;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#19968;&#31181;&#26032;&#22411;&#30340;&#37096;&#20998;AUC&#65292;&#21363;&#21453;&#36716;&#37096;&#20998;AUC&#65288;rpAUC&#65289;&#65292;&#23427;&#20316;&#20026;&#40065;&#26834;&#30340;AUC&#26368;&#22823;&#21270;&#35757;&#32451;&#30446;&#26631;&#65292;&#22312;&#23384;&#22312;&#27745;&#26579;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#21457;&#25381;&#20316;&#29992;&#12290;WSAUC&#20026;&#21508;&#31181;&#24369;&#30417;&#30563;&#22330;&#26223;&#19979;&#30340;AUC&#20248;&#21270;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since acquiring perfect supervision is usually difficult, real-world machine learning tasks often confront inaccurate, incomplete, or inexact supervision, collectively referred to as weak supervision. In this work, we present WSAUC, a unified framework for weakly supervised AUC optimization problems, which covers noisy label learning, positive-unlabeled learning, multi-instance learning, and semi-supervised learning scenarios. Within the WSAUC framework, we first frame the AUC optimization problems in various weakly supervised scenarios as a common formulation of minimizing the AUC risk on contaminated sets, and demonstrate that the empirical risk minimization problems are consistent with the true AUC. Then, we introduce a new type of partial AUC, specifically, the reversed partial AUC (rpAUC), which serves as a robust training objective for AUC maximization in the presence of contaminated labels. WSAUC offers a universal solution for AUC optimization in various weakly supervised scena
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#31639;&#27861;&#19981;&#20844;&#24179;&#24615;&#30340;&#31034;&#20363;&#65292;&#24182;&#20174;&#27431;&#30431;&#38750;&#27495;&#35270;&#27861;&#30340;&#35282;&#24230;&#35762;&#36848;&#20102;&#20854;&#20013;&#28041;&#21450;&#30340;&#19981;&#20844;&#27491;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#24314;&#31435;&#20102;&#26694;&#26550;&#20197;&#24110;&#21161;&#20915;&#31574;&#32773;&#30830;&#23450;&#31639;&#27861;&#20844;&#24179;&#24615;&#25351;&#26631;&#20197;&#31526;&#21512;&#27431;&#30431;&#38750;&#27495;&#35270;&#27861;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.13938</link><description>&lt;p&gt;
&#36890;&#36807;&#27431;&#30431;&#38750;&#27495;&#35270;&#27861;&#30340;&#35270;&#35282;&#35762;&#36848;&#31639;&#27861;&#19981;&#20844;&#24179;&#24615;&#65306;&#25110;&#35859;&#27861;&#24459;&#38750;&#20915;&#31574;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Unfairness through the Lens of EU Non-Discrimination Law: Or Why the Law is not a Decision Tree. (arXiv:2305.13938v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#31639;&#27861;&#19981;&#20844;&#24179;&#24615;&#30340;&#31034;&#20363;&#65292;&#24182;&#20174;&#27431;&#30431;&#38750;&#27495;&#35270;&#27861;&#30340;&#35282;&#24230;&#35762;&#36848;&#20102;&#20854;&#20013;&#28041;&#21450;&#30340;&#19981;&#20844;&#27491;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#24314;&#31435;&#20102;&#26694;&#26550;&#20197;&#24110;&#21161;&#20915;&#31574;&#32773;&#30830;&#23450;&#31639;&#27861;&#20844;&#24179;&#24615;&#25351;&#26631;&#20197;&#31526;&#21512;&#27431;&#30431;&#38750;&#27495;&#35270;&#27861;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#32654;&#24863;&#21040;&#19981;&#20844;&#24179;&#21644;&#27495;&#35270;&#30340;&#38382;&#39064;&#26368;&#36817;&#24341;&#36215;&#20102;&#27861;&#24459;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#23398;&#32773;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#31639;&#27861;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#20197;&#21450;&#27861;&#24459;&#19978;&#30340;&#27495;&#35270;&#21644;&#24179;&#31561;&#27010;&#24565;&#20043;&#38388;&#30340;&#37325;&#21472;&#31243;&#24230;&#36890;&#24120;&#19981;&#28165;&#26970;&#65292;&#23548;&#33268;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#27861;&#24459;&#20043;&#38388;&#30340;&#35823;&#35299;&#12290;&#26412;&#25991;&#26088;&#22312;&#38416;&#26126;&#27431;&#30431;&#38750;&#27495;&#35270;&#27861;&#19982;&#35745;&#31639;&#26426;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#27010;&#24565;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#37325;&#21512;&#20197;&#21450;&#23427;&#20204;&#30340;&#21306;&#21035;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#22914;&#19979;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#27431;&#30431;&#26696;&#20363;&#27861;&#30340;&#35282;&#24230;&#26469;&#20998;&#26512;&#31639;&#27861;&#19981;&#20844;&#24179;&#30340;&#20856;&#22411;&#20363;&#23376;&#65292;&#25214;&#20986;&#19982;&#27431;&#30431;&#26696;&#20363;&#27861;&#30340;&#31867;&#27604;&#20043;&#22788;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20197;&#24110;&#21161;&#20915;&#31574;&#32773;&#30830;&#23450;&#31639;&#27861;&#21644;AI&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#25351;&#26631;&#65292;&#20197;&#30830;&#20445;&#23427;&#20204;&#31526;&#21512;&#27431;&#30431;&#30340;&#38750;&#27495;&#35270;&#27861;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concerns regarding unfairness and discrimination in the context of artificial intelligence (AI) systems have recently received increased attention from both legal and computer science scholars. Yet, the degree of overlap between notions of algorithmic bias and fairness on the one hand, and legal notions of discrimination and equality on the other, is often unclear, leading to misunderstandings between computer science and law. What types of bias and unfairness does the law address when it prohibits discrimination? What role can fairness metrics play in establishing legal compliance? In this paper, we aim to illustrate to what extent European Union (EU) non-discrimination law coincides with notions of algorithmic fairness proposed in computer science literature and where they differ. The contributions of this paper are as follows. First, we analyse seminal examples of algorithmic unfairness through the lens of EU non-discrimination law, drawing parallels with EU case law. Second, we set
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36235;&#21183;&#30340;&#38646;-shot&#26463;&#27969;&#25511;&#21046;&#26041;&#27861;&#65292;&#22312; CAFe II &#21644; LPI &#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#21487;&#20197;&#23558;&#26657;&#27491;&#26102;&#38388;&#32553;&#30701;&#21040;&#20154;&#31867;&#19987;&#23478;&#25152;&#38656;&#26102;&#38388;&#30340;&#21313;&#20998;&#20043;&#19968;&#12290;</title><link>http://arxiv.org/abs/2305.13869</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#36235;&#21183;&#30340;&#36229;&#23548;&#32447;&#24615;&#21152;&#36895;&#22120;&#38646;-shot&#26463;&#27969;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Trend-Based SAC Beam Control Method with Zero-Shot in Superconducting Linear Accelerator. (arXiv:2305.13869v1 [physics.acc-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36235;&#21183;&#30340;&#38646;-shot&#26463;&#27969;&#25511;&#21046;&#26041;&#27861;&#65292;&#22312; CAFe II &#21644; LPI &#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#21487;&#20197;&#23558;&#26657;&#27491;&#26102;&#38388;&#32553;&#30701;&#21040;&#20154;&#31867;&#19987;&#23478;&#25152;&#38656;&#26102;&#38388;&#30340;&#21313;&#20998;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#23548;&#32447;&#24615;&#21152;&#36895;&#22120;&#26159;&#29616;&#20195;&#31185;&#23398;&#30740;&#31350;&#30340;&#39640;&#24230;&#28789;&#27963;&#30340;&#35774;&#26045;&#65292;&#38656;&#35201;&#27599;&#21608;&#37325;&#26032;&#37197;&#32622;&#21644;&#35843;&#25972;&#12290;&#22240;&#27492;&#65292;&#26368;&#23567;&#21270;&#35774;&#32622;&#26102;&#38388;&#23545;&#20110;&#25552;&#20379;&#20805;&#36275;&#30340;&#23454;&#39564;&#26102;&#38388;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36235;&#21183;&#30340;&#36719; actor-critic(TBSAC)&#26463;&#27969;&#25511;&#21046;&#26041;&#27861;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#20801;&#35768;&#20195;&#29702;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#30452;&#25509;&#24212;&#29992;&#20110;&#30495;&#27491;&#30340;&#21152;&#36895;&#22120;&#20013;&#65292;&#23454;&#29616;&#20102;&#38646;-shot&#25511;&#21046;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20998;&#21035;&#22312;&#20013;&#22269;&#36229;&#37325;&#20803;&#32032;&#21152;&#36895;&#22120;&#35774;&#26045;(CAFe II)&#21644;&#19968;&#20010;&#36731;&#36136;&#31890;&#23376;&#27880;&#20837;&#22120;(LPI)&#20013;&#25191;&#34892;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#20856;&#22411;&#26463;&#27969;&#25511;&#21046;&#20219;&#21153;&#12290;&#22312;CAFe II&#30340;&#19977;&#20010;&#20302;&#28201;&#27169;&#22359;&#20013;&#20998;&#21035;&#25191;&#34892;&#20102;&#36712;&#36947;&#26657;&#27491;&#20219;&#21153;&#65292;&#35843;&#35856;&#25152;&#38656;&#26102;&#38388;&#24050;&#32463;&#20943;&#23569;&#21040;&#20154;&#31867;&#19987;&#23478;&#25152;&#38656;&#26102;&#38388;&#30340;&#21313;&#20998;&#20043;&#19968;&#65292;&#26657;&#27491;&#21518;&#30340;RMS&#20540;&#37117;&#23567;&#20110;1&#27627;&#31859;&#12290;&#21478;&#19968;&#20010;&#20256;&#36755;&#25928;&#29575;&#20248;&#21270;&#20219;&#21153;&#22312;CAFe II&#30340;&#21152;&#36895;&#22120;&#27573;LPI&#20013;&#36827;&#34892;&#20102;
&lt;/p&gt;
&lt;p&gt;
The superconducting linear accelerator is a highly flexiable facility for modern scientific discoveries, necessitating weekly reconfiguration and tuning. Accordingly, minimizing setup time proves essential in affording users with ample experimental time. We propose a trend-based soft actor-critic(TBSAC) beam control method with strong robustness, allowing the agents to be trained in a simulated environment and applied to the real accelerator directly with zero-shot. To validate the effectiveness of our method, two different typical beam control tasks were performed on China Accelerator Facility for Superheavy Elements (CAFe II) and a light particle injector(LPI) respectively. The orbit correction tasks were performed in three cryomodules in CAFe II seperately, the time required for tuning has been reduced to one-tenth of that needed by human experts, and the RMS values of the corrected orbit were all less than 1mm. The other transmission efficiency optimization task was conducted in th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EXACT&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23433;&#20840;&#22320;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#36827;&#34892;&#26799;&#24230;&#20132;&#25442;&#65292;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#12289;&#20445;&#25345;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.12997</link><description>&lt;p&gt;
EXACT&#65306;&#29992;&#20110;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#20840;&#38754;&#25915;&#20987;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EXACT: Extensive Attack for Split Learning. (arXiv:2305.12997v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EXACT&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23433;&#20840;&#22320;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#36827;&#34892;&#26799;&#24230;&#20132;&#25442;&#65292;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#12289;&#20445;&#25345;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#35757;&#32451;&#21644;&#37096;&#32626;&#21033;&#29992;&#31169;&#20154;&#20449;&#24687;&#30340;&#27169;&#22411;&#12290;&#22312;&#35774;&#22791;&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#20351;&#25105;&#20204;&#22312;&#25512;&#26029;&#26399;&#38388;&#23436;&#20840;&#36991;&#20813;&#19982;&#31532;&#19977;&#26041;&#26381;&#21153;&#22120;&#20849;&#20139;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#19982;&#26381;&#21153;&#22120;&#31471;&#30456;&#27604;&#65292;&#35774;&#22791;&#19978;&#30340;&#27169;&#22411;&#36890;&#24120;&#36739;&#19981;&#20934;&#30830;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#21482;&#20381;&#36182;&#20110;&#19968;&#23567;&#32452;&#35774;&#22791;&#29305;&#24449;&#19988;&#38656;&#35201;&#36275;&#22815;&#23567;&#25165;&#33021;&#22312;&#32456;&#31471;&#29992;&#25143;&#35774;&#22791;&#19978;&#39640;&#25928;&#36816;&#34892;&#12290;&#20998;&#24067;&#24335;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#65292;&#23558;&#19968;&#20010;&#22823;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#25104;&#20004;&#37096;&#20998;&#65292;&#22823;&#37096;&#20998;&#20301;&#20110;&#26381;&#21153;&#22120;&#31471;&#65292;&#23567;&#37096;&#20998;&#22312;&#35774;&#22791;&#19978;&#25191;&#34892;&#65292;&#26088;&#22312;&#25972;&#21512;&#31169;&#26377;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#38656;&#35201;&#22312;&#20998;&#30028;&#22788;&#20132;&#25442;&#26799;&#24230;&#65292;&#36825;&#21487;&#33021;&#32534;&#30721;&#31169;&#26377;&#29305;&#24449;&#25110;&#26631;&#31614;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; EXACT&#65288;Extensive Attack for Split Learning&#65289;&#30340;&#26032;&#39062;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24191;&#27867;&#30340;&#22122;&#22768;&#23454;&#29616;&#23433;&#20840;&#30340;&#26799;&#24230;&#20132;&#25442;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#22312;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving machine learning (PPML) can help us train and deploy models that utilize private information. In particular, on-device Machine Learning allows us to completely avoid sharing information with a third-party server during inference. However, on-device models are typically less accurate when compared to the server counterparts due to the fact that (1) they typically only rely on a small set of on-device features and (2) they need to be small enough to run efficiently on end-user devices. Split Learning (SL) is a promising approach that can overcome these limitations. In SL, a large machine learning model is divided into two parts, with the bigger part residing on the server-side and a smaller part executing on-device, aiming to incorporate the private features. However, end-to-end training of such models requires exchanging gradients at the cut layer, which might encode private features or labels. In this paper, we provide insights into potential privacy risks associated
&lt;/p&gt;</description></item><item><title>FIT&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#23558;&#25968;&#25454;&#26631;&#35760;&#20998;&#32452;&#65292;&#20351;&#29992;&#23616;&#37096;&#23618;&#21644;&#20840;&#23616;&#23618;&#36827;&#34892;&#25805;&#20316;&#12290;&#36890;&#36807;&#20132;&#38169;&#20351;&#29992;&#36825;&#20123;&#23618;&#24182;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#20419;&#36827;&#20449;&#24687;&#20132;&#25442;&#65292;FIT&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#20013;&#22343;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.12689</link><description>&lt;p&gt;
FIT&#65306;&#36828;&#31243;&#20132;&#38169;Transformer
&lt;/p&gt;
&lt;p&gt;
FIT: Far-reaching Interleaved Transformers. (arXiv:2305.12689v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12689
&lt;/p&gt;
&lt;p&gt;
FIT&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#23558;&#25968;&#25454;&#26631;&#35760;&#20998;&#32452;&#65292;&#20351;&#29992;&#23616;&#37096;&#23618;&#21644;&#20840;&#23616;&#23618;&#36827;&#34892;&#25805;&#20316;&#12290;&#36890;&#36807;&#20132;&#38169;&#20351;&#29992;&#36825;&#20123;&#23618;&#24182;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#20419;&#36827;&#20449;&#24687;&#20132;&#25442;&#65292;FIT&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#20013;&#22343;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;FIT&#65306;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#20855;&#26377;&#39640;&#25928;&#30340;&#33258;&#25105;&#27880;&#24847;&#21147;&#21644;&#33258;&#36866;&#24212;&#35745;&#31639;&#33021;&#21147;&#12290;&#19982;&#21407;&#22987;Transformer&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#23558;&#25968;&#25454;&#26631;&#35760;&#20998;&#25104;&#32452;&#65292;&#27599;&#20010;&#32452;&#26159;&#19968;&#20010;&#36739;&#30701;&#30340;&#26631;&#35760;&#24207;&#21015;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#31181;&#31867;&#22411;&#30340;Transformer&#23618;&#65306;&#23616;&#37096;&#23618;&#22312;&#27599;&#20010;&#32452;&#20869;&#25805;&#20316;&#25968;&#25454;&#26631;&#35760;&#65292;&#32780;&#20840;&#23616;&#23618;&#22312;&#19968;&#20010;&#26356;&#23567;&#30340;&#24341;&#20837;&#30340;&#28508;&#22312;&#26631;&#35760;&#38598;&#21512;&#19978;&#25805;&#20316;&#12290;&#36825;&#20123;&#23618;&#21253;&#25324;&#19982;&#26631;&#20934;Transformer&#30456;&#21516;&#30340;&#33258;&#25105;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#23618;&#65292;&#34987;&#20132;&#38169;&#20351;&#29992;&#65292;&#20132;&#21449;&#27880;&#24847;&#21147;&#29992;&#20110;&#22312;&#21516;&#19968;&#32452;&#20869;&#25968;&#25454;&#21644;&#28508;&#22312;&#26631;&#35760;&#20043;&#38388;&#20419;&#36827;&#20449;&#24687;&#20132;&#25442;&#12290;&#27599;&#20010;&#22823;&#23567;&#20026;n&#30340;&#32452;&#20869;&#30340;&#27880;&#24847;&#21147;&#22797;&#26434;&#24230;&#20026;$O(n^2)$&#65292;&#20294;&#23545;&#20110;&#38271;&#24230;&#20026;L&#30340;&#24207;&#21015;&#65292;&#21487;&#20197;&#22312;&#20840;&#23616;&#33539;&#22260;&#20869;&#36798;&#21040;$O(L^{{4}/{3}})$&#12290;&#36890;&#36807;&#26356;&#22810;&#22320;&#20381;&#36182;&#25191;&#34892;&#20351;&#29992;&#26356;&#23567;&#28508;&#22312;&#26631;&#35760;&#38598;&#21512;&#30340;&#20840;&#23616;&#23618;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#25928;&#29575;&#12290;FIT&#26159;&#19968;&#31181;&#22810;&#29992;&#36884;&#30340;&#26550;&#26500;&#65292;&#21487;&#24212;&#29992;&#20110;&#24191;&#27867;&#30340;&#20219;&#21153;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#35821;&#35328;&#24314;&#27169;&#21644;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present FIT: a transformer-based architecture with efficient self-attention and adaptive computation. Unlike original transformers, which operate on a single sequence of data tokens, we divide the data tokens into groups, with each group being a shorter sequence of tokens. We employ two types of transformer layers: local layers operate on data tokens within each group, while global layers operate on a smaller set of introduced latent tokens. These layers, comprising the same set of self-attention and feed-forward layers as standard transformers, are interleaved, and cross-attention is used to facilitate information exchange between data and latent tokens within the same group. The attention complexity is $O(n^2)$ locally within each group of size $n$, but can reach $O(L^{{4}/{3}})$ globally for sequence length of $L$. The efficiency can be further enhanced by relying more on global layers that perform adaptive computation using a smaller set of latent tokens. FIT is a versatile arch
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#26469;&#30740;&#31350;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#65292;&#21363;&#39532;&#23572;&#21487;&#22827; $\alpha$-&#21183;&#21338;&#24328;&#12290;&#20171;&#32461;&#20102;&#20004;&#31181;&#31639;&#27861;&#26469;&#35745;&#31639;&#20854;&#20013;&#30340;&#32435;&#20160;&#22343;&#34913;&#65292;&#19988;&#34920;&#26126;&#36825;&#20123;&#31639;&#27861;&#33021;&#22815;&#25214;&#21040;&#36817;&#20284;&#22343;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.12553</link><description>&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;$\alpha$-&#21183;&#21338;&#24328;:&#22343;&#34913;&#36817;&#20284;&#19982;&#36951;&#25022;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Markov $\alpha$-Potential Games: Equilibrium Approximation and Regret Analysis. (arXiv:2305.12553v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#26469;&#30740;&#31350;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#65292;&#21363;&#39532;&#23572;&#21487;&#22827; $\alpha$-&#21183;&#21338;&#24328;&#12290;&#20171;&#32461;&#20102;&#20004;&#31181;&#31639;&#27861;&#26469;&#35745;&#31639;&#20854;&#20013;&#30340;&#32435;&#20160;&#22343;&#34913;&#65292;&#19988;&#34920;&#26126;&#36825;&#20123;&#31639;&#27861;&#33021;&#22815;&#25214;&#21040;&#36817;&#20284;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30740;&#31350;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#22810;&#20195;&#29702;&#20132;&#20114;&#30340;&#26032;&#26694;&#26550;:&#39532;&#23572;&#21487;&#22827;$\alpha$-&#21183;&#21338;&#24328;&#12290;&#39532;&#23572;&#21487;&#22827;&#21183;&#21338;&#24328;&#26159;&#39532;&#23572;&#21487;&#22827; $\alpha$-&#21183;&#21338;&#24328;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#39532;&#23572;&#21487;&#22827;&#25317;&#22581;&#21338;&#24328;&#21644;&#25200;&#21160;&#39532;&#23572;&#21487;&#22827;&#22242;&#38431;&#21338;&#24328;&#26159;&#20004;&#20010;&#37325;&#35201;&#19988;&#23454;&#38469;&#24847;&#20041;&#37325;&#22823;&#30340;&#21338;&#24328;&#31867;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#20004;&#20010;&#21338;&#24328;&#30340;$\alpha$-&#21183;&#20989;&#25968;&#65292;&#24182;&#38024;&#23545;&#21338;&#24328;&#21442;&#25968;&#34920;&#24449;&#20102;&#24046;&#36317; $\alpha$&#12290;&#24341;&#20837;&#20102;&#20004;&#31181;&#31639;&#27861;&#8212;&#8212;&#25237;&#24433;&#26799;&#24230;&#19978;&#21319;&#31639;&#27861;&#21644;&#39034;&#24207;&#26368;&#22823;&#25913;&#36827;&#24179;&#28369;&#26368;&#20339;&#21453;&#24212;&#21160;&#24577;&#8212;&#8212;&#26469;&#36817;&#20284;&#35745;&#31639;&#39532;&#23572;&#21487;&#22827;$\alpha$-&#21183;&#21338;&#24328;&#20013;&#30340;&#31283;&#24577;&#32435;&#20160;&#22343;&#34913;&#12290;&#27599;&#20010;&#31639;&#27861;&#30340;&#32435;&#20160;&#36951;&#25022;&#37117;&#26174;&#31034;&#20026;&#26102;&#38388;&#36328;&#24230;&#30340;&#20122;&#32447;&#24615;&#32553;&#25918;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#31616;&#21333;&#30340;&#31639;&#27861;&#33021;&#22815;&#25214;&#21040;&#39532;&#23572;&#21487;&#22827;$\alpha$-&#21183;&#21338;&#24328;&#30340;&#36817;&#20284;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new framework to study multi-agent interaction in Markov games: Markov $\alpha$-potential games. Markov potential games are special cases of Markov $\alpha$-potential games, so are two important and practically significant classes of games: Markov congestion games and perturbed Markov team games. In this paper, {$\alpha$-potential} functions for both games are provided and the gap $\alpha$ is characterized with respect to game parameters. Two algorithms -- the projected gradient-ascent algorithm and the sequential maximum improvement smoothed best response dynamics -- are introduced for approximating the stationary Nash equilibrium in Markov $\alpha$-potential games. The Nash-regret for each algorithm is shown to scale sub-linearly in time horizon. Our analysis and numerical experiments demonstrates that simple algorithms are capable of finding approximate equilibrium in Markov $\alpha$-potential games.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38647;&#36798;&#36870;&#21521;&#20256;&#24863;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#31232;&#30095;&#38647;&#36798;&#26816;&#27979;&#26144;&#23556;&#21040;&#26497;&#22352;&#26631;&#27979;&#37327;&#32593;&#26684;&#65292;&#24182;&#29983;&#25104;&#21160;&#24577;&#32593;&#26684;&#22320;&#22270;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#25163;&#24037;&#21046;&#20316;&#30340;&#20960;&#20309;ISM&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20026;&#20174;&#26377;&#38480;&#35270;&#22330;&#30340;&#38647;&#36798;&#20013;&#23398;&#20064;&#26497;&#22352;&#26631;&#26041;&#26696;&#30340;&#21333;&#24103;&#27979;&#37327;&#32593;&#26684;&#30340;&#31532;&#19968;&#20010;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.12409</link><description>&lt;p&gt;
&#21160;&#24577;&#21344;&#25454;&#32593;&#26684;&#22320;&#22270;&#30340;&#28145;&#24230;&#38647;&#36798;&#36870;&#21521;&#20256;&#24863;&#22120;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Deep Radar Inverse Sensor Models for Dynamic Occupancy Grid Maps. (arXiv:2305.12409v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12409
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38647;&#36798;&#36870;&#21521;&#20256;&#24863;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#31232;&#30095;&#38647;&#36798;&#26816;&#27979;&#26144;&#23556;&#21040;&#26497;&#22352;&#26631;&#27979;&#37327;&#32593;&#26684;&#65292;&#24182;&#29983;&#25104;&#21160;&#24577;&#32593;&#26684;&#22320;&#22270;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#25163;&#24037;&#21046;&#20316;&#30340;&#20960;&#20309;ISM&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20026;&#20174;&#26377;&#38480;&#35270;&#22330;&#30340;&#38647;&#36798;&#20013;&#23398;&#20064;&#26497;&#22352;&#26631;&#26041;&#26696;&#30340;&#21333;&#24103;&#27979;&#37327;&#32593;&#26684;&#30340;&#31532;&#19968;&#20010;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#30340;&#19968;&#20010;&#37325;&#35201;&#27493;&#39588;&#26159;&#22522;&#20110;&#20256;&#24863;&#22120;&#36755;&#20837;&#23545;&#36710;&#36742;&#29615;&#22659;&#36827;&#34892;&#24314;&#27169;&#12290;&#30001;&#20110;&#20854;&#20247;&#25152;&#21608;&#30693;&#30340;&#20248;&#21183;&#65292;&#38647;&#36798;&#25104;&#20026;&#25512;&#26029;&#22260;&#32469;&#36710;&#36742;&#30340;&#32593;&#26684;&#21333;&#20803;&#21344;&#29992;&#29366;&#24577;&#30340;&#27969;&#34892;&#36873;&#25321;&#12290;&#20026;&#20102;&#35299;&#20915;&#38647;&#36798;&#26816;&#27979;&#25968;&#25454;&#31232;&#30095;&#24615;&#21644;&#22122;&#22768;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36870;&#21521;&#20256;&#24863;&#22120;&#27169;&#22411;&#65288;ISM&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#20174;&#31232;&#30095;&#38647;&#36798;&#26816;&#27979;&#21040;&#26497;&#22352;&#26631;&#27979;&#37327;&#32593;&#26684;&#30340;&#26144;&#23556;&#12290;&#25913;&#36827;&#30340;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#27979;&#37327;&#30340;&#32593;&#26684;&#29992;&#20316;&#21442;&#32771;&#12290;&#23398;&#20064;&#21040;&#30340;&#38647;&#36798;&#27979;&#37327;&#32593;&#26684;&#19982;&#38647;&#36798;&#22810;&#26222;&#21202;&#36895;&#24230;&#27979;&#37327;&#30456;&#32467;&#21512;&#65292;&#36827;&#19968;&#27493;&#29992;&#20110;&#29983;&#25104;&#21160;&#24577;&#32593;&#26684;&#22320;&#22270;&#65288;DGM&#65289;&#12290;&#22312;&#23454;&#38469;&#30340;&#39640;&#36895;&#20844;&#36335;&#24773;&#26223;&#23454;&#39564;&#20013;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#25163;&#24037;&#21046;&#20316;&#30340;&#20960;&#20309;ISM&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#31532;&#19968;&#20010;&#20174;&#26377;&#38480;&#35270;&#22330;&#65288;FOV&#65289;&#30340;&#38647;&#36798;&#20013;&#23398;&#20064;&#26497;&#22352;&#26631;&#26041;&#26696;&#30340;&#21333;&#24103;&#27979;&#37327;&#32593;&#26684;&#30340;&#26041;&#27861;&#12290;&#23398;&#20064;&#26694;&#26550;&#20351;&#23398;&#20064;&#21040;&#30340;ISM&#21487;&#20197;&#30452;&#25509;&#23884;&#20837;&#21040;&#29616;&#26377;&#30340;&#36125;&#21494;&#26031;&#29366;&#24577;&#20272;&#35745;&#26041;&#26696;&#20013;&#65292;&#20197;&#25552;&#39640;&#29615;&#22659;&#24314;&#27169;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To implement autonomous driving, one essential step is to model the vehicle environment based on the sensor inputs. Radars, with their well-known advantages, became a popular option to infer the occupancy state of grid cells surrounding the vehicle. To tackle data sparsity and noise of radar detections, we propose a deep learning-based Inverse Sensor Model (ISM) to learn the mapping from sparse radar detections to polar measurement grids. Improved lidar-based measurement grids are used as reference. The learned radar measurement grids, combined with radar Doppler velocity measurements, are further used to generate a Dynamic Grid Map (DGM). Experiments in real-world highway scenarios show that our approach outperforms the hand-crafted geometric ISMs. In comparison to state-of-the-art deep learning methods, our approach is the first one to learn a single-frame measurement grid in the polar scheme from radars with a limited Field Of View (FOV). The learning framework makes the learned ISM
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;EEG&#21644;EMG&#35760;&#24405;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#35760;&#24405;&#26469;&#33258;&#34987;&#21327;&#21161;&#20351;&#29992;&#20027;&#21160;&#30699;&#24418;&#22120;&#31227;&#21160;&#21491;&#33218;&#30340;&#21463;&#35797;&#32773;&#12290;&#30740;&#31350;&#32773;&#25925;&#24847;&#24341;&#20837;&#20102;&#19968;&#20123;&#38169;&#35823;&#65292;&#20197;&#26816;&#27979;&#26032;&#30340;&#38169;&#35823;&#26816;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.11996</link><description>&lt;p&gt;
EEG&#21644;EMG&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#26816;&#27979;&#20027;&#21160;&#30699;&#24418;&#22120;&#24341;&#20837;&#30340;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
EEG and EMG dataset for the detection of errors introduced by an active orthosis device. (arXiv:2305.11996v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11996
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;EEG&#21644;EMG&#35760;&#24405;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#35760;&#24405;&#26469;&#33258;&#34987;&#21327;&#21161;&#20351;&#29992;&#20027;&#21160;&#30699;&#24418;&#22120;&#31227;&#21160;&#21491;&#33218;&#30340;&#21463;&#35797;&#32773;&#12290;&#30740;&#31350;&#32773;&#25925;&#24847;&#24341;&#20837;&#20102;&#19968;&#20123;&#38169;&#35823;&#65292;&#20197;&#26816;&#27979;&#26032;&#30340;&#38169;&#35823;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21253;&#21547;&#30005;&#33041;&#22270;&#35889;&#65288;EEG&#65289;&#21644;&#32908;&#30005;&#22270;&#65288;EMG&#65289;&#35760;&#24405;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#35760;&#24405;&#26469;&#33258;8&#21517;&#34987;&#21327;&#21161;&#20351;&#29992;&#20027;&#21160;&#30699;&#24418;&#22120;&#31227;&#21160;&#21491;&#33218;&#30340;&#21463;&#35797;&#32773;&#12290;&#35813;&#35774;&#22791;&#25903;&#25345;&#32920;&#20851;&#33410;&#36816;&#21160;&#65292;&#21363;&#21491;&#33218;&#30340;&#23624;&#26354;&#21644;&#20280;&#23637;&#12290;&#24403;&#30699;&#24418;&#22120;&#20027;&#21160;&#31227;&#21160;&#34987;&#35797;&#32773;&#30340;&#25163;&#33218;&#26102;&#65292;&#25925;&#24847;&#24341;&#20837;&#20102;&#19968;&#20123;&#38169;&#35823;&#65292;&#25345;&#32493;&#26102;&#38388;&#24456;&#30701;&#12290;&#22312;&#27492;&#26399;&#38388;&#65292;&#30699;&#24418;&#22120;&#21521;&#30456;&#21453;&#30340;&#26041;&#21521;&#31227;&#21160;&#12290;&#26412;&#25991;&#35299;&#37322;&#20102;&#23454;&#39564;&#35774;&#32622;&#65292;&#24182;&#23637;&#31034;&#20102;&#36328;&#25152;&#26377;&#21463;&#35797;&#32773;&#30340;&#19968;&#20123;&#34892;&#20026;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20026;&#19968;&#20010;&#21463;&#35797;&#32773;&#25552;&#20379;&#20102;&#24179;&#22343;&#20107;&#20214;&#30456;&#20851;&#30005;&#20301;&#20998;&#26512;&#65292;&#20197;&#25552;&#20379;&#20851;&#20110;&#25968;&#25454;&#36136;&#37327;&#21644;&#38169;&#35823;&#24341;&#20837;&#25152;&#24341;&#36215;&#30340;EEG&#27963;&#21160;&#30340;&#35265;&#35299;&#12290;&#22312;&#27492;&#20171;&#32461;&#30340;&#25968;&#25454;&#38598;&#26159;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#20026;&#30740;&#31350;&#31038;&#21306;&#25552;&#20379;&#25968;&#25454;&#38598;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#24322;&#27493;&#26816;&#27979;&#38169;&#35823;&#30340;&#26032;&#26041;&#27861;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a dataset containing recordings of the electroencephalogram (EEG) and the electromyogram (EMG) from eight subjects who were assisted in moving their right arm by an active orthosis device. The supported movements were elbow joint movements, i.e., flexion and extension of the right arm. While the orthosis was actively moving the subject's arm, some errors were deliberately introduced for a short duration of time. During this time, the orthosis moved in the opposite direction. In this paper, we explain the experimental setup and present some behavioral analyses across all subjects. Additionally, we present an average event-related potential analysis for one subject to offer insights into the data quality and the EEG activity caused by the error introduction. The dataset described herein is openly accessible. The aim of this study was to provide a dataset to the research community, particularly for the development of new methods in the asynchronous detection of erroneo
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;ChatGPT&#19982;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#30340;&#27604;&#36739;&#65292;&#35748;&#20026;&#20840;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#21487;&#20197;&#24110;&#21161;&#22238;&#31572;&#22522;&#20110;AI&#30340;&#35745;&#31639;&#26159;&#21542;&#21487;&#20197;&#25552;&#39640;&#29983;&#20135;&#21147;&#29978;&#33267;&#21462;&#20195;&#36719;&#20214;&#24037;&#31243;&#24072;&#36827;&#34892;&#36719;&#20214;&#24320;&#21457;&#30340;&#20105;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.11837</link><description>&lt;p&gt;
&#20351;&#29992;ChatGPT&#27604;&#36739;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#65306;&#19968;&#39033;&#23454;&#35777;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Comparing Software Developers with ChatGPT: An Empirical Investigation. (arXiv:2305.11837v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11837
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;ChatGPT&#19982;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#30340;&#27604;&#36739;&#65292;&#35748;&#20026;&#20840;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#21487;&#20197;&#24110;&#21161;&#22238;&#31572;&#22522;&#20110;AI&#30340;&#35745;&#31639;&#26159;&#21542;&#21487;&#20197;&#25552;&#39640;&#29983;&#20135;&#21147;&#29978;&#33267;&#21462;&#20195;&#36719;&#20214;&#24037;&#31243;&#24072;&#36827;&#34892;&#36719;&#20214;&#24320;&#21457;&#30340;&#20105;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#22312;&#36719;&#20214;&#24037;&#31243;&#65288;SE&#65289;&#20013;&#30340;&#24212;&#29992;&#24050;&#20174;&#29702;&#35770;&#21040;&#29616;&#23454;&#20013;&#36716;&#21464;&#12290;&#35768;&#22810;&#23398;&#26415;&#25991;&#31456;&#35760;&#24405;&#20102;&#20154;&#24037;&#26234;&#33021;&#25104;&#21151;&#24212;&#29992;&#20110;&#39033;&#30446;&#31649;&#29702;&#12289;&#24314;&#27169;&#12289;&#27979;&#35797;&#21644;&#24320;&#21457;&#31561;&#39046;&#22495;&#30340;&#26696;&#20363;&#12290;&#26368;&#36817;&#30340;&#21019;&#26032;&#26159;&#24341;&#20837;&#20102;ChatGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#34987;&#21561;&#25447;&#20026;&#33021;&#22815;&#29983;&#25104;&#32534;&#31243;&#20195;&#30721;&#21644;&#20026;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#36719;&#20214;&#27979;&#35797;&#31574;&#30053;&#30340;&#26426;&#22120;&#23398;&#20064;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#34429;&#28982;&#26377;&#29468;&#27979;&#35748;&#20026;&#22522;&#20110;AI&#30340;&#35745;&#31639;&#21487;&#20197;&#25552;&#39640;&#29983;&#20135;&#21147;&#29978;&#33267;&#21462;&#20195;&#36719;&#20214;&#24037;&#31243;&#24072;&#36827;&#34892;&#36719;&#20214;&#24320;&#21457;&#65292;&#20294;&#30446;&#21069;&#32570;&#20047;&#23454;&#35777;&#35777;&#25454;&#26469;&#39564;&#35777;&#36825;&#19968;&#28857;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;AI&#31995;&#32479;&#30340;&#20027;&#35201;&#37325;&#28857;&#22312;&#20110;&#22686;&#24378;&#20934;&#30830;&#24615;&#65292;&#20294;&#38750;&#21151;&#33021;&#24615;&#35201;&#27714;&#21253;&#25324;&#33021;&#28304;&#25928;&#29575;&#12289;&#28431;&#27934;&#12289;&#20844;&#24179;&#24615;&#65288;&#21363;&#20154;&#20026;&#20559;&#35265;&#65289;&#21644;&#23433;&#20840;&#24615;&#32463;&#24120;&#21463;&#21040;&#19981;&#36275;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#20840;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#21487;&#20197;&#24110;&#21161;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of automation in particular Software Engineering (SE) tasks has transitioned from theory to reality. Numerous scholarly articles have documented the successful application of Artificial Intelligence to address issues in areas such as project management, modeling, testing, and development. A recent innovation is the introduction of ChatGPT, an ML-infused chatbot, touted as a resource proficient in generating programming codes and formulating software testing strategies for developers and testers respectively. Although there is speculation that AI-based computation can increase productivity and even substitute software engineers in software development, there is currently a lack of empirical evidence to verify this. Moreover, despite the primary focus on enhancing the accuracy of AI systems, non-functional requirements including energy efficiency, vulnerability, fairness (i.e., human bias), and safety frequently receive insufficient attention. This paper posits that a comprehe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#23884;&#20837;&#24335;GPU&#23454;&#29616;&#30340;&#36731;&#37327;&#32423;&#31435;&#20307;&#21305;&#37197;&#31995;&#32479;-StereoVAE&#65292;&#35813;&#31995;&#32479;&#37319;&#29992;&#22522;&#20110;VAE&#30340;&#23567;&#22411;&#31070;&#32463;&#32593;&#32476;&#23545;&#20256;&#32479;&#21305;&#37197;&#26041;&#27861;&#29983;&#25104;&#30340;&#23567;&#23610;&#23544;&#31895;&#31961;&#35270;&#24046;&#22270;&#36827;&#34892;&#19978;&#37319;&#26679;&#19982;&#32454;&#21270;&#65292;&#36798;&#21040;&#20102;&#25552;&#39640;&#21305;&#37197;&#31934;&#24230;&#21644;&#20445;&#35777;&#23454;&#26102;&#22788;&#29702;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.11566</link><description>&lt;p&gt;
&#36890;&#36807;&#23884;&#20837;&#24335;GPU&#23454;&#29616;&#30340;&#36731;&#37327;&#32423;&#31435;&#20307;&#21305;&#37197;&#31995;&#32479;-StereoVAE
&lt;/p&gt;
&lt;p&gt;
StereoVAE: A lightweight stereo matching system through embedded GPUs. (arXiv:2305.11566v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#23884;&#20837;&#24335;GPU&#23454;&#29616;&#30340;&#36731;&#37327;&#32423;&#31435;&#20307;&#21305;&#37197;&#31995;&#32479;-StereoVAE&#65292;&#35813;&#31995;&#32479;&#37319;&#29992;&#22522;&#20110;VAE&#30340;&#23567;&#22411;&#31070;&#32463;&#32593;&#32476;&#23545;&#20256;&#32479;&#21305;&#37197;&#26041;&#27861;&#29983;&#25104;&#30340;&#23567;&#23610;&#23544;&#31895;&#31961;&#35270;&#24046;&#22270;&#36827;&#34892;&#19978;&#37319;&#26679;&#19982;&#32454;&#21270;&#65292;&#36798;&#21040;&#20102;&#25552;&#39640;&#21305;&#37197;&#31934;&#24230;&#21644;&#20445;&#35777;&#23454;&#26102;&#22788;&#29702;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#23884;&#20837;&#24335;GPU&#23454;&#29616;&#30340;&#36731;&#37327;&#32423;&#31435;&#20307;&#21305;&#37197;&#31995;&#32479;-StereoVAE&#65292;&#23427;&#25171;&#30772;&#20102;&#31435;&#20307;&#21305;&#37197;&#20013;&#31934;&#24230;&#21644;&#22788;&#29702;&#36895;&#24230;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#20351;&#24471;&#25105;&#20204;&#30340;&#23884;&#20837;&#24335;&#31995;&#32479;&#33021;&#22815;&#22312;&#20445;&#35777;&#23454;&#26102;&#22788;&#29702;&#30340;&#21516;&#26102;&#36827;&#19968;&#27493;&#25552;&#39640;&#21305;&#37197;&#31934;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#26500;&#24314;&#19968;&#20010;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#30340;&#23567;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#23545;&#20256;&#32479;&#21305;&#37197;&#26041;&#27861;&#29983;&#25104;&#30340;&#23567;&#23610;&#23544;&#31895;&#31961;&#35270;&#24046;&#22270;&#36827;&#34892;&#19978;&#37319;&#26679;&#19982;&#32454;&#21270;&#12290;&#36825;&#31181;&#28151;&#21512;&#32467;&#26500;&#19981;&#20165;&#21487;&#20197;&#24102;&#26469;&#20256;&#32479;&#26041;&#27861;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#20248;&#21183;&#65292;&#36824;&#21487;&#20197;&#20445;&#35777;&#31070;&#32463;&#32593;&#32476;&#30340;&#24433;&#21709;&#19979;&#30340;&#21305;&#37197;&#31934;&#24230;&#12290;&#23545;KITTI 2015&#22522;&#20934;&#27979;&#35797;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#36731;&#37327;&#32423;&#31435;&#20307;&#21305;&#37197;&#31995;&#32479;&#22312;&#25552;&#39640;&#30001;&#19981;&#21516;&#31639;&#27861;&#29983;&#25104;&#30340;&#31895;&#31961;&#35270;&#24046;&#22270;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#39640;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#22312;&#23884;&#20837;&#24335;GPU&#19978;&#23454;&#26102;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a lightweight system for stereo matching through embedded GPUs. It breaks the trade-off between accuracy and processing speed in stereo matching, enabling our embedded system to further improve the matching accuracy while ensuring real-time processing. The main idea of our method is to construct a tiny neural network based on variational auto-encoder (VAE) to upsample and refinement a small size of coarse disparity map, which is first generated by a traditional matching method. The proposed hybrid structure cannot only bring the advantage of traditional methods in terms of computational complexity, but also ensure the matching accuracy under the impact of neural network. Extensive experiments on the KITTI 2015 benchmark demonstrate that our tiny system exhibits high robustness in improving the accuracy of the coarse disparity maps generated by different algorithms, while also running in real-time on embedded GPUs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#35843;&#25972;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#26469;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#39044;&#27979;&#27573;&#33853;&#20013;&#30340;&#31532;&#19968;&#21477;&#35805;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#25512;&#26029;&#65292;&#27169;&#22411;&#19981;&#38656;&#35201;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#20803;&#35843;&#25972;&#65292;&#23545;&#27169;&#26495;&#30340;&#36873;&#25321;&#19981;&#25935;&#24863;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#19981;&#38169;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.11442</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#30417;&#30563;&#35843;&#25972;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Text Classification via Self-Supervised Tuning. (arXiv:2305.11442v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#35843;&#25972;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#26469;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#39044;&#27979;&#27573;&#33853;&#20013;&#30340;&#31532;&#19968;&#21477;&#35805;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#25512;&#26029;&#65292;&#27169;&#22411;&#19981;&#38656;&#35201;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#20803;&#35843;&#25972;&#65292;&#23545;&#27169;&#26495;&#30340;&#36873;&#25321;&#19981;&#25935;&#24863;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#19981;&#38169;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#35201;&#20040;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25552;&#31034;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#23545;&#27169;&#26495;&#30340;&#36873;&#25321;&#38750;&#24120;&#25935;&#24863;&#65307;&#35201;&#20040;&#20381;&#36182;&#20110;&#22823;&#37327;&#30456;&#20851;&#20219;&#21153;&#30340;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#20803;&#35843;&#25972;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#26469;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#65292;&#31216;&#20026;&#33258;&#30417;&#30563;&#35843;&#25972;&#12290;&#36890;&#36807;&#25506;&#32034;&#33258;&#30001;&#25991;&#26412;&#30340;&#20869;&#22312;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#31216;&#20026;&#39318;&#21477;&#39044;&#27979;&#65292;&#20197;&#24357;&#21512;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#35843;&#25972;&#27169;&#22411;&#20197;&#23398;&#20064;&#26681;&#25454;&#21097;&#20313;&#25991;&#26412;&#26469;&#39044;&#27979;&#27573;&#33853;&#20013;&#30340;&#31532;&#19968;&#21477;&#35805;&#21518;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#25512;&#26029;&#20986;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#65292;&#22914;&#20027;&#39064;&#20998;&#31867;&#21644;&#24773;&#24863;&#20998;&#26512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;10&#20010;&#20219;&#21153;&#20013;&#30340;7&#20010;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#32447;&#12290;&#27492;&#22806;&#65292;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23545;&#27169;&#26495;&#30340;&#36873;&#25321;&#19981;&#25935;&#24863;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#20803;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing solutions to zero-shot text classification either conduct prompting with pre-trained language models, which is sensitive to the choices of templates, or rely on large-scale annotated data of relevant tasks for meta-tuning. In this work, we propose a new paradigm based on self-supervised learning to solve zero-shot text classification tasks by tuning the language models with unlabeled data, called self-supervised tuning. By exploring the inherent structure of free texts, we propose a new learning objective called first sentence prediction to bridge the gap between unlabeled data and text classification tasks. After tuning the model to learn to predict the first sentence in a paragraph based on the rest, the model is able to conduct zero-shot inference on unseen tasks such as topic classification and sentiment analysis. Experimental results show that our model outperforms the state-of-the-art baselines on 7 out of 10 tasks. Moreover, the analysis reveals that our model is less s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;UniControl&#65292;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#22522;&#30784;&#27169;&#22411;&#65292;&#33021;&#22815;&#25972;&#21512;&#24191;&#27867;&#30340;&#21487;&#25511;&#26465;&#20214;&#19982;&#22270;&#20687;&#20219;&#21153;&#65292;&#24182;&#20173;&#28982;&#20801;&#35768;&#20219;&#24847;&#35821;&#35328;&#25552;&#31034;&#65292;UniControl&#33021;&#22815;&#36827;&#34892;&#20687;&#32032;&#32423;&#31934;&#30830;&#30340;&#22270;&#20687;&#29983;&#25104;&#65292;&#20854;&#20013;&#35270;&#35273;&#26465;&#20214;&#20027;&#35201;&#24433;&#21709;&#29983;&#25104;&#30340;&#32467;&#26500;&#65292;&#35821;&#35328;&#25552;&#31034;&#21017;&#24341;&#23548;&#26679;&#24335;&#21644;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#23427;&#23558;&#25193;&#25955;&#36807;&#31243;&#21644;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;UniControl&#22312;&#21487;&#25511;&#24615;&#21644;&#22270;&#20687;&#36136;&#37327;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.11147</link><description>&lt;p&gt;
UniControl&#65306;&#32479;&#19968;&#30340;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#21487;&#25511;&#30340;&#29983;&#21160;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild. (arXiv:2305.11147v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;UniControl&#65292;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#22522;&#30784;&#27169;&#22411;&#65292;&#33021;&#22815;&#25972;&#21512;&#24191;&#27867;&#30340;&#21487;&#25511;&#26465;&#20214;&#19982;&#22270;&#20687;&#20219;&#21153;&#65292;&#24182;&#20173;&#28982;&#20801;&#35768;&#20219;&#24847;&#35821;&#35328;&#25552;&#31034;&#65292;UniControl&#33021;&#22815;&#36827;&#34892;&#20687;&#32032;&#32423;&#31934;&#30830;&#30340;&#22270;&#20687;&#29983;&#25104;&#65292;&#20854;&#20013;&#35270;&#35273;&#26465;&#20214;&#20027;&#35201;&#24433;&#21709;&#29983;&#25104;&#30340;&#32467;&#26500;&#65292;&#35821;&#35328;&#25552;&#31034;&#21017;&#24341;&#23548;&#26679;&#24335;&#21644;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#23427;&#23558;&#25193;&#25955;&#36807;&#31243;&#21644;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;UniControl&#22312;&#21487;&#25511;&#24615;&#21644;&#22270;&#20687;&#36136;&#37327;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#26426;&#22120;&#33258;&#27835;&#21644;&#20154;&#31867;&#25511;&#21046;&#24448;&#24448;&#20195;&#34920;&#30528;&#30456;&#20114;&#30683;&#30462;&#30340;&#30446;&#26631;&#65292;&#29305;&#21035;&#26159;&#22312;&#20132;&#20114;&#24335;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#35774;&#35745;&#20013;&#12290;&#29616;&#23454;&#20013;&#30340;&#21487;&#25511;&#35270;&#35273;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;&#31283;&#23450;&#25193;&#25955;&#65289;&#22312;&#20351;&#29992;&#20219;&#24847;&#35821;&#35328;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#34920;&#29616;&#65292;&#20294;&#22312;&#29983;&#25104;&#20855;&#26377;&#31354;&#38388;&#12289;&#32467;&#26500;&#25110;&#20960;&#20309;&#25511;&#21046;&#30340;&#22270;&#20687;&#26041;&#38754;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;UniControl&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#22522;&#30784;&#27169;&#22411;&#65292;&#23427;&#23558;&#24191;&#27867;&#30340;&#21487;&#25511;&#26465;&#20214;&#19982;&#22270;&#20687;&#65288;C2I&#65289;&#20219;&#21153;&#25972;&#21512;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#26694;&#26550;&#20013;&#65292;&#21516;&#26102;&#20173;&#20801;&#35768;&#20219;&#24847;&#35821;&#35328;&#25552;&#31034;&#12290;UniControl&#33021;&#22815;&#36827;&#34892;&#20687;&#32032;&#32423;&#31934;&#30830;&#30340;&#22270;&#20687;&#29983;&#25104;&#65292;&#20854;&#20013;&#35270;&#35273;&#26465;&#20214;&#20027;&#35201;&#24433;&#21709;&#29983;&#25104;&#30340;&#32467;&#26500;&#65292;&#35821;&#35328;&#25552;&#31034;&#21017;&#24341;&#23548;&#26679;&#24335;&#21644;&#19978;&#19979;&#25991;&#12290;&#20026;&#20102;&#20351;UniControl&#20855;&#22791;&#22788;&#29702;&#21508;&#31181;&#35270;&#35273;&#26465;&#20214;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#23427;&#23558;&#25193;&#25955;&#36807;&#31243;&#21644;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;UniControl&#22312;&#21487;&#25511;&#24615;&#21644;&#22270;&#20687;&#36136;&#37327;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Achieving machine autonomy and human control often represent divergent objectives in the design of interactive AI systems. Visual generative foundation models such as Stable Diffusion show promise in navigating these goals, especially when prompted with arbitrary languages. However, they often fall short in generating images with spatial, structural, or geometric controls. The integration of such controls, which can accommodate various visual conditions in a single unified model, remains an unaddressed challenge. In response, we introduce UniControl, a new generative foundation model that consolidates a wide array of controllable condition-to-image (C2I) tasks within a singular framework, while still allowing for arbitrary language prompts. UniControl enables pixel-level-precise image generation, where visual conditions primarily influence the generated structures and language prompts guide the style and context. To equip UniControl with the capacity to handle diverse visual conditions
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#39033;&#30446;&#30456;&#20851;&#24615;&#30340;&#26032;&#22411;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25552;&#39640;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#23545;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10824</link><description>&lt;p&gt;
&#34701;&#21512;&#39033;&#30446;&#30456;&#20851;&#24615;&#30340;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Integrating Item Relevance in Training Loss for Sequential Recommender Systems. (arXiv:2305.10824v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#39033;&#30446;&#30456;&#20851;&#24615;&#30340;&#26032;&#22411;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25552;&#39640;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#23545;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#26159;&#19968;&#31181;&#21463;&#27426;&#36814;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#29992;&#25143;&#30340;&#21382;&#21490;&#25968;&#25454;&#26469;&#39044;&#27979;&#29992;&#25143;&#19979;&#19968;&#20010;&#21487;&#33021;&#19982;&#20043;&#20132;&#20114;&#30340;&#39033;&#30446;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#30340;&#20132;&#20114;&#21487;&#33021;&#20250;&#21463;&#21040;&#26469;&#33258;&#24080;&#25143;&#20849;&#20139;&#12289;&#19981;&#19968;&#33268;&#30340;&#20559;&#22909;&#25110;&#24847;&#22806;&#28857;&#20987;&#31561;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#65288;i&#65289;&#25552;&#20986;&#20102;&#19968;&#20010;&#32771;&#34385;&#22810;&#20010;&#26410;&#26469;&#39033;&#30446;&#30340;&#26032;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#65288;ii&#65289;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20851;&#27880;&#30456;&#20851;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#35757;&#32451;&#20855;&#26377;&#22810;&#20010;&#26410;&#26469;&#39033;&#30446;&#30340;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#65292;&#20197;&#20351;&#20854;&#23545;&#22122;&#22768;&#26356;&#21152;&#40065;&#26834;&#12290;&#25105;&#20204;&#30340;&#20851;&#27880;&#30456;&#20851;&#24615;&#27169;&#22411;&#22312;&#20256;&#32479;&#35780;&#20272;&#21327;&#35758;&#20013;&#25552;&#39640;&#20102;NDCG@10&#32422;1.2%&#21644;HR&#32422;0.88%&#65292;&#32780;&#22312;&#26032;&#35780;&#20272;&#21327;&#35758;&#20013;&#65292;&#25913;&#36827;&#30340;NDCG@10&#32422;1.63%&#21644;HR&#32422;1.5%&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential Recommender Systems (SRSs) are a popular type of recommender system that learns from a user's history to predict the next item they are likely to interact with. However, user interactions can be affected by noise stemming from account sharing, inconsistent preferences, or accidental clicks. To address this issue, we (i) propose a new evaluation protocol that takes multiple future items into account and (ii) introduce a novel relevance-aware loss function to train a SRS with multiple future items to make it more robust to noise. Our relevance-aware models obtain an improvement of ~1.2% of NDCG@10 and 0.88% in the traditional evaluation protocol, while in the new evaluation protocol, the improvement is ~1.63% of NDCG@10 and ~1.5% of HR w.r.t the best performing models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#19968;&#39033;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#23545;&#20027;&#27969;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22823;&#37327;&#26597;&#35810;&#21644;&#20998;&#26512;&#65292;&#32467;&#26524;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#30528;&#40065;&#26834;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#21487;&#20449;&#24615;&#26041;&#38754;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2305.10235</link><description>&lt;p&gt;
&#35780;&#20272;LLM&#30340;&#38544;&#34255;&#39118;&#38505;&#65306;&#20851;&#20110;&#40065;&#26834;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#21487;&#20449;&#24615;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility. (arXiv:2305.10235v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#19968;&#39033;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#23545;&#20027;&#27969;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22823;&#37327;&#26597;&#35810;&#21644;&#20998;&#26512;&#65292;&#32467;&#26524;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#30528;&#40065;&#26834;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#21487;&#20449;&#24615;&#26041;&#38754;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#23545;&#20110;&#35768;&#22810;&#39046;&#22495;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#20854;&#24320;&#25918;&#24335;&#29615;&#22659;&#65288;&#22914;API&#12289;&#24320;&#28304;&#27169;&#22411;&#21644;&#25554;&#20214;&#65289;&#20013;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;LLMs&#30340;&#24191;&#27867;&#37096;&#32626;&#65292;&#32570;&#20047;&#20840;&#38754;&#35752;&#35770;&#21644;&#20998;&#26512;&#28508;&#22312;&#39118;&#38505;&#30340;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#21021;&#27493;&#20294;&#24320;&#21019;&#24615;&#30340;&#30740;&#31350;&#65292;&#28085;&#30422;&#20102;LLMs&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#21487;&#20449;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#24037;&#20316;&#27969;&#31243;&#26469;&#22788;&#29702;&#22823;&#37327;&#26597;&#35810;/&#21709;&#24212;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;&#21253;&#25324;ChatGPT&#12289;LLaMA&#21644;OPT&#22312;&#20869;&#30340;&#20027;&#27969;LLMs&#36827;&#34892;&#20102;100&#22810;&#19975;&#20010;&#26597;&#35810;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#27969;&#26680;&#24515;&#21253;&#25324;&#25968;&#25454;&#21407;&#35821;&#65292;&#38543;&#21518;&#26159;&#33258;&#21160;&#35299;&#37322;&#22120;&#65292;&#35780;&#20272;&#36825;&#20123;LLMs&#22312;&#19981;&#21516;&#30340;&#23545;&#25239;&#24615;&#24230;&#37327;&#31995;&#32479;&#19979;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20960;&#20010;&#12289;&#20063;&#35768;&#26159;&#19981;&#24184;&#30340;&#32467;&#35770;&#65292;&#36825;&#20123;&#32467;&#35770;&#30456;&#24403;&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
The recent popularity of large language models (LLMs) has brought a significant impact to boundless fields, particularly through their open-ended ecosystem such as the APIs, open-sourced models, and plugins. However, with their widespread deployment, there is a general lack of research that thoroughly discusses and analyzes the potential risks concealed. In that case, we intend to conduct a preliminary but pioneering study covering the robustness, consistency, and credibility of LLMs systems. With most of the related literature in the era of LLM uncharted, we propose an automated workflow that copes with an upscaled number of queries/responses. Overall, we conduct over a million queries to the mainstream LLMs including ChatGPT, LLaMA, and OPT. Core to our workflow consists of a data primitive, followed by an automated interpreter that evaluates these LLMs under different adversarial metrical systems. As a result, we draw several, and perhaps unfortunate, conclusions that are quite unco
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35748;&#30693;&#36890;&#29992;&#27169;&#22411;&#30340; System-1 &#21644; System-2 &#23454;&#29616;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20854;&#21306;&#20998;&#19981;&#28165;&#30340;&#38382;&#39064;&#12290;&#36890;&#29992;&#27169;&#22411;&#25552;&#20379;&#20102;&#23545;&#28041;&#21450; System-1 &#21644; System-2 &#30340;&#35745;&#31639;&#21333;&#20803;&#12289;&#22522;&#30784;&#26426;&#21046;&#21644;&#20854;&#23545;&#23398;&#20064;&#12289;&#20803;&#35748;&#30693;&#21644;&#24773;&#24863;&#30340;&#24433;&#21709;&#30340;&#20840;&#38754;&#35270;&#37326;&#12290;</title><link>http://arxiv.org/abs/2305.09091</link><description>&lt;p&gt;
AAAI 2022 &#31179;&#23395;&#30740;&#35752;&#20250;&#65306;&#22522;&#20110;&#35748;&#30693;&#36890;&#29992;&#27169;&#22411;&#23454;&#29616;&#30340; System-1 &#21644; System-2
&lt;/p&gt;
&lt;p&gt;
AAAI 2022 Fall Symposium: System-1 and System-2 realized within the Common Model of Cognition. (arXiv:2305.09091v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09091
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35748;&#30693;&#36890;&#29992;&#27169;&#22411;&#30340; System-1 &#21644; System-2 &#23454;&#29616;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20854;&#21306;&#20998;&#19981;&#28165;&#30340;&#38382;&#39064;&#12290;&#36890;&#29992;&#27169;&#22411;&#25552;&#20379;&#20102;&#23545;&#28041;&#21450; System-1 &#21644; System-2 &#30340;&#35745;&#31639;&#21333;&#20803;&#12289;&#22522;&#30784;&#26426;&#21046;&#21644;&#20854;&#23545;&#23398;&#20064;&#12289;&#20803;&#35748;&#30693;&#21644;&#24773;&#24863;&#30340;&#24433;&#21709;&#30340;&#20840;&#38754;&#35270;&#37326;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI &#39046;&#22495;&#20013;&#23545; System-1 &#21644; System-2 &#20108;&#20803;&#27169;&#22411;&#30340;&#24341;&#20837;&#19968;&#30452;&#21463;&#21040;&#20854;&#21306;&#20998;&#19981;&#28165;&#30340;&#38459;&#30861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558; System-1 &#21644; System-2 &#32622;&#20110;&#35748;&#30693;&#36890;&#29992;&#27169;&#22411;&#20013;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#21644;&#20854;&#20182;&#38382;&#39064;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#34987;&#35748;&#20026;&#26159; System-1 &#21644; 2 &#29305;&#24449;&#30340;&#19981;&#21516;&#20043;&#22788;&#23454;&#38469;&#19978;&#24418;&#25104;&#20102;&#19968;&#31181;&#35748;&#30693;&#23646;&#24615;&#30340;&#33539;&#22260;&#12290;&#36890;&#29992;&#27169;&#22411;&#25552;&#20379;&#20102;&#26377;&#20851;&#28041;&#21450; System-1 &#21644; System-2 &#30340;&#35745;&#31639;&#21333;&#20803;&#12289;&#20854;&#22522;&#30784;&#26426;&#21046;&#20197;&#21450;&#23545;&#23398;&#20064;&#12289;&#20803;&#35748;&#30693;&#21644;&#24773;&#24863;&#30340;&#24433;&#21709;&#30340;&#20840;&#38754;&#35270;&#37326;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attempts to import dual-system descriptions of System-1 and System-2 into AI have been hindered by a lack of clarity over their distinction. We address this and other issues by situating System-1 and System-2 within the Common Model of Cognition. Results show that what are thought to be distinctive characteristics of System-1 and 2 instead form a spectrum of cognitive properties. The Common Model provides a comprehensive vision of the computational units involved in System-1 and System-2, their underlying mechanisms, and the implications for learning, metacognition, and emotion.
&lt;/p&gt;</description></item><item><title>sustain.AI&#26159;&#19968;&#20010;&#26234;&#33021;&#30340;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#23457;&#35745;&#24072;&#12289;&#37329;&#34701;&#25237;&#36164;&#32773;&#20197;&#21450;&#24191;&#22823;&#20844;&#20247;&#39640;&#25928;&#22320;&#20998;&#26512;&#20844;&#21496;&#30340;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#65292;&#24182;&#36890;&#36807;&#19982;GRI&#26631;&#20934;&#21305;&#37197;&#26469;&#25552;&#20379;&#26356;&#22909;&#30340;&#25512;&#33616;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.08711</link><description>&lt;p&gt;
sustain.AI: &#19968;&#31181;&#20998;&#26512;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#30340;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
sustain.AI: a Recommender System to analyze Sustainability Reports. (arXiv:2305.08711v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08711
&lt;/p&gt;
&lt;p&gt;
sustain.AI&#26159;&#19968;&#20010;&#26234;&#33021;&#30340;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#23457;&#35745;&#24072;&#12289;&#37329;&#34701;&#25237;&#36164;&#32773;&#20197;&#21450;&#24191;&#22823;&#20844;&#20247;&#39640;&#25928;&#22320;&#20998;&#26512;&#20844;&#21496;&#30340;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#65292;&#24182;&#36890;&#36807;&#19982;GRI&#26631;&#20934;&#21305;&#37197;&#26469;&#25552;&#20379;&#26356;&#22909;&#30340;&#25512;&#33616;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;sustain.AI&#65292;&#36825;&#26159;&#19968;&#20010;&#26234;&#33021;&#30340;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#24110;&#21161;&#23457;&#35745;&#24072;&#12289;&#37329;&#34701;&#25237;&#36164;&#32773;&#20197;&#21450;&#24191;&#22823;&#20844;&#20247;&#39640;&#25928;&#22320;&#20998;&#26512;&#20844;&#21496;&#30340;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#12290;&#35813;&#24037;&#20855;&#21033;&#29992;&#20102;&#31471;&#21040;&#31471;&#21487;&#35757;&#32451;&#30340;&#26550;&#26500;&#65292;&#23558;&#22522;&#20110;BERT&#30340;&#32534;&#30721;&#27169;&#22359;&#19982;&#22810;&#26631;&#31614;&#20998;&#31867;&#22836;&#30456;&#32467;&#21512;&#65292;&#23558;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#20013;&#30340;&#30456;&#20851;&#25991;&#26412;&#27573;&#33853;&#19982;&#20840;&#29699;&#25253;&#21578;&#20513;&#35758;&#65288;GRI&#65289;&#26631;&#20934;&#20013;&#30340;&#30456;&#24212;&#27861;&#24459;&#27861;&#35268;&#21305;&#37197;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#26032;&#39062;&#30340;&#24503;&#22269;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#22987;&#32456;&#23454;&#29616;&#20102;&#19982;&#22810;&#20010;&#24378;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#26356;&#39640;&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;sustain.AI&#24050;&#32463;&#20844;&#24320;&#22312;https://sustain.ki.nrw/&#19978;&#25552;&#20379;&#32473;&#25152;&#26377;&#20154;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present $\text{sustain.AI}$, an intelligent, context-aware recommender system that assists auditors and financial investors as well as the general public to efficiently analyze companies' sustainability reports. The tool leverages an end-to-end trainable architecture that couples a BERT-based encoding module with a multi-label classification head to match relevant text passages from sustainability reports to their respective law regulations from the Global Reporting Initiative (GRI) standards. We evaluate our model on two novel German sustainability reporting data sets and consistently achieve a significantly higher recommendation performance compared to multiple strong baselines. Furthermore, $\text{sustain.AI}$ is publicly available for everyone at https://sustain.ki.nrw/.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#33258;&#21160;&#26631;&#27880;&#25991;&#26412;&#30340;&#28508;&#21147;&#65292;&#37325;&#28857;&#32771;&#23519;&#20102;&#20174;&#26412;&#22320;&#35821;&#27861;&#35282;&#24230;&#35266;&#23519;&#36947;&#27465;&#35328;&#35821;&#34892;&#20026;&#26500;&#25104;&#30340;&#21151;&#33021;&#20803;&#32032;&#30340;&#31243;&#24230;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#27169;&#22411;&#22312;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;Bing&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;ChatGPT&#21644;&#20154;&#31867;&#26631;&#27880;&#21592;&#12290;</title><link>http://arxiv.org/abs/2305.08339</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#36741;&#21161;&#27880;&#37322;&#36827;&#34892;&#35821;&#26009;&#24211;&#35821;&#35328;&#23398;&#30740;&#31350;&#65306;&#26412;&#22320;&#35821;&#27861;&#20998;&#26512;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Using LLM-assisted Annotation for Corpus Linguistics: A Case Study of Local Grammar Analysis. (arXiv:2305.08339v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#33258;&#21160;&#26631;&#27880;&#25991;&#26412;&#30340;&#28508;&#21147;&#65292;&#37325;&#28857;&#32771;&#23519;&#20102;&#20174;&#26412;&#22320;&#35821;&#27861;&#35282;&#24230;&#35266;&#23519;&#36947;&#27465;&#35328;&#35821;&#34892;&#20026;&#26500;&#25104;&#30340;&#21151;&#33021;&#20803;&#32032;&#30340;&#31243;&#24230;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#27169;&#22411;&#22312;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;Bing&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;ChatGPT&#21644;&#20154;&#31867;&#26631;&#27880;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;LLMs&#22312;&#21327;&#21161;&#22522;&#20110;&#35821;&#26009;&#24211;&#30340;&#35821;&#35328;&#23398;&#30740;&#31350;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#33258;&#21160;&#26631;&#27880;&#20026;&#29305;&#23450;&#35821;&#35328;&#20449;&#24687;&#31867;&#21035;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#26412;&#22320;&#35821;&#27861;&#30340;&#35282;&#24230;&#35266;&#23519;&#36947;&#27465;&#35328;&#35821;&#34892;&#20026;&#26500;&#25104;&#30340;&#21151;&#33021;&#20803;&#32032;&#30340;&#31243;&#24230;&#65292;&#36890;&#36807;&#27604;&#36739;&#22522;&#20110;GPT-3.5&#30340;ChatGPT&#12289;&#22522;&#20110;GPT-4&#30340;Bing&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#32534;&#30721;&#22120;&#22312;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;Bing&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20219;&#21153;&#20013;&#34920;&#29616;&#26174;&#30528;&#20248;&#20110;ChatGPT&#12290;&#19982;&#20154;&#31867;&#26631;&#27880;&#21592;&#30456;&#27604;&#65292;Bing&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#25972;&#20307;&#34920;&#29616;&#30053;&#20302;&#20110;&#20154;&#31867;&#26631;&#27880;&#21592;&#30340;&#34920;&#29616;&#65292;&#20294;&#24050;&#32463;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;F1&#24471;&#20998;:&#36947;&#27465;&#26631;&#35760;99.95&#65285;&#65292;&#21407;&#22240;&#26631;&#35760;91.91&#65285;&#65292;&#36947;&#27465;&#32773;&#26631;&#35760;95.35&#65285;&#65292;&#34987;&#36947;&#27465;&#32773;&#26631;&#35760;89.74&#65285;&#21644;&#21152;&#24378;&#26631;&#35760;96.47&#65285;&#12290;&#36825;&#34920;&#26126;&#65292;&#22312;&#35821;&#35328;&#31867;&#21035;&#28165;&#26224;&#19988;&#21487;&#20197;&#36731;&#26494;&#35782;&#21035;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;LLM&#36741;&#21161;&#27880;&#37322;&#36827;&#34892;&#35821;&#26009;&#24211;&#35821;&#35328;&#23398;&#30740;&#31350;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chatbots based on Large Language Models (LLMs) have shown strong capabilities in language understanding. In this study, we explore the potential of LLMs in assisting corpus-based linguistic studies through automatic annotation of texts with specific categories of linguistic information. Specifically, we examined to what extent LLMs understand the functional elements constituting the speech act of apology from a local grammar perspective, by comparing the performance of ChatGPT (powered by GPT-3.5), the Bing chatbot (powered by GPT-4), and a human coder in the annotation task. The results demonstrate that the Bing chatbot significantly outperformed ChatGPT in the task. Compared to human annotator, the overall performance of the Bing chatbot was slightly less satisfactory. However, it already achieved high F1 scores: 99.95% for the tag of APOLOGISING, 91.91% for REASON, 95.35% for APOLOGISER, 89.74% for APOLOGISEE, and 96.47% for INTENSIFIER. This suggests that it is feasible to use LLM-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36830;&#36143;&#30340;&#33521;&#25991;&#25991;&#26412;&#38590;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21512;&#25104;&#25925;&#20107;&#25968;&#25454;&#38598; TinyStories&#65292;&#24182;&#25506;&#32034;&#23567;&#22411;&#27169;&#22411;&#35268;&#27169;&#12289;&#32467;&#26500;&#22797;&#26434;&#24230;&#21644;&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#20165;&#21547; 200 &#19975;&#21442;&#25968;&#30340;&#31616;&#21333;&#35821;&#35328;&#27169;&#22411;&#20063;&#33021;&#20135;&#29983;&#36830;&#36143;&#30340;&#30701;&#25925;&#20107;&#12290;</title><link>http://arxiv.org/abs/2305.07759</link><description>&lt;p&gt;
TinyStories: &#35821;&#35328;&#27169;&#22411;&#33021;&#31616;&#23567;&#21040;&#20160;&#20040;&#31243;&#24230;&#21364;&#20381;&#28982;&#33021;&#22815;&#35762;&#36848;&#36830;&#36143;&#30340;&#33521;&#25991;&#25925;&#20107;&#65311;
&lt;/p&gt;
&lt;p&gt;
TinyStories: How Small Can Language Models Be and Still Speak Coherent English?. (arXiv:2305.07759v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36830;&#36143;&#30340;&#33521;&#25991;&#25991;&#26412;&#38590;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21512;&#25104;&#25925;&#20107;&#25968;&#25454;&#38598; TinyStories&#65292;&#24182;&#25506;&#32034;&#23567;&#22411;&#27169;&#22411;&#35268;&#27169;&#12289;&#32467;&#26500;&#22797;&#26434;&#24230;&#21644;&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#20165;&#21547; 200 &#19975;&#21442;&#25968;&#30340;&#31616;&#21333;&#35821;&#35328;&#27169;&#22411;&#20063;&#33021;&#20135;&#29983;&#36830;&#36143;&#30340;&#30701;&#25925;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#20294;&#22312;&#23567;&#22411;&#21270;&#26102;&#32463;&#24120;&#38590;&#20197;&#20135;&#29983;&#36830;&#36143;&#21644;&#27969;&#30021;&#30340;&#25991;&#26412;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026; TinyStories &#30340;&#21512;&#25104;&#25925;&#20107;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#35268;&#27169;&#23567;&#12289;&#22797;&#26434;&#24230;&#20302;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#30701;&#25925;&#20107;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) are powerful tools for natural language processing, but they often struggle to produce coherent and fluent text when they are small. Models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can rarely generate coherent and consistent English text beyond a few words even after extensive training. This raises the question of whether the emergence of the ability to produce coherent English text only occurs at larger scales (with hundreds of millions of parameters or more) and complex architectures (with many layers of global attention).  In this work, we introduce TinyStories, a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train and evaluate LMs that are much smaller than the state-of-the-art models (below 10 million total parameters), or have much simpler architectures (with only one transformer block), yet stil
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;SemEval-2023 Task 2&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20219;&#21153;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;MultiCoNER V2&#25968;&#25454;&#38598;&#65292;&#35782;&#21035;12&#31181;&#35821;&#35328;&#20013;&#22797;&#26434;&#30340;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#12290;&#26368;&#20248;&#26041;&#27861;&#26159;&#23558;&#22806;&#37096;&#30693;&#35782;&#34701;&#20837;transformer&#27169;&#22411;&#65292;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#26159;&#23186;&#20307;&#26631;&#39064;&#21644;&#20135;&#21697;&#21517;&#31216;&#31561;&#23454;&#20307;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.06586</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;2&#65306;&#32454;&#31890;&#24230;&#22810;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;MultiCoNER 2&#65289;
&lt;/p&gt;
&lt;p&gt;
SemEval-2023 Task 2: Fine-grained Multilingual Named Entity Recognition (MultiCoNER 2). (arXiv:2305.06586v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06586
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;SemEval-2023 Task 2&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20219;&#21153;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;MultiCoNER V2&#25968;&#25454;&#38598;&#65292;&#35782;&#21035;12&#31181;&#35821;&#35328;&#20013;&#22797;&#26434;&#30340;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#12290;&#26368;&#20248;&#26041;&#27861;&#26159;&#23558;&#22806;&#37096;&#30693;&#35782;&#34701;&#20837;transformer&#27169;&#22411;&#65292;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#26159;&#23186;&#20307;&#26631;&#39064;&#21644;&#20135;&#21697;&#21517;&#31216;&#31561;&#23454;&#20307;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;SemEval-2023&#20219;&#21153;2&#20851;&#20110;&#32454;&#31890;&#24230;&#22810;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;MultiCoNER 2&#65289;&#30340;&#30740;&#31350;&#21457;&#29616;&#12290;&#35813;&#20219;&#21153;&#20998;&#20026;13&#20010;&#36712;&#36947;&#65292;&#37325;&#28857;&#20851;&#27880;12&#31181;&#35821;&#35328;&#21644;&#21333;&#35821;&#12289;&#22810;&#35821;&#21644;&#22024;&#26434;&#29615;&#22659;&#19979;&#35782;&#21035;&#22797;&#26434;&#30340;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#65288;&#22914;WRITTENWORK&#12289;VEHICLE&#12289;MUSICALGRP&#65289;&#30340;&#26041;&#27861;&#12290;&#20219;&#21153;&#20351;&#29992;MultiCoNER V2&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;Bangla&#12289;Chinese&#12289;English&#12289;Farsi&#12289;French&#12289;German&#12289;Hindi&#12289;Italian&#12289;Portuguese&#12289;Spanish&#12289;Swedish&#21644;Ukrainian&#32452;&#25104;&#65292;&#20849;&#26377;220&#19975;&#20010;&#23454;&#20363;&#12290;MultiCoNER 2&#26159;SemEval-2023&#20013;&#26368;&#21463;&#27426;&#36814;&#30340;&#20219;&#21153;&#20043;&#19968;&#65292;&#21560;&#24341;&#20102;47&#20010;&#38431;&#20237;&#25552;&#20132;842&#20010;&#32467;&#26524;&#65292;&#20854;&#20013;34&#20010;&#38431;&#20237;&#25552;&#20132;&#20102;&#31995;&#32479;&#35770;&#25991;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23186;&#20307;&#26631;&#39064;&#21644;&#20135;&#21697;&#21517;&#31216;&#31561;&#22797;&#26434;&#23454;&#20307;&#31867;&#22411;&#26159;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#65292;&#23558;&#22806;&#37096;&#30693;&#35782;&#34701;&#20837;transformer&#27169;&#22411;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;Creative Work&#21644;Group&#31867;&#21035;&#19978;&#33719;&#24471;&#20102;&#26368;&#22823;&#22686;&#30410;&#65292;&#21363;&#20351;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the findings of SemEval-2023 Task 2 on Fine-grained Multilingual Named Entity Recognition (MultiCoNER 2). Divided into 13 tracks, the task focused on methods to identify complex fine-grained named entities (like WRITTENWORK, VEHICLE, MUSICALGRP) across 12 languages, in both monolingual and multilingual scenarios, as well as noisy settings. The task used the MultiCoNER V2 dataset, composed of 2.2 million instances in Bangla, Chinese, English, Farsi, French, German, Hindi, Italian., Portuguese, Spanish, Swedish, and Ukrainian. MultiCoNER 2 was one of the most popular tasks of SemEval-2023. It attracted 842 submissions from 47 teams, and 34 teams submitted system papers. Results showed that complex entity types such as media titles and product names were the most challenging. Methods fusing external knowledge into transformer models achieved the best performance, and the largest gains were on the Creative Work and Group classes, which are still challenging even with external kn
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21010;&#20998;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#65288;PMPO&#65289;&#26041;&#27861;&#65292;&#23558;&#36719;&#25552;&#31034;&#20174;&#19968;&#20010;&#25193;&#23637;&#21040;&#22810;&#20010;&#65292;&#36890;&#36807;&#36830;&#25509;&#22810;&#20010;&#25552;&#31034;&#21040;&#35270;&#35273;&#32534;&#30721;&#22120;&#30340;&#19981;&#21516;&#28145;&#24230;&#19978;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#35270;&#35273;&#34920;&#31034;&#30340;&#19978;&#19979;&#25991;&#28145;&#24230;&#65292;&#19982;&#20256;&#32479;&#21333;&#25552;&#31034;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#19979;&#28216;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.06221</link><description>&lt;p&gt;
&#24102;&#28145;&#24230;&#21010;&#20998;&#30340;&#22810;&#25552;&#31034;&#27169;&#24577;&#20132;&#21449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Prompt with Depth Partitioned Cross-Modal Learning. (arXiv:2305.06221v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21010;&#20998;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#65288;PMPO&#65289;&#26041;&#27861;&#65292;&#23558;&#36719;&#25552;&#31034;&#20174;&#19968;&#20010;&#25193;&#23637;&#21040;&#22810;&#20010;&#65292;&#36890;&#36807;&#36830;&#25509;&#22810;&#20010;&#25552;&#31034;&#21040;&#35270;&#35273;&#32534;&#30721;&#22120;&#30340;&#19981;&#21516;&#28145;&#24230;&#19978;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#35270;&#35273;&#34920;&#31034;&#30340;&#19978;&#19979;&#25991;&#28145;&#24230;&#65292;&#19982;&#20256;&#32479;&#21333;&#25552;&#31034;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#19979;&#28216;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36719;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#34987;&#25552;&#20986;&#26469;&#24494;&#35843;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#23436;&#25104;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#23558;&#21487;&#23398;&#20064;&#30340;&#25991;&#26412;&#26631;&#35760;&#19982;&#31867;&#21035;&#26631;&#35760;&#32452;&#21512;&#20316;&#20026;&#27169;&#22411;&#30340;&#36755;&#20837;&#65292;&#20854;&#20013;&#27169;&#22411;&#30340;&#21442;&#25968;&#34987;&#20923;&#32467;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#32463;&#24120;&#20351;&#29992;&#21333;&#19968;&#25552;&#31034;&#26469;&#25551;&#36848;&#31867;&#21035;&#19978;&#19979;&#25991;&#65292;&#32780;&#19981;&#33021;&#20805;&#20998;&#25429;&#25417;&#31867;&#21035;&#30340;&#22810;&#26679;&#23646;&#24615;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#21010;&#20998;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#65288;PMPO&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#25552;&#31034;&#25216;&#26415;&#65292;&#23558;&#36719;&#25552;&#31034;&#20174;&#19968;&#20010;&#21487;&#23398;&#20064;&#25552;&#31034;&#25193;&#23637;&#21040;&#22810;&#20010;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#35270;&#35273;&#32534;&#30721;&#22120;&#28145;&#24230;&#36827;&#34892;&#20998;&#21106;&#65292;&#24182;&#23558;&#21487;&#23398;&#20064;&#25552;&#31034;&#36830;&#25509;&#21040;&#20998;&#31163;&#30340;&#35270;&#35273;&#28145;&#24230;&#19978;&#65292;&#20351;&#19981;&#21516;&#25552;&#31034;&#33021;&#22815;&#25429;&#25417;&#35270;&#35273;&#34920;&#31034;&#30340;&#23618;&#27425;&#19978;&#19979;&#25991;&#28145;&#24230;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#22810;&#25552;&#31034;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#23558;&#26469;&#33258;&#25163;&#21160;&#35774;&#35745;&#30340;&#27169;&#26495;&#21644;&#21487;&#23398;&#20064;&#30340;&#22810;&#25552;&#31034;&#30340;&#20808;&#39564;&#20449;&#24687;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, soft prompt learning methods have been proposed to fine-tune large-scale vision-language pre-trained models for various downstream tasks. These methods typically combine learnable textual tokens with class tokens as input for models with frozen parameters. However, they often employ a single prompt to describe class contexts, failing to capture categories' diverse attributes adequately. This study introduces the Partitioned Multi-modal Prompt (PMPO), a multi-modal prompting technique that extends the soft prompt from a single learnable prompt to multiple prompts. Our method divides the visual encoder depths and connects learnable prompts to the separated visual depths, enabling different prompts to capture the hierarchical contextual depths of visual representations. Furthermore, to maximize the advantages of multi-prompt learning, we incorporate prior information from manually designed templates and learnable multi-prompts, thus improving the generalization capabiliti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CoFe&#27979;&#35797;&#22871;&#20214;&#26469;&#35843;&#26597;&#19978;&#19979;&#25991;&#32452;&#21512;&#27867;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19978;&#19979;&#25991;&#31034;&#20363;&#24212;&#35813;&#22312;&#32467;&#26500;&#19978;&#19982;&#27979;&#35797;&#29992;&#20363;&#31867;&#20284;&#65292;&#30456;&#20114;&#20043;&#38388;&#24212;&#35813;&#19981;&#21516;&#65292;&#32780;&#19988;&#21333;&#29420;&#22320;&#31616;&#21333;&#12290;</title><link>http://arxiv.org/abs/2305.04835</link><description>&lt;p&gt;
&#22914;&#20309;&#24433;&#21709;&#19978;&#19979;&#25991;&#33539;&#20363;&#22312;&#32452;&#21512;&#36890;&#29992;&#24615;&#20013;&#30340;&#20316;&#29992;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Do In-Context Examples Affect Compositional Generalization?. (arXiv:2305.04835v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CoFe&#27979;&#35797;&#22871;&#20214;&#26469;&#35843;&#26597;&#19978;&#19979;&#25991;&#32452;&#21512;&#27867;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19978;&#19979;&#25991;&#31034;&#20363;&#24212;&#35813;&#22312;&#32467;&#26500;&#19978;&#19982;&#27979;&#35797;&#29992;&#20363;&#31867;&#20284;&#65292;&#30456;&#20114;&#20043;&#38388;&#24212;&#35813;&#19981;&#21516;&#65292;&#32780;&#19988;&#21333;&#29420;&#22320;&#31616;&#21333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#27867;&#21270;&#8212;&#8212;&#29702;&#35299;&#30475;&#19981;&#35265;&#30340;&#24050;&#30693;&#21407;&#22987;&#32452;&#21512;&#8212;&#8212;&#26159;&#20154;&#31867;&#26234;&#33021;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25512;&#29702;&#33021;&#21147;&#12290;AI&#31038;&#21306;&#20027;&#35201;&#36890;&#36807;&#22312;&#35768;&#22810;&#35757;&#32451;&#26679;&#26412;&#19978;&#24494;&#35843;&#31070;&#32463;&#32593;&#32476;&#26469;&#30740;&#31350;&#36825;&#31181;&#33021;&#21147;&#65292;&#28982;&#32780;&#36824;&#19981;&#28165;&#26970;&#19978;&#19979;&#25991;&#23398;&#20064;&#8212;&#8212;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#35201;&#23569;&#26679;&#26412;&#33539;&#24335;&#8212;&#8212;&#26159;&#21542;&#23637;&#31034;&#32452;&#21512;&#27867;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CoFe&#65292;&#19968;&#20010;&#27979;&#35797;&#22871;&#20214;&#26469;&#35843;&#26597;&#19978;&#19979;&#25991;&#32452;&#21512;&#27867;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#32452;&#21512;&#27867;&#21270;&#24615;&#33021;&#24456;&#23481;&#26131;&#21463;&#21040;&#19978;&#19979;&#25991;&#31034;&#20363;&#36873;&#25321;&#30340;&#24433;&#21709;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#30740;&#31350;&#38382;&#39064;&#65306;&#20160;&#20040;&#26159;&#22312;&#32452;&#21512;&#27867;&#21270;&#20013;&#21046;&#20316;&#22909;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#20010;&#28508;&#22312;&#22240;&#32032;&#65306;&#30456;&#20284;&#24615;&#12289;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#32452;&#21512;&#36890;&#29992;&#24615;&#20013;&#65292;&#19978;&#19979;&#25991;&#31034;&#20363;&#24212;&#35813;&#22312;&#32467;&#26500;&#19978;&#19982;&#27979;&#35797;&#29992;&#20363;&#31867;&#20284;&#65292;&#30456;&#20114;&#20043;&#38388;&#24212;&#35813;&#19981;&#21516;&#65292;&#32780;&#19988;&#21333;&#29420;&#22320;&#31616;&#21333;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compositional generalization--understanding unseen combinations of seen primitives--is an essential reasoning capability in human intelligence. The AI community mainly studies this capability by fine-tuning neural networks on lots of training samples, while it is still unclear whether and how in-context learning--the prevailing few-shot paradigm based on large language models--exhibits compositional generalization. In this paper, we present CoFe, a test suite to investigate in-context compositional generalization. We find that the compositional generalization performance can be easily affected by the selection of in-context examples, thus raising the research question what the key factors are to make good in-context examples for compositional generalization. We study three potential factors: similarity, diversity and complexity. Our systematic experiments indicate that in-context examples should be structurally similar to the test case, diverse from each other, and individually simple.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CAAFE&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#25968;&#25454;&#38598;&#25551;&#36848;&#29983;&#25104;&#26356;&#22810;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#33021;&#22815;&#25552;&#39640;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;ROC AUC&#34920;&#29616;&#25552;&#39640;&#33267;0.822&#12290;</title><link>http://arxiv.org/abs/2305.03403</link><description>&lt;p&gt;
GPT&#29992;&#20110;&#21322;&#33258;&#21160;&#21270;&#25968;&#25454;&#31185;&#23398;&#65306;&#24341;&#20837;CAAFE&#23454;&#29616;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
GPT for Semi-Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering. (arXiv:2305.03403v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03403
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CAAFE&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#25968;&#25454;&#38598;&#25551;&#36848;&#29983;&#25104;&#26356;&#22810;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#33021;&#22815;&#25552;&#39640;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;ROC AUC&#34920;&#29616;&#25552;&#39640;&#33267;0.822&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#32435;&#20837;&#36825;&#20123;&#31995;&#32479;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24378;&#22823;&#21151;&#33021;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#65292;&#21517;&#20026;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#65288;CAAFE&#65289;&#65292;&#23427;&#21033;&#29992;LLM&#26681;&#25454;&#25968;&#25454;&#38598;&#30340;&#25551;&#36848;&#29983;&#25104;&#26356;&#22810;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#20135;&#29983;&#29992;&#20110;&#21019;&#24314;&#26032;&#29305;&#24449;&#30340;Python&#20195;&#30721;&#65292;&#24182;&#25552;&#20379;&#29983;&#25104;&#29305;&#24449;&#30340;&#25928;&#29992;&#35828;&#26126;&#12290;&#23613;&#31649;&#26041;&#27861;&#35770;&#19978;&#24456;&#31616;&#21333;&#65292;&#20294;CAAFE&#25552;&#39640;&#20102;14&#20010;&#25968;&#25454;&#38598;&#20013;11&#20010;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#19982;2&#20010;&#25968;&#25454;&#38598;&#24182;&#21015;&#65292;&#21482;&#26377;1&#20010;&#25968;&#25454;&#38598;&#24615;&#33021;&#19979;&#38477;&#65292;&#20174;&#32780;&#20351;&#25152;&#26377;&#25968;&#25454;&#38598;&#30340;&#24179;&#22343;ROC AUC&#34920;&#29616;&#20174;0.798&#25552;&#21319;&#33267;0.822&#12290;&#23545;&#20110;&#25152;&#35780;&#20272;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#19968;&#25913;&#36827;&#19982;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#65288;AUC 0.782&#65289;&#20195;&#26367;&#36923;&#36753;&#22238;&#24402;&#65288;AUC 0.754&#65289;&#25152;&#33719;&#24471;&#30340;&#24179;&#22343;&#25913;&#36827;&#30456;&#20284;&#12290;&#27492;&#22806;&#65292;
&lt;/p&gt;
&lt;p&gt;
As the field of automated machine learning (AutoML) advances, it becomes increasingly important to include domain knowledge within these systems. We present an approach for doing so by harnessing the power of large language models (LLMs). Specifically, we introduce Context-Aware Automated Feature Engineering (CAAFE), a feature engineering method for tabular datasets that utilizes an LLM to generate additional semantically meaningful features for tabular datasets based on their descriptions. The method produces both Python code for creating new features and explanations for the utility of the generated features.  Despite being methodologically simple, CAAFE enhances performance on 11 out of 14 datasets, ties on 2 and looses on 1 - boosting mean ROC AUC performance from 0.798 to 0.822 across all datasets. On the evaluated datasets, this improvement is similar to the average improvement achieved by using a random forest (AUC 0.782) instead of logistic regression (AUC 0.754).  Furthermore,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#20004;&#20010;&#23454;&#29992;&#30340;&#35774;&#32622;&#20986;&#21457;&#65292;&#20998;&#26512;&#27604;&#36739;&#20102;&#21313;&#31181;&#20195;&#34920;&#24615;&#30340;&#23567;&#26679;&#26412;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#24402;&#32435;&#24635;&#32467;&#20986;&#20102;&#21407;&#22411;&#26041;&#27861;&#30340;&#24615;&#33021;&#20248;&#36234;&#24615;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.01901</link><description>&lt;p&gt;
&#23567;&#26679;&#26412;&#20107;&#20214;&#26816;&#27979;&#65306;&#32463;&#39564;&#30740;&#31350;&#21644;&#32479;&#19968;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Few-shot Event Detection: An Empirical Study and a Unified View. (arXiv:2305.01901v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#20004;&#20010;&#23454;&#29992;&#30340;&#35774;&#32622;&#20986;&#21457;&#65292;&#20998;&#26512;&#27604;&#36739;&#20102;&#21313;&#31181;&#20195;&#34920;&#24615;&#30340;&#23567;&#26679;&#26412;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#24402;&#32435;&#24635;&#32467;&#20986;&#20102;&#21407;&#22411;&#26041;&#27861;&#30340;&#24615;&#33021;&#20248;&#36234;&#24615;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#26679;&#26412;&#20107;&#20214;&#26816;&#27979; (ED) &#24050;&#32463;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#28982;&#32780;&#36825;&#20063;&#24102;&#26469;&#20102;&#26126;&#26174;&#30340;&#24046;&#24322;&#65292;&#20363;&#22914;&#21508;&#31181;&#21160;&#26426;&#12289;&#20219;&#21153;&#21644;&#23454;&#39564;&#35774;&#32622;&#65292;&#36825;&#20123;&#24046;&#24322;&#22952;&#30861;&#20102;&#27169;&#22411;&#30340;&#29702;&#35299;&#21644;&#26410;&#26469;&#36827;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#24443;&#24213;&#30340;&#32463;&#39564;&#30740;&#31350;&#12289;&#19968;&#20010;ED&#27169;&#22411;&#30340;&#32479;&#19968;&#35270;&#35282;&#21644;&#19968;&#20010;&#26356;&#22909;&#30340;&#32479;&#19968;&#22522;&#20934;&#32447;&#12290;&#20026;&#20102;&#20844;&#24179;&#35780;&#20272;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#20004;&#20010;&#23454;&#29992;&#30340;&#35774;&#32622;&#65306;&#20302;&#36164;&#28304;&#35774;&#32622;&#26469;&#35780;&#20272;&#27867;&#21270;&#33021;&#21147;&#21644;&#31867;&#36716;&#31227;&#35774;&#32622;&#26469;&#35780;&#20272;&#21487;&#36716;&#31227;&#24615;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#21313;&#31181;&#20195;&#34920;&#24615;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#22823;&#33268;&#34987;&#20998;&#20026;&#22522;&#20110;&#25552;&#31034;&#21644;&#22522;&#20110;&#21407;&#22411;&#30340;&#27169;&#22411;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#12290;&#20026;&#20102;&#35843;&#26597;&#22522;&#20110;&#21407;&#22411;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#65292;&#25105;&#20204;&#20998;&#35299;&#20102;&#35774;&#35745;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#19981;&#20165;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65288;&#20363;&#22914;&#22312;&#20302;&#36164;&#28304;&#35774;&#32622;&#19979;&#33719;&#24471;2.7&#65285;F1&#25910;&#30410;&#65289;&#65292;&#32780;&#19988;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#35768;&#22810;&#26377;&#20215;&#20540;&#30340;&#30740;&#31350;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot event detection (ED) has been widely studied, while this brings noticeable discrepancies, e.g., various motivations, tasks, and experimental settings, that hinder the understanding of models for future progress. This paper presents a thorough empirical study, a unified view of ED models, and a better unified baseline. For fair evaluation, we choose two practical settings: low-resource setting to assess generalization ability and class-transfer setting for transferability. We compare ten representative methods on three datasets, which are roughly grouped into prompt-based and prototype-based models for detailed analysis. To investigate the superior performance of prototype-based methods, we break down the design and build a unified framework. Based on that, we not only propose a simple yet effective method (e.g., 2.7% F1 gains under low-resource setting) but also offer many valuable research insights for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#32806;&#26041;&#27861;&#65292;&#23558;&#22797;&#26434;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#24418;&#24335;&#21270;&#20026;&#19978;&#19979;&#25991;&#30693;&#35782;&#22270;&#25628;&#32034;&#21644;&#25277;&#35937;&#36923;&#36753;&#26597;&#35810;&#25512;&#29702;&#30340;&#32452;&#21512;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#22312;&#22810;&#20010;&#36923;&#36753;&#26597;&#35810;&#32467;&#26500;&#30340;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#37117;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#26356;&#39640;&#22797;&#26434;&#24615;&#30340;&#26597;&#35810;&#20013;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.01157</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#22797;&#26434;&#36923;&#36753;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Complex Logical Reasoning over Knowledge Graphs using Large Language Models. (arXiv:2305.01157v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#32806;&#26041;&#27861;&#65292;&#23558;&#22797;&#26434;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#24418;&#24335;&#21270;&#20026;&#19978;&#19979;&#25991;&#30693;&#35782;&#22270;&#25628;&#32034;&#21644;&#25277;&#35937;&#36923;&#36753;&#26597;&#35810;&#25512;&#29702;&#30340;&#32452;&#21512;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#22312;&#22810;&#20010;&#36923;&#36753;&#26597;&#35810;&#32467;&#26500;&#30340;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#37117;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#26356;&#39640;&#22797;&#26434;&#24615;&#30340;&#26597;&#35810;&#20013;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#25512;&#29702;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23427;&#38656;&#35201;&#23545;&#23454;&#20307;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#22522;&#30784;&#36923;&#36753;&#36827;&#34892;&#28145;&#20837;&#29702;&#35299;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#23398;&#20064;&#20960;&#20309;&#26469;&#23884;&#20837;&#23454;&#20307;&#30340;&#21521;&#37327;&#31354;&#38388;&#36827;&#34892;&#36923;&#36753;&#26597;&#35810;&#25805;&#20316;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#22797;&#26434;&#26597;&#35810;&#21644;&#29305;&#23450;&#25968;&#25454;&#38598;&#34920;&#31034;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#32806;&#26041;&#27861;&#65292;&#31216;&#20026;&#22522;&#20110;&#35821;&#35328;&#24341;&#23548;&#30340;&#30693;&#35782;&#22270;&#35889;&#25277;&#35937;&#25512;&#29702;&#65288;LARK&#65289;&#65292;&#23558;&#22797;&#26434;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#24418;&#24335;&#21270;&#20026;&#19978;&#19979;&#25991;&#30693;&#35782;&#22270;&#25628;&#32034;&#21644;&#25277;&#35937;&#36923;&#36753;&#26597;&#35810;&#25512;&#29702;&#30340;&#32452;&#21512;&#65292;&#20197;&#20998;&#21035;&#21033;&#29992;&#22270;&#24418;&#25552;&#21462;&#31639;&#27861;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#36923;&#36753;&#26597;&#35810;&#32467;&#26500;&#30340;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#27861;&#65292;&#22312;&#26356;&#39640;&#22797;&#26434;&#24615;&#30340;&#26597;&#35810;&#20013;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning over knowledge graphs (KGs) is a challenging task that requires a deep understanding of the complex relationships between entities and the underlying logic of their relations. Current approaches rely on learning geometries to embed entities in vector space for logical query operations, but they suffer from subpar performance on complex queries and dataset-specific representations. In this paper, we propose a novel decoupled approach, Language-guided Abstract Reasoning over Knowledge graphs (LARK), that formulates complex KG reasoning as a combination of contextual KG search and abstract logical query reasoning, to leverage the strengths of graph extraction algorithms and large language models (LLM), respectively. Our experiments demonstrate that the proposed approach outperforms state-of-the-art KG reasoning methods on standard benchmark datasets across several logical query constructs, with significant performance gain for queries of higher complexity. Furthermore, we show t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#22522;&#20110;&#28216;&#25103;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#24179;&#21488;&#65292;&#35752;&#35770;&#20102;&#19981;&#21516;&#30740;&#31350;&#39046;&#22495;&#21644;&#21019;&#24847;&#35774;&#35745;&#22312;&#20854;&#20013;&#30340;&#24212;&#29992;&#21644;&#21457;&#23637;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#26410;&#26469;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2304.13269</link><description>&lt;p&gt;
&#22522;&#20110;&#28216;&#25103;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Game-based Platforms for Artificial Intelligence Research. (arXiv:2304.13269v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#22522;&#20110;&#28216;&#25103;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#24179;&#21488;&#65292;&#35752;&#35770;&#20102;&#19981;&#21516;&#30740;&#31350;&#39046;&#22495;&#21644;&#21019;&#24847;&#35774;&#35745;&#22312;&#20854;&#20013;&#30340;&#24212;&#29992;&#21644;&#21457;&#23637;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#26410;&#26469;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28216;&#25103;&#20855;&#26377;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#30340;&#24191;&#27867;&#29305;&#24449;&#65292;&#25104;&#20026;&#20102;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#29702;&#24819;&#27979;&#35797;&#22522;&#22320;&#65292;&#20849;&#21516;&#30340;&#30740;&#31350;&#39046;&#22495;&#21253;&#25324;&#23398;&#20064;&#21644;&#20248;&#21270;&#12289;&#21160;&#24577;&#21644;&#19981;&#30830;&#23450;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#21046;&#23450;&#12289;&#21338;&#24328;&#35770;&#12289;&#35745;&#21010;&#19982;&#25490;&#31243;&#12289;&#35774;&#35745;&#21644;&#25945;&#32946;&#31561;&#12290;&#24050;&#23454;&#26045;&#20102;&#35768;&#22810;&#24320;&#28304;&#28216;&#25103;&#25110;&#22522;&#20110;&#28216;&#25103;&#30340;&#29615;&#22659;&#29992;&#20110;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#12290;&#38500;&#20102;&#21333;&#20154;&#25110;&#22810;&#20154;&#12289;&#21512;&#20316;&#25110;&#23545;&#25239;&#24615;&#28216;&#25103;&#22806;&#65292;&#22312;&#21019;&#24847;&#35774;&#35745;&#26041;&#38754;&#20063;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#36825;&#20123;&#24179;&#21488;&#20026;&#25506;&#32034;&#21644;&#27604;&#36739;&#20154;&#24037;&#26234;&#33021;&#30340;&#24605;&#24819;&#21644;&#25216;&#26415;&#25552;&#20379;&#20102;&#29702;&#24819;&#22522;&#20934;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#22522;&#20110;&#28216;&#25103;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#24179;&#21488;&#65292;&#35752;&#35770;&#20102;&#30001;&#36825;&#20123;&#24179;&#21488;&#28436;&#21464;&#24341;&#36215;&#30340;&#30740;&#31350;&#36235;&#21183;&#65292;&#24182;&#23637;&#26395;&#20102;&#26410;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Games have been the perfect test-beds for artificial intelligence research for the characteristics that widely exist in real-world scenarios. Learning and optimisation, decision making in dynamic and uncertain environments, game theory, planning and scheduling, design and education are common research areas shared between games and real-world problems. Numerous open-sourced games or game-based environments have been implemented for studying artificial intelligence. In addition to single- or multi-player, collaborative or adversarial games, there has also been growing interest in implementing platforms for creative design in recent years. Those platforms provide ideal benchmarks for exploring and comparing artificial intelligence ideas and techniques. This paper reviews the game-based platforms for artificial intelligence research, discusses the research trend induced by the evolution of those platforms, and gives an outlook.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#37327;&#23376;&#28216;&#36208;&#30340;&#31639;&#27861;&#35299;&#20915;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#65292;&#23558;&#25506;&#32034;&#21644;&#21033;&#29992;&#19982;&#37327;&#23376;&#28216;&#36208;&#34892;&#20026;&#32852;&#31995;&#36215;&#26469;&#65292;&#30456;&#27604;&#20110;&#32463;&#20856;&#38543;&#26426;&#28216;&#36208;&#31574;&#30053;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.10118</link><description>&lt;p&gt;
&#19968;&#20010;&#30001;&#32463;&#20856;&#38543;&#26426;&#28216;&#36208;&#21644;&#37327;&#23376;&#28216;&#36208;&#39537;&#21160;&#30340;&#36172;&#21338;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bandit Algorithm Driven by a Classical Random Walk and a Quantum Walk. (arXiv:2304.10118v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#37327;&#23376;&#28216;&#36208;&#30340;&#31639;&#27861;&#35299;&#20915;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#65292;&#23558;&#25506;&#32034;&#21644;&#21033;&#29992;&#19982;&#37327;&#23376;&#28216;&#36208;&#34892;&#20026;&#32852;&#31995;&#36215;&#26469;&#65292;&#30456;&#27604;&#20110;&#32463;&#20856;&#38543;&#26426;&#28216;&#36208;&#31574;&#30053;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#28216;&#36208;&#20855;&#26377;&#32463;&#20856;&#38543;&#26426;&#28216;&#36208;&#25152;&#19981;&#20855;&#22791;&#30340;&#23646;&#24615;&#8212;&#8212;&#32447;&#24615;&#20256;&#25773;&#21644;&#23616;&#37096;&#21270;&#20849;&#23384;&#8212;&#8212;&#24182;&#19988;&#36825;&#31181;&#23646;&#24615;&#34987;&#29992;&#20110;&#23454;&#29616;&#21508;&#31181;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#28216;&#36208;&#30340;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#31639;&#27861;&#65292;&#23558;&#20351;&#36172;&#21338;&#38382;&#39064;&#22256;&#38590;&#30340;&#20004;&#31181;&#25805;&#20316;&#8212;&#8212;&#25506;&#32034;&#21644;&#21033;&#29992;&#8212;&#8212;&#19982;&#36825;&#20004;&#31181;&#37327;&#23376;&#28216;&#36208;&#34892;&#20026;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#22522;&#20110;&#37327;&#23376;&#28216;&#36208;&#30340;&#26032;&#31574;&#30053;&#30456;&#27604;&#20110;&#30456;&#24212;&#30340;&#38543;&#26426;&#28216;&#36208;&#31574;&#30053;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum walks (QWs) have the property that classical random walks (RWs) do not possess -- coexistence of linear spreading and localization -- and this property is utilized to implement various kinds of applications. This paper proposes a quantum-walk-based algorithm for multi-armed-bandit (MAB) problems by associating the two operations that make MAB problems difficult -exploration and exploitation -- with these two behaviors of QWs. We show that this new policy based on the QWs realizes high performance compared with the corresponding RW-based one.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22522;&#20110;Bandit&#26041;&#27861;&#23558;&#22806;&#37096;&#24314;&#35758;&#34701;&#20837;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#22609;&#24418;&#31639;&#27861;&#65306;UCB-PIES&#65288;UPIES&#65289;&#65292; Racing-PIES&#65288;RPIES&#65289;&#21644;Lazy PIES&#65288;LPIES&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#31639;&#27861;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#12289;&#23398;&#20064;&#36895;&#24230;&#21644;&#24418;&#29366;&#36136;&#37327;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.07163</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#22522;&#20110;Bandit&#26041;&#27861;&#30340;&#26174;&#24335;&#22609;&#24418;&#22806;&#37096;&#24314;&#35758;&#31639;&#27861;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Bandit-Based Policy Invariant Explicit Shaping for Incorporating External Advice in Reinforcement Learning. (arXiv:2304.07163v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22522;&#20110;Bandit&#26041;&#27861;&#23558;&#22806;&#37096;&#24314;&#35758;&#34701;&#20837;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#22609;&#24418;&#31639;&#27861;&#65306;UCB-PIES&#65288;UPIES&#65289;&#65292; Racing-PIES&#65288;RPIES&#65289;&#21644;Lazy PIES&#65288;LPIES&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#31639;&#27861;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#12289;&#23398;&#20064;&#36895;&#24230;&#21644;&#24418;&#29366;&#36136;&#37327;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#22914;&#20309;&#23558;&#22806;&#37096;&#25110;&#19987;&#23478;&#30340;&#24314;&#35758;&#34701;&#20837;&#21040;&#23398;&#20064;&#24403;&#20013;&#12290;&#26412;&#25991;&#23558;&#23558;&#23558;&#27492;&#38382;&#39064;&#34920;&#36848;&#20026;&#19968;&#31181;&#22810;&#33218;&#36172;&#21338;&#26426;&#31216;&#20026;&#22609;&#24418;&#36172;&#21338;&#26426;&#65288;shaping-bandits&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#22609;&#24418;&#31639;&#27861;&#65306;UCB-PIES&#65288;UPIES&#65289;&#65292; Racing-PIES&#65288;RPIES&#65289;&#21644;Lazy PIES&#65288;LPIES&#65289;&#12290;&#36890;&#36807;&#22312;&#27169;&#25311;&#29615;&#22659;&#21644;LQR&#21644;Atari&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#19977;&#31181;&#31639;&#27861;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#12289;&#23398;&#20064;&#36895;&#24230;&#21644;&#24418;&#29366;&#36136;&#37327;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key challenge for a reinforcement learning (RL) agent is to incorporate external/expert1 advice in its learning. The desired goals of an algorithm that can shape the learning of an RL agent with external advice include (a) maintaining policy invariance; (b) accelerating the learning of the agent; and (c) learning from arbitrary advice [3]. To address this challenge this paper formulates the problem of incorporating external advice in RL as a multi-armed bandit called shaping-bandits. The reward of each arm of shaping bandits corresponds to the return obtained by following the expert or by following a default RL algorithm learning on the true environment reward.We show that directly applying existing bandit and shaping algorithms that do not reason about the non-stationary nature of the underlying returns can lead to poor results. Thus we propose UCB-PIES (UPIES), Racing-PIES (RPIES), and Lazy PIES (LPIES) three different shaping algorithms built on different assumptions that reason a
&lt;/p&gt;</description></item><item><title>RAFT&#26694;&#26550;&#24341;&#20837;&#20102;&#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#40784;&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#24102;&#26469;&#30340;&#20302;&#25928;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.06767</link><description>&lt;p&gt;
RAFT: &#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#29992;&#20110;&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment. (arXiv:2304.06767v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06767
&lt;/p&gt;
&lt;p&gt;
RAFT&#26694;&#26550;&#24341;&#20837;&#20102;&#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#40784;&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#24102;&#26469;&#30340;&#20302;&#25928;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#24191;&#27867;&#30340;&#26080;&#30417;&#30563;&#35757;&#32451;&#25968;&#25454;&#24102;&#26469;&#30340;&#38544;&#24335;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#20559;&#35265;&#21487;&#33021;&#23548;&#33268;&#23376;&#20248;&#26679;&#26412;&#12289;&#25197;&#26354;&#30340;&#32467;&#26524;&#21644;&#19981;&#20844;&#24179;&#65292;&#21487;&#33021;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#19982;&#20154;&#30340;&#20262;&#29702;&#21644;&#20559;&#22909;&#23545;&#40784;&#26159;&#30830;&#20445;&#23427;&#20204;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#36127;&#36131;&#20219;&#21644;&#26377;&#25928;&#30340;&#37096;&#32626;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#37319;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288; RLHF&#65289;&#20316;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#25163;&#27573;&#12290;&#22312; RL &#31639;&#27861;&#30340;&#25351;&#23548;&#19979;&#65292;&#29992;&#20154;&#31867;&#21453;&#39304;&#25351;&#23548;&#30340;&#22870;&#21169;&#27169;&#22411;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292; RL &#31639;&#27861;&#30340;&#20302;&#25928;&#24615;&#21644;&#19981;&#31283;&#23450;&#24615;&#24120;&#24120;&#20250;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#25104;&#21151;&#23545;&#40784;&#20135;&#29983;&#37325;&#22823;&#38556;&#30861;&#65292;&#22240;&#27492;&#38656;&#35201;&#24320;&#21457;&#19968;&#31181;&#26356;&#20026;&#24378;&#22823;&#21644;&#31616;&#21270;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#65288; RAFT &#65289;&#65292;&#26088;&#22312;&#23545;&#40784;&#29983;&#25104;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative foundation models are susceptible to implicit biases that can arise from extensive unsupervised training data. Such biases can produce suboptimal samples, skewed outcomes, and unfairness, with potentially significant repercussions. Consequently, aligning these models with human ethics and preferences is an essential step toward ensuring their responsible and effective deployment in real-world applications. Prior research has primarily employed Reinforcement Learning from Human Feedback (RLHF) as a means of addressing this problem, wherein generative models are fine-tuned using RL algorithms guided by a human-feedback-informed reward model. However, the inefficiencies and instabilities associated with RL algorithms frequently present substantial obstacles to the successful alignment of generative models, necessitating the development of a more robust and streamlined approach. To this end, we introduce a new framework, Reward rAnked FineTuning (RAFT), designed to align generat
&lt;/p&gt;</description></item><item><title>LLMMaps&#26159;&#19968;&#31181;&#20998;&#23618;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#33021;&#22815;&#25581;&#31034;&#21462;&#24471;&#39640;&#20934;&#30830;&#24230;&#21644;&#20135;&#29983;&#24187;&#35273;&#30340;&#23376;&#39046;&#22495;&#65292;&#24182;&#25351;&#23548;&#27169;&#22411;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2304.00457</link><description>&lt;p&gt;
LLMMaps&#8212;&#8212;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#23618;&#35780;&#20215;&#30340;&#21487;&#35270;&#21270;&#38544;&#21947;
&lt;/p&gt;
&lt;p&gt;
LLMMaps -- A Visual Metaphor for Stratified Evaluation of Large Language Models. (arXiv:2304.00457v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00457
&lt;/p&gt;
&lt;p&gt;
LLMMaps&#26159;&#19968;&#31181;&#20998;&#23618;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#33021;&#22815;&#25581;&#31034;&#21462;&#24471;&#39640;&#20934;&#30830;&#24230;&#21644;&#20135;&#29983;&#24187;&#35273;&#30340;&#23376;&#39046;&#22495;&#65292;&#24182;&#25351;&#23548;&#27169;&#22411;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23481;&#26131;&#20135;&#29983;&#24187;&#35273;&#65292;&#21363;&#27169;&#22411;&#22312;&#21709;&#24212;&#20013;&#26292;&#38706;&#20986;&#19981;&#27491;&#30830;&#25110;&#38169;&#35823;&#30340;&#20449;&#24687;&#65292;&#36825;&#20351;&#24471;&#24517;&#39035;&#37319;&#29992;&#21220;&#22859;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;&#34429;&#28982;LLM&#22312;&#29305;&#23450;&#30693;&#35782;&#39046;&#22495;&#20013;&#30340;&#34920;&#29616;&#36890;&#24120;&#26159;&#22522;&#20110;&#38382;&#31572;(Q&amp;A)&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#20294;&#36825;&#20123;&#35780;&#20272;&#36890;&#24120;&#20165;&#25253;&#21578;&#25972;&#20010;&#39046;&#22495;&#30340;&#21333;&#20010;&#20934;&#30830;&#24230;&#25968;&#23383;&#65292;&#36825;&#19968;&#31243;&#24207;&#22312;&#36879;&#26126;&#24230;&#21644;&#27169;&#22411;&#25913;&#36827;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#20998;&#23618;&#35780;&#20272;&#21487;&#20197;&#25581;&#31034;&#21487;&#33021;&#26356;&#23481;&#26131;&#21457;&#29983;&#24187;&#35273;&#30340;&#23376;&#39046;&#22495;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#35780;&#20272;LLMs&#30340;&#39118;&#38505;&#24182;&#25351;&#23548;&#23427;&#20204;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;&#20026;&#25903;&#25345;&#36825;&#26679;&#30340;&#20998;&#23618;&#35780;&#20272;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLMMaps&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#26681;&#25454;Q&amp;A&#25968;&#25454;&#38598;&#35780;&#20272;LLMs&#30340;&#24615;&#33021;&#12290;LLMMaps&#25552;&#20379;&#20102;&#23545;LLMs&#22312;&#19981;&#21516;&#23376;&#39046;&#22495;&#20013;&#30340;&#30693;&#35782;&#20998;&#24067;&#30340;&#35814;&#32454;&#27934;&#23519;&#65292;&#20801;&#35768;&#29992;&#25143;&#25918;&#22823;&#39046;&#22495;&#30340;&#29305;&#23450;&#37096;&#20998;&#24182;&#25506;&#32034;&#27169;&#22411;&#24615;&#33021;&#19978;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;LLMMaps&#26377;&#21161;&#20110;&#35782;&#21035;&#20986;&#26356;&#23481;&#26131;&#20986;&#29616;LLM&#24187;&#35273;&#30340;&#23376;&#39046;&#22495;&#65292;&#24182;&#21487;&#20197;&#25351;&#23548;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#20197;&#25913;&#21892;&#36825;&#20123;&#39046;&#22495;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have revolutionized natural language processing and demonstrated impressive capabilities in various tasks. Unfortunately, they are prone to hallucinations, where the model exposes incorrect or false information in its responses, which renders diligent evaluation approaches mandatory. While LLM performance in specific knowledge fields is often evaluated based on question and answer (Q&amp;A) datasets, such evaluations usually report only a single accuracy number for the entire field, a procedure which is problematic with respect to transparency and model improvement. A stratified evaluation could instead reveal subfields, where hallucinations are more likely to occur and thus help to better assess LLMs' risks and guide their further development. To support such stratified evaluations, we propose LLMMaps as a novel visualization technique that enables users to evaluate LLMs' performance with respect to Q&amp;A datasets. LLMMaps provide detailed insights into LLMs' kn
&lt;/p&gt;</description></item><item><title>&#29992;ChatGPT&#20316;&#20026;&#20219;&#21153;&#35268;&#21010;&#24037;&#20855;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#25511;&#21046;&#22120;&#26469;&#25972;&#21512;&#29616;&#26377;&#30340;AI&#27169;&#22411;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;AI&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.17580</link><description>&lt;p&gt;
HuggingGPT: &#22312;HugingFace&#20013;&#20351;&#29992;ChatGPT&#21450;&#20854;&#20249;&#20276;&#35299;&#20915;AI&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace. (arXiv:2303.17580v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17580
&lt;/p&gt;
&lt;p&gt;
&#29992;ChatGPT&#20316;&#20026;&#20219;&#21153;&#35268;&#21010;&#24037;&#20855;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#25511;&#21046;&#22120;&#26469;&#25972;&#21512;&#29616;&#26377;&#30340;AI&#27169;&#22411;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;AI&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#21644;&#27169;&#24577;&#30340;&#22797;&#26434;AI&#20219;&#21153;&#26159;&#36890;&#21521;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#25511;&#21046;&#22120;&#26469;&#31649;&#29702;&#29616;&#26377;&#30340;AI&#27169;&#22411;&#20197;&#35299;&#20915;AI&#20219;&#21153;&#65292;&#35821;&#35328;&#25104;&#20026;&#36890;&#29992;&#25509;&#21475;&#26469;&#36171;&#33021;&#23427;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;ChatGPT&#20316;&#20026;&#20219;&#21153;&#35268;&#21010;&#24037;&#20855;&#65292;&#26681;&#25454;HuggingFace&#20013;&#21487;&#29992;&#30340;&#27169;&#22411;&#21151;&#33021;&#25551;&#36848;&#26469;&#36873;&#25321;&#27169;&#22411;&#65292;&#22312;&#36873;&#23450;AI&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#27599;&#20010;&#23376;&#20219;&#21153;&#65292;&#24182;&#24635;&#32467;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence (AGI). While there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks. Considering large language models (LLMs) have exhibited exceptional ability in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this. Based on this philosophy, we present HuggingGPT, a system that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., HuggingFace) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in HuggingFace, execute each subtask with the selected AI model, and summarize the response acco
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;ISimDL&#65292;&#21033;&#29992;&#31070;&#32463;&#20803;&#28789;&#25935;&#24230;&#29983;&#25104;&#37325;&#35201;&#24615;&#37319;&#26679;&#65292;&#21152;&#36895;&#25925;&#38556;&#27880;&#20837;&#27169;&#25311;&#65292;&#26377;&#25928;&#35780;&#20272;&#20102;&#20808;&#36827;&#30340;DL&#31995;&#32479;&#23545;&#30828;&#20214;&#25925;&#38556;&#30340;&#38887;&#24615;&#65292;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#27169;&#25311;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.08035</link><description>&lt;p&gt;
ISimDL: &#36890;&#36807;&#37325;&#35201;&#24615;&#37319;&#26679;&#39537;&#21160;&#30340;&#25925;&#38556;&#27880;&#20837;&#27169;&#25311;&#65292;&#21152;&#36895;&#28145;&#24230;&#23398;&#20064;&#24378;&#20581;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
ISimDL: Importance Sampling-Driven Acceleration of Fault Injection Simulations for Evaluating the Robustness of Deep Learning. (arXiv:2303.08035v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;ISimDL&#65292;&#21033;&#29992;&#31070;&#32463;&#20803;&#28789;&#25935;&#24230;&#29983;&#25104;&#37325;&#35201;&#24615;&#37319;&#26679;&#65292;&#21152;&#36895;&#25925;&#38556;&#27880;&#20837;&#27169;&#25311;&#65292;&#26377;&#25928;&#35780;&#20272;&#20102;&#20808;&#36827;&#30340;DL&#31995;&#32479;&#23545;&#30828;&#20214;&#25925;&#38556;&#30340;&#38887;&#24615;&#65292;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#27169;&#25311;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;(DL)&#31995;&#32479;&#24050;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#24191;&#27867;&#24212;&#29992;&#65292;&#38656;&#35201;&#19987;&#29992;&#30340;&#30828;&#20214;&#21152;&#36895;&#22120;&#21644;&#33455;&#29255;&#12290;&#22312;&#32435;&#31859;&#26102;&#20195;&#65292;&#35774;&#22791;&#36234;&#26469;&#36234;&#23481;&#26131;&#21463;&#21040;&#27704;&#20037;&#24615;&#21644;&#30636;&#21464;&#25925;&#38556;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#20998;&#26512;&#20808;&#36827;&#30340;DL&#31995;&#32479;&#23545;&#27492;&#31867;&#25925;&#38556;&#30340;&#38887;&#24615;&#65292;&#24182;&#20102;&#35299;&#31070;&#32463;&#21152;&#36895;&#22120;&#33455;&#29255;&#20013;&#30340;&#25925;&#38556;&#22914;&#20309;&#22312;DL&#24212;&#29992;&#32423;&#21035;&#19978;&#34920;&#29616;&#20026;&#38169;&#35823;&#65292;&#20854;&#20013;&#25925;&#38556;&#21487;&#33021;&#23548;&#33268;&#26080;&#27861;&#26816;&#27979;&#21644;&#24674;&#22797;&#30340;&#38169;&#35823;&#12290;&#20351;&#29992;&#25925;&#38556;&#27880;&#20837;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#22312;&#36719;&#20214;&#32423;&#21035;&#20462;&#25913;&#31070;&#32463;&#20803;&#26435;&#37325;&#21644;&#36755;&#20986;&#26469;&#25191;&#34892;DL&#31995;&#32479;&#30340;&#38887;&#24615;&#30740;&#31350;&#65292;&#23601;&#22909;&#20687;&#30828;&#20214;&#21463;&#21040;&#30636;&#21464;&#25925;&#38556;&#30340;&#24433;&#21709;&#19968;&#26679;&#12290;&#29616;&#26377;&#30340;&#25925;&#38556;&#27169;&#22411;&#20943;&#23569;&#20102;&#25628;&#32034;&#31354;&#38388;&#65292;&#20351;&#20998;&#26512;&#26356;&#24555;&#65292;&#20294;&#38656;&#35201;&#35813;&#27169;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#19988;&#19981;&#20801;&#35768;&#36827;&#19968;&#27493;&#20998;&#26512;&#31579;&#36873;&#20986;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ISimDL&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#31070;&#32463;&#20803;&#28789;&#25935;&#24230;&#29983;&#25104;&#37325;&#35201;&#24615;&#37319;&#26679;&#65292;&#24182;&#21152;&#36895;&#25925;&#38556;&#27880;&#20837;&#27169;&#25311;&#12290;ISimDL&#21487;&#20197;&#26377;&#25928;&#35780;&#20272;&#20808;&#36827;&#30340;DL&#31995;&#32479;&#23545;&#30828;&#20214;&#25925;&#38556;&#30340;&#38887;&#24615;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#30528;&#20943;&#23569;&#20102;&#25925;&#38556;&#27880;&#20837;&#20998;&#26512;&#25152;&#38656;&#30340;&#27169;&#25311;&#25968;&#37327;&#65292;&#21516;&#26102;&#20173;&#30830;&#20445;&#36275;&#22815;&#35206;&#30422;&#25628;&#32034;&#31354;&#38388;&#12290;&#25105;&#20204;&#23558;ISimDL&#24212;&#29992;&#20110;&#20195;&#34920;&#24615;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;CIFAR-10&#21644;ImageNet&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#23427;&#25552;&#20379;&#26174;&#33879;&#30340;&#21152;&#36895;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#25925;&#38556;&#27880;&#20837;&#26041;&#27861;&#30456;&#21516;&#30340;&#20934;&#30830;&#24615;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) systems have proliferated in many applications, requiring specialized hardware accelerators and chips. In the nano-era, devices have become increasingly more susceptible to permanent and transient faults. Therefore, we need an efficient methodology for analyzing the resilience of advanced DL systems against such faults, and understand how the faults in neural accelerator chips manifest as errors at the DL application level, where faults can lead to undetectable and unrecoverable errors. Using fault injection, we can perform resilience investigations of the DL system by modifying neuron weights and outputs at the software-level, as if the hardware had been affected by a transient fault. Existing fault models reduce the search space, allowing faster analysis, but requiring a-priori knowledge on the model, and not allowing further analysis of the filtered-out search space. Therefore, we propose ISimDL, a novel methodology that employs neuron sensitivity to generate impo
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#37327;&#23376;-&#32463;&#20856;&#28151;&#21512;&#26694;&#26550;&#65292;&#21517;&#20026;Q4RealBPP&#65292;&#21487;&#20197;&#32771;&#34385;&#30495;&#23454;&#19990;&#30028;&#30340;&#38480;&#21046;&#29305;&#24449;&#65292;&#35299;&#20915;&#19977;&#32500;&#35013;&#31665;&#38382;&#39064;&#65292;&#25903;&#25345;&#24037;&#19994;&#21644;&#29289;&#27969;&#34892;&#19994;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2303.01977</link><description>&lt;p&gt;
&#29992;&#37327;&#23376;&#36864;&#28779;&#22120;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#35013;&#31665;&#38382;&#39064;&#30340;&#28151;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hybrid Approach for Solving Real-World Bin Packing Problem Instances Using Quantum Annealers. (arXiv:2303.01977v2 [cs.ET] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01977
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#37327;&#23376;-&#32463;&#20856;&#28151;&#21512;&#26694;&#26550;&#65292;&#21517;&#20026;Q4RealBPP&#65292;&#21487;&#20197;&#32771;&#34385;&#30495;&#23454;&#19990;&#30028;&#30340;&#38480;&#21046;&#29305;&#24449;&#65292;&#35299;&#20915;&#19977;&#32500;&#35013;&#31665;&#38382;&#39064;&#65292;&#25903;&#25345;&#24037;&#19994;&#21644;&#29289;&#27969;&#34892;&#19994;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22320;&#23558;&#29289;&#21697;&#35013;&#20837;&#31665;&#23376;&#26159;&#19968;&#39033;&#24120;&#35265;&#30340;&#26085;&#24120;&#20219;&#21153;&#65292;&#20063;&#34987;&#31216;&#20026;&#8220;&#35013;&#31665;&#38382;&#39064;&#8221;&#12290;&#30001;&#20110;&#34892;&#19994;&#21644;&#29289;&#27969;&#39046;&#22495;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#23427;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#33258;&#20960;&#21313;&#24180;&#20197;&#26469;&#65292;&#35768;&#22810;&#21464;&#31181;&#24050;&#34987;&#25552;&#20986;&#65292;&#20854;&#20013;&#19977;&#32500;&#35013;&#31665;&#38382;&#39064;&#26159;&#26368;&#25509;&#36817;&#23454;&#38469;&#29992;&#20363;&#30340;&#19968;&#20010;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#28151;&#21512;&#30340;&#37327;&#23376;-&#32463;&#20856;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#32771;&#34385;&#19981;&#21516;&#29616;&#23454;&#29305;&#24449;&#30340;&#19977;&#32500;&#35013;&#31665;&#38382;&#39064;&#65288;Q4RealBPP&#65289;&#65292;&#20363;&#22914;&#65306;&#65288;i&#65289;&#21253;&#35013;&#21644;&#31665;&#23376;&#23610;&#23544;&#65292;&#65288;ii&#65289;&#36229;&#37325;&#38480;&#21046;&#65292;&#65288;iii&#65289;&#29289;&#21697;&#31867;&#21035;&#20043;&#38388;&#30340;&#20146;&#21644;&#21147;&#21644;&#65288;iv&#65289;&#29289;&#21697;&#25490;&#24207;&#30340;&#20559;&#22909;&#12290;Q4RealBPP&#20801;&#35768;&#35299;&#20915;&#32771;&#34385;&#21040;&#24037;&#19994;&#21644;&#29289;&#27969;&#37096;&#38376;&#24191;&#27867;&#35780;&#20215;&#30340;&#38480;&#21046;&#30340;3dBPP&#30340;&#29616;&#23454;&#23548;&#21521;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient packing of items into bins is a common daily task. Known as Bin Packing Problem, it has been intensively studied in the field of artificial intelligence, thanks to the wide interest from industry and logistics. Since decades, many variants have been proposed, with the three-dimensional Bin Packing Problem as the closest one to real-world use cases. We introduce a hybrid quantum-classical framework for solving real-world three-dimensional Bin Packing Problems (Q4RealBPP), considering different realistic characteristics, such as: i) package and bin dimensions, ii) overweight restrictions, iii) affinities among item categories and iv) preferences for item ordering. Q4RealBPP permits the solving of real-world oriented instances of 3dBPP, contemplating restrictions well appreciated by industrial and logistics sectors.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#33021;&#28304;&#24066;&#22330;&#28165;&#31639;&#21644;&#20986;&#20215;&#30340;&#24212;&#29992;&#26041;&#27861;&#12290;&#36890;&#36807;&#29992;&#23398;&#20064;&#30340;OPF&#20195;&#29702;&#27169;&#22411;&#20197;&#21450;&#26126;&#30830;&#30340;&#24066;&#22330;&#35268;&#21017;&#26367;&#20195;&#20256;&#32479;&#35745;&#31639;&#26041;&#27861;&#65292;&#26412;&#26041;&#27861;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#24182;&#36866;&#29992;&#20110;&#24066;&#22330;&#35774;&#35745;&#21644;&#26356;&#29616;&#23454;&#22320;&#24314;&#27169;&#24066;&#22330;&#21442;&#19982;&#32773;&#12290;</title><link>http://arxiv.org/abs/2303.01772</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;&#33021;&#28304;&#24066;&#22330;&#28165;&#31639;&#21644;&#20986;&#20215;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Approximating Energy Market Clearing and Bidding With Model-Based Reinforcement Learning. (arXiv:2303.01772v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#33021;&#28304;&#24066;&#22330;&#28165;&#31639;&#21644;&#20986;&#20215;&#30340;&#24212;&#29992;&#26041;&#27861;&#12290;&#36890;&#36807;&#29992;&#23398;&#20064;&#30340;OPF&#20195;&#29702;&#27169;&#22411;&#20197;&#21450;&#26126;&#30830;&#30340;&#24066;&#22330;&#35268;&#21017;&#26367;&#20195;&#20256;&#32479;&#35745;&#31639;&#26041;&#27861;&#65292;&#26412;&#26041;&#27861;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#24182;&#36866;&#29992;&#20110;&#24066;&#22330;&#35774;&#35745;&#21644;&#26356;&#29616;&#23454;&#22320;&#24314;&#27169;&#24066;&#22330;&#21442;&#19982;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#28304;&#24066;&#22330;&#21487;&#33021;&#20250;&#20026;&#24066;&#22330;&#21442;&#19982;&#32773;&#30340;&#19981;&#33391;&#34892;&#20026;&#25552;&#20379;&#28608;&#21169;&#12290;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26159;&#39044;&#27979;&#33021;&#28304;&#24066;&#22330;&#21442;&#19982;&#32773;&#39044;&#26399;&#34892;&#20026;&#30340;&#26377;&#21069;&#36884;&#30340;&#26032;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#38656;&#35201;&#35768;&#22810;&#19982;&#31995;&#32479;&#30340;&#20132;&#20114;&#25165;&#33021;&#25910;&#25947;&#65292;&#32780;&#30005;&#21147;&#31995;&#32479;&#29615;&#22659;&#36890;&#24120;&#21253;&#25324;&#24191;&#27867;&#30340;&#35745;&#31639;&#65292;&#20363;&#22914;&#29992;&#20110;&#24066;&#22330;&#28165;&#31639;&#30340;&#26368;&#20248;&#21151;&#29575;&#27969;&#37327;&#65288;OPF&#65289;&#35745;&#31639;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#33021;&#28304;&#24066;&#22330;&#30340;&#27169;&#22411;&#32473;&#22522;&#26412;&#30340;MARL&#31639;&#27861;&#65292;&#36825;&#20010;&#27169;&#22411;&#37319;&#29992;&#20102;&#23398;&#20064;&#30340;OPF&#36817;&#20284;&#20540;&#21644;&#26126;&#30830;&#30340;&#24066;&#22330;&#35268;&#21017;&#12290;&#23398;&#20064;&#30340;OPF&#20195;&#29702;&#27169;&#22411;&#20351;&#24471;OPF&#30340;&#26126;&#30830;&#35299;&#20915;&#21464;&#24471;&#19981;&#24517;&#35201;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#36824;&#23558;&#35757;&#32451;&#26102;&#38388;&#38477;&#20302;&#20102;&#32422;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#20294;&#20195;&#20215;&#26159;&#30053;&#24494;&#26356;&#24046;&#30340;&#32435;&#20160;&#22343;&#34913;&#36817;&#20284;&#20540;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#28508;&#22312;&#24212;&#29992;&#26159;&#24066;&#22330;&#35774;&#35745;&#65292;&#26356;&#29616;&#23454;&#22320;&#23545;&#24066;&#22330;&#21442;&#19982;&#32773;&#36827;&#34892;&#24314;&#27169;&#20197;&#21450;&#23545;&#24066;&#22330;&#21160;&#24577;&#30340;&#25913;&#36827;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy markets can provide incentives for undesired behavior of market participants. Multi-agent Reinforcement learning (MARL) is a promising new approach to predicting the expected behavior of energy market participants. However, reinforcement learning requires many interactions with the system to converge, and the power system environment often consists of extensive computations, e.g., optimal power flow (OPF) calculation for market clearing. To tackle this complexity, we provide a model of the energy market to a basic MARL algorithm in the form of a learned OPF approximation and explicit market rules. The learned OPF surrogate model makes an explicit solving of the OPF completely unnecessary. Our experiments demonstrate that the model additionally reduces training time by about one order of magnitude but at the cost of a slightly worse approximation of the Nash equilibrium. Potential applications of our method are market design, more realistic modeling of market participants, and an
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Frozen Pretrained Transformer (FPT) &#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21033;&#29992;&#20174;&#25968;&#21313;&#20159;&#26631;&#35760;&#35757;&#32451;&#20986;&#26469;&#30340;&#35821;&#35328;&#25110; CV &#27169;&#22411;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#25152;&#26377;&#20027;&#35201;&#31867;&#22411;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#36827;&#32780;&#20351;&#20854;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#37117;&#20855;&#22791;&#30528;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.11939</link><description>&lt;p&gt;
&#19968;&#31449;&#24335;&#35299;&#20915;&#26041;&#26696;&#65306;&#21033;&#29992;&#39044;&#35757;&#32451; LM &#36827;&#34892;&#24378;&#22823;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
One Fits All:Power General Time Series Analysis by Pretrained LM. (arXiv:2302.11939v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Frozen Pretrained Transformer (FPT) &#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21033;&#29992;&#20174;&#25968;&#21313;&#20159;&#26631;&#35760;&#35757;&#32451;&#20986;&#26469;&#30340;&#35821;&#35328;&#25110; CV &#27169;&#22411;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#25152;&#26377;&#20027;&#35201;&#31867;&#22411;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#36827;&#32780;&#20351;&#20854;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#37117;&#20855;&#22791;&#30528;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#21644;&#35745;&#31639;&#26426;&#35270;&#35273; (CV) &#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#22312;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#39046;&#22495;&#21462;&#24471;&#30340;&#36827;&#23637;&#26377;&#38480;&#12290;&#19982; NLP &#21644; CV &#19981;&#21516;&#30340;&#26159;&#65292;&#36825;&#20123;&#39046;&#22495;&#37319;&#29992;&#32479;&#19968;&#27169;&#22411;&#21363;&#21487;&#25191;&#34892;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#32780;&#22312;&#27599;&#20010;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#19987;&#38376;&#35774;&#35745;&#30340;&#26041;&#27861;&#20173;&#28982;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#22914;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#12289;&#39044;&#27979;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#38459;&#30861;&#39044;&#35757;&#32451;&#27169;&#22411;&#21457;&#23637;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#32570;&#20047;&#22823;&#37327;&#29992;&#20110;&#35757;&#32451;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#20174;&#25968;&#21313;&#20159;&#26631;&#35760;&#35757;&#32451;&#20986;&#26469;&#30340;&#35821;&#35328;&#25110; CV &#27169;&#22411;&#65292;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36991;&#20813;&#25913;&#21464;&#39044;&#35757;&#32451;&#35821;&#35328;&#25110;&#22270;&#20687;&#27169;&#22411;&#20013;&#27531;&#24046;&#22359;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#21644;&#21069;&#21521;&#20256;&#36882;&#23618;&#12290;&#36825;&#31181;&#27169;&#22411;&#34987;&#31216;&#20026;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120; (FPT)&#65292;&#36890;&#36807;&#23545;&#28041;&#21450;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#25152;&#26377;&#20027;&#35201;&#31867;&#22411;&#30340;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#12289;&#39044;&#27979;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#31561;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#65292;FPT &#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#37117;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although we have witnessed great success of pre-trained models in natural language processing (NLP) and computer vision (CV), limited progress has been made for general time series analysis. Unlike NLP and CV where a unified model can be used to perform different tasks, specially designed approach still dominates in each time series analysis task such as classification, anomaly detection, forecasting, and few-shot learning. The main challenge that blocks the development of pre-trained model for time series analysis is the lack of a large amount of data for training. In this work, we address this challenge by leveraging language or CV models, pre-trained from billions of tokens, for time series analysis. Specifically, we refrain from altering the self-attention and feedforward layers of the residual blocks in the pre-trained language or image model. This model, known as the Frozen Pretrained Transformer (FPT), is evaluated through fine-tuning on all major types of tasks involving time s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Projec and Probe&#65288;Pro$^2$&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#25554;&#20540;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#29305;&#24449;&#26469;&#36866;&#24212;&#30446;&#26631;&#20998;&#24067;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#20351;&#29992;&#23569;&#37327;&#30446;&#26631;&#25968;&#25454;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.05441</link><description>&lt;p&gt;
&#36890;&#36807;&#25554;&#20540;&#27491;&#20132;&#29305;&#24449;&#23454;&#29616;&#39640;&#25928;&#22495;&#33258;&#36866;&#24212;&#30340;Projec and Probe&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Project and Probe: Sample-Efficient Domain Adaptation by Interpolating Orthogonal Features. (arXiv:2302.05441v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Projec and Probe&#65288;Pro$^2$&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#25554;&#20540;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#29305;&#24449;&#26469;&#36866;&#24212;&#30446;&#26631;&#20998;&#24067;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#20351;&#29992;&#23569;&#37327;&#30446;&#26631;&#25968;&#25454;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23569;&#37327;&#30446;&#26631;&#25968;&#25454;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#26159;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;&#21040;&#20998;&#24067;&#21464;&#21270;&#30340;&#19968;&#20010;&#26377;&#25928;&#19988;&#24120;&#35265;&#30340;&#26041;&#27861;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#30446;&#26631;&#25968;&#25454;&#26631;&#31614;&#21487;&#33021;&#24456;&#38590;&#33719;&#24471;&#65292;&#22240;&#27492;&#25105;&#20204;&#21482;&#33021;&#35775;&#38382;&#26377;&#38480;&#25968;&#37327;&#30340;&#30446;&#26631;&#25968;&#25454;&#28857;&#12290;&#20026;&#20102;&#26368;&#22823;&#21270;&#21033;&#29992;&#26497;&#23567;&#30340;&#30446;&#26631;&#25968;&#25454;&#38598;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#12289;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25554;&#20540;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#29305;&#24449;&#26469;&#36866;&#24212;&#30446;&#26631;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;Projec and Probe&#65288;Pro$^2$&#65289;&#65292;&#39318;&#20808;&#23398;&#20064;&#19968;&#20010;&#32447;&#24615;&#25237;&#24433;&#65292;&#23558;&#39044;&#35757;&#32451;&#23884;&#20837;&#26144;&#23556;&#21040;&#27491;&#20132;&#26041;&#21521;&#19978;&#65292;&#21516;&#26102;&#21487;&#39044;&#27979;&#28304;&#25968;&#25454;&#38598;&#20013;&#30340;&#26631;&#31614;&#12290;&#36825;&#19968;&#27493;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#39044;&#27979;&#29305;&#24449;&#65292;&#20197;&#20415;&#22312;&#20998;&#24067;&#21464;&#21270;&#21518;&#20173;&#26377;&#19968;&#20123;&#29305;&#24449;&#26159;&#26377;&#29992;&#30340;&#12290;&#25509;&#30528;&#65292;Pro$^2$&#21033;&#29992;&#23569;&#37327;&#30446;&#26631;&#25968;&#25454;&#22312;&#36825;&#20123;&#25237;&#24433;&#29305;&#24449;&#20043;&#19978;&#23398;&#20064;&#19968;&#20010;&#32447;&#24615;&#20998;&#31867;&#22120;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;Pro$^2$&#33021;&#22815;&#20135;&#29983;&#26356;&#39640;&#25928;&#30340;&#27867;&#21270;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning with a small amount of target data is an effective and common approach to adapting a pre-trained model to distribution shifts. In some situations, target data labels may be expensive to obtain, so we may only have access to a limited number of target data points. To make the most of a very small target dataset, we propose a lightweight, sample-efficient approach that learns a diverse set of features and adapts to a target distribution by interpolating these features. Our approach, Project and Probe (Pro$^2$), first learns a linear projection that maps a pre-trained embedding onto orthogonal directions while being predictive of labels in the source dataset. The goal of this step is to learn a variety of predictive features, so that at least some of them remain useful after distribution shift. Pro$^2$ then learns a linear classifier on top of these projected features using a small target dataset. Theoretically, we find that Pro$^2$ results in more sample-efficient gener
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22522;&#20110;&#35760;&#24518;&#30340;&#20803;&#23398;&#20064;&#22312;&#38750;&#24179;&#31283;&#20998;&#24067;&#19978;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#21644;&#21160;&#20316;-&#35266;&#23519;&#24207;&#21015;&#65292;&#30740;&#31350;&#34920;&#26126;&#21508;&#31181;&#31867;&#22411;&#30340;&#22522;&#20110;&#35760;&#24518;&#30340;&#31070;&#32463;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#22320;&#36924;&#36817;&#36125;&#21494;&#26031;&#26368;&#20248;&#31639;&#27861;&#65292;&#24182;&#25191;&#34892;&#28508;&#22312;&#21442;&#25968;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2302.03067</link><description>&lt;p&gt;
&#22522;&#20110;&#35760;&#24518;&#30340;&#20803;&#23398;&#20064;&#22312;&#38750;&#24179;&#31283;&#20998;&#24067;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Memory-Based Meta-Learning on Non-Stationary Distributions. (arXiv:2302.03067v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22522;&#20110;&#35760;&#24518;&#30340;&#20803;&#23398;&#20064;&#22312;&#38750;&#24179;&#31283;&#20998;&#24067;&#19978;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#21644;&#21160;&#20316;-&#35266;&#23519;&#24207;&#21015;&#65292;&#30740;&#31350;&#34920;&#26126;&#21508;&#31181;&#31867;&#22411;&#30340;&#22522;&#20110;&#35760;&#24518;&#30340;&#31070;&#32463;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#22320;&#36924;&#36817;&#36125;&#21494;&#26031;&#26368;&#20248;&#31639;&#27861;&#65292;&#24182;&#25191;&#34892;&#28508;&#22312;&#21442;&#25968;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35760;&#24518;&#30340;&#20803;&#23398;&#20064;&#26159;&#19968;&#31181;&#36924;&#36817;&#36125;&#21494;&#26031;&#26368;&#20248;&#39044;&#27979;&#22120;&#30340;&#25216;&#26415;&#12290;&#22312;&#30456;&#24403;&#19968;&#33324;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#39034;&#24207;&#39044;&#27979;&#35823;&#24046;&#65288;&#30001;&#23545;&#25968;&#25439;&#22833;&#24230;&#37327;&#65289;&#20250;&#23548;&#33268;&#38544;&#24335;&#20803;&#23398;&#20064;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#35843;&#26597;&#24403;&#21069;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#21644;&#35757;&#32451;&#26041;&#26696;&#33021;&#21542;&#23454;&#29616;&#36825;&#31181;&#35299;&#37322;&#30340;&#28145;&#24230;&#12290;&#37325;&#28857;&#20851;&#27880;&#20855;&#26377;&#26410;&#35266;&#23519;&#21040;&#30340;&#20999;&#25442;&#28857;&#30340;&#20998;&#27573;&#24179;&#31283;&#28304;&#65292;&#24456;&#21487;&#33021;&#25429;&#25417;&#21040;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#21644;&#21160;&#20316;-&#35266;&#23519;&#24207;&#21015;&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#24449;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#22522;&#20110;&#35760;&#24518;&#30340;&#31070;&#32463;&#27169;&#22411;&#65288;&#21253;&#25324;&#21464;&#24418;&#37329;&#21018;&#27169;&#22411;&#12289;LSTM&#21644;RNN&#65289;&#21487;&#20197;&#23398;&#20064;&#20934;&#30830;&#22320;&#36924;&#36817;&#24050;&#30693;&#30340;&#36125;&#21494;&#26031;&#26368;&#20248;&#31639;&#27861;&#65292;&#24182;&#34920;&#29616;&#24471;&#22909;&#20687;&#22312;&#27599;&#20010;&#27573;&#20869;&#23545;&#28508;&#22312;&#20999;&#25442;&#28857;&#21644;&#25511;&#21046;&#25968;&#25454;&#20998;&#24067;&#30340;&#28508;&#22312;&#21442;&#25968;&#25191;&#34892;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memory-based meta-learning is a technique for approximating Bayes-optimal predictors. Under fairly general conditions, minimizing sequential prediction error, measured by the log loss, leads to implicit meta-learning. The goal of this work is to investigate how far this interpretation can be realized by current sequence prediction models and training regimes. The focus is on piecewise stationary sources with unobserved switching-points, which arguably capture an important characteristic of natural language and action-observation sequences in partially observable environments. We show that various types of memory-based neural models, including Transformers, LSTMs, and RNNs can learn to accurately approximate known Bayes-optimal algorithms and behave as if performing Bayesian inference over the latent switching-points and the latent parameters governing the data distribution within each segment.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#23567;&#22411;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#23398;&#20064;&#23454;&#29616;&#32676;&#32452;&#21512;&#26469;&#30740;&#31350;&#26222;&#36866;&#24615;&#20551;&#35774;&#12290;&#23427;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#36890;&#36807;&#20219;&#24847;&#26377;&#38480;&#32676;&#26469;&#23454;&#29616;&#32452;&#21512;&#65292;&#20174;&#32780;&#23436;&#20840;&#25551;&#36848;&#32593;&#32476;&#22312;&#27492;&#20219;&#21153;&#19978;&#23398;&#20064;&#30340;&#30005;&#36335;&#21644;&#29305;&#24449;&#26063;&#12290;</title><link>http://arxiv.org/abs/2302.03025</link><description>&lt;p&gt;
&#19968;&#31181;&#26222;&#36866;&#24615;&#30340;&#29609;&#20855;&#27169;&#22411;&#65306;&#36870;&#21521;&#24037;&#31243;&#32593;&#32476;&#22914;&#20309;&#23398;&#20064;&#32676;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations. (arXiv:2302.03025v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#23567;&#22411;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#23398;&#20064;&#23454;&#29616;&#32676;&#32452;&#21512;&#26469;&#30740;&#31350;&#26222;&#36866;&#24615;&#20551;&#35774;&#12290;&#23427;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#36890;&#36807;&#20219;&#24847;&#26377;&#38480;&#32676;&#26469;&#23454;&#29616;&#32452;&#21512;&#65292;&#20174;&#32780;&#23436;&#20840;&#25551;&#36848;&#32593;&#32476;&#22312;&#27492;&#20219;&#21153;&#19978;&#23398;&#20064;&#30340;&#30005;&#36335;&#21644;&#29305;&#24449;&#26063;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26222;&#36866;&#24615;&#26159;&#26426;&#26800;&#35299;&#37322;&#24615;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20551;&#35774;--&#19981;&#21516;&#30340;&#27169;&#22411;&#22312;&#31867;&#20284;&#20219;&#21153;&#19978;&#35757;&#32451;&#26102;&#23398;&#20064;&#30456;&#20284;&#30340;&#29305;&#24449;&#21644;&#30005;&#36335;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#23567;&#22411;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#23398;&#20064;&#23454;&#29616;&#32676;&#32452;&#21512;&#26469;&#30740;&#31350;&#26222;&#36866;&#24615;&#20551;&#35774;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#23398;&#34920;&#31034;&#35770;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#36890;&#36807;&#20219;&#24847;&#26377;&#38480;&#32676;&#26469;&#23454;&#29616;&#32452;&#21512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#36870;&#21521;&#24037;&#31243;&#27169;&#22411;&#30340;&#36923;&#36753;&#21644;&#26435;&#37325;&#26469;&#23637;&#31034;&#32593;&#32476;&#22987;&#32456;&#23398;&#20064;&#27492;&#31639;&#27861;&#65292;&#24182;&#20351;&#29992;&#28040;&#34701;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35299;&#12290;&#36890;&#36807;&#30740;&#31350;&#35757;&#32451;&#22312;&#19981;&#21516;&#32676;&#19978;&#30340;&#19981;&#21516;&#26550;&#26500;&#30340;&#32593;&#32476;&#65292;&#25105;&#20204;&#21457;&#29616;&#26222;&#36866;&#24615;&#30340;&#35777;&#25454;&#19981;&#19968;&#65306;&#20351;&#29992;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#23436;&#20840;&#25551;&#36848;&#32593;&#32476;&#22312;&#27492;&#20219;&#21153;&#19978;&#23398;&#20064;&#30340;&#30005;&#36335;&#21644;&#29305;&#24449;&#26063;&#65292;&#20294;&#23545;&#20110;&#32473;&#23450;&#30340;&#32593;&#32476;&#65292;&#23398;&#20064;&#30340;&#31934;&#30830;&#30005;&#36335;&#20197;&#21450;&#23427;&#20204;&#30340;&#21457;&#23637;&#39034;&#24207;&#26159;&#20219;&#24847;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Universality is a key hypothesis in mechanistic interpretability -- that different models learn similar features and circuits when trained on similar tasks. In this work, we study the universality hypothesis by examining how small neural networks learn to implement group composition. We present a novel algorithm by which neural networks may implement composition for any finite group via mathematical representation theory. We then show that networks consistently learn this algorithm by reverse engineering model logits and weights, and confirm our understanding using ablations. By studying networks of differing architectures trained on various groups, we find mixed evidence for universality: using our algorithm, we can completely characterize the family of circuits and features that networks learn on this task, but for a given network the precise circuits learned -- as well as the order they develop -- are arbitrary.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#36755;&#20837;&#29983;&#25104;&#36873;&#25321;&#24615;&#35299;&#37322;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20197;&#24357;&#21512;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#19982;&#20154;&#31867;&#35299;&#37322;&#30340;&#24046;&#36317;&#65292;&#24182;&#19988;&#22312;&#20915;&#31574;&#25903;&#25345;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.09656</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#35299;&#37322;&#65306;&#21033;&#29992;&#20154;&#31867;&#36755;&#20837;&#23545;&#40784;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Selective Explanations: Leveraging Human Input to Align Explainable AI. (arXiv:2301.09656v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#36755;&#20837;&#29983;&#25104;&#36873;&#25321;&#24615;&#35299;&#37322;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20197;&#24357;&#21512;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#19982;&#20154;&#31867;&#35299;&#37322;&#30340;&#24046;&#36317;&#65292;&#24182;&#19988;&#22312;&#20915;&#31574;&#25903;&#25345;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20986;&#29616;&#20102;&#22823;&#37327;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#31639;&#27861;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#22240;&#19982;&#20154;&#31867;&#35299;&#37322;&#30340;&#29983;&#20135;&#21644;&#28040;&#36153;&#26041;&#24335;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#32780;&#21463;&#21040;&#25209;&#35780;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#30340;XAI&#25216;&#26415;&#24448;&#24448;&#38590;&#20197;&#20351;&#29992;&#24182;&#32570;&#20047;&#26377;&#25928;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#20351;AI&#35299;&#37322;&#20855;&#26377;&#36873;&#25321;&#24615;&#65288;&#36825;&#26159;&#20154;&#31867;&#35299;&#37322;&#30340;&#22522;&#26412;&#23646;&#24615;&#20043;&#19968;&#65289;&#26469;&#24357;&#21512;&#36825;&#20123;&#24046;&#36317;&#65292;&#36890;&#36807;&#26681;&#25454;&#25509;&#25910;&#26041;&#30340;&#20559;&#22909;&#26377;&#36873;&#25321;&#24615;&#22320;&#21576;&#29616;&#22823;&#37327;&#27169;&#22411;&#21407;&#22240;&#30340;&#23376;&#38598;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#23567;&#26679;&#26412;&#19978;&#30340;&#20154;&#31867;&#36755;&#20837;&#26469;&#29983;&#25104;&#36873;&#25321;&#24615;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#24320;&#36767;&#20102;&#19968;&#20010;&#20016;&#23500;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#28085;&#30422;&#20102;&#19981;&#21516;&#30340;&#36873;&#25321;&#24615;&#30446;&#26631;&#12289;&#36755;&#20837;&#31867;&#22411;&#31561;&#12290;&#20316;&#20026;&#19968;&#20010;&#23637;&#31034;&#65292;&#25105;&#20204;&#20351;&#29992;&#20915;&#31574;&#25903;&#25345;&#20219;&#21153;&#26469;&#25506;&#32034;&#22522;&#20110;&#20915;&#31574;&#32773;&#35748;&#20026;&#30456;&#20851;&#30340;&#36873;&#25321;&#24615;&#35299;&#37322;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#39033;&#23454;&#39564;&#30740;&#31350;&#65292;&#20197;&#26816;&#26597;&#20174;&#22823;&#19968;&#32452;&#27169;&#22411;&#21407;&#22240;&#20013;&#36873;&#25321;&#30340;&#19977;&#20010;&#23376;&#38598;&#19982;&#26410;&#36873;&#25321;&#30340;&#23376;&#38598;&#30456;&#27604;&#65292;&#36873;&#25321;&#24615;&#35299;&#37322;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
While a vast collection of explainable AI (XAI) algorithms have been developed in recent years, they are often criticized for significant gaps with how humans produce and consume explanations. As a result, current XAI techniques are often found to be hard to use and lack effectiveness. In this work, we attempt to close these gaps by making AI explanations selective -- a fundamental property of human explanations -- by selectively presenting a subset from a large set of model reasons based on what aligns with the recipient's preferences. We propose a general framework for generating selective explanations by leveraging human input on a small sample. This framework opens up a rich design space that accounts for different selectivity goals, types of input, and more. As a showcase, we use a decision-support task to explore selective explanations based on what the decision-maker would consider relevant to the decision task. We conducted two experimental studies to examine three out of a bro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#35201;&#20171;&#32461;&#20102;&#22270;&#23884;&#20837;&#23398;&#20064;&#65292;&#23427;&#23558;&#19968;&#32452;&#22270;&#20316;&#20026;&#36755;&#20837;&#65292;&#33258;&#21160;&#25552;&#21462;&#29305;&#24449;&#24182;&#23558;&#22270;&#32534;&#30721;&#20026;&#20302;&#32500;&#34920;&#31034;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#20351;&#24471;&#22270;&#23884;&#20837;&#23398;&#20064;&#36866;&#24212;&#20102;&#26085;&#30410;&#22686;&#38271;&#30340;&#22270;&#35268;&#27169;&#65292;&#20174;&#32780;&#33719;&#24471;&#20102;&#24456;&#22810;&#25104;&#21151;&#65292;&#20294;&#26159;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20250;&#21463;&#21040;&#35745;&#31639;&#29942;&#39048;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2301.05860</link><description>&lt;p&gt;
&#22270;&#23884;&#20837;&#23398;&#20064;&#30340;&#29616;&#29366;&#21644;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
State of the Art and Potentialities of Graph-level Learning. (arXiv:2301.05860v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#20171;&#32461;&#20102;&#22270;&#23884;&#20837;&#23398;&#20064;&#65292;&#23427;&#23558;&#19968;&#32452;&#22270;&#20316;&#20026;&#36755;&#20837;&#65292;&#33258;&#21160;&#25552;&#21462;&#29305;&#24449;&#24182;&#23558;&#22270;&#32534;&#30721;&#20026;&#20302;&#32500;&#34920;&#31034;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#20351;&#24471;&#22270;&#23884;&#20837;&#23398;&#20064;&#36866;&#24212;&#20102;&#26085;&#30410;&#22686;&#38271;&#30340;&#22270;&#35268;&#27169;&#65292;&#20174;&#32780;&#33719;&#24471;&#20102;&#24456;&#22810;&#25104;&#21151;&#65292;&#20294;&#26159;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20250;&#21463;&#21040;&#35745;&#31639;&#29942;&#39048;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#29616;&#20102;&#20851;&#31995;&#25968;&#25454;&#30340;&#19968;&#31181;&#20248;&#36234;&#33021;&#21147;&#65292;&#22914;&#21270;&#21512;&#29289;&#12289;&#34507;&#30333;&#36136;&#21644;&#31038;&#20132;&#32593;&#32476;&#31561;&#12290;&#22240;&#27492;&#65292;&#22270;&#23884;&#20837;&#23398;&#20064;&#65292;&#23558;&#19968;&#32452;&#22270;&#20316;&#20026;&#36755;&#20837;&#65292;&#24050;&#24212;&#29992;&#20110;&#35768;&#22810;&#20219;&#21153;&#65292;&#21253;&#25324;&#27604;&#36739;&#12289;&#22238;&#24402;&#12289;&#20998;&#31867;&#31561;&#12290;&#20256;&#32479;&#30340;&#23398;&#20064;&#19968;&#32452;&#22270;&#30340;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#25163;&#24037;&#21046;&#20316;&#30340;&#29305;&#24449;&#65292;&#22914;&#20122;&#32467;&#26500;&#12290;&#20294;&#36825;&#20123;&#26041;&#27861;&#34429;&#28982;&#21463;&#30410;&#20110;&#33391;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20294;&#24120;&#24120;&#22240;&#26080;&#27861;&#36991;&#20813;&#22270;&#21516;&#26500;&#38382;&#39064;&#32780;&#21463;&#21040;&#35745;&#31639;&#29942;&#39048;&#30340;&#24433;&#21709;&#12290;&#30456;&#21453;&#65292;&#28145;&#24230;&#23398;&#20064;&#36890;&#36807;&#33258;&#21160;&#25552;&#21462;&#29305;&#24449;&#21644;&#23558;&#22270;&#32534;&#30721;&#20026;&#20302;&#32500;&#34920;&#31034;&#65292;&#24110;&#21161;&#22270;&#23884;&#20837;&#23398;&#20064;&#36866;&#24212;&#26085;&#30410;&#22686;&#38271;&#30340;&#22270;&#35268;&#27169;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#28145;&#24230;&#22270;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#36896;&#25104;&#20102;&#35768;&#22810;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#20840;&#38754;&#30340;&#32508;&#36848;&#26469;&#22238;&#39038;&#20174;&#20256;&#32479;&#23398;&#20064;&#21040;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#22270;&#23884;&#20837;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphs have a superior ability to represent relational data, like chemical compounds, proteins, and social networks. Hence, graph-level learning, which takes a set of graphs as input, has been applied to many tasks including comparison, regression, classification, and more. Traditional approaches to learning a set of graphs heavily rely on hand-crafted features, such as substructures. But while these methods benefit from good interpretability, they often suffer from computational bottlenecks as they cannot skirt the graph isomorphism problem. Conversely, deep learning has helped graph-level learning adapt to the growing scale of graphs by extracting features automatically and encoding graphs into low-dimensional representations. As a result, these deep graph learning methods have been responsible for many successes. Yet, there is no comprehensive survey that reviews graph-level learning starting with traditional learning and moving through to the deep learning approaches. This article 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36830;&#32493;&#23545;&#27604;&#24494;&#35843;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#20851;&#31995;&#25552;&#21462;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#33268;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;RE&#27169;&#22411;&#65292;&#20197;&#21450;&#22810;&#20013;&#24515;&#23545;&#27604;&#25439;&#22833;&#26469;&#20801;&#35768;&#19968;&#20010;&#20851;&#31995;&#24418;&#25104;&#22810;&#20010;&#32858;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#20302;&#36164;&#28304;&#24773;&#20917;&#21644;&#39046;&#22495;&#20013;&#30340;&#20851;&#31995;&#25552;&#21462;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.10823</link><description>&lt;p&gt;
&#36830;&#32493;&#23545;&#27604;&#24494;&#35843;&#25913;&#36827;&#20302;&#36164;&#28304;&#20851;&#31995;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Continual Contrastive Finetuning Improves Low-Resource Relation Extraction. (arXiv:2212.10823v1 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36830;&#32493;&#23545;&#27604;&#24494;&#35843;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#20851;&#31995;&#25552;&#21462;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#33268;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;RE&#27169;&#22411;&#65292;&#20197;&#21450;&#22810;&#20013;&#24515;&#23545;&#27604;&#25439;&#22833;&#26469;&#20801;&#35768;&#19968;&#20010;&#20851;&#31995;&#24418;&#25104;&#22810;&#20010;&#32858;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#20302;&#36164;&#28304;&#24773;&#20917;&#21644;&#39046;&#22495;&#20013;&#30340;&#20851;&#31995;&#25552;&#21462;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25552;&#21462;&#65288;RE&#65289;&#20381;&#36182;&#32467;&#26500;&#21270;&#27880;&#37322;&#35821;&#26009;&#24211;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#23588;&#20854;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#21644;&#39046;&#22495;&#20013;&#65292;&#35813;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36817;&#26399;&#30740;&#31350;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#35299;&#20915;&#20302;&#36164;&#28304;&#30340;RE&#65292;&#20854;&#20013;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#36890;&#36807;RE&#30446;&#26631;&#39044;&#35757;&#32451;&#20851;&#31995;&#23884;&#20837;&#65292;&#24182;&#36890;&#36807;&#20998;&#31867;&#20026;&#22522;&#30784;&#30340;&#30446;&#26631;&#23545;&#26377;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#23427;&#38459;&#27490;RE&#27169;&#22411;&#20805;&#20998;&#21033;&#29992;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#30340;&#30693;&#35782;&#12290;&#26412;&#25991;&#26088;&#22312;&#24357;&#21512;&#24046;&#36317;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#19968;&#33268;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;RE&#27169;&#22411;&#12290;&#30001;&#20110;&#22312;&#36825;&#31181;&#34920;&#31034;&#23398;&#20064;&#33539;&#24335;&#20013;&#65292;&#19968;&#20010;&#20851;&#31995;&#21487;&#33021;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#36731;&#26494;&#24418;&#25104;&#22810;&#20010;&#32858;&#31867;&#65292;&#22240;&#27492;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22810;&#20013;&#24515;&#23545;&#27604;&#25439;&#22833;&#65292;&#20801;&#35768;&#19968;&#20010;&#20851;&#31995;&#24418;&#25104;&#22810;&#20010;&#32858;&#31867;&#20197;&#26356;&#22909;&#22320;&#23545;&#40784;&#39044;&#35757;&#32451;&#12290;&#22312;&#20004;&#20010;&#25991;&#26723;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#21644;&#39046;&#22495;&#20013;&#26174;&#30528;&#25552;&#39640;&#20851;&#31995;&#25552;&#21462;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction (RE), which has relied on structurally annotated corpora for model training, has been particularly challenging in low-resource scenarios and domains. Recent literature has tackled low-resource RE by self-supervised learning, where the solution involves pretraining the relation embedding by RE-based objective and finetuning on labeled data by classification-based objective. However, a critical challenge to this approach is the gap in objectives, which prevents the RE model from fully utilizing the knowledge in pretrained representations. In this paper, we aim at bridging the gap and propose to pretrain and finetune the RE model using consistent objectives of contrastive learning. Since in this kind of representation learning paradigm, one relation may easily form multiple clusters in the representation space, we further propose a multi-center contrastive loss that allows one relation to form multiple clusters to better align with pretraining. Experiments on two docum
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#27169;&#22411;&#32570;&#20047;&#23545;&#26085;&#24120;&#29289;&#21697;&#30340;&#19968;&#33268;&#24615;&#24515;&#29702;&#27169;&#22411;&#65292;&#20250;&#22240;&#27492;&#20986;&#29616;&#33618;&#35884;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#34429;&#28982;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#36825;&#20123;&#23454;&#20307;&#30340;&#30693;&#35782;&#30862;&#29255;&#65292;&#20294;&#23427;&#20204;&#26080;&#27861;&#20026;&#25152;&#26377;&#23454;&#20307;&#20135;&#29983;&#19968;&#33268;&#19988;&#27491;&#30830;&#30340;&#24515;&#29702;&#27169;&#22411;&#12290;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21487;&#20197;&#25913;&#21892;&#36825;&#31181;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2212.10029</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#26085;&#24120;&#29289;&#21697;&#30340;&#19968;&#33268;&#24615;&#24515;&#29702;&#27169;&#22411;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do language models have coherent mental models of everyday things?. (arXiv:2212.10029v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10029
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#32570;&#20047;&#23545;&#26085;&#24120;&#29289;&#21697;&#30340;&#19968;&#33268;&#24615;&#24515;&#29702;&#27169;&#22411;&#65292;&#20250;&#22240;&#27492;&#20986;&#29616;&#33618;&#35884;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#34429;&#28982;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#36825;&#20123;&#23454;&#20307;&#30340;&#30693;&#35782;&#30862;&#29255;&#65292;&#20294;&#23427;&#20204;&#26080;&#27861;&#20026;&#25152;&#26377;&#23454;&#20307;&#20135;&#29983;&#19968;&#33268;&#19988;&#27491;&#30830;&#30340;&#24515;&#29702;&#27169;&#22411;&#12290;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21487;&#20197;&#25913;&#21892;&#36825;&#31181;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20154;&#20204;&#24819;&#21040;&#20687;&#8220;&#40481;&#34507;&#8221;&#36825;&#26679;&#30340;&#26085;&#24120;&#29992;&#21697;&#26102;&#65292;&#36890;&#24120;&#20250;&#26377;&#19968;&#20010;&#19982;&#20043;&#30456;&#20851;&#32852;&#30340;&#24515;&#29702;&#22270;&#20687;&#12290;&#36825;&#31181;&#24120;&#35782;&#24615;&#30693;&#35782;&#26377;&#21161;&#20110;&#25105;&#20204;&#29702;&#35299;&#36825;&#20123;&#26085;&#24120;&#29992;&#21697;&#30340;&#24037;&#20316;&#21407;&#29702;&#20197;&#21450;&#22914;&#20309;&#19982;&#23427;&#20204;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#31995;&#32479;&#23545;&#36825;&#26679;&#30340;&#26085;&#24120;&#29992;&#21697;&#27809;&#26377;&#19968;&#33268;&#30340;&#22270;&#20687;&#65292;&#27604;&#22914;&#35748;&#20026;&#40481;&#34507;&#40644;&#21253;&#22260;&#30528;&#22771;&#65292;&#37027;&#20040;&#23427;&#21487;&#33021;&#19981;&#24471;&#19981;&#37319;&#21462;&#33618;&#35884;&#30340;&#26041;&#27861;&#65292;&#27604;&#22914;&#35797;&#22270;&#25226;&#40481;&#34507;&#40644;&#21038;&#19979;&#22771;&#25918;&#20837;&#24179;&#24213;&#38149;&#20013;&#29006;&#29038;&#12290;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#36825;&#31181;&#26085;&#24120;&#29992;&#21697;&#30340;&#19968;&#33268;&#24615;&#24515;&#29702;&#27169;&#22411;&#65311;&#20026;&#20102;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;100&#31181;&#26085;&#24120;&#29992;&#21697;&#12289;&#23427;&#20204;&#30340;&#37096;&#20214;&#20197;&#21450;&#36825;&#20123;&#37096;&#20214;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20687;GPT-3&#21644;Macaw&#36825;&#26679;&#30340;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#36825;&#20123;&#23454;&#20307;&#30340;&#30693;&#35782;&#30862;&#29255;&#65292;&#20294;&#23427;&#20204;&#26080;&#27861;&#20026;&#25152;&#26377;&#23454;&#20307;&#20135;&#29983;&#19968;&#33268;&#19988;&#27491;&#30830;&#30340;&#24515;&#29702;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#23545;&#36825;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#23427;&#20204;&#22312;&#26576;&#20123;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
When people think of everyday things like an "egg," they typically have a mental image associated with it. This commonsense knowledge helps us understand how these everyday things work and how to interact with them. For example, when someone tries to make a fried egg, they know that it has a shell and that it can be cracked open to reveal the egg white and yolk inside. However, if a system does not have a coherent picture of such everyday things, thinking that the egg yolk surrounds the shell, then it might have to resort to ridiculous approaches such as trying to scrape the egg yolk off the shell into the pan. Do language models have a coherent picture of such everyday things? To investigate this, we propose a benchmark dataset consisting of 100 everyday things, their parts, and the relationships between these parts. We observe that state-of-the-art pre-trained language models (LMs) like GPT-3 and Macaw have fragments of knowledge about these entities, but they fail to produce consist
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;CALIME&#26041;&#27861;&#65292;&#23558;&#22240;&#26524;&#30693;&#35782;&#34701;&#20837;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#20013;&#65292;&#20197;&#35299;&#20915;&#29305;&#24449;&#29420;&#31435;&#24615;&#30340;&#32570;&#38519;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#20110;&#21021;&#22987;&#26041;&#27861;&#30340;&#40657;&#30418;&#27169;&#22411;&#27169;&#25311;&#20445;&#30495;&#24230;&#21644;&#35299;&#37322;&#31283;&#23450;&#24615;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.05256</link><description>&lt;p&gt;
CALIME: &#22240;&#26524;&#24863;&#30693;&#30340;&#23616;&#37096;&#21487;&#35299;&#37322;&#24615;&#27169;&#22411;-&#26080;&#20851;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
CALIME: Causality-Aware Local Interpretable Model-Agnostic Explanations. (arXiv:2212.05256v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;CALIME&#26041;&#27861;&#65292;&#23558;&#22240;&#26524;&#30693;&#35782;&#34701;&#20837;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#20013;&#65292;&#20197;&#35299;&#20915;&#29305;&#24449;&#29420;&#31435;&#24615;&#30340;&#32570;&#38519;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#20110;&#21021;&#22987;&#26041;&#27861;&#30340;&#40657;&#30418;&#27169;&#22411;&#27169;&#25311;&#20445;&#30495;&#24230;&#21644;&#35299;&#37322;&#31283;&#23450;&#24615;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#30340;&#19968;&#20010;&#37325;&#35201;&#32570;&#28857;&#26159;&#20551;&#35774;&#29305;&#24449;&#29420;&#31435;&#24615;&#12290;&#26412;&#25991;&#30528;&#30524;&#20110;&#23558;&#22240;&#26524;&#30693;&#35782;&#34701;&#20837;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#20013;&#65292;&#20197;&#22686;&#21152;&#20449;&#20219;&#24182;&#24110;&#21161;&#29992;&#25143;&#35780;&#20272;&#35299;&#37322;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#23637;&#26041;&#27861;&#65292;&#26126;&#30830;&#22320;&#22312;&#22260;&#32469;&#36755;&#20837;&#23454;&#20363;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#32534;&#30721;&#22240;&#26524;&#20851;&#31995;&#65292;&#20197;&#35299;&#37322;&#27169;&#22411;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27169;&#20223;&#40657;&#30418;&#23376;&#30340;&#20445;&#30495;&#24230;&#21644;&#35299;&#37322;&#30340;&#31283;&#23450;&#24615;&#26041;&#38754;&#22343;&#27604;&#21021;&#22987;&#26041;&#27861;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
A significant drawback of eXplainable Artificial Intelligence (XAI) approaches is the assumption of feature independence. This paper focuses on integrating causal knowledge in XAI methods to increase trust and help users assess explanations' quality. We propose a novel extension to a widely used local and model-agnostic explainer that explicitly encodes causal relationships in the data generated around the input instance to explain. Extensive experiments show that our method achieves superior performance comparing the initial one for both the fidelity in mimicking the black-box and the stability of the explanations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26080;&#26684;&#26629;&#35757;&#32451;&#30446;&#26631;&#65292;&#29992;&#20110;&#22522;&#20110;&#38899;&#32032;&#30340;&#31070;&#32463;&#20256;&#36882;&#22120;&#30340;&#26368;&#32456;&#21518;&#39564;&#36755;&#20986;&#65292;&#19982;&#20351;&#29992;N-best&#21015;&#34920;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#26080;&#26684;&#26629;&#26041;&#27861;&#22312;&#35757;&#32451;&#26399;&#38388;&#28040;&#38500;&#20102;&#20551;&#35774;&#29983;&#25104;&#30340;&#27493;&#39588;&#65292;&#20174;&#32780;&#23548;&#33268;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#22312;&#21333;&#35789;&#38169;&#35823;&#29575;&#19978;&#34920;&#29616;&#20063;&#26377;6.5&#65285;&#30340;&#30456;&#23545;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2212.04325</link><description>&lt;p&gt;
&#26080;&#26684;&#26629;&#24207;&#21015;&#37492;&#21035;&#35757;&#32451;&#29992;&#20110;&#22522;&#20110;&#38899;&#32032;&#30340;&#31070;&#32463;&#20256;&#36882;&#22120;
&lt;/p&gt;
&lt;p&gt;
Lattice-Free Sequence Discriminative Training for Phoneme-Based Neural Transducers. (arXiv:2212.04325v3 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26080;&#26684;&#26629;&#35757;&#32451;&#30446;&#26631;&#65292;&#29992;&#20110;&#22522;&#20110;&#38899;&#32032;&#30340;&#31070;&#32463;&#20256;&#36882;&#22120;&#30340;&#26368;&#32456;&#21518;&#39564;&#36755;&#20986;&#65292;&#19982;&#20351;&#29992;N-best&#21015;&#34920;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#26080;&#26684;&#26629;&#26041;&#27861;&#22312;&#35757;&#32451;&#26399;&#38388;&#28040;&#38500;&#20102;&#20551;&#35774;&#29983;&#25104;&#30340;&#27493;&#39588;&#65292;&#20174;&#32780;&#23548;&#33268;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#22312;&#21333;&#35789;&#38169;&#35823;&#29575;&#19978;&#34920;&#29616;&#20063;&#26377;6.5&#65285;&#30340;&#30456;&#23545;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;RNN-Transducer&#24050;&#32463;&#22312;&#21508;&#31181;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#26080;&#26684;&#26629;&#24207;&#21015;&#37492;&#21035;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#28151;&#21512;&#27169;&#22411;&#20013;&#33719;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;RNN-Transducer&#20013;&#24456;&#23569;&#34987;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#26080;&#26684;&#26629;&#35757;&#32451;&#30446;&#26631;&#65292;&#21363;&#26080;&#26684;&#26629;&#26368;&#22823;&#20114;&#20449;&#24687;&#12289;&#26080;&#26684;&#26629;&#27573;&#32423;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#21644;&#26080;&#26684;&#26629;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65292;&#29992;&#20110;&#20855;&#26377;&#26377;&#38480;&#19978;&#19979;&#25991;&#20381;&#36182;&#24615;&#30340;&#22522;&#20110;&#38899;&#32032;&#30340;&#31070;&#32463;&#20256;&#36882;&#22120;&#30340;&#26368;&#32456;&#21518;&#39564;&#36755;&#20986;&#12290;&#19982;&#20351;&#29992;N-best&#21015;&#34920;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#26080;&#26684;&#26629;&#26041;&#27861;&#22312;&#35757;&#32451;&#26399;&#38388;&#28040;&#38500;&#20102;&#20551;&#35774;&#29983;&#25104;&#30340;&#35299;&#30721;&#27493;&#39588;&#65292;&#20174;&#32780;&#23548;&#33268;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#24207;&#21015;&#32423;&#20132;&#21449;&#29109;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#65292;&#26080;&#26684;&#26629;&#26041;&#27861;&#22312;&#21333;&#35789;&#38169;&#35823;&#29575;&#19978;&#33719;&#24471;&#20102;&#39640;&#36798;6.5&#65285;&#30340;&#30456;&#23545;&#25913;&#36827;&#12290;&#19982;&#22522;&#20110;N-best&#21015;&#34920;&#30340;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#30446;&#26631;&#30456;&#27604;&#65292;&#26080;&#26684;&#26629;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#28789;&#27963;&#24615;&#21644;&#21487;&#34892;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;N-best&#21015;&#34920;&#20013;&#20855;&#26377;&#19968;&#20123;&#22122;&#22768;&#21644;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, RNN-Transducers have achieved remarkable results on various automatic speech recognition tasks. However, lattice-free sequence discriminative training methods, which obtain superior performance in hybrid models, are rarely investigated in RNN-Transducers. In this work, we propose three lattice-free training objectives, namely lattice-free maximum mutual information, lattice-free segment-level minimum Bayes risk, and lattice-free minimum Bayes risk, which are used for the final posterior output of the phoneme-based neural transducer with a limited context dependency. Compared to criteria using N-best lists, lattice-free methods eliminate the decoding step for hypotheses generation during training, which leads to more efficient training. Experimental results show that lattice-free methods gain up to 6.5% relative improvement in word error rate compared to a sequence-level cross-entropy trained model. Compared to the N-best-list based minimum Bayes risk objectives, lattice-free 
&lt;/p&gt;</description></item><item><title>MEGAN&#26159;&#19968;&#31181;&#21487;&#20197;&#22312;&#22810;&#20010;&#36890;&#36947;&#20013;&#20135;&#29983;&#33410;&#28857;&#21644;&#36793;&#30340;&#35828;&#26126;&#24615;&#35299;&#37322;&#30340;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#23545;&#20110;&#25913;&#36827;&#22270;&#22238;&#24402;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#23427;&#26159;&#23436;&#20840;&#21487;&#24494;&#30340;&#65292;&#21487;&#20197;&#22312;&#35299;&#37322;&#30417;&#30563;&#26041;&#24335;&#19979;&#20027;&#21160;&#22320;&#35757;&#32451;&#35828;&#26126;&#12290;</title><link>http://arxiv.org/abs/2211.13236</link><description>&lt;p&gt;
MEGAN: &#22810;&#35299;&#37322;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
MEGAN: Multi-Explanation Graph Attention Network. (arXiv:2211.13236v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13236
&lt;/p&gt;
&lt;p&gt;
MEGAN&#26159;&#19968;&#31181;&#21487;&#20197;&#22312;&#22810;&#20010;&#36890;&#36947;&#20013;&#20135;&#29983;&#33410;&#28857;&#21644;&#36793;&#30340;&#35828;&#26126;&#24615;&#35299;&#37322;&#30340;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#23545;&#20110;&#25913;&#36827;&#22270;&#22238;&#24402;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#23427;&#26159;&#23436;&#20840;&#21487;&#24494;&#30340;&#65292;&#21487;&#20197;&#22312;&#35299;&#37322;&#30417;&#30563;&#26041;&#24335;&#19979;&#20027;&#21160;&#22320;&#35757;&#32451;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35299;&#37322;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;MEGAN&#65289;&#12290;&#19982;&#29616;&#26377;&#30340;&#22270;&#21487;&#35299;&#37322;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#32593;&#32476;&#21487;&#20197;&#27839;&#22810;&#20010;&#36890;&#36947;&#20135;&#29983;&#33410;&#28857;&#21644;&#36793;&#30340;&#35828;&#26126;&#24615;&#35299;&#37322;&#65292;&#20854;&#25968;&#37327;&#29420;&#31435;&#20110;&#20219;&#21153;&#35268;&#26684;&#35828;&#26126;&#12290;&#36825;&#23545;&#20110;&#25913;&#36827;&#22270;&#22238;&#24402;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#21487;&#20197;&#23558;&#35299;&#37322;&#20998;&#20026;&#30456;&#23545;&#20110;&#21442;&#32771;&#20540;&#30340;&#27491;&#38754;&#21644;&#36127;&#38754;&#35777;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32593;&#32476;&#26159;&#23436;&#20840;&#21487;&#24494;&#30340;&#65292;&#21487;&#20197;&#22312;&#35299;&#37322;&#30417;&#30563;&#26041;&#24335;&#19979;&#20027;&#21160;&#22320;&#35757;&#32451;&#35828;&#26126;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#24050;&#30693;&#22320;&#38754;&#30495;&#30456;&#35828;&#26126;&#30340;&#21512;&#25104;&#22270;&#22238;&#24402;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32593;&#32476;&#22312;&#21333;&#19968;&#21644;&#22810;&#35299;&#37322;&#24773;&#20917;&#19979;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#21487;&#35299;&#37322;&#26041;&#27861;&#65292;&#22312;&#35299;&#37322;&#30417;&#30563;&#26399;&#38388;&#23454;&#29616;&#20102;&#25509;&#36817;&#23436;&#32654;&#30340;&#35299;&#37322;&#20934;&#30830;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a multi-explanation graph attention network (MEGAN). Unlike existing graph explainability methods, our network can produce node and edge attributional explanations along multiple channels, the number of which is independent of task specifications. This proves crucial to improve the interpretability of graph regression predictions, as explanations can be split into positive and negative evidence w.r.t to a reference value. Additionally, our attention-based network is fully differentiable and explanations can actively be trained in an explanation-supervised manner. We first validate our model on a synthetic graph regression dataset with known ground-truth explanations. Our network outperforms existing baseline explainability methods for the single- as well as the multi-explanation case, achieving near-perfect explanation accuracy during explanation supervision. Finally, we demonstrate our model's capabilities on multiple real-world datasets. We find that our model produces spa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23618;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#34701;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#36827;&#34892;&#25991;&#26412;&#20013;&#30340;&#31163;&#32676;&#26816;&#27979;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.11300</link><description>&lt;p&gt;
&#25991;&#26412;&#20013;&#30340;&#31163;&#32676;&#26816;&#27979;&#30340;&#22810;&#23618;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Multi-Level Knowledge Distillation for Out-of-Distribution Detection in Text. (arXiv:2211.11300v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23618;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#34701;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#36827;&#34892;&#25991;&#26412;&#20013;&#30340;&#31163;&#32676;&#26816;&#27979;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#24050;&#32463;&#35777;&#26126;&#26159;&#21482;&#20351;&#29992;&#20869;&#20998;&#24067;(ID)&#26679;&#20363;&#25991;&#26412;&#36827;&#34892;&#31163;&#32676;&#26816;&#27979;&#30340;&#23453;&#36149;&#32452;&#25104;&#37096;&#20998;&#12290;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#35201;&#20040;&#36890;&#36807;&#20351;&#29992;ID&#26679;&#20363;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#28982;&#21518;&#23558;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#22256;&#24785;&#24230;&#20316;&#20026;&#31163;&#32676;&#24471;&#20998;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#20004;&#31181;&#31163;&#32676;&#26816;&#27979;&#26041;&#27861;&#30340;&#20114;&#34917;&#29305;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#23427;&#20204;&#20248;&#21183;&#24182;&#20943;&#36731;&#23427;&#20204;&#30340;&#23616;&#38480;&#24615;&#30340;&#22810;&#23618;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;&#20316;&#20026;&#32769;&#24072;&#65292;&#22312;ID&#31034;&#20363;&#19978;&#25945;&#25480;&#19968;&#20010;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#38500;&#20102;&#39044;&#27979;&#23618;&#33976;&#39311;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#20013;&#38388;&#23618;&#33976;&#39311;&#26041;&#27861;&#65292;&#20197;&#20840;&#38754;&#25506;&#32034;&#32769;&#24072;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#23398;&#20064;&#30340;&#23398;&#29983;&#21487;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;ID&#25968;&#25454;&#27969;&#24418;&#65292;&#21516;&#26102;&#33719;&#24471;&#26356;&#24378;&#30340;&#23558;OoD&#31034;&#20363;&#26144;&#23556;&#21040;&#27969;&#24418;&#20043;&#22806;&#30340;&#33021;&#21147;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#31454;&#20105;&#22522;&#32447;&#30456;&#27604;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised representation learning has proved to be a valuable component for out-of-distribution (OoD) detection with only the texts of in-distribution (ID) examples. These approaches either train a language model from scratch or fine-tune a pre-trained language model using ID examples, and then take the perplexity output by the language model as OoD scores. In this paper, we analyze the complementary characteristics of both OoD detection methods and propose a multi-level knowledge distillation approach that integrates their strengths while mitigating their limitations. Specifically, we use a fine-tuned model as the teacher to teach a randomly initialized student model on the ID examples. Besides the prediction layer distillation, we present a similarity-based intermediate layer distillation method to thoroughly explore the representation space of the teacher model. In this way, the learned student can better represent the ID data manifold while gaining a stronger ability to map O
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#35299;&#20915;&#35768;&#22810;&#22270;&#20687;&#20219;&#21153;(&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;)&#26102;&#21487;&#20197;&#24573;&#30053;&#20559;&#32622;&#65292;&#24182;&#19988;&#38646;&#20559;&#32622;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#38469;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#21516;&#26102;&#20855;&#26377;&#26631;&#37327; (&#20056;&#27861;) &#19981;&#21464;&#24615;&#65292;&#20174;&#32780;&#22312;&#25913;&#21464;&#23545;&#27604;&#24230;&#26102;&#20173;&#33021;&#20445;&#25345;&#39044;&#27979;&#19981;&#21464;&#12290;</title><link>http://arxiv.org/abs/2211.08486</link><description>&lt;p&gt;
&#38646;&#20559;&#32622;&#26631;&#37327;&#19981;&#21464;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Scalar Invariant Networks with Zero Bias. (arXiv:2211.08486v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#35299;&#20915;&#35768;&#22810;&#22270;&#20687;&#20219;&#21153;(&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;)&#26102;&#21487;&#20197;&#24573;&#30053;&#20559;&#32622;&#65292;&#24182;&#19988;&#38646;&#20559;&#32622;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#38469;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#21516;&#26102;&#20855;&#26377;&#26631;&#37327; (&#20056;&#27861;) &#19981;&#21464;&#24615;&#65292;&#20174;&#32780;&#22312;&#25913;&#21464;&#23545;&#27604;&#24230;&#26102;&#20173;&#33021;&#20445;&#25345;&#39044;&#27979;&#19981;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#26435;&#37325;&#19968;&#26679;&#65292;&#20559;&#32622;&#39033;&#20063;&#26159;&#35768;&#22810;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;(&#21253;&#25324;&#31070;&#32463;&#32593;&#32476;)&#21487;&#23398;&#20064;&#30340;&#21442;&#25968;&#12290;&#20154;&#20204;&#35748;&#20026;&#20559;&#24046;&#33021;&#26377;&#25928;&#22320;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#33021;&#21147;&#26469;&#35299;&#20915;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#65292;&#22914;&#26524;&#25105;&#20204;&#20174;&#31532;&#19968;&#21407;&#29702;&#32771;&#34385;&#22270;&#20687;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#20869;&#22312;&#20998;&#24067;&#20197;&#21450;&#27169;&#22411;&#24212;&#20855;&#26377;&#30340;&#19968;&#20123;&#26399;&#26395;&#29305;&#24615;&#65292;&#21017;&#20559;&#24046;&#21487;&#20197;&#23436;&#20840;&#24573;&#30053;&#65292;&#20197;&#35299;&#20915;&#35768;&#22810;&#19982;&#22270;&#20687;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#35266;&#23519;&#32467;&#26524;&#34920;&#26126;&#65292;&#38646;&#20559;&#32622;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#38469;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#21487;&#33021;&#19982;&#24102;&#20559;&#32622;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#29616;&#30456;&#24403;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#38646;&#20559;&#32622;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#31216;&#20026;&#26631;&#37327;(&#20056;&#27861;)&#19981;&#21464;&#24615;&#30340;&#33391;&#22909;&#23646;&#24615;&#65292;&#36825;&#20351;&#24471;&#24403;&#25913;&#21464;&#36755;&#20837;&#22270;&#20687;&#30340;&#23545;&#27604;&#24230;&#26102;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#20445;&#25345;&#19981;&#21464;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#26631;&#37327;&#19981;&#21464;&#24615;&#25193;&#23637;&#21040;&#26356;&#19968;&#33324;&#30340;&#24773;&#20917;&#8230;
&lt;/p&gt;
&lt;p&gt;
Just like weights, bias terms are the learnable parameters of many popular machine learning models, including neural networks. Biases are believed to effectively increase the representational power of neural networks to solve a wide range of tasks in computer vision. However, we argue that if we consider the intrinsic distribution of images in the input space as well as some desired properties a model should have from the first principles, biases can be completely ignored in addressing many image-related tasks, such as image classification. Our observation indicates that zero-bias neural networks could perform comparably to neural networks with bias at least on practical image classification tasks. In addition, we prove that zero-bias neural networks possess a nice property called scalar (multiplication) invariance, which allows the prediction of neural networks remains the same when altering the contrast of the input image. We then extend scalar invariance to more general cases that a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#35828;&#26381;&#24615;&#20889;&#20316;&#25216;&#24039;&#30340;&#25991;&#26412;&#27573;&#33853;&#36827;&#34892;&#20998;&#31867;&#26469;&#22686;&#21152;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#26032;&#23618;&#27425;&#65292;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#29702;&#30001;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#24120;&#35265;&#35828;&#26381;&#24615;&#20889;&#20316;&#31574;&#30053;&#30340;&#27880;&#37322;&#26041;&#26696;&#21644;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992; RoBERTa &#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2211.05985</link><description>&lt;p&gt;
&#20351;&#29992;&#35828;&#26381;&#24615;&#20889;&#20316;&#31574;&#30053;&#26469;&#35299;&#37322;&#21644;&#26816;&#27979;&#20581;&#24247;&#38169;&#35823;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Using Persuasive Writing Strategies to Explain and Detect Health Misinformation. (arXiv:2211.05985v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#35828;&#26381;&#24615;&#20889;&#20316;&#25216;&#24039;&#30340;&#25991;&#26412;&#27573;&#33853;&#36827;&#34892;&#20998;&#31867;&#26469;&#22686;&#21152;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#26032;&#23618;&#27425;&#65292;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#29702;&#30001;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#24120;&#35265;&#35828;&#26381;&#24615;&#20889;&#20316;&#31574;&#30053;&#30340;&#27880;&#37322;&#26041;&#26696;&#21644;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992; RoBERTa &#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#20551;&#20449;&#24687;&#30340;&#20256;&#25773;&#26159;&#24403;&#20170;&#31038;&#20250;&#30340;&#19968;&#22823;&#38382;&#39064;&#65292;&#35768;&#22810;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#30740;&#31350;&#20154;&#21592;&#27491;&#22312;&#21162;&#21147;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#30001;&#20110;&#27599;&#22825;&#21019;&#36896;&#30340;&#34394;&#20551;&#20449;&#24687;&#25968;&#37327;&#24040;&#22823;&#65292;&#23558;&#27492;&#20219;&#21153;&#30041;&#32473;&#20154;&#24037;&#20107;&#23454;&#26816;&#26597;&#21592;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#30740;&#31350;&#20154;&#21592;&#22810;&#24180;&#26469;&#19968;&#30452;&#33268;&#21147;&#20110;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#65292;&#20294;&#20170;&#22825;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30446;&#26631;&#26159;&#20026;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#28155;&#21152;&#19968;&#20010;&#26032;&#23618;&#27425;&#65307;&#20351;&#29992;&#20855;&#26377;&#35828;&#26381;&#24615;&#20889;&#20316;&#25216;&#24039;&#30340;&#25991;&#26412;&#27573;&#33853;&#36827;&#34892;&#20998;&#31867;&#65292;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#29702;&#30001;&#65292;&#35828;&#26126;&#20026;&#20160;&#20040;&#36825;&#31687;&#25991;&#31456;&#21487;&#20197;&#26631;&#35760;&#20026;&#34394;&#20551;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#35768;&#22810;&#24120;&#35265;&#35828;&#26381;&#24615;&#20889;&#20316;&#31574;&#30053;&#30340;&#26032;&#27880;&#37322;&#26041;&#26696;&#65292;&#20197;&#21450;&#30456;&#24212;&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992; RoBERTa &#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#26469;&#23436;&#25104;&#27492;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#20855;&#26377;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20960;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#32467;&#26524;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The spread of misinformation is a prominent problem in today's society, and many researchers in academia and industry are trying to combat it. Due to the vast amount of misinformation that is created every day, it is unrealistic to leave this task to human fact-checkers. Data scientists and researchers have been working on automated misinformation detection for years, and it is still a challenging problem today. The goal of our research is to add a new level to automated misinformation detection; classifying segments of text with persuasive writing techniques in order to produce interpretable reasoning for why an article can be marked as misinformation. To accomplish this, we present a novel annotation scheme containing many common persuasive writing tactics, along with a dataset with human annotations accordingly. For this task, we make use of a RoBERTa model for text classification, due to its high performance in NLP. We develop several language model-based baselines and present the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22312;&#21464;&#21387;&#22120;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#21516;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#39044;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#25110;&#35757;&#32451;&#26102;&#38388;&#36755;&#20837;&#25200;&#21160;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#23884;&#20837;&#31354;&#38388;&#25200;&#21160;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27867;&#21270;&#24615;&#12290;&#31070;&#32463;&#20803;&#30340;&#35821;&#35328;&#30456;&#20851;&#24615;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#20123;&#25913;&#36827;&#26159;&#30001;&#20110;&#23384;&#22312;&#8220;&#26356;&#19987;&#19994;&#8221;&#30340;&#31070;&#32463;&#20803;&#12290;&#36825;&#26159;&#39318;&#20010;&#23545;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#35757;&#32451;&#19981;&#21516;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2211.05523</link><description>&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of Adversarial Training on Robustness and Generalizability of Language Models. (arXiv:2211.05523v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22312;&#21464;&#21387;&#22120;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#21516;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#39044;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#25110;&#35757;&#32451;&#26102;&#38388;&#36755;&#20837;&#25200;&#21160;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#23884;&#20837;&#31354;&#38388;&#25200;&#21160;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27867;&#21270;&#24615;&#12290;&#31070;&#32463;&#20803;&#30340;&#35821;&#35328;&#30456;&#20851;&#24615;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#20123;&#25913;&#36827;&#26159;&#30001;&#20110;&#23384;&#22312;&#8220;&#26356;&#19987;&#19994;&#8221;&#30340;&#31070;&#32463;&#20803;&#12290;&#36825;&#26159;&#39318;&#20010;&#23545;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#35757;&#32451;&#19981;&#21516;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#38450;&#24481;&#23545;&#25239;&#25915;&#20987;&#30340;&#26368;&#26377;&#25928;&#25163;&#27573;&#12290;&#20294;&#26159;&#65292;&#24050;&#32463;&#30830;&#35748;&#23545;&#25239;&#35757;&#32451;&#27169;&#22411;&#21516;&#26102;&#23454;&#29616;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#38656;&#35201;&#36827;&#34892;&#26435;&#34913;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#28145;&#20837;&#27604;&#36739;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#21516;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21464;&#21387;&#22120;&#35821;&#35328;&#27169;&#22411;&#20013;&#39044;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#12289;&#35757;&#32451;&#26102;&#38388;&#36755;&#20837;&#25200;&#21160;&#21644;&#23884;&#20837;&#31354;&#38388;&#25200;&#21160;&#23545;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#25110;&#35757;&#32451;&#26102;&#38388;&#36755;&#20837;&#25200;&#21160;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#23884;&#20837;&#31354;&#38388;&#25200;&#21160;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#26174;&#33879;&#22320;&#25552;&#39640;&#27867;&#21270;&#24615;&#12290;&#23398;&#20064;&#27169;&#22411;&#31070;&#32463;&#20803;&#30340;&#35821;&#35328;&#30456;&#20851;&#24615;&#20998;&#26512;&#34920;&#26126;&#65292;&#25913;&#21892;&#27867;&#21270;&#24615;&#26159;&#30001;&#20110;&#23384;&#22312;&#8220;&#26356;&#19987;&#19994;&#8221;&#30340;&#31070;&#32463;&#20803;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#20010;&#23545;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#35757;&#32451;&#19981;&#21516;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training is widely acknowledged as the most effective defense against adversarial attacks. However, it is also well established that achieving both robustness and generalization in adversarially trained models involves a trade-off. The goal of this work is to provide an in depth comparison of different approaches for adversarial training in language models. Specifically, we study the effect of pre-training data augmentation as well as training time input perturbations vs. embedding space perturbations on the robustness and generalization of transformer-based language models. Our findings suggest that better robustness can be achieved by pre-training data augmentation or by training with input space perturbation. However, training with embedding space perturbation significantly improves generalization. A linguistic correlation analysis of neurons of the learned models reveals that the improved generalization is due to 'more specialized' neurons. To the best of our knowledge,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#28151;&#21512;&#35268;&#21010;&#22120;&#36827;&#34892;&#20219;&#21153;&#24863;&#30693;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#22312;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#20102;93.59%&#30340;&#25972;&#20307;&#26631;&#20934;&#21270;&#24615;&#33021;&#65292;&#36229;&#36807;&#20043;&#21069;&#30340;&#22522;&#32447;&#65292;&#26159;&#35299;&#20915;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#20013;&#27867;&#21270;&#33021;&#21147;&#25552;&#21319;&#30340;&#19968;&#39033;&#37325;&#35201;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2209.12016</link><description>&lt;p&gt;
&#20174;&#20687;&#32032;&#24320;&#22987;&#25484;&#25569;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Mastering the Unsupervised Reinforcement Learning Benchmark from Pixels. (arXiv:2209.12016v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#28151;&#21512;&#35268;&#21010;&#22120;&#36827;&#34892;&#20219;&#21153;&#24863;&#30693;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#22312;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#20102;93.59%&#30340;&#25972;&#20307;&#26631;&#20934;&#21270;&#24615;&#33021;&#65292;&#36229;&#36807;&#20043;&#21069;&#30340;&#22522;&#32447;&#65292;&#26159;&#35299;&#20915;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#20013;&#27867;&#21270;&#33021;&#21147;&#25552;&#21319;&#30340;&#19968;&#39033;&#37325;&#35201;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35270;&#35273;&#24863;&#30693;&#25968;&#25454;&#20013;&#25511;&#21046;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#12290;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#25104;&#21151;&#65292;&#20294;&#38656;&#35201;&#26234;&#33021;&#20307;&#19982;&#29615;&#22659;&#20043;&#38388;&#22823;&#37327;&#30340;&#20132;&#20114;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#25552;&#20986;&#20102;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#20132;&#20114;&#21644;&#23398;&#20064;&#30340;&#31574;&#30053;&#65292;&#20197;&#26356;&#24555;&#22320;&#36866;&#24212;&#26410;&#26469;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#27491;&#22914;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#25152;&#26174;&#31034;&#30340;&#37027;&#26679;&#65292;&#24403;&#21069;&#30340;&#26080;&#30417;&#30563;&#31574;&#30053;&#26159;&#21542;&#33021;&#22815;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#22312;&#35270;&#35273;&#25511;&#21046;&#29615;&#22659;&#20013;&#23588;&#20854;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#23427;&#65292;&#20351;&#29992;&#26080;&#30417;&#30563;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#26469;&#39044;&#35757;&#32451;&#26234;&#33021;&#20307;&#65292;&#24182;&#32467;&#21512;&#26032;&#25552;&#20986;&#30340;&#28151;&#21512;&#35268;&#21010;&#22120;Dyna-MPC&#26469;&#36827;&#34892;&#20219;&#21153;&#24863;&#30693;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#20197;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#22312;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33719;&#24471;&#20102;93.59%&#30340;&#25972;&#20307;&#26631;&#20934;&#21270;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#20197;&#21069;&#30340;&#22522;&#32447;&#12290;&#25105;&#20204;&#36890;&#36807;&#22823;&#35268;&#27169;&#28040;&#34701;&#30740;&#31350;&#23545;&#35813;&#26041;&#27861;&#36827;&#34892;&#20102;&#32463;&#39564;&#35780;&#20272;&#65292;&#24182;&#34920;&#26126;&#39044;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;Dyna-MPC&#26159;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#20102;&#35299;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#22312;&#35270;&#35273;&#25511;&#21046;&#20013;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#25552;&#20379;&#20102;&#32463;&#39564;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controlling artificial agents from visual sensory data is an arduous task. Reinforcement learning (RL) algorithms can succeed but require large amounts of interactions between the agent and the environment. To alleviate the issue, unsupervised RL proposes to employ self-supervised interaction and learning, for adapting faster to future tasks. Yet, as shown in the Unsupervised RL Benchmark (URLB; Laskin et al. 2021), whether current unsupervised strategies can improve generalization capabilities is still unclear, especially in visual control settings. In this work, we study the URLB and propose a new method to solve it, using unsupervised model-based RL, for pre-training the agent, and a task-aware fine-tuning strategy combined with a new proposed hybrid planner, Dyna-MPC, to adapt the agent for downstream tasks. On URLB, our method obtains 93.59% overall normalized performance, surpassing previous baselines by a staggering margin. The approach is empirically evaluated through a large-s
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#31283;&#20581;&#39640;&#25928;&#30340;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#20020;&#24202;&#20107;&#20214;&#25277;&#21462;&#26041;&#27861;DICE&#65292;&#24341;&#20837;&#20102;&#23545;&#27604;&#24615;&#23398;&#20064;&#30446;&#26631;&#21644;&#29305;&#27530;&#26631;&#35760;&#65292;&#20849;&#21516;&#35757;&#32451;&#23454;&#20307;&#25552;&#21450;&#21644;&#20107;&#20214;&#25277;&#21462;&#31561;&#36741;&#21161;&#20219;&#21153;&#65292;&#25152;&#25552;&#20986;&#30340;MACCROBAT-EE&#25968;&#25454;&#38598;&#20026;&#20020;&#24202;&#20107;&#20214;&#25277;&#21462;&#25552;&#20379;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2208.07989</link><description>&lt;p&gt;
DICE&#65306;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#39640;&#25928;&#20020;&#24202;&#20107;&#20214;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
DICE: Data-Efficient Clinical Event Extraction with Generative Models. (arXiv:2208.07989v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07989
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#31283;&#20581;&#39640;&#25928;&#30340;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#20020;&#24202;&#20107;&#20214;&#25277;&#21462;&#26041;&#27861;DICE&#65292;&#24341;&#20837;&#20102;&#23545;&#27604;&#24615;&#23398;&#20064;&#30446;&#26631;&#21644;&#29305;&#27530;&#26631;&#35760;&#65292;&#20849;&#21516;&#35757;&#32451;&#23454;&#20307;&#25552;&#21450;&#21644;&#20107;&#20214;&#25277;&#21462;&#31561;&#36741;&#21161;&#20219;&#21153;&#65292;&#25152;&#25552;&#20986;&#30340;MACCROBAT-EE&#25968;&#25454;&#38598;&#20026;&#20020;&#24202;&#20107;&#20214;&#25277;&#21462;&#25552;&#20379;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#39046;&#22495;&#30340;&#20107;&#20214;&#25277;&#21462;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#21450;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#30340;&#25968;&#37327;&#20247;&#22810;&#21644;&#23454;&#20307;&#30028;&#38480;&#27169;&#31946;&#65292;&#20351;&#24471;&#36825;&#39033;&#20219;&#21153;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DICE&#65292;&#19968;&#31181;&#31283;&#20581;&#12289;&#39640;&#25928;&#30340;&#20020;&#24202;&#20107;&#20214;&#25277;&#21462;&#29983;&#25104;&#27169;&#22411;&#12290;DICE&#23558;&#20107;&#20214;&#25277;&#21462;&#20316;&#20026;&#26465;&#20214;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#23545;&#27604;&#24615;&#23398;&#20064;&#30446;&#26631;&#65292;&#20197;&#20934;&#30830;&#30830;&#23450;&#29983;&#29289;&#21307;&#23398;&#25552;&#21450;&#30340;&#36793;&#30028;&#12290;DICE&#36824;&#32852;&#21512;&#35757;&#32451;&#36741;&#21161;&#25552;&#21450;&#26631;&#35782;&#20219;&#21153;&#21644;&#20107;&#20214;&#25277;&#21462;&#20219;&#21153;&#65292;&#20197;&#26356;&#22909;&#22320;&#30830;&#23450;&#23454;&#20307;&#25552;&#21450;&#36793;&#30028;&#65292;&#24182;&#36827;&#19968;&#27493;&#24341;&#20837;&#29305;&#27530;&#30340;&#26631;&#35760;&#26469;&#20316;&#20026;&#35302;&#21457;&#22120;&#21644;&#21442;&#25968;&#20505;&#36873;&#39033;&#65292;&#20197;&#21253;&#21547;&#20854;&#21508;&#33258;&#30340;&#20219;&#21153;&#20013;&#30340;&#30830;&#23450;&#23454;&#20307;&#38382;&#39064;&#12290;&#20026;&#20102;&#23545;&#20020;&#24202;&#20107;&#20214;&#25277;&#21462;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#26681;&#25454;&#29616;&#26377;&#30340;&#20020;&#24202;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;MACCRO&#65292;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;&#24102;&#26377;&#21442;&#25968;&#25209;&#27880;&#30340;&#20020;&#24202;&#20107;&#20214;&#25277;&#21462;&#25968;&#25454;&#38598;MACCROBAT-EE&#12290;
&lt;/p&gt;
&lt;p&gt;
Event extraction for the clinical domain is an under-explored research area. The lack of training data along with the high volume of domain-specific terminologies with vague entity boundaries makes the task especially challenging. In this paper, we introduce DICE, a robust and data-efficient generative model for clinical event extraction. DICE frames event extraction as a conditional generation problem and introduces a contrastive learning objective to accurately decide the boundaries of biomedical mentions. DICE also trains an auxiliary mention identification task jointly with event extraction tasks to better identify entity mention boundaries, and further introduces special markers to incorporate identified entity mentions as trigger and argument candidates for their respective tasks. To benchmark clinical event extraction, we compose MACCROBAT-EE, the first clinical event extraction dataset with argument annotation, based on an existing clinical information extraction dataset MACCRO
&lt;/p&gt;</description></item><item><title>Claim-Dissector&#26159;&#19968;&#27454;&#32852;&#21512;&#37325;&#25490;&#21644;&#30495;&#23454;&#24615;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#30340;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;&#65292;&#21487;&#20197;&#35782;&#21035;&#19982;&#22768;&#26126;&#30456;&#20851;&#30340;&#35777;&#25454;&#65292;&#24182;&#30830;&#23450;&#22768;&#26126;&#30340;&#30495;&#23454;&#24615;&#12290;&#35813;&#31995;&#32479;&#30340;&#20010;&#20154;&#36129;&#29486;&#20197;&#21450;&#35777;&#25454;&#25152;&#25903;&#25345;&#25110;&#21453;&#39539;&#22768;&#26126;&#30340;&#36129;&#29486;&#37117;&#21487;&#20197;&#34987;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2207.14116</link><description>&lt;p&gt;
Claim-Dissector: &#19968;&#27454;&#32852;&#21512;&#37325;&#25490;&#21644;&#30495;&#23454;&#24615;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#30340;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Claim-Dissector: An Interpretable Fact-Checking System with Joint Re-ranking and Veracity Prediction. (arXiv:2207.14116v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.14116
&lt;/p&gt;
&lt;p&gt;
Claim-Dissector&#26159;&#19968;&#27454;&#32852;&#21512;&#37325;&#25490;&#21644;&#30495;&#23454;&#24615;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#30340;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;&#65292;&#21487;&#20197;&#35782;&#21035;&#19982;&#22768;&#26126;&#30456;&#20851;&#30340;&#35777;&#25454;&#65292;&#24182;&#30830;&#23450;&#22768;&#26126;&#30340;&#30495;&#23454;&#24615;&#12290;&#35813;&#31995;&#32479;&#30340;&#20010;&#20154;&#36129;&#29486;&#20197;&#21450;&#35777;&#25454;&#25152;&#25903;&#25345;&#25110;&#21453;&#39539;&#22768;&#26126;&#30340;&#36129;&#29486;&#37117;&#21487;&#20197;&#34987;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Claim-Dissector&#65292;&#19968;&#31181;&#38024;&#23545;&#20107;&#23454;&#26680;&#26597;&#21644;&#20998;&#26512;&#30340;&#26032;&#22411;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#32473;&#20986;&#19968;&#20010;&#22768;&#26126;&#21644;&#19968;&#32452;&#26816;&#32034;&#21040;&#30340;&#35777;&#25454;&#65292;&#32852;&#21512;&#23398;&#20064;&#35782;&#21035;&#65306;&#65288;i&#65289;&#19982;&#32473;&#23450;&#22768;&#26126;&#30456;&#20851;&#30340;&#35777;&#25454;&#65292;&#65288;ii&#65289;&#22768;&#26126;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#24314;&#35758;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#35299;&#24320;&#27599;&#20010;&#35777;&#25454;&#30340;&#30456;&#20851;&#24615;&#27010;&#29575;&#21450;&#20854;&#23545;&#26368;&#32456;&#30495;&#23454;&#24615;&#27010;&#29575;&#30340;&#24433;&#21709;-&#26368;&#32456;&#30495;&#23454;&#24615;&#27010;&#29575;&#19982;&#27599;&#20010;&#35777;&#25454;&#30456;&#20851;&#24615;&#27010;&#29575;&#30340;&#32447;&#24615;&#25972;&#21512;&#25104;&#27604;&#20363;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#27599;&#20010;&#35777;&#25454;&#23545;&#26368;&#32456;&#39044;&#27979;&#27010;&#29575;&#30340;&#20010;&#20154;&#36129;&#29486;&#12290;&#22312;&#27599;&#20010;&#35777;&#25454;&#30340;&#30456;&#20851;&#24615;&#27010;&#29575;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#21487;&#20197;&#36827;&#19968;&#27493;&#21306;&#20998;&#27599;&#20010;&#30456;&#20851;&#35777;&#25454;&#26159;&#25903;&#25345;&#65288;S&#65289;&#36824;&#26159;&#21453;&#39539;&#65288;R&#65289;&#22768;&#26126;&#12290;&#36825;&#26679;&#21487;&#20197;&#37327;&#21270;S/R&#27010;&#29575;&#23545;&#26368;&#32456;&#32467;&#35770;&#30340;&#36129;&#29486;&#25110;&#26816;&#27979;&#26377;&#24322;&#35758;&#30340;&#35777;&#25454;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#31995;&#32479;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#20294;&#22312;FEVER&#31454;&#36187;&#20013;&#65292;&#20854;&#32467;&#26524;&#19982;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Claim-Dissector: a novel latent variable model for fact-checking and analysis, which given a claim and a set of retrieved evidences jointly learns to identify: (i) the relevant evidences to the given claim, (ii) the veracity of the claim. We propose to disentangle the per-evidence relevance probability and its contribution to the final veracity probability in an interpretable way -- the final veracity probability is proportional to a linear ensemble of per-evidence relevance probabilities. In this way, the individual contributions of evidences towards the final predicted probability can be identified. In per-evidence relevance probability, our model can further distinguish whether each relevant evidence is supporting (S) or refuting (R) the claim. This allows to quantify how much the S/R probability contributes to the final verdict or to detect disagreeing evidence.  Despite its interpretable nature, our system achieves results competitive with state-of-the-art on the FEVER 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#36712;&#38899;&#20048;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#25903;&#25345;&#21508;&#31181;&#19981;&#21516;&#30340;&#20048;&#22120;&#65292;&#22312;&#30701;&#30340;&#24207;&#21015;&#38271;&#24230;&#19979;&#23454;&#29616;&#20102;&#24615;&#33021;&#19978;&#30340;&#26174;&#33879;&#25552;&#21319;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#29992;&#20110;&#20998;&#26512;&#38899;&#20048;&#33258;&#25105;&#20851;&#27880;&#65292;&#24182;&#39564;&#35777;&#20102;&#27169;&#22411;&#26356;&#20851;&#27880;&#19982;&#24403;&#21069;&#38899;&#31526;&#24418;&#25104;&#21644;&#35856;&#36328;&#24230;&#21644;&#20301;&#20110;&#21516;&#19968;&#20843;&#24230;&#30340;&#38899;&#31526;&#12290;</title><link>http://arxiv.org/abs/2207.06983</link><description>&lt;p&gt;
&#22810;&#36712;&#38899;&#20048; Transformer
&lt;/p&gt;
&lt;p&gt;
Multitrack Music Transformer. (arXiv:2207.06983v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.06983
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#36712;&#38899;&#20048;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#25903;&#25345;&#21508;&#31181;&#19981;&#21516;&#30340;&#20048;&#22120;&#65292;&#22312;&#30701;&#30340;&#24207;&#21015;&#38271;&#24230;&#19979;&#23454;&#29616;&#20102;&#24615;&#33021;&#19978;&#30340;&#26174;&#33879;&#25552;&#21319;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#29992;&#20110;&#20998;&#26512;&#38899;&#20048;&#33258;&#25105;&#20851;&#27880;&#65292;&#24182;&#39564;&#35777;&#20102;&#27169;&#22411;&#26356;&#20851;&#27880;&#19982;&#24403;&#21069;&#38899;&#31526;&#24418;&#25104;&#21644;&#35856;&#36328;&#24230;&#21644;&#20301;&#20110;&#21516;&#19968;&#20843;&#24230;&#30340;&#38899;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20351;&#29992; Transformer &#27169;&#22411;&#29983;&#25104;&#22810;&#36712;&#38899;&#20048;&#30340;&#26041;&#27861;&#22312;&#20048;&#22120;&#25968;&#37327;&#12289;&#38899;&#20048;&#29255;&#27573;&#38271;&#24230;&#21644;&#25512;&#29702;&#36895;&#24230;&#26041;&#38754;&#26377;&#38480;&#21046;&#65292;&#37096;&#20998;&#21407;&#22240;&#22312;&#20110;&#24050;&#26377;&#34920;&#31034;&#26041;&#24335;&#38656;&#35201;&#38271;&#24230;&#36739;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#65292;&#20174;&#32780;&#38656;&#35201;&#26356;&#22810;&#30340;&#20869;&#23384;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#36712;&#38899;&#20048;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#25903;&#25345;&#21508;&#31181;&#19981;&#21516;&#30340;&#20048;&#22120;&#65292;&#21516;&#26102;&#20351;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#26356;&#30701;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340; Multitrack Music Transformer&#65288;MMT&#65289;&#19982;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#30456;&#27604;&#20855;&#26377;&#21487;&#27604;&#24615;&#65292;&#22312;&#20027;&#35266;&#21548;&#27979;&#35797;&#20013;&#25490;&#22312;&#20004;&#20010;&#26368;&#36817;&#25552;&#20986;&#30340;&#27169;&#22411;&#20043;&#38388;&#65292;&#21516;&#26102;&#22312;&#36895;&#24230;&#21644;&#20869;&#23384;&#21344;&#29992;&#19978;&#37117;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#65292;&#20351;&#24471;&#35813;&#26041;&#27861;&#22312;&#23454;&#26102;&#21363;&#20852;&#21019;&#20316;&#25110;&#25509;&#36817;&#23454;&#26102;&#30340;&#21019;&#24847;&#24212;&#29992;&#20013;&#26356;&#20026;&#23454;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#38899;&#20048;&#33258;&#25105;&#20851;&#27880;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#35757;&#32451;&#27169;&#22411;&#26356;&#20851;&#27880;&#19982;&#24403;&#21069;&#38899;&#31526;&#24418;&#25104;&#21644;&#35856;&#36328;&#24230;&#21644;&#20301;&#20110;&#21516;&#19968;&#20843;&#24230;&#30340;&#38899;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing approaches for generating multitrack music with transformer models have been limited in terms of the number of instruments, the length of the music segments and slow inference. This is partly due to the memory requirements of the lengthy input sequences necessitated by existing representations. In this work, we propose a new multitrack music representation that allows a diverse set of instruments while keeping a short sequence length. Our proposed Multitrack Music Transformer (MMT) achieves comparable performance with state-of-the-art systems, landing in between two recently proposed models in a subjective listening test, while achieving substantial speedups and memory reductions over both, making the method attractive for real time improvisation or near real time creative applications. Further, we propose a new measure for analyzing musical self-attention and show that the trained model attends more to notes that form a consonant interval with the current note and to notes th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;DGraph&#30340;&#22823;&#35268;&#27169;&#37329;&#34701;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#35266;&#23519;&#21644;&#24191;&#27867;&#23454;&#39564;&#65292;&#25581;&#31034;&#20102;&#24322;&#24120;&#33410;&#28857;&#21644;&#27491;&#24120;&#33410;&#28857;&#19981;&#21516;&#30340;&#29305;&#24449;&#21644;&#32467;&#26500;&#65292;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#33410;&#28857;&#37117;&#26159;&#26816;&#27979;&#27450;&#35784;&#32773;&#30340;&#24517;&#19981;&#21487;&#23569;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2207.03579</link><description>&lt;p&gt;
DGraph: &#29992;&#20110;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#22823;&#35268;&#27169;&#37329;&#34701;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DGraph: A Large-Scale Financial Dataset for Graph Anomaly Detection. (arXiv:2207.03579v3 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.03579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;DGraph&#30340;&#22823;&#35268;&#27169;&#37329;&#34701;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#35266;&#23519;&#21644;&#24191;&#27867;&#23454;&#39564;&#65292;&#25581;&#31034;&#20102;&#24322;&#24120;&#33410;&#28857;&#21644;&#27491;&#24120;&#33410;&#28857;&#19981;&#21516;&#30340;&#29305;&#24449;&#21644;&#32467;&#26500;&#65292;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#33410;&#28857;&#37117;&#26159;&#26816;&#27979;&#27450;&#35784;&#32773;&#30340;&#24517;&#19981;&#21487;&#23569;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24322;&#24120;&#26816;&#27979;&#65288;GAD&#65289;&#30001;&#20110;&#20854;&#23454;&#29992;&#24615;&#21644;&#29702;&#35770;&#20215;&#20540;&#32780;&#25104;&#20026;&#36817;&#26399;&#28909;&#38376;&#30740;&#31350;&#39046;&#22495;&#12290;&#30001;&#20110;GAD&#24378;&#35843;&#24212;&#29992;&#21644;&#24322;&#24120;&#26679;&#26412;&#30340;&#32597;&#35265;&#24615;&#65292;&#20016;&#23500;&#20854;&#25968;&#25454;&#38598;&#30340;&#21464;&#21270;&#26159;&#22522;&#30784;&#24615;&#30340;&#24037;&#20316;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;DGraph&#65292;&#23427;&#26159;&#37329;&#34701;&#39046;&#22495;&#20013;&#30340;&#23454;&#38469;&#21160;&#24577;&#22270;&#12290;DGraph&#20811;&#26381;&#20102;&#24403;&#21069;GAD&#25968;&#25454;&#38598;&#30340;&#35768;&#22810;&#38480;&#21046;&#12290;&#23427;&#21253;&#21547;&#32422;3&#30334;&#19975;&#20010;&#33410;&#28857;&#12289;4&#30334;&#19975;&#20010;&#21160;&#24577;&#36793;&#32536;&#21644;1&#30334;&#19975;&#20010;&#22320;&#38754;&#30495;&#23454;&#33410;&#28857;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;DGraph&#30340;&#20840;&#38754;&#35266;&#23519;&#65292;&#25581;&#31034;&#20102;&#24322;&#24120;&#33410;&#28857;&#21644;&#27491;&#24120;&#33410;&#28857;&#36890;&#24120;&#20855;&#26377;&#19981;&#21516;&#30340;&#32467;&#26500;&#12289;&#37051;&#23621;&#20998;&#24067;&#21644;&#26102;&#38388;&#21160;&#24577;&#12290;&#27492;&#22806;&#65292;&#23427;&#34920;&#26126;&#26631;&#35760;&#33410;&#28857;&#23545;&#20110;&#26816;&#27979;&#27450;&#35784;&#32773;&#20063;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;DGraph&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#35266;&#23519;&#21644;&#23454;&#39564;&#34920;&#26126;&#65292;DGraph&#26159;&#25512;&#21160;GAD&#30740;&#31350;&#30340;&#26377;&#21147;&#24037;&#20855;&#65292;&#21487;&#20197;&#28145;&#20837;&#25506;&#32034;&#24322;&#24120;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Anomaly Detection (GAD) has recently become a hot research spot due to its practicability and theoretical value. Since GAD emphasizes the application and the rarity of anomalous samples, enriching the varieties of its datasets is fundamental work. Thus, this paper present DGraph, a real-world dynamic graph in the finance domain. DGraph overcomes many limitations of current GAD datasets. It contains about 3M nodes, 4M dynamic edges, and 1M ground-truth nodes. We provide a comprehensive observation of DGraph, revealing that anomalous nodes and normal nodes generally have different structures, neighbor distribution, and temporal dynamics. Moreover, it suggests that unlabeled nodes are also essential for detecting fraudsters. Furthermore, we conduct extensive experiments on DGraph. Observation and experiments demonstrate that DGraph is propulsive to advance GAD research and enable in-depth exploration of anomalous nodes.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21487;&#35782;&#21035;&#30340;&#27010;&#24565;&#21457;&#29616;&#26041;&#27861;&#65292;&#21487;&#20197;&#24674;&#22797;&#20986;&#22810;&#20010;&#24050;&#30693;&#30340;&#27010;&#24565;&#65292;&#20197;&#30830;&#20445;&#35299;&#37322;&#30340;&#21487;&#38752;&#24615;&#12290;&#23545;&#20110;&#20855;&#26377;&#20381;&#36182;&#20851;&#31995;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#30340;&#21151;&#33021;&#32452;&#21512;&#24615;&#36136;&#12290;&#35813;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.13872</link><description>&lt;p&gt;
&#21518;&#39564;&#27010;&#24565;&#35299;&#37322;&#20309;&#26102;&#21487;&#35782;&#21035;&#65311;
&lt;/p&gt;
&lt;p&gt;
When are Post-hoc Conceptual Explanations Identifiable?. (arXiv:2206.13872v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.13872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21487;&#35782;&#21035;&#30340;&#27010;&#24565;&#21457;&#29616;&#26041;&#27861;&#65292;&#21487;&#20197;&#24674;&#22797;&#20986;&#22810;&#20010;&#24050;&#30693;&#30340;&#27010;&#24565;&#65292;&#20197;&#30830;&#20445;&#35299;&#37322;&#30340;&#21487;&#38752;&#24615;&#12290;&#23545;&#20110;&#20855;&#26377;&#20381;&#36182;&#20851;&#31995;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#30340;&#21151;&#33021;&#32452;&#21512;&#24615;&#36136;&#12290;&#35813;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#23884;&#20837;&#36890;&#24120;&#38656;&#35201;&#36890;&#36807;&#27010;&#24565;&#35299;&#37322;&#26469;&#29702;&#35299;&#21644;&#20998;&#35299;&#65292;&#36825;&#31181;&#38656;&#27714;&#22312;&#35299;&#37322;&#20013;&#19981;&#21253;&#21547;&#26377;&#25928;&#27010;&#24565;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#23588;&#20026;&#26174;&#33879;&#12290;&#20026;&#20102;&#25552;&#20379;&#21518;&#39564;&#35299;&#37322;&#65292;&#27010;&#24565;&#21457;&#29616;&#26041;&#27861;&#20250;&#22312;&#24050;&#35757;&#32451;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#25628;&#32034;&#35299;&#37322;&#24615;&#24378;&#30340;&#27010;&#24565;&#65292;&#20363;&#22914;&#29289;&#20307;&#24418;&#29366;&#25110;&#39068;&#33394;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#35748;&#20026;&#27010;&#24565;&#21457;&#29616;&#24212;&#35813;&#26159;&#21487;&#35782;&#21035;&#30340;&#65292;&#36825;&#24847;&#21619;&#30528;&#21487;&#20197;&#34987;&#35777;&#26126;&#22320;&#24674;&#22797;&#20986;&#22810;&#20010;&#24050;&#30693;&#30340;&#27010;&#24565;&#65292;&#20197;&#30830;&#20445;&#35299;&#37322;&#30340;&#21487;&#38752;&#24615;&#12290;&#20026;&#20102;&#20316;&#20026;&#19968;&#20010;&#36215;&#28857;&#65292;&#25105;&#20204;&#26126;&#30830;&#22320;&#23558;&#27010;&#24565;&#21457;&#29616;&#19982;&#20256;&#32479;&#26041;&#27861;&#65288;&#20363;&#22914;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#65289;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#34920;&#26126;&#23427;&#20204;&#21487;&#20197;&#24674;&#22797;&#20855;&#26377;&#38750;&#39640;&#26031;&#20998;&#24067;&#30340;&#29420;&#31435;&#27010;&#24565;&#26469;&#38416;&#26126;&#36825;&#19968;&#28857;&#12290;&#23545;&#20110;&#20855;&#26377;&#20381;&#36182;&#20851;&#31995;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#30340;&#21151;&#33021;&#32452;&#21512;&#24615;&#36136;&#12290;&#25105;&#20204;&#30340;&#21487;&#35777;&#26126;&#21487;&#35782;&#21035;&#30340;&#27010;&#24565;&#21457;&#29616;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interest in understanding and factorizing learned embedding spaces through conceptual explanations is steadily growing. When no human concept labels are available, concept discovery methods search trained embedding spaces for interpretable concepts like object shape or color that can be used to provide post-hoc explanations for decisions. Unlike previous work, we argue that concept discovery should be identifiable, meaning that a number of known concepts can be provably recovered to guarantee reliability of the explanations. As a starting point, we explicitly make the connection between concept discovery and classical methods like Principal Component Analysis and Independent Component Analysis by showing that they can recover independent concepts with non-Gaussian distributions. For dependent concepts, we propose two novel approaches that exploit functional compositionality properties of image-generating processes. Our provably identifiable concept discovery methods substantially outpe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#33258;&#36866;&#24212;&#31163;&#31574;&#30053;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#29366;&#24577;&#20215;&#20540;&#20989;&#25968;&#26469;&#20026;&#26426;&#22120;&#20154;&#22312;&#30495;&#23454;&#25361;&#25112;&#24615;&#29615;&#22659;&#20013;&#25506;&#32034;&#25552;&#20379;&#25351;&#23548;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#31232;&#30095;&#22806;&#37096;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#26356;&#22810;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2204.03140</link><description>&lt;p&gt;
&#22312;&#25361;&#25112;&#24615;&#29615;&#22659;&#20013;&#36827;&#34892;&#26426;&#22120;&#20154;&#25506;&#32034;&#30340;&#22312;&#32447;&#33258;&#36866;&#24212;&#31163;&#31574;&#30053;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Off-Policy Evaluation with Online Adaptation for Robot Exploration in Challenging Environments. (arXiv:2204.03140v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.03140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#33258;&#36866;&#24212;&#31163;&#31574;&#30053;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#29366;&#24577;&#20215;&#20540;&#20989;&#25968;&#26469;&#20026;&#26426;&#22120;&#20154;&#22312;&#30495;&#23454;&#25361;&#25112;&#24615;&#29615;&#22659;&#20013;&#25506;&#32034;&#25552;&#20379;&#25351;&#23548;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#31232;&#30095;&#22806;&#37096;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#26356;&#22810;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#25506;&#32034;&#20855;&#26377;&#35768;&#22810;&#37325;&#35201;&#24212;&#29992;&#12290;&#20294;&#26159;&#65292;&#22522;&#20110;&#20449;&#24687;&#22686;&#30410;&#25110;&#22522;&#20110;&#21069;&#27839;&#30340;&#20256;&#32479;&#25506;&#32034;&#20165;&#20381;&#36182;&#20110;&#26426;&#22120;&#20154;&#24403;&#21069;&#29366;&#24577;&#26469;&#30830;&#23450;&#21363;&#26102;&#25506;&#32034;&#30446;&#26631;&#65292;&#32570;&#20047;&#39044;&#27979;&#26410;&#26469;&#29366;&#24577;&#20215;&#20540;&#30340;&#33021;&#21147;&#65292;&#22240;&#27492;&#23548;&#33268;&#25506;&#32034;&#20915;&#31574;&#20302;&#25928;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23398;&#20064;&#22914;&#20309;&#34913;&#37327;&#8220;&#22909;&#8221;&#29366;&#24577;&#65288;&#20197;&#29366;&#24577;&#20215;&#20540;&#20989;&#25968;&#34913;&#37327;&#65289;&#65292;&#20026;&#26426;&#22120;&#20154;&#22312;&#30495;&#23454;&#25361;&#25112;&#24615;&#29615;&#22659;&#20013;&#25506;&#32034;&#25552;&#20379;&#25351;&#23548;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#24037;&#20316;&#21046;&#23450;&#20026;&#26426;&#22120;&#20154;&#25506;&#32034;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#65288;OPERE&#65289;&#38382;&#39064;&#12290;&#23427;&#21253;&#25324;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#31163;&#32447;&#33945;&#29305;&#21345;&#32599;&#35757;&#32451;&#65292;&#24182;&#25191;&#34892;&#26102;&#38388;&#24046;&#20998;&#65288;TD&#65289;&#22312;&#32447;&#33258;&#36866;&#24212;&#26469;&#20248;&#21270;&#32463;&#36807;&#35757;&#32451;&#30340;&#20215;&#20540;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#20256;&#24863;&#22120;&#20449;&#24687;&#35206;&#30422;&#29575;&#30340;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#65292;&#20197;&#20351;&#26426;&#22120;&#20154;&#22312;&#31232;&#30095;&#22806;&#37096;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#26356;&#22810;&#20449;&#24687;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#39044;&#27979;&#26410;&#26469;&#29366;&#24577;&#30340;&#20215;&#20540;&#65292;&#24182;&#22312;&#25361;&#25112;&#24615;&#29615;&#22659;&#20013;&#20570;&#20986;&#39640;&#25928;&#30340;&#25506;&#32034;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous exploration has many important applications. However, classic information gain-based or frontier-based exploration only relies on the robot current state to determine the immediate exploration goal, which lacks the capability of predicting the value of future states and thus leads to inefficient exploration decisions. This paper presents a method to learn how "good" states are, measured by the state value function, to provide a guidance for robot exploration in real-world challenging environments. We formulate our work as an off-policy evaluation (OPE) problem for robot exploration (OPERE). It consists of offline Monte-Carlo training on real-world data and performs Temporal Difference (TD) online adaptation to optimize the trained value estimator. We also design an intrinsic reward function based on sensor information coverage to enable the robot to gain more information with sparse extrinsic rewards. Results show that our method enables the robot to predict the value of fut
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#35821;&#20041;&#26144;&#23556;&#30340;&#20998;&#23618;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#35821;&#38899;&#25351;&#20196;&#21644;waypoint&#28789;&#27963;&#22320;&#35268;&#21010;&#36335;&#24452;&#65292;&#21253;&#25324;&#22320;&#28857;&#36830;&#36890;&#24615;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#24555;&#36895;&#36817;&#20284;&#25512;&#29702;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2203.10820</link><description>&lt;p&gt;
&#21033;&#29992;&#22522;&#20110;&#31354;&#38388;&#27010;&#24565;&#30340;&#25299;&#25169;&#35821;&#20041;&#26144;&#23556;&#36827;&#34892;&#20174;&#35821;&#38899;&#25351;&#20196;&#30340;&#20998;&#23618;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Path-planning from Speech Instructions with Spatial Concept-based Topometric Semantic Mapping. (arXiv:2203.10820v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.10820
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#35821;&#20041;&#26144;&#23556;&#30340;&#20998;&#23618;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#35821;&#38899;&#25351;&#20196;&#21644;waypoint&#28789;&#27963;&#22320;&#35268;&#21010;&#36335;&#24452;&#65292;&#21253;&#25324;&#22320;&#28857;&#36830;&#36890;&#24615;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#24555;&#36895;&#36817;&#20284;&#25512;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#20351;&#29992;&#20154;&#31867;&#35821;&#38899;&#25351;&#20196;&#36827;&#34892;&#26426;&#22120;&#20154;&#23548;&#33322;&#33267;&#30446;&#30340;&#22320;&#26159;&#33258;&#20027;&#31227;&#21160;&#26426;&#22120;&#20154;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#37319;&#29992;&#19981;&#21516;&#30340;&#36335;&#24452;&#21040;&#36798;&#21516;&#19968;&#30446;&#26631;&#65292;&#32780;&#26368;&#30701;&#36335;&#24452;&#19981;&#19968;&#23450;&#26159;&#26368;&#20248;&#30340;&#12290;&#22240;&#27492;&#38656;&#35201;&#19968;&#31181;&#26041;&#27861;&#26469;&#28789;&#27963;&#22320;&#25509;&#21463;waypoint&#30340;&#25351;&#26631;&#65292;&#35268;&#21010;&#26356;&#22909;&#30340;&#26367;&#20195;&#36335;&#24452;&#65292;&#21363;&#20351;&#38656;&#35201;&#32469;&#36335;&#12290;&#27492;&#22806;&#65292;&#26426;&#22120;&#20154;&#38656;&#35201;&#23454;&#26102;&#25512;&#29702;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25299;&#25169;&#35821;&#20041;&#26144;&#23556;&#23454;&#29616;&#19968;&#20010;&#20998;&#23618;&#30340;&#31354;&#38388;&#34920;&#31034;&#65292;&#24182;&#32467;&#21512;&#35821;&#38899;&#25351;&#20196;&#21644;waypoint&#36827;&#34892;&#36335;&#24452;&#35268;&#21010;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SpCoTMHP&#65292;&#19968;&#31181;&#21033;&#29992;&#22810;&#27169;&#24335;&#31354;&#38388;&#27010;&#24565;&#30340;&#23618;&#27425;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#21253;&#25324;&#22320;&#28857;&#36830;&#36890;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#27010;&#29575;&#29983;&#25104;&#27169;&#22411;&#21644;&#24555;&#36895;&#36817;&#20284;&#25512;&#29702;&#26041;&#27861;&#65292;&#23618;&#27425;&#32467;&#26500;&#20013;&#30340;&#21508;&#20010;&#32423;&#21035;&#20043;&#38388;&#21487;&#20197;&#30456;&#20114;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
Navigating to destinations using human speech instructions is essential for autonomous mobile robots operating in the real world. Although robots can take different paths toward the same goal, the shortest path is not always optimal. A desired approach is to flexibly accommodate waypoint specifications, planning a better alternative path, even with detours. Furthermore, robots require real-time inference capabilities. Spatial representations include semantic, topological, and metric levels, each capturing different aspects of the environment. This study aims to realize a hierarchical spatial representation by a topometric semantic map and path planning with speech instructions, including waypoints. We propose SpCoTMHP, a hierarchical path-planning method that utilizes multimodal spatial concepts, incorporating place connectivity. This approach provides a novel integrated probabilistic generative model and fast approximate inference, with interaction among the hierarchy levels. A formul
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31038;&#20132;&#23186;&#20307;&#20013;&#31038;&#20250;&#35821;&#29992;&#24847;&#20041;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#22312;&#21508;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2203.07648</link><description>&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#20013;&#31038;&#20250;&#35821;&#29992;&#24847;&#20041;&#30340;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning of Sociopragmatic Meaning in Social Media. (arXiv:2203.07648v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.07648
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31038;&#20132;&#23186;&#20307;&#20013;&#31038;&#20250;&#35821;&#29992;&#24847;&#20041;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#22312;&#21508;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#31561;&#30740;&#31350;&#36827;&#23637;&#23578;&#26410;&#24191;&#27867;&#32771;&#34385;&#31038;&#20250;&#35821;&#29992;&#24847;&#20041;&#36825;&#19968;&#31867;&#21035;&#65288;&#21363;&#19981;&#21516;&#35821;&#35328;&#31038;&#21306;&#20869;&#30340;&#20132;&#27969;&#24847;&#20041;&#65289;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#21487;&#36801;&#31227;&#33267;&#21508;&#31181;&#31038;&#20250;&#35821;&#29992;&#20219;&#21153;&#65288;&#22914;&#24773;&#24863;&#12289;&#20167;&#24680;&#35328;&#35770;&#12289;&#24189;&#40664;&#12289;&#35773;&#21050;&#65289;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#25968;&#25454;&#20197;&#21450;&#19968;&#33324;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#21508;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;&#20363;&#22914;&#65292;&#19982;&#20004;&#20010;&#27969;&#34892;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27599;&#20010;&#25968;&#25454;&#38598;&#20165;&#29992;20&#20010;&#35757;&#32451;&#26679;&#26412;&#24494;&#35843;&#26102;&#65292;&#24179;&#22343;F1&#20540;&#22312;16&#20010;&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#20102;11.66&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in representation and contrastive learning in NLP has not widely considered the class of \textit{sociopragmatic meaning} (i.e., meaning in interaction within different language communities). To bridge this gap, we propose a novel framework for learning task-agnostic representations transferable to a wide range of sociopragmatic tasks (e.g., emotion, hate speech, humor, sarcasm). Our framework outperforms other contrastive learning frameworks for both in-domain and out-of-domain data, across both the general and few-shot settings. For example, compared to two popular pre-trained language models, our method obtains an improvement of $11.66$ average $F_1$ on $16$ datasets when fine-tuned on only $20$ training samples per dataset.
&lt;/p&gt;</description></item><item><title>HyperMixer&#26159;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#22522;&#20110;MLP&#30340;Transformer&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#21160;&#24577;&#24418;&#25104;&#26631;&#35760;&#28151;&#21512;MLP&#26469;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65292;&#20854;&#24615;&#33021;&#27604;&#26367;&#20195;&#26041;&#26696;&#22909;&#65292;&#24182;&#21487;&#19982;Transformer&#23218;&#32654;&#65292;&#25104;&#26412;&#26356;&#20302;&#12290;</title><link>http://arxiv.org/abs/2203.03691</link><description>&lt;p&gt;
HyperMixer&#65306;&#19968;&#31181;&#22522;&#20110;MLP&#30340;&#20302;&#25104;&#26412;Transformer&#26367;&#20195;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
HyperMixer: An MLP-based Low Cost Alternative to Transformers. (arXiv:2203.03691v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.03691
&lt;/p&gt;
&lt;p&gt;
HyperMixer&#26159;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#22522;&#20110;MLP&#30340;Transformer&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#21160;&#24577;&#24418;&#25104;&#26631;&#35760;&#28151;&#21512;MLP&#26469;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65292;&#20854;&#24615;&#33021;&#27604;&#26367;&#20195;&#26041;&#26696;&#22909;&#65292;&#24182;&#21487;&#19982;Transformer&#23218;&#32654;&#65292;&#25104;&#26412;&#26356;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26550;&#26500;&#26159;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#39318;&#36873;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#30340;&#25104;&#26412;&#30456;&#24403;&#39640;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#36755;&#20837;&#38271;&#24230;&#26041;&#38754;&#20855;&#26377;&#20108;&#27425;&#22797;&#26434;&#24230;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#33021;&#38590;&#20197;&#35843;&#25972;&#12290;&#20026;&#20102;&#38477;&#20302;&#25104;&#26412;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31616;&#21333;&#30340;&#22522;&#20110;MLP&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#26550;&#26500;&#65288;&#20363;&#22914;MLPMixer&#65289;&#36890;&#36807;&#38745;&#24577;&#30340;MLP&#29420;&#31435;&#22320;&#24212;&#29992;&#20110;&#27599;&#20010;&#29305;&#24449;&#65292;&#32780;&#36807;&#20110;&#33073;&#31163;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#25152;&#38656;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25913;&#36827;&#65292;&#21363;HyperMixer&#65292;&#23427;&#20351;&#29992;&#36229;&#32593;&#32476;&#21160;&#24577;&#22320;&#24418;&#25104;&#26631;&#35760;&#28151;&#21512;MLP&#12290;&#23454;&#39564;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#26367;&#20195;&#30340;&#22522;&#20110;MLP&#30340;&#27169;&#22411;&#65292;&#24182;&#19982;Transformer&#23218;&#32654;&#12290;&#19982;Transformer&#19981;&#21516;&#65292;HyperMixer&#22312;&#22788;&#29702;&#26102;&#38388;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#38754;&#20855;&#26377;&#22823;&#22823;&#38477;&#20302;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based architectures are the model of choice for natural language understanding, but they come at a significant cost, as they have quadratic complexity in the input length, require a lot of training data, and can be difficult to tune. In the pursuit of lower costs, we investigate simple MLP-based architectures. We find that existing architectures such as MLPMixer, which achieves token mixing through a static MLP applied to each feature independently, are too detached from the inductive biases required for natural language understanding. In this paper, we propose a simple variant, HyperMixer, which forms the token mixing MLP dynamically using hypernetworks. Empirically, we demonstrate that our model performs better than alternative MLP-based models, and on par with Transformers. In contrast to Transformers, HyperMixer achieves these results at substantially lower costs in terms of processing time, training data, and hyperparameter tuning.
&lt;/p&gt;</description></item><item><title>pNLP-Mixer&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;MLP-Mixer&#27169;&#22411;&#65292;&#19981;&#38656;&#35201;&#23884;&#20837;&#23618;&#65292;&#29992;&#20110;&#35774;&#22791;&#19978;&#39640;&#25928;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#21487;&#20197;&#36798;&#21040;&#22522;&#20110;transformer&#26550;&#26500;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30456;&#36817;&#30340;&#24615;&#33021;&#65292;&#21364;&#21482;&#38656;&#35201;&#24456;&#23569;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2202.04350</link><description>&lt;p&gt;
pNLP-Mixer&#65306;&#19968;&#31181;&#39640;&#25928;&#30340;&#20840;MLP&#26550;&#26500;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
pNLP-Mixer: an Efficient all-MLP Architecture for Language. (arXiv:2202.04350v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.04350
&lt;/p&gt;
&lt;p&gt;
pNLP-Mixer&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;MLP-Mixer&#27169;&#22411;&#65292;&#19981;&#38656;&#35201;&#23884;&#20837;&#23618;&#65292;&#29992;&#20110;&#35774;&#22791;&#19978;&#39640;&#25928;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#21487;&#20197;&#36798;&#21040;&#22522;&#20110;transformer&#26550;&#26500;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30456;&#36817;&#30340;&#24615;&#33021;&#65292;&#21364;&#21482;&#38656;&#35201;&#24456;&#23569;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#39046;&#22495;&#30340;&#26684;&#23616;&#12290;&#28982;&#32780;&#65292;&#22312;&#26234;&#33021;&#25163;&#34920;&#31561;&#21463;&#38480;&#35774;&#22791;&#19978;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#23436;&#20840;&#19981;&#21487;&#34892;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#22823;&#23567;&#21644;&#25512;&#29702;&#25104;&#26412;&#12290;&#20316;&#20026;Transformer&#26550;&#26500;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#26368;&#36817;&#20851;&#20110;&#39640;&#25928;NLP&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#26435;&#37325;&#39640;&#25928;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#20806;&#23383;&#33410;&#32423;&#30340;&#27169;&#22411;&#22823;&#23567;&#20013;&#33719;&#24471;&#31616;&#21333;&#20219;&#21153;(&#22914;&#27133;&#22635;&#20805;&#21644;&#24847;&#22270;&#20998;&#31867;)&#30340;&#31454;&#20105;&#24615;&#33021;&#12290;&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;pNLP-Mixer&#26550;&#26500;&#65292;&#19968;&#31181;&#29992;&#20110;&#35774;&#22791;&#19978;NLP&#30340;&#26080;&#23884;&#20837;MLP-Mixer&#27169;&#22411;&#65292;&#30001;&#20110;&#37319;&#29992;&#20102;&#26032;&#39062;&#30340;&#25237;&#24433;&#23618;&#65292;&#22240;&#27492;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#26435;&#37325;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22810;&#35821;&#20041;&#35299;&#26512;&#25968;&#25454;&#38598;MTOP&#21644;multiATIS&#19978;&#35780;&#20272;&#20102;&#19968;&#20010;&#22823;&#23567;&#20165;&#20026;1&#20806;&#23383;&#33410;&#30340;pNLP-Mixer&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#37327;&#21270;&#27169;&#22411;&#22312;MTOP&#21644;multi-ATIS&#19978;&#23454;&#29616;&#20102;mBERT&#30340;99.4&#65285;&#21644;97.8&#65285;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#29992;&#30340;&#36164;&#28304;&#20165;&#20026;mBERT&#30340;170&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained language models based on transformer architecture have drastically changed the natural language processing (NLP) landscape. However, deploying those models for on-device applications in constrained devices such as smart watches is completely impractical due to their size and inference cost. As an alternative to transformer-based architectures, recent work on efficient NLP has shown that weight-efficient models can attain competitive performance for simple tasks, such as slot filling and intent classification, with model sizes in the order of the megabyte. This work introduces the pNLP-Mixer architecture, an embedding-free MLP-Mixer model for on-device NLP that achieves high weight-efficiency thanks to a novel projection layer. We evaluate a pNLP-Mixer model of only one megabyte in size on two multi-lingual semantic parsing datasets, MTOP and multiATIS. Our quantized model achieves 99.4% and 97.8% the performance of mBERT on MTOP and multi-ATIS, while using 170x fewer 
&lt;/p&gt;</description></item></channel></rss>