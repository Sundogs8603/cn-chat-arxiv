<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;CPU&#19978;&#39640;&#25928;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26041;&#27861;&#65292;&#25903;&#25345;&#33258;&#21160;&#26435;&#37325;&#37327;&#21270;&#21644;&#20248;&#21270;&#20869;&#26680;&#65292;&#22312;&#27969;&#34892;&#30340;LLMs&#19978;&#23637;&#31034;&#20102;&#26497;&#39640;&#30340;&#25512;&#29702;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2311.00502</link><description>&lt;p&gt;
&#22312;CPU&#19978;&#39640;&#25928;&#30340;LLM&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Efficient LLM Inference on CPUs. (arXiv:2311.00502v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;CPU&#19978;&#39640;&#25928;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26041;&#27861;&#65292;&#25903;&#25345;&#33258;&#21160;&#26435;&#37325;&#37327;&#21270;&#21644;&#20248;&#21270;&#20869;&#26680;&#65292;&#22312;&#27969;&#34892;&#30340;LLMs&#19978;&#23637;&#31034;&#20102;&#26497;&#39640;&#30340;&#25512;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#21644;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#22411;&#21442;&#25968;&#30340;&#24222;&#22823;&#25968;&#37327;&#65292;LLMs&#30340;&#37096;&#32626;&#19968;&#30452;&#38754;&#20020;&#25361;&#25112;&#65292;&#23545;&#22823;&#20869;&#23384;&#23481;&#37327;&#21644;&#39640;&#20869;&#23384;&#24102;&#23485;&#30340;&#38656;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;LLMs&#30340;&#37096;&#32626;&#26356;&#39640;&#25928;&#12290;&#25105;&#20204;&#25903;&#25345;&#33258;&#21160;&#30340;INT4&#26435;&#37325;&#37327;&#21270;&#27969;&#31243;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#29305;&#27530;&#30340;LLM&#36816;&#34892;&#26102;&#65292;&#20855;&#26377;&#39640;&#24230;&#20248;&#21270;&#30340;&#20869;&#26680;&#65292;&#20197;&#21152;&#36895;&#22312;CPU&#19978;&#30340;LLM&#25512;&#29702;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27969;&#34892;&#30340;LLMs&#19978;&#30340;&#26222;&#36866;&#24615;&#65292;&#21253;&#25324;Llama2&#65292;Llama&#65292;GPT-NeoX&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;CPU&#19978;&#30340;&#26497;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;&#20195;&#30721;&#20844;&#24320;&#21487;&#29992;&#20110;: https://github.com/intel/intel-extension-for-transformers.
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable performance and tremendous potential across a wide range of tasks. However, deploying these models has been challenging due to the astronomical amount of model parameters, which requires a demand for large memory capacity and high memory bandwidth. In this paper, we propose an effective approach that can make the deployment of LLMs more efficiently. We support an automatic INT4 weight-only quantization flow and design a special LLM runtime with highly-optimized kernels to accelerate the LLM inference on CPUs. We demonstrate the general applicability of our approach on popular LLMs including Llama2, Llama, GPT-NeoX, and showcase the extreme inference efficiency on CPUs. The code is publicly available at: https://github.com/intel/intel-extension-for-transformers.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#24515;&#29702;&#27979;&#37327;&#29702;&#35770;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26222;&#36941;&#26234;&#33021;&#22240;&#23376;g&#30340;&#23384;&#22312;&#65292;&#24182;&#21457;&#29616;&#20102;&#35813;&#22240;&#23376;&#35299;&#37322;&#27169;&#22411;&#24615;&#33021;&#26041;&#24046;&#30340;85%&#65292;&#20026;&#27169;&#22411;&#35780;&#20272;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.11616</link><description>&lt;p&gt;
&#25581;&#31034;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26222;&#36941;&#26234;&#33021;&#22240;&#23376;&#65306;&#19968;&#31181;&#24515;&#29702;&#27979;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unveiling the General Intelligence Factor in Language Models: A Psychometric Approach. (arXiv:2310.11616v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#24515;&#29702;&#27979;&#37327;&#29702;&#35770;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26222;&#36941;&#26234;&#33021;&#22240;&#23376;g&#30340;&#23384;&#22312;&#65292;&#24182;&#21457;&#29616;&#20102;&#35813;&#22240;&#23376;&#35299;&#37322;&#27169;&#22411;&#24615;&#33021;&#26041;&#24046;&#30340;85%&#65292;&#20026;&#27169;&#22411;&#35780;&#20272;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#24515;&#29702;&#27979;&#37327;&#29702;&#35770;&#65292;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#26222;&#36941;&#26234;&#33021;&#22240;&#23376;g&#30340;&#23384;&#22312;&#65292;&#24182;&#25193;&#23637;&#20102;&#35813;&#29702;&#35770;&#22312;&#20154;&#31867;&#21644;&#26576;&#20123;&#21160;&#29289;&#29289;&#31181;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;Open LLM Leaderboard&#65288;&#21253;&#21547;1,232&#20010;&#27169;&#22411;&#65289;&#21644;General Language Understanding Evaluation&#65288;GLUE&#65289;Leaderboard&#65288;&#21253;&#21547;88&#20010;&#27169;&#22411;&#65289;&#36827;&#34892;&#22240;&#23376;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#20855;&#26377;&#19968;&#32500;&#24615;&#21644;&#39640;&#24230;&#31283;&#23450;&#24615;&#30340;g&#22240;&#23376;&#65292;&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#24615;&#33021;&#26041;&#24046;&#30340;85%&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#21644;g&#20043;&#38388;&#30340;&#20013;&#24230;&#30456;&#20851;&#24615;&#20026;0.48&#12290;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;g&#22240;&#23376;&#20026;&#27169;&#22411;&#35780;&#20272;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#25351;&#26631;&#65292;&#20026;&#26356;&#24378;&#22823;&#12289;&#22522;&#20110;g&#22240;&#23376;&#30340;&#27169;&#22411;&#33021;&#21147;&#35780;&#20272;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#20174;&#24515;&#29702;&#27979;&#37327;&#30340;&#35282;&#24230;&#29702;&#35299;&#21644;&#26410;&#26469;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#24182;&#23545;&#27169;&#22411;&#35780;&#20272;&#21644;&#24320;&#21457;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study uncovers the factor of general intelligence, or g, in language models, extending the psychometric theory traditionally applied to humans and certain animal species. Utilizing factor analysis on two extensive datasets Open LLM Leaderboard with 1,232 models and General Language Understanding Evaluation (GLUE) Leaderboard with 88 models - we find compelling evidence for a unidimensional, highly stable g factor that accounts for 85% of the variance in model performance. The study also finds a moderate correlation of .48 between model size and g. The discovery of g in language models offers a unified metric for model evaluation and opens new avenues for more robust, g-based model ability assessment. These findings lay the foundation for understanding and future research on artificial general intelligence from a psychometric perspective and have practical implications for model evaluation and development.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#22768;&#23398;&#29305;&#24615;&#29983;&#25104;&#38899;&#39057;&#25552;&#31034;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#26469;&#26356;&#22909;&#22320;&#23398;&#20064;&#24773;&#24863;&#34920;&#36798;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22768;&#23398;&#25552;&#31034;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#24773;&#24863;&#38899;&#39057;&#26816;&#32034;&#21644;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02298</link><description>&lt;p&gt;
&#20351;&#29992;&#22768;&#23398;&#29305;&#24615;&#20026;&#24773;&#24863;&#34920;&#36798;&#29983;&#25104;&#38899;&#39057;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Prompting Audios Using Acoustic Properties For Emotion Representation. (arXiv:2310.02298v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#22768;&#23398;&#29305;&#24615;&#29983;&#25104;&#38899;&#39057;&#25552;&#31034;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#26469;&#26356;&#22909;&#22320;&#23398;&#20064;&#24773;&#24863;&#34920;&#36798;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22768;&#23398;&#25552;&#31034;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#24773;&#24863;&#38899;&#39057;&#26816;&#32034;&#21644;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#23384;&#22312;&#20110;&#19968;&#20010;&#36830;&#32493;&#30340;&#33539;&#22260;&#20869;&#65292;&#20294;&#29616;&#26377;&#27169;&#22411;&#23558;&#24773;&#24863;&#35270;&#20026;&#26377;&#38480;&#31163;&#25955;&#20540;&#21464;&#37327;&#12290;&#36825;&#31181;&#34920;&#31034;&#26041;&#24335;&#26080;&#27861;&#25429;&#25417;&#24773;&#24863;&#34920;&#36798;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#34920;&#31034;&#24773;&#24863;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65288;&#25110;&#25552;&#31034;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#33258;&#21160;&#29983;&#25104;&#36825;&#20123;&#25552;&#31034;&#24182;&#35757;&#32451;&#27169;&#22411;&#20174;&#38899;&#39057;&#21644;&#25552;&#31034;&#23545;&#20013;&#26356;&#22909;&#22320;&#23398;&#20064;&#24773;&#24863;&#34920;&#36798;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#20351;&#29992;&#19982;&#24773;&#24863;&#30456;&#20851;&#30340;&#22768;&#23398;&#29305;&#24615;&#65288;&#22914;&#38899;&#35843;&#12289;&#24378;&#24230;&#12289;&#35821;&#36895;&#21644;&#21457;&#38899;&#36895;&#24230;&#65289;&#26469;&#33258;&#21160;&#29983;&#25104;&#25552;&#31034;&#65292;&#21363;&#8220;&#22768;&#23398;&#25552;&#31034;&#8221;&#12290;&#25105;&#20204;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#23558;&#35821;&#38899;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;&#22768;&#23398;&#25552;&#31034;&#12290;&#25105;&#20204;&#22312;&#24773;&#24863;&#38899;&#39057;&#26816;&#32034;&#21644;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22768;&#23398;&#25552;&#31034;&#22312;EAR&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#21508;&#31181;Precision@K&#25351;&#26631;&#19978;&#30340;&#24615;&#33021;&#12290;&#22312;SER&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;Ravdess&#25968;&#25454;&#19978;&#30340;&#30456;&#23545;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;3.8%&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotions lie on a continuum, but current models treat emotions as a finite valued discrete variable. This representation does not capture the diversity in the expression of emotion. To better represent emotions we propose the use of natural language descriptions (or prompts). In this work, we address the challenge of automatically generating these prompts and training a model to better learn emotion representations from audio and prompt pairs. We use acoustic properties that are correlated to emotion like pitch, intensity, speech rate, and articulation rate to automatically generate prompts i.e. 'acoustic prompts'. We use a contrastive learning objective to map speech to their respective acoustic prompts. We evaluate our model on Emotion Audio Retrieval and Speech Emotion Recognition. Our results show that the acoustic prompts significantly improve the model's performance in EAR, in various Precision@K metrics. In SER, we observe a 3.8% relative accuracy improvement on the Ravdess data
&lt;/p&gt;</description></item><item><title>UPAR&#25552;&#31034;&#26694;&#26550;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#32467;&#26500;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#21487;&#29702;&#35299;&#21644;&#21487;&#26816;&#26597;&#30340;&#25512;&#29702;&#36712;&#36857;&#65292;&#22686;&#24378;&#20102;&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#24182;&#20026;&#29616;&#26377;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#20379;&#20102;&#35748;&#35782;&#35770;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2310.01441</link><description>&lt;p&gt;
UPAR&#65306;&#19968;&#31181;&#21463;&#24247;&#24503;&#21551;&#21457;&#30340;&#20419;&#36827;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
UPAR: A Kantian-Inspired Prompting Framework for Enhancing Large Language Model Capabilities. (arXiv:2310.01441v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01441
&lt;/p&gt;
&lt;p&gt;
UPAR&#25552;&#31034;&#26694;&#26550;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#32467;&#26500;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#21487;&#29702;&#35299;&#21644;&#21487;&#26816;&#26597;&#30340;&#25512;&#29702;&#36712;&#36857;&#65292;&#22686;&#24378;&#20102;&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#24182;&#20026;&#29616;&#26377;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#20379;&#20102;&#35748;&#35782;&#35770;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#35768;&#22810;&#30740;&#31350;&#33268;&#21147;&#20110;&#36890;&#36807;&#25552;&#31034;&#25552;&#21319;&#36825;&#31181;&#33021;&#21147;&#12290;&#23613;&#31649;&#26377;&#36825;&#20123;&#21162;&#21147;&#65292;&#32479;&#19968;&#30340;&#35748;&#35782;&#35770;&#22522;&#30784;&#20173;&#28982;&#26126;&#26174;&#32570;&#22833;&#12290;&#21463;&#24247;&#24503;&#30340;&#20808;&#39564;&#21746;&#23398;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;UPAR&#25552;&#31034;&#26694;&#26550;&#65292;&#26088;&#22312;&#22312;LLMs&#20013;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#32467;&#26500;&#12290;UPAR&#26694;&#26550;&#20998;&#20026;&#22235;&#20010;&#38454;&#27573;&#65306;&#8220;&#29702;&#35299;&#8221;&#12289;&#8220;&#35745;&#21010;&#8221;&#12289;&#8220;&#34892;&#21160;&#8221;&#21644;&#8220;&#21453;&#24605;&#8221;&#65292;&#20351;&#24471;&#33021;&#22815;&#20174;&#22797;&#26434;&#32972;&#26223;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#65292;&#20107;&#20808;&#35268;&#21010;&#35299;&#20915;&#26041;&#26696;&#65292;&#25353;&#35745;&#21010;&#25191;&#34892;&#65292;&#24182;&#36827;&#34892;&#33258;&#25105;&#21453;&#24605;&#12290;&#36825;&#20010;&#32467;&#26500;&#26174;&#33879;&#22686;&#24378;&#20102;LLM&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#20135;&#29983;&#20102;&#20154;&#31867;&#21487;&#29702;&#35299;&#21644;&#21487;&#26816;&#26597;&#30340;&#25512;&#29702;&#36712;&#36857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#29616;&#26377;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#20379;&#20102;&#35748;&#35782;&#35770;&#22522;&#30784;&#65292;&#21487;&#33021;&#23454;&#29616;&#36825;&#20123;&#26041;&#27861;&#30340;&#31995;&#32479;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated impressive inferential capabilities, with numerous research endeavors devoted to enhancing this capacity through prompting. Despite these efforts, a unified epistemological foundation is still conspicuously absent. Drawing inspiration from Kant's a priori philosophy, we propose the UPAR prompting framework, designed to emulate the structure of human cognition within LLMs. The UPAR framework is delineated into four phases: "Understand", "Plan", "Act", and "Reflect", enabling the extraction of structured information from complex contexts, prior planning of solutions, execution according to plan, and self-reflection. This structure significantly augments the explainability and accuracy of LLM inference, producing a human-understandable and inspectable inferential trajectory. Furthermore, our work offers an epistemological foundation for existing prompting techniques, allowing for a possible systematic integration of these methods. With GPT-4,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#24615;&#26694;&#26550;ACTOR&#65292;&#36890;&#36807;&#29983;&#25104;&#20197;&#19968;&#33268;&#39537;&#21160;&#24773;&#24863;&#20026;&#26465;&#20214;&#30340;&#22810;&#27169;&#24577;&#34892;&#20026;&#65292;&#26088;&#22312;&#22686;&#24378;&#24863;&#30693;&#24773;&#24863;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20855;&#22791;&#34920;&#36798;&#34892;&#20026;&#30340;&#20855;&#36523;&#23545;&#35805;&#26234;&#33021;&#20307;&#20013;&#65292;&#22810;&#27169;&#24577;&#24773;&#24863;&#35843;&#33410;&#21644;&#24773;&#24863;&#19968;&#33268;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2309.15311</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#24773;&#24863;&#35843;&#33410;&#21644;&#24773;&#24863;&#19968;&#33268;&#24615;&#23545;&#20110;&#20855;&#36523;&#23545;&#35805;&#26234;&#33021;&#20307;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Importance of Multimodal Emotion Conditioning and Affect Consistency for Embodied Conversational Agents. (arXiv:2309.15311v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#24615;&#26694;&#26550;ACTOR&#65292;&#36890;&#36807;&#29983;&#25104;&#20197;&#19968;&#33268;&#39537;&#21160;&#24773;&#24863;&#20026;&#26465;&#20214;&#30340;&#22810;&#27169;&#24577;&#34892;&#20026;&#65292;&#26088;&#22312;&#22686;&#24378;&#24863;&#30693;&#24773;&#24863;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20855;&#22791;&#34920;&#36798;&#34892;&#20026;&#30340;&#20855;&#36523;&#23545;&#35805;&#26234;&#33021;&#20307;&#20013;&#65292;&#22810;&#27169;&#24577;&#24773;&#24863;&#35843;&#33410;&#21644;&#24773;&#24863;&#19968;&#33268;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#26377;&#20851;&#23545;&#20110;&#20855;&#36523;&#34394;&#25311;&#26234;&#33021;&#20307;&#24773;&#24863;&#24863;&#30693;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#34394;&#25311;&#35282;&#33394;&#36890;&#36807;&#19982;&#20154;&#31867;&#30340;&#20114;&#21160;&#20256;&#36798;&#24773;&#24863;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#21019;&#24314;&#19968;&#20010;&#20855;&#26377;&#34920;&#36798;&#34892;&#20026;&#30340;&#33258;&#20027;&#20855;&#36523;&#23545;&#35805;&#26234;&#33021;&#20307;&#38754;&#20020;&#30528;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#31532;&#19968;&#20010;&#25361;&#25112;&#26159;&#21512;&#25104;&#27599;&#31181;&#27169;&#24577;&#30340;&#23545;&#35805;&#34892;&#20026;&#38750;&#24120;&#34920;&#36798;&#24615;&#65292;&#20687;&#30495;&#23454;&#20154;&#31867;&#34892;&#20026;&#19968;&#26679;&#20855;&#26377;&#34920;&#36798;&#24615;&#30340;&#22256;&#38590;&#12290;&#31532;&#20108;&#20010;&#25361;&#25112;&#26159;&#24773;&#24863;&#34987;&#29420;&#31435;&#24314;&#27169;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#29983;&#25104;&#22312;&#25152;&#26377;&#27169;&#24577;&#19978;&#20855;&#26377;&#19968;&#33268;&#24773;&#24863;&#30340;&#22810;&#27169;&#24577;&#21709;&#24212;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#24615;&#26694;&#26550;ACTOR&#65288;&#19968;&#33268;&#24615;&#24773;&#24863;&#22810;&#27169;&#24577;&#34892;&#20026;&#29983;&#25104;&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#29983;&#25104;&#20197;&#19968;&#33268;&#39537;&#21160;&#24773;&#24863;&#20026;&#26465;&#20214;&#30340;&#22810;&#27169;&#24577;&#34892;&#20026;&#26469;&#22686;&#24378;&#24773;&#24863;&#24863;&#30693;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#29992;&#25143;&#30740;&#31350;&#65292;&#24182;&#25307;&#21215;&#20102;199&#21517;&#21442;&#19982;&#32773;&#65292;&#20197;&#35780;&#20272;&#26222;&#36890;&#20154;&#22914;&#20309;&#21028;&#26029;&#20174;&#22810;&#27169;&#24577;&#29366;&#24577;&#19979;&#24863;&#30693;&#21040;&#30340;&#24773;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous studies regarding the perception of emotions for embodied virtual agents have shown the effectiveness of using virtual characters in conveying emotions through interactions with humans. However, creating an autonomous embodied conversational agent with expressive behaviors presents two major challenges. The first challenge is the difficulty of synthesizing the conversational behaviors for each modality that are as expressive as real human behaviors. The second challenge is that the affects are modeled independently, which makes it difficult to generate multimodal responses with consistent emotions across all modalities. In this work, we propose a conceptual framework, ACTOR (Affect-Consistent mulTimodal behaviOR generation), that aims to increase the perception of affects by generating multimodal behaviors conditioned on a consistent driving affect. We have conducted a user study with 199 participants to assess how the average person judges the affects perceived from multimoda
&lt;/p&gt;</description></item><item><title>TSGBench&#26159;&#39318;&#20010;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#22522;&#20934;&#65292;&#29992;&#20110;&#32479;&#19968;&#21644;&#20840;&#38754;&#35780;&#20272;TSG&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#24615;&#33021;&#35780;&#20272;&#12289;&#25968;&#25454;&#38598;&#36873;&#25321;&#21644;&#35780;&#20272;&#25351;&#26631;&#19978;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.03755</link><description>&lt;p&gt;
TSGBench&#65306;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
TSGBench: Time Series Generation Benchmark. (arXiv:2309.03755v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03755
&lt;/p&gt;
&lt;p&gt;
TSGBench&#26159;&#39318;&#20010;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#22522;&#20934;&#65292;&#29992;&#20110;&#32479;&#19968;&#21644;&#20840;&#38754;&#35780;&#20272;TSG&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#24615;&#33021;&#35780;&#20272;&#12289;&#25968;&#25454;&#38598;&#36873;&#25321;&#21644;&#35780;&#20272;&#25351;&#26631;&#19978;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;(TSG)&#22312;&#25968;&#25454;&#22686;&#24378;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#38544;&#31169;&#20445;&#25252;&#31561;&#22810;&#20010;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#19977;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;(1)&#23427;&#20204;&#32463;&#24120;&#38024;&#23545;&#31867;&#20284;&#30340;&#27169;&#22411;&#31867;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#38480;&#21046;&#20102;&#23545;&#24615;&#33021;&#33021;&#21147;&#30340;&#25972;&#20307;&#35270;&#35282;&#12290;(2)&#20351;&#29992;&#19987;&#38376;&#30340;&#21512;&#25104;&#21644;&#31169;&#26377;&#25968;&#25454;&#38598;&#24341;&#20837;&#20102;&#20559;&#20506;&#65292;&#38459;&#30861;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;(3)&#27169;&#31946;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#24448;&#24448;&#19982;&#33258;&#23450;&#20041;&#32593;&#32476;&#25110;&#19979;&#28216;&#20219;&#21153;&#30456;&#32467;&#21512;&#65292;&#38459;&#30861;&#20102;&#19968;&#33268;&#21644;&#20844;&#24179;&#30340;&#27604;&#36739;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;\textsf {TSGBench}&#65292;&#20316;&#20026;&#39318;&#20010;TSG&#22522;&#20934;&#65292;&#26088;&#22312;&#32479;&#19968;&#21644;&#20840;&#38754;&#35780;&#20272;TSG&#26041;&#27861;&#12290;&#23427;&#21253;&#25324;&#19977;&#20010;&#27169;&#22359;&#65306;(1)&#19968;&#20010;&#31934;&#24515;&#31574;&#21010;&#30340;&#12289;&#38754;&#21521;TSG&#30340;&#20844;&#24320;&#23454;&#38469;&#25968;&#25454;&#38598;&#25910;&#38598;&#65292;&#20197;&#21450;&#26631;&#20934;&#21270;&#30340;&#39044;&#22788;&#29702;&#27969;&#31243;&#65307;(2)&#19968;&#22871;&#32508;&#21512;&#30340;&#35780;&#20272;&#25351;&#26631;&#22871;&#20214;&#65292;&#21253;&#25324;&#22522;&#26412;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Synthetic Time Series Generation (TSG) is crucial in a range of applications, including data augmentation, anomaly detection, and privacy preservation. Although significant strides have been made in this field, existing methods exhibit three key limitations: (1) They often benchmark against similar model types, constraining a holistic view of performance capabilities. (2) The use of specialized synthetic and private datasets introduces biases and hampers generalizability. (3) Ambiguous evaluation measures, often tied to custom networks or downstream tasks, hinder consistent and fair comparison.  To overcome these limitations, we introduce \textsf{TSGBench}, the inaugural TSG Benchmark, designed for a unified and comprehensive assessment of TSG methods. It comprises three modules: (1) a curated collection of publicly available, real-world datasets tailored for TSG, together with a standardized preprocessing pipeline; (2) a comprehensive evaluation measures suite including vanilla measur
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20248;&#21270;&#20219;&#21153;&#12290;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#32447;&#24615;&#22238;&#24402;&#21644;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#20248;&#21270;&#30340;&#26368;&#20339;&#25552;&#31034;&#36229;&#36807;&#20102;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.03409</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Optimizers. (arXiv:2309.03409v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20248;&#21270;&#20219;&#21153;&#12290;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#32447;&#24615;&#22238;&#24402;&#21644;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#20248;&#21270;&#30340;&#26368;&#20339;&#25552;&#31034;&#36229;&#36807;&#20102;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#26159;&#26080;&#22788;&#19981;&#22312;&#30340;&#12290;&#34429;&#28982;&#22522;&#20110;&#23548;&#25968;&#30340;&#31639;&#27861;&#22312;&#21508;&#31181;&#38382;&#39064;&#19978;&#26159;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#20294;&#26159;&#27809;&#26377;&#26799;&#24230;&#23545;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#20248;&#21270;&#22120;&#65292;&#20854;&#20013;&#20248;&#21270;&#20219;&#21153;&#20197;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#25551;&#36848;&#12290;&#22312;&#27599;&#19968;&#27425;&#20248;&#21270;&#27493;&#39588;&#20013;&#65292;LLM&#20174;&#21253;&#21547;&#20808;&#21069;&#29983;&#25104;&#30340;&#35299;&#19982;&#20854;&#20540;&#30340;&#25552;&#31034;&#20013;&#29983;&#25104;&#26032;&#30340;&#35299;&#65292;&#28982;&#21518;&#23545;&#26032;&#30340;&#35299;&#36827;&#34892;&#35780;&#20272;&#24182;&#28155;&#21152;&#21040;&#25552;&#31034;&#20013;&#65292;&#29992;&#20110;&#19979;&#19968;&#27425;&#20248;&#21270;&#27493;&#39588;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;OPRO&#22312;&#32447;&#24615;&#22238;&#24402;&#21644;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#65292;&#28982;&#21518;&#36716;&#21521;&#25552;&#31034;&#20248;&#21270;&#65292;&#30446;&#26631;&#26159;&#25214;&#21040;&#33021;&#26368;&#22823;&#21270;&#20219;&#21153;&#20934;&#30830;&#24615;&#30340;&#25351;&#20196;&#12290;&#36890;&#36807;&#20351;&#29992;&#21508;&#31181;LLM&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;OPRO&#20248;&#21270;&#30340;&#26368;&#20339;&#25552;&#31034;&#22312;GSM8K&#19978;&#20987;&#36133;&#20102;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#39640;&#36798;8%&#65292;&#22312;Big-Bench Hard&#20219;&#21153;&#19978;&#20987;&#36133;&#20102;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#39640;&#36798;50%&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to prompt optimization where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#25552;&#31034;-&#27169;&#22411;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#29983;&#25104;&#24335;&#25512;&#33616;&#20219;&#21153;&#12290;&#36890;&#36807;GEMRec-18K&#25968;&#25454;&#38598;&#30340;&#30740;&#31350;&#65292;&#20316;&#32773;&#35774;&#35745;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26694;&#26550;&#65292;&#20998;&#21035;&#26159;&#25552;&#31034;-&#27169;&#22411;&#26816;&#32034;&#21644;&#29983;&#25104;&#39033;&#25490;&#24207;&#65292;&#20197;&#35299;&#20915;&#36825;&#19968;&#26032;&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.02205</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#29983;&#25104;&#24335;&#25512;&#33616;&#30340;&#20010;&#24615;&#21270;&#25552;&#31034;-&#27169;&#22411;&#26816;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Personalized Prompt-Model Retrieval for Generative Recommendation. (arXiv:2308.02205v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02205
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#25552;&#31034;-&#27169;&#22411;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#29983;&#25104;&#24335;&#25512;&#33616;&#20219;&#21153;&#12290;&#36890;&#36807;GEMRec-18K&#25968;&#25454;&#38598;&#30340;&#30740;&#31350;&#65292;&#20316;&#32773;&#35774;&#35745;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26694;&#26550;&#65292;&#20998;&#21035;&#26159;&#25552;&#31034;-&#27169;&#22411;&#26816;&#32034;&#21644;&#29983;&#25104;&#39033;&#25490;&#24207;&#65292;&#20197;&#35299;&#20915;&#36825;&#19968;&#26032;&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#26088;&#22312;&#26816;&#32034;&#19982;&#29992;&#25143;&#20449;&#24687;&#38656;&#27714;&#30456;&#20851;&#30340;&#20869;&#23481;&#12290;&#29616;&#26377;&#30340;&#20505;&#36873;&#24211;&#36890;&#24120;&#30001;&#19968;&#32452;&#24050;&#20934;&#22791;&#22909;&#30340;&#39033;&#32452;&#25104;&#65292;&#22914;&#35270;&#39057;&#12289;&#20135;&#21697;&#25110;&#25991;&#31456;&#12290;&#38543;&#30528;&#29983;&#25104;&#24335;AI&#22914;GPT&#21644;Diffusion&#27169;&#22411;&#30340;&#36817;&#26399;&#36827;&#23637;&#65292;&#36824;&#26377;&#19968;&#31181;&#26032;&#24418;&#24335;&#30340;&#25512;&#33616;&#20219;&#21153;&#24453;&#25506;&#32034;&#65292;&#21363;&#36890;&#36807;&#20010;&#24615;&#21270;&#25552;&#31034;&#30001;&#29983;&#25104;&#27169;&#22411;&#21019;&#24314;&#39033;&#12290;&#20197;&#22270;&#20687;&#29983;&#25104;&#20026;&#20363;&#65292;&#20973;&#20511;&#29992;&#25143;&#30340;&#21333;&#20010;&#25552;&#31034;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#35775;&#38382;&#26435;&#38480;&#65292;&#22312;&#20960;&#20998;&#38047;&#20869;&#21487;&#20197;&#29983;&#25104;&#25968;&#30334;&#20010;&#26032;&#22270;&#20687;&#12290;&#22312;&#8220;&#26080;&#38480;&#8221;&#39033;&#30340;&#23384;&#22312;&#19979;&#65292;&#25105;&#20204;&#22914;&#20309;&#23454;&#29616;&#20010;&#24615;&#21270;&#65311;&#22312;&#36825;&#39033;&#21021;&#27493;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26694;&#26550;&#65292;&#21363;&#25552;&#31034;-&#27169;&#22411;&#26816;&#32034;&#21644;&#29983;&#25104;&#39033;&#25490;&#24207;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#26032;&#30340;&#20219;&#21153;&#24418;&#24335;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#21517;&#20026;GEMRec-18K&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#30001;200&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;18K&#22270;&#20687;&#65292;&#24182;&#19982;&#19968;&#22871;&#22810;&#26679;&#21270;&#30340;90&#20010;&#25991;&#26412;&#25552;&#31034;&#30456;&#37197;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender Systems are built to retrieve relevant items to satisfy users' information needs. The candidate corpus usually consists of a finite set of items that are ready to be served, such as videos, products, or articles. With recent advances in Generative AI such as GPT and Diffusion models, a new form of recommendation task is yet to be explored where items are to be created by generative models with personalized prompts. Taking image generation as an example, with a single prompt from the user and access to a generative model, it is possible to generate hundreds of new images in a few minutes. How shall we attain personalization in the presence of "infinite" items? In this preliminary study, we propose a two-stage framework, namely Prompt-Model Retrieval and Generated Item Ranking, to approach this new task formulation. We release GEMRec-18K, a prompt-model interaction dataset with 18K images generated by 200 publicly-available generative models paired with a diverse set of 90 te
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#21019;&#26032;&#30340;&#20851;&#31995;&#23548;&#21521;&#35270;&#35282;&#20986;&#21457;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#30340;&#24314;&#27169;&#33539;&#24335;&#20013;&#30340;&#35266;&#23519;&#27169;&#22411;&#19982;&#23454;&#38469;&#29702;&#35299;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#31995;&#23450;&#20041;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#23454;&#29616;&#20851;&#31995;&#23548;&#21521;&#24314;&#27169;&#30340;&#23454;&#36341;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.16387</link><description>&lt;p&gt;
Relation-Oriented: &#36808;&#21521;&#19982;&#30693;&#35782;&#23545;&#20934;&#30340;&#22240;&#26524;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Relation-Oriented: Toward Knowledge-Aligned Causal AI. (arXiv:2307.16387v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#21019;&#26032;&#30340;&#20851;&#31995;&#23548;&#21521;&#35270;&#35282;&#20986;&#21457;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#30340;&#24314;&#27169;&#33539;&#24335;&#20013;&#30340;&#35266;&#23519;&#27169;&#22411;&#19982;&#23454;&#38469;&#29702;&#35299;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#31995;&#23450;&#20041;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#23454;&#29616;&#20851;&#31995;&#23548;&#21521;&#24314;&#27169;&#30340;&#23454;&#36341;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#33258;&#28982;&#22320;&#24212;&#29992;&#19968;&#20010;&#35266;&#23519;&#23548;&#21521;&#30340;&#21407;&#21017;&#65292;&#20854;&#20013;&#35266;&#23519;&#21464;&#37327;&#20808;&#23384;&#22312;&#24182;&#20026;&#26500;&#24314;&#20851;&#31995;&#22880;&#23450;&#22522;&#30784;&#12290;&#34429;&#28982;&#23545;&#20110;&#20256;&#32479;&#27169;&#22411;&#26469;&#35828;&#36275;&#22815;&#20102;&#65292;&#20294;&#26159;&#20154;&#24037;&#26234;&#33021;&#19982;&#22823;&#25968;&#25454;&#30340;&#25972;&#21512;&#26292;&#38706;&#20102;&#35266;&#23519;&#27169;&#22411;&#19982;&#25105;&#20204;&#30340;&#23454;&#38469;&#29702;&#35299;&#20043;&#38388;&#30340;&#19981;&#23545;&#40784;&#12290;&#30456;&#21453;&#65292;&#20154;&#31867;&#22609;&#36896;&#20102;&#30001;&#20851;&#31995;&#23450;&#20041;&#30340;&#35748;&#30693;&#23454;&#20307;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36328;&#36234;&#26102;&#38388;&#21644;&#36229;&#32500;&#24230;&#31354;&#38388;&#21046;&#23450;&#30693;&#35782;&#65292;&#32780;&#19981;&#26159;&#34987;&#38480;&#21046;&#22312;&#35266;&#23519;&#26500;&#24314;&#20013;&#12290;&#20174;&#19968;&#31181;&#21019;&#26032;&#30340;&#20851;&#31995;&#23548;&#21521;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#26469;&#33258;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20581;&#24247;&#20449;&#24687;&#23398;&#30340;&#30452;&#35266;&#20363;&#23376;&#65292;&#20998;&#26512;&#20102;&#22312;&#25105;&#20204;&#24403;&#21069;&#30340;&#24314;&#27169;&#33539;&#24335;&#20013;&#36825;&#31181;&#19981;&#23545;&#40784;&#30340;&#26681;&#28304;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#20851;&#31995;&#23450;&#20041;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#20851;&#31995;&#23548;&#21521;&#24314;&#27169;&#30340;&#19968;&#31181;&#23454;&#38469;&#23454;&#26045;&#65292;&#25903;&#25345;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
In machine learning, we naturally apply an Observation-Oriented principle, in which observational variables preexist and set the stage for constructing relationships. While sufficient for traditional models, the integration of AI with big data exposes the misalignment between the observational models and our actual comprehension. Contrarily, humans shape cognitive entities defined by relationships, enabling us to formulate knowledge across temporal and hyper-dimensional spaces, rather than being confined to observational constructs. From an innovative Relation-Oriented perspective, this study examines the roots of this misalignment within our current modeling paradigm, illuminated by intuitive examples from computer vision and health informatics. We also introduce the relation-defined representation learning methodology as a practical implementation of Relation-Oriented modeling, supported by extensive experimental validation.
&lt;/p&gt;</description></item><item><title>XAI-TRIS&#25552;&#20379;&#20102;&#29992;&#20110;&#27979;&#35797;&#26426;&#22120;&#23398;&#20064;&#35299;&#37322;&#24615;&#33021;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38750;&#32447;&#24615;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25581;&#31034;&#20102;&#29616;&#26377;XAI&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12816</link><description>&lt;p&gt;
XAI-TRIS&#65306;&#29992;&#20110;&#37327;&#21270;&#26426;&#22120;&#23398;&#20064;&#35299;&#37322;&#24615;&#33021;&#30340;&#38750;&#32447;&#24615;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
XAI-TRIS: Non-linear benchmarks to quantify ML explanation performance. (arXiv:2306.12816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12816
&lt;/p&gt;
&lt;p&gt;
XAI-TRIS&#25552;&#20379;&#20102;&#29992;&#20110;&#27979;&#35797;&#26426;&#22120;&#23398;&#20064;&#35299;&#37322;&#24615;&#33021;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38750;&#32447;&#24615;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25581;&#31034;&#20102;&#29616;&#26377;XAI&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#21487;&#35299;&#37322;&#30340;&#8221;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#24050;&#32463;&#20135;&#29983;&#20102;&#39640;&#24230;&#24341;&#29992;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20351;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#30340;&#20915;&#31574;&#8220;&#21487;&#29702;&#35299;&#8221;&#32473;&#20154;&#31867;&#65292;&#20363;&#22914;&#36890;&#36807;&#23545;&#36755;&#20837;&#29305;&#24449;&#36827;&#34892;&#8220;&#37325;&#35201;&#24615;&#8221;&#35780;&#20998;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#27491;&#24335;&#30340;&#22522;&#30784;&#65292;&#20351;&#24471;&#26080;&#27861;&#20174;&#32473;&#23450;XAI&#26041;&#27861;&#30340;&#32467;&#26524;&#20013;&#23433;&#20840;&#22320;&#24471;&#20986;&#32467;&#35770;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#20063;&#38459;&#30861;&#20102;XAI&#26041;&#27861;&#30340;&#29702;&#35770;&#39564;&#35777;&#21644;&#23454;&#35777;&#39564;&#35777;&#12290;&#36825;&#24847;&#21619;&#30528;&#65292;&#30446;&#21069;&#32570;&#20047;&#36866;&#24403;&#30340;&#35299;&#20915;&#26041;&#27861;&#26469;&#35299;&#20915;&#36890;&#24120;&#30001;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38750;&#32447;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#38024;&#23545;&#19977;&#31181;&#19981;&#21516;&#30340;&#38750;&#32447;&#24615;&#20998;&#31867;&#24773;&#26223;&#21046;&#20316;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#36890;&#36807;&#35774;&#35745;&#24050;&#30693;&#37325;&#35201;&#30340;&#31867;&#26465;&#20214;&#29305;&#24449;&#65292;&#20316;&#20026;&#22320;&#38754;&#23454;&#20917;&#35299;&#37322;&#12290;&#21033;&#29992;&#26032;&#30340;&#23450;&#37327;&#25351;&#26631;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26550;&#26500;&#19978;&#27979;&#35797;&#20102;&#24191;&#27867;&#30340;XAI&#26041;&#27861;&#30340;&#35299;&#37322;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27969;&#34892;&#30340;XAI&#26041;&#27861;&#30340;&#24456;&#22810;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of 'explainable' artificial intelligence (XAI) has produced highly cited methods that seek to make the decisions of complex machine learning (ML) methods 'understandable' to humans, for example by attributing 'importance' scores to input features. Yet, a lack of formal underpinning leaves it unclear as to what conclusions can safely be drawn from the results of a given XAI method and has also so far hindered the theoretical verification and empirical validation of XAI methods. This means that challenging non-linear problems, typically solved by deep neural networks, presently lack appropriate remedies. Here, we craft benchmark datasets for three different non-linear classification scenarios, in which the important class-conditional features are known by design, serving as ground truth explanations. Using novel quantitative metrics, we benchmark the explanation performance of a wide set of XAI methods across three deep learning model architectures. We show that popular XAI met
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25511;&#21046;&#20449;&#21495;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#35270;&#39057;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#31354;&#38388;-&#26102;&#38388;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#27531;&#24046;&#22122;&#22768;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#36830;&#36143;&#30340;&#36229;&#39640;&#36136;&#37327;&#35270;&#39057;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#36164;&#28304;&#39640;&#25928;&#30340;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2305.13840</link><description>&lt;p&gt;
Control-A-Video: &#25511;&#21046;&#24615;&#25991;&#26412;&#29983;&#25104;&#35270;&#39057;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Control-A-Video: Controllable Text-to-Video Generation with Diffusion Models. (arXiv:2305.13840v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13840
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25511;&#21046;&#20449;&#21495;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#35270;&#39057;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#31354;&#38388;-&#26102;&#38388;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#27531;&#24046;&#22122;&#22768;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#36830;&#36143;&#30340;&#36229;&#39640;&#36136;&#37327;&#35270;&#39057;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#36164;&#28304;&#39640;&#25928;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25511;&#21046;&#20449;&#21495;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#35270;&#39057;&#65288;T2V&#65289;&#25193;&#25955;&#27169;&#22411;&#65292;&#31216;&#20026;Video-ControlNet&#12290;&#35813;&#27169;&#22411;&#26159;&#22312;&#39044;&#35757;&#32451;&#30340;&#26377;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#65288;T2I&#65289;&#25193;&#25955;&#27169;&#22411;&#22522;&#30784;&#19978;&#26500;&#24314;&#30340;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#21487;&#35757;&#32451;&#30340;&#26102;&#38388;&#23618;&#65292;&#29992;&#20110;&#26377;&#25928;&#30340;&#36328;&#24103;&#24314;&#27169;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#31532;&#19968;&#24103;&#26465;&#20214;&#31574;&#30053;&#65292;&#20197;&#20419;&#36827;&#27169;&#22411;&#22312;&#33258;&#22238;&#24402;&#26041;&#24335;&#19979;&#29983;&#25104;&#36716;&#25442;&#33258;&#22270;&#20687;&#39046;&#22495;&#20197;&#21450;&#20219;&#24847;&#38271;&#24230;&#35270;&#39057;&#12290;&#27492;&#22806;&#65292;Video-ControlNet&#37319;&#29992;&#19968;&#31181;&#22522;&#20110;&#27531;&#24046;&#30340;&#22122;&#22768;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#20174;&#36755;&#20837;&#35270;&#39057;&#20013;&#24341;&#20837;&#36816;&#21160;&#20808;&#39564;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#36830;&#36143;&#30340;&#35270;&#39057;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#26550;&#26500;&#21644;&#31574;&#30053;&#65292;Video-ControlNet&#21487;&#20197;&#23454;&#29616;&#36164;&#28304;&#39640;&#25928;&#30340;&#25910;&#25947;&#65292;&#29983;&#25104;&#20855;&#26377;&#32454;&#31890;&#24230;&#25511;&#21046;&#30340;&#20248;&#36136;&#19968;&#33268;&#35270;&#39057;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#30340;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a controllable text-to-video (T2V) diffusion model, named Video-ControlNet, that generates videos conditioned on a sequence of control signals, such as edge or depth maps. Video-ControlNet is built on a pre-trained conditional text-to-image (T2I) diffusion model by incorporating a spatial-temporal self-attention mechanism and trainable temporal layers for efficient cross-frame modeling. A first-frame conditioning strategy is proposed to facilitate the model to generate videos transferred from the image domain as well as arbitrary-length videos in an auto-regressive manner. Moreover, Video-ControlNet employs a novel residual-based noise initialization strategy to introduce motion prior from an input video, producing more coherent videos. With the proposed architecture and strategies, Video-ControlNet can achieve resource-efficient convergence and generate superior quality and consistent videos with fine-grained control. Extensive experiments demonstrate its success i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;AMPLIFY&#65292;&#21033;&#29992;&#21518;&#39564;&#35299;&#37322;&#33258;&#21160;&#21270;&#29983;&#25104;&#21407;&#22240;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;&#29616;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11426</link><description>&lt;p&gt;
&#21518;&#39564;&#35299;&#37322;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Post Hoc Explanations of Language Models Can Improve Language Models. (arXiv:2305.11426v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;AMPLIFY&#65292;&#21033;&#29992;&#21518;&#39564;&#35299;&#37322;&#33258;&#21160;&#21270;&#29983;&#25104;&#21407;&#22240;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;&#29616;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#36807;&#31243;&#20013;&#21152;&#20837;&#20154;&#31867;&#27880;&#37322;&#30340;&#21407;&#29702;&#65288;&#20363;&#22914;&#65292;&#24605;&#32500;&#38142;&#25552;&#31034;&#65289;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#25512;&#29702;&#33021;&#21147;&#30340;&#20219;&#21153;&#19978;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#21407;&#29702;&#21152;&#20837;&#22312;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#38656;&#35201;&#39640;&#24230;&#30340;&#20154;&#24037;&#21442;&#19982;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#21363;&#36890;&#36807;&#21033;&#29992;&#21518;&#39564;&#35299;&#37322;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#25918;&#22823;&#27169;&#22411;&#24615;&#33021;&#65292;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#21518;&#39564;&#35299;&#37322;&#26041;&#27861;&#30340;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#36755;&#20986;&#31216;&#20026;&#23646;&#24615;&#20998;&#25968;&#65288;&#35299;&#37322;&#65289;&#30340;&#20540;&#65292;&#29992;&#20110;&#25429;&#33719;&#27599;&#20010;&#36755;&#20837;&#29305;&#24449;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#33258;&#21160;&#21270;&#30340;&#33258;&#28982;&#35821;&#35328;&#21407;&#29702;&#65292;&#20854;&#20013;&#21253;&#21547;&#20174;&#23646;&#24615;&#20998;&#25968;&#20013;&#33719;&#24471;&#30340;&#20449;&#24687;&#65292;&#20197;&#20415;&#29992;&#25143;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AMPLIFY&#21487;&#20197;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;&#29616;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable capabilities in performing complex tasks. Moreover, recent research has shown that incorporating human-annotated rationales (e.g., Chain-of- Thought prompting) during in-context learning can significantly enhance the performance of these models, particularly on tasks that require reasoning capabilities. However, incorporating such rationales poses challenges in terms of scalability as this requires a high degree of human involvement. In this work, we present a novel framework, Amplifying Model Performance by Leveraging In-Context Learning with Post Hoc Explanations (AMPLIFY), which addresses the aforementioned challenges by automating the process of rationale generation. To this end, we leverage post hoc explanation methods which output attribution scores (explanations) capturing the influence of each of the input features on model predictions. More specifically, we construct automated natural language rationales that embed insi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21160;&#24577;&#21019;&#24314;&#30340;&#23376;&#22270;&#34920;&#31034;&#35805;&#35821;&#21450;&#19978;&#19979;&#25991;&#30340;&#20449;&#24687;&#26469;&#36827;&#34892;&#20250;&#35805;&#35821;&#20041;&#35299;&#26512;&#65292;&#24182;&#21033;&#29992;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#65292;&#21487;&#34920;&#31034;&#22823;&#37327;&#30475;&#19981;&#35265;&#30340;&#33410;&#28857;&#65292;&#27604;&#38745;&#24577;&#26041;&#27861;&#26356;&#20026;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2305.06164</link><description>&lt;p&gt;
&#21160;&#24577;&#19978;&#19979;&#25991;&#22270;&#24418;&#23454;&#29616;&#23545;&#19975;&#29289;&#30693;&#35782;&#22270;&#35889;&#30340;&#20250;&#35805;&#35821;&#20041;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Conversational Semantic Parsing using Dynamic Context Graphs. (arXiv:2305.06164v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21160;&#24577;&#21019;&#24314;&#30340;&#23376;&#22270;&#34920;&#31034;&#35805;&#35821;&#21450;&#19978;&#19979;&#25991;&#30340;&#20449;&#24687;&#26469;&#36827;&#34892;&#20250;&#35805;&#35821;&#20041;&#35299;&#26512;&#65292;&#24182;&#21033;&#29992;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#65292;&#21487;&#34920;&#31034;&#22823;&#37327;&#30475;&#19981;&#35265;&#30340;&#33410;&#28857;&#65292;&#27604;&#38745;&#24577;&#26041;&#27861;&#26356;&#20026;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#25317;&#26377;&#25968;&#30334;&#19975;&#20010;&#23454;&#20307;&#21644;&#25968;&#21315;&#31181;&#20851;&#31995;&#31867;&#22411;&#30340;&#36890;&#29992;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#20250;&#35805;&#35821;&#20041;&#35299;&#26512;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#33268;&#21147;&#20110;&#24320;&#21457;&#33021;&#22815;&#20132;&#20114;&#22320;&#23558;&#29992;&#25143;&#35821;&#35328;&#26144;&#23556;&#20026;&#21487;&#25191;&#34892;&#36923;&#36753;&#24418;&#24335;&#65288;&#20363;&#22914;SPARQL&#65289;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#23545;&#35805;&#21382;&#21490;&#30340;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24819;&#27861;&#26159;&#36890;&#36807;&#19968;&#20010;&#21160;&#24577;&#21019;&#24314;&#30340;&#23376;&#22270;&#26469;&#34920;&#31034;&#26377;&#20851;&#35805;&#35821;&#21450;&#20854;&#19978;&#19979;&#25991;&#30340;&#20449;&#24687;&#65292;&#21363;&#27599;&#20010;&#35805;&#35821;&#30340;&#33410;&#28857;&#25968;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#32780;&#19988;&#65292;&#25105;&#20204;&#21033;&#29992;&#23376;&#22270;&#30340;&#22522;&#26412;&#32467;&#26500;&#65292;&#32780;&#19981;&#26159;&#23558;&#20854;&#35270;&#20026;&#24207;&#21015;&#65292;&#20351;&#29992;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#32534;&#30721;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#20801;&#35768;&#25105;&#20204;&#34920;&#31034;&#22823;&#37327;&#65288;&#30475;&#19981;&#35265;&#30340;&#65289;&#33410;&#28857;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21160;&#24577;&#24314;&#27169;&#19978;&#19979;&#25991;&#20248;&#20110;&#38745;&#24577;&#26041;&#27861;&#65292;&#21487;&#22312;&#21508;&#20010;&#26041;&#38754;&#65288;&#21363;&#31616;&#21333;&#21644;&#22797;&#26434;&#38382;&#39064;&#65289;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36827;&#19968;&#27493;&#35777;&#23454;&#65292;&#27169;&#22411;&#21270;&#19978;&#19979;&#25991;&#32467;&#26500;&#27604;&#20165;&#32771;&#34385;&#21333;&#20010;&#35805;&#35821;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we consider the task of conversational semantic parsing over general purpose knowledge graphs (KGs) with millions of entities, and thousands of relation-types. We are interested in developing models capable of interactively mapping user utterances into executable logical forms (e.g., SPARQL) in the context of the conversational history. Our key idea is to represent information about an utterance and its context via a subgraph which is created dynamically, i.e., the number of nodes varies per utterance. Moreover, rather than treating the subgraph as a sequence we exploit its underlying structure, and thus encode it using a graph neural network which further allows us to represent a large number of (unseen) nodes. Experimental results show that modeling context dynamically is superior to static approaches, delivering performance improvements across the board (i.e., for simple and complex questions). Our results further confirm that modeling the structure of context is bette
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#21387;&#32553;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#28789;&#27963;&#22320;&#25429;&#33719;&#33258;&#28982;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#20998;&#24067;&#65292;&#26377;&#25928;&#22320;&#29983;&#25104;&#20855;&#26377;&#39640;&#35774;&#35745;&#33021;&#21147;&#21644;&#25928;&#29575;&#30340;&#26032;&#22411;&#34507;&#30333;&#36136;&#39592;&#26550;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.04120</link><description>&lt;p&gt;
&#19968;&#31181;&#34507;&#30333;&#36136;&#32467;&#26500;&#29983;&#25104;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Latent Diffusion Model for Protein Structure Generation. (arXiv:2305.04120v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04120
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#21387;&#32553;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#28789;&#27963;&#22320;&#25429;&#33719;&#33258;&#28982;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#20998;&#24067;&#65292;&#26377;&#25928;&#22320;&#29983;&#25104;&#20855;&#26377;&#39640;&#35774;&#35745;&#33021;&#21147;&#21644;&#25928;&#29575;&#30340;&#26032;&#22411;&#34507;&#30333;&#36136;&#39592;&#26550;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#26159;&#22797;&#26434;&#30340;&#29983;&#29289;&#20998;&#23376;&#65292;&#33021;&#22312;&#29983;&#29289;&#20307;&#20869;&#25191;&#34892;&#22810;&#31181;&#20851;&#38190;&#21151;&#33021;&#12290;&#35774;&#35745;&#21644;&#29983;&#25104;&#26032;&#22411;&#34507;&#30333;&#36136;&#21487;&#20026;&#26410;&#26469;&#30340;&#21512;&#25104;&#29983;&#29289;&#23398;&#24212;&#29992;&#65288;&#21253;&#25324;&#33647;&#29289;&#21457;&#29616;&#65289;&#38138;&#24179;&#36947;&#36335;&#12290;&#20294;&#30001;&#20110;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#22823;&#35268;&#27169;&#24314;&#27169;&#31354;&#38388;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35745;&#31639;&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20943;&#23569;&#34507;&#30333;&#36136;&#24314;&#27169;&#30340;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#28789;&#27963;&#22320;&#22312;&#21387;&#32553;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#25429;&#33719;&#33258;&#28982;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#20998;&#24067;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31561;&#21464;&#34507;&#30333;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#34507;&#30333;&#36136;&#23884;&#20837;&#21040;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#28982;&#21518;&#20351;&#29992;&#31561;&#21464;&#25193;&#25955;&#27169;&#22411;&#26469;&#23398;&#20064;&#28508;&#22312;&#34507;&#30333;&#36136;&#34920;&#31034;&#30340;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#29983;&#25104;&#20855;&#26377;&#39640;&#35774;&#35745;&#33021;&#21147;&#21644;&#25928;&#29575;&#30340;&#26032;&#22411;&#34507;&#30333;&#36136;&#39592;&#26550;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proteins are complex biomolecules that perform a variety of crucial functions within living organisms. Designing and generating novel proteins can pave the way for many future synthetic biology applications, including drug discovery. However, it remains a challenging computational task due to the large modeling space of protein structures. In this study, we propose a latent diffusion model that can reduce the complexity of protein modeling while flexibly capturing the distribution of natural protein structures in a condensed latent space. Specifically, we propose an equivariant protein autoencoder that embeds proteins into a latent space and then uses an equivariant diffusion model to learn the distribution of the latent protein representations. Experimental results demonstrate that our method can effectively generate novel protein backbone structures with high designability and efficiency.
&lt;/p&gt;</description></item><item><title>LAVA&#26159;&#19968;&#20010;&#23398;&#20064;&#31639;&#27861;&#26080;&#20851;&#30340;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#23398;&#20064;&#31639;&#27861;&#30340;&#32479;&#35745;&#29305;&#24615;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#23646;&#24615;&#65292;&#36890;&#36807;&#36845;&#20195;&#20272;&#35745;&#25968;&#25454;&#20540;&#26469;&#23454;&#29616;&#12290;LAVA&#27604;&#29616;&#26377;&#26041;&#27861;&#35745;&#31639;&#36895;&#24230;&#26356;&#24555;&#65292;&#31934;&#24230;&#26356;&#39640;&#65292;&#24182;&#19988;&#21487;&#20197;&#20026;&#19981;&#21516;&#30340;&#24212;&#29992;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2305.00054</link><description>&lt;p&gt;
LAVA: &#26080;&#38656;&#39044;&#23450;&#23398;&#20064;&#31639;&#27861;&#30340;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
LAVA: Data Valuation without Pre-Specified Learning Algorithms. (arXiv:2305.00054v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00054
&lt;/p&gt;
&lt;p&gt;
LAVA&#26159;&#19968;&#20010;&#23398;&#20064;&#31639;&#27861;&#26080;&#20851;&#30340;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#23398;&#20064;&#31639;&#27861;&#30340;&#32479;&#35745;&#29305;&#24615;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#23646;&#24615;&#65292;&#36890;&#36807;&#36845;&#20195;&#20272;&#35745;&#25968;&#25454;&#20540;&#26469;&#23454;&#29616;&#12290;LAVA&#27604;&#29616;&#26377;&#26041;&#27861;&#35745;&#31639;&#36895;&#24230;&#26356;&#24555;&#65292;&#31934;&#24230;&#26356;&#39640;&#65292;&#24182;&#19988;&#21487;&#20197;&#20026;&#19981;&#21516;&#30340;&#24212;&#29992;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#38382;&#39064;&#26159;&#22914;&#20309;&#20844;&#24179;&#22320;&#20998;&#37197;&#23398;&#20064;&#31639;&#27861;&#30340;&#39564;&#35777;&#24615;&#33021;&#65292;&#33268;&#20351;&#35745;&#31639;&#24471;&#21040;&#30340;&#25968;&#25454;&#20215;&#20540;&#20381;&#36182;&#20110;&#24213;&#23618;&#23398;&#20064;&#31639;&#27861;&#30340;&#35768;&#22810;&#35774;&#35745;&#36873;&#25321;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;LAVA&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#23398;&#20064;&#31639;&#27861;&#30340;&#32479;&#35745;&#29305;&#24615;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#23646;&#24615;&#65292;&#36845;&#20195;&#20272;&#35745;&#25968;&#25454;&#20540;&#65292;&#20351;&#20854;&#26080;&#35270;&#19979;&#28216;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LAVA&#27604;&#29616;&#26377;&#26041;&#27861;&#35745;&#31639;&#36895;&#24230;&#26356;&#24555;&#65292;&#31934;&#24230;&#26356;&#39640;&#65292;&#24182;&#19988;&#23427;&#21487;&#20197;&#20026;&#19981;&#21516;&#30340;&#24212;&#29992;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditionally, data valuation is posed as a problem of equitably splitting the validation performance of a learning algorithm among the training data. As a result, the calculated data values depend on many design choices of the underlying learning algorithm. However, this dependence is undesirable for many use cases of data valuation, such as setting priorities over different data sources in a data acquisition process and informing pricing mechanisms in a data marketplace. In these scenarios, data needs to be valued before the actual analysis and the choice of the learning algorithm is still undetermined then. Another side-effect of the dependence is that to assess the value of individual points, one needs to re-run the learning algorithm with and without a point, which incurs a large computation burden.  This work leapfrogs over the current limits of data valuation methods by introducing a new framework that can value training data in a way that is oblivious to the downstream learning
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;FIANCEE&#65292;&#36890;&#36807;&#28155;&#21152;&#26089;&#26399;&#36864;&#20986;&#20998;&#25903;&#24182;&#26681;&#25454;&#36755;&#20986;&#38590;&#24230;&#21160;&#24577;&#20999;&#25442;&#35745;&#31639;&#36335;&#24452;&#30340;&#26041;&#27861;&#26469;&#21152;&#36895;&#23545;&#25239;&#24615;&#32593;&#32476;&#25512;&#26029;&#65292;&#32463;&#36807;&#39564;&#35777;&#35813;&#26041;&#27861;&#23545;&#20110;&#29983;&#25104;&#20219;&#21153;&#21487;&#20197;&#22312;&#20445;&#25345;&#39640;&#36136;&#37327;&#30340;&#21516;&#26102;&#20943;&#23569;&#35745;&#31639;&#37327;&#65292;&#29305;&#21035;&#36866;&#21512;&#23454;&#26102;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.10306</link><description>&lt;p&gt;
FIANCEE: &#36890;&#36807;&#26465;&#20214;&#26089;&#26399;&#36864;&#20986;&#21152;&#36895;&#23545;&#25239;&#24615;&#32593;&#32476;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
FIANCEE: Faster Inference of Adversarial Networks via Conditional Early Exits. (arXiv:2304.10306v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;FIANCEE&#65292;&#36890;&#36807;&#28155;&#21152;&#26089;&#26399;&#36864;&#20986;&#20998;&#25903;&#24182;&#26681;&#25454;&#36755;&#20986;&#38590;&#24230;&#21160;&#24577;&#20999;&#25442;&#35745;&#31639;&#36335;&#24452;&#30340;&#26041;&#27861;&#26469;&#21152;&#36895;&#23545;&#25239;&#24615;&#32593;&#32476;&#25512;&#26029;&#65292;&#32463;&#36807;&#39564;&#35777;&#35813;&#26041;&#27861;&#23545;&#20110;&#29983;&#25104;&#20219;&#21153;&#21487;&#20197;&#22312;&#20445;&#25345;&#39640;&#36136;&#37327;&#30340;&#21516;&#26102;&#20943;&#23569;&#35745;&#31639;&#37327;&#65292;&#29305;&#21035;&#36866;&#21512;&#23454;&#26102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#22270;&#20687;&#21512;&#25104;&#30340;&#26377;&#21147;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#21463;&#21040;&#35745;&#31639;&#36127;&#36733;&#30340;&#38480;&#21046;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#38024;&#23545;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#21644;&#20219;&#21153;&#65288;&#20363;&#22914;&#22312;&#29305;&#23450;&#29305;&#24449;&#33539;&#22260;&#20869;&#29983;&#25104;&#20154;&#33080;&#65289;&#65292;&#19981;&#21516;&#29305;&#24449;&#19979;&#30340;&#22270;&#20687;&#36136;&#37327;&#20250;&#23384;&#22312;&#24046;&#24322;&#65292;&#22240;&#27492;&#21487;&#20197;&#22312;&#26576;&#20123;&#23454;&#20363;&#19978;&#38480;&#21046;&#27169;&#22411;&#30340;&#22797;&#26434;&#24230;&#65292;&#20445;&#25345;&#39640;&#36136;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21407;&#22987;&#26550;&#26500;&#20013;&#28155;&#21152;&#25152;&#35859;&#30340;&#26089;&#26399;&#36864;&#20986;&#20998;&#25903;&#65292;&#24182;&#26681;&#25454;&#36755;&#20986;&#38590;&#24230;&#21160;&#24577;&#20999;&#25442;&#35745;&#31639;&#36335;&#24452;&#65292;&#26469;&#20943;&#23569;&#35745;&#31639;&#37327;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#20004;&#20010;&#19981;&#21516;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#25191;&#34892;&#29983;&#25104;&#20219;&#21153;&#65306;&#20174;&#35821;&#20041;&#26144;&#23556;&#20013;&#29983;&#25104;&#21644;&#38754;&#37096;&#34920;&#24773;&#30340;&#20132;&#21449;&#37325;&#29616;&#65292;&#34920;&#26126;&#23427;&#33021;&#22815;&#36755;&#20986;&#20855;&#26377;&#33258;&#23450;&#20041;&#20302;&#36136;&#37327;&#38408;&#20540;&#30340;&#22270;&#20687;&#12290;&#23545;&#20110;LPIPS &lt;=0.1&#30340;&#38408;&#20540;&#65292;&#25105;&#20204;&#23558;&#35745;&#31639;&#37327;&#20943;&#23569;&#20102;&#19968;&#21322;&#12290;&#36825;&#23545;&#23454;&#26102;&#24212;&#29992;&#31243;&#24207;&#23588;&#20854;&#37325;&#35201;&#65292;&#20854;&#20013;&#24555;&#36895;&#25512;&#26029;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative DNNs are a powerful tool for image synthesis, but they are limited by their computational load. On the other hand, given a trained model and a task, e.g. faces generation within a range of characteristics, the output image quality will be unevenly distributed among images with different characteristics. It follows, that we might restrain the models complexity on some instances, maintaining a high quality. We propose a method for diminishing computations by adding so-called early exit branches to the original architecture, and dynamically switching the computational path depending on how difficult it will be to render the output. We apply our method on two different SOTA models performing generative tasks: generation from a semantic map, and cross-reenactment of face expressions; showing it is able to output images with custom lower-quality thresholds. For a threshold of LPIPS &lt;=0.1, we diminish their computations by up to a half. This is especially relevant for real-time app
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#21333;&#30524;&#27880;&#37322;&#35270;&#39057;&#20013;&#35757;&#32451;&#20986;&#31867;&#20284;&#28216;&#25103;&#24341;&#25806;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#36825;&#20010;&#27169;&#22411;&#34987;&#31216;&#20026;&#21487;&#23398;&#20064;&#28216;&#25103;&#24341;&#25806;(LGE)&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#25351;&#23450;&#39640;&#32423;&#21644;&#20302;&#32423;&#25805;&#20316;&#24207;&#21015;&#26469;&#29609;&#28216;&#25103;&#65292;&#24182;&#19988;&#35299;&#38145;&#20102;&#23548;&#28436;&#27169;&#24335;&#65292;&#21487;&#20197;&#20351;&#29992;&#39640;&#32423;&#32422;&#26463;&#26465;&#20214;&#25511;&#21046;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2303.13472</link><description>&lt;p&gt;
&#24149;&#21518;&#21046;&#20316;&#65306;&#38754;&#21521;&#21487;&#23398;&#20064;&#28216;&#25103;&#24341;&#25806;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Plotting Behind the Scenes: Towards Learnable Game Engines. (arXiv:2303.13472v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#21333;&#30524;&#27880;&#37322;&#35270;&#39057;&#20013;&#35757;&#32451;&#20986;&#31867;&#20284;&#28216;&#25103;&#24341;&#25806;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#36825;&#20010;&#27169;&#22411;&#34987;&#31216;&#20026;&#21487;&#23398;&#20064;&#28216;&#25103;&#24341;&#25806;(LGE)&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#25351;&#23450;&#39640;&#32423;&#21644;&#20302;&#32423;&#25805;&#20316;&#24207;&#21015;&#26469;&#29609;&#28216;&#25103;&#65292;&#24182;&#19988;&#35299;&#38145;&#20102;&#23548;&#28436;&#27169;&#24335;&#65292;&#21487;&#20197;&#20351;&#29992;&#39640;&#32423;&#32422;&#26463;&#26465;&#20214;&#25511;&#21046;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28216;&#25103;&#24341;&#25806;&#22312;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#20013;&#26159;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#20294;&#26159;&#23427;&#20204;&#30340;&#24320;&#21457;&#25104;&#26412;&#20063;&#26159;&#21313;&#20998;&#24040;&#22823;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#21333;&#30524;&#27880;&#37322;&#35270;&#39057;&#20013;&#35757;&#32451;&#20986;&#31867;&#20284;&#28216;&#25103;&#24341;&#25806;&#30340;&#31070;&#32463;&#27169;&#22411;&#12290;&#35813;&#32467;&#26524;&#34987;&#31216;&#20026;Learnable Game Engine (LGE)&#65292;&#21487;&#20197;&#32500;&#25252;&#22330;&#26223;&#12289;&#29289;&#20307;&#21644;&#20854;&#20013;&#30340;&#20195;&#29702;&#29366;&#24577;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#21487;&#25511;&#21046;&#30340;&#35270;&#35282;&#28210;&#26579;&#29615;&#22659;&#12290;&#31867;&#20284;&#20110;&#28216;&#25103;&#24341;&#25806;&#65292;&#23427;&#27169;&#25311;&#20102;&#28216;&#25103;&#30340;&#36923;&#36753;&#21644;&#24213;&#23618;&#29289;&#29702;&#35268;&#21017;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#25351;&#23450;&#39640;&#32423;&#21644;&#20302;&#32423;&#25805;&#20316;&#24207;&#21015;&#26469;&#29609;&#28216;&#25103;&#12290;&#26368;&#24341;&#20154;&#27880;&#30446;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;LGE&#35299;&#38145;&#20102;&#23548;&#28436;&#27169;&#24335;&#65292;&#29992;&#25143;&#36890;&#36807;&#26631;&#27880;&#39640;&#23618;&#27425;&#30340;&#21160;&#20316;&#21644;&#30446;&#26631;&#26469;&#25511;&#21046;&#20195;&#29702;&#12290;&#36825;&#35201;&#27714;&#23398;&#20064;&#8220;&#28216;&#25103;&#20154;&#24037;&#26234;&#33021;&#8221;&#65292;&#30001;&#25105;&#20204;&#30340;&#21160;&#30011;&#27169;&#22411;&#23553;&#35013;&#65292;&#20197;&#20351;&#29992;&#39640;&#32423;&#32422;&#26463;&#26465;&#20214;&#23548;&#33322;&#22330;&#26223;&#12289;&#19982;&#23545;&#25163;&#23545;&#25112;&#65292;&#35774;&#35745;&#36194;&#24471;&#28216;&#25103;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Game engines are powerful tools in computer graphics. Their power comes at the immense cost of their development. In this work, we present a framework to train game-engine-like neural models, solely from monocular annotated videos. The result-a Learnable Game Engine (LGE)-maintains states of the scene, objects and agents in it, and enables rendering the environment from a controllable viewpoint. Similarly to a game engine, it models the logic of the game and the underlying rules of physics, to make it possible for a user to play the game by specifying both high- and low-level action sequences. Most captivatingly, our LGE unlocks the director's mode, where the game is played by plotting behind the scenes, specifying high-level actions and goals for the agents in the form of language and desired states. This requires learning "game AI", encapsulated by our animation model, to navigate the scene using high-level constraints, play against an adversary, devise the strategy to win a point. T
&lt;/p&gt;</description></item><item><title>ZeroNLG&#26159;&#19968;&#20010;&#38646;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#22810;&#20010;NLG&#20219;&#21153;&#65292;&#21253;&#25324;&#22270;&#20687;&#21040;&#25991;&#26412;&#12289;&#35270;&#39057;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#65292;&#36328;&#36234;&#33521;&#35821;&#12289;&#20013;&#25991;&#12289;&#24503;&#35821;&#21644;&#27861;&#35821;&#12290;&#23427;&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#35760;&#30340;&#19979;&#28216;&#23545;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#30340;&#39046;&#22495;&#25237;&#24433;&#21040;&#20849;&#20139;&#30340;&#20844;&#20849;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#30456;&#24212;&#22352;&#26631;&#65292;&#26725;&#25509;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2303.06458</link><description>&lt;p&gt;
ZeroNLG: &#23558;&#39046;&#22495;&#23545;&#40784;&#21644;&#33258;&#32534;&#30721;&#29992;&#20110;&#38646;&#26679;&#26412;&#22810;&#27169;&#24577;&#21644;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
ZeroNLG: Aligning and Autoencoding Domains for Zero-Shot Multimodal and Multilingual Natural Language Generation. (arXiv:2303.06458v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06458
&lt;/p&gt;
&lt;p&gt;
ZeroNLG&#26159;&#19968;&#20010;&#38646;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#22810;&#20010;NLG&#20219;&#21153;&#65292;&#21253;&#25324;&#22270;&#20687;&#21040;&#25991;&#26412;&#12289;&#35270;&#39057;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#65292;&#36328;&#36234;&#33521;&#35821;&#12289;&#20013;&#25991;&#12289;&#24503;&#35821;&#21644;&#27861;&#35821;&#12290;&#23427;&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#35760;&#30340;&#19979;&#28216;&#23545;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#30340;&#39046;&#22495;&#25237;&#24433;&#21040;&#20849;&#20139;&#30340;&#20844;&#20849;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#30456;&#24212;&#22352;&#26631;&#65292;&#26725;&#25509;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
ZeroNLG is a zero-shot learning framework that can handle multiple NLG tasks, including image-to-text, video-to-text, and text-to-text, across English, Chinese, German, and French. It does not require any labeled downstream pairs for training, and bridges the differences between different domains by projecting them to corresponding coordinates in a shared common latent space.
&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#25509;&#21463;&#20197;&#22270;&#20687;&#12289;&#35270;&#39057;&#25110;&#25991;&#26412;&#24418;&#24335;&#30340;&#36755;&#20837;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#30456;&#24212;&#30340;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20316;&#20026;&#36755;&#20986;&#12290;&#29616;&#26377;&#30340;NLG&#26041;&#27861;&#20027;&#35201;&#37319;&#29992;&#30417;&#30563;&#26041;&#27861;&#65292;&#24182;&#19988;&#20005;&#37325;&#20381;&#36182;&#20110;&#32806;&#21512;&#30340;&#25968;&#25454;&#21040;&#25991;&#26412;&#23545;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#26377;&#38024;&#23545;&#24615;&#30340;&#22330;&#26223;&#21644;&#38750;&#33521;&#35821;&#35821;&#35328;&#65292;&#24448;&#24448;&#27809;&#26377;&#36275;&#22815;&#25968;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;&#20026;&#20102;&#25918;&#26494;&#23545;&#19979;&#28216;&#20219;&#21153;&#26631;&#35760;&#25968;&#25454;&#30340;&#20381;&#36182;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30452;&#35266;&#26377;&#25928;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;ZeroNLG&#65292;&#23427;&#21487;&#20197;&#22788;&#29702;&#22810;&#20010;NLG&#20219;&#21153;&#65292;&#21253;&#25324;&#22270;&#20687;&#21040;&#25991;&#26412;&#65288;&#22270;&#20687;&#23383;&#24149;&#65289;&#12289;&#35270;&#39057;&#21040;&#25991;&#26412;&#65288;&#35270;&#39057;&#23383;&#24149;&#65289;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#65288;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65289;&#65292;&#36328;&#36234;&#33521;&#35821;&#12289;&#20013;&#25991;&#12289;&#24503;&#35821;&#21644;&#27861;&#35821;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20869;&#12290;ZeroNLG&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#35760;&#30340;&#19979;&#28216;&#23545;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;ZeroNLG&#65288;i&#65289;&#23558;&#19981;&#21516;&#30340;&#39046;&#22495;&#65288;&#36328;&#27169;&#24577;&#21644;&#35821;&#35328;&#65289;&#25237;&#24433;&#21040;&#20849;&#20139;&#30340;&#20844;&#20849;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#30456;&#24212;&#22352;&#26631;&#65307;&#65288;ii&#65289;&#26725;&#25509;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Natural Language Generation (NLG) accepts input data in the form of images, videos, or text and generates corresponding natural language text as output. Existing NLG methods mainly adopt a supervised approach and rely heavily on coupled data-to-text pairs. However, for many targeted scenarios and for non-English languages, sufficient quantities of labeled data are often not available. To relax the dependency on labeled data of downstream tasks, we propose an intuitive and effective zero-shot learning framework, ZeroNLG, which can deal with multiple NLG tasks, including image-to-text (image captioning), video-to-text (video captioning), and text-to-text (neural machine translation), across English, Chinese, German, and French within a unified framework. ZeroNLG does not require any labeled downstream pairs for training. During training, ZeroNLG (i) projects different domains (across modalities and languages) to corresponding coordinates in a shared common latent space; (ii) bridges diff
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#33258;&#21160;&#36873;&#25321;&#21644;&#32452;&#21512;&#25628;&#32034;&#26041;&#27861;&#30340;&#26041;&#26696;&#65292;&#29992;&#20110;&#32463;&#27982;&#20195;&#29702;&#27169;&#22411;&#30340;&#21442;&#25968;&#26657;&#20934;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#26696;&#22312;&#25552;&#39640;&#25928;&#29575;&#30340;&#21516;&#26102;&#19981;&#38656;&#35201;&#19987;&#19994;&#39046;&#22495;&#30693;&#35782;&#25110;&#25163;&#21160;&#35843;&#25972;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2302.11835</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#32463;&#27982;&#20195;&#29702;&#27169;&#22411;&#21442;&#25968;&#26657;&#20934;&#30340;&#25628;&#32034;&#26041;&#27861;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning for Combining Search Methods in the Calibration of Economic ABMs. (arXiv:2302.11835v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#33258;&#21160;&#36873;&#25321;&#21644;&#32452;&#21512;&#25628;&#32034;&#26041;&#27861;&#30340;&#26041;&#26696;&#65292;&#29992;&#20110;&#32463;&#27982;&#20195;&#29702;&#27169;&#22411;&#30340;&#21442;&#25968;&#26657;&#20934;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#26696;&#22312;&#25552;&#39640;&#25928;&#29575;&#30340;&#21516;&#26102;&#19981;&#38656;&#35201;&#19987;&#19994;&#39046;&#22495;&#30693;&#35782;&#25110;&#25163;&#21160;&#35843;&#25972;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#27982;&#23398;&#21644;&#37329;&#34701;&#20013;&#30340;&#20195;&#29702;&#27169;&#22411;&#21442;&#25968;&#26657;&#20934;&#36890;&#24120;&#28041;&#21450;&#21040;&#23545;&#38750;&#24120;&#22823;&#30340;&#21442;&#25968;&#31354;&#38388;&#36827;&#34892;&#26080;&#23548;&#25968;&#25628;&#32034;&#12290;&#26412;&#25991;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#23545;&#20247;&#25152;&#21608;&#30693;&#30340;&#23439;&#35266;&#32463;&#27982;&#20195;&#29702;&#27169;&#22411;&#30340;&#33509;&#24178;&#25628;&#32034;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;&#36890;&#36807;&#32452;&#21512;&#19981;&#21516;&#26041;&#27861;&#25152;&#20570;&#20986;&#30340;&#8220;&#28151;&#21512;&#31574;&#30053;&#8221;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#38543;&#26426;&#26862;&#26519;&#26367;&#20195;&#27169;&#22411;&#30340;&#26041;&#27861;&#29305;&#21035;&#39640;&#25928;&#65292;&#24182;&#19988;&#32452;&#21512;&#25628;&#32034;&#26041;&#27861;&#36890;&#24120;&#20250;&#22686;&#21152;&#24615;&#33021;&#65292;&#22240;&#20026;&#20219;&#20309;&#21333;&#19968;&#26041;&#27861;&#30340;&#20559;&#24046;&#37117;&#20250;&#34987;&#32531;&#35299;&#12290;&#36890;&#36807;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#65292;&#22312;&#26657;&#20934;&#36816;&#34892;&#36807;&#31243;&#20013;&#33258;&#21160;&#36873;&#25321;&#21644;&#32452;&#21512;&#25628;&#32034;&#26041;&#27861;&#12290;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20154;&#21482;&#26377;&#22312;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#34920;&#29616;&#33391;&#22909;&#26102;&#25165;&#32487;&#32493;&#21033;&#29992;&#29305;&#23450;&#26041;&#27861;&#65292;&#20294;&#22312;&#35813;&#26041;&#27861;&#36798;&#21040;&#24615;&#33021;&#24179;&#21488;&#26102;&#25506;&#32034;&#26032;&#31574;&#30053;&#12290;&#24471;&#21040;&#30340;&#24378;&#21270;&#23398;&#20064;&#25628;&#32034;&#26041;&#26696;&#22312;&#20219;&#20309;&#20854;&#20182;&#27979;&#35797;&#30340;&#26041;&#27861;&#25110;&#26041;&#27861;&#32452;&#21512;&#19978;&#37117;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#19987;&#19994;&#30340;&#39046;&#22495;&#30693;&#35782;&#25110;&#25163;&#21160;&#21442;&#25968;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Calibrating agent-based models (ABMs) in economics and finance typically involves a derivative-free search in a very large parameter space. In this work, we benchmark a number of search methods in the calibration of a well-known macroeconomic ABM on real data, and further assess the performance of "mixed strategies" made by combining different methods. We find that methods based on random-forest surrogates are particularly efficient, and that combining search methods generally increases performance since the biases of any single method are mitigated. Moving from these observations, we propose a reinforcement learning (RL) scheme to automatically select and combine search methods on-the-fly during a calibration run. The RL agent keeps exploiting a specific method only as long as this keeps performing well, but explores new strategies when the specific method reaches a performance plateau. The resulting RL search scheme outperforms any other method or method combination tested, and does 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#27745;&#26579;&#30340;&#26102;&#24207;&#23041;&#32961;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#30340;&#26102;&#38388;&#25139;&#65292;&#24341;&#20837;&#20102;&#25552;&#21069;&#26102;&#38388;&#21644;&#25345;&#32493;&#26102;&#38388;&#36825;&#20004;&#20010;&#25351;&#26631;&#65292;&#20174;&#32780;&#23450;&#20041;&#20102;&#25968;&#25454;&#27745;&#26579;&#30340;&#26102;&#24207;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20445;&#25252;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.03684</link><description>&lt;p&gt;
&#25968;&#25454;&#27745;&#26579;&#20013;&#30340;&#26102;&#24207;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Temporal Robustness against Data Poisoning. (arXiv:2302.03684v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03684
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#27745;&#26579;&#30340;&#26102;&#24207;&#23041;&#32961;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#30340;&#26102;&#38388;&#25139;&#65292;&#24341;&#20837;&#20102;&#25552;&#21069;&#26102;&#38388;&#21644;&#25345;&#32493;&#26102;&#38388;&#36825;&#20004;&#20010;&#25351;&#26631;&#65292;&#20174;&#32780;&#23450;&#20041;&#20102;&#25968;&#25454;&#27745;&#26579;&#30340;&#26102;&#24207;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20445;&#25252;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#27745;&#26579;&#32771;&#34385;&#20102;&#36890;&#36807;&#24694;&#24847;&#35757;&#32451;&#25968;&#25454;&#25805;&#32437;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34892;&#20026;&#30340;&#24773;&#20917;&#12290;&#29616;&#26377;&#30340;&#25968;&#25454;&#27745;&#26579;&#23041;&#32961;&#27169;&#22411;&#37117;&#22260;&#32469;&#30528;&#19968;&#20010;&#21333;&#19968;&#25351;&#26631;&#65292;&#21363;&#34987;&#27745;&#26579;&#26679;&#26412;&#30340;&#25968;&#37327;&#12290;&#22240;&#27492;&#65292;&#22914;&#26524;&#25915;&#20987;&#32773;&#33021;&#22815;&#20197;&#21487;&#25215;&#21463;&#30340;&#20195;&#20215;&#27745;&#26579;&#27604;&#39044;&#26399;&#26356;&#22810;&#30340;&#26679;&#26412;&#65292;&#23601;&#20687;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#19968;&#26679;&#65292;&#20182;&#20204;&#21487;&#33021;&#33021;&#22815;&#22312;&#24456;&#30701;&#30340;&#26102;&#38388;&#20869;&#20351;&#29616;&#26377;&#30340;&#38450;&#24481;&#25514;&#26045;&#22833;&#25928;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#25968;&#25454;&#30340;&#20986;&#29983;&#26085;&#26399;&#26102;&#38388;&#25139;&#65292;&#36825;&#20123;&#26102;&#38388;&#25139;&#36890;&#24120;&#26159;&#21487;&#29992;&#30340;&#20294;&#36807;&#21435;&#34987;&#24573;&#30053;&#12290;&#21033;&#29992;&#36825;&#20123;&#26102;&#38388;&#25139;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#20004;&#20010;&#26032;&#22411;&#25351;&#26631;&#65288;&#25552;&#21069;&#26102;&#38388;&#21644;&#25345;&#32493;&#26102;&#38388;&#65289;&#30340;&#25968;&#25454;&#27745;&#26579;&#30340;&#26102;&#24207;&#23041;&#32961;&#27169;&#22411;&#65292;&#20998;&#21035;&#34913;&#37327;&#25915;&#20987;&#25552;&#21069;&#24320;&#22987;&#30340;&#26102;&#38388;&#21644;&#25915;&#20987;&#25345;&#32493;&#30340;&#26102;&#38388;&#12290;&#21033;&#29992;&#36825;&#20123;&#25351;&#26631;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#25968;&#25454;&#27745;&#26579;&#30340;&#26102;&#24207;&#40065;&#26834;&#24615;&#30340;&#27010;&#24565;&#65292;&#21363;&#20351;&#26377;&#22823;&#37327;&#34987;&#27745;&#26579;&#30340;&#26679;&#26412;&#65292;&#20063;&#33021;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#20445;&#25252;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data poisoning considers cases when an adversary manipulates the behavior of machine learning algorithms through malicious training data. Existing threat models of data poisoning center around a single metric, the number of poisoned samples. In consequence, if attackers can poison more samples than expected with affordable overhead, as in many practical scenarios, they may be able to render existing defenses ineffective in a short time. To address this issue, we leverage timestamps denoting the birth dates of data, which are often available but neglected in the past. Benefiting from these timestamps, we propose a temporal threat model of data poisoning with two novel metrics, earliness and duration, which respectively measure how long an attack started in advance and how long an attack lasted. Using these metrics, we define the notions of temporal robustness against data poisoning, providing a meaningful sense of protection even with unbounded amounts of poisoned samples. We present a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#20113;&#30340;&#27627;&#31859;&#27874;&#36890;&#20449;&#20027;&#21160;&#38142;&#36335;&#36136;&#37327;&#39044;&#27979;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#20854;&#36866;&#29992;&#24615;&#26356;&#24191;&#19988;&#19981;&#28041;&#21450;&#25935;&#24863;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2301.00752</link><description>&lt;p&gt;
&#22522;&#20110;&#28857;&#20113;&#30340;&#27627;&#31859;&#27874;&#36890;&#20449;&#20027;&#21160;&#38142;&#36335;&#36136;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Point Cloud-based Proactive Link Quality Prediction for Millimeter-wave Communications. (arXiv:2301.00752v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#20113;&#30340;&#27627;&#31859;&#27874;&#36890;&#20449;&#20027;&#21160;&#38142;&#36335;&#36136;&#37327;&#39044;&#27979;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#20854;&#36866;&#29992;&#24615;&#26356;&#24191;&#19988;&#19981;&#28041;&#21450;&#25935;&#24863;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;&#28857;&#20113;&#30340;&#27627;&#31859;&#27874;&#65288;mmWave&#65289;&#36890;&#20449;&#30340;&#20027;&#21160;&#38142;&#36335;&#36136;&#37327;&#39044;&#27979;&#30340;&#21487;&#34892;&#24615;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#22270;&#20687;&#30340;&#26102;&#38388;&#24207;&#21015;&#26469;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#27573;&#30340;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#65292;&#20197;&#32531;&#35299;&#34892;&#20154;&#38459;&#25377;&#22240;&#32032;&#23545;mmWave&#36890;&#20449;&#30340;&#24433;&#21709;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#65292;&#36825;&#20123;&#22522;&#20110;&#22270;&#20687;&#30340;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#26377;&#38480;&#65292;&#22240;&#20026;&#25668;&#20687;&#22836;&#22270;&#20687;&#21487;&#33021;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#20113;&#30340;mmWave&#38142;&#36335;&#36136;&#37327;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#21487;&#34892;&#24615;&#12290;&#28857;&#20113;&#23558;&#19977;&#32500;&#31354;&#38388;&#34920;&#31034;&#20026;&#28857;&#38598;&#65292;&#20854;&#31354;&#38388;&#24615;&#36136;&#26356;&#21152;&#31232;&#30095;&#65292;&#19981;&#22826;&#21487;&#33021;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#65292;&#24182;&#19988;&#36824;&#25552;&#20379;&#20102;3D&#20301;&#32622;&#21644;&#36816;&#21160;&#20449;&#24687;&#65292;&#36825;&#23545;&#20102;&#35299;&#28041;&#21450;&#34892;&#20154;&#30340;&#26080;&#32447;&#30005;&#20256;&#25773;&#29615;&#22659;&#26159;&#24517;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study demonstrates the feasibility of point cloud-based proactive link quality prediction for millimeter-wave (mmWave) communications. Previous studies have proposed machine learning-based methods to predict received signal strength for future time periods using time series of depth images to mitigate the line-of-sight (LOS) path blockage by pedestrians in mmWave communication. However, these image-based methods have limited applicability due to privacy concerns as camera images may contain sensitive information. This study proposes a point cloud-based method for mmWave link quality prediction and demonstrates its feasibility through experiments. Point clouds represent three-dimensional (3D) spaces as a set of points and are sparser and less likely to contain sensitive information than camera images. Additionally, point clouds provide 3D position and motion information, which is necessary for understanding the radio propagation environment involving pedestrians. This study designs
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36827;&#34892;&#20844;&#24179;&#24615;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#25105;&#20204;&#21457;&#29616;&#20559;&#35265;&#26159;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26412;&#36523;&#22266;&#26377;&#30340;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#35757;&#32451;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20154;&#33080;&#35782;&#21035;&#31561;&#38590;&#39064;&#19978;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.09943</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#20559;&#35265;&#32531;&#35299;&#65306;&#26356;&#20844;&#24179;&#30340;&#26550;&#26500;&#23454;&#29616;&#26356;&#20844;&#24179;&#30340;&#20154;&#33080;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Rethinking Bias Mitigation: Fairer Architectures Make for Fairer Face Recognition. (arXiv:2210.09943v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09943
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36827;&#34892;&#20844;&#24179;&#24615;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#25105;&#20204;&#21457;&#29616;&#20559;&#35265;&#26159;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26412;&#36523;&#22266;&#26377;&#30340;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#35757;&#32451;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20154;&#33080;&#35782;&#21035;&#31561;&#38590;&#39064;&#19978;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#24191;&#27867;&#24212;&#29992;&#20110;&#25191;&#27861;&#31561;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#65292;&#20294;&#23427;&#20204;&#22312;&#24615;&#21035;&#21644;&#31181;&#26063;&#31561;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#32500;&#24230;&#19978;&#23384;&#22312;&#20559;&#35265;&#12290;&#20256;&#32479;&#35266;&#28857;&#35748;&#20026;&#65292;&#27169;&#22411;&#20559;&#35265;&#28304;&#20110;&#26377;&#20559;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#20197;&#24448;&#20851;&#20110;&#20559;&#35265;&#32531;&#35299;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#39044;&#22788;&#29702;&#35757;&#32451;&#25968;&#25454;&#12289;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28155;&#21152;&#24809;&#32602;&#39033;&#20197;&#38450;&#27490;&#20559;&#35265;&#24433;&#21709;&#27169;&#22411;&#65292;&#25110;&#23545;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#21518;&#22788;&#29702;&#20197;&#28040;&#38500;&#20559;&#35265;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#20154;&#33080;&#35782;&#21035;&#31561;&#38590;&#39064;&#19978;&#30340;&#25104;&#21151;&#26377;&#38480;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20559;&#35265;&#23454;&#38469;&#19978;&#26681;&#28304;&#20110;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26412;&#36523;&#12290;&#22522;&#20110;&#36825;&#19968;&#37325;&#26032;&#23450;&#20041;&#65292;&#25105;&#20204;&#39318;&#27425;&#36827;&#34892;&#20102;&#20844;&#24179;&#24615;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#21516;&#26102;&#36827;&#34892;&#20102;&#36229;&#21442;&#25968;&#25628;&#32034;&#12290;&#25105;&#20204;&#30340;&#25628;&#32034;&#36755;&#20986;&#20102;&#19968;&#31995;&#21015;&#22312;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#20854;&#20182;&#39640;&#24615;&#33021;&#26550;&#26500;&#21644;&#29616;&#26377;&#20559;&#35265;&#32531;&#35299;&#26041;&#27861;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face recognition systems are widely deployed in safety-critical applications, including law enforcement, yet they exhibit bias across a range of socio-demographic dimensions, such as gender and race. Conventional wisdom dictates that model biases arise from biased training data. As a consequence, previous works on bias mitigation largely focused on pre-processing the training data, adding penalties to prevent bias from effecting the model during training, or post-processing predictions to debias them, yet these approaches have shown limited success on hard problems such as face recognition. In our work, we discover that biases are actually inherent to neural network architectures themselves. Following this reframing, we conduct the first neural architecture search for fairness, jointly with a search for hyperparameters. Our search outputs a suite of models which Pareto-dominate all other high-performance architectures and existing bias mitigation methods in terms of accuracy and fairne
&lt;/p&gt;</description></item></channel></rss>