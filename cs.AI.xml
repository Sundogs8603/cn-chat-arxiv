<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#25163;&#37096;&#26631;&#27880;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#19977;&#20010;&#39069;&#22806;&#30340;&#36890;&#36947;&#26469;&#25913;&#36827;&#29983;&#25104;&#25163;&#37096;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#29983;&#25104;&#27169;&#22411;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#25163;&#37096;&#26816;&#27979;&#22120;&#26469;&#34913;&#37327;&#29983;&#25104;&#25163;&#37096;&#22270;&#20687;&#36136;&#37327;&#30340;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2401.15075</link><description>&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#25163;&#37096;&#26631;&#27880;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Annotated Hands for Generative Models. (arXiv:2401.15075v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#25163;&#37096;&#26631;&#27880;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#19977;&#20010;&#39069;&#22806;&#30340;&#36890;&#36947;&#26469;&#25913;&#36827;&#29983;&#25104;&#25163;&#37096;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#29983;&#25104;&#27169;&#22411;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#25163;&#37096;&#26816;&#27979;&#22120;&#26469;&#34913;&#37327;&#29983;&#25104;&#25163;&#37096;&#22270;&#20687;&#36136;&#37327;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#22914;GAN&#21644;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#22312;&#29983;&#25104;&#25163;&#37096;&#22270;&#20687;&#26041;&#38754;&#21364;&#34920;&#29616;&#20986;&#24847;&#22806;&#30340;&#19981;&#36275;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#36825;&#20123;&#31995;&#32479;&#29983;&#25104;&#25163;&#37096;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#22312;&#35757;&#32451;&#22270;&#20687;&#20013;&#22686;&#21152;&#19977;&#20010;&#39069;&#22806;&#30340;&#36890;&#36947;&#26469;&#23545;&#25163;&#37096;&#36827;&#34892;&#26631;&#27880;&#12290;&#36825;&#20123;&#26631;&#27880;&#25552;&#20379;&#39069;&#22806;&#30340;&#32467;&#26500;&#65292;&#20419;&#20351;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#25163;&#37096;&#22270;&#20687;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#29983;&#25104;&#27169;&#22411;&#19978;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#65306;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#19968;&#20010;&#26032;&#30340;&#21512;&#25104;&#25163;&#37096;&#22270;&#20687;&#25968;&#25454;&#38598;&#21644;&#21253;&#21547;&#25163;&#37096;&#30340;&#30495;&#23454;&#29031;&#29255;&#19978;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#29616;&#26377;&#30340;&#25163;&#37096;&#26816;&#27979;&#22120;&#26469;&#27979;&#37327;&#29983;&#25104;&#25163;&#37096;&#22270;&#20687;&#36136;&#37327;&#30340;&#25552;&#39640;&#65292;&#20174;&#32780;&#24471;&#21040;&#26356;&#39640;&#30340;&#25351;&#20851;&#33410;&#35782;&#21035;&#32622;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models such as GANs and diffusion models have demonstrated impressive image generation capabilities. Despite these successes, these systems are surprisingly poor at creating images with hands. We propose a novel training framework for generative models that substantially improves the ability of such systems to create hand images. Our approach is to augment the training images with three additional channels that provide annotations to hands in the image. These annotations provide additional structure that coax the generative model to produce higher quality hand images. We demonstrate this approach on two different generative models: a generative adversarial network and a diffusion model. We demonstrate our method both on a new synthetic dataset of hand images and also on real photographs that contain hands. We measure the improved quality of the generated hands through higher confidence in finger joint identification using an off-the-shelf hand detector.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#20581;&#24247;&#25991;&#26412;&#31616;&#21270;&#30740;&#31350;&#30340;&#28040;&#21270;&#30284;&#30151;&#25945;&#32946;&#26448;&#26009;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#24494;&#35843;&#12289;&#22686;&#24378;&#23398;&#20064;&#12289;&#22686;&#24378;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.15043</link><description>&lt;p&gt;
&#20581;&#24247;&#25991;&#26412;&#31616;&#21270;&#65306;&#28040;&#21270;&#30284;&#30151;&#25945;&#32946;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#21644;&#22686;&#24378;&#23398;&#20064;&#30340;&#26032;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Health Text Simplification: An Annotated Corpus for Digestive Cancer Education and Novel Strategies for Reinforcement Learning. (arXiv:2401.15043v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15043
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#20581;&#24247;&#25991;&#26412;&#31616;&#21270;&#30740;&#31350;&#30340;&#28040;&#21270;&#30284;&#30151;&#25945;&#32946;&#26448;&#26009;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#24494;&#35843;&#12289;&#22686;&#24378;&#23398;&#20064;&#12289;&#22686;&#24378;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#20581;&#24247;&#25945;&#32946;&#26448;&#26009;&#30340;&#38405;&#35835;&#27700;&#24179;&#26174;&#33879;&#24433;&#21709;&#20449;&#24687;&#30340;&#21487;&#29702;&#35299;&#24615;&#21644;&#21487;&#25509;&#35302;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23569;&#25968;&#26063;&#35028;&#20154;&#32676;&#12290;&#35768;&#22810;&#24739;&#32773;&#25945;&#32946;&#36164;&#28304;&#36229;&#36807;&#20102;&#24191;&#27867;&#25509;&#21463;&#30340;&#26631;&#20934;&#30340;&#38405;&#35835;&#27700;&#24179;&#21644;&#22797;&#26434;&#24615;&#12290;&#22312;&#20581;&#24247;&#20449;&#24687;&#20013;&#65292;&#24613;&#38656;&#39640;&#24615;&#33021;&#30340;&#25991;&#26412;&#31616;&#21270;&#27169;&#22411;&#20197;&#22686;&#24378;&#20256;&#25773;&#21644;&#35782;&#23383;&#33021;&#21147;&#12290;&#36825;&#31181;&#38656;&#35201;&#22312;&#30284;&#30151;&#25945;&#32946;&#20013;&#23588;&#20026;&#36843;&#20999;&#65292;&#26377;&#25928;&#30340;&#39044;&#38450;&#21644;&#31579;&#26597;&#25945;&#32946;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#21457;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#24341;&#20837;&#20102;&#31616;&#21270;&#30340;&#28040;&#21270;&#30284;&#30151;&#65288;SimpleDC&#65289;&#24182;&#34892;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#20581;&#24247;&#25991;&#26412;&#31616;&#21270;&#30740;&#31350;&#12290;&#21033;&#29992;SimpleDC&#21644;&#29616;&#26377;&#30340;Med-EASi&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#24494;&#35843;&#12289;&#22686;&#24378;&#23398;&#20064;&#65288;RL&#65289;&#12289;&#22686;&#24378;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: The reading level of health educational materials significantly influences information understandability and accessibility, particularly for minoritized populations. Many patient educational resources surpass the reading level and complexity of widely accepted standards. There is a critical need for high-performing text simplification models in health information to enhance dissemination and literacy. This need is particularly acute in cancer education, where effective prevention and screening education can substantially reduce morbidity and mortality.  Methods: We introduce Simplified Digestive Cancer (SimpleDC), a parallel corpus of cancer education materials tailored for health text simplification research. Utilizing SimpleDC alongside the existing Med-EASi corpus, we explore Large Language Model (LLM)-based simplification methods, including fine-tuning, reinforcement learning (RL), reinforcement learning with human feedback (RLHF), domain adaptation, and prompt-based app
&lt;/p&gt;</description></item><item><title>PROXYQA&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38271;&#31687;&#25991;&#26412;&#29983;&#25104;&#30340;&#26367;&#20195;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#35814;&#23613;&#30340;&#20869;&#23481;&#65292;&#24182;&#21033;&#29992;&#35780;&#20272;&#22120;&#21644;&#29983;&#25104;&#20869;&#23481;&#20316;&#20026;&#32972;&#26223;&#29615;&#22659;&#65292;&#26681;&#25454;&#35780;&#20272;&#22120;&#22238;&#31572;&#20195;&#29702;&#38382;&#39064;&#30340;&#34920;&#29616;&#26469;&#35780;&#20272;&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.15042</link><description>&lt;p&gt;
PROXYQA&#65306;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38271;&#31687;&#25991;&#26412;&#29983;&#25104;&#30340;&#26367;&#20195;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PROXYQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models. (arXiv:2401.15042v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15042
&lt;/p&gt;
&lt;p&gt;
PROXYQA&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38271;&#31687;&#25991;&#26412;&#29983;&#25104;&#30340;&#26367;&#20195;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#35814;&#23613;&#30340;&#20869;&#23481;&#65292;&#24182;&#21033;&#29992;&#35780;&#20272;&#22120;&#21644;&#29983;&#25104;&#20869;&#23481;&#20316;&#20026;&#32972;&#26223;&#29615;&#22659;&#65292;&#26681;&#25454;&#35780;&#20272;&#22120;&#22238;&#31572;&#20195;&#29702;&#38382;&#39064;&#30340;&#34920;&#29616;&#26469;&#35780;&#20272;&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#38271;&#31687;&#25991;&#26412;&#29702;&#35299;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#29983;&#25104;&#38271;&#31687;&#20869;&#23481;&#65288;&#22914;&#25253;&#21578;&#21644;&#25991;&#31456;&#65289;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#24403;&#21069;&#30340;&#22522;&#20934;&#19981;&#36275;&#20197;&#20805;&#20998;&#35780;&#20272;LLMs&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#19988;&#20840;&#38754;&#30340;&#20869;&#23481;&#65292;&#22240;&#27492;&#38656;&#35201;&#19968;&#31181;&#26356;&#20005;&#26684;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;\textsc{ProxyQA}&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#38271;&#31687;&#25991;&#26412;&#29983;&#25104;&#65292;&#21253;&#25324;&#28145;&#20837;&#20154;&#24037;&#31574;&#21010;&#30340;&#28085;&#30422;&#22810;&#20010;&#39046;&#22495;&#30340;&#8220;&#20803;&#38382;&#39064;&#8221;&#12290;&#27599;&#20010;&#20803;&#38382;&#39064;&#37117;&#21253;&#21547;&#30456;&#24212;&#30340;&#24102;&#27880;&#37322;&#31572;&#26696;&#30340;&#8220;&#20195;&#29702;&#38382;&#39064;&#8221;&#12290;LLMs&#34987;&#35201;&#27714;&#26681;&#25454;&#36825;&#20123;&#20803;&#38382;&#39064;&#29983;&#25104;&#35814;&#23613;&#30340;&#20869;&#23481;&#12290;&#21033;&#29992;&#35780;&#20272;&#22120;&#24182;&#23558;&#29983;&#25104;&#30340;&#20869;&#23481;&#20316;&#20026;&#32972;&#26223;&#29615;&#22659;&#65292;\textsc{ProxyQA}&#26681;&#25454;&#35780;&#20272;&#22120;&#22238;&#31572;&#8220;&#20195;&#29702;&#38382;&#39064;&#8221;&#30340;&#34920;&#29616;&#35780;&#20272;&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#26816;&#39564;&#20102;&#22810;&#20010;LLMs&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have exhibited remarkable success in long-form context comprehension tasks. However, their capacity to generate long contents, such as reports and articles, remains insufficiently explored. Current benchmarks do not adequately assess LLMs' ability to produce informative and comprehensive content, necessitating a more rigorous evaluation approach. In this study, we introduce \textsc{ProxyQA}, a framework for evaluating long-form text generation, comprising in-depth human-curated \textit{meta-questions} spanning various domains. Each meta-question contains corresponding \textit{proxy-questions} with annotated answers. LLMs are prompted to generate extensive content in response to these meta-questions. Utilizing an evaluator and incorporating generated content as background context, \textsc{ProxyQA} evaluates the quality of generated content based on the evaluator's performance in answering the \textit{proxy-questions}. We examine multiple LLMs, emphasizing \t
&lt;/p&gt;</description></item><item><title>"Airavata"&#26159;&#19968;&#20010;&#38024;&#23545;&#21360;&#22320;&#35821;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;LLM&#65292;&#36890;&#36807;&#24494;&#35843;OpenHathi&#21644;IndicInstruct&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#21327;&#21161;&#20219;&#21153;&#24615;&#33021;&#65292;&#24182;&#35745;&#21010;&#25193;&#23637;&#21040;&#25152;&#26377;22&#31181;&#35745;&#21010;Indic&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2401.15006</link><description>&lt;p&gt;
Airavata: &#24341;&#20837;&#38024;&#23545;&#21360;&#22320;&#35821;&#25351;&#20196;&#35843;&#25972;&#30340;LLM
&lt;/p&gt;
&lt;p&gt;
Airavata: Introducing Hindi Instruction-tuned LLM. (arXiv:2401.15006v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15006
&lt;/p&gt;
&lt;p&gt;
"Airavata"&#26159;&#19968;&#20010;&#38024;&#23545;&#21360;&#22320;&#35821;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;LLM&#65292;&#36890;&#36807;&#24494;&#35843;OpenHathi&#21644;IndicInstruct&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#21327;&#21161;&#20219;&#21153;&#24615;&#33021;&#65292;&#24182;&#35745;&#21010;&#25193;&#23637;&#21040;&#25152;&#26377;22&#31181;&#35745;&#21010;Indic&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23459;&#24067;&#39318;&#27425;&#21457;&#24067;&#20102;"Airavata"&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#21360;&#22320;&#35821;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;LLM&#12290;&#36890;&#36807;&#23558;OpenHathi&#19982;&#21508;&#31181;&#25351;&#20196;&#35843;&#25972;&#30340;&#21360;&#22320;&#35821;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;Airavata&#26356;&#36866;&#21512;&#36741;&#21161;&#20219;&#21153;&#12290;&#38500;&#20102;&#27169;&#22411;&#22806;&#65292;&#25105;&#20204;&#36824;&#20998;&#20139;&#20102;IndicInstruct&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#32452;&#29992;&#20110;&#36827;&#19968;&#27493;&#30740;&#31350;Indic LLM&#30340;&#22810;&#26679;&#21270;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#35780;&#20272;&#22522;&#20934;&#21644;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#35780;&#20272;LLM&#22312;&#21360;&#22320;&#35821;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#30446;&#21069;&#65292;Airavata&#25903;&#25345;&#21360;&#22320;&#35821;&#65292;&#20294;&#25105;&#20204;&#35745;&#21010;&#23558;&#20854;&#25193;&#23637;&#21040;&#25152;&#26377;22&#31181;&#35745;&#21010;Indic&#35821;&#35328;&#12290;&#24744;&#21487;&#20197;&#22312;https://ai4bharat.github.io/airavata&#19978;&#35775;&#38382;&#25152;&#26377;&#24037;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
We announce the initial release of "Airavata," an instruction-tuned LLM for Hindi. Airavata was created by fine-tuning OpenHathi with diverse, instruction-tuning Hindi datasets to make it better suited for assistive tasks. Along with the model, we also share the IndicInstruct dataset, which is a collection of diverse instruction-tuning datasets to enable further research for Indic LLMs. Additionally, we present evaluation benchmarks and a framework for assessing LLM performance across tasks in Hindi. Currently, Airavata supports Hindi, but we plan to expand this to all 22 scheduled Indic languages. You can access all artifacts at https://ai4bharat.github.io/airavata.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36793;&#32536;-&#38654;-&#20113;&#35745;&#31639;&#20013;&#36719;&#20214;&#26550;&#26500;&#19981;&#36275;&#30340;&#26041;&#27861;&#65292;&#35813;&#26550;&#26500;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#21644;&#24773;&#22659;&#25968;&#25454;&#20998;&#26512;&#65292;&#24182;&#23454;&#29616;&#30456;&#37051;&#23618;&#27425;&#20043;&#38388;&#30340;&#21452;&#21521;&#36890;&#20449;&#12290;</title><link>http://arxiv.org/abs/2401.14968</link><description>&lt;p&gt;
&#22823;&#27668;&#65306;&#29992;&#20110;&#36793;&#32536;-&#38654;-&#20113;&#35745;&#31639;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#21327;&#21516;&#29289;&#32852;&#32593;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Atmosphere: Context and situational-aware collaborative IoT architecture for edge-fog-cloud computing. (arXiv:2401.14968v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36793;&#32536;-&#38654;-&#20113;&#35745;&#31639;&#20013;&#36719;&#20214;&#26550;&#26500;&#19981;&#36275;&#30340;&#26041;&#27861;&#65292;&#35813;&#26550;&#26500;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#21644;&#24773;&#22659;&#25968;&#25454;&#20998;&#26512;&#65292;&#24182;&#23454;&#29616;&#30456;&#37051;&#23618;&#27425;&#20043;&#38388;&#30340;&#21452;&#21521;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#30340;&#26222;&#21450;&#20351;&#24471;&#36890;&#20449;&#23481;&#37327;&#22686;&#21152;&#65292;&#25104;&#26412;&#38477;&#20302;&#65292;&#25216;&#26415;&#20063;&#24471;&#21040;&#20102;&#26497;&#22823;&#30340;&#21457;&#23637;&#12290;&#21516;&#26102;&#65292;&#22823;&#25968;&#25454;&#21644;&#23454;&#26102;&#25968;&#25454;&#20998;&#26512;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#24182;&#24341;&#36215;&#20102;&#24066;&#27665;&#12289;&#20844;&#20849;&#31649;&#29702;&#21644;&#20854;&#20182;&#32452;&#32455;&#20043;&#38388;&#20998;&#20139;&#25968;&#25454;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#20852;&#36259;&#65292;&#24418;&#25104;&#20102;&#21327;&#21516;&#29289;&#32852;&#32593;&#12290;&#20294;&#26159;&#65292;&#36825;&#31181;&#25968;&#25454;&#21644;&#22522;&#30784;&#35774;&#26045;&#30340;&#22686;&#38271;&#24517;&#39035;&#20276;&#38543;&#30528;&#20801;&#35768;&#21033;&#29992;&#30340;&#36719;&#20214;&#26550;&#26500;&#12290;&#23613;&#31649;&#26377;&#21508;&#31181;&#20851;&#27880;&#36793;&#32536;&#12289;&#38654;&#21644;/&#25110;&#20113;&#23618;&#19978;&#29289;&#32852;&#32593;&#21033;&#29992;&#30340;&#25552;&#35758;&#65292;&#20294;&#24456;&#38590;&#25214;&#21040;&#19968;&#31181;&#23558;&#36825;&#19977;&#20010;&#23618;&#27425;&#19968;&#36215;&#21033;&#29992;&#36215;&#26469;&#30340;&#36719;&#20214;&#35299;&#20915;&#26041;&#26696;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#27599;&#20010;&#23618;&#27425;&#30340;&#19978;&#19979;&#25991;&#21644;&#24773;&#22659;&#25968;&#25454;&#20998;&#26512;&#65292;&#24182;&#21033;&#29992;&#30456;&#37051;&#23618;&#27425;&#20043;&#38388;&#30340;&#21452;&#21521;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Internet of Things (IoT) has grown significantly in popularity, accompanied by increased capacity and lower cost of communications, and overwhelming development of technologies. At the same time, big data and real-time data analysis have taken on great importance and have been accompanied by unprecedented interest in sharing data among citizens, public administrations and other organisms, giving rise to what is known as the Collaborative Internet of Things. This growth in data and infrastructure must be accompanied by a software architecture that allows its exploitation. Although there are various proposals focused on the exploitation of the IoT at edge, fog and/or cloud levels, it is not easy to find a software solution that exploits the three tiers together, taking maximum advantage not only of the analysis of contextual and situational data at each tier, but also of two-way communications between adjacent ones. In this paper, we propose an architecture that solves these deficien
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#23558;Solomonoff Induction&#65288;SI&#65289;&#24341;&#20837;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#20351;&#29992;&#19975;&#33021;&#22270;&#28789;&#26426;&#65288;UTMs&#65289;&#29983;&#25104;&#30340;&#25968;&#25454;&#26469;&#36827;&#34892;&#20803;&#23398;&#20064;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;UTM&#25968;&#25454;&#26159;&#20803;&#23398;&#20064;&#30340;&#26377;&#20215;&#20540;&#36164;&#28304;&#65292;&#21487;&#20197;&#29992;&#26469;&#35757;&#32451;&#33021;&#22815;&#23398;&#20064;&#36890;&#29992;&#39044;&#27979;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2401.14953</link><description>&lt;p&gt;
&#23398;&#20064;&#36890;&#29992;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Learning Universal Predictors. (arXiv:2401.14953v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#23558;Solomonoff Induction&#65288;SI&#65289;&#24341;&#20837;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#20351;&#29992;&#19975;&#33021;&#22270;&#28789;&#26426;&#65288;UTMs&#65289;&#29983;&#25104;&#30340;&#25968;&#25454;&#26469;&#36827;&#34892;&#20803;&#23398;&#20064;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;UTM&#25968;&#25454;&#26159;&#20803;&#23398;&#20064;&#30340;&#26377;&#20215;&#20540;&#36164;&#28304;&#65292;&#21487;&#20197;&#29992;&#26469;&#35757;&#32451;&#33021;&#22815;&#23398;&#20064;&#36890;&#29992;&#39044;&#27979;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#24555;&#36895;&#20174;&#26377;&#38480;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#23545;&#19981;&#21516;&#20219;&#21153;&#30340;&#24191;&#27867;&#26292;&#38706;&#23548;&#33268;&#20102;&#22810;&#21151;&#33021;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#36890;&#29992;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#20294;&#26159;&#65292;&#20803;&#23398;&#20064;&#30340;&#38480;&#21046;&#26159;&#20160;&#20040;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#26368;&#24378;&#22823;&#30340;&#36890;&#29992;&#39044;&#27979;&#22120;Solomonoff Induction&#65288;SI&#65289;&#36890;&#36807;&#20803;&#23398;&#20064;&#30340;&#26497;&#38480;&#36827;&#34892;&#20998;&#25285;&#65292;&#25506;&#32034;&#20854;&#28508;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#19975;&#33021;&#22270;&#28789;&#26426;&#65288;UTMs&#65289;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#29992;&#20110;&#23558;&#32593;&#32476;&#26292;&#38706;&#20110;&#24191;&#27867;&#30340;&#27169;&#24335;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;UTM&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#21644;&#20803;&#35757;&#32451;&#21327;&#35758;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#22797;&#26434;&#24615;&#21644;&#26222;&#36866;&#24615;&#30340;&#31639;&#27861;&#25968;&#25454;&#29983;&#25104;&#22120;&#23545;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65288;&#22914;LSTMs&#12289;Transformers&#65289;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;UTM&#25968;&#25454;&#26159;&#20803;&#23398;&#20064;&#30340;&#23453;&#36149;&#36164;&#28304;&#65292;&#21487;&#20197;&#29992;&#26469;&#35757;&#32451;&#33021;&#22815;&#23398;&#20064;&#36890;&#29992;&#39044;&#27979;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-learning has emerged as a powerful approach to train neural networks to learn new tasks quickly from limited data. Broad exposure to different tasks leads to versatile representations enabling general problem solving. But, what are the limits of meta-learning? In this work, we explore the potential of amortizing the most powerful universal predictor, namely Solomonoff Induction (SI), into neural networks via leveraging meta-learning to its limits. We use Universal Turing Machines (UTMs) to generate training data used to expose networks to a broad range of patterns. We provide theoretical analysis of the UTM data generation processes and meta-training protocols. We conduct comprehensive experiments with neural architectures (e.g. LSTMs, Transformers) and algorithmic data generators of varying complexity and universality. Our results suggest that UTM data is a valuable resource for meta-learning, and that it can be used to train neural networks capable of learning universal predicti
&lt;/p&gt;</description></item><item><title>&#23545;&#25239;&#35757;&#32451;&#22312;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#26159;&#29306;&#29298;&#20102;&#26631;&#20934;&#21644;&#40065;&#26834;&#27867;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36873;&#25321;&#24615;&#22320;&#26356;&#26032;&#29305;&#23450;&#23618;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#32593;&#32476;&#23398;&#20064;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;CURE&#26694;&#26550;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#20445;&#30041;&#12289;&#26356;&#26032;&#21644;&#20462;&#35746;&#26435;&#37325;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#36825;&#19968;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#35760;&#24518;&#21270;&#21644;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2401.14948</link><description>&lt;p&gt;
&#20445;&#30041;-&#26356;&#26032;-&#20462;&#35746;&#20197;&#35299;&#20915;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Conserve-Update-Revise to Cure Generalization and Robustness Trade-off in Adversarial Training. (arXiv:2401.14948v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14948
&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#22312;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#26159;&#29306;&#29298;&#20102;&#26631;&#20934;&#21644;&#40065;&#26834;&#27867;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36873;&#25321;&#24615;&#22320;&#26356;&#26032;&#29305;&#23450;&#23618;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#32593;&#32476;&#23398;&#20064;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;CURE&#26694;&#26550;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#20445;&#30041;&#12289;&#26356;&#26032;&#21644;&#20462;&#35746;&#26435;&#37325;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#36825;&#19968;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#35760;&#24518;&#21270;&#21644;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#20294;&#20250;&#23548;&#33268;&#26631;&#20934;&#27867;&#21270;&#21644;&#40065;&#26834;&#27867;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20026;&#20102;&#25581;&#31034;&#39537;&#21160;&#36825;&#19968;&#29616;&#35937;&#30340;&#28508;&#22312;&#22240;&#32032;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#20174;&#26631;&#20934;&#35774;&#32622;&#21040;&#23545;&#25239;&#35774;&#32622;&#36807;&#28193;&#26102;&#30340;&#36880;&#23618;&#23398;&#20064;&#33021;&#21147;&#12290;&#32463;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36873;&#25321;&#24615;&#22320;&#26356;&#26032;&#29305;&#23450;&#23618;&#32780;&#20445;&#30041;&#20854;&#20182;&#23618;&#21487;&#20197;&#22823;&#24133;&#22686;&#24378;&#32593;&#32476;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26694;&#26550;CURE&#65292;&#21033;&#29992;&#26799;&#24230;&#26174;&#33879;&#24615;&#20934;&#21017;&#23545;&#26435;&#37325;&#36827;&#34892;&#36873;&#25321;&#24615;&#20445;&#30041;&#12289;&#26356;&#26032;&#21644;&#20462;&#35746;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;CURE&#30340;&#35774;&#35745;&#26159;&#25968;&#25454;&#38598;&#21644;&#26550;&#26500;&#19981;&#21487;&#30693;&#30340;&#65292;&#30830;&#20445;&#20854;&#36866;&#29992;&#20110;&#21508;&#31181;&#24773;&#20917;&#12290;&#23427;&#26377;&#25928;&#35299;&#20915;&#20102;&#35760;&#24518;&#21270;&#21644;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#19988;&#36825;&#31181;&#35757;&#32451;&#26041;&#27861;&#36824;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
Adversarial training improves the robustness of neural networks against adversarial attacks, albeit at the expense of the trade-off between standard and robust generalization. To unveil the underlying factors driving this phenomenon, we examine the layer-wise learning capabilities of neural networks during the transition from a standard to an adversarial setting. Our empirical findings demonstrate that selectively updating specific layers while preserving others can substantially enhance the network's learning capacity. We therefore propose CURE, a novel training framework that leverages a gradient prominence criterion to perform selective conservation, updating, and revision of weights. Importantly, CURE is designed to be dataset- and architecture-agnostic, ensuring its applicability across various scenarios. It effectively tackles both memorization and overfitting issues, thus enhancing the trade-off between robustness and generalization and additionally, this training approach also 
&lt;/p&gt;</description></item><item><title>SSDOnt&#26159;&#19968;&#31181;&#29992;&#20110;&#25551;&#36848;&#21644;&#27880;&#37322;&#21333;&#20307;&#35774;&#35745;&#30740;&#31350;&#30340;&#26412;&#20307;&#35770;&#65292;&#20026;&#36825;&#31867;&#30740;&#31350;&#25552;&#20379;&#20102;&#36866;&#24403;&#30340;&#26415;&#35821;&#21644;&#21442;&#32771;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.14933</link><description>&lt;p&gt;
SSDOnt&#65306;&#34920;&#31034;&#21333;&#20307;&#35774;&#35745;&#30740;&#31350;&#30340;&#26412;&#20307;&#35770;
&lt;/p&gt;
&lt;p&gt;
SSDOnt: an Ontology for representing Single-Subject Design Studies. (arXiv:2401.14933v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14933
&lt;/p&gt;
&lt;p&gt;
SSDOnt&#26159;&#19968;&#31181;&#29992;&#20110;&#25551;&#36848;&#21644;&#27880;&#37322;&#21333;&#20307;&#35774;&#35745;&#30740;&#31350;&#30340;&#26412;&#20307;&#35770;&#65292;&#20026;&#36825;&#31867;&#30740;&#31350;&#25552;&#20379;&#20102;&#36866;&#24403;&#30340;&#26415;&#35821;&#21644;&#21442;&#32771;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#21333;&#20307;&#35774;&#35745;&#22312;&#25945;&#32946;&#21644;&#29983;&#29289;&#21307;&#23398;&#31561;&#22810;&#20010;&#39046;&#22495;&#20013;&#34987;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#36866;&#21512;&#30340;&#27491;&#24335;&#35789;&#27719;&#26469;&#27880;&#37322;&#27492;&#31867;&#30740;&#31350;&#30340;&#35814;&#32454;&#37197;&#32622;&#21644;&#32467;&#26524;&#65292;&#20197;&#33719;&#24471;&#26377;&#20851;&#23427;&#20204;&#30340;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#23545;&#36825;&#20123;&#30740;&#31350;&#35774;&#35745;&#30340;&#25628;&#32034;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#23545;&#25688;&#35201;&#12289;&#20851;&#38190;&#35789;&#25110;&#20840;&#25991;&#36827;&#34892;&#35821;&#27861;&#25628;&#32034;&#65292;&#36825;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;&#30446;&#26631;&#65306;&#20171;&#32461;SSDOnt&#65292;&#19968;&#20010;&#29992;&#20110;&#25551;&#36848;&#21644;&#27880;&#37322;&#21333;&#20307;&#35774;&#35745;&#30740;&#31350;&#30340;&#29305;&#23450;&#30446;&#30340;&#26412;&#20307;&#35770;&#65292;&#20197;&#20415;&#38543;&#21518;&#21487;&#20197;&#23545;&#20854;&#36827;&#34892;&#22797;&#26434;&#38382;&#39064;&#30340;&#25552;&#38382;&#12290;&#26041;&#27861;&#65306;&#26412;&#20307;&#35770;&#26159;&#25353;&#29031;NeOn&#26041;&#27861;&#35770;&#24320;&#21457;&#30340;&#12290;&#22312;&#23450;&#20041;&#20102;&#26412;&#20307;&#35770;&#30340;&#35201;&#27714;&#21518;&#65292;&#20351;&#29992;&#25551;&#36848;&#36923;&#36753;&#25551;&#36848;&#20102;&#19968;&#20010;&#24418;&#24335;&#21270;&#27169;&#22411;&#65292;&#24182;&#22312;&#26412;&#20307;&#35770;&#35821;&#35328;OWL 2 DL&#20013;&#23454;&#29616;&#20102;&#35813;&#27169;&#22411;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#23637;&#31034;&#20102;&#26412;&#20307;&#35770;&#22914;&#20309;&#25552;&#20379;&#19968;&#20010;&#20855;&#26377;&#36866;&#24403;&#26415;&#35821;&#30340;&#21442;&#32771;&#27169;&#22411;&#65292;&#29992;&#20110;&#34920;&#31034;&#21333;&#20307;&#35774;&#35745;&#30740;&#31350;&#30340;&#35814;&#32454;&#37197;&#32622;&#21644;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Single-Subject Design is used in several areas such as education and biomedicine. However, no suited formal vocabulary exists for annotating the detailed configuration and the results of this type of research studies with the appropriate granularity for looking for information about them. Therefore, the search for those study designs relies heavily on a syntactical search on the abstract, keywords or full text of the publications about the study, which entails some limitations. Objective: To present SSDOnt, a specific purpose ontology for describing and annotating single-subject design studies, so that complex questions can be asked about them afterwards. Methods: The ontology was developed following the NeOn methodology. Once the requirements of the ontology were defined, a formal model was described in a Description Logic and later implemented in the ontology language OWL 2 DL. Results: We show how the ontology provides a reference model with a suitable terminology for th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#29992;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#35760;&#24518;&#20102;&#24050;&#30693;&#26412;&#20307;&#35770;&#30340;&#20449;&#24687;&#20197;&#21450;&#35760;&#24518;&#30340;&#31243;&#24230;&#65292;&#32467;&#26524;&#26174;&#31034;LLMs&#37096;&#20998;&#22320;&#20102;&#35299;&#26412;&#20307;&#35770;&#30340;&#27010;&#24565;&#65292;&#35760;&#24518;&#31243;&#24230;&#19982;&#20854;&#22312;Web&#19978;&#30340;&#27969;&#34892;&#31243;&#24230;&#25104;&#27491;&#27604;&#12290;</title><link>http://arxiv.org/abs/2401.14931</link><description>&lt;p&gt;
LLM&#26159;&#21542;&#33021;&#35760;&#24518;&#26412;&#20307;&#35770;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do LLMs Dream of Ontologies?. (arXiv:2401.14931v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#29992;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#35760;&#24518;&#20102;&#24050;&#30693;&#26412;&#20307;&#35770;&#30340;&#20449;&#24687;&#20197;&#21450;&#35760;&#24518;&#30340;&#31243;&#24230;&#65292;&#32467;&#26524;&#26174;&#31034;LLMs&#37096;&#20998;&#22320;&#20102;&#35299;&#26412;&#20307;&#35770;&#30340;&#27010;&#24565;&#65292;&#35760;&#24518;&#31243;&#24230;&#19982;&#20854;&#22312;Web&#19978;&#30340;&#27969;&#34892;&#31243;&#24230;&#25104;&#27491;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26368;&#36817;&#22312;&#33258;&#21160;&#25991;&#26412;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#20381;&#36182;&#20110;&#24213;&#23618;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#36825;&#20351;&#24471;LLMs&#33021;&#22815;&#35760;&#24518;&#35757;&#32451;&#36807;&#31243;&#20013;&#25509;&#35302;&#21040;&#30340;&#22823;&#37327;&#25968;&#25454;&#30340;&#19968;&#37096;&#20998;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#29992;&#39044;&#35757;&#32451;LLMs&#26159;&#21542;&#35760;&#24518;&#20102;&#24050;&#30693;&#26412;&#20307;&#35770;&#30340;&#20449;&#24687;&#20197;&#21450;&#35760;&#24518;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#37096;&#20998;&#22320;&#20102;&#35299;&#26412;&#20307;&#35770;&#65306;&#23427;&#20204;&#21487;&#20197;&#35760;&#24518;&#25991;&#26412;&#20013;&#25552;&#21040;&#30340;&#26412;&#20307;&#35770;&#27010;&#24565;&#65292;&#20294;&#20854;&#23545;&#27010;&#24565;&#30340;&#35760;&#24518;&#31243;&#24230;&#20284;&#20046;&#19982;&#20854;&#22312;Web&#19978;&#30340;&#27969;&#34892;&#31243;&#24230;&#25104;&#27604;&#20363;&#21464;&#21270;&#65292;&#22240;&#20026;Web&#26159;&#23427;&#20204;&#35757;&#32451;&#26448;&#26009;&#30340;&#20027;&#35201;&#26469;&#28304;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#36890;&#36807;&#27979;&#37327;&#19981;&#21516;&#25552;&#31034;&#37325;&#22797;&#12289;&#26597;&#35810;&#35821;&#35328;&#21644;&#30830;&#23450;&#24230;&#30340;&#36755;&#20986;&#19968;&#33268;&#24615;&#26469;&#20272;&#35745;LLMs&#23545;&#26412;&#20307;&#35770;&#20449;&#24687;&#30340;&#35760;&#24518;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently revolutionized automated text understanding and generation. The performance of these models relies on the high number of parameters of the underlying neural architectures, which allows LLMs to memorize part of the vast quantity of data seen during the training. This paper investigates whether and to what extent general-purpose pre-trained LLMs have memorized information from known ontologies. Our results show that LLMs partially know ontologies: they can, and do indeed, memorize concepts from ontologies mentioned in the text, but the level of memorization of their concepts seems to vary proportionally to their popularity on the Web, the primary source of their training material. We additionally propose new metrics to estimate the degree of memorization of ontological information in LLMs by measuring the consistency of the output produced across different prompt repetitions, query languages, and degrees of determinism.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#34892;&#20026;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#65288;BMRL&#65289;&#26694;&#26550;&#65292;&#22312;&#25705;&#25830;&#24615;&#20219;&#21153;&#20013;&#65292;AI&#20195;&#29702;&#23545;&#26377;&#38480;&#29702;&#24615;&#20154;&#31867;&#20195;&#29702;&#30340;&#21442;&#25968;&#36827;&#34892;&#24178;&#39044;&#65292;&#36890;&#36807;&#23545;&#20154;&#31867;&#31574;&#30053;&#36827;&#34892;&#35299;&#37322;&#65292;&#24110;&#21161;&#29702;&#35299;&#34892;&#20026;&#24178;&#39044;&#12290;</title><link>http://arxiv.org/abs/2401.14923</link><description>&lt;p&gt;
&#22312;&#26377;&#38480;&#29702;&#24615;&#20154;&#31867;&#20195;&#29702;&#22312;&#25705;&#25830;&#20219;&#21153;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#24178;&#39044;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning Interventions on Boundedly Rational Human Agents in Frictionful Tasks. (arXiv:2401.14923v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#34892;&#20026;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#65288;BMRL&#65289;&#26694;&#26550;&#65292;&#22312;&#25705;&#25830;&#24615;&#20219;&#21153;&#20013;&#65292;AI&#20195;&#29702;&#23545;&#26377;&#38480;&#29702;&#24615;&#20154;&#31867;&#20195;&#29702;&#30340;&#21442;&#25968;&#36827;&#34892;&#24178;&#39044;&#65292;&#36890;&#36807;&#23545;&#20154;&#31867;&#31574;&#30053;&#36827;&#34892;&#35299;&#37322;&#65292;&#24110;&#21161;&#29702;&#35299;&#34892;&#20026;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#37325;&#35201;&#30340;&#34892;&#20026;&#21464;&#21270;&#26159;&#20855;&#26377;&#25705;&#25830;&#30340;&#65307;&#23427;&#20204;&#35201;&#27714;&#20010;&#20307;&#38271;&#26399;&#20184;&#20986;&#21162;&#21147;&#65292;&#20294;&#27809;&#26377;&#21363;&#26102;&#30340;&#28385;&#36275;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20195;&#29702;&#21487;&#20197;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#24178;&#39044;&#65292;&#24110;&#21161;&#20010;&#20307;&#22362;&#25345;&#33258;&#24049;&#30340;&#30446;&#26631;&#12290;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#65292;AI&#20195;&#29702;&#24517;&#39035;&#24555;&#36895;&#20010;&#24615;&#21270;&#65288;&#22312;&#20010;&#20307;&#22833;&#21435;&#20852;&#36259;&#20043;&#21069;&#65289;&#24182;&#20855;&#26377;&#35299;&#37322;&#24615;&#65292;&#20197;&#24110;&#21161;&#25105;&#20204;&#29702;&#35299;&#34892;&#20026;&#24178;&#39044;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#34892;&#20026;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#65288;BMRL&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;AI&#20195;&#29702;&#23545;&#23646;&#20110;&#26377;&#38480;&#29702;&#24615;&#20154;&#31867;&#20195;&#29702;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#30340;&#21442;&#25968;&#36827;&#34892;&#24178;&#39044;&#12290;&#25105;&#20204;&#23558;&#20154;&#31867;&#20915;&#31574;&#32773;&#30340;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#35268;&#21010;&#20195;&#29702;&#65292;&#36825;&#26679;&#25105;&#20204;&#23601;&#33021;&#22815;&#23558;&#19981;&#29702;&#24819;&#30340;&#20154;&#31867;&#31574;&#30053;&#65288;&#19981;&#20250;&#36798;&#21040;&#30446;&#26631;&#30340;&#31574;&#30053;&#65289;&#24402;&#22240;&#20110;&#20854;&#19981;&#36866;&#24212;&#30340;MDP&#21442;&#25968;&#65292;&#27604;&#22914;&#26497;&#20302;&#30340;&#25240;&#25187;&#22240;&#23376;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31867;&#21487;&#35299;&#37322;&#30340;&#20154;&#31867;&#27169;&#22411;&#65292;&#25429;&#25417;&#20102;&#25705;&#25830;&#20219;&#21153;&#20013;&#30340;&#22522;&#26412;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many important behavior changes are frictionful; they require individuals to expend effort over a long period with little immediate gratification. Here, an artificial intelligence (AI) agent can provide personalized interventions to help individuals stick to their goals. In these settings, the AI agent must personalize rapidly (before the individual disengages) and interpretably, to help us understand the behavioral interventions. In this paper, we introduce Behavior Model Reinforcement Learning (BMRL), a framework in which an AI agent intervenes on the parameters of a Markov Decision Process (MDP) belonging to a boundedly rational human agent. Our formulation of the human decision-maker as a planning agent allows us to attribute undesirable human policies (ones that do not lead to the goal) to their maladapted MDP parameters, such as an extremely low discount factor. Furthermore, we propose a class of tractable human models that captures fundamental behaviors in frictionful tasks. Int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#19982;&#23398;&#29983;&#20849;&#21516;&#35774;&#35745;&#30340;&#30740;&#31350;&#65292;&#25506;&#32034;&#23398;&#29983;&#22312;&#39033;&#30446;&#21270;&#23398;&#20064;&#20013;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#28508;&#21147;&#65292;&#24182;&#22522;&#20110;&#23398;&#29983;&#30340;&#35270;&#35273;&#23545;&#25945;&#32946;&#30446;&#26631;&#36716;&#21464;&#36827;&#34892;&#20998;&#26512;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19981;&#21516;&#24577;&#24230;&#30340;&#23398;&#29983;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#20351;&#29992;&#26377;&#19981;&#21516;&#30340;&#20559;&#22909;&#12290;&#36825;&#20026;&#26410;&#26469;&#30740;&#31350;&#23398;&#29983;&#19982;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#21644;&#29702;&#35299;&#22686;&#24378;&#23398;&#20064;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2401.14915</link><description>&lt;p&gt;
AI&#22312;&#22522;&#20110;&#39033;&#30446;&#30340;&#23398;&#20064;&#20013;&#30340;&#26410;&#26469;&#35268;&#21010;&#65306;&#19982;&#23398;&#29983;&#19968;&#36215;&#36827;&#34892;&#20849;&#21516;&#35774;&#35745;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Charting the Future of AI in Project-Based Learning: A Co-Design Exploration with Students. (arXiv:2401.14915v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#19982;&#23398;&#29983;&#20849;&#21516;&#35774;&#35745;&#30340;&#30740;&#31350;&#65292;&#25506;&#32034;&#23398;&#29983;&#22312;&#39033;&#30446;&#21270;&#23398;&#20064;&#20013;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#28508;&#21147;&#65292;&#24182;&#22522;&#20110;&#23398;&#29983;&#30340;&#35270;&#35273;&#23545;&#25945;&#32946;&#30446;&#26631;&#36716;&#21464;&#36827;&#34892;&#20998;&#26512;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19981;&#21516;&#24577;&#24230;&#30340;&#23398;&#29983;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#20351;&#29992;&#26377;&#19981;&#21516;&#30340;&#20559;&#22909;&#12290;&#36825;&#20026;&#26410;&#26469;&#30740;&#31350;&#23398;&#29983;&#19982;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#21644;&#29702;&#35299;&#22686;&#24378;&#23398;&#20064;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#29983;&#22312;&#23398;&#20064;&#20013;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#65292;&#36825;&#32473;&#22522;&#20110;&#39033;&#30446;&#30340;&#23398;&#20064;&#65288;PBL&#65289;&#30340;&#35780;&#20272;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#20849;&#21516;&#35774;&#35745;&#30740;&#31350;&#65292;&#25506;&#32034;&#23398;&#29983;&#30340;AI&#20351;&#29992;&#25968;&#25454;&#20316;&#20026;PBL&#35780;&#20272;&#30340;&#26032;&#26448;&#26009;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#19982;18&#21517;&#22823;&#23398;&#29983;&#36827;&#34892;&#20102;&#30740;&#35752;&#20250;&#65292;&#40723;&#21169;&#20182;&#20204;&#35774;&#24819;&#19968;&#20010;&#21487;&#20197;&#33258;&#30001;&#20351;&#29992;AI&#36827;&#34892;PBL&#30340;&#21478;&#19968;&#20010;&#19990;&#30028;&#65292;&#21516;&#26102;&#38656;&#35201;&#25253;&#21578;&#36825;&#20010;&#36807;&#31243;&#20197;&#35780;&#20272;&#20182;&#20204;&#30340;&#25216;&#33021;&#21644;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#30740;&#35752;&#20250;&#20135;&#29983;&#20102;&#21508;&#31181;&#23398;&#29983;&#22312;PBL&#20013;&#20351;&#29992;AI&#30340;&#24773;&#26223;&#65292;&#20197;&#21450;&#22522;&#20110;&#23398;&#29983;&#23545;&#25945;&#32946;&#30446;&#26631;&#36716;&#21464;&#30340;&#35270;&#35273;&#36827;&#34892;&#20998;&#26512;&#36825;&#20123;&#20351;&#29992;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#23545;AI&#25345;&#26377;&#19981;&#21516;&#24577;&#24230;&#30340;&#23398;&#29983;&#22312;&#20998;&#26512;&#21644;&#29702;&#35299;AI&#20351;&#29992;&#26041;&#38754;&#26377;&#19981;&#21516;&#30340;&#20559;&#22909;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20851;&#20110;&#23398;&#29983;&#19982;AI&#20132;&#20114;&#21644;&#29702;&#35299;AI&#22686;&#24378;&#23398;&#20064;&#30340;&#26410;&#26469;&#30740;&#31350;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing use of Artificial Intelligence (AI) by students in learning presents new challenges for assessing their learning outcomes in project-based learning (PBL). This paper introduces a co-design study to explore the potential of students' AI usage data as a novel material for PBL assessment. We conducted workshops with 18 college students, encouraging them to speculate an alternative world where they could freely employ AI in PBL while needing to report this process to assess their skills and contributions. Our workshops yielded various scenarios of students' use of AI in PBL and ways of analyzing these uses grounded by students' vision of education goal transformation. We also found students with different attitudes toward AI exhibited distinct preferences in how to analyze and understand the use of AI. Based on these findings, we discuss future research opportunities on student-AI interactions and understanding AI-enhanced learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#31354;&#38388;&#33258;&#36866;&#24212;&#28388;&#27874;&#22120;&#65288;CSF&#65289;&#65292;&#21487;&#20197;&#20174;&#22270;&#25299;&#25169;&#21644;&#33410;&#28857;&#23646;&#24615;&#31354;&#38388;&#20013;&#25552;&#21462;&#33258;&#36866;&#24212;&#39057;&#29575;&#20449;&#24687;&#65292;&#20197;&#20943;&#36731;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14876</link><description>&lt;p&gt;
&#36328;&#31354;&#38388;&#33258;&#36866;&#24212;&#28388;&#27874;&#22120;&#65306;&#38598;&#25104;&#22270;&#25299;&#25169;&#21644;&#33410;&#28857;&#23646;&#24615;&#20197;&#20943;&#36731;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Cross-Space Adaptive Filter: Integrating Graph Topology and Node Attributes for Alleviating the Over-smoothing Problem. (arXiv:2401.14876v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#31354;&#38388;&#33258;&#36866;&#24212;&#28388;&#27874;&#22120;&#65288;CSF&#65289;&#65292;&#21487;&#20197;&#20174;&#22270;&#25299;&#25169;&#21644;&#33410;&#28857;&#23646;&#24615;&#31354;&#38388;&#20013;&#25552;&#21462;&#33258;&#36866;&#24212;&#39057;&#29575;&#20449;&#24687;&#65292;&#20197;&#20943;&#36731;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#20351;&#29992;&#20302;&#36890;&#28388;&#27874;&#22120;&#20174;&#22270;&#25299;&#25169;&#20013;&#25552;&#21462;&#20302;&#39057;&#20449;&#21495;&#65292;&#20294;&#24403;GCN&#28145;&#24230;&#22686;&#21152;&#26102;&#21487;&#33021;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#20174;&#22270;&#25299;&#25169;&#20013;&#25552;&#21462;&#30340;&#39069;&#22806;&#28388;&#27874;&#22120;&#65288;&#22914;&#39640;&#36890;&#28388;&#27874;&#22120;&#65289;&#26469;&#21019;&#24314;&#33258;&#36866;&#24212;&#28388;&#27874;&#22120;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#25299;&#25169;&#20449;&#24687;&#65292;&#24182;&#24573;&#35270;&#20102;&#33410;&#28857;&#23646;&#24615;&#31354;&#38388;&#65292;&#36825;&#20005;&#37325;&#29306;&#29298;&#20102;&#28145;&#23618;GCN&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#38750;&#21516;&#37197;&#22270;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#31354;&#38388;&#33258;&#36866;&#24212;&#28388;&#27874;&#22120;&#65292;&#31216;&#20026;CSF&#65292;&#33021;&#22815;&#20174;&#25299;&#25169;&#21644;&#23646;&#24615;&#31354;&#38388;&#20013;&#25552;&#21462;&#33258;&#36866;&#24212;&#39057;&#29575;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#22522;&#20110;&#23646;&#24615;&#30340;&#39640;&#36890;&#28388;&#27874;&#22120;&#65292;&#21487;&#20197;&#20174;&#29702;&#35770;&#19978;&#35299;&#37322;&#20026;&#21322;&#30417;&#30563;&#26680;&#23725;&#22238;&#24402;&#30340;&#26368;&#23567;&#21270;&#22120;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#22522;&#20110;&#25299;&#25169;&#30340;&#20302;&#36890;&#28388;&#27874;&#22120;&#35270;&#20026;Mercer's&#26680;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The vanilla Graph Convolutional Network (GCN) uses a low-pass filter to extract low-frequency signals from graph topology, which may lead to the over-smoothing problem when GCN goes deep. To this end, various methods have been proposed to create an adaptive filter by incorporating an extra filter (e.g., a high-pass filter) extracted from the graph topology. However, these methods heavily rely on topological information and ignore the node attribute space, which severely sacrifices the expressive power of the deep GCNs, especially when dealing with disassortative graphs. In this paper, we propose a cross-space adaptive filter, called CSF, to produce the adaptive-frequency information extracted from both the topology and attribute spaces. Specifically, we first derive a tailored attribute-based high-pass filter that can be interpreted theoretically as a minimizer for semi-supervised kernel ridge regression. Then, we cast the topology-based low-pass filter as a Mercer's kernel within the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35760;&#24518;&#30340;&#26242;&#26102;&#25552;&#31034;&#20132;&#20114;&#31574;&#30053;&#65292;&#29992;&#20110;&#25991;&#26412;-&#22270;&#20687;&#20998;&#31867;&#12290;&#35813;&#31574;&#30053;&#21463;&#21040;&#20154;&#31867;&#35760;&#24518;&#31574;&#30053;&#30340;&#21551;&#21457;&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#25805;&#20316;&#26469;&#36827;&#34892;&#27169;&#24577;&#23545;&#40784;&#21644;&#27169;&#25311;&#35760;&#24518;&#36807;&#31243;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.14856</link><description>&lt;p&gt;
&#22522;&#20110;&#35760;&#24518;&#30340;&#26242;&#26102;&#25552;&#31034;&#20132;&#20114;&#29992;&#20110;&#25991;&#26412;-&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Memory-Inspired Temporal Prompt Interaction for Text-Image Classification. (arXiv:2401.14856v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35760;&#24518;&#30340;&#26242;&#26102;&#25552;&#31034;&#20132;&#20114;&#31574;&#30053;&#65292;&#29992;&#20110;&#25991;&#26412;-&#22270;&#20687;&#20998;&#31867;&#12290;&#35813;&#31574;&#30053;&#21463;&#21040;&#20154;&#31867;&#35760;&#24518;&#31574;&#30053;&#30340;&#21551;&#21457;&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#25805;&#20316;&#26469;&#36827;&#34892;&#27169;&#24577;&#23545;&#40784;&#21644;&#27169;&#25311;&#35760;&#24518;&#36807;&#31243;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMM&#65289;&#26222;&#36941;&#20986;&#29616;&#65292;&#23558;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#25972;&#21512;&#36215;&#26469;&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;LMM&#30340;&#22686;&#21152;&#35268;&#27169;&#23548;&#33268;&#20102;&#23558;&#36825;&#20123;&#27169;&#22411;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#26174;&#30528;&#35745;&#31639;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#20132;&#20114;&#31574;&#30053;&#20197;&#26356;&#26377;&#25928;&#22320;&#23545;&#40784;&#27169;&#24577;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#20154;&#31867;&#35760;&#24518;&#31574;&#30053;&#21551;&#21457;&#30340;&#26032;&#22411;&#22522;&#20110;&#25552;&#31034;&#30340;&#22810;&#27169;&#24577;&#20132;&#20114;&#31574;&#30053;&#65292;&#21363;Memory-Inspired Temporal Prompt Interaction&#65288;MITP&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20004;&#20010;&#38454;&#27573;&#65292;&#23601;&#20687;&#20154;&#31867;&#35760;&#24518;&#31574;&#30053;&#19968;&#26679;&#65306;&#33719;&#21462;&#38454;&#27573;&#21644;&#24041;&#22266;&#28608;&#27963;&#38454;&#27573;&#12290;&#25105;&#20204;&#21033;&#29992;&#20013;&#38388;&#23618;&#19978;&#30340;&#20020;&#26102;&#25552;&#31034;&#26469;&#27169;&#25311;&#33719;&#21462;&#38454;&#27573;&#65292;&#21033;&#29992;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#25552;&#31034;&#20132;&#20114;&#26469;&#27169;&#25311;&#35760;&#24518;&#24041;&#22266;&#65292;&#20351;&#29992;&#25552;&#31034;&#29983;&#25104;&#31574;&#30053;&#26469;&#27169;&#25311;&#35760;&#24518;&#28608;&#27963;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, large-scale pre-trained multimodal models (LMM) generally emerge to integrate the vision and language modalities, achieving considerable success in various natural language processing and computer vision tasks. The growing size of LMMs, however, results in a significant computational cost for fine-tuning these models for downstream tasks. Hence, prompt-based interaction strategy is studied to align modalities more efficiently. In this contex, we propose a novel prompt-based multimodal interaction strategy inspired by human memory strategy, namely Memory-Inspired Temporal Prompt Interaction (MITP). Our proposed method involves in two stages as in human memory strategy: the acquiring stage, and the consolidation and activation stage. We utilize temporal prompts on intermediate layers to imitate the acquiring stage, leverage similarity-based prompt interaction to imitate memory consolidation, and employ prompt generation strategy to imitate memory activation. The main str
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#31890;&#24230;&#31561;&#32423;&#30340;&#23618;&#27425;&#32423;&#21035;&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#26426;&#22120;&#35270;&#35273;&#22312;&#19981;&#21516;&#29615;&#22659;&#26465;&#20214;&#19979;&#30340;&#25805;&#20316;&#12290;&#36825;&#19968;&#27169;&#22411;&#26088;&#22312;&#25552;&#20379;&#23545;&#24433;&#21709;&#26426;&#22120;&#35270;&#35273;&#21151;&#33021;&#30340;&#21508;&#20010;&#23454;&#20307;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;</title><link>http://arxiv.org/abs/2401.14831</link><description>&lt;p&gt;
&#26426;&#22120;&#35270;&#35273;&#20912;&#23665;&#30340;&#35299;&#37322;&#65306;&#36890;&#36807;&#32771;&#34385;&#20840;&#38754;&#29615;&#22659;&#26465;&#20214;&#25512;&#36827;&#21160;&#24577;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
The Machine Vision Iceberg Explained: Advancing Dynamic Testing by Considering Holistic Environmental Circumstances. (arXiv:2401.14831v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#31890;&#24230;&#31561;&#32423;&#30340;&#23618;&#27425;&#32423;&#21035;&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#26426;&#22120;&#35270;&#35273;&#22312;&#19981;&#21516;&#29615;&#22659;&#26465;&#20214;&#19979;&#30340;&#25805;&#20316;&#12290;&#36825;&#19968;&#27169;&#22411;&#26088;&#22312;&#25552;&#20379;&#23545;&#24433;&#21709;&#26426;&#22120;&#35270;&#35273;&#21151;&#33021;&#30340;&#21508;&#20010;&#23454;&#20307;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#26426;&#22120;&#35270;&#35273;&#27979;&#35797;&#26159;&#21542;&#20250;&#24102;&#26469;&#20912;&#23665;&#30340;&#21361;&#38505;&#65311;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#26426;&#22120;&#35270;&#35273;&#65288;MV&#65289;&#27979;&#35797;&#30340;&#39046;&#22495;&#65292;&#36825;&#22312;&#39640;&#24230;&#33258;&#21160;&#39550;&#39542;&#65288;HAD&#65289;&#31995;&#32479;&#20013;&#26159;&#38750;&#24120;&#24517;&#35201;&#30340;&#12290;&#20511;&#21161;&#21521;&#20912;&#23665;&#33322;&#34892;&#30340;&#38544;&#21947;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24403;&#21069;&#27979;&#35797;&#31574;&#30053;&#20013;&#28508;&#22312;&#30340;&#32570;&#38519;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#23545;&#22914;&#20309;&#22788;&#29702;MV&#22312;&#24320;&#21457;&#36807;&#31243;&#20013;&#30340;&#19981;&#36879;&#26126;&#21151;&#33021;&#30340;&#26356;&#28145;&#20837;&#20102;&#35299;&#30340;&#32039;&#36843;&#38656;&#35201;&#65292;&#22240;&#20026;&#24573;&#35270;&#20102;&#36825;&#20123;&#32771;&#34385;&#21487;&#33021;&#20250;&#36896;&#25104;&#29983;&#21629;&#30340;&#20007;&#22833;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#23618;&#27425;&#32423;&#21035;&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#31890;&#24230;&#31561;&#32423;&#12290;&#35813;&#27169;&#22411;&#40723;&#21169;&#23545;MV&#25805;&#20316;&#29615;&#22659;&#26465;&#20214;&#30340;&#21508;&#20010;&#23618;&#27425;&#36827;&#34892;&#31934;&#32454;&#25506;&#32034;&#65292;&#20174;&#20010;&#20307;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#21040;&#25972;&#20010;&#29615;&#22659;&#22330;&#26223;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#25552;&#20379;&#23545;&#21487;&#33021;&#24433;&#21709;MV&#21151;&#33021;&#30340;&#25152;&#26377;&#23454;&#20307;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are we heading for an iceberg with the current testing of machine vision? This work delves into the landscape of Machine Vision (MV) testing, which is heavily required in Highly Automated Driving (HAD) systems. Utilizing the metaphorical notion of navigating towards an iceberg, we discuss the potential shortcomings concealed within current testing strategies. We emphasize the urgent need for a deeper understanding of how to deal with the opaque functions of MV in development processes. As overlooked considerations can cost lives. Our main contribution is the hierarchical level model, which we call Granularity Grades. The model encourages a refined exploration of the multi-scaled depths of understanding about the circumstances of environments in which MV is intended to operate. This model aims to provide a holistic overview of all entities that may impact MV functions, ranging from relations of individual entities like object attributes to entire environmental scenes. The application of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#26631;&#37327;&#12289;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#20989;&#25968;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#20204;&#30340;&#19968;&#20123;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#22870;&#21169;&#20989;&#25968;&#26080;&#27861;&#34920;&#36798;&#22810;&#30446;&#26631;&#12289;&#39118;&#38505;&#25935;&#24863;&#21644;&#27169;&#24577;&#20219;&#21153;&#20013;&#30340;&#22823;&#37096;&#20998;&#23454;&#20363;&#12290;</title><link>http://arxiv.org/abs/2401.14811</link><description>&lt;p&gt;
&#20851;&#20110;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#22312;&#22810;&#30446;&#26631;&#12289;&#39118;&#38505;&#25935;&#24863;&#21644;&#27169;&#24577;&#20219;&#21153;&#20013;&#34920;&#36798;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Limitations of Markovian Rewards to Express Multi-Objective, Risk-Sensitive, and Modal Tasks. (arXiv:2401.14811v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#26631;&#37327;&#12289;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#20989;&#25968;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#20204;&#30340;&#19968;&#20123;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#22870;&#21169;&#20989;&#25968;&#26080;&#27861;&#34920;&#36798;&#22810;&#30446;&#26631;&#12289;&#39118;&#38505;&#25935;&#24863;&#21644;&#27169;&#24577;&#20219;&#21153;&#20013;&#30340;&#22823;&#37096;&#20998;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#26631;&#37327;&#12289;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#20989;&#25968;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#20204;&#30340;&#19968;&#20123;&#23616;&#38480;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31867;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65306;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#12289;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#24577;&#24378;&#21270;&#23398;&#20064;&#12290;&#23545;&#20110;&#27599;&#19968;&#31867;&#20219;&#21153;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#25551;&#36848;&#20160;&#20040;&#26679;&#30340;&#38382;&#39064;&#21487;&#20197;&#20351;&#29992;&#26631;&#37327;&#12289;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#20989;&#25968;&#26469;&#34920;&#36798;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#36825;&#19977;&#20010;&#31867;&#21035;&#20013;&#65292;&#26631;&#37327;&#12289;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#20989;&#25968;&#26080;&#27861;&#34920;&#36798;&#22823;&#37096;&#20998;&#23454;&#20363;&#12290;&#36890;&#36807;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#20026;&#20102;&#35299;&#26631;&#20934;&#22870;&#21169;&#20989;&#25968;&#21487;&#20197;&#21644;&#19981;&#33021;&#34920;&#36798;&#30340;&#20869;&#23481;&#20570;&#20986;&#20102;&#26356;&#23436;&#25972;&#30340;&#36129;&#29486;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#27169;&#24577;&#38382;&#39064;&#20316;&#20026;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#31867;&#21035;&#24341;&#20837;&#65292;&#22240;&#20026;&#22312;&#24378;&#21270;&#23398;&#20064;&#25991;&#29486;&#20013;&#65292;&#30446;&#21069;&#23578;&#26410;&#23545;&#20854;&#36827;&#34892;&#31995;&#32479;&#24615;&#30740;&#31350;&#12290;&#25105;&#20204;&#36824;&#31616;&#35201;&#27010;&#36848;&#20102;&#36890;&#36807;&#23450;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35299;&#20915;&#25152;&#35752;&#35770;&#38382;&#39064;&#30340;&#19968;&#20123;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the expressivity of scalar, Markovian reward functions in Reinforcement Learning (RL), and identify several limitations to what they can express. Specifically, we look at three classes of RL tasks; multi-objective RL, risk-sensitive RL, and modal RL. For each class, we derive necessary and sufficient conditions that describe when a problem in this class can be expressed using a scalar, Markovian reward. Moreover, we find that scalar, Markovian rewards are unable to express most of the instances in each of these three classes. We thereby contribute to a more complete understanding of what standard reward functions can and cannot express. In addition to this, we also call attention to modal problems as a new class of problems, since they have so far not been given any systematic treatment in the RL literature. We also briefly outline some approaches for solving some of the problems we discuss, by means of bespoke RL algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#37329;&#34701;&#39046;&#22495;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#26041;&#27861;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#12290;&#36890;&#36807;&#22312;&#37329;&#34701;&#25991;&#20214;&#21644;&#35828;&#26126;&#20070;&#19978;&#36827;&#34892;&#31934;&#32454;&#35843;&#20248;&#65292;&#23637;&#31034;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#30446;&#26631;&#39046;&#22495;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.14777</link><description>&lt;p&gt;
&#29992;&#20110;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Adaptation for Financial Sentiment Analysis. (arXiv:2401.14777v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#37329;&#34701;&#39046;&#22495;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#26041;&#27861;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#12290;&#36890;&#36807;&#22312;&#37329;&#34701;&#25991;&#20214;&#21644;&#35828;&#26126;&#20070;&#19978;&#36827;&#34892;&#31934;&#32454;&#35843;&#20248;&#65292;&#23637;&#31034;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#30446;&#26631;&#39046;&#22495;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#22312;&#37329;&#34701;&#26426;&#26500;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;&#20844;&#21496;&#21644;&#24066;&#22330;&#30340;&#37329;&#34701;&#25991;&#20214;&#30340;&#39640;&#24230;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#37329;&#34701;&#39046;&#22495;&#30340;&#26223;&#35266;&#23545;NLP&#25552;&#20986;&#20102;&#39069;&#22806;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#25991;&#26412;&#30340;&#22797;&#26434;&#24615;&#21644;&#29305;&#23450;&#26415;&#35821;&#30340;&#20351;&#29992;&#12290;&#21363;&#20351;&#20351;&#29992;&#20855;&#26377;&#20986;&#33394;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#22312;&#19987;&#38376;&#38024;&#23545;&#37329;&#34701;&#30340;&#20219;&#21153;&#19978;&#20063;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#20851;&#20110;&#20197;&#37329;&#34701;&#39046;&#22495;&#20026;&#30446;&#26631;&#30340;LLM&#36866;&#24212;&#26041;&#27861;&#30740;&#31350;&#65292;&#24182;&#39640;&#24230;&#20851;&#27880;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#21508;&#31181;&#24191;&#27867;&#30340;&#31574;&#30053;&#26469;&#36866;&#24212;&#20004;&#20010;&#21442;&#25968;&#20302;&#20110;15&#20159;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#22312;&#37329;&#34701;&#25991;&#26723;&#21644;&#35828;&#26126;&#20070;&#19978;&#36827;&#34892;&#31934;&#32454;&#35843;&#20248;&#65292;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#36866;&#24212;&#30446;&#26631;&#39046;&#22495;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23567;&#22411;LLM&#20855;&#26377;&#30456;&#20284;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language processing (NLP) has recently gained relevance within financial institutions by providing highly valuable insights into companies and markets' financial documents. However, the landscape of the financial domain presents extra challenges for NLP, due to the complexity of the texts and the use of specific terminology. Generalist language models tend to fall short in tasks specifically tailored for finance, even when using large language models (LLMs) with great natural language understanding and generative capabilities. This paper presents a study on LLM adaptation methods targeted at the financial domain and with high emphasis on financial sentiment analysis. To this purpose, two foundation models with less than 1.5B parameters have been adapted using a wide range of strategies. We show that through careful fine-tuning on both financial documents and instructions, these foundation models can be adapted to the target domain. Moreover, we observe that small LLMs have comp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#19981;&#22343;&#21248;&#20998;&#24067;&#30340;&#30005;&#33655;&#21644;&#19981;&#35268;&#21017;&#32593;&#26684;&#19978;&#23454;&#29616;&#33021;&#37327;&#27169;&#22411;&#22343;&#34913;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;QC-LDPC&#30721;&#21644;&#29627;&#23572;&#20857;&#26364;&#26426;&#65292;&#23558;&#31995;&#32479;&#30340;&#32500;&#24230;&#25193;&#23637;&#65292;&#23558;&#30005;&#33655;&#26367;&#25442;&#20026;&#24490;&#29615;&#29289;&#36136;&#65292;&#24182;&#36890;&#36807;&#24490;&#29615;&#31227;&#20301;&#34920;&#31034;&#36317;&#31163;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#19981;&#35268;&#21017;&#32593;&#26684;&#36716;&#21270;&#20026;&#22343;&#21248;&#37197;&#32622;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;&#35813;&#26041;&#27861;&#36824;&#35299;&#20915;&#20102;&#20195;&#30721;&#22312;&#22270;&#24418;&#27010;&#29575;&#27169;&#22411;&#20013;&#30340;&#35780;&#20272;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#19981;&#21516;&#25299;&#25169;&#19979;&#29627;&#23572;&#20857;&#26364;&#26426;&#36798;&#21040;&#22343;&#34913;&#29366;&#24577;&#30340;&#20005;&#26684;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2401.14749</link><description>&lt;p&gt;
&#22522;&#20110;&#25299;&#25169;&#24863;&#30693;&#30340;&#33021;&#37327;&#27169;&#22411;&#22343;&#34913;&#25506;&#32034;&#65306;Toric QC-LDPC&#30721;&#21644;Hyperbolic MET QC-LDPC&#30721;
&lt;/p&gt;
&lt;p&gt;
Topology-Aware Exploration of Energy-Based Models Equilibrium: Toric QC-LDPC Codes and Hyperbolic MET QC-LDPC Codes. (arXiv:2401.14749v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#19981;&#22343;&#21248;&#20998;&#24067;&#30340;&#30005;&#33655;&#21644;&#19981;&#35268;&#21017;&#32593;&#26684;&#19978;&#23454;&#29616;&#33021;&#37327;&#27169;&#22411;&#22343;&#34913;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;QC-LDPC&#30721;&#21644;&#29627;&#23572;&#20857;&#26364;&#26426;&#65292;&#23558;&#31995;&#32479;&#30340;&#32500;&#24230;&#25193;&#23637;&#65292;&#23558;&#30005;&#33655;&#26367;&#25442;&#20026;&#24490;&#29615;&#29289;&#36136;&#65292;&#24182;&#36890;&#36807;&#24490;&#29615;&#31227;&#20301;&#34920;&#31034;&#36317;&#31163;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#19981;&#35268;&#21017;&#32593;&#26684;&#36716;&#21270;&#20026;&#22343;&#21248;&#37197;&#32622;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;&#35813;&#26041;&#27861;&#36824;&#35299;&#20915;&#20102;&#20195;&#30721;&#22312;&#22270;&#24418;&#27010;&#29575;&#27169;&#22411;&#20013;&#30340;&#35780;&#20272;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#19981;&#21516;&#25299;&#25169;&#19979;&#29627;&#23572;&#20857;&#26364;&#26426;&#36798;&#21040;&#22343;&#34913;&#29366;&#24577;&#30340;&#20005;&#26684;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#22343;&#21248;&#20998;&#24067;&#30340;&#30005;&#33655;&#21644;&#19981;&#35268;&#21017;&#32593;&#26684;&#19978;&#23454;&#29616;ISING&#21704;&#23494;&#39039;&#37327;&#22343;&#34913;&#30340;&#26041;&#27861;&#12290;&#37319;&#29992;&#65288;&#22810;&#36793;&#32536;&#65289;QC-LDPC&#30721;&#21644;&#29627;&#23572;&#20857;&#26364;&#26426;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#23545;&#31995;&#32479;&#30340;&#32500;&#24230;&#25193;&#23637;&#65292;&#29992;&#24490;&#29615;&#29289;&#36136;&#26367;&#20195;&#30005;&#33655;&#65292;&#24182;&#36890;&#36807;&#24490;&#29615;&#31227;&#20301;&#34920;&#31034;&#36317;&#31163;&#12290;&#36825;&#23548;&#33268;&#30005;&#33655;&#31995;&#32479;&#22312;&#31354;&#38388;&#19978;&#30340;&#31995;&#32479;&#26144;&#23556;&#65292;&#23558;&#19981;&#35268;&#21017;&#32593;&#26684;&#36716;&#21270;&#20026;&#22343;&#21248;&#37197;&#32622;&#65292;&#36866;&#29992;&#20110;Torical&#21644;Circular Hyperboloid&#25299;&#25169;&#12290;&#26412;&#25991;&#28085;&#30422;&#20102;&#19982;QC-LDPC&#30721;&#12289;&#22810;&#36793;&#32536;QC-LDPC&#30721;&#21644;&#29627;&#23572;&#20857;&#26364;&#26426;&#30456;&#20851;&#30340;&#22522;&#26412;&#23450;&#20041;&#21644;&#31526;&#21495;&#12290;&#23427;&#25506;&#35752;&#20102;&#29992;&#20110;&#35780;&#20272;&#20998;&#21306;&#20989;&#25968;&#30340;&#20195;&#30721;&#22312;&#22270;&#24418;&#27010;&#29575;&#27169;&#22411;&#20013;&#30340;&#36793;&#38469;&#21270;&#38382;&#39064;&#65292;&#21253;&#25324;&#31934;&#30830;&#21644;&#36817;&#20284;&#20272;&#35745;&#25216;&#26415;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#35777;&#26126;&#65292;&#35777;&#26126;&#20102;&#22312;Torical&#21644;Circular Hyper&#25299;&#25169;&#19979;&#65292;&#29627;&#23572;&#20857;&#26364;&#26426;&#21487;&#20197;&#36798;&#21040;&#22343;&#34913;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a method for achieving equilibrium in the ISING Hamiltonian when confronted with unevenly distributed charges on an irregular grid. Employing (Multi-Edge) QC-LDPC codes and the Boltzmann machine, our approach involves dimensionally expanding the system, substituting charges with circulants, and representing distances through circulant shifts. This results in a systematic mapping of the charge system onto a space, transforming the irregular grid into a uniform configuration, applicable to Torical and Circular Hyperboloid Topologies. The paper covers fundamental definitions and notations related to QC-LDPC Codes, Multi-Edge QC-LDPC codes, and the Boltzmann machine. It explores the marginalization problem in code on the graph probabilistic models for evaluating the partition function, encompassing exact and approximate estimation techniques. Rigorous proof is provided for the attainability of equilibrium states for the Boltzmann machine under Torical and Circular Hyper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21512;&#25104;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#32467;&#21512;&#20102;3D&#34394;&#25311;&#31354;&#38388;&#27169;&#25311;&#22120;&#30340;&#35270;&#39057;&#25968;&#25454;&#21644;&#25551;&#36848;&#27963;&#21160;&#26102;&#31354;&#32972;&#26223;&#30340;&#30693;&#35782;&#22270;&#65292;&#26088;&#22312;&#35299;&#20915;&#23478;&#24237;&#29615;&#22659;&#20013;&#30340;&#21361;&#38505;&#24773;&#20917;&#65292;&#24182;&#25552;&#20379;&#32473;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#20316;&#20026;&#36164;&#28304;&#26469;&#24320;&#21457;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#25552;&#21319;&#23433;&#20840;&#21644;&#31119;&#31049;&#12290;</title><link>http://arxiv.org/abs/2401.14743</link><description>&lt;p&gt;
&#29992;&#20110;&#22686;&#24378;&#23478;&#24237;&#29615;&#22659;&#23433;&#20840;&#21644;&#31119;&#31049;&#30340;&#21512;&#25104;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Synthetic Multimodal Dataset for Empowering Safety and Well-being in Home Environments. (arXiv:2401.14743v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21512;&#25104;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#32467;&#21512;&#20102;3D&#34394;&#25311;&#31354;&#38388;&#27169;&#25311;&#22120;&#30340;&#35270;&#39057;&#25968;&#25454;&#21644;&#25551;&#36848;&#27963;&#21160;&#26102;&#31354;&#32972;&#26223;&#30340;&#30693;&#35782;&#22270;&#65292;&#26088;&#22312;&#35299;&#20915;&#23478;&#24237;&#29615;&#22659;&#20013;&#30340;&#21361;&#38505;&#24773;&#20917;&#65292;&#24182;&#25552;&#20379;&#32473;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#20316;&#20026;&#36164;&#28304;&#26469;&#24320;&#21457;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#25552;&#21319;&#23433;&#20840;&#21644;&#31119;&#31049;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21512;&#25104;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#23558;&#26469;&#33258;3D&#34394;&#25311;&#31354;&#38388;&#27169;&#25311;&#22120;&#30340;&#35270;&#39057;&#25968;&#25454;&#19982;&#25551;&#36848;&#27963;&#21160;&#30340;&#26102;&#31354;&#32972;&#26223;&#30340;&#30693;&#35782;&#22270;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#20026;&#31038;&#20250;&#38382;&#39064;&#30693;&#35782;&#22270;&#25512;&#29702;&#25361;&#25112;&#65288;KGRC4SI&#65289;&#24320;&#21457;&#30340;&#65292;&#37325;&#28857;&#26159;&#35782;&#21035;&#21644;&#35299;&#20915;&#23478;&#24237;&#29615;&#22659;&#20013;&#30340;&#21361;&#38505;&#24773;&#20917;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20379;&#20844;&#20247;&#20351;&#29992;&#65292;&#20316;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#24320;&#21457;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#30340;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#65292;&#20197;&#35782;&#21035;&#20154;&#31867;&#34892;&#20026;&#24182;&#25552;&#21319;&#23433;&#20840;&#21644;&#31119;&#31049;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a synthetic multimodal dataset of daily activities that fuses video data from a 3D virtual space simulator with knowledge graphs depicting the spatiotemporal context of the activities. The dataset is developed for the Knowledge Graph Reasoning Challenge for Social Issues (KGRC4SI), which focuses on identifying and addressing hazardous situations in the home environment. The dataset is available to the public as a valuable resource for researchers and practitioners developing innovative solutions recognizing human behaviors to enhance safety and well-being in
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22768;&#23398;&#21644;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20132;&#26367;&#21644;&#22238;&#24212;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#36825;&#20004;&#31181;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#21475;&#35821;&#23545;&#35805;&#20013;&#23454;&#29616;&#26356;&#21152;&#33258;&#28982;&#21644;&#23545;&#35805;&#24335;&#30340;&#20114;&#21160;&#12290;</title><link>http://arxiv.org/abs/2401.14717</link><description>&lt;p&gt;
&#29992;&#22768;&#23398;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34701;&#21512;&#36827;&#34892;&#20132;&#26367;&#21644;&#22238;&#24212;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Turn-taking and Backchannel Prediction with Acoustic and Large Language Model Fusion. (arXiv:2401.14717v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14717
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22768;&#23398;&#21644;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20132;&#26367;&#21644;&#22238;&#24212;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#36825;&#20004;&#31181;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#21475;&#35821;&#23545;&#35805;&#20013;&#23454;&#29616;&#26356;&#21152;&#33258;&#28982;&#21644;&#23545;&#35805;&#24335;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#39044;&#27979;&#21475;&#35821;&#23545;&#35805;&#20013;&#20132;&#26367;&#21644;&#22238;&#24212;&#20301;&#32622;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#31070;&#32463;&#22768;&#23398;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#22312;Switchboard&#20154;&#38469;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#21333;&#27169;&#24577;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#25351;&#20196;&#24494;&#35843;&#31574;&#30053;&#65292;&#20197;&#36827;&#19968;&#27493;&#20174;LLM&#32534;&#30721;&#30340;&#30693;&#35782;&#20013;&#21463;&#30410;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#20351;&#29992;&#32452;&#21512;&#30340;LLM&#21644;&#22768;&#23398;&#27169;&#22411;&#22312;&#20154;&#31867;&#21644;&#35821;&#38899;AI&#20195;&#29702;&#20043;&#38388;&#23454;&#29616;&#26356;&#33258;&#28982;&#21644;&#23545;&#35805;&#24335;&#20114;&#21160;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an approach for continuous prediction of turn-taking and backchanneling locations in spoken dialogue by fusing a neural acoustic model with a large language model (LLM). Experiments on the Switchboard human-human conversation dataset demonstrate that our approach consistently outperforms the baseline models with single modality. We also develop a novel multi-task instruction fine-tuning strategy to further benefit from LLM-encoded knowledge for understanding the tasks and conversational contexts, leading to additional improvements. Our approach demonstrates the potential of combined LLMs and acoustic models for a more natural and conversational interaction between humans and speech-enabled AI agents.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29305;&#24449;&#35299;&#32544;&#26469;&#32531;&#35299;&#23545;&#25239;&#40065;&#26834;&#24615;&#20013;&#29305;&#24449;&#24046;&#36317;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26126;&#30830;&#24314;&#27169;&#21644;&#28040;&#38500;&#23548;&#33268;&#29305;&#24449;&#24046;&#36317;&#30340;&#28508;&#22312;&#29305;&#24449;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.14707</link><description>&lt;p&gt;
&#36890;&#36807;&#29305;&#24449;&#35299;&#32544;&#26469;&#32531;&#35299;&#23545;&#25239;&#40065;&#26834;&#24615;&#20013;&#30340;&#29305;&#24449;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Mitigating Feature Gap for Adversarial Robustness by Feature Disentanglement. (arXiv:2401.14707v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14707
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29305;&#24449;&#35299;&#32544;&#26469;&#32531;&#35299;&#23545;&#25239;&#40065;&#26834;&#24615;&#20013;&#29305;&#24449;&#24046;&#36317;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26126;&#30830;&#24314;&#27169;&#21644;&#28040;&#38500;&#23548;&#33268;&#29305;&#24449;&#24046;&#36317;&#30340;&#28508;&#22312;&#29305;&#24449;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#23545;&#25239;&#26679;&#26412;&#24456;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#23545;&#25239;&#24494;&#35843;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#23545;&#24050;&#32463;&#22312;&#33258;&#28982;&#24773;&#20917;&#19979;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#24335;&#24494;&#35843;&#26469;&#25552;&#21319;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#23545;&#25239;&#26679;&#26412;&#20013;&#30340;&#19968;&#20123;&#28508;&#22312;&#29305;&#24449;&#34987;&#23545;&#25239;&#25200;&#21160;&#25152;&#28151;&#28102;&#65292;&#24182;&#23548;&#33268;&#33258;&#28982;&#26679;&#26412;&#21644;&#23545;&#25239;&#26679;&#26412;&#22312;&#26368;&#21518;&#19968;&#23618;&#38544;&#34255;&#23618;&#30340;&#29305;&#24449;&#20043;&#38388;&#20986;&#29616;&#24847;&#22806;&#22686;&#21152;&#30340;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#32544;&#30340;&#26041;&#27861;&#26469;&#26126;&#30830;&#24314;&#27169;&#21644;&#36827;&#19968;&#27493;&#28040;&#38500;&#23548;&#33268;&#29305;&#24449;&#24046;&#36317;&#30340;&#28508;&#22312;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29305;&#24449;&#35299;&#32544;&#22120;&#65292;&#23558;&#23545;&#25239;&#26679;&#26412;&#30340;&#28508;&#22312;&#29305;&#24449;&#19982;&#23545;&#25239;&#26679;&#26412;&#30340;&#29305;&#24449;&#20998;&#31163;&#24320;&#26469;&#65292;&#20174;&#32780;&#36890;&#36807;&#28040;&#38500;&#28508;&#22312;&#29305;&#24449;&#26469;&#25552;&#21319;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#29305;&#24449;&#19982;&#23545;&#25239;&#26679;&#26412;&#22312;&#24494;&#35843;&#27169;&#22411;&#20013;&#30340;&#29305;&#24449;&#23545;&#40784;&#65292;&#36827;&#19968;&#27493;&#20174;&#33258;&#28982;&#26679;&#26412;&#30340;&#29305;&#24449;&#20013;&#33719;&#30410;&#65292;&#36991;&#20813;&#28151;&#28102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are vulnerable to adversarial samples. Adversarial fine-tuning methods aim to enhance adversarial robustness through fine-tuning the naturally pre-trained model in an adversarial training manner. However, we identify that some latent features of adversarial samples are confused by adversarial perturbation and lead to an unexpectedly increasing gap between features in the last hidden layer of natural and adversarial samples. To address this issue, we propose a disentanglement-based approach to explicitly model and further remove the latent features that cause the feature gap. Specifically, we introduce a feature disentangler to separate out the latent features from the features of the adversarial samples, thereby boosting robustness by eliminating the latent features. Besides, we align features in the pre-trained model with features of adversarial samples in the fine-tuned model, to further benefit from the features from natural samples without confusion. Empirical 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FairSample&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#39640;&#25928;&#35757;&#32451;&#20844;&#24179;&#20934;&#30830;&#30340;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23545;&#22270;&#32467;&#26500;&#36827;&#34892;&#32416;&#27491;&#65292;&#24182;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#37051;&#23621;&#37319;&#26679;&#31574;&#30053;&#26469;&#21516;&#26102;&#20943;&#36731;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22270;&#32467;&#26500;&#20559;&#35265;&#12289;&#33410;&#28857;&#23646;&#24615;&#20559;&#35265;&#21644;&#27169;&#22411;&#21442;&#25968;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2401.14702</link><description>&lt;p&gt;
FairSample: &#39640;&#25928;&#35757;&#32451;&#20844;&#24179;&#20934;&#30830;&#30340;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
FairSample: Training Fair and Accurate Graph Convolutional Neural Networks Efficiently. (arXiv:2401.14702v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FairSample&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#39640;&#25928;&#35757;&#32451;&#20844;&#24179;&#20934;&#30830;&#30340;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23545;&#22270;&#32467;&#26500;&#36827;&#34892;&#32416;&#27491;&#65292;&#24182;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#37051;&#23621;&#37319;&#26679;&#31574;&#30053;&#26469;&#21516;&#26102;&#20943;&#36731;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22270;&#32467;&#26500;&#20559;&#35265;&#12289;&#33410;&#28857;&#23646;&#24615;&#20559;&#35265;&#21644;&#27169;&#22411;&#21442;&#25968;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#65292;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20844;&#24179;&#24615;&#36234;&#26469;&#36234;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#20013;&#23384;&#22312;&#23545;&#25935;&#24863;&#32676;&#20307;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20844;&#24179;&#24615;&#30340;&#32463;&#20856;&#27010;&#24565;&#8220;&#20154;&#21475;&#32479;&#35745;&#24179;&#22343;&#20540;&#8221;&#65292;&#24182;&#35299;&#20915;&#20102;&#39640;&#25928;&#35757;&#32451;&#20844;&#24179;&#20934;&#30830;&#30340;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#23545;&#22270;&#32467;&#26500;&#20559;&#35265;&#12289;&#33410;&#28857;&#23646;&#24615;&#20559;&#35265;&#21644;&#27169;&#22411;&#21442;&#25968;&#23545;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20154;&#21475;&#32479;&#35745;&#24179;&#22343;&#24615;&#33021;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#27934;&#23519;&#21147;&#23548;&#33268;&#20102;FairSample&#65292;&#19968;&#20010;&#21487;&#20197;&#21516;&#26102;&#20943;&#36731;&#36825;&#19977;&#31181;&#20559;&#35265;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#31181;&#30452;&#35266;&#30340;&#31574;&#30053;&#26469;&#32416;&#27491;&#22270;&#32467;&#26500;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#19981;&#21516;&#25935;&#24863;&#32676;&#20307;&#20294;&#20855;&#26377;&#30456;&#20284;&#33410;&#28857;&#29305;&#24449;&#30340;&#33410;&#28857;&#20043;&#38388;&#25554;&#20837;&#36793;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#22686;&#24378;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#24182;&#20445;&#25345;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#37051;&#23621;&#37319;&#26679;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairness in Graph Convolutional Neural Networks (GCNs) becomes a more and more important concern as GCNs are adopted in many crucial applications. Societal biases against sensitive groups may exist in many real world graphs. GCNs trained on those graphs may be vulnerable to being affected by such biases. In this paper, we adopt the well-known fairness notion of demographic parity and tackle the challenge of training fair and accurate GCNs efficiently. We present an in-depth analysis on how graph structure bias, node attribute bias, and model parameters may affect the demographic parity of GCNs. Our insights lead to FairSample, a framework that jointly mitigates the three types of biases. We employ two intuitive strategies to rectify graph structures. First, we inject edges across nodes that are in different sensitive groups but similar in node features. Second, to enhance model fairness and retain model quality, we develop a learnable neighbor sampling policy using reinforcement learni
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#30340;&#20154;&#24037;&#25968;&#25454;&#30340;&#36861;&#36394;&#30740;&#31350;&#65292;&#23558;&#21508;&#31181;&#31867;&#22411;&#30340;LLM&#29983;&#25104;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#20102;&#27719;&#24635;&#21644;&#27979;&#35797;&#65292;&#24182;&#25581;&#31034;&#20102;&#38544;&#34255;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#38382;&#39064;&#12290;&#36825;&#26159;&#31532;&#19968;&#27425;&#23545;LLM&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#32508;&#21512;&#20998;&#26512;&#21644;&#27604;&#36739;&#65292;&#24182;&#24341;&#21457;&#20102;&#23545;&#20154;&#24037;&#25968;&#25454;&#36136;&#37327;&#30340;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2401.14698</link><description>&lt;p&gt;
&#21453;&#24605;LLM&#29983;&#25104;&#25968;&#25454;&#30340;&#30495;&#23454;&#24615;&#65306;&#23545;LLM&#29983;&#25104;&#25968;&#25454;&#30340;&#36861;&#36394;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Under the Surface: Tracking the Artifactuality of LLM-Generated Data. (arXiv:2401.14698v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#30340;&#20154;&#24037;&#25968;&#25454;&#30340;&#36861;&#36394;&#30740;&#31350;&#65292;&#23558;&#21508;&#31181;&#31867;&#22411;&#30340;LLM&#29983;&#25104;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#20102;&#27719;&#24635;&#21644;&#27979;&#35797;&#65292;&#24182;&#25581;&#31034;&#20102;&#38544;&#34255;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#38382;&#39064;&#12290;&#36825;&#26159;&#31532;&#19968;&#27425;&#23545;LLM&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#32508;&#21512;&#20998;&#26512;&#21644;&#27604;&#36739;&#65292;&#24182;&#24341;&#21457;&#20102;&#23545;&#20154;&#24037;&#25968;&#25454;&#36136;&#37327;&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29983;&#25104;&#20154;&#24037;&#25968;&#25454;&#26041;&#38754;&#30340;&#19981;&#26029;&#25193;&#22823;&#30340;&#20316;&#29992;&#12290;LLM&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#29983;&#25104;&#22810;&#31181;&#36755;&#20986;&#65292;&#21253;&#25324;&#27880;&#37322;&#12289;&#20559;&#22909;&#12289;&#25351;&#20196;&#25552;&#31034;&#12289;&#27169;&#25311;&#23545;&#35805;&#21644;&#33258;&#30001;&#25991;&#26412;&#12290;&#30001;&#20110;&#36825;&#20123;LLM&#29983;&#25104;&#25968;&#25454;&#24418;&#24335;&#22312;&#24212;&#29992;&#20013;&#32463;&#24120;&#20132;&#21449;&#65292;&#23427;&#20204;&#30456;&#20114;&#24433;&#21709;&#65292;&#24182;&#24341;&#21457;&#20102;&#23545;&#35757;&#32451;&#24490;&#29615;&#20013;&#21512;&#24182;&#30340;&#20154;&#24037;&#25968;&#25454;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#37325;&#22823;&#20851;&#27880;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#20154;&#24037;&#25968;&#25454;&#29983;&#24577;&#31995;&#32479;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#39033;&#30740;&#31350;&#23558;&#21508;&#31181;&#31867;&#22411;&#30340;LLM&#29983;&#25104;&#25991;&#26412;&#25968;&#25454;&#27719;&#24635;&#36215;&#26469;&#65292;&#20174;&#26356;&#20005;&#26684;&#21463;&#38480;&#30340;&#25968;&#25454;&#22914;&#8220;&#20219;&#21153;&#26631;&#31614;&#8221;&#21040;&#26356;&#33258;&#30001;&#30340;&#8220;&#33258;&#30001;&#25991;&#26412;&#8221;&#12290;&#28982;&#21518;&#25105;&#20204;&#23545;LLM&#29983;&#25104;&#30340;&#20154;&#24037;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#24433;&#21709;&#36827;&#34892;&#20102;&#21387;&#21147;&#27979;&#35797;&#65292;&#24182;&#19982;&#20154;&#24037;&#25968;&#25454;&#22312;&#21508;&#31181;&#29616;&#26377;&#22522;&#20934;&#19978;&#36827;&#34892;&#27604;&#36739;&#12290;&#23613;&#31649;&#20154;&#24037;&#25968;&#25454;&#33021;&#22815;&#21305;&#37197;&#20154;&#31867;&#34920;&#29616;&#65292;&#20294;&#26412;&#25991;&#25581;&#31034;&#20102;&#38544;&#34255;&#30340;&#24040;&#22823;&#38544;&#24739;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work delves into the expanding role of large language models (LLMs) in generating artificial data. LLMs are increasingly employed to create a variety of outputs, including annotations, preferences, instruction prompts, simulated dialogues, and free text. As these forms of LLM-generated data often intersect in their application, they exert mutual influence on each other and raise significant concerns about the quality and diversity of the artificial data incorporated into training cycles, leading to an artificial data ecosystem. To the best of our knowledge, this is the first study to aggregate various types of LLM-generated text data, from more tightly constrained data like "task labels" to more lightly constrained "free-form text". We then stress test the quality and implications of LLM-generated artificial data, comparing it with human data across various existing benchmarks. Despite artificial data's capability to match human performance, this paper reveals significant hidden d
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#28176;&#36827;&#20013;&#28857;&#28151;&#21512;&#26041;&#27861;&#26469;&#35299;&#20915;&#31895;&#32454;&#36716;&#31227;&#23398;&#20064;&#20013;&#30340;&#31867;&#20869;&#23849;&#22604;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36880;&#28176;&#23558;&#22686;&#24378;&#29305;&#24449;&#31227;&#21160;&#33267;&#31867;&#38388;&#29305;&#24449;&#23545;&#30340;&#20013;&#28857;&#65292;&#23454;&#29616;&#20102;&#36793;&#36317;&#24179;&#34913;&#20197;&#21450;&#36866;&#24230;&#25193;&#23637;&#36793;&#36317;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.14696</link><description>&lt;p&gt;
&#28176;&#36827;&#20013;&#28857;&#28151;&#21512;&#29992;&#20110;&#36793;&#36317;&#24179;&#34913;&#21644;&#36866;&#24230;&#25193;&#23637;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Asymptotic Midpoint Mixup for Margin Balancing and Moderate Broadening. (arXiv:2401.14696v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14696
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#28176;&#36827;&#20013;&#28857;&#28151;&#21512;&#26041;&#27861;&#26469;&#35299;&#20915;&#31895;&#32454;&#36716;&#31227;&#23398;&#20064;&#20013;&#30340;&#31867;&#20869;&#23849;&#22604;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36880;&#28176;&#23558;&#22686;&#24378;&#29305;&#24449;&#31227;&#21160;&#33267;&#31867;&#38388;&#29305;&#24449;&#23545;&#30340;&#20013;&#28857;&#65292;&#23454;&#29616;&#20102;&#36793;&#36317;&#24179;&#34913;&#20197;&#21450;&#36866;&#24230;&#25193;&#23637;&#36793;&#36317;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#65292;&#29305;&#24449;&#20043;&#38388;&#30340;&#23849;&#22604;&#23548;&#33268;&#20102;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#19968;&#20123;&#20851;&#38190;&#38382;&#39064;&#65292;&#20351;&#24471;&#29305;&#24449;&#26080;&#27861;&#21306;&#20998;&#12290;&#22522;&#20110;&#25554;&#20540;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#22914;mixup&#65292;&#22312;&#20943;&#36731;&#19981;&#21516;&#31867;&#21035;&#20043;&#38388;&#30340;&#23849;&#22604;&#38382;&#39064;&#19978;&#26174;&#31034;&#20986;&#20102;&#26377;&#25928;&#24615;&#65292;&#36825;&#34987;&#31216;&#20026;&#31867;&#38388;&#23849;&#22604;&#12290;&#28982;&#32780;&#65292;&#31895;&#32454;&#36716;&#31227;&#23398;&#20064;&#20013;&#24341;&#36215;&#30340;&#31867;&#20869;&#23849;&#22604;&#24182;&#26410;&#22312;&#22686;&#24378;&#26041;&#27861;&#20013;&#35752;&#35770;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#22909;&#30340;&#29305;&#24449;&#22686;&#24378;&#26041;&#27861;&#65292;&#21363;&#28176;&#36827;&#20013;&#28857;&#28151;&#21512;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25554;&#20540;&#29983;&#25104;&#22686;&#24378;&#29305;&#24449;&#65292;&#20294;&#23558;&#23427;&#20204;&#36880;&#28176;&#31227;&#21160;&#21040;&#31867;&#38388;&#29305;&#24449;&#23545;&#30340;&#20013;&#28857;&#12290;&#32467;&#26524;&#26159;&#65292;&#35813;&#26041;&#27861;&#20135;&#29983;&#20102;&#20004;&#20010;&#25928;&#26524;&#65306;1&#65289;&#24179;&#34913;&#25152;&#26377;&#31867;&#21035;&#30340;&#36793;&#36317;&#65292;2&#65289;&#20165;&#36866;&#24230;&#25193;&#23637;&#36793;&#36317;&#65292;&#30452;&#21040;&#36798;&#21040;&#26368;&#22823;&#32622;&#20449;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;&#21487;&#35270;&#21270;&#34920;&#31034;&#30340;&#23545;&#40784;&#24615;&#21644;&#22343;&#21248;&#24615;&#26469;&#32463;&#39564;&#20998;&#26512;&#20102;&#23849;&#22604;&#25928;&#24212;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#22686;&#24378;&#26041;&#27861;&#25152;&#24102;&#26469;&#30340;&#31867;&#20869;&#23849;&#22604;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the feature space, the collapse between features invokes critical problems in representation learning by remaining the features undistinguished. Interpolation-based augmentation methods such as mixup have shown their effectiveness in relieving the collapse problem between different classes, called inter-class collapse. However, intra-class collapse raised in coarse-to-fine transfer learning has not been discussed in the augmentation approach. To address them, we propose a better feature augmentation method, asymptotic midpoint mixup. The method generates augmented features by interpolation but gradually moves them toward the midpoint of inter-class feature pairs. As a result, the method induces two effects: 1) balancing the margin for all classes and 2) only moderately broadening the margin until it holds maximal confidence. We empirically analyze the collapse effects by measuring alignment and uniformity with visualizing representations. Then, we validate the intra-class collapse e
&lt;/p&gt;</description></item><item><title>TA-RNN&#21644;TA-RNN-AE&#26159;&#20004;&#31181;&#22522;&#20110;RNN&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#20998;&#26512;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#24182;&#39044;&#27979;&#24739;&#32773;&#30340;&#20020;&#24202;&#32467;&#26524;&#12290;&#36825;&#20123;&#26550;&#26500;&#32771;&#34385;&#20102;EHR&#25968;&#25454;&#30340;&#19981;&#35268;&#21017;&#24615;&#21644;&#26102;&#38388;&#38388;&#38548;&#65292;&#24182;&#37319;&#29992;&#26102;&#38388;&#23884;&#20837;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14694</link><description>&lt;p&gt;
TA-RNN&#65306;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#38754;&#21521;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#26102;&#38388;&#24863;&#30693;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
TA-RNN: an Attention-based Time-aware Recurrent Neural Network Architecture for Electronic Health Records. (arXiv:2401.14694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14694
&lt;/p&gt;
&lt;p&gt;
TA-RNN&#21644;TA-RNN-AE&#26159;&#20004;&#31181;&#22522;&#20110;RNN&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#20998;&#26512;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#24182;&#39044;&#27979;&#24739;&#32773;&#30340;&#20020;&#24202;&#32467;&#26524;&#12290;&#36825;&#20123;&#26550;&#26500;&#32771;&#34385;&#20102;EHR&#25968;&#25454;&#30340;&#19981;&#35268;&#21017;&#24615;&#21644;&#26102;&#38388;&#38388;&#38548;&#65292;&#24182;&#37319;&#29992;&#26102;&#38388;&#23884;&#20837;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#26426;&#65306;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#26159;&#24739;&#32773;&#21307;&#30103;&#21382;&#21490;&#30340;&#20840;&#38754;&#36164;&#28304;&#12290;EHR&#23545;&#20110;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#31561;&#20808;&#36827;&#25216;&#26415;&#33267;&#20851;&#37325;&#35201;&#65292;&#20351;&#21307;&#30103;&#25552;&#20379;&#32773;&#33021;&#22815;&#20998;&#26512;&#22823;&#37327;&#25968;&#25454;&#65292;&#25552;&#21462;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#24182;&#20570;&#20986;&#31934;&#30830;&#12289;&#25968;&#25454;&#39537;&#21160;&#30340;&#20020;&#24202;&#20915;&#31574;&#12290;DL&#26041;&#27861;&#22914;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#24050;&#34987;&#29992;&#20110;&#20998;&#26512;EHR&#20197;&#24314;&#27169;&#30142;&#30149;&#36827;&#23637;&#24182;&#39044;&#27979;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24182;&#27809;&#26377;&#35299;&#20915;EHR&#25968;&#25454;&#20013;&#19968;&#20123;&#22266;&#26377;&#30340;&#19981;&#35268;&#21017;&#24615;&#65292;&#22914;&#20020;&#24202;&#35775;&#38382;&#20043;&#38388;&#30340;&#19981;&#35268;&#21017;&#26102;&#38388;&#38388;&#38548;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;DL&#27169;&#22411;&#37117;&#19981;&#21487;&#35299;&#37322;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;RNN&#30340;&#21487;&#35299;&#37322;DL&#26550;&#26500;&#65292;&#20998;&#21035;&#26159;&#26102;&#38388;&#24863;&#30693;RNN&#65288;TA-RNN&#65289;&#21644;TA-RNN-Autoencoder&#65288;TA-RNN-AE&#65289;&#65292;&#29992;&#20110;&#39044;&#27979;&#19979;&#19968;&#27425;&#35775;&#38382;&#21644;&#22810;&#27425;&#26410;&#26469;&#35775;&#38382;&#20013;&#24739;&#32773;&#30340;&#20020;&#24202;&#32467;&#26524;&#12290;&#20026;&#20102;&#20943;&#36731;&#19981;&#35268;&#21017;&#26102;&#38388;&#38388;&#38548;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26102;&#38388;&#23884;&#20837;&#30340;&#26041;&#27861;&#23558;&#26102;&#38388;&#20449;&#24687;&#32435;&#20837;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivation: Electronic Health Records (EHR) represent a comprehensive resource of a patient's medical history. EHR are essential for utilizing advanced technologies such as deep learning (DL), enabling healthcare providers to analyze extensive data, extract valuable insights, and make precise and data-driven clinical decisions. DL methods such as Recurrent Neural Networks (RNN) have been utilized to analyze EHR to model disease progression and predict diagnosis. However, these methods do not address some inherent irregularities in EHR data such as irregular time intervals between clinical visits. Furthermore, most DL models are not interpretable. In this study, we propose two interpretable DL architectures based on RNN, namely Time-Aware RNN (TA-RNN) and TA-RNN-Autoencoder (TA-RNN-AE) to predict patient's clinical outcome in EHR at next visit and multiple visits ahead, respectively. To mitigate the impact of irregular time intervals, we propose incorporating time embedding of the elaps
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PepGB&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#32454;&#31890;&#24230;&#30340;&#25200;&#21160;&#27169;&#22359;&#21644;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#32957;&#27573;&#39044;&#35757;&#32451;&#34920;&#31034;&#65292;&#20197;&#20419;&#36827;&#32957;&#27573;&#26089;&#26399;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#32957;-&#34507;&#30333;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.14665</link><description>&lt;p&gt;
PepGB: &#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#20419;&#36827;&#32957;&#33647;&#29289;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
PepGB: Facilitating peptide drug discovery via graph neural networks. (arXiv:2401.14665v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PepGB&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#32454;&#31890;&#24230;&#30340;&#25200;&#21160;&#27169;&#22359;&#21644;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#32957;&#27573;&#39044;&#35757;&#32451;&#34920;&#31034;&#65292;&#20197;&#20419;&#36827;&#32957;&#27573;&#26089;&#26399;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#32957;-&#34507;&#30333;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32957;&#27573;&#20855;&#26377;&#24456;&#22823;&#30340;&#29983;&#29289;&#21307;&#23398;&#28508;&#21147;&#65292;&#26159;&#26377;&#21069;&#26223;&#30340;&#33647;&#29289;&#20505;&#36873;&#29289;&#12290;&#30446;&#21069;&#65292;&#22823;&#37096;&#20998;&#33719;&#25209;&#30340;&#32957;&#33647;&#29289;&#30452;&#25509;&#26469;&#33258;&#20110;&#32463;&#36807;&#28145;&#20837;&#30740;&#31350;&#30340;&#33258;&#28982;&#20154;&#31867;&#32957;&#27573;&#12290;&#22312;&#24191;&#38420;&#32780;&#26410;&#24320;&#21457;&#30340;&#29983;&#29289;&#21270;&#23398;&#31354;&#38388;&#20013;&#65292;&#21033;&#29992;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#35782;&#21035;&#26032;&#22411;&#32957;&#33647;&#29289;&#26159;&#38750;&#24120;&#24517;&#35201;&#30340;&#12290;&#23613;&#31649;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#22522;&#20110;&#35745;&#31639;&#26041;&#27861;&#26469;&#21152;&#36895;&#32957;&#27573;&#26089;&#26399;&#33647;&#29289;&#21457;&#29616;&#65292;&#20294;&#29616;&#26377;&#27169;&#22411;&#38754;&#20020;&#36807;&#25311;&#21512;&#21644;&#32570;&#20047;&#26222;&#36866;&#24615;&#30340;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#23454;&#39564;&#25968;&#25454;&#35268;&#27169;&#26377;&#38480;&#65292;&#20998;&#24067;&#19981;&#24179;&#34913;&#65292;&#36136;&#37327;&#19981;&#19968;&#33268;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PepGB&#65292;&#19968;&#20010;&#29992;&#20110;&#36890;&#36807;&#39044;&#27979;&#32957;-&#34507;&#30333;&#30456;&#20114;&#20316;&#29992;&#65288;PepPIs&#65289;&#26469;&#20419;&#36827;&#32957;&#27573;&#26089;&#26399;&#33647;&#29289;&#21457;&#29616;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12290;PepGB&#37319;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#32467;&#21512;&#20102;&#32454;&#31890;&#24230;&#30340;&#25200;&#21160;&#27169;&#22359;&#21644;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#32957;&#27573;&#39044;&#35757;&#32451;&#34920;&#31034;&#30340;&#21452;&#35270;&#22270;&#30446;&#26631;&#26469;&#39044;&#27979;PepPIs&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;PepGB&#22312;&#39044;&#27979;PepPIs&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Peptides offer great biomedical potential and serve as promising drug candidates. Currently, the majority of approved peptide drugs are directly derived from well-explored natural human peptides. It is quite necessary to utilize advanced deep learning techniques to identify novel peptide drugs in the vast, unexplored biochemical space. Despite various in silico methods having been developed to accelerate peptide early drug discovery, existing models face challenges of overfitting and lacking generalizability due to the limited size, imbalanced distribution and inconsistent quality of experimental data. In this study, we propose PepGB, a deep learning framework to facilitate peptide early drug discovery by predicting peptide-protein interactions (PepPIs). Employing graph neural networks, PepGB incorporates a fine-grained perturbation module and a dual-view objective with contrastive learning-based peptide pre-trained representation to predict PepPIs. Through rigorous evaluations, we dem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#32422;&#26463;&#29983;&#25104;&#25216;&#26415;&#65292;&#36890;&#36807;&#24573;&#30053;&#27425;&#20248;&#25805;&#20316;&#21644;&#36991;&#20813;&#19981;&#24517;&#35201;&#30340;&#25104;&#26412;&#35745;&#31639;&#65292;&#35299;&#20915;&#20102;&#38543;&#26426;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#20013;&#30340;&#35745;&#31639;&#25928;&#29575;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#24212;&#29992;&#36825;&#19968;&#25216;&#26415;&#30340;CG-iLAO*&#31639;&#27861;&#22312;&#35299;&#20915;&#38382;&#39064;&#30340;&#36895;&#24230;&#19978;&#27604;LRTDP&#21644;iLAO*&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.14636</link><description>&lt;p&gt;
&#39640;&#25928;&#32422;&#26463;&#29983;&#25104;&#22312;&#38543;&#26426;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient Constraint Generation for Stochastic Shortest Path Problems. (arXiv:2401.14636v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#32422;&#26463;&#29983;&#25104;&#25216;&#26415;&#65292;&#36890;&#36807;&#24573;&#30053;&#27425;&#20248;&#25805;&#20316;&#21644;&#36991;&#20813;&#19981;&#24517;&#35201;&#30340;&#25104;&#26412;&#35745;&#31639;&#65292;&#35299;&#20915;&#20102;&#38543;&#26426;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#20013;&#30340;&#35745;&#31639;&#25928;&#29575;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#24212;&#29992;&#36825;&#19968;&#25216;&#26415;&#30340;CG-iLAO*&#31639;&#27861;&#22312;&#35299;&#20915;&#38382;&#39064;&#30340;&#36895;&#24230;&#19978;&#27604;LRTDP&#21644;iLAO*&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#35299;&#20915;&#38543;&#26426;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#24212;&#29992;Bellman&#22791;&#20221;&#26469;&#25214;&#21040;&#29366;&#24577;&#30340;&#25104;&#26412;&#65292;&#32780;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20351;&#29992;&#21551;&#21457;&#24335;&#31574;&#30053;&#36873;&#25321;&#38656;&#35201;&#22791;&#20221;&#21644;&#20462;&#21098;&#30340;&#29366;&#24577;&#12290;&#36825;&#20123;&#31639;&#27861;&#30340;&#19968;&#20010;&#22522;&#26412;&#38480;&#21046;&#26159;&#38656;&#35201;&#22312;&#27599;&#20010;&#29366;&#24577;&#22791;&#20221;&#26399;&#38388;&#35745;&#31639;&#27599;&#20010;&#36866;&#29992;&#25805;&#20316;&#30340;&#25104;&#26412;&#65292;&#36825;&#23548;&#33268;&#23545;&#20110;&#34987;&#35782;&#21035;&#20026;&#27425;&#20248;&#30340;&#25805;&#20316;&#36827;&#34892;&#20102;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35268;&#21010;&#21644;&#36816;&#31609;&#23398;&#20043;&#38388;&#30340;&#26032;&#32852;&#31995;&#65292;&#24182;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#35299;&#20915;&#20102;&#36825;&#20010;&#19981;&#24517;&#35201;&#35745;&#31639;&#30340;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#38543;&#26426;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#32422;&#26463;&#29983;&#25104;&#25216;&#26415;&#12290;&#36825;&#31181;&#25216;&#26415;&#20351;&#31639;&#27861;&#33021;&#22815;&#24573;&#30053;&#27425;&#20248;&#25805;&#20316;&#24182;&#36991;&#20813;&#35745;&#31639;&#23427;&#20204;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#36824;&#23558;&#36825;&#31181;&#26032;&#25216;&#26415;&#24212;&#29992;&#20110;iLAO*&#31639;&#27861;&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#26032;&#30340;&#31639;&#27861;CG-iLAO*&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;CG-iLAO*&#24573;&#30053;&#20102;&#26368;&#22810;57%&#30340;iLAO*&#25805;&#20316;&#65292;&#24182;&#19988;&#27604;LRTDP&#21644;iLAO*&#35299;&#20915;&#38382;&#39064;&#30340;&#36895;&#24230;&#25552;&#39640;&#20102;8&#20493;&#21644;3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current methods for solving Stochastic Shortest Path Problems (SSPs) find states' costs-to-go by applying Bellman backups, where state-of-the-art methods employ heuristics to select states to back up and prune. A fundamental limitation of these algorithms is their need to compute the cost-to-go for every applicable action during each state backup, leading to unnecessary computation for actions identified as sub-optimal. We present new connections between planning and operations research and, using this framework, we address this issue of unnecessary computation by introducing an efficient version of constraint generation for SSPs. This technique allows algorithms to ignore sub-optimal actions and avoid computing their costs-to-go. We also apply our novel technique to iLAO* resulting in a new algorithm, CG-iLAO*. Our experiments show that CG-iLAO* ignores up to 57% of iLAO*'s actions and it solves problems up to 8x and 3x faster than LRTDP and iLAO*.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#26500;&#24314;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#23545;&#21508;&#31181;&#20856;&#22411;&#30340;&#20013;&#25991;&#25340;&#20889;&#26816;&#26597;&#65288;CSC&#65289;&#27169;&#22411;&#30340;&#39046;&#22495;&#36866;&#24212;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#26032;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2401.14630</link><description>&lt;p&gt;
&#12298;&#23545;&#20013;&#25991;&#25340;&#20889;&#26816;&#26597;&#27169;&#22411;&#30340;&#39046;&#22495;&#36866;&#24212;&#33021;&#21147;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#12299;
&lt;/p&gt;
&lt;p&gt;
An Empirical Investigation of Domain Adaptation Ability for Chinese Spelling Check Models. (arXiv:2401.14630v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14630
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#26500;&#24314;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#23545;&#21508;&#31181;&#20856;&#22411;&#30340;&#20013;&#25991;&#25340;&#20889;&#26816;&#26597;&#65288;CSC&#65289;&#27169;&#22411;&#30340;&#39046;&#22495;&#36866;&#24212;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#26032;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#25991;&#25340;&#20889;&#26816;&#26597;&#65288;CSC&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#20013;&#19968;&#39033;&#26377;&#24847;&#20041;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#26816;&#27979;&#20013;&#25991;&#25991;&#26412;&#20013;&#30340;&#25340;&#20889;&#38169;&#35823;&#65292;&#28982;&#21518;&#23545;&#36825;&#20123;&#38169;&#35823;&#36827;&#34892;&#32416;&#27491;&#12290;&#28982;&#32780;&#65292;CSC&#27169;&#22411;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#19968;&#33324;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#24403;&#38754;&#20020;&#28041;&#21450;&#29305;&#23450;&#39046;&#22495;&#26415;&#35821;&#30340;&#19979;&#28216;&#20219;&#21153;&#26102;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19977;&#20010;&#21253;&#21547;&#36130;&#32463;&#12289;&#21307;&#30103;&#21644;&#27861;&#24459;&#39046;&#22495;&#20016;&#23500;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#23545;&#21508;&#31181;&#20856;&#22411;&#30340;CSC&#27169;&#22411;&#30340;&#39046;&#22495;&#36866;&#24212;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#30456;&#24212;&#39046;&#22495;&#29305;&#23450;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#20197;&#30830;&#23450;&#20960;&#31181;&#20856;&#22411;CSC&#27169;&#22411;&#30340;&#36328;&#22495;&#36866;&#24212;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#27979;&#35797;&#20102;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#26032;&#39046;&#22495;&#20013;&#65292;CSC&#27169;&#22411;&#30340;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chinese Spelling Check (CSC) is a meaningful task in the area of Natural Language Processing (NLP) which aims at detecting spelling errors in Chinese texts and then correcting these errors. However, CSC models are based on pretrained language models, which are trained on a general corpus. Consequently, their performance may drop when confronted with downstream tasks involving domain-specific terms. In this paper, we conduct a thorough evaluation about the domain adaption ability of various typical CSC models by building three new datasets encompassing rich domain-specific terms from the financial, medical, and legal domains. Then we conduct empirical investigations in the corresponding domain-specific test datasets to ascertain the cross-domain adaptation ability of several typical CSC models. We also test the performance of the popular large language model ChatGPT. As shown in our experiments, the performances of the CSC models drop significantly in the new domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#26426;&#22120;/&#28145;&#24230;&#23398;&#20064;&#30340;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#20013;&#21487;&#35299;&#37322;&#24615;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;XAI&#25216;&#26415;&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#30340;&#24212;&#29992;&#24773;&#20917;&#65292;&#26088;&#22312;&#25552;&#39640;AI&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#20197;&#35299;&#20915;&#23454;&#38469;&#37096;&#32626;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#39118;&#38505;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14617</link><description>&lt;p&gt;
&#12298;&#22522;&#20110;&#26426;&#22120;/&#28145;&#24230;&#23398;&#20064;&#30340;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#20013;&#21487;&#35299;&#37322;&#24615;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#12299;
&lt;/p&gt;
&lt;p&gt;
A Systematic Literature Review on Explainability for Machine/Deep Learning-based Software Engineering Research. (arXiv:2401.14617v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#26426;&#22120;/&#28145;&#24230;&#23398;&#20064;&#30340;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#20013;&#21487;&#35299;&#37322;&#24615;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;XAI&#25216;&#26415;&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#30340;&#24212;&#29992;&#24773;&#20917;&#65292;&#26088;&#22312;&#25552;&#39640;AI&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#20197;&#35299;&#20915;&#23454;&#38469;&#37096;&#32626;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#39118;&#38505;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#65292;&#22312;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#23601;&#65292;&#24182;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20294;&#30001;&#20110;&#23427;&#20204;&#30340;&#40657;&#30418;&#29305;&#24615;&#65292;&#36825;&#20123;&#20855;&#26377;&#28508;&#21147;&#30340;AI&#39537;&#21160;&#30340;&#36719;&#20214;&#24037;&#31243;&#27169;&#22411;&#31163;&#23454;&#38469;&#37096;&#32626;&#36824;&#26377;&#24456;&#22823;&#30340;&#24046;&#36317;&#12290;&#36825;&#31181;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#22312;&#20851;&#38190;&#20219;&#21153;&#20013;&#24212;&#29992;&#36825;&#20123;&#27169;&#22411;&#65292;&#22914;&#28431;&#27934;&#26816;&#27979;&#65292;&#20915;&#31574;&#36879;&#26126;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#21364;&#24102;&#26469;&#20102;&#19981;&#24517;&#35201;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;SE&#39046;&#22495;&#20013;&#26088;&#22312;&#25552;&#39640;AI&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#36827;&#34892;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#26469;&#38416;&#26126;&#36825;&#20010;&#36328;&#23398;&#31185;&#39046;&#22495;&#12290;&#35813;&#32508;&#36848;&#35206;&#30422;&#20102;SE&#21644;AI&#23398;&#26415;&#20250;&#35758;&#21644;&#26399;&#21002;&#20013;&#20986;&#29616;&#30340;&#30740;&#31350;&#65292;&#28085;&#30422;&#20102;21&#20010;&#29420;&#29305;&#30340;SE&#20219;&#21153;&#30340;63&#31687;&#35770;&#25991;&#12290;&#22522;&#20110;&#19977;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#25105;&#20204;&#26088;&#22312;&#24635;&#32467;XAI&#25216;&#26415;&#22312;SE&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
The remarkable achievements of Artificial Intelligence (AI) algorithms, particularly in Machine Learning (ML) and Deep Learning (DL), have fueled their extensive deployment across multiple sectors, including Software Engineering (SE). However, due to their black-box nature, these promising AI-driven SE models are still far from being deployed in practice. This lack of explainability poses unwanted risks for their applications in critical tasks, such as vulnerability detection, where decision-making transparency is of paramount importance. This paper endeavors to elucidate this interdisciplinary domain by presenting a systematic literature review of approaches that aim to improve the explainability of AI models within the context of SE. The review canvasses work appearing in the most prominent SE &amp; AI conferences and journals, and spans 63 papers across 21 unique SE tasks. Based on three key Research Questions (RQs), we aim to (1) summarize the SE tasks where XAI techniques have shown s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#8220;&#26367;&#20195;&#24615;&#35328;&#35770;&#8221;&#20316;&#20026;&#30452;&#25509;&#23545;&#25239;&#20167;&#24680;&#35328;&#35770;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#26500;&#24314;&#25152;&#38656;&#25968;&#25454;&#38598;&#30340;&#25351;&#23548;&#12290;&#26367;&#20195;&#24615;&#35328;&#35770;&#36890;&#36807;&#25552;&#20379;&#23454;&#38469;&#21487;&#34892;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#32771;&#34385;&#29615;&#22659;&#22240;&#32032;&#24182;&#20419;&#20351;&#28436;&#35762;&#32773;&#25913;&#21464;&#65292;&#20026;&#35299;&#20915;&#31038;&#20250;&#38382;&#39064;&#25552;&#20379;&#26377;&#29992;&#24037;&#20855;&#12290;&#23558;&#26367;&#20195;&#24615;&#35328;&#35770;&#19982;&#23545;&#25239;&#21465;&#20107;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#23545;&#25239;&#20167;&#24680;&#35328;&#35770;&#65292;&#34917;&#20805;&#20102;&#23545;&#25239;&#21465;&#20107;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.14616</link><description>&lt;p&gt;
&#26367;&#20195;&#24615;&#35328;&#35770;&#65306;&#34917;&#20805;&#23545;&#25239;&#21465;&#20107;&#30340;&#26041;&#27861;&#20197;&#25913;&#21892;&#35752;&#35770;&#65288;arXiv:2401.14616v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
Alternative Speech: Complementary Method to Counter-Narrative for Better Discourse. (arXiv:2401.14616v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14616
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#8220;&#26367;&#20195;&#24615;&#35328;&#35770;&#8221;&#20316;&#20026;&#30452;&#25509;&#23545;&#25239;&#20167;&#24680;&#35328;&#35770;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#26500;&#24314;&#25152;&#38656;&#25968;&#25454;&#38598;&#30340;&#25351;&#23548;&#12290;&#26367;&#20195;&#24615;&#35328;&#35770;&#36890;&#36807;&#25552;&#20379;&#23454;&#38469;&#21487;&#34892;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#32771;&#34385;&#29615;&#22659;&#22240;&#32032;&#24182;&#20419;&#20351;&#28436;&#35762;&#32773;&#25913;&#21464;&#65292;&#20026;&#35299;&#20915;&#31038;&#20250;&#38382;&#39064;&#25552;&#20379;&#26377;&#29992;&#24037;&#20855;&#12290;&#23558;&#26367;&#20195;&#24615;&#35328;&#35770;&#19982;&#23545;&#25239;&#21465;&#20107;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#23545;&#25239;&#20167;&#24680;&#35328;&#35770;&#65292;&#34917;&#20805;&#20102;&#23545;&#25239;&#21465;&#20107;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#26367;&#20195;&#24615;&#35328;&#35770;&#8221;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#30452;&#25509;&#23545;&#25239;&#20167;&#24680;&#35328;&#35770;&#21644;&#34917;&#20805;&#21465;&#20107;&#38480;&#21046;&#30340;&#26032;&#26041;&#24335;&#12290;&#26367;&#20195;&#24615;&#35328;&#35770;&#36890;&#36807;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#25552;&#20379;&#35328;&#35770;&#32423;&#21035;&#30340;&#20462;&#27491;&#65292;&#32771;&#34385;&#21608;&#22260;&#29615;&#22659;&#24182;&#20419;&#20351;&#28436;&#35762;&#32773;&#25913;&#21464;&#65292;&#20026;&#20167;&#24680;&#35328;&#35770;&#25552;&#20379;&#23454;&#38469;&#21487;&#34892;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#26367;&#20195;&#24615;&#35328;&#35770;&#21487;&#20197;&#19982;&#23545;&#25239;&#21465;&#20107;&#19968;&#36215;&#23545;&#25239;&#20167;&#24680;&#35328;&#35770;&#65292;&#20026;&#35299;&#20915;&#31181;&#26063;&#27495;&#35270;&#21644;&#24615;&#21035;&#19981;&#24179;&#31561;&#31561;&#31038;&#20250;&#38382;&#39064;&#25552;&#20379;&#26377;&#29992;&#24037;&#20855;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#20010;&#26032;&#27010;&#24565;&#65292;&#24182;&#25552;&#20379;&#20102;&#26500;&#24314;&#25152;&#38656;&#25968;&#25454;&#38598;&#30340;&#35814;&#32454;&#25351;&#23548;&#12290;&#36890;&#36807;&#35752;&#35770;&#65292;&#25105;&#20204;&#35777;&#26126;&#23558;&#26367;&#20195;&#24615;&#35328;&#35770;&#19982;&#23545;&#25239;&#21465;&#20107;&#30456;&#32467;&#21512;&#21487;&#20197;&#26159;&#23545;&#25239;&#20167;&#24680;&#35328;&#35770;&#26356;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#34917;&#20805;&#20102;&#23545;&#25239;&#21465;&#20107;&#30340;&#20855;&#20307;&#24615;&#21644;&#24341;&#23548;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22788;&#29702;&#20167;&#24680;&#35328;&#35770;&#30340;&#21478;&#19968;&#31181;&#35270;&#35282;&#65292;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#34917;&#25937;&#25514;&#26045;&#26469;&#34917;&#20805;&#24403;&#21069;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the concept of "Alternative Speech" as a new way to directly combat hate speech and complement the limitations of counter-narrative. An alternative speech provides practical alternatives to hate speech in real-world scenarios by offering speech-level corrections to speakers while considering the surrounding context and promoting speakers to reform. Further, an alternative speech can combat hate speech alongside counter-narratives, offering a useful tool to address social issues such as racial discrimination and gender inequality. We propose the new concept and provide detailed guidelines for constructing the necessary dataset. Through discussion, we demonstrate that combining alternative speech and counter-narrative can be a more effective strategy for combating hate speech by complementing specificity and guiding capacity of counter-narrative. This paper presents another perspective for dealing with hate speech, offering viable remedies to complement the constraints of cu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26234;&#33021;&#20307;&#23545;&#35805;&#30340;&#26041;&#24335;&#26469;&#20943;&#36731;&#20020;&#24202;&#20915;&#31574;&#20013;&#30340;&#35748;&#30693;&#20559;&#24046;&#65292;&#24182;&#35780;&#20272;&#20854;&#23545;&#25552;&#39640;&#35786;&#26029;&#20934;&#30830;&#24615;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.14589</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#23545;&#35805;&#25552;&#39640;&#35786;&#26029;&#20934;&#30830;&#24230;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20943;&#23569;&#35748;&#30693;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Enhancing Diagnostic Accuracy through Multi-Agent Conversations: Using Large Language Models to Mitigate Cognitive Bias. (arXiv:2401.14589v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26234;&#33021;&#20307;&#23545;&#35805;&#30340;&#26041;&#24335;&#26469;&#20943;&#36731;&#20020;&#24202;&#20915;&#31574;&#20013;&#30340;&#35748;&#30693;&#20559;&#24046;&#65292;&#24182;&#35780;&#20272;&#20854;&#23545;&#25552;&#39640;&#35786;&#26029;&#20934;&#30830;&#24615;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#20020;&#24202;&#20915;&#31574;&#20013;&#30340;&#35748;&#30693;&#20559;&#24046;&#26174;&#33879;&#23548;&#33268;&#35786;&#26029;&#38169;&#35823;&#21644;&#27425;&#20248;&#24739;&#32773;&#32467;&#26524;&#12290;&#35299;&#20915;&#36825;&#20123;&#20559;&#24046;&#38382;&#39064;&#22312;&#21307;&#30103;&#39046;&#22495;&#38754;&#20020;&#24040;&#22823;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#26234;&#33021;&#20307;&#26694;&#26550;&#20013;&#20943;&#36731;&#36825;&#20123;&#20559;&#24046;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#23545;&#35805;&#27169;&#25311;&#20020;&#24202;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#35780;&#20272;&#20854;&#23545;&#25913;&#21892;&#35786;&#26029;&#20934;&#30830;&#24615;&#30340;&#26377;&#25928;&#24615;&#12290;&#26041;&#27861;&#65306;&#20174;&#25991;&#29486;&#20013;&#25214;&#21040;&#20102;&#24635;&#20849;16&#20010;&#24050;&#21457;&#34920;&#21644;&#26410;&#21457;&#34920;&#30340;&#30149;&#20363;&#25253;&#21578;&#65292;&#20854;&#20013;&#35748;&#30693;&#20559;&#24046;&#23548;&#33268;&#35823;&#35786;&#12290;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992; GPT-4 Turbo &#20419;&#36827;&#22235;&#20010;&#27169;&#25311;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#20197;&#22797;&#21046;&#20020;&#24202;&#22242;&#38431;&#21160;&#24577;&#12290;&#27599;&#20010;&#26234;&#33021;&#20307;&#37117;&#26377;&#29420;&#29305;&#30340;&#35282;&#33394;&#65306;1) &#22312;&#32771;&#34385;&#35752;&#35770;&#21518;&#36827;&#34892;&#21021;&#27493;&#21644;&#26368;&#32456;&#35786;&#26029;&#12290;2) &#20805;&#24403;&#39764;&#39740;&#30340;&#20195;&#35328;&#20154;&#65292;&#20197;&#32416;&#27491;&#30830;&#35748;&#20559;&#24046;&#21644;&#38170;&#23450;&#20559;&#24046;&#12290;3) &#20805;&#24403;&#23548;&#24072;&#21644;&#20419;&#36827;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Cognitive biases in clinical decision-making significantly contribute to errors in diagnosis and suboptimal patient outcomes. Addressing these biases presents a formidable challenge in the medical field. This study explores the role of large language models (LLMs) in mitigating these biases through the utilization of a multi-agent framework. We simulate the clinical decision-making processes through multi-agent conversation and evaluate its efficacy in improving diagnostic accuracy. Methods: A total of 16 published and unpublished case reports where cognitive biases have resulted in misdiagnoses were identified from the literature. In the multi-agent system, we leveraged GPT-4 Turbo to facilitate interactions among four simulated agents to replicate clinical team dynamics. Each agent has a distinct role: 1) To make the initial and final diagnosis after considering the discussions, 2) The devil's advocate and correct confirmation and anchoring bias, 3) The tutor and facilita
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#36710;&#20869;&#20154;&#26426;&#20132;&#20114;&#30340;&#29616;&#29366;&#21644;&#26032;&#20852;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#21253;&#23481;&#24615;HCI&#35774;&#35745;&#21407;&#21017;&#65292;&#26088;&#22312;&#22686;&#24378;&#20056;&#23458;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2401.14571</link><description>&lt;p&gt;
&#21521;&#21253;&#23481;&#24615;&#39537;&#21160;&#65306;&#37325;&#26032;&#23457;&#35270;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#30340;&#36710;&#20869;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Driving Towards Inclusion: Revisiting In-Vehicle Interaction in Autonomous Vehicles. (arXiv:2401.14571v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#36710;&#20869;&#20154;&#26426;&#20132;&#20114;&#30340;&#29616;&#29366;&#21644;&#26032;&#20852;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#21253;&#23481;&#24615;HCI&#35774;&#35745;&#21407;&#21017;&#65292;&#26088;&#22312;&#22686;&#24378;&#20056;&#23458;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#30446;&#21069;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#36710;&#20869;&#20154;&#26426;&#20132;&#20114;&#65288;HCI&#65289;&#30340;&#29616;&#29366;&#65292;&#29305;&#21035;&#20851;&#27880;&#21253;&#23481;&#24615;&#21644;&#21487;&#35775;&#38382;&#24615;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#32771;&#23519;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#21253;&#23481;&#24615;HCI&#30340;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#35774;&#35745;&#21407;&#21017;&#65292;&#35780;&#20272;&#29616;&#26377;HCI&#31995;&#32479;&#65292;&#24182;&#30830;&#23450;&#21487;&#33021;&#22686;&#24378;&#20056;&#23458;&#20307;&#39564;&#30340;&#26032;&#20852;&#25216;&#26415;&#12290;&#26412;&#25991;&#39318;&#20808;&#27010;&#36848;&#20102;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#25216;&#26415;&#30340;&#29616;&#29366;&#65292;&#28982;&#21518;&#23545;&#36825;&#19968;&#32972;&#26223;&#19979;HCI&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#25509;&#19979;&#26469;&#65292;&#26412;&#25991;&#32508;&#36848;&#20102;&#21253;&#23481;&#24615;HCI&#35774;&#35745;&#21407;&#21017;&#30340;&#29616;&#26377;&#25991;&#29486;&#65292;&#24182;&#35780;&#20272;&#20102;&#24403;&#21069;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;HCI&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#25991;&#36824;&#30830;&#23450;&#20102;&#21487;&#33021;&#22686;&#24378;&#20056;&#23458;&#20307;&#39564;&#30340;&#26032;&#20852;&#25216;&#26415;&#65292;&#22914;&#35821;&#38899;&#28608;&#27963;&#30028;&#38754;&#12289;&#35302;&#35273;&#21453;&#39304;&#31995;&#32479;&#21644;&#22686;&#24378;&#29616;&#23454;&#26174;&#31034;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#24635;&#32467;&#20102;&#30740;&#31350;&#30340;&#37325;&#35201;&#21457;&#29616;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive literature review of the current state of in-vehicle human-computer interaction (HCI) in the context of self-driving vehicles, with a specific focus on inclusion and accessibility. This study's aim is to examine the user-centered design principles for inclusive HCI in self-driving vehicles, evaluate existing HCI systems, and identify emerging technologies that have the potential to enhance the passenger experience. The paper begins by providing an overview of the current state of self-driving vehicle technology, followed by an examination of the importance of HCI in this context. Next, the paper reviews the existing literature on inclusive HCI design principles and evaluates the effectiveness of current HCI systems in self-driving vehicles. The paper also identifies emerging technologies that have the potential to enhance the passenger experience, such as voice-activated interfaces, haptic feedback systems, and augmented reality displays. Finally, th
&lt;/p&gt;</description></item><item><title>&#33258;&#36866;&#24212;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#22312;&#29305;&#23450;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#12289;&#25913;&#36827;&#32763;&#35793;&#36136;&#37327;&#65292;&#35299;&#20915;&#20102;&#22312;&#22495;&#36866;&#24212;&#20013;&#22240;&#32570;&#20047;&#22495;&#20869;&#25968;&#25454;&#32780;&#23548;&#33268;&#32763;&#35793;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14559</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#26426;&#22120;&#32763;&#35793;&#30340;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Language Modelling Approaches to Adaptive Machine Translation. (arXiv:2401.14559v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14559
&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#22312;&#29305;&#23450;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#12289;&#25913;&#36827;&#32763;&#35793;&#36136;&#37327;&#65292;&#35299;&#20915;&#20102;&#22312;&#22495;&#36866;&#24212;&#20013;&#22240;&#32570;&#20047;&#22495;&#20869;&#25968;&#25454;&#32780;&#23548;&#33268;&#32763;&#35793;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#33268;&#24615;&#26159;&#39640;&#36136;&#37327;&#32763;&#35793;&#30340;&#20851;&#38190;&#35201;&#27714;&#12290;&#22312;&#29305;&#23450;&#39046;&#22495;&#39033;&#30446;&#20013;&#65292;&#36981;&#24490;&#39044;&#23450;&#20041;&#30340;&#26415;&#35821;&#21644;&#36866;&#24212;&#26356;&#27491;&#21518;&#30340;&#32763;&#35793;&#23588;&#20026;&#37325;&#35201;&#12290;&#26426;&#22120;&#32763;&#35793;&#22312;&#22495;&#36866;&#24212;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#32763;&#35793;&#29615;&#22659;&#20013;&#65292;&#30001;&#20110;&#32570;&#20047;&#19987;&#38376;&#30340;&#25968;&#25454;&#38598;&#21644;&#26415;&#35821;&#65292;&#25110;&#32773;&#21487;&#29992;&#30340;&#22495;&#20869;&#32763;&#35793;&#19981;&#19968;&#33268;&#19988;&#19981;&#20934;&#30830;&#65292;&#23548;&#33268;&#22495;&#20869;&#25968;&#25454;&#31232;&#32570;&#29616;&#35937;&#26222;&#36941;&#23384;&#22312;&#12290;&#22312;&#27809;&#26377;&#36275;&#22815;&#22495;&#20869;&#25968;&#25454;&#29992;&#20110;&#24494;&#35843;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;&#19982;&#30456;&#20851;&#19978;&#19979;&#25991;&#19968;&#33268;&#30340;&#32763;&#35793;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#34429;&#28982;&#23454;&#26102;&#36866;&#24212;&#21487;&#20197;&#21033;&#29992;&#36739;&#23569;&#37327;&#30340;&#22495;&#20869;&#25968;&#25454;&#23454;&#26102;&#25913;&#36827;&#32763;&#35793;&#36136;&#37327;&#65292;&#20294;&#30001;&#20110;&#21463;&#21040;&#19978;&#19979;&#25991;&#38480;&#21046;&#21644;&#25928;&#29575;&#32422;&#26463;&#65292;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#30340;&#26377;&#36259;&#33021;&#21147;&#65292;&#36890;&#36807;&#23398;&#20064;&#22797;&#21046;&#29305;&#23450;&#30340;&#36755;&#20837;&#36755;&#20986;&#26469;&#25913;&#36827;&#32763;&#35793;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consistency is a key requirement of high-quality translation. It is especially important to adhere to pre-approved terminology and adapt to corrected translations in domain-specific projects. Machine translation (MT) has achieved significant progress in the area of domain adaptation. However, in-domain data scarcity is common in translation settings, due to the lack of specialised datasets and terminology, or inconsistency and inaccuracy of available in-domain translations. In such scenarios where there is insufficient in-domain data to fine-tune MT models, producing translations that are consistent with the relevant context is challenging. While real-time adaptation can make use of smaller amounts of in-domain data to improve the translation on the fly, it remains challenging due to supported context limitations and efficiency constraints. Large language models (LLMs) have recently shown interesting capabilities of in-context learning, where they learn to replicate certain input-outpu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35782;&#21035;&#38899;&#20048;&#38899;&#39057;&#20013;&#30456;&#20284;&#30340;&#20316;&#21697;&#65292;&#20197;&#20415;&#20102;&#35299;&#29983;&#25104;&#38899;&#20048;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#24402;&#22240;&#12290;&#36890;&#36807;&#24212;&#29992;CLMR&#21644;CLAP&#23884;&#20837;&#21040;&#30456;&#20284;&#24230;&#27979;&#37327;&#20013;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35266;&#23519;&#20102;&#23545;&#38899;&#39057;&#31034;&#20363;&#36827;&#34892;&#20462;&#25913;&#23545;&#30456;&#20284;&#24230;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.14542</link><description>&lt;p&gt;
&#25506;&#32034;&#38899;&#20048;&#26681;&#28304;&#65306;&#24212;&#29992;&#38899;&#39057;&#23884;&#20837;&#26469;&#22686;&#24378;&#29983;&#25104;&#38899;&#20048;&#27169;&#22411;&#30340;&#24433;&#21709;&#21147;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
Exploring Musical Roots: Applying Audio Embeddings to Empower Influence Attribution for a Generative Music Model. (arXiv:2401.14542v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35782;&#21035;&#38899;&#20048;&#38899;&#39057;&#20013;&#30456;&#20284;&#30340;&#20316;&#21697;&#65292;&#20197;&#20415;&#20102;&#35299;&#29983;&#25104;&#38899;&#20048;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#24402;&#22240;&#12290;&#36890;&#36807;&#24212;&#29992;CLMR&#21644;CLAP&#23884;&#20837;&#21040;&#30456;&#20284;&#24230;&#27979;&#37327;&#20013;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35266;&#23519;&#20102;&#23545;&#38899;&#39057;&#31034;&#20363;&#36827;&#34892;&#20462;&#25913;&#23545;&#30456;&#20284;&#24230;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#20010;&#33402;&#26415;&#23478;&#37117;&#26377;&#33258;&#24049;&#30340;&#21019;&#20316;&#36807;&#31243;&#65292;&#20174;&#20808;&#21069;&#30340;&#33402;&#26415;&#23478;&#21644;&#20316;&#21697;&#20013;&#27762;&#21462;&#28789;&#24863;&#12290;&#22914;&#20170;&#65292;&#29983;&#25104;&#38899;&#20048;&#27169;&#22411;&#24050;&#32463;&#33258;&#21160;&#21270;&#20102;&#36825;&#31181;&#8220;&#28789;&#24863;&#8221;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#40657;&#21283;&#23376;&#24615;&#36136;&#20351;&#24471;&#20854;&#21019;&#20316;&#36755;&#20986;&#30340;&#24433;&#21709;&#21147;&#26469;&#28304;&#26080;&#27861;&#34987;&#28165;&#26970;&#22320;&#36776;&#35748;&#12290;&#22240;&#27492;&#65292;&#29992;&#25143;&#21487;&#33021;&#20250;&#19981;&#32463;&#24847;&#22320;&#30423;&#29992;&#12289;&#28389;&#29992;&#25110;&#22797;&#21046;&#29616;&#26377;&#33402;&#26415;&#23478;&#30340;&#20316;&#21697;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#21487;&#22797;&#21046;&#30340;&#26041;&#27861;&#26469;&#31995;&#32479;&#22320;&#35782;&#21035;&#38899;&#20048;&#38899;&#39057;&#20013;&#30456;&#20284;&#30340;&#20316;&#21697;&#65292;&#20197;&#20415;&#20102;&#35299;&#35757;&#32451;&#25968;&#25454;&#30340;&#24402;&#22240;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#20043;&#19968;&#26159;&#21033;&#29992;&#26377;&#25928;&#30340;&#38899;&#20048;&#38899;&#39057;&#30456;&#20284;&#24230;&#24230;&#37327;&#12290;&#25105;&#20204;&#23558;&#24212;&#29992;CLMR&#21644;CLAP&#23884;&#20837;&#21040;&#30456;&#20284;&#24230;&#27979;&#37327;&#20013;&#65292;&#24182;&#27604;&#36739;&#20854;&#25928;&#26524;&#65292;&#36825;&#20123;&#23884;&#20837;&#29992;&#20110;&#35757;&#32451;VampNet&#65292;&#19968;&#20010;&#26368;&#36817;&#30340;&#24320;&#28304;&#29983;&#25104;&#38899;&#20048;&#27169;&#22411;&#20013;&#30340;500&#19975;&#20010;&#38899;&#39057;&#29255;&#27573;&#12290;&#25105;&#20204;&#36890;&#36807;&#20154;&#31867;&#21548;&#35273;&#30740;&#31350;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#23545;&#38899;&#39057;&#31034;&#20363;&#36827;&#34892;&#20462;&#25913;&#65288;&#22914;&#38899;&#39640;&#21464;&#21270;&#12289;&#26102;&#38388;&#25289;&#20280;&#12289;&#32972;&#26223;&#22122;&#38899;&#65289;&#23545;&#30456;&#20284;&#24230;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Every artist has a creative process that draws inspiration from previous artists and their works. Today, "inspiration" has been automated by generative music models. The black box nature of these models obscures the identity of the works that influence their creative output. As a result, users may inadvertently appropriate, misuse, or copy existing artists' works. We establish a replicable methodology to systematically identify similar pieces of music audio in a manner that is useful for understanding training data attribution. A key aspect of our approach is to harness an effective music audio similarity measure. We compare the effect of applying CLMR and CLAP embeddings to similarity measurement in a set of 5 million audio clips used to train VampNet, a recent open source generative music model. We validate this approach with a human listening study. We also explore the effect that modifications of an audio example (e.g., pitch shifting, time stretching, background noise) have on sim
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20570;&#36873;&#25321;&#26102;&#34920;&#29616;&#20986;&#20102;&#19982;&#20154;&#31867;&#21644;&#21160;&#29289;&#30456;&#20284;&#30340;&#30456;&#23545;&#20215;&#20540;&#20559;&#24046;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#20154;&#31867;&#36873;&#25321;&#20013;&#30340;&#32972;&#26223;&#20381;&#36182;&#24615;&#26426;&#21046;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.14530</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30456;&#23545;&#20215;&#20540;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Relative Value Biases in Large Language Models. (arXiv:2401.14530v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14530
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20570;&#36873;&#25321;&#26102;&#34920;&#29616;&#20986;&#20102;&#19982;&#20154;&#31867;&#21644;&#21160;&#29289;&#30456;&#20284;&#30340;&#30456;&#23545;&#20215;&#20540;&#20559;&#24046;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#20154;&#31867;&#36873;&#25321;&#20013;&#30340;&#32972;&#26223;&#20381;&#36182;&#24615;&#26426;&#21046;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21644;&#21160;&#29289;&#22312;&#24378;&#21270;&#23398;&#20064;&#26041;&#38754;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#37027;&#20123;&#36873;&#39033;&#19982;&#36739;&#20302;&#30340;&#32477;&#23545;&#22870;&#21169;&#30456;&#20851;&#65292;&#20182;&#20204;&#26356;&#20542;&#21521;&#20110;&#36873;&#25321;&#36807;&#21435;&#30456;&#23545;&#26356;&#22909;&#32467;&#26524;&#30340;&#36873;&#39033;&#12290;&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20250;&#34920;&#29616;&#20986;&#31867;&#20284;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#35753;gpt-4-1106-preview(GPT-4 Turbo)&#21644;Llama-2-70B&#22312;&#26368;&#22823;&#21270;&#22238;&#25253;&#30340;&#30446;&#26631;&#19979;&#21453;&#22797;&#22312;&#36873;&#39033;&#23545;&#20043;&#38388;&#36827;&#34892;&#36873;&#25321;&#12290;&#27599;&#20010;&#25552;&#31034;&#20013;&#37117;&#21253;&#21547;&#20102;&#20808;&#21069;&#32467;&#26524;&#30340;&#23436;&#25972;&#35760;&#24405;&#12290;&#20004;&#20010;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#19982;&#20154;&#31867;&#21644;&#21160;&#29289;&#35266;&#23519;&#21040;&#30340;&#30456;&#23545;&#20215;&#20540;&#20915;&#31574;&#20559;&#24046;&#31867;&#20284;&#30340;&#34892;&#20026;&#12290;&#26356;&#26126;&#30830;&#22320;&#36827;&#34892;&#32467;&#26524;&#20043;&#38388;&#30340;&#30456;&#23545;&#27604;&#36739;&#20250;&#25918;&#22823;&#36825;&#31181;&#20559;&#24046;&#65292;&#32780;&#20419;&#20351;&#27169;&#22411;&#20272;&#35745;&#39044;&#26399;&#32467;&#26524;&#20250;&#20351;&#20559;&#24046;&#28040;&#22833;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#20110;&#20102;&#35299;&#20154;&#31867;&#36873;&#25321;&#20013;&#36129;&#29486;&#21040;&#32972;&#26223;&#20381;&#36182;&#24615;&#30340;&#28508;&#22312;&#26426;&#21046;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Studies of reinforcement learning in humans and animals have demonstrated a preference for options that yielded relatively better outcomes in the past, even when those options are associated with lower absolute reward. The present study tested whether large language models would exhibit a similar bias. We had gpt-4-1106-preview (GPT-4 Turbo) and Llama-2-70B make repeated choices between pairs of options with the goal of maximizing payoffs. A complete record of previous outcomes was included in each prompt. Both models exhibited relative value decision biases similar to those observed in humans and animals. Making relative comparisons among outcomes more explicit magnified the bias, whereas prompting the models to estimate expected outcomes caused the bias to disappear. These results have implications for the potential mechanisms that contribute to context-dependent choice in human agents.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;GPT-3.5&#27169;&#22411;&#65292;&#22312;&#22810;&#22269;&#23466;&#27861;&#25991;&#26412;&#20013;&#36827;&#34892;&#23453;&#36149;&#30340;&#25688;&#35201;&#24635;&#32467;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#19982;&#20844;&#27665;&#26435;&#21033;&#21644;&#20041;&#21153;&#30456;&#20851;&#30340;&#27431;&#27954;&#22269;&#23478;&#23466;&#27861;&#27573;&#33853;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#33021;&#22815;&#20934;&#30830;&#12289;&#36830;&#36143;&#19988;&#24544;&#23454;&#22320;&#25429;&#25417;&#21040;RD&#20027;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14524</link><description>&lt;p&gt;
&#35780;&#20272;GPT-3.5&#22312;&#20849;&#20139;&#20027;&#39064;&#30340;&#27431;&#27954;&#23466;&#27861;&#25991;&#26412;&#20013;&#30340;&#24847;&#35782;&#21644;&#25688;&#35201;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating GPT-3.5's Awareness and Summarization Abilities for European Constitutional Texts with Shared Topics. (arXiv:2401.14524v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;GPT-3.5&#27169;&#22411;&#65292;&#22312;&#22810;&#22269;&#23466;&#27861;&#25991;&#26412;&#20013;&#36827;&#34892;&#23453;&#36149;&#30340;&#25688;&#35201;&#24635;&#32467;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#19982;&#20844;&#27665;&#26435;&#21033;&#21644;&#20041;&#21153;&#30456;&#20851;&#30340;&#27431;&#27954;&#22269;&#23478;&#23466;&#27861;&#27573;&#33853;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#33021;&#22815;&#20934;&#30830;&#12289;&#36830;&#36143;&#19988;&#24544;&#23454;&#22320;&#25429;&#25417;&#21040;RD&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23466;&#27861;&#26159;&#25903;&#25745;&#25919;&#24220;&#21644;&#31038;&#20250;&#32467;&#26500;&#30340;&#22522;&#30784;&#27861;&#24459;&#25991;&#20214;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#19981;&#20165;&#26159;&#22269;&#23478;&#25991;&#21270;&#21644;&#31038;&#20250;&#29420;&#29305;&#24615;&#30340;&#21453;&#26144;&#65292;&#36824;&#26377;&#21161;&#20110;&#30830;&#23450;&#26222;&#36941;&#37325;&#35201;&#30340;&#20027;&#39064;&#65292;&#22914;&#20844;&#27665;&#30340;&#26435;&#21033;&#21644;&#20041;&#21153;&#65288;RD&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#33879;&#21517;&#30340;GPT-3.5&#65292;&#21033;&#29992;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#29702;&#35299;&#36229;&#36234;&#22269;&#30028;&#30340;&#23466;&#27861;&#27573;&#33853;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#19968;&#20010;&#37325;&#35201;&#36129;&#29486;&#26159;&#25277;&#35937;&#25688;&#35201;&#22312;&#22810;&#28304;&#23466;&#27861;&#25991;&#26412;&#38598;&#21512;&#19978;&#30340;&#26032;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#19982;RD&#20027;&#39064;&#30456;&#20851;&#30340;&#27431;&#27954;&#22269;&#23478;&#23466;&#27861;&#27573;&#33853;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-3.5&#20855;&#26377;&#24847;&#20041;&#65292;&#33021;&#22815;&#20135;&#29983;&#28085;&#30422;&#27431;&#27954;&#22269;&#23478;RD&#20027;&#39064;&#30340;&#20449;&#24687;&#20016;&#23500;&#12289;&#36830;&#36143;&#21644;&#24544;&#23454;&#30340;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Constitutions are foundational legal documents that underpin the governmental and societal structures. As such, they are a reflection of a nation's cultural and social uniqueness, but also contribute to establish topics of universal importance, like citizens' rights and duties (RD). In this work, using the renowned GPT-3.5, we leverage generative large language models to understand constitutional passages that transcend national boundaries. A key contribution of our study is the introduction of a novel application of abstractive summarization on a multi-source collection of constitutional texts, with a focus on European countries' constitution passages related to RD topics. Our results show the meaningfulness of GPT-3.5 to produce informative, coherent and faithful summaries capturing RD topics across European countries.
&lt;/p&gt;</description></item><item><title>LLMs&#26082;&#26377;&#33021;&#21147;&#24402;&#22240;&#20110;&#20449;&#24565;&#12289;&#27442;&#26395;&#12289;&#24847;&#22270;&#21644;&#24773;&#24863;&#65292;&#20063;&#33021;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#19981;&#26029;&#25552;&#39640;&#65292;&#20294;&#20182;&#20204;&#26080;&#27861;&#36890;&#36807;&#20849;&#24773;&#26041;&#27861;&#26469;&#23562;&#37325;&#20010;&#20307;&#25104;&#20026;&#20363;&#22806;&#30340;&#26435;&#21033;&#12290;&#20182;&#20204;&#20165;&#36890;&#36807;&#35782;&#21035;&#35821;&#35328;&#27169;&#24335;&#21028;&#26029;&#26696;&#20214;&#30456;&#20284;&#24615;&#65292;&#32780;&#26080;&#27861;&#32771;&#34385;&#20010;&#20307;&#30340;&#20869;&#37096;&#24515;&#29702;&#29366;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20849;&#24773;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.14523</link><description>&lt;p&gt;
&#20849;&#24773;&#19982;&#25104;&#20026;&#19968;&#20010;&#20363;&#22806;&#30340;&#26435;&#21033;&#65306;LLMs&#21487;&#20197;&#20570;&#20160;&#20040;&#65292;&#19981;&#33021;&#20570;&#20160;&#20040;
&lt;/p&gt;
&lt;p&gt;
Empathy and the Right to Be an Exception: What LLMs Can and Cannot Do. (arXiv:2401.14523v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14523
&lt;/p&gt;
&lt;p&gt;
LLMs&#26082;&#26377;&#33021;&#21147;&#24402;&#22240;&#20110;&#20449;&#24565;&#12289;&#27442;&#26395;&#12289;&#24847;&#22270;&#21644;&#24773;&#24863;&#65292;&#20063;&#33021;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#19981;&#26029;&#25552;&#39640;&#65292;&#20294;&#20182;&#20204;&#26080;&#27861;&#36890;&#36807;&#20849;&#24773;&#26041;&#27861;&#26469;&#23562;&#37325;&#20010;&#20307;&#25104;&#20026;&#20363;&#22806;&#30340;&#26435;&#21033;&#12290;&#20182;&#20204;&#20165;&#36890;&#36807;&#35782;&#21035;&#35821;&#35328;&#27169;&#24335;&#21028;&#26029;&#26696;&#20214;&#30456;&#20284;&#24615;&#65292;&#32780;&#26080;&#27861;&#32771;&#34385;&#20010;&#20307;&#30340;&#20869;&#37096;&#24515;&#29702;&#29366;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20849;&#24773;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24615;&#33021;&#30340;&#36827;&#27493;&#23548;&#33268;&#19968;&#20123;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20013;&#29702;&#35770;&#24515;&#26234;&#65288;ToM&#65289;&#30340;&#20986;&#29616;&#12290;LLMs&#33021;&#22815;&#24402;&#22240;&#20110;&#20449;&#24565;&#12289;&#27442;&#26395;&#12289;&#24847;&#22270;&#21644;&#24773;&#24863;&#65292;&#24182;&#19988;&#23427;&#20204;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#20250;&#26377;&#25152;&#25552;&#39640;&#12290;&#19982;&#20154;&#31867;&#29305;&#26377;&#30340;&#20849;&#24773;&#26041;&#27861;&#19981;&#21516;&#65292;&#23427;&#20204;&#36890;&#36807;&#35782;&#21035;&#25968;&#25454;&#38598;&#20013;&#36890;&#24120;&#19981;&#21253;&#25324;&#30340;&#35821;&#35328;&#27169;&#24335;&#26469;&#23398;&#20064;&#24402;&#22240;&#24515;&#29702;&#29366;&#24577;&#12290;&#25105;&#20204;&#38382;LLMs&#30340;&#26080;&#27861;&#20849;&#24773;&#26159;&#21542;&#20250;&#22952;&#30861;&#23427;&#20204;&#23562;&#37325;&#20010;&#20307;&#25104;&#20026;&#19968;&#20010;&#20363;&#22806;&#30340;&#26435;&#21033;&#65292;&#21363;&#26159;&#21542;&#20250;&#22952;&#30861;&#23427;&#20204;&#22522;&#20110;&#23545;&#20010;&#20307;&#30340;&#20010;&#24615;&#25935;&#24863;&#24615;&#36827;&#34892;&#24615;&#26684;&#35780;&#20272;&#21644;&#34892;&#20026;&#39044;&#27979;&#12290;LLMs&#33021;&#21542;&#35748;&#30495;&#32771;&#34385;&#20010;&#20307;&#30340;&#20027;&#24352;&#65292;&#21363;&#20182;&#20204;&#30340;&#24773;&#20917;&#26159;&#22522;&#20110;&#20449;&#24565;&#12289;&#27442;&#26395;&#21644;&#24847;&#22270;&#31561;&#20869;&#37096;&#24515;&#29702;&#29366;&#24577;&#32780;&#19981;&#21516;&#65292;&#36824;&#26159;&#20165;&#38480;&#20110;&#22522;&#20110;&#20854;&#19982;&#20182;&#20154;&#30340;&#30456;&#20284;&#20043;&#22788;&#26469;&#21028;&#26029;&#35813;&#26696;&#20214;&#65311;&#25105;&#20204;&#25552;&#20986;&#20849;&#24773;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Advances in the performance of large language models (LLMs) have led some researchers to propose the emergence of theory of mind (ToM) in artificial intelligence (AI). LLMs can attribute beliefs, desires, intentions, and emotions, and they will improve in their accuracy. Rather than employing the characteristically human method of empathy, they learn to attribute mental states by recognizing linguistic patterns in a dataset that typically do not include that individual. We ask whether LLMs' inability to empathize precludes them from honoring an individual's right to be an exception, that is, from making assessments of character and predictions of behavior that reflect appropriate sensitivity to a person's individuality. Can LLMs seriously consider an individual's claim that their case is different based on internal mental states like beliefs, desires, and intentions, or are they limited to judging that case based on its similarities to others? We propose that the method of empathy has 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#36136;&#37327;&#23432;&#24658;&#24863;&#30693;&#22120;&#26500;&#24314;&#22522;&#20110;&#26377;&#21521;&#22270;&#32467;&#26500;&#30340;&#27700;&#25991;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#38598;&#27700;&#21306;&#23610;&#24230;&#27700;&#25991;&#36807;&#31243;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#22312;&#20445;&#25345;&#31616;&#27905;&#24615;&#30340;&#21516;&#26102;&#33021;&#22815;&#20934;&#30830;&#22320;&#27169;&#25311;&#21508;&#31181;&#27969;&#37327;&#21160;&#21147;&#23398;&#34892;&#20026;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36755;&#20837;&#26049;&#36335;&#26426;&#21046;&#36827;&#19968;&#27493;&#20248;&#21270;&#20102;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.14521</link><description>&lt;p&gt;
&#20197;&#36136;&#37327;&#23432;&#24658;&#24863;&#30693;&#22120;&#20026;&#22522;&#30784;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#29289;&#29702;-&#27010;&#24565;&#38598;&#27700;&#21306;&#23610;&#24230;&#27700;&#25991;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Towards Interpretable Physical-Conceptual Catchment-Scale Hydrological Modeling using the Mass-Conserving-Perceptron. (arXiv:2401.14521v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#36136;&#37327;&#23432;&#24658;&#24863;&#30693;&#22120;&#26500;&#24314;&#22522;&#20110;&#26377;&#21521;&#22270;&#32467;&#26500;&#30340;&#27700;&#25991;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#38598;&#27700;&#21306;&#23610;&#24230;&#27700;&#25991;&#36807;&#31243;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#22312;&#20445;&#25345;&#31616;&#27905;&#24615;&#30340;&#21516;&#26102;&#33021;&#22815;&#20934;&#30830;&#22320;&#27169;&#25311;&#21508;&#31181;&#27969;&#37327;&#21160;&#21147;&#23398;&#34892;&#20026;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36755;&#20837;&#26049;&#36335;&#26426;&#21046;&#36827;&#19968;&#27493;&#20248;&#21270;&#20102;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#31616;&#27905;&#21487;&#35299;&#37322;&#30340;&#38598;&#27700;&#21306;&#23610;&#24230;&#27700;&#25991;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#65292;&#37319;&#29992;&#22522;&#20110;&#36136;&#37327;&#23432;&#24658;&#24863;&#30693;&#22120;&#65288;MCP&#65289;&#30340;&#26377;&#21521;&#22270;&#32467;&#26500;&#20316;&#20026;&#22522;&#26412;&#35745;&#31639;&#21333;&#20803;&#12290;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#21333;&#20010;&#20301;&#32622;&#30340;&#32467;&#26500;&#22797;&#26434;&#24615;&#65288;&#28145;&#24230;&#65289;&#65292;&#32780;&#19981;&#26159;&#23545;&#22823;&#26679;&#26412;&#38598;&#27700;&#21306;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#24191;&#24230;&#12290;&#30446;&#26631;&#26159;&#21457;&#29616;&#19968;&#20010;&#26368;&#23567;&#30340;&#34920;&#31034;&#65288;&#21333;&#20803;&#29366;&#24577;&#25968;&#21644;&#27969;&#37327;&#36335;&#24452;&#25968;&#65289;&#65292;&#29992;&#20110;&#34920;&#31034;&#33021;&#22815;&#35299;&#37322;&#32473;&#23450;&#38598;&#27700;&#21306;&#36755;&#20837;&#29366;&#24577;&#21644;&#36755;&#20986;&#34892;&#20026;&#30340;&#20027;&#35201;&#36807;&#31243;&#65292;&#29305;&#21035;&#24378;&#35843;&#27169;&#25311;&#20840;&#33539;&#22260;&#65288;&#39640;&#12289;&#20013;&#12289;&#20302;&#65289;&#30340;&#27969;&#37327;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#21306;&#22495;&#65292;&#37319;&#29992;&#31867;&#20284;HyMod&#30340;&#26550;&#26500;&#65292;&#20855;&#26377;3&#20010;&#21333;&#20803;&#29366;&#24577;&#21644;2&#20010;&#20027;&#35201;&#27969;&#21160;&#36335;&#24452;&#65292;&#33021;&#22815;&#23454;&#29616;&#36825;&#26679;&#30340;&#34920;&#31034;&#65292;&#20294;&#24341;&#20837;&#36755;&#20837;&#26049;&#36335;&#26426;&#21046;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#27700;&#25991;&#22270;&#30340;&#26102;&#38388;&#21644;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the applicability of machine learning technologies to the development of parsimonious, interpretable, catchment-scale hydrologic models using directed-graph architectures based on the mass-conserving perceptron (MCP) as the fundamental computational unit. Here, we focus on architectural complexity (depth) at a single location, rather than universal applicability (breadth) across large samples of catchments. The goal is to discover a minimal representation (numbers of cell-states and flow paths) that represents the dominant processes that can explain the input-state-output behaviors of a given catchment, with particular emphasis given to simulating the full range (high, medium, and low) of flow dynamics. We find that a HyMod-like architecture with three cell-states and two major flow pathways achieves such a representation at our study location, but that the additional incorporation of an input-bypass mechanism significantly improves the timing and shape of the hydrograph
&lt;/p&gt;</description></item><item><title>s(LAW)&#26159;&#19968;&#31181;&#20351;&#29992;s(CASP)&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#27861;&#24459;&#25512;&#29702;&#21644;&#34892;&#20026;&#35009;&#37327;&#65292;&#20197;&#35299;&#20915;&#33258;&#39030;&#21521;&#19979;&#21644;&#33258;&#24213;&#21521;&#19978;&#25191;&#34892;&#27169;&#22411;&#26080;&#27861;&#34920;&#36798;&#30340;&#27169;&#31946;&#27010;&#24565;&#21644;&#23436;&#25972;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14511</link><description>&lt;p&gt;
&#20351;&#29992;s(LAW)&#36827;&#34892;&#33258;&#21160;&#21270;&#27861;&#24459;&#25512;&#29702;&#19982;&#34892;&#20026;&#35009;&#37327;
&lt;/p&gt;
&lt;p&gt;
Automated legal reasoning with discretion to act using s(LAW). (arXiv:2401.14511v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14511
&lt;/p&gt;
&lt;p&gt;
s(LAW)&#26159;&#19968;&#31181;&#20351;&#29992;s(CASP)&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#27861;&#24459;&#25512;&#29702;&#21644;&#34892;&#20026;&#35009;&#37327;&#65292;&#20197;&#35299;&#20915;&#33258;&#39030;&#21521;&#19979;&#21644;&#33258;&#24213;&#21521;&#19978;&#25191;&#34892;&#27169;&#22411;&#26080;&#27861;&#34920;&#36798;&#30340;&#27169;&#31946;&#27010;&#24565;&#21644;&#23436;&#25972;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#27861;&#24459;&#25512;&#29702;&#21450;&#20854;&#22312;&#26234;&#33021;&#21512;&#32422;&#21644;&#33258;&#21160;&#21270;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#36947;&#24503;&#21644;&#27861;&#24459;&#38382;&#39064;&#20351;&#24471;&#33258;&#21160;&#25512;&#29702;&#22120;&#38656;&#35201;&#20197;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#26041;&#24335;&#26469;&#35299;&#37322;&#32473;&#20986;&#30340;&#24314;&#35758;&#12290;&#36923;&#36753;&#32534;&#31243;&#65292;&#29305;&#21035;&#26159;&#31572;&#26696;&#38598;&#32534;&#31243;&#65292;&#20855;&#26377;&#20016;&#23500;&#30340;&#35821;&#20041;&#65292;&#21487;&#20197;&#38750;&#24120;&#31616;&#27905;&#22320;&#34920;&#36798;&#22797;&#26434;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#22312;&#22522;&#20110;Prolog&#30340;&#33258;&#39030;&#21521;&#19979;&#25191;&#34892;&#27169;&#22411;&#21644;&#22522;&#20110;ASP&#30340;&#33258;&#24213;&#21521;&#19978;&#25191;&#34892;&#27169;&#22411;&#20013;&#65292;&#26080;&#27861;&#34920;&#36798;&#34892;&#20026;&#35009;&#37327;&#21644;&#20854;&#20182;&#27169;&#31946;&#27010;&#24565;&#65292;&#22914;&#27495;&#20041;&#65292;&#24182;&#19988;&#22312;ASP&#20013;&#30340;&#29702;&#30001;&#26159;&#19981;&#23436;&#25972;&#21644;/&#25110;&#19981;&#21487;&#25193;&#23637;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;s(CASP)&#65292;&#19968;&#31181;&#29992;&#20110;&#35859;&#35789;ASP&#30340;&#33258;&#39030;&#21521;&#19979;&#25191;&#34892;&#27169;&#22411;&#65292;&#26681;&#25454;&#19968;&#32452;&#27169;&#24335;&#26469;&#24314;&#27169;&#27169;&#31946;&#27010;&#24565;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#31216;&#20026;s(LAW)&#65292;&#29992;&#20110;&#24314;&#27169;&#12289;&#25512;&#29702;&#21644;&#35299;&#37322;&#36866;&#29992;&#30340;&#27861;&#24459;&#65292;&#24182;&#36890;&#36807;&#32763;&#35793;&#65288;&#21644;&#22522;&#20934;&#27979;&#35797;&#65289;&#19968;&#20010;&#20195;&#34920;&#24615;&#29992;&#20363;&#65292;&#21363;&#20837;&#23398;&#26631;&#20934;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated legal reasoning and its application in smart contracts and automated decisions are increasingly attracting interest. In this context, ethical and legal concerns make it necessary for automated reasoners to justify in human-understandable terms the advice given. Logic Programming, specially Answer Set Programming, has a rich semantics and has been used to very concisely express complex knowledge. However, modelling discretionality to act and other vague concepts such as ambiguity cannot be expressed in top-down execution models based on Prolog, and in bottom-up execution models based on ASP the justifications are incomplete and/or not scalable. We propose to use s(CASP), a top-down execution model for predicate ASP, to model vague concepts following a set of patterns. We have implemented a framework, called s(LAW), to model, reason, and justify the applicable legislation and validate it by translating (and benchmarking) a representative use case, the criteria for the admission
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;&#22522;&#20110;&#26694;&#26550;&#65292;&#36890;&#36807;&#31574;&#30053;&#24615;&#22320;&#20915;&#23450;&#35266;&#27979;&#26102;&#38388;&#24182;&#20174;&#31232;&#30095;&#37319;&#26679;&#30340;&#35266;&#27979;&#20013;&#37325;&#24314;&#25968;&#25454;&#27969;&#65292;&#20197;&#23454;&#29616;&#22312;&#30005;&#21147;&#21463;&#38480;&#30340;&#35774;&#22791;&#19978;&#36827;&#34892;&#38271;&#26399;&#20132;&#36890;&#25968;&#25454;&#25910;&#38598;&#30340;&#26368;&#23567;&#24615;&#33021;&#25439;&#22833;&#21644;&#26174;&#33879;&#24310;&#38271;&#31995;&#32479;&#23551;&#21629;&#12290;</title><link>http://arxiv.org/abs/2401.14504</link><description>&lt;p&gt;
&#23398;&#20064;&#20309;&#26102;&#22312;&#30005;&#21147;&#21463;&#38480;&#35774;&#22791;&#19978;&#36827;&#34892;&#38271;&#26399;&#20132;&#36890;&#25968;&#25454;&#25910;&#38598;
&lt;/p&gt;
&lt;p&gt;
Learning When to See for Long-term Traffic Data Collection on Power-constrained Devices. (arXiv:2401.14504v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;&#22522;&#20110;&#26694;&#26550;&#65292;&#36890;&#36807;&#31574;&#30053;&#24615;&#22320;&#20915;&#23450;&#35266;&#27979;&#26102;&#38388;&#24182;&#20174;&#31232;&#30095;&#37319;&#26679;&#30340;&#35266;&#27979;&#20013;&#37325;&#24314;&#25968;&#25454;&#27969;&#65292;&#20197;&#23454;&#29616;&#22312;&#30005;&#21147;&#21463;&#38480;&#30340;&#35774;&#22791;&#19978;&#36827;&#34892;&#38271;&#26399;&#20132;&#36890;&#25968;&#25454;&#25910;&#38598;&#30340;&#26368;&#23567;&#24615;&#33021;&#25439;&#22833;&#21644;&#26174;&#33879;&#24310;&#38271;&#31995;&#32479;&#23551;&#21629;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25910;&#38598;&#20132;&#36890;&#25968;&#25454;&#23545;&#20110;&#20132;&#36890;&#31995;&#32479;&#21644;&#22478;&#24066;&#35268;&#21010;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#36890;&#24120;&#26356;&#24076;&#26395;&#36890;&#36807;&#26131;&#20110;&#37096;&#32626;&#20294;&#30005;&#21147;&#21463;&#38480;&#30340;&#35774;&#22791;&#36827;&#34892;&#65292;&#36825;&#26159;&#30001;&#20110;&#30005;&#21147;&#21644;&#32593;&#32476;&#22522;&#30784;&#35774;&#26045;&#30340;&#19981;&#21487;&#29992;&#24615;&#25110;&#39640;&#25104;&#26412;&#25152;&#33268;&#12290;&#26377;&#38480;&#30340;&#30005;&#21147;&#24847;&#21619;&#30528;&#25968;&#25454;&#25910;&#38598;&#25345;&#32493;&#26102;&#38388;&#21644;&#20934;&#30830;&#24615;/&#20998;&#36776;&#29575;&#20043;&#38388;&#19981;&#21487;&#36991;&#20813;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#31574;&#30053;&#24615;&#22320;&#20915;&#23450;&#30005;&#27744;&#20379;&#30005;&#35774;&#22791;&#30340;&#35266;&#27979;&#26102;&#38388;&#65292;&#24182;&#20174;&#31232;&#30095;&#37319;&#26679;&#30340;&#35266;&#27979;&#20013;&#37325;&#24314;&#23436;&#25972;&#30340;&#25968;&#25454;&#27969;&#65292;&#20174;&#32780;&#23454;&#29616;&#26368;&#23567;&#30340;&#24615;&#33021;&#25439;&#22833;&#21644;&#26174;&#33879;&#24310;&#38271;&#31995;&#32479;&#23551;&#21629;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30001;&#39044;&#27979;&#22120;&#12289;&#25511;&#21046;&#22120;&#21644;&#20272;&#35745;&#22120;&#32452;&#25104;&#12290;&#39044;&#27979;&#22120;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#26469;&#39044;&#27979;&#22266;&#23450;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#26410;&#26469;&#36235;&#21183;&#12290;&#25511;&#21046;&#22120;&#20351;&#29992;&#36825;&#20123;&#39044;&#27979;&#26469;&#30830;&#23450;&#19979;&#19968;&#20010;&#26368;&#20248;&#30340;&#25968;&#25454;&#25910;&#38598;&#26102;&#38388;&#12290;&#26368;&#21518;&#65292;&#20272;&#35745;&#22120;&#20174;&#37319;&#26679;&#35266;&#27979;&#20013;&#37325;&#24314;&#23436;&#25972;&#30340;&#25968;&#25454;&#37197;&#32622;&#25991;&#20214;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#35813;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collecting traffic data is crucial for transportation systems and urban planning, and is often more desirable through easy-to-deploy but power-constrained devices, due to the unavailability or high cost of power and network infrastructure. The limited power means an inevitable trade-off between data collection duration and accuracy/resolution. We introduce a novel learning-based framework that strategically decides observation timings for battery-powered devices and reconstructs the full data stream from sparsely sampled observations, resulting in minimal performance loss and a significantly prolonged system lifetime. Our framework comprises a predictor, a controller, and an estimator. The predictor utilizes historical data to forecast future trends within a fixed time horizon. The controller uses the forecasts to determine the next optimal timing for data collection. Finally, the estimator reconstructs the complete data profile from the sampled observations. We evaluate the performanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#32452;&#25351;&#21335;&#65292;&#36890;&#36807;&#32771;&#34385;&#27169;&#22411;&#36229;&#21442;&#25968;&#23545;GPU&#19978;&#25191;&#34892;&#30340;&#35745;&#31639;&#20869;&#26680;&#30340;&#25928;&#29575;&#30340;&#24433;&#21709;&#65292;&#26469;&#26368;&#22823;&#21270;&#21464;&#25442;&#22120;&#27169;&#22411;&#30340;&#36816;&#34892;&#26102;&#24615;&#33021;&#12290;&#30456;&#27604;&#20110;&#20855;&#26377;&#30456;&#20284;&#21442;&#25968;&#25968;&#37327;&#20294;&#24418;&#29366;&#26410;&#32463;&#20248;&#21270;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#39640;&#25928;&#27169;&#22411;&#24418;&#29366;&#30340;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;39%&#30340;&#21534;&#21520;&#37327;&#19988;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.14489</link><description>&lt;p&gt;
&#19982;&#30828;&#20214;&#20849;&#21516;&#35774;&#35745;&#27169;&#22411;&#26550;&#26500;&#30340;&#29702;&#30001;
&lt;/p&gt;
&lt;p&gt;
The Case for Co-Designing Model Architectures with Hardware. (arXiv:2401.14489v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#32452;&#25351;&#21335;&#65292;&#36890;&#36807;&#32771;&#34385;&#27169;&#22411;&#36229;&#21442;&#25968;&#23545;GPU&#19978;&#25191;&#34892;&#30340;&#35745;&#31639;&#20869;&#26680;&#30340;&#25928;&#29575;&#30340;&#24433;&#21709;&#65292;&#26469;&#26368;&#22823;&#21270;&#21464;&#25442;&#22120;&#27169;&#22411;&#30340;&#36816;&#34892;&#26102;&#24615;&#33021;&#12290;&#30456;&#27604;&#20110;&#20855;&#26377;&#30456;&#20284;&#21442;&#25968;&#25968;&#37327;&#20294;&#24418;&#29366;&#26410;&#32463;&#20248;&#21270;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#39640;&#25928;&#27169;&#22411;&#24418;&#29366;&#30340;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;39%&#30340;&#21534;&#21520;&#37327;&#19988;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;GPU&#36127;&#36131;&#35757;&#32451;&#22823;&#37096;&#20998;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#22312;&#35774;&#35745;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26102;&#24448;&#24448;&#24573;&#35270;&#20102;&#20854;&#26550;&#26500;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20462;&#25913;&#20026;&#26356;&#36866;&#21512;&#30446;&#26631;&#30828;&#20214;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#36816;&#34892;&#26102;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#32452;&#25351;&#21335;&#65292;&#29992;&#20110;&#20351;&#29992;&#25143;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#20182;&#20204;&#30340;&#21464;&#25442;&#22120;&#27169;&#22411;&#30340;&#36816;&#34892;&#26102;&#24615;&#33021;&#12290;&#36825;&#20123;&#25351;&#21335;&#26159;&#36890;&#36807;&#20180;&#32454;&#32771;&#34385;&#25511;&#21046;&#27169;&#22411;&#24418;&#29366;&#30340;&#21508;&#31181;&#27169;&#22411;&#36229;&#21442;&#25968;&#23545;GPU&#19978;&#25191;&#34892;&#30340;&#24213;&#23618;&#35745;&#31639;&#20869;&#26680;&#30340;&#25928;&#29575;&#30340;&#24433;&#21709;&#32780;&#21019;&#24314;&#30340;&#12290;&#25105;&#20204;&#21457;&#29616;&#20855;&#26377;&#39640;&#25928;&#27169;&#22411;&#24418;&#29366;&#30340;&#27169;&#22411;&#30340;&#21534;&#21520;&#37327;&#27604;&#20855;&#26377;&#30456;&#20284;&#21442;&#25968;&#25968;&#37327;&#20294;&#24418;&#29366;&#26410;&#32463;&#20248;&#21270;&#30340;&#27169;&#22411;&#39640;&#20986;39&#65285;&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While GPUs are responsible for training the vast majority of state-of-the-art deep learning models, the implications of their architecture are often overlooked when designing new deep learning (DL) models. As a consequence, modifying a DL model to be more amenable to the target hardware can significantly improve the runtime performance of DL training and inference. In this paper, we provide a set of guidelines for users to maximize the runtime performance of their transformer models. These guidelines have been created by carefully considering the impact of various model hyperparameters controlling model shape on the efficiency of the underlying computation kernels executed on the GPU. We find the throughput of models with efficient model shapes is up to 39\% higher while preserving accuracy compared to models with a similar number of parameters but with unoptimized shapes.
&lt;/p&gt;</description></item><item><title>Scilab-RL&#26159;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#20154;&#20195;&#29702;&#30340;&#35748;&#30693;&#24314;&#27169;&#21644;&#22686;&#24378;&#23398;&#20064;&#30740;&#31350;&#30340;&#36719;&#20214;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#31283;&#23450;&#30340;&#22522;&#32447;3&#21644;OpenAI gym&#25509;&#21475;&#65292;&#20197;&#21450;&#23454;&#39564;&#21487;&#35270;&#21270;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#21151;&#33021;&#65292;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#20102;&#30740;&#31350;&#20135;&#20986;&#12290;</title><link>http://arxiv.org/abs/2401.14488</link><description>&lt;p&gt;
Scilab-RL&#65306;&#29992;&#20110;&#39640;&#25928;&#22686;&#24378;&#23398;&#20064;&#21644;&#35748;&#30693;&#24314;&#27169;&#30740;&#31350;&#30340;&#36719;&#20214;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Scilab-RL: A software framework for efficient reinforcement learning and cognitive modeling research. (arXiv:2401.14488v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14488
&lt;/p&gt;
&lt;p&gt;
Scilab-RL&#26159;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#20154;&#20195;&#29702;&#30340;&#35748;&#30693;&#24314;&#27169;&#21644;&#22686;&#24378;&#23398;&#20064;&#30740;&#31350;&#30340;&#36719;&#20214;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#31283;&#23450;&#30340;&#22522;&#32447;3&#21644;OpenAI gym&#25509;&#21475;&#65292;&#20197;&#21450;&#23454;&#39564;&#21487;&#35270;&#21270;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#21151;&#33021;&#65292;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#20102;&#30740;&#31350;&#20135;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35748;&#30693;&#24314;&#27169;&#21644;&#22686;&#24378;&#23398;&#20064;&#30340;&#19968;&#20010;&#38382;&#39064;&#26159;&#65292;&#30740;&#31350;&#20154;&#21592;&#33457;&#36153;&#22826;&#22810;&#26102;&#38388;&#26469;&#35774;&#32622;&#36866;&#24403;&#30340;&#35745;&#31639;&#26694;&#26550;&#36827;&#34892;&#23454;&#39564;&#12290;&#23384;&#22312;&#35768;&#22810;&#24403;&#21069;&#22686;&#24378;&#23398;&#20064;&#31639;&#27861;&#30340;&#24320;&#28304;&#23454;&#29616;&#65292;&#20294;&#32570;&#20047;&#19968;&#20010;&#27169;&#22359;&#21270;&#30340;&#24037;&#20855;&#22871;&#20214;&#65292;&#32467;&#21512;&#19981;&#21516;&#30340;&#26426;&#22120;&#20154;&#27169;&#25311;&#22120;&#21644;&#24179;&#21488;&#12289;&#25968;&#25454;&#21487;&#35270;&#21270;&#12289;&#36229;&#21442;&#25968;&#20248;&#21270;&#21644;&#22522;&#20934;&#23454;&#39564;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Scilab-RL&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#26426;&#22120;&#20154;&#20195;&#29702;&#30340;&#35748;&#30693;&#24314;&#27169;&#21644;&#22686;&#24378;&#23398;&#20064;&#30740;&#31350;&#30340;&#36719;&#20214;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#19987;&#27880;&#20110;&#20351;&#29992;&#31283;&#23450;&#30340;&#22522;&#32447;3&#21644;OpenAI gym&#25509;&#21475;&#36827;&#34892;&#30446;&#26631;&#26465;&#20214;&#22686;&#24378;&#23398;&#20064;&#12290;&#23427;&#25552;&#20379;&#20102;&#21407;&#29983;&#30340;&#23454;&#39564;&#21487;&#35270;&#21270;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#36825;&#20123;&#21151;&#33021;&#22914;&#20309;&#20351;&#30740;&#31350;&#20154;&#21592;&#21482;&#38656;&#26368;&#23569;&#30340;&#26102;&#38388;&#21644;&#31934;&#21147;&#23601;&#33021;&#36827;&#34892;&#23454;&#39564;&#65292;&#20174;&#32780;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#30740;&#31350;&#20135;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
One problem with researching cognitive modeling and reinforcement learning (RL) is that researchers spend too much time on setting up an appropriate computational framework for their experiments. Many open source implementations of current RL algorithms exist, but there is a lack of a modular suite of tools combining different robotic simulators and platforms, data visualization, hyperparameter optimization, and baseline experiments. To address this problem, we present Scilab-RL, a software framework for efficient research in cognitive modeling and reinforcement learning for robotic agents. The framework focuses on goal-conditioned reinforcement learning using Stable Baselines 3 and the OpenAI gym interface. It enables native possibilities for experiment visualizations and hyperparameter optimization. We describe how these features enable researchers to conduct experiments with minimal time effort, thus maximizing research output.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20845;&#39033;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#35774;&#35745;&#21407;&#21017;&#65292;&#20197;&#24212;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#29992;&#25143;&#20307;&#39564;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#31574;&#30053;&#23454;&#29616;&#36825;&#20123;&#21407;&#21017;&#12290;&#36825;&#20123;&#21407;&#21017;&#23558;&#26377;&#21161;&#20110;&#20419;&#36827;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2401.14484</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#35774;&#35745;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
Design Principles for Generative AI Applications. (arXiv:2401.14484v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14484
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20845;&#39033;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#35774;&#35745;&#21407;&#21017;&#65292;&#20197;&#24212;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#29992;&#25143;&#20307;&#39564;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#31574;&#30053;&#23454;&#29616;&#36825;&#20123;&#21407;&#21017;&#12290;&#36825;&#20123;&#21407;&#21017;&#23558;&#26377;&#21161;&#20110;&#20419;&#36827;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#35774;&#35745;&#25361;&#25112;&#12290;&#38543;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#20027;&#27969;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#36843;&#20999;&#38656;&#35201;&#25351;&#23548;&#22914;&#20309;&#35774;&#35745;&#29992;&#25143;&#20307;&#39564;&#20197;&#20419;&#36827;&#26377;&#25928;&#21644;&#23433;&#20840;&#20351;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20845;&#39033;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#35774;&#35745;&#21407;&#21017;&#65292;&#38024;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#29992;&#25143;&#20307;&#39564;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#23545;AI&#24212;&#29992;&#35774;&#35745;&#20013;&#24050;&#30693;&#38382;&#39064;&#36827;&#34892;&#20102;&#26032;&#30340;&#35299;&#37322;&#21644;&#25193;&#23637;&#12290;&#27599;&#20010;&#21407;&#21017;&#37117;&#37197;&#26377;&#19968;&#22871;&#36890;&#36807;&#29992;&#25143;&#20307;&#39564;&#33021;&#21147;&#25110;&#35774;&#35745;&#36807;&#31243;&#26469;&#23454;&#29616;&#35813;&#21407;&#21017;&#30340;&#35774;&#35745;&#31574;&#30053;&#12290;&#36825;&#20123;&#21407;&#21017;&#21644;&#31574;&#30053;&#26159;&#36890;&#36807;&#25991;&#29486;&#32508;&#36848;&#12289;&#35774;&#35745;&#23454;&#36341;&#21453;&#39304;&#12289;&#19982;&#30495;&#23454;&#19990;&#30028;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#39564;&#35777;&#20197;&#21450;&#20004;&#20010;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#35774;&#35745;&#36807;&#31243;&#20013;&#30340;&#34701;&#20837;&#32780;&#21457;&#23637;&#36215;&#26469;&#30340;&#12290;&#25105;&#20204;&#39044;&#35745;&#36825;&#20123;&#21407;&#21017;&#23558;&#26377;&#21161;&#20110;&#20419;&#36827;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI applications present unique design challenges. As generative AI technologies are increasingly being incorporated into mainstream applications, there is an urgent need for guidance on how to design user experiences that foster effective and safe use. We present six principles for the design of generative AI applications that address unique characteristics of generative AI UX and offer new interpretations and extensions of known issues in the design of AI applications. Each principle is coupled with a set of design strategies for implementing that principle via UX capabilities or through the design process. The principles and strategies were developed through an iterative process involving literature review, feedback from design practitioners, validation against real-world generative AI applications, and incorporation into the design process of two generative AI applications. We anticipate the principles to usefully inform the design of generative AI applications by driving
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#28145;&#24230;&#21487;&#20998;&#31163;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#35757;&#32451;&#30340;&#21367;&#31215;&#26680;&#20013;&#20986;&#29616;&#30340;&#21487;&#36776;&#21035;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#24335;&#65292;&#36825;&#20123;&#27169;&#24335;&#31867;&#20284;&#20110;&#39640;&#26031;&#24046;&#20998;&#20989;&#25968;&#21644;&#23427;&#20204;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#23548;&#25968;&#12290;&#30740;&#31350;&#36890;&#36807;&#23545;&#25968;&#30334;&#19975;&#20010;&#35757;&#32451;&#28388;&#27874;&#22120;&#36827;&#34892;&#26080;&#30417;&#30563;&#32858;&#31867;&#65292;&#25104;&#21151;&#23558;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20013;&#30340;&#22823;&#37096;&#20998;&#28388;&#27874;&#22120;&#36827;&#34892;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2401.14469</link><description>&lt;p&gt;
&#25581;&#31034;&#30475;&#19981;&#35265;&#30340;&#65306;&#35757;&#32451;&#30340;&#28145;&#24230;&#21487;&#20998;&#31163;&#21367;&#31215;&#26680;&#20013;&#30340;&#21487;&#35782;&#21035;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Unseen: Identifiable Clusters in Trained Depthwise Convolutional Kernels. (arXiv:2401.14469v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#28145;&#24230;&#21487;&#20998;&#31163;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#35757;&#32451;&#30340;&#21367;&#31215;&#26680;&#20013;&#20986;&#29616;&#30340;&#21487;&#36776;&#21035;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#24335;&#65292;&#36825;&#20123;&#27169;&#24335;&#31867;&#20284;&#20110;&#39640;&#26031;&#24046;&#20998;&#20989;&#25968;&#21644;&#23427;&#20204;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#23548;&#25968;&#12290;&#30740;&#31350;&#36890;&#36807;&#23545;&#25968;&#30334;&#19975;&#20010;&#35757;&#32451;&#28388;&#27874;&#22120;&#36827;&#34892;&#26080;&#30417;&#30563;&#32858;&#31867;&#65292;&#25104;&#21151;&#23558;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20013;&#30340;&#22823;&#37096;&#20998;&#28388;&#27874;&#22120;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#28145;&#24230;&#21487;&#20998;&#31163;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(DS-CNNs)&#30340;&#36827;&#23637;&#24050;&#32463;&#23548;&#33268;&#20102;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#26174;&#30528;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#20934;&#30830;&#24615;&#24046;&#36317;&#36229;&#36234;&#20102;&#32463;&#20856;CNNs&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;DS-CNN&#26550;&#26500;&#30340;&#21478;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#29305;&#24615;&#65306;&#22312;&#20854;&#25152;&#26377;&#23618;&#30340;&#35757;&#32451;&#28145;&#24230;&#21367;&#31215;&#26680;&#20013;&#20986;&#29616;&#20102;&#21487;&#36776;&#21035;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#24335;&#12290;&#36890;&#36807;&#23545;&#25968;&#30334;&#19975;&#20010;&#19981;&#21516;&#22823;&#23567;&#21644;&#26469;&#33258;&#21508;&#31181;&#27169;&#22411;&#30340;&#35757;&#32451;&#28388;&#27874;&#22120;&#30340;&#24191;&#27867;&#20998;&#26512;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#32534;&#30721;&#22120;&#30340;&#26080;&#30417;&#30563;&#32858;&#31867;&#23558;&#36825;&#20123;&#28388;&#27874;&#22120;&#20998;&#31867;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#36825;&#20123;&#27169;&#24335;&#25910;&#25947;&#25104;&#20960;&#20010;&#20027;&#35201;&#30340;&#32858;&#31867;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recent advances in depthwise-separable convolutional neural networks (DS-CNNs) have led to novel architectures, that surpass the performance of classical CNNs, by a considerable scalability and accuracy margin. This paper reveals another striking property of DS-CNN architectures: discernible and explainable patterns emerge in their trained depthwise convolutional kernels in all layers. Through an extensive analysis of millions of trained filters, with different sizes and from various models, we employed unsupervised clustering with autoencoders, to categorize these filters. Astonishingly, the patterns converged into a few main clusters, each resembling the difference of Gaussian (DoG) functions, and their first and second-order derivatives. Notably, we were able to classify over 95\% and 90\% of the filters from state-of-the-art ConvNextV2 and ConvNeXt models, respectively. This finding is not merely a technological curiosity; it echoes the foundational models neuroscientists have long
&lt;/p&gt;</description></item><item><title>Marabou 2.0&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#31070;&#32463;&#32593;&#32476;&#24418;&#24335;&#20998;&#26512;&#22120;&#65292;&#20855;&#26377;&#21019;&#26032;&#30340;&#26550;&#26500;&#35774;&#35745;&#21644;&#24341;&#20837;&#30340;&#20027;&#35201;&#21151;&#33021;&#21644;&#32452;&#20214;&#12290;</title><link>http://arxiv.org/abs/2401.14461</link><description>&lt;p&gt;
Marabou 2.0: &#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#31070;&#32463;&#32593;&#32476;&#24418;&#24335;&#20998;&#26512;&#22120;
&lt;/p&gt;
&lt;p&gt;
Marabou 2.0: A Versatile Formal Analyzer of Neural Networks. (arXiv:2401.14461v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14461
&lt;/p&gt;
&lt;p&gt;
Marabou 2.0&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#31070;&#32463;&#32593;&#32476;&#24418;&#24335;&#20998;&#26512;&#22120;&#65292;&#20855;&#26377;&#21019;&#26032;&#30340;&#26550;&#26500;&#35774;&#35745;&#21644;&#24341;&#20837;&#30340;&#20027;&#35201;&#21151;&#33021;&#21644;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#20851;&#20110;Marabou&#26694;&#26550;2.0&#29256;&#26412;&#30340;&#32508;&#21512;&#31995;&#32479;&#25551;&#36848;&#65292;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24418;&#24335;&#20998;&#26512;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#24037;&#20855;&#30340;&#26550;&#26500;&#35774;&#35745;&#65292;&#24182;&#20171;&#32461;&#20102;&#33258;&#21021;&#22987;&#21457;&#24067;&#20197;&#26469;&#24341;&#20837;&#30340;&#20027;&#35201;&#21151;&#33021;&#21644;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper serves as a comprehensive system description of version 2.0 of the Marabou framework for formal analysis of neural networks. We discuss the tool's architectural design and highlight the major features and components introduced since its initial release.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Wordflow&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#31038;&#20132;&#25552;&#31034;&#24037;&#31243;&#30340;&#26041;&#24335;&#35753;&#38750;&#19987;&#23478;&#29992;&#25143;&#26356;&#22909;&#22320;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#21019;&#24314;&#12289;&#36816;&#34892;&#12289;&#20849;&#20139;&#21644;&#21457;&#29616;LLM&#25552;&#31034;&#12290;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;&#32593;&#32476;&#25216;&#26415;&#65292;Wordflow&#20801;&#35768;&#29992;&#25143;&#22312;&#27983;&#35272;&#22120;&#20013;&#26412;&#22320;&#21644;&#31169;&#19979;&#36816;&#34892;LLM&#12290;</title><link>http://arxiv.org/abs/2401.14447</link><description>&lt;p&gt;
Wordflow: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31038;&#20132;&#25552;&#31034;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Wordflow: Social Prompt Engineering for Large Language Models. (arXiv:2401.14447v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Wordflow&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#31038;&#20132;&#25552;&#31034;&#24037;&#31243;&#30340;&#26041;&#24335;&#35753;&#38750;&#19987;&#23478;&#29992;&#25143;&#26356;&#22909;&#22320;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#21019;&#24314;&#12289;&#36816;&#34892;&#12289;&#20849;&#20139;&#21644;&#21457;&#29616;LLM&#25552;&#31034;&#12290;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;&#32593;&#32476;&#25216;&#26415;&#65292;Wordflow&#20801;&#35768;&#29992;&#25143;&#22312;&#27983;&#35272;&#22120;&#20013;&#26412;&#22320;&#21644;&#31169;&#19979;&#36816;&#34892;LLM&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#25165;&#33021;&#26377;&#25928;&#20351;&#29992;&#12290;&#23545;&#20110;&#38750;&#19987;&#23478;&#26469;&#35828;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36807;&#31243;&#65292;&#22240;&#20026;&#20182;&#20204;&#23545;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#19981;&#37027;&#20040;&#29087;&#24713;&#12290;&#34429;&#28982;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#20123;&#25216;&#26415;&#21644;&#24037;&#20855;&#26469;&#24110;&#21161;LLM&#29992;&#25143;&#35774;&#35745;&#25552;&#31034;&#65292;&#20294;&#36825;&#20123;&#20316;&#21697;&#20027;&#35201;&#38024;&#23545;&#30340;&#26159;AI&#24212;&#29992;&#24320;&#21457;&#32773;&#32780;&#19981;&#26159;&#38750;&#19987;&#23478;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31038;&#20132;&#25552;&#31034;&#24037;&#31243;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#31038;&#20132;&#35745;&#31639;&#25216;&#26415;&#20419;&#36827;&#21327;&#20316;&#25552;&#31034;&#35774;&#35745;&#30340;&#26032;&#33539;&#24335;&#12290;&#20026;&#20102;&#30740;&#31350;&#31038;&#20132;&#25552;&#31034;&#24037;&#31243;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Wordflow&#65292;&#19968;&#20010;&#24320;&#28304;&#30340;&#31038;&#20132;&#25991;&#26412;&#32534;&#36753;&#22120;&#65292;&#20351;&#26222;&#36890;&#29992;&#25143;&#21487;&#20197;&#36731;&#26494;&#21019;&#24314;&#12289;&#36816;&#34892;&#12289;&#20849;&#20139;&#21644;&#21457;&#29616;LLM&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;&#32593;&#32476;&#25216;&#26415;&#65292;Wordflow&#20801;&#35768;&#29992;&#25143;&#22312;&#20854;&#27983;&#35272;&#22120;&#20013;&#26412;&#22320;&#21644;&#31169;&#19979;&#36816;&#34892;LLM&#12290;&#20004;&#20010;&#20351;&#29992;&#22330;&#26223;&#31361;&#20986;&#20102;&#31038;&#20132;&#25552;&#31034;&#24037;&#31243;&#21644;&#25105;&#20204;&#30340;&#24037;&#20855;&#22914;&#20309;&#22686;&#24378;&#26222;&#36890;&#20154;&#19982;LLM&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) require well-crafted prompts for effective use. Prompt engineering, the process of designing prompts, is challenging, particularly for non-experts who are less familiar with AI technologies. While researchers have proposed techniques and tools to assist LLM users in prompt design, these works primarily target AI application developers rather than non-experts. To address this research gap, we propose social prompt engineering, a novel paradigm that leverages social computing techniques to facilitate collaborative prompt design. To investigate social prompt engineering, we introduce Wordflow, an open-source and social text editor that enables everyday users to easily create, run, share, and discover LLM prompts. Additionally, by leveraging modern web technologies, Wordflow allows users to run LLMs locally and privately in their browsers. Two usage scenarios highlight how social prompt engineering and our tool can enhance laypeople's interaction with LLMs. Wor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#40657;&#30418;&#23457;&#35745;&#30340;&#23616;&#38480;&#24615;&#20197;&#21450;&#30333;&#30418;&#21644;&#36229;&#36234;&#26694;&#26550;&#23457;&#35745;&#30340;&#20248;&#21183;&#65292;&#40657;&#30418;&#35775;&#38382;&#23545;&#20110;&#20005;&#26684;&#30340;&#20154;&#24037;&#26234;&#33021;&#23457;&#35745;&#26159;&#19981;&#20805;&#20998;&#30340;&#12290;</title><link>http://arxiv.org/abs/2401.14446</link><description>&lt;p&gt;
&#40657;&#30418;&#35775;&#38382;&#23545;&#20110;&#20005;&#26684;&#30340;&#20154;&#24037;&#26234;&#33021;&#23457;&#35745;&#26159;&#19981;&#20805;&#20998;&#30340;
&lt;/p&gt;
&lt;p&gt;
Black-Box Access is Insufficient for Rigorous AI Audits. (arXiv:2401.14446v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#40657;&#30418;&#23457;&#35745;&#30340;&#23616;&#38480;&#24615;&#20197;&#21450;&#30333;&#30418;&#21644;&#36229;&#36234;&#26694;&#26550;&#23457;&#35745;&#30340;&#20248;&#21183;&#65292;&#40657;&#30418;&#35775;&#38382;&#23545;&#20110;&#20005;&#26684;&#30340;&#20154;&#24037;&#26234;&#33021;&#23457;&#35745;&#26159;&#19981;&#20805;&#20998;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22806;&#37096;&#23545;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#23457;&#35745;&#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#35748;&#20026;&#26159;&#20154;&#24037;&#26234;&#33021;&#27835;&#29702;&#30340;&#19968;&#20010;&#20851;&#38190;&#26426;&#21046;&#12290;&#23457;&#35745;&#30340;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#23457;&#35745;&#21592;&#34987;&#25480;&#20104;&#30340;&#31995;&#32479;&#35775;&#38382;&#31243;&#24230;&#12290;&#36817;&#26399;&#23545;&#26368;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#23457;&#35745;&#20027;&#35201;&#20381;&#36182;&#20110;&#40657;&#30418;&#35775;&#38382;&#65292;&#23457;&#35745;&#21592;&#21482;&#33021;&#26597;&#35810;&#31995;&#32479;&#24182;&#35266;&#23519;&#20854;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#23545;&#31995;&#32479;&#20869;&#37096;&#24037;&#20316;&#26041;&#24335;&#65288;&#20363;&#22914;&#26435;&#37325;&#12289;&#28608;&#27963;&#12289;&#26799;&#24230;&#65289;&#30340;&#36879;&#26126;&#35775;&#38382;&#20801;&#35768;&#23457;&#35745;&#21592;&#36827;&#34892;&#26356;&#24378;&#30340;&#25915;&#20987;&#65292;&#26356;&#20840;&#38754;&#22320;&#35299;&#37322;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#12290;&#21516;&#26102;&#65292;&#23545;&#20854;&#22521;&#35757;&#21644;&#37096;&#32626;&#20449;&#24687;&#30340;&#36229;&#36234;&#26694;&#26550;&#35775;&#38382;&#65288;&#20363;&#22914;&#26041;&#27861;&#35770;&#12289;&#20195;&#30721;&#12289;&#25991;&#26723;&#12289;&#36229;&#21442;&#25968;&#12289;&#25968;&#25454;&#12289;&#37096;&#32626;&#32454;&#33410;&#12289;&#20869;&#37096;&#35780;&#20272;&#32467;&#26524;&#65289;&#20801;&#35768;&#23457;&#35745;&#21592;&#23457;&#26597;&#24320;&#21457;&#36807;&#31243;&#65292;&#24182;&#35774;&#35745;&#26356;&#20855;&#38024;&#23545;&#24615;&#30340;&#35780;&#20272;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#40657;&#30418;&#23457;&#35745;&#30340;&#23616;&#38480;&#24615;&#20197;&#21450;&#30333;&#30418;&#21644;&#36229;&#36234;&#26694;&#26550;&#23457;&#35745;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#25216;&#26415;&#21644;&#29983;&#29702;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
External audits of AI systems are increasingly recognized as a key mechanism for AI governance. The effectiveness of an audit, however, depends on the degree of system access granted to auditors. Recent audits of state-of-the-art AI systems have primarily relied on black-box access, in which auditors can only query the system and observe its outputs. However, white-box access to the system's inner workings (e.g., weights, activations, gradients) allows an auditor to perform stronger attacks, more thoroughly interpret models, and conduct fine-tuning. Meanwhile, outside-the-box access to its training and deployment information (e.g., methodology, code, documentation, hyperparameters, data, deployment details, findings from internal evaluations) allows for auditors to scrutinize the development process and design more targeted evaluations. In this paper, we examine the limitations of black-box audits and the advantages of white- and outside-the-box audits. We also discuss technical, physi
&lt;/p&gt;</description></item><item><title>ICASSP 2024&#35821;&#38899;&#20449;&#21495;&#25913;&#36827;&#22823;&#25361;&#25112;&#26088;&#22312;&#20419;&#36827;&#36890;&#20449;&#31995;&#32479;&#20013;&#25552;&#39640;&#35821;&#38899;&#20449;&#21495;&#36136;&#37327;&#30340;&#30740;&#31350;&#12290;&#36890;&#36807;&#24341;&#20837;&#25968;&#25454;&#38598;&#21512;&#25104;&#22120;&#12289;&#23458;&#35266;&#25351;&#26631;&#12289;&#27979;&#35797;&#38598;&#36716;&#24405;&#21644;&#26032;&#30340;&#25351;&#26631;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;13&#20010;&#23454;&#26102;&#31995;&#32479;&#21644;11&#20010;&#38750;&#23454;&#26102;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2401.14444</link><description>&lt;p&gt;
ICASSP 2024&#35821;&#38899;&#20449;&#21495;&#25913;&#36827;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
ICASSP 2024 Speech Signal Improvement Challenge. (arXiv:2401.14444v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14444
&lt;/p&gt;
&lt;p&gt;
ICASSP 2024&#35821;&#38899;&#20449;&#21495;&#25913;&#36827;&#22823;&#25361;&#25112;&#26088;&#22312;&#20419;&#36827;&#36890;&#20449;&#31995;&#32479;&#20013;&#25552;&#39640;&#35821;&#38899;&#20449;&#21495;&#36136;&#37327;&#30340;&#30740;&#31350;&#12290;&#36890;&#36807;&#24341;&#20837;&#25968;&#25454;&#38598;&#21512;&#25104;&#22120;&#12289;&#23458;&#35266;&#25351;&#26631;&#12289;&#27979;&#35797;&#38598;&#36716;&#24405;&#21644;&#26032;&#30340;&#25351;&#26631;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;13&#20010;&#23454;&#26102;&#31995;&#32479;&#21644;11&#20010;&#38750;&#23454;&#26102;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ICASSP 2024&#35821;&#38899;&#20449;&#21495;&#25913;&#36827;&#22823;&#25361;&#25112;&#26088;&#22312;&#20419;&#36827;&#36890;&#20449;&#31995;&#32479;&#20013;&#25552;&#39640;&#35821;&#38899;&#20449;&#21495;&#36136;&#37327;&#30340;&#30740;&#31350;&#12290;&#36825;&#26159;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#25361;&#25112;&#65292;&#24314;&#31435;&#22312;&#21069;&#19968;&#27425;ICASSP 2023&#22823;&#25361;&#25112;&#30340;&#25104;&#21151;&#22522;&#30784;&#19978;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#25968;&#25454;&#38598;&#21512;&#25104;&#22120;&#12289;&#20026;&#25105;&#20204;&#30340;&#25193;&#23637;P.804&#27979;&#35797;&#24341;&#20837;&#23458;&#35266;&#25351;&#26631;&#12289;2023&#27979;&#35797;&#38598;&#30340;&#36716;&#24405;&#20197;&#21450;&#23558;Word Accuracy (WA)&#20316;&#20026;&#19968;&#20010;&#25351;&#26631;&#65292;&#26469;&#22686;&#24378;&#31454;&#20105;&#12290;&#25105;&#20204;&#20351;&#29992;&#23458;&#35266;&#30340;Word Accuracy&#25351;&#26631;&#21644;&#20027;&#35266;&#30340;P.804&#25351;&#26631;&#35780;&#20272;&#20102;&#24635;&#20849;13&#20010;&#23454;&#26102;&#31995;&#32479;&#21644;11&#20010;&#38750;&#23454;&#26102;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ICASSP 2024 Speech Signal Improvement Grand Challenge is intended to stimulate research in the area of improving the speech signal quality in communication systems. This marks our second challenge, building upon the success from the previous ICASSP 2023 Grand Challenge. We enhance the competition by introducing a dataset synthesizer, enabling all participating teams to start at a higher baseline, an objective metric for our extended P.804 tests, transcripts for the 2023 test set, and we add Word Accuracy (WAcc) as a metric. We evaluate a total of 13 systems in the real-time track and 11 systems in the non-real-time track using both subjective P.804 and objective Word Accuracy metrics.
&lt;/p&gt;</description></item><item><title>&#36825;&#20221;&#35770;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#26368;&#20808;&#36827;&#30340;NLI&#27169;&#22411;&#23545;&#24494;&#23567;&#30340;&#35821;&#20041;&#20445;&#25345;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#38750;&#24120;&#25935;&#24863;&#65292;&#23548;&#33268;&#25512;&#26029;&#32467;&#26524;&#19981;&#19968;&#33268;&#12290;&#20854;&#34892;&#20026;&#19982;&#23545;&#32452;&#21512;&#35821;&#20041;&#30340;&#26377;&#25928;&#29702;&#35299;&#19981;&#21516;&#65292;&#36825;&#23545;&#24403;&#21069;NLI&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.14440</link><description>&lt;p&gt;
&#35821;&#20041;&#25935;&#24863;&#24615;&#21644;&#19981;&#19968;&#33268;&#30340;&#39044;&#27979;&#65306;&#34913;&#37327;NLI&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
Semantic Sensitivities and Inconsistent Predictions: Measuring the Fragility of NLI Models. (arXiv:2401.14440v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14440
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#35770;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#26368;&#20808;&#36827;&#30340;NLI&#27169;&#22411;&#23545;&#24494;&#23567;&#30340;&#35821;&#20041;&#20445;&#25345;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#38750;&#24120;&#25935;&#24863;&#65292;&#23548;&#33268;&#25512;&#26029;&#32467;&#26524;&#19981;&#19968;&#33268;&#12290;&#20854;&#34892;&#20026;&#19982;&#23545;&#32452;&#21512;&#35821;&#20041;&#30340;&#26377;&#25928;&#29702;&#35299;&#19981;&#21516;&#65292;&#36825;&#23545;&#24403;&#21069;NLI&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#22522;&#20110;transformer&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#27169;&#22411;&#30340;&#26032;&#33021;&#21147;&#36827;&#34892;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23427;&#20204;&#20855;&#22791;&#23545;&#35789;&#27719;&#21644;&#32452;&#21512;&#35821;&#20041;&#30340;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#36825;&#20123;&#35828;&#27861;&#24212;&#35813;&#25345;&#20445;&#30041;&#24577;&#24230;&#65306;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#27169;&#22411;&#23545;&#24494;&#23567;&#30340;&#20445;&#30041;&#35821;&#20041;&#30340;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#25935;&#24863;&#65292;&#36825;&#23548;&#33268;&#25512;&#26029;&#36807;&#31243;&#20013;&#20986;&#29616;&#22823;&#37327;&#19981;&#19968;&#33268;&#30340;&#27169;&#22411;&#20915;&#31574;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#31181;&#34892;&#20026;&#19982;&#23545;&#32452;&#21512;&#35821;&#20041;&#30340;&#26377;&#25928;&#21644;&#28145;&#20837;&#29702;&#35299;&#19981;&#21516;&#65292;&#32780;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#27169;&#22411;&#20934;&#30830;&#24230;&#25110;&#25506;&#31350;&#21477;&#27861;&#12289;&#21333;&#35843;&#24615;&#21644;&#36923;&#36753;&#40065;&#26834;&#24615;&#25512;&#29702;&#26102;&#22343;&#19981;&#20250;&#20986;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#26469;&#34913;&#37327;&#35821;&#20041;&#25935;&#24863;&#24615;&#30340;&#31243;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#21547;&#26377;&#24494;&#23567;&#20445;&#30041;&#35821;&#20041;&#30340;&#34920;&#38754;&#24418;&#24335;&#36755;&#20837;&#22122;&#22768;&#30340;&#23545;&#25239;&#29983;&#25104;&#26679;&#20363;&#26469;&#35780;&#20272;NLI&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies of the emergent capabilities of transformer-based Natural Language Understanding (NLU) models have indicated that they have an understanding of lexical and compositional semantics. We provide evidence that suggests these claims should be taken with a grain of salt: we find that state-of-the-art Natural Language Inference (NLI) models are sensitive towards minor semantics preserving surface-form variations, which lead to sizable inconsistent model decisions during inference. Notably, this behaviour differs from valid and in-depth comprehension of compositional semantics, however does neither emerge when evaluating model accuracy on standard benchmarks nor when probing for syntactic, monotonic, and logically robust reasoning. We propose a novel framework to measure the extent of semantic sensitivity. To this end, we evaluate NLI models on adversarially generated examples containing minor semantics-preserving surface-form input noise. This is achieved using conditional text
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#21512;&#20316;&#29289;&#27969;&#38382;&#39064;&#30340;&#20851;&#27880;&#38544;&#31169;&#12289;&#20855;&#22791;&#24773;&#32490;&#24847;&#35782;&#30340;&#20195;&#29702;&#20154;&#30340;&#20449;&#20219;&#27169;&#22411;&#12290;&#26368;&#20855;&#21019;&#26032;&#24615;&#30340;&#36129;&#29486;&#26159;&#22312;&#21512;&#20316;&#20915;&#31574;&#20013;&#24341;&#20837;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#24773;&#32490;&#21644;&#20449;&#20219;&#22914;&#20309;&#20419;&#36827;&#20195;&#29702;&#20154;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2401.14436</link><description>&lt;p&gt;
&#38754;&#21521;&#21512;&#20316;&#29289;&#27969;&#38382;&#39064;&#30340;&#20851;&#27880;&#38544;&#31169;&#12289;&#20855;&#22791;&#24773;&#32490;&#24847;&#35782;&#30340;&#20195;&#29702;&#20154;&#30340;&#20449;&#20219;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Trust model of privacy-concerned, emotionally-aware agents in a cooperative logistics problem. (arXiv:2401.14436v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#21512;&#20316;&#29289;&#27969;&#38382;&#39064;&#30340;&#20851;&#27880;&#38544;&#31169;&#12289;&#20855;&#22791;&#24773;&#32490;&#24847;&#35782;&#30340;&#20195;&#29702;&#20154;&#30340;&#20449;&#20219;&#27169;&#22411;&#12290;&#26368;&#20855;&#21019;&#26032;&#24615;&#30340;&#36129;&#29486;&#26159;&#22312;&#21512;&#20316;&#20915;&#31574;&#20013;&#24341;&#20837;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#24773;&#32490;&#21644;&#20449;&#20219;&#22914;&#20309;&#20419;&#36827;&#20195;&#29702;&#20154;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#20219;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#20154;&#31867;&#21644;&#26080;&#20154;&#36710;&#30456;&#20114;&#21512;&#20316;&#30340;&#34394;&#25311;&#28151;&#21512;&#29615;&#22659;&#20013;&#20351;&#29992;&#12290;&#25105;&#20204;&#20197;&#19968;&#31181;&#36830;&#36143;&#30340;&#26041;&#24335;&#23558;&#24773;&#32490;&#24341;&#20837;&#21040;&#20449;&#20219;&#27169;&#22411;&#20013;&#65292;&#24182;&#19982;&#24403;&#21069;&#30340;&#24515;&#29702;&#23398;&#29702;&#35770;&#30340;&#23454;&#38469;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#26368;&#20855;&#21019;&#26032;&#24615;&#30340;&#36129;&#29486;&#26159;&#38544;&#31169;&#38382;&#39064;&#22914;&#20309;&#22312;&#24773;&#32490;&#20449;&#20219;&#27169;&#22411;&#30340;&#21512;&#20316;&#20915;&#31574;&#20013;&#36215;&#20316;&#29992;&#12290;&#24773;&#32490;&#21644;&#20449;&#20219;&#37117;&#36890;&#36807;&#33258;&#20027;&#20195;&#29702;&#20154;&#30340;&#35748;&#30693;&#24314;&#27169;&#21644;&#31649;&#29702;&#20197;&#21450;&#20351;&#29992;IEEE FIPA&#26631;&#20934;&#36827;&#34892;&#36890;&#20449;&#30340;GAMA&#20195;&#29702;&#24179;&#21488;&#30340;GAML&#32534;&#31243;&#35821;&#35328;&#26469;&#23454;&#29616;&#12290;&#36825;&#20123;&#24773;&#32490;&#20195;&#29702;&#20154;&#30340;&#20449;&#20219;&#34892;&#20026;&#22312;&#19968;&#20010;&#21512;&#20316;&#24615;&#29289;&#27969;&#38382;&#39064;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65306;&#20195;&#29702;&#20154;&#24517;&#39035;&#23558;&#29289;&#21697;&#31227;&#21160;&#21040;&#30446;&#30340;&#22320;&#65292;&#20854;&#20013;&#19968;&#20123;&#29289;&#21697;&#21644;&#22320;&#28857;&#23384;&#22312;&#38544;&#31169;&#38382;&#39064;&#12290;&#23545;&#36825;&#20010;&#29289;&#27969;&#38382;&#39064;&#30340;&#27169;&#25311;&#25191;&#34892;&#26174;&#31034;&#24773;&#32490;&#21644;&#20449;&#20219;&#22914;&#20309;&#20419;&#36827;&#20195;&#29702;&#20154;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we propose a trust model to be used into a hypothetical mixed environment where humans and unmanned vehicles cooperate. We address the inclusion of emotions inside a trust model in a coherent way to the practical approaches to the current psychology theories. The most innovative contribution is how privacy issues play a role in the cooperation decisions of the emotional trust model. Both, emotions and trust have been cognitively modeled and managed with the Beliefs, Desires and Intentions (BDI) paradigm into autonomous agents implemented in GAML (the programming language of GAMA agent platform) that communicates using the IEEE FIPA standard. The trusting behaviour of these emotional agents is tested in a cooperative logistics problem where: agents have to move objects to destinations and some of the objects and places have privacy issues. The execution of simulations of this logistic problem shows how emotions and trust contribute to improve the performance of agents in t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#25216;&#26415;&#25903;&#25345;&#26694;&#26550;&#65292;&#36890;&#36807;&#24314;&#31435;&#21306;&#21035;&#26469;&#24378;&#35843;&#37325;&#35201;&#21306;&#22495;, &#24182;&#20943;&#23569;&#22270;&#20687;&#22122;&#38899;&#12290;&#23454;&#35777;&#35843;&#26597;&#34920;&#26126;&#36825;&#20123;&#21306;&#22495;&#22312;&#20419;&#36827;&#31867;&#21035;&#21306;&#20998;&#26041;&#38754;&#36215;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.14434</link><description>&lt;p&gt;
&#23558;&#22522;&#20110;&#26799;&#24230;&#30340;&#25216;&#26415;&#36716;&#21270;&#20026;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Transforming gradient-based techniques into interpretable methods. (arXiv:2401.14434v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#25216;&#26415;&#25903;&#25345;&#26694;&#26550;&#65292;&#36890;&#36807;&#24314;&#31435;&#21306;&#21035;&#26469;&#24378;&#35843;&#37325;&#35201;&#21306;&#22495;, &#24182;&#20943;&#23569;&#22270;&#20687;&#22122;&#38899;&#12290;&#23454;&#35777;&#35843;&#26597;&#34920;&#26126;&#36825;&#20123;&#21306;&#22495;&#22312;&#20419;&#36827;&#31867;&#21035;&#21306;&#20998;&#26041;&#38754;&#36215;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;xAI&#25216;&#26415;&#35299;&#37322;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#36890;&#24120;&#22312;&#35299;&#37322;&#19978;&#38754;&#20020;&#25361;&#25112;&#12290;&#22270;&#20687;&#25552;&#21462;&#30340;&#20687;&#32032;&#31561;&#36755;&#20837;&#29305;&#24449;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#24341;&#21457;&#20102;&#22797;&#26434;&#30340;&#30456;&#20851;&#24615;&#12290;&#38598;&#25104;&#26799;&#24230;&#65288;IG&#65289;&#31561;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#23637;&#31034;&#20102;&#36825;&#20123;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#35299;&#37322;&#36716;&#21270;&#20026;&#22270;&#20687;&#26102;&#24120;&#24120;&#20135;&#29983;&#22823;&#37327;&#22122;&#38899;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#26799;&#24230;&#20154;&#24037;&#20998;&#31163;&#65288;GAD&#65289;&#20316;&#20026;&#26799;&#24230;&#22522;&#20110;&#25216;&#26415;&#30340;&#25903;&#25345;&#26694;&#26550;&#12290;&#20854;&#20027;&#35201;&#30446;&#26631;&#26159;&#36890;&#36807;&#24314;&#31435;&#31867;&#21035;&#20043;&#38388;&#30340;&#21306;&#21035;&#26469;&#24378;&#35843;&#26377;&#24433;&#21709;&#21147;&#30340;&#21306;&#22495;&#12290;GAD&#30340;&#26680;&#24515;&#26159;&#22312;&#21487;&#35270;&#21270;&#36807;&#31243;&#20013;&#38480;&#21046;&#20998;&#26512;&#33539;&#22260;&#65292;&#20174;&#32780;&#20943;&#23569;&#22270;&#20687;&#22122;&#38899;&#12290;&#36890;&#36807;&#23545;&#34987;&#36974;&#25377;&#22270;&#20687;&#30340;&#23454;&#35777;&#35843;&#26597;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#30830;&#23450;&#30340;&#21306;&#22495;&#30830;&#23454;&#22312;&#20419;&#36827;&#31867;&#21035;&#21306;&#20998;&#26041;&#38754;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The explication of Convolutional Neural Networks (CNN) through xAI techniques often poses challenges in interpretation. The inherent complexity of input features, notably pixels extracted from images, engenders complex correlations. Gradient-based methodologies, exemplified by Integrated Gradients (IG), effectively demonstrate the significance of these features. Nevertheless, the conversion of these explanations into images frequently yields considerable noise. Presently, we introduce GAD (Gradient Artificial Distancing) as a supportive framework for gradient-based techniques. Its primary objective is to accentuate influential regions by establishing distinctions between classes. The essence of GAD is to limit the scope of analysis during visualization and, consequently reduce image noise. Empirical investigations involving occluded images have demonstrated that the identified regions through this methodology indeed play a pivotal role in facilitating class differentiation.
&lt;/p&gt;</description></item><item><title>M$^3$TN&#26159;&#19968;&#31181;&#29992;&#20110;&#25552;&#21319;&#24314;&#27169;&#30340;&#26032;&#39062;&#32593;&#32476;&#65292;&#36890;&#36807;&#22810;&#38376;&#19987;&#23478;&#28151;&#21512;&#21644;&#26126;&#30830;&#24314;&#27169;&#25552;&#21319;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#19968;&#33268;&#24615;&#21644;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14426</link><description>&lt;p&gt;
M$^3$TN&#65306;&#22522;&#20110;&#22810;&#38376;&#19987;&#23478;&#28151;&#21512;&#30340;&#22810;&#20540;&#22788;&#29702;&#32593;&#32476;&#30340;&#25552;&#21319;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
M$^3$TN: Multi-gate Mixture-of-Experts based Multi-valued Treatment Network for Uplift Modeling. (arXiv:2401.14426v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14426
&lt;/p&gt;
&lt;p&gt;
M$^3$TN&#26159;&#19968;&#31181;&#29992;&#20110;&#25552;&#21319;&#24314;&#27169;&#30340;&#26032;&#39062;&#32593;&#32476;&#65292;&#36890;&#36807;&#22810;&#38376;&#19987;&#23478;&#28151;&#21512;&#21644;&#26126;&#30830;&#24314;&#27169;&#25552;&#21319;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#19968;&#33268;&#24615;&#21644;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#21319;&#24314;&#27169;&#26159;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#22788;&#29702;&#65288;&#22914;&#25240;&#25187;&#65289;&#23545;&#20010;&#20307;&#21453;&#24212;&#30340;&#25216;&#26415;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#29992;&#20110;&#22810;&#20540;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#37117;&#26159;&#20174;&#20108;&#20540;&#22788;&#29702;&#26041;&#27861;&#25193;&#23637;&#32780;&#26469;&#30340;&#65292;&#23384;&#22312;&#19968;&#20123;&#23616;&#38480;&#24615;&#12290;&#39318;&#20808;&#65292;&#29616;&#26377;&#26041;&#27861;&#22522;&#20110;&#39044;&#27979;&#30340;&#21709;&#24212;&#35745;&#31639;&#25552;&#21319;&#65292;&#36825;&#21487;&#33021;&#19981;&#33021;&#20445;&#35777;&#22788;&#29702;&#32452;&#21644;&#23545;&#29031;&#32452;&#20043;&#38388;&#30340;&#19968;&#33268;&#25552;&#21319;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#22810;&#20540;&#22788;&#29702;&#20135;&#29983;&#32047;&#31215;&#35823;&#24046;&#12290;&#20854;&#27425;&#65292;&#38543;&#30528;&#35768;&#22810;&#39044;&#27979;&#22836;&#65292;&#27169;&#22411;&#21442;&#25968;&#21464;&#24471;&#38750;&#24120;&#22797;&#26434;&#65292;&#23548;&#33268;&#25928;&#29575;&#38477;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22810;&#38376;&#19987;&#23478;&#28151;&#21512;&#30340;&#22810;&#20540;&#22788;&#29702;&#32593;&#32476;&#65288;M$^3$TN&#65289;&#12290;M$^3$TN&#30001;&#20004;&#20010;&#32452;&#20214;&#32452;&#25104;&#65306;1) &#22522;&#20110;&#22810;&#38376;&#19987;&#23478;&#28151;&#21512;&#30340;&#29305;&#24449;&#34920;&#31034;&#27169;&#22359;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#65307;2) &#36890;&#36807;&#26126;&#30830;&#24314;&#27169;&#25552;&#21319;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uplift modeling is a technique used to predict the effect of a treatment (e.g., discounts) on an individual's response. Although several methods have been proposed for multi-valued treatment, they are extended from binary treatment methods. There are still some limitations. Firstly, existing methods calculate uplift based on predicted responses, which may not guarantee a consistent uplift distribution between treatment and control groups. Moreover, this may cause cumulative errors for multi-valued treatment. Secondly, the model parameters become numerous with many prediction heads, leading to reduced efficiency. To address these issues, we propose a novel \underline{M}ulti-gate \underline{M}ixture-of-Experts based \underline{M}ulti-valued \underline{T}reatment \underline{N}etwork (M$^3$TN). M$^3$TN consists of two components: 1) a feature representation module with Multi-gate Mixture-of-Experts to improve the efficiency; 2) a reparameterization module by modeling uplift explicitly to i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#29983;&#25104;&#22411;AI&#36827;&#34892;&#22270;&#20687;&#29983;&#25104;&#65292;&#36890;&#36807;&#20998;&#26512;&#25552;&#31034;&#21644;&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#21457;&#29616;&#25552;&#31034;&#20027;&#35201;&#20851;&#27880;&#34920;&#38754;&#32654;&#24863;&#21644;&#27969;&#34892;&#35805;&#39064;&#65292;&#32780;&#38750;&#33402;&#26415;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.14425</link><description>&lt;p&gt;
&#19981;&#20877;&#22312;Artstation&#19978;&#36235;&#21183;&#65306;&#29983;&#25104;&#22411;AI&#33402;&#26415;&#20316;&#21697;&#30340;&#25552;&#31034;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
No Longer Trending on Artstation: Prompt Analysis of Generative AI Art. (arXiv:2401.14425v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#29983;&#25104;&#22411;AI&#36827;&#34892;&#22270;&#20687;&#29983;&#25104;&#65292;&#36890;&#36807;&#20998;&#26512;&#25552;&#31034;&#21644;&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#21457;&#29616;&#25552;&#31034;&#20027;&#35201;&#20851;&#27880;&#34920;&#38754;&#32654;&#24863;&#21644;&#27969;&#34892;&#35805;&#39064;&#65292;&#32780;&#38750;&#33402;&#26415;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#22411;AI&#36827;&#34892;&#22270;&#20687;&#29983;&#25104;&#27491;&#22312;&#36805;&#36895;&#25104;&#20026;&#35270;&#35273;&#23186;&#20307;&#30340;&#20027;&#35201;&#26032;&#26469;&#28304;&#65292;&#36807;&#21435;&#20960;&#24180;&#24050;&#32463;&#20351;&#29992;&#31283;&#23450;&#25193;&#25955;&#21644;&#20013;&#36884;&#20986;&#34892;&#31561;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#20102;&#25968;&#21313;&#20159;&#24352;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#26412;&#25991;&#25910;&#38598;&#21644;&#20998;&#26512;&#20102;&#36229;&#36807;300&#19975;&#20010;&#25552;&#31034;&#21450;&#20854;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#20027;&#39064;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#26088;&#22312;&#20849;&#21516;&#20102;&#35299;&#20154;&#20204;&#22914;&#20309;&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#65292;&#36825;&#20123;&#31995;&#32479;&#23545;&#33402;&#26415;&#23478;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#23427;&#20204;&#24191;&#27867;&#25512;&#24191;&#30340;&#35270;&#35273;&#25991;&#21270;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#31034;&#20027;&#35201;&#38598;&#20013;&#22312;&#34920;&#38754;&#32654;&#24863;&#19978;&#65292;&#24378;&#21270;&#25991;&#21270;&#35268;&#33539;&#12289;&#27969;&#34892;&#30340;&#20256;&#32479;&#34920;&#24449;&#21644;&#24847;&#35937;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#35768;&#22810;&#29992;&#25143;&#20851;&#27880;&#28909;&#38376;&#35805;&#39064;&#65288;&#20363;&#22914;&#21046;&#20316;&#28034;&#33394;&#20070;&#12289;&#24187;&#24819;&#33402;&#26415;&#25110;&#22307;&#35806;&#21345;&#29255;&#65289;&#65292;&#36825;&#34920;&#26126;&#25152;&#20998;&#26512;&#31995;&#32479;&#30340;&#20027;&#35201;&#29992;&#36884;&#26159;&#23089;&#20048;&#32780;&#38750;&#33402;&#26415;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image generation using generative AI is rapidly becoming a major new source of visual media, with billions of AI generated images created using diffusion models such as Stable Diffusion and Midjourney over the last few years. In this paper we collect and analyse over 3 million prompts and the images they generate. Using natural language processing, topic analysis and visualisation methods we aim to understand collectively how people are using text prompts, the impact of these systems on artists, and more broadly on the visual cultures they promote. Our study shows that prompting focuses largely on surface aesthetics, reinforcing cultural norms, popular conventional representations and imagery. We also find that many users focus on popular topics (such as making colouring books, fantasy art, or Christmas cards), suggesting that the dominant use for the systems analysed is recreational rather than artistic.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;MCTS&#21644;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31526;&#21495;&#22238;&#24402;&#31639;&#27861;SR-GPT&#65292;&#22312;&#21457;&#29616;&#25968;&#25454;&#20013;&#30340;&#25968;&#23398;&#20844;&#24335;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.14424</link><description>&lt;p&gt;
&#36890;&#36807;GPT&#24341;&#23548;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#25968;&#23398;&#20844;&#24335;
&lt;/p&gt;
&lt;p&gt;
Discovering Mathematical Formulas from Data via GPT-guided Monte Carlo Tree Search. (arXiv:2401.14424v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14424
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;MCTS&#21644;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31526;&#21495;&#22238;&#24402;&#31639;&#27861;SR-GPT&#65292;&#22312;&#21457;&#29616;&#25968;&#25454;&#20013;&#30340;&#25968;&#23398;&#20844;&#24335;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#30740;&#31350;&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#25214;&#21040;&#19968;&#20010;&#31616;&#27905;&#19988;&#21487;&#35299;&#37322;&#30340;&#25968;&#23398;&#20844;&#24335;&#26469;&#20934;&#30830;&#25551;&#36848;&#25968;&#25454;&#20013;&#27599;&#20010;&#21464;&#37327;&#19982;&#39044;&#27979;&#20540;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#20063;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#20010;&#38382;&#39064;&#34987;&#31216;&#20026;&#31526;&#21495;&#22238;&#24402;&#65292;&#26159;&#19968;&#20010;NP&#22256;&#38590;&#38382;&#39064;&#12290;&#21435;&#24180;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#30340;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;sota&#12290;&#34429;&#28982;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#31639;&#27861;&#22312;&#24674;&#22797;&#30446;&#26631;&#34920;&#36798;&#24335;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#25913;&#36827;&#65292;&#20294;&#26159;&#22312;MCTS&#36807;&#31243;&#20013;&#32570;&#20047;&#24341;&#23548;&#20005;&#37325;&#38459;&#30861;&#20102;&#20854;&#25628;&#32034;&#25928;&#29575;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#31639;&#27861;&#22312;MCTS&#30340;&#25628;&#32034;&#20013;&#28155;&#21152;&#20102;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#31574;&#30053;&#32593;&#32476;&#65292;&#20294;&#26159;&#36825;&#20010;&#39044;&#35757;&#32451;&#30340;&#31574;&#30053;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#24456;&#24046;&#12290;&#20026;&#20102;&#24179;&#34913;&#25928;&#29575;&#21644;&#36890;&#29992;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SR-GPT&#65292;&#32467;&#21512;&#20102;AlphaZero&#30340;&#24605;&#24819;&#12290;SR-GPT&#26159;&#19968;&#31181;&#26032;&#30340;&#31526;&#21495;&#22238;&#24402;&#31639;&#27861;&#65292;&#23558;MCTS&#19982;&#19968;&#20010;&#36890;&#29992;&#24615;&#36739;&#22909;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding a concise and interpretable mathematical formula that accurately describes the relationship between each variable and the predicted value in the data is a crucial task in scientific research, as well as a significant challenge in artificial intelligence. This problem is referred to as symbolic regression, which is an NP-hard problem. Last year, a symbolic regression method based on Monte Carlo Tree Search (MCTS) was proposed and sota was obtained on multiple datasets. While this algorithm has shown considerable improvement in recovering target expressions compared to previous methods, the lack of guidance during the MCTS process severely hampers its search efficiency. Recently, some algorithms have added a pre-trained policy network to guide the search of MCTS, but the pre-trained policy network generalizes poorly. To balance efficiency and generality, we propose SR-GPT combining ideas from AlphaZero. SR-GPT is a new symbolic regression algorithm that combines MCTS with a Gener
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#27169;&#31946;&#36923;&#36753;&#20989;&#25968;&#20316;&#20026;&#20998;&#31867;&#22120;&#30340;&#20107;&#21518;&#35299;&#37322;&#22120;&#65292;&#36890;&#36807;&#19982;&#40657;&#30418;&#20998;&#31867;&#22120;&#36827;&#34892;&#24182;&#34892;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#19982;&#40657;&#30418;&#20998;&#31867;&#22120;&#30456;&#21516;&#30340;&#20915;&#31574;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.14417</link><description>&lt;p&gt;
&#27169;&#31946;&#36923;&#36753;&#20989;&#25968;&#20316;&#20026;&#38750;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#20107;&#21518;&#35299;&#37322;&#22120;
&lt;/p&gt;
&lt;p&gt;
Fuzzy Logic Function as a Post-hoc Explanator of the Nonlinear Classifier. (arXiv:2401.14417v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#27169;&#31946;&#36923;&#36753;&#20989;&#25968;&#20316;&#20026;&#20998;&#31867;&#22120;&#30340;&#20107;&#21518;&#35299;&#37322;&#22120;&#65292;&#36890;&#36807;&#19982;&#40657;&#30418;&#20998;&#31867;&#22120;&#36827;&#34892;&#24182;&#34892;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#19982;&#40657;&#30418;&#20998;&#31867;&#22120;&#30456;&#21516;&#30340;&#20915;&#31574;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#30340;&#27169;&#24335;&#35782;&#21035;&#31995;&#32479;&#27604;&#32447;&#24615;&#27169;&#22411;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#32570;&#28857;&#26159;&#40657;&#30418;&#23646;&#24615;&#12290;&#36825;&#24847;&#21619;&#30528;&#27809;&#26377;&#32463;&#39564;&#20351;&#29992;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#20154;&#21487;&#33021;&#38656;&#35201;&#24110;&#21161;&#29702;&#35299;&#20915;&#31574;&#32467;&#26524;&#12290;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#23545;&#20110;&#36127;&#36131;&#26368;&#32456;&#20915;&#31574;&#30340;&#29992;&#25143;&#26469;&#35828;&#26159;&#19981;&#21487;&#25509;&#21463;&#30340;&#12290;&#20182;&#19981;&#20165;&#24517;&#39035;&#30456;&#20449;&#20915;&#31574;&#65292;&#36824;&#24517;&#39035;&#29702;&#35299;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#35782;&#21035;&#22120;&#24517;&#39035;&#20855;&#26377;&#20801;&#35768;&#35299;&#37322;&#32773;&#35299;&#37322;&#32467;&#26524;&#30340;&#26550;&#26500;&#12290;&#20107;&#21518;&#21487;&#35299;&#37322;&#30340;&#20998;&#31867;&#22120;&#30340;&#24819;&#27861;&#26159;&#35774;&#35745;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#20998;&#31867;&#22120;&#65292;&#19982;&#40657;&#30418;&#20998;&#31867;&#22120;&#24182;&#34892;&#65292;&#32473;&#20986;&#19982;&#40657;&#30418;&#20998;&#31867;&#22120;&#30456;&#21516;&#30340;&#20915;&#31574;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#22914;&#26524;Zadeh&#30340;&#27169;&#31946;&#36923;&#36753;&#20989;&#25968;&#24418;&#25104;&#20998;&#31867;&#22120;&#65292;DeconvNet&#30340;&#37325;&#35201;&#24615;&#32473;&#20986;&#20102;&#30495;&#20540;&#65292;&#21487;&#35299;&#37322;&#30340;&#20998;&#31867;&#22120;&#22312;MNIST&#21644;FashionMNIST&#25968;&#25454;&#24211;&#19978;&#19982;&#40657;&#30418;&#20998;&#31867;&#22120;&#20135;&#29983;&#30456;&#21516;&#30340;&#20998;&#31867;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pattern recognition systems implemented using deep neural networks achieve better results than linear models. However, their drawback is the black box property. This property means that one with no experience utilising nonlinear systems may need help understanding the outcome of the decision. Such a solution is unacceptable to the user responsible for the final decision. He must not only believe in the decision but also understand it. Therefore, recognisers must have an architecture that allows interpreters to interpret the findings. The idea of post-hoc explainable classifiers is to design an interpretable classifier parallel to the black box classifier, giving the same decisions as the black box classifier. This paper shows that the explainable classifier completes matching classification decisions with the black box classifier on the MNIST and FashionMNIST databases if Zadeh`s fuzzy logic function forms the classifier and DeconvNet importance gives the truth values. Since the other 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#30005;&#21270;&#23398;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#24773;&#20917;&#65292;&#24182;&#20171;&#32461;&#20102;&#20854;&#22312;&#21307;&#23398;&#35786;&#26029;&#12289;&#21270;&#23398;&#21697;&#20998;&#31867;&#21644;&#29615;&#22659;&#30417;&#27979;&#31561;&#26041;&#38754;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.14413</link><description>&lt;p&gt;
&#24212;&#29992;&#20110;&#30005;&#21270;&#23398;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Aprendizado de m\'aquina aplicado na eletroqu\'imica. (arXiv:2401.14413v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#30005;&#21270;&#23398;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#24773;&#20917;&#65292;&#24182;&#20171;&#32461;&#20102;&#20854;&#22312;&#21307;&#23398;&#35786;&#26029;&#12289;&#21270;&#23398;&#21697;&#20998;&#31867;&#21644;&#29615;&#22659;&#30417;&#27979;&#31561;&#26041;&#38754;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31995;&#32479;&#24615;&#32508;&#36848;&#26088;&#22312;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#35782;&#21035;&#21644;&#37327;&#21270;&#21508;&#31181;&#30005;&#21270;&#23398;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#24773;&#20917;&#65292;&#24182;&#20171;&#32461;&#20102;&#25991;&#29486;&#20013;&#21487;&#29992;&#30340;&#24212;&#29992;&#12290;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#31181;&#33021;&#22815;&#20419;&#36827;&#20998;&#26512;&#24182;&#22686;&#24378;&#28041;&#21450;&#21508;&#31181;&#20998;&#26512;&#29289;&#30340;&#36807;&#31243;&#29702;&#35299;&#30340;&#24037;&#20855;&#12290;&#22312;&#30005;&#21270;&#23398;&#29983;&#29289;&#20256;&#24863;&#22120;&#20013;&#65292;&#23427;&#25552;&#39640;&#20102;&#21307;&#23398;&#35786;&#26029;&#30340;&#31934;&#30830;&#24615;&#65292;&#25552;&#39640;&#20102;&#35782;&#21035;&#20855;&#26377;&#39640;&#21487;&#38752;&#24615;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#21644;&#30149;&#21407;&#20307;&#30340;&#33021;&#21147;&#12290;&#23427;&#36824;&#21487;&#20197;&#26377;&#25928;&#29992;&#20110;&#22797;&#26434;&#21270;&#23398;&#21697;&#30340;&#20998;&#31867;&#65307;&#22312;&#29615;&#22659;&#30417;&#27979;&#20013;&#65292;&#20351;&#29992;&#20302;&#25104;&#26412;&#20256;&#24863;&#22120;&#65307;&#22312;&#20415;&#25658;&#35774;&#22791;&#21644;&#21487;&#31359;&#25140;&#31995;&#32479;&#20013;&#31561;&#31561;&#12290;&#30446;&#21069;&#65292;&#26576;&#20123;&#20998;&#26512;&#29289;&#30340;&#20998;&#26512;&#20173;&#28982;&#38656;&#35201;&#19987;&#23478;&#30340;&#19987;&#19994;&#30693;&#35782;&#24182;&#25163;&#21160;&#25191;&#34892;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#32467;&#26524;&#30340;&#26222;&#36941;&#21270;&#12290;&#22312;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#19979;&#65292;&#26412;&#30740;&#31350;&#25311;&#36827;&#34892;&#19968;&#39033;&#31995;&#32479;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
This systematic review focuses on analyzing the use of machine learning techniques for identifying and quantifying analytes in various electrochemical applications, presenting the available applications in the literature. Machine learning is a tool that can facilitate the analysis and enhance the understanding of processes involving various analytes. In electrochemical biosensors, it increases the precision of medical diagnostics, improving the identification of biomarkers and pathogens with high reliability. It can be effectively used for the classification of complex chemical products; in environmental monitoring, using low-cost sensors; in portable devices and wearable systems; among others. Currently, the analysis of some analytes is still performed manually, requiring the expertise of a specialist in the field and thus hindering the generalization of results. In light of the advancements in artificial intelligence today, this work proposes to carry out a systematic review of the l
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;VeriStable&#26041;&#27861;&#65292;&#22312;DNN&#39564;&#35777;&#20013;&#21033;&#29992;&#31283;&#23450;&#30340;&#31070;&#32463;&#20803;&#20943;&#23569;&#32452;&#21512;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#25277;&#35937;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#19982;&#24037;&#19994;&#21270;SAT&#22522;&#20934;&#20849;&#20139;&#37325;&#35201;&#29305;&#24449;&#65292;&#24182;&#22312;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.14412</link><description>&lt;p&gt;
&#21033;&#29992;&#31070;&#32463;&#20803;&#30340;&#31283;&#23450;&#24615;&#25913;&#36827;DNN&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Harnessing Neuron Stability to Improve DNN Verification. (arXiv:2401.14412v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;VeriStable&#26041;&#27861;&#65292;&#22312;DNN&#39564;&#35777;&#20013;&#21033;&#29992;&#31283;&#23450;&#30340;&#31070;&#32463;&#20803;&#20943;&#23569;&#32452;&#21512;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#25277;&#35937;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#19982;&#24037;&#19994;&#21270;SAT&#22522;&#20934;&#20849;&#20139;&#37325;&#35201;&#29305;&#24449;&#65292;&#24182;&#22312;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20687;&#20154;&#31867;&#32534;&#20889;&#30340;&#36719;&#20214;&#19968;&#26679;&#65292;DNN&#20063;&#23481;&#26131;&#21463;&#21040;&#38169;&#35823;&#21644;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#36825;&#24341;&#21457;&#20102;&#23545;&#26377;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;DNN&#39564;&#35777;&#25216;&#26415;&#21644;&#24037;&#20855;&#30340;&#37325;&#22823;&#20851;&#27880;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;VeriStable&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;DPLL&#32422;&#26463;DNN&#39564;&#35777;&#26041;&#27861;&#30340;&#25193;&#23637;&#12290;VeriStable&#21033;&#29992;&#20102;&#36825;&#26679;&#19968;&#20010;&#27934;&#35265;&#65306;&#23613;&#31649;&#31070;&#32463;&#20803;&#22312;&#25972;&#20010;DNN&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#34892;&#20026;&#21487;&#33021;&#26159;&#38750;&#32447;&#24615;&#30340;&#65292;&#22312;&#39564;&#35777;&#36807;&#31243;&#20013;&#35745;&#31639;&#24471;&#21040;&#30340;&#20013;&#38388;&#29366;&#24577;&#20013;&#65292;&#35768;&#22810;&#31070;&#32463;&#20803;&#21487;&#33021;&#34987;&#32422;&#26463;&#20026;&#20855;&#26377;&#32447;&#24615;&#34892;&#20026;-&#36825;&#20123;&#31070;&#32463;&#20803;&#26159;&#31283;&#23450;&#30340;&#12290;&#39640;&#25928;&#22320;&#26816;&#27979;&#31283;&#23450;&#30340;&#31070;&#32463;&#20803;&#21487;&#20197;&#20943;&#23569;&#32452;&#21512;&#22797;&#26434;&#24615;&#65292;&#32780;&#19981;&#20250;&#25439;&#23475;&#25277;&#35937;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;DNN&#39564;&#35777;&#38382;&#39064;&#20013;&#20135;&#29983;&#30340;&#23376;&#21477;&#32467;&#26500;&#19982;&#24037;&#19994;&#21270;SAT&#22522;&#20934;&#20855;&#26377;&#37325;&#35201;&#29305;&#24449;&#12290;&#25105;&#20204;&#35843;&#25972;&#24182;&#34701;&#21512;&#20102;&#22810;&#32447;&#31243;&#21644;&#37325;&#21551;&#20248;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNN) have emerged as an effective approach to tackling real-world problems. However, like human-written software, DNNs are susceptible to bugs and attacks. This has generated significant interests in developing effective and scalable DNN verification techniques and tools. In this paper, we present VeriStable, a novel extension of recently proposed DPLL-based constraint DNN verification approach. VeriStable leverages the insight that while neuron behavior may be non-linear across the entire DNN input space, at intermediate states computed during verification many neurons may be constrained to have linear behavior - these neurons are stable. Efficiently detecting stable neurons reduces combinatorial complexity without compromising the precision of abstractions. Moreover, the structure of clauses arising in DNN verification problems shares important characteristics with industrial SAT benchmarks. We adapt and incorporate multi-threading and restart optimizations targ
&lt;/p&gt;</description></item><item><title>CMMU&#26159;&#19968;&#20010;&#29992;&#20110;&#20013;&#25991;&#22810;&#27169;&#24577;&#22810;&#31867;&#22411;&#38382;&#39064;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#20174;&#23567;&#23398;&#21040;&#39640;&#20013;&#30340;&#30693;&#35782;&#65292;&#25552;&#20379;&#20102;&#22810;&#39033;&#36873;&#25321;&#39064;&#12289;&#22810;&#39033;&#22238;&#31572;&#39064;&#21644;&#22635;&#31354;&#39064;&#19977;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#65292;&#23545;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#27700;&#24179;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.14011</link><description>&lt;p&gt;
CMMU: &#19968;&#20010;&#29992;&#20110;&#20013;&#25991;&#22810;&#27169;&#24577;&#22810;&#31867;&#22411;&#38382;&#39064;&#29702;&#35299;&#19982;&#25512;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CMMU: A Benchmark for Chinese Multi-modal Multi-type Question Understanding and Reasoning. (arXiv:2401.14011v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14011
&lt;/p&gt;
&lt;p&gt;
CMMU&#26159;&#19968;&#20010;&#29992;&#20110;&#20013;&#25991;&#22810;&#27169;&#24577;&#22810;&#31867;&#22411;&#38382;&#39064;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#20174;&#23567;&#23398;&#21040;&#39640;&#20013;&#30340;&#30693;&#35782;&#65292;&#25552;&#20379;&#20102;&#22810;&#39033;&#36873;&#25321;&#39064;&#12289;&#22810;&#39033;&#22238;&#31572;&#39064;&#21644;&#22635;&#31354;&#39064;&#19977;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#65292;&#23545;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#27700;&#24179;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#24182;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#30693;&#35782;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;MLLM&#30340;&#26234;&#33021;&#27700;&#24179;&#25152;&#38656;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#25484;&#25569;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#24403;&#21069;&#29992;&#20110;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#20027;&#35201;&#38598;&#20013;&#22312;&#33521;&#35821;&#22810;&#39033;&#36873;&#25321;&#39064;&#19978;&#65292;&#24182;&#19988;&#22312;&#35780;&#20272;&#30340;&#20840;&#38754;&#24615;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CMMU&#65292;&#19968;&#20010;&#29992;&#20110;&#20013;&#25991;&#22810;&#27169;&#24577;&#22810;&#31867;&#22411;&#38382;&#39064;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#12290;CMMU&#21253;&#21547;7&#20010;&#23398;&#31185;&#30340;3603&#20010;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#20174;&#23567;&#23398;&#21040;&#39640;&#20013;&#30340;&#30693;&#35782;&#12290;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#20998;&#20026;&#22810;&#39033;&#36873;&#25321;&#39064;&#12289;&#22810;&#39033;&#22238;&#31572;&#39064;&#21644;&#22635;&#31354;&#39064;&#19977;&#31867;&#65292;&#23545;MLLMs&#25552;&#20986;&#26356;&#22823;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#26684;&#30340;&#35780;&#20272;&#31574;&#30053;&#65292;&#31216;&#20026;ShiftCheck&#65292;&#29992;&#20110;&#35780;&#20272;&#22810;&#39033;&#36873;&#25321;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal large language models(MLLMs) have achieved remarkable progress and demonstrated powerful knowledge comprehension and reasoning abilities. However, the mastery of domain-specific knowledge, which is essential for evaluating the intelligence of MLLMs, continues to be a challenge. Current multi-modal benchmarks for domain-specific knowledge concentrate on multiple-choice questions and are predominantly available in English, which imposes limitations on the comprehensiveness of the evaluation. To this end, we introduce CMMU, a novel benchmark for multi-modal and multi-type question understanding and reasoning in Chinese. CMMU consists of 3,603 questions in 7 subjects, covering knowledge from primary to high school. The questions can be categorized into 3 types: multiple-choice, multiple-response, and fill-in-the-blank, bringing greater challenges to MLLMs. In addition, we propose a rigorous evaluation strategy called ShiftCheck for assessing multiple-choice questions. The strat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#24037;&#20855;&#21464;&#37327;&#65288;IV&#65289;&#27169;&#22411;&#20013;&#65292;&#20855;&#26377;&#20108;&#36827;&#21046;&#21709;&#24212;$Y$&#21644;&#20108;&#36827;&#21046;&#22788;&#29702;$X$&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#25509;&#21463;$K$&#20010;&#29366;&#24577;&#30340;&#20202;&#22120;$Z$&#26102;&#65292;&#20851;&#20110;ACE&#30028;&#38480;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.13758</link><description>&lt;p&gt;
&#24037;&#20855;&#21464;&#37327;&#27169;&#22411;&#20013;&#30340;&#20551;&#35774;&#21644;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Assumptions and Bounds in the Instrumental Variable Model. (arXiv:2401.13758v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#24037;&#20855;&#21464;&#37327;&#65288;IV&#65289;&#27169;&#22411;&#20013;&#65292;&#20855;&#26377;&#20108;&#36827;&#21046;&#21709;&#24212;$Y$&#21644;&#20108;&#36827;&#21046;&#22788;&#29702;$X$&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#25509;&#21463;$K$&#20010;&#29366;&#24577;&#30340;&#20202;&#22120;$Z$&#26102;&#65292;&#20851;&#20110;ACE&#30028;&#38480;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#20851;&#20110;&#20855;&#26377;&#20108;&#36827;&#21046;&#21709;&#24212;$Y$&#21644;&#20108;&#36827;&#21046;&#22788;&#29702;$X$&#30340;&#24037;&#20855;&#21464;&#37327;&#65288;IV&#65289;&#27169;&#22411;&#30340;&#32467;&#26524;&#35777;&#26126;&#65292;&#20294;&#26159;&#20202;&#22120;$Z$&#25509;&#21463;$K$&#20010;&#29366;&#24577;&#65292;&#36825;&#20123;&#29366;&#24577;&#26368;&#21021;&#26159;&#22312;Richardson&#65286;Robins&#65288;2014&#65289;&#30340;&#35770;&#25991;&#8220;ACE Bounds; SEMS with Equilibrium Conditions&#8221;&#20013;&#25552;&#20986;&#30340;&#65288;arXiv:1410.0470&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this note we give proofs for results relating to the Instrumental Variable (IV) model with binary response $Y$ and binary treatment $X$, but with an instrument $Z$ that takes $K$ states that were originally stated in Richardson &amp; Robins (2014), "ACE Bounds; SEMS with Equilibrium Conditions," arXiv:1410.0470.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21463;&#31639;&#27861;&#20915;&#31574;&#24433;&#21709;&#30340;&#20154;&#30340;&#20449;&#24687;&#38656;&#27714;&#65292;&#21457;&#29616;&#35299;&#37322;&#24448;&#24448;&#19981;&#33021;&#28385;&#36275;&#20182;&#20204;&#30340;&#20851;&#27880;&#28857;&#65292;&#23548;&#33268;&#23545;&#30417;&#31649;&#26694;&#26550;&#30340;&#29702;&#35299;&#21644;&#36981;&#23432;&#20135;&#29983;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#22242;&#38431;&#25552;&#20986;&#20102;XAI&#21021;&#23398;&#32773;&#38382;&#39064;&#24211;&#65292;&#28085;&#30422;&#20102;&#23601;&#19994;&#39044;&#27979;&#21644;&#20581;&#24247;&#30417;&#27979;&#20004;&#20010;&#39046;&#22495;&#20013;&#21463;&#24433;&#21709;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.13324</link><description>&lt;p&gt;
&#26377;&#20851;&#31639;&#27861;&#20915;&#31574;&#30340;&#20449;&#24687;&#65306;&#25506;&#32034;&#21463;&#21040;&#31639;&#27861;&#20915;&#31574;&#24433;&#21709;&#30340;&#20154;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information That Matters: Exploring Information Needs of People Affected by Algorithmic Decisions. (arXiv:2401.13324v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21463;&#31639;&#27861;&#20915;&#31574;&#24433;&#21709;&#30340;&#20154;&#30340;&#20449;&#24687;&#38656;&#27714;&#65292;&#21457;&#29616;&#35299;&#37322;&#24448;&#24448;&#19981;&#33021;&#28385;&#36275;&#20182;&#20204;&#30340;&#20851;&#27880;&#28857;&#65292;&#23548;&#33268;&#23545;&#30417;&#31649;&#26694;&#26550;&#30340;&#29702;&#35299;&#21644;&#36981;&#23432;&#20135;&#29983;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#22242;&#38431;&#25552;&#20986;&#20102;XAI&#21021;&#23398;&#32773;&#38382;&#39064;&#24211;&#65292;&#28085;&#30422;&#20102;&#23601;&#19994;&#39044;&#27979;&#21644;&#20581;&#24247;&#30417;&#27979;&#20004;&#20010;&#39046;&#22495;&#20013;&#21463;&#24433;&#21709;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#31995;&#32479;&#30340;&#35299;&#37322;&#24456;&#23569;&#28041;&#21450;&#21040;&#21463;&#31639;&#27861;&#20915;&#31574;&#24433;&#21709;&#30340;&#20154;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;&#36825;&#31181;&#20256;&#36798;&#20449;&#24687;&#19982;&#21463;&#24433;&#21709;&#21033;&#30410;&#30456;&#20851;&#32773;&#25152;&#20851;&#24515;&#30340;&#20449;&#24687;&#20043;&#38388;&#30340;&#24046;&#36317;&#21487;&#33021;&#38459;&#30861;&#23545;&#30417;&#31649;&#26694;&#26550;&#65288;&#22914;AI&#27861;&#26696;&#65289;&#30340;&#29702;&#35299;&#21644;&#36981;&#23432;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;XAI&#21021;&#23398;&#32773;&#38382;&#39064;&#24211;&#8221;&#65306;&#36825;&#26159;&#19968;&#20010;&#28085;&#30422;&#20004;&#20010;&#31639;&#27861;&#20915;&#31574;&#24212;&#29992;&#39046;&#22495;&#65288;&#23601;&#19994;&#39044;&#27979;&#21644;&#20581;&#24247;&#30417;&#27979;&#65289;&#20013;&#21463;&#24433;&#21709;&#21033;&#30410;&#30456;&#20851;&#32773;&#20449;&#24687;&#38656;&#27714;&#30340;&#30446;&#24405;&#65292;&#21253;&#25324;&#25968;&#25454;&#12289;&#31995;&#32479;&#32972;&#26223;&#12289;&#31995;&#32479;&#20351;&#29992;&#21644;&#31995;&#32479;&#35268;&#33539;&#31561;&#31867;&#21035;&#12290;&#20449;&#24687;&#38656;&#27714;&#26159;&#36890;&#36807;&#35775;&#35848;&#30740;&#31350;&#25910;&#38598;&#30340;&#65292;&#21442;&#19982;&#32773;&#26681;&#25454;&#33258;&#24049;&#30340;&#38382;&#39064;&#33719;&#24471;&#35299;&#37322;&#12290;&#21442;&#19982;&#32773;&#36824;&#25253;&#21578;&#20102;&#20182;&#20204;&#30340;&#29702;&#35299;&#21644;&#20915;&#31574;&#20449;&#24515;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#22312;&#25509;&#21463;&#35299;&#37322;&#21518;&#20449;&#24515;&#20542;&#21521;&#20110;&#22686;&#21152;&#65292;&#20294;&#21442;&#19982;&#32773;&#20063;&#38754;&#20020;&#30528;&#29702;&#35299;&#19978;&#30340;&#25361;&#25112;&#65292;&#22914;&#26080;&#27861;&#35299;&#37322;&#20026;&#20160;&#20040;&#33258;&#24049;&#30340;&#29702;&#35299;&#24863;&#35273;&#19981;&#23436;&#25972;&#12290;&#35299;&#37322;&#36824;&#23545;&#29702;&#35299;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explanations of AI systems rarely address the information needs of people affected by algorithmic decision-making (ADM). This gap between conveyed information and information that matters to affected stakeholders can impede understanding and adherence to regulatory frameworks such as the AI Act. To address this gap, we present the "XAI Novice Question Bank": A catalog of affected stakeholders' information needs in two ADM use cases (employment prediction and health monitoring), covering the categories data, system context, system usage, and system specifications. Information needs were gathered in an interview study where participants received explanations in response to their inquiries. Participants further reported their understanding and decision confidence, showing that while confidence tended to increase after receiving explanations, participants also met understanding challenges, such as being unable to tell why their understanding felt incomplete. Explanations further influenced
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25972;&#20307;&#21644;&#22810;&#31890;&#24230;&#22806;&#31185;&#22330;&#26223;&#29702;&#35299;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#22522;&#20110;&#21464;&#24418;&#22120;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#20840;&#23616;&#35270;&#39057;&#29305;&#24449;&#25552;&#21462;&#21644;&#23616;&#37096;&#22120;&#26800;&#20998;&#21106;&#65292;&#21487;&#29992;&#20110;&#22810;&#23618;&#27425;&#29702;&#35299;&#22806;&#31185;&#27963;&#21160;&#12290;</title><link>http://arxiv.org/abs/2401.11174</link><description>&lt;p&gt;
&#20687;&#32032;&#32423;&#21035;&#35782;&#21035;&#29992;&#20110;&#25972;&#20307;&#22806;&#31185;&#22330;&#26223;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Pixel-Wise Recognition for Holistic Surgical Scene Understanding. (arXiv:2401.11174v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25972;&#20307;&#21644;&#22810;&#31890;&#24230;&#22806;&#31185;&#22330;&#26223;&#29702;&#35299;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#22522;&#20110;&#21464;&#24418;&#22120;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#20840;&#23616;&#35270;&#39057;&#29305;&#24449;&#25552;&#21462;&#21644;&#23616;&#37096;&#22120;&#26800;&#20998;&#21106;&#65292;&#21487;&#29992;&#20110;&#22810;&#23618;&#27425;&#29702;&#35299;&#22806;&#31185;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Prostatectomies&#30340;&#25972;&#20307;&#21644;&#22810;&#31890;&#24230;&#22806;&#31185;&#22330;&#26223;&#29702;&#35299;&#65288;GraSP&#65289;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#23545;&#22806;&#31185;&#22330;&#26223;&#29702;&#35299;&#36827;&#34892;&#20102;&#23618;&#27425;&#21270;&#24314;&#27169;&#65292;&#21253;&#25324;&#19981;&#21516;&#31890;&#24230;&#30340;&#20114;&#34917;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#22806;&#31185;&#27963;&#21160;&#30340;&#22810;&#23618;&#27425;&#29702;&#35299;&#65292;&#21253;&#25324;&#22806;&#31185;&#38454;&#27573;&#21644;&#27493;&#39588;&#30340;&#35782;&#21035;&#20197;&#21450;&#21253;&#25324;&#22806;&#31185;&#22120;&#26800;&#20998;&#21106;&#21644;&#21407;&#23376;&#21487;&#35270;&#21160;&#20316;&#26816;&#27979;&#22312;&#20869;&#30340;&#30701;&#26399;&#20219;&#21153;&#12290;&#20026;&#20102;&#21033;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#21464;&#24418;&#22120;&#65288;Transformers&#65289;&#30340;&#34892;&#21160;&#12289;&#38454;&#27573;&#12289;&#27493;&#39588;&#21644;&#22120;&#26800;&#20998;&#21106;&#65288;TAPIS&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#20840;&#23616;&#35270;&#39057;&#29305;&#24449;&#25552;&#21462;&#22120;&#19982;&#26469;&#33258;&#22120;&#26800;&#20998;&#21106;&#27169;&#22411;&#30340;&#23616;&#37096;&#21306;&#22495;&#24314;&#35758;&#30456;&#32467;&#21512;&#65292;&#20197;&#24212;&#23545;&#25105;&#20204;&#25968;&#25454;&#38598;&#30340;&#22810;&#31890;&#24230;&#38382;&#39064;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#30701;&#26399;&#35782;&#21035;&#20219;&#21153;&#20013;&#21253;&#25324;&#20998;&#21106;&#27880;&#37322;&#30340;&#24433;&#21709;&#65292;&#24182;&#31361;&#26174;&#20102;&#19981;&#21516;&#30340;&#31890;&#24230;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the Holistic and Multi-Granular Surgical Scene Understanding of Prostatectomies (GraSP) dataset, a curated benchmark that models surgical scene understanding as a hierarchy of complementary tasks with varying levels of granularity. Our approach enables a multi-level comprehension of surgical activities, encompassing long-term tasks such as surgical phases and steps recognition and short-term tasks including surgical instrument segmentation and atomic visual actions detection. To exploit our proposed benchmark, we introduce the Transformers for Actions, Phases, Steps, and Instrument Segmentation (TAPIS) model, a general architecture that combines a global video feature extractor with localized region proposals from an instrument segmentation model to tackle the multi-granularity of our benchmark. Through extensive experimentation, we demonstrate the impact of including segmentation annotations in short-term recognition tasks, highlight the varying granularity require
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GAAM&#30340;&#22810;&#22836;&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#22686;&#24378;&#36328;&#22810;&#20010;&#27169;&#24577;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;&#36890;&#36807;&#23558;&#21487;&#23398;&#20064;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#32435;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;GAAM&#33021;&#22815;&#21160;&#24577;&#22320;&#37325;&#26032;&#35843;&#25972;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#25968;&#25454;&#26102;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36229;&#36807;&#20102;&#30446;&#21069;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#30340;&#36866;&#24212;&#24615;&#24378;&#19988;&#21442;&#25968;&#25968;&#37327;&#36739;&#23569;&#65292;&#20855;&#26377;&#25913;&#36827;&#29616;&#26377;&#27880;&#24847;&#21147;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.11143</link><description>&lt;p&gt;
&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26159;&#21807;&#19968;&#25152;&#38656;&#30340;&#65306;&#36328;&#22810;&#20010;&#27169;&#24577;&#30340;&#20581;&#22766;&#19978;&#19979;&#25991;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Gaussian Adaptive Attention is All You Need: Robust Contextual Representations Across Multiple Modalities. (arXiv:2401.11143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11143
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GAAM&#30340;&#22810;&#22836;&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#22686;&#24378;&#36328;&#22810;&#20010;&#27169;&#24577;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;&#36890;&#36807;&#23558;&#21487;&#23398;&#20064;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#32435;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;GAAM&#33021;&#22815;&#21160;&#24577;&#22320;&#37325;&#26032;&#35843;&#25972;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#25968;&#25454;&#26102;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36229;&#36807;&#20102;&#30446;&#21069;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#30340;&#36866;&#24212;&#24615;&#24378;&#19988;&#21442;&#25968;&#25968;&#37327;&#36739;&#23569;&#65292;&#20855;&#26377;&#25913;&#36827;&#29616;&#26377;&#27880;&#24847;&#21147;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#22836;&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;GAAM&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#24182;&#35774;&#35745;&#20102;&#39640;&#26031;&#33258;&#36866;&#24212;&#21464;&#21387;&#22120;&#65288;GAT&#65289;&#65292;&#26088;&#22312;&#22686;&#24378;&#36328;&#22810;&#20010;&#27169;&#24577;&#65288;&#21253;&#25324;&#35821;&#38899;&#12289;&#25991;&#26412;&#21644;&#35270;&#35273;&#65289;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;GAAM&#23558;&#21487;&#23398;&#20064;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#34701;&#20837;&#20854;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;&#37319;&#29992;&#22810;&#22836;&#26694;&#26550;&#23454;&#29616;&#65292;&#20351;&#20854;&#33021;&#22815;&#38598;&#20307;&#24314;&#27169;&#20219;&#20309;&#27010;&#29575;&#20998;&#24067;&#65292;&#20197;&#21160;&#24577;&#37325;&#26032;&#35843;&#25972;&#29305;&#24449;&#37325;&#35201;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#39640;&#24230;&#38750;&#24179;&#31283;&#25968;&#25454;&#26102;&#34920;&#29616;&#20986;&#26174;&#33879;&#25913;&#36827;&#65292;&#36890;&#36807;&#35782;&#21035;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#20851;&#38190;&#20803;&#32032;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#25216;&#26415;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#30340;&#29366;&#24577;&#65288;&#31934;&#24230;&#22686;&#21152;&#32422;20%&#65289;&#12290;GAAM&#19982;&#22522;&#20110;&#28857;&#31215;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#20860;&#23481;&#65292;&#24182;&#20855;&#26377;&#30456;&#23545;&#36739;&#20302;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#23637;&#31034;&#20102;&#20854;&#36866;&#24212;&#24615;&#21644;&#25552;&#21319;&#29616;&#26377;&#27880;&#24847;&#21147;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;GAAM&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#36866;&#24212;&#24615;&#21644;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the Multi-Head Gaussian Adaptive Attention Mechanism (GAAM), a novel probabilistic attention framework, and the Gaussian Adaptive Transformer (GAT), designed to enhance information aggregation across multiple modalities, including Speech, Text and Vision. GAAM integrates learnable mean and variance into its attention mechanism, implemented in a Multi-Headed framework enabling it to collectively model any Probability Distribution for dynamic recalibration of feature significance. This method demonstrates significant improvements, especially with highly non-stationary data, surpassing the state-of-the-art attention techniques in model performance (up to approximately +20% in accuracy) by identifying key elements within the feature space. GAAM's compatibility with dot-product-based attention models and relatively low number of parameters showcases its adaptability and potential to boost existing attention frameworks. Empirically, GAAM exhibits superior adaptability and efficacy
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Chem-FINESE&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#21270;&#23398;&#39046;&#22495;&#20013;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#21644;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#26469;&#20174;&#36755;&#20837;&#21477;&#23376;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#24182;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10189</link><description>&lt;p&gt;
Chem-FINESE: &#36890;&#36807;&#25991;&#26412;&#37325;&#26500;&#39564;&#35777;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction. (arXiv:2401.10189v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10189
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Chem-FINESE&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#21270;&#23398;&#39046;&#22495;&#20013;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#21644;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#26469;&#20174;&#36755;&#20837;&#21477;&#23376;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#24182;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21270;&#23398;&#39046;&#22495;&#20013;&#65292;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#38754;&#20020;&#20004;&#20010;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#19982;&#19968;&#33324;&#39046;&#22495;&#30340;&#23454;&#20307;&#25552;&#21462;&#20219;&#21153;&#30456;&#27604;&#65292;&#21270;&#23398;&#35770;&#25991;&#20013;&#30340;&#21477;&#23376;&#36890;&#24120;&#21253;&#21547;&#26356;&#22810;&#30340;&#23454;&#20307;&#12290;&#27492;&#22806;&#65292;&#23454;&#20307;&#25552;&#21462;&#27169;&#22411;&#36890;&#24120;&#38590;&#20197;&#25552;&#21462;&#38271;&#23614;&#31867;&#22411;&#30340;&#23454;&#20307;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#26041;&#27861;Chem-FINESE&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;Chem-FINESE&#21253;&#21547;&#20004;&#20010;&#32452;&#20214;&#65306;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#29992;&#20110;&#20174;&#36755;&#20837;&#21477;&#23376;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#65292;&#20197;&#21450;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#29992;&#20110;&#20174;&#25552;&#21462;&#30340;&#23454;&#20307;&#20013;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#21463;&#21040;&#19968;&#20010;&#22909;&#30340;&#23454;&#20307;&#25552;&#21462;&#31995;&#32479;&#38656;&#35201;&#24544;&#23454;&#25552;&#21462;&#23454;&#20307;&#30340;&#20107;&#23454;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26032;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#21033;&#29992;&#23454;&#20307;&#25552;&#21462;&#32467;&#26524;&#26469;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#25439;&#22833;&#26469;&#20943;&#23569;&#22312;&#25552;&#21462;&#36807;&#31243;&#20013;&#30340;&#36807;&#24230;&#22797;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-grained few-shot entity extraction in the chemical domain faces two unique challenges. First, compared with entity extraction tasks in the general domain, sentences from chemical papers usually contain more entities. Moreover, entity extraction models usually have difficulty extracting entities of long-tailed types. In this paper, we propose Chem-FINESE, a novel sequence-to-sequence (seq2seq) based few-shot entity extraction approach, to address these two challenges. Our Chem-FINESE has two components: a seq2seq entity extractor to extract named entities from the input sentence and a seq2seq self-validation module to reconstruct the original input sentence from extracted entities. Inspired by the fact that a good entity extraction system needs to extract entities faithfully, our new self-validation module leverages entity extraction results to reconstruct the original input sentence. Besides, we design a new contrastive loss to reduce excessive copying during the extraction proces
&lt;/p&gt;</description></item><item><title>DiConStruct&#26159;&#19968;&#31181;&#22522;&#20110;&#40657;&#30418;&#27169;&#22411;&#30340;&#22240;&#26524;&#27010;&#24565;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#21644;&#27010;&#24565;&#24402;&#22240;&#26041;&#24335;&#25552;&#20379;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#30340;&#23616;&#37096;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2401.08534</link><description>&lt;p&gt;
DiConStruct: &#22522;&#20110;&#40657;&#30418;&#31934;&#21326;&#30340;&#22240;&#26524;&#27010;&#24565;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
DiConStruct: Causal Concept-based Explanations through Black-Box Distillation. (arXiv:2401.08534v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08534
&lt;/p&gt;
&lt;p&gt;
DiConStruct&#26159;&#19968;&#31181;&#22522;&#20110;&#40657;&#30418;&#27169;&#22411;&#30340;&#22240;&#26524;&#27010;&#24565;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#21644;&#27010;&#24565;&#24402;&#22240;&#26041;&#24335;&#25552;&#20379;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#30340;&#23616;&#37096;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#22312;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#31995;&#32479;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#35299;&#37322;&#24212;&#35813;&#20351;&#29992;&#20154;&#21487;&#35299;&#37322;&#30340;&#35821;&#20041;&#27010;&#24565;&#26469;&#34920;&#36798;&#12290;&#27492;&#22806;&#65292;&#35299;&#37322;&#22120;&#24212;&#35813;&#25429;&#25417;&#36825;&#20123;&#27010;&#24565;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#20197;&#20415;&#23545;&#35299;&#37322;&#36827;&#34892;&#25512;&#29702;&#12290;&#26368;&#21518;&#65292;&#35299;&#37322;&#26041;&#27861;&#24212;&#35813;&#39640;&#25928;&#65292;&#24182;&#19981;&#25439;&#23475;&#39044;&#27979;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;AI&#35299;&#37322;&#24615;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#33267;&#20170;&#27809;&#26377;&#19968;&#31181;&#26041;&#27861;&#28385;&#36275;&#36825;&#19977;&#20010;&#26465;&#20214;&#12290;&#20107;&#23454;&#19978;&#65292;&#20027;&#27969;&#30340;&#23616;&#37096;&#27010;&#24565;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#19981;&#20135;&#29983;&#22240;&#26524;&#35299;&#37322;&#65292;&#24182;&#22312;&#35299;&#37322;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DiConStruct&#65292;&#19968;&#31181;&#26082;&#22522;&#20110;&#27010;&#24565;&#21448;&#20855;&#26377;&#22240;&#26524;&#24615;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#21644;&#27010;&#24565;&#24402;&#22240;&#26041;&#24335;&#21019;&#24314;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#30340;&#23616;&#37096;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#35299;&#37322;&#22120;&#20316;&#20026;&#19968;&#20010;&#31934;&#21326;&#27169;&#22411;&#36866;&#29992;&#20110;&#20219;&#20309;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model interpretability plays a central role in human-AI decision-making systems. Ideally, explanations should be expressed using human-interpretable semantic concepts. Moreover, the causal relations between these concepts should be captured by the explainer to allow for reasoning about the explanations. Lastly, explanation methods should be efficient and not compromise the performance of the predictive task. Despite the rapid advances in AI explainability in recent years, as far as we know to date, no method fulfills these three properties. Indeed, mainstream methods for local concept explainability do not produce causal explanations and incur a trade-off between explainability and prediction performance. We present DiConStruct, an explanation method that is both concept-based and causal, with the goal of creating more interpretable local explanations in the form of structural causal models and concept attributions. Our explainer works as a distillation model to any black-box machine l
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#28165;&#27927;&#21442;&#32771;&#22270;&#20687;&#30340;&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;&#20174;&#22122;&#22768;&#20302;&#20998;&#36776;&#29575;&#30340;&#30005;&#23376;&#26174;&#24494;&#38236;&#22270;&#20687;&#37325;&#24314;&#28165;&#27905;&#30340;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#12290;&#20197;&#21450;&#20351;&#29992;&#26080;&#38656;&#28165;&#27927;&#21442;&#32771;&#30340;&#35757;&#32451;&#21644;&#24341;&#20837;&#26032;&#30340;&#32593;&#32476;&#26550;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;&#30495;&#23454;&#35757;&#32451;&#25968;&#25454;&#21487;&#20197;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2401.08115</link><description>&lt;p&gt;
&#26080;&#38656;&#28165;&#27927;&#21442;&#32771;&#22270;&#20687;&#30340;&#36229;&#20998;&#36776;&#29575;&#65306;&#24212;&#29992;&#20110;&#30005;&#23376;&#26174;&#24494;&#38236;
&lt;/p&gt;
&lt;p&gt;
No-Clean-Reference Image Super-Resolution: Application to Electron Microscopy. (arXiv:2401.08115v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08115
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#28165;&#27927;&#21442;&#32771;&#22270;&#20687;&#30340;&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;&#20174;&#22122;&#22768;&#20302;&#20998;&#36776;&#29575;&#30340;&#30005;&#23376;&#26174;&#24494;&#38236;&#22270;&#20687;&#37325;&#24314;&#28165;&#27905;&#30340;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#12290;&#20197;&#21450;&#20351;&#29992;&#26080;&#38656;&#28165;&#27927;&#21442;&#32771;&#30340;&#35757;&#32451;&#21644;&#24341;&#20837;&#26032;&#30340;&#32593;&#32476;&#26550;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;&#30495;&#23454;&#35757;&#32451;&#25968;&#25454;&#21487;&#20197;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#27861;&#33719;&#24471;&#28165;&#27905;&#30340;&#39640;&#20998;&#36776;&#29575;&#30005;&#23376;&#26174;&#24494;&#38236;&#65288;EM&#65289;&#22270;&#20687;&#38480;&#21046;&#20102;&#35768;&#22810;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65288;SR&#65289;&#26041;&#27861;&#65292;&#20197;&#20174;&#22122;&#22768;&#20302;&#20998;&#36776;&#29575;&#65288;LR&#65289;&#37319;&#38598;&#20013;&#35745;&#31639;&#37325;&#24314;&#20855;&#26377;&#22823;&#35270;&#22330;&#65288;FoV&#65289;&#30340;&#28165;&#27905;&#30340;&#39640;&#20998;&#36776;&#29575;3D-EM&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26159;&#65306;I&#65289;&#30740;&#31350;&#20351;&#29992;&#26080;&#38656;&#28165;&#27927;&#21442;&#32771;&#30340;$\ell_2$&#21644;$\ell_1$&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#65307;II&#65289;&#24341;&#20837;&#19968;&#31181;&#21517;&#20026;EMSR&#30340;&#26032;&#22411;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#22686;&#24378;LR EM&#22270;&#20687;&#30340;&#20998;&#36776;&#29575;&#21516;&#26102;&#20943;&#23569;&#22266;&#26377;&#22122;&#22768;&#65307;&#20197;&#21450;III&#65289;&#27604;&#36739;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#21253;&#25324;&#20351;&#29992;&#33719;&#21462;&#30340;LR&#21644;HR&#22270;&#20687;&#23545;&#65292;&#21363;&#20855;&#26377;&#30495;&#23454;&#27745;&#26579;&#30340;&#26080;&#38656;&#28165;&#27927;&#21442;&#32771;&#30340;&#30495;&#23454;&#23545;&#65292;&#21512;&#25104;LR&#21644;&#33719;&#21462;&#30340;HR&#23545;&#20197;&#21450;&#33719;&#21462;&#30340;LR&#21644;&#21435;&#22122;HR&#23545;&#12290;&#22312;&#20061;&#20010;&#33041;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#30495;&#23454;&#23545;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#36229;&#20998;&#36776;&#29575;&#37325;&#24314;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The inability to acquire clean high-resolution (HR) electron microscopy (EM) images over a large brain tissue volume hampers many neuroscience studies. To address this challenge, we propose a deep-learning-based image super-resolution (SR) approach to computationally reconstruct clean HR 3D-EM with a large field of view (FoV) from noisy low-resolution (LR) acquisition. Our contributions are I) Investigating training with no-clean references for $\ell_2$ and $\ell_1$ loss functions; II) Introducing a novel network architecture, named EMSR, for enhancing the resolution of LR EM images while reducing inherent noise; and, III) Comparing different training strategies including using acquired LR and HR image pairs, i.e., real pairs with no-clean references contaminated with real corruptions, the pairs of synthetic LR and acquired HR, as well as acquired LR and denoised HR pairs. Experiments with nine brain datasets showed that training with real pairs can produce high-quality super-resolved 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#21452;&#33218;&#31934;&#32454;&#25805;&#20316;&#21644;&#38656;&#35201;&#31934;&#32454;&#25805;&#20316;&#30340;&#20219;&#21153;&#65292;&#24182;&#19988;&#20844;&#24320;&#21487;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.07603</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#26426;&#22120;&#20154;&#25968;&#25454;&#29992;&#20110;&#21452;&#33218;&#31934;&#32454;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Multi-task robot data for dual-arm fine manipulation. (arXiv:2401.07603v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07603
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#21452;&#33218;&#31934;&#32454;&#25805;&#20316;&#21644;&#38656;&#35201;&#31934;&#32454;&#25805;&#20316;&#30340;&#20219;&#21153;&#65292;&#24182;&#19988;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#25805;&#32437;&#39046;&#22495;&#65292;&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;&#34987;&#35748;&#20026;&#26159;&#33719;&#21462;&#25805;&#20316;&#25216;&#33021;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#20174;&#22810;&#26679;&#21270;&#30340;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#34987;&#35748;&#20026;&#26159;&#23454;&#29616;&#22810;&#26679;&#24615;&#21644;&#36866;&#24212;&#24615;&#30340;&#21487;&#34892;&#26041;&#27861;&#12290;&#22312;&#36825;&#26679;&#30340;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#23398;&#20064;&#21508;&#31181;&#20219;&#21153;&#65292;&#26426;&#22120;&#20154;&#22312;&#22810;&#20010;&#29289;&#20307;&#19978;&#23454;&#29616;&#20102;&#26222;&#36866;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#22810;&#20219;&#21153;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;&#20027;&#35201;&#38598;&#20013;&#22312;&#30456;&#23545;&#19981;&#31934;&#30830;&#30340;&#21333;&#33218;&#20219;&#21153;&#19978;&#65292;&#24182;&#27809;&#26377;&#35299;&#20915;&#26426;&#22120;&#20154;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#38656;&#35201;&#25191;&#34892;&#30340;&#32454;&#31890;&#24230;&#29289;&#20307;&#25805;&#20316;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#25324;&#21452;&#33218;&#20219;&#21153;&#21644;/&#25110;&#38656;&#35201;&#31934;&#32454;&#25805;&#20316;&#30340;&#22810;&#26679;&#21270;&#29289;&#20307;&#25805;&#32437;&#25968;&#25454;&#38598;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#21253;&#21547;224k&#20010;&#22330;&#26223;&#30340;&#25968;&#25454;&#38598;&#65288;150&#23567;&#26102;&#65292;1,104&#20010;&#35821;&#35328;&#25351;&#20196;&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;&#21452;&#33218;&#31934;&#32454;&#20219;&#21153;&#65292;&#22914;&#31227;&#21160;&#30871;&#12289;&#25171;&#24320;&#31508;&#34955;&#25110;&#21093;&#39321;&#34121;&#65292;&#24182;&#19988;&#36825;&#20123;&#25968;&#25454;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#12290;&#27492;&#22806;&#65292;&#35813;&#25968;&#25454;&#38598;&#36824;&#21253;&#25324;&#35270;&#35273;&#27880;&#24847;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of robotic manipulation, deep imitation learning is recognized as a promising approach for acquiring manipulation skills. Additionally, learning from diverse robot datasets is considered a viable method to achieve versatility and adaptability. In such research, by learning various tasks, robots achieved generality across multiple objects. However, such multi-task robot datasets have mainly focused on single-arm tasks that are relatively imprecise, not addressing the fine-grained object manipulation that robots are expected to perform in the real world. This paper introduces a dataset of diverse object manipulations that includes dual-arm tasks and/or tasks requiring fine manipulation. To this end, we have generated dataset with 224k episodes (150 hours, 1,104 language instructions) which includes dual-arm fine tasks such as bowl-moving, pencil-case opening or banana-peeling, and this data is publicly available. Additionally, this dataset includes visual attention signals a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DevEval&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#23454;&#38469;&#36719;&#20214;&#39033;&#30446;&#20013;&#30340;&#20195;&#30721;&#29983;&#25104;&#12290;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#30456;&#27604;&#65292;DevEval&#22312;&#30495;&#23454;&#30340;&#39033;&#30446;&#20998;&#24067;&#12289;&#20805;&#36275;&#30340;&#20381;&#36182;&#21644;&#36275;&#22815;&#35268;&#27169;&#30340;&#39033;&#30446;&#32972;&#26223;&#31561;&#26041;&#38754;&#26356;&#36148;&#21512;&#23454;&#38469;&#12290;&#36890;&#36807;&#23545;&#20116;&#20010;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#23454;&#38469;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.06401</link><description>&lt;p&gt;
DevEval: &#35780;&#20272;&#23454;&#38469;&#36719;&#20214;&#39033;&#30446;&#20013;&#30340;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DevEval: Evaluating Code Generation in Practical Software Projects. (arXiv:2401.06401v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DevEval&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#23454;&#38469;&#36719;&#20214;&#39033;&#30446;&#20013;&#30340;&#20195;&#30721;&#29983;&#25104;&#12290;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#30456;&#27604;&#65292;DevEval&#22312;&#30495;&#23454;&#30340;&#39033;&#30446;&#20998;&#24067;&#12289;&#20805;&#36275;&#30340;&#20381;&#36182;&#21644;&#36275;&#22815;&#35268;&#27169;&#30340;&#39033;&#30446;&#32972;&#26223;&#31561;&#26041;&#38754;&#26356;&#36148;&#21512;&#23454;&#38469;&#12290;&#36890;&#36807;&#23545;&#20116;&#20010;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#23454;&#38469;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#34920;&#29616;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#35768;&#22810;&#22522;&#20934;&#27979;&#35797;&#24050;&#32463;&#25552;&#20986;&#65292;&#20294;&#26159;&#19982;&#23454;&#38469;&#36719;&#20214;&#39033;&#30446;&#19981;&#19968;&#33268;&#65292;&#20363;&#22914;&#34394;&#26500;&#30340;&#31243;&#24207;&#20998;&#24067;&#65292;&#20381;&#36182;&#19981;&#36275;&#21644;&#23567;&#35268;&#27169;&#39033;&#30446;&#32972;&#26223;&#12290;&#22240;&#27492;&#65292;LLMs&#22312;&#23454;&#38469;&#39033;&#30446;&#20013;&#30340;&#33021;&#21147;&#36824;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DevEval&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#19982;&#24320;&#21457;&#20154;&#21592;&#22312;&#23454;&#38469;&#39033;&#30446;&#20013;&#30340;&#32463;&#39564;&#30456;&#21563;&#21512;&#12290;DevEval&#36890;&#36807;&#19968;&#20010;&#20005;&#26684;&#30340;&#27969;&#31243;&#25910;&#38598;&#21040;&#20102;&#26469;&#33258;119&#20010;&#23454;&#38469;&#39033;&#30446;&#30340;2690&#20010;&#26679;&#26412;&#65292;&#28085;&#30422;10&#20010;&#39046;&#22495;&#12290;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#30456;&#27604;&#65292;DevEval&#22312;&#22810;&#20010;&#32500;&#24230;&#19978;&#19982;&#23454;&#38469;&#39033;&#30446;&#30456;&#21563;&#21512;&#65292;&#20363;&#22914;&#30495;&#23454;&#30340;&#31243;&#24207;&#20998;&#24067;&#65292;&#20805;&#36275;&#30340;&#20381;&#36182;&#21644;&#36275;&#22815;&#35268;&#27169;&#30340;&#39033;&#30446;&#32972;&#26223;&#12290;&#25105;&#20204;&#22312;DevEval&#19978;&#35780;&#20272;&#20102;&#20116;&#20010;&#27969;&#34892;&#30340;LLMs&#65288;&#20363;&#22914;gpt-4&#65292;gpt-3.5-turbo&#65292;CodeLLaMa&#21644;StarCoder&#65289;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#23454;&#38469;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;gpt-3.5-turbo&#30340;&#26368;&#39640;Pass@1&#21482;&#26377;42&#12290;
&lt;/p&gt;
&lt;p&gt;
How to evaluate Large Language Models (LLMs) in code generation is an open question. Many benchmarks have been proposed but are inconsistent with practical software projects, e.g., unreal program distributions, insufficient dependencies, and small-scale project contexts. Thus, the capabilities of LLMs in practical projects are still unclear. In this paper, we propose a new benchmark named DevEval, aligned with Developers' experiences in practical projects. DevEval is collected through a rigorous pipeline, containing 2,690 samples from 119 practical projects and covering 10 domains. Compared to previous benchmarks, DevEval aligns to practical projects in multiple dimensions, e.g., real program distributions, sufficient dependencies, and enough-scale project contexts. We assess five popular LLMs on DevEval (e.g., gpt-4, gpt-3.5-turbo, CodeLLaMa, and StarCoder) and reveal their actual abilities in code generation. For instance, the highest Pass@1 of gpt-3.5-turbo only is 42 in our experim
&lt;/p&gt;</description></item><item><title>Agent AI&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#20132;&#20114;&#31995;&#32479;&#65292;&#21487;&#20197;&#24863;&#30693;&#35270;&#35273;&#21050;&#28608;&#12289;&#35821;&#35328;&#36755;&#20837;&#21644;&#20854;&#20182;&#29615;&#22659;&#30456;&#20851;&#25968;&#25454;&#65292;&#36890;&#36807;&#23558;&#20195;&#29702;&#20307;&#23884;&#20837;&#29289;&#29702;&#25110;&#34394;&#25311;&#29615;&#22659;&#20013;&#26469;&#23454;&#29616;&#26356;&#22797;&#26434;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2401.03568</link><description>&lt;p&gt;
Agent AI: &#23545;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#27178;&#21521;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Agent AI: Surveying the Horizons of Multimodal Interaction. (arXiv:2401.03568v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03568
&lt;/p&gt;
&lt;p&gt;
Agent AI&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#20132;&#20114;&#31995;&#32479;&#65292;&#21487;&#20197;&#24863;&#30693;&#35270;&#35273;&#21050;&#28608;&#12289;&#35821;&#35328;&#36755;&#20837;&#21644;&#20854;&#20182;&#29615;&#22659;&#30456;&#20851;&#25968;&#25454;&#65292;&#36890;&#36807;&#23558;&#20195;&#29702;&#20307;&#23884;&#20837;&#29289;&#29702;&#25110;&#34394;&#25311;&#29615;&#22659;&#20013;&#26469;&#23454;&#29616;&#26356;&#22797;&#26434;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24456;&#21487;&#33021;&#20250;&#25104;&#20026;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#26080;&#22788;&#19981;&#22312;&#30340;&#23384;&#22312;&#12290;&#20351;&#20854;&#26356;&#20114;&#21160;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#26159;&#23558;&#23427;&#20204;&#20316;&#20026;&#20195;&#29702;&#20307;&#29616;&#22312;&#29289;&#29702;&#21644;&#34394;&#25311;&#29615;&#22659;&#20013;&#12290;&#30446;&#21069;&#65292;&#31995;&#32479;&#21033;&#29992;&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#21019;&#24314;&#20195;&#29702;&#20307;&#30340;&#22522;&#26412;&#26500;&#24314;&#27169;&#22359;&#12290;&#23558;&#20195;&#29702;&#20307;&#23884;&#20837;&#36825;&#26679;&#30340;&#29615;&#22659;&#20013;&#26377;&#21161;&#20110;&#27169;&#22411;&#22788;&#29702;&#21644;&#35299;&#37322;&#35270;&#35273;&#21644;&#29615;&#22659;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#36825;&#23545;&#20110;&#21019;&#24314;&#26356;&#22797;&#26434;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#20363;&#22914;&#65292;&#19968;&#20010;&#31995;&#32479;&#21487;&#20197;&#24863;&#30693;&#29992;&#25143;&#21160;&#20316;&#12289;&#20154;&#31867;&#34892;&#20026;&#12289;&#29615;&#22659;&#29289;&#20307;&#12289;&#38899;&#39057;&#34920;&#36798;&#21644;&#22330;&#26223;&#30340;&#38598;&#20307;&#24773;&#24863;&#65292;&#20174;&#32780;&#22312;&#32473;&#23450;&#29615;&#22659;&#20013;&#20026;&#20195;&#29702;&#20307;&#25552;&#20379;&#20449;&#24687;&#21644;&#25351;&#23548;&#12290;&#20026;&#20102;&#21152;&#36895;&#20195;&#29702;&#20307;&#22810;&#27169;&#24577;&#26234;&#33021;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#23558;&#8220;Agent AI&#8221;&#23450;&#20041;&#20026;&#19968;&#31867;&#21487;&#20197;&#24863;&#30693;&#35270;&#35273;&#21050;&#28608;&#12289;&#35821;&#35328;&#36755;&#20837;&#21644;&#20854;&#20182;&#29615;&#22659;&#30456;&#20851;&#25968;&#25454;&#30340;&#20132;&#20114;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal AI systems will likely become a ubiquitous presence in our everyday lives. A promising approach to making these systems more interactive is to embody them as agents within physical and virtual environments. At present, systems leverage existing foundation models as the basic building blocks for the creation of embodied agents. Embedding agents within such environments facilitates the ability of models to process and interpret visual and contextual data, which is critical for the creation of more sophisticated and context-aware AI systems. For example, a system that can perceive user actions, human behavior, environmental objects, audio expressions, and the collective sentiment of a scene can be used to inform and direct agent responses within the given environment. To accelerate research on agent-based multimodal intelligence, we define "Agent AI" as a class of interactive systems that can perceive visual stimuli, language inputs, and other environmentally-grounded data, an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#37327;&#21270;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#20010;&#20307;&#26234;&#33021;&#20307;&#37325;&#35201;&#24615;&#30340;&#26041;&#27861;&#65292;&#30456;&#23545;&#20110;&#26234;&#33021;&#20307;&#25968;&#37327;&#20855;&#26377;&#32447;&#24615;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19982;&#30495;&#23454;&#30340;Shapley&#20540;&#20197;&#21450;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#20010;&#20307;&#26234;&#33021;&#20307;&#22870;&#21169;&#23494;&#20999;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2312.08466</link><description>&lt;p&gt;
&#39640;&#25928;&#37327;&#21270;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#20010;&#20307;&#26234;&#33021;&#20307;&#37325;&#35201;&#24615;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficiently Quantifying Individual Agent Importance in Cooperative MARL. (arXiv:2312.08466v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.08466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#37327;&#21270;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#20010;&#20307;&#26234;&#33021;&#20307;&#37325;&#35201;&#24615;&#30340;&#26041;&#27861;&#65292;&#30456;&#23545;&#20110;&#26234;&#33021;&#20307;&#25968;&#37327;&#20855;&#26377;&#32447;&#24615;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19982;&#30495;&#23454;&#30340;Shapley&#20540;&#20197;&#21450;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#20010;&#20307;&#26234;&#33021;&#20307;&#22870;&#21169;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#27979;&#37327;&#20010;&#20307;&#26234;&#33021;&#20307;&#30340;&#36129;&#29486;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#22242;&#38431;&#34920;&#29616;&#36890;&#24120;&#26159;&#36890;&#36807;&#21333;&#19968;&#20849;&#20139;&#30340;&#20840;&#23616;&#22870;&#21169;&#25512;&#26029;&#20986;&#26469;&#30340;&#12290;&#30446;&#21069;&#26368;&#22909;&#30340;&#26377;&#25928;&#27979;&#37327;&#20010;&#20307;&#26234;&#33021;&#20307;&#36129;&#29486;&#30340;&#26041;&#27861;&#20043;&#19968;&#26159;&#20351;&#29992;Shapley&#20540;&#12290;&#28982;&#32780;&#65292;&#35745;&#31639;&#36825;&#20123;&#20540;&#30340;&#25104;&#26412;&#24456;&#39640;&#65292;&#22240;&#20026;&#35745;&#31639;&#22797;&#26434;&#24230;&#19982;&#26234;&#33021;&#20307;&#25968;&#37327;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#24046;&#24322;&#22870;&#21169;&#26041;&#27861;&#25913;&#36827;&#20026;&#19968;&#31181;&#37327;&#21270;&#20010;&#20307;&#26234;&#33021;&#20307;&#36129;&#29486;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#31216;&#20026;Agent Importance&#65292;&#23427;&#30456;&#23545;&#20110;&#26234;&#33021;&#20307;&#25968;&#37327;&#20855;&#26377;&#32447;&#24615;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35745;&#31639;&#24471;&#21040;&#30340;&#20540;&#19982;&#30495;&#23454;&#30340;Shapley&#20540;&#20197;&#21450;&#29992;&#20316;&#29615;&#22659;&#20013;&#30495;&#23454;&#20010;&#20307;&#26234;&#33021;&#20307;&#22870;&#21169;&#30340;&#22522;&#26412;&#20107;&#23454;&#23494;&#20999;&#30456;&#20851;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Agent Importance&#22914;&#20309;&#24110;&#21161;&#30740;&#31350;MARL&#12290;
&lt;/p&gt;
&lt;p&gt;
Measuring the contribution of individual agents is challenging in cooperative multi-agent reinforcement learning (MARL). In cooperative MARL, team performance is typically inferred from a single shared global reward. Arguably, among the best current approaches to effectively measure individual agent contributions is to use Shapley values. However, calculating these values is expensive as the computational complexity grows exponentially with respect to the number of agents. In this paper, we adapt difference rewards into an efficient method for quantifying the contribution of individual agents, referred to as Agent Importance, offering a linear computational complexity relative to the number of agents. We show empirically that the computed values are strongly correlated with the true Shapley values, as well as the true underlying individual agent rewards, used as the ground truth in environments where these are available. We demonstrate how Agent Importance can be used to help study MAR
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#35775;&#20102;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#39046;&#22495;&#30340;&#35780;&#20272;&#38382;&#39064;&#65292;&#24182;&#25193;&#23637;&#20102;&#20043;&#21069;&#30340;&#25968;&#25454;&#24211;&#65292;&#21457;&#29616;&#35768;&#22810;&#20196;&#20154;&#25285;&#24551;&#30340;&#24615;&#33021;&#25253;&#21578;&#36235;&#21183;&#20173;&#28982;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2312.08463</link><description>&lt;p&gt;
&#19968;&#24180;&#33021;&#21457;&#29983;&#22810;&#22823;&#21464;&#21270;&#65311;&#37325;&#35775;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
How much can change in a year? Revisiting Evaluation in Multi-Agent Reinforcement Learning. (arXiv:2312.08463v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.08463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#35775;&#20102;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#39046;&#22495;&#30340;&#35780;&#20272;&#38382;&#39064;&#65292;&#24182;&#25193;&#23637;&#20102;&#20043;&#21069;&#30340;&#25968;&#25454;&#24211;&#65292;&#21457;&#29616;&#35768;&#22810;&#20196;&#20154;&#25285;&#24551;&#30340;&#24615;&#33021;&#25253;&#21578;&#36235;&#21183;&#20173;&#28982;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20219;&#20309;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#20013;&#65292;&#24314;&#31435; sound experimental standards &#21644; rigour &#37117;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#28145;&#24230;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#23601;&#26159;&#36825;&#26679;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#20196;&#20154;&#28608;&#21160;&#30340;&#36827;&#23637;&#65292;&#20294;MARL&#26368;&#36817;&#22240;&#20026;&#21487;&#22797;&#21046;&#24615;&#38382;&#39064;&#21644;&#32570;&#20047;&#26631;&#20934;&#21270;&#35780;&#20272;&#26041;&#27861;&#36973;&#21040;&#20102;&#36136;&#30097;&#65292;&#23588;&#20854;&#26159;&#22312;&#21512;&#20316;&#35774;&#32622;&#20013;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#21327;&#35758;&#26469;&#24110;&#21161;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#31215;&#26497;&#30417;&#27979;&#35813;&#39046;&#22495;&#30340;&#20581;&#24247;&#29366;&#20917;&#20173;&#28982;&#26159;&#37325;&#35201;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#20043;&#21069;&#25152;&#21457;&#34920;&#30340;&#20851;&#20110;&#35780;&#20272;&#26041;&#27861;&#30340;&#25968;&#25454;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#26469;&#33258;&#39030;&#32423;&#20250;&#35758;&#30340;MARL&#20986;&#29256;&#29289;&#30340;&#20803;&#25968;&#25454;&#65292;&#24182;&#23558;&#20174;&#27492;&#26356;&#26032;&#21518;&#30340;&#25968;&#25454;&#24211;&#20013;&#25552;&#21462;&#30340;&#21457;&#29616;&#19982;&#20182;&#20204;&#30340;&#24037;&#20316;&#20013;&#30830;&#23450;&#30340;&#36235;&#21183;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#35768;&#22810;&#20196;&#20154;&#25285;&#24551;&#30340;&#24615;&#33021;&#25253;&#21578;&#36235;&#21183;&#20173;&#28982;&#23384;&#22312;&#12290;&#36825;&#21253;&#25324;&#30465;&#30053;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#19981;&#25253;&#21578;&#25152;&#26377;&#30456;&#20851;&#30340;&#35780;&#20272;&#32454;&#33410;&#20197;&#21450;&#31639;&#27861;&#24320;&#21457;&#31867;&#21035;&#30340;&#25910;&#31364;&#12290;
&lt;/p&gt;
&lt;p&gt;
Establishing sound experimental standards and rigour is important in any growing field of research. Deep Multi-Agent Reinforcement Learning (MARL) is one such nascent field. Although exciting progress has been made, MARL has recently come under scrutiny for replicability issues and a lack of standardised evaluation methodology, specifically in the cooperative setting. Although protocols have been proposed to help alleviate the issue, it remains important to actively monitor the health of the field. In this work, we extend the database of evaluation methodology previously published by containing meta-data on MARL publications from top-rated conferences and compare the findings extracted from this updated database to the trends identified in their work. Our analysis shows that many of the worrying trends in performance reporting remain. This includes the omission of uncertainty quantification, not reporting all relevant evaluation details and a narrowing of algorithmic development classe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#23433;&#20840;&#21644;&#38544;&#31169;&#30340;&#30456;&#20851;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;LLMs&#22312;&#23433;&#20840;&#21644;&#38544;&#31169;&#20445;&#25252;&#26041;&#38754;&#20855;&#26377;&#31215;&#26497;&#24433;&#21709;&#65292;&#20294;&#21516;&#26102;&#20063;&#23384;&#22312;&#28508;&#22312;&#30340;&#39118;&#38505;&#12289;&#23041;&#32961;&#21644;&#28431;&#27934;&#12290;</title><link>http://arxiv.org/abs/2312.02003</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23433;&#20840;&#19982;&#38544;&#31169;&#30340;&#35843;&#30740;&#65306;&#32654;&#22909;&#12289;&#24694;&#21155;&#21644;&#19985;&#38475;(arXiv:2312.02003v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly. (arXiv:2312.02003v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.02003
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#23433;&#20840;&#21644;&#38544;&#31169;&#30340;&#30456;&#20851;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;LLMs&#22312;&#23433;&#20840;&#21644;&#38544;&#31169;&#20445;&#25252;&#26041;&#38754;&#20855;&#26377;&#31215;&#26497;&#24433;&#21709;&#65292;&#20294;&#21516;&#26102;&#20063;&#23384;&#22312;&#28508;&#22312;&#30340;&#39118;&#38505;&#12289;&#23041;&#32961;&#21644;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#21644;Bard&#65292;&#24050;&#32463;&#38761;&#26032;&#20102;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#12290;&#23427;&#20204;&#20855;&#26377;&#28145;&#20837;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12289;&#20154;&#31867;&#33324;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#12289;&#35821;&#22659;&#24863;&#30693;&#21644;&#24378;&#22823;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#65288;&#22914;&#25628;&#32034;&#24341;&#25806;&#12289;&#23458;&#25143;&#25903;&#25345;&#12289;&#32763;&#35793;&#65289;&#20013;&#20855;&#26377;&#19981;&#21487;&#20272;&#37327;&#30340;&#20215;&#20540;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;LLMs&#20063;&#22312;&#23433;&#20840;&#39046;&#22495;&#24341;&#36215;&#20102;&#20851;&#27880;&#65292;&#25581;&#31034;&#20102;&#23433;&#20840;&#28431;&#27934;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#23433;&#20840;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;LLMs&#19982;&#23433;&#20840;&#21644;&#38544;&#31169;&#30340;&#20132;&#21449;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22914;&#20309;&#23545;&#23433;&#20840;&#21644;&#38544;&#31169;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#65292;&#23427;&#20204;&#20351;&#29992;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#39118;&#38505;&#21644;&#23041;&#32961;&#65292;&#20197;&#21450;LLMs&#20869;&#22312;&#30340;&#28431;&#27934;&#12290;&#36890;&#36807;&#32508;&#21512;&#25991;&#29486;&#22238;&#39038;&#65292;&#26412;&#25991;&#23558;&#25991;&#29486;&#20998;&#20026;&#8220;&#32654;&#22909;&#8221;&#65288;&#26377;&#30410;&#30340;LLM&#24212;&#29992;&#65289;&#12289;&#8220;&#24694;&#21155;&#8221;&#65288;&#25915;&#20987;&#24615;&#24212;&#29992;&#65289;&#21644;&#8220;&#19985;&#38475;&#8221;&#65288;LLMs&#30340;&#28431;&#27934;&#21450;&#20854;&#38450;&#24481;&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionized natural language understanding and generation. They possess deep language comprehension, human-like text generation capabilities, contextual awareness, and robust problem-solving skills, making them invaluable in various domains (e.g., search engines, customer support, translation). In the meantime, LLMs have also gained traction in the security community, revealing security vulnerabilities and showcasing their potential in security-related tasks. This paper explores the intersection of LLMs with security and privacy. Specifically, we investigate how LLMs positively impact security and privacy, potential risks and threats associated with their use, and inherent vulnerabilities within LLMs. Through a comprehensive literature review, the paper categorizes the papers into "The Good" (beneficial LLM applications), "The Bad" (offensive applications), and "The Ugly" (vulnerabilities of LLMs and their defenses). We ha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#38463;&#25289;&#20271;&#35821;&#22635;&#23383;&#28216;&#25103;&#29983;&#25104;&#22120;ArabIcros&#65292;&#21033;&#29992;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#29420;&#29305;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32447;&#32034;&#65292;&#24182;&#36890;&#36807;&#25945;&#32946;&#24615;&#22635;&#23383;&#28216;&#25103;&#25552;&#21319;&#23398;&#20064;&#20307;&#39564;&#65292;&#25913;&#21464;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#30340;&#26684;&#23616;&#12290;</title><link>http://arxiv.org/abs/2312.01339</link><description>&lt;p&gt;
ArabIcros: &#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#38463;&#25289;&#20271;&#35821;&#22635;&#23383;&#28216;&#25103;&#29983;&#25104;&#22120;&#29992;&#20110;&#25945;&#32946;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
ArabIcros: AI-Powered Arabic Crossword Puzzle Generation for Educational Applications. (arXiv:2312.01339v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.01339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#38463;&#25289;&#20271;&#35821;&#22635;&#23383;&#28216;&#25103;&#29983;&#25104;&#22120;ArabIcros&#65292;&#21033;&#29992;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#29420;&#29305;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32447;&#32034;&#65292;&#24182;&#36890;&#36807;&#25945;&#32946;&#24615;&#22635;&#23383;&#28216;&#25103;&#25552;&#21319;&#23398;&#20064;&#20307;&#39564;&#65292;&#25913;&#21464;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#30340;&#26684;&#23616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#27454;&#30001;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#39537;&#21160;&#30340;&#38463;&#25289;&#20271;&#35821;&#22635;&#23383;&#28216;&#25103;&#29983;&#25104;&#22120;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;GPT4&#12289;GPT3-Davinci&#12289;GPT3-Curie&#12289;GPT3-Babbage&#12289;GPT3-Ada&#21644;BERT&#31561;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29983;&#25104;&#29420;&#29305;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32447;&#32034;&#12290;&#22522;&#20110;&#21253;&#21547;&#36229;&#36807;50,000&#20010;&#32447;&#32034;-&#31572;&#26696;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#29983;&#25104;&#22120;&#37319;&#29992;&#24494;&#35843;&#12289;&#23569;&#37327;/&#38646;&#26679;&#26412;&#23398;&#20064;&#31574;&#30053;&#21644;&#20005;&#26684;&#30340;&#36136;&#37327;&#26816;&#26597;&#21327;&#35758;&#65292;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#32447;&#32034;-&#31572;&#26696;&#23545;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25945;&#32946;&#24615;&#22635;&#23383;&#28216;&#25103;&#26377;&#21161;&#20110;&#22686;&#24378;&#35760;&#24518;&#21147;&#12289;&#25193;&#23637;&#35789;&#27719;&#37327;&#21644;&#20419;&#36827;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#20174;&#32780;&#36890;&#36807;&#26377;&#36259;&#21644;&#21560;&#24341;&#20154;&#30340;&#26041;&#24335;&#22686;&#24378;&#23398;&#20064;&#20307;&#39564;&#65292;&#25913;&#21464;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#30340;&#26684;&#23616;&#12290;&#25972;&#20010;&#31995;&#32479;&#21487;&#20197;&#34987;&#29992;&#20316;&#24378;&#22823;&#30340;&#25945;&#32946;&#24037;&#20855;&#65292;&#34701;&#21512;&#20154;&#24037;&#26234;&#33021;&#21644;&#21019;&#26032;&#30340;&#23398;&#20064;&#25216;&#26415;&#65292;&#20026;&#38463;&#25289;&#20271;&#35821;&#22635;&#23383;&#28216;&#25103;&#21644;&#25216;&#26415;&#19982;&#25945;&#32946;&#20132;&#21449;&#39046;&#22495;&#24102;&#26469;&#21464;&#38761;&#26102;&#20195;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the first Arabic crossword puzzle generator driven by advanced AI technology. Leveraging cutting-edge large language models including GPT4, GPT3-Davinci, GPT3-Curie, GPT3-Babbage, GPT3-Ada, and BERT, the system generates distinctive and challenging clues. Based on a dataset comprising over 50,000 clue-answer pairs, the generator employs fine-tuning, few/zero-shot learning strategies, and rigorous quality-checking protocols to enforce the generation of high-quality clue-answer pairs. Importantly, educational crosswords contribute to enhancing memory, expanding vocabulary, and promoting problem-solving skills, thereby augmenting the learning experience through a fun and engaging approach, reshaping the landscape of traditional learning methods. The overall system can be exploited as a powerful educational tool that amalgamates AI and innovative learning techniques, heralding a transformative era for Arabic crossword puzzles and the intersection of technology and educa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;GPT-4V&#22312;&#33258;&#21160;&#39550;&#39542;&#30340;&#34892;&#20154;&#34892;&#20026;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#38590;&#20197;&#25429;&#25417;&#34892;&#20154;&#21644;&#20132;&#36890;&#20043;&#38388;&#21160;&#24577;&#20114;&#21160;&#20197;&#21450;&#32570;&#20047;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.14786</link><description>&lt;p&gt;
GPT-4V&#39550;&#39542;&#65306;&#34892;&#20154;&#34892;&#20026;&#39044;&#27979;&#30340;&#25361;&#25112;&#19982;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
GPT-4V Takes the Wheel: Promises and Challenges for Pedestrian Behavior Prediction. (arXiv:2311.14786v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.14786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;GPT-4V&#22312;&#33258;&#21160;&#39550;&#39542;&#30340;&#34892;&#20154;&#34892;&#20026;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#38590;&#20197;&#25429;&#25417;&#34892;&#20154;&#21644;&#20132;&#36890;&#20043;&#38388;&#21160;&#24577;&#20114;&#21160;&#20197;&#21450;&#32570;&#20047;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#34892;&#20154;&#34892;&#20026;&#26159;&#30830;&#20445;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#23433;&#20840;&#21487;&#38752;&#30340;&#20851;&#38190;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#27880;&#37322;&#30340;&#35270;&#39057;&#24207;&#21015;&#34920;&#29616;&#20986;&#20102;&#24456;&#22823;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#26080;&#27861;&#23436;&#20840;&#25226;&#25569;&#34892;&#20154;&#21644;&#20132;&#36890;&#20043;&#38388;&#30340;&#21160;&#24577;&#20114;&#21160;&#65292;&#36825;&#23545;&#20934;&#30830;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20123;&#27169;&#22411;&#36824;&#32570;&#20047;&#32454;&#33268;&#20837;&#24494;&#30340;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#20026;&#36825;&#20123;&#27169;&#22411;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#38598;&#26082;&#26114;&#36149;&#21448;&#38590;&#20197;&#36866;&#24212;&#26032;&#24773;&#20917;&#12290;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#20986;&#29616;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#22791;&#20808;&#36827;&#30340;&#35270;&#35273;&#21644;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#30740;&#31350;&#26159;&#39318;&#27425;&#22312;&#33258;&#21160;&#39550;&#39542;&#30340;&#34892;&#20154;&#34892;&#20026;&#39044;&#27979;&#32972;&#26223;&#19979;&#23545;VLMs&#36827;&#34892;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#12290;&#25105;&#20204;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#34892;&#20154;&#25968;&#25454;&#38598;JAAD&#21644;WiDEVIEW&#19978;&#35780;&#20272;&#20102;GPT-4V(ision)&#12290;&#25105;&#20204;&#30340;&#23450;&#37327;&#20998;&#26512;&#37325;&#28857;&#20851;&#27880;GPT-4V&#39044;&#27979;&#34892;&#20154;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting pedestrian behavior is the key to ensure safety and reliability of autonomous vehicles. While deep learning methods have been promising by learning from annotated video frame sequences, they often fail to fully grasp the dynamic interactions between pedestrians and traffic, crucial for accurate predictions. These models also lack nuanced common sense reasoning. Moreover, the manual annotation of datasets for these models is expensive and challenging to adapt to new situations. The advent of Vision Language Models (VLMs) introduces promising alternatives to these issues, thanks to their advanced visual and causal reasoning skills. To our knowledge, this research is the first to conduct both quantitative and qualitative evaluations of VLMs in the context of pedestrian behavior prediction for autonomous driving. We evaluate GPT-4V(ision) on publicly available pedestrian datasets: JAAD and WiDEVIEW. Our quantitative analysis focuses on GPT-4V's ability to predict pedestrian beha
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#22238;&#39038;&#20102;&#26426;&#22120;&#23398;&#20064;&#22914;&#20309;&#25913;&#21464;&#25105;&#20204;&#27169;&#25311;&#22320;&#29699;&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#24320;&#21457;&#28151;&#21512;&#30340;&#20154;&#24037;&#26234;&#33021;-&#29289;&#29702;&#27169;&#22411;&#12289;&#24378;&#35843;AI&#38477;&#23610;&#24230;&#26041;&#27861;&#30340;&#31283;&#20581;&#24615;&#20197;&#21450;&#25512;&#21160;&#21253;&#23481;&#24615;&#27169;&#22411;&#24320;&#21457;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2311.13691</link><description>&lt;p&gt;
&#19979;&#19968;&#20195;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#65306;&#38754;&#21521;&#21487;&#38752;&#30340;&#28151;&#21512;&#27169;&#22411;&#30340;&#22825;&#27668;&#21644;&#27668;&#20505;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Next-Generation Earth System Models: Towards Reliable Hybrid Models for Weather and Climate Applications. (arXiv:2311.13691v2 [physics.ao-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13691
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#22238;&#39038;&#20102;&#26426;&#22120;&#23398;&#20064;&#22914;&#20309;&#25913;&#21464;&#25105;&#20204;&#27169;&#25311;&#22320;&#29699;&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#24320;&#21457;&#28151;&#21512;&#30340;&#20154;&#24037;&#26234;&#33021;-&#29289;&#29702;&#27169;&#22411;&#12289;&#24378;&#35843;AI&#38477;&#23610;&#24230;&#26041;&#27861;&#30340;&#31283;&#20581;&#24615;&#20197;&#21450;&#25512;&#21160;&#21253;&#23481;&#24615;&#27169;&#22411;&#24320;&#21457;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22238;&#39038;&#20102;&#26426;&#22120;&#23398;&#20064;&#22914;&#20309;&#25913;&#21464;&#25105;&#20204;&#27169;&#25311;&#22320;&#29699;&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#24182;&#26399;&#26395;&#36817;&#26399;&#30340;&#31361;&#30772;&#23558;&#20351;&#29790;&#22763;&#30340;&#26368;&#32456;&#29992;&#25143;&#21463;&#30410;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#22238;&#39038;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#24314;&#35758;&#12290;&#24314;&#35758;1: &#24320;&#21457;&#28151;&#21512;&#30340;&#20154;&#24037;&#26234;&#33021;-&#29289;&#29702;&#27169;&#22411;&#65306;&#24378;&#35843;&#25972;&#21512;&#20154;&#24037;&#26234;&#33021;&#21644;&#29289;&#29702;&#24314;&#27169;&#65292;&#20197;&#25552;&#39640;&#21487;&#38752;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36739;&#38271;&#30340;&#39044;&#27979;&#26102;&#27573;&#65292;&#21516;&#26102;&#24179;&#34913;&#22522;&#20110;&#30693;&#35782;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#32452;&#20214;&#20197;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;&#24314;&#35758;2: &#24378;&#35843;AI&#38477;&#23610;&#24230;&#26041;&#27861;&#30340;&#31283;&#20581;&#24615;&#65292;&#20248;&#20808;&#20351;&#29992;&#23562;&#37325;&#29289;&#29702;&#23450;&#24459;&#12289;&#20445;&#30041;&#21464;&#37327;&#38388;&#20381;&#36182;&#20851;&#31995;&#21644;&#31354;&#38388;&#32467;&#26500;&#12289;&#20934;&#30830;&#34920;&#31034;&#23616;&#37096;&#26497;&#31471;&#24773;&#20917;&#30340;&#25216;&#26415;&#12290;&#24314;&#35758;3: &#25512;&#21160;&#21253;&#23481;&#24615;&#27169;&#22411;&#24320;&#21457;&#65306;&#30830;&#20445;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#30340;&#24320;&#21457;&#23545;&#22810;&#20803;&#21033;&#30410;&#30456;&#20851;&#32773;&#26159;&#24320;&#25918;&#21644;&#21487;&#35775;&#38382;&#30340;&#65292;&#20351;&#39044;&#27979;&#21592;&#12289;&#20844;&#20247;&#21644;AI/&#32479;&#35745;&#19987;&#23478;&#33021;&#22815;&#20351;&#29992;&#12289;&#24320;&#21457;&#21644;&#21442;&#19982;&#20854;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We review how machine learning has transformed our ability to model the Earth system, and how we expect recent breakthroughs to benefit end-users in Switzerland in the near future. Drawing from our review, we identify three recommendations.  Recommendation 1: Develop Hybrid AI-Physical Models: Emphasize the integration of AI and physical modeling for improved reliability, especially for longer prediction horizons, acknowledging the delicate balance between knowledge-based and data-driven components required for optimal performance. Recommendation 2: Emphasize Robustness in AI Downscaling Approaches, favoring techniques that respect physical laws, preserve inter-variable dependencies and spatial structures, and accurately represent extremes at the local scale. Recommendation 3: Promote Inclusive Model Development: Ensure Earth System Model development is open and accessible to diverse stakeholders, enabling forecasters, the public, and AI/statistics experts to use, develop, and engage w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#25972;&#25968;&#35268;&#21010;&#23545;&#28201;&#39034;&#20989;&#25968;&#36827;&#34892;&#20998;&#27573;&#22810;&#39033;&#24335;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#36825;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#21253;&#21547;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#28201;&#39034;&#20989;&#25968;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#35745;&#31639;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.13544</link><description>&lt;p&gt;
&#36890;&#36807;&#25972;&#25968;&#35268;&#21010;&#23545;&#28201;&#39034;&#20989;&#25968;&#36827;&#34892;&#20998;&#27573;&#22810;&#39033;&#24335;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Piecewise polynomial regression of tame functions via integer programming. (arXiv:2311.13544v1 [math.OC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#25972;&#25968;&#35268;&#21010;&#23545;&#28201;&#39034;&#20989;&#25968;&#36827;&#34892;&#20998;&#27573;&#22810;&#39033;&#24335;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#36825;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#21253;&#21547;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#28201;&#39034;&#20989;&#25968;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#35745;&#31639;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20272;&#35745;&#23646;&#20110;&#19968;&#31867;&#29305;&#23450;&#30340;&#38750;&#20809;&#28369;&#20989;&#25968;&#30340;&#20989;&#25968;&#30340;&#20219;&#21153;&#65292;&#21363;&#25152;&#35859;&#30340;&#28201;&#39034;&#20989;&#25968;&#12290;&#36825;&#20123;&#20989;&#25968;&#20986;&#29616;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#65306;&#28145;&#24230;&#23398;&#20064;&#30340;&#35757;&#32451;&#12289;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#20215;&#20540;&#20989;&#25968;&#25110;&#23567;&#20998;&#23376;&#30340;&#27874;&#20989;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#28201;&#39034;&#20989;&#25968;&#22312;&#20219;&#20309;&#23436;&#20840;&#32500;&#24230;&#30340;&#31435;&#26041;&#20307;&#19978;&#21487;&#29992;&#20998;&#27573;&#22810;&#39033;&#24335;&#26469;&#36924;&#36817;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20998;&#27573;&#22810;&#39033;&#24335;&#22238;&#24402;&#30340;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#24418;&#24335;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#29992;&#20110;&#20272;&#35745;&#28201;&#39034;&#20989;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#35745;&#31639;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the task of estimating functions belonging to a specific class of nonsmooth functions, namely so-called tame functions. These functions appear in a wide range of applications: training deep learning, value functions of mixed-integer programs, or wave functions of small molecules. We show that tame functions are approximable by piecewise polynomials on any full-dimensional cube. We then present the first ever mixed-integer programming formulation of piecewise polynomial regression. Together, these can be used to estimate tame functions. We demonstrate promising computational results.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35752;&#35770;&#20102;&#21033;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#36827;&#34892;&#20020;&#24202;&#35777;&#25454;&#25688;&#35201;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#23588;&#20854;&#20851;&#27880;&#24320;&#21457;&#38382;&#36131;&#21046;&#12289;&#20844;&#24179;&#21644;&#21253;&#23481;&#24615;&#30340;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2311.11211</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#36827;&#34892;&#20020;&#24202;&#35777;&#25454;&#25688;&#35201;&#38656;&#35201;&#30830;&#20445;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Leveraging Generative AI for Clinical Evidence Summarization Needs to Ensure Trustworthiness. (arXiv:2311.11211v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.11211
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35752;&#35770;&#20102;&#21033;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#36827;&#34892;&#20020;&#24202;&#35777;&#25454;&#25688;&#35201;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#23588;&#20854;&#20851;&#27880;&#24320;&#21457;&#38382;&#36131;&#21046;&#12289;&#20844;&#24179;&#21644;&#21253;&#23481;&#24615;&#30340;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35777;&#25454;&#21307;&#23398;&#25215;&#35834;&#36890;&#36807;&#21033;&#29992;&#26368;&#20339;&#21487;&#29992;&#35777;&#25454;&#26469;&#22686;&#24378;&#21307;&#30103;&#20915;&#31574;&#21644;&#23454;&#36341;&#65292;&#20174;&#32780;&#25552;&#39640;&#21307;&#30103;&#36136;&#37327;&#12290;&#21307;&#23398;&#35777;&#25454;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#21487;&#20197;&#20174;&#21508;&#31181;&#26469;&#28304;&#33719;&#21462;&#65292;&#32473;&#25910;&#38598;&#12289;&#35780;&#20272;&#21644;&#32508;&#21512;&#35777;&#25454;&#20449;&#24687;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26368;&#36817;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#65292;&#20363;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26377;&#26395;&#20419;&#36827;&#36825;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#20986;&#20855;&#26377;&#38382;&#36131;&#21046;&#12289;&#20844;&#24179;&#21644;&#21253;&#23481;&#24615;&#30340;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#20010;&#35270;&#35282;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#22312;&#33258;&#21160;&#21270;&#21307;&#30103;&#35777;&#25454;&#25688;&#35201;&#26041;&#38754;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evidence-based medicine promises to improve the quality of healthcare by empowering medical decisions and practices with the best available evidence. The rapid growth of medical evidence, which can be obtained from various sources, poses a challenge in collecting, appraising, and synthesizing the evidential information. Recent advancements in generative AI, exemplified by large language models, hold promise in facilitating the arduous task. However, developing accountable, fair, and inclusive models remains a complicated undertaking. In this perspective, we discuss the trustworthiness of generative AI in the context of automated summarization of medical evidence.
&lt;/p&gt;</description></item><item><title>ViR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#65292;&#37319;&#29992;&#21452;&#37325;&#24182;&#34892;&#21644;&#36882;&#24402;&#20844;&#24335;&#65292;&#20174;&#32780;&#22312;&#24555;&#36895;&#25512;&#29702;&#21644;&#24182;&#34892;&#35757;&#32451;&#20043;&#38388;&#21462;&#24471;&#20102;&#26368;&#20339;&#24179;&#34913;&#65292;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.19731</link><description>&lt;p&gt;
ViR: &#36808;&#21521;&#39640;&#25928;&#35270;&#35273;&#20445;&#30041;&#39592;&#24178;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
ViR: Towards Efficient Vision Retention Backbones. (arXiv:2310.19731v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19731
&lt;/p&gt;
&lt;p&gt;
ViR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#65292;&#37319;&#29992;&#21452;&#37325;&#24182;&#34892;&#21644;&#36882;&#24402;&#20844;&#24335;&#65292;&#20174;&#32780;&#22312;&#24555;&#36895;&#25512;&#29702;&#21644;&#24182;&#34892;&#35757;&#32451;&#20043;&#38388;&#21462;&#24471;&#20102;&#26368;&#20339;&#24179;&#34913;&#65292;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViTs&#65289;&#22240;&#20854;&#22312;&#24314;&#27169;&#38271;&#31243;&#31354;&#38388;&#20381;&#36182;&#24615;&#21644;&#22823;&#35268;&#27169;&#35757;&#32451;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#23613;&#31649;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#35757;&#32451;&#24182;&#34892;&#24615;&#22312;&#20445;&#25345;&#20986;&#33394;&#24615;&#33021;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#20854;&#20108;&#27425;&#22797;&#26434;&#24230;&#38459;&#30861;&#20102;ViTs&#22312;&#35768;&#22810;&#38656;&#35201;&#24555;&#36895;&#25512;&#29702;&#30340;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#36825;&#31181;&#24433;&#21709;&#22312;&#38656;&#35201;&#33258;&#22238;&#24402;&#24314;&#27169;&#36755;&#20837;&#29305;&#24449;&#30340;&#24212;&#29992;&#20013;&#23588;&#20026;&#26126;&#26174;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#65292;&#19968;&#31181;&#26032;&#30340;&#21162;&#21147;&#26041;&#21521;&#25552;&#20986;&#20102;&#20855;&#26377;&#21487;&#24182;&#34892;&#21270;&#27169;&#22411;&#21644;&#36882;&#24402;&#20844;&#24335;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#29983;&#25104;&#24212;&#29992;&#20013;&#23454;&#29616;&#39640;&#25928;&#25512;&#29702;&#12290;&#21463;&#21040;&#36825;&#19968;&#36235;&#21183;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#65292;&#21517;&#20026;Vision Retention Networks&#65288;ViR&#65289;&#65292;&#20855;&#26377;&#21452;&#37325;&#24182;&#34892;&#21644;&#36882;&#24402;&#20844;&#24335;&#65292;&#21487;&#20197;&#22312;&#24555;&#36895;&#25512;&#29702;&#21644;&#24182;&#34892;&#35757;&#32451;&#26041;&#38754;&#21462;&#24471;&#26368;&#20339;&#24179;&#34913;&#65292;&#24182;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViTs) have attracted a lot of popularity in recent years, due to their exceptional capabilities in modeling long-range spatial dependencies and scalability for large scale training. Although the training parallelism of self-attention mechanism plays an important role in retaining great performance, its quadratic complexity baffles the application of ViTs in many scenarios which demand fast inference. This effect is even more pronounced in applications in which autoregressive modeling of input features is required. In Natural Language Processing (NLP), a new stream of efforts has proposed parallelizable models with recurrent formulation that allows for efficient inference in generative applications. Inspired by this trend, we propose a new class of computer vision models, dubbed Vision Retention Networks (ViR), with dual parallel and recurrent formulations, which strike an optimal balance between fast inference and parallel training with competitive performance. In 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;G-&#19977;&#37325;&#30456;&#20851;&#23618;&#65292;&#22312;G-&#31561;&#21464;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#24378;&#40065;&#26834;&#24615;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23436;&#22791;&#30340;&#19977;&#37325;&#30456;&#20851;&#29702;&#35770;&#65292;&#36825;&#20351;&#24471;G-TC&#23618;&#33021;&#22815;&#22312;&#38754;&#23545;&#19981;&#21464;&#24615;&#25915;&#20987;&#26102;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#20998;&#31867;&#20934;&#30830;&#29575;&#19978;&#30456;&#27604;&#26631;&#20934;&#30340;Max G-Pooling&#26377;&#26126;&#26174;&#30340;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2310.18564</link><description>&lt;p&gt;
&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#23454;&#29616;G-&#31561;&#21464;&#32593;&#32476;&#20013;&#30340;&#24378;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
A General Framework for Robust G-Invariance in G-Equivariant Networks. (arXiv:2310.18564v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18564
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;G-&#19977;&#37325;&#30456;&#20851;&#23618;&#65292;&#22312;G-&#31561;&#21464;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#24378;&#40065;&#26834;&#24615;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23436;&#22791;&#30340;&#19977;&#37325;&#30456;&#20851;&#29702;&#35770;&#65292;&#36825;&#20351;&#24471;G-TC&#23618;&#33021;&#22815;&#22312;&#38754;&#23545;&#19981;&#21464;&#24615;&#25915;&#20987;&#26102;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#20998;&#31867;&#20934;&#30830;&#29575;&#19978;&#30456;&#27604;&#26631;&#20934;&#30340;Max G-Pooling&#26377;&#26126;&#26174;&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;G-&#31561;&#21464;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;G-CNNs&#65289;&#20013;&#30340;&#24378;&#32452;&#19981;&#21464;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;G-&#19977;&#37325;&#30456;&#20851;&#65288;G-TC&#65289;&#23618;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#32676;&#19978;&#30340;&#19977;&#37325;&#30456;&#20851;&#29702;&#35770;&#65292;&#35813;&#29702;&#35770;&#26159;&#21807;&#19968;&#30340;&#12289;&#26368;&#20302;&#27425;&#25968;&#30340;&#22810;&#39033;&#24335;&#19981;&#21464;&#26144;&#23556;&#65292;&#21516;&#26102;&#20063;&#26159;&#23436;&#22791;&#30340;&#12290;&#35768;&#22810;&#24120;&#29992;&#30340;&#19981;&#21464;&#26144;&#23556;&#65292;&#20363;&#22914;max&#65292;&#26159;&#19981;&#23436;&#22791;&#30340;&#65306;&#23427;&#20204;&#20250;&#21516;&#26102;&#21435;&#38500;&#32676;&#21644;&#20449;&#21495;&#32467;&#26500;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#23436;&#22791;&#30340;&#19981;&#21464;&#26144;&#23556;&#21482;&#31227;&#38500;&#30001;&#20110;&#32676;&#20316;&#29992;&#24341;&#36215;&#30340;&#21464;&#21270;&#65292;&#21516;&#26102;&#20445;&#30041;&#26377;&#20851;&#20449;&#21495;&#32467;&#26500;&#30340;&#25152;&#26377;&#20449;&#24687;&#12290;&#19977;&#37325;&#30456;&#20851;&#30340;&#23436;&#22791;&#24615;&#36171;&#20104;&#20102;G-TC&#23618;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#36825;&#21487;&#20197;&#22312;&#20854;&#23545;&#19981;&#21464;&#24615;&#25915;&#20987;&#30340;&#25269;&#25239;&#20013;&#35266;&#23519;&#21040;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23427;&#30456;&#27604;&#20110;G-CNN&#26550;&#26500;&#20013;&#30340;&#26631;&#20934;Max G-Pooling&#22312;&#20998;&#31867;&#20934;&#30830;&#29575;&#19978;&#26377;&#26126;&#26174;&#30340;&#25913;&#21892;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35813;&#26041;&#27861;&#30340;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a general method for achieving robust group-invariance in group-equivariant convolutional neural networks ($G$-CNNs), which we call the $G$-triple-correlation ($G$-TC) layer. The approach leverages the theory of the triple-correlation on groups, which is the unique, lowest-degree polynomial invariant map that is also complete. Many commonly used invariant maps--such as the max--are incomplete: they remove both group and signal structure. A complete invariant, by contrast, removes only the variation due to the actions of the group, while preserving all information about the structure of the signal. The completeness of the triple correlation endows the $G$-TC layer with strong robustness, which can be observed in its resistance to invariance-based adversarial attacks. In addition, we observe that it yields measurable improvements in classification accuracy over standard Max $G$-Pooling in $G$-CNN architectures. We provide a general and efficient implementation of the method 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#27169;&#22411;-&#19981;&#21487;&#30693;&#30340;&#26694;&#26550;RLMRec&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#20256;&#32479;&#30340;&#22522;&#20110;ID&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#24182;&#35299;&#20915;&#20102;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12289;&#20165;&#20381;&#36182;&#25991;&#26412;&#30340;&#38480;&#21046;&#20197;&#21450;&#25552;&#31034;&#36755;&#20837;&#38480;&#21046;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.15950</link><description>&lt;p&gt;
&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#33616;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning with Large Language Models for Recommendation. (arXiv:2310.15950v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15950
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#27169;&#22411;-&#19981;&#21487;&#30693;&#30340;&#26694;&#26550;RLMRec&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#20256;&#32479;&#30340;&#22522;&#20110;ID&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#24182;&#35299;&#20915;&#20102;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12289;&#20165;&#20381;&#36182;&#25991;&#26412;&#30340;&#38480;&#21046;&#20197;&#21450;&#25552;&#31034;&#36755;&#20837;&#38480;&#21046;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#28145;&#24230;&#23398;&#20064;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24433;&#21709;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#25429;&#25417;&#22797;&#26434;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20851;&#31995;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#31995;&#32479;&#20005;&#37325;&#20381;&#36182;&#20110;&#22522;&#20110;ID&#30340;&#25968;&#25454;&#65292;&#21487;&#33021;&#24573;&#30053;&#20102;&#19982;&#29992;&#25143;&#21644;&#29289;&#21697;&#30456;&#20851;&#30340;&#26377;&#20215;&#20540;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#23548;&#33268;&#23398;&#21040;&#30340;&#34920;&#31034;&#19981;&#22815;&#23500;&#26377;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#38544;&#24335;&#21453;&#39304;&#25968;&#25454;&#30340;&#21033;&#29992;&#24341;&#20837;&#20102;&#28508;&#22312;&#30340;&#22122;&#22768;&#21644;&#20559;&#24046;&#65292;&#32473;&#29992;&#25143;&#20559;&#22909;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#23613;&#31649;&#23558;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;ID&#30340;&#25512;&#33616;&#31995;&#32479;&#30456;&#32467;&#21512;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#20294;&#22312;&#23454;&#38469;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#25928;&#23454;&#26045;&#36824;&#38656;&#35201;&#35299;&#20915;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12289;&#20165;&#20381;&#36182;&#25991;&#26412;&#30340;&#38480;&#21046;&#20197;&#21450;&#25552;&#31034;&#36755;&#20837;&#38480;&#21046;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#26694;&#26550;RLMRec&#65292;&#26088;&#22312;&#36890;&#36807;LLM&#24378;&#21270;&#34920;&#31034;&#26469;&#22686;&#24378;&#29616;&#26377;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems have seen significant advancements with the influence of deep learning and graph neural networks, particularly in capturing complex user-item relationships. However, these graph-based recommenders heavily depend on ID-based data, potentially disregarding valuable textual information associated with users and items, resulting in less informative learned representations. Moreover, the utilization of implicit feedback data introduces potential noise and bias, posing challenges for the effectiveness of user preference learning. While the integration of large language models (LLMs) into traditional ID-based recommenders has gained attention, challenges such as scalability issues, limitations in text-only reliance, and prompt input constraints need to be addressed for effective implementation in practical recommender systems. To address these challenges, we propose a model-agnostic framework RLMRec that aims to enhance existing recommenders with LLM-empowered representati
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;SafeDPA&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#29992;&#20110;&#21516;&#26102;&#35299;&#20915;&#31574;&#30053;&#36866;&#24212;&#21644;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;SafeDPA&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#32852;&#21512;&#23398;&#20064;&#33258;&#36866;&#24212;&#31574;&#30053;&#21644;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#23569;&#37327;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#37096;&#32626;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#65292;&#30830;&#20445;&#20102;SafeDPA&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08602</link><description>&lt;p&gt;
&#23433;&#20840;&#28145;&#24230;&#31574;&#30053;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Safe Deep Policy Adaptation. (arXiv:2310.08602v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08602
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;SafeDPA&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#29992;&#20110;&#21516;&#26102;&#35299;&#20915;&#31574;&#30053;&#36866;&#24212;&#21644;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;SafeDPA&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#32852;&#21512;&#23398;&#20064;&#33258;&#36866;&#24212;&#31574;&#30053;&#21644;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#23569;&#37327;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#37096;&#32626;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#65292;&#30830;&#20445;&#20102;SafeDPA&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#37325;&#35201;&#30446;&#26631;&#26159;&#20351;&#33258;&#20027;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#21160;&#24577;&#21644;&#19981;&#30830;&#23450;&#30340;&#29615;&#22659;&#20013;&#24555;&#36895;&#36866;&#24212;&#12290;&#32463;&#20856;&#30340;&#33258;&#36866;&#24212;&#25511;&#21046;&#21644;&#23433;&#20840;&#25511;&#21046;&#25552;&#20379;&#20102;&#31283;&#23450;&#24615;&#21644;&#23433;&#20840;&#24615;&#20445;&#35777;&#65292;&#20294;&#20165;&#38480;&#20110;&#29305;&#23450;&#30340;&#31995;&#32479;&#31867;&#21035;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#31574;&#30053;&#36866;&#24212;&#25552;&#20379;&#20102;&#36890;&#29992;&#24615;&#21644;&#27867;&#21270;&#24615;&#65292;&#20294;&#21516;&#26102;&#20063;&#24102;&#26469;&#20102;&#23433;&#20840;&#24615;&#21644;&#31283;&#20581;&#24615;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SafeDPA&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;RL&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#31574;&#30053;&#36866;&#24212;&#21644;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;SafeDPA&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#32852;&#21512;&#23398;&#20064;&#33258;&#36866;&#24212;&#31574;&#30053;&#21644;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#39044;&#27979;&#29615;&#22659;&#37197;&#32622;&#65292;&#24182;&#20351;&#29992;&#23569;&#37327;&#30495;&#23454;&#25968;&#25454;&#23545;&#21160;&#21147;&#23398;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;RL&#31574;&#30053;&#20043;&#19978;&#24341;&#20837;&#22522;&#20110;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#65288;CBF&#65289;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#65292;&#20197;&#30830;&#20445;&#22312;&#30495;&#23454;&#19990;&#30028;&#37096;&#32626;&#36807;&#31243;&#20013;&#30340;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;SafeDPA&#30340;&#29702;&#35770;&#23433;&#20840;&#24615;&#20445;&#35777;&#65292;&#24182;&#23637;&#31034;&#20102;SafeDPA&#23545;&#23398;&#20064;&#35823;&#24046;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A critical goal of autonomy and artificial intelligence is enabling autonomous robots to rapidly adapt in dynamic and uncertain environments. Classic adaptive control and safe control provide stability and safety guarantees but are limited to specific system classes. In contrast, policy adaptation based on reinforcement learning (RL) offers versatility and generalizability but presents safety and robustness challenges. We propose SafeDPA, a novel RL and control framework that simultaneously tackles the problems of policy adaptation and safe reinforcement learning. SafeDPA jointly learns adaptive policy and dynamics models in simulation, predicts environment configurations, and fine-tunes dynamics models with few-shot real-world data. A safety filter based on the Control Barrier Function (CBF) on top of the RL policy is introduced to ensure safety during real-world deployment. We provide theoretical safety guarantees of SafeDPA and show the robustness of SafeDPA against learning errors 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#21644;&#20998;&#31867;&#20102;AI-&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#21253;&#25324;&#25915;&#20987;&#38754;&#12289;&#36947;&#24503;&#21644;&#27861;&#24459;&#38382;&#39064;&#20197;&#21450;&#20154;&#26426;&#20132;&#20114;&#23433;&#20840;&#12290;&#26088;&#22312;&#20026;&#29992;&#25143;&#12289;&#24320;&#21457;&#32773;&#21644;&#20854;&#20182;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2310.08565</link><description>&lt;p&gt;
AI-&#26426;&#22120;&#20154;&#20013;&#30340;&#23433;&#20840;&#32771;&#34385;&#65306;&#24403;&#21069;&#26041;&#27861;&#12289;&#25361;&#25112;&#21644;&#26426;&#20250;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Security Considerations in AI-Robotics: A Survey of Current Methods, Challenges, and Opportunities. (arXiv:2310.08565v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#21644;&#20998;&#31867;&#20102;AI-&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#21253;&#25324;&#25915;&#20987;&#38754;&#12289;&#36947;&#24503;&#21644;&#27861;&#24459;&#38382;&#39064;&#20197;&#21450;&#20154;&#26426;&#20132;&#20114;&#23433;&#20840;&#12290;&#26088;&#22312;&#20026;&#29992;&#25143;&#12289;&#24320;&#21457;&#32773;&#21644;&#20854;&#20182;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#23427;&#20204;&#35806;&#29983;&#20197;&#26469;&#65292;&#26426;&#22120;&#20154;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#23601;&#23494;&#19981;&#21487;&#20998;&#22320;&#32852;&#31995;&#22312;&#19968;&#36215;&#12290;&#22914;&#20170;&#65292;AI-&#26426;&#22120;&#20154;&#31995;&#32479;&#24050;&#32463;&#25104;&#20026;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20174;&#26426;&#22120;&#20154;&#21560;&#23576;&#22120;&#21040;&#21322;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#12290;&#36825;&#20123;&#31995;&#32479;&#24314;&#31435;&#22312;&#19977;&#20010;&#22522;&#26412;&#30340;&#26550;&#26500;&#20803;&#32032;&#19978;&#65306;&#24863;&#30693;&#12289;&#23548;&#33322;&#19982;&#35268;&#21010;&#65292;&#20197;&#21450;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;AI-&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#25972;&#21512;&#25552;&#39640;&#20102;&#25105;&#20204;&#29983;&#27963;&#30340;&#36136;&#37327;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#19968;&#20010;&#20005;&#37325;&#30340;&#38382;&#39064; - &#36825;&#20123;&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#23433;&#20840;&#25915;&#20987;&#12290;&#26500;&#25104;AI-&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#29289;&#29702;&#32452;&#20214;&#12289;&#31639;&#27861;&#21644;&#25968;&#25454;&#21487;&#33021;&#20250;&#34987;&#24694;&#24847;&#34892;&#20026;&#32773;&#21033;&#29992;&#65292;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#12290;&#22522;&#20110;&#24212;&#23545;AI-&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#23433;&#20840;&#38382;&#39064;&#30340;&#38656;&#27714;&#65292;&#26412;&#25991;&#22312;&#25915;&#20987;&#38754;&#12289;&#36947;&#24503;&#21644;&#27861;&#24459;&#38382;&#39064;&#20197;&#21450;&#20154;&#26426;&#20132;&#20114;&#23433;&#20840;&#19977;&#20010;&#32500;&#24230;&#19978;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#35843;&#26597;&#21644;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20026;&#29992;&#25143;&#12289;&#24320;&#21457;&#32773;&#21644;&#20854;&#20182;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotics and Artificial Intelligence (AI) have been inextricably intertwined since their inception. Today, AI-Robotics systems have become an integral part of our daily lives, from robotic vacuum cleaners to semi-autonomous cars. These systems are built upon three fundamental architectural elements: perception, navigation and planning, and control. However, while the integration of AI-Robotics systems has enhanced the quality our lives, it has also presented a serious problem - these systems are vulnerable to security attacks. The physical components, algorithms, and data that make up AI-Robotics systems can be exploited by malicious actors, potentially leading to dire consequences. Motivated by the need to address the security concerns in AI-Robotics systems, this paper presents a comprehensive survey and taxonomy across three dimensions: attack surfaces, ethical and legal concerns, and Human-Robot Interaction (HRI) security. Our goal is to provide users, developers and other stakehol
&lt;/p&gt;</description></item><item><title>ECoFLaP&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31895;&#21040;&#32454;&#30340;&#36880;&#23618;&#21098;&#26525;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#21387;&#32553;&#21644;&#37096;&#32626;&#26102;&#30340;&#35745;&#31639;&#21644;&#33021;&#32791;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02998</link><description>&lt;p&gt;
ECoFLaP: &#39640;&#25928;&#30340;&#31895;&#21040;&#32454;&#30340;&#36880;&#23618;&#21098;&#26525;&#26041;&#27861;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models. (arXiv:2310.02998v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02998
&lt;/p&gt;
&lt;p&gt;
ECoFLaP&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31895;&#21040;&#32454;&#30340;&#36880;&#23618;&#21098;&#26525;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#21387;&#32553;&#21644;&#37096;&#32626;&#26102;&#30340;&#35745;&#31639;&#21644;&#33021;&#32791;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#27169;&#24577;&#30340;&#20016;&#23500;&#20449;&#24687;&#65292;&#20840;&#38754;&#29702;&#35299;&#19990;&#30028;&#65292;&#24182;&#22312;&#21508;&#31181;&#22810;&#27169;&#24577;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#35745;&#31639;/&#33021;&#32791;&#21644;&#30899;&#25490;&#25918;&#65292;&#37096;&#32626;LVLMs&#24448;&#24448;&#23384;&#22312;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#20351;&#24471;&#37319;&#29992;&#20256;&#32479;&#30340;&#36845;&#20195;&#20840;&#23616;&#21098;&#26525;&#21464;&#24471;&#19981;&#21487;&#34892;&#65292;&#22240;&#20026;&#20854;&#38656;&#35201;&#35745;&#31639;&#25972;&#20010;&#22823;&#22411;&#27169;&#22411;&#30340;Hessian&#30697;&#38453;&#20197;&#36827;&#34892;&#31232;&#30095;&#21270;&#12290;&#30456;&#21453;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#36880;&#23618;&#21098;&#26525;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#20840;&#23616;&#21098;&#26525;&#30340;&#26114;&#36149;&#35745;&#31639;&#65292;&#24182;&#26681;&#25454;&#23618;&#20869;&#26435;&#37325;&#30340;&#37325;&#35201;&#24615;&#26377;&#25928;&#21387;&#32553;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#30001;&#20110;&#32570;&#20047;&#20840;&#23616;&#35270;&#35282;&#32780;&#23548;&#33268;&#27169;&#22411;&#21387;&#32553;&#19981;&#22815;&#20248;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#22823;&#22411;&#27169;&#22411;&#26368;&#36817;&#39640;&#25928;&#21098;&#26525;&#26041;&#27861;&#30340;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#31895;&#21040;&#32454;&#30340;&#36880;&#23618;&#21098;&#26525;&#26041;&#27861;&#65288;ECoFLaP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Vision-Language Models (LVLMs) can understand the world comprehensively by integrating rich information from different modalities, achieving remarkable performance improvements on various multimodal downstream tasks. However, deploying LVLMs is often problematic due to their massive computational/energy costs and carbon consumption. Such issues make it infeasible to adopt conventional iterative global pruning, which is costly due to computing the Hessian matrix of the entire large model for sparsification. Alternatively, several studies have recently proposed layer-wise pruning approaches to avoid the expensive computation of global pruning and efficiently compress model weights according to their importance within a layer. However, these methods often suffer from suboptimal model compression due to their lack of a global perspective. To address this limitation in recent efficient pruning methods for large models, we propose Efficient Coarse-to-Fine Layer-Wise Pruning (ECoFLaP), 
&lt;/p&gt;</description></item><item><title>MagicDrive&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#34903;&#26223;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#22810;&#26679;&#21270;&#30340;&#19977;&#32500;&#20960;&#20309;&#25511;&#21046;&#65292;&#21253;&#25324;&#30456;&#26426;&#23039;&#24577;&#12289;&#36947;&#36335;&#22320;&#22270;&#21644;&#19977;&#32500;&#36793;&#30028;&#26694;&#65292;&#20197;&#21450;&#25991;&#26412;&#25551;&#36848;&#65292;&#23454;&#29616;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;&#34903;&#26223;&#21512;&#25104;&#65292;&#24182;&#25429;&#25417;&#20102;&#32454;&#33268;&#30340;&#19977;&#32500;&#20960;&#20309;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2310.02601</link><description>&lt;p&gt;
MagicDrive: &#22810;&#26679;&#21270;&#30340;&#19977;&#32500;&#20960;&#20309;&#25511;&#21046;&#19979;&#30340;&#34903;&#26223;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
MagicDrive: Street View Generation with Diverse 3D Geometry Control. (arXiv:2310.02601v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02601
&lt;/p&gt;
&lt;p&gt;
MagicDrive&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#34903;&#26223;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#22810;&#26679;&#21270;&#30340;&#19977;&#32500;&#20960;&#20309;&#25511;&#21046;&#65292;&#21253;&#25324;&#30456;&#26426;&#23039;&#24577;&#12289;&#36947;&#36335;&#22320;&#22270;&#21644;&#19977;&#32500;&#36793;&#30028;&#26694;&#65292;&#20197;&#21450;&#25991;&#26412;&#25551;&#36848;&#65292;&#23454;&#29616;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;&#34903;&#26223;&#21512;&#25104;&#65292;&#24182;&#25429;&#25417;&#20102;&#32454;&#33268;&#30340;&#19977;&#32500;&#20960;&#20309;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#26174;&#33879;&#25552;&#39640;&#20102;&#20855;&#26377;2D&#25511;&#21046;&#30340;&#25968;&#25454;&#21512;&#25104;&#12290;&#28982;&#32780;&#65292;&#22312;&#34903;&#26223;&#29983;&#25104;&#20013;&#31934;&#30830;&#30340;&#19977;&#32500;&#25511;&#21046;&#22312;&#19977;&#32500;&#24863;&#30693;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20173;&#28982;&#38590;&#20197;&#23454;&#29616;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20351;&#29992;&#40479;&#30640;&#22270;&#20316;&#20026;&#20027;&#35201;&#26465;&#20214;&#24120;&#24120;&#23548;&#33268;&#20960;&#20309;&#25511;&#21046;&#65288;&#22914;&#39640;&#24230;&#65289;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#24433;&#21709;&#29289;&#20307;&#24418;&#29366;&#12289;&#36974;&#25377;&#27169;&#24335;&#21644;&#36947;&#36335;&#34920;&#38754;&#39640;&#31243;&#31561;&#23545;&#24863;&#30693;&#25968;&#25454;&#21512;&#25104;&#33267;&#20851;&#37325;&#35201;&#30340;&#22240;&#32032;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#20219;&#21153;&#32780;&#35328;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MagicDrive&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#34903;&#26223;&#29983;&#25104;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#22810;&#26679;&#21270;&#30340;&#19977;&#32500;&#20960;&#20309;&#25511;&#21046;&#65292;&#21253;&#25324;&#30456;&#26426;&#23039;&#24577;&#12289;&#36947;&#36335;&#22320;&#22270;&#21644;&#19977;&#32500;&#36793;&#30028;&#26694;&#65292;&#20197;&#21450;&#36890;&#36807;&#23450;&#21046;&#30340;&#32534;&#30721;&#31574;&#30053;&#23454;&#29616;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#35774;&#35745;&#36824;&#37319;&#29992;&#20102;&#36328;&#35270;&#22270;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#30830;&#20445;&#22810;&#20010;&#30456;&#26426;&#35270;&#22270;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;MagicDrive&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;&#34903;&#26223;&#21512;&#25104;&#65292;&#25429;&#25417;&#21040;&#20102;&#31934;&#32454;&#30340;&#19977;&#32500;&#20960;&#20309;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in diffusion models have significantly enhanced the data synthesis with 2D control. Yet, precise 3D control in street view generation, crucial for 3D perception tasks, remains elusive. Specifically, utilizing Bird's-Eye View (BEV) as the primary condition often leads to challenges in geometry control (e.g., height), affecting the representation of object shapes, occlusion patterns, and road surface elevations, all of which are essential to perception data synthesis, especially for 3D object detection tasks. In this paper, we introduce MagicDrive, a novel street view generation framework offering diverse 3D geometry controls, including camera poses, road maps, and 3D bounding boxes, together with textual descriptions, achieved through tailored encoding strategies. Besides, our design incorporates a cross-view attention module, ensuring consistency across multiple camera views. With MagicDrive, we achieve high-fidelity street-view synthesis that captures nuanced 3D ge
&lt;/p&gt;</description></item><item><title>ChaCha&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#40723;&#21169;&#20799;&#31461;&#20998;&#20139;&#20010;&#20154;&#20107;&#20214;&#21644;&#30456;&#20851;&#24773;&#32490;&#12290;&#36890;&#36807;&#19968;&#20010;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#21457;&#29616;&#20799;&#31461;&#23558;ChaCha&#35270;&#20026;&#20146;&#23494;&#30340;&#26379;&#21451;&#65292;&#24182;&#24895;&#24847;&#19982;&#20854;&#20998;&#20139;&#21508;&#31181;&#20027;&#39064;&#30340;&#25925;&#20107;&#12290;</title><link>http://arxiv.org/abs/2309.12244</link><description>&lt;p&gt;
ChaCha&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#20799;&#31461;&#20998;&#20139;&#19982;&#20010;&#20154;&#20107;&#20214;&#30456;&#20851;&#30340;&#24773;&#32490;
&lt;/p&gt;
&lt;p&gt;
ChaCha: Leveraging Large Language Models to Prompt Children to Share Their Emotions about Personal Events. (arXiv:2309.12244v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12244
&lt;/p&gt;
&lt;p&gt;
ChaCha&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#40723;&#21169;&#20799;&#31461;&#20998;&#20139;&#20010;&#20154;&#20107;&#20214;&#21644;&#30456;&#20851;&#24773;&#32490;&#12290;&#36890;&#36807;&#19968;&#20010;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#21457;&#29616;&#20799;&#31461;&#23558;ChaCha&#35270;&#20026;&#20146;&#23494;&#30340;&#26379;&#21451;&#65292;&#24182;&#24895;&#24847;&#19982;&#20854;&#20998;&#20139;&#21508;&#31181;&#20027;&#39064;&#30340;&#25925;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20799;&#31461;&#36890;&#24120;&#36890;&#36807;&#19982;&#23478;&#20154;&#25110;&#20182;&#20154;&#20998;&#20139;&#25925;&#20107;&#21644;&#24863;&#21463;&#26469;&#23398;&#20064;&#36776;&#35782;&#21644;&#34920;&#36798;&#24773;&#32490;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#20799;&#31461;&#27491;&#22312;&#21457;&#23637;&#20182;&#20204;&#30340;&#20132;&#27969;&#25216;&#33021;&#65292;&#29238;&#27597;&#25110;&#20804;&#24351;&#22992;&#22969;&#24456;&#38590;&#19982;&#20182;&#20204;&#36827;&#34892;&#24773;&#24863;&#27807;&#36890;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ChaCha&#65292;&#19968;&#20010;&#40723;&#21169;&#21644;&#24341;&#23548;&#20799;&#31461;&#20998;&#20139;&#20010;&#20154;&#20107;&#20214;&#21644;&#30456;&#20851;&#24773;&#32490;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;ChaCha&#32467;&#21512;&#20102;&#29366;&#24577;&#26426;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22312;&#36827;&#34892;&#33258;&#30001;&#23545;&#35805;&#30340;&#21516;&#26102;&#20445;&#25345;&#23545;&#35805;&#30340;&#26041;&#21521;&#24615;&#12290;&#36890;&#36807;&#19982;20&#21517;&#24180;&#40836;&#22312;8-12&#23681;&#30340;&#20799;&#31461;&#36827;&#34892;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;ChaCha&#22914;&#20309;&#20419;&#20351;&#20799;&#31461;&#20998;&#20139;&#20010;&#20154;&#20107;&#20214;&#24182;&#24341;&#23548;&#20182;&#20204;&#25551;&#36848;&#30456;&#20851;&#24773;&#32490;&#12290;&#21442;&#19982;&#32773;&#35748;&#20026;ChaCha&#23601;&#20687;&#19968;&#20010;&#20146;&#23494;&#30340;&#26379;&#21451;&#65292;&#24182;&#20998;&#20139;&#20102;&#21508;&#31181;&#20027;&#39064;&#30340;&#25925;&#20107;&#65292;&#22914;&#23478;&#24237;&#26053;&#34892;&#21644;&#20010;&#20154;&#25104;&#23601;&#12290;&#22522;&#20110;&#23450;&#37327;&#21644;&#23450;&#24615;&#21457;&#29616;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#21033;&#29992;LLMs&#35774;&#35745;&#36866;&#21512;&#20799;&#31461;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
Children typically learn to identify and express emotions through sharing their stories and feelings with others, particularly their family. However, it is challenging for parents or siblings to have emotional communication with children since children are still developing their communication skills. We present ChaCha, a chatbot that encourages and guides children to share personal events and associated emotions. ChaCha combines a state machine and large language models (LLMs) to keep the dialogue on track while carrying on free-form conversations. Through an exploratory study with 20 children (aged 8-12), we examine how ChaCha prompts children to share personal events and guides them to describe associated emotions. Participants perceived ChaCha as a close friend and shared their stories on various topics, such as family trips and personal achievements. Based on the quantitative and qualitative findings, we discuss opportunities for leveraging LLMs to design child-friendly chatbots to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#26102;&#38388;&#28382;&#21518;&#20449;&#24687;&#29942;&#39048;&#30340;&#26041;&#27861;&#65292;&#23558;&#22797;&#26434;&#31995;&#32479;&#26144;&#23556;&#21040;&#31616;&#21270;&#34920;&#31034;&#31354;&#38388;&#24182;&#27169;&#25311;&#26102;&#38388;&#19978;&#30340;&#22823;&#36339;&#36291;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#27169;&#25311;&#21407;&#22987;&#36807;&#31243;&#30340;&#32479;&#35745;&#29305;&#24615;&#21644;&#21160;&#21147;&#23398;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26102;&#38388;&#28382;&#21518;&#38477;&#32500;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.07200</link><description>&lt;p&gt;
&#36890;&#36807;&#26102;&#38388;&#28382;&#21518;&#20449;&#24687;&#29942;&#39048;&#30340;&#28508;&#22312;&#34920;&#31034;&#21644;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Latent Representation and Simulation of Markov Processes via Time-Lagged Information Bottleneck. (arXiv:2309.07200v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#26102;&#38388;&#28382;&#21518;&#20449;&#24687;&#29942;&#39048;&#30340;&#26041;&#27861;&#65292;&#23558;&#22797;&#26434;&#31995;&#32479;&#26144;&#23556;&#21040;&#31616;&#21270;&#34920;&#31034;&#31354;&#38388;&#24182;&#27169;&#25311;&#26102;&#38388;&#19978;&#30340;&#22823;&#36339;&#36291;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#27169;&#25311;&#21407;&#22987;&#36807;&#31243;&#30340;&#32479;&#35745;&#29305;&#24615;&#21644;&#21160;&#21147;&#23398;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26102;&#38388;&#28382;&#21518;&#38477;&#32500;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#26159;&#25551;&#36848;&#21508;&#20010;&#39046;&#22495;&#20013;&#21160;&#24577;&#31995;&#32479;&#30340;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#23398;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#20934;&#30830;&#31215;&#20998;&#30340;&#30701;&#26102;&#38388;&#27493;&#38271;&#65292;&#31934;&#30830;&#27169;&#25311;&#38271;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#22823;&#35268;&#27169;&#31995;&#32479;&#35745;&#31639;&#37327;&#24456;&#22823;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23558;&#22797;&#26434;&#31995;&#32479;&#26144;&#23556;&#21040;&#31616;&#21270;&#34920;&#31034;&#31354;&#38388;&#24182;&#27169;&#25311;&#26102;&#38388;&#19978;&#30340;&#22823;&#36339;&#36291;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#30340;&#21407;&#21017;&#30446;&#26631;-&#26102;&#38388;&#28382;&#21518;&#20449;&#24687;&#29942;&#39048;&#65288;T-IB&#65289;&#65292;&#23427;&#26088;&#22312;&#25429;&#25417;&#30456;&#20851;&#30340;&#26102;&#38388;&#29305;&#24449;&#65292;&#21516;&#26102;&#20002;&#24323;&#39640;&#39057;&#20449;&#24687;&#20197;&#31616;&#21270;&#27169;&#25311;&#20219;&#21153;&#24182;&#26368;&#23567;&#21270;&#25512;&#29702;&#35823;&#24046;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;T-IB&#23398;&#20064;&#20102;&#20449;&#24687;&#26368;&#20248;&#30340;&#34920;&#31034;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#27169;&#25311;&#21407;&#22987;&#36807;&#31243;&#22312;&#36873;&#25321;&#30340;&#26102;&#38388;&#28382;&#21518;&#19979;&#30340;&#32479;&#35745;&#29305;&#24615;&#21644;&#21160;&#21147;&#23398;&#65292;&#24182;&#19988;&#20248;&#20110;&#29616;&#26377;&#30340;&#26102;&#38388;&#28382;&#21518;&#38477;&#32500;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Markov processes are widely used mathematical models for describing dynamic systems in various fields. However, accurately simulating large-scale systems at long time scales is computationally expensive due to the short time steps required for accurate integration. In this paper, we introduce an inference process that maps complex systems into a simplified representational space and models large jumps in time. To achieve this, we propose Time-lagged Information Bottleneck (T-IB), a principled objective rooted in information theory, which aims to capture relevant temporal features while discarding high-frequency information to simplify the simulation task and minimize the inference error. Our experiments demonstrate that T-IB learns information-optimal representations for accurately modeling the statistical properties and dynamics of the original process at a selected time lag, outperforming existing time-lagged dimensionality reduction methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;HC3 Plus&#65292;&#19968;&#20010;&#35821;&#20041;&#19981;&#21464;&#30340;&#20154;&#31867;ChatGPT&#23545;&#27604;&#35821;&#26009;&#24211;&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#35813;&#35821;&#26009;&#24211;&#32771;&#34385;&#20102;&#26356;&#22810;&#31867;&#22411;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#35821;&#20041;&#19981;&#21464;&#20219;&#21153;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#35821;&#20041;&#19981;&#21464;&#20219;&#21153;&#20013;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#26356;&#21152;&#22256;&#38590;&#12290;&#36890;&#36807;&#22823;&#37327;&#20219;&#21153;&#25351;&#20196;&#24494;&#35843;&#21644;Tk-instruct&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.02731</link><description>&lt;p&gt;
HC3 Plus&#65306;&#19968;&#20010;&#35821;&#20041;&#19981;&#21464;&#30340;&#20154;&#31867;ChatGPT&#23545;&#27604;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
HC3 Plus: A Semantic-Invariant Human ChatGPT Comparison Corpus. (arXiv:2309.02731v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;HC3 Plus&#65292;&#19968;&#20010;&#35821;&#20041;&#19981;&#21464;&#30340;&#20154;&#31867;ChatGPT&#23545;&#27604;&#35821;&#26009;&#24211;&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#35813;&#35821;&#26009;&#24211;&#32771;&#34385;&#20102;&#26356;&#22810;&#31867;&#22411;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#35821;&#20041;&#19981;&#21464;&#20219;&#21153;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#35821;&#20041;&#19981;&#21464;&#20219;&#21153;&#20013;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#26356;&#21152;&#22256;&#38590;&#12290;&#36890;&#36807;&#22823;&#37327;&#20219;&#21153;&#25351;&#20196;&#24494;&#35843;&#21644;Tk-instruct&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22240;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#32780;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#20154;&#20204;&#23545;&#20854;&#28508;&#22312;&#39118;&#38505;&#65292;&#23588;&#20854;&#26159;&#23545;AI&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#30340;&#26816;&#27979;&#36234;&#26469;&#36234;&#20851;&#27880;&#65292;&#36825;&#23545;&#26410;&#32463;&#35757;&#32451;&#30340;&#20154;&#31867;&#26469;&#35828;&#24448;&#24448;&#24456;&#38590;&#35782;&#21035;&#12290;&#30446;&#21069;&#29992;&#20110;&#26816;&#27979;ChatGPT&#29983;&#25104;&#25991;&#26412;&#30340;&#25968;&#25454;&#38598;&#20027;&#35201;&#38598;&#20013;&#22312;&#38382;&#31572;&#26041;&#38754;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;&#20855;&#26377;&#35821;&#20041;&#19981;&#21464;&#24615;&#30340;&#20219;&#21153;&#65292;&#22914;&#25688;&#35201;&#12289;&#32763;&#35793;&#21644;&#25913;&#20889;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35821;&#20041;&#19981;&#21464;&#20219;&#21153;&#19978;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#26356;&#21152;&#22256;&#38590;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26356;&#24191;&#27867;&#12289;&#26356;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#65292;&#32771;&#34385;&#20102;&#27604;&#20197;&#21069;&#30340;&#24037;&#20316;&#26356;&#22810;&#31867;&#22411;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#35821;&#20041;&#19981;&#21464;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#32463;&#36807;&#22823;&#37327;&#20219;&#21153;&#25351;&#20196;&#24494;&#35843;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#24615;&#33021;&#12290;&#22522;&#20110;&#20197;&#21069;&#30340;&#25104;&#21151;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25351;&#23548;&#24494;&#35843;&#20102;Tk-instruct&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT has gained significant interest due to its impressive performance, but people are increasingly concerned about its potential risks, particularly around the detection of AI-generated content (AIGC), which is often difficult for untrained humans to identify. Current datasets utilized for detecting ChatGPT-generated text primarily center around question-answering, yet they tend to disregard tasks that possess semantic-invariant properties, such as summarization, translation, and paraphrasing. Our primary studies demonstrate that detecting model-generated text on semantic-invariant tasks is more difficult. To fill this gap, we introduce a more extensive and comprehensive dataset that considers more types of tasks than previous work, including semantic-invariant tasks. In addition, the model after a large number of task instruction fine-tuning shows a strong powerful performance. Owing to its previous success, we further instruct fine-tuning Tk-instruct and built a more powerful det
&lt;/p&gt;</description></item><item><title>WeatherBench 2&#26356;&#26032;&#20102;&#20840;&#29699;&#20013;&#26399;&#22825;&#27668;&#39044;&#27979;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#21152;&#36895;&#25968;&#25454;&#39537;&#21160;&#22825;&#27668;&#24314;&#27169;&#30340;&#36827;&#23637;&#65292;&#25552;&#20379;&#20102;&#24320;&#28304;&#35780;&#20272;&#26694;&#26550;&#21644;&#26368;&#26032;&#30340;&#27169;&#22411;&#24615;&#33021;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#36824;&#35752;&#35770;&#20102;&#24403;&#21069;&#35780;&#20272;&#35774;&#32622;&#20013;&#30340;&#27880;&#24847;&#20107;&#39033;&#21644;&#26410;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.15560</link><description>&lt;p&gt;
WeatherBench 2&#65306;&#19979;&#19968;&#20195;&#25968;&#25454;&#39537;&#21160;&#20840;&#29699;&#22825;&#27668;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
WeatherBench 2: A benchmark for the next generation of data-driven global weather models. (arXiv:2308.15560v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15560
&lt;/p&gt;
&lt;p&gt;
WeatherBench 2&#26356;&#26032;&#20102;&#20840;&#29699;&#20013;&#26399;&#22825;&#27668;&#39044;&#27979;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#21152;&#36895;&#25968;&#25454;&#39537;&#21160;&#22825;&#27668;&#24314;&#27169;&#30340;&#36827;&#23637;&#65292;&#25552;&#20379;&#20102;&#24320;&#28304;&#35780;&#20272;&#26694;&#26550;&#21644;&#26368;&#26032;&#30340;&#27169;&#22411;&#24615;&#33021;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#36824;&#35752;&#35770;&#20102;&#24403;&#21069;&#35780;&#20272;&#35774;&#32622;&#20013;&#30340;&#27880;&#24847;&#20107;&#39033;&#21644;&#26410;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
WeatherBench 2&#26159;&#23545;Rasp&#31561;&#20154;&#65288;2020&#65289;&#25552;&#20986;&#30340;&#20840;&#29699;&#20013;&#26399;&#65288;1-14&#22825;&#65289;&#22825;&#27668;&#39044;&#27979;&#22522;&#20934;&#27979;&#35797;&#30340;&#26356;&#26032;&#65292;&#26088;&#22312;&#21152;&#36895;&#25968;&#25454;&#39537;&#21160;&#22825;&#27668;&#24314;&#27169;&#30340;&#36827;&#23637;&#12290;WeatherBench 2&#21253;&#25324;&#19968;&#20010;&#24320;&#28304;&#35780;&#20272;&#26694;&#26550;&#65292;&#20844;&#24320;&#21487;&#29992;&#30340;&#35757;&#32451;&#12289;&#22522;&#20934;&#25968;&#25454;&#21644;&#22522;&#32447;&#25968;&#25454;&#65292;&#20197;&#21450;&#19968;&#20010;&#25345;&#32493;&#26356;&#26032;&#30340;&#32593;&#31449;&#65292;&#20854;&#20013;&#21253;&#21547;&#26368;&#26032;&#30340;&#25351;&#26631;&#21644;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65306;https://sites.research.google/weatherbench&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#35780;&#20272;&#26694;&#26550;&#30340;&#35774;&#35745;&#21407;&#21017;&#65292;&#24182;&#25552;&#20379;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#29289;&#29702;&#21644;&#25968;&#25454;&#39537;&#21160;&#22825;&#27668;&#27169;&#22411;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#25351;&#26631;&#22522;&#20110;&#39046;&#20808;&#25805;&#20316;&#24615;&#22825;&#27668;&#20013;&#24515;&#35780;&#20272;&#22825;&#27668;&#39044;&#25253;&#30340;&#23454;&#36341;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#32452;&#20027;&#35201;&#24471;&#20998;&#65292;&#20197;&#25552;&#20379;&#27169;&#22411;&#24615;&#33021;&#30340;&#27010;&#35272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#24403;&#21069;&#35780;&#20272;&#35774;&#32622;&#20013;&#30340;&#27880;&#24847;&#20107;&#39033;&#21644;&#25968;&#25454;&#39537;&#21160;&#22825;&#27668;&#39044;&#27979;&#26410;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
WeatherBench 2 is an update to the global, medium-range (1-14 day) weather forecasting benchmark proposed by Rasp et al. (2020), designed with the aim to accelerate progress in data-driven weather modeling. WeatherBench 2 consists of an open-source evaluation framework, publicly available training, ground truth and baseline data as well as a continuously updated website with the latest metrics and state-of-the-art models: https://sites.research.google/weatherbench. This paper describes the design principles of the evaluation framework and presents results for current state-of-the-art physical and data-driven weather models. The metrics are based on established practices for evaluating weather forecasts at leading operational weather centers. We define a set of headline scores to provide an overview of model performance. In addition, we also discuss caveats in the current evaluation setup and challenges for the future of data-driven weather forecasting.
&lt;/p&gt;</description></item><item><title>FlexKBQA&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#31243;&#24207;&#32763;&#35793;&#22120;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#31639;&#27861;&#20174;&#30693;&#35782;&#24211;&#20013;&#25277;&#26679;&#22810;&#26679;&#30340;&#31243;&#24207;&#65292;&#23558;&#20854;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#65292;&#20174;&#32780;&#35299;&#20915;&#23569;&#26679;&#26412;&#30693;&#35782;&#24211;&#38382;&#31572;&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.12060</link><description>&lt;p&gt;
FlexKBQA&#65306;&#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#30693;&#35782;&#24211;&#38382;&#31572;&#30340;&#28789;&#27963;LLM&#39537;&#21160;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FlexKBQA: A Flexible LLM-Powered Framework for Few-Shot Knowledge Base Question Answering. (arXiv:2308.12060v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12060
&lt;/p&gt;
&lt;p&gt;
FlexKBQA&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#31243;&#24207;&#32763;&#35793;&#22120;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#31639;&#27861;&#20174;&#30693;&#35782;&#24211;&#20013;&#25277;&#26679;&#22810;&#26679;&#30340;&#31243;&#24207;&#65292;&#23558;&#20854;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#65292;&#20174;&#32780;&#35299;&#20915;&#23569;&#26679;&#26412;&#30693;&#35782;&#24211;&#38382;&#31572;&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#26159;&#19968;&#20010;&#20851;&#38190;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#30693;&#35782;&#24211;&#20013;&#30340;&#23454;&#20307;&#25968;&#37327;&#24222;&#22823;&#65292;&#24182;&#19988;&#29992;&#25143;&#25552;&#20986;&#30340;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#22810;&#26679;&#21270;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;KBQA&#27169;&#22411;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#22240;&#20026;&#39640;&#36136;&#37327;&#30340;&#27880;&#37322;&#25968;&#25454;&#19981;&#36275;&#12290;&#20026;&#20102;&#20943;&#36731;&#25163;&#21160;&#27880;&#37322;&#30340;&#36127;&#25285;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#31243;&#24207;&#32763;&#35793;&#22120;&#65292;&#20171;&#32461;&#20102;FlexKBQA&#26469;&#35299;&#20915;&#23569;&#26679;&#26412;KBQA&#20219;&#21153;&#20013;&#22266;&#26377;&#30340;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FlexKBQA&#21033;&#29992;&#33258;&#21160;&#21270;&#31639;&#27861;&#20174;&#30693;&#35782;&#24211;&#20013;&#25277;&#26679;&#22810;&#26679;&#30340;&#31243;&#24207;&#65288;&#22914;SPARQL&#26597;&#35810;&#65289;&#65292;&#28982;&#21518;&#36890;&#36807;LLMs&#23558;&#20854;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12290;&#36825;&#20010;&#21512;&#25104;&#30340;&#25968;&#25454;&#38598;&#26377;&#21161;&#20110;&#35757;&#32451;&#19968;&#20010;&#19987;&#38376;&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;&#26469;&#22788;&#29702;&#30693;&#35782;&#24211;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20943;&#23569;&#21512;&#25104;&#25968;&#25454;&#19982;&#30495;&#23454;&#29992;&#25143;&#38382;&#39064;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#30340;&#38556;&#30861;&#65292;FlexKBQA&#24341;&#20837;&#20102;&#19968;&#20010;&#25191;&#34892;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge base question answering (KBQA) is a critical yet challenging task due to the vast number of entities within knowledge bases and the diversity of natural language questions posed by users. Unfortunately, the performance of most KBQA models tends to decline significantly in real-world scenarios where high-quality annotated data is insufficient. To mitigate the burden associated with manual annotation, we introduce FlexKBQA by utilizing Large Language Models (LLMs) as program translators for addressing the challenges inherent in the few-shot KBQA task. Specifically, FlexKBQA leverages automated algorithms to sample diverse programs, such as SPARQL queries, from the knowledge base, which are subsequently converted into natural language questions via LLMs. This synthetic dataset facilitates training a specialized lightweight model for the KB. Additionally, to reduce the barriers of distribution shift between synthetic data and real user questions, FlexKBQA introduces an executiong
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#24310;&#32493;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21017;&#21270;&#36335;&#24452;&#65292;&#20197;&#35299;&#20915;DNNs&#20013;&#31232;&#30095;&#24615;&#21644;&#25968;&#20540;&#25928;&#29575;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;</title><link>http://arxiv.org/abs/2308.12044</link><description>&lt;p&gt;
&#29992;&#20110;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#36335;&#24452;&#30340;&#22810;&#30446;&#26631;&#24310;&#32493;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A multiobjective continuation method to compute the regularization path of deep neural networks. (arXiv:2308.12044v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#24310;&#32493;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21017;&#21270;&#36335;&#24452;&#65292;&#20197;&#35299;&#20915;DNNs&#20013;&#31232;&#30095;&#24615;&#21644;&#25968;&#20540;&#25928;&#29575;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#24615;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#20013;&#38750;&#24120;&#29702;&#24819;&#30340;&#29305;&#24449;&#65292;&#22240;&#20026;&#23427;&#30830;&#20445;&#20102;&#25968;&#20540;&#25928;&#29575;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;(&#30001;&#20110;&#30456;&#20851;&#29305;&#24449;&#30340;&#25968;&#37327;&#36739;&#23569;)&#21644;&#40065;&#26834;&#24615;&#12290;&#22312;&#22522;&#20110;&#32447;&#24615;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#20247;&#25152;&#21608;&#30693;&#22312;$\ell^1$&#33539;&#25968;(&#21363;&#38646;&#26435;&#37325;)&#30340;&#26368;&#31232;&#30095;&#35299;&#21644;&#38750;&#27491;&#21017;&#21270;&#35299;&#20043;&#38388;&#23384;&#22312;&#19968;&#26465;&#36830;&#25509;&#36335;&#24452;&#65292;&#36825;&#26465;&#36335;&#24452;&#34987;&#31216;&#20026;&#27491;&#21017;&#21270;&#36335;&#24452;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#23558;&#32463;&#39564;&#25439;&#22833;&#21644;&#31232;&#30095;&#24615;($\ell^1$&#33539;&#25968;)&#20316;&#20026;&#20004;&#20010;&#20914;&#31361;&#30340;&#26631;&#20934;&#65292;&#24182;&#35299;&#20915;&#30001;&#27492;&#20135;&#29983;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#39318;&#27425;&#23581;&#35797;&#23558;&#27491;&#21017;&#21270;&#36335;&#24452;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;DNNs&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;$\ell^1$&#33539;&#25968;&#30340;&#19981;&#20809;&#28369;&#24615;&#21644;&#21442;&#25968;&#25968;&#37327;&#30340;&#39640;&#24230;&#65292;&#20174;&#35745;&#31639;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#26041;&#27861;&#24182;&#19981;&#26159;&#24456;&#26377;&#25928;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#36817;&#20284;&#35745;&#31639;&#25972;&#20010;&#24085;&#32047;&#25176;&#26354;&#32447;
&lt;/p&gt;
&lt;p&gt;
Sparsity is a highly desired feature in deep neural networks (DNNs) since it ensures numerical efficiency, improves the interpretability of models (due to the smaller number of relevant features), and robustness. In machine learning approaches based on linear models, it is well known that there exists a connecting path between the sparsest solution in terms of the $\ell^1$ norm (i.e., zero weights) and the non-regularized solution, which is called the regularization path. Very recently, there was a first attempt to extend the concept of regularization paths to DNNs by means of treating the empirical loss and sparsity ($\ell^1$ norm) as two conflicting criteria and solving the resulting multiobjective optimization problem. However, due to the non-smoothness of the $\ell^1$ norm and the high number of parameters, this approach is not very efficient from a computational perspective. To overcome this limitation, we present an algorithm that allows for the approximation of the entire Pareto
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#22635;&#34917;&#20102;&#26102;&#38388;KG&#21644;&#36229;&#20851;&#31995;KG&#25512;&#29702;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#30340;&#22522;&#20934;&#36229;&#20851;&#31995;TKG&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2307.10219</link><description>&lt;p&gt;
&#22312;&#22686;&#24378;&#30340;&#19981;&#21464;&#20851;&#31995;&#30693;&#35782;&#19978;&#25506;&#32034;&#36229;&#20851;&#31995;&#26102;&#38388;&#30693;&#35782;&#22270;&#30340;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Exploring Link Prediction over Hyper-Relational Temporal Knowledge Graphs Enhanced with Time-Invariant Relational Knowledge. (arXiv:2307.10219v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10219
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#22635;&#34917;&#20102;&#26102;&#38388;KG&#21644;&#36229;&#20851;&#31995;KG&#25512;&#29702;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#30340;&#22522;&#20934;&#36229;&#20851;&#31995;TKG&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;(HKGs)&#26159;&#20256;&#32479;&#30693;&#35782;&#22270;(KGs)&#30340;&#24310;&#20280;&#65292;&#20026;&#27599;&#20010;KG&#20107;&#23454;&#25552;&#20379;&#39069;&#22806;&#30340;&#38190;&#20540;&#23545;(&#21363;&#38480;&#23450;&#35789;)&#65292;&#20197;&#26356;&#22909;&#22320;&#38480;&#21046;&#20107;&#23454;&#30340;&#26377;&#25928;&#24615;&#12290;&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#22312;HKGs&#19978;&#36827;&#34892;&#22270;&#25512;&#29702;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#30001;&#20110;&#19990;&#30028;&#30693;&#35782;&#30340;&#19981;&#26029;&#28436;&#21464;&#65292;&#22823;&#37327;&#24179;&#34892;&#24037;&#20316;&#38598;&#20013;&#22312;&#23545;&#26102;&#38388;KGs(TKGs)&#36827;&#34892;&#25512;&#29702;&#65292;&#20854;&#20013;&#27599;&#20010;TKG&#20107;&#23454;&#21487;&#20197;&#34987;&#35270;&#20026;&#24102;&#26377;&#26102;&#38388;&#25139;(&#25110;&#26102;&#38388;&#27573;)&#30340;KG&#20107;&#23454;&#65292;&#25351;&#23450;&#20854;&#26102;&#38388;&#26377;&#25928;&#24615;&#12290;&#29616;&#26377;&#30340;HKG&#25512;&#29702;&#26041;&#27861;&#19981;&#32771;&#34385;&#26102;&#38388;&#20449;&#24687;&#65292;&#22240;&#20026;&#22312;&#20043;&#21069;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#27809;&#26377;&#26174;&#24335;&#22320;&#25351;&#23450;&#12290;&#27492;&#22806;&#65292;&#25152;&#26377;&#20197;&#21069;&#30340;TKG&#25512;&#29702;&#26041;&#27861;&#21482;&#37325;&#35270;&#26102;&#38388;&#25512;&#29702;&#65292;&#24182;&#27809;&#26377;&#21150;&#27861;&#20174;&#38480;&#23450;&#35789;&#20013;&#23398;&#20064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22635;&#34917;TKG&#25512;&#29702;&#21644;HKG&#25512;&#29702;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#30340;&#22522;&#20934;&#36229;&#20851;&#31995;TKG(HTKG)&#25968;&#25454;&#38598;&#65292;&#21363;Wiki-hy&#21644;...
&lt;/p&gt;
&lt;p&gt;
Stemming from traditional knowledge graphs (KGs), hyper-relational KGs (HKGs) provide additional key-value pairs (i.e., qualifiers) for each KG fact that help to better restrict the fact validity. In recent years, there has been an increasing interest in studying graph reasoning over HKGs. In the meantime, due to the ever-evolving nature of world knowledge, extensive parallel works have been focusing on reasoning over temporal KGs (TKGs), where each TKG fact can be viewed as a KG fact coupled with a timestamp (or time period) specifying its time validity. The existing HKG reasoning approaches do not consider temporal information because it is not explicitly specified in previous benchmark datasets. Besides, all the previous TKG reasoning methods only lay emphasis on temporal reasoning and have no way to learn from qualifiers. To this end, we aim to fill the gap between TKG reasoning and HKG reasoning. We develop two new benchmark hyper-relational TKG (HTKG) datasets, i.e., Wiki-hy and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28176;&#36827;&#20613;&#37324;&#21494;&#31070;&#32463;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#35757;&#32451;&#20250;&#35805;&#20013;&#25214;&#21040;&#33258;&#36866;&#24212;&#19988;&#32039;&#20945;&#30340;&#20613;&#37324;&#21494;&#31354;&#38388;&#23376;&#27169;&#22359;&#26469;&#32534;&#30721;&#39034;&#24207;&#35270;&#39057;&#25968;&#25454;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#26041;&#27861;&#22312;&#22810;&#20010;&#22797;&#26434;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.11305</link><description>&lt;p&gt;
&#28176;&#36827;&#20613;&#37324;&#21494;&#31070;&#32463;&#34920;&#31034;&#29992;&#20110;&#39034;&#24207;&#35270;&#39057;&#32534;&#35793;
&lt;/p&gt;
&lt;p&gt;
Progressive Fourier Neural Representation for Sequential Video Compilation. (arXiv:2306.11305v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11305
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28176;&#36827;&#20613;&#37324;&#21494;&#31070;&#32463;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#35757;&#32451;&#20250;&#35805;&#20013;&#25214;&#21040;&#33258;&#36866;&#24212;&#19988;&#32039;&#20945;&#30340;&#20613;&#37324;&#21494;&#31354;&#38388;&#23376;&#27169;&#22359;&#26469;&#32534;&#30721;&#39034;&#24207;&#35270;&#39057;&#25968;&#25454;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#26041;&#27861;&#22312;&#22810;&#20010;&#22797;&#26434;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;(NIR)&#22240;&#20854;&#23558;&#22797;&#26434;&#21644;&#39640;&#32500;&#25968;&#25454;&#32534;&#30721;&#20026;&#34920;&#31034;&#31354;&#38388;&#24182;&#36890;&#36807;&#21487;&#35757;&#32451;&#30340;&#26144;&#23556;&#20989;&#25968;&#36731;&#26494;&#37325;&#26500;&#25968;&#25454;&#30340;&#38750;&#20961;&#33021;&#21147;&#32780;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;NIR&#26041;&#27861;&#20551;&#23450;&#30446;&#26631;&#25968;&#25454;&#21644;&#34920;&#31034;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#19968;&#23545;&#19968;&#30340;&#26144;&#23556;&#65292;&#32780;&#19981;&#32771;&#34385;&#25968;&#25454;&#30340;&#30456;&#20851;&#24615;&#25110;&#30456;&#20284;&#24615;&#12290;&#36825;&#23548;&#33268;&#22312;&#22810;&#32452;&#22797;&#26434;&#25968;&#25454;&#19978;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#65292;&#24182;&#38480;&#21046;&#20102;&#20854;&#25928;&#29575;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;&#21463;&#25345;&#32493;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#39034;&#24207;&#32534;&#30721;&#20250;&#35805;&#20013;&#32047;&#31215;&#21644;&#20256;&#36882;&#22810;&#20010;&#22797;&#26434;&#35270;&#39057;&#25968;&#25454;&#30340;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#12290;&#20026;&#20102;&#20811;&#26381;NIR&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#28176;&#36827;&#20613;&#37324;&#21494;&#31070;&#32463;&#34920;&#31034;(PFNR)&#65292;&#26088;&#22312;&#25214;&#21040;&#19968;&#20010;&#33258;&#36866;&#24212;&#21644;&#32039;&#20945;&#30340;&#20613;&#37324;&#21494;&#31354;&#38388;&#23376;&#27169;&#22359;&#65292;&#20197;&#32534;&#30721;&#27599;&#20010;&#35757;&#32451;&#20250;&#35805;&#20013;&#30340;&#35270;&#39057;&#12290;&#36825;&#31181;&#31232;&#30095;&#30340;&#31070;&#32463;&#32534;&#30721;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#25345;&#26377;&#33258;&#30001;&#26435;&#37325;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#21487;&#36845;&#20195;&#22320;&#32534;&#30721;&#21644;&#35299;&#30721;&#22810;&#20010;&#39034;&#24207;&#35270;&#39057;&#25968;&#25454;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Implicit Representation (NIR) has recently gained significant attention due to its remarkable ability to encode complex and high-dimensional data into representation space and easily reconstruct it through a trainable mapping function. However, NIR methods assume a one-to-one mapping between the target data and representation models regardless of data relevancy or similarity. This results in poor generalization over multiple complex data and limits their efficiency and scalability. Motivated by continual learning, this work investigates how to accumulate and transfer neural implicit representations for multiple complex video data over sequential encoding sessions. To overcome the limitation of NIR, we propose a novel method, Progressive Fourier Neural Representation (PFNR), that aims to find an adaptive and compact sub-module in Fourier space to encode videos in each training session. This sparsified neural encoding allows the neural network to hold free weights, enabling an imp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#22810;&#39046;&#22495;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;FedWon&#65292;&#36890;&#36807;&#28040;&#38500;&#35268;&#33539;&#21270;&#27493;&#39588;&#26469;&#26377;&#25928;&#22320;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2306.05879</link><description>&lt;p&gt;
&#22810;&#39046;&#22495;&#32852;&#37030;&#23398;&#20064;&#26159;&#21542;&#31163;&#19981;&#24320;&#26631;&#20934;&#21270;?
&lt;/p&gt;
&lt;p&gt;
Is Normalization Indispensable for Multi-domain Federated Learning?. (arXiv:2306.05879v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#22810;&#39046;&#22495;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;FedWon&#65292;&#36890;&#36807;&#28040;&#38500;&#35268;&#33539;&#21270;&#27493;&#39588;&#26469;&#26377;&#25928;&#22320;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#20998;&#25955;&#22312;&#23458;&#25143;&#31471;&#19978;&#30340;&#21327;&#20316;&#24335;&#20869;&#37096;&#35757;&#32451;&#22686;&#24378;&#20102;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;&#23398;&#20064;&#38754;&#20020;&#35832;&#22810;&#25361;&#25112;&#65292;&#20854;&#20013;&#21253;&#25324;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#65288;non-i.i.d&#65289;&#23548;&#33268;&#30340;&#28508;&#22312;&#24615;&#33021;&#19979;&#38477;&#21644;&#25910;&#25947;&#21463;&#38459;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35299;&#20915;&#20102;&#19968;&#20010;&#20851;&#38190;&#20294;&#24120;&#24120;&#34987;&#24573;&#35270;&#30340;&#38382;&#39064;&#8212;&#8212;&#22810;&#39046;&#22495;&#32852;&#37030;&#23398;&#20064;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23458;&#25143;&#31471;&#25968;&#25454;&#26469;&#28304;&#20110;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#20998;&#24067;&#30340;&#21508;&#31181;&#39046;&#22495;&#65292;&#32780;&#19981;&#26159;&#26631;&#31614;&#20998;&#24067;&#12290;&#20026;&#20102;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#22810;&#39046;&#22495;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#31216;&#20026;&#19981;&#20351;&#29992;&#35268;&#33539;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FedWon&#65289;&#12290;FedWon&#20174;&#19968;&#20010;&#35266;&#23519;&#20986;&#21457;&#65292;&#21363;&#25209;&#37327;&#24402;&#19968;&#21270;&#65288;BN&#65289;&#22312;&#26377;&#25928;&#22320;&#24314;&#27169;&#22810;&#20010;&#39046;&#22495;&#30340;&#32479;&#35745;&#20449;&#24687;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#32780;&#26367;&#20195;&#35268;&#33539;&#21270;&#25216;&#26415;&#20855;&#26377;&#33258;&#36523;&#30340;&#23616;&#38480;&#24615;&#12290;FedWon&#36890;&#36807;&#28040;&#38500;&#35268;&#33539;&#21270;&#27493;&#39588;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enhances data privacy with collaborative in-situ training on decentralized clients. Nevertheless, FL encounters challenges due to non-independent and identically distributed (non-i.i.d) data, leading to potential performance degradation and hindered convergence. While prior studies predominantly addressed the issue of skewed label distribution, our research addresses a crucial yet frequently overlooked problem known as multi-domain FL. In this scenario, clients' data originate from diverse domains with distinct feature distributions, as opposed to label distributions. To address the multi-domain problem in FL, we propose a novel method called Federated learning Without normalizations (FedWon). FedWon draws inspiration from the observation that batch normalization (BN) faces challenges in effectively modeling the statistics of multiple domains, while alternative normalization techniques possess their own limitations. In order to address these issues, FedWon elimi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31163;&#32447;&#20248;&#20808;&#32463;&#39564;&#37325;&#25918;&#65288;OPER&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#31867;&#20248;&#20808;&#32423;&#20989;&#25968;&#26469;&#23545;&#39640;&#22238;&#25253;&#30340;&#36716;&#25442;&#36827;&#34892;&#20248;&#20808;&#22788;&#29702;&#65292;&#20174;&#32780;&#25913;&#21892;&#34892;&#20026;&#31574;&#30053;&#65292;&#24182;&#22312;&#27492;&#25913;&#36827;&#30340;&#31574;&#30053;&#32422;&#26463;&#19979;&#20248;&#21270;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23545;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;OPER&#26041;&#27861;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.05412</link><description>&lt;p&gt;
&#31163;&#32447;&#20248;&#20808;&#32463;&#39564;&#37325;&#25918;
&lt;/p&gt;
&lt;p&gt;
Offline Prioritized Experience Replay. (arXiv:2306.05412v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31163;&#32447;&#20248;&#20808;&#32463;&#39564;&#37325;&#25918;&#65288;OPER&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#31867;&#20248;&#20808;&#32423;&#20989;&#25968;&#26469;&#23545;&#39640;&#22238;&#25253;&#30340;&#36716;&#25442;&#36827;&#34892;&#20248;&#20808;&#22788;&#29702;&#65292;&#20174;&#32780;&#25913;&#21892;&#34892;&#20026;&#31574;&#30053;&#65292;&#24182;&#22312;&#27492;&#25913;&#36827;&#30340;&#31574;&#30053;&#32422;&#26463;&#19979;&#20248;&#21270;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23545;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;OPER&#26041;&#27861;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30528;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#35774;&#35745;&#23398;&#20064;&#31574;&#30053;&#21644;&#34892;&#20026;&#31574;&#30053;&#20043;&#38388;&#30340;&#22797;&#26434;&#32422;&#26463;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32422;&#26463;&#36890;&#36807;&#22343;&#21248;&#37319;&#26679;&#31561;&#26041;&#24335;&#34987;&#24212;&#29992;&#21040;&#34920;&#29616;&#33391;&#22909;&#21644;&#34920;&#29616;&#24046;&#30340;&#34892;&#21160;&#19978;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#23398;&#20064;&#31574;&#30053;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31163;&#32447;&#20248;&#20808;&#32463;&#39564;&#37325;&#25918;&#65288;OPER&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31867;&#20248;&#20808;&#32423;&#20989;&#25968;&#65292;&#29992;&#20110;&#23558;&#39640;&#22238;&#25253;&#30340;&#36716;&#25442;&#32622;&#20110;&#26356;&#39057;&#32321;&#30340;&#35775;&#38382;&#20013;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31867;&#20248;&#20808;&#32423;&#20989;&#25968;&#33021;&#22815;&#24341;&#36215;&#34892;&#20026;&#31574;&#30053;&#30340;&#25913;&#21892;&#65292;&#24403;&#31574;&#30053;&#32422;&#26463;&#21040;&#36825;&#20010;&#25913;&#36827;&#30340;&#31574;&#30053;&#19978;&#26102;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24456;&#21487;&#33021;&#24471;&#21040;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#23454;&#29992;&#31574;&#30053;&#26469;&#33719;&#24471;&#22522;&#20110;&#25311;&#21512;&#20540;&#32593;&#32476;&#30340;&#20248;&#20808;&#26435;&#37325;&#65288;OPER-A&#65289;&#25110;&#32773;u
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) is challenged by the distributional shift problem. To address this problem, existing works mainly focus on designing sophisticated policy constraints between the learned policy and the behavior policy. However, these constraints are applied equally to well-performing and inferior actions through uniform sampling, which might negatively affect the learned policy. To alleviate this issue, we propose Offline Prioritized Experience Replay (OPER), featuring a class of priority functions designed to prioritize highly-rewarding transitions, making them more frequently visited during training. Through theoretical analysis, we show that this class of priority functions induce an improved behavior policy, and when constrained to this improved policy, a policy-constrained offline RL algorithm is likely to yield a better solution. We develop two practical strategies to obtain priority weights by estimating advantages based on a fitted value network (OPER-A) or u
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#22343;&#22330;&#21338;&#24328;&#20013;&#24341;&#20837;&#32593;&#32476;&#36890;&#20449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#39640;&#20998;&#24067;&#24335;&#26234;&#33021;&#20307;&#23398;&#20064;&#25928;&#29575;&#30340;&#26041;&#26696;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#38469;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.02766</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#26234;&#33021;&#20307;&#22312;&#22343;&#22330;&#21338;&#24328;&#20013;&#30340;&#32593;&#32476;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Networked Communication for Decentralised Agents in Mean-Field Games. (arXiv:2306.02766v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#22343;&#22330;&#21338;&#24328;&#20013;&#24341;&#20837;&#32593;&#32476;&#36890;&#20449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#39640;&#20998;&#24067;&#24335;&#26234;&#33021;&#20307;&#23398;&#20064;&#25928;&#29575;&#30340;&#26041;&#26696;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#38469;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#32593;&#32476;&#36890;&#20449;&#24341;&#20837;&#22343;&#22330;&#21338;&#24328;&#26694;&#26550;&#65292;&#29305;&#21035;&#26159;&#22312;&#26080;oracle&#30340;&#24773;&#20917;&#19979;&#65292;N&#20010;&#20998;&#24067;&#24335;&#26234;&#33021;&#20307;&#27839;&#30528;&#32463;&#36807;&#30340;&#32463;&#39564;&#31995;&#32479;&#30340;&#21333;&#19968;&#38750;&#21608;&#26399;&#28436;&#21270;&#36335;&#24452;&#23398;&#20064;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26550;&#26500;&#22312;&#21482;&#26377;&#19968;&#20123;&#20851;&#20110;&#32593;&#32476;&#32467;&#26500;&#30340;&#21512;&#29702;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#26679;&#26412;&#20445;&#35777;&#65292;&#22312;&#38598;&#20013;&#23398;&#20064;&#21644;&#29420;&#31435;&#23398;&#20064;&#24773;&#20917;&#20043;&#38388;&#26377;&#30028;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#19977;&#20010;&#29702;&#35770;&#31639;&#27861;&#30340;&#26679;&#26412;&#20445;&#35777;&#23454;&#38469;&#19978;&#24182;&#19981;&#20250;&#23548;&#33268;&#23454;&#38469;&#25910;&#25947;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#65292;&#24403;&#29702;&#35770;&#21442;&#25968;&#26410;&#34987;&#35266;&#23519;&#21040;&#65288;&#23548;&#33268;Q&#20989;&#25968;&#30340;&#20272;&#35745;&#19981;&#20934;&#30830;&#65289;&#26102;&#65292;&#25105;&#20204;&#30340;&#36890;&#20449;&#26041;&#26696;&#26174;&#33879;&#21152;&#36895;&#20102;&#25910;&#25947;&#36895;&#24230;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#20110;&#19968;&#20010;&#19981;&#21487;&#21462;&#30340;&#38598;&#20013;&#24335;&#25511;&#21046;&#22120;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#23545;&#19977;&#20010;&#29702;&#35770;&#31639;&#27861;&#36827;&#34892;&#20102;&#20960;&#31181;&#23454;&#38469;&#30340;&#25913;&#36827;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23637;&#31034;&#23427;&#20204;&#30340;&#31532;&#19968;&#20010;&#23454;&#35777;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce networked communication to the mean-field game framework, in particular to oracle-free settings where $N$ decentralised agents learn along a single, non-episodic evolution path of the empirical system. We prove that our architecture, with only a few reasonable assumptions about network structure, has sample guarantees bounded between those of the centralised- and independent-learning cases. We discuss how the sample guarantees of the three theoretical algorithms do not actually result in practical convergence. Accordingly, we show that in practical settings where the theoretical parameters are not observed (leading to poor estimation of the Q-function), our communication scheme significantly accelerates convergence over the independent case, without relying on the undesirable assumption of a centralised controller. We contribute several further practical enhancements to all three theoretical algorithms, allowing us to showcase their first empirical demonstrations. Our expe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24494;&#22411;&#36229;&#22768;&#22270;&#20687;&#21069;&#21015;&#33146;&#20998;&#21106;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#23610;&#24230;&#27880;&#37322;&#24341;&#23548;&#30340;Transformer UNet&#27169;&#22411;&#21644;&#27880;&#37322;&#24341;&#23548;&#30340;&#20108;&#20998;&#31867;&#20132;&#21449;&#29109;&#25439;&#22833;&#35299;&#20915;&#20302;&#20998;&#36776;&#29575;&#21644;&#30028;&#38480;&#19981;&#28165;&#30340;&#25361;&#25112;&#65292;&#35813;&#26041;&#27861;&#26356;&#21152;&#20851;&#27880;&#38590;&#20197;&#20998;&#21106;&#30340;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.19956</link><description>&lt;p&gt;
MicroSegNet&#65306;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24494;&#22411;&#36229;&#22768;&#22270;&#20687;&#21069;&#21015;&#33146;&#20998;&#21106;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MicroSegNet: A Deep Learning Approach for Prostate Segmentation on Micro-Ultrasound Images. (arXiv:2305.19956v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24494;&#22411;&#36229;&#22768;&#22270;&#20687;&#21069;&#21015;&#33146;&#20998;&#21106;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#23610;&#24230;&#27880;&#37322;&#24341;&#23548;&#30340;Transformer UNet&#27169;&#22411;&#21644;&#27880;&#37322;&#24341;&#23548;&#30340;&#20108;&#20998;&#31867;&#20132;&#21449;&#29109;&#25439;&#22833;&#35299;&#20915;&#20302;&#20998;&#36776;&#29575;&#21644;&#30028;&#38480;&#19981;&#28165;&#30340;&#25361;&#25112;&#65292;&#35813;&#26041;&#27861;&#26356;&#21152;&#20851;&#27880;&#38590;&#20197;&#20998;&#21106;&#30340;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#22411;&#36229;&#22768;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;29MHz&#36229;&#22768;&#25216;&#26415;&#65292;&#25552;&#20379;&#27604;&#20256;&#32479;&#36229;&#22768;&#39640;3-4&#20493;&#30340;&#20998;&#36776;&#29575;&#65292;&#22312;&#35786;&#26029;&#21069;&#21015;&#33146;&#30284;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#19982;MRI&#30456;&#24403;&#65292;&#20294;&#25104;&#26412;&#26356;&#20302;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20302;&#20998;&#36776;&#29575;&#21644;&#21069;&#21015;&#33146;&#12289;&#33152;&#33009;&#21644;&#23615;&#36947;&#20013;&#32447;&#20043;&#38388;&#30340;&#30028;&#38480;&#19981;&#28165;&#65292;&#22522;&#20110;&#24494;&#22411;&#36229;&#22768;&#30340;&#21069;&#21015;&#33146;&#20998;&#21106;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MicroSegNet&#65292;&#36825;&#26159;&#19968;&#20010;&#29305;&#21035;&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#22810;&#23610;&#24230;&#27880;&#37322;&#24341;&#23548;&#30340;Transformer UNet&#27169;&#22411;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;MicroSegNet&#26356;&#21152;&#20851;&#27880;&#38590;&#20197;&#20998;&#21106;&#65288;&#38590;&#21306;&#22495;&#65289;&#30340;&#21306;&#22495;&#65292;&#36825;&#20123;&#21306;&#22495;&#20855;&#26377;&#19987;&#23478;&#21644;&#38750;&#19987;&#23478;&#27880;&#37322;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27880;&#37322;&#24341;&#23548;&#30340;&#20108;&#20998;&#31867;&#20132;&#21449;&#29109;&#65288;AG-BCE&#65289;&#25439;&#22833;&#65292;&#23427;&#22312;&#38590;&#21306;&#22495;&#20013;&#32473;&#39044;&#27979;&#35823;&#24046;&#20998;&#37197;&#26356;&#22823;&#30340;&#26435;&#37325;&#21644;&#36739;&#20302;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
Micro-ultrasound (micro-US) is a novel 29-MHz ultrasound technique that provides 3-4 times higher resolution than traditional ultrasound, delivering comparable accuracy for diagnosing prostate cancer to MRI but at a lower cost. Accurate prostate segmentation is crucial for prostate volume measurement, cancer diagnosis, prostate biopsy, and treatment planning. However, prostate segmentation on microUS is challenging due to artifacts and indistinct borders between the prostate, bladder, and urethra in the midline. This paper presents MicroSegNet, a multi-scale annotation-guided transformer UNet model designed specifically to tackle these challenges. During the training process, MicroSegNet focuses more on regions that are hard to segment (hard regions), characterized by discrepancies between expert and non-expert annotations. We achieve this by proposing an annotation-guided binary cross entropy (AG-BCE) loss that assigns a larger weight to prediction errors in hard regions and a lower w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26234;&#33021;&#23478;&#23621;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#29992;&#25143;&#21629;&#20196;&#30340;&#21019;&#24847;&#30446;&#26631;&#23548;&#21521;&#25512;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21019;&#36896;&#24615;&#22320;&#25512;&#29702;&#65292;&#20197;&#23454;&#29616;&#25361;&#25112;&#24615;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.09802</link><description>&lt;p&gt;
Sasha: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26234;&#33021;&#23478;&#23621;&#20013;&#30340;&#21019;&#24847;&#30446;&#26631;&#23548;&#21521;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Sasha: creative goal-oriented reasoning in smart homes with large language models. (arXiv:2305.09802v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26234;&#33021;&#23478;&#23621;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#29992;&#25143;&#21629;&#20196;&#30340;&#21019;&#24847;&#30446;&#26631;&#23548;&#21521;&#25512;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21019;&#36896;&#24615;&#22320;&#25512;&#29702;&#65292;&#20197;&#23454;&#29616;&#25361;&#25112;&#24615;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#23478;&#23621;&#30340;&#20351;&#29992;&#32773;&#19982;&#35774;&#22791;&#30340;&#20132;&#20114;&#37117;&#26377;&#26126;&#30830;&#25110;&#38544;&#21547;&#30340;&#30446;&#26631;&#12290;&#29616;&#26377;&#30340;&#23478;&#24237;&#21161;&#25163;&#33021;&#22815;&#36731;&#26494;&#22320;&#23454;&#29616;&#26126;&#30830;&#30340;&#30446;&#26631;&#65292;&#20363;&#22914;"&#25171;&#24320;&#28783;"&#12290;&#28982;&#32780;&#65292;&#22312;&#26356;&#33258;&#28982;&#30340;&#20132;&#27969;&#20013;&#65292;&#20154;&#20204;&#24448;&#24448;&#20250;&#25551;&#36848;&#38544;&#21547;&#30340;&#30446;&#26631;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21487;&#20197;&#35753;&#21035;&#20154;"&#35753;&#25151;&#38388;&#21464;&#24471;&#33298;&#36866;"&#32780;&#19981;&#26159;&#25551;&#36848;&#20855;&#20307;&#30340;&#27493;&#39588;&#12290;&#24403;&#21069;&#30340;&#31995;&#32479;&#24456;&#38590;&#35299;&#20915;&#36825;&#31181;&#27495;&#20041;&#65292;&#22240;&#20026;&#38656;&#35201;&#23558;&#27169;&#31946;&#30340;&#24847;&#22270;&#19982;&#20855;&#20307;&#30340;&#35774;&#22791;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#20174;&#36890;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35282;&#24230;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#20123;&#27169;&#22411;&#32463;&#36807;&#22823;&#37327;&#35821;&#26009;&#24211;&#30340;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#28789;&#27963;&#30340;&#26041;&#24335;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;LLMs&#25511;&#21046;&#35774;&#22791;&#24182;&#21019;&#24314;&#33258;&#21160;&#21270;&#31243;&#24207;&#20197;&#28385;&#36275;&#29992;&#25143;&#21629;&#20196;&#30340;&#38544;&#21547;&#30446;&#26631;&#12290;&#22312;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#21487;&#20197;&#21019;&#36896;&#24615;&#22320;&#25512;&#29702;&#65292;&#20197;&#23454;&#29616;&#25361;&#25112;&#24615;&#30340;&#30446;&#26631;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#38477;&#20302;&#20854;&#26377;&#29992;&#24615;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#20351;&#29992;Sasha&#35299;&#20915;&#20102;&#36825;&#20123;&#24046;&#36317;&#65306;&#36825;&#26159;&#19968;&#20010;&#22312;&#26234;&#33021;&#23478;&#23621;&#20013;&#20351;&#29992;LLMs&#36827;&#34892;&#28789;&#27963;&#35299;&#37322;&#29992;&#25143;&#21629;&#20196;&#21644;&#35774;&#22791;&#25511;&#21046;&#30340;&#21019;&#24847;&#30446;&#26631;&#23548;&#21521;&#25512;&#29702;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Every smart home user interaction has an explicit or implicit goal. Existing home assistants easily achieve explicit goals, e.g., "turn on the light". In more natural communication, however, humans tend to describe implicit goals. We can, for example, ask someone to "make it cozy" rather than describe the specific steps involved. Current systems struggle with this ambiguity since it requires them to relate vague intent to specific devices. We approach this problem of flexibly achieving user goals from the perspective of general-purpose large language models (LLMs) trained on gigantic corpora and adapted to downstream tasks with remarkable flexibility. We explore the use of LLMs for controlling devices and creating automation routines to meet the implicit goals of user commands. In a user-focused study, we find that LLMs can reason creatively to achieve challenging goals, while also revealing gaps that diminish their usefulness. We address these gaps with Sasha: a system for creative, g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20266;&#26631;&#31614;&#30340;&#27867;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;&#36125;&#21494;&#26031;&#20266;&#26631;&#31614;&#65292;&#22312;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#24212;&#29992;&#25928;&#26524;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.01747</link><description>&lt;p&gt;
&#24102;&#26377;&#26377;&#38480;&#27880;&#37322;&#30340;&#20998;&#21106;&#20219;&#21153;&#20013;&#30340;&#26399;&#26395;&#26368;&#22823;&#21270;&#20266;&#26631;&#31614;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Expectation Maximization Pseudo Labelling for Segmentation with Limited Annotations. (arXiv:2305.01747v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20266;&#26631;&#31614;&#30340;&#27867;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;&#36125;&#21494;&#26031;&#20266;&#26631;&#31614;&#65292;&#22312;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#24212;&#29992;&#25928;&#26524;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#20266;&#26631;&#31614;&#21450;&#20854;&#25512;&#24191;&#65292;&#20266;&#26631;&#31614;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#21407;&#22987;&#25512;&#26029;&#20316;&#20026;&#33258;&#35757;&#32451;&#30340;&#20266;&#26631;&#31614;&#65292;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#23454;&#35777;&#25104;&#21151;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#20266;&#26631;&#31614;&#21644;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#37096;&#20998;&#35299;&#37322;&#20102;&#20854;&#23454;&#35777;&#25104;&#21151;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36125;&#21494;&#26031;&#21407;&#29702;&#19979;&#20266;&#26631;&#31614;&#30340;&#23436;&#20840;&#27867;&#21270;&#65292;&#31216;&#20026;&#36125;&#21494;&#26031;&#20266;&#26631;&#31614;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#21464;&#20998;&#26041;&#27861;&#26469;&#23398;&#20064;&#36924;&#36817;&#36125;&#21494;&#26031;&#20266;&#26631;&#31614;&#65292;&#36890;&#36807;&#23398;&#20064;&#36873;&#25321;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#30340;&#38408;&#20540;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#23637;&#31034;&#20102;&#20266;&#26631;&#31614;&#21644;&#20854;&#25512;&#24191;&#36125;&#21494;&#26031;&#20266;&#26631;&#31614;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study pseudo labelling and its generalisation for semi-supervised segmentation of medical images. Pseudo labelling has achieved great empirical successes in semi-supervised learning, by utilising raw inferences on unlabelled data as pseudo labels for self-training. In our paper, we build a connection between pseudo labelling and the Expectation Maximization algorithm which partially explains its empirical successes. We thereby realise that the original pseudo labelling is an empirical estimation of its underlying full formulation. Following this insight, we demonstrate the full generalisation of pseudo labels under Bayes' principle, called Bayesian Pseudo Labels. We then provide a variational approach to learn to approximate Bayesian Pseudo Labels, by learning a threshold to select good quality pseudo labels. In the rest of the paper, we demonstrate the applications of Pseudo Labelling and its generalisation Bayesian Psuedo Labelling in semi-supervised segmentation of medical images
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#23460;&#20869;&#29615;&#22659;&#19979;&#36827;&#34892;&#36816;&#21160;&#30446;&#26631;&#30340;&#23450;&#20301;&#65292;&#20351;&#29992;&#20102;&#32467;&#26500;&#36816;&#21160;&#19982;&#27169;&#25311;&#25968;&#25454;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#12290;&#30740;&#31350;&#32773;&#25972;&#21512;&#20809;&#27969;&#21644;&#30456;&#23545;&#23039;&#24577;&#22238;&#24402;&#26041;&#27861;&#24110;&#21161;&#35299;&#20915;&#20102;&#22240;&#36816;&#21160;&#27169;&#31946;&#12289;&#20809;&#29031;&#21464;&#21270;&#12289;&#37325;&#22797;&#22270;&#26696;&#21644;&#32570;&#20047;&#29305;&#24449;&#32467;&#26500;&#31561;&#38382;&#39064;&#32780;&#24102;&#26469;&#30340;&#29942;&#39048;&#65292;&#20026;&#23460;&#20869;&#30446;&#26631;&#23450;&#20301;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.07250</link><description>&lt;p&gt;
&#36890;&#36807;&#20174;&#20809;&#27969;&#20449;&#24687;&#20013;&#34701;&#21512;&#36816;&#21160;&#32467;&#26500;&#19982;&#27169;&#25311;&#25968;&#25454;&#30340;&#32477;&#23545;&#20301;&#32622;&#22238;&#24402;&#65292;&#35299;&#20915;&#23460;&#20869;&#29615;&#22659;&#23450;&#20301;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Fusing Structure from Motion and Simulation-Augmented Pose Regression from Optical Flow for Challenging Indoor Environments. (arXiv:2304.07250v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#23460;&#20869;&#29615;&#22659;&#19979;&#36827;&#34892;&#36816;&#21160;&#30446;&#26631;&#30340;&#23450;&#20301;&#65292;&#20351;&#29992;&#20102;&#32467;&#26500;&#36816;&#21160;&#19982;&#27169;&#25311;&#25968;&#25454;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#12290;&#30740;&#31350;&#32773;&#25972;&#21512;&#20809;&#27969;&#21644;&#30456;&#23545;&#23039;&#24577;&#22238;&#24402;&#26041;&#27861;&#24110;&#21161;&#35299;&#20915;&#20102;&#22240;&#36816;&#21160;&#27169;&#31946;&#12289;&#20809;&#29031;&#21464;&#21270;&#12289;&#37325;&#22797;&#22270;&#26696;&#21644;&#32570;&#20047;&#29305;&#24449;&#32467;&#26500;&#31561;&#38382;&#39064;&#32780;&#24102;&#26469;&#30340;&#29942;&#39048;&#65292;&#20026;&#23460;&#20869;&#30446;&#26631;&#23450;&#20301;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#30340;&#23450;&#20301;&#26159;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#27604;&#22914;&#26426;&#22120;&#20154;&#12289;&#34394;&#25311;&#21644;&#22686;&#24378;&#29616;&#23454;&#12289;&#21644;&#22312;&#20179;&#24211;&#20013;&#36816;&#36865;&#36135;&#29289;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#20808;&#36827;&#21457;&#23637;&#24050;&#32463;&#20351;&#24471;&#20351;&#29992;&#21333;&#30446;&#35270;&#35273;&#30456;&#26426;&#36827;&#34892;&#23450;&#20301;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#26159;&#30001;&#20110;&#29615;&#22659;&#26412;&#36523;&#24341;&#36215;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#36816;&#21160;&#27169;&#31946;&#12289;&#20809;&#29031;&#21464;&#21270;&#12289;&#37325;&#22797;&#22270;&#26696;&#21644;&#32570;&#20047;&#29305;&#24449;&#30340;&#32467;&#26500;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#34701;&#21512;&#38468;&#21152;&#20449;&#24687;&#21644;&#20351;&#29992;&#30456;&#23545;&#20301;&#32622;&#22238;&#24402;&#65288;RPR&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20351;&#29992;Lucas-Kanade&#31639;&#27861;&#35745;&#31639;&#36830;&#32493;&#22270;&#20687;&#20043;&#38388;&#30340;&#20809;&#27969;&#65292;&#24182;&#20351;&#29992;&#36741;&#21161;&#23567;&#22411;&#24490;&#29615;&#21367;&#31215;&#32593;&#32476;&#26469;&#39044;&#27979;&#30456;&#23545;&#23039;&#24577;&#12290;&#23558;&#32477;&#23545;&#23039;&#24577;&#21644;&#30456;&#23545;&#23039;&#24577;&#36827;&#34892;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The localization of objects is a crucial task in various applications such as robotics, virtual and augmented reality, and the transportation of goods in warehouses. Recent advances in deep learning have enabled the localization using monocular visual cameras. While structure from motion (SfM) predicts the absolute pose from a point cloud, absolute pose regression (APR) methods learn a semantic understanding of the environment through neural networks. However, both fields face challenges caused by the environment such as motion blur, lighting changes, repetitive patterns, and feature-less structures. This study aims to address these challenges by incorporating additional information and regularizing the absolute pose using relative pose regression (RPR) methods. The optical flow between consecutive images is computed using the Lucas-Kanade algorithm, and the relative pose is predicted using an auxiliary small recurrent convolutional network. The fusion of absolute and relative poses is
&lt;/p&gt;</description></item><item><title>FDRL&#26159;&#19968;&#31181;&#22522;&#20110;&#27969;&#24341;&#23548;&#30340;&#23494;&#24230;&#27604;&#23398;&#20064;&#30340;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#23494;&#24230;&#27604;&#20272;&#35745;&#22120;&#20174;&#36880;&#28176;&#25913;&#36827;&#30340;&#26679;&#26412;&#20013;&#23398;&#20064;&#65292;&#32531;&#35299;&#20102;&#23494;&#24230;&#40511;&#27807;&#38382;&#39064;&#65292;&#24182;&#22312;&#29983;&#25104;&#39640;&#23610;&#23544;&#22270;&#20687;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.03714</link><description>&lt;p&gt;
&#20351;&#29992;&#27969;&#24341;&#23548;&#30340;&#23494;&#24230;&#27604;&#23398;&#20064;&#36827;&#34892;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Generative Modeling with Flow-Guided Density Ratio Learning. (arXiv:2303.03714v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03714
&lt;/p&gt;
&lt;p&gt;
FDRL&#26159;&#19968;&#31181;&#22522;&#20110;&#27969;&#24341;&#23548;&#30340;&#23494;&#24230;&#27604;&#23398;&#20064;&#30340;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#23494;&#24230;&#27604;&#20272;&#35745;&#22120;&#20174;&#36880;&#28176;&#25913;&#36827;&#30340;&#26679;&#26412;&#20013;&#23398;&#20064;&#65292;&#32531;&#35299;&#20102;&#23494;&#24230;&#40511;&#27807;&#38382;&#39064;&#65292;&#24182;&#22312;&#29983;&#25104;&#39640;&#23610;&#23544;&#22270;&#20687;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#31216;&#20026;&#27969;&#24341;&#23548;&#30340;&#23494;&#24230;&#27604;&#23398;&#20064;&#65288;FDRL&#65289;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;DGflow&#20013;&#24341;&#20837;&#30340;&#22522;&#20110;&#29109;&#27491;&#21017;&#21270;f-&#25955;&#24230;&#30340;&#26799;&#24230;&#27969;&#30340;&#36807;&#26102;&#65288;&#26102;&#38388;&#26080;&#20851;&#65289;&#36817;&#20284;&#65292;&#24182;&#19988;&#36890;&#36807;GAN&#37492;&#21035;&#22120;&#32473;&#20986;&#30340;&#36807;&#26102;&#20272;&#35745;&#22120;&#36817;&#20284;&#20102;&#19981;&#21487;&#35745;&#31639;&#30340;&#26102;&#38388;&#30456;&#20851;&#23494;&#24230;&#27604;&#12290;&#22312;&#26679;&#26412;&#32454;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#36817;&#20284;&#36275;&#22815;&#65292;&#22240;&#20026;&#27969;&#30340;&#28304;&#20998;&#24067;&#21644;&#30446;&#26631;&#20998;&#24067;&#26159;&#30456;&#36817;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#25104;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20010;&#20551;&#35774;&#26159;&#26080;&#25928;&#30340;&#65292;&#32780;&#19988;&#36807;&#26102;&#20272;&#35745;&#22120;&#30340;&#26420;&#32032;&#24212;&#29992;&#30001;&#20110;&#20004;&#20010;&#20998;&#24067;&#20043;&#38388;&#30340;&#22823;&#40511;&#27807;&#32780;&#22833;&#36133;&#12290;FDRL&#25552;&#20986;&#20102;&#35757;&#32451;&#23494;&#24230;&#27604;&#20272;&#35745;&#22120;&#30340;&#26041;&#27861;&#65292;&#20351;&#20854;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20174;&#36880;&#28176;&#25913;&#36827;&#30340;&#26679;&#26412;&#20013;&#23398;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#32531;&#35299;&#20102;&#23494;&#24230;&#40511;&#27807;&#38382;&#39064;&#65292;&#20351;&#24471;FDRL&#33021;&#22815;&#29983;&#25104;&#39640;&#36798;$128\times128$&#23610;&#23544;&#30340;&#22270;&#20687;&#65292;&#24182;&#19988;&#22312;&#36136;&#37327;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26799;&#24230;&#27969;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Flow-Guided Density Ratio Learning (FDRL), a simple and scalable approach to generative modeling which builds on the stale (time-independent) approximation of the gradient flow of entropy-regularized f-divergences introduced in DGflow. In DGflow, the intractable time-dependent density ratio is approximated by a stale estimator given by a GAN discriminator. This is sufficient in the case of sample refinement, where the source and target distributions of the flow are close to each other. However, this assumption is invalid for generation and a naive application of the stale estimator fails due to the large chasm between the two distributions. FDRL proposes to train a density ratio estimator such that it learns from progressively improving samples during the training process. We show that this simple method alleviates the density chasm problem, allowing FDRL to generate images of dimensions as high as $128\times128$, as well as outperform existing gradient flow baselines on qua
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#21452;&#37325;&#24378;&#21270;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#24182;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#19979;&#35299;&#37322;&#20102;&#20960;&#31181;&#26368;&#26032;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21450;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#21452;&#37325;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65288;DIL&#65289;&#30452;&#25509;&#26368;&#23567;&#21270;&#31574;&#30053;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.08560</link><description>&lt;p&gt;
&#21452;&#37325;&#24378;&#21270;&#23398;&#20064;&#65306;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#32479;&#19968;&#21644;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dual RL: Unification and New Methods for Reinforcement and Imitation Learning. (arXiv:2302.08560v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08560
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#21452;&#37325;&#24378;&#21270;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#24182;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#19979;&#35299;&#37322;&#20102;&#20960;&#31181;&#26368;&#26032;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21450;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#21452;&#37325;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65288;DIL&#65289;&#30452;&#25509;&#26368;&#23567;&#21270;&#31574;&#30053;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#26399;&#26395;&#32047;&#31215;&#22238;&#25253;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20010;&#30446;&#26631;&#21487;&#20197;&#36890;&#36807;&#22312;&#32447;&#24615;&#32422;&#26463;&#19979;&#20248;&#21270;&#29366;&#24577;-&#21160;&#20316;&#35775;&#38382;&#20998;&#24067;&#30340;&#20248;&#21270;&#38382;&#39064;&#26469;&#34920;&#31034;&#12290;&#36825;&#20010;&#34920;&#36848;&#30340;&#23545;&#20598;&#38382;&#39064;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#21452;&#37325;&#24378;&#21270;&#23398;&#20064;&#65292;&#26159;&#26080;&#32422;&#26463;&#30340;&#24182;&#19988;&#26356;&#23481;&#26131;&#20248;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20197;&#21450;&#27169;&#20223;&#23398;&#20064;&#21487;&#20197;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#19979;&#34987;&#35270;&#20026;&#21452;&#37325;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36825;&#31181;&#32479;&#19968;&#25552;&#20379;&#20102;&#19968;&#20010;&#20849;&#21516;&#30340;&#22522;&#30784;&#65292;&#21487;&#20197;&#30740;&#31350;&#21644;&#35782;&#21035;&#36825;&#20123;&#26041;&#27861;&#25104;&#21151;&#30340;&#26500;&#25104;&#37096;&#20998;&#65292;&#24182;&#25581;&#31034;&#36825;&#20123;&#26041;&#27861;&#30340;&#20849;&#21516;&#32570;&#28857;&#21644;&#25913;&#36827;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20197;&#21069;&#30340;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#19981;&#29616;&#23454;&#30340;&#35206;&#30422;&#29575;&#20551;&#35774;&#65292;&#24182;&#26368;&#23567;&#21270;&#20102;&#23398;&#20064;&#20195;&#29702;&#21644;&#19987;&#23478;&#35775;&#38382;&#20998;&#24067;&#20043;&#38388;&#30340;&#29305;&#23450;f-&#20998;&#24067;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#21452;&#37325;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65288;DIL&#65289;&#30452;&#25509;&#26368;&#23567;&#21270;&#31574;&#30053;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#22312;&#21516;&#26679;&#30340;&#21452;&#37325;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65292;&#23545;&#20960;&#20010;&#22522;&#20934;&#20219;&#21153;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of reinforcement learning (RL) is to maximize the expected cumulative return. It has been shown that this objective can be represented by an optimization problem of the state-action visitation distribution under linear constraints. The dual problem of this formulation, which we refer to as dual RL, is unconstrained and easier to optimize. We show that several state-of-the-art off-policy deep reinforcement learning (RL) algorithms, under both online and offline, RL and imitation learning (IL) settings, can be viewed as dual RL approaches in a unified framework. This unification provides a common ground to study and identify the components that contribute to the success of these methods and also reveals the common shortcomings across methods with new insights for improvement. Our analysis shows that prior off-policy imitation learning methods are based on an unrealistic coverage assumption and are minimizing a particular f-divergence between the visitation distributions of the l
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#23454;&#39564;&#26694;&#26550;&#65292;PSI-AVA&#25968;&#25454;&#38598;&#65292;&#20197;&#23454;&#29616;&#23545;&#26426;&#22120;&#20154;&#36741;&#21161;&#19979;&#30340;&#21069;&#21015;&#33146;&#30284;&#26681;&#27835;&#26415;&#35270;&#39057;&#30340;&#25972;&#20307;&#29702;&#35299;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#22522;&#20110;TAPIR&#30340;&#25163;&#26415;&#22330;&#26223;&#29702;&#35299;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.04582</link><description>&lt;p&gt;
&#23454;&#29616;&#23545;&#25163;&#26415;&#22330;&#26223;&#30340;&#25972;&#20307;&#29702;&#35299;&#65306;&#38024;&#23545;&#26426;&#22120;&#20154;&#36741;&#21161;&#19979;&#30340;&#21069;&#21015;&#33146;&#30284;&#26681;&#27835;&#26415;&#35270;&#39057;
&lt;/p&gt;
&lt;p&gt;
Towards Holistic Surgical Scene Understanding. (arXiv:2212.04582v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#23454;&#39564;&#26694;&#26550;&#65292;PSI-AVA&#25968;&#25454;&#38598;&#65292;&#20197;&#23454;&#29616;&#23545;&#26426;&#22120;&#20154;&#36741;&#21161;&#19979;&#30340;&#21069;&#21015;&#33146;&#30284;&#26681;&#27835;&#26415;&#35270;&#39057;&#30340;&#25972;&#20307;&#29702;&#35299;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#22522;&#20110;TAPIR&#30340;&#25163;&#26415;&#22330;&#26223;&#29702;&#35299;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29992;&#20110;&#30740;&#31350;&#25163;&#26415;&#24178;&#39044;&#30340;&#22522;&#20934;&#27979;&#35797;&#37117;&#38598;&#20013;&#22312;&#29305;&#23450;&#25361;&#25112;&#19978;&#65292;&#32780;&#24573;&#30053;&#20102;&#19981;&#21516;&#20219;&#21153;&#38388;&#20869;&#22312;&#30340;&#20114;&#34917;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#39564;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#23545;&#25163;&#26415;&#22330;&#26223;&#30340;&#25972;&#20307;&#29702;&#35299;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Phase&#12289;Step&#12289;Instrument&#21644;Atomic Visual Action&#65288;PSI-AVA&#65289;&#25968;&#25454;&#38598;&#12290;PSI-AVA&#22312;&#26426;&#22120;&#20154;&#36741;&#21161;&#19979;&#30340;&#21069;&#21015;&#33146;&#30284;&#26681;&#27835;&#26415;&#35270;&#39057;&#20013;&#65292;&#23545;&#38271;&#26399;&#65288;&#38454;&#27573;&#21644;&#27493;&#39588;&#30340;&#35782;&#21035;&#65289;&#21644;&#30701;&#26399;&#25512;&#29702;&#65288;&#22120;&#26800;&#26816;&#27979;&#21644;&#26032;&#22411;&#21407;&#23376;&#21160;&#20316;&#35782;&#21035;&#65289;&#36827;&#34892;&#27880;&#37322;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TAPIR&#65292;&#21363;Transformers for Action&#65292;Phase&#65292;Instrument&#21644;Steps Recognition&#65292;&#20316;&#20026;&#25163;&#26415;&#22330;&#26223;&#29702;&#35299;&#30340;&#24378;&#22522;&#20934;&#12290;TAPIR&#21033;&#29992;&#25105;&#20204;&#25968;&#25454;&#38598;&#30340;&#22810;&#32423;&#27880;&#37322;&#65292;&#36890;&#36807;&#22120;&#26800;&#26816;&#27979;&#20219;&#21153;&#19978;&#23398;&#20064;&#30340;&#34920;&#31034;&#26041;&#24335;&#65292;&#25552;&#39640;&#20102;&#20854;&#20998;&#31867;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;PSI-AVA&#21644;&#20854;&#20182;&#20844;&#24320;&#25968;&#25454;&#24211;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25972;&#20307;&#25163;&#26415;&#22330;&#26223;&#29702;&#35299;&#26694;&#26550;&#26159;&#20805;&#20998;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most benchmarks for studying surgical interventions focus on a specific challenge instead of leveraging the intrinsic complementarity among different tasks. In this work, we present a new experimental framework towards holistic surgical scene understanding. First, we introduce the Phase, Step, Instrument, and Atomic Visual Action recognition (PSI-AVA) Dataset. PSI-AVA includes annotations for both long-term (Phase and Step recognition) and short-term reasoning (Instrument detection and novel Atomic Action recognition) in robot-assisted radical prostatectomy videos. Second, we present Transformers for Action, Phase, Instrument, and steps Recognition (TAPIR) as a strong baseline for surgical scene understanding. TAPIR leverages our dataset's multi-level annotations as it benefits from the learned representation on the instrument detection task to improve its classification capacity. Our experimental results in both PSI-AVA and other publicly available databases demonstrate the adequacy o
&lt;/p&gt;</description></item><item><title>&#22312;&#22240;&#26524;&#22270;&#20013;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#21069;&#38376;&#35843;&#25972;&#30340;&#32447;&#24615;&#26102;&#38388;&#31639;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#21040;&#30340;&#20013;&#20171;&#21464;&#37327;&#65292;&#21363;&#20351;&#23384;&#22312;&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#65292;&#20063;&#21487;&#20197;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2211.16468</link><description>&lt;p&gt;
&#22240;&#26524;&#22270;&#20013;&#21069;&#38376;&#35843;&#25972;&#30340;&#32447;&#24615;&#26102;&#38388;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Linear-Time Algorithms for Front-Door Adjustment in Causal Graphs. (arXiv:2211.16468v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16468
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22240;&#26524;&#22270;&#20013;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#21069;&#38376;&#35843;&#25972;&#30340;&#32447;&#24615;&#26102;&#38388;&#31639;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#21040;&#30340;&#20013;&#20171;&#21464;&#37327;&#65292;&#21363;&#20351;&#23384;&#22312;&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#65292;&#20063;&#21487;&#20197;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#26159;&#23454;&#35777;&#31185;&#23398;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#24403;&#31995;&#32479;&#20013;&#28041;&#21450;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#26102;&#65292;&#36825;&#21464;&#24471;&#23588;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#21069;&#38376;&#35843;&#25972;&#8212;&#8212;&#19968;&#31181;&#32463;&#20856;&#25216;&#26415;&#65292;&#23427;&#20351;&#29992;&#35266;&#23519;&#21040;&#30340;&#20013;&#20171;&#21464;&#37327;&#65292;&#21363;&#20351;&#23384;&#22312;&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#65292;&#20063;&#21487;&#20197;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#12290;&#34429;&#28982;&#21069;&#38376;&#20272;&#35745;&#30340;&#32479;&#35745;&#29305;&#24615;&#24050;&#32463;&#24456;&#22909;&#22320;&#29702;&#35299;&#20102;&#65292;&#20294;&#23427;&#30340;&#31639;&#27861;&#26041;&#38754;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26410;&#24471;&#21040;&#25506;&#31350;&#12290;&#26368;&#36817;&#65292;Jeong&#65292;Tian&#21644;Barenboim [NeurIPS 2022]&#25552;&#20986;&#20102;&#19968;&#31181;&#31532;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32473;&#23450;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#20013;&#25214;&#21040;&#28385;&#36275;&#21069;&#38376;&#20934;&#21017;&#30340;&#38598;&#21512;&#65292;&#20854;&#36816;&#34892;&#26102;&#38388;&#20026;$O&#65288;n^3&#65288;n+m&#65289;&#65289;$&#65292;&#20854;&#20013;$n$&#34920;&#31034;&#21464;&#37327;&#30340;&#25968;&#37327;&#65292;$m$&#34920;&#31034;&#22240;&#26524;&#22270;&#30340;&#36793;&#30340;&#25968;&#37327;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#31639;&#27861;&#65292;&#21363;$O&#65288;n+m&#65289;$&#65292;&#29992;&#20110;&#36825;&#39033;&#20219;&#21153;&#65292;&#20174;&#32780;&#36798;&#21040;&#20102;&#28176;&#36817;&#26368;&#20248;&#30340;&#26102;&#38388;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal effect estimation from observational data is a fundamental task in empirical sciences. It becomes particularly challenging when unobserved confounders are involved in a system. This paper focuses on front-door adjustment -- a classic technique which, using observed mediators allows to identify causal effects even in the presence of unobserved confounding. While the statistical properties of the front-door estimation are quite well understood, its algorithmic aspects remained unexplored for a long time. Recently, Jeong, Tian, and Barenboim [NeurIPS 2022] have presented the first polynomial-time algorithm for finding sets satisfying the front-door criterion in a given directed acyclic graph (DAG), with an $O(n^3(n+m))$ run time, where $n$ denotes the number of variables and $m$ the number of edges of the causal graph. In our work, we give the first linear-time, i.e., $O(n+m)$, algorithm for this task, which thus reaches the asymptotically optimal time complexity. This result impli
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28857;&#23545;&#28857;&#36890;&#20449;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#20154;&#32593;&#32476;&#65292;&#36890;&#36807;&#39640;&#26031;&#20449;&#24565;&#20256;&#25773;&#23454;&#29616;&#20840;&#23616;&#23450;&#20301;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#65292;&#24182;&#19988;&#23545;&#39640;&#25925;&#38556;&#29575;&#26377;&#19968;&#23450;&#30340;&#23481;&#24525;&#24230;&#12290;</title><link>http://arxiv.org/abs/2202.03314</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#20998;&#24067;&#24335;&#22810;&#35774;&#22791;&#23450;&#20301;&#30340;&#26426;&#22120;&#20154;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A Robot Web for Distributed Many-Device Localisation. (arXiv:2202.03314v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03314
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28857;&#23545;&#28857;&#36890;&#20449;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#20154;&#32593;&#32476;&#65292;&#36890;&#36807;&#39640;&#26031;&#20449;&#24565;&#20256;&#25773;&#23454;&#29616;&#20840;&#23616;&#23450;&#20301;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#65292;&#24182;&#19988;&#23545;&#39640;&#25925;&#38556;&#29575;&#26377;&#19968;&#23450;&#30340;&#23481;&#24525;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#20154;&#25110;&#20854;&#20182;&#35774;&#22791;&#30340;&#32593;&#32476;&#65292;&#21487;&#20197;&#36890;&#36807;&#39640;&#25928;&#30340;&#28857;&#23545;&#28857;&#36890;&#20449;&#21327;&#20316;&#23454;&#29616;&#20840;&#23616;&#23450;&#20301;&#12290;&#25105;&#20204;&#30340;&#26426;&#22120;&#20154;&#32593;&#32476;&#35299;&#20915;&#26041;&#26696;&#22522;&#20110;&#39640;&#26031;&#20449;&#24565;&#20256;&#25773;&#65292;&#36890;&#36807;&#25551;&#36848;&#26426;&#22120;&#20154;&#20869;&#37096;&#25110;&#24444;&#27492;&#35266;&#27979;&#30340;&#27010;&#29575;&#32467;&#26500;&#30340;&#38750;&#32447;&#24615;&#22240;&#23376;&#22270;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#31867;&#22411;&#30340;&#26426;&#22120;&#20154;&#12289;&#36816;&#21160;&#25110;&#20256;&#24863;&#22120;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#36890;&#20449;&#21327;&#35758;&#65292;&#21487;&#20197;&#36890;&#36807;&#21457;&#24067;&#21644;&#35835;&#21462;&#32593;&#39029;&#25110;&#20854;&#20182;&#24322;&#27493;&#36890;&#20449;&#25216;&#26415;&#26469;&#23454;&#29616;&#12290;&#36890;&#36807;&#19982;&#26368;&#22810;1000&#20010;&#26426;&#22120;&#20154;&#22312;&#20219;&#24847;&#27169;&#24335;&#19979;&#36827;&#34892;&#30340;&#20223;&#30495;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#20840;&#23616;&#20934;&#30830;&#24615;&#19978;&#33021;&#36798;&#21040;&#19982;&#38598;&#20013;&#24335;&#38750;&#32447;&#24615;&#22240;&#23376;&#22270;&#27714;&#35299;&#22120;&#19968;&#26679;&#20934;&#30830;&#30340;&#27700;&#24179;&#65292;&#21516;&#26102;&#20855;&#26377;&#39640;&#20998;&#24067;&#24335;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;&#36890;&#36807;&#22312;GBP&#20013;&#20351;&#29992;&#40065;&#26834;&#22240;&#23376;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#39640;&#25925;&#38556;&#29575;&#26377;&#19968;&#23450;&#30340;&#23481;&#24525;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that a distributed network of robots or other devices which make measurements of each other can collaborate to globally localise via efficient ad-hoc peer to peer communication. Our Robot Web solution is based on Gaussian Belief Propagation on the fundamental non-linear factor graph describing the probabilistic structure of all of the observations robots make internally or of each other, and is flexible for any type of robot, motion or sensor. We define a simple and efficient communication protocol which can be implemented by the publishing and reading of web pages or other asynchronous communication technologies. We show in simulations with up to 1000 robots interacting in arbitrary patterns that our solution convergently achieves global accuracy as accurate as a centralised non-linear factor graph solver while operating with high distributed efficiency of computation and communication. Via the use of robust factors in GBP, our method is tolerant to a high percentage of faults
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#21704;&#24076;&#20989;&#25968;&#23646;&#24615;&#36827;&#34892;&#25968;&#23398;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#20998;&#31867;&#23383;&#27597;&#34920;&#19978;&#30340;&#24207;&#21015;&#20998;&#26512;&#20013;&#65292;&#20351;&#29992;&#38543;&#26426;&#39640;&#26031;&#21021;&#22987;&#21270;&#30340;&#21367;&#31215;&#28388;&#27874;&#22120;&#21644;&#26368;&#22823;&#27744;&#21270;&#31561;&#20215;&#20110;&#36873;&#25321;&#19968;&#31181;&#26368;&#23567;&#21270;&#22120;&#25490;&#24207;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#21462;&#19982;&#20854;&#20182;&#26368;&#23567;&#21270;&#22120;&#36317;&#31163;&#36739;&#36817;&#20294;&#19982;&#24207;&#21015;&#20013;&#30340;k-mer&#30456;&#36317;&#36739;&#36828;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2111.08452</link><description>&lt;p&gt;
&#20851;&#20110;&#26368;&#23567;&#21270;&#22120;&#21644;&#21367;&#31215;&#28388;&#27874;&#22120;&#30340;&#29702;&#35770;&#36830;&#25509;&#21450;&#20854;&#22312;&#22522;&#22240;&#32452;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On minimizers and convolutional filters: theoretical connections and applications to genome analysis. (arXiv:2111.08452v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.08452
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#21704;&#24076;&#20989;&#25968;&#23646;&#24615;&#36827;&#34892;&#25968;&#23398;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#20998;&#31867;&#23383;&#27597;&#34920;&#19978;&#30340;&#24207;&#21015;&#20998;&#26512;&#20013;&#65292;&#20351;&#29992;&#38543;&#26426;&#39640;&#26031;&#21021;&#22987;&#21270;&#30340;&#21367;&#31215;&#28388;&#27874;&#22120;&#21644;&#26368;&#22823;&#27744;&#21270;&#31561;&#20215;&#20110;&#36873;&#25321;&#19968;&#31181;&#26368;&#23567;&#21270;&#22120;&#25490;&#24207;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#21462;&#19982;&#20854;&#20182;&#26368;&#23567;&#21270;&#22120;&#36317;&#31163;&#36739;&#36817;&#20294;&#19982;&#24207;&#21015;&#20013;&#30340;k-mer&#30456;&#36317;&#36739;&#36828;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#23567;&#21270;&#22120;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#26159;&#20004;&#31181;&#23436;&#20840;&#19981;&#21516;&#30340;&#27969;&#34892;&#25216;&#26415;&#65292;&#22343;&#34987;&#29992;&#20110;&#20998;&#26512;&#29983;&#29289;&#24207;&#21015;&#12290;&#20174;&#34920;&#38754;&#19978;&#30475;&#65292;&#36825;&#20123;&#26041;&#27861;&#20284;&#20046;&#23436;&#20840;&#19981;&#21516;&#12290;&#26368;&#23567;&#21270;&#22120;&#20351;&#29992;&#28378;&#21160;&#31383;&#21475;&#30340;&#26368;&#23567;&#21704;&#24076;&#26041;&#27861;&#25552;&#21462;&#27599;&#20010;&#31383;&#21475;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;k-mer&#29305;&#24449;&#12290;CNN&#21017;&#20197;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#21367;&#31215;&#28388;&#27874;&#22120;&#21644;&#27744;&#21270;&#25805;&#20316;&#20026;&#22522;&#30784;&#65292;&#36890;&#36807;&#22810;&#20010;&#31070;&#32463;&#23618;&#26469;&#23398;&#20064;&#28388;&#27874;&#22120;&#26412;&#36523;&#21450;&#20854;&#29992;&#20110;&#20998;&#31867;&#24207;&#21015;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#20027;&#35201;&#32467;&#26524;&#26159;&#23545;&#21704;&#24076;&#20989;&#25968;&#23646;&#24615;&#36827;&#34892;&#20102;&#20180;&#32454;&#30340;&#25968;&#23398;&#20998;&#26512;&#65292;&#26174;&#31034;&#23545;&#20110;&#20998;&#31867;&#23383;&#27597;&#34920;&#19978;&#30340;&#24207;&#21015;&#65292;&#20351;&#29992;&#38543;&#26426;&#39640;&#26031;&#21021;&#22987;&#21270;&#30340;&#21367;&#31215;&#28388;&#27874;&#22120;&#21644;&#26368;&#22823;&#27744;&#21270;&#31561;&#20215;&#20110;&#36873;&#25321;&#19968;&#20010;&#26368;&#23567;&#21270;&#22120;&#25490;&#24207;&#65292;&#20351;&#24471;&#36873;&#25321;&#30340;k-mer&#19982;&#24207;&#21015;&#20013;&#30340;k-mer&#65288;&#25353;&#27721;&#26126;&#36317;&#31163;&#65289;&#30456;&#36317;&#36739;&#36828;&#65292;&#20294;&#19982;&#20854;&#20182;&#26368;&#23567;&#21270;&#22120;&#30456;&#36317;&#36739;&#36817;&#12290;&#22312;&#23454;&#35777;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#19982;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Minimizers and convolutional neural networks (CNNs) are two quite distinct popular techniques that have both been employed to analyze categorical biological sequences. At face value, the methods seem entirely dissimilar. Minimizers use min-wise hashing on a rolling window to extract a single important k-mer feature per window. CNNs start with a wide array of randomly initialized convolutional filters, paired with a pooling operation, and then multiple additional neural layers to learn both the filters themselves and how they can be used to classify the sequence.  Here, our main result is a careful mathematical analysis of hash function properties showing that for sequences over a categorical alphabet, random Gaussian initialization of convolutional filters with max-pooling is equivalent to choosing a minimizer ordering such that selected k-mers are (in Hamming distance) far from the k-mers within the sequence but close to other minimizers. In empirical experiments, we find that this pr
&lt;/p&gt;</description></item></channel></rss>