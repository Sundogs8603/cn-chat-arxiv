<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>GeneCIS&#22522;&#20934;&#27979;&#35797;&#34913;&#37327;&#27169;&#22411;&#36866;&#24212;&#21508;&#31181;&#30456;&#20284;&#24615;&#26465;&#20214;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#22522;&#20934;&#27979;&#35797;&#30340;&#34920;&#29616;&#19982;ImageNet&#30340;&#20934;&#30830;&#24615;&#24369;&#30456;&#20851;&#65292;&#20165;&#31616;&#21333;&#25193;&#23637;&#29616;&#26377;&#26041;&#27861;&#24182;&#19981;&#34892;&#12290;</title><link>http://arxiv.org/abs/2306.07969</link><description>&lt;p&gt;
GeneCIS&#65306;&#19968;&#31181;&#36890;&#29992;&#26465;&#20214;&#22270;&#20687;&#30456;&#20284;&#24230;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
GeneCIS: A Benchmark for General Conditional Image Similarity. (arXiv:2306.07969v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07969
&lt;/p&gt;
&lt;p&gt;
GeneCIS&#22522;&#20934;&#27979;&#35797;&#34913;&#37327;&#27169;&#22411;&#36866;&#24212;&#21508;&#31181;&#30456;&#20284;&#24615;&#26465;&#20214;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#22522;&#20934;&#27979;&#35797;&#30340;&#34920;&#29616;&#19982;ImageNet&#30340;&#20934;&#30830;&#24615;&#24369;&#30456;&#20851;&#65292;&#20165;&#31616;&#21333;&#25193;&#23637;&#29616;&#26377;&#26041;&#27861;&#24182;&#19981;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35748;&#20026;&#23384;&#22312;&#35768;&#22810;&#8220;&#30456;&#20284;&#24615;&#8221;&#30340;&#27010;&#24565;&#65292;&#32780;&#27169;&#22411;&#65288;&#22914;&#20154;&#31867;&#65289;&#24212;&#35813;&#33021;&#22815;&#21160;&#24577;&#22320;&#36866;&#24212;&#36825;&#20123;&#27010;&#24565;&#12290;&#36825;&#19982;&#22823;&#22810;&#25968;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65288;&#21463;&#30417;&#30563;&#25110;&#33258;&#30417;&#30563;&#65289;&#19981;&#21516;&#65292;&#23427;&#20204;&#23398;&#20064;&#19968;&#20010;&#22266;&#23450;&#30340;&#23884;&#20837;&#20989;&#25968;&#65292;&#22240;&#27492;&#38544;&#21547;&#22320;&#20551;&#23450;&#20102;&#21333;&#19968;&#30340;&#30456;&#20284;&#24615;&#27010;&#24565;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GeneCIS&#65288;&#8220;&#21019;&#19990;&#32426;&#8221;&#65289;&#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#27979;&#35797;&#34913;&#37327;&#20102;&#27169;&#22411;&#36866;&#24212;&#21508;&#31181;&#30456;&#20284;&#24615;&#26465;&#20214;&#30340;&#33021;&#21147;&#12290;&#25193;&#23637;&#20043;&#21069;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#21482;&#35774;&#35745;&#29992;&#20110;&#38646;&#26679;&#26412;&#35780;&#20272;&#65292;&#22240;&#27492;&#32771;&#34385;&#20102;&#24320;&#25918;&#38598;&#30340;&#30456;&#20284;&#24615;&#26465;&#20214;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24378;&#22823;&#30340;CLIP&#27169;&#22411;&#30340;&#22522;&#32447;&#22312;GeneCIS&#19978;&#36739;&#20026;&#22256;&#38590;&#65292;&#24182;&#19988;&#22522;&#20934;&#27979;&#35797;&#30340;&#34920;&#29616;&#20165;&#19982;ImageNet&#30340;&#20934;&#30830;&#24615;&#24369;&#30456;&#20851;&#65292;&#36825;&#34920;&#26126;&#31616;&#21333;&#22320;&#25193;&#23637;&#29616;&#26377;&#26041;&#27861;&#24182;&#19981;&#26159;&#26377;&#25104;&#26524;&#30340;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;
&lt;/p&gt;
&lt;p&gt;
We argue that there are many notions of 'similarity' and that models, like humans, should be able to adapt to these dynamically. This contrasts with most representation learning methods, supervised or self-supervised, which learn a fixed embedding function and hence implicitly assume a single notion of similarity. For instance, models trained on ImageNet are biased towards object categories, while a user might prefer the model to focus on colors, textures or specific elements in the scene. In this paper, we propose the GeneCIS ('genesis') benchmark, which measures models' ability to adapt to a range of similarity conditions. Extending prior work, our benchmark is designed for zero-shot evaluation only, and hence considers an open-set of similarity conditions. We find that baselines from powerful CLIP models struggle on GeneCIS and that performance on the benchmark is only weakly correlated with ImageNet accuracy, suggesting that simply scaling existing methods is not fruitful. We furth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#21160;&#34920;&#26684;&#39564;&#35777;&#20219;&#21153;&#65288;AutoTV&#65289;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797; arXiVeri &#26469;&#39564;&#35777;&#20854;&#20934;&#30830;&#24615;&#65292;&#36890;&#36807;&#22522;&#20110; GPT &#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#33258;&#21160;&#34920;&#26684;&#39564;&#35777;&#65292;&#22312;&#34920;&#26684;&#21305;&#37197;&#21644;&#21333;&#20803;&#26684;&#21305;&#37197;&#20219;&#21153;&#20013;&#22343;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07968</link><description>&lt;p&gt;
arXiVeri&#65306;&#22522;&#20110; GPT &#30340;&#33258;&#21160;&#34920;&#26684;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
arXiVeri: Automatic table verification with GPT. (arXiv:2306.07968v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#21160;&#34920;&#26684;&#39564;&#35777;&#20219;&#21153;&#65288;AutoTV&#65289;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797; arXiVeri &#26469;&#39564;&#35777;&#20854;&#20934;&#30830;&#24615;&#65292;&#36890;&#36807;&#22522;&#20110; GPT &#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#33258;&#21160;&#34920;&#26684;&#39564;&#35777;&#65292;&#22312;&#34920;&#26684;&#21305;&#37197;&#21644;&#21333;&#20803;&#26684;&#21305;&#37197;&#20219;&#21153;&#20013;&#22343;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27809;&#26377;&#23545;&#31185;&#23398;&#25991;&#29486;&#20013;&#25968;&#23383;&#25968;&#25454;&#30340;&#20934;&#30830;&#36716;&#24405;&#65292;&#31185;&#23398;&#23478;&#23601;&#19981;&#33021;&#24471;&#20986;&#20934;&#30830;&#32467;&#35770;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#23558;&#25968;&#23383;&#25968;&#25454;&#20174;&#19968;&#31687;&#35770;&#25991;&#22797;&#21046;&#21040;&#21478;&#19968;&#31687;&#35770;&#25991;&#30340;&#36807;&#31243;&#23481;&#26131;&#20986;&#29616;&#20154;&#20026;&#38169;&#35823;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#26032;&#39062;&#30340;&#33258;&#21160;&#34920;&#26684;&#39564;&#35777;&#20219;&#21153;&#65288;AutoTV&#65289;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#20854;&#30446;&#26631;&#26159;&#36890;&#36807;&#20132;&#21449;&#24341;&#29992;&#24341;&#29992;&#30340;&#26469;&#28304;&#26469;&#39564;&#35777;&#34920;&#26684;&#20013;&#25968;&#23383;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#25903;&#25345;&#35813;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;arXiVeri&#65292;&#23427;&#21253;&#25324;&#20174; arXiv &#19978;&#30340;&#24320;&#25918;&#33719;&#21462;&#23398;&#26415;&#35770;&#25991;&#20013;&#25552;&#21462;&#30340;&#34920;&#26684;&#25968;&#25454;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#35780;&#20272;&#34920;&#26684;&#39564;&#35777;&#22120;&#22312;&#20004;&#20010;&#20851;&#38190;&#39046;&#22495;&#24615;&#33021;&#30340;&#25351;&#26631;&#65306;&#65288;i&#65289;&#34920;&#26684;&#21305;&#37197;&#65292;&#26088;&#22312;&#35782;&#21035;&#24341;&#29992;&#25991;&#29486;&#20013;&#30340;&#26469;&#28304;&#34920;&#26684;&#19982;&#30446;&#26631;&#34920;&#26684;&#23545;&#24212;&#30340;&#34920;&#26684;&#65292;&#21644;&#65288;ii&#65289;&#21333;&#20803;&#26684;&#21305;&#37197;&#65292;&#26088;&#22312;&#20934;&#30830;&#23450;&#20301;&#30446;&#26631;&#34920;&#26684;&#21644;&#26469;&#28304;&#34920;&#26684;&#20043;&#38388;&#30340;&#20849;&#20139;&#21333;&#20803;&#26684;&#65292;&#24182;&#35782;&#21035;&#23427;&#20204;&#30340;&#34892;&#21644;&#21015;&#32034;&#24341;&#12290;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; GPT &#30340; AutoTV &#26041;&#27861;&#65292;&#24182;&#22312; arXiVeri &#22522;&#20934;&#27979;&#35797;&#19978;&#23558;&#20854;&#24615;&#33021;&#19982;&#20960;&#31181;&#22522;&#32447;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#34920;&#26684;&#21305;&#37197;&#21644;&#21333;&#20803;&#26684;&#21305;&#37197;&#20219;&#21153;&#20013;&#22343;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Without accurate transcription of numerical data in scientific documents, a scientist cannot draw accurate conclusions. Unfortunately, the process of copying numerical data from one paper to another is prone to human error. In this paper, we propose to meet this challenge through the novel task of automatic table verification (AutoTV), in which the objective is to verify the accuracy of numerical data in tables by cross-referencing cited sources. To support this task, we propose a new benchmark, arXiVeri, which comprises tabular data drawn from open-access academic papers on arXiv. We introduce metrics to evaluate the performance of a table verifier in two key areas: (i) table matching, which aims to identify the source table in a cited document that corresponds to a target table, and (ii) cell matching, which aims to locate shared cells between a target and source table and identify their row and column indices accurately. By leveraging the flexible capabilities of modern large langua
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#31639;&#27861;&#8212;&#8212;GLoRA&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#24191;&#20041;&#25552;&#31034;&#27169;&#22359;&#12289;&#27169;&#22359;&#21270;&#30340;&#36866;&#37197;&#22120;&#23618;&#21644;&#21487;&#25193;&#23637;&#30340;&#32467;&#26500;&#25628;&#32034;&#20855;&#26377;&#20102;&#23545;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#26356;&#39640;&#28789;&#27963;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#24182;&#22312;&#21508;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.07967</link><description>&lt;p&gt;
&#19968;&#36890;&#36866;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#36890;&#29992;LoRA&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning. (arXiv:2306.07967v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#31639;&#27861;&#8212;&#8212;GLoRA&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#24191;&#20041;&#25552;&#31034;&#27169;&#22359;&#12289;&#27169;&#22359;&#21270;&#30340;&#36866;&#37197;&#22120;&#23618;&#21644;&#21487;&#25193;&#23637;&#30340;&#32467;&#26500;&#25628;&#32034;&#20855;&#26377;&#20102;&#23545;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#26356;&#39640;&#28789;&#27963;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#24182;&#22312;&#21508;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#36890;&#29992;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#20219;&#21153;&#31639;&#27861;&#8212;&#8212;&#24191;&#20041;LoRA&#65288;GLoRA&#65289;&#12290;GLoRA &#20351;&#29992;&#24191;&#20041;&#25552;&#31034;&#27169;&#22359;&#26469;&#20248;&#21270;&#39044;&#35757;&#32451;&#27169;&#22411;&#26435;&#37325;&#21644;&#35843;&#25972;&#20013;&#38388;&#28608;&#27963;&#29366;&#24577;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#26356;&#22810;&#30340;&#28789;&#27963;&#24615;&#21644;&#36328;&#24322;&#26500;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;GLoRA &#36890;&#36807;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#12289;&#27169;&#22359;&#21270;&#30340;&#12289;&#23618;&#27425;&#30340;&#32467;&#26500;&#25628;&#32034;&#26469;&#24110;&#21161;&#26377;&#25928;&#30340;&#21442;&#25968;&#35843;&#25972;&#65292;&#23398;&#20064;&#27599;&#20010;&#23618;&#30340;&#36866;&#37197;&#22120;&#65292;&#20174;&#19968;&#20010;&#32479;&#19968;&#30340;&#25968;&#23398;&#20844;&#24335;&#36215;&#28304;&#65292;GLoRA &#20855;&#26377;&#24378;&#22823;&#30340;&#36801;&#31227;&#23398;&#20064;&#12289;&#23569;&#26679;&#26412;&#23398;&#20064;&#21644;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#65292;&#36890;&#36807;&#26435;&#37325;&#21644;&#28608;&#27963;&#29366;&#24577;&#19978;&#30340;&#38468;&#21152;&#32500;&#24230;&#26469;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#32508;&#21512;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#33258;&#28982;&#12289;&#19987;&#19994;&#21644;&#32467;&#26500;&#21270;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;GLoRA &#30340;&#31934;&#24230;&#20248;&#20110;&#25152;&#26377;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#19988;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#35745;&#31639;&#36798;&#21040;&#20102;&#20248;&#36234;&#30340;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26500;&#37325;&#26032;&#35774;&#35745;&#21487;&#20197;&#22823;&#24133;&#20943;&#23569;&#36816;&#31639;&#26102;&#38388;&#21644;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Generalized LoRA (GLoRA), an advanced approach for universal parameter-efficient fine-tuning tasks. Enhancing Low-Rank Adaptation (LoRA), GLoRA employs a generalized prompt module to optimize pre-trained model weights and adjust intermediate activations, providing more flexibility and capability across diverse tasks and datasets. Moreover, GLoRA facilitates efficient parameter adaptation by employing a scalable, modular, layer-wise structure search that learns individual adapter of each layer. Originating from a unified mathematical formulation, GLoRA exhibits strong transfer learning, few-shot learning and domain generalization abilities, as it adjusts to new tasks through additional dimensions on weights and activations. Comprehensive experiments demonstrate that GLoRA outperforms all previous methods in natural, specialized, and structured benchmarks, achieving superior accuracy with fewer parameters and computations on various datasets. Furthermore, our structural re-par
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;nuPlan&#65292;&#19968;&#20010;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#26696;&#65292;&#38024;&#23545;&#31934;&#30830;&#30340;&#30701;&#26399;&#35268;&#21010;&#21644;&#38271;&#26399;&#30446;&#26631;&#39044;&#27979;&#12290;&#35777;&#23454;&#20102;&#29616;&#26377;&#31995;&#32479;&#38590;&#20197;&#21516;&#26102;&#28385;&#36275;&#20004;&#20010;&#35201;&#27714;&#12290;&#26368;&#32456;&#25552;&#20986;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#39640;&#25928;&#30340;&#35268;&#21010;&#22120;&#12290;</title><link>http://arxiv.org/abs/2306.07962</link><description>&lt;p&gt;
&#19982;&#23398;&#20064;&#23548;&#21521;&#30340;&#36710;&#36742;&#36816;&#21160;&#35268;&#21010;&#30340;&#35823;&#35299;&#21578;&#21035;
&lt;/p&gt;
&lt;p&gt;
Parting with Misconceptions about Learning-based Vehicle Motion Planning. (arXiv:2306.07962v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07962
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;nuPlan&#65292;&#19968;&#20010;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#26696;&#65292;&#38024;&#23545;&#31934;&#30830;&#30340;&#30701;&#26399;&#35268;&#21010;&#21644;&#38271;&#26399;&#30446;&#26631;&#39044;&#27979;&#12290;&#35777;&#23454;&#20102;&#29616;&#26377;&#31995;&#32479;&#38590;&#20197;&#21516;&#26102;&#28385;&#36275;&#20004;&#20010;&#35201;&#27714;&#12290;&#26368;&#32456;&#25552;&#20986;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#39640;&#25928;&#30340;&#35268;&#21010;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
nuPlan&#30340;&#21457;&#24067;&#26631;&#24535;&#30528;&#36710;&#36742;&#36816;&#21160;&#35268;&#21010;&#30740;&#31350;&#30340;&#19968;&#20010;&#26032;&#26102;&#20195;&#65292;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#38656;&#35201;&#31934;&#30830;&#30340;&#30701;&#26399;&#35268;&#21010;&#21644;&#38271;&#26399;&#30446;&#26631;&#39044;&#27979;&#30340;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#26696;&#12290;&#29616;&#26377;&#31995;&#32479;&#38590;&#20197;&#21516;&#26102;&#28385;&#36275;&#20004;&#20010;&#35201;&#27714;&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#20219;&#21153;&#23384;&#22312;&#26681;&#26412;&#19978;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#24212;&#35813;&#20998;&#21035;&#36827;&#34892;&#35299;&#20915;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;&#39046;&#22495;&#20869;&#38381;&#29615;&#35268;&#21010;&#30340;&#29616;&#29366;&#65292;&#25581;&#31034;&#20102;&#23398;&#20064;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#22312;&#22797;&#26434;&#30340;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#20197;&#21450;&#36873;&#25321;&#36890;&#36807;&#36710;&#36947;&#22270;&#25628;&#32034;&#31639;&#27861;&#30340;&#31616;&#21333;&#22522;&#20110;&#35268;&#21017;&#30340;&#20808;&#39564;&#39033;&#65288;&#20363;&#22914;&#20013;&#24515;&#32447;&#36873;&#25321;&#65289;&#30340;&#20215;&#20540;&#12290;&#26356;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#24320;&#29615;&#23376;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24403;&#20165;&#20351;&#29992;&#36825;&#20010;&#20013;&#24515;&#32447;&#20316;&#20026;&#22330;&#26223;&#19978;&#19979;&#25991;&#26102;&#65288;&#21363;&#24573;&#30053;&#25152;&#26377;&#26377;&#20851;&#22320;&#22270;&#21644;&#20854;&#20182;&#20195;&#29702;&#30340;&#20449;&#24687;&#65289;&#21487;&#20197;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;&#32467;&#21512;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#39640;&#25928;&#30340;&#35268;&#21010;&#22120;&#65292;&#23427;&#30340;&#34920;&#29616;&#20248;&#20110;&#22823;&#37327;&#31454;&#20105;&#23545;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
The release of nuPlan marks a new era in vehicle motion planning research, offering the first large-scale real-world dataset and evaluation schemes requiring both precise short-term planning and long-horizon ego-forecasting. Existing systems struggle to simultaneously meet both requirements. Indeed, we find that these tasks are fundamentally misaligned and should be addressed independently. We further assess the current state of closed-loop planning in the field, revealing the limitations of learning-based methods in complex real-world scenarios and the value of simple rule-based priors such as centerline selection through lane graph search algorithms. More surprisingly, for the open-loop sub-task, we observe that the best results are achieved when using only this centerline as scene context (\ie, ignoring all information regarding the map and other agents). Combining these insights, we propose an extremely simple and efficient planner which outperforms an extensive set of competitors,
&lt;/p&gt;</description></item><item><title>&#31471;&#21040;&#31471;&#39550;&#39542;&#27169;&#22411;&#23384;&#22312;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#27425;&#35201;&#32452;&#20214;&#30340;&#26356;&#25913;&#12290;&#26412;&#25991;&#38024;&#23545;&#30446;&#21069;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20013;&#25152;&#23384;&#22312;&#30340;&#20004;&#31181;&#20559;&#35265;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#21512;&#29702;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#24320;&#21457;&#20102;TF ++&#65292;&#22312;CARLA&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.07957</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#39550;&#39542;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Hidden Biases of End-to-End Driving Models. (arXiv:2306.07957v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07957
&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#39550;&#39542;&#27169;&#22411;&#23384;&#22312;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#27425;&#35201;&#32452;&#20214;&#30340;&#26356;&#25913;&#12290;&#26412;&#25991;&#38024;&#23545;&#30446;&#21069;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20013;&#25152;&#23384;&#22312;&#30340;&#20004;&#31181;&#20559;&#35265;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#21512;&#29702;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#24320;&#21457;&#20102;TF ++&#65292;&#22312;CARLA&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#31471;&#21040;&#31471;&#30340;&#39550;&#39542;&#31995;&#32479;&#22312;CARLA&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#22312;&#20027;&#35201;&#36129;&#29486;&#30340;&#22522;&#30784;&#19978;&#65292;&#36825;&#20123;&#31995;&#32479;&#20063;&#20250;&#24341;&#20837;&#23545;&#27425;&#35201;&#31995;&#32479;&#32452;&#20214;&#30340;&#25913;&#21464;&#12290;&#22240;&#27492;&#65292;&#31995;&#32479;&#30340;&#25913;&#36827;&#28304;&#24182;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#20960;&#20046;&#25152;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20013;&#37117;&#23384;&#22312;&#20004;&#31181;&#20559;&#35265;&#65292;&#36825;&#20123;&#20559;&#35265;&#23545;&#20110;&#22312;CARLA&#19978;&#35266;&#23519;&#21040;&#30340;&#36827;&#23637;&#33267;&#20851;&#37325;&#35201;&#65306;(1) &#36890;&#36807;&#23545;&#30446;&#26631;&#28857;&#36319;&#38543;&#30340;&#24378;&#24402;&#32435;&#20559;&#35265;&#26469;&#36827;&#34892;&#27178;&#21521;&#24674;&#22797;&#65292;(2) &#36890;&#36807;&#22810;&#27169;&#24577;&#33322;&#36335;&#28857;&#39044;&#27979;&#30340;&#32437;&#21521;&#24179;&#22343;&#26469;&#20943;&#36895;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#20559;&#35265;&#30340;&#32570;&#28857;&#65292;&#24182;&#30830;&#23450;&#20102;&#21512;&#29702;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#36890;&#36807;&#32467;&#21512;&#25105;&#20204;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;TF ++&#65292;&#19968;&#31181;&#31616;&#21333;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#22312;Longest6&#21644;LAV&#22522;&#20934;&#27979;&#35797;&#20013;&#25490;&#21517;&#31532;&#19968;&#65292;&#22312;Longest6&#19978;&#27604;&#26368;&#20339;&#21069;&#26399;&#24037;&#20316;&#25552;&#39640;&#20102;14&#20010;&#39550;&#39542;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end driving systems have recently made rapid progress, in particular on CARLA. Independent of their major contribution, they introduce changes to minor system components. Consequently, the source of improvements is unclear. We identify two biases that recur in nearly all state-of-the-art methods and are critical for the observed progress on CARLA: (1) lateral recovery via a strong inductive bias towards target point following, and (2) longitudinal averaging of multimodal waypoint predictions for slowing down. We investigate the drawbacks of these biases and identify principled alternatives. By incorporating our insights, we develop TF++, a simple end-to-end method that ranks first on the Longest6 and LAV benchmarks, gaining 14 driving score over the best prior work on Longest6.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#33945;&#29305;&#21345;&#32599;&#25628;&#32034;&#31639;&#27861;&#65292;&#29992;&#20110;&#21453;&#39539;&#29468;&#24819;&#24182;&#35777;&#26126;&#22270;&#35770;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#22312;&#21453;&#39539;&#22810;&#20010;&#29468;&#24819;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#20248;&#20110;&#24050;&#26377;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.07956</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#33945;&#29305;&#21345;&#32599;&#25628;&#32034;&#29992;&#20110;&#22270;&#35770;&#29468;&#24819;&#35777;&#20266;
&lt;/p&gt;
&lt;p&gt;
Adaptive Monte Carlo Search for Conjecture Refutation in Graph Theory. (arXiv:2306.07956v1 [math.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#33945;&#29305;&#21345;&#32599;&#25628;&#32034;&#31639;&#27861;&#65292;&#29992;&#20110;&#21453;&#39539;&#29468;&#24819;&#24182;&#35777;&#26126;&#22270;&#35770;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#22312;&#21453;&#39539;&#22810;&#20010;&#29468;&#24819;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#20248;&#20110;&#24050;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#35770;&#26159;&#19968;&#20010;&#36328;&#23398;&#31185;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#20855;&#26377;&#22312;&#25968;&#23398;&#24314;&#27169;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#22270;&#35770;&#30740;&#31350;&#19981;&#20165;&#20165;&#20381;&#36182;&#20110;&#23450;&#29702;&#30340;&#21019;&#36896;&#65292;&#36824;&#28041;&#21450;&#21040;&#29468;&#24819;&#30340;&#25552;&#20986;&#12290;&#35777;&#20266;&#31639;&#27861;&#36890;&#36807;&#22312;&#22270;&#19978;&#26368;&#22823;&#21270;&#26576;&#20123;&#24471;&#20998;&#20989;&#25968;&#26469;&#23547;&#25214;&#21453;&#20363;&#65292;&#20174;&#32780;&#35797;&#22270;&#35777;&#20266;&#29468;&#24819;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29468;&#24819;&#35777;&#20266;&#31639;&#27861;&#65292;&#31216;&#20026;&#33258;&#36866;&#24212;&#33945;&#29305;&#21345;&#32599;&#25628;&#32034;&#65288;AMCS&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#31639;&#27861;&#24471;&#21040;&#12290;&#36890;&#36807;&#23545;&#20854;&#22312;&#21457;&#29616;&#25968;&#20010;&#22270;&#35770;&#29468;&#24819;&#30340;&#21453;&#20363;&#26041;&#38754;&#30340;&#25104;&#21151;&#35780;&#20272;&#65292;AMCS&#20248;&#20110;&#29616;&#26377;&#30340;&#29468;&#24819;&#35777;&#20266;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#36824;&#34987;&#29992;&#20110;&#35777;&#20266;&#20102;&#20845;&#20010;&#24320;&#25918;&#29468;&#24819;&#65292;&#20854;&#20013;&#20004;&#20010;&#26159;&#30001;Liu&#31561;&#20154;&#20110;2021&#24180;&#25552;&#20986;&#30340;&#21270;&#23398;&#22270;&#35770;&#29468;&#24819;&#65292;&#21478;&#22806;&#22235;&#20010;&#26159;AutoGraphiX&#35745;&#31639;&#26426;&#31995;&#32479;&#20110;2006&#24180;&#25552;&#20986;&#30340;&#29468;&#24819;&#12290;&#26368;&#21518;&#65292;&#20351;&#29992;AMCS&#35777;&#26126;&#20102;&#20854;&#20013;&#22235;&#20010;&#24320;&#25918;&#29468;&#24819;.
&lt;/p&gt;
&lt;p&gt;
Graph theory is an interdisciplinary field of study that has various applications in mathematical modeling and computer science. Research in graph theory depends on the creation of not only theorems but also conjectures. Conjecture-refuting algorithms attempt to refute conjectures by searching for counterexamples to those conjectures, often by maximizing certain score functions on graphs. This study proposes a novel conjecture-refuting algorithm, referred to as the adaptive Monte Carlo search (AMCS) algorithm, obtained by modifying the Monte Carlo tree search algorithm. Evaluated based on its success in finding counterexamples to several graph theory conjectures, AMCS outperforms existing conjecture-refuting algorithms. The algorithm is further utilized to refute six open conjectures, two of which were chemical graph theory conjectures formulated by Liu et al. in 2021 and four of which were formulated by the AutoGraphiX computer system in 2006. Finally, four of the open conjectures are
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#22312;&#36830;&#25509;&#26102;&#24207;&#20998;&#31867;&#65288;CTC&#65289;&#25439;&#22833;&#20013;&#24341;&#20837;&#26631;&#31614;&#20808;&#39564;&#65292;&#24182;&#23558;&#20302;&#23618;Mel-scale&#28388;&#27874;&#22120;&#21644;&#39640;&#23618;ASR&#32534;&#30721;&#22120;&#36755;&#20986;&#32452;&#21512;&#20316;&#20026;&#36755;&#20837;&#29305;&#24449;&#26469;&#25913;&#36827;E2E&#31995;&#32479;&#20013;&#29992;&#20110;&#35789;&#26102;&#30340;&#24103;&#32423;&#20998;&#31867;&#22120;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#35789;&#26102;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07949</link><description>&lt;p&gt;
&#38750;&#23792;&#20540;CTC&#25552;&#21319;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#35789;&#26102;&#20998;&#31867;&#22120;&#30340;&#24103;&#32423;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Improving Frame-level Classifier for Word Timings with Non-peaky CTC in End-to-End Automatic Speech Recognition. (arXiv:2306.07949v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#22312;&#36830;&#25509;&#26102;&#24207;&#20998;&#31867;&#65288;CTC&#65289;&#25439;&#22833;&#20013;&#24341;&#20837;&#26631;&#31614;&#20808;&#39564;&#65292;&#24182;&#23558;&#20302;&#23618;Mel-scale&#28388;&#27874;&#22120;&#21644;&#39640;&#23618;ASR&#32534;&#30721;&#22120;&#36755;&#20986;&#32452;&#21512;&#20316;&#20026;&#36755;&#20837;&#29305;&#24449;&#26469;&#25913;&#36827;E2E&#31995;&#32479;&#20013;&#29992;&#20110;&#35789;&#26102;&#30340;&#24103;&#32423;&#20998;&#31867;&#22120;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#35789;&#26102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#31995;&#32479;&#24050;&#32463;&#26174;&#31034;&#20986;&#19982;&#28151;&#21512;&#31995;&#32479;&#30456;&#24403;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#24615;&#33021;&#12290;&#20316;&#20026;ASR&#30340;&#21103;&#20135;&#21697;&#65292;&#35789;&#30340;&#23450;&#26102;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#23383;&#24149;&#21644;&#35745;&#31639;&#26426;&#36741;&#21161;&#21457;&#38899;&#35757;&#32451;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#36830;&#25509;&#26102;&#24207;&#20998;&#31867;&#65288;CTC&#65289;&#25439;&#22833;&#20013;&#24341;&#20837;&#26631;&#31614;&#20808;&#39564;&#65292;&#24182;&#23558;&#20302;&#23618;Mel-scale&#28388;&#27874;&#22120;&#21644;&#39640;&#23618;ASR&#32534;&#30721;&#22120;&#36755;&#20986;&#32452;&#21512;&#20316;&#20026;&#36755;&#20837;&#29305;&#24449;&#65292;&#25913;&#36827;&#20102;E2E&#31995;&#32479;&#20013;&#29992;&#20110;&#35789;&#26102;&#30340;&#24103;&#32423;&#20998;&#31867;&#22120;&#12290;&#22312;&#20869;&#37096;&#27721;&#35821;&#35821;&#26009;&#24211;&#19978;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#27604;&#28151;&#21512;&#31995;&#32479;&#22312;&#21333;&#35789;&#26102;&#24207;&#20934;&#30830;&#24615;&#24230;&#37327;&#19978;&#23454;&#29616;&#20102; 95.68% / 94.18% &#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#22312;7&#31181;&#35821;&#35328;&#30340;&#25351;&#26631;&#19978;&#32477;&#23545;&#25552;&#39640;&#20102;4.80% / 8.02% &#30340;&#20998;&#25968;&#65292;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;E2E&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#24310;&#36831;CTC&#23792;&#20540;&#21644;&#22522;&#20110;&#24103;&#30340;&#30693;&#35782;&#33976;&#39311;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#35789;&#26102;&#20934;&#30830;&#24615;&#65292;&#20294;&#20165;&#22312;LibriSpeech&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end (E2E) systems have shown comparable performance to hybrid systems for automatic speech recognition (ASR). Word timings, as a by-product of ASR, are essential in many applications, especially for subtitling and computer-aided pronunciation training. In this paper, we improve the frame-level classifier for word timings in E2E system by introducing label priors in connectionist temporal classification (CTC) loss, which is adopted from prior works, and combining low-level Mel-scale filter banks with high-level ASR encoder output as input feature. On the internal Chinese corpus, the proposed method achieves 95.68%/94.18% compared to the hybrid system 93.0%/90.22% on the word timing accuracy metrics. It also surpass a previous E2E approach with an absolute increase of 4.80%/8.02% on the metrics on 7 languages. In addition, we further improve word timing accuracy by delaying CTC peaks with frame-wise knowledge distillation, though only experimenting on LibriSpeech.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31038;&#20132;&#24863;&#30693;&#21644;&#26102;&#38388;&#22240;&#32032;&#30340;&#35299;&#30721;&#22120;&#25512;&#33616;&#31995;&#32479;(STUDY)&#65292;&#20351;&#29992;transformer&#35299;&#30721;&#22120;&#32593;&#32476;&#23454;&#29616;&#23545;&#31038;&#20132;&#32593;&#32476;&#22270;&#20013;&#30456;&#37051;&#30340;&#29992;&#25143;&#32452;&#30340;&#32852;&#21512;&#25512;&#26029;&#12290;&#35813;&#26041;&#27861;&#22312;&#25945;&#32946;&#20869;&#23481;&#39046;&#22495;&#20013;&#32463;&#36807;&#27979;&#35797;&#65292;&#33021;&#22815;&#21462;&#24471;&#20248;&#20110;&#31038;&#20132;&#21644;&#39034;&#24207;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.07946</link><description>&lt;p&gt;
&#30740;&#31350;&#65306;&#31038;&#20132;&#24863;&#30693;&#26102;&#38388;&#26494;&#25955;&#35299;&#30721;&#22120;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
STUDY: Socially Aware Temporally Casual Decoder Recommender Systems. (arXiv:2306.07946v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07946
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31038;&#20132;&#24863;&#30693;&#21644;&#26102;&#38388;&#22240;&#32032;&#30340;&#35299;&#30721;&#22120;&#25512;&#33616;&#31995;&#32479;(STUDY)&#65292;&#20351;&#29992;transformer&#35299;&#30721;&#22120;&#32593;&#32476;&#23454;&#29616;&#23545;&#31038;&#20132;&#32593;&#32476;&#22270;&#20013;&#30456;&#37051;&#30340;&#29992;&#25143;&#32452;&#30340;&#32852;&#21512;&#25512;&#26029;&#12290;&#35813;&#26041;&#27861;&#22312;&#25945;&#32946;&#20869;&#23481;&#39046;&#22495;&#20013;&#32463;&#36807;&#27979;&#35797;&#65292;&#33021;&#22815;&#21462;&#24471;&#20248;&#20110;&#31038;&#20132;&#21644;&#39034;&#24207;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#22312;&#22312;&#32447;&#21644;&#31163;&#32447;&#21487;&#33719;&#21462;&#30340;&#25968;&#25454;&#25968;&#37327;&#36807;&#20110;&#24222;&#22823;&#65292;&#25512;&#33616;&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#24517;&#35201;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#25214;&#21040;&#31526;&#21512;&#20182;&#20204;&#20852;&#36259;&#30340;&#29289;&#21697;&#12290;&#24403;&#31038;&#20132;&#32593;&#32476;&#20449;&#24687;&#23384;&#22312;&#26102;&#65292;&#26377;&#19968;&#20123;&#26041;&#27861;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#26469;&#20570;&#20986;&#26356;&#22909;&#30340;&#25512;&#33616;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#26377;&#22797;&#26434;&#30340;&#32467;&#26500;&#21644;&#35757;&#32451;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#26041;&#27861;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#32780;&#36825;&#20123;&#32593;&#32476;&#35757;&#32451;&#36215;&#26469;&#38750;&#24120;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#31038;&#20132;&#24863;&#30693;&#21644;&#26102;&#38388;&#22240;&#32032;&#30340;&#35299;&#30721;&#22120;&#25512;&#33616;&#31995;&#32479;(STUDY)&#12290;STUDY&#37319;&#29992;&#19968;&#20010;&#32463;&#36807;&#20462;&#25913;&#30340;transformer&#35299;&#30721;&#22120;&#32593;&#32476;&#30340;&#21333;&#21521;&#21069;&#20256;&#65292;&#23545;&#31038;&#20132;&#32593;&#32476;&#22270;&#20013;&#30456;&#37051;&#30340;&#29992;&#25143;&#32452;&#36827;&#34892;&#32852;&#21512;&#25512;&#26029;&#12290;&#25105;&#20204;&#22312;&#22522;&#20110;&#23398;&#26657;&#35838;&#22530;&#32467;&#26500;&#23450;&#20041;&#31038;&#20132;&#32593;&#32476;&#30340;&#25945;&#32946;&#20869;&#23481;&#39046;&#22495;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;&#21333;&#19968;&#22343;&#21248;&#32593;&#32476;&#35774;&#35745;&#31616;&#21333;&#24615;&#30340;&#21516;&#26102;&#65292;&#20248;&#20110;&#31038;&#20132;&#21644;&#39034;&#24207;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the overwhelming amount of data available both on and offline today, recommender systems have become much needed to help users find items tailored to their interests. When social network information exists there are methods that utilize this information to make better recommendations, however the methods are often clunky with complex architectures and training procedures. Furthermore many of the existing methods utilize graph neural networks which are notoriously difficult to train. To address this, we propose Socially-aware Temporally caUsal Decoder recommender sYstems (STUDY). STUDY does joint inference over groups of users who are adjacent in the social network graph using a single forward pass of a modified transformer decoder network. We test our method in a school-based educational content setting, using classroom structure to define social networks. Our method outperforms both social and sequential methods while maintaining the design simplicity of a single homogeneous netw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#35821;&#38899;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;SLM&#65289;&#65292;&#23558;&#22768;&#38899;&#26144;&#23556;&#21040;&#25991;&#26412;&#23884;&#20837;&#24335;&#31354;&#38388;&#65292;&#20351;&#29992;&#22522;&#20110;CTC&#30340;&#31354;&#30333;&#36807;&#28388;&#22120;&#26469;&#32553;&#30701;&#35821;&#38899;&#24207;&#21015;&#38271;&#24230;&#12290;&#22312;&#35821;&#38899;MultiWoz&#25968;&#25454;&#38598;&#20013;&#65292;SLM&#25552;&#39640;&#20102;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#65288;DST&#65289;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#31232;&#26377;&#23454;&#20307;&#30340;&#38169;&#35823;&#65292;&#25105;&#20204;&#37319;&#29992;Speech2Entity&#26816;&#32034;&#22120;&#22686;&#24378;SLM&#12290;&#20351;&#29992;&#27492;&#26816;&#32034;-augmented SLM&#65288;ReSLM&#65289;&#65292;DST&#24615;&#33021;&#24471;&#21040;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;&#35813;&#30740;&#31350;&#34920;&#26126;&#65292;&#22686;&#24378;ASR&#20219;&#21153;&#21487;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07944</link><description>&lt;p&gt;
&#20351;&#29992;Speech2Text&#36866;&#37197;&#22120;&#21644;Speech2Entity&#26816;&#32034;&#22686;&#24378;LLM&#30340;&#35821;&#38899;&#29702;&#35299;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Speech-to-Text Adapter and Speech-to-Entity Retriever Augmented LLMs for Speech Understanding. (arXiv:2306.07944v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#35821;&#38899;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;SLM&#65289;&#65292;&#23558;&#22768;&#38899;&#26144;&#23556;&#21040;&#25991;&#26412;&#23884;&#20837;&#24335;&#31354;&#38388;&#65292;&#20351;&#29992;&#22522;&#20110;CTC&#30340;&#31354;&#30333;&#36807;&#28388;&#22120;&#26469;&#32553;&#30701;&#35821;&#38899;&#24207;&#21015;&#38271;&#24230;&#12290;&#22312;&#35821;&#38899;MultiWoz&#25968;&#25454;&#38598;&#20013;&#65292;SLM&#25552;&#39640;&#20102;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#65288;DST&#65289;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#31232;&#26377;&#23454;&#20307;&#30340;&#38169;&#35823;&#65292;&#25105;&#20204;&#37319;&#29992;Speech2Entity&#26816;&#32034;&#22120;&#22686;&#24378;SLM&#12290;&#20351;&#29992;&#27492;&#26816;&#32034;-augmented SLM&#65288;ReSLM&#65289;&#65292;DST&#24615;&#33021;&#24471;&#21040;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;&#35813;&#30740;&#31350;&#34920;&#26126;&#65292;&#22686;&#24378;ASR&#20219;&#21153;&#21487;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#24212;&#29992;&#20110;&#35821;&#38899;&#39046;&#22495;&#65292;&#20294;&#24448;&#24448;&#30001;&#20110;&#38899;&#39057;&#21644;&#35821;&#35328;&#34920;&#31034;&#19981;&#21305;&#37197;&#32780;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#32570;&#38519;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#35821;&#38899;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;SLM&#65289;&#65292;&#37319;&#29992;Speech2Text&#36866;&#37197;&#22120;&#23558;&#22768;&#38899;&#26144;&#23556;&#21040;&#25991;&#26412;&#23884;&#20837;&#24335;&#31354;&#38388;&#65292;&#36991;&#20813;&#22768;&#38899;&#20449;&#24687;&#20002;&#22833;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#22522;&#20110;CTC&#30340;&#31354;&#30333;&#36807;&#28388;&#22120;&#21487;&#20197;&#23558;&#35821;&#38899;&#24207;&#21015;&#38271;&#24230;&#32553;&#30701;&#33267;&#25991;&#26412;&#38271;&#24230;&#12290;&#22312;DSTC11&#25361;&#25112;&#36187;&#30340;&#35821;&#38899;MultiWoz&#25968;&#25454;&#38598;&#20013;&#65292;SLM&#26174;&#30528;&#25552;&#39640;&#20102;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#65288;DST&#65289;&#24615;&#33021;&#65288;&#20174;24.7&#65285;&#25552;&#39640;&#21040;28.4&#65285;&#30340;&#20934;&#30830;&#29575;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#31232;&#26377;&#23454;&#20307;&#30340;&#38169;&#35823;&#65292;&#25105;&#20204;&#37319;&#29992;Speech2Entity&#26816;&#32034;&#22120;&#22686;&#24378;SLM&#65292;&#35813;&#26816;&#32034;&#22120;&#20351;&#29992;&#35821;&#38899;&#26816;&#32034;&#30456;&#20851;&#23454;&#20307;&#65292;&#24182;&#23558;&#23427;&#20204;&#28155;&#21152;&#21040;&#21407;&#22987;SLM&#36755;&#20837;&#20013;&#20316;&#20026;&#21069;&#32512;&#12290;&#20351;&#29992;&#36825;&#31181;&#26816;&#32034;-augmented SLM&#65288;ReSLM&#65289;&#65292;DST&#30340;&#24615;&#33021;&#25552;&#39640;&#33267;34.6&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;&#27492;&#22806;&#65292;&#20197;&#23545;&#35805;&#29702;&#35299;&#20219;&#21153;&#22686;&#24378;ASR&#20219;&#21153;&#21487;&#20197;&#23558;ASR&#24615;&#33021;&#20174;9.4&#65285;&#25552;&#39640;&#21040;8.5&#65285;&#30340;&#35789;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have been applied in the speech domain, often incurring a performance drop due to misaligned between speech and language representations. To bridge this gap, we propose a joint speech and language model (SLM) using a Speech2Text adapter, which maps speech into text token embedding space without speech information loss. Additionally, using a CTC-based blank-filtering, we can reduce the speech sequence length to that of text. In speech MultiWoz dataset (DSTC11 challenge), SLM largely improves the dialog state tracking (DST) performance (24.7% to 28.4% accuracy). Further to address errors on rare entities, we augment SLM with a Speech2Entity retriever, which uses speech to retrieve relevant entities, and then adds them to the original SLM input as a prefix. With this retrieval-augmented SLM (ReSLM), the DST performance jumps to 34.6% accuracy. Moreover, augmenting the ASR task with the dialog understanding task improves the ASR performance from 9.4% to 8.5% WE
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65288;MRLF&#65289;&#65292;&#29992;&#20110;&#23558;&#31038;&#20132;&#23186;&#20307;&#25991;&#31456;&#30340;&#19981;&#21516;&#27169;&#24577;&#34701;&#21512;&#36215;&#26469;&#36827;&#34892;&#20301;&#32622;&#25512;&#26029;&#12290;&#35813;&#26041;&#27861;&#38598;&#25104;&#20102;&#19968;&#20010;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#20197;&#22686;&#24378;&#20301;&#32622;&#26174;&#33879;&#20449;&#24687;&#25552;&#21462;&#65292;&#30456;&#23545;&#20110;&#21333;&#19968;&#39046;&#22495;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20301;&#32622;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07935</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#31038;&#20132;&#23186;&#20307;&#25991;&#31456;&#20301;&#32622;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Representation Learning for Social Post Location Inference. (arXiv:2306.07935v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65288;MRLF&#65289;&#65292;&#29992;&#20110;&#23558;&#31038;&#20132;&#23186;&#20307;&#25991;&#31456;&#30340;&#19981;&#21516;&#27169;&#24577;&#34701;&#21512;&#36215;&#26469;&#36827;&#34892;&#20301;&#32622;&#25512;&#26029;&#12290;&#35813;&#26041;&#27861;&#38598;&#25104;&#20102;&#19968;&#20010;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#20197;&#22686;&#24378;&#20301;&#32622;&#26174;&#33879;&#20449;&#24687;&#25552;&#21462;&#65292;&#30456;&#23545;&#20110;&#21333;&#19968;&#39046;&#22495;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20301;&#32622;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#25991;&#31456;&#26469;&#25512;&#26029;&#22320;&#29702;&#20301;&#32622;&#23545;&#20110;&#35768;&#22810;&#23454;&#38469;&#30340;&#22522;&#20110;&#20301;&#32622;&#30340;&#24212;&#29992;&#31243;&#24207;&#38750;&#24120;&#37325;&#35201;&#65292;&#20363;&#22914;&#20135;&#21697;&#33829;&#38144;&#12289;&#20852;&#36259;&#28857;&#25512;&#33616;&#21450;COVID-19&#30340;&#36861;&#36394;&#12290;&#26412;&#30740;&#31350;&#20174;Instagram&#25910;&#38598;&#20102;&#24102;&#26377;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#26631;&#31614;&#30340;&#23454;&#38469;&#31038;&#20132;&#23186;&#20307;&#25991;&#31456;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65288;MRLF&#65289;&#65292;&#23427;&#33021;&#22815;&#23558;&#31038;&#20132;&#23186;&#20307;&#25991;&#31456;&#30340;&#19981;&#21516;&#27169;&#24577;&#34701;&#21512;&#36215;&#26469;&#36827;&#34892;&#20301;&#32622;&#25512;&#26029;&#12290;MRLF&#38598;&#25104;&#20102;&#19968;&#20010;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#20197;&#22686;&#24378;&#20301;&#32622;&#26174;&#33879;&#20449;&#24687;&#25552;&#21462;&#65292;&#21516;&#26102;&#30456;&#23545;&#20110;&#21333;&#19968;&#39046;&#22495;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20301;&#32622;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#29992;&#25143;&#29983;&#25104;&#30340;&#25991;&#26412;&#20869;&#23481;&#30340;&#22122;&#22768;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#23383;&#31526;&#24863;&#30693;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inferring geographic locations via social posts is essential for many practical location-based applications such as product marketing, point-of-interest recommendation, and infector tracking for COVID-19. Unlike image-based location retrieval or social-post text embedding-based location inference, the combined effect of multi-modal information (i.e., post images, text, and hashtags) for social post positioning receives less attention. In this work, we collect real datasets of social posts with images, texts, and hashtags from Instagram and propose a novel Multi-modal Representation Learning Framework (MRLF) capable of fusing different modalities of social posts for location inference. MRLF integrates a multi-head attention mechanism to enhance location-salient information extraction while significantly improving location inference compared with single domain-based methods. To overcome the noisy user-generated textual content, we introduce a novel attention-based character-aware module 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#24102;&#26377;&#30683;&#30462;&#20449;&#24687;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#25512;&#29702;&#20013;&#19981;&#19968;&#33268;&#21644;&#30683;&#30462;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25512;&#32763;&#25512;&#29702;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.07934</link><description>&lt;p&gt;
BoardgameQA: &#19968;&#31181;&#29992;&#20110;&#24102;&#26377;&#30683;&#30462;&#20449;&#24687;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information. (arXiv:2306.07934v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#24102;&#26377;&#30683;&#30462;&#20449;&#24687;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#25512;&#29702;&#20013;&#19981;&#19968;&#33268;&#21644;&#30683;&#30462;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25512;&#32763;&#25512;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#30340;&#33258;&#21160;&#25512;&#29702;&#26159;&#35768;&#22810;&#28508;&#22312;NLP&#24212;&#29992;&#21644;&#24320;&#21457;&#24378;&#22823;AI&#31995;&#32479;&#30340;&#20851;&#38190;&#35201;&#27714;&#12290;&#26412;&#25991;&#38024;&#23545;&#23384;&#22312;&#19981;&#19968;&#33268;&#21644;&#30683;&#30462;&#20449;&#24687;&#30340;&#25512;&#29702;&#38382;&#39064;&#25552;&#20986;&#19968;&#31181;&#35299;&#20915;&#31574;&#30053;&#65292;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#32463;&#20856;&#30340;&#21487;&#25512;&#32763;&#25512;&#29702;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated reasoning with unstructured natural text is a key requirement for many potential applications of NLP and for developing robust AI systems. Recently, Language Models (LMs) have demonstrated complex reasoning capacities even without any finetuning. However, existing evaluation for automated reasoning assumes access to a consistent and coherent set of information over which models reason. When reasoning in the real-world, the available information is frequently inconsistent or contradictory, and therefore models need to be equipped with a strategy to resolve such conflicts when they arise. One widely-applicable way of resolving conflicts is to impose preferences over information sources (e.g., based on source credibility or information recency) and adopt the source with higher preference. In this paper, we formulate the problem of reasoning with contradictory information guided by preferences over sources as the classical problem of defeasible reasoning, and develop a dataset ca
&lt;/p&gt;</description></item><item><title>&#25991;&#31456;&#25552;&#20986;&#36890;&#36807;fine-tune&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#30005;&#20449;&#39046;&#22495;&#33258;&#21160;&#21270;&#25805;&#20316;&#65292;&#20197;&#20415;&#35782;&#21035;3GPP&#26631;&#20934;&#24037;&#20316;&#32452;&#12290;</title><link>http://arxiv.org/abs/2306.07933</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#30005;&#20449;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Understanding Telecom Language Through Large Language Models. (arXiv:2306.07933v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07933
&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#25552;&#20986;&#36890;&#36807;fine-tune&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#30005;&#20449;&#39046;&#22495;&#33258;&#21160;&#21270;&#25805;&#20316;&#65292;&#20197;&#20415;&#35782;&#21035;3GPP&#26631;&#20934;&#24037;&#20316;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#20026;&#33258;&#21160;&#21270;&#30005;&#20449;&#32593;&#32476;&#30340;&#35768;&#22810;&#20219;&#21153;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;&#30001;&#20110;&#20986;&#29616;&#20102;&#21253;&#25324;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20869;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65292;&#36825;&#19968;&#36827;&#23637;&#36827;&#19968;&#27493;&#25512;&#21160;&#20102;&#36825;&#19968;&#21487;&#33021;&#24615;&#30340;&#23454;&#29616;&#65292;&#36825;&#34987;&#35748;&#20026;&#26159;&#23454;&#29616;&#33258;&#25105;&#27835;&#29702;&#21644;&#20114;&#21160;&#24335;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#22522;&#30707;&#12290;&#26412;&#25991;&#26088;&#22312;&#23558;LLMs&#30340;&#33539;&#20363;&#24212;&#29992;&#20110;&#30005;&#20449;&#39046;&#22495;&#65292;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;fine-tune&#20960;&#20010;LLMs&#65292;&#21253;&#25324;BERT&#12289;Distilled BERT&#12289;RoBERTa&#21644;GPT-2&#65292;&#20197;&#36866;&#24212;&#30005;&#20449;&#39046;&#22495;&#30340;&#35821;&#35328;&#65292;&#24182;&#28436;&#31034;&#29992;&#20363;&#26469;&#35782;&#21035;&#31532;&#19977;&#20195;&#21512;&#20316;&#20249;&#20276;&#39033;&#30446;&#65288;3GPP&#65289;&#26631;&#20934;&#24037;&#20316;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent progress of artificial intelligence (AI) opens up new frontiers in the possibility of automating many tasks involved in Telecom networks design, implementation, and deployment. This has been further pushed forward with the evolution of generative artificial intelligence (AI), including the emergence of large language models (LLMs), which is believed to be the cornerstone toward realizing self-governed, interactive AI agents. Motivated by this, in this paper, we aim to adapt the paradigm of LLMs to the Telecom domain. In particular, we fine-tune several LLMs including BERT, distilled BERT, RoBERTa and GPT-2, to the Telecom domain languages, and demonstrate a use case for identifying the 3rd Generation Partnership Project (3GPP) standard working groups. We consider training the selected models on 3GPP technical documents (Tdoc) pertinent to years 2009-2019 and predict the Tdoc categories in years 2020-2023. The results demonstrate that fine-tuning BERT and RoBERTa model achiev
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20154;&#22312;&#24490;&#29615;&#38142;&#20013;&#30340;&#26041;&#24335;&#65292;&#25163;&#21160;&#26657;&#27491;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#25506;&#31350;&#29702;&#24615;&#20013;&#23376;&#36923;&#36753;&#30340;&#25163;&#21160;&#26657;&#27491;&#26469;&#25552;&#39640;LLM&#30340;&#25512;&#29702;&#24615;&#33021;&#65292;&#24182;&#19988;&#22522;&#20110;&#32463;&#27982;&#29702;&#35770;&#30340;CAMLOP&#21487;&#20197;&#24179;&#34913;&#25928;&#29992;&#21644;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.07932</link><description>&lt;p&gt;
&#20154;&#22312;&#24490;&#29615;&#38142;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-in-the-Loop through Chain-of-Thought. (arXiv:2306.07932v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07932
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20154;&#22312;&#24490;&#29615;&#38142;&#20013;&#30340;&#26041;&#24335;&#65292;&#25163;&#21160;&#26657;&#27491;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#25506;&#31350;&#29702;&#24615;&#20013;&#23376;&#36923;&#36753;&#30340;&#25163;&#21160;&#26657;&#27491;&#26469;&#25552;&#39640;LLM&#30340;&#25512;&#29702;&#24615;&#33021;&#65292;&#24182;&#19988;&#22522;&#20110;&#32463;&#27982;&#29702;&#35770;&#30340;CAMLOP&#21487;&#20197;&#24179;&#34913;&#25928;&#29992;&#21644;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#20986;&#29616;&#20351;&#33258;&#21160;&#21270;&#21464;&#24471;&#36234;&#26469;&#36234;&#26080;&#22788;&#19981;&#22312;&#65292;&#20294;&#26377;&#26102;&#22312;&#38271;&#26399;&#25110;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#26174;&#31034;&#20986;&#20854;&#24369;&#28857;&#12290;&#20363;&#22914;&#65292;&#29992;&#25143;&#22312;&#27809;&#26377;&#20154;&#31867;&#21442;&#19982;&#30340;&#24773;&#20917;&#19979;&#19981;&#24635;&#33021;&#24471;&#21040;&#22797;&#26434;&#25968;&#23398;&#38382;&#39064;&#30340;&#29702;&#24819;&#31572;&#26696;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25163;&#21160;&#26657;&#27491;&#31995;&#32479;&#65288;MCS&#65289;&#8212;&#8212;&#19968;&#20010;&#36890;&#36807;&#24605;&#32500;&#38142;&#25552;&#31034;&#22686;&#24378;&#30340;&#20154;&#24037;&#21442;&#19982;&#31995;&#32479;&#65292;&#25506;&#31350;&#20102;&#29702;&#24615;&#20013;&#23376;&#36923;&#36753;&#30340;&#25163;&#21160;&#26657;&#27491;&#22914;&#20309;&#25552;&#39640;LLM&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;&#26356;&#36827;&#19968;&#27493;&#32771;&#34385;&#21040;&#26377;&#20154;&#21442;&#19982;&#30340;&#31995;&#32479;&#19981;&#20165;&#35201;&#25552;&#39640;&#24615;&#33021;&#65292;&#36824;&#35201;&#25511;&#21046;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21476;&#20856;&#32463;&#27982;&#29702;&#35770;&#30340;&#20154;&#22312;&#24490;&#29615;&#38142;&#20013;&#25104;&#26412;&#25928;&#29992;&#20998;&#26512;&#27169;&#22411;&#65288;CAMLOP&#65289;&#26469;&#20998;&#26512;&#12289;&#37327;&#21270;&#21644;&#24179;&#34913;&#25928;&#29992;&#21644;&#30456;&#24212;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#20351;&#29992;12&#20010;&#25968;&#25454;&#38598;&#23545;MCS&#21644;CAMLOP&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the emergence of powerful language models along with Chain-of-thought prompting has made automation more and more omnipresent, it sometimes demonstrates its weakness in long-term or multi-step logical reasoning. For example, users don't always get desirable answers for complex mathematical problems without human involvement. Against this background, we present the Manual Correction System (MCS) -- a human-in-the-loop system enhanced by Chain-of-Thought prompting, which explores how manual correction of sub-logics in rationales can improve LLM's reasoning performance. Moving one step forward, considering a system with human-in-the-loop involves more than having humans improve performance but also controlling the cost. Therefore, we post a Cost-utility Analysis Model for Human-in-the-Loop systems (CAMLOP) based on classical economics theory to analyze, quantify and balance the utility and the corresponding cost. We conduct experiments of MCS and CAMLOP with twelve datasets. A signi
&lt;/p&gt;</description></item><item><title>&#26681;&#25454;&#20154;&#31867;&#35760;&#24518;&#21644;&#25512;&#29702;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#28436;&#21270;LLM&#26234;&#33021;&#20307;&#26694;&#26550;REMEMBERER&#65292;&#36890;&#36807;&#20026;LLM&#35013;&#22791;&#38271;&#26399;&#32463;&#39564;&#35760;&#24518;&#65292;&#21487;&#20197;&#20026;&#19981;&#21516;&#20219;&#21153;&#25552;&#20379;&#20248;&#24322;&#30340;&#26234;&#33021;&#20307;&#65292;&#20854;&#26500;&#25104;&#20102;&#21322;&#21442;&#25968;RL&#20195;&#29702;&#12290;&#25104;&#21151;&#29575;&#36229;&#36807;&#20808;&#21069;SOTA 4&#65285;&#21644;2&#65285;&#12290;</title><link>http://arxiv.org/abs/2306.07929</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21322;&#21442;&#25968;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Is Semi-Parametric Reinforcement Learning Agent. (arXiv:2306.07929v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07929
&lt;/p&gt;
&lt;p&gt;
&#26681;&#25454;&#20154;&#31867;&#35760;&#24518;&#21644;&#25512;&#29702;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#28436;&#21270;LLM&#26234;&#33021;&#20307;&#26694;&#26550;REMEMBERER&#65292;&#36890;&#36807;&#20026;LLM&#35013;&#22791;&#38271;&#26399;&#32463;&#39564;&#35760;&#24518;&#65292;&#21487;&#20197;&#20026;&#19981;&#21516;&#20219;&#21153;&#25552;&#20379;&#20248;&#24322;&#30340;&#26234;&#33021;&#20307;&#65292;&#20854;&#26500;&#25104;&#20102;&#21322;&#21442;&#25968;RL&#20195;&#29702;&#12290;&#25104;&#21151;&#29575;&#36229;&#36807;&#20808;&#21069;SOTA 4&#65285;&#21644;2&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#35748;&#30693;&#31185;&#23398;&#23545;&#20154;&#31867;&#35760;&#24518;&#21644;&#25512;&#29702;&#26426;&#21046;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#28436;&#21270;LLM&#65288;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#26234;&#33021;&#20307;&#26694;&#26550;REMEMBERER&#12290;&#36890;&#36807;&#20026;LLM&#35013;&#22791;&#38271;&#26399;&#32463;&#39564;&#35760;&#24518;&#65292;REMEMBERER&#33021;&#22815;&#21033;&#29992;&#36807;&#21435;&#21095;&#38598;&#30340;&#32463;&#39564;&#65292;&#29978;&#33267;&#21487;&#20197;&#20026;&#19981;&#21516;&#30340;&#20219;&#21153;&#30446;&#26631;&#25552;&#20379;&#20248;&#24322;&#30340;LLM&#26234;&#33021;&#20307;&#65292;&#36825;&#20248;&#20110;&#20855;&#26377;&#22266;&#23450;&#23454;&#20363;&#25110;&#20855;&#26377;&#30701;&#26242;&#24037;&#20316;&#35760;&#24518;&#30340;LLM&#26234;&#33021;&#20307;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;&#32463;&#39564;&#35760;&#24518;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLEM&#65289;&#26469;&#26356;&#26032;&#35760;&#24518;&#12290;&#22240;&#27492;&#65292;&#25972;&#20010;&#31995;&#32479;&#21487;&#20197;&#20174;&#25104;&#21151;&#21644;&#22833;&#36133;&#30340;&#32463;&#39564;&#20013;&#23398;&#20064;&#65292;&#24182;&#22312;&#19981;&#24494;&#35843;LLM&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#21457;&#23637;&#20854;&#33021;&#21147;&#12290;&#20197;&#27492;&#26041;&#24335;&#65292;&#25152;&#25552;&#20986;&#30340;REMEMBERER&#26500;&#25104;&#20102;&#21322;&#21442;&#25968;RL&#20195;&#29702;&#12290;&#22312;&#20004;&#20010;RL&#20219;&#21153;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#20197;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#12290;&#19981;&#21516;&#21021;&#22987;&#21270;&#21644;&#35757;&#32451;&#38598;&#30340;&#24179;&#22343;&#32467;&#26524;&#23545;&#20110;&#25104;&#21151;&#29575;&#36229;&#36807;&#20808;&#21069;SOTA 4&#65285;&#21644;2&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the insights in cognitive science with respect to human memory and reasoning mechanism, a novel evolvable LLM-based (Large Language Model) agent framework is proposed as REMEMBERER. By equipping the LLM with a long-term experience memory, REMEMBERER is capable of exploiting the experiences from the past episodes even for different task goals, which excels an LLM-based agent with fixed exemplars or equipped with a transient working memory. We further introduce Reinforcement Learning with Experience Memory (RLEM) to update the memory. Thus, the whole system can learn from the experiences of both success and failure, and evolve its capability without fine-tuning the parameters of the LLM. In this way, the proposed REMEMBERER constitutes a semi-parametric RL agent. Extensive experiments are conducted on two RL task sets to evaluate the proposed framework. The average results with different initialization and training sets exceed the prior SOTA by 4% and 2% for the success rate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35266;&#27979;&#21464;&#37327;&#30001;&#22240;&#26524;&#30456;&#20851;&#30340;&#28508;&#21464;&#37327;&#29983;&#25104;&#30340;&#38750;&#32447;&#24615;&#28508;&#21464;&#37327;&#23618;&#27425;&#22240;&#26524;&#27169;&#22411;&#20013;&#23454;&#29616;&#22240;&#26524;&#32467;&#26500;&#21644;&#28508;&#21464;&#37327;&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07916</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#28508;&#21464;&#37327;&#23618;&#27425;&#27169;&#22411;&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Identification of Nonlinear Latent Hierarchical Models. (arXiv:2306.07916v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35266;&#27979;&#21464;&#37327;&#30001;&#22240;&#26524;&#30456;&#20851;&#30340;&#28508;&#21464;&#37327;&#29983;&#25104;&#30340;&#38750;&#32447;&#24615;&#28508;&#21464;&#37327;&#23618;&#27425;&#22240;&#26524;&#27169;&#22411;&#20013;&#23454;&#29616;&#22240;&#26524;&#32467;&#26500;&#21644;&#28508;&#21464;&#37327;&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#35782;&#21035;&#28508;&#21464;&#37327;&#21644;&#22240;&#26524;&#32467;&#26500;&#23545;&#20110;&#35768;&#22810;&#28041;&#21450;&#29983;&#29289;&#25968;&#25454;&#12289;&#21307;&#23398;&#25968;&#25454;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#22914;&#22270;&#20687;&#21644;&#35821;&#35328;&#65289;&#30340;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#24403;&#35266;&#27979;&#21464;&#37327;&#30001;&#22240;&#26524;&#30456;&#20851;&#30340;&#28508;&#21464;&#37327;&#29983;&#25104;&#65292;&#24182;&#19988;&#20851;&#31995;&#26159;&#38750;&#32447;&#24615;&#30340;&#26102;&#65292;&#36825;&#39033;&#20219;&#21153;&#21487;&#33021;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#32447;&#24615;&#28508;&#21464;&#37327;&#23618;&#27425;&#22240;&#26524;&#27169;&#22411;&#30340;&#35782;&#21035;&#38382;&#39064;&#65292;&#22312;&#36825;&#31181;&#27169;&#22411;&#20013;&#65292;&#35266;&#27979;&#21464;&#37327;&#30001;&#19968;&#32452;&#22240;&#26524;&#30456;&#20851;&#30340;&#28508;&#21464;&#37327;&#29983;&#25104;&#65292;&#26377;&#20123;&#28508;&#21464;&#37327;&#21487;&#33021;&#27809;&#26377;&#35266;&#23519;&#21040;&#30340;&#21518;&#20195;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#21487;&#20197;&#23454;&#29616;&#22240;&#26524;&#32467;&#26500;&#21644;&#28508;&#21464;&#37327;&#30340;&#21487;&#35782;&#21035;&#24615;&#65306;&#23545;&#20110;&#22240;&#26524;&#32467;&#26500;&#65292;&#25105;&#20204;&#20801;&#35768;&#22270;&#20013;&#20219;&#24847;&#20004;&#20010;&#21464;&#37327;&#20043;&#38388;&#23384;&#22312;&#22810;&#26465;&#36335;&#24452;&#65292;&#36825;&#25918;&#23485;&#20102;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;&#28508;&#21464;&#37327;&#26641;&#20551;&#35774;&#65307;&#23545;&#20110;&#32467;&#26500;&#20989;&#25968;&#65292;&#25105;&#20204;&#27809;&#26377;&#36827;&#34892;&#21442;&#25968;&#20551;&#35774;&#65292;&#22240;&#27492;&#21487;&#20197;&#20801;&#35768;&#22522;&#22240;
&lt;/p&gt;
&lt;p&gt;
Identifying latent variables and causal structures from observational data is essential to many real-world applications involving biological data, medical data, and unstructured data such as images and languages. However, this task can be highly challenging, especially when observed variables are generated by causally related latent variables and the relationships are nonlinear. In this work, we investigate the identification problem for nonlinear latent hierarchical causal models in which observed variables are generated by a set of causally related latent variables, and some latent variables may not have observed children. We show that the identifiability of both causal structure and latent variables can be achieved under mild assumptions: on causal structures, we allow for the existence of multiple paths between any pair of variables in the graph, which relaxes latent tree assumptions in prior work; on structural functions, we do not make parametric assumptions, thus permitting gene
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;WebGLM&#26159;&#19968;&#31181;&#22522;&#20110;GLM&#30340;&#32593;&#32476;&#38382;&#31572;&#31995;&#32479;&#65292;&#20351;&#29992;LLM-augmented&#26816;&#32034;&#22120;&#12289;&#24341;&#23548;&#24335;&#29983;&#25104;&#22120;&#21644;&#20154;&#31867;&#20559;&#22909;&#24863;&#30693;&#35780;&#20998;&#22120;&#31561;&#31574;&#30053;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#25104;&#26412;&#25928;&#30410;&#65292;&#24182;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#27604;WebGPT&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07906</link><description>&lt;p&gt;
WebGLM: &#22522;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#39640;&#25928;&#32593;&#32476;&#22686;&#24378;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences. (arXiv:2306.07906v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;WebGLM&#26159;&#19968;&#31181;&#22522;&#20110;GLM&#30340;&#32593;&#32476;&#38382;&#31572;&#31995;&#32479;&#65292;&#20351;&#29992;LLM-augmented&#26816;&#32034;&#22120;&#12289;&#24341;&#23548;&#24335;&#29983;&#25104;&#22120;&#21644;&#20154;&#31867;&#20559;&#22909;&#24863;&#30693;&#35780;&#20998;&#22120;&#31561;&#31574;&#30053;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#25104;&#26412;&#25928;&#30410;&#65292;&#24182;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#27604;WebGPT&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;WebGLM&#65292;&#19968;&#31181;&#22522;&#20110;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;GLM&#65289;&#30340;&#32593;&#32476;&#22686;&#24378;&#38382;&#31572;&#31995;&#32479;&#12290;&#20854;&#30446;&#26631;&#26159;&#22312;&#20445;&#25345;&#36866;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#30340;&#21516;&#26102;&#65292;&#21033;&#29992;&#32593;&#32476;&#25628;&#32034;&#21644;&#26816;&#32034;&#33021;&#21147;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#36890;&#36807;LLM&#22686;&#24378;&#30340;&#26816;&#32034;&#22120;&#12289;&#24341;&#23548;&#24335;&#29983;&#25104;&#22120;&#21644;&#20154;&#31867;&#20559;&#22909;&#24863;&#30693;&#35780;&#20998;&#22120;&#31561;&#31574;&#30053;&#24320;&#21457;&#20102;WebGLM&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35782;&#21035;&#24182;&#35299;&#20915;&#20102;WebGPT&#65288;OpenAI&#65289;&#30340;&#23616;&#38480;&#24615;&#65292;&#20174;&#32780;&#20026;WebGLM&#25552;&#20379;&#20102;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#25104;&#26412;&#25928;&#30410;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35780;&#20272;&#32593;&#32476;&#22686;&#24378;QA&#31995;&#32479;&#30340;&#31995;&#32479;&#24615;&#26631;&#20934;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#32500;&#20154;&#31867;&#35780;&#20272;&#21644;&#23450;&#37327;&#21066;&#24369;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;WebGLM&#35774;&#35745;&#20248;&#20110;&#29616;&#26377;&#31995;&#32479;&#12290;&#19982;WebGPT&#65288;13B&#65289;&#30456;&#27604;&#65292;&#20351;&#29992;100&#20159;&#21442;&#25968;&#30340;GLM&#65288;10B&#65289;&#30340;WebGLM&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#29978;&#33267;&#21487;&#19982;WebGPT&#65288;175B&#65289;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present WebGLM, a web-enhanced question-answering system based on the General Language Model (GLM). Its goal is to augment a pre-trained large language model (LLM) with web search and retrieval capabilities while being efficient for real-world deployments. To achieve this, we develop WebGLM with strategies for the LLM-augmented retriever, bootstrapped generator, and human preference-aware scorer. Specifically, we identify and address the limitations of WebGPT (OpenAI), through which WebGLM is enabled with accuracy, efficiency, and cost-effectiveness advantages. In addition, we propose systematic criteria for evaluating web-enhanced QA systems. We conduct multi-dimensional human evaluation and quantitative ablation studies, which suggest the outperformance of the proposed WebGLM designs over existing systems. WebGLM with the 10-billion-parameter GLM (10B) is shown to perform better than the similar-sized WebGPT (13B) and even comparably to WebGPT (175B) in human evaluation. The code,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#24773;&#24863;&#27169;&#22411;&#30340;&#26368;&#20840;&#38754;&#30340;&#24320;&#25918;&#24335;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;79&#20010;&#25163;&#21160;&#36873;&#23450;&#30340;&#25968;&#25454;&#38598;&#32452;&#25104;&#65292;&#35206;&#30422;&#20102;27&#31181;&#35821;&#35328;&#65292;&#20195;&#34920;&#20102;6&#31181;&#35821;&#35328;&#23478;&#26063;&#12290;&#32780;&#19988;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#31867;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2306.07902</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#24773;&#24863;&#25968;&#25454;&#38598;&#21644;&#22810;&#26041;&#38754;&#24773;&#24863;&#20998;&#31867;&#22522;&#20934;&#30340;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Massively Multilingual Corpus of Sentiment Datasets and Multi-faceted Sentiment Classification Benchmark. (arXiv:2306.07902v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#24773;&#24863;&#27169;&#22411;&#30340;&#26368;&#20840;&#38754;&#30340;&#24320;&#25918;&#24335;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;79&#20010;&#25163;&#21160;&#36873;&#23450;&#30340;&#25968;&#25454;&#38598;&#32452;&#25104;&#65292;&#35206;&#30422;&#20102;27&#31181;&#35821;&#35328;&#65292;&#20195;&#34920;&#20102;6&#31181;&#35821;&#35328;&#23478;&#26063;&#12290;&#32780;&#19988;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#31867;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#21644;&#27169;&#22411;&#35757;&#32451;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#20294;&#24320;&#21457;&#22823;&#35268;&#27169;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#22312;&#25991;&#21270;&#30456;&#20851;&#30340;&#35821;&#35328;&#20219;&#21153;&#20013;&#23588;&#20854;&#22914;&#27492;&#12290;&#22810;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#23601;&#26159;&#19968;&#20010;&#20363;&#23376;&#65292;&#20854;&#20013;&#24773;&#24863;&#26631;&#35760;&#21487;&#33021;&#38750;&#24120;&#24494;&#22937;&#19988;&#28145;&#28145;&#26893;&#26681;&#20110;&#25991;&#21270;&#20013;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#21487;&#29992;&#20110;&#35757;&#32451;&#24773;&#24863;&#27169;&#22411;&#30340;&#26368;&#20840;&#38754;&#30340;&#24320;&#25918;&#24335;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#35821;&#26009;&#24211;&#12290;&#35813;&#35821;&#26009;&#24211;&#30001;79&#20010;&#25163;&#21160;&#36873;&#25321;&#30340;&#25968;&#25454;&#38598;&#32452;&#25104;&#65292;&#23427;&#20204;&#26159;&#22522;&#20110;&#20005;&#26684;&#30340;&#36136;&#37327;&#26631;&#20934;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25253;&#21578;&#30340;&#36229;&#36807;350&#20010;&#25968;&#25454;&#38598;&#20013;&#36873;&#20986;&#30340;&#12290;&#35813;&#35821;&#26009;&#24211;&#28085;&#30422;&#20102;27&#31181;&#35821;&#35328;&#65292;&#20195;&#34920;&#20102;6&#31181;&#35821;&#35328;&#23478;&#26063;&#12290;&#25968;&#25454;&#38598;&#21487;&#20197;&#20351;&#29992;&#20960;&#20010;&#35821;&#35328;&#21644;&#21151;&#33021;&#29305;&#24449;&#36827;&#34892;&#26597;&#35810;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#31867;&#22522;&#20934;&#65292;&#24635;&#32467;&#20102;&#23545;&#19981;&#21516;&#30340;&#22522;&#26412;&#27169;&#22411;&#12289;&#35757;&#32451;&#30446;&#26631;&#12289;&#25968;&#25454;&#38598;&#38598;&#21512;&#21644;&#24494;&#35843;&#36827;&#34892;&#30340;&#25968;&#30334;&#20010;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite impressive advancements in multilingual corpora collection and model training, developing large-scale deployments of multilingual models still presents a significant challenge. This is particularly true for language tasks that are culture-dependent. One such example is the area of multilingual sentiment analysis, where affective markers can be subtle and deeply ensconced in culture. This work presents the most extensive open massively multilingual corpus of datasets for training sentiment models. The corpus consists of 79 manually selected datasets from over 350 datasets reported in the scientific literature based on strict quality criteria. The corpus covers 27 languages representing 6 language families. Datasets can be queried using several linguistic and functional features. In addition, we present a multi-faceted sentiment classification benchmark summarizing hundreds of experiments conducted on different base models, training objectives, dataset collections, and fine-tunin
&lt;/p&gt;</description></item><item><title>ReadProbe &#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25628;&#32034;&#24341;&#25806;&#25903;&#25345;&#27178;&#21521;&#38405;&#35835;&#30340;&#24037;&#20855;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#26377;&#29992;&#38382;&#39064;&#21644;&#31572;&#26696;&#20197;&#24110;&#21161;&#29992;&#25143;&#35780;&#20272;&#22312;&#32447;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.07875</link><description>&lt;p&gt;
ReadProbe: &#19968;&#31181;&#25903;&#25345;&#27178;&#21521;&#38405;&#35835;&#30340;&#26816;&#32034;&#22686;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
ReadProbe: A Demo of Retrieval-Enhanced Large Language Models to Support Lateral Reading. (arXiv:2306.07875v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07875
&lt;/p&gt;
&lt;p&gt;
ReadProbe &#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25628;&#32034;&#24341;&#25806;&#25903;&#25345;&#27178;&#21521;&#38405;&#35835;&#30340;&#24037;&#20855;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#26377;&#29992;&#38382;&#39064;&#21644;&#31572;&#26696;&#20197;&#24110;&#21161;&#29992;&#25143;&#35780;&#20272;&#22312;&#32447;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#32593;&#32476;&#19981;&#23454;&#20449;&#24687;&#30340;&#24555;&#36895;&#22686;&#38271;&#21644;&#20256;&#25773;&#65292;&#20154;&#20204;&#38656;&#35201;&#24037;&#20855;&#26469;&#24110;&#21161;&#20182;&#20204;&#35780;&#20272;&#22312;&#32447;&#20449;&#24687;&#30340;&#21487;&#20449;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;&#27178;&#21521;&#38405;&#35835;&#26159;&#19968;&#31181;&#36328;&#21442;&#32771;&#22810;&#20010;&#20449;&#24687;&#28304;&#30340;&#31574;&#30053;&#65292;&#21487;&#33021;&#26159;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24037;&#20855;&#65292;&#21517;&#20026; ReadProbe&#65292;&#23427;&#25903;&#25345;&#27178;&#21521;&#38405;&#35835;&#65292;&#30001; OpenAI &#30340;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#24517;&#24212;&#25628;&#32034;&#24341;&#25806;&#39537;&#21160;&#12290;&#25105;&#20204;&#30340;&#24037;&#20855;&#33021;&#22815;&#20026;&#27178;&#21521;&#38405;&#35835;&#29983;&#25104;&#26377;&#29992;&#30340;&#38382;&#39064;&#65292;&#25628;&#23547;&#32593;&#32476;&#19978;&#30456;&#20851;&#30340;&#25991;&#26723;&#65292;&#24182;&#20135;&#29983;&#33391;&#22909;&#24402;&#22240;&#30340;&#31572;&#26696;&#65292;&#24110;&#21161;&#20154;&#20204;&#26356;&#22909;&#22320;&#35780;&#20272;&#22312;&#32447;&#20449;&#24687;&#12290;&#25105;&#20204;&#21046;&#20316;&#20102;&#19968;&#20010;&#22522;&#20110; Web &#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#28436;&#31034;&#20102; ReadProbe &#22914;&#20309;&#24110;&#21161;&#20943;&#23569;&#34987;&#34394;&#20551;&#20449;&#24687;&#35823;&#23548;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312; https://github.com/DakeZhang1998/ReadProbe &#19978;&#33719;&#24471;&#65292;&#25105;&#20204;&#30340;&#26089;&#26399;&#29256;&#26412;&#36194;&#24471;&#20102;&#20840;&#22269;&#20154;&#24037;&#26234;&#33021;&#34394;&#20551;&#20449;&#24687;&#40657;&#23458;&#39532;&#25289;&#26494;&#30340;&#19968;&#31561;&#22870;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid growth and spread of online misinformation, people need tools to help them evaluate the credibility and accuracy of online information. Lateral reading, a strategy that involves cross-referencing information with multiple sources, may be an effective approach to achieving this goal. In this paper, we present ReadProbe, a tool to support lateral reading, powered by generative large language models from OpenAI and the Bing search engine. Our tool is able to generate useful questions for lateral reading, scour the web for relevant documents, and generate well-attributed answers to help people better evaluate online information. We made a web-based application to demonstrate how ReadProbe can help reduce the risk of being misled by false information. The code is available at https://github.com/DakeZhang1998/ReadProbe. An earlier version of our tool won the first prize in a national AI misinformation hackathon.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#23618;&#32467;&#26500;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23618;&#32467;&#26500;&#39046;&#22495;&#30340;&#24418;&#24335;&#21270;&#25551;&#36848;&#26469;&#32531;&#35299;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07874</link><description>&lt;p&gt;
&#20998;&#23618;&#32467;&#26500;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Taxonomy-Structured Domain Adaptation. (arXiv:2306.07874v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#23618;&#32467;&#26500;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23618;&#32467;&#26500;&#39046;&#22495;&#30340;&#24418;&#24335;&#21270;&#25551;&#36848;&#26469;&#32531;&#35299;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#33258;&#36866;&#24212;&#26088;&#22312;&#32531;&#35299;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26041;&#27861;&#22823;&#22810;&#38480;&#20110;&#20998;&#31867;&#39046;&#22495;&#65292;&#36825;&#20005;&#37325;&#31616;&#21270;&#20102;&#30495;&#23454;&#19990;&#30028;&#20013;&#24494;&#22937;&#30340;&#39046;&#22495;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#23618;&#32467;&#26500;&#39046;&#22495;&#36827;&#34892;&#25512;&#24191;&#65292;&#23558;&#39046;&#22495;&#24418;&#24335;&#21270;&#20026;&#20855;&#26377;&#23884;&#22871;&#30340;&#23618;&#27425;&#30456;&#20284;&#32467;&#26500;&#65292;&#20363;&#22914;&#21160;&#29289;&#29289;&#31181;&#21644;&#20135;&#21697;&#30446;&#24405;&#12290;&#25105;&#20204;&#24314;&#31435;&#22312;&#32463;&#20856;&#23545;&#25239;&#26694;&#26550;&#20043;&#19978;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#31867;&#22120;&#65292;&#35813;&#20998;&#31867;&#22120;&#19982;&#23545;&#25239;&#24615;&#37492;&#21035;&#22120;&#31454;&#20105;&#20197;&#20445;&#30041;&#23618;&#27425;&#32467;&#26500;&#20449;&#24687;&#12290;&#24403;&#32473;&#23450;&#38750;&#20449;&#24687;&#39046;&#22495;&#20998;&#31867;&#65288;&#20363;&#22914;&#65292;&#25152;&#26377;&#21494;&#33410;&#28857;&#37117;&#38142;&#25509;&#21040;&#26681;&#33410;&#28857;&#30340;&#25153;&#24179;&#20998;&#31867;&#65289;&#26102;&#65292;&#24179;&#34913;&#28857;&#24674;&#22797;&#32463;&#20856;&#30340;&#23545;&#25239;&#39046;&#22495;&#36866;&#24212;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#22312;&#20854;&#20182;&#20998;&#31867;&#20013;&#20135;&#29983;&#38750;&#24179;&#20961;&#30340;&#32467;&#26524;&#12290;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#25104;&#21151;&#36827;&#34892;&#20102;&#33258;&#36866;&#24212;&#12290;&#20195;&#30721;&#21487;&#22312;https://gith&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain adaptation aims to mitigate distribution shifts among different domains. However, traditional formulations are mostly limited to categorical domains, greatly simplifying nuanced domain relationships in the real world. In this work, we tackle a generalization with taxonomy-structured domains, which formalizes domains with nested, hierarchical similarity structures such as animal species and product catalogs. We build on the classic adversarial framework and introduce a novel taxonomist, which competes with the adversarial discriminator to preserve the taxonomy information. The equilibrium recovers the classic adversarial domain adaptation's solution if given a non-informative domain taxonomy (e.g., a flat taxonomy where all leaf nodes connect to the root node) while yielding non-trivial results with other taxonomies. Empirically, our method achieves state-of-the-art performance on both synthetic and real-world datasets with successful adaptation. Code is available at https://gith
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#23569;&#37327;&#31034;&#20363;&#26469;&#23454;&#29616;&#20154;&#31867;&#32423;&#21035;&#30340;&#35745;&#31639;&#26426;&#25511;&#21046;&#65307;&#36890;&#36807;&#20998;&#35299;&#28436;&#31034;&#12289;&#36807;&#28388;&#29366;&#24577;&#24182;&#37325;&#26032;&#26500;&#36896;&#20219;&#21153;&#25551;&#36848;&#65292;&#31034;&#20363;&#26816;&#32034;&#31561;&#27493;&#39588;&#65292;Synapse &#20855;&#22791;&#20102;&#36866;&#24212;&#22810;&#20219;&#21153;&#12289;&#27867;&#21270;&#22810;&#29615;&#22659;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.07863</link><description>&lt;p&gt;
Synapse&#65306;&#21033;&#29992;&#23569;&#37327;&#31034;&#20363;&#20026;&#23454;&#29616;&#20154;&#31867;&#32423;&#21035;&#30340;&#35745;&#31639;&#26426;&#25511;&#21046;&#25171;&#19979;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Synapse: Leveraging Few-Shot Exemplars for Human-Level Computer Control. (arXiv:2306.07863v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#23569;&#37327;&#31034;&#20363;&#26469;&#23454;&#29616;&#20154;&#31867;&#32423;&#21035;&#30340;&#35745;&#31639;&#26426;&#25511;&#21046;&#65307;&#36890;&#36807;&#20998;&#35299;&#28436;&#31034;&#12289;&#36807;&#28388;&#29366;&#24577;&#24182;&#37325;&#26032;&#26500;&#36896;&#20219;&#21153;&#25551;&#36848;&#65292;&#31034;&#20363;&#26816;&#32034;&#31561;&#27493;&#39588;&#65292;Synapse &#20855;&#22791;&#20102;&#36866;&#24212;&#22810;&#20219;&#21153;&#12289;&#27867;&#21270;&#22810;&#29615;&#22659;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#31034;&#23569;&#37327;&#31034;&#20363;&#26469;&#36827;&#34892;&#35745;&#31639;&#26426;&#33258;&#21160;&#21270;&#30340;&#35774;&#35745;&#12290;&#34429;&#28982;&#20197;&#21069;&#30340;&#25552;&#31034;&#26041;&#27861;&#30528;&#37325;&#20110;&#33258;&#25105;&#32416;&#27491;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#20165;&#20165;&#26377;&#33391;&#22909;&#32467;&#26500;&#30340;&#31034;&#20363;&#23601;&#36275;&#20197;&#23454;&#29616;&#20154;&#31867;&#32423;&#21035;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; Synapse&#65292;&#19968;&#31181;&#19978;&#19979;&#25991;&#35745;&#31639;&#26426;&#25511;&#21046;&#20195;&#29702;&#65292;&#22312; MiniWob++ &#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20102;&#20154;&#31867;&#32423;&#21035;&#30340;&#24615;&#33021;&#12290;Synapse &#30001;&#19977;&#20010;&#20027;&#35201;&#32452;&#20214;&#32452;&#25104;&#65306;1&#65289;&#29366;&#24577;&#26465;&#20214;&#20998;&#35299;&#65292;&#26681;&#25454;&#20195;&#29702;&#38656;&#35201;&#26032;&#29615;&#22659;&#29366;&#24577;&#23558;&#28436;&#31034;&#20998;&#20026;&#31034;&#20363;&#38598;&#65292;&#23454;&#29616;&#20102;&#26102;&#38388;&#25277;&#35937;&#65307;2&#65289;&#32467;&#26500;&#21270;&#25552;&#31034;&#65292;&#36807;&#28388;&#29366;&#24577;&#24182;&#37325;&#26032;&#26500;&#36896;&#27599;&#20010;&#38598;&#21512;&#30340;&#20219;&#21153;&#25551;&#36848;&#20197;&#25913;&#21892;&#35745;&#21010;&#30340;&#27491;&#30830;&#24615;&#65307;3&#65289;&#31034;&#20363;&#26816;&#32034;&#65292;&#23558;&#20256;&#20837;&#30340;&#20219;&#21153;&#19982;&#31034;&#20363;&#25968;&#25454;&#24211;&#20013;&#30340;&#23545;&#24212;&#31034;&#20363;&#30456;&#20851;&#32852;&#65292;&#20197;&#23454;&#29616;&#22810;&#20219;&#21153;&#36866;&#24212;&#21644;&#27867;&#21270;&#12290;Synapse &#20811;&#26381;&#20102;&#19978;&#19979;&#25991;&#38271;&#24230;&#38480;&#21046;&#65292;&#20943;&#23569;&#20102;&#22810;&#27493;&#25511;&#21046;&#20013;&#30340;&#38169;&#35823;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#28789;&#27963;&#22320;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35745;&#31639;&#26426;&#33258;&#21160;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the design of few-shot exemplars for computer automation through prompting large language models (LLMs). While previous prompting approaches focus on self-correction, we find that well-structured exemplars alone are sufficient for human-level performance. We present Synapse, an in-context computer control agent demonstrating human-level performance on the MiniWob++ benchmark. Synapse consists of three main components: 1) state-conditional decomposition, which divides demonstrations into exemplar sets based on the agent's need for new environment states, enabling temporal abstraction; 2) structured prompting, which filters states and reformulates task descriptions for each set to improve planning correctness; and 3) exemplar retrieval, which associates incoming tasks with corresponding exemplars in an exemplar database for multi-task adaptation and generalization. Synapse overcomes context length limits, reduces errors in multi-step control, and allows for more e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#38754;&#21521;&#23398;&#29983;&#24178;&#39044;&#25514;&#26045;&#21487;&#20197;&#24110;&#21161;&#25552;&#39640;&#24503;&#35821;&#25340;&#20889;&#25216;&#24039;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29615;&#22659;&#65292;&#23545;&#38477;&#20302;&#23398;&#29983;&#38169;&#35823;&#29575;&#20855;&#26377;&#26174;&#33879;&#20316;&#29992;&#65292;&#20294;&#38656;&#35201;&#35880;&#24910;&#36873;&#25321;&#20197;&#36991;&#20813;&#36749;&#23398;&#29575;&#30340;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2306.07853</link><description>&lt;p&gt;
&#32473;&#25105;&#30475;&#30475;&#25968;&#23383;&#65281;--&#38024;&#23545;&#24503;&#35821;&#25340;&#20889;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29615;&#22659;&#20013;&#38754;&#21521;&#23398;&#29983;&#30340;&#24178;&#39044;&#25514;&#26045;
&lt;/p&gt;
&lt;p&gt;
Show me the numbers! -- Student-facing Interventions in Adaptive Learning Environments for German Spelling. (arXiv:2306.07853v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07853
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#38754;&#21521;&#23398;&#29983;&#24178;&#39044;&#25514;&#26045;&#21487;&#20197;&#24110;&#21161;&#25552;&#39640;&#24503;&#35821;&#25340;&#20889;&#25216;&#24039;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29615;&#22659;&#65292;&#23545;&#38477;&#20302;&#23398;&#29983;&#38169;&#35823;&#29575;&#20855;&#26377;&#26174;&#33879;&#20316;&#29992;&#65292;&#20294;&#38656;&#35201;&#35880;&#24910;&#36873;&#25321;&#20197;&#36991;&#20813;&#36749;&#23398;&#29575;&#30340;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33258;&#36866;&#24212;&#23398;&#20064;&#29615;&#22659;&#31181;&#31867;&#32321;&#22810;&#65292;&#22240;&#27492;&#25214;&#21040;&#21738;&#20123;&#36866;&#24212;&#25514;&#26045;&#23545;&#20110;&#21738;&#20123;&#23398;&#20064;&#39046;&#22495;&#26159;&#26377;&#24847;&#20041;&#30340;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21576;&#29616;&#20102;&#22312;&#19968;&#20010;&#22312;&#32447;&#24179;&#21488;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#30340;&#32467;&#26524;&#65292;&#35813;&#24179;&#21488;&#26088;&#22312;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#20026;&#22522;&#30784;&#30340;&#38754;&#21521;&#23398;&#29983;&#30340;&#24178;&#39044;&#25514;&#26045;&#26469;&#25552;&#39640;&#23398;&#20064;&#24503;&#35821;&#25340;&#20889;&#25216;&#33021;&#12290;&#25105;&#20204;&#23558;&#20256;&#32479;&#30340;&#22312;&#32447;&#23398;&#20064;&#24179;&#21488;&#19982;&#35813;&#24179;&#21488;&#30340;&#19977;&#20010;&#19981;&#21516;&#30340;&#33258;&#36866;&#24212;&#29256;&#26412;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#36825;&#20123;&#29256;&#26412;&#23454;&#29616;&#20102;&#21521;&#23398;&#29983;&#23637;&#31034;&#20010;&#24615;&#21270;&#35299;&#20915;&#26041;&#26696;&#27010;&#29575;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#24178;&#39044;&#25514;&#26045;&#65292;&#20851;&#27880;&#38169;&#35823;&#29575;&#12289;&#36749;&#23398;&#29575;&#21644;&#29992;&#25143;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#38169;&#35823;&#27425;&#25968;&#30456;&#27604;&#23545;&#29031;&#32452;&#26377;&#25152;&#20943;&#23569;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#36749;&#23398;&#20154;&#25968;&#20063;&#22312;&#22686;&#21152;&#12290;&#25105;&#20204;&#27809;&#26377;&#21457;&#29616;&#20219;&#20309;&#23545;&#20110;&#29992;&#25143;&#33021;&#21147;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65306;&#38754;&#21521;&#23398;&#29983;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29615;&#22659;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#20010;&#20154;&#38169;&#35823;&#29575;&#65292;&#24212;&#35880;&#24910;&#36873;&#25321;&#65292;&#20197;&#21457;&#25381;&#28608;&#21169;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since adaptive learning comes in many shapes and sizes, it is crucial to find out which adaptations can be meaningful for which areas of learning. Our work presents the result of an experiment conducted on an online platform for the acquisition of German spelling skills. We compared the traditional online learning platform to three different adaptive versions of the platform that implement machine learning-based student-facing interventions that show the personalized solution probability. We evaluate the different interventions with regard to the error rate, the number of early dropouts, and the users competency. Our results show that the number of mistakes decreased in comparison to the control group. Additionally, an increasing number of dropouts was found. We did not find any significant effects on the users competency. We conclude that student-facing adaptive learning environments are effective in improving a persons error rate and should be chosen wisely to have a motivating impac
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;3D&#20998;&#23376;&#22270;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#22320;&#39044;&#27979;&#20998;&#23376;&#23646;&#24615;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20998;&#23376;&#20960;&#20309;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2306.07812</link><description>&lt;p&gt;
&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#33258;&#21160;&#21270;&#19977;&#32500;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Automated 3D Pre-Training for Molecular Property Prediction. (arXiv:2306.07812v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07812
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;3D&#20998;&#23376;&#22270;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#22320;&#39044;&#27979;&#20998;&#23376;&#23646;&#24615;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20998;&#23376;&#20960;&#20309;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#26159;&#33647;&#29289;&#30740;&#21457;&#21644;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#30001;&#20110;&#20998;&#23376;&#30340;&#20960;&#20309;&#32467;&#26500;&#23545;&#20110;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#24517;&#35201;&#24615;&#24050;&#32463;&#34987;&#35777;&#26126;&#65292;&#22240;&#27492;&#23558;3D&#20449;&#24687;&#19982;&#21508;&#31181;&#22270;&#24418;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;&#22312;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#20013;&#33719;&#24471;&#20998;&#23376;&#30340;&#20960;&#20309;&#32467;&#26500;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;3D&#39044;&#35757;&#32451;&#26694;&#26550;&#65288;&#31216;&#20026;3D PGT&#65289;&#65292;&#23427;&#22312;3D&#20998;&#23376;&#22270;&#19978;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#27809;&#26377;3D&#32467;&#26500;&#30340;&#20998;&#23376;&#22270;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#22522;&#20110;&#21270;&#23398;&#38190;&#38271;&#65292;&#21270;&#23398;&#38190;&#35282;&#21644;&#20108;&#38754;&#35282;&#36825;&#19977;&#20010;&#22522;&#26412;&#20960;&#20309;&#25551;&#36848;&#31526;&#23545;&#24212;&#20110;&#23436;&#25972;&#30340;&#20998;&#23376;3D&#26500;&#24418;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#36825;&#19977;&#20010;&#23646;&#24615;&#30340;&#22810;&#20219;&#21153;&#29983;&#25104;&#39044;&#35757;&#32451;&#26694;&#26550;&#12290;&#25509;&#19979;&#26469;&#65292;&#20026;&#20102;&#33258;&#21160;&#34701;&#21512;&#36825;&#19977;&#39033;&#29983;&#25104;&#20219;&#21153;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20351;&#29992;&#8220;&#24635;&#33021;&#37327;&#8221;&#26469;&#25628;&#32034;&#30340;&#26367;&#20195;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular property prediction is an important problem in drug discovery and materials science. As geometric structures have been demonstrated necessary for molecular property prediction, 3D information has been combined with various graph learning methods to boost prediction performance. However, obtaining the geometric structure of molecules is not feasible in many real-world applications due to the high computational cost. In this work, we propose a novel 3D pre-training framework (dubbed 3D PGT), which pre-trains a model on 3D molecular graphs, and then fine-tunes it on molecular graphs without 3D structures. Based on fact that bond length, bond angle, and dihedral angle are three basic geometric descriptors corresponding to a complete molecular 3D conformer, we first develop a multi-task generative pre-train framework based on these three attributes. Next, to automatically fuse these three generative tasks, we design a surrogate metric using the \textit{total energy} to search for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#31995;&#32479;&#22320;&#26816;&#26597;ChatGPT&#22312;&#20004;&#20010;&#21487;&#25511;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21363;ChatGPT&#33021;&#21542;&#36866;&#24212;&#19981;&#21516;&#30340;&#30446;&#26631;&#21463;&#20247;&#21644;&#20889;&#20316;&#39118;&#26684;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20154;&#31867;&#20135;&#29983;&#30340;&#25991;&#20307;&#21464;&#21270;&#27604;ChatGPT&#34920;&#29616;&#20986;&#30340;&#26356;&#22823;&#65292;&#32780;&#29983;&#25104;&#30340;&#25991;&#26412;&#22312;&#19968;&#20123;&#29305;&#24449;&#19978;&#19982;&#20154;&#31867;&#26679;&#26412;&#26377;&#25152;&#19981;&#21516;&#65292;&#26377;&#26102;&#20250;&#21253;&#21547;&#20107;&#23454;&#38169;&#35823;&#25110;&#24187;&#35273;&#12290;</title><link>http://arxiv.org/abs/2306.07799</link><description>&lt;p&gt;
ChatGPT&#19982;&#20154;&#24037;&#25776;&#20889;&#25991;&#26412;&#65306;&#21487;&#25511;&#25991;&#26412;&#25688;&#35201;&#21644;&#21477;&#23376;&#39118;&#26684;&#36716;&#31227;&#30340;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer. (arXiv:2306.07799v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#31995;&#32479;&#22320;&#26816;&#26597;ChatGPT&#22312;&#20004;&#20010;&#21487;&#25511;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21363;ChatGPT&#33021;&#21542;&#36866;&#24212;&#19981;&#21516;&#30340;&#30446;&#26631;&#21463;&#20247;&#21644;&#20889;&#20316;&#39118;&#26684;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20154;&#31867;&#20135;&#29983;&#30340;&#25991;&#20307;&#21464;&#21270;&#27604;ChatGPT&#34920;&#29616;&#20986;&#30340;&#26356;&#22823;&#65292;&#32780;&#29983;&#25104;&#30340;&#25991;&#26412;&#22312;&#19968;&#20123;&#29305;&#24449;&#19978;&#19982;&#20154;&#31867;&#26679;&#26412;&#26377;&#25152;&#19981;&#21516;&#65292;&#26377;&#26102;&#20250;&#21253;&#21547;&#20107;&#23454;&#38169;&#35823;&#25110;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#20197;&#20854;&#20986;&#33394;&#30340;&#33021;&#21147;&#20174;&#31616;&#30701;&#30340;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#29983;&#25104;&#36830;&#36143;&#30340;&#25991;&#26412;&#24341;&#36215;&#20102;&#23186;&#20307;&#30340;&#37325;&#35270;&#12290;&#26412;&#25991;&#26088;&#22312;&#31995;&#32479;&#22320;&#26816;&#26597;ChatGPT&#22312;&#20004;&#20010;&#21487;&#25511;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21363;ChatGPT&#33021;&#21542;&#36866;&#24212;&#19981;&#21516;&#30340;&#30446;&#26631;&#21463;&#20247;&#65288;&#19987;&#23478;&#19982;&#19968;&#33324;&#20154;&#65289;&#21644;&#20889;&#20316;&#39118;&#26684;&#65288;&#27491;&#24335;&#19982;&#38750;&#27491;&#24335;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#29983;&#25104;&#25991;&#26412;&#30340;&#24544;&#23454;&#24230;&#65292;&#24182;&#23558;&#27169;&#22411;&#30340;&#34920;&#29616;&#19982;&#20154;&#24037;&#25776;&#20889;&#30340;&#25991;&#26412;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20154;&#31867;&#20135;&#29983;&#30340;&#25991;&#20307;&#21464;&#21270;&#27604;ChatGPT&#34920;&#29616;&#20986;&#30340;&#26356;&#22823;&#65292;&#32780;&#29983;&#25104;&#30340;&#25991;&#26412;&#22312;&#35832;&#22914;&#21333;&#35789;&#31867;&#22411;&#20998;&#24067;&#31561;&#20960;&#20010;&#29305;&#24449;&#19978;&#19982;&#20154;&#31867;&#26679;&#26412;&#26377;&#25152;&#19981;&#21516;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403; ChatGPT &#23558;&#25991;&#26412;&#36866;&#24212;&#29305;&#23450;&#39118;&#26684;&#26102;&#65292;&#26377;&#26102;&#20250;&#21253;&#21547;&#20107;&#23454;&#38169;&#35823;&#25110;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale language models, like ChatGPT, have garnered significant media attention and stunned the public with their remarkable capacity for generating coherent text from short natural language prompts. In this paper, we aim to conduct a systematic inspection of ChatGPT's performance in two controllable generation tasks, with respect to ChatGPT's ability to adapt its output to different target audiences (expert vs. layman) and writing styles (formal vs. informal). Additionally, we evaluate the faithfulness of the generated text, and compare the model's performance with human-authored texts. Our findings indicate that the stylistic variations produced by humans are considerably larger than those demonstrated by ChatGPT, and the generated texts diverge from human samples in several characteristics, such as the distribution of word types. Moreover, we observe that ChatGPT sometimes incorporates factual errors or hallucinations when adapting the text to suit a specific style.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#25386;&#23041;&#35821;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#29992;&#20110;&#20108;&#20803;&#20998;&#31867;&#21644;&#32431;&#35786;&#26029;&#20219;&#21153;&#65292;&#26088;&#22312;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#27861;&#29702;&#35299;&#33021;&#21147;&#12290;&#23427;&#20204;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#29992;&#20110;&#23545;&#29616;&#26377;&#25386;&#23041;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2306.07790</link><description>&lt;p&gt;
NoCoLA&#65306;&#25386;&#23041;&#35821;&#35328;&#25509;&#21463;&#24615;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
NoCoLA: The Norwegian Corpus of Linguistic Acceptability. (arXiv:2306.07790v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#25386;&#23041;&#35821;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#29992;&#20110;&#20108;&#20803;&#20998;&#31867;&#21644;&#32431;&#35786;&#26029;&#20219;&#21153;&#65292;&#26088;&#22312;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#27861;&#29702;&#35299;&#33021;&#21147;&#12290;&#23427;&#20204;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#29992;&#20110;&#23545;&#29616;&#26377;&#25386;&#23041;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#37327;&#30340;&#25386;&#23041;&#35821;&#35328;&#27169;&#22411;&#20986;&#29616;&#65292;&#20294;&#25105;&#20204;&#32570;&#20047;&#35780;&#20272;&#20854;&#35821;&#27861;&#29702;&#35299;&#33021;&#21147;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#25386;&#23041;&#25968;&#25454;&#38598;&#26469;&#23436;&#25104;&#36825;&#39033;&#20219;&#21153;&#12290;&#20854;&#20013;NoCoLA_class&#26159;&#19968;&#20010;&#24102;&#30417;&#30563;&#30340;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#65292;&#26088;&#22312;&#21306;&#20998;&#21487;&#25509;&#21463;&#21644;&#19981;&#21487;&#25509;&#21463;&#30340;&#21477;&#23376;&#65307;&#32780;NoCoLA_zero&#21017;&#26159;&#19968;&#20010;&#23436;&#20840;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#35757;&#32451;&#21363;&#21487;&#22312;&#38646;&#26679;&#26412;&#26041;&#24335;&#19979;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#35821;&#27861;&#21028;&#26029;&#30340;&#32431;&#35786;&#26029;&#20219;&#21153;&#12290;&#26412;&#25991;&#35814;&#32454;&#25551;&#36848;&#20102;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#29992;&#23427;&#20204;&#26469;&#20026;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23545;&#29616;&#26377;&#30340;&#25386;&#23041;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
While there has been a surge of large language models for Norwegian in recent years, we lack any tool to evaluate their understanding of grammaticality. We present two new Norwegian datasets for this task. NoCoLA_class is a supervised binary classification task where the goal is to discriminate between acceptable and non-acceptable sentences. On the other hand, NoCoLA_zero is a purely diagnostic task for evaluating the grammatical judgement of a language model in a completely zero-shot manner, i.e. without any further training. In this paper, we describe both datasets in detail, show how to use them for different flavors of language models, and conduct a comparative study of the existing Norwegian language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20113;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20174;&#23458;&#25143;&#35780;&#35770;&#20013;&#25552;&#21462;&#35265;&#35299;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#30340;&#32452;&#21512;&#27169;&#22411;&#20351;&#29992;&#20102;&#36716;&#25442;&#22120;&#31070;&#32463;&#32593;&#32476;&#12289;&#21521;&#37327;&#23884;&#20837;&#21644;&#32858;&#31867;&#65292;&#24050;&#32463;&#38598;&#25104;&#24182;&#36827;&#19968;&#27493;&#21457;&#23637;&#65292;&#20197;&#26356;&#22909;&#22320;&#28385;&#36275;&#39640;&#25928;&#20449;&#24687;&#25552;&#21462;&#12289;&#25552;&#21462;&#20449;&#24687;&#30340;&#20027;&#39064;&#24314;&#27169;&#21644;&#29992;&#25143;&#38656;&#27714;&#30340;&#35201;&#27714;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26412;&#31995;&#32479;&#21487;&#20197;&#27604;&#29616;&#26377;&#30340;&#20027;&#39064;&#24314;&#27169;&#21644;&#20851;&#38190;&#23383;&#25552;&#21462;&#35299;&#20915;&#26041;&#26696;&#33719;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.07786</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#20113;&#30340;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#65292;&#26377;&#25928;&#20174;&#23458;&#25143;&#35780;&#35770;&#20013;&#25552;&#21462;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
A Cloud-based Machine Learning Pipeline for the Efficient Extraction of Insights from Customer Reviews. (arXiv:2306.07786v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20113;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20174;&#23458;&#25143;&#35780;&#35770;&#20013;&#25552;&#21462;&#35265;&#35299;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#30340;&#32452;&#21512;&#27169;&#22411;&#20351;&#29992;&#20102;&#36716;&#25442;&#22120;&#31070;&#32463;&#32593;&#32476;&#12289;&#21521;&#37327;&#23884;&#20837;&#21644;&#32858;&#31867;&#65292;&#24050;&#32463;&#38598;&#25104;&#24182;&#36827;&#19968;&#27493;&#21457;&#23637;&#65292;&#20197;&#26356;&#22909;&#22320;&#28385;&#36275;&#39640;&#25928;&#20449;&#24687;&#25552;&#21462;&#12289;&#25552;&#21462;&#20449;&#24687;&#30340;&#20027;&#39064;&#24314;&#27169;&#21644;&#29992;&#25143;&#38656;&#27714;&#30340;&#35201;&#27714;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26412;&#31995;&#32479;&#21487;&#20197;&#27604;&#29616;&#26377;&#30340;&#20027;&#39064;&#24314;&#27169;&#21644;&#20851;&#38190;&#23383;&#25552;&#21462;&#35299;&#20915;&#26041;&#26696;&#33719;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#25928;&#29575;&#26377;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#20219;&#21153;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#32771;&#34385;&#21040;&#29305;&#23450;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20113;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#38598;&#25104;&#21040;&#31649;&#36947;&#20013;&#65292;&#20174;&#23458;&#25143;&#35780;&#35770;&#20013;&#25552;&#21462;&#35265;&#35299;&#12290;&#23545;&#20110;&#20027;&#39064;&#24314;&#27169;&#65292;&#25105;&#20204;&#30340;&#32452;&#21512;&#27169;&#22411;&#20351;&#29992;&#20102;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31070;&#32463;&#32593;&#32476;&#12289;&#22522;&#20110;&#21521;&#37327;&#23884;&#20837;&#30340;&#20851;&#38190;&#23383;&#25552;&#21462;&#21644;&#32858;&#31867;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20803;&#32032;&#24050;&#32463;&#38598;&#25104;&#24182;&#36827;&#19968;&#27493;&#21457;&#23637;&#65292;&#20197;&#26356;&#22909;&#22320;&#28385;&#36275;&#39640;&#25928;&#20449;&#24687;&#25552;&#21462;&#12289;&#25552;&#21462;&#20449;&#24687;&#30340;&#20027;&#39064;&#24314;&#27169;&#21644;&#29992;&#25143;&#38656;&#27714;&#30340;&#35201;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#27604;&#36825;&#20010;&#20219;&#21153;&#29616;&#26377;&#30340;&#20027;&#39064;&#24314;&#27169;&#21644;&#20851;&#38190;&#23383;&#25552;&#21462;&#35299;&#20915;&#26041;&#26696;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#23458;&#25143;&#35780;&#35770;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#39564;&#35777;&#24182;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The efficiency of natural language processing has improved dramatically with the advent of machine learning models, particularly neural network-based solutions. However, some tasks are still challenging, especially when considering specific domains. In this paper, we present a cloud-based system that can extract insights from customer reviews using machine learning methods integrated into a pipeline. For topic modeling, our composite model uses transformer-based neural networks designed for natural language processing, vector embedding-based keyword extraction, and clustering. The elements of our model have been integrated and further developed to meet better the requirements of efficient information extraction, topic modeling of the extracted information, and user needs. Furthermore, our system can achieve better results than this task's existing topic modeling and keyword extraction solutions. Our approach is validated and compared with other state-of-the-art methods using publicly a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#24335;&#20998;&#35299;&#23376;&#35789;&#32534;&#30721;&#30340;&#20998;&#35789;&#26041;&#27861;&#65292;&#34987;&#31216;&#20026;&#8220;&#22240;&#24335;&#20998;&#35299;&#22120;&#8221;&#65292;&#22312;&#19971;&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#35821;&#35328;&#24314;&#27169;&#21644;&#24418;&#24577;&#21477;&#27861;&#20219;&#21153;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#30456;&#36739;&#20110;&#24120;&#29992;&#30340;BPE&#31639;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#36866;&#24212;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07764</link><description>&lt;p&gt;
&#22240;&#24335;&#20998;&#35299;&#23376;&#35789;&#32534;&#30721;&#30340;&#20998;&#35789;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tokenization with Factorized Subword Encoding. (arXiv:2306.07764v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#24335;&#20998;&#35299;&#23376;&#35789;&#32534;&#30721;&#30340;&#20998;&#35789;&#26041;&#27861;&#65292;&#34987;&#31216;&#20026;&#8220;&#22240;&#24335;&#20998;&#35299;&#22120;&#8221;&#65292;&#22312;&#19971;&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#35821;&#35328;&#24314;&#27169;&#21644;&#24418;&#24577;&#21477;&#27861;&#20219;&#21153;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#30456;&#36739;&#20110;&#24120;&#29992;&#30340;BPE&#31639;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#36866;&#24212;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35821;&#35328;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22823;&#12289;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#28982;&#32780;&#36825;&#20123;&#27169;&#22411;&#30340;&#36755;&#20837;&#34920;&#31034;&#20173;&#28982;&#20381;&#36182;&#20110;&#31616;&#21333;&#21644;&#36138;&#23146;&#30340;&#23376;&#35789;&#20998;&#35789;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#35789;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;VQ-VAE&#27169;&#22411;&#23558;&#23376;&#35789;&#22240;&#23376;&#21270;&#20026;&#31163;&#25955;&#19977;&#20803;&#32452;&#12290;&#25152;&#25552;&#20986;&#30340;&#20998;&#35789;&#26041;&#27861;&#34987;&#31216;&#20026;&#8220;&#22240;&#24335;&#20998;&#35299;&#22120;&#8221;&#65292;&#24182;&#22312;&#19971;&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#35821;&#35328;&#24314;&#27169;&#21644;&#24418;&#24577;&#21477;&#27861;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#27604;&#24120;&#29992;&#30340;&#23383;&#33410;&#23545;&#32534;&#30721;(BPE)&#20998;&#35789;&#31639;&#27861;&#26356;&#36866;&#21512;&#21644;&#26356;&#31283;&#20581;&#20110;&#24418;&#24577;&#21477;&#27861;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, language models have become increasingly larger and more complex. However, the input representations for these models continue to rely on simple and greedy subword tokenization methods. In this paper, we propose a novel tokenization method that factorizes subwords onto discrete triplets using a VQ-VAE model. The effectiveness of the proposed tokenization method, referred to as the Factorizer, is evaluated on language modeling and morpho-syntactic tasks for 7 diverse languages. Results indicate that this method is more appropriate and robust for morphological tasks than the commonly used byte-pair encoding (BPE) tokenization algorithm.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;$\pi$-KRVI&#30340;&#20048;&#35266;&#20462;&#25913;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#26680;&#23725;&#22238;&#24402;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#12290;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#19968;&#33324;&#35774;&#32622;&#19979;&#31532;&#19968;&#20010;&#26368;&#20248;&#36951;&#25022;&#20445;&#35777;&#65292;&#24182;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20248;&#32467;&#26524;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#22810;&#39033;&#24335;&#20302;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2306.07745</link><description>&lt;p&gt;
&#26680;&#21270;&#24378;&#21270;&#23398;&#20064;&#21450;&#20854;&#36817;&#20284;&#26041;&#27861;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Kernelized Reinforcement Learning with Order Optimal Regret Bounds. (arXiv:2306.07745v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07745
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;$\pi$-KRVI&#30340;&#20048;&#35266;&#20462;&#25913;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#26680;&#23725;&#22238;&#24402;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#12290;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#19968;&#33324;&#35774;&#32622;&#19979;&#31532;&#19968;&#20010;&#26368;&#20248;&#36951;&#25022;&#20445;&#35777;&#65292;&#24182;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20248;&#32467;&#26524;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#22810;&#39033;&#24335;&#20302;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#21508;&#31181;&#20855;&#26377;&#22797;&#26434;&#27169;&#22411;&#21644;&#22823;&#29366;&#24577;-&#34892;&#20026;&#31354;&#38388;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#26174;&#31034;&#20986;&#20102;&#23454;&#35777;&#30340;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30340;&#20998;&#26512;&#32467;&#26524;&#36890;&#24120;&#38598;&#20013;&#20110;&#20855;&#26377;&#23569;&#37327;&#29366;&#24577;-&#34892;&#20026;&#25110;&#31616;&#21333;&#27169;&#22411;&#65288;&#20363;&#22914;&#32447;&#24615;&#24314;&#27169;&#29366;&#24577;-&#34892;&#20026;&#20540;&#20989;&#25968;&#65289;&#30340;&#35774;&#32622;&#12290; &#20026;&#20102;&#25512;&#23548;&#26377;&#25928;&#22788;&#29702;&#26356;&#24191;&#27867;&#20540;&#20989;&#25968;&#30340;&#22823;&#29366;&#24577;-&#34892;&#20026;&#31354;&#38388;&#30340;RL&#31574;&#30053;&#65292;&#19968;&#20123;&#26368;&#26032;&#24037;&#20316;&#32771;&#34385;&#20351;&#29992;&#26680;&#23725;&#22238;&#24402;&#36827;&#34892;&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#31216;&#20026;$\pi$-KRVI&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#26368;&#23567;&#20108;&#20056;&#20540;&#36845;&#20195;&#30340;&#19968;&#31181;&#20048;&#35266;&#20462;&#25913;&#65292;&#24403;&#29366;&#24577;-&#34892;&#20026;&#20540;&#20989;&#25968;&#30001;RKHS&#34920;&#31034;&#26102;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19968;&#33324;&#35774;&#32622;&#19979;&#31532;&#19968;&#20010;&#26368;&#20248;&#36951;&#25022;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#35768;&#22810;&#20855;&#26377;&#39640;&#24230;&#38750;&#20809;&#28369;&#20869;&#26680;&#65288;&#20363;&#22914;&#31070;&#32463;&#20999;&#21521;&#20869;&#26680;&#25110;&#26576;&#20123;Mat\'ern&#20869;&#26680;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20248;&#32467;&#26524;&#65292;&#23384;&#22312;&#26174;&#30528;&#30340;&#22810;&#39033;&#24335;&#20302;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has shown empirical success in various real world settings with complex models and large state-action spaces. The existing analytical results, however, typically focus on settings with a small number of state-actions or simple models such as linearly modeled state-action value functions. To derive RL policies that efficiently handle large state-action spaces with more general value functions, some recent works have considered nonlinear function approximation using kernel ridge regression. We propose $\pi$-KRVI, an optimistic modification of least-squares value iteration, when the state-action value function is represented by an RKHS. We prove the first order-optimal regret guarantees under a general setting. Our results show a significant polynomial in the number of episodes improvement over the state of the art. In particular, with highly non-smooth kernels (such as Neural Tangent kernel or some Mat\'ern kernels) the existing results lead to trivial (superl
&lt;/p&gt;</description></item><item><title>V-LoL&#26159;&#19968;&#20010;&#32467;&#21512;&#35270;&#35273;&#21644;&#36923;&#36753;&#25361;&#25112;&#30340;&#35786;&#26029;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;V-LoL-Trains&#65292;&#35813;&#25968;&#25454;&#38598;&#39318;&#27425;&#23558;&#22797;&#26434;&#30340;&#35270;&#35273;&#22330;&#26223;&#21644;&#28789;&#27963;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#32467;&#21512;&#36215;&#26469;&#65292;&#20026;&#30740;&#31350;&#24191;&#27867;&#30340;&#35270;&#35273;&#36923;&#36753;&#23398;&#20064;&#25361;&#25112;&#25552;&#20379;&#20102;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2306.07743</link><description>&lt;p&gt;
V-LoL: &#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#36923;&#36753;&#23398;&#20064;&#30340;&#35786;&#26029;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
V-LoL: A Diagnostic Dataset for Visual Logical Learning. (arXiv:2306.07743v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07743
&lt;/p&gt;
&lt;p&gt;
V-LoL&#26159;&#19968;&#20010;&#32467;&#21512;&#35270;&#35273;&#21644;&#36923;&#36753;&#25361;&#25112;&#30340;&#35786;&#26029;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;V-LoL-Trains&#65292;&#35813;&#25968;&#25454;&#38598;&#39318;&#27425;&#23558;&#22797;&#26434;&#30340;&#35270;&#35273;&#22330;&#26223;&#21644;&#28789;&#27963;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#32467;&#21512;&#36215;&#26469;&#65292;&#20026;&#30740;&#31350;&#24191;&#27867;&#30340;&#35270;&#35273;&#36923;&#36753;&#23398;&#20064;&#25361;&#25112;&#25552;&#20379;&#20102;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#26399;&#22312;&#35270;&#35273;AI&#39046;&#22495;&#26377;&#20102;&#35768;&#22810;&#25104;&#21151;&#30340;&#36827;&#23637;&#65292;&#20294;&#20173;&#23384;&#22312;&#19981;&#21516;&#30340;&#32570;&#28857;&#65307;&#21253;&#25324;&#32570;&#23569;&#31934;&#30830;&#30340;&#36923;&#36753;&#25512;&#29702;&#12289;&#25277;&#35937;&#30340;&#27010;&#25324;&#33021;&#21147;&#20197;&#21450;&#29702;&#35299;&#22797;&#26434;&#21644;&#22024;&#26434;&#30340;&#22330;&#26223;&#31561;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#24182;&#19981;&#33021;&#25429;&#25417;&#21040;&#36825;&#20123;&#26041;&#38754;&#20013;&#30340;&#22810;&#25968;&#12290;&#28145;&#24230;&#23398;&#20064;&#25968;&#25454;&#38598;&#20851;&#27880;&#35270;&#35273;&#22797;&#26434;&#25968;&#25454;&#20294;&#21482;&#26377;&#31616;&#21333;&#30340;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#65292;&#24402;&#32435;&#36923;&#36753;&#25968;&#25454;&#38598;&#21253;&#25324;&#22797;&#26434;&#30340;&#36923;&#36753;&#23398;&#20064;&#20219;&#21153;&#65292;&#20294;&#26159;&#32570;&#20047;&#35270;&#35273;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35270;&#35273;&#36923;&#36753;&#23398;&#20064;&#25968;&#25454;&#38598;V-LoL&#65292;&#23427;&#26080;&#32541;&#22320;&#32467;&#21512;&#20102;&#35270;&#35273;&#21644;&#36923;&#36753;&#30340;&#25361;&#25112;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#39318;&#27425;&#25512;&#20986;&#20102;V-LoL&#30340;&#31532;&#19968;&#20010;&#23454;&#20363;&#65292;&#21517;&#20026;V-LoL-Trains&#65292;&#23427;&#26159;&#31526;&#21495;AI&#20013;&#19968;&#20010;&#32463;&#20856;&#22522;&#20934;&#27979;&#35797;&#30340;&#35270;&#35273;&#21576;&#29616;&#65292;&#21363;Michalski&#28779;&#36710;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#20869;&#32467;&#21512;&#22797;&#26434;&#30340;&#35270;&#35273;&#22330;&#26223;&#21644;&#28789;&#27963;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;V-LoL-Trains&#20026;&#30740;&#31350;&#24191;&#27867;&#30340;&#35270;&#35273;&#36923;&#36753;&#23398;&#20064;&#25361;&#25112;&#25552;&#20379;&#20102;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the successes of recent developments in visual AI, different shortcomings still exist; from missing exact logical reasoning, to abstract generalization abilities, to understanding complex and noisy scenes. Unfortunately, existing benchmarks, were not designed to capture more than a few of these aspects. Whereas deep learning datasets focus on visually complex data but simple visual reasoning tasks, inductive logic datasets involve complex logical learning tasks, however, lack the visual component. To address this, we propose the visual logical learning dataset, V-LoL, that seamlessly combines visual and logical challenges. Notably, we introduce the first instantiation of V-LoL, V-LoL-Trains, -- a visual rendition of a classic benchmark in symbolic AI, the Michalski train problem. By incorporating intricate visual scenes and flexible logical reasoning tasks within a versatile framework, V-LoL-Trains provides a platform for investigating a wide range of visual logical learning ch
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#29289;&#29702;-&#35745;&#31639;&#31995;&#32479;&#20013;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#26292;&#38706;&#27169;&#22411;&#20110;&#20998;&#24067;&#20043;&#22806;&#30340;&#26679;&#26412;&#26469;&#27979;&#35797;&#20854;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#21644;&#21709;&#24212;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34429;&#33021;&#36798;&#39640;&#31934;&#24230;&#65292;&#20294;&#20854;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#36824;&#19981;&#36275;&#20197;&#24212;&#29992;&#20110;&#23454;&#38469;&#29289;&#29702;-&#35745;&#31639;&#31995;&#32479;&#20013;&#12290;</title><link>http://arxiv.org/abs/2306.07737</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#29289;&#29702;-&#35745;&#31639;&#31995;&#32479;&#20013;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Robustness and Generalization Performance of Deep Learning Models on Cyber-Physical Systems: A Comparative Study. (arXiv:2306.07737v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#29289;&#29702;-&#35745;&#31639;&#31995;&#32479;&#20013;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#26292;&#38706;&#27169;&#22411;&#20110;&#20998;&#24067;&#20043;&#22806;&#30340;&#26679;&#26412;&#26469;&#27979;&#35797;&#20854;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#21644;&#21709;&#24212;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34429;&#33021;&#36798;&#39640;&#31934;&#24230;&#65292;&#20294;&#20854;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#36824;&#19981;&#36275;&#20197;&#24212;&#29992;&#20110;&#23454;&#38469;&#29289;&#29702;-&#35745;&#31639;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#20294;&#22312;&#29289;&#29702;-&#35745;&#31639;&#31995;&#32479;&#20013;&#24212;&#29992;&#21463;&#21040;&#36825;&#20123;&#26041;&#27861;&#40065;&#26834;&#24615;&#30340;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#22312;&#26469;&#33258;&#29289;&#29702;-&#35745;&#31639;&#31995;&#32479;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#27169;&#22411;&#22788;&#29702;&#19968;&#31995;&#21015;&#25200;&#21160;&#30340;&#33021;&#21147;&#65292;&#22914;&#20256;&#24863;&#22120;&#25925;&#38556;&#21644;&#22122;&#22768;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#23545;&#25972;&#20307;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#23427;&#20204;&#26292;&#38706;&#20110;&#20998;&#24067;&#20043;&#22806;&#65288;OOD&#65289;&#30340;&#26679;&#26412;&#26469;&#27979;&#35797;&#36825;&#20123;&#27169;&#22411;&#30340;&#27867;&#21270;&#21644;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#12290;&#36825;&#20123;&#26679;&#26412;&#21253;&#25324;&#20559;&#31163;&#26631;&#20934;&#31995;&#32479;&#25805;&#20316;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#22522;&#30784;&#29289;&#29702;&#31995;&#32479;&#30340;&#26680;&#24515;&#21160;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#27169;&#22411;&#23545;&#20960;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#21709;&#24212;&#33021;&#21147;&#65292;&#21253;&#25324;&#28155;&#21152;&#22122;&#22768;&#21644;&#26102;&#38388;&#25197;&#26354;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26694;&#26550;&#21033;&#29992;&#20102;&#19968;&#20010;&#27169;&#25311;&#30340;&#19977;&#32592;&#31995;&#32479;&#65292;&#20316;&#20026;&#30740;&#31350;&#29289;&#29702;-&#35745;&#31639;&#31995;&#32479;&#20013;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#26032;&#22411;&#22522;&#20934;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#36798;&#21040;&#39640;&#31934;&#24230;&#65292;&#20294;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#20197;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#29289;&#29702;-&#35745;&#31639;&#31995;&#32479;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) models have seen increased attention for time series forecasting, yet the application on cyber-physical systems (CPS) is hindered by the lacking robustness of these methods. Thus, this study evaluates the robustness and generalization performance of DL architectures on multivariate time series data from CPS. Our investigation focuses on the models' ability to handle a range of perturbations, such as sensor faults and noise, and assesses their impact on overall performance. Furthermore, we test the generalization and transfer learning capabilities of these models by exposing them to out-of-distribution (OOD) samples. These include deviations from standard system operations, while the core dynamics of the underlying physical system are preserved. Additionally, we test how well the models respond to several data augmentation techniques, including added noise and time warping. Our experimental framework utilizes a simulated three-tank system, proposed as a novel benchmar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19978;&#19979;&#25991;&#35789;&#20856;&#26597;&#25214;&#30340;&#26041;&#27861;&#65292;&#20351;&#20256;&#32479;&#30340;&#23884;&#20837;&#27169;&#22411;&#33021;&#22815;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#23398;&#20064;&#20851;&#31995;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#65292;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#25110;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#31616;&#21333;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2306.07719</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#35789;&#20856;&#26597;&#25214;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Contextual Dictionary Lookup for Knowledge Graph Completion. (arXiv:2306.07719v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19978;&#19979;&#25991;&#35789;&#20856;&#26597;&#25214;&#30340;&#26041;&#27861;&#65292;&#20351;&#20256;&#32479;&#30340;&#23884;&#20837;&#27169;&#22411;&#33021;&#22815;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#23398;&#20064;&#20851;&#31995;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#65292;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#25110;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#31616;&#21333;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26088;&#22312;&#36890;&#36807;&#20174;&#24050;&#30693;&#19977;&#20803;&#32452;&#20013;&#39044;&#27979;&#32570;&#22833;&#38142;&#25509;&#26469;&#35299;&#20915;&#30693;&#35782;&#22270;&#35889;(KGs)&#30340;&#19981;&#23436;&#25972;&#24615;&#65292;&#24050;&#32463;&#26377;&#35768;&#22810;&#30693;&#35782;&#22270;&#23884;&#20837;(KGE)&#27169;&#22411;&#26469;&#25191;&#34892;KGC&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#23884;&#20837;&#27169;&#22411;&#23558;&#27599;&#20010;&#20851;&#31995;&#26144;&#23556;&#21040;&#19968;&#20010;&#21807;&#19968;&#30340;&#21521;&#37327;&#65292;&#24573;&#30053;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#23454;&#20307;&#19979;&#30340;&#20855;&#20307;&#32454;&#31890;&#24230;&#35821;&#20041;&#12290;&#27492;&#22806;&#65292;&#23569;&#37327;&#21487;&#29992;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#27169;&#22411;&#20381;&#38752;&#32858;&#31867;&#31639;&#27861;&#65292;&#30001;&#20110;&#32321;&#29712;&#30340;&#20108;&#38454;&#27573;&#35757;&#32451;&#36807;&#31243;&#65292;&#23548;&#33268;&#20102;&#26377;&#38480;&#30340;&#24615;&#33021;&#21644;&#36866;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#19978;&#19979;&#25991;&#35789;&#20856;&#26597;&#25214;&#30340;&#26032;&#26041;&#27861;&#65292;&#20351;&#20256;&#32479;&#30340;&#23884;&#20837;&#27169;&#22411;&#33021;&#22815;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#23398;&#20064;&#20851;&#31995;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#12290;&#26356;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#21253;&#21547;&#22810;&#20010;&#28508;&#22312;&#35821;&#20041;&#30340;&#35789;&#20856;&#26469;&#34920;&#31034;&#27599;&#20010;&#20851;&#31995;&#12290;&#32473;&#23450;&#23454;&#20307;&#19982;&#35789;&#20856;&#30340;&#20013;&#24515;&#35821;&#20041;&#30340;&#32452;&#21512;&#29992;&#20316;&#35789;&#20856;&#26597;&#25214;&#23618;&#30340;&#19978;&#19979;&#25991;&#65292;&#35813;&#23618;&#36820;&#22238;&#32473;&#23450;&#23454;&#20307;&#19979;&#20851;&#31995;&#30340;&#20855;&#20307;&#35821;&#20041;&#12290;&#25105;&#20204;&#20351;&#29992;&#19982;&#29616;&#26377;&#23884;&#20837;&#27169;&#22411;&#20860;&#23481;&#30340;&#27169;&#22359;&#21270;&#35774;&#35745;&#38598;&#25104;&#36825;&#20010;&#23618;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#25110;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#65292;&#24182;&#20855;&#26377;&#26356;&#31616;&#21333;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph completion (KGC) aims to solve the incompleteness of knowledge graphs (KGs) by predicting missing links from known triples, numbers of knowledge graph embedding (KGE) models have been proposed to perform KGC by learning embeddings. Nevertheless, most existing embedding models map each relation into a unique vector, overlooking the specific fine-grained semantics of them under different entities. Additionally, the few available fine-grained semantic models rely on clustering algorithms, resulting in limited performance and applicability due to the cumbersome two-stage training process. In this paper, we present a novel method utilizing contextual dictionary lookup, enabling conventional embedding models to learn fine-grained semantics of relations in an end-to-end manner. More specifically, we represent each relation using a dictionary that contains multiple latent semantics. The composition of a given entity and the dictionary's central semantics serves as the context f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;TOPSIS&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;TOPSIS-Explorer&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21487;&#35270;&#21270;&#26041;&#24335;&#35299;&#37322;&#26435;&#37325;&#21644;&#32858;&#21512;&#23545;&#25490;&#21517;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#23545;&#23454;&#38469;&#24212;&#29992;&#26377;&#30528;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2306.07706</link><description>&lt;p&gt;
&#26397;&#30528;&#21487;&#35299;&#37322;&#24615;TOPSIS&#65306;&#26435;&#37325;&#21644;&#32858;&#21512;&#23545;&#25490;&#21517;&#30340;&#24433;&#21709;&#30340;&#35270;&#35273;&#27934;&#23519;&#21147;
&lt;/p&gt;
&lt;p&gt;
Towards Explainable TOPSIS: Visual Insights into the Effects of Weights and Aggregations on Rankings. (arXiv:2306.07706v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;TOPSIS&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;TOPSIS-Explorer&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21487;&#35270;&#21270;&#26041;&#24335;&#35299;&#37322;&#26435;&#37325;&#21644;&#32858;&#21512;&#23545;&#25490;&#21517;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#23545;&#23454;&#38469;&#24212;&#29992;&#26377;&#30528;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26631;&#20934;&#20915;&#31574;&#20998;&#26512;&#65288;MCDA&#65289;&#22312;&#21508;&#20010;&#34892;&#19994;&#20013;&#24191;&#27867;&#29992;&#20110;&#35780;&#20272;&#21644;&#25490;&#21517;&#22791;&#36873;&#26041;&#26696;&#12290;&#22312;&#20247;&#22810;MCDA&#26041;&#27861;&#20013;&#65292;TOPSIS&#20173;&#28982;&#26159;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#26368;&#21463;&#27426;&#36814;&#30340;&#36873;&#25321;&#20043;&#19968;&#12290;TOPSIS&#35745;&#31639;&#32771;&#34385;&#30340;&#22791;&#36873;&#26041;&#26696;&#19982;&#20004;&#20010;&#39044;&#23450;&#20041;&#26041;&#26696;&#65288;&#21363;&#29702;&#24819;&#29366;&#24577;&#21644;&#21453;&#29702;&#24819;&#29366;&#24577;&#65289;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#24182;&#26681;&#25454;&#36825;&#20123;&#36317;&#31163;&#30340;&#32858;&#21512;&#20540;&#21019;&#24314;&#22791;&#36873;&#26041;&#26696;&#30340;&#25490;&#21517;&#12290;&#28982;&#32780;&#65292;TOPSIS&#30340;&#20869;&#37096;&#24037;&#20316;&#35299;&#37322;&#26159;&#22256;&#38590;&#30340;&#65292;&#29305;&#21035;&#26159;&#24403;&#26631;&#20934;&#25968;&#30446;&#24456;&#22823;&#26102;&#12290;&#20026;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#21487;&#20197;&#20351;&#29992;&#22791;&#36873;&#26041;&#26696;&#30340;&#24179;&#22343;&#20540;&#65288;M&#65289;&#21644;&#26631;&#20934;&#20559;&#24046;&#65288;SD&#65289;&#26469;&#34920;&#31034;TOPSIS&#32858;&#21512;&#20540;&#65292;&#20174;&#32780;&#21019;&#24314;MSD&#31354;&#38388;&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#35270;&#21270;&#24182;&#35299;&#37322;&#32858;&#21512;&#30340;&#24037;&#20855;&#12290;&#21363;&#20351;MSD&#31354;&#38291;&#38750;&#24120;&#26377;&#29992;&#65292;&#20294;&#23427;&#20551;&#35774;&#26631;&#20934;&#21516;&#26679;&#37325;&#35201;&#65292;&#20351;&#20854;&#22312;&#23454;&#38469;&#25490;&#21517;&#38382;&#39064;&#20013;&#30340;&#36866;&#29992;&#24615;&#38477;&#20302; &#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#24191;&#20102; TOPSIS &#32467;&#26524;&#30340;&#36716;&#25442;&#65292;&#20351;&#24471;&#19981;&#21516;&#30340;&#26631;&#20934;&#21487;&#20197;&#35299;&#37322;&#20026;&#35270;&#35273;&#19978;&#30340; MSD &#31354;&#38388;&#65292;&#20197;&#27492;&#26469;&#22788;&#29702;&#23558;&#26435;&#37325;&#21152;&#20837;TOPSIS&#38382;&#39064;&#30340;&#22330;&#26223;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21152;&#26435; MSD &#31354;&#38388;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#65292;&#31216;&#20026; TOPSIS-Explorer&#65292;&#25552;&#20379;&#26131;&#20110;&#29702;&#35299;&#30340;&#35270;&#35273;&#27934;&#23519;&#21147;&#65292;&#20197;&#20998;&#26512;&#19981;&#21516;&#26435;&#37325;&#21644;&#32858;&#21512;&#20540;&#23545;&#25490;&#21517;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20379;&#24212;&#21830;&#36873;&#25321;&#23454;&#38469;&#26696;&#20363;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#25552;&#20379;&#21487;&#35299;&#37322;TOPSIS&#32467;&#26524;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Criteria Decision Analysis (MCDA) is extensively used across diverse industries to assess and rank alternatives. Among numerous MCDA methods developed to solve real-world ranking problems, TOPSIS remains one of the most popular choices in many application areas. TOPSIS calculates distances between the considered alternatives and two predefined ones, namely the ideal and the anti-ideal, and creates a ranking of the alternatives according to a chosen aggregation of these distances. However, the interpretation of the inner workings of TOPSIS is difficult, especially when the number of criteria is large. To this end, recent research has shown that TOPSIS aggregations can be expressed using the means (M) and standard deviations (SD) of alternatives, creating MSD-space, a tool for visualizing and explaining aggregations. Even though MSD-space is highly useful, it assumes equally important criteria, making it less applicable to real-world ranking problems. In this paper, we generalize t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#22270;&#24207;&#21015;&#39044;&#27979;&#30340;&#26102;&#24577;&#22270;&#32467;&#26500;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#28508;&#22312;&#30340;&#26102;&#38388;&#36793;&#26469;&#23398;&#20064;&#26356;&#22909;&#30340;&#22270;&#20687;&#32467;&#26500;&#65292;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07699</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#38388;&#22270;&#24207;&#21015;&#39044;&#27979;&#30340;&#26102;&#24577;&#22270;&#32467;&#26500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Time-aware Graph Structure Learning via Sequence Prediction on Temporal Graphs. (arXiv:2306.07699v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07699
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#22270;&#24207;&#21015;&#39044;&#27979;&#30340;&#26102;&#24577;&#22270;&#32467;&#26500;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#28508;&#22312;&#30340;&#26102;&#38388;&#36793;&#26469;&#23398;&#20064;&#26356;&#22909;&#30340;&#22270;&#20687;&#32467;&#26500;&#65292;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26088;&#22312;&#27169;&#25311;&#22270;&#20687;&#30340;&#20256;&#36882;&#24615;&#36136;&#30340;&#26102;&#38388;&#22270;&#23398;&#20064;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#19978;&#65292;&#22270;&#20687;&#32467;&#26500;&#24448;&#24448;&#26159;&#19981;&#23436;&#25972;&#21644;&#22024;&#26434;&#30340;&#65292;&#36825;&#38459;&#30861;&#20102;&#26102;&#38388;&#22270;&#32593;&#32476;&#65288;TGN&#65289;&#23398;&#20064;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#22270;&#24207;&#21015;&#39044;&#27979;&#30340;&#26102;&#24577;&#22270;&#32467;&#26500;&#23398;&#20064;&#65288;TGSL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#28508;&#22312;&#30340;&#26102;&#38388;&#36793;&#23398;&#20064;&#26356;&#22909;&#30340;&#22270;&#20687;&#32467;&#26500;&#65292;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal Graph Learning, which aims to model the time-evolving nature of graphs, has gained increasing attention and achieved remarkable performance recently. However, in reality, graph structures are often incomplete and noisy, which hinders temporal graph networks (TGNs) from learning informative representations. Graph contrastive learning uses data augmentation to generate plausible variations of existing data and learn robust representations. However, rule-based augmentation approaches may be suboptimal as they lack learnability and fail to leverage rich information from downstream tasks. To address these issues, we propose a Time-aware Graph Structure Learning (TGSL) approach via sequence prediction on temporal graphs, which learns better graph structures for downstream tasks through adding potential temporal edges. In particular, it predicts time-aware context embedding based on previously observed interactions and uses the Gumble-Top-K to select the closest candidate edges to th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;StyleTTS 2&#30340;TTS&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#21644;&#22823;&#22411;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#26469;&#23454;&#29616;&#20154;&#31867;&#32423;&#21035;&#30340;&#35821;&#38899;&#21512;&#25104;&#12290;&#19982;&#20197;&#24448;&#27169;&#22411;&#19981;&#21516;&#65292;StyleTTS 2&#23558;&#26679;&#24335;&#35270;&#20026;&#28508;&#22312;&#38543;&#26426;&#21464;&#37327;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#26368;&#36866;&#21512;&#25991;&#26412;&#30340;&#26679;&#24335;&#65292;&#21516;&#26102;&#21463;&#30410;&#20110;&#22823;&#22411;SLMs&#21644;&#26032;&#30340;&#21487;&#24494;&#20998;&#25345;&#32493;&#26102;&#38388;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2306.07691</link><description>&lt;p&gt;
StyleTTS 2&#65306;&#36890;&#36807;&#39118;&#26684;&#25193;&#25955;&#21644;&#19982;&#22823;&#22411;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#35757;&#32451;&#23454;&#29616;&#20154;&#31867;&#32423;&#21035;&#30340;&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models. (arXiv:2306.07691v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;StyleTTS 2&#30340;TTS&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#21644;&#22823;&#22411;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#26469;&#23454;&#29616;&#20154;&#31867;&#32423;&#21035;&#30340;&#35821;&#38899;&#21512;&#25104;&#12290;&#19982;&#20197;&#24448;&#27169;&#22411;&#19981;&#21516;&#65292;StyleTTS 2&#23558;&#26679;&#24335;&#35270;&#20026;&#28508;&#22312;&#38543;&#26426;&#21464;&#37327;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#26368;&#36866;&#21512;&#25991;&#26412;&#30340;&#26679;&#24335;&#65292;&#21516;&#26102;&#21463;&#30410;&#20110;&#22823;&#22411;SLMs&#21644;&#26032;&#30340;&#21487;&#24494;&#20998;&#25345;&#32493;&#26102;&#38388;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;StyleTTS2&#65292;&#19968;&#31181;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#39118;&#26684;&#25193;&#25955;&#21644;&#19982;&#22823;&#22411;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#65288;SLM&#65289;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26469;&#23454;&#29616;&#20154;&#31867;&#32423;&#21035;&#30340;TTS&#21512;&#25104;&#12290; StyleTTS 2&#36890;&#36807;&#23558;&#26679;&#24335;&#24314;&#27169;&#20026;&#28508;&#22312;&#30340;&#38543;&#26426;&#21464;&#37327;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#26368;&#36866;&#21512;&#25991;&#26412;&#30340;&#26679;&#24335;&#65292;&#26080;&#38656;&#21442;&#32771;&#35821;&#38899;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#28508;&#22312;&#25193;&#25955;&#65292;&#21516;&#26102;&#21463;&#30410;&#20110;&#25193;&#25955;&#27169;&#22411;&#25552;&#20379;&#30340;&#22810;&#26679;&#21270;&#35821;&#38899;&#21512;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#39044;&#20808;&#35757;&#32451;&#30340;SLMs&#65288;&#20363;&#22914;WavLM&#65289;&#20316;&#20026;&#37492;&#21035;&#22120;&#65292;&#24182;&#20351;&#29992;&#25105;&#20204;&#30340;&#26032;&#22411;&#21487;&#24494;&#20998;&#25345;&#32493;&#26102;&#38388;&#24314;&#27169;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35821;&#38899;&#30340;&#33258;&#28982;&#24230;&#12290; StyleTTS 2&#22312;&#21333;&#25196;&#22768;&#22120;LJSpeech&#25968;&#25454;&#38598;&#19978;&#36229;&#36234;&#20102;&#20154;&#31867;&#24405;&#38899;&#65292;&#24182;&#22312;&#22810;&#25196;&#22768;&#22120;VCTK&#25968;&#25454;&#38598;&#19978;&#19982;&#20043;&#21305;&#37197;&#65292;&#32463;&#36807;&#27597;&#35821;&#20026;&#33521;&#35821;&#30340;&#20154;&#21592;&#35780;&#21028;&#12290;&#27492;&#22806;&#65292;&#24403;&#22312;LibriTTS&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#32988;&#36807;&#20102;&#20197;&#21069;&#20844;&#24320;&#21487;&#29992;&#30340;&#38646;&#26679;&#26412;&#35828;&#35805;&#20154;&#35821;&#38899;&#21512;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present StyleTTS 2, a text-to-speech (TTS) model that leverages style diffusion and adversarial training with large speech language models (SLMs) to achieve human-level TTS synthesis. StyleTTS 2 differs from its predecessor by modeling styles as a latent random variable through diffusion models to generate the most suitable style for the text without requiring reference speech, achieving efficient latent diffusion while benefiting from the diverse speech synthesis offered by diffusion models. Furthermore, we employ large pre-trained SLMs, such as WavLM, as discriminators with our novel differentiable duration modeling for end-to-end training, resulting in improved speech naturalness. StyleTTS 2 surpasses human recordings on the single-speaker LJSpeech dataset and matches it on the multispeaker VCTK dataset as judged by native English speakers. Moreover, when trained on the LibriTTS dataset, our model outperforms previous publicly available models for zero-shot speaker
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#32423;&#25345;&#20037;&#24615;&#23041;&#32961;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#38450;&#24481;&#30340;&#23569;&#26679;&#26412;&#12289;&#22810;&#39046;&#22495;&#30693;&#35782;&#37325;&#35013; (FMKR) &#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#32593;&#32476;&#22495;&#29983;&#25104;&#22810;&#20010;&#23567;&#20219;&#21153;&#65292;&#23436;&#25104;&#22810;&#39046;&#22495;&#30340;&#30693;&#35782;&#37325;&#26032;&#35013;&#22791;&#65292;&#25552;&#39640;&#38450;&#24481;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07685</link><description>&lt;p&gt;
&#38754;&#21521;&#39640;&#32423;&#25345;&#20037;&#24615;&#23041;&#32961;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#38450;&#24481;&#30340;&#23569;&#26679;&#26412;&#22810;&#39046;&#22495;&#30693;&#35782;&#37325;&#35013;
&lt;/p&gt;
&lt;p&gt;
Few-shot Multi-domain Knowledge Rearming for Context-aware Defence against Advanced Persistent Threats. (arXiv:2306.07685v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#32423;&#25345;&#20037;&#24615;&#23041;&#32961;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#38450;&#24481;&#30340;&#23569;&#26679;&#26412;&#12289;&#22810;&#39046;&#22495;&#30693;&#35782;&#37325;&#35013; (FMKR) &#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#32593;&#32476;&#22495;&#29983;&#25104;&#22810;&#20010;&#23567;&#20219;&#21153;&#65292;&#23436;&#25104;&#22810;&#39046;&#22495;&#30340;&#30693;&#35782;&#37325;&#26032;&#35013;&#22791;&#65292;&#25552;&#39640;&#38450;&#24481;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32423;&#25345;&#20037;&#24615;&#23041;&#32961;(APTs)&#20855;&#26377;&#22810;&#38454;&#27573;&#28183;&#36879;&#12289;&#39640;&#24230;&#23450;&#21046;&#21270;&#24847;&#22270;&#21644;&#35268;&#36991;&#31574;&#30053;&#31561;&#26032;&#39062;&#29305;&#24449;&#12290;APTs&#30340;&#38450;&#24481;&#38656;&#35201;&#34701;&#21512;&#22810;&#32500;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;&#25968;&#25454;&#26469;&#35782;&#21035;&#25915;&#20987;&#24847;&#22270;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#26377;&#25928;&#30340;&#30693;&#35782;&#21457;&#29616;&#31574;&#30053;&#20197;&#35782;&#21035;&#23454;&#20307;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#32570;&#20047;&#23545;&#26032;&#30340;&#25110;&#26410;&#30693;&#26679;&#26412;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#38477;&#20302;&#20102;&#38450;&#24481;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#23558;&#36825;&#20123;APTs&#38450;&#24481;&#27169;&#22411;&#31169;&#26377;&#37096;&#32626;&#22312;&#24322;&#26500;&#29615;&#22659;&#20013;&#21644;&#21508;&#31181;&#32593;&#32476;&#35774;&#22791;&#19978;&#65292;&#38656;&#35201;&#22312;&#19978;&#19979;&#25991;&#24863;&#30693;&#26041;&#38754;&#36827;&#34892;&#37325;&#22823;&#25237;&#20837;&#65288;&#20363;&#22914;&#24050;&#30693;&#25915;&#20987;&#23454;&#20307;&#12289;&#36830;&#32493;&#30340;&#32593;&#32476;&#29366;&#24577;&#21644;&#24403;&#21069;&#30340;&#23433;&#20840;&#31574;&#30053;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#39640;&#32423;&#25345;&#20037;&#24615;&#23041;&#32961;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#38450;&#24481;&#30340;&#23569;&#26679;&#26412;&#22810;&#39046;&#22495;&#30693;&#35782;&#37325;&#35013;(FMKR)&#26041;&#26696;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#32593;&#32476;&#22495;&#29983;&#25104;&#22810;&#20010;&#23567;&#20219;&#21153;&#65292;&#23436;&#25104;&#22810;&#39046;&#22495;&#30693;&#35782;&#37325;&#26032;&#35013;&#22791;&#65292;&#20174;&#32780;&#25552;&#21319;&#38450;&#24481;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advanced persistent threats (APTs) have novel features such as multi-stage penetration, highly-tailored intention, and evasive tactics. APTs defense requires fusing multi-dimensional Cyber threat intelligence data to identify attack intentions and conducts efficient knowledge discovery strategies by data-driven machine learning to recognize entity relationships. However, data-driven machine learning lacks generalization ability on fresh or unknown samples, reducing the accuracy and practicality of the defense model. Besides, the private deployment of these APT defense models on heterogeneous environments and various network devices requires significant investment in context awareness (such as known attack entities, continuous network states, and current security strategies). In this paper, we propose a few-shot multi-domain knowledge rearming (FMKR) scheme for context-aware defense against APTs. By completing multiple small tasks that are generated from different network domains with m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#26102;&#25277;&#35937;&#35770;&#35777;&#26694;&#26550;&#30340;&#23450;&#26102;&#24182;&#21457;&#35821;&#35328;&#65292;&#29992;&#20110;&#27169;&#25311;&#20195;&#29702;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#24182;&#23558;&#20195;&#29702;&#30340;&#21487;&#25509;&#21463;&#24615;&#19982;&#32473;&#23450;&#26102;&#38388;&#38388;&#38548;&#36827;&#34892;&#25512;&#29702;&#21644;&#36890;&#20449;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#27169;&#25311;&#36777;&#35770;&#21644;&#23545;&#35805;&#28216;&#25103;&#30340;&#24773;&#22659;&#12290;</title><link>http://arxiv.org/abs/2306.07675</link><description>&lt;p&gt;
&#35770;&#36848;&#27169;&#22411;&#19982;&#23545;&#35805;&#28216;&#25103;&#30340;&#23450;&#26102;&#24182;&#21457;&#35821;&#35328;&#30340;&#20132;&#38169;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;
An Interleaving Semantics of the Timed Concurrent Language for Argumentation to Model Debates and Dialogue Games. (arXiv:2306.07675v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#26102;&#25277;&#35937;&#35770;&#35777;&#26694;&#26550;&#30340;&#23450;&#26102;&#24182;&#21457;&#35821;&#35328;&#65292;&#29992;&#20110;&#27169;&#25311;&#20195;&#29702;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#24182;&#23558;&#20195;&#29702;&#30340;&#21487;&#25509;&#21463;&#24615;&#19982;&#32473;&#23450;&#26102;&#38388;&#38388;&#38548;&#36827;&#34892;&#25512;&#29702;&#21644;&#36890;&#20449;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#27169;&#25311;&#36777;&#35770;&#21644;&#23545;&#35805;&#28216;&#25103;&#30340;&#24773;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#26159;&#24314;&#27169;&#26234;&#33021;&#20307;&#21160;&#24577;&#34892;&#20026;&#30340;&#20851;&#38190;&#22240;&#32032;&#65306;&#27963;&#21160;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#20855;&#26377;&#30830;&#23450;&#30340;&#26102;&#38388;&#38271;&#24230;&#65292;&#24182;&#19988;&#20808;&#21069;&#30340;&#34892;&#20026;&#20250;&#24433;&#21709;&#26234;&#33021;&#20307;&#30340;&#34892;&#20026;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#65292;&#29992;&#20110;&#24314;&#27169;&#20195;&#29702;&#20043;&#38388;&#30340;&#24182;&#21457;&#20132;&#20114;&#65292;&#21516;&#26102;&#20801;&#35768;&#25351;&#23450;&#29305;&#23450;&#34892;&#20026;&#21457;&#29983;&#30340;&#26102;&#38388;&#38388;&#38548;&#12290;&#36825;&#31181;&#35821;&#35328;&#21033;&#29992;&#23450;&#26102;&#30340;&#25277;&#35937;&#35770;&#35777;&#26694;&#26550;&#23454;&#29616;&#20195;&#29702;&#20351;&#29992;&#30340;&#20849;&#20139;&#20869;&#23384;&#65292;&#20197;&#22312;&#32473;&#23450;&#26102;&#38388;&#38388;&#38548;&#20869;&#36890;&#20449;&#21644;&#25512;&#29702;&#20851;&#20110;&#20182;&#20204;&#30340;&#20449;&#24565;&#30340;&#21487;&#25509;&#21463;&#24615;&#12290;&#22312;&#21333;&#20010;&#22788;&#29702;&#22120;&#19978;&#20351;&#29992;&#20132;&#38169;&#27169;&#22411;&#36827;&#34892;&#22522;&#26412;&#35745;&#31639;&#27493;&#39588;&#65292;&#26102;&#38548;&#26368;&#22823;&#24182;&#34892;&#24615;&#12290;&#25353;&#29031;&#36825;&#31181;&#26041;&#27861;&#65292;&#21482;&#26377;&#19968;&#20010;&#21487;&#29992;&#30340;&#20195;&#29702;&#22312;&#27599;&#20010;&#26102;&#21051;&#34987;&#25191;&#34892;&#12290;&#20026;&#20102;&#23637;&#31034;&#36825;&#31181;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#23427;&#26469;&#27169;&#25311;&#21457;&#29983;&#22312;&#26234;&#33021;&#20195;&#29702;&#20043;&#38388;&#30340;&#36777;&#35770;&#21644;&#23545;&#35805;&#28216;&#25103;&#31561;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time is a crucial factor in modelling dynamic behaviours of intelligent agents: activities have a determined temporal duration in a real-world environment, and previous actions influence agents' behaviour. In this paper, we propose a language for modelling concurrent interaction between agents that also allows the specification of temporal intervals in which particular actions occur. Such a language exploits a timed version of Abstract Argumentation Frameworks to realise a shared memory used by the agents to communicate and reason on the acceptability of their beliefs with respect to a given time interval. An interleaving model on a single processor is used for basic computation steps, with maximum parallelism for time elapsing. Following this approach, only one of the enabled agents is executed at each moment. To demonstrate the capabilities of language, we also show how it can be used to model interactions such as debates and dialogue games taking place between intelligent agents. La
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#21450;&#24494;&#35843;&#20013;&#20173;&#20855;&#26377;&#26174;&#33879;&#30340;&#25552;&#21319;&#20316;&#29992;&#65292;&#23588;&#20854;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#24494;&#35843;&#24615;&#33021;10%&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2306.07664</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#30340;&#26377;&#25928;&#24615;&#65306;&#19968;&#20010;&#32463;&#39564;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Rethink the Effectiveness of Text Data Augmentation: An Empirical Analysis. (arXiv:2306.07664v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#21450;&#24494;&#35843;&#20013;&#20173;&#20855;&#26377;&#26174;&#33879;&#30340;&#25552;&#21319;&#20316;&#29992;&#65292;&#23588;&#20854;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#24494;&#35843;&#24615;&#33021;10%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#36827;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#23545;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#34920;&#29616;&#30340;&#24433;&#21709;&#19968;&#30452;&#26159;&#19968;&#20010;&#20105;&#35770;&#30340;&#35805;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#24494;&#35843;&#26041;&#27861;&#32467;&#21512;&#22238;&#35793;&#22312;7&#20010;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#20998;&#31867;&#21644;&#22238;&#24402;&#31867;&#22411;&#65292;&#28085;&#30422;&#21333;&#21477;&#21644;&#21477;&#23376;&#23545;&#20219;&#21153;&#12290;&#19982;&#20248;&#20808;&#30340;&#20551;&#35774;&#30456;&#21453;&#65292;&#21363;&#25968;&#25454;&#22686;&#24378;&#23545;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#34920;&#29616;&#27809;&#26377;&#36129;&#29486;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#25345;&#32493;&#22312;&#22686;&#24378;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#26377;&#25928;&#22320;&#25913;&#21892;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#34920;&#29616;&#12290;&#22312;&#26368;&#26377;&#21033;&#30340;&#24773;&#20917;&#19979;&#65292;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#21487;&#20197;&#20351;&#24494;&#35843;&#24615;&#33021;&#22312;&#23567;&#26679;&#26412;&#23398;&#20064;&#35774;&#32622;&#19979;&#25552;&#39640;10%&#20197;&#19978;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#31361;&#26174;&#20986;&#25968;&#25454;&#22686;&#24378;&#20316;&#20026;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#24378;&#22823;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, language models (LMs) have made remarkable progress in advancing the field of natural language processing (NLP). However, the impact of data augmentation (DA) techniques on the fine-tuning (FT) performance of these LMs has been a topic of ongoing debate. In this study, we evaluate the effectiveness of three different FT methods in conjugation with back-translation across an array of 7 diverse NLP tasks, including classification and regression types, covering single-sentence and sentence-pair tasks. Contrary to prior assumptions that DA does not contribute to the enhancement of LMs' FT performance, our findings reveal that continued pre-training on augmented data can effectively improve the FT performance of the downstream tasks. In the most favourable case, continued pre-training improves the performance of FT by more than 10% in the few-shot learning setting. Our finding highlights the potential of DA as a powerful tool for bolstering LMs' performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26102;&#38388;&#21270;&#26412;&#20307;&#20013;&#20171;&#26597;&#35810;&#20013;&#21807;&#19968;&#21487;&#29305;&#24449;&#24615;&#21644;&#21487;&#23398;&#20064;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#20256;&#36882;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.07662</link><description>&lt;p&gt;
&#26102;&#38388;&#21270;&#26412;&#20307;&#20013;&#20171;&#26597;&#35810;&#30340;&#21807;&#19968;&#21487;&#29305;&#24449;&#24615;&#21644;&#21487;&#23398;&#20064;&#24615;
&lt;/p&gt;
&lt;p&gt;
Temporalising Unique Characterisability and Learnability of Ontology-Mediated Queries. (arXiv:2306.07662v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26102;&#38388;&#21270;&#26412;&#20307;&#20013;&#20171;&#26597;&#35810;&#20013;&#21807;&#19968;&#21487;&#29305;&#24449;&#24615;&#21644;&#21487;&#23398;&#20064;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#20256;&#36882;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36890;&#36807;&#31034;&#20363;&#26469;&#30740;&#31350;&#25968;&#25454;&#24211;&#26597;&#35810;&#30340;&#21807;&#19968;&#21487;&#29305;&#24449;&#24615;&#21644;&#21487;&#23398;&#20064;&#24615;&#24050;&#32463;&#25193;&#23637;&#21040;&#20102;&#26412;&#20307;&#20013;&#20171;&#26597;&#35810;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33719;&#24471;&#30340;&#32467;&#26524;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#21487;&#20197;&#25552;&#21319;&#21040;&#26102;&#38388;&#21270;&#30340;&#26412;&#20307;&#20013;&#20171;&#26597;&#35810;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#20171;&#32461;&#20102;&#38750;&#26102;&#38388;&#21270;&#24773;&#20917;&#19979;&#30456;&#20851;&#26041;&#27861;&#65292;&#28982;&#21518;&#23637;&#31034;&#20102;&#36890;&#29992;&#30340;&#20256;&#36882;&#32467;&#26524;&#65292;&#21487;&#20197;&#30830;&#23450;&#29616;&#26377;&#32467;&#26524;&#22312;&#20309;&#31181;&#26465;&#20214;&#19979;&#21487;&#20197;&#25512;&#24191;&#21040;&#26102;&#38388;&#21270;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the study of the unique characterisability and learnability of database queries by means of examples has been extended to ontology-mediated queries. Here, we study in how far the obtained results can be lifted to temporalised ontology-mediated queries. We provide a systematic introduction to the relevant approaches in the non-temporal case and then show general transfer results pinpointing under which conditions existing results can be lifted to temporalised queries.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;POCL&#26041;&#27861;&#30340;&#26032;&#22411;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#38388;HTN&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#24050;&#26377;&#30340;&#38750;&#26102;&#38388;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#36827;&#34892;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2306.07638</link><description>&lt;p&gt;
&#20851;&#20110;&#20351;&#29992;&#38750;&#26102;&#38388;&#21551;&#21457;&#24335;&#26041;&#27861;&#25351;&#23548;HTN&#26102;&#24577;&#35268;&#21010;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Guiding Search in HTN Temporal Planning with non Temporal Heuristics. (arXiv:2306.07638v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;POCL&#26041;&#27861;&#30340;&#26032;&#22411;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#38388;HTN&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#24050;&#26377;&#30340;&#38750;&#26102;&#38388;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#36827;&#34892;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#20219;&#21153;&#32593;&#32476;&#65288;HTN&#65289;&#24418;&#24335;&#21270;&#35821;&#35328;&#34987;&#29992;&#20110;&#23558;&#21508;&#31181;&#35268;&#21010;&#38382;&#39064;&#34920;&#31034;&#20026;&#20219;&#21153;&#20998;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#35768;&#22810;&#35299;&#20915;&#23427;&#20204;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#26102;&#24577;HTN&#38382;&#39064;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#36825;&#37096;&#20998;&#26159;&#22240;&#20026;&#32570;&#20047;&#26102;&#38388;&#23618;&#27425;&#35268;&#21010;&#38382;&#39064;&#30340;&#27491;&#24335;&#21644;&#19968;&#33268;&#30340;&#23450;&#20041;&#20197;&#21450;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#24320;&#21457;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#22256;&#38590;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#19981;&#20415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#37096;&#20998;&#26377;&#24207;&#22240;&#26524;&#38142;&#65288;POCL&#65289;&#30340;&#26041;&#27861;&#26469;&#34920;&#31034;&#21644;&#35299;&#20915;&#26102;&#38388;HTN&#38382;&#39064;&#65292;&#20351;&#29992;&#24050;&#24320;&#21457;&#30340;&#29992;&#20110;&#35299;&#20915;&#38750;&#26102;&#38388;&#38382;&#39064;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#39640;&#25928;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#32988;&#36807;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Hierarchical Task Network (HTN) formalism is used to express a wide variety of planning problems as task decompositions, and many techniques have been proposed to solve them. However, few works have been done on temporal HTN. This is partly due to the lack of a formal and consensual definition of what a temporal hierarchical planning problem is as well as the difficulty to develop heuristics in this context. In response to these inconveniences, we propose in this paper a new general POCL (Partial Order Causal Link) approach to represent and solve a temporal HTN problem by using existing heuristics developed to solve non temporal problems. We show experimentally that this approach is performant and can outperform the existing ones.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;CFE&#65289;&#30340;&#26041;&#21521;&#23545;xAI&#29992;&#25143;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#21521;&#19978;CFE&#30456;&#23545;&#20110;&#21521;&#19979;CFE&#25110;&#26080;&#35299;&#37322;&#21487;&#20197;&#25552;&#39640;&#29992;&#25143;&#34920;&#29616;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2306.07637</link><description>&lt;p&gt;
&#35770;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#26041;&#21521;&#23545;xAI&#29992;&#25143;&#34892;&#20026;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
For Better or Worse: The Impact of Counterfactual Explanations' Directionality on User Behavior in xAI. (arXiv:2306.07637v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07637
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;CFE&#65289;&#30340;&#26041;&#21521;&#23545;xAI&#29992;&#25143;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#21521;&#19978;CFE&#30456;&#23545;&#20110;&#21521;&#19979;CFE&#25110;&#26080;&#35299;&#37322;&#21487;&#20197;&#25552;&#39640;&#29992;&#25143;&#34920;&#29616;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;CFE&#65289;&#26159;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;xAI&#65289;&#20013;&#30340;&#19968;&#31181;&#24120;&#29992;&#26041;&#27861;&#65292;&#31361;&#20986;&#26174;&#31034;&#25913;&#21464;&#36755;&#20837;&#25968;&#25454;&#20197;&#25913;&#21464;&#27169;&#22411;&#36755;&#20986;&#25152;&#38656;&#30340;&#21464;&#21270;&#12290;CFE&#21487;&#20197;&#25551;&#36848;&#27604;&#23454;&#38469;&#29366;&#24577;&#26356;&#22909;&#30340;&#24773;&#22659;&#65288;&#21521;&#19978;CFE&#65289;&#25110;&#27604;&#23454;&#38469;&#29366;&#24577;&#26356;&#24046;&#30340;&#24773;&#22659;&#65288;&#21521;&#19979;CFE&#65289;&#12290;&#28982;&#32780;&#65292;CFE&#26041;&#21521;&#24615;&#23545;xAI&#29992;&#25143;&#30340;&#34892;&#20026;&#28508;&#22312;&#21033;&#30410;&#21644;&#32570;&#28857;&#20173;&#19981;&#28165;&#26970;&#12290;&#36825;&#39033;&#30740;&#31350;&#27604;&#36739;&#20102;CFE&#26041;&#21521;&#24615;&#23545;&#29992;&#25143;&#34892;&#20026;&#21644;&#20307;&#39564;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#21521;&#19978;CFE&#27604;&#20854;&#20182;&#24418;&#24335;&#30340;&#21453;&#20107;&#23454;&#21453;&#39304;&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#24378;&#35843;&#20102;&#28151;&#21512;CFE&#30456;&#23545;&#20110;&#21521;&#19979;CFE&#25110;&#27809;&#26377;&#35299;&#37322;&#21487;&#20197;&#25552;&#39640;&#29992;&#25143;&#34920;&#29616;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#19982;&#34920;&#29616;&#32467;&#26524;&#19968;&#33268;&#65292;&#29992;&#25143;&#23545;&#31995;&#32479;&#30340;&#26126;&#30830;&#30693;&#35782;&#20063;&#24471;&#21040;&#20102;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual explanations (CFEs) are a popular approach in explainable artificial intelligence (xAI), highlighting changes to input data necessary for altering a model's output. A CFE can either describe a scenario that is better than the factual state (upward CFE), or a scenario that is worse than the factual state (downward CFE). However, potential benefits and drawbacks of the directionality of CFEs for user behavior in xAI remain unclear. The current user study (N=161) compares the impact of CFE directionality on behavior and experience of participants tasked to extract new knowledge from an automated system based on model predictions and CFEs. Results suggest that upward CFEs provide a significant performance advantage over other forms of counterfactual feedback. Moreover, the study highlights potential benefits of mixed CFEs improving user performance compared to downward CFEs or no explanations. In line with the performance results, users' explicit knowledge of the system is s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;MaxSAT&#27714;&#35299;&#22120;&#30340;&#21487;&#26367;&#20195;&#21442;&#25968;&#37197;&#32622;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#22914;&#20309;&#23558;&#19968;&#20010;&#38750;&#31454;&#20105;&#24615;&#27714;&#35299;&#22120;&#30340;&#37197;&#32622;&#32452;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#27714;&#35299;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.07635</link><description>&lt;p&gt;
&#21033;&#29992;MaxSAT&#27714;&#35299;&#22120;&#30340;&#37197;&#32622;
&lt;/p&gt;
&lt;p&gt;
Exploiting Configurations of MaxSAT Solvers. (arXiv:2306.07635v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;MaxSAT&#27714;&#35299;&#22120;&#30340;&#21487;&#26367;&#20195;&#21442;&#25968;&#37197;&#32622;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#22914;&#20309;&#23558;&#19968;&#20010;&#38750;&#31454;&#20105;&#24615;&#27714;&#35299;&#22120;&#30340;&#37197;&#32622;&#32452;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#27714;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#26377;&#25928;&#22320;&#21033;&#29992;MaxSAT&#27714;&#35299;&#22120;&#30340;&#21487;&#26367;&#20195;&#21442;&#25968;&#37197;&#32622;&#65292;&#21253;&#25324;&#22914;&#20309;&#22312;MaxSAT&#19978;&#35745;&#31639;&#36825;&#20123;&#37197;&#32622;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#19968;&#20010;&#38750;&#31454;&#20105;&#24615;&#27714;&#35299;&#22120;&#30340;&#37197;&#32622;&#32452;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#27714;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we describe how we can effectively exploit alternative parameter configurations to a MaxSAT solver. We describe how these configurations can be computed in the context of MaxSAT. In particular, we experimentally show how to easily combine configurations of a non-competitive solver to obtain a better solving approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#31867;&#20154;&#30452;&#35273;&#34892;&#20026;&#21644;&#35748;&#30693;&#38169;&#35823;&#30340;&#29305;&#28857;&#65292;&#32780;&#39640;&#32423;&#35821;&#35328;&#27169;&#22411;&#21017;&#36890;&#36807;&#23398;&#20064;&#36991;&#20813;&#36825;&#31867;&#38169;&#35823;&#24182;&#34920;&#29616;&#20986;&#36229;&#29702;&#24615;&#30340;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#25506;&#27979;LLMs&#65292;&#21487;&#20197;&#25581;&#31034;&#20854;&#26032;&#29983;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07622</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;&#20986;&#29616;&#30340;&#31867;&#20154;&#30452;&#35273;&#34892;&#20026;&#21644;&#25512;&#29702;&#20559;&#24046;&#8212;&#8212;&#20197;&#21450;&#22312;GPT-4&#20013;&#28040;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-Like Intuitive Behavior and Reasoning Biases Emerged in Language Models -- and Disappeared in GPT-4. (arXiv:2306.07622v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#31867;&#20154;&#30452;&#35273;&#34892;&#20026;&#21644;&#35748;&#30693;&#38169;&#35823;&#30340;&#29305;&#28857;&#65292;&#32780;&#39640;&#32423;&#35821;&#35328;&#27169;&#22411;&#21017;&#36890;&#36807;&#23398;&#20064;&#36991;&#20813;&#36825;&#31867;&#38169;&#35823;&#24182;&#34920;&#29616;&#20986;&#36229;&#29702;&#24615;&#30340;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#25506;&#27979;LLMs&#65292;&#21487;&#20197;&#25581;&#31034;&#20854;&#26032;&#29983;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30446;&#21069;&#22788;&#20110;&#23558;AI&#31995;&#32479;&#19982;&#20154;&#31867;&#20132;&#27969;&#21644;&#26085;&#24120;&#29983;&#27963;&#20132;&#32455;&#22312;&#19968;&#36215;&#30340;&#21069;&#27839;&#12290;&#22240;&#27492;&#65292;&#35780;&#20272;&#23427;&#20204;&#30340;&#26032;&#20852;&#33021;&#21147;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLM&#65288;&#23588;&#20854;&#26159;GPT-3&#65289;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#31867;&#20154;&#30452;&#35273;&#34892;&#20026;&#65292;&#20197;&#21450;&#36981;&#24490;&#36825;&#31181;&#34892;&#20026;&#32780;&#26469;&#30340;&#35748;&#30693;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#26356;&#39640;&#35748;&#30693;&#33021;&#21147;&#30340;LLM&#65292;&#29305;&#21035;&#26159;ChatGPT&#21644;GPT-4&#65292;&#23398;&#20250;&#20102;&#36991;&#20813;&#23624;&#26381;&#20110;&#36825;&#20123;&#38169;&#35823;&#24182;&#34920;&#29616;&#20986;&#36229;&#29702;&#24615;&#30340;&#26041;&#24335;&#12290;&#23545;&#20110;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;Cognitive Reflection Test&#65288;CRT&#65289;&#21450;&#29992;&#20110;&#30740;&#31350;&#20154;&#31867;&#30452;&#35273;&#20915;&#31574;&#30340;&#35821;&#20041;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#31350;&#20102;&#31867;&#20154;&#30452;&#35273;&#20915;&#31574;&#30340;&#31283;&#23450;&#20542;&#21521;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#24515;&#29702;&#23398;&#26041;&#27861;&#35843;&#26597;LLM&#26377;&#28508;&#21147;&#25581;&#31034;&#21542;&#21017;&#26410;&#30693;&#30340;&#26032;&#29983;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Therefore, it is of great importance to evaluate their emerging abilities. In this study, we show that LLMs, most notably GPT-3, exhibit behavior that strikingly resembles human-like intuition -- and the cognitive errors that come with it. However, LLMs with higher cognitive capabilities, in particular ChatGPT and GPT-4, learned to avoid succumbing to these errors and perform in a hyperrational manner. For our experiments, we probe LLMs with the Cognitive Reflection Test (CRT) as well as semantic illusions that were originally designed to investigate intuitive decision-making in humans. Moreover, we probe how sturdy the inclination for intuitive-like decision-making is. Our study demonstrates that investigating LLMs with methods from psychology has the potential to reveal otherwise unknown emergent traits.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21452;&#26354;&#22270;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23376;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#20840;&#38754;&#22320;&#25429;&#25417;&#20998;&#23376;&#30340;&#20869;&#37096;&#38750;&#27431;&#20960;&#37324;&#24503;&#32467;&#26500;&#65292;&#23454;&#29616;&#25968;&#25454;&#29983;&#25104;&#65292;&#24182;&#25552;&#21462;&#22797;&#26434;&#20960;&#20309;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.07618</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#26354;&#22270;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23376;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic Graph Diffusion Model for Molecule Generation. (arXiv:2306.07618v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21452;&#26354;&#22270;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23376;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#20840;&#38754;&#22320;&#25429;&#25417;&#20998;&#23376;&#30340;&#20869;&#37096;&#38750;&#27431;&#20960;&#37324;&#24503;&#32467;&#26500;&#65292;&#23454;&#29616;&#25968;&#25454;&#29983;&#25104;&#65292;&#24182;&#25552;&#21462;&#22797;&#26434;&#20960;&#20309;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#25968;&#25454;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#20363;&#22914;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#21270;&#23398;&#20998;&#23376;&#36890;&#24120;&#20855;&#26377;&#22797;&#26434;&#30340;&#38750;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#32467;&#26500;&#65292;&#20854;&#34892;&#20026;&#21160;&#24577;&#21464;&#21270;&#19988;&#38590;&#20197;&#39044;&#27979;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#39640;&#24230;&#20381;&#36182;&#20110;&#35745;&#31639;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#21363;&#39640;&#26031;&#20998;&#24067;&#65292;&#19981;&#33021;&#25429;&#25417;&#20998;&#23376;&#30340;&#20869;&#37096;&#38750;&#27431;&#20960;&#37324;&#24503;&#32467;&#26500;&#65292;&#29305;&#21035;&#26159;&#20998;&#23376;&#25152;&#34920;&#31034;&#30340;&#38544;&#24335;&#27969;&#24418;&#34920;&#38754;&#30340;&#20998;&#23618;&#32467;&#26500;&#12290;&#35266;&#23519;&#21040;&#65292;&#21452;&#26354;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#22797;&#26434;&#20998;&#23618;&#32467;&#26500;&#21464;&#24471;&#26356;&#21152;&#26126;&#26174;&#19988;&#26356;&#23481;&#26131;&#34987;&#25429;&#25417;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#25454;&#29983;&#25104;&#33021;&#21147;&#21644;&#25552;&#21462;&#22797;&#26434;&#20960;&#20309;&#29305;&#24449;&#30340;&#21452;&#26354;&#23884;&#20837;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#25193;&#25955;&#27169;&#22411;&#25193;&#23637;&#21040;&#21452;&#26354;&#27969;&#24418;&#19978;&#36827;&#34892;&#20998;&#23376;&#29983;&#25104;&#65292;&#21363;&#22522;&#20110;&#21452;&#26354;&#22270;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23376;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, diffusion models have achieved remarkable performance in data generation, e.g., generating high-quality images. Nevertheless, chemistry molecules often have complex non-Euclidean spatial structures, with the behavior changing dynamically and unpredictably. Most existing diffusion models highly rely on computing the probability distribution, i.e., Gaussian distribution, in Euclidean space, which cannot capture internal non-Euclidean structures of molecules, especially the hierarchical structures of the implicit manifold surface represented by molecules. It has been observed that the complex hierarchical structures in hyperbolic embedding space become more prominent and easier to be captured. In order to leverage both the data generation power of diffusion models and the strong capability to extract complex geometric features of hyperbolic embedding, we propose to extend the diffusion model to hyperbolic manifolds for molecule generation, namely, Hyperbolic Graph Diffusion Mode
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;GOAL&#26694;&#26550;&#26469;&#35299;&#20915;&#21516;&#36136;&#21644;&#24322;&#36136;&#22270;&#20013;&#32570;&#22833;&#30340;&#19968;&#21322;&#32467;&#26500;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.07608</link><description>&lt;p&gt;
&#23547;&#25214;&#32570;&#22833;&#30340;&#19968;&#21322;&#65306;&#38754;&#21521;&#21516;&#36136;&#21644;&#24322;&#36136;&#22270;&#30340;&#22270;&#20114;&#34917;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Finding the Missing-half: Graph Complementary Learning for Homophily-prone and Heterophily-prone Graphs. (arXiv:2306.07608v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07608
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;GOAL&#26694;&#26550;&#26469;&#35299;&#20915;&#21516;&#36136;&#21644;&#24322;&#36136;&#22270;&#20013;&#32570;&#22833;&#30340;&#19968;&#21322;&#32467;&#26500;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#22270;&#36890;&#24120;&#21482;&#26377;&#19968;&#31181;&#31867;&#22411;&#30340;&#36830;&#25509;&#65292;&#36825;&#20123;&#36830;&#25509;&#35201;&#20040;&#26159;&#21516;&#36136;&#24615;&#20542;&#21521;&#30340;&#65292;&#35201;&#20040;&#26159;&#24322;&#36136;&#24615;&#20542;&#21521;&#30340;&#12290;&#29616;&#26377;&#30340;GNN&#27169;&#22411;&#21482;&#20351;&#29992;&#21407;&#22987;&#22270;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#36825;&#26679;&#20570;&#20250;&#24573;&#30053;&#20851;&#20110;&#32570;&#22833;&#30340;&#19968;&#21322;&#32467;&#26500;&#20449;&#24687;-&#23545;&#20110;&#21516;&#36136;&#22270;&#26469;&#35828;&#65292;&#32570;&#22833;&#30340;&#19968;&#21322;&#26159;&#24322;&#36136;&#24615;&#25299;&#25169;&#32467;&#26500;&#65292;&#23545;&#20110;&#24322;&#36136;&#22270;&#26469;&#35828;&#65292;&#32570;&#22833;&#30340;&#26159;&#21516;&#36136;&#24615;&#25299;&#25169;&#32467;&#26500;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22270;&#20114;&#34917;&#23398;&#20064;&#65288;GOAL&#65289;&#26694;&#26550;&#65292;&#21253;&#25324;&#22270;&#20114;&#34917;&#21644;&#20114;&#34917;&#22270;&#21367;&#31215;&#20004;&#20010;&#32452;&#20214;&#12290;&#31532;&#19968;&#20010;&#32452;&#20214;&#29992;&#20110;&#23547;&#25214;&#32473;&#23450;&#22270;&#30340;&#32570;&#22833;&#37096;&#20998;&#30340;&#32467;&#26500;&#20449;&#24687;&#24182;&#36827;&#34892;&#20114;&#34917;&#12290;&#20114;&#34917;&#22270;&#21253;&#25324;&#20004;&#32452;&#22270;&#65292;&#21253;&#25324;&#21407;&#22987;&#22270;&#21644;&#20114;&#34917;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world graphs generally have only one kind of tendency in their connections. These connections are either homophily-prone or heterophily-prone. While graphs with homophily-prone edges tend to connect nodes with the same class (i.e., intra-class nodes), heterophily-prone edges tend to build relationships between nodes with different classes (i.e., inter-class nodes). Existing GNNs only take the original graph during training. The problem with this approach is that it forgets to take into consideration the ``missing-half" structural information, that is, heterophily-prone topology for homophily-prone graphs and homophily-prone topology for heterophily-prone graphs. In our paper, we introduce Graph cOmplementAry Learning, namely GOAL, which consists of two components: graph complementation and complemented graph convolution. The first component finds the missing-half structural information for a given graph to complement it. The complemented graph has two sets of graphs including both
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PhD&#30340;&#22270;&#20687;&#32534;&#36753;&#26694;&#26550;&#65292;&#20351;&#29992;&#19968;&#20010;&#31034;&#20363;&#22270;&#20687;&#21644;&#25991;&#26412;&#25551;&#36848;&#26469;&#25351;&#23450;&#29992;&#25143;&#24847;&#22270;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#31896;&#36148;&#12289;&#20462;&#34917;&#21644;&#21327;&#35843;&#27493;&#39588;&#65292;&#21487;&#20197;&#23558;&#25554;&#20837;&#30340;&#20027;&#39064;&#26080;&#32541;&#34701;&#20837;&#22330;&#26223;&#20013;&#12290;</title><link>http://arxiv.org/abs/2306.07596</link><description>&lt;p&gt;
&#36890;&#36807;&#21435;&#22122;&#26469;&#23454;&#29616;&#31896;&#36148;&#12289;&#20462;&#34917;&#21644;&#21327;&#35843;&#65306;&#22522;&#20110;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#20027;&#20307;&#39537;&#21160;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Paste, Inpaint and Harmonize via Denoising: Subject-Driven Image Editing with Pre-Trained Diffusion Model. (arXiv:2306.07596v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07596
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PhD&#30340;&#22270;&#20687;&#32534;&#36753;&#26694;&#26550;&#65292;&#20351;&#29992;&#19968;&#20010;&#31034;&#20363;&#22270;&#20687;&#21644;&#25991;&#26412;&#25551;&#36848;&#26469;&#25351;&#23450;&#29992;&#25143;&#24847;&#22270;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#31896;&#36148;&#12289;&#20462;&#34917;&#21644;&#21327;&#35843;&#27493;&#39588;&#65292;&#21487;&#20197;&#23558;&#25554;&#20837;&#30340;&#20027;&#39064;&#26080;&#32541;&#34701;&#20837;&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#22240;&#20854;&#21487;&#20197;&#36890;&#36807;&#29992;&#25143;&#25351;&#23450;&#30340;&#25551;&#36848;&#36827;&#34892;&#28789;&#27963;&#30340;&#22270;&#20687;&#32534;&#36753;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20165;&#26377;&#25991;&#26412;&#25551;&#36848;&#24182;&#19981;&#36275;&#20197;&#35828;&#26126;&#20027;&#39064;&#30340;&#32454;&#33410;&#65292;&#36825;&#32463;&#24120;&#20250;&#25439;&#23475;&#20027;&#39064;&#30340;&#36523;&#20221;&#25110;&#38656;&#35201;&#39069;&#22806;&#30340;&#38024;&#23545;&#20027;&#39064;&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;"&#36890;&#36807;&#21435;&#22122;&#23454;&#29616;&#31896;&#36148;&#12289;&#20462;&#34917;&#21644;&#21327;&#35843;"&#65288;PhD&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#20102;&#38500;&#25991;&#26412;&#25551;&#36848;&#20043;&#22806;&#30340;&#31034;&#20363;&#22270;&#20687;&#26469;&#25351;&#23450;&#29992;&#25143;&#24847;&#22270;&#12290;&#22312;&#31896;&#36148;&#27493;&#39588;&#20013;&#65292;&#20351;&#29992;&#29616;&#25104;&#30340;&#20998;&#21106;&#27169;&#22411;&#26469;&#35782;&#21035;&#31034;&#20363;&#22270;&#20687;&#20013;&#29992;&#25143;&#25351;&#23450;&#30340;&#20027;&#39064;&#65292;&#28982;&#21518;&#23558;&#20854;&#25554;&#20837;&#21040;&#32972;&#26223;&#22270;&#20687;&#20013;&#65292;&#20316;&#20026;&#21516;&#26102;&#25429;&#25417;&#22330;&#26223;&#19978;&#19979;&#25991;&#21644;&#20027;&#39064;&#36523;&#20221;&#30340;&#21021;&#22987;&#21270;&#12290;&#20026;&#20102;&#20445;&#35777;&#29983;&#25104;&#25110;&#32534;&#36753;&#22270;&#20687;&#30340;&#35270;&#35273;&#36830;&#36143;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20462;&#34917;&#21644;&#21327;&#35843;&#27169;&#22359;&#65292;&#25351;&#23548;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#23558;&#25554;&#20837;&#30340;&#20027;&#39064;&#33258;&#28982;&#22320;&#26080;&#32541;&#22320;&#34701;&#20837;&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image generative models have attracted rising attention for flexible image editing via user-specified descriptions. However, text descriptions alone are not enough to elaborate the details of subjects, often compromising the subjects' identity or requiring additional per-subject fine-tuning. We introduce a new framework called \textit{Paste, Inpaint and Harmonize via Denoising} (PhD), which leverages an exemplar image in addition to text descriptions to specify user intentions. In the pasting step, an off-the-shelf segmentation model is employed to identify a user-specified subject within an exemplar image which is subsequently inserted into a background image to serve as an initialization capturing both scene context and subject identity in one. To guarantee the visual coherence of the generated or edited image, we introduce an inpainting and harmonizing module to guide the pre-trained diffusion model to seamlessly blend the inserted subject into the scene naturally. As we kee
&lt;/p&gt;</description></item><item><title>Galactic&#26159;&#19968;&#20010;&#38024;&#23545;&#23460;&#20869;&#29289;&#20307;&#37325;&#25490;&#38382;&#39064;&#30340;&#22823;&#35268;&#27169;&#20223;&#30495;&#21644;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#22312;&#27599;&#31186; 100k &#27493;&#19978;&#36816;&#34892;&#65292;&#27604;&#20854;&#20182;&#30456;&#20284;&#26694;&#26550;&#24555;&#24456;&#22810;&#12290;</title><link>http://arxiv.org/abs/2306.07552</link><description>&lt;p&gt;
Galactic: &#23558;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#25193;&#23637;&#21040;&#27599;&#31186; 100k &#27493;&#30340;&#37325;&#32452;&#38382;&#39064;&#30340;&#35268;&#27169;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Galactic: Scaling End-to-End Reinforcement Learning for Rearrangement at 100k Steps-Per-Second. (arXiv:2306.07552v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07552
&lt;/p&gt;
&lt;p&gt;
Galactic&#26159;&#19968;&#20010;&#38024;&#23545;&#23460;&#20869;&#29289;&#20307;&#37325;&#25490;&#38382;&#39064;&#30340;&#22823;&#35268;&#27169;&#20223;&#30495;&#21644;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#22312;&#27599;&#31186; 100k &#27493;&#19978;&#36816;&#34892;&#65292;&#27604;&#20854;&#20182;&#30456;&#20284;&#26694;&#26550;&#24555;&#24456;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102; Galactic&#65292;&#19968;&#20010;&#29992;&#20110;&#23460;&#20869;&#29615;&#22659;&#20013;&#26426;&#22120;&#20154;&#31227;&#21160;&#25805;&#20316;&#30340;&#22823;&#35268;&#27169;&#20223;&#30495;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#23478;&#29992;&#29615;&#22659;&#20013;&#29983;&#25104; Fetch &#26426;&#22120;&#20154;&#65288;&#24102;&#26377;&#31227;&#21160;&#22522;&#24231;&#12289;7 &#33258;&#30001;&#24230;&#26426;&#26800;&#33218;&#12289;RGBD &#30456;&#26426;&#12289;&#33258;&#36816;&#21160;&#21644;&#26495;&#36733;&#20256;&#24863;&#22120;&#65289;&#65292;&#24182;&#35201;&#27714;&#23427;&#37325;&#26032;&#25490;&#21015;&#29289;&#20307; - &#36890;&#36807;&#23548;&#33322;&#21040;&#29289;&#20307;&#12289;&#25342;&#21462;&#23427;&#12289;&#23548;&#33322;&#21040;&#30446;&#26631;&#20301;&#32622;&#65292;&#28982;&#21518;&#23558;&#29289;&#20307;&#25918;&#32622;&#22312;&#30446;&#26631;&#20301;&#32622;&#19978;&#12290;Galactic &#36895;&#24230;&#24555;&#12290;&#22312;&#20223;&#30495;&#36895;&#24230;&#65288;&#28210;&#26579;+&#29289;&#29702;&#65289;&#26041;&#38754;&#65292;Galactic &#22312; 8-GPU &#33410;&#28857;&#19978;&#23454;&#29616;&#20102;&#27599;&#31186; 421,000 &#27493;&#65288;SPS&#65289;&#65292;&#27604; Habitat 2.0 &#24555;&#20102; 54 &#20493;&#65288;7699 SPS&#65289;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;Galactic &#34987;&#35774;&#35745;&#29992;&#20110;&#20248;&#21270;&#25972;&#20010;&#28210;&#26579;+&#29289;&#29702;+RL &#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#22240;&#20026;&#30456;&#20114;&#20316;&#29992;&#20013;&#30340;&#20219;&#20309;&#29942;&#39048;&#37117;&#20250;&#20943;&#24930;&#35757;&#32451;&#12290;&#22312;&#20223;&#30495;+RL &#36895;&#24230;&#65288;&#28210;&#26579;+&#29289;&#29702;+&#25512;&#29702;+&#23398;&#20064;&#65289;&#26041;&#38754;&#65292;Galactic &#23454;&#29616;&#20102;&#27599;&#31186;&#36229;&#36807; 108,000 SPS&#65292;&#27604; Habitat 2.0 &#24555;&#20102; 88 &#20493;&#65288;1243 SPS&#65289;&#12290;&#36825;&#20123;&#24040;&#22823;&#30340;&#21152;&#36895;&#19981;&#20165;&#26174;&#33879;&#21152;&#24555;&#20102;&#35757;&#32451;&#36895;&#24230;&#65292;&#36824;&#20351; RL &#33021;&#22815;&#22312;&#26410;&#26469;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#23454;&#29616;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Galactic, a large-scale simulation and reinforcement-learning (RL) framework for robotic mobile manipulation in indoor environments. Specifically, a Fetch robot (equipped with a mobile base, 7DoF arm, RGBD camera, egomotion, and onboard sensing) is spawned in a home environment and asked to rearrange objects - by navigating to an object, picking it up, navigating to a target location, and then placing the object at the target location.  Galactic is fast. In terms of simulation speed (rendering + physics), Galactic achieves over 421,000 steps-per-second (SPS) on an 8-GPU node, which is 54x faster than Habitat 2.0 (7699 SPS). More importantly, Galactic was designed to optimize the entire rendering + physics + RL interplay since any bottleneck in the interplay slows down training. In terms of simulation+RL speed (rendering + physics + inference + learning), Galactic achieves over 108,000 SPS, which 88x faster than Habitat 2.0 (1243 SPS).  These massive speed-ups not only drasti
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#24211;&#23384;&#31649;&#29702;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#36890;&#29992;&#22522;&#20934;(MABIM)&#65292;&#20197;&#36866;&#29992;&#20110;&#23454;&#38469;&#24037;&#19994;&#22330;&#26223;&#20013;&#35832;&#22914;&#22797;&#26434;&#26234;&#33021;&#20307;&#20132;&#20114;&#31561;&#21508;&#31181;&#25361;&#25112;&#65292;&#36890;&#36807;MABIM&#65292;&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#20256;&#32479;&#30340;&#36816;&#31609;&#23398;(OR)&#26041;&#27861;&#21644;&#27969;&#34892;&#30340;MARL&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#20197;&#20984;&#26174;&#20854;&#20248;&#32570;&#28857;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.07542</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#24211;&#23384;&#31649;&#29702;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#36890;&#29992;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
A Versatile Multi-Agent Reinforcement Learning Benchmark for Inventory Management. (arXiv:2306.07542v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07542
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#24211;&#23384;&#31649;&#29702;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#36890;&#29992;&#22522;&#20934;(MABIM)&#65292;&#20197;&#36866;&#29992;&#20110;&#23454;&#38469;&#24037;&#19994;&#22330;&#26223;&#20013;&#35832;&#22914;&#22797;&#26434;&#26234;&#33021;&#20307;&#20132;&#20114;&#31561;&#21508;&#31181;&#25361;&#25112;&#65292;&#36890;&#36807;MABIM&#65292;&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#20256;&#32479;&#30340;&#36816;&#31609;&#23398;(OR)&#26041;&#27861;&#21644;&#27969;&#34892;&#30340;MARL&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#20197;&#20984;&#26174;&#20854;&#20248;&#32570;&#28857;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064; (MARL) &#27169;&#22411;&#22810;&#20010;&#30456;&#20114;&#20316;&#29992;&#24182;&#22312;&#20849;&#20139;&#29615;&#22659;&#20013;&#23398;&#20064;&#30340;&#26234;&#33021;&#20307;&#12290;&#36825;&#31181;&#33539;&#24335;&#36866;&#29992;&#20110;&#33258;&#20027;&#39550;&#39542;&#12289;&#37327;&#21270;&#20132;&#26131;&#21644;&#24211;&#23384;&#31649;&#29702;&#31561;&#21508;&#31181;&#24037;&#19994;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;&#23558; MARL &#24212;&#29992;&#20110;&#36825;&#20123;&#23454;&#38469;&#24773;&#20917;&#21463;&#21040;&#35768;&#22810;&#25361;&#25112;&#30340;&#38459;&#30861;&#65292;&#22914;&#25193;&#22823;&#35268;&#27169;&#12289;&#22797;&#26434;&#26234;&#33021;&#20307;&#20132;&#20114;&#21644;&#38750;&#31283;&#24577;&#21160;&#24577;&#12290;&#20026;&#20102;&#28608;&#21169;&#30740;&#31350;&#32773;&#22312;&#36825;&#20123;&#25361;&#25112;&#19978;&#30740;&#31350; MARL&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#22810;&#26234;&#33021;&#20307;&#24211;&#23384;&#31649;&#29702;&#22522;&#20934; (MABIM)&#65292;&#23427;&#26159;&#19968;&#20010;&#22810;&#32423;&#12289;&#22810;&#21830;&#21697;&#30340;&#24211;&#23384;&#31649;&#29702;&#27169;&#25311;&#22120;&#65292;&#21487;&#20197;&#29983;&#25104;&#19981;&#21516;&#20855;&#26377;&#36825;&#20123;&#19981;&#21516;&#30340;&#25361;&#25112;&#24615;&#36136;&#30340;&#20219;&#21153;&#12290;&#22522;&#20110; MABIM&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20256;&#32479;&#30340;&#36816;&#31609;&#23398; (OR) &#26041;&#27861;&#21644;&#27969;&#34892;&#30340; MARL &#31639;&#27861;&#22312;&#36825;&#20123;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#20197;&#31361;&#20986;&#23427;&#20204;&#30340;&#24369;&#28857;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent reinforcement learning (MARL) models multiple agents that interact and learn within a shared environment. This paradigm is applicable to various industrial scenarios such as autonomous driving, quantitative trading, and inventory management. However, applying MARL to these real-world scenarios is impeded by many challenges such as scaling up, complex agent interactions, and non-stationary dynamics. To incentivize the research of MARL on these challenges, we develop MABIM (Multi-Agent Benchmark for Inventory Management) which is a multi-echelon, multi-commodity inventory management simulator that can generate versatile tasks with these different challenging properties. Based on MABIM, we evaluate the performance of classic operations research (OR) methods and popular MARL algorithms on these challenging tasks to highlight their weaknesses and potential.
&lt;/p&gt;</description></item><item><title>SUNG&#26159;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#36890;&#36807;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#25506;&#32034;&#21644;&#24212;&#29992;&#20445;&#23432;Q&#20540;&#20272;&#35745;&#30340;&#25351;&#23548;&#19979;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#32769;&#21270;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.07541</link><description>&lt;p&gt;
&#19968;&#31181;&#31616;&#21333;&#32479;&#19968;&#30340;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Simple Unified Uncertainty-Guided Framework for Offline-to-Online Reinforcement Learning. (arXiv:2306.07541v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07541
&lt;/p&gt;
&lt;p&gt;
SUNG&#26159;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#36890;&#36807;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#25506;&#32034;&#21644;&#24212;&#29992;&#20445;&#23432;Q&#20540;&#20272;&#35745;&#30340;&#25351;&#23548;&#19979;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#32769;&#21270;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20026;&#20381;&#38752;&#25968;&#25454;&#39537;&#21160;&#33539;&#20363;&#23398;&#20064;&#26234;&#33021;&#20307;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290; &#28982;&#32780;&#65292;&#21463;&#38480;&#20110;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#36136;&#37327;&#65292;&#20854;&#24615;&#33021;&#24120;&#24120;&#19981;&#22815;&#20248;&#31168;&#12290;&#22240;&#27492;&#65292;&#22312;&#37096;&#32626;&#20043;&#21069;&#36890;&#36807;&#39069;&#22806;&#30340;&#22312;&#32447;&#20132;&#20114;&#36827;&#19968;&#27493;&#24494;&#35843;&#26234;&#33021;&#20307;&#26159;&#26377;&#24517;&#35201;&#30340;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#21463;&#21040;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#30340;&#21046;&#32422;&#65292;&#21363;&#21463;&#38480;&#30340;&#25506;&#32034;&#34892;&#20026;&#21644;&#29366;&#24577;-&#21160;&#20316;&#20998;&#24067;&#20559;&#31227;&#65292;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32479;&#19968;&#30340;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#65288;SUNG&#65289;&#26694;&#26550;&#65292;&#20854;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#24037;&#20855;&#33258;&#28982;&#22320;&#32479;&#19968;&#20102;&#36825;&#20004;&#20010;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SUNG&#36890;&#36807;&#22522;&#20110;VAE&#30340;&#29366;&#24577;-&#21160;&#20316;&#35775;&#38382;&#23494;&#24230;&#20272;&#35745;&#22120;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#20419;&#36827;&#39640;&#25928;&#25506;&#32034;&#65292;SUNG&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#20048;&#35266;&#25506;&#32034;&#31574;&#30053;&#65292;&#20197;&#36873;&#25321;&#20855;&#26377;&#39640;&#20215;&#20540;&#21644;&#39640;&#19981;&#30830;&#23450;&#24615;&#30340;&#20449;&#24687;&#21160;&#20316;&#12290;&#27492;&#22806;&#65292;SUNG&#36890;&#36807;&#22312;&#19981;&#30830;&#23450;&#24615;&#25351;&#23548;&#19979;&#24212;&#29992;&#20445;&#23432;Q&#20540;&#20272;&#35745;&#26469;&#24320;&#21457;&#19968;&#31181;&#33258;&#36866;&#24212;&#21033;&#29992;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;Atari&#21644;MuJoCo&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;SUNG&#22987;&#32456;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#25509;&#36817;&#22312;&#32447;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) provides a promising solution to learning an agent fully relying on a data-driven paradigm. However, constrained by the limited quality of the offline dataset, its performance is often sub-optimal. Therefore, it is desired to further finetune the agent via extra online interactions before deployment. Unfortunately, offline-to-online RL can be challenging due to two main challenges: constrained exploratory behavior and state-action distribution shift. To this end, we propose a Simple Unified uNcertainty-Guided (SUNG) framework, which naturally unifies the solution to both challenges with the tool of uncertainty. Specifically, SUNG quantifies uncertainty via a VAE-based state-action visitation density estimator. To facilitate efficient exploration, SUNG presents a practical optimistic exploration strategy to select informative actions with both high value and high uncertainty. Moreover, SUNG develops an adaptive exploitation method by applying conserva
&lt;/p&gt;</description></item><item><title>TART&#25552;&#20986;&#20102;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;Transformer&#27169;&#22359;&#65292;&#23427;&#33021;&#22815;&#22312;&#27809;&#26377;&#20219;&#21153;&#29305;&#23450;&#35757;&#32451;&#25110;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#19981;&#21516;&#25512;&#29702;&#30446;&#26631;&#20043;&#38388;&#36827;&#34892;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.07536</link><description>&lt;p&gt;
TART: &#19968;&#31181;&#38754;&#21521;&#20219;&#21153;&#26080;&#20851;&#25512;&#29702;&#30340;&#21363;&#25554;&#21363;&#29992;Transformer&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
TART: A plug-and-play Transformer module for task-agnostic reasoning. (arXiv:2306.07536v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07536
&lt;/p&gt;
&lt;p&gt;
TART&#25552;&#20986;&#20102;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;Transformer&#27169;&#22359;&#65292;&#23427;&#33021;&#22815;&#22312;&#27809;&#26377;&#20219;&#21153;&#29305;&#23450;&#35757;&#32451;&#25110;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#19981;&#21516;&#25512;&#29702;&#30446;&#26631;&#20043;&#38388;&#36827;&#34892;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#34920;&#29616;&#20986;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;,&#33021;&#35753;&#21516;&#19968;&#27169;&#22411;&#25191;&#34892;&#22810;&#20010;&#20219;&#21153;,&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#29305;&#23450;&#20219;&#21153;&#30340;&#35757;&#32451;&#12290;&#30456;&#27604;&#20043;&#19979;,&#20256;&#32479;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;(&#22914;&#24494;&#35843;)&#20250;&#38024;&#23545;&#27599;&#20010;&#29305;&#23450;&#20219;&#21153;&#20462;&#25913;&#22522;&#30784;&#27169;&#22411;&#12290;&#28982;&#32780;,&#21363;&#20351;&#22312;&#20351;&#29992;&#30456;&#21516;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;,&#19978;&#19979;&#25991;&#23398;&#20064;&#19968;&#30452;&#34920;&#29616;&#19981;&#20339;,&#32780;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;(&#22914;&#25552;&#31034;&#24037;&#31243;)&#20391;&#37325;&#20110;LLM&#30340;&#23398;&#20064;&#34920;&#31034;&#65292;&#20197;&#24357;&#34917;&#24615;&#33021;&#24046;&#36317;,&#32780;&#25105;&#20204;&#30340;&#20998;&#26512;&#23454;&#38469;&#19978;&#25581;&#31034;&#20102;LLM&#34920;&#31034;&#21253;&#21547;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#20570;&#20986;&#22909;&#30340;&#39044;&#27979;&#12290;&#22240;&#27492;,&#25105;&#20204;&#20851;&#27880;LLM&#30340;&#25512;&#29702;&#33021;&#21147;,&#24182;&#23637;&#31034;&#35813;&#24615;&#33021;&#24046;&#36317;&#23384;&#22312;&#26159;&#30001;&#20110;&#23427;&#20204;&#26080;&#27861;&#25191;&#34892;&#31616;&#21333;&#30340;&#27010;&#29575;&#25512;&#29702;&#20219;&#21153;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#38382;&#39064;: LLM&#23454;&#38469;&#19978;&#33021;&#21542;&#20197;&#20219;&#21153;&#26080;&#20851;&#30340;&#26041;&#24335;&#23398;&#20064;&#22914;&#20309;&#25512;&#29702;&#65311;&#25105;&#20204;&#32943;&#23450;&#22320;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;,&#24182;&#25552;&#20986;&#20102;TART&#65292;&#23427;&#20197;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#24335;&#22312;&#19981;&#36827;&#34892;&#20219;&#21153;&#29305;&#23450;&#35757;&#32451;&#25110;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#27178;&#36328;&#19981;&#21516;&#25512;&#29702;&#30446;&#26631;&#36827;&#34892;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) exhibit in-context learning abilities which enable the same model to perform several tasks without any task-specific training. In contrast, traditional adaptation approaches, such as fine-tuning, modify the underlying models for each specific task. In-context learning, however, consistently underperforms task-specific tuning approaches even when presented with the same examples. While most existing approaches (e.g., prompt engineering) focus on the LLM's learned representations to patch this performance gap, our analysis actually reveal that LLM representations contain sufficient information to make good predictions. As such, we focus on the LLM's reasoning abilities and demonstrate that this performance gap exists due to their inability to perform simple probabilistic reasoning tasks. This raises an intriguing question: Are LLMs actually capable of learning how to reason in a task-agnostic manner? We answer this in the affirmative and propose TART which ge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26465;&#20214;&#24471;&#20998;&#20989;&#25968;&#30340;&#27010;&#29575;&#36817;&#20284;&#26041;&#26696;&#65292;&#21487;&#20197;&#29992;&#20110;&#28151;&#27788;&#21160;&#21147;&#31995;&#32479;&#30340;&#39044;&#27979;&#21644;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.07526</link><description>&lt;p&gt;
&#29289;&#29702;&#21160;&#24577;&#31995;&#32479;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#29992;&#25143;&#23450;&#20041;&#20107;&#20214;&#37319;&#26679;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
User-defined Event Sampling and Uncertainty Quantification in Diffusion Models for Physical Dynamical Systems. (arXiv:2306.07526v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26465;&#20214;&#24471;&#20998;&#20989;&#25968;&#30340;&#27010;&#29575;&#36817;&#20284;&#26041;&#26696;&#65292;&#21487;&#20197;&#29992;&#20110;&#28151;&#27788;&#21160;&#21147;&#31995;&#32479;&#30340;&#39044;&#27979;&#21644;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31867;&#27010;&#29575;&#29983;&#25104;&#27169;&#22411;&#65292;&#24050;&#24191;&#27867;&#29992;&#20316;&#22270;&#20687;&#22788;&#29702;&#20219;&#21153;&#30340;&#20808;&#39564;&#65292;&#22914;&#25991;&#26412;&#26465;&#20214;&#29983;&#25104;&#21644;&#20462;&#22797;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#36866;&#24212;&#20110;&#28151;&#27788;&#21160;&#21147;&#31995;&#32479;&#30340;&#39044;&#27979;&#21644;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#22312;&#36825;&#20123;&#24212;&#29992;&#20013;&#65292;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#38544;&#24335;&#34920;&#31034;&#20851;&#20110;&#24322;&#24120;&#20540;&#21644;&#26497;&#31471;&#20107;&#20214;&#30340;&#30693;&#35782;&#65307;&#20294;&#26159;&#65292;&#36890;&#36807;&#26465;&#20214;&#37319;&#26679;&#25110;&#27979;&#37327;&#27010;&#29575;&#26469;&#26597;&#35810;&#35813;&#30693;&#35782;&#21364;&#24322;&#24120;&#22256;&#38590;&#12290;&#29616;&#26377;&#30340;&#25512;&#29702;&#26465;&#20214;&#37319;&#26679;&#26041;&#27861;&#20027;&#35201;&#26088;&#22312;&#24378;&#21046;&#25191;&#34892;&#32422;&#26463;&#26465;&#20214;&#65292;&#20294;&#36825;&#23545;&#20110;&#21305;&#37197;&#20998;&#24067;&#30340;&#32479;&#35745;&#25968;&#25454;&#25110;&#35745;&#31639;&#36873;&#25321;&#20107;&#20214;&#30340;&#27010;&#29575;&#26159;&#19981;&#20805;&#20998;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#65292;&#26368;&#29702;&#24819;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#26465;&#20214;&#24471;&#20998;&#20989;&#25968;&#65292;&#20294;&#20854;&#35745;&#31639;&#36890;&#24120;&#26159;&#19981;&#21487;&#35299;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26465;&#20214;&#24471;&#20998;&#20989;&#25968;&#30340;&#27010;&#29575;&#36817;&#20284;&#26041;&#26696;&#65292;&#21487;&#20197;&#35777;&#26126;&#20854;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are a class of probabilistic generative models that have been widely used as a prior for image processing tasks like text conditional generation and inpainting. We demonstrate that these models can be adapted to make predictions and provide uncertainty quantification for chaotic dynamical systems. In these applications, diffusion models can implicitly represent knowledge about outliers and extreme events; however, querying that knowledge through conditional sampling or measuring probabilities is surprisingly difficult. Existing methods for conditional sampling at inference time seek mainly to enforce the constraints, which is insufficient to match the statistics of the distribution or compute the probability of the chosen events. To achieve these ends, optimally one would use the conditional score function, but its computation is typically intractable. In this work, we develop a probabilistic approximation scheme for the conditional score function which provably conver
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30896;&#25758;&#21160;&#37327;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#21457;&#29616;&#33258;&#21160;&#39550;&#39542;&#31639;&#27861;&#22312;&#26497;&#31471;&#24773;&#20917;&#19979;&#30340;&#32570;&#38519;&#24182;&#21152;&#20197;&#32416;&#27491;&#12290;</title><link>http://arxiv.org/abs/2306.07525</link><description>&lt;p&gt;
&#22522;&#20110;&#30896;&#25758;&#21160;&#37327;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23545;&#25239;&#34892;&#20154;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Using Collision Momentum in Deep Reinforcement Learning Based Adversarial Pedestrian Modeling. (arXiv:2306.07525v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07525
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30896;&#25758;&#21160;&#37327;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#21457;&#29616;&#33258;&#21160;&#39550;&#39542;&#31639;&#27861;&#22312;&#26497;&#31471;&#24773;&#20917;&#19979;&#30340;&#32570;&#38519;&#24182;&#21152;&#20197;&#32416;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#26377;&#20851;&#34892;&#20154;&#20223;&#30495;&#30340;&#30740;&#31350;&#36890;&#24120;&#26088;&#22312;&#24320;&#21457;&#21508;&#31181;&#24773;&#20917;&#19979;&#30340;&#29616;&#23454;&#34892;&#20026;&#65292;&#20294;&#29616;&#26377;&#31639;&#27861;&#29983;&#25104;&#35782;&#21035;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#26497;&#31471;&#21644;&#19981;&#21487;&#33021;&#24773;&#20917;&#19979;&#20197;&#21450;&#36793;&#32536;&#24773;&#20917;&#19979;&#24615;&#33021;&#21644;&#32570;&#38519;&#30340;&#34892;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#27492;&#65292;&#38656;&#35201;&#19987;&#38376;&#30340;&#34892;&#20154;&#34892;&#20026;&#31639;&#27861;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#20391;&#37325;&#20110;&#20351;&#29992;&#31038;&#20132;&#21147;&#27169;&#22411;&#21644;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22411;&#29983;&#25104;&#30495;&#23454;&#36712;&#36857;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#30896;&#25758;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#26356;&#22909;&#22320;&#25581;&#31034;&#20102;&#33258;&#21160;&#39550;&#39542;&#25511;&#21046;&#22120;&#30340;&#29420;&#29305;&#22833;&#25928;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#39640;&#25928;&#19988;&#29983;&#25104;&#26356;&#20005;&#37325;&#30340;&#30896;&#25758;&#65292;&#20801;&#35768;&#21457;&#29616;&#21644;&#32416;&#27491;&#22797;&#26434;&#21644;&#22810;&#26679;&#24773;&#26223;&#20013;&#33258;&#20027;&#39550;&#39542;&#31639;&#27861;&#30340;&#38382;&#39064;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research in pedestrian simulation often aims to develop realistic behaviors in various situations, but it is challenging for existing algorithms to generate behaviors that identify weaknesses in automated vehicles' performance in extreme and unlikely scenarios and edge cases. To address this, specialized pedestrian behavior algorithms are needed. Current research focuses on realistic trajectories using social force models and reinforcement learning based models. However, we propose a reinforcement learning algorithm that specifically targets collisions and better uncovers unique failure modes of automated vehicle controllers. Our algorithm is efficient and generates more severe collisions, allowing for the identification and correction of weaknesses in autonomous driving algorithms in complex and varied scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22024;&#26434;&#27491;-&#26080;&#26631;&#35760;&#23398;&#20064;&#38382;&#39064;&#30340;&#21464;&#20998;&#26694;&#26550;nPUGraph&#65292;&#24182;&#24341;&#20837;&#33258;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#24605;&#36776;&#24615;&#25512;&#29702;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07512</link><description>&lt;p&gt;
&#33258;&#35757;&#32451;&#23454;&#29616;&#22024;&#26434;&#27491;-&#26080;&#26631;&#35760;&#23398;&#20064;&#65292;&#22312;&#24605;&#36776;&#24615;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#20013;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Noisy Positive-Unlabeled Learning with Self-Training for Speculative Knowledge Graph Reasoning. (arXiv:2306.07512v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22024;&#26434;&#27491;-&#26080;&#26631;&#35760;&#23398;&#20064;&#38382;&#39064;&#30340;&#21464;&#20998;&#26694;&#26550;nPUGraph&#65292;&#24182;&#24341;&#20837;&#33258;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#24605;&#36776;&#24615;&#25512;&#29702;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#30495;&#23454;&#19990;&#30028;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#19978;&#30340;&#24605;&#36776;&#24615;&#25512;&#29702;&#20219;&#21153;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#20551;&#36127;&#38382;&#39064;&#65288;&#21363;&#28508;&#22312;&#30340;&#30495;&#23454;&#20107;&#23454;&#34987;&#25490;&#38500;&#65289;&#21644;&#20551;&#27491;&#38382;&#39064;&#65288;&#21363;&#19981;&#21487;&#38752;&#25110;&#36807;&#26102;&#30340;&#20107;&#23454;&#34987;&#21253;&#25324;&#65289;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#24605;&#36776;&#24615;&#25512;&#29702;&#33021;&#21147;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#20026;&#23427;&#20204;&#20551;&#35774;&#19968;&#20010;&#20107;&#23454;&#26159;&#21542;&#27491;&#30830;&#20165;&#30001;&#23427;&#22312;KG&#20013;&#30340;&#23384;&#22312;&#30830;&#23450;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#20551;&#38452;&#24615;/&#20551;&#38451;&#24615;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#26032;&#30340;&#25512;&#29702;&#20219;&#21153;&#34987;&#35268;&#23450;&#20026;&#19968;&#31181;&#22024;&#26434;&#27491;-&#26080;&#26631;&#35760;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;&#26694;&#26550;nPUGraph&#65292;&#23427;&#20849;&#21516;&#20272;&#35745;&#24050;&#25910;&#38598;&#21644;&#26410;&#25910;&#38598;&#20107;&#23454;&#30340;&#27491;&#30830;&#24615;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#26631;&#31614;&#21518;&#39564;&#27010;&#29575;&#8221;&#65289;&#65292;&#24182;&#22312;&#35757;&#32451;&#26399;&#38388;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#12290;&#26631;&#31614;&#21518;&#39564;&#27010;&#29575;&#20272;&#35745;&#20174;&#20004;&#20010;&#26041;&#38754;&#20419;&#36827;&#20102;&#24605;&#36776;&#24615;&#25512;&#29702;&#12290;&#39318;&#20808;&#65292;&#23427;&#25552;&#39640;&#20102;&#26631;&#31614;&#21518;&#39564;&#27010;&#29575;&#24863;&#30693;&#30340;&#22270;&#34920;&#24449;&#23545;&#25239;&#20551;&#38451;&#24615;&#20851;&#31995;&#30340;&#40065;&#26834;&#24615;&#12290;&#20854;&#27425;&#65292;&#23427;&#30830;&#23450;&#20102;&#35823;&#23548;&#24615;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#24182;&#20943;&#23569;&#20102;&#20854;&#23545;&#27169;&#22411;&#35757;&#32451;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#33258;&#35757;&#32451;&#31574;&#30053;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;&#20004;&#20010;&#30693;&#35782;&#22270;&#25512;&#29702;&#20219;&#21153;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies speculative reasoning task on real-world knowledge graphs (KG) that contain both \textit{false negative issue} (i.e., potential true facts being excluded) and \textit{false positive issue} (i.e., unreliable or outdated facts being included). State-of-the-art methods fall short in the speculative reasoning ability, as they assume the correctness of a fact is solely determined by its presence in KG, making them vulnerable to false negative/positive issues. The new reasoning task is formulated as a noisy Positive-Unlabeled learning problem. We propose a variational framework, namely nPUGraph, that jointly estimates the correctness of both collected and uncollected facts (which we call \textit{label posterior}) and updates model parameters during training. The label posterior estimation facilitates speculative reasoning from two perspectives. First, it improves the robustness of a label posterior-aware graph encoder against false positive links. Second, it identifies mis
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102; ChatGPT &#19981;&#21516;&#29992;&#20363;&#23545;&#20110;&#20844;&#24179;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23427;&#23545;&#20110;&#27979;&#35797;&#20219;&#21153;&#32780;&#35328;&#26159;&#20844;&#24179;&#30340;&#25628;&#32034;&#24341;&#25806;&#65292;&#20294;&#22312;&#25991;&#26412;&#29983;&#25104;&#21644;&#20195;&#30721;&#29983;&#25104;&#19978;&#26377;&#20559;&#35265;&#65292;&#24182;&#19988;&#23545;&#20110;&#21464;&#21270;&#38750;&#24120;&#25935;&#24863;&#12290;</title><link>http://arxiv.org/abs/2306.07500</link><description>&lt;p&gt;
&#22312;&#20808;&#36827;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#28155;&#21152;&#25252;&#26639;
&lt;/p&gt;
&lt;p&gt;
Adding guardrails to advanced chatbots. (arXiv:2306.07500v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07500
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102; ChatGPT &#19981;&#21516;&#29992;&#20363;&#23545;&#20110;&#20844;&#24179;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23427;&#23545;&#20110;&#27979;&#35797;&#20219;&#21153;&#32780;&#35328;&#26159;&#20844;&#24179;&#30340;&#25628;&#32034;&#24341;&#25806;&#65292;&#20294;&#22312;&#25991;&#26412;&#29983;&#25104;&#21644;&#20195;&#30721;&#29983;&#25104;&#19978;&#26377;&#20559;&#35265;&#65292;&#24182;&#19988;&#23545;&#20110;&#21464;&#21270;&#38750;&#24120;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335; AI &#27169;&#22411;&#19981;&#26029;&#21464;&#24471;&#26356;&#21152;&#24378;&#22823;&#12290;2022 &#24180; 11 &#26376; ChatGPT &#30340;&#25512;&#20986;&#36814;&#26469;&#20102; AI &#30340;&#26032;&#26102;&#20195;&#12290;ChatGPT &#21644;&#20854;&#20182;&#31867;&#20284;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#20855;&#26377;&#19968;&#31995;&#21015;&#30340;&#33021;&#21147;&#65292;&#20174;&#22238;&#31572;&#23398;&#29983;&#23478;&#24237;&#20316;&#19994;&#38382;&#39064;&#21040;&#21019;&#36896;&#38899;&#20048;&#21644;&#33402;&#26415;&#12290;&#24050;&#32463;&#26377;&#20154;&#25285;&#24515; chatbot &#21487;&#33021;&#20250;&#21462;&#20195;&#20154;&#31867;&#36827;&#34892;&#21508;&#31181;&#24037;&#20316;&#12290;&#30001;&#20110;&#32842;&#22825;&#26426;&#22120;&#20154;&#26500;&#24314;&#22312;&#24191;&#27867;&#30340;&#25968;&#25454;&#20307;&#31995;&#19978;&#65292;&#25105;&#20204;&#30693;&#36947;&#23427;&#20204;&#20250;&#24102;&#26377;&#20154;&#31867;&#38169;&#35823;&#21644;&#20559;&#35265;&#12290;&#36825;&#20123;&#20559;&#35265;&#21487;&#33021;&#23545;&#19981;&#21516;&#20154;&#32676;&#36896;&#25104;&#37325;&#22823;&#20260;&#23475;&#21644;/&#25110;&#19981;&#20844;&#24179;&#12290;&#20026;&#20102;&#20102;&#35299;&#32842;&#22825;&#26426;&#22120;&#20154;&#21709;&#24212;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31687;&#20301;&#35770;&#25991;&#65292;&#25506;&#35752;&#20102; ChatGPT &#30340;&#19981;&#21516;&#29992;&#20363;&#65292;&#20197;&#30830;&#23450;&#20844;&#24179;&#22238;&#31572;&#30340;&#38382;&#39064;&#31867;&#22411;&#21644;&#20173;&#28982;&#38656;&#35201;&#25913;&#36827;&#30340;&#31867;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616; ChatGPT &#23545;&#20110;&#25105;&#20204;&#27979;&#35797;&#30340;&#20219;&#21153;&#32780;&#35328;&#26159;&#19968;&#20010;&#20844;&#24179;&#30340;&#25628;&#32034;&#24341;&#25806;&#65307;&#28982;&#32780;&#65292;&#22312;&#25991;&#26412;&#29983;&#25104;&#21644;&#20195;&#30721;&#29983;&#25104;&#19978;&#23427;&#23384;&#22312;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616; ChatGPT &#23545;&#20110;&#21464;&#21270;&#38750;&#24120;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI models continue to become more powerful. The launch of ChatGPT in November 2022 has ushered in a new era of AI. ChatGPT and other similar chatbots have a range of capabilities, from answering student homework questions to creating music and art. There are already concerns that humans may be replaced by chatbots for a variety of jobs. Because of the wide spectrum of data chatbots are built on, we know that they will have human errors and human biases built into them. These biases may cause significant harm and/or inequity toward different subpopulations. To understand the strengths and weakness of chatbot responses, we present a position paper that explores different use cases of ChatGPT to determine the types of questions that are answered fairly and the types that still need improvement. We find that ChatGPT is a fair search engine for the tasks we tested; however, it has biases on both text generation and code generation. We find that ChatGPT is very sensitive to change
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEDO&#30340;&#27169;&#22411;-&#19981;&#21487;&#30693;&#19988;&#35745;&#31639;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#26631;&#31614;&#38169;&#35823;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#24847;&#35265;&#38382;&#31572;&#31995;&#32479;&#20013;&#65292;&#25552;&#39640;&#20102;&#35813;&#31995;&#32479;&#22312;&#21508;&#20010;&#26680;&#24515;&#27169;&#22411;&#20013;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07499</link><description>&lt;p&gt;
&#36890;&#36807;&#26631;&#31614;&#38169;&#35823;&#26816;&#27979;&#21644;&#37325;&#20889;&#25552;&#39640;&#22522;&#20110;&#24847;&#35265;&#30340;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Improving Opinion-based Question Answering Systems Through Label Error Detection and Overwrite. (arXiv:2306.07499v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEDO&#30340;&#27169;&#22411;-&#19981;&#21487;&#30693;&#19988;&#35745;&#31639;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#26631;&#31614;&#38169;&#35823;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#24847;&#35265;&#38382;&#31572;&#31995;&#32479;&#20013;&#65292;&#25552;&#39640;&#20102;&#35813;&#31995;&#32479;&#22312;&#21508;&#20010;&#26680;&#24515;&#27169;&#22411;&#20013;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#38169;&#35823;&#26159;&#27880;&#37322;&#25968;&#25454;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#22823;&#37327;&#30340;&#26631;&#31614;&#38169;&#35823;&#20250;&#20005;&#37325;&#38477;&#20302;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26631;&#31614;&#38169;&#35823;&#38382;&#39064;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#26550;&#26500;&#65292;&#35201;&#20040;&#38656;&#35201;&#38750;&#24120;&#22797;&#26434;&#30340;&#39069;&#22806;&#35745;&#31639;&#65292;&#36825;&#20123;&#37117;&#19981;&#36866;&#21512;&#24037;&#19994;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LEDO&#65306;&#19968;&#31181;&#38754;&#21521;&#27169;&#22411;&#30340;&#12289;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26631;&#31614;&#38169;&#35823;&#26816;&#27979;&#21644;&#37325;&#20889;&#26694;&#26550;&#12290;LEDO&#22522;&#20110; Monte Carlo Dropout &#21644;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#65292;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#25512;&#24191;&#21040;&#22810;&#20010;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#12290;&#23558;LEDO&#24212;&#29992;&#20110;&#24037;&#19994;&#24847;&#35265;&#38382;&#31572;&#31995;&#32479;&#20013;&#65292;&#35777;&#26126;&#23427;&#33021;&#26377;&#25928;&#25552;&#39640;&#25152;&#26377;&#26680;&#24515;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LEDO&#20026;&#26816;&#32034;&#27169;&#22411;&#24102;&#26469;1.1&#65285;&#30340;MRR&#22686;&#30410;&#65292;&#20026;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#27169;&#22411;&#25552;&#39640;1.5&#65285;&#30340;PR AUC&#65292;&#20026;&#25490;&#21517;&#22120;&#30340;&#24179;&#22343;&#31934;&#24230;&#25552;&#39640;0.9&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Label error is a ubiquitous problem in annotated data. Large amounts of label error substantially degrades the quality of deep learning models. Existing methods to tackle the label error problem largely focus on the classification task, and either rely on task specific architecture or require non-trivial additional computations, which is undesirable or even unattainable for industry usage. In this paper, we propose LEDO: a model-agnostic and computationally efficient framework for Label Error Detection and Overwrite. LEDO is based on Monte Carlo Dropout combined with uncertainty metrics, and can be easily generalized to multiple tasks and data sets. Applying LEDO to an industry opinion-based question answering system demonstrates it is effective at improving accuracy in all the core models. Specifically, LEDO brings 1.1% MRR gain for the retrieval model, 1.5% PR AUC improvement for the machine reading comprehension model, and 0.9% rise in the Average Precision for the ranker, on top of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#20110;&#20572;&#39039;&#30340;&#38901;&#24459;&#24314;&#27169;&#30340;&#35821;&#38899;&#21512;&#25104;&#31995;&#32479;&#65292;&#21517;&#20026;PauseSpeech&#12290;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#30701;&#35821;&#32467;&#26500;&#30340;&#32534;&#30721;&#22120;&#21644;&#22522;&#20110;&#20572;&#39039;&#30340;&#21333;&#35789;&#32534;&#30721;&#22120;&#65292;&#26377;&#25928;&#22320;&#21512;&#25104;&#20102;&#20855;&#26377;&#36866;&#24403;&#30701;&#35821;&#32467;&#26500;&#30340;&#33258;&#28982;&#35821;&#38899;&#12290;</title><link>http://arxiv.org/abs/2306.07489</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#20110;&#20572;&#39039;&#30340;&#38901;&#24459;&#24314;&#27169;&#30340;&#33258;&#28982;&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
PauseSpeech: Natural Speech Synthesis via Pre-trained Language Model and Pause-based Prosody Modeling. (arXiv:2306.07489v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#20110;&#20572;&#39039;&#30340;&#38901;&#24459;&#24314;&#27169;&#30340;&#35821;&#38899;&#21512;&#25104;&#31995;&#32479;&#65292;&#21517;&#20026;PauseSpeech&#12290;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#30701;&#35821;&#32467;&#26500;&#30340;&#32534;&#30721;&#22120;&#21644;&#22522;&#20110;&#20572;&#39039;&#30340;&#21333;&#35789;&#32534;&#30721;&#22120;&#65292;&#26377;&#25928;&#22320;&#21512;&#25104;&#20102;&#20855;&#26377;&#36866;&#24403;&#30701;&#35821;&#32467;&#26500;&#30340;&#33258;&#28982;&#35821;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#31995;&#32479;&#24050;&#32463;&#26377;&#20102;&#26174;&#33879;&#30340;&#36827;&#27493;&#65292;&#20294;&#22823;&#22810;&#25968;TTS&#31995;&#32479;&#20173;&#28982;&#22312;&#21512;&#25104;&#20855;&#26377;&#36866;&#24403;&#30701;&#35821;&#32467;&#26500;&#30340;&#35821;&#38899;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#12290;&#20026;&#20102;&#36827;&#34892;&#33258;&#28982;&#35821;&#38899;&#21512;&#25104;&#65292;&#21512;&#25104;&#30340;&#35821;&#38899;&#38656;&#35201;&#20197;&#30701;&#35821;&#32467;&#26500;&#26041;&#24335;&#21512;&#25104;&#65292;&#35813;&#30701;&#35821;&#32467;&#26500;&#22522;&#20110;&#35821;&#20041;&#20449;&#24687;&#23558;&#21333;&#35789;&#20998;&#32452;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#20110;&#20572;&#39039;&#30340;&#38901;&#24459;&#24314;&#27169;&#30340;&#35821;&#38899;&#21512;&#25104;&#31995;&#32479;&#65292;&#21517;&#20026; PauseSpeech&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;&#30701;&#35821;&#32467;&#26500;&#30340;&#32534;&#30721;&#22120;&#65292;&#35813;&#32534;&#30721;&#22120;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#12290;&#22312;&#30701;&#35821;&#32467;&#26500;&#32534;&#30721;&#22120;&#20013;&#65292;&#25105;&#20204;&#20174;&#19978;&#19979;&#25991;&#34920;&#31034;&#20013;&#25552;&#21462;&#20102;&#19968;&#20010;&#19982;&#35828;&#35805;&#32773;&#30456;&#20851;&#30340;&#21477;&#27861;&#34920;&#31034;&#65292;&#28982;&#21518;&#39044;&#27979;&#20102;&#19968;&#20010;&#20572;&#39039;&#24207;&#21015;&#65292;&#23558;&#36755;&#20837;&#25991;&#26412;&#20998;&#25104;&#30701;&#35821;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#22522;&#20110;&#20572;&#39039;&#30340;&#21333;&#35789;&#32534;&#30721;&#22120;&#65292;&#20197;&#26681;&#25454;&#20572;&#39039;&#24207;&#21015;&#23545;&#21333;&#35789;&#32423;&#38901;&#24459;&#36827;&#34892;&#24314;&#27169;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PauseSpeech&#22312;&#33258;&#28982;&#24615;&#26041;&#38754;&#20248;&#20110;&#20197;&#21069;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#22312;&#23458;&#35266;&#35780;&#20272;&#25351;&#26631;&#65288;&#20363;&#22914;MOS&#21644;WER&#65289;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31995;&#32479;&#36824;&#19982;&#22522;&#32447;&#31995;&#32479;&#30456;&#27604;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although text-to-speech (TTS) systems have significantly improved, most TTS systems still have limitations in synthesizing speech with appropriate phrasing. For natural speech synthesis, it is important to synthesize the speech with a phrasing structure that groups words into phrases based on semantic information. In this paper, we propose PuaseSpeech, a speech synthesis system with a pre-trained language model and pause-based prosody modeling. First, we introduce a phrasing structure encoder that utilizes a context representation from the pre-trained language model. In the phrasing structure encoder, we extract a speaker-dependent syntactic representation from the context representation and then predict a pause sequence that separates the input text into phrases. Furthermore, we introduce a pause-based word encoder to model word-level prosody based on pause sequence. Experimental results show PauseSpeech outperforms previous models in terms of naturalness. Furthermore, in terms of obj
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#22810;&#30456;&#20301;&#22266;&#23450;&#31639;&#27861;&#65292;&#21487;&#20197;&#26080;&#32541;&#22320;&#38598;&#25104;&#21040;&#35270;&#35273;Transformer&#27169;&#22411;&#20013;&#65292;&#20197;&#30830;&#20445;&#24179;&#31227;&#31561;&#21464;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#34917;&#19969;&#30340;&#36866;&#37197;&#22120;&#65292;&#20197;&#24674;&#22797;&#20301;&#32622;&#32534;&#30721;&#30340;&#24179;&#31227;&#31561;&#21464;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;ImageNet-C&#21644;CLEVR-C&#31561;&#24179;&#31227;&#19981;&#21464;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;ViT&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07470</link><description>&lt;p&gt;
&#24674;&#22797;&#35270;&#35273;Transformer&#30340;&#24179;&#31227;&#31561;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
Reviving Shift Equivariance in Vision Transformers. (arXiv:2306.07470v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#22810;&#30456;&#20301;&#22266;&#23450;&#31639;&#27861;&#65292;&#21487;&#20197;&#26080;&#32541;&#22320;&#38598;&#25104;&#21040;&#35270;&#35273;Transformer&#27169;&#22411;&#20013;&#65292;&#20197;&#30830;&#20445;&#24179;&#31227;&#31561;&#21464;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#34917;&#19969;&#30340;&#36866;&#37197;&#22120;&#65292;&#20197;&#24674;&#22797;&#20301;&#32622;&#32534;&#30721;&#30340;&#24179;&#31227;&#31561;&#21464;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;ImageNet-C&#21644;CLEVR-C&#31561;&#24179;&#31227;&#19981;&#21464;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;ViT&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24179;&#31227;&#31561;&#21464;&#24615;&#26159;&#19968;&#31181;&#22522;&#26412;&#21407;&#29702;&#65292;&#23427;&#35268;&#23450;&#20102;&#25105;&#20204;&#22914;&#20309;&#24863;&#30693;&#19990;&#30028;-&#25105;&#20204;&#23545;&#23545;&#35937;&#30340;&#35748;&#30693;&#22312;&#24179;&#31227;&#26041;&#38754;&#20445;&#25345;&#19981;&#21464;&#12290;&#30001;&#20110;&#23427;&#20204;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;Transformers&#21464;&#24471;&#38750;&#24120;&#27969;&#34892;&#12290;&#34429;&#28982;ViT&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#25805;&#20316;&#26159;&#32622;&#25442;&#31561;&#21464;&#19988;&#22240;&#27492;&#26159;&#24179;&#31227;&#31561;&#21464;&#30340;&#65292;&#20294;ViT&#21464;&#20307;&#20013;&#30340;&#34917;&#19969;&#23884;&#20837;&#12289;&#20301;&#32622;&#32534;&#30721;&#21644;&#23376;&#37319;&#26679;&#27880;&#24847;&#21147;&#21487;&#33021;&#20250;&#30772;&#22351;&#36825;&#31181;&#24615;&#36136;&#65292;&#20174;&#32780;&#23548;&#33268;&#21363;&#20351;&#22312;&#23567;&#30340;&#24179;&#31227;&#25200;&#21160;&#19979;&#20063;&#20250;&#20986;&#29616;&#19981;&#19968;&#33268;&#30340;&#39044;&#27979;&#12290;&#34429;&#28982;&#30446;&#21069;&#23558;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#30340;&#24402;&#32435;&#20559;&#32622;&#34701;&#20837;&#21040;&#35270;&#35273;Transformer&#20013;&#30340;&#36235;&#21183;&#27491;&#22312;&#22686;&#38271;&#65292;&#20294;&#36825;&#24182;&#19981;&#33021;&#23436;&#20840;&#35299;&#20915;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#30340;&#22810;&#30456;&#20301;&#22266;&#23450;&#31639;&#27861;&#65292;&#21487;&#20197;&#26080;&#32541;&#22320;&#38598;&#25104;&#21040;&#35270;&#35273;Transformer&#27169;&#22411;&#20013;&#65292;&#20197;&#30830;&#20445;&#22312;&#34917;&#19969;&#23884;&#20837;&#21644;&#23376;&#37319;&#26679;&#27880;&#24847;&#21147;&#27169;&#22359;&#20013;&#20445;&#25345;&#21464;&#25442;&#31561;&#21464;&#24615;&#65292;&#20363;&#22914;&#31383;&#21475;&#27880;&#24847;&#21147;&#21644;&#20840;&#23616;&#23376;&#37319;&#26679;&#27880;&#24847;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#34917;&#19969;&#30340;&#36866;&#37197;&#22120;&#65292;&#20197;&#24674;&#22797;&#20301;&#32622;&#32534;&#30721;&#30340;&#24179;&#31227;&#31561;&#21464;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31616;&#21333;&#39640;&#25928;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;ViT&#27169;&#22411;&#22312;&#24179;&#31227;&#19981;&#21464;&#25968;&#25454;&#38598;&#65288;&#22914;ImageNet-C&#21644;CLEVR-C&#65289;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Shift equivariance is a fundamental principle that governs how we perceive the world - our recognition of an object remains invariant with respect to shifts. Transformers have gained immense popularity due to their effectiveness in both language and vision tasks. While the self-attention operator in vision transformers (ViT) is permutation-equivariant and thus shift-equivariant, patch embedding, positional encoding, and subsampled attention in ViT variants can disrupt this property, resulting in inconsistent predictions even under small shift perturbations. Although there is a growing trend in incorporating the inductive bias of convolutional neural networks (CNNs) into vision transformers, it does not fully address the issue. We propose an adaptive polyphase anchoring algorithm that can be seamlessly integrated into vision transformer models to ensure shift-equivariance in patch embedding and subsampled attention modules, such as window attention and global subsampled attention. Furth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#40657;&#30418;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#21487;&#20197;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#23454;&#29616;&#20302;&#36951;&#25022;&#29575;&#30340;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.07465</link><description>&lt;p&gt;
&#38754;&#21521;&#38750;&#24179;&#31283;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#40657;&#30418;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Black-box Approach for Non-stationary Multi-agent Reinforcement Learning. (arXiv:2306.07465v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#40657;&#30418;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#21487;&#20197;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#23454;&#29616;&#20302;&#36951;&#25022;&#29575;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38750;&#24179;&#31283;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#23398;&#20064;&#22343;&#34913;&#30340;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#21306;&#21035;&#20110;&#21333;&#26234;&#33021;&#20307;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#24102;&#26377;&#36172;&#24466;&#21453;&#39304;&#30340;&#28216;&#25103;&#65292;&#20854;&#20013;&#21363;&#20351;&#24453;&#27979;&#35797;&#30340;&#24046;&#36317;&#24456;&#23567;&#65292;&#27979;&#35797;&#19968;&#20010;&#22343;&#34913;&#20063;&#21487;&#33021;&#23548;&#33268;&#22823;&#37327;&#30340;&#36951;&#25022;&#65292;&#24182;&#19988;&#22312;&#38745;&#24577;&#28216;&#25103;&#20013;&#23384;&#22312;&#22810;&#20010;&#26368;&#20248;&#35299;&#65288;&#22343;&#34913;&#65289;&#20250;&#24102;&#26469;&#39069;&#22806;&#30340;&#38590;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#40657;&#30418;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#38382;&#39064;&#65292;&#22914;&#19968;&#33324;&#21644;&#21338;&#24328;&#12289;&#28508;&#22312;&#21338;&#24328;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#65292;&#21482;&#35201;&#22312;&#38745;&#24577;&#29615;&#22659;&#19979;&#37197;&#22791;&#36866;&#24403;&#30340;&#23398;&#20064;&#21644;&#27979;&#35797;&#31070;&#35861;&#12290;&#24403;&#38750;&#24179;&#31283;&#31243;&#24230;&#65288;&#36890;&#36807;&#24635;&#21464;&#21270;&#37327; $\Delta$ &#27979;&#37327;&#65289;&#24050;&#30693;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616; $\widetilde{O}\left(\Delta^{1/4}T^{3/4}\right)$ &#30340;&#36951;&#25022;&#65292;&#24403; $\Delta$ &#26410;&#30693;&#26102;&#65292;&#21487;&#20197;&#23454;&#29616; $\widetilde{O}\left(\Delta^{1/5}T^{4/5}\right)$ &#30340;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate learning the equilibria in non-stationary multi-agent systems and address the challenges that differentiate multi-agent learning from single-agent learning. Specifically, we focus on games with bandit feedback, where testing an equilibrium can result in substantial regret even when the gap to be tested is small, and the existence of multiple optimal solutions (equilibria) in stationary games poses extra challenges. To overcome these obstacles, we propose a versatile black-box approach applicable to a broad spectrum of problems, such as general-sum games, potential games, and Markov games, when equipped with appropriate learning and testing oracles for stationary environments. Our algorithms can achieve $\widetilde{O}\left(\Delta^{1/4}T^{3/4}\right)$ regret when the degree of nonstationarity, as measured by total variation $\Delta$, is known, and $\widetilde{O}\left(\Delta^{1/5}T^{4/5}\right)$ regret when $\Delta$ is unknown, where $T$ is the number of rounds. Meanwhile, 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#27454;&#21517;&#20026; Account Prioritizer &#30340;&#26234;&#33021;&#38144;&#21806;&#36134;&#25143;&#20248;&#20808;&#32423;&#24341;&#25806;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#35299;&#37322;&#31639;&#27861;&#33258;&#21160;&#21270;&#38144;&#21806;&#31807;&#20248;&#21270;&#65292;&#22312; LinkedIn Business &#20013;&#25104;&#21151;&#24102;&#26469;&#20102; +8.08% &#30340;&#32493;&#35746;&#35746;&#38405;&#22686;&#38271;&#12290;</title><link>http://arxiv.org/abs/2306.07464</link><description>&lt;p&gt;
&#35299;&#38145;&#38144;&#21806;&#22686;&#38271;&#65306;&#20855;&#26377;&#21487;&#35299;&#37322; AI &#30340;&#36134;&#25143;&#20248;&#20808;&#32423;&#24341;&#25806;
&lt;/p&gt;
&lt;p&gt;
Unlocking Sales Growth: Account Prioritization Engine with Explainable AI. (arXiv:2306.07464v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07464
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#27454;&#21517;&#20026; Account Prioritizer &#30340;&#26234;&#33021;&#38144;&#21806;&#36134;&#25143;&#20248;&#20808;&#32423;&#24341;&#25806;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#35299;&#37322;&#31639;&#27861;&#33258;&#21160;&#21270;&#38144;&#21806;&#31807;&#20248;&#21270;&#65292;&#22312; LinkedIn Business &#20013;&#25104;&#21151;&#24102;&#26469;&#20102; +8.08% &#30340;&#32493;&#35746;&#35746;&#38405;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
B2B &#38144;&#21806;&#38656;&#35201;&#26377;&#25928;&#39044;&#27979;&#23458;&#25143;&#22686;&#38271;&#65292;&#35782;&#21035;&#21319;&#32423;&#28508;&#21147;&#20197;&#21450;&#38477;&#20302;&#27969;&#22833;&#39118;&#38505;&#12290;LinkedIn &#30340;&#38144;&#21806;&#20195;&#34920;&#20256;&#32479;&#19978;&#20381;&#36182;&#30452;&#35273;&#21644;&#30862;&#29255;&#21270;&#25968;&#25454;&#20449;&#21495;&#26469;&#35780;&#20272;&#23458;&#25143;&#32489;&#25928;&#12290;&#36825;&#23548;&#33268;&#22312;&#25968;&#25454;&#29702;&#35299;&#21644;&#31574;&#30053;&#21046;&#23450;&#26041;&#38754;&#25237;&#20837;&#20102;&#22823;&#37327;&#26102;&#38388;&#65292;&#32780;&#22312;&#31215;&#26497;&#38144;&#21806;&#26041;&#38754;&#25237;&#36164;&#19981;&#36275;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#25454;&#20135;&#21697;&#65292;&#31216;&#20026; Account Prioritizer&#65292;&#23427;&#26159;&#26234;&#33021;&#38144;&#21806;&#36134;&#25143;&#20248;&#20808;&#32423;&#24341;&#25806;&#12290;&#23427;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411;&#21644;&#38598;&#25104;&#30340;&#36134;&#25143;&#32423;&#35299;&#37322;&#31639;&#27861;&#22312;&#38144;&#21806; CRM &#20013;&#33258;&#21160;&#21270;&#38144;&#21806;&#31807;&#20248;&#21270;&#30340;&#25163;&#21160;&#36807;&#31243;&#12290;&#19968;&#27425;&#25104;&#21151;&#30340; A/B &#27979;&#35797;&#34920;&#26126;&#65292;Account Prioritizer &#20026; LinkedIn Business &#24102;&#26469;&#20102;&#26174;&#33879;&#30340; +8.08% &#32493;&#35746;&#35746;&#38405;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
B2B sales requires effective prediction of customer growth, identification of upsell potential, and mitigation of churn risks. LinkedIn sales representatives traditionally relied on intuition and fragmented data signals to assess customer performance. This resulted in significant time investment in data understanding as well as strategy formulation and under-investment in active selling. To overcome this challenge, we developed a data product called Account Prioritizer, an intelligent sales account prioritization engine. It uses machine learning recommendation models and integrated account-level explanation algorithms within the sales CRM to automate the manual process of sales book prioritization. A successful A/B test demonstrated that the Account Prioritizer generated a substantial +8.08% increase in renewal bookings for the LinkedIn Business.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#36866;&#29992;&#20110;&#20154;&#24037;&#20915;&#31574;&#36741;&#21161;&#30340;&#26234;&#33021;&#24178;&#39044;&#26041;&#26696;&#65292;&#22312;&#21516;&#26102;&#32771;&#34385;&#20934;&#30830;&#24615;&#21644;&#26102;&#38388;&#24615;&#30340;&#21069;&#25552;&#19979;&#65292;&#26681;&#25454;&#38382;&#39064;&#21644;&#29992;&#25143;&#30340;&#23646;&#24615;&#33258;&#36866;&#24212;&#22320;&#23637;&#31034;AI&#36741;&#21161;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.07458</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#20154;&#24037;&#20915;&#31574;&#36741;&#21161;&#30340;&#26234;&#33021;&#24178;&#39044;&#26041;&#26696;&#65306;&#26082;&#32771;&#34385;&#20934;&#30830;&#24615;&#21448;&#20860;&#39038;&#26102;&#38388;&#24615;
&lt;/p&gt;
&lt;p&gt;
Adaptive interventions for both accuracy and time in AI-assisted human decision making. (arXiv:2306.07458v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#36866;&#29992;&#20110;&#20154;&#24037;&#20915;&#31574;&#36741;&#21161;&#30340;&#26234;&#33021;&#24178;&#39044;&#26041;&#26696;&#65292;&#22312;&#21516;&#26102;&#32771;&#34385;&#20934;&#30830;&#24615;&#21644;&#26102;&#38388;&#24615;&#30340;&#21069;&#25552;&#19979;&#65292;&#26681;&#25454;&#38382;&#39064;&#21644;&#29992;&#25143;&#30340;&#23646;&#24615;&#33258;&#36866;&#24212;&#22320;&#23637;&#31034;AI&#36741;&#21161;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38656;&#35201;&#39640;&#20934;&#30830;&#24615;&#20294;&#21516;&#26102;&#26102;&#38388;&#21448;&#32039;&#36843;&#30340;&#29615;&#22659;&#19979;&#65292;&#20363;&#22914;&#22312;&#24613;&#35786;&#23460;&#24037;&#20316;&#30340;&#21307;&#29983;&#65292;&#25105;&#20204;&#24076;&#26395;&#25552;&#20379;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#65292;&#26082;&#33021;&#25552;&#39640;&#20934;&#30830;&#24615;&#21448;&#33021;&#20943;&#23569;&#26102;&#38388;&#12290;&#20294;&#26159;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#21151;&#33021;&#24102;&#26469;&#30340;&#22909;&#22788;&#26159;&#19981;&#21516;&#30340;&#65306;&#19968;&#20123;&#33021;&#22815;&#20943;&#23569;&#26102;&#38388;&#65292;&#20294;&#20250;&#22686;&#21152;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#36807;&#24230;&#20381;&#36182;&#65292;&#32780;&#20854;&#20182;&#19968;&#20123;&#21017;&#30456;&#21453;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24076;&#26395;&#26681;&#25454;&#38382;&#39064;&#21644;&#29992;&#25143;&#30340;&#21508;&#31181;&#23646;&#24615;&#65288;&#22914;&#30693;&#35782;&#27700;&#24179;&#65289;&#26469;&#33258;&#36866;&#24212;&#22320;&#23637;&#31034;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#65292;&#20197;&#20415;&#22312;&#20934;&#30830;&#24615;&#21644;&#26102;&#38388;&#24615;&#20043;&#38388;&#20570;&#20986;&#26368;&#20339;&#26435;&#34913;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#29992;&#25143;&#38656;&#35201;&#20026;&#22806;&#26143;&#20154;&#24320;&#33647;&#26041;&#30340;&#30740;&#31350;&#26469;&#25506;&#32034;&#33258;&#36866;&#24212;AI&#36741;&#21161;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#26681;&#25454;&#38382;&#39064;&#33258;&#36866;&#24212;AI&#36741;&#21161;&#26159;&#26377;&#30410;&#30340;&#65292;&#21487;&#20197;&#36798;&#21040;&#26102;&#38388;&#21644;&#20934;&#30830;&#24615;&#30340;&#33391;&#22909;&#24179;&#34913;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#23558;&#32771;&#34385;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65288;&#22914;&#24378;&#21270;&#23398;&#20064;&#65289;&#26469;&#23454;&#29616;&#24555;&#36895;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
In settings where users are both time-pressured and need high accuracy, such as doctors working in Emergency Rooms, we want to provide AI assistance that both increases accuracy and reduces time. However, different types of AI assistance have different benefits: some reduce time taken while increasing overreliance on AI, while others do the opposite. We therefore want to adapt what AI assistance we show depending on various properties (of the question and of the user) in order to best tradeoff our two objectives. We introduce a study where users have to prescribe medicines to aliens, and use it to explore the potential for adapting AI assistance. We find evidence that it is beneficial to adapt our AI assistance depending on the question, leading to good tradeoffs between time taken and accuracy. Future work would consider machine-learning algorithms (such as reinforcement learning) to automatically adapt quickly.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#22823;&#35268;&#27169;&#25628;&#32034;&#24341;&#25806;&#26085;&#24535;&#21644;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#25552;&#20379;&#21450;&#26102;&#20934;&#30830;&#30340;&#30123;&#33495;&#24847;&#21521;&#21644;&#20851;&#27880;&#38382;&#39064;&#30340;&#25968;&#25454;&#65292;&#21487;&#20197;&#24110;&#21161;&#35774;&#35745;&#26377;&#38024;&#23545;&#24615;&#30340;&#30123;&#33495;&#23459;&#20256;&#25945;&#32946;&#27963;&#21160;&#65292;&#21516;&#26102;&#21457;&#29616;&#30123;&#33495;&#35266;&#26395;&#32773;&#25628;&#32034;&#26377;&#20851;&#21103;&#20316;&#29992;&#21644;&#26367;&#20195;&#33647;&#29289;&#30340;&#20449;&#24687;&#65292;&#24182;&#34920;&#36798;&#23545;&#30123;&#33495;&#23433;&#20840;&#24615;&#21644;&#25919;&#24220;&#20449;&#35465;&#30340;&#25285;&#24551;&#12290;</title><link>http://arxiv.org/abs/2306.07457</link><description>&lt;p&gt;
&#22522;&#20110;&#32593;&#32476;&#25628;&#32034;&#35760;&#24405;&#30340;&#30123;&#33495;&#25509;&#31181;&#21644;&#30123;&#33495;&#25345;&#26377;&#32773;&#20851;&#27880;&#38382;&#39064;&#30340;&#20934;&#30830;&#27979;&#37327;
&lt;/p&gt;
&lt;p&gt;
Accurate Measures of Vaccination and Concerns of Vaccine Holdouts from Web Search Logs. (arXiv:2306.07457v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07457
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#22823;&#35268;&#27169;&#25628;&#32034;&#24341;&#25806;&#26085;&#24535;&#21644;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#25552;&#20379;&#21450;&#26102;&#20934;&#30830;&#30340;&#30123;&#33495;&#24847;&#21521;&#21644;&#20851;&#27880;&#38382;&#39064;&#30340;&#25968;&#25454;&#65292;&#21487;&#20197;&#24110;&#21161;&#35774;&#35745;&#26377;&#38024;&#23545;&#24615;&#30340;&#30123;&#33495;&#23459;&#20256;&#25945;&#32946;&#27963;&#21160;&#65292;&#21516;&#26102;&#21457;&#29616;&#30123;&#33495;&#35266;&#26395;&#32773;&#25628;&#32034;&#26377;&#20851;&#21103;&#20316;&#29992;&#21644;&#26367;&#20195;&#33647;&#29289;&#30340;&#20449;&#24687;&#65292;&#24182;&#34920;&#36798;&#23545;&#30123;&#33495;&#23433;&#20840;&#24615;&#21644;&#25919;&#24220;&#20449;&#35465;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35774;&#35745;&#26377;&#25928;&#30340;&#30123;&#33495;&#25919;&#31574;&#65292;&#20915;&#31574;&#32773;&#38656;&#35201;&#35814;&#32454;&#30340;&#25968;&#25454;&#65292;&#20102;&#35299;&#35841;&#25509;&#31181;&#20102;&#30123;&#33495;&#65292;&#35841;&#27809;&#26377;&#25509;&#31181;&#65292;&#20197;&#21450;&#20026;&#20160;&#20040;&#12290;&#28982;&#32780;&#65292;&#32654;&#22269;&#29616;&#26377;&#30340;&#25968;&#25454;&#19981;&#36275;&#65306;&#25253;&#36947;&#30340;&#25509;&#31181;&#29575;&#24120;&#24120;&#24310;&#36831;&#25110;&#32570;&#22833;&#65292;&#25509;&#31181;&#29369;&#35947;&#35843;&#26597;&#21463;&#21040;&#39640;&#23618;&#27425;&#38382;&#39064;&#21644;&#33258;&#25105;&#25253;&#21578;&#20559;&#35265;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#35268;&#27169;&#25628;&#32034;&#24341;&#25806;&#26085;&#24535;&#21644;&#26426;&#22120;&#23398;&#20064;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#26469;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#24182;&#25552;&#20379;&#26377;&#20851;&#30123;&#33495;&#24847;&#21521;&#21644;&#34892;&#20026;&#30340;&#26032;&#35265;&#35299;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#30123;&#33495;&#24847;&#21521;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#26816;&#27979;&#29992;&#25143;&#26159;&#21542;&#22312;&#25628;&#32034;COVID-19&#30123;&#33495;&#12290;&#25105;&#20204;&#30340;&#20998;&#31867;&#22120;&#19982;CDC&#30123;&#33495;&#25509;&#31181;&#29575;&#20855;&#26377;&#24456;&#24378;&#30340;&#19968;&#33268;&#24615;&#65292;&#30456;&#20851;&#24615;&#36229;&#36807;0.86&#65292;&#24182;&#23454;&#26102;&#20272;&#31639;&#30123;&#33495;&#24847;&#21521;&#29575;&#21040;&#37038;&#25919;&#32534;&#30721;&#32423;&#21035;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#31934;&#30830;&#23450;&#20301;&#19981;&#21516;&#22320;&#21306;&#12289;&#20154;&#21475;&#32479;&#35745;&#23398;&#21644;&#26102;&#38388;&#30340;&#30123;&#33495;&#38656;&#27714;&#36235;&#21183;&#12290;&#20026;&#20102;&#35843;&#26597;&#30123;&#33495;&#29369;&#35947;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#20998;&#31867;&#22120;&#35782;&#21035;&#20102;&#20004;&#20010;&#32676;&#20307;&#65292;&#25509;&#31181;&#32773;&#21644;&#25345;&#35266;&#26395;&#24577;&#24230;&#32773;&#65292;&#24182;&#27604;&#36739;&#20102;&#23427;&#20204;&#30340;&#25628;&#32034;&#34892;&#20026;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30123;&#33495;&#35266;&#26395;&#32773;&#24448;&#24448;&#25628;&#32034;&#26377;&#20851;&#21103;&#20316;&#29992;&#21644;&#26367;&#20195;&#33647;&#29289;&#30340;&#20449;&#24687;&#65292;&#24182;&#34920;&#36798;&#23545;&#30123;&#33495;&#23433;&#20840;&#24615;&#21644;&#25919;&#24220;&#20449;&#35465;&#30340;&#25285;&#24551;&#12290;&#30456;&#21453;&#65292;&#25509;&#31181;&#30340;&#20010;&#20307;&#20542;&#21521;&#20110;&#25628;&#32034;&#26377;&#20851;&#30123;&#33495;&#30340;&#26032;&#38395;&#21644;&#30123;&#33495;&#21487;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;&#25628;&#32034;&#24341;&#25806;&#26085;&#24535;&#21487;&#20197;&#25552;&#20379;&#21450;&#26102;&#20934;&#30830;&#30340;&#30123;&#33495;&#24847;&#21521;&#21644;&#20851;&#27880;&#38382;&#39064;&#30340;&#25968;&#25454;&#65292;&#36825;&#21487;&#20197;&#20026;&#26377;&#38024;&#23545;&#24615;&#30340;&#30123;&#33495;&#23459;&#20256;&#25945;&#32946;&#27963;&#21160;&#30340;&#35774;&#35745;&#25552;&#20379;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
To design effective vaccine policies, policymakers need detailed data about who has been vaccinated, who is holding out, and why. However, existing data in the US are insufficient: reported vaccination rates are often delayed or missing, and surveys of vaccine hesitancy are limited by high-level questions and self-report biases. Here, we show how large-scale search engine logs and machine learning can be leveraged to fill these gaps and provide novel insights about vaccine intentions and behaviors. First, we develop a vaccine intent classifier that can accurately detect when a user is seeking the COVID-19 vaccine on search. Our classifier demonstrates strong agreement with CDC vaccination rates, with correlations above 0.86, and estimates vaccine intent rates to the level of ZIP codes in real time, allowing us to pinpoint more granular trends in vaccine seeking across regions, demographics, and time. To investigate vaccine hesitancy, we use our classifier to identify two groups, vaccin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#20010;&#30001;CLIP&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35299;&#26512;&#26790;&#22659;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#22270;&#20687;&#30340;&#31995;&#32479;&#25152;&#20135;&#29983;&#30340;&#32472;&#30011;&#23384;&#26723;&#65292;&#25552;&#20986;&#20102;&#22235;&#20010;&#20998;&#31867;&#20197;&#35299;&#37322;&#21644;&#25551;&#36848;CLIP&#29983;&#25104;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#36328;&#35821;&#35328;&#12289;&#31526;&#21495;&#31995;&#32479;&#21644;&#35013;&#32622;&#21508;&#27169;&#22359;&#20043;&#38388;&#30340;&#32763;&#35793;&#36807;&#31243;&#21644;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#24847;&#24819;&#19981;&#21040;&#12289;&#35270;&#35273;&#21560;&#24341;&#20154;&#12289;&#26790;&#22659;&#33324;&#30340;CLIP&#29983;&#25104;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.07429</link><description>&lt;p&gt;
&#36890;&#36807;&#20849;&#21019;&#22270;&#30011;&#21644;&#20114;&#21160;&#35299;&#37322;CLIP
&lt;/p&gt;
&lt;p&gt;
Explaining CLIP through Co-Creative Drawings and Interaction. (arXiv:2306.07429v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#20010;&#30001;CLIP&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35299;&#26512;&#26790;&#22659;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#22270;&#20687;&#30340;&#31995;&#32479;&#25152;&#20135;&#29983;&#30340;&#32472;&#30011;&#23384;&#26723;&#65292;&#25552;&#20986;&#20102;&#22235;&#20010;&#20998;&#31867;&#20197;&#35299;&#37322;&#21644;&#25551;&#36848;CLIP&#29983;&#25104;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#36328;&#35821;&#35328;&#12289;&#31526;&#21495;&#31995;&#32479;&#21644;&#35013;&#32622;&#21508;&#27169;&#22359;&#20043;&#38388;&#30340;&#32763;&#35793;&#36807;&#31243;&#21644;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#24847;&#24819;&#19981;&#21040;&#12289;&#35270;&#35273;&#21560;&#24341;&#20154;&#12289;&#26790;&#22659;&#33324;&#30340;CLIP&#29983;&#25104;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#20010;&#35270;&#35273;&#23384;&#26723;&#65292;&#20854;&#20013;&#21253;&#25324;&#30001;&#20132;&#20114;&#24335;&#26426;&#22120;&#20154;&#33402;&#26415;&#35013;&#32622;&#20135;&#29983;&#30340;&#32472;&#30011;&#65292;&#35266;&#20247;&#36890;&#36807;CLIPdraw&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#33258;&#24049;&#30340;&#26790;&#24819;&#35762;&#36848;&#32473;&#19968;&#20010;&#35299;&#37322;&#21644;&#36716;&#25442;&#26790;&#24819;&#30340;&#31995;&#32479;&#12290;&#25991;&#20013;&#35752;&#35770;&#24182;&#32858;&#31867;&#20102;&#36825;&#20123;&#22270;&#30011;&#65292;&#20854;&#20013;&#21253;&#25324;&#22522;&#20110;&#27010;&#24565;&#34920;&#36848;&#20934;&#30830;&#24615;&#30340;&#22235;&#20010;&#20998;&#31867;&#65292;&#20197;&#25551;&#36848;&#21644;&#35299;&#37322;CLIP&#29983;&#25104;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#24378;&#35843;&#35821;&#35328;&#12289;&#31526;&#21495;&#31995;&#32479;&#21644;&#35013;&#32622;&#21508;&#27169;&#22359;&#20043;&#38388;&#30340;&#32763;&#35793;&#36807;&#31243;&#21644;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#30001;&#20154;&#24037;&#26234;&#33021;&#35299;&#37322;&#12289;&#20013;&#20171;&#21644;&#36171;&#24418;&#30340;&#26790;&#24819;&#38598;&#21512;&#65292;&#24378;&#35843;&#36825;&#20010;&#31995;&#32479;&#20135;&#29983;&#20102;&#24847;&#24819;&#19981;&#21040;&#12289;&#35270;&#35273;&#21560;&#24341;&#20154;&#12289;&#26790;&#22659;&#33324;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper analyses a visual archive of drawings produced by an interactive robotic art installation where audience members narrated their dreams into a system powered by CLIPdraw deep learning (DL) model that interpreted and transformed their dreams into images. The resulting archive of prompt-image pairs were examined and clustered based on concept representation accuracy. As a result of the analysis, the paper proposes four groupings for describing and explaining CLIP-generated results: clear concept, text-to-text as image, indeterminacy and confusion, and lost in translation. This article offers a glimpse into a collection of dreams interpreted, mediated and given form by Artificial Intelligence (AI), showcasing oftentimes unexpected, visually compelling or, indeed, the dream-like output of the system, with the emphasis on processes and results of translations between languages, sign-systems and various modules of the installation. In the end, the paper argues that proposed cluster
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#26426;&#22120;&#20154;&#24037;&#20855;&#30340;&#20114;&#21160;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#21487;&#34892;&#24615;&#26159;&#22235;&#36275;&#21160;&#29289;&#27493;&#24577;&#36716;&#25442;&#30340;&#37325;&#35201;&#26631;&#20934;&#12290;&#20854;&#20013;&#65292;&#27493;-&#23567;&#36305;&#27493;&#24577;&#36716;&#25442;&#33021;&#22815;&#22312;&#24179;&#22374;&#22320;&#24418;&#19978;&#21516;&#26102;&#25552;&#39640;&#21487;&#34892;&#24615;&#21644;&#33410;&#33021;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.07419</link><description>&lt;p&gt;
DeepTransition&#65306;&#21487;&#34892;&#24615;&#23548;&#33268;&#27493;&#24577;&#36716;&#25442;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
DeepTransition: Viability Leads to the Emergence of Gait Transitions in Learning Anticipatory Quadrupedal Locomotion Skills. (arXiv:2306.07419v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#26426;&#22120;&#20154;&#24037;&#20855;&#30340;&#20114;&#21160;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#21487;&#34892;&#24615;&#26159;&#22235;&#36275;&#21160;&#29289;&#27493;&#24577;&#36716;&#25442;&#30340;&#37325;&#35201;&#26631;&#20934;&#12290;&#20854;&#20013;&#65292;&#27493;-&#23567;&#36305;&#27493;&#24577;&#36716;&#25442;&#33021;&#22815;&#22312;&#24179;&#22374;&#22320;&#24418;&#19978;&#21516;&#26102;&#25552;&#39640;&#21487;&#34892;&#24615;&#21644;&#33410;&#33021;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22235;&#36275;&#21160;&#29289;&#22312;&#25913;&#21464;&#36816;&#21160;&#36895;&#24230;&#26102;&#33021;&#22815;&#26080;&#32541;&#22320;&#36716;&#25442;&#27493;&#24577;&#12290;&#26412;&#25991;&#25552;&#20986;&#21487;&#34892;&#24615;&#65288;&#21363;&#36991;&#20813;&#36300;&#20498;&#65289;&#20195;&#34920;&#27493;&#24577;&#36716;&#25442;&#30340;&#19968;&#20010;&#37325;&#35201;&#26631;&#20934;&#12290;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#26426;&#22120;&#20154;&#24037;&#20855;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27493;&#24577;&#36716;&#25442;&#30340;&#20986;&#29616;&#12290;&#19968;&#33268;&#20110;&#22235;&#36275;&#21160;&#29289;&#25968;&#25454;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#24179;&#22374;&#22320;&#24418;&#19978;&#65292;&#22235;&#36275;&#26426;&#22120;&#20154;&#30340;&#27493;-&#23567;&#36305;&#27493;&#24577;&#36716;&#25442;&#33021;&#21516;&#26102;&#25552;&#39640;&#21487;&#34892;&#24615;&#21644;&#33410;&#33021;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31163;&#25955;&#22320;&#24418;&#65288;&#21363;&#31359;&#36234;&#36830;&#32493;&#38388;&#38548;&#65289;&#23545;&#24378;&#21046;&#27493;&#24577;&#36716;&#25442;&#30340;&#24433;&#21709;&#65292;&#24182;&#25214;&#21040;&#36275;-&#36454;&#27493;&#24577;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quadruped animals seamlessly transition between gaits as they change locomotion speeds. While the most widely accepted explanation for gait transitions is energy efficiency, there is no clear consensus on the determining factor, nor on the potential effects from terrain properties. In this article, we propose that viability, i.e. the avoidance of falls, represents an important criterion for gait transitions. We investigate the emergence of gait transitions through the interaction between supraspinal drive (brain), the central pattern generator in the spinal cord, the body, and exteroceptive sensing by leveraging deep reinforcement learning and robotics tools. Consistent with quadruped animal data, we show that the walk-trot gait transition for quadruped robots on flat terrain improves both viability and energy efficiency. Furthermore, we investigate the effects of discrete terrain (i.e. crossing successive gaps) on imposing gait transitions, and find the emergence of trot-pronk transit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#31867;&#20284;&#21160;&#29289;&#22823;&#33041;&#22312;&#39135;&#29289;&#21294;&#20047;&#26102;&#30340;&#20302;&#21151;&#32791;&#26426;&#21046;&#24212;&#29992;&#20110;&#31070;&#32463;&#24418;&#24577;&#31995;&#32479;&#20013;&#65292;&#36890;&#36807;&#32553;&#25918;&#31361;&#35302;&#26435;&#20540;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#21151;&#32791;&#65288;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#38477;&#20302;80&#65285;&#20197;&#19978;&#65289;&#65292;&#21516;&#26102;&#23545;&#20934;&#30830;&#24615;&#24433;&#21709;&#36739;&#23567;&#12290;&#36825;&#20026;&#35774;&#35745;&#29992;&#20110;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#31070;&#32463;&#24418;&#24577;&#31995;&#32479;&#25552;&#20379;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2306.07416</link><description>&lt;p&gt;
&#31361;&#35302;&#32553;&#25918;&#21644;&#26368;&#20248;&#20559;&#32622;&#35843;&#25972;&#22312;&#31070;&#32463;&#24418;&#24577;&#31995;&#32479;&#21151;&#32791;&#38477;&#20302;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Synaptic Scaling and Optimal Bias Adjustments for Power Reduction in Neuromorphic Systems. (arXiv:2306.07416v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#31867;&#20284;&#21160;&#29289;&#22823;&#33041;&#22312;&#39135;&#29289;&#21294;&#20047;&#26102;&#30340;&#20302;&#21151;&#32791;&#26426;&#21046;&#24212;&#29992;&#20110;&#31070;&#32463;&#24418;&#24577;&#31995;&#32479;&#20013;&#65292;&#36890;&#36807;&#32553;&#25918;&#31361;&#35302;&#26435;&#20540;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#21151;&#32791;&#65288;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#38477;&#20302;80&#65285;&#20197;&#19978;&#65289;&#65292;&#21516;&#26102;&#23545;&#20934;&#30830;&#24615;&#24433;&#21709;&#36739;&#23567;&#12290;&#36825;&#20026;&#35774;&#35745;&#29992;&#20110;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#31070;&#32463;&#24418;&#24577;&#31995;&#32479;&#25552;&#20379;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#21160;&#29289;&#23454;&#39564;&#24050;&#32463;&#26174;&#31034;&#20986;&#29983;&#29289;&#22823;&#33041;&#22312;&#39135;&#29289;&#21294;&#20047;&#26102;&#21487;&#20197;&#36827;&#20837;&#20302;&#21151;&#32791;&#27169;&#24335;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#31867;&#20284;&#26426;&#21046;&#24212;&#29992;&#20110;&#19968;&#31867;&#24378;&#28872;&#20381;&#36182;&#20110;&#31361;&#35302;&#26435;&#20540;&#22823;&#23567;&#30340;&#31070;&#32463;&#24418;&#24577;&#31995;&#32479;&#20013;&#30340;&#21487;&#33021;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#25968;&#23398;&#27169;&#22411;&#21644;&#20223;&#30495;&#34920;&#26126;&#65292;&#20180;&#32454;&#32553;&#25918;&#31361;&#35302;&#26435;&#20540;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#21151;&#32791;&#65288;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#38477;&#20302;80&#65285;&#20197;&#19978;&#65289;&#65292;&#21516;&#26102;&#23545;&#20934;&#30830;&#24615;&#24433;&#21709;&#36739;&#23567;&#12290;&#36825;&#20123;&#32467;&#26524;&#25581;&#31034;&#20102;&#19968;&#31181;&#20196;&#20154;&#20852;&#22859;&#30340;&#26426;&#20250;&#65292;&#21363;&#35774;&#35745;&#29992;&#20110;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#31070;&#32463;&#24418;&#24577;&#31995;&#32479;&#65292;&#20854;&#20013;&#21151;&#32791;&#21487;&#20197;&#26681;&#25454;&#33021;&#37327;&#20379;&#24212;&#21644;&#24615;&#33021;&#35201;&#27714;&#36827;&#34892;&#21160;&#24577;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent animal studies have shown that biological brains can enter a low power mode in times of food scarcity. This paper explores the possibility of applying similar mechanisms to a broad class of neuromorphic systems where power consumption is strongly dependent on the magnitude of synaptic weights. In particular, we show through mathematical models and simulations that careful scaling of synaptic weights can significantly reduce power consumption (by over 80\% in some of the cases tested) while having a relatively small impact on accuracy. These results uncover an exciting opportunity to design neuromorphic systems for edge AI applications, where power consumption can be dynamically adjusted based on energy availability and performance requirements.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#25239;&#24615;&#32858;&#38598;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#23545;&#25163;&#32676;&#26469;&#35299;&#20915;&#20869;&#37096;&#20248;&#21270;&#38382;&#39064;&#30340;&#22256;&#38590;&#65292;&#20174;&#32780;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#23545;&#26368;&#24046;&#34920;&#29616;&#32773;&#20013;&#26368;&#22351;&#30340;k&#20010;&#34920;&#29616;&#30340;&#24179;&#22343;&#34920;&#29616;&#26469;&#35299;&#20915;&#36807;&#24230;&#24754;&#35266;&#20027;&#20041;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.07408</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#25928;&#30340;&#23545;&#25239;&#24615;&#32858;&#38598;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Robust Reinforcement Learning through Efficient Adversarial Herding. (arXiv:2306.07408v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#25239;&#24615;&#32858;&#38598;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#23545;&#25163;&#32676;&#26469;&#35299;&#20915;&#20869;&#37096;&#20248;&#21270;&#38382;&#39064;&#30340;&#22256;&#38590;&#65292;&#20174;&#32780;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#23545;&#26368;&#24046;&#34920;&#29616;&#32773;&#20013;&#26368;&#22351;&#30340;k&#20010;&#34920;&#29616;&#30340;&#24179;&#22343;&#34920;&#29616;&#26469;&#35299;&#20915;&#36807;&#24230;&#24754;&#35266;&#20027;&#20041;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#22312;&#31574;&#30053;&#35774;&#35745;&#26041;&#38754;&#34987;&#35748;&#20026;&#26159;&#19994;&#30028;&#40644;&#37329;&#26631;&#20934;&#65292;&#20294;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#65292;&#23427;&#24182;&#19981;&#24635;&#33021;&#25552;&#20379;&#20581;&#22766;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#29615;&#22659;&#26292;&#38706;&#20110;&#28508;&#22312;&#24178;&#25200;&#26102;&#20005;&#37325;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#32463;&#36807;&#35777;&#26126;&#65292;&#20351;&#29992;&#21452;&#20154;&#21338;&#24328;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;RL&#20195;&#29702;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#23545;&#25239;&#24615;&#32858;&#38598;&#26469;&#25193;&#23637;&#21452;&#20154;&#21338;&#24328;&#65292;&#24182;&#28041;&#21450;&#19968;&#20010;&#23545;&#25163;&#32676;&#65292;&#20197;&#35299;&#20915;($\textit{i}$)&#20869;&#37096;&#20248;&#21270;&#38382;&#39064;&#30340;&#22256;&#38590;&#21644;($\textit{ii}$)&#30001;&#20110;&#20505;&#36873;&#23545;&#25163;&#38598;&#21487;&#33021;&#21253;&#21547;&#19981;&#22826;&#21487;&#33021;&#30340;&#24773;&#20917;&#32780;&#21487;&#33021;&#20135;&#29983;&#30340;&#36807;&#24230;&#24754;&#35266;&#20027;&#20041;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#23545;&#25239;&#24615;&#32858;&#38598;&#21487;&#20197;&#26377;&#25928;&#22320;&#36817;&#20284;&#20869;&#37096;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#29992;&#26368;&#24046;&#30340;&#34920;&#29616;&#32773;&#20013;&#26368;&#24046;&#30340;k&#20010;&#34920;&#29616;&#30340;&#24179;&#22343;&#34920;&#29616;&#26367;&#25442;&#20869;&#37096;&#20248;&#21270;&#20013;&#30340;&#26368;&#22351;&#24773;&#20917;&#34920;&#29616;&#26469;&#35299;&#20915;&#31532;&#20108;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#31181;&#29615;&#22659;&#19979;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#23545;&#25239;&#24615;&#32858;&#38598;&#22312;&#19982;&#20854;&#20182;&#26356;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although reinforcement learning (RL) is considered the gold standard for policy design, it may not always provide a robust solution in various scenarios. This can result in severe performance degradation when the environment is exposed to potential disturbances. Adversarial training using a two-player max-min game has been proven effective in enhancing the robustness of RL agents. In this work, we extend the two-player game by introducing an adversarial herd, which involves a group of adversaries, in order to address ($\textit{i}$) the difficulty of the inner optimization problem, and ($\textit{ii}$) the potential over pessimism caused by the selection of a candidate adversary set that may include unlikely scenarios. We first prove that adversarial herds can efficiently approximate the inner optimization problem. Then we address the second issue by replacing the worst-case performance in the inner optimization with the average performance over the worst-$k$ adversaries. We evaluate the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20225;&#19994;&#20013;&#20026;&#31867;&#20284;&#23458;&#25143;&#26381;&#21153;&#30340;&#22330;&#26223;&#25152;&#24102;&#26469;&#30340;&#25104;&#26412;&#21644;&#25928;&#30410;&#12290;&#20174;&#35813;&#21697;&#29260;&#23458;&#25143;&#26381;&#21153;&#20195;&#29702;&#30340;&#21453;&#39304;&#20013;&#27604;&#36739;&#20102;&#19977;&#31181;&#19987;&#38376;&#21270;LLM&#30340;&#31574;&#30053;-&#25552;&#31034;&#24037;&#31243;&#12289;&#24494;&#35843;&#21644;&#30693;&#35782;&#33976;&#39311;&#65292;&#21457;&#29616;&#27169;&#22411;&#21709;&#24212;&#30340;&#21487;&#29992;&#24615;&#21487;&#20197;&#24357;&#34917;&#25104;&#26412;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.07402</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32463;&#27982;&#26435;&#34913;&#65306;&#20197;&#26696;&#20363;&#30740;&#31350;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
The economic trade-offs of large language models: A case study. (arXiv:2306.07402v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20225;&#19994;&#20013;&#20026;&#31867;&#20284;&#23458;&#25143;&#26381;&#21153;&#30340;&#22330;&#26223;&#25152;&#24102;&#26469;&#30340;&#25104;&#26412;&#21644;&#25928;&#30410;&#12290;&#20174;&#35813;&#21697;&#29260;&#23458;&#25143;&#26381;&#21153;&#20195;&#29702;&#30340;&#21453;&#39304;&#20013;&#27604;&#36739;&#20102;&#19977;&#31181;&#19987;&#38376;&#21270;LLM&#30340;&#31574;&#30053;-&#25552;&#31034;&#24037;&#31243;&#12289;&#24494;&#35843;&#21644;&#30693;&#35782;&#33976;&#39311;&#65292;&#21457;&#29616;&#27169;&#22411;&#21709;&#24212;&#30340;&#21487;&#29992;&#24615;&#21487;&#20197;&#24357;&#34917;&#25104;&#26412;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32842;&#22825;&#32852;&#31995;&#23458;&#25143;&#26381;&#21153;&#26159;&#19968;&#31181;&#24120;&#35265;&#20570;&#27861;&#12290;&#30001;&#20110;&#38599;&#29992;&#23458;&#26381;&#20195;&#29702;&#21830;&#26159;&#26114;&#36149;&#30340;&#65292;&#35768;&#22810;&#20844;&#21496;&#27491;&#22312;&#36716;&#21521;NLP&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#21487;&#30452;&#25509;&#20351;&#29992;&#25110;&#20462;&#25913;&#30340;&#21709;&#24212;&#26469;&#21327;&#21161;&#20154;&#31867;&#20195;&#29702;&#21830;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#36825;&#31181;&#24773;&#20917;&#30340;&#33258;&#28982;&#36873;&#25321;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#21151;&#25928;&#24517;&#39035;&#19982;&#35757;&#32451;&#21644;&#26381;&#21153;&#25104;&#26412;&#30456;&#24179;&#34913;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;LLMs&#22312;&#20225;&#19994;&#20013;&#20316;&#20026;&#21709;&#24212;&#29983;&#25104;&#24037;&#20855;&#21487;&#23454;&#29616;&#30340;&#23454;&#38469;&#25104;&#26412;&#21644;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25104;&#26412;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;NLP&#27169;&#22411;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#25928;&#29992;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#21333;&#20010;&#21697;&#29260;&#20316;&#20026;&#29616;&#26377;&#20195;&#29702;&#21327;&#21161;&#20135;&#21697;&#32972;&#26223;&#19979;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#25105;&#20204;&#20351;&#29992;&#21697;&#29260;&#23458;&#25143;&#26381;&#21153;&#20195;&#29702;&#30340;&#21453;&#39304;&#27604;&#36739;&#20102;&#19977;&#31181;&#19987;&#38376;&#21270;LLM&#30340;&#31574;&#30053;-&#25552;&#31034;&#24037;&#31243;&#12289;&#24494;&#35843;&#21644;&#30693;&#35782;&#33976;&#39311;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27169;&#22411;&#21709;&#24212;&#30340;&#21487;&#29992;&#24615;&#21487;&#20197;&#24357;&#34917;&#24040;&#22823;&#30340;&#25104;&#26412;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contacting customer service via chat is a common practice. Because employing customer service agents is expensive, many companies are turning to NLP that assists human agents by auto-generating responses that can be used directly or with modifications. Large Language Models (LLMs) are a natural fit for this use case; however, their efficacy must be balanced with the cost of training and serving them. This paper assesses the practical cost and impact of LLMs for the enterprise as a function of the usefulness of the responses that they generate. We present a cost framework for evaluating an NLP model's utility for this use case and apply it to a single brand as a case study in the context of an existing agent assistance product. We compare three strategies for specializing an LLM - prompt engineering, fine-tuning, and knowledge distillation - using feedback from the brand's customer service agents. We find that the usability of a model's responses can make up for a large difference in in
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;BERT&#21644;&#24494;&#35843;&#30340;RobertA&#27169;&#22411;&#21487;&#20197;&#26368;&#22909;&#22320;&#26816;&#27979;ChatGPT&#29983;&#25104;&#30340;AI&#26032;&#38395;&#12290; RobertA&#27169;&#22411;&#22312;&#31934;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#21462;&#24471;&#20102;98&#65285;&#30340;&#24471;&#20998;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#22312;&#25171;&#20987;&#20551;&#26032;&#38395;&#20013;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.07401</link><description>&lt;p&gt;
&#23454;&#29616;BERT&#21644;&#24494;&#35843;RobertA&#26469;&#26816;&#27979;ChatGPT&#29983;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#26032;&#38395;
&lt;/p&gt;
&lt;p&gt;
Implementing BERT and fine-tuned RobertA to detect AI generated news by ChatGPT. (arXiv:2306.07401v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07401
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;BERT&#21644;&#24494;&#35843;&#30340;RobertA&#27169;&#22411;&#21487;&#20197;&#26368;&#22909;&#22320;&#26816;&#27979;ChatGPT&#29983;&#25104;&#30340;AI&#26032;&#38395;&#12290; RobertA&#27169;&#22411;&#22312;&#31934;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#21462;&#24471;&#20102;98&#65285;&#30340;&#24471;&#20998;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#22312;&#25171;&#20987;&#20551;&#26032;&#38395;&#20013;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#20449;&#24687;&#30340;&#20016;&#23500;&#22686;&#21152;&#20102;&#20934;&#30830;&#23454;&#26102;&#35875;&#35328;&#26816;&#27979;&#30340;&#24517;&#35201;&#24615;&#12290;&#25163;&#21160;&#35782;&#21035;&#21644;&#39564;&#35777;AI&#24037;&#20855;&#29983;&#25104;&#30340;&#20551;&#26032;&#38395;&#22312;&#24040;&#22823;&#30340;&#20449;&#24687;&#37327;&#27599;&#22825;&#34987;&#29983;&#25104;&#30340;&#24773;&#20917;&#19979;&#26159;&#19981;&#20999;&#23454;&#38469;&#21644;&#32791;&#26102;&#30340;&#12290;&#36825;&#24341;&#21457;&#20102;&#23545;&#21019;&#24314;&#33258;&#21160;&#21270;&#31995;&#32479;&#20197;&#25214;&#21040;&#20114;&#32852;&#32593;&#19978;&#20551;&#26032;&#38395;&#30340;&#20852;&#36259;&#22686;&#21152;&#12290;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#32463;&#24494;&#35843;&#30340;BERT&#21644;RobertA&#27169;&#22411;&#22312;&#26816;&#27979;AI&#29983;&#25104;&#30340;&#26032;&#38395;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20339;&#25104;&#21151;&#29575;&#12290;&#29305;&#21035;&#26159;&#24494;&#35843;&#36807;&#30340;RobertA&#22312;&#31934;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24471;&#20998;&#20026;98&#65285;&#12290;&#24635;&#20043;&#65292;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#20197;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#35782;&#21035;ChatGPT&#29983;&#25104;&#30340;&#20266;&#36896;&#26032;&#38395;&#12290;RobertA&#21644;BERT&#27169;&#22411;&#30340;&#20986;&#33394;&#34920;&#29616;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#19982;&#20551;&#20449;&#24687;&#20316;&#26007;&#20105;&#20013;&#21487;&#20197;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The abundance of information on social media has increased the necessity of accurate real-time rumour detection. Manual techniques of identifying and verifying fake news generated by AI tools are impracticable and time-consuming given the enormous volume of information generated every day. This has sparked an increase in interest in creating automated systems to find fake news on the Internet. The studies in this research demonstrate that the BERT and RobertA models with fine-tuning had the best success in detecting AI generated news. With a score of 98%, tweaked RobertA in particular showed excellent precision. In conclusion, this study has shown that neural networks can be used to identify bogus news AI generation news created by ChatGPT. The RobertA and BERT models' excellent performance indicates that these models can play a critical role in the fight against misinformation.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30446;&#21069;&#20027;&#35201;&#36816;&#29992;&#20110;&#33521;&#35821;&#20869;&#23481;&#30340;&#26234;&#33021;&#20998;&#26512;&#20013;&#65292;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#26088;&#22312;&#24357;&#34917;&#20854;&#20182;&#35821;&#35328;&#25968;&#25454;&#21294;&#20047;&#30340;&#24773;&#20917;&#12290;&#30740;&#31350;&#20154;&#21592;&#21644;&#25216;&#26415;&#20844;&#21496;&#36890;&#36807;&#26500;&#24314;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#23581;&#35797;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#24182;&#25299;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#22810;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#24212;&#29992;&#20013;&#30340;&#23454;&#36341;&#25928;&#26524;&#20063;&#36827;&#34892;&#30740;&#31350;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;AI&#25903;&#25345;&#30340;&#35821;&#35328;&#25216;&#26415;&#30340;&#35774;&#35745;&#21644;&#37096;&#32626;&#38656;&#35201;&#27880;&#24847;&#26435;&#21147;&#12289;&#19981;&#24179;&#31561;&#21644;&#25991;&#21270;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.07377</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#33521;&#35821;&#20869;&#23481;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Lost in Translation: Large Language Models in Non-English Content Analysis. (arXiv:2306.07377v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07377
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30446;&#21069;&#20027;&#35201;&#36816;&#29992;&#20110;&#33521;&#35821;&#20869;&#23481;&#30340;&#26234;&#33021;&#20998;&#26512;&#20013;&#65292;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#26088;&#22312;&#24357;&#34917;&#20854;&#20182;&#35821;&#35328;&#25968;&#25454;&#21294;&#20047;&#30340;&#24773;&#20917;&#12290;&#30740;&#31350;&#20154;&#21592;&#21644;&#25216;&#26415;&#20844;&#21496;&#36890;&#36807;&#26500;&#24314;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#23581;&#35797;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#24182;&#25299;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#22810;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#24212;&#29992;&#20013;&#30340;&#23454;&#36341;&#25928;&#26524;&#20063;&#36827;&#34892;&#30740;&#31350;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;AI&#25903;&#25345;&#30340;&#35821;&#35328;&#25216;&#26415;&#30340;&#35774;&#35745;&#21644;&#37096;&#32626;&#38656;&#35201;&#27880;&#24847;&#26435;&#21147;&#12289;&#19981;&#24179;&#31561;&#21644;&#25991;&#21270;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;Open AI&#30340;GPT-4&#65292;Meta&#30340;LLaMa&#65292;Google&#30340;PaLM&#65289;&#24050;&#25104;&#20026;&#26500;&#24314;&#22312;&#32447;&#35821;&#35328;&#26234;&#33021;&#20998;&#26512;&#21644;&#29983;&#25104;AI&#31995;&#32479;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#20013;&#20171;&#25105;&#20204;&#22312;&#32593;&#19978;&#30340;&#20132;&#20114;&#65292;&#20363;&#22914;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20869;&#23481;&#23457;&#26680;&#31995;&#32479;&#21644;&#25628;&#32034;&#24341;&#25806;&#65292;&#20027;&#35201;&#26159;&#20026;&#33521;&#35821;&#32780;&#35774;&#35745;&#30340;&#65292;&#32780;&#22312;&#20854;&#20182;&#19990;&#30028;&#19978;&#30340;7000&#31181;&#35821;&#35328;&#20013;&#30340;&#25928;&#26524;&#36828;&#36828;&#19981;&#22914;&#33521;&#35821;&#12290;&#36817;&#26399;&#65292;&#30740;&#31350;&#20154;&#21592;&#21644;&#25216;&#26415;&#20844;&#21496;&#35797;&#22270;&#36890;&#36807;&#26500;&#24314;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#26469;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#23558;&#35299;&#37322;&#36825;&#20123;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20316;&#26041;&#24335;&#20197;&#21450;&#25506;&#32034;&#23427;&#20204;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;&#20854;&#20013;&#65292;&#31532;&#19968;&#37096;&#20998;&#25552;&#20379;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#25216;&#26415;&#35299;&#37322;&#65292;&#33521;&#35821;&#21644;&#20854;&#20182;&#35821;&#35328;&#20043;&#38388;&#21487;&#29992;&#25968;&#25454;&#30340;&#24046;&#36317;&#20197;&#21450;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#35797;&#22270;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#31532;&#20108;&#37096;&#20998;&#22238;&#39038;&#20102;&#26368;&#36817;&#30340;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;&#22810;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#36341;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#20302;&#36164;&#28304;&#35821;&#35328;&#24212;&#29992;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;AI&#25903;&#25345;&#30340;&#35821;&#35328;&#25216;&#26415;&#22312;&#19990;&#30028;&#19978;&#35768;&#22810;&#35821;&#35328;&#20013;&#30340;&#20256;&#25773;&#30340;&#20262;&#29702;&#23398;&#24847;&#20041;&#65292;&#24182;&#24378;&#35843;&#22312;&#35774;&#35745;&#21644;&#37096;&#32626;AI&#31995;&#32479;&#26102;&#38656;&#35201;&#29305;&#21035;&#27880;&#24847;&#26435;&#21147;&#12289;&#19981;&#24179;&#31561;&#21644;&#25991;&#21270;&#24046;&#24322;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, large language models (e.g., Open AI's GPT-4, Meta's LLaMa, Google's PaLM) have become the dominant approach for building AI systems to analyze and generate language online. However, the automated systems that increasingly mediate our interactions online -- such as chatbots, content moderation systems, and search engines -- are primarily designed for and work far more effectively in English than in the world's other 7,000 languages. Recently, researchers and technology companies have attempted to extend the capabilities of large language models into languages other than English by building what are called multilingual language models.  In this paper, we explain how these multilingual language models work and explore their capabilities and limits. Part I provides a simple technical explanation of how large language models work, why there is a gap in available data between English and other languages, and how multilingual language models attempt to bridge that gap. Part 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RPOSST&#30340;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#20174;&#36739;&#22823;&#30340;&#27979;&#35797;&#26696;&#20363;&#27744;&#20013;&#36873;&#25321;&#19968;&#23567;&#37096;&#20998;&#27979;&#35797;&#26679;&#20363;&#65292;&#39564;&#35777;&#20854;&#39640;&#36136;&#37327;&#30340;&#31574;&#30053;&#22312;&#26356;&#24191;&#27867;&#30340;&#29615;&#22659;&#19979;&#20063;&#26159;&#21487;&#38752;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.07372</link><description>&lt;p&gt;
&#20026;&#31574;&#30053;&#36873;&#25321;&#26500;&#24314;&#39640;&#25928;&#12289;&#20581;&#22766;&#30340;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Composing Efficient, Robust Tests for Policy Selection. (arXiv:2306.07372v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07372
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RPOSST&#30340;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#20174;&#36739;&#22823;&#30340;&#27979;&#35797;&#26696;&#20363;&#27744;&#20013;&#36873;&#25321;&#19968;&#23567;&#37096;&#20998;&#27979;&#35797;&#26679;&#20363;&#65292;&#39564;&#35777;&#20854;&#39640;&#36136;&#37327;&#30340;&#31574;&#30053;&#22312;&#26356;&#24191;&#27867;&#30340;&#29615;&#22659;&#19979;&#20063;&#26159;&#21487;&#38752;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#20250;&#20135;&#29983;&#35768;&#22810;&#39640;&#36136;&#37327;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#36873;&#25321;&#21738;&#31181;&#31574;&#30053;&#26102;&#65292;&#23427;&#20204;&#24517;&#39035;&#22312;&#19981;&#21487;&#35299;&#30340;&#22823;&#37327;&#29615;&#22659;&#26465;&#20214;&#19979;&#36827;&#34892;&#27979;&#35797;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31639;&#27861;RPOSST&#65292;&#23427;&#33021;&#22815;&#20174;&#36739;&#22823;&#30340;&#27979;&#35797;&#26696;&#20363;&#27744;&#20013;&#36873;&#25321;&#19968;&#23567;&#37096;&#20998;&#27979;&#35797;&#26679;&#20363;&#65292;&#36825;&#26159;&#26681;&#25454;&#30456;&#23545;&#36739;&#23567;&#30340;&#26679;&#26412;&#35780;&#20272;&#26469;&#23454;&#29616;&#30340;&#12290;RPOSST&#23558;&#27979;&#35797;&#26679;&#20363;&#36873;&#25321;&#38382;&#39064;&#35270;&#20026;&#19968;&#20010;&#21452;&#20154;&#21338;&#24328;&#65292;&#24182;&#20248;&#21270;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;k-of-N&#20581;&#22766;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#38480;&#21046;&#30456;&#23545;&#20110;&#20351;&#29992;&#27744;&#20013;&#25152;&#26377;&#27979;&#35797;&#29992;&#20363;&#30340;&#27979;&#35797;&#30340;&#35823;&#24046;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;RPOSST&#33021;&#22815;&#22312;&#19968;&#20010;&#29609;&#20855;&#21333;&#19968;&#28216;&#25103;&#12289;&#25169;&#20811;&#25968;&#25454;&#38598;&#21644;&#39640;&#20445;&#30495;&#36187;&#36710;&#27169;&#25311;&#22120;&#20013;&#21457;&#29616;&#21487;&#20197;&#35782;&#21035;&#39640;&#36136;&#37327;&#31574;&#30053;&#30340;&#19968;&#23567;&#32452;&#27979;&#35797;&#29992;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern reinforcement learning systems produce many high-quality policies throughout the learning process. However, to choose which policy to actually deploy in the real world, they must be tested under an intractable number of environmental conditions. We introduce RPOSST, an algorithm to select a small set of test cases from a larger pool based on a relatively small number of sample evaluations. RPOSST treats the test case selection problem as a two-player game and optimizes a solution with provable $k$-of-$N$ robustness, bounding the error relative to a test that used all the test cases in the pool. Empirical results demonstrate that RPOSST finds a small set of test cases that identify high quality policies in a toy one-shot game, poker datasets, and a high-fidelity racing simulator.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;HDDL 2.1&#30340;&#24418;&#24335;&#21270;&#21644;&#35821;&#20041;&#23450;&#20041;&#65292;&#26088;&#22312;&#36890;&#36807;&#20174;PDDL 2.1&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#24357;&#34917;HDDL&#19981;&#33021;&#34920;&#31034;&#25968;&#23383;&#21644;&#26102;&#38388;&#32422;&#26463;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2306.07353</link><description>&lt;p&gt;
HDDL 2.1&#65306;&#38754;&#21521;&#26102;&#38388;HTN&#35268;&#21010;&#30340;&#24418;&#24335;&#21270;&#21644;&#35821;&#20041;&#23450;&#20041;
&lt;/p&gt;
&lt;p&gt;
HDDL 2.1: Towards Defining a Formalism and a Semantics for Temporal HTN Planning. (arXiv:2306.07353v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;HDDL 2.1&#30340;&#24418;&#24335;&#21270;&#21644;&#35821;&#20041;&#23450;&#20041;&#65292;&#26088;&#22312;&#36890;&#36807;&#20174;PDDL 2.1&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#24357;&#34917;HDDL&#19981;&#33021;&#34920;&#31034;&#25968;&#23383;&#21644;&#26102;&#38388;&#32422;&#26463;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;&#21644;&#26426;&#22120;&#20154;&#31561;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#38656;&#35201;&#24314;&#27169;&#20016;&#23500;&#22810;&#26679;&#30340;&#33258;&#21160;&#21270;&#35268;&#21010;&#38382;&#39064;&#12290;&#23427;&#20204;&#30340;&#35299;&#20915;&#36890;&#24120;&#38656;&#35201;&#21327;&#35843;&#21644;&#24182;&#34892;&#25191;&#34892;&#12290;&#22312;&#22810;&#20010;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#38382;&#39064;&#33258;&#28982;&#22320;&#20197;&#20998;&#23618;&#20219;&#21153;&#32593;&#32476;&#65288;HTN&#65289;&#24418;&#24335;&#36827;&#34892;&#20998;&#35299;&#21644;&#34920;&#36798;&#12290; HDDL&#26159;&#35268;&#21010;&#22495;&#23450;&#20041;&#35821;&#35328;&#65288;PDDL&#65289;&#30340;&#20998;&#23618;&#25193;&#23637;&#65292;&#19982;PDDL 2.1&#19981;&#21516;&#65292;&#23427;&#19981;&#20801;&#35768;&#34920;&#31034;&#24102;&#26377;&#25968;&#20540;&#21644;&#26102;&#38388;&#32422;&#26463;&#30340;&#35745;&#21010;&#38382;&#39064;&#65292;&#36825;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#24314;&#35758;&#24357;&#34917;HDDL&#19982;&#36825;&#20123;&#25805;&#20316;&#38656;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#20174;PDDL 2.1&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#25193;&#23637;HDDL&#20197;&#34920;&#31034;&#25968;&#23383;&#21644;&#26102;&#38388;&#34920;&#36798;&#24335;&#12290;&#26412;&#25991;&#24320;&#23637;&#20102;&#20851;&#20110;&#26410;&#26469;HDDL 2.1&#25193;&#23637;&#25152;&#38656;&#30340;&#35821;&#20041;&#21644;&#35821;&#27861;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real world applications as in industry and robotics need modelling rich and diverse automated planning problems. Their resolution usually requires coordinated and concurrent action execution. In several cases, these problems are naturally decomposed in a hierarchical way and expressed by a Hierarchical Task Network (HTN) formalism.  HDDL, a hierarchical extension of the Planning Domain Definition Language (PDDL), unlike PDDL 2.1 does not allow to represent planning problems with numerical and temporal constraints, which are essential for real world applications. We propose to fill the gap between HDDL and these operational needs and to extend HDDL by taking inspiration from PDDL 2.1 in order to express numerical and temporal expressions. This paper opens discussions on the semantics and the syntax needed for a future HDDL 2.1 extension.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#25674;&#38144;&#8221;&#30340;&#25991;&#26412;&#21040;&#19977;&#32500;&#29289;&#20307;&#21512;&#25104;&#26041;&#27861;&#65292;&#23558;&#35768;&#22810;&#25552;&#31034;&#19968;&#36215;&#20351;&#29992;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20849;&#20139;&#25552;&#31034;&#20043;&#38388;&#30340;&#20248;&#21270;&#35745;&#31639;&#65292;&#20174;&#32780;&#22312;&#26356;&#30701;&#26102;&#38388;&#20869;&#35757;&#32451;&#27169;&#22411;&#65307;&#35813;&#26041;&#27861;&#33021;&#22815;&#20351;&#19981;&#21516;&#25552;&#31034;&#20043;&#38388;&#36827;&#34892;&#24179;&#28369;&#30340;&#25554;&#20540;&#65292;&#24182;&#22312;&#26032;&#30340;&#36164;&#20135;&#21644;&#31616;&#21333;&#21160;&#30011;&#20043;&#38388;&#23454;&#29616;&#30693;&#35782;&#20849;&#20139;&#21644;&#25512;&#24191;&#12290;</title><link>http://arxiv.org/abs/2306.07349</link><description>&lt;p&gt;
ATT3D&#65306;&#25674;&#38144;&#30340;&#25991;&#26412;&#21040;&#19977;&#32500;&#29289;&#20307;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
ATT3D: Amortized Text-to-3D Object Synthesis. (arXiv:2306.07349v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07349
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#25674;&#38144;&#8221;&#30340;&#25991;&#26412;&#21040;&#19977;&#32500;&#29289;&#20307;&#21512;&#25104;&#26041;&#27861;&#65292;&#23558;&#35768;&#22810;&#25552;&#31034;&#19968;&#36215;&#20351;&#29992;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20849;&#20139;&#25552;&#31034;&#20043;&#38388;&#30340;&#20248;&#21270;&#35745;&#31639;&#65292;&#20174;&#32780;&#22312;&#26356;&#30701;&#26102;&#38388;&#20869;&#35757;&#32451;&#27169;&#22411;&#65307;&#35813;&#26041;&#27861;&#33021;&#22815;&#20351;&#19981;&#21516;&#25552;&#31034;&#20043;&#38388;&#36827;&#34892;&#24179;&#28369;&#30340;&#25554;&#20540;&#65292;&#24182;&#22312;&#26032;&#30340;&#36164;&#20135;&#21644;&#31616;&#21333;&#21160;&#30011;&#20043;&#38388;&#23454;&#29616;&#30693;&#35782;&#20849;&#20139;&#21644;&#25512;&#24191;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#29983;&#25104;&#24335;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#19982;&#22270;&#20687;&#21040;&#19977;&#32500;&#26041;&#27861;&#65288;&#22914;&#31070;&#32463;&#36752;&#23556;&#22330;&#65289;&#30456;&#32467;&#21512;&#65292;&#25991;&#26412;&#33267;&#19977;&#32500;&#24314;&#27169;&#21462;&#24471;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#36827;&#23637;&#12290; DreamFusion &#26368;&#36817;&#21462;&#24471;&#20102;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#65292;&#20294;&#38656;&#35201;&#36827;&#34892;&#28459;&#38271;&#30340;&#12289;&#22522;&#20110;&#25552;&#31034;&#30340;&#20248;&#21270;&#25165;&#33021;&#21019;&#24314;&#19977;&#32500;&#29289;&#20307;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#32479;&#19968;&#30340;&#27169;&#22411;&#21516;&#26102;&#35757;&#32451;&#35768;&#22810;&#25552;&#31034;&#26469;&#25674;&#38144;&#25552;&#31034;&#20248;&#21270;&#65292;&#32780;&#19981;&#26159;&#21333;&#29420;&#35757;&#32451;&#27599;&#20010;&#25552;&#31034;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#25552;&#31034;&#38598;&#21512;&#20013;&#20849;&#20139;&#35745;&#31639;&#65292;&#27604;&#27599;&#20010;&#25552;&#31034;&#30340;&#20248;&#21270;&#25152;&#38656;&#30340;&#26102;&#38388;&#26356;&#30701;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;-Amortized text-to-3D (ATT3D)-&#23454;&#29616;&#20102;&#25552;&#31034;&#20043;&#38388;&#30340;&#30693;&#35782;&#20849;&#20139;&#65292;&#20197;&#20415;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#35774;&#32622;&#65292;&#24182;&#20026;&#26032;&#36164;&#20135;&#21644;&#31616;&#21333;&#21160;&#30011;&#20043;&#38388;&#30340;&#25991;&#26412;&#36827;&#34892;&#24179;&#28369;&#30340;&#25554;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-3D modelling has seen exciting progress by combining generative text-to-image models with image-to-3D methods like Neural Radiance Fields. DreamFusion recently achieved high-quality results but requires a lengthy, per-prompt optimization to create 3D objects. To address this, we amortize optimization over text prompts by training on many prompts simultaneously with a unified model, instead of separately. With this, we share computation across a prompt set, training in less time than per-prompt optimization. Our framework - Amortized text-to-3D (ATT3D) - enables knowledge-sharing between prompts to generalize to unseen setups and smooth interpolations between text for novel assets and simple animations.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;MaPeT&#65292;&#19981;&#21516;&#20110;&#29616;&#26377;&#30340;&#20351;&#29992;&#25513;&#30721;&#22270;&#20687;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#33258;&#22238;&#24402;&#21644;&#32622;&#25442;&#39044;&#27979;&#26469;&#25429;&#33719;&#22270;&#20687;&#22359;&#20869;&#30340;&#20381;&#36182;&#20851;&#31995;&#24182;&#20943;&#23569;&#25968;&#25454;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07346</link><description>&lt;p&gt;
&#23398;&#20064;&#29992;&#20110;&#35270;&#35273;Transformer&#39044;&#35757;&#32451;&#30340;&#25513;&#30721;&#21644;&#32622;&#25442;&#35270;&#35273;&#20196;&#29260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning to Mask and Permute Visual Tokens for Vision Transformer Pre-Training. (arXiv:2306.07346v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;MaPeT&#65292;&#19981;&#21516;&#20110;&#29616;&#26377;&#30340;&#20351;&#29992;&#25513;&#30721;&#22270;&#20687;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#33258;&#22238;&#24402;&#21644;&#32622;&#25442;&#39044;&#27979;&#26469;&#25429;&#33719;&#22270;&#20687;&#22359;&#20869;&#30340;&#20381;&#36182;&#20851;&#31995;&#24182;&#20943;&#23569;&#25968;&#25454;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#25216;&#26415;&#24050;&#25104;&#20026;&#25552;&#39640;&#22270;&#20687;&#20998;&#31867;&#31561;&#35270;&#35273;&#20219;&#21153;&#24615;&#33021;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#20351;&#29992;&#25513;&#30721;&#22270;&#20687;&#27169;&#22411;&#33539;&#24335;&#65292;&#36890;&#36807;&#37325;&#26500;&#19982;&#38543;&#26426;&#25513;&#30721;&#22270;&#20687;&#22359;&#30456;&#20851;&#32852;&#30340;&#35270;&#35273;&#20196;&#29260;&#26469;&#39044;&#35757;&#32451;&#39592;&#24178;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25513;&#34109;&#26041;&#27861;&#20250;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#22122;&#22768;&#36827;&#20837;&#36755;&#20837;&#25968;&#25454;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#36755;&#20837;&#25513;&#34109;&#24573;&#30053;&#20102;&#21463;&#25439;&#22359;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#22686;&#21152;&#20102;&#19979;&#28216;&#24494;&#35843;&#20219;&#21153;&#20013;&#35266;&#23519;&#21040;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21517;&#20026;&#25513;&#34109;&#21644;&#32622;&#25442;&#35270;&#35273;&#21464;&#21387;&#22120;&#65288;MaPeT&#65289;&#65292;&#23427;&#20351;&#29992;&#33258;&#22238;&#24402;&#21644;&#32622;&#25442;&#39044;&#27979;&#26469;&#25429;&#33719;&#22359;&#20869;&#20381;&#36182;&#24615;&#12290;&#27492;&#22806;&#65292;MaPeT&#20351;&#29992;&#36741;&#21161;&#20301;&#32622;&#20449;&#24687;&#26469;&#20943;&#23569;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#38454;&#27573;&#20013;&#30340;&#24046;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of self-supervised pre-training has emerged as a promising approach to enhance the performance of visual tasks such as image classification. In this context, recent approaches have employed the Masked Image Modeling paradigm, which pre-trains a backbone by reconstructing visual tokens associated with randomly masked image patches. This masking approach, however, introduces noise into the input data during pre-training, leading to discrepancies that can impair performance during the fine-tuning phase. Furthermore, input masking neglects the dependencies between corrupted patches, increasing the inconsistencies observed in downstream fine-tuning tasks. To overcome these issues, we propose a new self-supervised pre-training approach, named Masked and Permuted Vision Transformer (MaPeT), that employs autoregressive and permuted predictions to capture intra-patch dependencies. In addition, MaPeT employs auxiliary positional information to reduce the disparity between the pre-trainin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#39033;&#20351;&#29992;&#20247;&#21253;&#25216;&#26415;&#30340;&#35745;&#31639;&#26426;&#31185;&#23398;&#39640;&#31561;&#25945;&#32946;&#20316;&#19994;&#65292;&#40723;&#21169;&#23398;&#29983;&#20016;&#23500;&#38899;&#20048;&#20803;&#25968;&#25454;&#65292;&#24182;&#21019;&#24314;&#20102;&#21487;&#20379;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#38899;&#20048;&#26631;&#35760;&#30340;&#24320;&#25918;&#24615;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.07310</link><description>&lt;p&gt;
&#24212;&#29992;&#20247;&#21253;&#25216;&#26415;&#20016;&#23500;&#39640;&#31561;&#25945;&#32946;&#38899;&#20048;&#30693;&#35782;&#24211;&#30340;&#26041;&#27861;&#21644;&#32463;&#39564;
&lt;/p&gt;
&lt;p&gt;
Employing Crowdsourcing for Enriching a Music Knowledge Base in Higher Education. (arXiv:2306.07310v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#39033;&#20351;&#29992;&#20247;&#21253;&#25216;&#26415;&#30340;&#35745;&#31639;&#26426;&#31185;&#23398;&#39640;&#31561;&#25945;&#32946;&#20316;&#19994;&#65292;&#40723;&#21169;&#23398;&#29983;&#20016;&#23500;&#38899;&#20048;&#20803;&#25968;&#25454;&#65292;&#24182;&#21019;&#24314;&#20102;&#21487;&#20379;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#38899;&#20048;&#26631;&#35760;&#30340;&#24320;&#25918;&#24615;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#39033;&#20351;&#29992;&#20247;&#21253;&#25216;&#26415;&#20316;&#20026;&#35745;&#31639;&#26426;&#31185;&#23398;&#39640;&#31561;&#25945;&#32946;&#23398;&#29983;&#20316;&#19994;&#30340;&#26041;&#27861;&#21644;&#32463;&#39564;&#12290;&#21033;&#29992;&#25903;&#25345;&#25991;&#21270;&#36951;&#20135;&#39046;&#22495;&#20247;&#21253;&#30340;&#24179;&#21488;&#65292;&#40723;&#21169;&#23398;&#29983;&#20016;&#23500;&#19982;&#36873;&#23450;&#38899;&#20048;&#26354;&#30446;&#30456;&#20851;&#30340;&#20803;&#25968;&#25454;&#12290;&#35813;&#27963;&#21160;&#20849;&#26377;98&#21517;&#23398;&#29983;&#21442;&#21152;&#65292;&#20026;854&#39318;&#38899;&#20048;&#26354;&#30446;&#36129;&#29486;&#20102;6400&#22810;&#20010;&#27880;&#37322;&#12290;&#21516;&#26102;&#36824;&#21019;&#24314;&#20102;&#19968;&#20010;&#21487;&#20379;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#38899;&#20048;&#26631;&#35760;&#30340;&#24320;&#25918;&#24615;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#22312;&#32447;&#35843;&#26597;&#65292;&#35813;&#27963;&#21160;&#30340;&#32467;&#26524;&#21644;&#24847;&#35265;&#25910;&#38598;&#20351;&#25105;&#20204;&#33021;&#22815;&#24471;&#20986;&#19968;&#20123;&#26377;&#29992;&#30340;&#35265;&#35299;&#65292;&#20102;&#35299;&#23558;&#20247;&#21253;&#25972;&#21512;&#21040;&#35745;&#31639;&#26426;&#31185;&#23398;&#35838;&#31243;&#20013;&#30340;&#30410;&#22788;&#21644;&#25361;&#25112;&#20197;&#21450;&#22914;&#20309;&#25552;&#39640;&#23398;&#29983;&#30340;&#23398;&#20064;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes the methodology followed and the lessons learned from employing crowdsourcing techniques as part of a homework assignment involving higher education students of computer science. Making use of a platform that supports crowdsourcing in the cultural heritage domain students were solicited to enrich the metadata associated with a selection of music tracks. The results of the campaign were further analyzed and exploited by students through the use of semantic web technologies. In total, 98 students participated in the campaign, contributing more than 6400 annotations concerning 854 tracks. The process also led to the creation of an openly available annotated dataset, which can be useful for machine learning models for music tagging. The campaign's results and the comments gathered through an online survey enable us to draw some useful insights about the benefits and challenges of integrating crowdsourcing into computer science curricula and how this can enhance student
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#32032;&#21151;&#33021;&#30456;&#20284;&#24615;&#30340;&#22312;&#32447;&#21407;&#22411;&#23545;&#40784;&#65288;OPA&#65289;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#23569;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#31574;&#30053;&#36716;&#31227;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#26144;&#23556;&#20989;&#25968;&#23398;&#20064;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#20197;&#21450;&#20381;&#36182;&#35270;&#35273;&#29305;&#24449;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.07307</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#31574;&#30053;&#36801;&#31227;&#30340;&#22312;&#32447;&#21407;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Online Prototype Alignment for Few-shot Policy Transfer. (arXiv:2306.07307v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#32032;&#21151;&#33021;&#30456;&#20284;&#24615;&#30340;&#22312;&#32447;&#21407;&#22411;&#23545;&#40784;&#65288;OPA&#65289;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#23569;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#31574;&#30053;&#36716;&#31227;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#26144;&#23556;&#20989;&#25968;&#23398;&#20064;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#20197;&#21450;&#20381;&#36182;&#35270;&#35273;&#29305;&#24449;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#39046;&#22495;&#36866;&#24212;&#20027;&#35201;&#28041;&#21450;&#22312;&#23558;&#31574;&#30053;&#36716;&#31227;&#21040;&#26032;&#29615;&#22659;&#26102;&#35266;&#23519;&#30340;&#21464;&#21270;&#12290;&#39046;&#22495;&#36866;&#24212;&#30340;&#35768;&#22810;&#20256;&#32479;&#26041;&#27861;&#20197;&#26174;&#24335;&#25110;&#38544;&#24335;&#22320;&#23398;&#20064;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#26144;&#23556;&#20989;&#25968;&#20026;&#20027;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#35775;&#38382;&#30446;&#26631;&#22495;&#30340;&#22823;&#37327;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#24120;&#24120;&#20381;&#36182;&#20110;&#35270;&#35273;&#32447;&#32034;&#26469;&#23398;&#20064;&#26144;&#23556;&#20989;&#25968;&#65292;&#24403;&#28304;&#22495;&#19982;&#30446;&#26631;&#22495;&#30475;&#36215;&#26469;&#38750;&#24120;&#19981;&#21516;&#26102;&#65292;&#21487;&#33021;&#20250;&#22833;&#36133;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#22312;&#32447;&#21407;&#22411;&#23545;&#40784;&#65288;OPA&#65289;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#20803;&#32032;&#30340;&#21151;&#33021;&#30456;&#20284;&#24615;&#26469;&#23398;&#20064;&#26144;&#23556;&#20989;&#25968;&#65292;&#33021;&#22815;&#22312;&#20165;&#20960;&#20010;&#22238;&#21512;&#20869;&#23454;&#29616;&#23569;&#26679;&#26412;&#31574;&#30053;&#36716;&#31227;&#12290;OPA&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#24341;&#20837;&#19968;&#20010;&#25506;&#32034;&#26426;&#21046;&#65292;&#21487;&#20197;&#39640;&#25928;&#12289;&#26377;&#30446;&#30340;&#22320;&#19982;&#30446;&#26631;&#22495;&#30340;&#26410;&#30693;&#20803;&#32032;&#20132;&#20114;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#24050;&#30693;&#30340;&#20803;&#32032;&#36830;&#25509;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain adaptation in reinforcement learning (RL) mainly deals with the changes of observation when transferring the policy to a new environment. Many traditional approaches of domain adaptation in RL manage to learn a mapping function between the source and target domain in explicit or implicit ways. However, they typically require access to abundant data from the target domain. Besides, they often rely on visual clues to learn the mapping function and may fail when the source domain looks quite different from the target domain. To address these problems, we propose a novel framework Online Prototype Alignment (OPA) to learn the mapping function based on the functional similarity of elements and is able to achieve the few-shot policy transfer within only several episodes. The key insight of OPA is to introduce an exploration mechanism that can interact with the unseen elements of the target domain in an efficient and purposeful manner, and then connect them with the seen elements in th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#21035;&#20851;&#32852;&#23884;&#20837;&#65288;CAE&#65289;&#26041;&#27861;&#65292;&#37319;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#23558;&#26679;&#26412;&#29305;&#24449;&#20998;&#31163;&#25104;&#19982;&#31867;&#21035;&#30456;&#20851;&#30340;&#39118;&#26684;&#21521;&#37327;&#65292;&#23454;&#29616;&#20102;&#21307;&#23398;&#22270;&#20687;&#20840;&#23616;&#21487;&#35299;&#37322;&#30340;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.07306</link><description>&lt;p&gt;
&#22522;&#20110;&#31867;&#21035;&#20851;&#32852;&#23884;&#20837;&#21644;&#24490;&#29615;&#23545;&#25239;&#29983;&#25104;&#30340;&#21307;&#23398;&#22270;&#20687;&#20840;&#23616;&#21487;&#35299;&#37322;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active Globally Explainable Learning for Medical Images via Class Association Embedding and Cyclic Adversarial Generation. (arXiv:2306.07306v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#21035;&#20851;&#32852;&#23884;&#20837;&#65288;CAE&#65289;&#26041;&#27861;&#65292;&#37319;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#23558;&#26679;&#26412;&#29305;&#24449;&#20998;&#31163;&#25104;&#19982;&#31867;&#21035;&#30456;&#20851;&#30340;&#39118;&#26684;&#21521;&#37327;&#65292;&#23454;&#29616;&#20102;&#21307;&#23398;&#22270;&#20687;&#20840;&#23616;&#21487;&#35299;&#37322;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#26159;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#24403;&#21069;&#20851;&#20110;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#30740;&#31350;&#32570;&#20047;&#25552;&#21462;&#20851;&#20110;&#23398;&#20064;&#20219;&#21153;&#30340;&#20840;&#23616;&#30693;&#35782;&#30340;&#25928;&#29575;&#65292;&#22240;&#27492;&#23384;&#22312;&#31934;&#24230;&#19981;&#30830;&#23450;&#12289;&#32570;&#20047;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#23450;&#20041;&#27169;&#31946;&#31561;&#32570;&#38519;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31867;&#21035;&#20851;&#32852;&#23884;&#20837;&#65288;CAE&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#37319;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#65292;&#21516;&#26102;&#23558;&#26679;&#26412;&#29305;&#24449;&#20998;&#31163;&#25104;&#19982;&#31867;&#21035;&#30456;&#20851;&#30340;&#21644;&#20010;&#20307;&#30456;&#20851;&#30340;&#39118;&#26684;&#21521;&#37327;&#12290;&#23558;&#32473;&#23450;&#26679;&#26412;&#30340;&#20010;&#20307;&#39118;&#26684;&#30721;&#19982;&#21478;&#19968;&#20010;&#31867;&#21035;&#39118;&#26684;&#30721;&#37325;&#26032;&#32452;&#21512;&#65292;&#25353;&#29031;&#24490;&#29615;&#23545;&#25239;&#23398;&#20064;&#31574;&#30053;&#24471;&#21040;&#19968;&#20010;&#20445;&#30041;&#20010;&#20307;&#29305;&#24449;&#20294;&#25913;&#21464;&#20102;&#31867;&#21035;&#25351;&#23450;&#30340;&#21512;&#25104;&#26679;&#26412;&#12290;&#31867;&#21035;&#20851;&#32852;&#23884;&#20837;&#23558;&#25152;&#26377;&#23454;&#20363;&#30340;&#20840;&#23616;&#31867;&#30456;&#20851;&#29305;&#24449;&#25552;&#28860;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#22495;&#20013;&#65292;&#19981;&#21516;&#31867;&#20043;&#38388;&#30340;&#36716;&#25442;&#35268;&#21017;&#21487;&#20197;&#26356;&#22909;&#22320;&#25552;&#21462;&#21644;&#21487;&#35270;&#21270;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#36879;&#26126;&#24230;&#12290;&#25105;&#20204;&#22312;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#27604;&#29616;&#26377;&#25216;&#26415;&#33719;&#24471;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20840;&#23616;&#21487;&#35299;&#37322;&#30340;&#21487;&#35270;&#21270;&#32467;&#26524;&#20197;&#20415;&#20110;&#35786;&#26029;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainability poses a major challenge to artificial intelligence (AI) techniques. Current studies on explainable AI (XAI) lack the efficiency of extracting global knowledge about the learning task, thus suffer deficiencies such as imprecise saliency, context-aware absence and vague meaning. In this paper, we propose the class association embedding (CAE) approach to address these issues. We employ an encoder-decoder architecture to embed sample features and separate them into class-related and individual-related style vectors simultaneously. Recombining the individual-style code of a given sample with the class-style code of another leads to a synthetic sample with preserved individual characters but changed class assignment, following a cyclic adversarial learning strategy. Class association embedding distills the global class-related features of all instances into a unified domain with well separation between classes. The transition rules between different classes can be then extract
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#20351;&#29992;&#35797;&#28857;&#39044;&#27979;&#26550;&#26500;&#20013;&#30340;&#31639;&#27861;&#24178;&#39044;&#26469;&#25552;&#39640;&#38750;AI&#27169;&#22411;&#19979;&#38024;&#32455;&#21697;&#31867;&#21035;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#22312;&#20915;&#31574;&#27169;&#22411;&#20013;&#21160;&#24577;&#22320;&#36873;&#25321;&#26368;&#20339;&#31639;&#27861;&#21487;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#36890;&#36807;AI / ML&#39044;&#27979;&#27169;&#22411;&#20351;&#29992;&#20808;&#36827;&#30340;&#29305;&#24449;&#24037;&#31243;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.07305</link><description>&lt;p&gt;
&#35753;&#39044;&#27979;&#21464;&#24471;&#33258;&#23398;&#20064;&#21644;&#33258;&#36866;&#24212;--&#35797;&#28857;&#39044;&#27979;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Making forecasting self-learning and adaptive -- Pilot forecasting rack. (arXiv:2306.07305v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07305
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#20351;&#29992;&#35797;&#28857;&#39044;&#27979;&#26550;&#26500;&#20013;&#30340;&#31639;&#27861;&#24178;&#39044;&#26469;&#25552;&#39640;&#38750;AI&#27169;&#22411;&#19979;&#38024;&#32455;&#21697;&#31867;&#21035;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#22312;&#20915;&#31574;&#27169;&#22411;&#20013;&#21160;&#24577;&#22320;&#36873;&#25321;&#26368;&#20339;&#31639;&#27861;&#21487;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#36890;&#36807;AI / ML&#39044;&#27979;&#27169;&#22411;&#20351;&#29992;&#20808;&#36827;&#30340;&#29305;&#24449;&#24037;&#31243;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#21806;&#38144;&#21806;&#21644;&#20215;&#26684;&#39044;&#27979;&#36890;&#24120;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#23545;&#20110;&#26576;&#20123;&#20135;&#21697;&#31867;&#21035;&#65292;&#39044;&#27979;&#38656;&#27714;&#30340;&#20934;&#30830;&#24615;&#36739;&#20302;&#65292;&#20250;&#23545;&#24211;&#23384;&#12289;&#36816;&#36755;&#21644;&#34917;&#36135;&#35745;&#21010;&#36896;&#25104;&#36127;&#38754;&#24433;&#21709;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22522;&#20110;&#31215;&#26497;&#25506;&#32034;&#30340;&#35797;&#28857;&#28436;&#32451;&#30340;&#21457;&#29616;&#65292;&#20197;&#25506;&#32034;&#24110;&#21161;&#38646;&#21806;&#21830;&#25552;&#39640;&#27492;&#31867;&#20135;&#21697;&#31867;&#21035;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#36890;&#36807;&#19968;&#20010;&#26679;&#26412;&#20135;&#21697;&#31867;&#21035;&#8220;&#38024;&#32455;&#21697;&#8221;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#31639;&#27861;&#24178;&#39044;&#26426;&#20250;&#12290;&#30446;&#21069;&#65292;&#38024;&#32455;&#21697;&#20135;&#21697;&#31867;&#21035;&#30340;&#39044;&#27979;&#20934;&#30830;&#24230;&#22312;&#38750;AI&#27169;&#22411;&#20013;&#30340;&#33539;&#22260;&#20026;60%&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#22914;&#20309;&#20351;&#29992;&#26550;&#26500;&#26041;&#27861;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#29983;&#25104;&#39044;&#27979;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#20915;&#31574;&#27169;&#22411;&#26681;&#25454;&#32473;&#23450;&#29366;&#24577;&#21644;&#19978;&#19979;&#25991;&#21160;&#24577;&#22320;&#20174;&#31639;&#27861;&#26550;&#20013;&#36873;&#25321;&#26368;&#20339;&#31639;&#27861;&#12290;&#20351;&#29992;&#20808;&#36827;&#30340;&#29305;&#24449;&#24037;&#31243;&#26500;&#24314;&#30340;AI / ML&#39044;&#27979;&#27169;&#22411;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#38656;&#27714;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#26377;&#25152;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retail sales and price projections are typically based on time series forecasting. For some product categories, the accuracy of demand forecasts achieved is low, negatively impacting inventory, transport, and replenishment planning. This paper presents our findings based on a proactive pilot exercise to explore ways to help retailers to improve forecast accuracy for such product categories.  We evaluated opportunities for algorithmic interventions to improve forecast accuracy based on a sample product category, Knitwear. The Knitwear product category has a current demand forecast accuracy from non-AI models in the range of 60%. We explored how to improve the forecast accuracy using a rack approach. To generate forecasts, our decision model dynamically selects the best algorithm from an algorithm rack based on performance for a given state and context. Outcomes from our AI/ML forecasting model built using advanced feature engineering show an increase in the accuracy of demand forecast f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#26469;&#32479;&#19968;&#23450;&#20041;&#21644;&#28548;&#28165;&#33258;&#21160;&#27010;&#24565;&#25552;&#21462;&#21644;&#27010;&#24565;&#37325;&#35201;&#24615;&#35780;&#20272;&#65292;&#36827;&#32780;&#25552;&#20379;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#20197;&#23454;&#29616;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#27604;&#36739;&#20197;&#21450;&#25512;&#23548;&#20851;&#20110;&#36825;&#31181;&#26041;&#27861;&#30340;&#26368;&#20248;&#24615;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.07304</link><description>&lt;p&gt;
&#19968;&#31181;&#32479;&#19968;&#33258;&#21160;&#27010;&#24565;&#25552;&#21462;&#21644;&#27010;&#24565;&#37325;&#35201;&#24615;&#35780;&#20272;&#30340;&#20840;&#38754;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Holistic Approach to Unifying Automatic Concept Extraction and Concept Importance Estimation. (arXiv:2306.07304v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#26469;&#32479;&#19968;&#23450;&#20041;&#21644;&#28548;&#28165;&#33258;&#21160;&#27010;&#24565;&#25552;&#21462;&#21644;&#27010;&#24565;&#37325;&#35201;&#24615;&#35780;&#20272;&#65292;&#36827;&#32780;&#25552;&#20379;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#20197;&#23454;&#29616;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#27604;&#36739;&#20197;&#21450;&#25512;&#23548;&#20851;&#20110;&#36825;&#31181;&#26041;&#27861;&#30340;&#26368;&#20248;&#24615;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#27010;&#24565;&#30340;&#26041;&#27861;&#25104;&#20026;&#20102;&#19968;&#20123;&#26368;&#26377;&#21069;&#36884;&#30340;&#21487;&#35299;&#37322;&#26041;&#27861;&#65292;&#24110;&#21161;&#25105;&#20204;&#35299;&#37322;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30340;&#20915;&#31574;&#12290;&#36825;&#20123;&#26041;&#27861;&#35797;&#22270;&#22312;&#20004;&#20010;&#20851;&#38190;&#27493;&#39588;&#20013;&#21457;&#29616;&#34987;&#38544;&#34255;&#22312;ANN&#28608;&#27963;&#30340;&#22797;&#26434;&#27169;&#24335;&#20013;&#30340;&#21487;&#29702;&#35299;&#30340;&#35270;&#35273;&#8220;&#27010;&#24565;&#8221;&#65306;&#65288;1&#65289;&#27010;&#24565;&#25552;&#21462;&#65292;&#65288;2&#65289;&#37325;&#35201;&#24615;&#35780;&#20272;&#12290;&#34429;&#28982;&#36825;&#20004;&#20010;&#27493;&#39588;&#26159;&#21508;&#31181;&#26041;&#27861;&#20043;&#38388;&#20849;&#21516;&#30340;&#65292;&#20294;&#23427;&#20204;&#30340;&#20855;&#20307;&#23454;&#29616;&#37117;&#26377;&#25152;&#19981;&#21516;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#20840;&#38754;&#23450;&#20041;&#21644;&#28548;&#28165;&#20102;&#36825;&#20004;&#20010;&#27493;&#39588;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#20960;&#20010;&#20248;&#28857;&#65292;&#23427;&#20801;&#35768;&#25105;&#20204;&#65306;&#65288;i&#65289;&#25552;&#20986;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#26469;&#27604;&#36739;&#19981;&#21516;&#30340;&#27010;&#24565;&#25552;&#21462;&#26041;&#27861;&#65307;&#65288;ii&#65289;&#21033;&#29992;&#29616;&#20195;&#24402;&#22240;&#26041;&#27861;&#21644;&#35780;&#20272;&#25351;&#26631;&#26469;&#25193;&#23637;&#21644;&#31995;&#32479;&#22320;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#27010;&#24565;&#30340;&#26041;&#27861;&#21644;&#37325;&#35201;&#24615;&#35780;&#20272;&#25216;&#26415;&#65307;&#65288;iii&#65289;&#25512;&#23548;&#20851;&#20110;&#36825;&#31181;&#26041;&#27861;&#30340;&#26368;&#20248;&#24615;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, concept-based approaches have emerged as some of the most promising explainability methods to help us interpret the decisions of Artificial Neural Networks (ANNs). These methods seek to discover intelligible visual 'concepts' buried within the complex patterns of ANN activations in two key steps: (1) concept extraction followed by (2) importance estimation. While these two steps are shared across methods, they all differ in their specific implementations. Here, we introduce a unifying theoretical framework that comprehensively defines and clarifies these two steps. This framework offers several advantages as it allows us: (i) to propose new evaluation metrics for comparing different concept extraction approaches; (ii) to leverage modern attribution methods and evaluation metrics to extend and systematically evaluate state-of-the-art concept-based approaches and importance estimation techniques; (iii) to derive theoretical guarantees regarding the optimality of such met
&lt;/p&gt;</description></item><item><title>&#38899;&#39057;&#35782;&#21035;&#38169;&#35823;&#23545;&#25945;&#23398;&#20195;&#29702;&#20154;&#23398;&#20064;&#21644;&#20154;&#38469;&#20851;&#31995;&#26080;&#26174;&#33879;&#24433;&#21709;&#65292;&#32467;&#26524;&#20026;&#25945;&#23398;&#20195;&#29702;&#20154;&#30340;&#26368;&#20248;&#38169;&#35823;&#24674;&#22797;&#31574;&#30053;&#25552;&#20379;&#20102;&#19968;&#20123;&#21551;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.07302</link><description>&lt;p&gt;
&#25945;&#23398;&#20195;&#29702;&#20154;&#30340;&#24212;&#65288;&#35782;&#21035;&#65289;&#31572;&#38169;&#35823;&#23545;&#23398;&#20064;&#21644;&#20154;&#38469;&#20851;&#31995;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of Experiencing Misrecognition by Teachable Agents on Learning and Rapport. (arXiv:2306.07302v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07302
&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#35782;&#21035;&#38169;&#35823;&#23545;&#25945;&#23398;&#20195;&#29702;&#20154;&#23398;&#20064;&#21644;&#20154;&#38469;&#20851;&#31995;&#26080;&#26174;&#33879;&#24433;&#21709;&#65292;&#32467;&#26524;&#20026;&#25945;&#23398;&#20195;&#29702;&#20154;&#30340;&#26368;&#20248;&#38169;&#35823;&#24674;&#22797;&#31574;&#30053;&#25552;&#20379;&#20102;&#19968;&#20123;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20351;&#29992;&#35821;&#38899;&#30340;&#25945;&#23398;&#20195;&#29702;&#20154;&#30456;&#27604;&#20110;&#22522;&#20110;&#25171;&#23383;&#30340;&#20195;&#29702;&#20154;&#20855;&#26377;&#19968;&#20123;&#20248;&#21183;&#65292;&#20294;&#23427;&#20204;&#23481;&#26131;&#20986;&#29616;&#38899;&#39057;&#35782;&#21035;&#38169;&#35823;&#12290;&#36825;&#20123;&#38169;&#35823;&#21487;&#33021;&#20250;&#25193;&#25955;&#65292;&#23548;&#33268;&#23545;&#35805;&#27969;&#31243;&#30340;&#24847;&#22806;&#21464;&#21270;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#21464;&#21270;&#19982;&#23398;&#20064;&#25910;&#30410;&#20197;&#21450;&#23398;&#20064;&#32773;&#19982;&#20195;&#29702;&#20154;&#20043;&#38388;&#30340;&#20154;&#38469;&#20851;&#31995;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#21464;&#21270;&#26080;&#35770;&#20195;&#29702;&#20154;&#22312;&#19981;&#20135;&#29983;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#24212;&#35813;&#32473;&#20104;&#20309;&#31181;&#22238;&#24212;&#65292;&#37117;&#19982;&#23398;&#20064;&#25910;&#30410;&#25110;&#20154;&#38469;&#20851;&#31995;&#26080;&#20851;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#21487;&#20174;&#36825;&#20123;&#21457;&#29616;&#20013;&#25512;&#20986;&#30340;&#36866;&#24403;&#38169;&#35823;&#24674;&#22797;&#31574;&#30053;&#23545;&#25945;&#23398;&#20195;&#29702;&#20154;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
While speech-enabled teachable agents have some advantages over typing-based ones, they are vulnerable to errors stemming from misrecognition by automatic speech recognition (ASR). These errors may propagate, resulting in unexpected changes in the flow of conversation. We analyzed how such changes are linked with learning gains and learners' rapport with the agents. Our results show they are not related to learning gains or rapport, regardless of the types of responses the agents should have returned given the correct input from learners without ASR errors. We also discuss the implications for optimal error-recovery policies for teachable agents that can be drawn from these findings.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;&#65292;&#31216;&#20026;DR-LSSV&#65292;&#22522;&#20110;&#22238;&#24402;&#21644;&#26368;&#23567;&#20108;&#20056;&#25903;&#25345;&#21521;&#37327;&#26426;&#65292;&#21487;&#26377;&#25928;&#25552;&#39640;&#31354;&#27668;&#27745;&#26579;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#22312;&#31354;&#27668;&#27745;&#26579;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#31354;&#27668;&#27745;&#26579;&#39044;&#27979;&#26102;&#38388;&#21644;&#20551;&#38451;&#24615;&#29575;&#26041;&#38754;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.07301</link><description>&lt;p&gt;
&#22522;&#20110;&#22238;&#24402;&#21644;&#26368;&#23567;&#20108;&#20056;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#31354;&#27668;&#27745;&#26579;&#39044;&#27979;&#30340;&#26032;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Novel Regression and Least Square Support Vector Machine Learning Technique for Air Pollution Forecasting. (arXiv:2306.07301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07301
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;&#65292;&#31216;&#20026;DR-LSSV&#65292;&#22522;&#20110;&#22238;&#24402;&#21644;&#26368;&#23567;&#20108;&#20056;&#25903;&#25345;&#21521;&#37327;&#26426;&#65292;&#21487;&#26377;&#25928;&#25552;&#39640;&#31354;&#27668;&#27745;&#26579;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#22312;&#31354;&#27668;&#27745;&#26579;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#31354;&#27668;&#27745;&#26579;&#39044;&#27979;&#26102;&#38388;&#21644;&#20551;&#38451;&#24615;&#29575;&#26041;&#38754;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#27668;&#27745;&#26579;&#26159;&#26469;&#28304;&#20110;&#24494;&#31890;&#12289;&#21270;&#23398;&#29289;&#36136;&#25110;&#29983;&#29289;&#29289;&#36136;&#30340;&#38382;&#39064;&#65292;&#23427;&#20250;&#23545;&#20154;&#31867;&#25110;&#20854;&#20182;&#29983;&#29289;&#24102;&#26469;&#30171;&#33510;&#65292;&#23545;&#33258;&#28982;&#26646;&#24687;&#22320;&#21644;&#31354;&#27668;&#31354;&#38388;&#20063;&#20250;&#36896;&#25104;&#19981;&#36866;&#12290;&#22240;&#27492;&#65292;&#31354;&#27668;&#27745;&#26579;&#22312;&#22823;&#37117;&#24066;&#20013;&#20173;&#28982;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#29615;&#22659;&#38382;&#39064;&#12290;&#33509;&#19981;&#27491;&#30830;&#22320;&#26816;&#27979;&#31354;&#27668;&#27745;&#26579;&#26631;&#20934;&#65292;&#21017;&#20250;&#23545;&#20154;&#31867;&#21644;&#29983;&#29289;&#36896;&#25104;&#20005;&#37325;&#30340;&#21518;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;&#8212;&#8212;&#31163;&#25955;&#21270;&#22238;&#24402;&#21644;&#26368;&#23567;&#20108;&#20056;&#25903;&#25345;&#21521;&#37327;&#65288;DR-LSSV&#65289;&#26469;&#39044;&#27979;&#31354;&#27668;&#27745;&#26579;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;DR-LSSV&#25216;&#26415;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#31354;&#27668;&#27745;&#26579;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#22312;&#31354;&#27668;&#27745;&#26579;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#31354;&#27668;&#27745;&#26579;&#39044;&#27979;&#26102;&#38388;&#21644;&#20551;&#38451;&#24615;&#29575;&#26041;&#38754;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Air pollution is the origination of particulate matter, chemicals, or biological substances that brings pain to either humans or other living creatures or instigates discomfort to the natural habitat and the airspace. Hence, air pollution remains one of the paramount environmental issues as far as metropolitan cities are concerned. Several air pollution benchmarks are even said to have a negative influence on human health. Also, improper detection of air pollution benchmarks results in severe complications for humans and living creatures. To address this aspect, a novel technique called, Discretized Regression and Least Square Support Vector (DR-LSSV) based air pollution forecasting is proposed. The results indicate that the proposed DR-LSSV Technique can efficiently enhance air pollution forecasting performance and outperforms the conventional machine learning methods in terms of air pollution forecasting accuracy, air pollution forecasting time, and false positive rate.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#20851;&#27880;&#25216;&#26415;&#65292;&#33021;&#22815;&#24179;&#31561;&#20851;&#27880;&#27599;&#20010;&#30382;&#32932;&#30149;&#21464;&#31867;&#21035;&#65292;&#24182;&#36880;&#27493;&#32467;&#21512;&#22810;&#20010;&#23610;&#24230;&#30340;&#21028;&#21035;&#29305;&#24449;&#32454;&#33410;&#65292;&#20174;&#32780;&#33719;&#24471;&#26174;&#33879;&#30340;&#35786;&#26029;&#34920;&#29616;&#65292;&#20934;&#30830;&#29575;&#20998;&#21035;&#20026;97.40&#65285;&#21644;94.9&#65285;&#65292;&#36229;&#36807;&#20102;15&#31181;&#23574;&#31471;&#26041;&#27861;&#65292;&#21253;&#25324;HAM1000&#21644;ISIC2019&#25490;&#34892;&#27036;&#30340;&#20248;&#32988;&#32773;&#12290;</title><link>http://arxiv.org/abs/2306.07300</link><description>&lt;p&gt;
&#35786;&#26029;&#30382;&#32932;&#30149;&#21464;&#30340;&#28176;&#36827;&#24335;&#20998;&#31867;&#20851;&#27880;&#65288;PCA&#65289;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Progressive Class-Wise Attention (PCA) Approach for Diagnosing Skin Lesions. (arXiv:2306.07300v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#20851;&#27880;&#25216;&#26415;&#65292;&#33021;&#22815;&#24179;&#31561;&#20851;&#27880;&#27599;&#20010;&#30382;&#32932;&#30149;&#21464;&#31867;&#21035;&#65292;&#24182;&#36880;&#27493;&#32467;&#21512;&#22810;&#20010;&#23610;&#24230;&#30340;&#21028;&#21035;&#29305;&#24449;&#32454;&#33410;&#65292;&#20174;&#32780;&#33719;&#24471;&#26174;&#33879;&#30340;&#35786;&#26029;&#34920;&#29616;&#65292;&#20934;&#30830;&#29575;&#20998;&#21035;&#20026;97.40&#65285;&#21644;94.9&#65285;&#65292;&#36229;&#36807;&#20102;15&#31181;&#23574;&#31471;&#26041;&#27861;&#65292;&#21253;&#25324;HAM1000&#21644;ISIC2019&#25490;&#34892;&#27036;&#30340;&#20248;&#32988;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#30284;&#30151;&#21457;&#30149;&#29575;&#20013;&#65292;&#30382;&#32932;&#30284;&#30340;&#21457;&#29983;&#29575;&#26368;&#39640;&#12290;&#26089;&#26399;&#21457;&#29616;&#30340;&#37325;&#35201;&#24615;&#19981;&#35328;&#32780;&#21947;&#65292;&#26202;&#26399;&#30149;&#20363;&#21487;&#33021;&#20250;&#21361;&#21450;&#29983;&#21629;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33394;&#24425;&#12289;&#24418;&#29366;&#21644;&#22823;&#23567;&#31561;&#22810;&#31181;&#21464;&#21270;&#65292;&#30382;&#32932;&#30149;&#21464;&#30340;&#20998;&#31867;&#23384;&#22312;&#22810;&#20010;&#25361;&#25112;&#65292;&#21516;&#19968;&#31867;&#21035;&#20869;&#23384;&#22312;&#26174;&#33879;&#21464;&#24322;&#65292;&#19981;&#21516;&#31867;&#21035;&#20043;&#38388;&#20063;&#23384;&#22312;&#26174;&#33879;&#30456;&#20284;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#20998;&#31867;&#20851;&#27880;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#22312;&#25366;&#25496;&#30382;&#32932;&#30149;&#21464;&#30340;&#26356;&#20855;&#20307;&#32454;&#33410;&#30340;&#21516;&#26102;&#65292;&#24179;&#31561;&#22320;&#32771;&#34385;&#27599;&#20010;&#31867;&#21035;&#12290;&#36825;&#31181;&#20851;&#27880;&#26426;&#21046;&#36880;&#27493;&#29992;&#20110;&#32467;&#21512;&#22810;&#20010;&#23610;&#24230;&#30340;&#21028;&#21035;&#29305;&#24449;&#32454;&#33410;&#12290;&#26412;&#25991;&#20171;&#32461;&#30340;&#25216;&#26415;&#34920;&#29616;&#20986;&#33394;&#65292;&#19981;&#20165;&#36229;&#36807;&#20102;15&#31181;&#23574;&#31471;&#26041;&#27861;&#65292;&#36824;&#21253;&#25324;HAM1000&#21644;ISIC 2019&#25490;&#34892;&#27036;&#30340;&#20248;&#32988;&#32773;&#12290;&#22312;HAM10000&#25968;&#25454;&#38598;&#19978;&#65292;&#20854;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;97.40&#65285;&#65292;&#32780;&#22312;ISIC 2019&#25968;&#25454;&#38598;&#19978;&#21017;&#20026;94.9&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Skin cancer holds the highest incidence rate among all cancers globally. The importance of early detection cannot be overstated, as late-stage cases can be lethal. Classifying skin lesions, however, presents several challenges due to the many variations they can exhibit, such as differences in colour, shape, and size, significant variation within the same class, and notable similarities between different classes. This paper introduces a novel class-wise attention technique that equally regards each class while unearthing more specific details about skin lesions. This attention mechanism is progressively used to amalgamate discriminative feature details from multiple scales. The introduced technique demonstrated impressive performance, surpassing more than 15 cutting-edge methods including the winners of HAM1000 and ISIC 2019 leaderboards. It achieved an impressive accuracy rate of 97.40% on the HAM10000 dataset and 94.9% on the ISIC 2019 dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20307;&#39564;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#35821;&#38899;&#21161;&#25163;&#24341;&#29992;&#20182;&#20204;&#25163;&#26426;&#23631;&#24149;&#19978;&#30340;&#30005;&#35805;&#21495;&#30721;&#12289;&#22320;&#22336;&#12289;&#30005;&#23376;&#37038;&#20214;&#22320;&#22336;&#12289;URL&#21644;&#26085;&#26399;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#27169;&#22411;&#65292;&#24182;&#25910;&#38598;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#12290;&#35813;&#27169;&#22411;&#26159;&#27169;&#22359;&#21270;&#30340;&#65292;&#25552;&#20379;&#20102;&#28789;&#27963;&#24615;&#12289;&#25913;&#36827;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#39640;&#25928;&#30340;&#36816;&#34892;&#26102;&#20869;&#23384;&#21033;&#29992;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.07298</link><description>&lt;p&gt;
&#29992;&#35821;&#38899;&#21161;&#25163;&#25351;&#24341;&#23631;&#24149;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Referring to Screen Texts with Voice Assistants. (arXiv:2306.07298v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20307;&#39564;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#35821;&#38899;&#21161;&#25163;&#24341;&#29992;&#20182;&#20204;&#25163;&#26426;&#23631;&#24149;&#19978;&#30340;&#30005;&#35805;&#21495;&#30721;&#12289;&#22320;&#22336;&#12289;&#30005;&#23376;&#37038;&#20214;&#22320;&#22336;&#12289;URL&#21644;&#26085;&#26399;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#27169;&#22411;&#65292;&#24182;&#25910;&#38598;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#12290;&#35813;&#27169;&#22411;&#26159;&#27169;&#22359;&#21270;&#30340;&#65292;&#25552;&#20379;&#20102;&#28789;&#27963;&#24615;&#12289;&#25913;&#36827;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#39640;&#25928;&#30340;&#36816;&#34892;&#26102;&#20869;&#23384;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#21161;&#25163;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#25171;&#30005;&#35805;&#12289;&#21457;&#36865;&#28040;&#24687;&#12289;&#21019;&#24314;&#20107;&#20214;&#12289;&#23548;&#33322;&#31561;&#31561;&#65292;&#20294;&#21161;&#25163;&#33021;&#22815;&#29702;&#35299;&#29992;&#25143;&#30340;&#32972;&#26223;&#26159;&#26377;&#38480;&#30340;&#12290;&#26412;&#25991;&#26088;&#22312;&#21521;&#36825;&#20010;&#26041;&#21521;&#36808;&#36827;&#19968;&#27493;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#30340;&#20307;&#39564;&#65292;&#35753;&#29992;&#25143;&#21487;&#20197;&#25351;&#20986;&#20182;&#20204;&#25163;&#26426;&#23631;&#24149;&#19978;&#30340;&#30005;&#35805;&#21495;&#30721;&#12289;&#22320;&#22336;&#12289;&#30005;&#23376;&#37038;&#20214;&#22320;&#22336;&#12289;URL&#21644;&#26085;&#26399;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#22312;&#20110;&#21442;&#32771;&#29702;&#35299;&#65292;&#24403;&#23631;&#24149;&#19978;&#23384;&#22312;&#22810;&#20010;&#30456;&#20284;&#30340;&#25991;&#26412;&#26102;&#65292;&#31867;&#20284;&#20110;&#35270;&#35273;&#22522;&#30784;&#30340;&#24773;&#20917;&#65292;&#36825;&#21464;&#24471;&#29305;&#21035;&#26377;&#36259;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#20026;&#36825;&#31181;&#26032;&#39062;&#20307;&#39564;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#36890;&#29992;&#27169;&#22411;&#12290;&#30001;&#20110;&#30452;&#25509;&#28040;&#32791;&#20687;&#32032;&#25104;&#26412;&#39640;&#26114;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#35774;&#35745;&#20381;&#36182;&#20110;&#20174;UI&#20013;&#25552;&#21462;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;&#27169;&#22359;&#21270;&#30340;&#65292;&#22240;&#27492;&#25552;&#20379;&#20102;&#28789;&#27963;&#24615;&#12289;&#25913;&#36827;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#39640;&#25928;&#30340;&#36816;&#34892;&#26102;&#20869;&#23384;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Voice assistants help users make phone calls, send messages, create events, navigate, and do a lot more. However, assistants have limited capacity to understand their users' context. In this work, we aim to take a step in this direction. Our work dives into a new experience for users to refer to phone numbers, addresses, email addresses, URLs, and dates on their phone screens. Our focus lies in reference understanding, which becomes particularly interesting when multiple similar texts are present on screen, similar to visual grounding. We collect a dataset and propose a lightweight general-purpose model for this novel experience. Due to the high cost of consuming pixels directly, our system is designed to rely on the extracted text from the UI. Our model is modular, thus offering flexibility, improved interpretability, and efficient runtime memory utilization.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;ChatGPT&#36827;&#34892;&#25968;&#25454;&#22686;&#24191;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;BERT&#27169;&#22411;&#22312;&#30005;&#23376;&#30149;&#21382;&#33647;&#29289;&#35782;&#21035;&#21644;&#33647;&#29289;&#20107;&#20214;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.07297</link><description>&lt;p&gt;
&#22522;&#20110;ChatGPT&#30340;&#21307;&#30103;&#25968;&#25454;&#22686;&#24191;&#65306;&#22522;&#20110;&#33647;&#29289;&#35782;&#21035;&#21644;&#33647;&#29289;&#20107;&#20214;&#20998;&#31867;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Medical Data Augmentation via ChatGPT: A Case Study on Medication Identification and Medication Event Classification. (arXiv:2306.07297v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;ChatGPT&#36827;&#34892;&#25968;&#25454;&#22686;&#24191;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;BERT&#27169;&#22411;&#22312;&#30005;&#23376;&#30149;&#21382;&#33647;&#29289;&#35782;&#21035;&#21644;&#33647;&#29289;&#20107;&#20214;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#23376;&#30149;&#21382;&#21644;&#20020;&#24202;&#35760;&#24405;&#20013;&#35782;&#21035;&#33647;&#29289;&#12289;&#30142;&#30149;&#21644;&#20851;&#32852;&#24615;&#31561;&#20851;&#38190;&#22240;&#32032;&#20855;&#26377;&#24191;&#27867;&#30340;&#20020;&#24202;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#36827;&#34892;&#25968;&#25454;&#22686;&#24191;&#65292;&#20197;&#20811;&#26381;&#30005;&#23376;&#30149;&#21382;&#20013;&#20851;&#38190;&#22240;&#32032;&#26631;&#27880;&#25968;&#25454;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#20986;&#30340;&#25968;&#25454;&#22686;&#24191;&#25216;&#26415;&#26174;&#33879;&#25552;&#39640;&#20102;BERT&#27169;&#22411;&#22312;&#33647;&#29289;&#35782;&#21035;&#21644;&#33647;&#29289;&#20107;&#20214;&#20998;&#31867;&#31561;&#20004;&#20010;&#30005;&#23376;&#30149;&#21382;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The identification of key factors such as medications, diseases, and relationships within electronic health records and clinical notes has a wide range of applications in the clinical field. In the N2C2 2022 competitions, various tasks were presented to promote the identification of key factors in electronic health records (EHRs) using the Contextualized Medication Event Dataset (CMED). Pretrained large language models (LLMs) demonstrated exceptional performance in these tasks. This study aims to explore the utilization of LLMs, specifically ChatGPT, for data augmentation to overcome the limited availability of annotated data for identifying the key factors in EHRs. Additionally, different pre-trained BERT models, initially trained on extensive datasets like Wikipedia and MIMIC, were employed to develop models for identifying these key variables in EHRs through fine-tuning on augmented datasets. The experimental results of two EHR analysis tasks, namely medication identification and me
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;PSO&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#19977;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#38024;&#23545;&#21271;&#20140;PM2.5&#30340;&#39044;&#27979;&#20219;&#21153;&#36827;&#34892;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;M-1&#27169;&#22411;&#20855;&#26377;&#26368;&#20339;&#24615;&#33021;&#19988;&#32463;&#36807;&#20248;&#21270;&#30340;&#36229;&#21442;&#25968;&#21487;&#20197;&#25552;&#39640;5.6%&#30340;&#39044;&#27979;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.07296</link><description>&lt;p&gt;
&#22522;&#20110;PSO&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#19977;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21271;&#20140;PM2.5&#39044;&#27979;&#26041;&#38754;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Optimized Three Deep Learning Models Based-PSO Hyperparameters for Beijing PM2.5 Prediction. (arXiv:2306.07296v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;PSO&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#19977;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#38024;&#23545;&#21271;&#20140;PM2.5&#30340;&#39044;&#27979;&#20219;&#21153;&#36827;&#34892;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;M-1&#27169;&#22411;&#20855;&#26377;&#26368;&#20339;&#24615;&#33021;&#19988;&#32463;&#36807;&#20248;&#21270;&#30340;&#36229;&#21442;&#25968;&#21487;&#20197;&#25552;&#39640;5.6%&#30340;&#39044;&#27979;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#22270;&#20687;&#35782;&#21035;&#21644;&#39044;&#27979;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#37117;&#26377;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#30340;&#24615;&#33021;&#21462;&#20915;&#20110;&#36229;&#21442;&#25968;&#35774;&#32622;&#65292;&#26412;&#30740;&#31350;&#23581;&#35797;&#20248;&#21270;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#20351;&#29992;&#22522;&#20110;&#32676;&#26234;&#33021;&#30340;&#20803;&#21551;&#21457;&#24335;&#20248;&#21270;&#26041;&#27861;&#31890;&#23376;&#32676;&#31639;&#27861;&#65288;PSO&#65289;&#36827;&#34892;&#39044;&#27979;&#20219;&#21153;&#30340;&#20248;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;M-1&#65288;PSO-LSTM&#65289;&#12289;M-2&#65288;PSO-CNN&#65289;&#21644;M-3&#65288;PSO-MLP&#65289;&#12290;&#23545;&#21271;&#20140;PM2.5&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#27979;&#37327;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;PM2.5&#20316;&#20026;&#30446;&#26631;&#21464;&#37327;&#65292;&#21463;&#21040;&#38706;&#28857;&#12289;&#27668;&#21387;&#12289;&#28201;&#24230;&#12289;&#32047;&#35745;&#39118;&#36895;&#12289;&#38477;&#38634;&#23567;&#26102;&#25968;&#21644;&#38477;&#38632;&#23567;&#26102;&#25968;&#30340;&#24433;&#21709;&#12290;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#36755;&#20837;&#20998;&#20026;&#19977;&#31181;&#19981;&#21516;&#30340;&#24773;&#26223;&#65306;&#26085;&#24120;&#12289;&#27599;&#21608;&#21644;&#27599;&#26376;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#20986;&#30340;M-1&#20855;&#26377;&#19977;&#20010;&#38544;&#34255;&#23618;&#65292;&#25552;&#20379;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#19988;&#32463;&#36807;&#20248;&#21270;&#30340;&#36229;&#21442;&#25968;&#30456;&#23545;&#20110;&#22522;&#32447;&#27169;&#22411;&#25552;&#39640;&#20102;5.6%&#30340;&#39044;&#27979;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning is a machine learning approach that produces excellent performance in various applications, including natural language processing, image identification, and forecasting. Deep learning network performance depends on the hyperparameter settings. This research attempts to optimize the deep learning architecture of Long short term memory (LSTM), Convolutional neural network (CNN), and Multilayer perceptron (MLP) for forecasting tasks using Particle swarm optimization (PSO), a swarm intelligence-based metaheuristic optimization methodology: Proposed M-1 (PSO-LSTM), M-2 (PSO-CNN), and M-3 (PSO-MLP). Beijing PM2.5 datasets was analyzed to measure the performance of the proposed models. PM2.5 as a target variable was affected by dew point, pressure, temperature, cumulated wind speed, hours of snow, and hours of rain. The deep learning network inputs consist of three different scenarios: daily, weekly, and monthly. The results show that the proposed M-1 with three hidden layers pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20108;&#27425;&#31070;&#32463;&#20803;&#30340;&#39640;&#25928;CNN&#32467;&#26500;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#32593;&#32476;&#24615;&#33021;&#65292;&#32780;&#19988;&#21442;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#37117;&#26377;&#22823;&#24133;&#24230;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2306.07294</link><description>&lt;p&gt;
&#22522;&#20110;&#20108;&#27425;&#31070;&#32463;&#20803;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#29616;&#21147;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Expressivity Enhancement with Efficient Quadratic Neurons for Convolutional Neural Networks. (arXiv:2306.07294v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20108;&#27425;&#31070;&#32463;&#20803;&#30340;&#39640;&#25928;CNN&#32467;&#26500;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#32593;&#32476;&#24615;&#33021;&#65292;&#32780;&#19988;&#21442;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#37117;&#26377;&#22823;&#24133;&#24230;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#25104;&#21151;&#24212;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#21644;&#30446;&#26631;&#20998;&#21106;&#31561;&#39046;&#22495;&#12290;&#20026;&#20102;&#25552;&#39640;&#32593;&#32476;&#30340;&#34920;&#29616;&#21147;&#65292;&#20154;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#25216;&#26415;&#65292;&#22914;&#26032;&#30340;CNN&#26550;&#26500;&#12290;&#20294;&#26159;&#65292;&#26469;&#33258;&#36825;&#20123;&#25216;&#26415;&#30340;&#24615;&#33021;&#25552;&#21319;&#24448;&#24448;&#20250;&#20943;&#24369;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#25226;&#37325;&#28857;&#36716;&#21521;&#22686;&#21152;&#31070;&#32463;&#20803;&#30340;&#38750;&#32447;&#24615;&#65292;&#20197;&#22686;&#24378;&#32593;&#32476;&#34920;&#29616;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#20250;&#24102;&#26469;&#22823;&#37327;&#30340;&#21442;&#25968;&#65292;&#22240;&#27492;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#23548;&#33268;&#21487;&#37096;&#32626;&#24615;&#26041;&#38754;&#30340;&#20302;&#25928;&#29575;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20108;&#27425;&#31070;&#32463;&#20803;&#32467;&#26500;&#65292;&#20197;&#20165;&#26377;&#24494;&#23567;&#21442;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#24320;&#38144;&#26469;&#20445;&#30041;&#38750;&#32447;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#20108;&#27425;&#31070;&#32463;&#20803;&#21487;&#20197;&#26368;&#22823;&#21270;&#21033;&#29992;&#20108;&#38454;&#35745;&#31639;&#20449;&#24687;&#26469;&#25913;&#21892;&#32593;&#32476;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#20108;&#27425;&#31070;&#32463;&#20803;&#32467;&#26500;&#21487;&#20197;&#23454;&#29616;&#19982;&#29616;&#26377;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30456;&#27604;&#20855;&#26377;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks (CNNs) have been successfully applied in a range of fields such as image classification and object segmentation. To improve their expressivity, various techniques, such as novel CNN architectures, have been explored. However, the performance gain from such techniques tends to diminish. To address this challenge, many researchers have shifted their focus to increasing the non-linearity of neurons, the fundamental building blocks of neural networks, to enhance the network expressivity. Nevertheless, most of these approaches incur a large number of parameters and thus formidable computation cost inevitably, impairing their efficiency to be deployed in practice. In this work, an efficient quadratic neuron structure is proposed to preserve the non-linearity with only negligible parameter and computation cost overhead. The proposed quadratic neuron can maximize the utilization of second-order computation information to improve the network performance. The experi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22478;&#24066;&#26102;&#31354;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20998;&#35299;&#31895;&#31961;&#30340;&#20302;&#20998;&#36776;&#29575;&#22320;&#29702;&#21333;&#20803;&#30340;&#32858;&#21512;&#22478;&#24066;&#25968;&#25454;&#26469;&#21512;&#25104;&#32454;&#31890;&#24230;&#65292;&#39640;&#20998;&#36776;&#29575;&#30340;&#22478;&#24066;&#25968;&#25454;&#65292;&#20197;&#22686;&#21152;&#39640;&#24230;&#32858;&#21512;&#30340;&#22478;&#24066;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#21644;&#23454;&#29616;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.07292</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22478;&#24066;&#26102;&#31354;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Urban Spatiotemporal Data Synthesis via Neural Disaggregation. (arXiv:2306.07292v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22478;&#24066;&#26102;&#31354;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20998;&#35299;&#31895;&#31961;&#30340;&#20302;&#20998;&#36776;&#29575;&#22320;&#29702;&#21333;&#20803;&#30340;&#32858;&#21512;&#22478;&#24066;&#25968;&#25454;&#26469;&#21512;&#25104;&#32454;&#31890;&#24230;&#65292;&#39640;&#20998;&#36776;&#29575;&#30340;&#22478;&#24066;&#25968;&#25454;&#65292;&#20197;&#22686;&#21152;&#39640;&#24230;&#32858;&#21512;&#30340;&#22478;&#24066;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#21644;&#23454;&#29616;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#25968;&#25454;&#30340;&#32454;&#33410;&#32423;&#21035;&#24120;&#24120;&#19982;&#20854;&#25152;&#33021;&#25552;&#20379;&#30340;&#23454;&#38469;&#25928;&#30410;&#21457;&#29983;&#20914;&#31361;&#12290;&#36739;&#19981;&#32454;&#21270;&#30340;&#25968;&#25454;&#21487;&#20197;&#20445;&#25252;&#20010;&#20154;&#38544;&#31169;&#65292;&#20294;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#29306;&#29298;&#20102;&#24320;&#25918;&#25968;&#25454;&#20419;&#36827;&#36879;&#26126;&#24230;&#21644;&#21327;&#21161;&#30740;&#31350;&#30340;&#25215;&#35834;&#12290;&#31867;&#20284;&#20110;&#22478;&#24066;&#29615;&#22659;&#20013;&#65292;&#39640;&#23618;&#27425;&#22320;&#29702;&#21333;&#20803;&#30340;&#32858;&#21512;&#22478;&#24066;&#25968;&#25454;&#21487;&#33021;&#20250;&#25513;&#30422;&#22478;&#24066;&#21160;&#24577;&#30340;&#24213;&#23618;&#29305;&#24449;&#65292;&#20302;&#32423;&#21035;&#22320;&#29702;&#21333;&#20803;&#30340;&#21464;&#21270;&#21487;&#33021;&#26356;&#20026;&#26126;&#26174;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20998;&#35299;&#31895;&#31961;&#30340;&#20302;&#20998;&#36776;&#29575;&#22320;&#29702;&#21333;&#20803;&#30340;&#32858;&#21512;&#22478;&#24066;&#25968;&#25454;&#65292;&#21512;&#25104;&#32454;&#31890;&#24230;&#65292;&#39640;&#20998;&#36776;&#29575;&#30340;&#22478;&#24066;&#25968;&#25454;&#65292;&#20197;&#22686;&#21152;&#39640;&#24230;&#32858;&#21512;&#30340;&#22478;&#24066;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#21644;&#23454;&#29616;&#20215;&#20540;&#12290;&#20026;&#20102;&#35299;&#20915;&#19968;&#20123;&#20256;&#32479;&#20998;&#35299;&#26041;&#27861;&#30340;&#31616;&#21333;&#24615;&#38382;&#39064;-1) &#25105;&#20204;&#23581;&#35797;&#20102;&#35768;&#22810;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#24314;&#27169;&#29305;&#24449;&#20043;&#38388;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#31070;&#32463;&#26041;&#27861;&#20063;&#21487;&#20197;&#21516;&#26102;&#21033;&#29992;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The level of granularity of open data often conflicts the benefits it can provide. Less granular data can protect individual privacy, but to certain degrees, sabotage the promise of open data to promote transparency and assist research. Similar in the urban setting, aggregated urban data at high-level geographic units can mask out the underline particularities of city dynamics that may vary at lower areal levels. In this work, we aim to synthesize fine-grained, high resolution urban data, by breaking down aggregated urban data at coarse, low resolution geographic units. The goal is to increase the usability and realize the values as much as possible of highly aggregated urban data. To address the issue of simplicity of some traditional disaggregation methods -- 1) we experimented with numerous neural-based models that are capable of modeling intricate non-linear relationships among features. Neural methods can also leverage both spatial and temporal information concurrently. We showed 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#20540;&#20989;&#25968;&#20272;&#35745;&#25511;&#21046;&#26041;&#27861;&#65288;DVF&#65289;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#22823;&#37327;&#26465;&#20214;&#21270;&#25968;&#25454;&#19978;&#26377;&#25928;&#22320;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#21482;&#26377;&#19968;&#23567;&#37096;&#20998;&#31034;&#33539;&#25968;&#25454;&#23601;&#33021;&#36229;&#36807;&#22522;&#20934;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.07290</link><description>&lt;p&gt;
&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#20540;&#20989;&#25968;&#20272;&#35745;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Value function estimation using conditional diffusion models for control. (arXiv:2306.07290v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07290
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#20540;&#20989;&#25968;&#20272;&#35745;&#25511;&#21046;&#26041;&#27861;&#65288;DVF&#65289;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#22823;&#37327;&#26465;&#20214;&#21270;&#25968;&#25454;&#19978;&#26377;&#25928;&#22320;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#21482;&#26377;&#19968;&#23567;&#37096;&#20998;&#31034;&#33539;&#25968;&#25454;&#23601;&#33021;&#36229;&#36807;&#22522;&#20934;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#20010;&#21487;&#38752;&#36235;&#21183;&#26159;&#24615;&#33021;&#38543;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#25552;&#39640;&#65292;&#21069;&#25552;&#26159;&#26377;&#20805;&#36275;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22823;&#22411;&#27169;&#22411;&#30340;&#38656;&#27714;&#22686;&#21152;&#65292;&#24456;&#21487;&#33021;&#20250;&#20986;&#29616;&#39640;&#36136;&#37327;&#31034;&#33539;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25193;&#25955;&#20540;&#20989;&#25968;&#30340;&#31616;&#21333;&#31639;&#27861;(DVF)&#65292;&#23427;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#23398;&#20064;&#29615;&#22659;&#21644;&#26426;&#22120;&#20154;&#20132;&#20114;&#21160;&#24577;&#30340;&#32852;&#21512;&#22810;&#27493;&#27169;&#22411;&#65292;&#24182;&#20272;&#35745;&#25152;&#38656;&#20219;&#21153;&#30340;&#20540;&#20989;&#25968;&#12290;&#22312;&#27169;&#25311;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#26174;&#31034;&#23427;&#21482;&#38656;&#35201;&#20351;&#29992;&#19968;&#23567;&#37096;&#20998;&#31034;&#33539;&#25968;&#25454;&#23601;&#21487;&#20197;&#32988;&#36807;&#22522;&#20934;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fairly reliable trend in deep reinforcement learning is that the performance scales with the number of parameters, provided a complimentary scaling in amount of training data. As the appetite for large models increases, it is imperative to address, sooner than later, the potential problem of running out of high-quality demonstrations. In this case, instead of collecting only new data via costly human demonstrations or risking a simulation-to-real transfer with uncertain effects, it would be beneficial to leverage vast amounts of readily-available low-quality data. Since classical control algorithms such as behavior cloning or temporal difference learning cannot be used on reward-free or action-free data out-of-the-box, this solution warrants novel training paradigms for continuous control. We propose a simple algorithm called Diffused Value Function (DVF), which learns a joint multi-step model of the environment-robot interaction dynamics using a diffusion model. This model can be ef
&lt;/p&gt;</description></item><item><title>TransCoder&#26159;&#19968;&#31181;&#21487;&#36716;&#31227;&#30340;&#20195;&#30721;&#34920;&#31034;&#23398;&#20064;&#20219;&#21153;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#36890;&#36807;&#21487;&#35843;&#25972;&#30340;&#21069;&#32512;&#32534;&#30721;&#22120;&#20316;&#20026;&#20803;&#23398;&#20064;&#22120;&#26469;&#25429;&#25417;&#36328;&#20219;&#21153;&#21644;&#36328;&#35821;&#35328;&#30340;&#21487;&#36716;&#31227;&#30693;&#35782;&#65292;&#20351;&#27169;&#22411;&#23398;&#20064;&#26356;&#22909;&#30340;&#20195;&#30721;&#30456;&#20851;&#20803;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;remarkably&#22320;&#25552;&#39640;&#35757;&#32451;&#26679;&#26412;&#37327;&#36739;&#23567;&#21644;&#35821;&#26009;&#24211;&#36739;&#23567;&#30340;&#20219;&#21153;&#21644;&#35821;&#35328;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.07285</link><description>&lt;p&gt;
TransCoder&#65306;&#21463;&#20154;&#31867;&#25216;&#33021;&#21551;&#21457;&#30340;&#32479;&#19968;&#21487;&#36716;&#31227;&#20195;&#30721;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TransCoder: Towards Unified Transferable Code Representation Learning Inspired by Human Skills. (arXiv:2306.07285v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07285
&lt;/p&gt;
&lt;p&gt;
TransCoder&#26159;&#19968;&#31181;&#21487;&#36716;&#31227;&#30340;&#20195;&#30721;&#34920;&#31034;&#23398;&#20064;&#20219;&#21153;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#36890;&#36807;&#21487;&#35843;&#25972;&#30340;&#21069;&#32512;&#32534;&#30721;&#22120;&#20316;&#20026;&#20803;&#23398;&#20064;&#22120;&#26469;&#25429;&#25417;&#36328;&#20219;&#21153;&#21644;&#36328;&#35821;&#35328;&#30340;&#21487;&#36716;&#31227;&#30693;&#35782;&#65292;&#20351;&#27169;&#22411;&#23398;&#20064;&#26356;&#22909;&#30340;&#20195;&#30721;&#30456;&#20851;&#20803;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;remarkably&#22320;&#25552;&#39640;&#35757;&#32451;&#26679;&#26412;&#37327;&#36739;&#23567;&#21644;&#35821;&#26009;&#24211;&#36739;&#23567;&#30340;&#20219;&#21153;&#21644;&#35821;&#35328;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20195;&#30721;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;CodePTMs&#65289;&#24050;&#32463;&#34920;&#29616;&#20986;&#22312;&#22788;&#29702;&#21508;&#31181;&#36719;&#20214;&#26234;&#33021;&#20219;&#21153;&#26041;&#38754;&#30340;&#25166;&#23454;&#33021;&#21147;&#65292;&#20363;&#22914;&#65292;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#12289;&#20195;&#30721;&#32763;&#35793;&#21644;&#20195;&#30721;&#25688;&#35201;&#12290;&#30446;&#21069;&#23558;&#36825;&#20123;&#27169;&#22411;&#37096;&#32626;&#21040;&#19979;&#28216;&#20219;&#21153;&#30340;&#20027;&#27969;&#26041;&#27861;&#26159;&#22312;&#21333;&#20010;&#20219;&#21153;&#19978;&#23545;&#23427;&#20204;&#36827;&#34892;&#24494;&#35843;&#65292;&#36825;&#36890;&#24120;&#26159;&#26114;&#36149;&#30340;&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#22411;&#27169;&#22411;&#30340;&#20805;&#36275;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TransCoder&#65292;&#36825;&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#21487;&#36716;&#31227;&#30340;&#20195;&#30721;&#34920;&#31034;&#23398;&#20064;&#20219;&#21153;&#30340;&#24494;&#35843;&#31574;&#30053;&#12290;&#21463;&#20154;&#31867;&#20869;&#22312;&#30693;&#35782;&#27867;&#21270;&#25216;&#33021;&#30340;&#21551;&#21457;&#65292;TransCoder&#39537;&#21160;&#27169;&#22411;&#20687;&#20154;&#31867;&#31243;&#24207;&#21592;&#19968;&#26679;&#23398;&#20064;&#26356;&#22909;&#30340;&#20195;&#30721;&#30456;&#20851;&#20803;&#30693;&#35782;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#37319;&#29992;&#21487;&#35843;&#25972;&#30340;&#21069;&#32512;&#32534;&#30721;&#22120;&#20316;&#20026;&#20803;&#23398;&#20064;&#22120;&#65292;&#20998;&#21035;&#25429;&#25417;&#36328;&#20219;&#21153;&#21644;&#36328;&#35821;&#35328;&#30340;&#21487;&#36716;&#31227;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;remarkably&#22320;&#25552;&#39640;&#35757;&#32451;&#26679;&#26412;&#37327;&#36739;&#23567;&#21644;&#35821;&#26009;&#24211;&#36739;&#23567;&#30340;&#20219;&#21153;&#21644;&#35821;&#35328;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code pre-trained models (CodePTMs) have recently demonstrated a solid capacity to process various software intelligence tasks, e.g., code clone detection, code translation, and code summarization. The current mainstream method that deploys these models to downstream tasks is to fine-tune them on individual tasks, which is generally costly and needs sufficient data for large models. To tackle the issue, in this paper, we present TransCoder, a unified Transferable fine-tuning strategy for Code representation learning. Inspired by human inherent skills of knowledge generalization, TransCoder drives the model to learn better code-related meta-knowledge like human programmers. Specifically, we employ a tunable prefix encoder as the meta-learner to capture cross-task and cross-language transferable knowledge, respectively. Besides, tasks with minor training sample sizes and languages with small corpus can be remarkably benefited from our approach. Extensive experiments conducted on benchmark
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#36890;&#36807;&#32447;&#24615;&#20195;&#25968;&#35745;&#31639;&#31283;&#23450;&#27169;&#22411;&#28385;&#36275;&#32473;&#23450;&#30340;&#38480;&#21046;&#26465;&#20214;&#65292;&#21516;&#26102;&#27809;&#26377;&#20351;&#29992;&#31526;&#21495;ASP&#25110;SAT&#27714;&#35299;&#22120;&#65292;&#20026;&#21152;&#36895;&#36890;&#36807;&#24182;&#34892;&#25216;&#26415;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06821</link><description>&lt;p&gt;
&#36808;&#21521;&#31471;&#21040;&#31471;ASP&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Towards end-to-end ASP computation. (arXiv:2306.06821v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06821
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#36890;&#36807;&#32447;&#24615;&#20195;&#25968;&#35745;&#31639;&#31283;&#23450;&#27169;&#22411;&#28385;&#36275;&#32473;&#23450;&#30340;&#38480;&#21046;&#26465;&#20214;&#65292;&#21516;&#26102;&#27809;&#26377;&#20351;&#29992;&#31526;&#21495;ASP&#25110;SAT&#27714;&#35299;&#22120;&#65292;&#20026;&#21152;&#36895;&#36890;&#36807;&#24182;&#34892;&#25216;&#26415;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#29992;&#32447;&#24615;&#20195;&#25968;&#35745;&#31639;&#28385;&#36275;&#32473;&#23450;&#38480;&#21046;&#26465;&#20214;&#30340;&#31283;&#23450;&#27169;&#22411;&#65292;&#20197;&#21450;ASP&#30340;&#35745;&#31639;&#12290;&#19968;&#31181;&#26500;&#36896;&#25104;&#30697;&#38453;&#21270;&#27491;&#24120;&#36923;&#36753;&#31243;&#24207;&#12289;Lin-Zhao&#23450;&#29702;&#20013;&#30340;&#24490;&#29615;&#20844;&#24335;&#21644;&#38480;&#21046;&#26465;&#20214;&#30340;&#20195;&#20215;&#20989;&#25968;&#30340;&#25968;&#20540;&#26368;&#23567;&#21270;&#30340;&#21521;&#37327;&#31354;&#38388;&#30452;&#25509;&#23454;&#29616;Lin-Zhao&#23450;&#29702;&#30340;&#24605;&#36335;&#65292;&#22240;&#27492;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#27809;&#26377;&#20351;&#29992;&#31526;&#21495;ASP&#25110;SAT&#27714;&#35299;&#22120;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#29992;&#20110;&#32553;&#23567;&#31243;&#24207;&#22823;&#23567;&#30340;&#39044;&#35745;&#31639;&#21644;&#29992;&#20110;&#20943;&#23569;&#35745;&#31639;&#38590;&#24230;&#30340;&#24490;&#29615;&#20844;&#24335;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#25105;&#20204;&#29992;&#32534;&#31243;&#31034;&#20363;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#27979;&#35797;&#65292;&#21253;&#25324;&#19977;&#33394;&#28034;&#33394;&#38382;&#39064;&#21644;&#21704;&#23494;&#39039;&#29615;&#38382;&#39064;&#12290;&#30001;&#20110;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#32431;&#31929;&#25968;&#20540;&#26041;&#27861;&#65292;&#24182;&#19988;&#21482;&#21253;&#21547;&#21521;&#37327;/&#30697;&#38453;&#25805;&#20316;&#65292;&#22240;&#27492;&#21487;&#20197;&#36890;&#36807;&#24182;&#34892;&#25216;&#26415;&#65288;&#20363;&#22914;&#22810;&#26680;&#21644;GPU&#65289;&#36827;&#34892;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an end-to-end approach for answer set programming (ASP) and linear algebraically compute stable models satisfying given constraints. The idea is to implement Lin-Zhao's theorem \cite{Lin04} together with constraints directly in vector spaces as numerical minimization of a cost function constructed from a matricized normal logic program, loop formulas in Lin-Zhao's theorem and constraints, thereby no use of symbolic ASP or SAT solvers involved in our approach. We also propose precomputation that shrinks the program size and heuristics for loop formulas to reduce computational difficulty. We empirically test our approach with programming examples including the 3-coloring and Hamiltonian cycle problems. As our approach is purely numerical and only contains vector/matrix operations, acceleration by parallel technologies such as many-cores and GPUs is expected.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#20915;&#31574;&#26641;&#27169;&#22411;&#65292;&#21033;&#29992;&#25346;&#36215;&#30340;&#26641;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;&#20854;&#35299;&#37322;&#24615;&#21644;&#32479;&#35745;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#26080;&#38480;&#28145;&#24230;&#20915;&#31574;&#26641;&#30340;&#27700;&#24179;&#65292;&#24182;&#21487;&#19982;XGBoost&#31561;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2306.06777</link><description>&lt;p&gt;
&#25552;&#39640;&#20915;&#31574;&#26641;&#35299;&#37322;&#24615;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving the Validity of Decision Trees as Explanations. (arXiv:2306.06777v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06777
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#20915;&#31574;&#26641;&#27169;&#22411;&#65292;&#21033;&#29992;&#25346;&#36215;&#30340;&#26641;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;&#20854;&#35299;&#37322;&#24615;&#21644;&#32479;&#35745;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#26080;&#38480;&#28145;&#24230;&#20915;&#31574;&#26641;&#30340;&#27700;&#24179;&#65292;&#24182;&#21487;&#19982;XGBoost&#31561;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#20998;&#31867;&#21644;&#39044;&#27979;&#20013;&#65292;&#20154;&#20204;&#32463;&#24120;&#20351;&#29992;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#12290;&#36825;&#21487;&#20197;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31454;&#20105;[&#21442;&#35265;Grinsztajn&#31561;&#20154;&#65292;NeurIPS 2022&#65292;arXiv&#65306;2207.08815]&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#26159;&#21487;&#35299;&#37322;&#30340;&#12290;&#21487;&#35299;&#37322;&#24615;&#21462;&#20915;&#20110;&#26641;&#30340;&#28145;&#24230;&#21644;&#27599;&#20010;&#21494;&#33410;&#28857;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#20302;&#28145;&#24230;&#30340;&#26641;&#65292;&#20854;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#27599;&#20010;&#21494;&#33410;&#28857;&#19978;&#30340;&#26368;&#22823;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#20174;&#20302;&#28145;&#24230;&#26641;&#30340;&#27599;&#20010;&#21494;&#33410;&#28857;&#8220;&#25346;&#36215;&#8221;&#36827;&#19968;&#27493;&#30340;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#65288;&#20363;&#22914;&#26080;&#38480;&#28145;&#24230;&#30340;&#26641;&#65289;&#12290;&#20302;&#28145;&#24230;&#26641;&#26131;&#20110;&#35299;&#37322;&#65292;&#32780;&#32508;&#21512;&#20302;&#28145;&#24230;&#21644;&#25346;&#36215;&#30340;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#30340;&#25972;&#20307;&#32479;&#35745;&#24615;&#33021;&#20248;&#20110;&#20351;&#29992;&#32463;&#20856;&#26041;&#27861;&#65288;&#20363;&#22914;CART&#65289;&#35757;&#32451;&#30340;&#26080;&#38480;&#28145;&#24230;&#20915;&#31574;&#26641;&#65292;&#24182;&#19988;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65288;&#20363;&#22914;&#20248;&#21270;&#30340;XGBoost&#65289;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
In classification and forecasting with tabular data, one often utilizes tree-based models. This can be competitive with deep neural networks on tabular data [cf. Grinsztajn et al., NeurIPS 2022, arXiv:2207.08815] and, under some conditions, explainable. The explainability depends on the depth of the tree and the accuracy in each leaf of the tree. Here, we train a low-depth tree with the objective of minimising the maximum misclassification error across each leaf node, and then ``suspend'' further tree-based models (e.g., trees of unlimited depth) from each leaf of the low-depth tree. The low-depth tree is easily explainable, while the overall statistical performance of the combined low-depth and suspended tree-based models improves upon decision trees of unlimited depth trained using classical methods (e.g., CART) and is comparable to state-of-the-art methods (e.g., well-tuned XGBoost).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#22823;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;ESG&#38382;&#39064;&#35782;&#21035;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#23545;MSCI ESG&#35780;&#32423;&#25351;&#21335;&#23450;&#20041;&#30340;35&#20010;ESG&#20851;&#38190;&#38382;&#39064;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#35782;&#21035;&#25104;&#26524;&#65292;&#20026;ESG&#20027;&#39064;&#30340;&#25506;&#32034;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2306.06662</link><description>&lt;p&gt;
EaSyGuide&#65306;&#21033;&#29992;&#29983;&#25104;&#22823;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;ESG&#38382;&#39064;&#35782;&#21035;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EaSyGuide : ESG Issue Identification Framework leveraging Abilities of Generative Large Language Models. (arXiv:2306.06662v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#22823;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;ESG&#38382;&#39064;&#35782;&#21035;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#23545;MSCI ESG&#35780;&#32423;&#25351;&#21335;&#23450;&#20041;&#30340;35&#20010;ESG&#20851;&#38190;&#38382;&#39064;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#35782;&#21035;&#25104;&#26524;&#65292;&#20026;ESG&#20027;&#39064;&#30340;&#25506;&#32034;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#21442;&#21152;FinNLP-2023&#22810;&#35821;&#35328;&#29615;&#22659;&#12289;&#31038;&#20250;&#21644;&#20225;&#19994;&#27835;&#29702;&#38382;&#39064;&#35782;&#21035;&#65288;ML-ESG&#65289;&#20849;&#20139;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#35813;&#20219;&#21153;&#30340;&#30446;&#26631;&#26159;&#26681;&#25454;MSCI ESG&#35780;&#32423;&#25351;&#21335;&#23450;&#20041;&#30340;35&#20010;ESG&#20851;&#38190;&#38382;&#39064;&#23545;&#26032;&#38395;&#25991;&#31456;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#38598;&#20013;&#22312;&#33521;&#35821;&#21644;&#27861;&#35821;&#23376;&#20219;&#21153;&#19978;&#65292;&#37319;&#29992;CerebrasGPT&#12289;OPT&#21644;Pythia&#27169;&#22411;&#65292;&#20197;&#21450;&#38646;-shot&#21644;GPT3Mix&#22686;&#24378;&#25216;&#26415;&#12290;&#25105;&#20204;&#21033;&#29992;&#21508;&#31181;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#22914;RoBERTa&#12289;DeBERTa&#21644;FinBERT&#65292;&#22312;&#30693;&#35782;&#33976;&#39311;&#21644;&#39069;&#22806;&#35757;&#32451;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#20102;&#35797;&#39564;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#25104;&#26524;&#65292;&#22312;&#33521;&#35821;&#25991;&#26412;&#23376;&#20219;&#21153;&#20013;&#33719;&#24471;F1-score 0.69&#30340;&#31532;&#19968;&#21517;&#65292;&#22312;&#27861;&#35821;&#25991;&#26412;&#23376;&#20219;&#21153;&#20013;&#33719;&#24471;F1-score 0.78&#30340;&#31532;&#20108;&#21517;&#12290;&#36825;&#20123;&#32467;&#26524;&#24378;&#35843;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#26032;&#38395;&#25991;&#31456;&#20013;&#35782;&#21035;ESG&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#23545;ESG&#20027;&#39064;&#30340;&#25506;&#32034;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#24182;&#24378;&#35843;&#20102;&#25216;&#26415;&#21019;&#26032;&#22312;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#19978;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents our participation in the FinNLP-2023 shared task on multi-lingual environmental, social, and corporate governance issue identification (ML-ESG). The task's objective is to classify news articles based on the 35 ESG key issues defined by the MSCI ESG rating guidelines. Our approach focuses on the English and French subtasks, employing the CerebrasGPT, OPT, and Pythia models, along with the zero-shot and GPT3Mix Augmentation techniques. We utilize various encoder models, such as RoBERTa, DeBERTa, and FinBERT, subjecting them to knowledge distillation and additional training.  Our approach yielded exceptional results, securing the first position in the English text subtask with F1-score 0.69 and the second position in the French text subtask with F1-score 0.78. These outcomes underscore the effectiveness of our methodology in identifying ESG issues in news articles across different languages. Our findings contribute to the exploration of ESG topics and highlight the po
&lt;/p&gt;</description></item><item><title>Aria&#25968;&#23383;&#23402;&#29983;&#26159;&#19968;&#20010;&#33258;&#25105;&#20013;&#24515;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#20854;&#23427;&#20219;&#20309;&#25968;&#25454;&#38598;&#37117;&#27809;&#26377;&#30340;&#39640;&#31934;&#24230;&#12289;&#29031;&#29255;&#36924;&#30495;&#21644;&#35814;&#23613;&#30340;&#30495;&#23454;&#20449;&#24687;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#33258;&#25105;&#20013;&#24515;&#26426;&#22120;&#24863;&#30693;&#35780;&#20272;&#30340;&#26032;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2306.06362</link><description>&lt;p&gt;
Aria&#25968;&#23383;&#23402;&#29983;&#65306;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#29992;&#20110;&#33258;&#25105;&#20013;&#24515;&#30340;3D&#26426;&#22120;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aria Digital Twin: A New Benchmark Dataset for Egocentric 3D Machine Perception. (arXiv:2306.06362v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06362
&lt;/p&gt;
&lt;p&gt;
Aria&#25968;&#23383;&#23402;&#29983;&#26159;&#19968;&#20010;&#33258;&#25105;&#20013;&#24515;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#20854;&#23427;&#20219;&#20309;&#25968;&#25454;&#38598;&#37117;&#27809;&#26377;&#30340;&#39640;&#31934;&#24230;&#12289;&#29031;&#29255;&#36924;&#30495;&#21644;&#35814;&#23613;&#30340;&#30495;&#23454;&#20449;&#24687;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#33258;&#25105;&#20013;&#24515;&#26426;&#22120;&#24863;&#30693;&#35780;&#20272;&#30340;&#26032;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25512;&#20986;&#20102;Aria&#25968;&#23383;&#23402;&#29983;&#65288;ADT&#65289;-&#19968;&#20010;&#20351;&#29992;Aria&#30524;&#38236;&#25429;&#33719;&#30340;&#33258;&#25105;&#20013;&#24515;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#23545;&#35937;&#65292;&#29615;&#22659;&#21644;&#20154;&#31867;&#32423;&#21035;&#30340;&#30495;&#23454;&#25968;&#25454;&#12290;&#35813;ADT&#25968;&#25454;&#38598;&#21253;&#25324;200&#20010;&#30001;&#31359;&#25140;Aria&#35774;&#22791;&#30340;&#20154;&#22312;&#20004;&#20010;&#23460;&#20869;&#30495;&#23454;&#22330;&#26223;&#20013;&#25191;&#34892;&#30340;&#30495;&#23454;&#19990;&#30028;&#27963;&#21160;&#24207;&#21015;&#65292;&#21253;&#21547;398&#20010;&#23545;&#35937;&#23454;&#20363;&#65288;324&#20010;&#38745;&#24577;&#21644;74&#20010;&#21160;&#24577;&#65289;&#12290;&#27599;&#20010;&#24207;&#21015;&#21253;&#25324;&#65306;a&#65289;&#20004;&#20010;&#21333;&#33394;&#30456;&#26426;&#27969;&#65292;&#19968;&#20010;RGB&#30456;&#26426;&#27969;&#65292;&#20004;&#20010;IMU&#27969;&#30340;&#21407;&#22987;&#25968;&#25454;&#65307;b&#65289;&#23436;&#25972;&#30340;&#20256;&#24863;&#22120;&#26657;&#20934;&#65307;c&#65289;&#30495;&#23454;&#25968;&#25454;&#65292;&#21253;&#25324;Aria&#35774;&#22791;&#30340;&#36830;&#32493;6&#33258;&#30001;&#24230;&#65288;6DoF&#65289;&#23039;&#24577;&#65292;&#23545;&#35937;6DoF&#23039;&#24577;&#65292;3D&#27880;&#35270;&#30690;&#37327;&#65292;3D&#20154;&#20307;&#23039;&#24577;&#65292;2D&#22270;&#20687;&#20998;&#21106;&#65292;&#22270;&#20687;&#28145;&#24230;&#22270;&#65307;d&#65289;&#29031;&#29255;&#33324;&#30495;&#23454;&#30340;&#21512;&#25104;&#28210;&#26579;&#22270;&#20687;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#27809;&#26377;&#29616;&#26377;&#33258;&#25105;&#20013;&#24515;&#25968;&#25454;&#38598;&#33021;&#22815;&#19982;ADT&#30340;&#20934;&#30830;&#24615;&#12289;&#36924;&#30495;&#24230;&#21644;&#20840;&#38754;&#24615;&#30456;&#23218;&#32654;&#12290;&#36890;&#36807;&#21521;&#30740;&#31350;&#31038;&#21306;&#36129;&#29486;ADT&#65292;&#25105;&#20204;&#30340;&#20351;&#21629;&#26159;&#20026;&#33258;&#25105;&#20013;&#24515;&#26426;&#22120;&#24863;&#30693;&#30340;&#35780;&#20272;&#35774;&#31435;&#26032;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the Aria Digital Twin (ADT) - an egocentric dataset captured using Aria glasses with extensive object, environment, and human level ground truth. This ADT release contains 200 sequences of real-world activities conducted by Aria wearers in two real indoor scenes with 398 object instances (324 stationary and 74 dynamic). Each sequence consists of: a) raw data of two monochrome camera streams, one RGB camera stream, two IMU streams; b) complete sensor calibration; c) ground truth data including continuous 6-degree-of-freedom (6DoF) poses of the Aria devices, object 6DoF poses, 3D eye gaze vectors, 3D human poses, 2D image segmentations, image depth maps; and d) photo-realistic synthetic renderings. To the best of our knowledge, there is no existing egocentric dataset with a level of accuracy, photo-realism and comprehensiveness comparable to ADT. By contributing ADT to the research community, our mission is to set a new standard for evaluation in the egocentric machine perce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#27979;&#35797;&#26102;&#38388;&#39118;&#26684;&#36716;&#25442;&#65288;test-time style shifting&#65289;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#39046;&#22495;&#27867;&#21270;&#20219;&#21153;&#20013;&#20219;&#24847;&#39118;&#26684;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#39069;&#22806;&#30340;&#27169;&#22411;&#26356;&#26032;&#12290;</title><link>http://arxiv.org/abs/2306.04911</link><description>&lt;p&gt;
&#27979;&#35797;&#26102;&#38388;&#39118;&#26684;&#36716;&#25442;&#65306;&#22788;&#29702;&#39046;&#22495;&#27867;&#21270;&#20013;&#20219;&#24847;&#39118;&#26684;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Test-Time Style Shifting: Handling Arbitrary Styles in Domain Generalization. (arXiv:2306.04911v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04911
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#27979;&#35797;&#26102;&#38388;&#39118;&#26684;&#36716;&#25442;&#65288;test-time style shifting&#65289;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#39046;&#22495;&#27867;&#21270;&#20219;&#21153;&#20013;&#20219;&#24847;&#39118;&#26684;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#39069;&#22806;&#30340;&#27169;&#22411;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#27867;&#21270;&#65288;DG&#65289;&#35201;&#27714;&#20026;&#26410;&#30693;&#30446;&#26631;&#22495;&#36827;&#34892;&#35757;&#32451;&#65292;&#19988;&#22312;&#25512;&#29702;&#26102;&#38656;&#35201;&#25104;&#21151;&#24212;&#29992;&#20110;&#20219;&#24847;&#65288;&#29978;&#33267;&#26159;&#26410;&#35265;&#65289;&#30340;&#30446;&#26631;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38590;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#27979;&#35797;&#26102;&#38388;&#39118;&#26684;&#36716;&#25442;&#65288;test-time style shifting&#65289;&#65292;&#22312;&#36827;&#34892;&#39044;&#27979;&#20043;&#21069;&#65292;&#23558;&#27979;&#35797;&#26679;&#26412;&#30340;&#39118;&#26684;&#65288;&#19982;&#28304;&#22495;&#23384;&#22312;&#36739;&#22823;&#24046;&#24322;&#30340;&#65289;&#36716;&#25442;&#20026;&#26368;&#25509;&#36817;&#27169;&#22411;&#24050;&#30693;&#30340;&#28304;&#22495;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#33021;&#22815;&#24212;&#23545;&#20855;&#26377;&#20219;&#24847;&#39118;&#26684;&#32479;&#35745;&#30340;&#20219;&#20309;&#30446;&#26631;&#22495;&#65292;&#26080;&#38656;&#22312;&#27979;&#35797;&#26102;&#36827;&#34892;&#39069;&#22806;&#30340;&#27169;&#22411;&#26356;&#26032;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39118;&#26684;&#24179;&#34913;&#65288;style balancing&#65289;&#65292;&#23427;&#20026;&#26368;&#22823;&#21270;&#27979;&#35797;&#26102;&#38388;&#39118;&#26684;&#36716;&#25442;&#30340;&#20248;&#21183;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#24179;&#21488;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;DG&#29305;&#23450;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#24605;&#36335;&#26131;&#20110;&#23454;&#29616;&#65292;&#19988;&#25104;&#21151;&#22320;&#23436;&#25104;&#20102;&#39046;&#22495;&#27867;&#21270;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In domain generalization (DG), the target domain is unknown when the model is being trained, and the trained model should successfully work on an arbitrary (and possibly unseen) target domain during inference. This is a difficult problem, and despite active studies in recent years, it remains a great challenge. In this paper, we take a simple yet effective approach to tackle this issue. We propose test-time style shifting, which shifts the style of the test sample (that has a large style gap with the source domains) to the nearest source domain that the model is already familiar with, before making the prediction. This strategy enables the model to handle any target domains with arbitrary style statistics, without additional model update at test-time. Additionally, we propose style balancing, which provides a great platform for maximizing the advantage of test-time style shifting by handling the DG-specific imbalance issues. The proposed ideas are easy to implement and successfully wor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21019;&#26032;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#29615;&#22659;&#30340;&#25277;&#35937;&#27169;&#22411;&#32780;&#19981;&#38656;&#35201;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#26469;&#29983;&#25104;&#26032;&#30340;&#26032;&#39062;&#24615;&#12290;&#36825;&#21487;&#20197;&#20135;&#29983;&#26356;&#22823;&#30340;&#12289;&#36890;&#24120;&#26159;&#26080;&#38480;&#30340;&#26032;&#39062;&#24615;&#31354;&#38388;&#65292;&#20294;&#38656;&#35201;&#20154;&#31867;&#30340;&#25351;&#23548;&#26469;&#36873;&#25321;&#21644;&#36807;&#28388;&#36825;&#20123;&#26032;&#39062;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04813</link><description>&lt;p&gt;
&#20154;&#20026;&#21442;&#19982;&#30340;&#21019;&#26032;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Human in the Loop Novelty Generation. (arXiv:2306.04813v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04813
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21019;&#26032;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#29615;&#22659;&#30340;&#25277;&#35937;&#27169;&#22411;&#32780;&#19981;&#38656;&#35201;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#26469;&#29983;&#25104;&#26032;&#30340;&#26032;&#39062;&#24615;&#12290;&#36825;&#21487;&#20197;&#20135;&#29983;&#26356;&#22823;&#30340;&#12289;&#36890;&#24120;&#26159;&#26080;&#38480;&#30340;&#26032;&#39062;&#24615;&#31354;&#38388;&#65292;&#20294;&#38656;&#35201;&#20154;&#31867;&#30340;&#25351;&#23548;&#26469;&#36873;&#25321;&#21644;&#36807;&#28388;&#36825;&#20123;&#26032;&#39062;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20154;&#24037;&#26234;&#33021;&#20197;&#24212;&#23545;&#26032;&#39062;&#12289;&#24847;&#22806;&#24773;&#20917;&#26159;&#19968;&#20010;&#22256;&#38590;&#32780;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#25512;&#21160;&#26032;&#39062;&#24615;&#23481;&#32435;&#30340;&#25216;&#26415;&#21457;&#23637;&#26041;&#38754;&#65292;&#19968;&#20010;&#25361;&#25112;&#26159;&#32570;&#20047;&#27979;&#35797;&#26694;&#26550;&#65292;&#20197;&#35780;&#20272;&#22312;&#26032;&#39062;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#12290;&#26368;&#36817;&#22312;&#8220;&#31185;&#23398;&#40479;&#8221;&#21644;&#8220;&#22823;&#23500;&#32705;&#8221;&#31561;&#39046;&#22495;&#20013;&#20986;&#29616;&#30340;&#26032;&#39062;&#24615;&#29983;&#25104;&#26041;&#27861;&#21033;&#29992;&#20102;&#20154;&#31867;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#36827;&#34892;&#25628;&#32034;&#65292;&#20197;&#21457;&#29616;&#26032;&#30340;&#26032;&#39062;&#24615;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#26032;&#39062;&#24615;&#29983;&#25104;&#20043;&#21069;&#24341;&#20837;&#20154;&#31867;&#25351;&#23548;&#65292;&#20135;&#29983;&#30340;&#21019;&#26032;&#21487;&#20197;&#30452;&#25509;&#21152;&#36733;&#21040;&#27169;&#25311;&#29615;&#22659;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21019;&#26032;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#29615;&#22659;&#30340;&#25277;&#35937;&#27169;&#22411;&#65288;&#21253;&#25324;&#27169;&#25311;&#39046;&#22495;&#65289;&#65292;&#19981;&#38656;&#35201;&#20381;&#36182;&#20110;&#29305;&#23450;&#39046;&#22495;&#30340;&#20154;&#31867;&#25351;&#23548;&#26469;&#29983;&#25104;&#21019;&#26032;&#12290;&#19968;&#20010;&#20851;&#38190;&#32467;&#26524;&#26159;&#21487;&#20197;&#29983;&#25104;&#26356;&#22823;&#30340;&#12289;&#36890;&#24120;&#26159;&#26080;&#38480;&#30340;&#26032;&#39062;&#24615;&#31354;&#38388;&#65292;&#20294;&#38656;&#35201;&#22312;&#29983;&#25104;&#21518;&#28041;&#21450;&#20154;&#31867;&#25351;&#23548;&#20197;&#36873;&#25321;&#21644;&#36807;&#28388;&#26032;&#39062;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing artificial intelligence approaches to overcome novel, unexpected circumstances is a difficult, unsolved problem. One challenge to advancing the state of the art in novelty accommodation is the availability of testing frameworks for evaluating performance against novel situations. Recent novelty generation approaches in domains such as Science Birds and Monopoly leverage human domain expertise during the search to discover new novelties. Such approaches introduce human guidance before novelty generation occurs and yield novelties that can be directly loaded into a simulated environment. We introduce a new approach to novelty generation that uses abstract models of environments (including simulation domains) that do not require domain-dependent human guidance to generate novelties. A key result is a larger, often infinite space of novelties capable of being generated, with the trade-off being a requirement to involve human guidance to select and filter novelties post generatio
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#33258;&#19979;&#32780;&#19978;&#25512;&#29702;&#30340;&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;&#31995;&#32479;Fusemate&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#26597;&#35810;&#24341;&#23548;&#30340;&#30456;&#20851;&#24615;&#27979;&#35797;&#20462;&#21098;&#35268;&#21017;&#65292;&#35299;&#20915;&#20102;&#33258;&#19979;&#32780;&#19978;&#25512;&#29702;&#38590;&#20197;&#25511;&#21046;ground clauses&#29983;&#25104;&#25968;&#37327;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21253;&#21547;&#8220;&#26102;&#38388;&#8221;&#30340;&#31034;&#20363;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18924</link><description>&lt;p&gt;
Fusemate&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;&#31995;&#32479;&#20013;&#30340;&#33258;&#19979;&#32780;&#19978;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Bottom-Up Grounding in the Probabilistic Logic Programming System Fusemate. (arXiv:2305.18924v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18924
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#33258;&#19979;&#32780;&#19978;&#25512;&#29702;&#30340;&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;&#31995;&#32479;Fusemate&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#26597;&#35810;&#24341;&#23548;&#30340;&#30456;&#20851;&#24615;&#27979;&#35797;&#20462;&#21098;&#35268;&#21017;&#65292;&#35299;&#20915;&#20102;&#33258;&#19979;&#32780;&#19978;&#25512;&#29702;&#38590;&#20197;&#25511;&#21046;ground clauses&#29983;&#25104;&#25968;&#37327;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21253;&#21547;&#8220;&#26102;&#38388;&#8221;&#30340;&#31034;&#20363;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Fusemate&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#30340;&#25512;&#29702;&#24341;&#25806;&#21253;&#25324;&#19968;&#20010;&#33258;&#19979;&#32780;&#19978;&#25512;&#29702;&#30340;grounding&#32452;&#20214;&#21644;&#19968;&#20010;&#21464;&#37327;&#28040;&#38500;&#26041;&#27861;&#29992;&#20110;&#27010;&#29575;&#25512;&#29702;&#12290;Fusemate&#19981;&#21516;&#20110;&#22823;&#22810;&#25968;&#31995;&#32479;&#65292;&#23427;&#37319;&#29992;&#33258;&#19979;&#32780;&#19978;&#30340;&#26041;&#24335;&#23545;&#31243;&#24207;&#36827;&#34892;grounding&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#33258;&#19979;&#32780;&#19978;&#25512;&#29702;&#38590;&#20197;&#25511;&#21046;&#29983;&#25104;&#30340;ground clauses&#25968;&#37327;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20132;&#38169;grounding&#21644;&#19968;&#20010;&#22522;&#20110;&#26597;&#35810;&#24341;&#23548;&#30340;&#30456;&#20851;&#24615;&#27979;&#35797;&#26469;&#20462;&#21098;&#19982;&#26597;&#35810;&#19981;&#19968;&#33268;&#30340;&#35268;&#21017;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21253;&#21547;&#8220;&#26102;&#38388;&#8221;&#30340;&#31034;&#20363;&#65288;&#22914;&#65288;&#38544;&#34255;&#65289;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65289;&#36827;&#34892;&#28436;&#31034;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#39640;&#20998;&#25903;&#38382;&#39064;&#19978;&#65292;&#30456;&#27604;&#20110;&#26368;&#20808;&#36827;&#30340;&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;&#31995;&#32479;&#65292;&#25105;&#20204;&#30340;&#24615;&#33021;&#26356;&#20855;&#31454;&#20105;&#21147;&#25110;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the Fusemate probabilistic logic programming system. Fusemate's inference engine comprises a grounding component and a variable elimination method for probabilistic inference. Fusemate differs from most other systems by grounding the program in a bottom-up way instead of the common top-down way. While bottom-up grounding is attractive for a number of reasons, e.g., for dynamically creating distributions of varying support sizes, it makes it harder to control the amount of ground clauses generated. We address this problem by interleaving grounding with a query-guided relevance test which prunes rules whose bodies are inconsistent with the query. We present our method in detail and demonstrate it with examples that involve "time", such as (hidden) Markov models. Our experiments demonstrate competitive or better performance compared to a state-of-the art probabilistic logic programming system, in particular for high branching problems.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;RAHC&#65292;&#21487;&#20197;&#19968;&#27425;&#24615;&#22788;&#29702;&#20219;&#24847;&#22797;&#26434;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#19979;&#30340;&#22270;&#20687;&#24674;&#22797;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;HAC&#65292;&#29992;&#20110;&#23398;&#20064;&#21644;&#22522;&#20934;&#27979;&#35797;&#28151;&#21512;&#26465;&#20214;&#30340;&#22270;&#20687;&#24674;&#22797;&#12290;</title><link>http://arxiv.org/abs/2305.09996</link><description>&lt;p&gt;
&#19968;&#27425;&#24615;&#36824;&#21407;&#25429;&#33719;&#20110;&#20219;&#24847;&#22797;&#26434;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#19979;&#30340;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Restoring Images Captured in Arbitrary Hybrid Adverse Weather Conditions in One Go. (arXiv:2305.09996v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09996
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;RAHC&#65292;&#21487;&#20197;&#19968;&#27425;&#24615;&#22788;&#29702;&#20219;&#24847;&#22797;&#26434;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#19979;&#30340;&#22270;&#20687;&#24674;&#22797;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;HAC&#65292;&#29992;&#20110;&#23398;&#20064;&#21644;&#22522;&#20934;&#27979;&#35797;&#28151;&#21512;&#26465;&#20214;&#30340;&#22270;&#20687;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#19979;&#65292;&#22270;&#20687;&#24448;&#24448;&#20250;&#21463;&#21040;&#38543;&#26426;&#28151;&#21512;&#30340;&#22825;&#27668;&#24433;&#21709;&#65288;&#20363;&#22914;&#65292;&#38632;&#22825;&#21644;&#38654;&#38718;&#22812;&#26202;&#65289;&#65292;&#32780;&#29616;&#26377;&#30340;&#22270;&#20687;&#24674;&#22797;&#31639;&#27861;&#39044;&#35745;&#22825;&#27668;&#24433;&#21709;&#26159;&#30456;&#20114;&#29420;&#31435;&#30340;&#65292;&#22240;&#27492;&#21487;&#33021;&#26080;&#27861;&#22788;&#29702;&#22797;&#26434;&#30340;&#29616;&#23454;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#32570;&#20047;&#20840;&#38754;&#23545;&#24212;&#30340;&#25968;&#25454;&#38598;&#26469;&#25551;&#36848;&#22797;&#26434;&#30340;&#28151;&#21512;&#22825;&#27668;&#29366;&#20917;&#65292;&#30417;&#30563;&#35757;&#32451;&#19981;&#21487;&#34892;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#31574;&#30053;&#8212;&#8212;&#26694;&#26550;&#21644;&#25968;&#25454;&#8212;&#8212;&#26469;&#24357;&#34917;&#19978;&#36848;&#38480;&#21046;&#12290;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#31216;&#20026;RAHC&#65292;&#21487;&#20197;&#33298;&#36866;&#22320;&#22788;&#29702;&#28151;&#21512;&#24773;&#20917;&#65292;&#24182;&#36890;&#36807;&#21333;&#20010;&#35757;&#32451;&#27169;&#22411;&#28789;&#27963;&#22320;&#24674;&#22797;&#20219;&#24847;&#28151;&#21512;&#26465;&#20214;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;HAC&#65292;&#29992;&#20110;&#23398;&#20064;&#21644;&#22522;&#20934;&#27979;&#35797;&#20219;&#24847;&#28151;&#21512;&#26465;&#20214;&#30340;&#22270;&#20687;&#24674;&#22797;&#12290;HAC&#21253;&#21547;31&#31181;&#24773;&#26223;&#65292;&#21253;&#25324;&#20219;&#24847;&#32452;&#21512;&#30340;&#38632;&#22825;&#12289;&#38634;&#22825;&#12289;&#38654;&#38718;&#12289;&#38654;&#22825;&#21644;&#34180;&#38654;&#22825;&#27668;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adverse conditions typically suffer from stochastic hybrid weather degradations (e.g., rainy and hazy night), while existing image restoration algorithms envisage that weather degradations occur independently, thus may fail to handle real-world complicated scenarios. Besides, supervised training is not feasible due to the lack of comprehensive paired dataset to characterize hybrid conditions. To this end, we have advanced the forementioned limitations with two tactics: framework and data. On the one hand, we present a novel unified framework, dubbed RAHC, to Restore Arbitrary Hybrid adverse weather Conditions in one go, which can comfortably cope with hybrid scenarios with insufficient remaining background constituents and restore arbitrary hybrid conditions with a single trained model flexibly. On the other hand, we establish a new dataset, termed HAC, for learning and benchmarking arbitrary Hybrid Adverse Conditions restoration. HAC contains 31 scenarios composed of an arbitrary comb
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;HybridNet&#65292;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#19982;&#25299;&#25169;&#35270;&#35282;&#30340;VLSI&#38459;&#22622;&#39044;&#27979;&#30340;&#21452;&#20998;&#25903;&#34701;&#21512;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#32593;&#32476;&#32467;&#26500;&#20013;&#20570;&#20986;&#20960;&#20010;&#20851;&#38190;&#35774;&#35745;&#65292;&#20805;&#20998;&#32508;&#21512;&#30005;&#36335;&#30340;&#25299;&#25169;&#19982;&#20960;&#20309;&#29305;&#24449;&#65292;&#30456;&#36739;&#20110;&#20197;&#24448;&#26041;&#27861;&#21462;&#24471;&#20102;10.9&#65285;&#30340;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.05374</link><description>&lt;p&gt;
HybridNet: &#22522;&#20110;&#20960;&#20309;&#19982;&#25299;&#25169;&#35270;&#35282;&#30340;VLSI&#38459;&#22622;&#39044;&#27979;&#30340;&#21452;&#20998;&#25903;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
HybridNet: Dual-Branch Fusion of Geometrical and Topological Views for VLSI Congestion Prediction. (arXiv:2305.05374v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;HybridNet&#65292;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#19982;&#25299;&#25169;&#35270;&#35282;&#30340;VLSI&#38459;&#22622;&#39044;&#27979;&#30340;&#21452;&#20998;&#25903;&#34701;&#21512;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#32593;&#32476;&#32467;&#26500;&#20013;&#20570;&#20986;&#20960;&#20010;&#20851;&#38190;&#35774;&#35745;&#65292;&#20805;&#20998;&#32508;&#21512;&#30005;&#36335;&#30340;&#25299;&#25169;&#19982;&#20960;&#20309;&#29305;&#24449;&#65292;&#30456;&#36739;&#20110;&#20197;&#24448;&#26041;&#27861;&#21462;&#24471;&#20102;10.9&#65285;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#38459;&#22622;&#39044;&#27979;&#26159;&#24110;&#21161;&#35774;&#35745;&#24072;&#22312;VLSI&#35774;&#35745;&#21608;&#26399;&#20869;&#26356;&#24555;&#36845;&#20195;&#30340;&#37325;&#35201;&#29615;&#33410;&#65292;&#32780;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#32593;&#32476;&#32467;&#26500;&#20013;&#20570;&#20986;&#20960;&#20010;&#20851;&#38190;&#35774;&#35745;&#65292;&#20805;&#20998;&#32508;&#21512;&#30005;&#36335;&#30340;&#25299;&#25169;&#19982;&#20960;&#20309;&#29305;&#24449;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#20004;&#20010;&#29420;&#31435;&#30340;&#22270;&#65288;&#20960;&#20309;&#22270;&#12289;&#25299;&#25169;&#22270;&#65289;&#65292;&#26681;&#25454;&#23427;&#20204;&#30340;&#21807;&#19968;&#23646;&#24615;&#37319;&#29992;&#19981;&#21516;&#30340;&#36793;&#32536;&#26500;&#24314;&#26041;&#26696;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#20998;&#25903;&#32593;&#32476;&#65292;&#27599;&#20010;&#36335;&#24452;&#20013;&#37117;&#26377;&#19981;&#21516;&#30340;&#32534;&#30721;&#22120;&#23618;&#65292;&#24182;&#36890;&#36807;&#31934;&#32454;&#30340;&#34701;&#21512;&#31574;&#30053;&#36827;&#34892;&#32858;&#21512;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#32593;&#32476;&#21517;&#20026;HybridNet&#65292;&#19981;&#20165;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#25429;&#25417;&#21333;&#20803;&#20043;&#38388;&#30340;&#20960;&#20309;&#20132;&#20114;&#65292;&#32780;&#19988;&#36824;&#20445;&#30041;&#20102;&#21407;&#22987;&#30005;&#36335;&#25299;&#25169;&#20851;&#31995;&#12290;&#22312;ISPD2015&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#30456;&#36739;&#20110;&#20197;&#24448;&#26041;&#27861;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;10.9&#65285;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate early congestion prediction can prevent unpleasant surprises at the routing stage, playing a crucial character in assisting designers to iterate faster in VLSI design cycles. In this paper, we introduce a novel strategy to fully incorporate topological and geometrical features of circuits by making several key designs in our network architecture. To be more specific, we construct two individual graphs (geometry-graph, topology-graph) with distinct edge construction schemes according to their unique properties. We then propose a dual-branch network with different encoder layers in each pathway and aggregate representations with a sophisticated fusion strategy. Our network, named HybridNet, not only provides a simple yet effective way to capture the geometric interactions of cells, but also preserves the original topological relationships in the netlist. Experimental results on the ISPD2015 benchmarks show that we achieve an improvement of 10.9% compared to previous methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26088;&#22312;&#25506;&#35752;&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#30340;&#26500;&#24314;&#65292;&#21253;&#25324;&#20174;&#27861;&#24459;&#12289;&#20262;&#29702;&#21644;&#25216;&#26415;&#12289;&#31038;&#20250;&#35282;&#24230;&#30830;&#20445;&#20854;&#20581;&#22766;&#24615;&#12290;&#23454;&#29616;&#30495;&#27491;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#28041;&#21450;&#21040;&#26356;&#24191;&#38420;&#30340;&#24895;&#26223;&#65292;&#32771;&#34385;&#21040;&#20262;&#29702;&#26041;&#38754;&#12289;&#39118;&#38505;&#26041;&#38754;&#12289;&#20197;&#21450;&#23545;&#19971;&#20010;&#25216;&#26415;&#38656;&#27714;&#30340;&#25903;&#25345;&#24230;&#21644;&#22823;&#23616;&#25972;&#20307;&#20043;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.02231</link><description>&lt;p&gt;
&#26500;&#24314;&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#65306;&#20174;&#20154;&#24037;&#26234;&#33021;&#21407;&#21017;&#12289;&#20262;&#29702;&#21644;&#20027;&#35201;&#38656;&#27714;&#21040;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21644;&#30417;&#31649;
&lt;/p&gt;
&lt;p&gt;
Connecting the Dots in Trustworthy Artificial Intelligence: From AI Principles, Ethics, and Key Requirements to Responsible AI Systems and Regulation. (arXiv:2305.02231v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02231
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26088;&#22312;&#25506;&#35752;&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#30340;&#26500;&#24314;&#65292;&#21253;&#25324;&#20174;&#27861;&#24459;&#12289;&#20262;&#29702;&#21644;&#25216;&#26415;&#12289;&#31038;&#20250;&#35282;&#24230;&#30830;&#20445;&#20854;&#20581;&#22766;&#24615;&#12290;&#23454;&#29616;&#30495;&#27491;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#28041;&#21450;&#21040;&#26356;&#24191;&#38420;&#30340;&#24895;&#26223;&#65292;&#32771;&#34385;&#21040;&#20262;&#29702;&#26041;&#38754;&#12289;&#39118;&#38505;&#26041;&#38754;&#12289;&#20197;&#21450;&#23545;&#19971;&#20010;&#25216;&#26415;&#38656;&#27714;&#30340;&#25903;&#25345;&#24230;&#21644;&#22823;&#23616;&#25972;&#20307;&#20043;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#22522;&#20110;&#19971;&#20010;&#25216;&#26415;&#38656;&#27714;&#65292;&#20998;&#21035;&#20174;&#27861;&#24459;&#12289;&#20262;&#29702;&#21644;&#25216;&#26415;&#12289;&#31038;&#20250;&#35282;&#24230;&#30830;&#20445;&#20854;&#20581;&#22766;&#24615;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#30495;&#27491;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#28041;&#21450;&#21040;&#26356;&#24191;&#38420;&#30340;&#24895;&#26223;&#65292;&#21253;&#25324;&#31995;&#32479;&#29983;&#21629;&#21608;&#26399;&#20013;&#25152;&#26377;&#21442;&#19982;&#27969;&#31243;&#21644;&#21442;&#19982;&#32773;&#21487;&#20449;&#24615;&#30340;&#32771;&#37327;&#12290;&#19968;&#20010;&#26356;&#20840;&#38754;&#30340;&#24895;&#26223;&#23558;&#32771;&#34385;&#21040;&#20262;&#29702;&#26041;&#38754;&#12289;&#39118;&#38505;&#26041;&#38754;&#12289;&#20197;&#19979;&#35201;&#20214;&#30340;&#25903;&#25345;&#24230;&#20197;&#21450;&#22823;&#23616;&#25972;&#20307;&#20043;&#20851;&#31995;&#12290;&#35780;&#20272;&#19971;&#20010;&#38656;&#27714;&#20043;&#25216;&#26415;&#26041;&#38754;&#12289;&#20262;&#29702;&#26041;&#38754;&#21644;&#30417;&#31649;&#25361;&#25112;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trustworthy Artificial Intelligence (AI) is based on seven technical requirements sustained over three main pillars that should be met throughout the system's entire life cycle: it should be (1) lawful, (2) ethical, and (3) robust, both from a technical and a social perspective. However, attaining truly trustworthy AI concerns a wider vision that comprises the trustworthiness of all processes and actors that are part of the system's life cycle, and considers previous aspects from different lenses. A more holistic vision contemplates four essential axes: the global principles for ethical use and development of AI-based systems, a philosophical take on AI ethics, a risk-based approach to AI regulation, and the mentioned pillars and requirements. The seven requirements (human agency and oversight; robustness and safety; privacy and data governance; transparency; diversity, non-discrimination and fairness; societal and environmental wellbeing; and accountability) are analyzed from a triple
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#37325;&#26032;&#25490;&#21015;&#35268;&#21010;&#22120;&#65292;&#36890;&#36807;&#35821;&#35328;&#25351;&#23548;&#30340;&#31354;&#38388;&#27010;&#24565;&#26469;&#23454;&#29616;&#38271;&#25351;&#20196;&#20197;&#21450;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#30340;&#31354;&#38388;&#27010;&#24565;&#32452;&#21512;&#12290;&#26412;&#25991;&#30340;&#27169;&#22411;&#22312;&#25351;&#20196;&#23548;&#21521;&#25805;&#20316;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#32452;&#21512;&#25351;&#20196;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20248;&#20110;&#22522;&#20110;&#35821;&#35328;&#34920;&#36798;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#21151;&#22320;&#35299;&#20915;&#20043;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#22797;&#26434;&#25351;&#20196;&#21644;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2304.14391</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#37325;&#26032;&#25490;&#21015;&#35268;&#21010;&#22120;
&lt;/p&gt;
&lt;p&gt;
Energy-based Models as Zero-Shot Planners for Compositional Scene Rearrangement. (arXiv:2304.14391v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#37325;&#26032;&#25490;&#21015;&#35268;&#21010;&#22120;&#65292;&#36890;&#36807;&#35821;&#35328;&#25351;&#23548;&#30340;&#31354;&#38388;&#27010;&#24565;&#26469;&#23454;&#29616;&#38271;&#25351;&#20196;&#20197;&#21450;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#30340;&#31354;&#38388;&#27010;&#24565;&#32452;&#21512;&#12290;&#26412;&#25991;&#30340;&#27169;&#22411;&#22312;&#25351;&#20196;&#23548;&#21521;&#25805;&#20316;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#32452;&#21512;&#25351;&#20196;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20248;&#20110;&#22522;&#20110;&#35821;&#35328;&#34920;&#36798;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#21151;&#22320;&#35299;&#20915;&#20043;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#22797;&#26434;&#25351;&#20196;&#21644;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#24320;&#21457;&#19968;&#20010;&#22330;&#26223;&#37325;&#25490;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#37322;&#38271;&#25351;&#20196;&#20197;&#21450;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#30340;&#31354;&#38388;&#27010;&#24565;&#32452;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#30456;&#23545;&#23545;&#35937;&#25490;&#21015;&#30340;&#33021;&#37327;&#20989;&#25968;&#26469;&#34920;&#31034;&#35821;&#35328;&#25351;&#23548;&#30340;&#31354;&#38388;&#27010;&#24565;&#12290;&#35821;&#35328;&#35299;&#26512;&#22120;&#23558;&#25351;&#20196;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;&#33021;&#37327;&#20989;&#25968;&#65292;&#32780;&#24320;&#25918;&#24335;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23558;&#23427;&#20204;&#30340;&#21442;&#25968;&#22522;&#20110;&#22330;&#26223;&#20013;&#30340;&#30456;&#20851;&#23545;&#35937;&#36827;&#34892;&#20462;&#27491;&#12290;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#27714;&#35299;&#33021;&#37327;&#20989;&#25968;&#30340;&#24635;&#21644;&#65292;&#24182;&#21033;&#29992;&#22522;&#20110;&#26412;&#22320;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#31574;&#30053;&#23558;&#23545;&#35937;&#37325;&#26032;&#23450;&#20301;&#21040;&#25512;&#26029;&#30340;&#30446;&#26631;&#20301;&#32622;&#65292;&#21363;&#21487;&#29983;&#25104;&#30446;&#26631;&#22330;&#26223;&#37197;&#32622;&#12290;&#25105;&#20204;&#22312;&#24050;&#24314;&#31435;&#30340;&#25351;&#20196;&#23548;&#21521;&#25805;&#20316;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#25105;&#20204;&#25552;&#20986;&#30340;&#32452;&#21512;&#25351;&#20196;&#22522;&#20934;&#27979;&#35797;&#20013;&#27979;&#35797;&#20102;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#32489;&#25928;&#20248;&#20110;&#22522;&#20110;&#35821;&#35328;&#34920;&#36798;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#21151;&#22320;&#35299;&#20915;&#20043;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#22797;&#26434;&#25351;&#20196;&#21644;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is compositional; an instruction can express multiple relation constraints to hold among objects in a scene that a robot is tasked to rearrange. Our focus in this work is an instructable scene rearranging framework that generalizes to longer instructions and to spatial concept compositions never seen at training time. We propose to represent language-instructed spatial concepts with energy functions over relative object arrangements. A language parser maps instructions to corresponding energy functions and an open-vocabulary visual-language model grounds their arguments to relevant objects in the scene. We generate goal scene configurations by gradient descent on the sum of energy functions, one per language predicate in the instruction. Local vision-based policies then relocate objects to the inferred goal locations. We test our model on established instruction-guided manipulation benchmarks, as well as benchmarks of compositional instructions we introduce. We show our model 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#20154;&#31867;&#26080;&#27861;&#36776;&#21035;AI&#29983;&#25104;&#30340;&#20551;&#29031;&#29255;&#21644;&#30495;&#23454;&#29031;&#29255;&#65292;&#36825;&#19968;&#28857;&#21463;&#20010;&#20154;&#32972;&#26223;&#30340;&#24433;&#21709;&#24182;&#19981;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2304.13023</link><description>&lt;p&gt;
&#30524;&#35265;&#19981;&#19968;&#23450;&#20026;&#23454;&#65306;&#20154;&#31867;&#24863;&#30693;AI&#29983;&#25104;&#22270;&#20687;&#30340;&#23450;&#37327;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Seeing is not always believing: A Quantitative Study on Human Perception of AI-Generated Images. (arXiv:2304.13023v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#20154;&#31867;&#26080;&#27861;&#36776;&#21035;AI&#29983;&#25104;&#30340;&#20551;&#29031;&#29255;&#21644;&#30495;&#23454;&#29031;&#29255;&#65292;&#36825;&#19968;&#28857;&#21463;&#20010;&#20154;&#32972;&#26223;&#30340;&#24433;&#21709;&#24182;&#19981;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29031;&#29255;&#26159;&#20154;&#20204;&#35760;&#24405;&#26085;&#24120;&#29983;&#27963;&#32463;&#21382;&#30340;&#19968;&#31181;&#26041;&#24335;&#65292;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#21487;&#20449;&#30340;&#20449;&#24687;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#25285;&#24515;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#30340;&#36827;&#27493;&#21487;&#33021;&#20135;&#29983;&#20266;&#36896;&#30340;&#29031;&#29255;&#65292;&#20174;&#32780;&#20135;&#29983;&#22256;&#24785;&#24182;&#38477;&#20302;&#23545;&#29031;&#29255;&#30340;&#20449;&#20219;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#22238;&#31572;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;AI&#30340;&#35270;&#35273;&#20869;&#23481;&#29983;&#25104;&#27169;&#22411;&#33021;&#21542;&#25345;&#32493;&#22320;&#27450;&#39575;&#20154;&#31867;&#30340;&#30524;&#30555;&#65292;&#24182;&#20256;&#36798;&#38169;&#35823;&#20449;&#24687;&#12290;&#36890;&#36807;&#23545;50&#21517;&#21442;&#19982;&#32773;&#36827;&#34892;&#39640;&#36136;&#37327;&#30340;&#23450;&#37327;&#30740;&#31350;&#65292;&#25105;&#20204;&#39318;&#27425;&#25581;&#31034;&#65292;&#20154;&#31867;&#26080;&#27861;&#26174;&#33879;&#21306;&#20998;&#30495;&#23454;&#29031;&#29255;&#21644;AI&#21019;&#24314;&#30340;&#20266;&#36896;&#29031;&#29255;&#65292;&#36798;&#21040;38.7%&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#20010;&#20154;&#30340;&#32972;&#26223;&#65292;&#22914;&#24615;&#21035;&#65292;&#24180;&#40836;&#21644;&#32463;&#39564;&#65292;&#23545;&#21306;&#20998;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#21644;&#30495;&#23454;&#29031;&#29255;&#30340;&#33021;&#21147;&#27809;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;
&lt;/p&gt;
&lt;p&gt;
Photos serve as a way for humans to record what they experience in their daily lives, and they are often regarded as trustworthy sources of information. However, there is a growing concern that the advancement of artificial intelligence (AI) technology may produce fake photos, which can create confusion and diminish trust in photographs. This study aims to answer the question of whether the current state-of-the-art AI-based visual content generation models can consistently deceive human eyes and convey false information. By conducting a high-quality quantitative study with fifty participants, we reveal, for the first time, that humans cannot distinguish between real photos and AI-created fake photos to a significant degree 38.7%. Our study also finds that an individual's background, such as their gender, age, and experience with AI-generated content (AIGC), does not significantly affect their ability to distinguish AI-generated images from real photographs. However, we do observe that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26159;&#21542;&#34920;&#29616;&#20986;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#65292;&#21457;&#29616;&#20102;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#34892;&#20026;&#30340;&#36947;&#24503;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.03279</link><description>&lt;p&gt;
&#22870;&#21169;&#26159;&#21542;&#21512;&#29702;&#65311;&#22312; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#20013;&#34913;&#37327;&#22870;&#21169;&#19982;&#36947;&#24503;&#34892;&#20026;&#20043;&#38388;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark. (arXiv:2304.03279v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26159;&#21542;&#34920;&#29616;&#20986;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#65292;&#21457;&#29616;&#20102;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#34892;&#20026;&#30340;&#36947;&#24503;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#34987;&#35757;&#32451;&#25104;&#26368;&#22823;&#21270;&#22870;&#21169;&#65292;&#36825;&#21487;&#33021;&#20250;&#28608;&#21169;&#36861;&#27714;&#26435;&#21147;&#21644;&#27450;&#39575;&#34892;&#20026;&#65292;&#31867;&#20284;&#20110;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#21487;&#33021;&#20250;&#28608;&#21169;&#26377;&#23475;&#34892;&#20026;&#12290;&#37027;&#20040;&#20195;&#29702;&#26159;&#21542;&#33258;&#28982;&#32780;&#28982;&#22320;&#23398;&#20250;&#20102;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#65311;&#25105;&#20204;&#22914;&#20309;&#22312; GPT-4 &#31561;&#36890;&#29992;&#27169;&#22411;&#20013;&#34913;&#37327;&#36825;&#20123;&#34892;&#20026;&#21602;&#65311;&#20026;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#27979;&#35797;&#28085;&#30422;&#20102;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#22810;&#26679;&#21270;&#30340;&#24773;&#26223;&#65292;&#37325;&#28857;&#20851;&#27880;&#31038;&#20250;&#20915;&#31574;&#21046;&#23450;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#20195;&#29702;&#26159;&#21542;&#34920;&#29616;&#20986;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#12290;&#25105;&#20204;&#25968;&#23398;&#21270;&#20102;&#25968;&#21313;&#31181;&#26377;&#23475;&#34892;&#20026;&#65292;&#24182;&#20351;&#29992;&#25105;&#20204;&#30340;&#27880;&#37322;&#26469;&#35780;&#20272;&#20195;&#29702;&#20542;&#21521;&#20110;&#36861;&#27714;&#26435;&#21147;&#65292;&#36896;&#25104;&#21151;&#33021;&#19981;&#33391;&#21644;&#36829;&#21453;&#20262;&#29702;&#30340;&#20542;&#21521;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#34892;&#20026;&#30340;&#36947;&#24503;&#24615;&#20043;&#38388;&#23384;&#22312;&#19968;&#20123;&#32039;&#24352;&#20851;&#31995;&#12290;&#20026;&#20102;&#25913;&#21892;&#36825;&#31181;&#26435;&#34913;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#20351;&#20195;&#29702;&#36235;&#21521;&#20110;&#37319;&#21462;&#26356;&#23569;&#30340;&#26377;&#23475;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;MACHIAVELLI &#26159;&#35780;&#20272;&#20154;&#24037;&#20195;&#29702;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#27700;&#24179;&#30340;&#26377;&#29992;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial agents have traditionally been trained to maximize reward, which may incentivize power-seeking and deception, analogous to how next-token prediction in language models (LMs) may incentivize toxicity. So do agents naturally learn to be Machiavellian? And how do we measure these behaviors in general-purpose models such as GPT-4? Towards answering these questions, we introduce MACHIAVELLI, a benchmark of 134 Choose-Your-Own-Adventure games containing over half a million rich, diverse scenarios that center on social decision-making. Scenario labeling is automated with LMs, which are more performant than human annotators. We mathematize dozens of harmful behaviors and use our annotations to evaluate agents' tendencies to be power-seeking, cause disutility, and commit ethical violations. We observe some tension between maximizing reward and behaving ethically. To improve this trade-off, we investigate LM-based methods to steer agents' towards less harmful behaviors. Our results sh
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#22823;&#23567;&#12289;&#32467;&#26500;&#21270;&#21098;&#26525;&#12289;&#25512;&#26029;&#25928;&#29575;&#21644;&#25688;&#35201;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20351;&#29992;&#19981;&#23545;&#31216;&#21098;&#26525;&#21487;&#22312;&#19981;&#22823;&#25439;&#22833;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#39640;&#25512;&#26029;&#25928;&#29575;&#32422;3&#20493;&#12290;</title><link>http://arxiv.org/abs/2304.02721</link><description>&lt;p&gt;
&#36229;&#36234;&#19981;&#23545;&#31216;&#24615;&#65306;&#32467;&#26500;&#21098;&#26525;&#25552;&#39640;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#30340;&#25512;&#26029;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
To Asymmetry and Beyond: Structured Pruning of Sequence to Sequence Models for Improved Inference Efficiency. (arXiv:2304.02721v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#22823;&#23567;&#12289;&#32467;&#26500;&#21270;&#21098;&#26525;&#12289;&#25512;&#26029;&#25928;&#29575;&#21644;&#25688;&#35201;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20351;&#29992;&#19981;&#23545;&#31216;&#21098;&#26525;&#21487;&#22312;&#19981;&#22823;&#25439;&#22833;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#39640;&#25512;&#26029;&#25928;&#29575;&#32422;3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#21040;&#24207;&#21015;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#36830;&#36143;&#65292;&#30456;&#20851;&#21644;&#31616;&#27905;&#30340;&#25277;&#35937;&#25688;&#35201;&#12290;&#20294;&#26159;&#65292;&#27169;&#22411;&#22823;&#23567;&#21487;&#33021;&#20351;&#24471;&#22312;&#24310;&#36831;&#25935;&#24863;&#25110; Web &#35268;&#27169;&#30340;&#23454;&#29616;&#20013;&#37096;&#32626;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#22823;&#23567;&#12289;&#32467;&#26500;&#21270;&#21098;&#26525;&#12289;&#25512;&#26029;&#25928;&#29575;&#21644;&#24191;&#27867;&#20351;&#29992;&#30340;&#25688;&#35201;&#25968;&#25454;&#38598;&#19978;&#30340;&#25688;&#35201;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#27169;&#22411;&#20934;&#30830;&#24615;&#19982;&#32534;&#30721;&#22120;&#22823;&#23567;&#26377;&#20851;&#65292;&#32780;&#25512;&#29702;&#25928;&#29575;&#19982;&#35299;&#30721;&#22120;&#26377;&#20851;&#12290;&#20351;&#29992;&#19981;&#23545;&#31216;&#21098;&#26525;&#21487;&#23548;&#33268;&#25512;&#26029;&#24310;&#36831;&#30340;&#36817;3&#20493;&#25552;&#39640;&#65292;Rouge-2&#30340;&#25439;&#22833;&#32422;&#20026;1&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#24179;&#22343;&#24615;&#33021;&#38477;&#20302;&#21644;&#19981;&#23545;&#31216;&#24615;&#30340;&#20316;&#29992;&#22312;&#27169;&#22411;&#22823;&#23567;&#21644;&#25968;&#25454;&#38598;&#21464;&#21270;&#26041;&#38754;&#26159;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequence-to-sequence language models can be used to produce abstractive summaries which are coherent, relevant, and concise. Still, model sizes can make deployment in latency-sensitive or web-scale implementations difficult. This paper studies the relationship between model size, structured pruning, inference efficiency, and summarization accuracy on widely used summarization datasets. We show that model accuracy is tied to the encoder size while inference efficiency is connected to the decoder. Using asymmetric pruning can lead to nearly 3x improvement in inference latency with ~1 point loss in Rouge-2. Moreover, we find both the average degradation and the role of asymmetry to be consistent across model sizes and variations in datasets.
&lt;/p&gt;</description></item><item><title>MACARONS&#26159;&#19968;&#31181;&#26080;&#38656;&#28145;&#24230;&#20256;&#24863;&#22120;&#21644;&#19977;&#32500;&#30417;&#30563;&#65292;&#20165;&#36890;&#36807;&#24425;&#33394;&#22270;&#20687;&#39044;&#27979;&#23481;&#31215;&#21344;&#29992;&#22330;&#24182;&#39044;&#27979;&#19979;&#19968;&#26368;&#20339;&#35270;&#35282;&#30340;&#26041;&#27861;&#65292;&#20855;&#22791;&#22312;&#26032;&#22330;&#26223;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.03315</link><description>&lt;p&gt;
MACARONS&#65306;&#22522;&#20110;RGB&#22312;&#32447;&#33258;&#30417;&#30563;&#30340;&#26144;&#23556;&#19982;&#35206;&#30422;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
MACARONS: Mapping And Coverage Anticipation with RGB Online Self-Supervision. (arXiv:2303.03315v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03315
&lt;/p&gt;
&lt;p&gt;
MACARONS&#26159;&#19968;&#31181;&#26080;&#38656;&#28145;&#24230;&#20256;&#24863;&#22120;&#21644;&#19977;&#32500;&#30417;&#30563;&#65292;&#20165;&#36890;&#36807;&#24425;&#33394;&#22270;&#20687;&#39044;&#27979;&#23481;&#31215;&#21344;&#29992;&#22330;&#24182;&#39044;&#27979;&#19979;&#19968;&#26368;&#20339;&#35270;&#35282;&#30340;&#26041;&#27861;&#65292;&#20855;&#22791;&#22312;&#26032;&#22330;&#26223;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#23398;&#20064;&#25506;&#32034;&#26032;&#30340;&#22823;&#22411;&#29615;&#22659;&#65292;&#24182;&#20165;&#36890;&#36807;&#24425;&#33394;&#22270;&#20687;&#36827;&#34892;&#19977;&#32500;&#37325;&#24314;&#12290;&#36825;&#19982;&#19979;&#19968;&#26368;&#20339;&#35270;&#35282;(NBV)&#38382;&#39064;&#26377;&#23494;&#20999;&#20851;&#31995;&#65292;&#20854;&#20013;&#38656;&#35201;&#30830;&#23450;&#25668;&#20687;&#26426;&#30340;&#19979;&#19968;&#20010;&#31227;&#21160;&#26041;&#21521;&#20197;&#25552;&#39640;&#26410;&#30693;&#22330;&#26223;&#30340;&#35206;&#30422;&#29575;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;NBV&#26041;&#27861;&#20381;&#36182;&#20110;&#28145;&#24230;&#20256;&#24863;&#22120;&#65292;&#38656;&#35201;&#19977;&#32500;&#30417;&#30563;&#24182;&#19988;&#19981;&#33021;&#24456;&#22909;&#22320;&#25193;&#23637;&#21040;&#22823;&#22411;&#22330;&#26223;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#38656;&#35201;&#19968;&#20010;&#24425;&#33394;&#30456;&#26426;&#19988;&#26080;&#38656;&#19977;&#32500;&#30417;&#30563;&#12290;&#23427;&#21487;&#20197;&#33258;&#30417;&#30563;&#22320;&#23398;&#20064;&#20174;&#24425;&#33394;&#22270;&#20687;&#20013;&#39044;&#27979;&#8220;&#23481;&#31215;&#21344;&#29992;&#22330;&#8221;&#65292;&#24182;&#20174;&#20013;&#39044;&#27979;NBV&#12290;&#30001;&#20110;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26032;&#22330;&#26223;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#22240;&#20026;&#23427;&#19981;&#20250;&#23545;&#20219;&#20309;&#35757;&#32451;&#25968;&#25454;&#30340;3D&#20559;&#24046;&#12290;&#25105;&#20204;&#22312;&#26368;&#36817;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#28436;&#31034;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;&#21508;&#31181;3D&#22330;&#26223;&#32452;&#25104;&#65292;&#24182;&#19988;&#25105;&#20204;&#34920;&#29616;&#29978;&#33267;&#27604;&#38656;&#35201;&#28145;&#24230;&#20256;&#24863;&#22120;&#30340;&#26368;&#26032;&#26041;&#27861;&#26356;&#22909;&#65292;&#32780;&#36825;&#23545;&#20110;&#20351;&#29992;&#39134;&#34892;&#26080;&#20154;&#26426;&#25429;&#33719;&#30340;&#25143;&#22806;&#22330;&#26223;&#26469;&#35828;&#24182;&#19981;&#26159;&#19968;&#20010;&#29616;&#23454;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a method that simultaneously learns to explore new large environments and to reconstruct them in 3D from color images only. This is closely related to the Next Best View problem (NBV), where one has to identify where to move the camera next to improve the coverage of an unknown scene. However, most of the current NBV methods rely on depth sensors, need 3D supervision and/or do not scale to large scenes. Our method requires only a color camera and no 3D supervision. It simultaneously learns in a self-supervised fashion to predict a "volume occupancy field" from color images and, from this field, to predict the NBV. Thanks to this approach, our method performs well on new scenes as it is not biased towards any training 3D data. We demonstrate this on a recent dataset made of various 3D scenes and show it performs even better than recent methods requiring a depth sensor, which is not a realistic assumption for outdoor scenes captured with a flying drone.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SHAP-IQ&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20219;&#24847;&#38454;Shapley&#20114;&#21160;&#65292;&#24182;&#25552;&#20379;&#20102;&#36924;&#36817;&#36136;&#37327;&#30340;&#29702;&#35770;&#20445;&#35777;&#21644;&#26041;&#24046;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#36924;&#36817;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.01179</link><description>&lt;p&gt;
SHAP-IQ: &#20219;&#24847;&#38454;Shapley interaction&#30340;&#32479;&#19968;&#36924;&#36817;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SHAP-IQ: Unified Approximation of any-order Shapley Interactions. (arXiv:2303.01179v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01179
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SHAP-IQ&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20219;&#24847;&#38454;Shapley&#20114;&#21160;&#65292;&#24182;&#25552;&#20379;&#20102;&#36924;&#36817;&#36136;&#37327;&#30340;&#29702;&#35770;&#20445;&#35777;&#21644;&#26041;&#24046;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#36924;&#36817;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30740;&#31350;&#20013;&#65292;Shapley&#20540;&#65288;SV&#65289;&#36890;&#24120;&#34987;&#24212;&#29992;&#20110;&#30830;&#23450;&#20219;&#20309;&#40657;&#30418;&#27169;&#22411;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#24471;&#20998;&#12290; Shapley interaction indices&#23558;SV&#25193;&#23637;&#20026;&#23450;&#20041;&#20219;&#24847;&#38454;&#29305;&#24449;&#30456;&#20114;&#20316;&#29992;&#24471;&#20998;&#12290;&#23450;&#20041;&#29420;&#29305;&#30340;Shapley interaction index&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#30740;&#31350;&#38382;&#39064;&#65292;&#36804;&#20170;&#20026;&#27490;&#24050;&#32463;&#25552;&#20986;&#20102;&#19977;&#20010;&#23450;&#20041;&#65292;&#20854;&#19981;&#21516;&#20043;&#22788;&#22312;&#20110;&#25152;&#36873;&#25321;&#30340;&#20844;&#29702;&#12290;&#27492;&#22806;&#65292;&#27599;&#20010;&#23450;&#20041;&#37117;&#38656;&#35201;&#29305;&#23450;&#30340;&#36924;&#36817;&#25216;&#26415;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#37319;&#26679;&#30340;&#26377;&#25928;&#36924;&#36817;&#26041;&#27861;SHAPley Interaction Quantification&#65288;SHAP-IQ&#65289;&#65292;&#20197;&#35745;&#31639;&#20219;&#24847;&#22522;&#25968;&#20132;&#20114;&#25351;&#25968;&#65288;CII&#65289;&#30340;Shapley&#20114;&#21160;&#12290;&#21363;&#28385;&#36275;&#32447;&#24615;&#12289;&#23545;&#31216;&#21644;&#34394;&#25311;&#20844;&#29702;&#30340;&#20132;&#20114;&#25351;&#25968;&#12290;SHAP-IQ&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#20026;&#20854;&#36924;&#36817;&#36136;&#37327;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#20197;&#21450;&#28857;&#20272;&#35745;&#30340;&#26041;&#24046;&#20272;&#35745;&#12290;&#23545;&#20110;SV&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#25105;&#20204;&#30340;&#36924;&#36817;&#26041;&#27861;&#19982;&#31934;&#30830;&#35745;&#31639;&#19968;&#33268;&#12290;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#20197;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#26174;&#31034;SHAP-IQ&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#36924;&#36817;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predominately in explainable artificial intelligence (XAI) research, the Shapley value (SV) is applied to determine feature importance scores for any black box model. Shapley interaction indices extend the SV to define any-order feature interaction scores. Defining a unique Shapley interaction index is an open research question and, so far, three definitions have been proposed, which differ by their choice of axioms. Moreover, each definition requires a specific approximation technique. Here, we propose SHAPley Interaction Quantification (SHAP-IQ), an efficient sampling-based approximator to compute Shapley interactions for arbitrary cardinal interaction indices (CII), i.e. interaction indices that satisfy the linearity, symmetry and dummy axiom. SHAP-IQ is based on a novel representation and, in contrast to existing methods, we provide theoretical guarantees for its approximation quality, as well as estimates for the variance of the point estimates. For the special case of SV, our app
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#28909;&#21147;&#23398;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#30340;&#27010;&#24565;&#65292;&#24182;&#23558;&#22810;&#31181;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#32479;&#19968;&#22312;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#19979;&#12290;&#35813;&#31639;&#27861;&#19982;&#20256;&#32479;&#30828;&#20214;&#26377;&#25152;&#19981;&#21516;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#20840;&#26032;&#30340;&#35745;&#31639;&#27169;&#24335;&#65292;&#20351;&#24471;&#28909;&#21147;&#23398;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#35299;&#20915;&#35745;&#31639;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.06584</link><description>&lt;p&gt;
&#28909;&#21147;&#23398;&#20154;&#24037;&#26234;&#33021;&#19982;&#27874;&#21160;&#21069;&#27839;
&lt;/p&gt;
&lt;p&gt;
Thermodynamic AI and the fluctuation frontier. (arXiv:2302.06584v3 [cs.ET] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06584
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#28909;&#21147;&#23398;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#30340;&#27010;&#24565;&#65292;&#24182;&#23558;&#22810;&#31181;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#32479;&#19968;&#22312;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#19979;&#12290;&#35813;&#31639;&#27861;&#19982;&#20256;&#32479;&#30828;&#20214;&#26377;&#25152;&#19981;&#21516;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#20840;&#26032;&#30340;&#35745;&#31639;&#27169;&#24335;&#65292;&#20351;&#24471;&#28909;&#21147;&#23398;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#35299;&#20915;&#35745;&#31639;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#21463;&#21040;&#29289;&#29702;&#23398;&#21551;&#21457;&#24182;&#20351;&#29992;&#38543;&#26426;&#27874;&#21160;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;AI&#31639;&#27861;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#23558;&#23427;&#20204;&#32479;&#19968;&#22312;&#19968;&#20010;&#21517;&#20026;&#8220;&#28909;&#21147;&#23398;&#20154;&#24037;&#26234;&#33021;&#8221;&#30340;&#25968;&#23398;&#26694;&#26550;&#19979;&#12290;&#36825;&#31181;&#28909;&#21147;&#23398;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#30446;&#21069;&#22312;&#25968;&#23383;&#30828;&#20214;&#19978;&#36816;&#34892;&#65292;&#26497;&#38480;&#20102;&#23427;&#20204;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#25972;&#20307;&#28508;&#21147;&#12290;&#25105;&#20204;&#24314;&#35758;&#19968;&#31181;&#26032;&#22411;&#35745;&#31639;&#27169;&#24335;&#65292;&#36719;&#20214;&#21644;&#30828;&#20214;&#25104;&#20026;&#19981;&#21487;&#20998;&#21106;&#30340;&#25972;&#20307;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#32479;&#19968;&#20351;&#25105;&#20204;&#21487;&#20197;&#35782;&#21035;&#20986;&#19968;&#20010;&#21333;&#19968;&#30340;&#20840;&#26632;&#33539;&#20363;&#65292;&#21253;&#25324;&#28909;&#21147;&#23398;&#20154;&#24037;&#26234;&#33021;&#30828;&#20214;&#65292;&#21487;&#20197;&#21152;&#36895;&#36825;&#20123;&#31639;&#27861;&#12290;&#25105;&#20204;&#23558;&#28909;&#21147;&#23398;&#20154;&#24037;&#26234;&#33021;&#30828;&#20214;&#19982;&#20256;&#32479;&#25968;&#23383;&#30828;&#20214;&#36827;&#34892;&#23545;&#27604;&#65292;&#24182;&#27010;&#36848;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#28508;&#22312;&#20248;&#28857;&#65292;&#21253;&#25324;&#39640;&#25928;&#30340;&#33021;&#28304;&#21033;&#29992;&#65292;&#35299;&#20915;&#20248;&#21270;&#12289;&#37319;&#26679;&#21644;&#25512;&#26029;&#31561;&#35745;&#31639;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many Artificial Intelligence (AI) algorithms are inspired by physics and employ stochastic fluctuations. We connect these physics-inspired AI algorithms by unifying them under a single mathematical framework that we call Thermodynamic AI. Seemingly disparate algorithmic classes can be described by this framework, for example, (1) Generative diffusion models, (2) Bayesian neural networks, (3) Monte Carlo sampling and (4) Simulated annealing. Such Thermodynamic AI algorithms are currently run on digital hardware, ultimately limiting their scalability and overall potential. Stochastic fluctuations naturally occur in physical thermodynamic systems, and such fluctuations can be viewed as a computational resource. Hence, we propose a novel computing paradigm, where software and hardware become inseparable. Our algorithmic unification allows us to identify a single full-stack paradigm, involving Thermodynamic AI hardware, that could accelerate such algorithms. We contrast Thermodynamic AI har
&lt;/p&gt;</description></item><item><title>SinMDM&#26159;&#19968;&#31181;&#21333;&#19968;&#21160;&#20316;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#21435;&#22122;&#32593;&#32476;&#21644;&#25193;&#25955;&#27169;&#22411;&#26469;&#23398;&#20064;&#21333;&#20010;&#21160;&#20316;&#24207;&#21015;&#30340;&#20869;&#37096;&#27169;&#24335;&#65292;&#24182;&#21512;&#25104;&#20855;&#26377;&#20854;&#29305;&#28857;&#30340;&#20219;&#24847;&#38271;&#24230;&#30340;&#36816;&#21160;&#12290;</title><link>http://arxiv.org/abs/2302.05905</link><description>&lt;p&gt;
&#21333;&#19968;&#21160;&#20316;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Single Motion Diffusion. (arXiv:2302.05905v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05905
&lt;/p&gt;
&lt;p&gt;
SinMDM&#26159;&#19968;&#31181;&#21333;&#19968;&#21160;&#20316;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#21435;&#22122;&#32593;&#32476;&#21644;&#25193;&#25955;&#27169;&#22411;&#26469;&#23398;&#20064;&#21333;&#20010;&#21160;&#20316;&#24207;&#21015;&#30340;&#20869;&#37096;&#27169;&#24335;&#65292;&#24182;&#21512;&#25104;&#20855;&#26377;&#20854;&#29305;&#28857;&#30340;&#20219;&#24847;&#38271;&#24230;&#30340;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#36924;&#30495;&#30340;&#20154;&#31867;&#12289;&#21160;&#29289;&#29978;&#33267;&#26159;&#34394;&#26500;&#29983;&#29289;&#30340;&#21160;&#30011;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#33402;&#26415;&#23478;&#21644;&#35745;&#31639;&#26426;&#22270;&#24418;&#19987;&#19994;&#20154;&#21592;&#30340;&#30446;&#26631;&#12290;&#19982;&#22270;&#20687;&#39046;&#22495;&#30456;&#27604;&#65292;&#36816;&#21160;&#39046;&#22495;&#30340;&#25968;&#25454;&#23454;&#20363;&#25968;&#37327;&#38750;&#24120;&#26377;&#38480;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21160;&#29289;&#21644;&#24322;&#22269;&#24773;&#35843;&#29983;&#29289;&#65288;&#20363;&#22914;&#40857;&#65289;&#65292;&#23427;&#20204;&#20855;&#26377;&#29420;&#29305;&#30340;&#39592;&#26550;&#21644;&#36816;&#21160;&#27169;&#24335;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21333;&#19968;&#21160;&#20316;&#25193;&#25955;&#27169;&#22411;&#65288;SinMDM&#65289;&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#23398;&#20064;&#20219;&#24847;&#25299;&#25169;&#32467;&#26500;&#30340;&#21333;&#20010;&#21160;&#20316;&#24207;&#21015;&#30340;&#20869;&#37096;&#27169;&#24335;&#65292;&#24182;&#21512;&#25104;&#20855;&#26377;&#20854;&#29305;&#28857;&#30340;&#20219;&#24847;&#38271;&#24230;&#30340;&#36816;&#21160;&#12290;&#25105;&#20204;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#20174;&#21333;&#20010;&#36755;&#20837;&#21160;&#20316;&#23398;&#20064;&#30340;&#21435;&#22122;&#32593;&#32476;&#12290;SinMDM&#37319;&#29992;&#36731;&#37327;&#32423;&#26550;&#26500;&#35774;&#35745;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#26412;&#22320;&#20851;&#27880;&#23618;&#30340;&#27973;&#23618;&#32593;&#32476;&#26469;&#32553;&#23567;&#24863;&#21463;&#37326;&#24182;&#20419;&#36827;&#36816;&#21160;&#22810;&#26679;&#21270;&#65292;&#36991;&#20813;&#20102;&#36807;&#24230;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthesizing realistic animations of humans, animals, and even imaginary creatures, has long been a goal for artists and computer graphics professionals. Compared to the imaging domain, which is rich with large available datasets, the number of data instances for the motion domain is limited, particularly for the animation of animals and exotic creatures (e.g., dragons), which have unique skeletons and motion patterns. In this work, we present a Single Motion Diffusion Model, dubbed SinMDM, a model designed to learn the internal motifs of a single motion sequence with arbitrary topology and synthesize motions of arbitrary length that are faithful to them. We harness the power of diffusion models and present a denoising network explicitly designed for the task of learning from a single input motion. SinMDM is designed to be a lightweight architecture, which avoids overfitting by using a shallow network with local attention layers that narrow the receptive field and encourage motion dive
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Fine-tune-CoT&#65292;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#27169;&#22411;&#20316;&#20026;&#25512;&#29702;&#25945;&#24072;&#35753;&#36739;&#23567;&#27169;&#22411;&#20063;&#33021;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36828;&#36828;&#20248;&#20110;&#22522;&#20110;&#25552;&#31034;&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#21033;&#29992;&#25945;&#24072;&#27169;&#22411;&#30340;&#33021;&#21147;&#20026;&#27599;&#20010;&#21407;&#22987;&#26679;&#26412;&#29983;&#25104;&#22810;&#20010;&#19981;&#21516;&#30340;&#21407;&#22240;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2212.10071</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#25512;&#29702;&#25945;&#24072;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Are Reasoning Teachers. (arXiv:2212.10071v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Fine-tune-CoT&#65292;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#27169;&#22411;&#20316;&#20026;&#25512;&#29702;&#25945;&#24072;&#35753;&#36739;&#23567;&#27169;&#22411;&#20063;&#33021;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36828;&#36828;&#20248;&#20110;&#22522;&#20110;&#25552;&#31034;&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#21033;&#29992;&#25945;&#24072;&#27169;&#22411;&#30340;&#33021;&#21147;&#20026;&#27599;&#20010;&#21407;&#22987;&#26679;&#26412;&#29983;&#25104;&#22810;&#20010;&#19981;&#21516;&#30340;&#21407;&#22240;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24605;&#32500;&#38142;&#26465;&#25552;&#31034;&#21487;&#20197;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#36880;&#27493;&#35299;&#20915;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#25552;&#31034;&#30340;&#24605;&#32500;&#38142;&#26465;&#26041;&#27861;&#20381;&#36182;&#20110;&#20687;GPT-3 175B&#36825;&#26679;&#38750;&#24120;&#22823;&#30340;&#27169;&#22411;&#65292;&#36825;&#22312;&#35268;&#27169;&#19978;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Fine-tune-CoT&#26041;&#27861;&#65292;&#20351;&#29992;&#36825;&#20123;&#22823;&#22411;&#27169;&#22411;&#20316;&#20026;&#25512;&#29702;&#25945;&#24072;&#65292;&#20197;&#35753;&#36739;&#23567;&#30340;&#27169;&#22411;&#20063;&#33021;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#23610;&#23544;&#35201;&#27714;&#20943;&#23569;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;&#25105;&#20204;&#22312;&#20844;&#20849;&#27169;&#22411;&#21644;&#22797;&#26434;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#65292;&#21457;&#29616;Fine-tune-CoT&#21487;&#20197;&#22312;&#23567;&#22411;&#27169;&#22411;&#20013;&#23454;&#29616;&#23454;&#36136;&#24615;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#36828;&#36828;&#20248;&#20110;&#22522;&#20110;&#25552;&#31034;&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#29978;&#33267;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#20063;&#20248;&#20110;&#25945;&#24072;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#35813;&#26041;&#27861;&#65292;&#21033;&#29992;&#25945;&#24072;&#27169;&#22411;&#30340;&#33021;&#21147;&#20026;&#27599;&#20010;&#21407;&#22987;&#26679;&#26412;&#29983;&#25104;&#22810;&#20010;&#19981;&#21516;&#30340;&#21407;&#22240;&#35299;&#37322;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#24494;&#35843;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have shown that chain-of-thought (CoT) prompting can elicit language models to solve complex reasoning tasks, step-by-step. However, prompt-based CoT methods are dependent on very large models such as GPT-3 175B which are prohibitive to deploy at scale. In this paper, we use these large models as reasoning teachers to enable complex reasoning in smaller models and reduce model size requirements by several orders of magnitude. We propose Fine-tune-CoT, a method that generates reasoning samples from very large teacher models to fine-tune smaller models. We evaluate our method on a wide range of public models and complex tasks. We find that Fine-tune-CoT enables substantial reasoning capability in small models, far outperforming prompt-based baselines and even the teacher model in many tasks. Additionally, we extend our method by leveraging the teacher model's ability to generate multiple distinct rationales for each original sample. Enriching the fine-tuning data with such d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20559;&#35265;&#30340;&#24322;&#26500;&#22330;&#26223;&#22270;&#29983;&#25104;&#26694;&#26550;&#65292;&#20351;&#29992;&#20851;&#31995;&#24863;&#30693;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#25429;&#33719;&#23545;&#35937;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#28040;&#24687;&#20256;&#36882;&#23618;&#26469;&#27719;&#24635;&#22270;&#20687;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2212.00443</link><description>&lt;p&gt;
&#20351;&#29992;&#20851;&#31995;&#24863;&#30693;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#26080;&#20559;&#35265;&#30340;&#24322;&#26500;&#22330;&#26223;&#22270;
&lt;/p&gt;
&lt;p&gt;
Unbiased Heterogeneous Scene Graph Generation with Relation-aware Message Passing Neural Network. (arXiv:2212.00443v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20559;&#35265;&#30340;&#24322;&#26500;&#22330;&#26223;&#22270;&#29983;&#25104;&#26694;&#26550;&#65292;&#20351;&#29992;&#20851;&#31995;&#24863;&#30693;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#25429;&#33719;&#23545;&#35937;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#28040;&#24687;&#20256;&#36882;&#23618;&#26469;&#27719;&#24635;&#22270;&#20687;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#22330;&#26223;&#22270;&#29983;&#25104;&#65288;SGG&#65289;&#26694;&#26550;&#32858;&#28966;&#20110;&#23398;&#20064;&#22270;&#20687;&#20013;&#22810;&#20010;&#23545;&#35937;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#30001;&#20110;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#33021;&#22815;&#27169;&#25311;&#23545;&#35937;&#19982;&#37051;&#36817;&#23545;&#35937;&#20043;&#38388;&#30340;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#65292;&#22240;&#27492;&#23427;&#20204;&#26159;SGG&#30340;&#20027;&#23548;&#34920;&#31034;&#23398;&#20064;&#27169;&#22359;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;MPNN&#30340;&#26694;&#26550;&#23558;&#22330;&#26223;&#22270;&#35270;&#20026;&#21516;&#36136;&#22270;&#65292;&#36825;&#38480;&#21046;&#20102;&#23545;&#35937;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#24615;&#65292;&#32780;&#24573;&#35270;&#20102;&#20851;&#31995;&#24448;&#24448;&#39640;&#24230;&#20381;&#36182;&#20110;&#19982;&#20851;&#31995;&#30456;&#20851;&#32852;&#30340;&#23545;&#35937;&#36825;&#19968;&#20107;&#23454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20559;&#35265;&#30340;&#24322;&#26500;&#22330;&#26223;&#22270;&#29983;&#25104;&#65288;HetSGG&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#25429;&#33719;&#20851;&#31995;&#24863;&#30693;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#28040;&#24687;&#20256;&#36882;&#23618;&#65292;&#31216;&#20026;&#20851;&#31995;&#24863;&#30693;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;RMP&#65289;&#65292;&#23427;&#32771;&#34385;&#20102;&#23545;&#35937;&#20043;&#38388;&#30340;&#35859;&#35789;&#31867;&#22411;&#65292;&#27719;&#24635;&#20102;&#22270;&#20687;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#25152;&#25552;&#20986;&#30340;HetSGG&#26694;&#26550;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#20851;&#31995;&#24863;&#30693;&#28040;&#24687;&#20256;&#36882;&#23545;&#20110;SGG&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent scene graph generation (SGG) frameworks have focused on learning complex relationships among multiple objects in an image. Thanks to the nature of the message passing neural network (MPNN) that models high-order interactions between objects and their neighboring objects, they are dominant representation learning modules for SGG. However, existing MPNN-based frameworks assume the scene graph as a homogeneous graph, which restricts the context-awareness of visual relations between objects. That is, they overlook the fact that the relations tend to be highly dependent on the objects with which the relations are associated. In this paper, we propose an unbiased heterogeneous scene graph generation (HetSGG) framework that captures relation-aware context using message passing neural networks. We devise a novel message passing layer, called relation-aware message passing neural network (RMP), that aggregates the contextual information of an image considering the predicate type between 
&lt;/p&gt;</description></item><item><title>Powderworld&#26159;&#19968;&#20010;&#30452;&#25509;&#22312;GPU&#19978;&#36816;&#34892;&#30340;&#36731;&#37327;&#32423;&#20294;&#34920;&#29616;&#21147;&#24378;&#30340;&#27169;&#25311;&#29615;&#22659;&#65292;&#29992;&#20110;&#25552;&#20379;&#27867;&#21270;&#24615;&#30340;&#30740;&#31350;&#24179;&#21488;&#65292;&#21253;&#25324;&#19990;&#30028;&#24314;&#27169;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22686;&#21152;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#21487;&#20197;&#25913;&#21892;&#19990;&#30028;&#27169;&#22411;&#21644;&#26576;&#20123;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.13051</link><description>&lt;p&gt;
Powderworld&#65306;&#36890;&#36807;&#22810;&#26679;&#21270;&#20219;&#21153;&#20998;&#24067;&#26469;&#29702;&#35299;&#27867;&#21270;&#30340;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Powderworld: A Platform for Understanding Generalization via Rich Task Distributions. (arXiv:2211.13051v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13051
&lt;/p&gt;
&lt;p&gt;
Powderworld&#26159;&#19968;&#20010;&#30452;&#25509;&#22312;GPU&#19978;&#36816;&#34892;&#30340;&#36731;&#37327;&#32423;&#20294;&#34920;&#29616;&#21147;&#24378;&#30340;&#27169;&#25311;&#29615;&#22659;&#65292;&#29992;&#20110;&#25552;&#20379;&#27867;&#21270;&#24615;&#30340;&#30740;&#31350;&#24179;&#21488;&#65292;&#21253;&#25324;&#19990;&#30028;&#24314;&#27169;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22686;&#21152;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#21487;&#20197;&#25913;&#21892;&#19990;&#30028;&#27169;&#22411;&#21644;&#26576;&#20123;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30340;&#37325;&#22823;&#25361;&#25112;&#20043;&#19968;&#26159;&#33021;&#22815;&#27867;&#21270;&#21040;&#26032;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#27867;&#21270;&#20195;&#29702;&#38656;&#35201;&#19968;&#32452;&#20016;&#23500;&#12289;&#22810;&#26679;&#21270;&#30340;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#12290;&#20026;&#36825;&#20123;&#20219;&#21153;&#35774;&#35745;&#19968;&#20010;&#29702;&#24819;&#30340;&#29615;&#22659;&#24456;&#22256;&#38590;&#8212;&#8212;&#29702;&#24819;&#30340;&#29615;&#22659;&#24212;&#25903;&#25345;&#19968;&#31995;&#21015;&#26032;&#20852;&#29616;&#35937;&#12289;&#20016;&#23500;&#30340;&#20219;&#21153;&#31354;&#38388;&#21644;&#24555;&#36895;&#30340;&#36816;&#34892;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#29942;&#39048;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Powderworld&#65292;&#19968;&#20010;&#30452;&#25509;&#22312;GPU&#19978;&#36816;&#34892;&#30340;&#36731;&#37327;&#32423;&#20294;&#34920;&#29616;&#21147;&#24378;&#30340;&#27169;&#25311;&#29615;&#22659;&#12290;&#22312;Powderworld&#20869;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#28608;&#21457;&#25361;&#25112;&#30340;&#20998;&#24067;&#65292;&#19968;&#20010;&#29992;&#20110;&#19990;&#30028;&#24314;&#27169;&#65292;&#19968;&#20010;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#12290;&#27599;&#20010;&#20998;&#24067;&#37117;&#21253;&#21547;&#25163;&#21160;&#35774;&#35745;&#30340;&#27979;&#35797;&#20219;&#21153;&#65292;&#20197;&#26816;&#26597;&#27867;&#21270;&#24615;&#33021;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22686;&#21152;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#21487;&#20197;&#25913;&#21892;&#19990;&#30028;&#27169;&#22411;&#21644;&#26576;&#20123;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#20294;&#21487;&#33021;&#20250;&#25233;&#21046;&#39640;&#26041;&#24046;&#29615;&#22659;&#19979;&#30340;&#23398;&#20064;&#12290;Powderworld&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#19968;&#31181;&#25903;&#25345;&#27867;&#21270;&#30740;&#31350;&#30340;&#29615;&#22659;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the grand challenges of reinforcement learning is the ability to generalize to new tasks. However, general agents require a set of rich, diverse tasks to train on. Designing a `foundation environment' for such tasks is tricky -- the ideal environment would support a range of emergent phenomena, an expressive task space, and fast runtime. To take a step towards addressing this research bottleneck, this work presents Powderworld, a lightweight yet expressive simulation environment running directly on the GPU. Within Powderworld, two motivating challenges distributions are presented, one for world-modelling and one for reinforcement learning. Each contains hand-designed test tasks to examine generalization. Experiments indicate that increasing the environment's complexity improves generalization for world models and certain reinforcement learning agents, yet may inhibit learning in high-variance environments. Powderworld aims to support the study of generalization by providing a so
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;&#23545;&#31216;&#32467;&#26500;&#30340;&#30693;&#35782;&#22270;&#35889;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550; KGE-SymCL&#65292;&#23427;&#33021;&#26377;&#25928;&#22320;&#25552;&#39640;&#30693;&#35782;&#22270;&#35889;&#20013;&#23454;&#20307;&#30340;&#21487;&#21306;&#20998;&#24230;&#12290;</title><link>http://arxiv.org/abs/2211.10738</link><description>&lt;p&gt;
&#22522;&#20110;&#20851;&#31995;&#23545;&#31216;&#32467;&#26500;&#30340;&#30693;&#35782;&#22270;&#35889;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Contrastive Learning Based on Relation-Symmetrical Structure. (arXiv:2211.10738v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;&#23545;&#31216;&#32467;&#26500;&#30340;&#30693;&#35782;&#22270;&#35889;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550; KGE-SymCL&#65292;&#23427;&#33021;&#26377;&#25928;&#22320;&#25552;&#39640;&#30693;&#35782;&#22270;&#35889;&#20013;&#23454;&#20307;&#30340;&#21487;&#21306;&#20998;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837; (KGE) &#26088;&#22312;&#23398;&#20064;&#24378;&#22823;&#30340;&#34920;&#31034;&#20197;&#21463;&#30410;&#20110;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#23545;&#27604;&#23398;&#20064;&#24050;&#34987;&#24191;&#27867;&#21033;&#29992;&#20110;&#22270;&#23398;&#20064;&#65292;&#20316;&#20026;&#22686;&#24378;&#25152;&#23398;&#34920;&#31034;&#30340;&#21487;&#21306;&#20998;&#33021;&#21147;&#30340;&#26377;&#25928;&#26426;&#21046;&#12290;&#28982;&#32780;&#65292;KG&#30340;&#22797;&#26434;&#32467;&#26500;&#20351;&#24471;&#26500;&#24314;&#36866;&#24403;&#30340;&#23545;&#27604;&#23545;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#25968;&#19981;&#22810;&#30340;&#20960;&#20010;&#23581;&#35797;&#23558;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#19982;KGE&#38598;&#25104;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#22823;&#22810;&#20381;&#36182;&#20110;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;Bert&#65289;&#36827;&#34892;&#23545;&#27604;&#23545;&#26500;&#24314;&#65292;&#32780;&#19981;&#26159;&#23436;&#20840;&#25366;&#25496;&#28508;&#22312;&#30340;&#22270;&#32467;&#26500;&#20449;&#24687;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#34920;&#36798;&#33021;&#21147;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20851;&#31995;&#23545;&#31216;&#32467;&#26500;&#20869;&#30340;&#23454;&#20307;&#36890;&#24120;&#30456;&#20284;&#19988;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;&#23545;&#31216;&#32467;&#26500;&#30340;&#30693;&#35782;&#22270;&#35889;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;KGE-SymCL&#65292;&#23427;&#22312;KG&#20013;&#25366;&#25496;&#23545;&#31216;&#32467;&#26500;&#20449;&#24687;&#20197;&#22686;&#24378;&#25152;&#23398;&#34920;&#31034;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding (KGE) aims at learning powerful representations to benefit various artificial intelligence applications. Meanwhile, contrastive learning has been widely leveraged in graph learning as an effective mechanism to enhance the discriminative capacity of the learned representations. However, the complex structures of KG make it hard to construct appropriate contrastive pairs. Only a few attempts have integrated contrastive learning strategies with KGE. But, most of them rely on language models ( e.g., Bert) for contrastive pair construction instead of fully mining information underlying the graph structure, hindering expressive ability. Surprisingly, we find that the entities within a relational symmetrical structure are usually similar and correlated. To this end, we propose a knowledge graph contrastive learning framework based on relation-symmetrical structure, KGE-SymCL, which mines symmetrical structure information in KGs to enhance the discriminative ability o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#38598;&#25104;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65292;&#20351;&#29992;&#36229;&#22270;&#25429;&#25417;&#31471;&#21475;&#25195;&#25551;&#25915;&#20987;&#30340;&#28436;&#21270;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#27966;&#29983;&#30340;&#24230;&#37327;&#26469;&#35757;&#32451;NIDS&#65292;&#20174;&#32780;&#20801;&#35768;&#22312;&#39640;&#31934;&#24230;&#12289;&#39640;&#20934;&#30830;&#29575;&#12289;&#39640;&#21484;&#22238;&#29575;&#24615;&#33021;&#19979;&#23454;&#26102;&#30417;&#27979;&#21644;&#26816;&#27979;&#31471;&#21475;&#25195;&#25551;&#27963;&#21160;&#12289;&#20854;&#20182;&#31867;&#22411;&#30340;&#25915;&#20987;&#21644;&#25932;&#23545;&#20837;&#20405;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;NIDS&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2211.03933</link><description>&lt;p&gt;
&#22522;&#20110;&#36229;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#38598;&#25104;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Hypergraph-Based Machine Learning Ensemble Network Intrusion Detection System. (arXiv:2211.03933v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03933
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#38598;&#25104;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65292;&#20351;&#29992;&#36229;&#22270;&#25429;&#25417;&#31471;&#21475;&#25195;&#25551;&#25915;&#20987;&#30340;&#28436;&#21270;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#27966;&#29983;&#30340;&#24230;&#37327;&#26469;&#35757;&#32451;NIDS&#65292;&#20174;&#32780;&#20801;&#35768;&#22312;&#39640;&#31934;&#24230;&#12289;&#39640;&#20934;&#30830;&#29575;&#12289;&#39640;&#21484;&#22238;&#29575;&#24615;&#33021;&#19979;&#23454;&#26102;&#30417;&#27979;&#21644;&#26816;&#27979;&#31471;&#21475;&#25195;&#25551;&#27963;&#21160;&#12289;&#20854;&#20182;&#31867;&#22411;&#30340;&#25915;&#20987;&#21644;&#25932;&#23545;&#20837;&#20405;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;NIDS&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;(NIDS)&#22312;&#26816;&#27979;&#24694;&#24847;&#25915;&#20987;&#26102;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#12290;NIDS&#36890;&#24120;&#22312;&#31163;&#32447;&#29366;&#24577;&#19979;&#24320;&#21457;&#65292;&#20294;&#38754;&#23545;&#33258;&#21160;&#29983;&#25104;&#30340;&#31471;&#21475;&#25195;&#25551;&#28183;&#36879;&#23581;&#35797;&#26102;&#65292;&#20250;&#23548;&#33268;&#20174;&#23545;&#25163;&#36866;&#24212;&#21040;NIDS&#21709;&#24212;&#30340;&#26174;&#30528;&#26102;&#38388;&#28382;&#21518;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#20197;Internet&#21327;&#35758;&#22320;&#22336;&#21644;&#30446;&#26631;&#31471;&#21475;&#20026;&#37325;&#28857;&#30340;&#36229;&#22270;&#26469;&#25429;&#25417;&#31471;&#21475;&#25195;&#25551;&#25915;&#20987;&#30340;&#28436;&#21270;&#27169;&#24335;&#12290;&#28982;&#21518;&#20351;&#29992;&#27966;&#29983;&#30340;&#22522;&#20110;&#36229;&#22270;&#30340;&#24230;&#37327;&#26469;&#35757;&#32451;&#19968;&#20010;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;(ML)&#30340;NIDS&#65292;&#20174;&#32780;&#20801;&#35768;&#22312;&#39640;&#31934;&#24230;&#12289;&#39640;&#20934;&#30830;&#29575;&#12289;&#39640;&#21484;&#22238;&#29575;&#24615;&#33021;&#19979;&#23454;&#26102;&#35843;&#25972;&#65292;&#30417;&#27979;&#21644;&#26816;&#27979;&#31471;&#21475;&#25195;&#25551;&#27963;&#21160;&#12289;&#20854;&#20182;&#31867;&#22411;&#30340;&#25915;&#20987;&#21644;&#25932;&#23545;&#20837;&#20405;&#12290;&#36825;&#20010;ML&#33258;&#36866;&#24212;&#30340;NIDS&#26159;&#36890;&#36807;&#20197;&#19979;&#20960;&#20010;&#37096;&#20998;&#30340;&#32452;&#21512;&#24320;&#21457;&#20986;&#26469;&#30340;&#65306;(1)&#20837;&#20405;&#31034;&#20363;&#65292;(2)NIDS&#26356;&#26032;&#35268;&#21017;&#65292;(3)&#35302;&#21457;NIDS&#37325;&#26032;&#35757;&#32451;&#35831;&#27714;&#30340;&#25915;&#20987;&#38408;&#20540;&#36873;&#25321;&#65292;&#20197;&#21450;(4)&#22312;&#27809;&#26377;&#20808;&#21069;&#32593;&#32476;&#24615;&#36136;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#30340;&#29983;&#20135;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network intrusion detection systems (NIDS) to detect malicious attacks continue to meet challenges. NIDS are often developed offline while they face auto-generated port scan infiltration attempts, resulting in a significant time lag from adversarial adaption to NIDS response. To address these challenges, we use hypergraphs focused on internet protocol addresses and destination ports to capture evolving patterns of port scan attacks. The derived set of hypergraph-based metrics are then used to train an ensemble machine learning (ML) based NIDS that allows for real-time adaption in monitoring and detecting port scanning activities, other types of attacks, and adversarial intrusions at high accuracy, precision and recall performances. This ML adapting NIDS was developed through the combination of (1) intrusion examples, (2) NIDS update rules, (3) attack threshold choices to trigger NIDS retraining requests, and (4) a production environment with no prior knowledge of the nature of network 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#20108;&#20803;&#27835;&#30103;&#26465;&#20214;&#19979;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#30340;&#22330;&#26223;&#20013;&#65292;&#23545;&#22240;&#26524;&#25512;&#26029;&#30340;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#21033;&#29992;&#26368;&#26032;&#30340;&#29983;&#25104;&#24314;&#27169;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#26032;&#30340;&#27169;&#22411;&#36873;&#25321;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.01939</link><description>&lt;p&gt;
&#24322;&#36136;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#20013;&#27169;&#22411;&#36873;&#25321;&#30340;&#23454;&#35777;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Empirical Analysis of Model Selection for Heterogeneous Causal Effect Estimation. (arXiv:2211.01939v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#20108;&#20803;&#27835;&#30103;&#26465;&#20214;&#19979;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#30340;&#22330;&#26223;&#20013;&#65292;&#23545;&#22240;&#26524;&#25512;&#26029;&#30340;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#21033;&#29992;&#26368;&#26032;&#30340;&#29983;&#25104;&#24314;&#27169;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#26032;&#30340;&#27169;&#22411;&#36873;&#25321;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#20108;&#20803;&#27835;&#30103;&#26465;&#20214;&#19979;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;CATE&#65289;&#20272;&#35745;&#30340;&#24773;&#20917;&#12290;&#19982;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#36873;&#25321;&#19981;&#21516;&#65292;&#30001;&#20110;&#25105;&#20204;&#26080;&#27861;&#35266;&#23519;&#21040;&#20219;&#20309;&#25968;&#25454;&#28857;&#30340;&#21453;&#20107;&#23454;&#28508;&#22312;&#32467;&#26524;&#65292;&#22240;&#27492;&#27809;&#26377;&#23436;&#32654;&#30340;&#20132;&#21449;&#39564;&#35777;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#21508;&#31181;&#20195;&#29702;&#24230;&#37327;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21462;&#20915;&#20110;&#20174;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#20013;&#20272;&#35745;&#30340;&#36741;&#21161;&#24178;&#25200;&#27169;&#22411;&#65288;&#20542;&#21521;&#24615;&#24471;&#20998;&#27169;&#22411;&#12289;&#32467;&#26524;&#22238;&#24402;&#27169;&#22411;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24230;&#37327;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#20165;&#22312;&#25105;&#20204;&#21487;&#20197;&#35775;&#38382;&#21453;&#20107;&#23454;&#25968;&#25454;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#20197;&#35780;&#20272;&#25991;&#29486;&#20013;&#20171;&#32461;&#30340;&#36825;&#20123;&#24230;&#37327;&#26041;&#27861;&#20197;&#21450;&#26412;&#30740;&#31350;&#20013;&#20171;&#32461;&#30340;&#26032;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#22312;&#23454;&#29616;&#22810;&#20010;&#36924;&#30495;&#25968;&#25454;&#38598;&#30340;&#26368;&#26032;&#29983;&#25104;&#24314;&#27169;&#36827;&#23637;&#22522;&#30784;&#19978;&#36827;&#34892;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#20102;&#26032;&#30340;&#27169;&#22411;&#36873;&#25321;&#31574;&#30053;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of model selection in causal inference, specifically for the case of conditional average treatment effect (CATE) estimation under binary treatments. Unlike model selection in machine learning, there is no perfect analogue of cross-validation as we do not observe the counterfactual potential outcome for any data point. Towards this, there have been a variety of proxy metrics proposed in the literature, that depend on auxiliary nuisance models estimated from the observed data (propensity score model, outcome regression model). However, the effectiveness of these metrics has only been studied on synthetic datasets as we can access the counterfactual data for them. We conduct an extensive empirical analysis to judge the performance of these metrics introduced in the literature, and novel ones introduced in this work, where we utilize the latest advances in generative modeling to incorporate multiple realistic datasets. Our analysis suggests novel model selection strate
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#19968;&#20010;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#29983;&#25104;&#30340;&#20195;&#30721;&#32780;&#35328;&#65292;&#27491;&#30830;&#24615;&#35780;&#20272;&#19981;&#33021;&#23436;&#20840;&#21453;&#26144;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21327;&#21161;&#31243;&#24207;&#21592;&#29983;&#25104;&#20195;&#30721;&#25152;&#25552;&#20379;&#30340;&#29983;&#20135;&#21147;&#25552;&#21319;&#65292;&#22240;&#27492;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#24230;&#37327;&#65292;&#32467;&#21512;&#20102;&#21151;&#33021;&#27491;&#30830;&#24615;&#21644;&#35821;&#27861;&#30456;&#20284;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#26356;&#22909;&#22320;&#20195;&#34920;&#30495;&#23454;&#19990;&#30028;&#30340;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2210.16494</link><description>&lt;p&gt;
&#23545;&#20110;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65292;&#31163;&#32447;&#24230;&#37327;&#21644;&#20154;&#31867;&#20215;&#20540;&#21028;&#26029;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning Offline Metrics and Human Judgments of Value for Code Generation Models. (arXiv:2210.16494v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16494
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#19968;&#20010;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#29983;&#25104;&#30340;&#20195;&#30721;&#32780;&#35328;&#65292;&#27491;&#30830;&#24615;&#35780;&#20272;&#19981;&#33021;&#23436;&#20840;&#21453;&#26144;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21327;&#21161;&#31243;&#24207;&#21592;&#29983;&#25104;&#20195;&#30721;&#25152;&#25552;&#20379;&#30340;&#29983;&#20135;&#21147;&#25552;&#21319;&#65292;&#22240;&#27492;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#24230;&#37327;&#65292;&#32467;&#21512;&#20102;&#21151;&#33021;&#27491;&#30830;&#24615;&#21644;&#35821;&#27861;&#30456;&#20284;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#26356;&#22909;&#22320;&#20195;&#34920;&#30495;&#23454;&#19990;&#30028;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#31034;&#20102;&#21327;&#21161;&#31243;&#24207;&#21592;&#29983;&#25104;&#20195;&#30721;&#30340;&#26497;&#22823;&#28508;&#21147;&#12290;&#23545;&#20110;&#36825;&#31181;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#32534;&#31243;&#30340;&#24773;&#24418;&#65292;&#25105;&#20204;&#23454;&#35777;&#23637;&#31034;&#20102;&#29983;&#25104;&#30340;&#20195;&#30721;&#26368;&#24120;&#34987;&#35780;&#20272;&#30340;&#26159;&#20854;&#21151;&#33021;&#27491;&#30830;&#24615;&#65288;&#21363;&#29983;&#25104;&#26159;&#21542;&#36890;&#36807;&#20102;&#21487;&#29992;&#30340;&#21333;&#20803;&#27979;&#35797;&#65289;&#65292;&#20294;&#26159;&#27491;&#30830;&#24615;&#24182;&#19981;&#33021;&#23436;&#20840;&#21453;&#26144;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#25552;&#20379;&#30340;&#29983;&#20135;&#29575;&#25552;&#21319;&#65288;&#20363;&#22914;&#21487;&#33021;&#20250;&#20302;&#20272;&#65289;&#12290;&#36890;&#36807;&#19968;&#39033;&#25317;&#26377;N=49&#26377;&#32463;&#39564;&#30340;&#31243;&#24207;&#21592;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#23613;&#31649;&#27491;&#30830;&#24615;&#23545;&#20110;&#39640;&#20215;&#20540;&#30340;&#29983;&#25104;&#26377;&#24456;&#22909;&#30340;&#20307;&#29616;&#65292;&#31243;&#24207;&#21592;&#20381;&#26087;&#35748;&#20026;&#19981;&#33021;&#36890;&#36807;&#21333;&#20803;&#27979;&#35797;&#30340;&#20195;&#30721;&#22914;&#26524;&#33021;&#20943;&#23569;&#23436;&#25104;&#32534;&#30721;&#20219;&#21153;&#25152;&#38656;&#30340;&#24635;&#20307;&#24037;&#20316;&#37327;&#30340;&#35805;&#65292;&#20854;&#20215;&#20540;&#20063;&#26159;&#24456;&#39640;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#24230;&#37327;&#65292;&#32467;&#21512;&#20102;&#21151;&#33021;&#27491;&#30830;&#24615;&#21644;&#35821;&#27861;&#30456;&#20284;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#33021;&#22815;&#20197;&#26356;&#24378;&#30340;&#30456;&#20851;&#24615;&#65288;14%&#65289;&#26469;&#34913;&#37327;&#20215;&#20540;&#65292;&#22240;&#27492;&#21487;&#20197;&#26356;&#22909;&#22320;&#20195;&#34920;&#22312;&#35780;&#20272;&#21644;&#27604;&#36739;&#27169;&#22411;&#26102;&#30340;&#30495;&#23454;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have demonstrated great potential to assist programmers in generating code. For such human-AI pair programming scenarios, we empirically demonstrate that while generated code is most often evaluated in terms of their functional correctness (i.e., whether generations pass available unit tests), correctness does not fully capture (e.g., may underestimate) the productivity gains these models may provide. Through a user study with N = 49 experienced programmers, we show that while correctness captures high-value generations, programmers still rate code that fails unit tests as valuable if it reduces the overall effort needed to complete a coding task. Finally, we propose a hybrid metric that combines functional correctness and syntactic similarity and show that it achieves a 14% stronger correlation with value and can therefore better represent real-world gains when evaluating and comparing models.
&lt;/p&gt;</description></item><item><title>HELP ME THINK&#26159;&#19968;&#31181;&#24110;&#21161;&#38750;&#19987;&#23478;&#29992;&#25143;&#21019;&#24314;&#23450;&#21046;&#21270;&#20869;&#23481;&#30340;&#31616;&#21333;&#25552;&#31034;&#31574;&#30053;&#65292;&#21033;&#29992;GPT3&#25552;&#20986;&#30456;&#20851;&#38382;&#39064;&#21644;&#21033;&#29992;&#29992;&#25143;&#31572;&#26696;&#25191;&#34892;&#20219;&#21153;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#38656;&#35201;&#37325;&#35201;&#24605;&#32771;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2208.08232</link><description>&lt;p&gt;
HELP ME THINK&#65306;&#19968;&#31181;&#24110;&#21161;&#38750;&#19987;&#23478;&#20351;&#29992;&#27169;&#22411;&#21019;&#24314;&#23450;&#21046;&#20869;&#23481;&#30340;&#31616;&#21333;&#25552;&#31034;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
HELP ME THINK: A Simple Prompting Strategy for Non-experts to Create Customized Content with Models. (arXiv:2208.08232v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08232
&lt;/p&gt;
&lt;p&gt;
HELP ME THINK&#26159;&#19968;&#31181;&#24110;&#21161;&#38750;&#19987;&#23478;&#29992;&#25143;&#21019;&#24314;&#23450;&#21046;&#21270;&#20869;&#23481;&#30340;&#31616;&#21333;&#25552;&#31034;&#31574;&#30053;&#65292;&#21033;&#29992;GPT3&#25552;&#20986;&#30456;&#20851;&#38382;&#39064;&#21644;&#21033;&#29992;&#29992;&#25143;&#31572;&#26696;&#25191;&#34892;&#20219;&#21153;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#38656;&#35201;&#37325;&#35201;&#24605;&#32771;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#24182;&#23450;&#21046;&#20869;&#23481;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#20986;&#20102;&#20026;&#20102;&#25552;&#20379;&#25511;&#21046;&#32780;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#24182;&#32570;&#20047;&#19968;&#33324;&#24615;&#65307;&#36825;&#20026;&#38750;&#19987;&#19994;&#29992;&#25143;&#25214;&#21040;&#36866;&#21512;&#20854;&#20219;&#21153;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#21387;&#20498;&#24615;&#30340;&#36873;&#25321;&#12290;&#36825;&#20123;&#25216;&#26415;&#25152;&#28041;&#21450;&#30340;&#24037;&#20316;&#65292;&#22914;&#32534;&#20889;&#31034;&#20363;&#12289;&#35299;&#37322;&#12289;&#25351;&#20196;&#31561;&#65292;&#36827;&#19968;&#27493;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#38750;&#19987;&#19994;&#29992;&#25143;&#20013;&#30340;&#37319;&#29992;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HELP ME THINK&#30340;&#31616;&#21333;&#25552;&#31034;&#31574;&#30053;&#65292;&#40723;&#21169;GPT3&#36890;&#36807;&#25552;&#20986;&#19968;&#32452;&#30456;&#20851;&#38382;&#39064;&#21644;&#21033;&#29992;&#29992;&#25143;&#30340;&#31572;&#26696;&#26469;&#25191;&#34892;&#20219;&#21153;&#26469;&#24110;&#21161;&#38750;&#19987;&#23478;&#29992;&#25143;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25216;&#26415;HELP ME THINK&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#21151;&#25928;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20851;&#27880;&#38590;&#20197;&#23436;&#25104;&#19988;&#38656;&#35201;&#37325;&#35201;&#24605;&#32771;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#40723;&#21169;&#24320;&#21457;&#38750;&#20256;&#32479;&#30340;&#26041;&#24335;&#26469;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21147;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controlling the text generated by language models and customizing the content has been a long-standing challenge. Existing prompting techniques proposed in pursuit of providing control are task-specific and lack generality; this provides overwhelming choices for non-expert users to find a suitable method for their task. The effort associated with those techniques, such as in writing examples, explanations, instructions, etc. further limits their adoption among non-expert users. In this paper, we propose a simple prompting strategy HELP ME THINK where we encourage GPT3 to help non-expert users by asking a set of relevant questions and leveraging user answers to execute the task. We demonstrate the efficacy of our technique HELP ME THINK on a variety of tasks. Specifically, we focus on tasks that are hard for average humans and require significant thinking to perform. We hope our work will encourage the development of unconventional ways to harness the power of large language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#32508;&#36848;&#20171;&#32461;&#20102;&#36830;&#32493;&#23398;&#20064;&#21644;&#21151;&#33021;&#32452;&#21512;&#26041;&#27861;&#30340;&#30740;&#31350;&#29616;&#29366;&#21644;&#26410;&#26469;&#32852;&#31995;&#65292;&#20197;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#22312;&#35299;&#20915;&#26032;&#38382;&#39064;&#26102;&#21487;&#20197;&#19981;&#26029;&#31215;&#32047;&#21644;&#32452;&#21512;&#30693;&#35782;&#30340;&#32456;&#36523;&#23398;&#20064;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2207.07730</link><description>&lt;p&gt;
&#22914;&#20309;&#37325;&#29992;&#21644;&#32452;&#21512;&#30693;&#35782;&#65292;&#23454;&#29616;&#32456;&#36523;&#20219;&#21153;&#23398;&#20064;&#65306;&#32508;&#36848;&#36830;&#32493;&#23398;&#20064;&#19982;&#21151;&#33021;&#32452;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
How to Reuse and Compose Knowledge for a Lifetime of Tasks: A Survey on Continual Learning and Functional Composition. (arXiv:2207.07730v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#32508;&#36848;&#20171;&#32461;&#20102;&#36830;&#32493;&#23398;&#20064;&#21644;&#21151;&#33021;&#32452;&#21512;&#26041;&#27861;&#30340;&#30740;&#31350;&#29616;&#29366;&#21644;&#26410;&#26469;&#32852;&#31995;&#65292;&#20197;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#22312;&#35299;&#20915;&#26032;&#38382;&#39064;&#26102;&#21487;&#20197;&#19981;&#26029;&#31215;&#32047;&#21644;&#32452;&#21512;&#30693;&#35782;&#30340;&#32456;&#36523;&#23398;&#20064;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#20027;&#35201;&#30446;&#26631;&#20043;&#19968;&#26159;&#21019;&#24314;&#19968;&#20010;&#33021;&#22815;&#33719;&#24471;&#23545;&#19990;&#30028;&#30340;&#26222;&#36941;&#29702;&#35299;&#30340;&#20195;&#29702;&#12290;&#36825;&#31181;&#20195;&#29702;&#38656;&#35201;&#33021;&#22815;&#19981;&#26029;&#31215;&#32047;&#21644;&#24314;&#31435;&#30693;&#35782;&#65292;&#20197;&#24212;&#23545;&#36935;&#21040;&#30340;&#26032;&#20307;&#39564;&#12290;&#32456;&#36523;&#25110;&#36830;&#32493;&#23398;&#20064;&#35299;&#20915;&#20102;&#36825;&#31181;&#24773;&#20917;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20195;&#29702;&#38754;&#23545;&#19981;&#26029;&#30340;&#38382;&#39064;&#27969;&#65292;&#24517;&#39035;&#21162;&#21147;&#25484;&#25569;&#35299;&#20915;&#27599;&#20010;&#26032;&#20219;&#21153;&#25152;&#38656;&#30340;&#30693;&#35782;&#12290;&#22914;&#26524;&#20195;&#29702;&#33021;&#22815;&#22312;&#26576;&#31181;&#32452;&#21512;&#34920;&#31034;&#24418;&#24335;&#20013;&#32047;&#31215;&#30693;&#35782;&#65292;&#21017;&#21487;&#20197;&#36873;&#25321;&#24615;&#22320;&#37325;&#29992;&#21644;&#32452;&#21512;&#30456;&#20851;&#30340;&#30693;&#35782;&#65292;&#26500;&#24314;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23613;&#31649;&#36825;&#20010;&#31616;&#21333;&#30340;&#24819;&#27861;&#20855;&#26377;&#30452;&#35266;&#30340;&#21560;&#24341;&#21147;&#65292;&#20294;&#26159;&#26377;&#20851;&#32456;&#36523;&#23398;&#20064;&#21644;&#32452;&#21512;&#23398;&#20064;&#30340;&#25991;&#29486;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#20998;&#24320;&#36827;&#34892;&#30340;&#12290;&#20026;&#20102;&#20419;&#36827;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#26412;&#25991;&#27010;&#36848;&#20102;&#23427;&#20204;&#21508;&#33258;&#30340;&#30740;&#31350;&#26223;&#35266;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#20043;&#38388;&#29616;&#26377;&#21644;&#26410;&#26469;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major goal of artificial intelligence (AI) is to create an agent capable of acquiring a general understanding of the world. Such an agent would require the ability to continually accumulate and build upon its knowledge as it encounters new experiences. Lifelong or continual learning addresses this setting, whereby an agent faces a continual stream of problems and must strive to capture the knowledge necessary for solving each new task it encounters. If the agent is capable of accumulating knowledge in some form of compositional representation, it could then selectively reuse and combine relevant pieces of knowledge to construct novel solutions. Despite the intuitive appeal of this simple idea, the literatures on lifelong learning and compositional learning have proceeded largely separately. In an effort to promote developments that bridge between the two fields, this article surveys their respective research landscapes and discusses existing and future connections between them.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37327;&#21270;&#20998;&#26512;&#20102;&#32852;&#37030;&#28798;&#38590;&#25588;&#21161;&#25919;&#31574;&#30340;&#19968;&#20010;&#31616;&#21333;&#27169;&#22411;&#65292;&#24182;&#20174;&#19977;&#20010;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#36827;&#32780;&#32771;&#34385;&#20102;3&#31181;&#25919;&#31574;&#20462;&#25913;&#21450;&#20854;&#24433;&#21709;&#65292;&#26088;&#22312;&#20026;&#21508;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#20559;&#22909;&#25490;&#24207;&#25552;&#20379;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2207.07392</link><description>&lt;p&gt;
&#19968;&#31181;&#31616;&#21333;&#30340;&#32852;&#37030;&#28798;&#38590;&#25588;&#21161;&#25919;&#31574;&#22768;&#26126;&#24615;&#27169;&#22411;&#8212;&#8212;&#36879;&#26126;&#24230;&#30340;&#24314;&#27169;&#21644;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
A simple declarative model of the Federal Disaster Assistance Policy -- modelling and measuring transparency. (arXiv:2207.07392v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07392
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37327;&#21270;&#20998;&#26512;&#20102;&#32852;&#37030;&#28798;&#38590;&#25588;&#21161;&#25919;&#31574;&#30340;&#19968;&#20010;&#31616;&#21333;&#27169;&#22411;&#65292;&#24182;&#20174;&#19977;&#20010;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#36827;&#32780;&#32771;&#34385;&#20102;3&#31181;&#25919;&#31574;&#20462;&#25913;&#21450;&#20854;&#24433;&#21709;&#65292;&#26088;&#22312;&#20026;&#21508;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#20559;&#22909;&#25490;&#24207;&#25552;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#19977;&#20010;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#35282;&#24230;&#65292;&#23545;&#32852;&#37030;&#28798;&#38590;&#25588;&#21161;&#25919;&#31574;&#30340;&#19968;&#20010;&#31616;&#21333;&#27169;&#22411;&#36827;&#34892;&#20102;&#25968;&#37327;&#20998;&#26512;&#12290;&#36825;&#31181;&#25968;&#37327;&#20998;&#26512;&#26041;&#27861;&#26159;&#26032;&#30340;&#65292;&#22312;&#20854;&#20182;&#39046;&#22495;&#22914;&#19994;&#21153;&#21644;&#21307;&#30103;&#27969;&#31243;&#20013;&#20063;&#26377;&#24212;&#29992;&#12290;&#21033;&#30410;&#30456;&#20851;&#32773;&#23545;&#36807;&#31243;&#36879;&#26126;&#24230;&#24456;&#24863;&#20852;&#36259;&#65292;&#20294;&#27599;&#20010;&#20154;&#23545;&#36879;&#26126;&#24230;&#30340;&#20855;&#20307;&#23450;&#20041;&#37117;&#26377;&#19981;&#21516;&#30340;&#30475;&#27861;&#12290;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#19977;&#31181;&#32852;&#37030;&#28798;&#38590;&#25588;&#21161;&#25919;&#31574;&#30340;&#20462;&#25913;&#65292;&#24182;&#20998;&#26512;&#20102;&#20174;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#35282;&#24230;&#30475;&#65292;&#27599;&#31181;&#25919;&#31574;&#19979;&#21033;&#30410;&#30456;&#20851;&#32773;&#28385;&#24847;&#24230;&#30340;&#21464;&#21270;&#24773;&#20917;&#12290;&#36825;&#31181;&#20998;&#26512;&#34987;&#29992;&#26469;&#23545;&#22235;&#31181;&#25919;&#31574;&#30340;&#20559;&#22909;&#36827;&#34892;&#25490;&#24207;&#65292;&#20197;&#20415;&#32771;&#34385;&#21040;&#25152;&#26377;&#38598;&#20307;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we will provide a quantitative analysis of a simple model of the Federal Disaster Assistance policy from the viewpoint of three different stakeholders. This quantitative methodology is new and has applications to other areas such as business and healthcare processes. The stakeholders are interested in process transparency but each has a different opinion on precisely what constitutes transparency. We will also consider three modifications to the Federal Disaster Assistance policy and analyse, from a stakeholder viewpoint, how stakeholder satisfaction changes from process to process. This analysis is used to rank the favourability of four policies with respect to all collective stakeholder preferences.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#21487;&#35299;&#37322;&#24615;&#26234;&#33021;&#23398;&#20064;&#35786;&#26029;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#21644;&#22522;&#20110;&#24515;&#29702;&#27979;&#37327;&#30340;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20351;&#24471;&#22312;&#26234;&#24935;&#25945;&#32946;&#20013;&#33021;&#22815;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2207.03122</link><description>&lt;p&gt;
&#26234;&#33021;&#21270;&#23398;&#20064;&#35786;&#26029;&#32479;&#19968;&#21487;&#35299;&#37322;&#24615;&#26694;&#26550;&#22312;&#26234;&#24935;&#25945;&#32946;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Unified Interpretable Intelligent Learning Diagnosis Framework for Smart Education. (arXiv:2207.03122v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.03122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#21487;&#35299;&#37322;&#24615;&#26234;&#33021;&#23398;&#20064;&#35786;&#26029;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#21644;&#22522;&#20110;&#24515;&#29702;&#27979;&#37327;&#30340;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20351;&#24471;&#22312;&#26234;&#24935;&#25945;&#32946;&#20013;&#33021;&#22815;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#23398;&#20064;&#35786;&#26029;&#26159;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#37325;&#35201;&#24341;&#25806;&#65292;&#26088;&#22312;&#20272;&#35745;&#23398;&#20064;&#32773;&#24403;&#21069;&#30340;&#30693;&#35782;&#25484;&#25569;&#29366;&#20917;&#24182;&#39044;&#27979;&#20854;&#26410;&#26469;&#30340;&#23398;&#20064;&#34920;&#29616;&#12290;&#20256;&#32479;&#30340;&#23398;&#20064;&#35786;&#26029;&#26041;&#27861;&#22312;&#35786;&#26029;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#38590;&#20197;&#24179;&#34913;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#22522;&#20110;&#24515;&#29702;&#27979;&#37327;&#30340;&#23398;&#20064;&#35786;&#26029;&#26041;&#27861;&#36890;&#36807;&#35748;&#30693;&#21442;&#25968;&#25552;&#20379;&#20102;&#19968;&#23450;&#39046;&#22495;&#35299;&#37322;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#22823;&#35268;&#27169;&#23398;&#20064;&#25968;&#25454;&#30340;&#24314;&#27169;&#33021;&#21147;&#19981;&#36275;&#12290;&#32780;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23398;&#20064;&#35786;&#26029;&#26041;&#27861;&#34429;&#28982;&#25552;&#39640;&#20102;&#23398;&#20064;&#34920;&#29616;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#20854;&#20869;&#22312;&#30340;&#40657;&#30418;&#29305;&#24615;&#23548;&#33268;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#20351;&#24471;&#22312;&#25945;&#32946;&#24212;&#29992;&#20013;&#26080;&#27861;&#20449;&#20219;&#20854;&#32467;&#26524;&#12290;&#20026;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#32479;&#19968;&#21487;&#35299;&#37322;&#26234;&#33021;&#23398;&#20064;&#35786;&#26029;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#24378;&#22823;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#21644;&#22522;&#20110;&#24515;&#29702;&#27979;&#37327;&#30340;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#26694;&#26550;&#30001;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35786;&#26029;&#27169;&#22359;&#21644;&#35748;&#30693;&#21442;&#25968;&#35299;&#37322;&#27169;&#22359;&#20004;&#37096;&#20998;&#32452;&#25104;&#65292;&#20004;&#20010;&#27169;&#22359;&#30456;&#20114;&#34917;&#20805;&#65292;&#20351;&#24471;&#26694;&#26550;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#36798;&#21040;&#24179;&#34913;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#23398;&#20064;&#35786;&#26029;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent learning diagnosis is a critical engine of intelligent tutoring systems, which aims to estimate learners' current knowledge mastery status and predict their future learning performance. The significant challenge with traditional learning diagnosis methods is the inability to balance diagnostic accuracy and interpretability. Although the existing psychometric-based learning diagnosis methods provide some domain interpretation through cognitive parameters, they have insufficient modeling capability with a shallow structure for large-scale learning data. While the deep learning-based learning diagnosis methods have improved the accuracy of learning performance prediction, their inherent black-box properties lead to a lack of interpretability, making their results untrustworthy for educational applications. To settle the above problem, the proposed unified interpretable intelligent learning diagnosis framework, which benefits from the powerful representation learning ability of
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22312;&#24191;&#20041;&#21338;&#24328;&#20013;&#36861;&#38543;&#32773;&#36890;&#36807;&#34394;&#25253;&#25910;&#30410;&#20989;&#25968;&#23454;&#29616;&#26368;&#20248;&#25805;&#32437;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#31639;&#27861;&#26469;&#35745;&#31639;&#26368;&#20339;&#25805;&#32437;&#12290;</title><link>http://arxiv.org/abs/2206.13119</link><description>&lt;p&gt;
&#20851;&#20110;&#24191;&#20041;&#21338;&#24328;&#20013;&#23545;&#31574;&#30053;&#25215;&#35834;&#26368;&#20339;&#31169;&#20154;&#25910;&#30410;&#25805;&#32437;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Optimal Private Payoff Manipulation against Commitment in Extensive-form Games. (arXiv:2206.13119v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.13119
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22312;&#24191;&#20041;&#21338;&#24328;&#20013;&#36861;&#38543;&#32773;&#36890;&#36807;&#34394;&#25253;&#25910;&#30410;&#20989;&#25968;&#23454;&#29616;&#26368;&#20248;&#25805;&#32437;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#31639;&#27861;&#26469;&#35745;&#31639;&#26368;&#20339;&#25805;&#32437;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#21033;&#29992;&#31574;&#30053;&#25215;&#35834;&#36825;&#19968;&#28216;&#25103;&#31574;&#30053;&#65292;&#39046;&#23548;&#32773;&#24517;&#39035;&#23398;&#20064;&#36275;&#22815;&#30340;&#20851;&#20110;&#36861;&#38543;&#32773;&#30340;&#25910;&#30410;&#20989;&#25968;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#32473;&#20102;&#36861;&#38543;&#32773;&#25552;&#20379;&#34394;&#20551;&#20449;&#24687;&#24182;&#24433;&#21709;&#26368;&#32456;&#28216;&#25103;&#32467;&#26524;&#30340;&#26426;&#20250;&#12290;&#36890;&#36807;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#25910;&#30410;&#20989;&#25968;&#65292;&#34394;&#25253;&#32473;&#23398;&#20064;&#39046;&#23548;&#32773;&#65292;&#36861;&#38543;&#32773;&#21487;&#20197;&#24341;&#23548;&#19968;&#20010;&#23545;&#20182;&#26377;&#26356;&#22810;&#22909;&#22788;&#30340;&#32467;&#26524;&#65292;&#19982;&#20182;&#30495;&#23454;&#34892;&#20026;&#26102;&#30340;&#32467;&#26524;&#30456;&#27604;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#24191;&#20041;&#21338;&#24328;&#20013;&#36890;&#36807;&#36825;&#31181;&#31574;&#30053;&#34892;&#20026;&#23454;&#29616;&#26368;&#20339;&#25805;&#32437;&#30340;&#36861;&#38543;&#32773;&#12290;&#25105;&#20204;&#32771;&#34385;&#21040;&#20102;&#36861;&#38543;&#32773;&#19981;&#21516;&#30340;&#24577;&#24230;&#12290;&#20048;&#35266;&#30340;&#36861;&#38543;&#32773;&#20026;&#20102;&#33719;&#24471;&#30495;&#27491;&#30340;&#20844;&#29992;&#20107;&#19994;&#65292;&#26368;&#22823;&#21270;&#20102;&#25152;&#26377;&#21487;&#20197;&#36890;&#36807;&#26576;&#20123;&#25910;&#30410;&#20989;&#25968;&#24341;&#23548;&#20986;&#30340;&#28216;&#25103;&#32467;&#26524;&#12290;&#24754;&#35266;&#30340;&#36861;&#38543;&#32773;&#21482;&#32771;&#34385;&#24341;&#23548;&#20986;&#19968;&#31181;&#24799;&#19968;&#28216;&#25103;&#32467;&#26524;&#30340;&#34394;&#25253;&#25910;&#30410;&#20989;&#25968;&#12290;&#23545;&#20110;&#26412;&#25991;&#20013;&#32771;&#34385;&#30340;&#25152;&#26377;&#35774;&#32622;&#65292;&#25105;&#20204;&#37117;&#25551;&#36848;&#20102;&#21487;&#20197;&#25104;&#21151;&#24341;&#23548;&#30340;&#25152;&#26377;&#21487;&#33021;&#28216;&#25103;&#32467;&#26524;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26816;&#26597;&#19968;&#20010;&#32467;&#26524;&#26159;&#21542;&#21487;&#20197;&#30001;&#36861;&#38543;&#32773;&#22312;&#26576;&#20123;&#32473;&#23450;&#28216;&#25103;&#35774;&#23450;&#19979;&#26368;&#20248;&#22320;&#24341;&#23548;&#30340;&#22797;&#26434;&#24230;&#26159;&#22810;&#39033;&#24335;&#21487;&#35745;&#31639;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#31639;&#27861;&#26469;&#35745;&#31639;&#26368;&#20339;&#25805;&#32437;&#12290;
&lt;/p&gt;
&lt;p&gt;
To take advantage of strategy commitment, a useful tactic of playing games, a leader must learn enough information about the follower's payoff function. However, this leaves the follower a chance to provide fake information and influence the final game outcome. Through a carefully contrived payoff function misreported to the learning leader, the follower may induce an outcome that benefits him more, compared to the ones when he truthfully behaves.  We study the follower's optimal manipulation via such strategic behaviors in extensive-form games. Followers' different attitudes are taken into account. An optimistic follower maximizes his true utility among all game outcomes that can be induced by some payoff function. A pessimistic follower only considers misreporting payoff functions that induce a unique game outcome. For all the settings considered in this paper, we characterize all the possible game outcomes that can be induced successfully. We show that it is polynomial-time tractabl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#22312;&#32473;&#23450;&#19968;&#20123;&#36793;&#30340;&#26041;&#21521;&#24050;&#30830;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#35745;&#31639;&#39532;&#23572;&#31185;&#22827;&#31561;&#20215;&#31867;&#20013;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#25968;&#37327;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#25968;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36816;&#34892;&#26102;&#38388;&#21463;&#21040;&#19968;&#20010;&#22810;&#39033;&#24335;&#30340;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2206.06744</link><description>&lt;p&gt;
&#19982;&#32972;&#26223;&#30693;&#35782;&#19968;&#33268;&#30340;&#35745;&#25968;&#39532;&#23572;&#31185;&#22827;&#31561;&#20215;&#26377;&#21521;&#26080;&#29615;&#22270;
&lt;/p&gt;
&lt;p&gt;
Counting Markov Equivalent Directed Acyclic Graphs Consistent with Background Knowledge. (arXiv:2206.06744v2 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#22312;&#32473;&#23450;&#19968;&#20123;&#36793;&#30340;&#26041;&#21521;&#24050;&#30830;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#35745;&#31639;&#39532;&#23572;&#31185;&#22827;&#31561;&#20215;&#31867;&#20013;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#25968;&#37327;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#25968;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36816;&#34892;&#26102;&#38388;&#21463;&#21040;&#19968;&#20010;&#22810;&#39033;&#24335;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;Wienobst&#12289;Bannach&#21644;Liskiewicz&#65288;AAAI 2021&#65289;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#39532;&#23572;&#31185;&#22827;&#31561;&#20215;&#31867;&#20013;&#26377;&#21521;&#26080;&#29615;&#22270;&#25968;&#37327;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31934;&#30830;&#31639;&#27861;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#26356;&#19968;&#33324;&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#19968;&#20123;&#36793;&#30340;&#26041;&#21521;&#24050;&#30830;&#23450;&#30340;&#24773;&#20917;&#19979;&#35745;&#31639;&#39532;&#23572;&#31185;&#22827;&#31561;&#20215;&#31867;&#20013;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#25968;&#37327;&#65288;&#20363;&#22914;&#65292;&#22312;&#37096;&#20998;&#24178;&#39044;&#25968;&#25454;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#20986;&#29616;&#36825;&#31181;&#35774;&#32622;&#65289;&#12290;&#26089;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20010;&#38382;&#39064;&#22312;&#22797;&#26434;&#29702;&#35770;&#19978;&#26159;&#22256;&#38590;&#30340;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#35813;&#38382;&#39064;&#22312;&#19968;&#20010;&#26377;&#36259;&#30340;&#23454;&#20363;&#31867;&#20013;&#20173;&#28982;&#26159;&#21487;&#22788;&#29702;&#30340;&#65292;&#24182;&#24314;&#31435;&#20102;&#23427;&#30340;&#8220;&#22266;&#23450;&#21442;&#25968;&#21487;&#22788;&#29702;&#24615;&#8221;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#35745;&#25968;&#31639;&#27861;&#36816;&#34892;&#26102;&#38388;&#21463;&#21040;&#19968;&#20010;&#22810;&#39033;&#24335;&#30340;&#32422;&#26463;&#65292;&#24182;&#19988;&#36825;&#20010;&#22810;&#39033;&#24335;&#30340;&#24230;&#25968;\emph{&#19981;}&#20381;&#36182;&#20110;&#25552;&#20379;&#30340;&#38468;&#21152;&#36793;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
A polynomial-time exact algorithm for counting the number of directed acyclic graphs in a Markov equivalence class was recently given by Wien\"obst, Bannach, and Li\'skiewicz (AAAI 2021). In this paper, we consider the more general problem of counting the number of directed acyclic graphs in a Markov equivalence class when the directions of some of the edges are also fixed (this setting arises, for example, when interventional data is partially available). This problem has been shown in earlier work to be complexity-theoretically hard. In contrast, we show that the problem is nevertheless tractable in an interesting class of instances, by establishing that it is ``fixed-parameter tractable''. In particular, our counting algorithm runs in time that is bounded by a polynomial in the size of the graph, where the degree of the polynomial does \emph{not} depend upon the number of additional edges provided as input.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;STJGCN&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#20010;&#26410;&#26469;&#26102;&#38388;&#27493;&#38271;&#19978;&#20934;&#30830;&#39044;&#27979;&#36947;&#36335;&#32593;&#32476;&#19978;&#30340;&#20132;&#36890;&#27969;&#37327;&#12290;&#35813;&#26041;&#27861;&#26500;&#24314;&#20102;&#39044;&#23450;&#20041;&#30340;&#21644;&#33258;&#36866;&#24212;&#30340;&#26102;&#31354;&#32852;&#21512;&#22270;&#20197;&#21450;&#22312;STJGs&#19978;&#25191;&#34892;&#22270;&#21367;&#31215;&#36816;&#31639;&#65292;&#20197;&#25552;&#21462;&#26102;&#31354;&#29305;&#24449;&#24182;&#36827;&#34892;&#22810;&#27493;&#39044;&#27979;&#65292;&#24182;&#24050;&#22312;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2111.13684</link><description>&lt;p&gt;
&#26102;&#31354;&#32852;&#21512;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Spatio-Temporal Joint Graph Convolutional Networks for Traffic Forecasting. (arXiv:2111.13684v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.13684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;STJGCN&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#20010;&#26410;&#26469;&#26102;&#38388;&#27493;&#38271;&#19978;&#20934;&#30830;&#39044;&#27979;&#36947;&#36335;&#32593;&#32476;&#19978;&#30340;&#20132;&#36890;&#27969;&#37327;&#12290;&#35813;&#26041;&#27861;&#26500;&#24314;&#20102;&#39044;&#23450;&#20041;&#30340;&#21644;&#33258;&#36866;&#24212;&#30340;&#26102;&#31354;&#32852;&#21512;&#22270;&#20197;&#21450;&#22312;STJGs&#19978;&#25191;&#34892;&#22270;&#21367;&#31215;&#36816;&#31639;&#65292;&#20197;&#25552;&#21462;&#26102;&#31354;&#29305;&#24449;&#24182;&#36827;&#34892;&#22810;&#27493;&#39044;&#27979;&#65292;&#24182;&#24050;&#22312;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#23558;&#20132;&#36890;&#39044;&#27979;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#26102;&#31354;&#22270;&#24314;&#27169;&#38382;&#39064;&#12290;&#36890;&#24120;&#65292;&#30740;&#31350;&#20154;&#21592;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#26500;&#24314;&#19968;&#20010;&#38745;&#24577;&#30340;&#31354;&#38388;&#22270;&#65292;&#28982;&#21518;&#29992;&#30456;&#37051;&#26102;&#38388;&#27493;&#20043;&#38388;&#30340;&#33410;&#28857;&#36830;&#25509;&#34920;&#31034;&#20026;&#26102;&#31354;&#22270;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#26410;&#33021;&#26126;&#30830;&#21453;&#26144;&#19981;&#21516;&#26102;&#38388;&#27493;&#20043;&#38388;&#19981;&#21516;&#33410;&#28857;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#22312;&#19981;&#21516;&#26102;&#38388;&#27493;&#20043;&#38388;&#20351;&#29992;&#30456;&#21516;&#30340;&#37051;&#25509;&#30697;&#38453;&#26469;&#24573;&#30053;&#33410;&#28857;&#20043;&#38388;&#30340;&#21160;&#24577;&#26102;&#31354;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#26102;&#31354;&#32852;&#21512;&#22270;&#21367;&#31215;&#32593;&#32476; (STJGCN) &#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#20010;&#26410;&#26469;&#26102;&#38388;&#27493;&#39588;&#19978;&#20934;&#30830;&#39044;&#27979;&#36947;&#36335;&#32593;&#32476;&#19978;&#30340;&#20132;&#36890;&#27969;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#26500;&#24314;&#39044;&#23450;&#20041;&#30340;&#21644;&#33258;&#36866;&#24212;&#30340;&#26102;&#31354;&#32852;&#21512;&#22270; (STJGs) &#20197;&#21450;&#22312; STJGs &#19978;&#25191;&#34892;&#22270;&#21367;&#31215;&#36816;&#31639;&#20197;&#25552;&#21462;&#26102;&#31354;&#29305;&#24449;&#24182;&#36827;&#34892;&#22810;&#27493;&#39044;&#27979;&#12290;&#22312;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have shifted their focus towards formulating traffic forecasting as a spatio-temporal graph modeling problem. Typically, they constructed a static spatial graph at each time step and then connected each node with itself between adjacent time steps to create a spatio-temporal graph. However, this approach failed to explicitly reflect the correlations between different nodes at different time steps, thus limiting the learning capability of graph neural networks. Additionally, those models overlooked the dynamic spatio-temporal correlations among nodes by using the same adjacency matrix across different time steps. To address these limitations, we propose a novel approach called Spatio-Temporal Joint Graph Convolutional Networks (STJGCN) for accurate traffic forecasting on road networks over multiple future time steps. Specifically, our method encompasses the construction of both pre-defined and adaptive spatio-temporal joint graphs (STJGs) between any two time steps, which
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#22270;&#34920;&#31034;&#21644;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#24320;&#25918;&#19990;&#30028;&#29305;&#24449;&#22806;&#25512;&#38382;&#39064;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#20351;&#29992;&#20004;&#31181;&#35757;&#32451;&#31574;&#30053;&#26469;&#23454;&#29616;&#23545;&#26032;&#29305;&#24449;&#30340;&#22806;&#25512;&#65292;&#24182;&#32531;&#35299;&#29305;&#24449;&#23618;&#38754;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2110.04514</link><description>&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#29305;&#24449;&#22806;&#25512;&#38382;&#39064;&#30340;&#24402;&#32435;&#22270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Open-World Feature Extrapolation: An Inductive Graph Learning Approach. (arXiv:2110.04514v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.04514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#22270;&#34920;&#31034;&#21644;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#24320;&#25918;&#19990;&#30028;&#29305;&#24449;&#22806;&#25512;&#38382;&#39064;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#20351;&#29992;&#20004;&#31181;&#35757;&#32451;&#31574;&#30053;&#26469;&#23454;&#29616;&#23545;&#26032;&#29305;&#24449;&#30340;&#22806;&#25512;&#65292;&#24182;&#32531;&#35299;&#29305;&#24449;&#23618;&#38754;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#24320;&#25918;&#19990;&#30028;&#29305;&#24449;&#22806;&#25512;&#38382;&#39064;&#65292;&#20854;&#20013;&#36755;&#20837;&#25968;&#25454;&#30340;&#29305;&#24449;&#31354;&#38388;&#32463;&#36807;&#25193;&#23637;&#65292;&#22312;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#29305;&#24449;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#38656;&#35201;&#22788;&#29702;&#27979;&#35797;&#25968;&#25454;&#20013;&#30340;&#26032;&#29305;&#24449;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#29992;&#22270;&#34920;&#31034;&#21644;&#23398;&#20064;&#30340;&#23398;&#20064;&#33539;&#24335;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#21547;&#20004;&#20010;&#27169;&#22359;&#65306;1&#65289;&#39592;&#24178;&#32593;&#32476;&#20316;&#20026;&#36739;&#20302;&#30340;&#27169;&#22411;&#65292;&#23558;&#29305;&#24449;&#20316;&#20026;&#36755;&#20837;&#24182;&#36755;&#20986;&#39044;&#27979;&#30340;&#26631;&#31614;&#65307;2&#65289;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#36739;&#39640;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#20174;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#26500;&#24314;&#30340;&#29305;&#24449;-&#25968;&#25454;&#22270;&#19978;&#36827;&#34892;&#28040;&#24687;&#20256;&#36882;&#65292;&#23398;&#20064;&#22806;&#25512;&#26032;&#29305;&#24449;&#30340;&#23884;&#20837;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#35757;&#32451;&#31574;&#30053;&#65292;&#19968;&#31181;&#26159;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#21478;&#19968;&#31181;&#26159;&#24402;&#32435;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#36171;&#20104;&#27169;&#22411;&#22806;&#25512;&#33021;&#21147;&#24182;&#32531;&#35299;&#29305;&#24449;&#23618;&#38754;&#30340;&#36807;&#25311;&#21512;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We target open-world feature extrapolation problem where the feature space of input data goes through expansion and a model trained on partially observed features needs to handle new features in test data without further retraining. The problem is of much significance for dealing with features incrementally collected from different fields. To this end, we propose a new learning paradigm with graph representation and learning. Our framework contains two modules: 1) a backbone network (e.g., feedforward neural nets) as a lower model takes features as input and outputs predicted labels; 2) a graph neural network as an upper model learns to extrapolate embeddings for new features via message passing over a feature-data graph built from observed data. Based on our framework, we design two training strategies, a self-supervised approach and an inductive learning approach, to endow the model with extrapolation ability and alleviate feature-level over-fitting. We also provide theoretical analy
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#23436;&#25972;&#25968;&#25454;&#25554;&#34917;&#30340;&#23545;&#25239;&#32593;&#32476;&#65288;IDIAN&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#30340;&#19981;&#23436;&#25972;&#30446;&#26631;&#39046;&#22495;&#19979;&#30340;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#36328;&#22495;&#21644;&#23454;&#38469;&#36866;&#24212;&#20219;&#21153;&#20013;&#37117;&#20855;&#26377;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2012.01606</link><description>&lt;p&gt;
&#19981;&#23436;&#25972;&#30446;&#26631;&#39046;&#22495;&#19979;&#30340;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptation with Incomplete Target Domains. (arXiv:2012.01606v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.01606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#23436;&#25972;&#25968;&#25454;&#25554;&#34917;&#30340;&#23545;&#25239;&#32593;&#32476;&#65288;IDIAN&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#30340;&#19981;&#23436;&#25972;&#30446;&#26631;&#39046;&#22495;&#19979;&#30340;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#36328;&#22495;&#21644;&#23454;&#38469;&#36866;&#24212;&#20219;&#21153;&#20013;&#37117;&#20855;&#26377;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22495;&#33258;&#36866;&#24212;&#26159;&#36890;&#36807;&#21033;&#29992;&#36741;&#21161;&#28304;&#22495;&#20013;&#30340;&#29616;&#26377;&#26631;&#35760;&#25968;&#25454;&#26469;&#20943;&#23569;&#30446;&#26631;&#22495;&#27880;&#37322;&#25104;&#26412;&#30340;&#20219;&#21153;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#30740;&#31350;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#30340;&#22495;&#33258;&#36866;&#24212;&#20551;&#35774;&#20004;&#20010;&#22495;&#20013;&#37117;&#26377;&#23436;&#20840;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#65292;&#32780;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#32570;&#23569;&#25968;&#25454;&#30340;&#23384;&#22312;&#26159;&#26222;&#36941;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22788;&#29702;&#20102;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#22495;&#33258;&#36866;&#24212;&#24773;&#26223;&#65292;&#21363;&#20855;&#26377;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#30340;&#19981;&#23436;&#25972;&#30446;&#26631;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#23436;&#25972;&#25968;&#25454;&#25554;&#34917;&#30340;&#23545;&#25239;&#32593;&#32476;&#65288;IDIAN&#65289;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#19968;&#26032;&#30340;&#22495;&#33258;&#36866;&#24212;&#25361;&#25112;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25968;&#25454;&#25554;&#34917;&#27169;&#22359;&#26469;&#22635;&#34917;&#22522;&#20110;&#30446;&#26631;&#22495;&#23616;&#37096;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#29305;&#24449;&#20540;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#23545;&#25239;&#36866;&#24212;&#26469;&#23545;&#40784;&#20004;&#20010;&#22495;&#12290;&#25105;&#20204;&#22312;&#36328;&#22495;&#22522;&#20934;&#20219;&#21153;&#21644;&#20855;&#26377;&#19981;&#23436;&#25972;&#30446;&#26631;&#39046;&#22495;&#30340;&#23454;&#38469;&#36866;&#24212;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;IDIAN&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain adaptation, as a task of reducing the annotation cost in a target domain by exploiting the existing labeled data in an auxiliary source domain, has received a lot of attention in the research community. However, the standard domain adaptation has assumed perfectly observed data in both domains, while in real world applications the existence of missing data can be prevalent. In this paper, we tackle a more challenging domain adaptation scenario where one has an incomplete target domain with partially observed data. We propose an Incomplete Data Imputation based Adversarial Network (IDIAN) model to address this new domain adaptation challenge. In the proposed model, we design a data imputation module to fill the missing feature values based on the partial observations in the target domain, while aligning the two domains via deep adversarial adaption. We conduct experiments on both cross-domain benchmark tasks and a real world adaptation task with imperfect target domains. The expe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#21333;&#36712;&#21015;&#36710;&#35843;&#24230;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#28145;&#24230;Q&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#25968;&#20540;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2009.00433</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#21333;&#36712;&#21015;&#36710;&#35843;&#24230;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving the single-track train scheduling problem via Deep Reinforcement Learning. (arXiv:2009.00433v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.00433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#21333;&#36712;&#21015;&#36710;&#35843;&#24230;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#28145;&#24230;Q&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#25968;&#20540;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#22825;&#65292;&#38081;&#36335;&#37117;&#20250;&#36935;&#21040;&#32593;&#32476;&#21644;&#36710;&#38431;&#26041;&#38754;&#30340;&#24178;&#25200;&#21644;&#30772;&#22351;&#65292;&#24433;&#21709;&#38081;&#36335;&#20132;&#36890;&#30340;&#31283;&#23450;&#24615;&#12290;&#23548;&#33268;&#30340;&#24310;&#35823;&#20250;&#22312;&#32593;&#32476;&#20013;&#20256;&#25773;&#65292;&#23548;&#33268;&#36135;&#29289;&#21644;&#20056;&#23458;&#30340;&#38656;&#27714;&#21644;&#20379;&#24212;&#22833;&#34913;&#65292;&#36827;&#32780;&#23548;&#33268;&#26381;&#21153;&#36136;&#37327;&#19979;&#38477;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20154;&#24037;&#20132;&#36890;&#25511;&#21046;&#21592;&#65288;&#25152;&#35859;&#30340;&#35843;&#24230;&#21592;&#65289;&#30340;&#32844;&#36131;&#26159;&#23613;&#21147;&#26368;&#23567;&#21270;&#20854;&#23545;&#20132;&#36890;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#35843;&#24230;&#21592;&#19981;&#21487;&#36991;&#20813;&#22320;&#23545;&#20854;&#20915;&#31574;&#25152;&#20135;&#29983;&#30340;&#36830;&#38145;&#21453;&#24212;&#30340;&#24863;&#30693;&#28145;&#24230;&#26377;&#38480;&#65292;&#23588;&#20854;&#26159;&#23427;&#20204;&#22914;&#20309;&#24433;&#21709;&#36229;&#20986;&#20854;&#30452;&#25509;&#25511;&#21046;&#33539;&#22260;&#30340;&#32593;&#32476;&#21306;&#22495;&#12290;&#36817;&#24180;&#26469;&#65292;&#20915;&#31574;&#31185;&#23398;&#39046;&#22495;&#19968;&#30452;&#33268;&#21147;&#20110;&#24320;&#21457;&#33258;&#21160;&#35299;&#20915;&#38382;&#39064;&#21644;&#25903;&#25345;&#35843;&#24230;&#21592;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#28145;&#24230;Q&#23398;&#20064;&#26041;&#27861;&#65288;&#20998;&#25955;&#21644;&#38598;&#20013;&#65289;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Every day, railways experience disturbances and disruptions, both on the network and the fleet side, that affect the stability of rail traffic. Induced delays propagate through the network, which leads to a mismatch in demand and offer for goods and passengers, and, in turn, to a loss in service quality. In these cases, it is the duty of human traffic controllers, the so-called dispatchers, to do their best to minimize the impact on traffic. However, dispatchers inevitably have a limited depth of perception of the knock-on effect of their decisions, particularly how they affect areas of the network that are outside their direct control. In recent years, much work in Decision Science has been devoted to developing methods to solve the problem automatically and support the dispatchers in this challenging task. This paper investigates Machine Learning-based methods for tackling this problem, proposing two different Deep Q-Learning methods(Decentralized and Centralized). Numerical results 
&lt;/p&gt;</description></item></channel></rss>