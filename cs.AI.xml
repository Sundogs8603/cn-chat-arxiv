<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20998;&#21035;&#35299;&#20915;&#20102;&#26053;&#34892;&#36141;&#20080;&#32773;&#38382;&#39064;&#20013;&#30340;&#36335;&#30001;&#26500;&#24314;&#21644;&#36141;&#20080;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#20174;&#20840;&#23616;&#35282;&#24230;&#35780;&#20272;&#21644;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2404.02476</link><description>&lt;p&gt;
&#29992;&#20110;&#26053;&#34892;&#36141;&#20080;&#32773;&#38382;&#39064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Traveling Purchaser Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02476
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20998;&#21035;&#35299;&#20915;&#20102;&#26053;&#34892;&#36141;&#20080;&#32773;&#38382;&#39064;&#20013;&#30340;&#36335;&#30001;&#26500;&#24314;&#21644;&#36141;&#20080;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#20174;&#20840;&#23616;&#35282;&#24230;&#35780;&#20272;&#21644;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26053;&#34892;&#36141;&#20080;&#32773;&#38382;&#39064;&#65288;TPP&#65289;&#26159;&#19968;&#31181;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#37325;&#35201;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20998;&#21035;&#35299;&#20915;&#20102;&#36335;&#30001;&#26500;&#24314;&#21644;&#36141;&#20080;&#35268;&#21010;&#38382;&#39064;&#65292;&#21516;&#26102;&#20174;&#20840;&#23616;&#35282;&#24230;&#35780;&#20272;&#21644;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#21253;&#25324;&#29992;&#20110;&#25429;&#25417;&#24066;&#22330;-&#20135;&#21697;&#20851;&#31995;&#30340;TPP&#30340;&#20108;&#37096;&#22270;&#34920;&#31034;&#65292;&#20197;&#21450;&#20174;&#20108;&#37096;&#22270;&#20013;&#25552;&#21462;&#20449;&#24687;&#24182;&#23558;&#20854;&#29992;&#20110;&#39034;&#24207;&#26500;&#24314;&#36335;&#30001;&#30340;&#31574;&#30053;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02476v1 Announce Type: cross  Abstract: The traveling purchaser problem (TPP) is an important combinatorial optimization problem with broad applications. Due to the coupling between routing and purchasing, existing works on TPPs commonly address route construction and purchase planning simultaneously, which, however, leads to exact methods with high computational cost and heuristics with sophisticated design but limited performance. In sharp contrast, we propose a novel approach based on deep reinforcement learning (DRL), which addresses route construction and purchase planning separately, while evaluating and optimizing the solution from a global perspective. The key components of our approach include a bipartite graph representation for TPPs to capture the market-product relations, and a policy network that extracts information from the bipartite graph and uses it to sequentially construct the route. One significant benefit of our framework is that we can efficiently const
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;Hessian-free&#22312;&#32447;&#36951;&#24536;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36817;&#20046;&#30636;&#26102;&#30340;&#22312;&#32447;&#36951;&#24536;&#65292;&#20165;&#38656;&#35201;&#36827;&#34892;&#30690;&#37327;&#21152;&#27861;&#25805;&#20316;&#12290;</title><link>https://arxiv.org/abs/2404.01712</link><description>&lt;p&gt;
&#36890;&#36807;&#20813;Hessian&#37325;&#26032;&#25972;&#21512;&#20010;&#20307;&#25968;&#25454;&#32479;&#35745;&#23454;&#29616;&#39640;&#25928;&#22312;&#32447;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Efficient Online Unlearning via Hessian-Free Recollection of Individual Data Statistics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01712
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;Hessian-free&#22312;&#32447;&#36951;&#24536;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36817;&#20046;&#30636;&#26102;&#30340;&#22312;&#32447;&#36951;&#24536;&#65292;&#20165;&#38656;&#35201;&#36827;&#34892;&#30690;&#37327;&#21152;&#27861;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#26088;&#22312;&#36890;&#36807;&#20351;&#27169;&#22411;&#33021;&#22815;&#36873;&#25321;&#24615;&#22320;&#24536;&#35760;&#29305;&#23450;&#25968;&#25454;&#26469;&#32500;&#25252;&#25968;&#25454;&#25152;&#26377;&#32773;&#30340;&#34987;&#36951;&#24536;&#26435;&#21033;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#34920;&#26126;&#65292;&#19968;&#31181;&#25968;&#25454;&#36951;&#24536;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#39044;&#20808;&#35745;&#31639;&#21644;&#23384;&#20648;&#25658;&#24102;&#20108;&#38454;&#20449;&#24687;&#30340;&#32479;&#35745;&#25968;&#25454;&#65292;&#20197;&#25913;&#36827;&#35745;&#31639;&#21644;&#20869;&#23384;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20381;&#36182;&#20110;&#33499;&#21051;&#30340;&#20551;&#35774;&#65292;&#32780;&#19988;&#35745;&#31639;/&#23384;&#20648;&#21463;&#21040;&#27169;&#22411;&#21442;&#25968;&#32500;&#24230;&#30340;&#35781;&#21650;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#24212;&#29992;&#21040;&#22823;&#22810;&#25968;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20813;Hessian&#22312;&#32447;&#36951;&#24536;&#26041;&#27861;&#12290;&#25105;&#20204;&#24314;&#35758;&#20026;&#27599;&#20010;&#25968;&#25454;&#28857;&#32500;&#25252;&#19968;&#20010;&#32479;&#35745;&#21521;&#37327;&#65292;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#21644;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#30340;&#20223;&#23556;&#38543;&#26426;&#36882;&#24402;&#36924;&#36817;&#26469;&#35745;&#31639;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#36817;&#20046;&#30636;&#26102;&#30340;&#22312;&#32447;&#36951;&#24536;&#65292;&#22240;&#20026;&#23427;&#21482;&#38656;&#35201;&#36827;&#34892;&#30690;&#37327;&#21152;&#27861;&#25805;&#20316;&#12290;&#22522;&#20110;&#37325;&#26032;&#25910;&#38598;&#36951;&#24536;&#25968;&#25454;&#32479;&#35745;&#30340;&#31574;&#30053;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01712v1 Announce Type: cross  Abstract: Machine unlearning strives to uphold the data owners' right to be forgotten by enabling models to selectively forget specific data. Recent methods suggest that one approach of data forgetting is by precomputing and storing statistics carrying second-order information to improve computational and memory efficiency. However, they rely on restrictive assumptions and the computation/storage suffer from the curse of model parameter dimensionality, making it challenging to apply to most deep neural networks. In this work, we propose a Hessian-free online unlearning method. We propose to maintain a statistical vector for each data point, computed through affine stochastic recursion approximation of the difference between retrained and learned models. Our proposed algorithm achieves near-instantaneous online unlearning as it only requires a vector addition operation. Based on the strategy that recollecting statistics for forgetting data, the p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22810;&#23454;&#20363;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#20013;&#20934;&#30830;&#39044;&#27979;&#30284;&#30151;&#34920;&#22411;&#21644;&#22312;&#22270;&#22359;&#32423;&#21035;&#25214;&#21040;&#19982;&#20043;&#30456;&#20851;&#30340;&#32454;&#32990;&#24418;&#24577;&#23398;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2404.01446</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#23454;&#20363;&#23398;&#20064;&#22312;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#20013;&#25214;&#21040;&#24863;&#20852;&#36259;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Finding Regions of Interest in Whole Slide Images Using Multiple Instance Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22810;&#23454;&#20363;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#20013;&#20934;&#30830;&#39044;&#27979;&#30284;&#30151;&#34920;&#22411;&#21644;&#22312;&#22270;&#22359;&#32423;&#21035;&#25214;&#21040;&#19982;&#20043;&#30456;&#20851;&#30340;&#32454;&#32990;&#24418;&#24577;&#23398;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Whole Slide Images (WSI)&#26159;&#36890;&#36807;&#39640;&#20998;&#36776;&#29575;&#25968;&#23383;&#25195;&#25551;&#26174;&#24494;&#38236;&#20999;&#29255;&#22312;&#22810;&#20010;&#23610;&#24230;&#19979;&#33719;&#24471;&#30340;&#65292;&#26159;&#29616;&#20195;&#25968;&#23383;&#30149;&#29702;&#23398;&#30340;&#22522;&#30707;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;/&#20154;&#24037;&#26234;&#33021;&#20013;&#20171;&#20998;&#26512;&#25552;&#20986;&#29305;&#27530;&#25361;&#25112;&#65292;&#22240;&#20026;&#30149;&#29702;&#26631;&#27880;&#36890;&#24120;&#26159;&#22312;&#20999;&#29255;&#32423;&#21035;&#32780;&#19981;&#26159;&#22270;&#22359;&#32423;&#21035;&#36827;&#34892;&#30340;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20004;&#31181;&#24120;&#35265;&#30284;&#30151;&#31867;&#22411;&#65292;&#20405;&#34989;&#24615;&#20083;&#33146;&#30284;&#65288;TCGA-BRCA&#65289;&#21644;&#32954;&#40158;&#29366;&#32454;&#32990;&#30284;&#65288;TCGA-LUSC&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01446v1 Announce Type: cross  Abstract: Whole Slide Images (WSI), obtained by high-resolution digital scanning of microscope slides at multiple scales, are the cornerstone of modern Digital Pathology. However, they represent a particular challenge to AI-based/AI-mediated analysis because pathology labeling is typically done at slide-level, instead of tile-level. It is not just that medical diagnostics is recorded at the specimen level, the detection of oncogene mutation is also experimentally obtained, and recorded by initiatives like The Cancer Genome Atlas (TCGA), at the slide level. This configures a dual challenge: a) accurately predicting the overall cancer phenotype and b) finding out what cellular morphologies are associated with it at the tile level. To address these challenges, a weakly supervised Multiple Instance Learning (MIL) approach was explored for two prevalent cancer types, Invasive Breast Carcinoma (TCGA-BRCA) and Lung Squamous Cell Carcinoma (TCGA-LUSC). 
&lt;/p&gt;</description></item><item><title>&#28304;&#24863;&#30693;&#35757;&#32451;&#20351;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#30693;&#35782;&#24402;&#22240;&#33021;&#21147;&#65292;&#36827;&#32780;&#22686;&#24378;&#20102;&#20854;&#36879;&#26126;&#24230;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.01019</link><description>&lt;p&gt;
&#28304;&#24863;&#30693;&#35757;&#32451;&#20351;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#30693;&#35782;&#24402;&#22240;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Source-Aware Training Enables Knowledge Attribution in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01019
&lt;/p&gt;
&lt;p&gt;
&#28304;&#24863;&#30693;&#35757;&#32451;&#20351;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#30693;&#35782;&#24402;&#22240;&#33021;&#21147;&#65292;&#36827;&#32780;&#22686;&#24378;&#20102;&#20854;&#36879;&#26126;&#24230;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#23398;&#21040;&#20102;&#22823;&#37327;&#30693;&#35782;&#65292;&#20294;&#24448;&#24448;&#23545;&#27492;&#31867;&#30693;&#35782;&#30340;&#26469;&#28304;&#27627;&#19981;&#22312;&#24847;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20869;&#22312;&#28304;&#24341;&#29992;&#38382;&#39064;&#65292;&#35201;&#27714;LLMs&#24341;&#29992;&#25903;&#25345;&#29983;&#25104;&#21709;&#24212;&#30340;&#39044;&#35757;&#32451;&#26469;&#28304;&#12290;&#20869;&#22312;&#28304;&#24341;&#29992;&#21487;&#20197;&#22686;&#24378;LLMs&#30340;&#36879;&#26126;&#24230;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#12290;&#20026;&#36171;&#20104;LLMs&#36825;&#31181;&#33021;&#21147;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#28304;&#24863;&#30693;&#35757;&#32451;&#8212;&#8212;&#19968;&#20010;&#21518;&#39044;&#35757;&#32451;&#37197;&#26041;&#65292;&#21253;&#25324;&#65288;i&#65289;&#35757;&#32451;LLMs&#23558;&#21807;&#19968;&#28304;&#25991;&#26723;&#26631;&#35782;&#31526;&#19982;&#27599;&#20010;&#25991;&#26723;&#20013;&#30340;&#30693;&#35782;&#20851;&#32852;&#36215;&#26469;&#65292;&#28982;&#21518;&#65288;ii&#65289;&#36827;&#34892;&#25351;&#31034;&#35843;&#25972;&#65292;&#25945;&#23548;LLMs&#22312;&#34987;&#25552;&#31034;&#26102;&#24341;&#29992;&#25903;&#25345;&#30340;&#39044;&#35757;&#32451;&#26469;&#28304;&#12290;&#28304;&#24863;&#30693;&#35757;&#32451;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#21363;&#25554;&#21363;&#29992;&#30340;&#39044;&#35757;&#32451;LLMs&#65292;&#24182;&#19982;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;/&#24494;&#35843;&#26694;&#26550;&#30340;&#24046;&#24322;&#26368;&#23567;&#12290;&#36890;&#36807;&#23545;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35757;&#32451;&#37197;&#26041;&#21487;&#20197;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01019v1 Announce Type: cross  Abstract: Large language models (LLMs) learn a vast amount of knowledge during pretraining, but they are often oblivious to the source(s) of such knowledge. We investigate the problem of intrinsic source citation, where LLMs are required to cite the pretraining source supporting a generated response. Intrinsic source citation can enhance LLM transparency, interpretability, and verifiability. To give LLMs such ability, we explore source-aware training -- a post pretraining recipe that involves (i) training the LLM to associate unique source document identifiers with the knowledge in each document, followed by (ii) an instruction-tuning to teach the LLM to cite a supporting pretraining source when prompted. Source-aware training can easily be applied to pretrained LLMs off the shelf, and diverges minimally from existing pretraining/fine-tuning frameworks. Through experiments on carefully curated data, we demonstrate that our training recipe can en
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36879;&#38236;&#65292;&#36890;&#36807;&#20854;&#38544;&#21547;&#30340;&#39640;&#23618;&#27425;&#27010;&#24565;&#26469;&#36827;&#34892;&#23545;&#35270;&#35273;&#27169;&#22411;&#30340;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.19837</link><description>&lt;p&gt;
&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22522;&#20110;&#27010;&#24565;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Concept-based Analysis of Neural Networks via Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36879;&#38236;&#65292;&#36890;&#36807;&#20854;&#38544;&#21547;&#30340;&#39640;&#23618;&#27425;&#27010;&#24565;&#26469;&#36827;&#34892;&#23545;&#35270;&#35273;&#27169;&#22411;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#24418;&#24335;&#21270;&#20998;&#26512;&#38750;&#24120;&#21487;&#21462;&#65292;&#20294;&#30001;&#20110;&#38590;&#20197;&#34920;&#36798;&#35270;&#35273;&#20219;&#21153;&#30340;&#24418;&#24335;&#21270;&#35268;&#33539;&#20197;&#21450;&#32570;&#20047;&#39640;&#25928;&#30340;&#39564;&#35777;&#31243;&#24207;&#65292;&#36825;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#26032;&#20852;&#30340;&#22810;&#27169;&#24577;&#12289;&#35270;&#35273;&#35821;&#35328;&#12289;&#22522;&#30784;&#27169;&#22411;&#65288;VLMs&#65289;&#20316;&#20026;&#19968;&#31181;&#36890;&#36807;&#20854;&#21487;&#20197;&#25512;&#29702;&#35270;&#35273;&#27169;&#22411;&#30340;&#36879;&#38236;&#12290;VLMs&#24050;&#32463;&#22312;&#22823;&#37327;&#22270;&#20687;&#21450;&#20854;&#25991;&#26412;&#25551;&#36848;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#22240;&#27492;&#38544;&#24335;&#22320;&#20102;&#35299;&#25551;&#36848;&#36825;&#20123;&#22270;&#20687;&#30340;&#39640;&#23618;&#27425;&#12289;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#21517;&#20026;$\texttt{Con}_{\texttt{spec}}$&#30340;&#36923;&#36753;&#35268;&#33539;&#35821;&#35328;&#65292;&#26088;&#22312;&#20415;&#20110;&#25353;&#29031;&#36825;&#20123;&#27010;&#24565;&#32534;&#20889;&#35268;&#33539;&#12290;&#20026;&#20102;&#23450;&#20041;&#21644;&#24418;&#24335;&#21270;&#26816;&#26597;$\texttt{Con}_{\texttt{spec}}$&#35268;&#33539;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#20010;VLM&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#32534;&#30721;&#21644;&#39640;&#25928;&#26816;&#26597;&#35270;&#35273;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#23646;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;te
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19837v1 Announce Type: cross  Abstract: Formal analysis of vision-based deep neural networks (DNNs) is highly desirable but it is very challenging due to the difficulty of expressing formal specifications for vision tasks and the lack of efficient verification procedures. In this paper, we propose to leverage emerging multimodal, vision-language, foundation models (VLMs) as a lens through which we can reason about vision models. VLMs have been trained on a large body of images accompanied by their textual description, and are thus implicitly aware of high-level, human-understandable concepts describing the images. We describe a logical specification language $\texttt{Con}_{\texttt{spec}}$ designed to facilitate writing specifications in terms of these concepts. To define and formally check $\texttt{Con}_{\texttt{spec}}$ specifications, we leverage a VLM, which provides a means to encode and efficiently check natural-language properties of vision models. We demonstrate our te
&lt;/p&gt;</description></item><item><title>KTbench&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#27844;&#28431;&#30340;&#30693;&#35782;&#36861;&#36394;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;KT&#27169;&#22411;&#20013;KC&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23398;&#20064;&#21487;&#33021;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.15304</link><description>&lt;p&gt;
KTbench&#65306;&#19968;&#31181;&#20840;&#26032;&#30340;&#26080;&#25968;&#25454;&#27844;&#28431;&#30340;&#30693;&#35782;&#36861;&#36394;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
KTbench: A Novel Data Leakage-Free Framework for Knowledge Tracing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15304
&lt;/p&gt;
&lt;p&gt;
KTbench&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#27844;&#28431;&#30340;&#30693;&#35782;&#36861;&#36394;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;KT&#27169;&#22411;&#20013;KC&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23398;&#20064;&#21487;&#33021;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#36861;&#36394;&#65288;KT&#65289;&#28041;&#21450;&#22312;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#39044;&#27979;&#23398;&#29983;&#23545;&#23398;&#20064;&#39033;&#30446;&#30340;&#26410;&#26469;&#34920;&#29616;&#12290;&#23398;&#20064;&#39033;&#30446;&#34987;&#26631;&#35760;&#20026;&#31216;&#20026;&#30693;&#35782;&#27010;&#24565;&#65288;KCs&#65289;&#30340;&#25216;&#33021;&#26631;&#31614;&#12290;&#35768;&#22810;KT&#27169;&#22411;&#36890;&#36807;&#29992;&#26500;&#25104;KC&#30340;&#23398;&#20064;&#39033;&#30446;&#21462;&#20195;&#23398;&#20064;&#39033;&#30446;&#26469;&#23558;&#23398;&#20064;&#39033;&#30446;-&#23398;&#29983;&#20132;&#20114;&#24207;&#21015;&#25193;&#23637;&#20026;KC-&#23398;&#29983;&#20132;&#20114;&#24207;&#21015;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#31232;&#30095;&#30340;&#23398;&#20064;&#39033;&#30446;-&#23398;&#29983;&#20132;&#20114;&#38382;&#39064;&#24182;&#26368;&#23567;&#21270;&#20102;&#27169;&#22411;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#12290;&#31532;&#19968;&#20010;&#38382;&#39064;&#26159;&#27169;&#22411;&#23398;&#20064;&#21516;&#19968;&#39033;&#30446;&#20869;&#30340;KC&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#30340;&#33021;&#21147;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#22522;&#26412;&#20107;&#23454;&#26631;&#31614;&#30340;&#27844;&#28431;&#24182;&#38459;&#30861;&#27169;&#22411;&#24615;&#33021;&#12290;&#31532;&#20108;&#20010;&#38382;&#39064;&#26159;&#29616;&#26377;&#30340;&#22522;&#20934;&#23454;&#29616;&#24573;&#30053;&#20102;&#35745;&#25968;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15304v1 Announce Type: cross  Abstract: Knowledge Tracing (KT) is concerned with predicting students' future performance on learning items in intelligent tutoring systems. Learning items are tagged with skill labels called knowledge concepts (KCs). Many KT models expand the sequence of item-student interactions into KC-student interactions by replacing learning items with their constituting KCs. This often results in a longer sequence length. This approach addresses the issue of sparse item-student interactions and minimises model parameters. However, two problems have been identified with such models.   The first problem is the model's ability to learn correlations between KCs belonging to the same item, which can result in the leakage of ground truth labels and hinder performance. This problem can lead to a significant decrease in performance on datasets with a higher number of KCs per item. The second problem is that the available benchmark implementations ignore accounti
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#33021;&#22815;&#35780;&#20272;&#38750;&#31169;&#23494;&#25968;&#25454;&#30456;&#20851;&#39044;&#22788;&#29702;&#31639;&#27861;&#24341;&#36215;&#30340;&#39069;&#22806;&#38544;&#31169;&#25104;&#26412;&#65292;&#24182;&#21033;&#29992;&#24179;&#28369;DP&#21644;&#39044;&#22788;&#29702;&#31639;&#27861;&#30340;&#26377;&#30028;&#25935;&#24863;&#24615;&#24314;&#31435;&#25972;&#20307;&#38544;&#31169;&#20445;&#35777;&#30340;&#19978;&#38480;</title><link>https://arxiv.org/abs/2403.13041</link><description>&lt;p&gt;
&#20855;&#26377;&#38750;&#31169;&#23494;&#39044;&#22788;&#29702;&#30340;&#21487;&#35777;&#26126;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Provable Privacy with Non-Private Pre-Processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13041
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#33021;&#22815;&#35780;&#20272;&#38750;&#31169;&#23494;&#25968;&#25454;&#30456;&#20851;&#39044;&#22788;&#29702;&#31639;&#27861;&#24341;&#36215;&#30340;&#39069;&#22806;&#38544;&#31169;&#25104;&#26412;&#65292;&#24182;&#21033;&#29992;&#24179;&#28369;DP&#21644;&#39044;&#22788;&#29702;&#31639;&#27861;&#30340;&#26377;&#30028;&#25935;&#24863;&#24615;&#24314;&#31435;&#25972;&#20307;&#38544;&#31169;&#20445;&#35777;&#30340;&#19978;&#38480;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20998;&#26512;&#24046;&#20998;&#31169;&#23494;&#65288;DP&#65289;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#26102;&#65292;&#36890;&#24120;&#20250;&#24573;&#30053;&#25968;&#25454;&#30456;&#20851;&#30340;&#39044;&#22788;&#29702;&#30340;&#28508;&#22312;&#38544;&#31169;&#25104;&#26412;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#30001;&#38750;&#31169;&#23494;&#25968;&#25454;&#30456;&#20851;&#39044;&#22788;&#29702;&#31639;&#27861;&#24341;&#36215;&#30340;&#39069;&#22806;&#38544;&#31169;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#26032;&#30340;&#25216;&#26415;&#27010;&#24565;&#24314;&#31435;&#20102;&#25972;&#20307;&#38544;&#31169;&#20445;&#35777;&#30340;&#19978;&#38480;&#65306;&#19968;&#31181;&#31216;&#20026;&#24179;&#28369;DP&#30340;DP&#21464;&#20307;&#20197;&#21450;&#39044;&#22788;&#29702;&#31639;&#27861;&#30340;&#26377;&#30028;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13041v1 Announce Type: cross  Abstract: When analysing Differentially Private (DP) machine learning pipelines, the potential privacy cost of data-dependent pre-processing is frequently overlooked in privacy accounting. In this work, we propose a general framework to evaluate the additional privacy cost incurred by non-private data-dependent pre-processing algorithms. Our framework establishes upper bounds on the overall privacy guarantees by utilising two new technical notions: a variant of DP termed Smooth DP and the bounded sensitivity of the pre-processing algorithms. In addition to the generic framework, we provide explicit overall privacy guarantees for multiple data-dependent pre-processing algorithms, such as data imputation, quantization, deduplication and PCA, when used in combination with several DP algorithms. Notably, this framework is also simple to implement, allowing direct integration into existing DP pipelines.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;StateFlow&#30340;&#26032;&#39062;LLM&#20219;&#21153;&#35299;&#20915;&#33539;&#24335;&#65292;&#23558;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#27010;&#24565;&#21270;&#20026;&#29366;&#24577;&#26426;&#65292;&#36890;&#36807;&#29366;&#24577;&#36716;&#25442;&#30830;&#20445;LLM&#21709;&#24212;&#30340;&#28165;&#26224;&#36319;&#36394;&#21644;&#31649;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.11322</link><description>&lt;p&gt;
&#20351;&#29992;StateFlow&#22686;&#24378;LLM&#20219;&#21153;&#35299;&#20915;&#33021;&#21147;&#36890;&#36807;&#29366;&#24577;&#39537;&#21160;&#24037;&#20316;&#27969;
&lt;/p&gt;
&lt;p&gt;
StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11322
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;StateFlow&#30340;&#26032;&#39062;LLM&#20219;&#21153;&#35299;&#20915;&#33539;&#24335;&#65292;&#23558;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#27010;&#24565;&#21270;&#20026;&#29366;&#24577;&#26426;&#65292;&#36890;&#36807;&#29366;&#24577;&#36716;&#25442;&#30830;&#20445;LLM&#21709;&#24212;&#30340;&#28165;&#26224;&#36319;&#36394;&#21644;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#36235;&#21183;&#26085;&#30410;&#26126;&#26174;&#65292;&#20363;&#22914;&#38656;&#35201;&#19968;&#31995;&#21015;&#25805;&#20316;&#21644;&#19982;&#24037;&#20855;&#29615;&#22659;&#21160;&#24577;&#20132;&#20114;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;StateFlow&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;LLM&#30340;&#20219;&#21153;&#27714;&#35299;&#33539;&#24335;&#65292;&#23558;&#30001;LLM&#25903;&#25345;&#30340;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#27010;&#24565;&#21270;&#20026;&#29366;&#24577;&#26426;&#12290;&#36890;&#36807;&#27491;&#30830;&#26500;&#24314;&#29366;&#24577;&#21644;&#23450;&#20041;&#29366;&#24577;&#36716;&#25442;&#65292;StateFlow&#30830;&#23450;&#20102;&#20219;&#21153;&#27714;&#35299;&#30340;&#36827;&#23637;&#65292;&#30830;&#20445;&#28165;&#26224;&#36319;&#36394;&#21644;&#31649;&#29702;LLM&#22312;&#25972;&#20010;&#20219;&#21153;&#27714;&#35299;&#36807;&#31243;&#20013;&#30340;&#21709;&#24212;&#12290;&#22312;&#27599;&#20010;&#29366;&#24577;&#20013;&#65292;StateFlow&#20801;&#35768;&#25191;&#34892;&#19968;&#31995;&#21015;&#21160;&#20316;&#65292;&#19981;&#20165;&#21253;&#25324;&#26681;&#25454;&#29305;&#23450;&#25552;&#31034;&#25351;&#23548;&#29983;&#25104;LLM&#21709;&#24212;&#65292;&#36824;&#21253;&#25324;&#26681;&#25454;&#38656;&#35201;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#12290;&#29366;&#24577;&#36716;&#25442;&#30001;LLM&#20570;&#20986;&#30340;&#29305;&#23450;&#35268;&#21017;&#25110;&#20915;&#31574;&#25511;&#21046;&#65292;&#20801;&#35768;&#36890;&#36807;&#20219;&#21153;&#30340;&#39044;&#23450;&#20041;StateFlow&#27169;&#22411;&#21160;&#24577;&#33258;&#36866;&#24212;&#22320;&#36827;&#34892;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11322v1 Announce Type: cross  Abstract: It is a notable trend to use Large Language Models (LLMs) to tackle complex tasks, e.g., tasks that require a sequence of actions and dynamic interaction with tools and environments. In this paper, we propose StateFlow, a novel LLM-based task-solving paradigm that conceptualizes complex task-solving processes backed by LLMs as state machines. With proper construction of states and definition of state transitions, StateFlow grounds the progress of task-solving, ensuring clear tracking and management of LLMs' responses throughout the task-solving process. Within each state, StateFlow allows execution of a series of actions, involving not only the generation of LLM's responses guided by a specific prompt, but also the utilization of external tools as needed. State transitions are controlled by specific rules or decisions made by the LLM, allowing for a dynamic and adaptive progression through the task's pre-defined StateFlow model. Evalua
&lt;/p&gt;</description></item><item><title>Gemma&#26159;&#22522;&#20110;Gemini&#30740;&#31350;&#21644;&#25216;&#26415;&#25152;&#26500;&#24314;&#30340;&#24320;&#25918;&#27169;&#22411;&#31995;&#21015;&#65292;&#22312;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#23433;&#20840;&#24615;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36127;&#36131;&#20219;&#22320;&#21457;&#24067;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#25552;&#39640;&#21069;&#27839;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2403.08295</link><description>&lt;p&gt;
Gemma&#65306;&#22522;&#20110;Gemini&#30740;&#31350;&#21644;&#25216;&#26415;&#30340;&#24320;&#25918;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gemma: Open Models Based on Gemini Research and Technology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08295
&lt;/p&gt;
&lt;p&gt;
Gemma&#26159;&#22522;&#20110;Gemini&#30740;&#31350;&#21644;&#25216;&#26415;&#25152;&#26500;&#24314;&#30340;&#24320;&#25918;&#27169;&#22411;&#31995;&#21015;&#65292;&#22312;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#23433;&#20840;&#24615;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36127;&#36131;&#20219;&#22320;&#21457;&#24067;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#25552;&#39640;&#21069;&#27839;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Gemma&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Gemini&#27169;&#22411;&#30740;&#31350;&#21644;&#25216;&#26415;&#26500;&#24314;&#30340;&#36731;&#37327;&#32423;&#12289;&#26368;&#20808;&#36827;&#30340;&#24320;&#25918;&#27169;&#22411;&#31995;&#21015;&#12290;Gemma&#27169;&#22411;&#22312;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#23433;&#20840;&#24615;&#31561;&#23398;&#26415;&#22522;&#20934;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#20004;&#20010;&#35268;&#27169;&#30340;&#27169;&#22411;&#65288;20&#20159;&#21644;70&#20159;&#21442;&#25968;&#65289;&#65292;&#24182;&#25552;&#20379;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;&#26816;&#26597;&#28857;&#12290;Gemma&#22312;18&#20010;&#22522;&#20110;&#25991;&#26412;&#30340;&#20219;&#21153;&#20013;&#65292;&#26377;11&#20010;&#20219;&#21153;&#20248;&#20110;&#31867;&#20284;&#35268;&#27169;&#30340;&#24320;&#25918;&#27169;&#22411;&#65292;&#24182;&#23545;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#36131;&#20219;&#26041;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21516;&#26102;&#35814;&#32454;&#25551;&#36848;&#20102;&#27169;&#22411;&#24320;&#21457;&#36807;&#31243;&#12290;&#25105;&#20204;&#30456;&#20449;&#36127;&#36131;&#20219;&#22320;&#21457;&#24067;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#25552;&#39640;&#21069;&#27839;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#23454;&#29616;&#19979;&#19968;&#27874;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21019;&#26032;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08295v1 Announce Type: cross  Abstract: This work introduces Gemma, a family of lightweight, state-of-the art open models built from the research and technology used to create Gemini models. Gemma models demonstrate strong performance across academic benchmarks for language understanding, reasoning, and safety. We release two sizes of models (2 billion and 7 billion parameters), and provide both pretrained and fine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out of 18 text-based tasks, and we present comprehensive evaluations of safety and responsibility aspects of the models, alongside a detailed description of model development. We believe the responsible release of LLMs is critical for improving the safety of frontier models, and for enabling the next wave of LLM innovations.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#36523;&#20221;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#36890;&#36807;&#36523;&#20221;&#35270;&#35282;&#20419;&#36827;AI&#22810;&#26679;&#24615;&#22312;&#21019;&#36896;&#32773;&#12289;&#21019;&#20316;&#29289;&#21644;&#21518;&#26524;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07924</link><description>&lt;p&gt;
AI&#36523;&#20221;&#30340;&#24847;&#20041;&#65306;&#21019;&#36896;&#32773;&#12289;&#21019;&#20316;&#29289;&#21644;&#21518;&#26524;&#30340;&#21547;&#20041;
&lt;/p&gt;
&lt;p&gt;
Implications of Identity of AI: Creators, Creations, and Consequences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07924
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#36523;&#20221;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#36890;&#36807;&#36523;&#20221;&#35270;&#35282;&#20419;&#36827;AI&#22810;&#26679;&#24615;&#22312;&#21019;&#36896;&#32773;&#12289;&#21019;&#20316;&#29289;&#21644;&#21518;&#26524;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39046;&#22495;&#27491;&#22312;&#24555;&#36895;&#21457;&#23637;&#65292;&#20855;&#26377;&#26174;&#33879;&#28508;&#21147;&#25913;&#21464;&#31038;&#20250;&#12290;&#28982;&#32780;&#65292;&#23427;&#38754;&#20020;&#30528;&#19968;&#20010;&#26126;&#26174;&#30340;&#25361;&#25112;&#65306;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#36825;&#26159;STEM&#39046;&#22495;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#26412;&#31435;&#22330;&#35770;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#36523;&#20221;&#30340;&#20132;&#21449;&#28857;&#65292;&#20316;&#20026;&#29702;&#35299;AI&#21457;&#23637;&#21644;&#24212;&#29992;&#20013;&#30340;&#20559;&#35265;&#12289;&#19981;&#24179;&#31561;&#21644;&#36947;&#24503;&#32771;&#34385;&#30340;&#36884;&#24452;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#32500;&#24230;&#30340;AI&#36523;&#20221;&#23450;&#20041;&#65292;&#28085;&#30422;&#20854;&#21019;&#36896;&#32773;&#12289;&#24212;&#29992;&#20197;&#21450;&#23427;&#20204;&#30340;&#26356;&#24191;&#27867;&#24433;&#21709;&#12290;&#29702;&#35299;AI&#30340;&#36523;&#20221;&#28041;&#21450;&#20998;&#26512;&#21442;&#19982;AI&#21457;&#23637;&#30340;&#21508;&#31181;&#20010;&#20307;&#12289;&#25152;&#20135;&#29983;&#30340;&#25216;&#26415;&#20197;&#21450;&#31038;&#20250;&#12289;&#36947;&#24503;&#21644;&#24515;&#29702;&#24433;&#21709;&#12290;&#22312;&#25506;&#35752;AI&#36523;&#20221;&#29983;&#24577;&#31995;&#32479;&#21450;&#20854;&#31038;&#20250;&#21160;&#24577;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#24378;&#35843;&#20102;&#36890;&#36807;&#36523;&#20221;&#35270;&#35282;&#36328;&#19977;&#20010;&#32500;&#24230;&#8212;&#8212;&#21019;&#36896;&#32773;&#12289;&#21019;&#20316;&#29289;&#21644;&#21518;&#26524;&#65292;&#22312;AI&#20013;&#38656;&#35201;&#22810;&#26679;&#24615;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07924v1 Announce Type: cross  Abstract: The field of Artificial Intelligence (AI) is rapidly advancing, with significant potential to transform society. However, it faces a notable challenge: lack of diversity, a longstanding issue in STEM fields. In this context, This position paper examines the intersection of AI and identity as a pathway to understand biases, inequalities, and ethical considerations in AI development and deployment. We present a multifaceted definition of AI identity, which encompasses its creators, applications, and their broader impacts. Understanding AI's identity involves analyzing the diverse individuals involved in AI's development, the technologies produced, and the social, ethical, and psychological implications. After exploring the AI identity ecosystem and its societal dynamics, We propose a framework that highlights the need for diversity in AI across three dimensions: Creators, Creations, and Consequences through the lens of identity. This pap
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;SAFIM&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#20195;&#30721;&#22635;&#31354;&#20219;&#21153;&#19978;&#30340;&#21477;&#27861;&#24863;&#30693;&#23436;&#25104;&#34920;&#29616;&#65292;&#21457;&#29616;FIM&#39044;&#35757;&#32451;&#19981;&#20165;&#25552;&#39640;&#20102;FIM&#30340;&#29087;&#32451;&#24230;&#65292;&#36824;&#25913;&#21892;&#20102;LLMs&#30340;&#24038;&#21040;&#21491;&#25512;&#29702;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#35266;&#24565;&#24182;&#34920;&#26126;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#25968;&#25454;&#21697;&#36136;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#26356;&#29978;&#20110;&#27169;&#22411;&#22823;&#23567;&#12290;</title><link>https://arxiv.org/abs/2403.04814</link><description>&lt;p&gt;
&#22312;&#21477;&#27861;&#24863;&#30693;&#20195;&#30721;&#22635;&#31354;&#20219;&#21153;&#19978;&#35780;&#20272;LLMs
&lt;/p&gt;
&lt;p&gt;
Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04814
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;SAFIM&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#20195;&#30721;&#22635;&#31354;&#20219;&#21153;&#19978;&#30340;&#21477;&#27861;&#24863;&#30693;&#23436;&#25104;&#34920;&#29616;&#65292;&#21457;&#29616;FIM&#39044;&#35757;&#32451;&#19981;&#20165;&#25552;&#39640;&#20102;FIM&#30340;&#29087;&#32451;&#24230;&#65292;&#36824;&#25913;&#21892;&#20102;LLMs&#30340;&#24038;&#21040;&#21491;&#25512;&#29702;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#35266;&#24565;&#24182;&#34920;&#26126;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#25968;&#25454;&#21697;&#36136;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#26356;&#29978;&#20110;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Syntax-Aware Fill-In-the-Middle&#65288;SAFIM&#65289;&#30340;&#26032;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#22635;&#31354;&#65288;FIM&#65289;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#35813;&#22522;&#20934;&#20391;&#37325;&#20110;&#31243;&#24207;&#32467;&#26500;&#30340;&#21477;&#27861;&#24863;&#30693;&#23436;&#25104;&#65292;&#22914;&#20195;&#30721;&#22359;&#21644;&#26465;&#20214;&#34920;&#36798;&#24335;&#65292;&#24182;&#21253;&#25324;&#26469;&#33258;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;17,720&#20010;&#31034;&#20363;&#65292;&#26469;&#28304;&#20110;2022&#24180;4&#26376;&#20043;&#21518;&#30340;&#26368;&#26032;&#20195;&#30721;&#25552;&#20132;&#65292;&#20197;&#26368;&#23567;&#21270;&#25968;&#25454;&#27745;&#26579;&#12290; SAFIM&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#20855;&#26377;&#21508;&#31181;&#25552;&#31034;&#35774;&#35745;&#21644;&#26032;&#39062;&#30340;&#21477;&#27861;&#24863;&#30693;&#21518;&#22788;&#29702;&#25216;&#26415;&#65292;&#26377;&#21161;&#20110;&#22312;LLMs&#20043;&#38388;&#36827;&#34892;&#20934;&#30830;&#21644;&#20844;&#24179;&#30340;&#27604;&#36739;&#12290;&#25105;&#20204;&#23545;15&#20010;LLMs&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;FIM&#39044;&#35757;&#32451;&#19981;&#20165;&#25552;&#21319;&#20102;FIM&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#36824;&#25913;&#36827;&#20102;LLMs&#30340;&#24038;&#21040;&#21491;&#65288;L2R&#65289;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25361;&#25112;&#20102;&#20256;&#32479;&#35266;&#24565;&#65292;&#24182;&#34920;&#26126;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#25968;&#25454;&#36136;&#37327;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#22823;&#20110;&#27169;&#22411;&#22823;&#23567;&#12290;&#22240;&#27492;&#65292;SAFIM&#20026;&#26410;&#26469;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04814v1 Announce Type: cross  Abstract: We introduce Syntax-Aware Fill-In-the-Middle (SAFIM), a new benchmark for evaluating Large Language Models (LLMs) on the code Fill-in-the-Middle (FIM) task. This benchmark focuses on syntax-aware completions of program structures such as code blocks and conditional expressions, and includes 17,720 examples from multiple programming languages, sourced from recent code submissions after April 2022 to minimize data contamination. SAFIM provides a robust framework with various prompt designs and novel syntax-aware post-processing techniques, facilitating accurate and fair comparisons across LLMs. Our comprehensive evaluation of 15 LLMs shows that FIM pretraining not only enhances FIM proficiency but also improves Left-to-Right (L2R) inference using LLMs. Our findings challenge conventional beliefs and suggest that pretraining methods and data quality have more impact than model size. SAFIM thus serves as a foundational platform for future 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#23376;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#34920;&#24449;&#32454;&#31890;&#24230;&#35821;&#20041;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#22312;&#22270;&#22686;&#24378;&#21518;&#21407;&#26377;&#20551;&#35774;&#19981;&#20877;&#25104;&#31435;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.02719</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#23376;&#22270;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Scale Subgraph Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02719
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#23376;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#34920;&#24449;&#32454;&#31890;&#24230;&#35821;&#20041;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#22312;&#22270;&#22686;&#24378;&#21518;&#21407;&#26377;&#20551;&#35774;&#19981;&#20877;&#25104;&#31435;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32423;&#23545;&#27604;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#23545;&#27604;&#20004;&#20010;&#22686;&#24378;&#22270;&#26469;&#23398;&#20064;&#27599;&#20010;&#22270;&#30340;&#34920;&#31034;&#65292;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#31616;&#21333;&#22320;&#20551;&#35774;&#19968;&#20010;&#22270;&#21450;&#20854;&#22686;&#24378;&#22270;&#20026;&#27491;&#23545;&#65292;&#21542;&#21017;&#20026;&#36127;&#23545;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#22270;&#32467;&#26500;&#36890;&#24120;&#22797;&#26434;&#19988;&#22810;&#23610;&#24230;&#65292;&#36825;&#24102;&#26469;&#20102;&#19968;&#20010;&#26681;&#26412;&#38382;&#39064;&#65306;&#22312;&#22270;&#22686;&#24378;&#21518;&#65292;&#20808;&#21069;&#30340;&#20551;&#35774;&#26159;&#21542;&#20173;&#28982;&#25104;&#31435;&#65311;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#22686;&#24378;&#22270;&#32467;&#26500;&#30340;&#35821;&#20041;&#20449;&#24687;&#21487;&#33021;&#19981;&#19968;&#33268;&#20110;&#21407;&#22987;&#22270;&#32467;&#26500;&#65292;&#24182;&#19988;&#20004;&#20010;&#22686;&#24378;&#22270;&#26159;&#27491;&#23545;&#36824;&#26159;&#36127;&#23545;&#19982;&#22810;&#23610;&#24230;&#32467;&#26500;&#23494;&#20999;&#30456;&#20851;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#34920;&#24449;&#32454;&#31890;&#24230;&#35821;&#20041;&#20449;&#24687;&#30340;&#22810;&#23610;&#24230;&#23376;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#29983;&#25104;&#20840;&#23616;&#21644;&#23616;&#37096;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02719v1 Announce Type: new  Abstract: Graph-level contrastive learning, aiming to learn the representations for each graph by contrasting two augmented graphs, has attracted considerable attention. Previous studies usually simply assume that a graph and its augmented graph as a positive pair, otherwise as a negative pair. However, it is well known that graph structure is always complex and multi-scale, which gives rise to a fundamental question: after graph augmentation, will the previous assumption still hold in reality? By an experimental analysis, we discover the semantic information of an augmented graph structure may be not consistent as original graph structure, and whether two augmented graphs are positive or negative pairs is highly related with the multi-scale structures. Based on this finding, we propose a multi-scale subgraph contrastive learning method which is able to characterize the fine-grained semantic information. Specifically, we generate global and local 
&lt;/p&gt;</description></item><item><title>GEM3D&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#12289;&#25299;&#25169;&#24863;&#30693;&#30340;&#19977;&#32500;&#24418;&#29366;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#31070;&#32463;&#39592;&#26550;&#32534;&#30721;&#20102;&#25299;&#25169;&#21644;&#20960;&#20309;&#20449;&#24687;&#65292;&#36890;&#36807;&#39592;&#26550;&#39537;&#21160;&#30340;&#31070;&#32463;&#38544;&#24335;&#20844;&#24335;&#29983;&#25104;&#20934;&#30830;&#21644;&#22810;&#26679;&#21270;&#30340;&#34920;&#38754;&#12290;</title><link>https://arxiv.org/abs/2402.16994</link><description>&lt;p&gt;
GEM3D&#65306;&#19977;&#32500;&#24418;&#29366;&#21512;&#25104;&#30340;&#29983;&#25104;&#23186;&#20307;&#25277;&#35937;
&lt;/p&gt;
&lt;p&gt;
GEM3D: GEnerative Medial Abstractions for 3D Shape Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16994
&lt;/p&gt;
&lt;p&gt;
GEM3D&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#12289;&#25299;&#25169;&#24863;&#30693;&#30340;&#19977;&#32500;&#24418;&#29366;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#31070;&#32463;&#39592;&#26550;&#32534;&#30721;&#20102;&#25299;&#25169;&#21644;&#20960;&#20309;&#20449;&#24687;&#65292;&#36890;&#36807;&#39592;&#26550;&#39537;&#21160;&#30340;&#31070;&#32463;&#38544;&#24335;&#20844;&#24335;&#29983;&#25104;&#20934;&#30830;&#21644;&#22810;&#26679;&#21270;&#30340;&#34920;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;GEM3D&#8212;&#8212;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#12289;&#25299;&#25169;&#24863;&#30693;&#30340;&#19977;&#32500;&#24418;&#29366;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#37096;&#20998;&#26159;&#22522;&#20110;&#31070;&#32463;&#39592;&#26550;&#30340;&#34920;&#31034;&#65292;&#32534;&#30721;&#20102;&#20851;&#20110;&#24418;&#29366;&#25299;&#25169;&#21644;&#20960;&#20309;&#30340;&#20449;&#24687;&#12290;&#36890;&#36807;&#19968;&#20010;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#29983;&#25104;&#36981;&#24490;&#20013;&#36724;&#21464;&#25442;&#65288;MAT&#65289;&#30340;&#22522;&#20110;&#39592;&#26550;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#36890;&#36807;&#19968;&#20010;&#39592;&#26550;&#39537;&#21160;&#30340;&#31070;&#32463;&#38544;&#24335;&#20844;&#24335;&#29983;&#25104;&#34920;&#38754;&#12290;&#31070;&#32463;&#38544;&#24335;&#32771;&#34385;&#20102;&#22312;&#29983;&#25104;&#30340;&#39592;&#26550;&#34920;&#31034;&#20013;&#23384;&#20648;&#30340;&#25299;&#25169;&#21644;&#20960;&#20309;&#20449;&#24687;&#65292;&#20135;&#29983;&#30340;&#34920;&#38754;&#19982;&#20043;&#21069;&#30340;&#31070;&#32463;&#22330;&#20844;&#24335;&#30456;&#27604;&#26356;&#21152;&#25299;&#25169;&#21644;&#20960;&#20309;&#20934;&#30830;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24418;&#29366;&#21512;&#25104;&#21644;&#28857;&#20113;&#37325;&#24314;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#23450;&#24615;&#21644;&#23450;&#37327;&#35780;&#20272;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27604;&#20197;&#21069;&#26356;&#20026;&#20934;&#30830;&#21644;&#22810;&#26679;&#21270;&#30340;&#24418;&#29366;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16994v1 Announce Type: cross  Abstract: We introduce GEM3D -- a new deep, topology-aware generative model of 3D shapes. The key ingredient of our method is a neural skeleton-based representation encoding information on both shape topology and geometry. Through a denoising diffusion probabilistic model, our method first generates skeleton-based representations following the Medial Axis Transform (MAT), then generates surfaces through a skeleton-driven neural implicit formulation. The neural implicit takes into account the topological and geometric information stored in the generated skeleton representations to yield surfaces that are more topologically and geometrically accurate compared to previous neural field formulations. We discuss applications of our method in shape synthesis and point cloud reconstruction tasks, and evaluate our method both qualitatively and quantitatively. We demonstrate significantly more faithful surface reconstruction and diverse shape generation r
&lt;/p&gt;</description></item><item><title>Me LLaMA&#26159;&#19968;&#20010;&#21307;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#36890;&#36807;&#25345;&#32493;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#22312;&#22823;&#22411;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#32780;&#25104;&#65292;&#20854;&#22312;&#38646;-shot&#21644;&#23569;-shot&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#21644;&#21830;&#19994;&#24040;&#22836;ChatGPT&#12290;</title><link>https://arxiv.org/abs/2402.12749</link><description>&lt;p&gt;
Me LLaMA: &#20026;&#21307;&#30103;&#24212;&#29992;&#26500;&#24314;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Me LLaMA: Foundation Large Language Models for Medical Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12749
&lt;/p&gt;
&lt;p&gt;
Me LLaMA&#26159;&#19968;&#20010;&#21307;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#36890;&#36807;&#25345;&#32493;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#22312;&#22823;&#22411;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#32780;&#25104;&#65292;&#20854;&#22312;&#38646;-shot&#21644;&#23569;-shot&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#21644;&#21830;&#19994;&#24040;&#22836;ChatGPT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35832;&#22914;ChatGPT&#21644;LLaMA&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#19981;&#22815;&#29702;&#24819;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#22312;&#22823;&#22411;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;Me LLaMA&#65292;&#19968;&#20010;&#21307;&#23398;LLM&#31995;&#21015;&#65292;&#21253;&#25324;&#22522;&#30784;&#27169;&#22411;- Me LLaMA 13/70B&#21450;&#20854; chat-enhanced &#29256;&#26412;- Me LLaMA 13/70B-chat&#65292;&#36890;&#36807;&#25345;&#32493;&#23545;LLaMA2&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#21307;&#23398;&#25968;&#25454;&#24320;&#21457;&#32780;&#25104;&#12290;&#25105;&#20204;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#22871;&#20214;&#21253;&#25324;&#19968;&#20010;&#20855;&#26377;129B tokens&#30340;&#22823;&#35268;&#27169;&#25345;&#32493;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#19968;&#20010;&#21253;&#21547;214k&#20010;&#26679;&#26412;&#30340;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#36328;&#36234;14&#20010;&#25968;&#25454;&#38598;&#30340;&#20845;&#39033;&#20219;&#21153;&#30340;&#21307;&#23398;&#35780;&#20272;&#22522;&#20934;(MIBE)&#12290;&#25105;&#20204;&#20351;&#29992;MIBE&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#26174;&#31034;&#65292;Me LLaMA&#27169;&#22411;&#22312;&#38646;-shot&#21644;&#23569;-shot&#23398;&#20064;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#24320;&#28304;&#21307;&#23398;LLMs&#65292;&#24182;&#19988;&#22312;&#21830;&#19994;&#24040;&#22836;&#22914;ChatGPT&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12749v1 Announce Type: cross  Abstract: Recent large language models (LLMs) like ChatGPT and LLaMA have shown great promise in many AI applications. However, their performance on medical tasks is suboptimal and can be further improved by training on large domain-specific datasets. This study introduces Me LLaMA, a medical LLM family including foundation models - Me LLaMA 13/70B and their chat-enhanced versions - Me LLaMA 13/70B-chat, developed through the continual pre-training and instruction tuning of LLaMA2 using large medical data. Our domain-specific data suite for training and evaluation, includes a large-scale continual pre-training dataset with 129B tokens, an instruction tuning dataset with 214k samples, and a medical evaluation benchmark (MIBE) across six tasks with 14 datasets. Our extensive evaluation using MIBE shows that Me LLaMA models surpass existing open-source medical LLMs in zero-shot and few-shot learning and outperform commercial giants like ChatGPT on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20219;&#21153;&#20998;&#37197;&#21644;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#20914;&#31361;&#30340;&#25628;&#32034;&#26041;&#27861;&#35299;&#20915;&#20855;&#26377;&#20808;&#20915;&#21644;&#26102;&#38388;&#32422;&#26463;&#30340;&#20219;&#21153;&#20998;&#37197;&#21644;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#26368;&#22823;&#21270;&#29992;&#25143;&#23450;&#20041;&#30340;&#25351;&#26631;&#30340;&#22238;&#25253;&#12290;</title><link>https://arxiv.org/abs/2402.08772</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#20914;&#31361;&#30340;&#25628;&#32034;&#21644;&#20808;&#20915;&#21644;&#26102;&#38388;&#32422;&#26463;&#30340;&#26368;&#20248;&#20219;&#21153;&#20998;&#37197;&#21644;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Optimal Task Assignment and Path Planning using Conflict-Based Search with Precedence and Temporal Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20219;&#21153;&#20998;&#37197;&#21644;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#20914;&#31361;&#30340;&#25628;&#32034;&#26041;&#27861;&#35299;&#20915;&#20855;&#26377;&#20808;&#20915;&#21644;&#26102;&#38388;&#32422;&#26463;&#30340;&#20219;&#21153;&#20998;&#37197;&#21644;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#26368;&#22823;&#21270;&#29992;&#25143;&#23450;&#20041;&#30340;&#25351;&#26631;&#30340;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#65288;MAPF&#65289;&#38382;&#39064;&#28041;&#21450;&#20026;&#19968;&#32452;&#26234;&#33021;&#20307;&#25214;&#21040;&#26080;&#30896;&#25758;&#36335;&#24452;&#65292;&#24341;&#23548;&#23427;&#20204;&#20174;&#36215;&#28857;&#21040;&#30446;&#26631;&#20301;&#32622;&#12290;&#28982;&#32780;&#65292;MAPF&#27809;&#26377;&#32771;&#34385;&#20960;&#20010;&#23454;&#38469;&#30340;&#20219;&#21153;&#30456;&#20851;&#32422;&#26463;&#12290;&#20363;&#22914;&#65292;&#26234;&#33021;&#20307;&#21487;&#33021;&#38656;&#35201;&#22312;&#30446;&#26631;&#20301;&#32622;&#25191;&#34892;&#20855;&#26377;&#29305;&#23450;&#25191;&#34892;&#26102;&#38388;&#30340;&#21160;&#20316;&#65292;&#36981;&#24490;&#39044;&#23450;&#30340;&#39034;&#24207;&#21644;&#26102;&#38388;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#30446;&#26631;&#20998;&#37197;&#21487;&#33021;&#19981;&#26159;&#39044;&#20808;&#23450;&#20041;&#30340;&#65292;&#20248;&#21270;&#30446;&#26631;&#21487;&#33021;&#32570;&#20047;&#26126;&#30830;&#23450;&#20041;&#12290;&#20026;&#20102;&#23558;&#20219;&#21153;&#20998;&#37197;&#12289;&#36335;&#24452;&#35268;&#21010;&#21644;&#29992;&#25143;&#23450;&#20041;&#30340;&#30446;&#26631;&#32467;&#21512;&#25104;&#19968;&#20010;&#36830;&#36143;&#30340;&#26694;&#26550;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#20808;&#20915;&#21644;&#26102;&#38388;&#32422;&#26463;&#30340;&#20219;&#21153;&#20998;&#37197;&#21644;&#36335;&#24452;&#35268;&#21010;&#65288;TAPF-PTC&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#22686;&#21152;&#20102;&#22522;&#20110;&#20914;&#31361;&#30340;&#25628;&#32034;&#65288;CBS&#65289;&#20197;&#21516;&#26102;&#29983;&#25104;&#36981;&#23432;&#20808;&#20915;&#21644;&#26102;&#38388;&#32422;&#26463;&#30340;&#20219;&#21153;&#20998;&#37197;&#21644;&#26080;&#30896;&#25758;&#36335;&#24452;&#65292;&#24182;&#26368;&#22823;&#21270;&#22522;&#20110;&#29992;&#25143;&#23450;&#20041;&#30340;&#25351;&#26631;&#30340;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08772v1 Announce Type: new Abstract: The Multi-Agent Path Finding (MAPF) problem entails finding collision-free paths for a set of agents, guiding them from their start to goal locations. However, MAPF does not account for several practical task-related constraints. For example, agents may need to perform actions at goal locations with specific execution times, adhering to predetermined orders and timeframes. Moreover, goal assignments may not be predefined for agents, and the optimization objective may lack an explicit definition. To incorporate task assignment, path planning, and a user-defined objective into a coherent framework, this paper examines the Task Assignment and Path Finding with Precedence and Temporal Constraints (TAPF-PTC) problem. We augment Conflict-Based Search (CBS) to simultaneously generate task assignments and collision-free paths that adhere to precedence and temporal constraints, maximizing an objective quantified by the return from a user-defined r
&lt;/p&gt;</description></item><item><title>LLaGA&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#27169;&#22411;&#65292;&#23427;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#65292;&#20197;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#37325;&#26032;&#32452;&#32455;&#22270;&#33410;&#28857;&#20197;&#20316;&#20026;&#32467;&#26500;&#24863;&#30693;&#30340;&#24207;&#21015;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#22810;&#21151;&#33021;&#25237;&#24433;&#20202;&#23558;&#20854;&#26144;&#23556;&#21040;&#26631;&#35760;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;LLaGA&#22312;&#22810;&#26679;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.08170</link><description>&lt;p&gt;
LLaGA: &#22823;&#22411;&#35821;&#35328;&#21644;&#22270;&#24418;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
LLaGA: Large Language and Graph Assistant
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08170
&lt;/p&gt;
&lt;p&gt;
LLaGA&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#27169;&#22411;&#65292;&#23427;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#65292;&#20197;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#37325;&#26032;&#32452;&#32455;&#22270;&#33410;&#28857;&#20197;&#20316;&#20026;&#32467;&#26500;&#24863;&#30693;&#30340;&#24207;&#21015;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#22810;&#21151;&#33021;&#25237;&#24433;&#20202;&#23558;&#20854;&#26144;&#23556;&#21040;&#26631;&#35760;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;LLaGA&#22312;&#22810;&#26679;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#32463;&#25512;&#21160;&#20102;&#22270;&#32467;&#26500;&#25968;&#25454;&#20998;&#26512;&#30340;&#36827;&#27493;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;GPT-4&#30340;&#23835;&#36215;&#39044;&#31034;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#19968;&#20010;&#26032;&#26102;&#20195;&#12290;&#28982;&#32780;&#65292;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#22270;&#25968;&#25454;&#36824;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#30001;&#20110;&#23558;&#22270;&#32467;&#26500;&#36716;&#21270;&#20026;&#25991;&#26412;&#30340;&#22266;&#26377;&#38590;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21019;&#26032;&#27169;&#22411;&#8212;&#8212;&#22823;&#22411;&#35821;&#35328;&#21644;&#22270;&#24418;&#21161;&#25163;&#65288;LLaGA&#65289;&#65292;&#23427;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;LLM&#30340;&#33021;&#21147;&#65292;&#20197;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;LLaGA&#20445;&#30041;&#20102;LLM&#30340;&#36890;&#29992;&#24615;&#65292;&#21516;&#26102;&#23558;&#22270;&#25968;&#25454;&#36716;&#21270;&#20026;&#19982;LLM&#36755;&#20837;&#20860;&#23481;&#30340;&#26684;&#24335;&#12290;LLaGA&#36890;&#36807;&#37325;&#26032;&#32452;&#32455;&#22270;&#33410;&#28857;&#20197;&#20316;&#20026;&#32467;&#26500;&#24863;&#30693;&#30340;&#24207;&#21015;&#65292;&#28982;&#21518;&#36890;&#36807;&#19968;&#20010;&#22810;&#21151;&#33021;&#25237;&#24433;&#20202;&#23558;&#20854;&#26144;&#23556;&#21040;&#26631;&#35760;&#23884;&#20837;&#31354;&#38388;&#20013;&#12290;LLaGA&#22312;&#22810;&#26679;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have empowered the advance in graph-structured data analysis. Recently, the rise of Large Language Models (LLMs) like GPT-4 has heralded a new era in deep learning. However, their application to graph data poses distinct challenges due to the inherent difficulty of translating graph structures to language. To this end, we introduce the \textbf{L}arge \textbf{L}anguage \textbf{a}nd \textbf{G}raph \textbf{A}ssistant (\textbf{LLaGA}), an innovative model that effectively integrates LLM capabilities to handle the complexities of graph-structured data. LLaGA retains the general-purpose nature of LLMs while adapting graph data into a format compatible with LLM input. LLaGA achieves this by reorganizing graph nodes to structure-aware sequences and then mapping these into the token embedding space through a versatile projector. LLaGA excels in versatility, generalizability and interpretability, allowing it to perform consistently well across different datasets and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;LLaDA&#65292;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#65292;&#20351;&#24471;&#39550;&#39542;&#21592;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#33021;&#22815;&#22312;&#21508;&#22320;&#39550;&#39542;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#21644;&#36816;&#21160;&#35745;&#21010;&#35843;&#25972;&#21040;&#26032;&#20301;&#32622;&#30340;&#20132;&#36890;&#35268;&#21017;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;LLaDA&#25351;&#23548;&#22312;&#35299;&#20915;&#24847;&#22806;&#24773;&#20917;&#21644;&#36866;&#24212;AV&#36816;&#21160;&#35745;&#21010;&#31574;&#30053;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05932</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25919;&#31574;&#36866;&#24212;&#22312;&#21508;&#22788;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Driving Everywhere with Large Language Model Policy Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;LLaDA&#65292;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#65292;&#20351;&#24471;&#39550;&#39542;&#21592;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#33021;&#22815;&#22312;&#21508;&#22320;&#39550;&#39542;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#21644;&#36816;&#21160;&#35745;&#21010;&#35843;&#25972;&#21040;&#26032;&#20301;&#32622;&#30340;&#20132;&#36890;&#35268;&#21017;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;LLaDA&#25351;&#23548;&#22312;&#35299;&#20915;&#24847;&#22806;&#24773;&#20917;&#21644;&#36866;&#24212;AV&#36816;&#21160;&#35745;&#21010;&#31574;&#30053;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#20013;&#65292;&#23558;&#39550;&#39542;&#34892;&#20026;&#36866;&#24212;&#26032;&#29615;&#22659;&#12289;&#20064;&#20439;&#21644;&#27861;&#24459;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#36825;&#38480;&#21046;&#20102;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;(AVs)&#30340;&#24191;&#27867;&#37096;&#32626;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;LLaDA&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#20351;&#20154;&#31867;&#39550;&#39542;&#21592;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#37117;&#33021;&#22312;&#21508;&#22788;&#34892;&#39542;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#21644;&#36816;&#21160;&#35745;&#21010;&#35843;&#25972;&#21040;&#26032;&#20301;&#32622;&#30340;&#20132;&#36890;&#35268;&#21017;&#12290;LLaDA&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35299;&#37322;&#26412;&#22320;&#39550;&#39542;&#25163;&#20876;&#20013;&#30340;&#20132;&#36890;&#35268;&#21017;&#26102;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;-shot&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLaDA&#30340;&#25351;&#23548;&#22312;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#24847;&#22806;&#24773;&#20917;&#26102;&#30340;&#26377;&#29992;&#24615;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;LLaDA&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#35843;&#25972;AV&#36816;&#21160;&#35745;&#21010;&#31574;&#30053;&#30340;&#33021;&#21147;&#65307;LLaDA&#22312;&#25152;&#26377;&#25351;&#26631;&#19978;&#20248;&#20110;&#22522;&#32447;&#35268;&#21010;&#26041;&#27861;&#12290;&#35831;&#35775;&#38382;&#25105;&#20204;&#30340;&#32593;&#31449;&#20102;&#35299;&#26356;&#22810;&#35814;&#32454;&#20449;&#24687;&#65306;https://boyiliee.github.io/llada.
&lt;/p&gt;
&lt;p&gt;
Adapting driving behavior to new environments, customs, and laws is a long-standing problem in autonomous driving, precluding the widespread deployment of autonomous vehicles (AVs). In this paper, we present LLaDA, a simple yet powerful tool that enables human drivers and autonomous vehicles alike to drive everywhere by adapting their tasks and motion plans to traffic rules in new locations. LLaDA achieves this by leveraging the impressive zero-shot generalizability of large language models (LLMs) in interpreting the traffic rules in the local driver handbook. Through an extensive user study, we show that LLaDA's instructions are useful in disambiguating in-the-wild unexpected situations. We also demonstrate LLaDA's ability to adapt AV motion planning policies in real-world datasets; LLaDA outperforms baseline planning approaches on all our metrics. Please check our website for more details: https://boyiliee.github.io/llada.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;RMSProp&#21450;&#20854;&#21160;&#37327;&#25193;&#23637;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;$\ell_1$&#33539;&#25968;&#27979;&#24230;&#26102;&#65292;&#25910;&#25947;&#36895;&#24230;&#20026;$O(\frac{\sqrt{d}}{T^{1/4}})$&#65292;&#22312;&#32500;&#24230;&#26497;&#22823;&#30340;&#38382;&#39064;&#20013;&#20855;&#26377;&#25913;&#36827;&#20381;&#36182;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00389</link><description>&lt;p&gt;
&#20851;&#20110;RMSProp&#21450;&#20854;&#21160;&#37327;&#25193;&#23637;&#26041;&#27861;&#30340;$O(\frac{\sqrt{d}}{T^{1/4}})$&#25910;&#25947;&#36895;&#24230;&#21644;&#23545;&#32500;&#24230;&#30340;&#25913;&#36827;&#20381;&#36182;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the $O(\frac{\sqrt{d}}{T^{1/4}})$ Convergence Rate of RMSProp and Its Momentum Extension Measured by $\ell_1$ Norm: Better Dependence on the Dimension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00389
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;RMSProp&#21450;&#20854;&#21160;&#37327;&#25193;&#23637;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;$\ell_1$&#33539;&#25968;&#27979;&#24230;&#26102;&#65292;&#25910;&#25947;&#36895;&#24230;&#20026;$O(\frac{\sqrt{d}}{T^{1/4}})$&#65292;&#22312;&#32500;&#24230;&#26497;&#22823;&#30340;&#38382;&#39064;&#20013;&#20855;&#26377;&#25913;&#36827;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#20854;&#25910;&#25947;&#36895;&#24230;&#23578;&#26410;&#24471;&#21040;&#24443;&#24213;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20854;&#23545;&#32500;&#24230;&#30340;&#20381;&#36182;&#24615;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#32463;&#20856;&#30340;RMSProp&#21450;&#20854;&#21160;&#37327;&#25193;&#23637;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;$\ell_1$&#33539;&#25968;&#24314;&#31435;&#20102;&#25910;&#25947;&#29575;$\frac{1}{T}\sum_{k=1}^TE\left[\|\nabla f(x^k)\|_1\right]\leq O(\frac{\sqrt{d}}{T^{1/4}})$&#65292;&#26080;&#38656;&#20551;&#35774;&#26799;&#24230;&#26377;&#30028;&#65292;&#20854;&#20013;$d$&#26159;&#20248;&#21270;&#21464;&#37327;&#30340;&#32500;&#24230;&#65292;$T$&#26159;&#36845;&#20195;&#27425;&#25968;&#12290;&#30001;&#20110;&#23545;&#20110;&#32500;&#24230;&#26497;&#22823;&#30340;&#38382;&#39064;&#65292;$\|x\|_2\ll\|x\|_1\leq\sqrt{d}\|x\|_2$&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#25910;&#25947;&#36895;&#24230;&#21487;&#20197;&#31867;&#27604;&#20026;SGD&#30340;$\frac{1}{T}\sum_{k=1}^TE\left[\|\nabla f(x^k)\|_2\right]\leq O(\frac{1}{T^{1/4}})$&#65292;&#27979;&#24230;&#20026;$\ell_1$&#33539;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although adaptive gradient methods have been extensively used in deep learning, their convergence rates have not been thoroughly studied, particularly with respect to their dependence on the dimension. This paper considers the classical RMSProp and its momentum extension and establishes the convergence rate of $\frac{1}{T}\sum_{k=1}^TE\left[\|\nabla f(x^k)\|_1\right]\leq O(\frac{\sqrt{d}}{T^{1/4}})$ measured by $\ell_1$ norm without the bounded gradient assumption, where $d$ is the dimension of the optimization variable and $T$ is the iteration number. Since $\|x\|_2\ll\|x\|_1\leq\sqrt{d}\|x\|_2$ for problems with extremely large $d$, our convergence rate can be considered to be analogous to the $\frac{1}{T}\sum_{k=1}^TE\left[\|\nabla f(x^k)\|_2\right]\leq O(\frac{1}{T^{1/4}})$ one of SGD measured by $\ell_1$ norm.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20998;&#23376;&#37197;&#32622;&#36716;&#25442;&#22120;&#29983;&#25104;&#39640;&#31934;&#24230;&#21147;&#22330;&#65292;&#20197;&#35299;&#20915;&#35745;&#31639;&#36895;&#24230;&#38480;&#21046;&#65292;&#20174;&#32780;&#22312;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#20013;&#30740;&#31350;&#21270;&#23398;&#21453;&#24212;&#26426;&#29702;&#12290;</title><link>https://arxiv.org/abs/2401.00499</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#23376;&#37197;&#32622;&#36716;&#25442;&#22120;&#29983;&#25104;&#29992;&#20110;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#30340;&#39640;&#31934;&#24230;&#21147;&#22330;&#65292;&#30740;&#31350;&#21270;&#23398;&#21453;&#24212;&#26426;&#29702;
&lt;/p&gt;
&lt;p&gt;
Generating High-Precision Force Fields for Molecular Dynamics Simulations to Study Chemical Reaction Mechanisms using Molecular Configuration Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.00499
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20998;&#23376;&#37197;&#32622;&#36716;&#25442;&#22120;&#29983;&#25104;&#39640;&#31934;&#24230;&#21147;&#22330;&#65292;&#20197;&#35299;&#20915;&#35745;&#31639;&#36895;&#24230;&#38480;&#21046;&#65292;&#20174;&#32780;&#22312;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#20013;&#30740;&#31350;&#21270;&#23398;&#21453;&#24212;&#26426;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26377;&#26426;&#21270;&#23398;&#20013;&#65292;&#21270;&#23398;&#21453;&#24212;&#26426;&#29702;&#30340;&#29702;&#35770;&#30740;&#31350;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#19978;&#65292;&#20351;&#29992;&#37327;&#23376;&#21270;&#23398;&#35745;&#31639;&#35745;&#31639;&#25163;&#21160;&#26500;&#24314;&#30340;&#36807;&#28193;&#24577;&#20998;&#23376;&#26500;&#22411;&#26159;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#20010;&#20154;&#32463;&#39564;&#21644;&#21270;&#23398;&#30452;&#35273;&#12290;&#22312;&#25105;&#20204;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#20013;&#30340;&#22686;&#24378;&#37319;&#26679;&#26469;&#30740;&#31350;&#21270;&#23398;&#21453;&#24212;&#30340;&#30740;&#31350;&#33539;&#24335;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#30452;&#25509;&#27169;&#25311;&#21270;&#23398;&#21453;&#24212;&#30340;&#25972;&#20010;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#35745;&#31639;&#36895;&#24230;&#38480;&#21046;&#20102;&#29992;&#20110;&#27169;&#25311;&#30340;&#39640;&#31934;&#24230;&#21183;&#33021;&#20989;&#25968;&#30340;&#20351;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#26696;&#65292;&#20351;&#29992;&#20808;&#21069;&#24320;&#21457;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#23376;&#27169;&#22411;-&#20998;&#23376;&#37197;&#32622;&#36716;&#25442;&#22120;&#26469;&#35757;&#32451;&#39640;&#31934;&#24230;&#20998;&#23376;&#24314;&#27169;&#21147;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.00499v2 Announce Type: replace-cross  Abstract: Theoretical studies on chemical reaction mechanisms have been crucial in organic chemistry. Traditionally, calculating the manually constructed molecular conformations of transition states for chemical reactions using quantum chemical calculations is the most commonly used method. However, this way is heavily dependent on individual experience and chemical intuition. In our previous study, we proposed a research paradigm that uses enhanced sampling in molecular dynamics simulations to study chemical reactions. This approach can directly simulate the entire process of a chemical reaction. However, the computational speed limits the use of high-precision potential energy functions for simulations. To address this issue, we present a scheme for training high-precision force fields for molecular modeling using a previously developed graph-neural-network-based molecular model, molecular configuration transformer. This potential ener
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;ViSFT&#65288;Vision SFT&#65289;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39046;&#22495;&#20869;&#20219;&#21153;&#19978;&#36827;&#34892;&#35270;&#35273;&#32852;&#21512;&#23398;&#20064;&#26469;&#25552;&#21319;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#39046;&#22495;&#22806;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.10222</link><description>&lt;p&gt;
&#30417;&#30563;&#24494;&#35843;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Supervised Fine-tuning in turn Improves Visual Foundation Models. (arXiv:2401.10222v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;ViSFT&#65288;Vision SFT&#65289;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39046;&#22495;&#20869;&#20219;&#21153;&#19978;&#36827;&#34892;&#35270;&#35273;&#32852;&#21512;&#23398;&#20064;&#26469;&#25552;&#21319;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#39046;&#22495;&#22806;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20687;CLIP&#36825;&#26679;&#30340;&#22270;&#20687;-&#25991;&#26412;&#35757;&#32451;&#26041;&#27861;&#24050;&#32463;&#20027;&#23548;&#20102;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#12290;&#38543;&#21518;&#65292;&#20026;&#20102;&#23558;&#21306;&#22495;&#32423;&#21035;&#30340;&#35270;&#35273;&#23398;&#20064;&#24341;&#20837;CLIP&#30340;&#39044;&#35757;&#32451;&#20013;&#65292;&#22312;&#22823;&#35268;&#27169;&#21306;&#22495;&#32423;&#21035;&#25968;&#25454;&#38598;&#30340;&#32570;&#20047;&#19979;&#38754;&#20020;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#12290;&#21463;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#30340;&#21551;&#21457;&#65292;&#27604;&#22914;&#25351;&#20196;&#24494;&#35843;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22312;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#20043;&#21518;&#65292;&#24494;&#31890;SFT&#33021;&#22815;&#25552;&#21319;&#20854;&#29983;&#25104;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26041;&#27861;ViSFT&#65288;Vision SFT&#65289;&#65292;&#26469;&#37322;&#25918;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#30693;&#35782;&#12290;&#22312;ViSFT&#20013;&#65292;&#36890;&#36807;&#22312;&#19968;&#20123;&#39046;&#22495;&#20869;&#20219;&#21153;&#19978;&#36827;&#34892;&#35270;&#35273;&#32852;&#21512;&#23398;&#20064;&#26469;&#22686;&#24378;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#39046;&#22495;&#22806;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#27979;&#35797;&#12290;&#36890;&#36807;&#20351;&#29992;ViSFT&#22312;&#23569;&#20110;2&#22825;&#20869;&#22312;8&#20010;V100 GPU&#19978;&#36827;&#34892;&#26356;&#26032;&#65292;&#19968;&#20010;&#20855;&#26377;&#36229;&#36807;4.4B&#21442;&#25968;&#30340;&#35270;&#35273;Transformer&#22312;&#21508;&#31181;&#39046;&#22495;&#22806;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-text training like CLIP has dominated the pretraining of vision foundation models in recent years. Subsequent efforts have been made to introduce region-level visual learning into CLIP's pretraining but face scalability challenges due to the lack of large-scale region-level datasets. Drawing inspiration from supervised fine-tuning (SFT) in natural language processing such as instruction tuning, we explore the potential of fine-grained SFT in enhancing the generation of vision foundation models after their pretraining. Thus a two-stage method ViSFT (Vision SFT) is proposed to unleash the fine-grained knowledge of vision foundation models. In ViSFT, the vision foundation model is enhanced by performing visual joint learning on some in-domain tasks and then tested on out-of-domain benchmarks. With updating using ViSFT on 8 V100 GPUs in less than 2 days, a vision transformer with over 4.4B parameters shows improvements across various out-of-domain benchmarks including vision and visi
&lt;/p&gt;</description></item><item><title>EgoGen&#26159;&#19968;&#31181;&#26032;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#23427;&#21487;&#20197;&#20026;&#20855;&#20307;&#21270;&#33258;&#25105;&#30340;&#24863;&#30693;&#20219;&#21153;&#29983;&#25104;&#20934;&#30830;&#21644;&#20016;&#23500;&#30340;&#22320;&#38754;&#30495;&#23454;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#21033;&#29992;&#33258;&#25105;&#35270;&#35273;&#36755;&#20837;&#26469;&#24863;&#30693;3D&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2401.08739</link><description>&lt;p&gt;
EgoGen:&#19968;&#31181;&#33258;&#25105;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
EgoGen: An Egocentric Synthetic Data Generator. (arXiv:2401.08739v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08739
&lt;/p&gt;
&lt;p&gt;
EgoGen&#26159;&#19968;&#31181;&#26032;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#23427;&#21487;&#20197;&#20026;&#20855;&#20307;&#21270;&#33258;&#25105;&#30340;&#24863;&#30693;&#20219;&#21153;&#29983;&#25104;&#20934;&#30830;&#21644;&#20016;&#23500;&#30340;&#22320;&#38754;&#30495;&#23454;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#21033;&#29992;&#33258;&#25105;&#35270;&#35273;&#36755;&#20837;&#26469;&#24863;&#30693;3D&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22686;&#24378;&#29616;&#23454;&#65288;AR&#65289;&#20013;&#65292;&#20197;&#31532;&#19968;&#20154;&#31216;&#35270;&#35282;&#29702;&#35299;&#19990;&#30028;&#26159;&#22522;&#30784;&#24615;&#30340;&#12290;&#36825;&#31181;&#36523;&#20020;&#20854;&#22659;&#30340;&#35270;&#35282;&#30456;&#23545;&#20110;&#31532;&#19977;&#20154;&#31216;&#35270;&#35282;&#24102;&#26469;&#20102;&#25103;&#21095;&#24615;&#30340;&#35270;&#35273;&#21464;&#21270;&#21644;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#21512;&#25104;&#25968;&#25454;&#24050;&#32463;&#36171;&#20104;&#20102;&#31532;&#19977;&#20154;&#31216;&#35270;&#35282;&#35270;&#35273;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20294;&#20854;&#24212;&#29992;&#20110;&#20855;&#20307;&#21270;&#33258;&#25105;&#30340;&#24863;&#30693;&#20219;&#21153;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#22312;&#20110;&#27169;&#25311;&#33258;&#28982;&#30340;&#20154;&#31867;&#36816;&#21160;&#21644;&#34892;&#20026;&#65292;&#20197;&#26377;&#25928;&#22320;&#25805;&#32437;&#23454;&#20307;&#30456;&#26426;&#25429;&#25417;&#21040;3D&#19990;&#30028;&#30340;&#30495;&#23454;&#33258;&#25105;&#30340;&#34920;&#36798;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;EgoGen&#65292;&#23427;&#21487;&#20197;&#20026;&#20855;&#20307;&#21270;&#33258;&#25105;&#30340;&#24863;&#30693;&#20219;&#21153;&#29983;&#25104;&#20934;&#30830;&#21644;&#20016;&#23500;&#30340;&#22320;&#38754;&#30495;&#23454;&#35757;&#32451;&#25968;&#25454;&#12290;EgoGen&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;&#20307;&#36816;&#21160;&#21512;&#25104;&#27169;&#22411;&#65292;&#23427;&#30452;&#25509;&#21033;&#29992;&#34394;&#25311;&#20154;&#31867;&#30340;&#33258;&#25105;&#35270;&#35273;&#36755;&#20837;&#24863;&#30693;3D&#29615;&#22659;&#12290;&#32467;&#21512;&#36991;&#20813;&#30896;&#25758;&#30340;&#36816;&#21160;&#22522;&#20803;&#21644;&#20004;&#38454;&#27573;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#36816;&#21160;&#21512;&#25104;&#27169;&#22411;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#33258;&#25105;&#30340;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the world in first-person view is fundamental in Augmented Reality (AR). This immersive perspective brings dramatic visual changes and unique challenges compared to third-person views. Synthetic data has empowered third-person-view vision models, but its application to embodied egocentric perception tasks remains largely unexplored. A critical challenge lies in simulating natural human movements and behaviors that effectively steer the embodied cameras to capture a faithful egocentric representation of the 3D world. To address this challenge, we introduce EgoGen, a new synthetic data generator that can produce accurate and rich ground-truth training data for egocentric perception tasks. At the heart of EgoGen is a novel human motion synthesis model that directly leverages egocentric visual inputs of a virtual human to sense the 3D environment. Combined with collision-avoiding motion primitives and a two-stage reinforcement learning approach, our motion synthesis model off
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#33258;&#21160;&#21270;&#30340;&#26426;&#21046;&#35774;&#35745;&#65292;&#29992;&#20110;&#22312;Feed&#20013;&#38598;&#25104;&#24191;&#21578;&#25293;&#21334;&#21644;&#20998;&#37197;&#12290;&#36825;&#31181;&#35774;&#35745;&#35299;&#20915;&#20102;&#24191;&#21578;&#25293;&#21334;&#19981;&#32771;&#34385;&#22806;&#37096;&#24615;&#21644;&#24191;&#21578;&#20998;&#37197;&#26080;&#27861;&#20445;&#25345;&#28608;&#21169;&#20860;&#23481;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.01656</link><description>&lt;p&gt;
&#22312;Feed&#20013;&#38598;&#25104;&#24191;&#21578;&#25293;&#21334;&#21644;&#20998;&#37197;&#30340;&#28145;&#24230;&#33258;&#21160;&#21270;&#26426;&#21046;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Deep Automated Mechanism Design for Integrating Ad Auction and Allocation in Feed. (arXiv:2401.01656v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#33258;&#21160;&#21270;&#30340;&#26426;&#21046;&#35774;&#35745;&#65292;&#29992;&#20110;&#22312;Feed&#20013;&#38598;&#25104;&#24191;&#21578;&#25293;&#21334;&#21644;&#20998;&#37197;&#12290;&#36825;&#31181;&#35774;&#35745;&#35299;&#20915;&#20102;&#24191;&#21578;&#25293;&#21334;&#19981;&#32771;&#34385;&#22806;&#37096;&#24615;&#21644;&#24191;&#21578;&#20998;&#37197;&#26080;&#27861;&#20445;&#25345;&#28608;&#21169;&#20860;&#23481;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#21709;&#24212;&#27599;&#20010;&#29992;&#25143;&#30340;&#39029;&#38754;&#35831;&#27714;&#65292;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#36890;&#24120;&#21576;&#29616;&#19968;&#20010;&#26377;&#24207;&#21015;&#34920;&#65292;&#20854;&#20013;&#21253;&#21547;&#20960;&#20010;&#26377;&#26426;&#29289;&#21697;&#21644;&#19968;&#20010;&#24191;&#21578;&#12290;&#36825;&#20010;&#21015;&#34920;&#26159;&#24191;&#21578;&#25293;&#21334;&#21644;&#20998;&#37197;&#36807;&#31243;&#30340;&#32467;&#26524;&#65292;&#30452;&#25509;&#24433;&#21709;&#24179;&#21488;&#30340;&#24191;&#21578;&#25910;&#20837;&#21644;&#24635;&#20132;&#26131;&#39069;&#12290;&#24191;&#21578;&#25293;&#21334;&#30830;&#23450;&#26174;&#31034;&#30340;&#24191;&#21578;&#21644;&#30456;&#24212;&#30340;&#25903;&#20184;&#65292;&#32780;&#24191;&#21578;&#20998;&#37197;&#20915;&#23450;&#24191;&#21578;&#21644;&#26377;&#26426;&#29289;&#21697;&#30340;&#26174;&#31034;&#20301;&#32622;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#23558;&#24191;&#21578;&#25293;&#21334;&#21644;&#20998;&#37197;&#20998;&#20026;&#20004;&#20010;&#19981;&#21516;&#30340;&#38454;&#27573;&#65292;&#38754;&#20020;&#20004;&#20010;&#38382;&#39064;&#65306;1&#65289;&#24191;&#21578;&#25293;&#21334;&#27809;&#26377;&#32771;&#34385;&#22806;&#37096;&#24615;&#65292;&#22914;&#23454;&#38469;&#26174;&#31034;&#20301;&#32622;&#21644;&#19978;&#19979;&#25991;&#23545;&#24191;&#21578;&#28857;&#20987;&#29575;&#30340;&#24433;&#21709;&#65307;2&#65289;&#24191;&#21578;&#20998;&#37197;&#21033;&#29992;&#25293;&#21334;&#33719;&#32988;&#24191;&#21578;&#30340;&#25903;&#20184;&#26469;&#21160;&#24577;&#30830;&#23450;&#26174;&#31034;&#20301;&#32622;&#65292;&#26410;&#33021;&#20445;&#25345;&#24191;&#21578;&#30340;&#28608;&#21169;&#20860;&#23481;&#24615;&#12290;&#20363;&#22914;&#65292;&#22312;&#20351;&#29992;&#20256;&#32479;&#30340;&#24191;&#20041;&#20108;&#20215;(GSP)&#25293;&#21334;&#38454;&#27573;&#65292;&#24191;&#21578;&#20998;&#37197;&#38454;&#27573;&#26080;&#27861;&#32500;&#25345;&#28608;&#21169;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
E-commerce platforms usually present an ordered list, mixed with several organic items and an advertisement, in response to each user's page view request. This list, the outcome of ad auction and allocation processes, directly impacts the platform's ad revenue and gross merchandise volume (GMV). Specifically, the ad auction determines which ad is displayed and the corresponding payment, while the ad allocation decides the display positions of the advertisement and organic items. The prevalent methods of segregating the ad auction and allocation into two distinct stages face two problems: 1) Ad auction does not consider externalities, such as the influence of actual display position and context on ad Click-Through Rate (CTR); 2) The ad allocation, which utilizes the auction-winning ad's payment to determine the display position dynamically, fails to maintain incentive compatibility (IC) for the advertisement. For instance, in the auction stage employing the traditional Generalized Secon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20219;&#21153;&#38656;&#27714;&#30340;&#33258;&#20027;&#24230;&#35780;&#20272;&#26694;&#26550;&#65292;&#20027;&#35201;&#20851;&#27880;&#23436;&#20840;&#33258;&#20027;&#27169;&#24335;&#65292;&#36890;&#36807;&#30830;&#23450;&#26426;&#22120;&#20154;&#20219;&#21153;&#29305;&#24615;&#65292;&#25512;&#23548;&#20986;&#33258;&#20027;&#24230;&#30340;&#19977;&#20010;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#23558;&#33258;&#20027;&#24230;&#20998;&#20026;&#33258;&#20027;&#24230;&#27700;&#24179;&#21644;&#33258;&#20027;&#24230;&#31243;&#24230;&#20004;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2311.01939</link><description>&lt;p&gt;
&#33258;&#20027;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#37327;&#21270;&#33258;&#20027;&#24230;&#35780;&#20215;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Quantitative Autonomy Quantification Framework for Fully Autonomous Robotic Systems. (arXiv:2311.01939v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20219;&#21153;&#38656;&#27714;&#30340;&#33258;&#20027;&#24230;&#35780;&#20272;&#26694;&#26550;&#65292;&#20027;&#35201;&#20851;&#27880;&#23436;&#20840;&#33258;&#20027;&#27169;&#24335;&#65292;&#36890;&#36807;&#30830;&#23450;&#26426;&#22120;&#20154;&#20219;&#21153;&#29305;&#24615;&#65292;&#25512;&#23548;&#20986;&#33258;&#20027;&#24230;&#30340;&#19977;&#20010;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#23558;&#33258;&#20027;&#24230;&#20998;&#20026;&#33258;&#20027;&#24230;&#27700;&#24179;&#21644;&#33258;&#20027;&#24230;&#31243;&#24230;&#20004;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#20027;&#21151;&#33021;&#20419;&#36827;&#20102;&#26426;&#22120;&#20154;&#31995;&#32479;&#22312;&#26377;&#38480;&#20154;&#31867;&#30417;&#30563;&#30340;&#39046;&#22495;&#21450;&#26356;&#36828;&#22788;&#30340;&#37096;&#32626;&#65292;&#20294;&#22312;&#20219;&#21153;&#38656;&#27714;&#21644;&#33258;&#20027;&#33021;&#21147;&#20043;&#38388;&#25214;&#21040;&#23545;&#24212;&#20851;&#31995;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#22312;&#36807;&#21435;&#19977;&#21313;&#24180;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#37327;&#21270;&#33258;&#20027;&#24230;&#30340;&#26041;&#27861;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25152;&#26377;&#36825;&#20123;&#26041;&#27861;&#37117;&#27809;&#26377;&#36776;&#21035;&#33258;&#20027;&#24230;&#21464;&#21270;&#30340;&#23376;&#27169;&#24335;&#29305;&#24449;&#65292;&#24182;&#19988;&#19968;&#20123;&#26041;&#27861;&#22522;&#20110;&#36829;&#21453;Goodhart&#23450;&#24459;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#23436;&#20840;&#33258;&#20027;&#27169;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20219;&#21153;&#38656;&#27714;&#30340;&#33258;&#20027;&#24230;&#35780;&#20272;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#30830;&#23450;&#26426;&#22120;&#20154;&#20219;&#21153;&#29305;&#24615;&#65292;&#25512;&#23548;&#20986;&#19977;&#20010;&#33258;&#20027;&#24230;&#24230;&#37327;&#26631;&#20934;&#65292;&#21363;&#25152;&#38656;&#33021;&#21147;&#12289;&#21487;&#38752;&#24615;&#21644;&#21709;&#24212;&#24615;&#65292;&#24182;&#36890;&#36807;&#30830;&#23450;&#33258;&#20027;&#24230;&#27700;&#24179;&#21644;&#33258;&#20027;&#24230;&#31243;&#24230;&#30340;&#20989;&#25968;&#65292;&#23558;&#33258;&#20027;&#24230;&#20316;&#20026;&#20004;&#37096;&#20998;&#24230;&#37327;&#26469;&#34913;&#37327;&#12290;&#36825;&#20123;&#29305;&#24615;&#22522;&#20110;&#26426;&#22120;&#20154;&#26368;&#32456;&#21462;&#20195;&#20154;&#31867;&#30340;&#35266;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although autonomous functioning facilitates deployment of robotic systems in domains that admit limited human oversight on our planet and beyond, finding correspondence between task requirements and autonomous capability is still an open challenge. Consequently, a number of methods for quantifying autonomy have been proposed over the last three decades, but to our knowledge all these have no discernment of sub-mode features of variation of autonomy and some are based on metrics that violet the Goodhart's law. This paper focuses on the full autonomous mode and proposes a task-requirements based autonomy assessment framework. The framework starts by establishing robot task characteristics from which three autonomy metrics, namely requisite capability, reliability and responsiveness, and functions for determining autonomy as a two-part measure, namely of level of autonomy and degree of autonomy are derived. These characteristics are founded on the realization that robots ultimately replac
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#19977;&#27573;&#35770;&#25512;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#36739;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#21512;&#36923;&#36753;&#65292;&#29978;&#33267;&#27604;&#20154;&#31867;&#26356;&#21512;&#36923;&#36753;&#65292;&#20294;&#21363;&#20351;&#26368;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#20063;&#20250;&#20986;&#29616;&#19982;&#20154;&#31867;&#25512;&#29702;&#31867;&#20284;&#30340;&#38169;&#35823;&#65292;&#24635;&#20307;&#19978;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#33021;&#22815;&#20811;&#26381;&#20154;&#31867;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2311.00445</link><description>&lt;p&gt;
&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19977;&#27573;&#35770;&#25512;&#29702;&#30340;&#31995;&#32479;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models. (arXiv:2311.00445v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00445
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#19977;&#27573;&#35770;&#25512;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#36739;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#21512;&#36923;&#36753;&#65292;&#29978;&#33267;&#27604;&#20154;&#31867;&#26356;&#21512;&#36923;&#36753;&#65292;&#20294;&#21363;&#20351;&#26368;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#20063;&#20250;&#20986;&#29616;&#19982;&#20154;&#31867;&#25512;&#29702;&#31867;&#20284;&#30340;&#38169;&#35823;&#65292;&#24635;&#20307;&#19978;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#33021;&#22815;&#20811;&#26381;&#20154;&#31867;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#24615;&#34892;&#20026;&#30340;&#19968;&#20010;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#26159;&#36923;&#36753;&#25512;&#29702;&#65306;&#30830;&#23450;&#21738;&#20123;&#32467;&#35770;&#21487;&#20197;&#20174;&#19968;&#32452;&#21069;&#25552;&#20013;&#24471;&#20986;&#12290;&#24515;&#29702;&#23398;&#23478;&#24050;&#32463;&#35760;&#24405;&#19979;&#20154;&#31867;&#25512;&#29702;&#19982;&#36923;&#36753;&#35268;&#21017;&#19981;&#31526;&#30340;&#20960;&#31181;&#26041;&#24335;&#12290;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#22797;&#21046;&#36825;&#20123;&#20559;&#24046;&#65292;&#25110;&#32773;&#23427;&#20204;&#33021;&#22815;&#20811;&#26381;&#36825;&#20123;&#20559;&#24046;&#65311;&#25105;&#20204;&#20851;&#27880;&#19977;&#27573;&#35770;&#30340;&#24773;&#20917; - &#20174;&#20004;&#20010;&#31616;&#21333;&#21069;&#25552;&#20013;&#25512;&#23548;&#20986;&#30340;&#25512;&#29702;&#65292;&#36825;&#22312;&#24515;&#29702;&#23398;&#20013;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350; - &#25105;&#20204;&#21457;&#29616;&#36739;&#22823;&#30340;&#27169;&#22411;&#27604;&#36739;&#21512;&#36923;&#36753;&#65292;&#32780;&#19988;&#27604;&#20154;&#31867;&#26356;&#21512;&#36923;&#36753;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21363;&#20351;&#26159;&#26368;&#22823;&#30340;&#27169;&#22411;&#20063;&#20250;&#20986;&#29616;&#31995;&#32479;&#24615;&#38169;&#35823;&#65292;&#20854;&#20013;&#19968;&#20123;&#38169;&#35823;&#19982;&#20154;&#31867;&#25512;&#29702;&#30340;&#20559;&#35265;&#30456;&#20284;&#65292;&#20363;&#22914;&#25490;&#24207;&#25928;&#24212;&#21644;&#36923;&#36753;&#35884;&#35823;&#12290;&#24635;&#20307;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#27169;&#20223;&#20102;&#35757;&#32451;&#25968;&#25454;&#20013;&#21253;&#21547;&#30340;&#20154;&#31867;&#20559;&#35265;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#33021;&#22815;&#20811;&#26381;&#36825;&#20123;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
A central component of rational behavior is logical inference: the process of determining which conclusions follow from a set of premises. Psychologists have documented several ways in which humans' inferences deviate from the rules of logic. Do language models, which are trained on text generated by humans, replicate these biases, or are they able to overcome them? Focusing on the case of syllogisms -- inferences from two simple premises, which have been studied extensively in psychology -- we show that larger models are more logical than smaller ones, and also more logical than humans. At the same time, even the largest models make systematic errors, some of which mirror human reasoning biases such as ordering effects and logical fallacies. Overall, we find that language models mimic the human biases included in their training data, but are able to overcome them in some cases.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;SE(3)-Stochastic Flow Matching&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;FoldFlow&#65292;&#21487;&#20197;&#20934;&#30830;&#24314;&#27169;&#34507;&#30333;&#36136;&#20027;&#38142;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#26080;&#38656;&#27169;&#25311;&#35757;&#32451;&#21644;Riemannian&#26368;&#20248;&#20256;&#36755;&#30340;&#32467;&#21512;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#31283;&#23450;&#24615;&#21644;&#24314;&#27169;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02391</link><description>&lt;p&gt;
SE(3)-&#34507;&#30333;&#36136;&#20027;&#38142;&#29983;&#25104;&#20013;&#30340;&#38543;&#26426;&#27969;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
SE(3)-Stochastic Flow Matching for Protein Backbone Generation. (arXiv:2310.02391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02391
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;SE(3)-Stochastic Flow Matching&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;FoldFlow&#65292;&#21487;&#20197;&#20934;&#30830;&#24314;&#27169;&#34507;&#30333;&#36136;&#20027;&#38142;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#26080;&#38656;&#27169;&#25311;&#35757;&#32451;&#21644;Riemannian&#26368;&#20248;&#20256;&#36755;&#30340;&#32467;&#21512;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#31283;&#23450;&#24615;&#21644;&#24314;&#27169;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#19977;&#32500;&#21018;&#20307;&#36816;&#21160;&#65288;&#21363;SE(3)&#32676;&#65289;&#30340;&#27969;&#21305;&#37197;&#33539;&#24335;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#20855;&#26377;&#19981;&#26029;&#22686;&#24378;&#24314;&#27169;&#33021;&#21147;&#30340;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#65306;FoldFlow&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#20027;&#38142;&#30340;&#20934;&#30830;&#24314;&#27169;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FoldFlow-Base&#65292;&#19968;&#31181;&#26080;&#38656;&#27169;&#25311;&#30340;&#23398;&#20064;&#30830;&#23450;&#24615;&#36830;&#32493;&#26102;&#38388;&#21160;&#21147;&#23398;&#21644;&#21305;&#37197;&#19981;&#21464;&#30446;&#26631;&#20998;&#24067;&#30340;&#26041;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;Riemannian&#26368;&#20248;&#20256;&#36755;&#26469;&#21152;&#36895;&#35757;&#32451;&#65292;&#21019;&#24314;&#20102;FoldFlow-OT&#65292;&#20174;&#32780;&#26500;&#24314;&#20102;&#26356;&#31616;&#21333;&#21644;&#31283;&#23450;&#30340;&#27969;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;FoldFlow-SFM&#65292;&#23558;Riemannian&#26368;&#20248;&#20256;&#36755;&#21644;&#26080;&#38656;&#27169;&#25311;&#35757;&#32451;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#23398;&#20064;SE(3)&#19978;&#30340;&#38543;&#26426;&#36830;&#32493;&#26102;&#38388;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#30340;FoldFlow&#29983;&#25104;&#27169;&#22411;&#23478;&#26063;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#20855;&#26377;&#20960;&#20010;&#20851;&#38190;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The computational design of novel protein structures has the potential to impact numerous scientific disciplines greatly. Toward this goal, we introduce $\text{FoldFlow}$ a series of novel generative models of increasing modeling power based on the flow-matching paradigm over $3\text{D}$ rigid motions -i.e. the group $\text{SE(3)}$ -- enabling accurate modeling of protein backbones. We first introduce $\text{FoldFlow-Base}$, a simulation-free approach to learning deterministic continuous-time dynamics and matching invariant target distributions on $\text{SE(3)}$. We next accelerate training by incorporating Riemannian optimal transport to create $\text{FoldFlow-OT}$, leading to the construction of both more simple and stable flows. Finally, we design $\text{FoldFlow-SFM}$ coupling both Riemannian OT and simulation-free training to learn stochastic continuous-time dynamics over $\text{SE(3)}$. Our family of $\text{FoldFlow}$ generative models offer several key advantages over previous
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#35821;&#35328;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#25351;&#20986;&#20854;&#22312;&#31616;&#21333;&#35821;&#35328;&#25512;&#26029;&#20219;&#21153;&#20013;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#25913;&#21892;&#20854;&#23545;&#22522;&#26412;&#35821;&#35328;&#27010;&#24565;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.14785</link><description>&lt;p&gt;
ChatGPT&#21644;&#31616;&#21333;&#30340;&#35821;&#35328;&#25512;&#26029;&#65306;&#30450;&#28857;&#21644;&#32570;&#38519;
&lt;/p&gt;
&lt;p&gt;
ChatGPT and Simple Linguistic Inferences: Blind Spots and Blinds. (arXiv:2305.14785v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#35821;&#35328;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#25351;&#20986;&#20854;&#22312;&#31616;&#21333;&#35821;&#35328;&#25512;&#26029;&#20219;&#21153;&#20013;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#25913;&#21892;&#20854;&#23545;&#22522;&#26412;&#35821;&#35328;&#27010;&#24565;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#30340;&#29702;&#35299;&#33021;&#21147;&#38480;&#21046;&#65292;&#38024;&#23545;&#23545;&#20110;&#20154;&#31867;&#26469;&#35828;&#36890;&#24120;&#31616;&#21333;&#30340;&#25512;&#26029;&#20219;&#21153;&#65292;&#20294;&#36825;&#20123;&#20284;&#20046;&#23545;&#35813;&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#38024;&#23545;(i)&#35821;&#27861;&#35268;&#23450;&#30340;&#34164;&#21547;&#65292;(ii)&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#35777;&#25454;&#21103;&#35789;&#30340;&#21069;&#25552;&#65292;&#20197;&#21450;(iii)&#21333;&#35843;&#34164;&#21547;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#20026;&#36825;&#20123;&#25512;&#29702;&#31867;&#22411;&#25552;&#20379;&#20102;&#19987;&#23478;&#35774;&#35745;&#30340;&#35780;&#20272;&#38598;&#65292;&#24182;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#36827;&#34892;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#36825;&#20123;&#25512;&#29702;&#31867;&#22411;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#23637;&#31034;&#20013;&#31561;&#21040;&#20302;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;ChatGPT&#22312;&#30452;&#25509;&#25552;&#31034;&#19979;&#34920;&#29616;&#20986;&#23545;&#24213;&#23618;&#35821;&#35328;&#27010;&#24565;&#30340;&#20102;&#35299;&#65292;&#20294;&#23427;&#32463;&#24120;&#19981;&#33021;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#20316;&#20986;&#27491;&#30830;&#30340;&#25512;&#26029;&#12290;&#26356;&#26377;&#36259;&#30340;&#26159;&#65292;&#36827;&#19968;&#27493;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23558;&#21069;&#25552;&#23884;&#20837;&#21069;&#25552;&#26465;&#20214;&#35302;&#21457;&#25110;&#38750;&#23454;&#38469;&#24615;&#21160;&#35789;&#20250;&#23548;&#33268;&#27169;&#22411;&#26356;&#39057;&#32321;&#22320;&#39044;&#27979;&#34164;&#21547;&#65292;&#32780;&#19981;&#32771;&#34385;&#27491;&#30830;&#30340;&#35821;&#20041;&#26631;&#31614;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#36825;&#20123;&#32467;&#26524;&#25581;&#31034;&#20102;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#20197;&#21450;&#32487;&#32493;&#25913;&#21892;&#23427;&#20204;&#23545;&#22522;&#26412;&#35821;&#35328;&#27010;&#24565;&#30340;&#29702;&#35299;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper sheds light on the limitations of ChatGPT's understanding capabilities, focusing on simple inference tasks that are typically easy for humans but appear to be challenging for the model. Specifically, we target (i) grammatically-specified entailments, (ii) premises with evidential adverbs of uncertainty, and (iii) monotonicity entailments. We present expert-designed evaluation sets for these inference types and conduct experiments in a zero-shot setup. Our results show that the model struggles with these types of inferences, exhibiting moderate to low accuracy. Moreover, while ChatGPT demonstrates knowledge of the underlying linguistic concepts when prompted directly, it often fails to incorporate this knowledge to make correct inferences. Even more strikingly, further experiments show that embedding the premise under presupposition triggers or non-factive verbs causes the model to predict entailment more frequently {regardless} of the correct semantic label. Overall these re
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#36890;&#29992;&#26694;&#26550;TGC &#29992;&#20110; deep temporal graph clustering, &#35299;&#20915;&#20102;&#26102;&#38388;&#22270;&#21482;&#33021;&#20316;&#20026;&#38745;&#24577;&#22270;&#22788;&#29702;&#30340;&#38590;&#39064;&#65292;&#23454;&#29616;&#20102;&#23545;&#21160;&#24577;&#20449;&#24687;&#30340;&#32858;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102; TGC &#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10738</link><description>&lt;p&gt;
&#28145;&#24230;&#26102;&#38388;&#22270;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Deep Temporal Graph Clustering. (arXiv:2305.10738v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10738
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#36890;&#29992;&#26694;&#26550;TGC &#29992;&#20110; deep temporal graph clustering, &#35299;&#20915;&#20102;&#26102;&#38388;&#22270;&#21482;&#33021;&#20316;&#20026;&#38745;&#24577;&#22270;&#22788;&#29702;&#30340;&#38590;&#39064;&#65292;&#23454;&#29616;&#20102;&#23545;&#21160;&#24577;&#20449;&#24687;&#30340;&#32858;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102; TGC &#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#28145;&#24230;&#22270;&#32858;&#31867;&#24050;&#32463;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#22312;&#26080;&#30417;&#30563;&#22330;&#26223;&#19979;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36866;&#29992;&#20110;&#26102;&#38388;&#22270;&#30340;&#28145;&#24230;&#32858;&#31867;&#26041;&#27861; - &#21487;&#20197;&#25429;&#33719;&#20851;&#38190;&#30340;&#21160;&#24577;&#20132;&#20114;&#20449;&#24687;&#65292;&#24182;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#30340;&#25506;&#32034;&#12290;&#36825;&#24847;&#21619;&#30528;&#22312;&#35768;&#22810;&#38754;&#21521;&#32858;&#31867;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#26102;&#38388;&#22270;&#21482;&#33021;&#20316;&#20026;&#38745;&#24577;&#22270;&#26469;&#22788;&#29702;&#12290;&#36825;&#19981;&#20165;&#23548;&#33268;&#20102;&#21160;&#24577;&#20449;&#24687;&#30340;&#20002;&#22833;&#65292;&#20063;&#24341;&#21457;&#20102;&#24040;&#22823;&#30340;&#35745;&#31639;&#28040;&#32791;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TGC&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#26102;&#38388;&#22270;&#28145;&#24230;&#32858;&#31867;&#65292;&#23427;&#35843;&#25972;&#20102;&#28145;&#24230;&#32858;&#31867;&#25216;&#26415;&#65288;&#32858;&#31867;&#20998;&#37197;&#20998;&#24067;&#21644;&#37051;&#25509;&#30697;&#38453;&#37325;&#26500;&#65289;&#65292;&#20197;&#36866;&#24212;&#26102;&#38388;&#22270;&#22522;&#20110;&#20132;&#20114;&#24207;&#21015;&#30340;&#25209;&#22788;&#29702;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20174;&#20960;&#20010;&#26041;&#38754;&#35752;&#35770;&#20102;&#26102;&#38388;&#22270;&#32858;&#31867;&#19982;&#29616;&#26377;&#38745;&#24577;&#22270;&#32858;&#31867;&#30340;&#24046;&#24322;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;TGC&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep graph clustering has recently received significant attention due to its ability to enhance the representation learning capabilities of models in unsupervised scenarios. Nevertheless, deep clustering for temporal graphs, which could capture crucial dynamic interaction information, has not been fully explored. It means that in many clustering-oriented real-world scenarios, temporal graphs can only be processed as static graphs. This not only causes the loss of dynamic information but also triggers huge computational consumption. To solve the problem, we propose a general framework for deep Temporal Graph Clustering called TGC, which adjusts deep clustering techniques (clustering assignment distribution and adjacency matrix reconstruction) to suit the interaction sequence-based batch-processing pattern of temporal graphs. In addition, we discuss differences between temporal graph clustering and existing static graph clustering from several levels. To verify the superiority of the pro
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;token&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#22312;&#20855;&#20307;&#20219;&#21153;&#20013;&#65292;BERT&#12289;ERNIE&#12289;ELECTRA&#31561;&#32534;&#30721;&#22120;&#20197;&#21450;GPT2&#21644;BLOOM&#31561;&#35299;&#30721;&#22120;&#30340;&#24179;&#22343;&#24615;&#33021;&#19979;&#38477;&#20102;3%&#21644;9%&#12290;</title><link>http://arxiv.org/abs/2304.13567</link><description>&lt;p&gt;
&#20301;&#32622;&#20559;&#24046;&#23545;token&#20998;&#31867;&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of Position Bias on Language Models in Token Classification. (arXiv:2304.13567v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13567
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;token&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#22312;&#20855;&#20307;&#20219;&#21153;&#20013;&#65292;BERT&#12289;ERNIE&#12289;ELECTRA&#31561;&#32534;&#30721;&#22120;&#20197;&#21450;GPT2&#21644;BLOOM&#31561;&#35299;&#30721;&#22120;&#30340;&#24179;&#22343;&#24615;&#33021;&#19979;&#38477;&#20102;3%&#21644;9%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(NER)&#25110;&#35789;&#24615;&#26631;&#27880;&#31561;&#19979;&#28216;&#20219;&#21153;&#24050;&#30693;&#23384;&#22312;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#27491;&#36127;&#31034;&#20363;&#30340;&#27604;&#20363;&#21644;&#31867;&#19981;&#24179;&#34913;&#26041;&#38754;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#21478;&#19968;&#20010;&#29305;&#23450;&#38382;&#39064;&#65292;&#21363;token&#20998;&#31867;&#20219;&#21153;&#20013;&#27491;&#31034;&#20363;&#30340;&#20301;&#32622;&#20559;&#24046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23545;&#22522;&#20110;Token&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#20301;&#32622;&#20559;&#24046;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21253;&#25324;CoNLL03&#21644;OntoNote5.0&#29992;&#20110;NER&#65292;English Tree Bank UD_en&#21644;TweeBank&#29992;&#20110;POS&#26631;&#35760;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#30740;&#31350;Transformer&#27169;&#22411;&#20013;&#30340;&#20301;&#32622;&#20559;&#24046;&#12290;&#25105;&#20204;&#21457;&#29616;&#20687;BERT&#12289;ERNIE&#12289;ELECTRA&#36825;&#26679;&#30340;&#32534;&#30721;&#22120;&#21644;&#20687;GPT2 &#21644;BLOOM&#36825;&#26679;&#30340;&#35299;&#30721;&#22120;&#24179;&#22343;&#24615;&#33021;&#19979;&#38477;&#20102;3%&#21644;9%&#12290;
&lt;/p&gt;
&lt;p&gt;
Language Models (LMs) have shown state-of-the-art performance in Natural Language Processing (NLP) tasks. Downstream tasks such as Named Entity Recognition (NER) or Part-of-Speech (POS) tagging are known to suffer from data imbalance issues, specifically in terms of the ratio of positive to negative examples, and class imbalance. In this paper, we investigate an additional specific issue for language models, namely the position bias of positive examples in token classification tasks. Therefore, we conduct an in-depth evaluation of the impact of position bias on the performance of LMs when fine-tuned on Token Classification benchmarks. Our study includes CoNLL03 and OntoNote5.0 for NER, English Tree Bank UD_en and TweeBank for POS tagging. We propose an evaluation approach to investigate position bias in Transformer models. We show that encoders like BERT, ERNIE, ELECTRA, and decoders such as GPT2 and BLOOM can suffer from this bias with an average drop of 3\% and 9\% in their performan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26426;&#26800;&#21270;&#35299;&#37322;&#65292;&#28041;&#21450;&#24847;&#20041;&#12289;&#20132;&#27969;&#21644;&#31526;&#21495;&#20986;&#29616;&#30340;&#29616;&#35937;&#65292;&#25581;&#31034;&#20102;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#24182;&#19981;&#20855;&#22791;&#19982;&#20154;&#31867;&#30456;&#21516;&#30340;&#24847;&#20041;&#29702;&#35299;&#65292;&#20316;&#32773;&#24314;&#35758;&#27169;&#25311;&#20154;&#31867;&#24773;&#24863;&#24182;&#20248;&#21270;&#27169;&#22411;&#20197;&#26500;&#24314;&#24369;&#34920;&#31034;&#24418;&#24335;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.12686</link><description>&lt;p&gt;
&#20851;&#20110;&#24847;&#20041;&#35745;&#31639;&#12289;&#35821;&#35328;&#27169;&#22411;&#19982;&#19981;&#21487;&#29702;&#35299;&#24656;&#24807;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Computation of Meaning, Language Models and Incomprehensible Horrors. (arXiv:2304.12686v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26426;&#26800;&#21270;&#35299;&#37322;&#65292;&#28041;&#21450;&#24847;&#20041;&#12289;&#20132;&#27969;&#21644;&#31526;&#21495;&#20986;&#29616;&#30340;&#29616;&#35937;&#65292;&#25581;&#31034;&#20102;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#24182;&#19981;&#20855;&#22791;&#19982;&#20154;&#31867;&#30456;&#21516;&#30340;&#24847;&#20041;&#29702;&#35299;&#65292;&#20316;&#32773;&#24314;&#35758;&#27169;&#25311;&#20154;&#31867;&#24773;&#24863;&#24182;&#20248;&#21270;&#27169;&#22411;&#20197;&#26500;&#24314;&#24369;&#34920;&#31034;&#24418;&#24335;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#24847;&#20041;&#30340;&#22522;&#30784;&#29702;&#35770;&#19982;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#25968;&#23398;&#24418;&#24335;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26426;&#26800;&#21270;&#35299;&#37322;&#65292;&#28041;&#21450;&#24847;&#20041;&#12289;&#20132;&#27969;&#21644;&#31526;&#21495;&#20986;&#29616;&#30340;&#29616;&#35937;&#65292;&#36825;&#23545;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#20851;&#20110;&#35821;&#35328;&#26412;&#36136;&#30340;&#36777;&#35770;&#37117;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36890;&#36807;&#30740;&#31350;&#26426;&#22120;&#22312;&#20160;&#20040;&#26465;&#20214;&#19979;&#33021;&#22815;&#29983;&#25104;&#26377;&#24847;&#20041;&#30340;&#35805;&#35821;&#25110;&#29702;&#35299;&#20154;&#31867;&#30340;&#24847;&#20041;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#24182;&#19981;&#20855;&#22791;&#19982;&#20154;&#31867;&#30456;&#21516;&#30340;&#24847;&#20041;&#29702;&#35299;&#65292;&#20063;&#19981;&#24847;&#21619;&#30528;&#23427;&#20204;&#30340;&#22238;&#31572;&#20855;&#26377;&#25105;&#20204;&#25152;&#24402;&#23646;&#30340;&#20219;&#20309;&#24847;&#20041;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#27169;&#25311;&#20154;&#31867;&#24773;&#24863;&#24182;&#20248;&#21270;&#27169;&#22411;&#20197;&#26500;&#24314;&#24369;&#34920;&#31034;&#24418;&#24335;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25104;&#26524;&#25581;&#31034;&#20102;&#24847;&#20041;&#19982;...
&lt;/p&gt;
&lt;p&gt;
We integrate foundational theories of meaning with a mathematical formalism of artificial general intelligence (AGI) to offer a comprehensive mechanistic explanation of meaning, communication, and symbol emergence. This synthesis holds significance for both AGI and broader debates concerning the nature of language, as it unifies pragmatics, logical truth conditional semantics, Peircean semiotics, and a computable model of enactive cognition, addressing phenomena that have traditionally evaded mechanistic explanation. By examining the conditions under which a machine can generate meaningful utterances or comprehend human meaning, we establish that the current generation of language models do not possess the same understanding of meaning as humans nor intend any meaning that we might attribute to their responses. To address this, we propose simulating human feelings and optimising models to construct weak representations. Our findings shed light on the relationship between meaning and in
&lt;/p&gt;</description></item><item><title>&#35752;&#35770;&#20102;&#19981;&#38656;&#35201;&#26174;&#24335;&#23454;&#29616;&#24178;&#39044;&#34920;&#31034;&#30340; Pareto &#26368;&#20248;&#25968;&#23398;&#24418;&#24335;&#65292;&#36890;&#36807;&#24402;&#32435;&#24615;&#22320;&#20986;&#29616;&#30456;&#20851;&#22240;&#26524;&#24178;&#39044;&#30340;&#34920;&#31034;&#65292;&#20316;&#20026;&#33258;&#25105;&#21644;&#20219;&#20309;&#20854;&#20182;&#23545;&#35937;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2302.03189</link><description>&lt;p&gt;
&#26032;&#20852;&#22240;&#26524;&#24615;&#19982;&#24847;&#35782;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Emergent Causality &amp; the Foundation of Consciousness. (arXiv:2302.03189v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03189
&lt;/p&gt;
&lt;p&gt;
&#35752;&#35770;&#20102;&#19981;&#38656;&#35201;&#26174;&#24335;&#23454;&#29616;&#24178;&#39044;&#34920;&#31034;&#30340; Pareto &#26368;&#20248;&#25968;&#23398;&#24418;&#24335;&#65292;&#36890;&#36807;&#24402;&#32435;&#24615;&#22320;&#20986;&#29616;&#30456;&#20851;&#22240;&#26524;&#24178;&#39044;&#30340;&#34920;&#31034;&#65292;&#20316;&#20026;&#33258;&#25105;&#21644;&#20219;&#20309;&#20854;&#20182;&#23545;&#35937;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#20132;&#20114;&#29615;&#22659;&#20013;&#20570;&#20986;&#20934;&#30830;&#30340;&#25512;&#26029;&#65292;&#26234;&#33021;&#20307;&#19981;&#33021;&#25226;&#34987;&#21160;&#35266;&#23519;&#20107;&#20214;&#19982;&#24178;&#39044;&#24341;&#36215;&#23427;&#20204;&#28151;&#28102;&#12290;$do$ &#25805;&#20316;&#31526;&#23558;&#24178;&#39044;&#24418;&#24335;&#21270;&#65292;&#20197;&#20415;&#25105;&#20204;&#21487;&#20197;&#25512;&#26029;&#20854;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#20132;&#20114;&#29615;&#22659;&#20013;&#23384;&#22312;&#26080;&#38656;&#26174;&#24335;&#23454;&#29616;&#24178;&#39044;&#34920;&#31034;&#30340; Pareto &#26368;&#20248;&#25968;&#23398;&#24418;&#24335;&#65292;&#21487;&#20197;&#20570;&#20986;&#26368;&#22823;&#31243;&#24230;&#30340;&#20934;&#30830;&#25512;&#26029;&#12290;&#26412;&#25991;&#32771;&#23519;&#20102;&#20854;&#20013;&#19968;&#20010;&#36825;&#26679;&#30340;&#24418;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19981;&#23384;&#22312; $do$ &#25805;&#20316;&#31526;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#21464;&#37327;&#34920;&#31034;&#24178;&#39044;&#12290;&#28982;&#21518;&#25105;&#20204;&#35748;&#20026;&#21464;&#37327;&#26159;&#25277;&#35937;&#30340;&#65292;&#38656;&#35201;&#26174;&#24335;&#39044;&#20808;&#34920;&#31034;&#24178;&#39044;&#21482;&#26159;&#22240;&#20026;&#25105;&#20204;&#39044;&#35774;&#20102;&#36825;&#31867;&#25277;&#35937;&#27010;&#24565;&#12290;&#21069;&#36848;&#30340;&#24418;&#24335;&#36991;&#20813;&#20102;&#36825;&#31181;&#22256;&#25200;&#65292;&#22240;&#27492;&#65292;&#22914;&#26524;&#26377;&#36866;&#24403;&#30340;&#21021;&#22987;&#26465;&#20214;&#65292;&#21017;&#30456;&#20851;&#22240;&#26524;&#24178;&#39044;&#30340;&#34920;&#31034;&#23558;&#36890;&#36807;&#24402;&#32435;&#24615;&#22320;&#20986;&#29616;&#12290;&#36825;&#20123;&#26032;&#20852;&#30340;&#25277;&#35937;&#20316;&#20026;&#33258;&#25105;&#21644;&#20219;&#20309;&#20854;&#20182;&#23545;&#35937;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
To make accurate inferences in an interactive setting, an agent must not confuse passive observation of events with having intervened to cause them. The $do$ operator formalises interventions so that we may reason about their effect. Yet there exist pareto optimal mathematical formalisms of general intelligence in an interactive setting which, presupposing no explicit representation of intervention, make maximally accurate inferences. We examine one such formalism. We show that in the absence of a $do$ operator, an intervention can be represented by a variable. We then argue that variables are abstractions, and that need to explicitly represent interventions in advance arises only because we presuppose these sorts of abstractions. The aforementioned formalism avoids this and so, initial conditions permitting, representations of relevant causal interventions will emerge through induction. These emergent abstractions function as representations of one`s self and of any other object, inas
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26500;&#24314;&#20551;&#35774;&#30340;&#36807;&#31243;&#20013;&#65292;&#36873;&#25321;&#26368;&#30701;&#30340;&#20551;&#35774;&#19981;&#22914;&#36873;&#25321;&#26368;&#24369;&#30340;&#20551;&#35774;&#65292;&#32780;&#24369;&#28857;&#26159;&#27604;&#38271;&#24230;&#25110;&#31616;&#21333;&#24615;&#26356;&#22909;&#30340;&#25512;&#24191;&#34920;&#29616;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2301.12987</link><description>&lt;p&gt;
&#20551;&#35774;&#30340;&#26368;&#20339;&#36873;&#25321;&#26159;&#26368;&#24369;&#30340;&#32780;&#19981;&#26159;&#26368;&#30701;&#30340;
&lt;/p&gt;
&lt;p&gt;
The Optimal Choice of Hypothesis Is the Weakest, Not the Shortest. (arXiv:2301.12987v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26500;&#24314;&#20551;&#35774;&#30340;&#36807;&#31243;&#20013;&#65292;&#36873;&#25321;&#26368;&#30701;&#30340;&#20551;&#35774;&#19981;&#22914;&#36873;&#25321;&#26368;&#24369;&#30340;&#20551;&#35774;&#65292;&#32780;&#24369;&#28857;&#26159;&#27604;&#38271;&#24230;&#25110;&#31616;&#21333;&#24615;&#26356;&#22909;&#30340;&#25512;&#24191;&#34920;&#29616;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;$A$&#21644;$B$&#26159;&#36825;&#26679;&#30340;&#38598;&#21512;&#65292;&#21363;$A \subset B$&#65292;&#37027;&#20040;&#19968;&#33324;&#21270;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#20174;$A$&#25512;&#26029;&#20986;&#19968;&#20010;&#36275;&#20197;&#26500;&#24314;$B$&#30340;&#20551;&#35774;&#12290;&#21487;&#20197;&#20174;$A$&#25512;&#26029;&#20986;&#20219;&#24847;&#25968;&#37327;&#30340;&#20551;&#35774;&#65292;&#20294;&#21482;&#26377;&#20854;&#20013;&#30340;&#19968;&#20123;&#21487;&#20197;&#25512;&#24191;&#21040;$B$&#12290;&#24590;&#26679;&#30693;&#36947;&#21738;&#20123;&#20551;&#35774;&#21487;&#33021;&#25512;&#24191;&#65311;&#19968;&#31181;&#31574;&#30053;&#26159;&#36873;&#25321;&#26368;&#30701;&#30340;&#65292;&#23558;&#21387;&#32553;&#20449;&#24687;&#30340;&#33021;&#21147;&#19982;&#25512;&#24191;&#30340;&#33021;&#21147;&#65288;&#26234;&#33021;&#30340;&#20195;&#29702;&#65289;&#31561;&#21516;&#36215;&#26469;&#12290;&#25105;&#20204;&#22312;&#20027;&#21160;&#35748;&#30693;&#30340;&#25968;&#23398;&#24418;&#24335;&#20027;&#20041;&#32972;&#26223;&#19979;&#30740;&#31350;&#20102;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#21387;&#32553;&#26082;&#19981;&#26159;&#26368;&#22823;&#21270;&#34920;&#29616;&#65288;&#29992;&#20551;&#35774;&#25512;&#24191;&#30340;&#27010;&#29575;&#34913;&#37327;&#65289;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#20063;&#19981;&#26159;&#20805;&#20998;&#26465;&#20214;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#19982;&#38271;&#24230;&#25110;&#31616;&#21333;&#24615;&#26080;&#20851;&#30340;&#20195;&#29702;&#65292;&#31216;&#20026;&#24369;&#28857;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22914;&#26524;&#20219;&#21153;&#26159;&#22343;&#21248;&#20998;&#24067;&#30340;&#65292;&#21017;&#19981;&#23384;&#22312;&#20219;&#20309;&#20195;&#29702;&#30340;&#36873;&#25321;&#65292;&#21487;&#20197;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#33267;&#23569;&#19982;&#24369;&#28857;&#26368;&#22823;&#21270;&#30340;&#34920;&#29616;&#30456;&#21516;&#65292;&#21516;&#26102;&#22312;&#33267;&#23569;&#19968;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;&#22312;&#27604;&#36739;&#26368;&#22823;&#24369;&#28857;&#21644;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;(MDL)&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#26368;&#22823;&#24369;&#28857;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;MDL&#12290;&#25105;&#20204;&#35748;&#20026;&#24369;&#28857;&#26159;&#27604;&#38271;&#24230;&#25110;&#31616;&#21333;&#24615;&#26356;&#22909;&#30340;&#25512;&#24191;&#34920;&#29616;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
If $A$ and $B$ are sets such that $A \subset B$, generalisation may be understood as the inference from $A$ of a hypothesis sufficient to construct $B$. One might infer any number of hypotheses from $A$, yet only some of those may generalise to $B$. How can one know which are likely to generalise? One strategy is to choose the shortest, equating the ability to compress information with the ability to generalise (a proxy for intelligence). We examine this in the context of a mathematical formalism of enactive cognition. We show that compression is neither necessary nor sufficient to maximise performance (measured in terms of the probability of a hypothesis generalising). We formulate a proxy unrelated to length or simplicity, called weakness. We show that if tasks are uniformly distributed, then there is no choice of proxy that performs at least as well as weakness maximisation in all tasks while performing strictly better in at least one. In experiments comparing maximum weakness and m
&lt;/p&gt;</description></item></channel></rss>