<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20849;&#21019;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#21152;&#36895;&#23547;&#25214;&#26367;&#20195;&#8220;&#27704;&#20037;&#21270;&#23398;&#21697;&#8221;&#30340;&#36807;&#31243;&#65292;&#20197;&#20943;&#23569;&#23545;&#29615;&#22659;&#21644;&#20154;&#20307;&#20581;&#24247;&#36896;&#25104;&#30340;&#20260;&#23475;&#12290;</title><link>http://arxiv.org/abs/2304.05389</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20849;&#21019;&#65306;&#23547;&#25214;&#8220;&#27704;&#20037;&#21270;&#23398;&#21697;&#8221;&#26367;&#20195;&#21697;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Human-AI Co-Creation Approach to Find Forever Chemicals Replacements. (arXiv:2304.05389v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05389
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20849;&#21019;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#21152;&#36895;&#23547;&#25214;&#26367;&#20195;&#8220;&#27704;&#20037;&#21270;&#23398;&#21697;&#8221;&#30340;&#36807;&#31243;&#65292;&#20197;&#20943;&#23569;&#23545;&#29615;&#22659;&#21644;&#20154;&#20307;&#20581;&#24247;&#36896;&#25104;&#30340;&#20260;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#26448;&#26009;&#21457;&#29616;&#39046;&#22495;&#65292;&#29983;&#25104;&#27169;&#22411;&#26159;&#38750;&#24120;&#24378;&#22823;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#27491;&#22312;&#35774;&#35745;&#19968;&#20010;&#36719;&#20214;&#26694;&#26550;&#65292;&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#20849;&#21516;&#21019;&#36896;&#36807;&#31243;&#65292;&#20197;&#21152;&#24555;&#23547;&#25214;&#26367;&#20195;&#8220;&#27704;&#20037;&#21270;&#23398;&#21697;&#8221;&#30340;&#36827;&#31243;&#65292;&#20174;&#32780;&#20943;&#23569;&#23427;&#20204;&#23545;&#29615;&#22659;&#21644;&#20154;&#20307;&#20581;&#24247;&#36896;&#25104;&#30340;&#20260;&#23475;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20154;&#24037;&#26234;&#33021;&#33021;&#21147;&#19982;&#19987;&#19994;&#39046;&#22495;&#30340;&#20869;&#38544;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#21152;&#24555;&#26448;&#26009;&#21457;&#29616;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#20849;&#21516;&#21019;&#36896;&#36807;&#31243;&#20197;&#19987;&#19994;&#39046;&#22495;&#19987;&#23478;&#21644;&#33021;&#29983;&#25104;&#26032;&#20998;&#23376;&#35774;&#35745;&#30340;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#20132;&#20114;&#24320;&#22987;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#20551;&#35774;&#65306;&#36825;&#20123;&#19987;&#19994;&#39046;&#22495;&#30340;&#19987;&#23478;&#21487;&#20197;&#36890;&#36807;&#26356;&#36845;&#20195;&#30340;&#29983;&#25104;&#27169;&#22411;&#20132;&#20114;&#65292;&#21033;&#29992;&#20182;&#20204;&#30340;&#30693;&#35782;&#25351;&#24341;&#23545;&#25506;&#32034;&#31354;&#38388;&#30340;&#26356;&#31934;&#30830;&#30340;&#25506;&#32034;&#65292;&#20174;&#32780;&#21463;&#30410;&#20110;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models are a powerful tool in AI for material discovery. We are designing a software framework that supports a human-AI co-creation process to accelerate finding replacements for the ``forever chemicals''-- chemicals that enable our modern lives, but are harmful to the environment and the human health. Our approach combines AI capabilities with the domain-specific tacit knowledge of subject matter experts to accelerate the material discovery. Our co-creation process starts with the interaction between the subject matter experts and a generative model that can generate new molecule designs. In this position paper, we discuss our hypothesis that these subject matter experts can benefit from a more iterative interaction with the generative model, asking for smaller samples and ``guiding'' the exploration of the discovery space with their knowledge.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27979;&#37327;OpenAI ChatGPT&#21644;Google Bard&#31561;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21487;&#38752;&#24615;&#21457;&#29616;&#65292;&#20854;&#22312;&#24863;&#30693;&#21644;&#35780;&#20272;&#20889;&#20316;&#25552;&#31034;&#30340;&#22797;&#26434;&#24615;&#26041;&#38754;&#19982;&#32463;&#39564;&#20016;&#23500;&#30340;&#20154;&#31867;&#35780;&#20998;&#32773;&#30340;&#19968;&#33268;&#24615;&#36739;&#20302;&#12290;</title><link>http://arxiv.org/abs/2304.05372</link><description>&lt;p&gt;
ChatGPT&#21644;Bard&#33021;&#22815;&#29983;&#25104;&#19968;&#33268;&#30340;&#35780;&#20272;&#39033;&#30446;&#21527;&#65311;&#38024;&#23545;&#20154;&#31867;&#34920;&#29616;&#30340;&#21487;&#38752;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT and Bard Generate Aligned Assessment Items? A Reliability Analysis against Human Performance. (arXiv:2304.05372v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27979;&#37327;OpenAI ChatGPT&#21644;Google Bard&#31561;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21487;&#38752;&#24615;&#21457;&#29616;&#65292;&#20854;&#22312;&#24863;&#30693;&#21644;&#35780;&#20272;&#20889;&#20316;&#25552;&#31034;&#30340;&#22797;&#26434;&#24615;&#26041;&#38754;&#19982;&#32463;&#39564;&#20016;&#23500;&#30340;&#20154;&#31867;&#35780;&#20998;&#32773;&#30340;&#19968;&#33268;&#24615;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#21644;Bard&#26159;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#34987;&#35748;&#20026;&#33021;&#22815;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#24212;&#29992;&#12290;&#22312;&#25945;&#32946;&#39046;&#22495;&#65292;&#36825;&#20123;AI&#25216;&#26415;&#24050;&#34987;&#29992;&#20110;&#35780;&#20272;&#21644;&#25945;&#23398;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;AI&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#29992;&#20110;&#33258;&#21160;&#21270;&#30340;&#35770;&#25991;&#35780;&#20998;&#21644;&#33258;&#21160;&#21270;&#30340;&#39033;&#30446;&#29983;&#25104;&#12290;&#36825;&#20123;&#24037;&#20855;&#24517;&#39035;&#20855;&#22791;&#30340;&#19968;&#39033;&#24515;&#29702;&#27979;&#37327;&#23646;&#24615;&#26159;&#21487;&#38752;&#24615;&#39640;&#65292;&#21363;AI&#20998;&#25968;&#19982;&#20154;&#31867;&#35780;&#20998;&#32773;&#24847;&#35265;&#19968;&#33268;&#12290;&#26412;&#25991;&#27979;&#37327;&#20102;OpenAI ChatGP&#21644;Google Bard&#30340;&#21487;&#38752;&#24615;&#65292;&#20197;&#35780;&#20272;&#36825;&#20123;&#24037;&#20855;&#22312;&#24863;&#30693;&#21644;&#35780;&#20272;&#20889;&#20316;&#25552;&#31034;&#30340;&#22797;&#26434;&#24615;&#26041;&#38754;&#19982;&#32463;&#39564;&#20016;&#23500;&#30340;&#20154;&#31867;&#35780;&#20998;&#32773;&#30340;&#19968;&#33268;&#24615;&#12290;&#20316;&#20026;&#32489;&#25928;&#25351;&#26631;&#30340;&#20869;&#37096;&#30456;&#20851;&#31995;&#25968;&#65288;ICC&#65289;&#26174;&#31034;&#65292;OpenAI ChatGPT&#21644;Google Bard&#30340;&#20114;&#21487;&#38752;&#24615;&#20302;&#20110;&#20154;&#31867;&#35780;&#20998;&#30340;&#37329;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT and Bard are AI chatbots based on Large Language Models (LLM) that are slated to promise different applications in diverse areas. In education, these AI technologies have been tested for applications in assessment and teaching. In assessment, AI has long been used in automated essay scoring and automated item generation. One psychometric property that these tools must have to assist or replace humans in assessment is high reliability in terms of agreement between AI scores and human raters. In this paper, we measure the reliability of OpenAI ChatGP and Google Bard LLMs tools against experienced and trained humans in perceiving and rating the complexity of writing prompts. Intraclass correlation (ICC) as a performance metric showed that the inter-reliability of both the OpenAI ChatGPT and the Google Bard were low against the gold standard of human ratings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#19968;&#20010;&#26032;&#21457;&#23637;&#65306;&#38271;&#26399;&#35760;&#24518;&#26426;&#21046;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26426;&#21046;&#21487;&#33021;&#20250;&#23548;&#33268;&#26426;&#22120;&#20154;&#35760;&#20303;&#20102;&#38169;&#35823;&#21644;&#34394;&#20551;&#30340;&#20449;&#24687;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#20107;&#23454;&#38472;&#36848;&#37325;&#22797;&#12290;</title><link>http://arxiv.org/abs/2304.05371</link><description>&lt;p&gt;
&#37027;&#19981;&#26159;&#20320;&#30340;&#35760;&#24518;&#65292;&#23427;&#26159;&#21035;&#20154;&#30340;&#65306;&#22312;&#32842;&#22825;&#26426;&#22120;&#20154;&#35760;&#24518;&#20013;&#25773;&#25746;&#38169;&#35823;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Those Aren't Your Memories, They're Somebody Else's: Seeding Misinformation in Chat Bot Memories. (arXiv:2304.05371v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#19968;&#20010;&#26032;&#21457;&#23637;&#65306;&#38271;&#26399;&#35760;&#24518;&#26426;&#21046;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26426;&#21046;&#21487;&#33021;&#20250;&#23548;&#33268;&#26426;&#22120;&#20154;&#35760;&#20303;&#20102;&#38169;&#35823;&#21644;&#34394;&#20551;&#30340;&#20449;&#24687;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#20107;&#23454;&#38472;&#36848;&#37325;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#19968;&#20010;&#26032;&#21457;&#23637;&#26159;&#38271;&#26399;&#35760;&#24518;&#26426;&#21046;&#65292;&#21487;&#20197;&#35760;&#20303;&#36807;&#21435;&#23545;&#35805;&#20013;&#30340;&#20449;&#24687;&#65292;&#20197;&#22686;&#21152;&#21709;&#24212;&#30340;&#36830;&#36143;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;&#26426;&#22120;&#20154;&#34987;&#35774;&#35745;&#20026;&#20174;&#20854;&#23545;&#35805;&#20249;&#20276;&#20013;&#25552;&#21462;&#20010;&#20154;&#24615;&#36136;&#30340;&#30693;&#35782;&#65292;&#20363;&#22914;&#34920;&#26126;&#23545;&#29305;&#23450;&#39068;&#33394;&#30340;&#20559;&#22909;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#35760;&#24518;&#26426;&#21046;&#21487;&#33021;&#20250;&#23548;&#33268;&#24847;&#22806;&#34892;&#20026;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#20010;&#20154;&#21487;&#20197;&#23558;&#20010;&#20154;&#38472;&#36848;&#19982;&#20449;&#24687;&#38472;&#36848;&#32467;&#21512;&#36215;&#26469;&#65292;&#23548;&#33268;&#26426;&#22120;&#20154;&#23558;&#20449;&#24687;&#38472;&#36848;&#19982;&#20010;&#20154;&#30693;&#35782;&#19968;&#36215;&#35760;&#24405;&#22312;&#20854;&#38271;&#26399;&#35760;&#24518;&#20013;&#12290;&#36825;&#24847;&#21619;&#30528;&#26426;&#22120;&#20154;&#21487;&#33021;&#34987;&#27450;&#39575;&#35760;&#20303;&#38169;&#35823;&#20449;&#24687;&#65292;&#24182;&#22312;&#22238;&#24518;&#19982;&#23545;&#35805;&#20027;&#39064;&#30456;&#20851;&#30340;&#20449;&#24687;&#26102;&#23558;&#20854;&#20316;&#20026;&#20107;&#23454;&#38472;&#36848;&#37325;&#22797;&#12290;&#25105;&#20204;&#22312;&#22522;&#20110;ParlAI&#24179;&#21488;&#23454;&#29616;&#30340;BlenderBot 2&#26694;&#26550;&#19978;&#23637;&#31034;&#20102;&#36825;&#31181;&#28431;&#27934;&#65292;&#24182;&#22312;&#26356;&#36817;&#26399;&#12289;&#35268;&#27169;&#26356;&#22823;&#30340;BlenderBot 3&#27169;&#22411;&#19978;&#25552;&#20379;&#20102;&#20363;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the new developments in chit-chat bots is a long-term memory mechanism that remembers information from past conversations for increasing engagement and consistency of responses. The bot is designed to extract knowledge of personal nature from their conversation partner, e.g., stating preference for a particular color. In this paper, we show that this memory mechanism can result in unintended behavior. In particular, we found that one can combine a personal statement with an informative statement that would lead the bot to remember the informative statement alongside personal knowledge in its long term memory. This means that the bot can be tricked into remembering misinformation which it would regurgitate as statements of fact when recalling information relevant to the topic of conversation. We demonstrate this vulnerability on the BlenderBot 2 framework implemented on the ParlAI platform and provide examples on the more recent and significantly larger BlenderBot 3 model. We gen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#24341;&#20837;&#33258;&#38382;&#33258;&#31572;&#25552;&#31034;&#31574;&#30053;&#26469;&#25552;&#39640;LLMs&#22312;&#21307;&#30103;&#20445;&#20581;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.05368</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#20934;&#22791;&#23601;&#32490;&#20102;&#21527;&#65311;&#20020;&#24202;&#35821;&#35328;&#29702;&#35299;&#30340;&#27604;&#36739;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models Ready for Healthcare? A Comparative Study on Clinical Language Understanding. (arXiv:2304.05368v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#24341;&#20837;&#33258;&#38382;&#33258;&#31572;&#25552;&#31034;&#31574;&#30053;&#26469;&#25552;&#39640;LLMs&#22312;&#21307;&#30103;&#20445;&#20581;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#21253;&#25324;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#20020;&#24202;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#30340;&#19987;&#19994;&#24615;&#36136;&#24102;&#26469;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#26368;&#20808;&#36827;&#30340;LLMs&#8212;&#8212;GPT-3.5&#12289;GPT-4&#21644;Bard&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#35813;&#35780;&#20272;&#33539;&#22260;&#28085;&#30422;&#20102;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#20851;&#31995;&#25552;&#21462;&#12289;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#12289;&#25991;&#26723;&#20998;&#31867;&#21644;&#38382;&#31572;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#31574;&#30053;&#8212;&#8212;&#33258;&#38382;&#33258;&#31572;&#25552;&#31034;&#65288;SQP&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#24341;&#21457;&#19982;&#30456;&#20851;&#20020;&#24202;&#22330;&#26223;&#30456;&#20851;&#30340;&#20449;&#24687;&#24615;&#38382;&#39064;&#21644;&#31572;&#26696;&#65292;&#23450;&#21046;&#21270;&#25552;&#39640;LLMs&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#24378;&#35843;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#23398;&#20064;&#31574;&#30053;&#21644;&#25552;&#31034;&#25216;&#26415;&#23545;&#20110;&#25552;&#39640;LLMs&#22312;&#21307;&#30103;&#20445;&#20581;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have made significant progress in various domains, including healthcare. However, the specialized nature of clinical language understanding tasks presents unique challenges and limitations that warrant further investigation. In this study, we conduct a comprehensive evaluation of state-of-the-art LLMs, namely GPT-3.5, GPT-4, and Bard, within the realm of clinical language understanding tasks. These tasks span a diverse range, including named entity recognition, relation extraction, natural language inference, semantic textual similarity, document classification, and question-answering. We also introduce a novel prompting strategy, self-questioning prompting (SQP), tailored to enhance LLMs' performance by eliciting informative questions and answers pertinent to the clinical scenarios at hand. Our evaluation underscores the significance of task-specific learning strategies and prompting techniques for improving LLMs' effectiveness in healthcare-related tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#38750;&#23545;&#31216;&#22810;&#39033;&#24335;&#25439;&#22833;&#65288;APL&#65289;&#65292;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#20219;&#21153;&#20248;&#21270;&#27169;&#22411;&#65292;&#24182;&#22312;&#20851;&#31995;&#25552;&#21462;&#12289;&#25991;&#26412;&#20998;&#31867;&#21644;&#22270;&#20687;&#20998;&#31867;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.05361</link><description>&lt;p&gt;
&#38024;&#23545;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#38750;&#23545;&#31216;&#22810;&#39033;&#24335;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Asymmetric Polynomial Loss For Multi-Label Classification. (arXiv:2304.05361v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#38750;&#23545;&#31216;&#22810;&#39033;&#24335;&#25439;&#22833;&#65288;APL&#65289;&#65292;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#20219;&#21153;&#20248;&#21270;&#27169;&#22411;&#65292;&#24182;&#22312;&#20851;&#31995;&#25552;&#21462;&#12289;&#25991;&#26412;&#20998;&#31867;&#21644;&#22270;&#20687;&#20998;&#31867;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#31181;&#20219;&#21153;&#34987;&#37325;&#26032;&#26500;&#36896;&#20026;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#65292;&#20854;&#20013;&#20108;&#20803;&#20132;&#21449;&#29109;&#65288;BCE&#65289;&#25439;&#22833;&#32463;&#24120;&#29992;&#20110;&#20248;&#21270;&#35774;&#35745;&#33391;&#22909;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#32431;BCE&#25439;&#22833;&#26080;&#27861;&#26681;&#25454;&#19981;&#21516;&#20219;&#21153;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#23548;&#33268;&#19981;&#21516;&#27169;&#22411;&#30340;&#34920;&#29616;&#20122;&#20248;&#12290;&#27492;&#22806;&#65292;&#20887;&#20313;&#36127;&#26679;&#21697;&#21644;&#32597;&#35265;&#27491;&#26679;&#21697;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#21487;&#33021;&#20250;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#38750;&#23545;&#31216;&#22810;&#39033;&#24335;&#25439;&#22833;&#65288;APL&#65289;&#26469;&#32531;&#35299;&#19978;&#36848;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;BCE&#25439;&#22833;&#36827;&#34892;&#27888;&#21202;&#23637;&#24320;&#65292;&#28982;&#21518;&#25913;&#21892;&#22810;&#39033;&#24335;&#20989;&#25968;&#30340;&#31995;&#25968;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#37319;&#29992;&#38750;&#23545;&#31216;&#32858;&#28966;&#26426;&#21046;&#26469;&#35299;&#32806;&#26469;&#33258;&#36127;&#26679;&#26412;&#21644;&#27491;&#26679;&#26412;&#30340;&#26799;&#24230;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39564;&#35777;&#65292;&#22810;&#39033;&#24335;&#31995;&#25968;&#21487;&#20197;&#37325;&#26032;&#26657;&#20934;&#38750;&#23545;&#31216;&#32858;&#28966;&#36229;&#21442;&#25968;&#12290;&#22312;&#20851;&#31995;&#25552;&#21462;&#12289;&#25991;&#26412;&#20998;&#31867;&#21644;&#22270;&#20687;&#20998;&#31867;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;APL&#25439;&#22833;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various tasks are reformulated as multi-label classification problems, in which the binary cross-entropy (BCE) loss is frequently utilized for optimizing well-designed models. However, the vanilla BCE loss cannot be tailored for diverse tasks, resulting in a suboptimal performance for different models. Besides, the imbalance between redundant negative samples and rare positive samples could degrade the model performance. In this paper, we propose an effective Asymmetric Polynomial Loss (APL) to mitigate the above issues. Specifically, we first perform Taylor expansion on BCE loss. Then we ameliorate the coefficients of polynomial functions. We further employ the asymmetric focusing mechanism to decouple the gradient contribution from the negative and positive samples. Moreover, we validate that the polynomial coefficients can recalibrate the asymmetric focusing hyperparameters. Experiments on relation extraction, text classification, and image classification show that our APL loss can 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21306;&#22359;&#38142;&#30340;&#28608;&#21169;&#26426;&#21046;&#65292;&#35753;&#29992;&#25143;&#36164;&#28304;&#36129;&#29486;&#21464;&#24471;&#21487;&#34892;&#65292;&#23454;&#29616;&#21435;&#20013;&#24515;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2304.05354</link><description>&lt;p&gt;
iDML: &#28608;&#21169;&#21435;&#20013;&#24515;&#21270;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
iDML: Incentivized Decentralized Machine Learning. (arXiv:2304.05354v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05354
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21306;&#22359;&#38142;&#30340;&#28608;&#21169;&#26426;&#21046;&#65292;&#35753;&#29992;&#25143;&#36164;&#28304;&#36129;&#29486;&#21464;&#24471;&#21487;&#34892;&#65292;&#23454;&#29616;&#21435;&#20013;&#24515;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21435;&#20013;&#24515;&#21270;&#21644;&#26426;&#20250;&#20027;&#20041;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#20852;&#36215;&#65292;&#32456;&#31471;&#35774;&#22791;&#36234;&#26469;&#36234;&#38656;&#35201;&#20351;&#29992;&#20854;&#33258;&#24049;&#25910;&#38598;&#30340;&#20247;&#21253;&#25968;&#25454;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#36825;&#20123;&#26041;&#27861;&#20174;&#36164;&#28304;&#28040;&#32791;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#35282;&#24230;&#37117;&#26159;&#21487;&#21462;&#30340;&#12290;&#24403;&#35774;&#22791;&#30452;&#25509;&#20174;&#35757;&#32451;&#27169;&#22411;&#20013;&#21463;&#30410;&#26102;&#65292;&#36164;&#28304;&#36129;&#29486;&#26159;&#34987;&#28608;&#21169;&#30340;&#65292;&#22240;&#20026;&#21327;&#20316;&#20250;&#20135;&#29983;&#26356;&#39640;&#20934;&#30830;&#24230;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#24403;&#35201;&#27714;&#32456;&#31471;&#29992;&#25143;&#35774;&#22791;&#20026;&#20165;&#20026;&#20182;&#20154;&#21463;&#30410;&#30340;&#20219;&#21153;&#65288;&#20363;&#22914;&#20026;&#19968;&#20010;&#26080;&#36275;&#36731;&#37325;&#30340;&#37051;&#23621;&#35757;&#32451;&#27169;&#22411;&#65289;&#36129;&#29486;&#33258;&#24049;&#30340;&#36164;&#28304;&#65288;&#20363;&#22914;&#35745;&#31639;&#12289;&#36890;&#20449;&#21644;&#25968;&#25454;&#65289;&#26102;&#65292;&#38656;&#35201;&#25552;&#20379;&#26126;&#30830;&#30340;&#28608;&#21169;&#26426;&#21046;&#12290;&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#28608;&#21169;&#26426;&#21046;&#65292;&#29992;&#20110;&#23436;&#20840;&#21435;&#20013;&#24515;&#21270;&#21644;&#40723;&#21169;&#29992;&#25143;&#36164;&#28304;&#36129;&#29486;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rising emergence of decentralized and opportunistic approaches to machine learning, end devices are increasingly tasked with training deep learning models on-devices using crowd-sourced data that they collect themselves. These approaches are desirable from a resource consumption perspective and also from a privacy preservation perspective. When the devices benefit directly from the trained models, the incentives are implicit contributing devices' resources are incentivized by the availability of the higher-accuracy model that results from collaboration. However, explicit incentive mechanisms must be provided when end-user devices are asked to contribute their resources (e.g., computation, communication, and data) to a task performed primarily for the benefit of others, e.g., training a model for a task that a neighbor device needs but the device owner is uninterested in. In this project, we propose a novel blockchain-based incentive mechanism for completely decentralized and
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#20998;&#26512;&#20102;&#22522;&#20110;&#23545;&#35805;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#20013;&#30340;&#27602;&#24615;&#65292;&#36890;&#36807;&#25351;&#23450;&#20154;&#29289;&#35282;&#33394;&#65292;&#36755;&#20986;&#20250;&#28041;&#21450;&#21051;&#26495;&#21360;&#35937;&#12289;&#26377;&#23475;&#23545;&#35805;&#21644;&#20260;&#20154;&#30340;&#35328;&#35770;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#20805;&#20998;&#29702;&#35299;LLMs&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#20197;&#30830;&#20445;&#36825;&#20123;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.05335</link><description>&lt;p&gt;
&#32842;&#22825;GPT&#20013;&#30340;&#27602;&#24615;&#65306;&#20998;&#26512;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Toxicity in ChatGPT: Analyzing Persona-assigned Language Models. (arXiv:2304.05335v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05335
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20998;&#26512;&#20102;&#22522;&#20110;&#23545;&#35805;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#20013;&#30340;&#27602;&#24615;&#65292;&#36890;&#36807;&#25351;&#23450;&#20154;&#29289;&#35282;&#33394;&#65292;&#36755;&#20986;&#20250;&#28041;&#21450;&#21051;&#26495;&#21360;&#35937;&#12289;&#26377;&#23475;&#23545;&#35805;&#21644;&#20260;&#20154;&#30340;&#35328;&#35770;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#20805;&#20998;&#29702;&#35299;LLMs&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#20197;&#30830;&#20445;&#36825;&#20123;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20102;&#20196;&#20154;&#38590;&#20197;&#32622;&#20449;&#30340;&#33021;&#21147;&#65292;&#36229;&#36234;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31038;&#21306;&#65292;&#24182;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#12289;&#27835;&#30103;&#12289;&#25945;&#32946;&#21644;&#23458;&#25143;&#26381;&#21153;&#31561;&#22810;&#31181;&#26381;&#21153;&#20013;&#12290;&#30001;&#20110;&#29992;&#25143;&#21253;&#25324;&#26377;&#37325;&#35201;&#20449;&#24687;&#38656;&#27714;&#30340;&#20154;&#65292;&#22914;&#19982;&#32842;&#22825;&#26426;&#22120;&#20154;&#20132;&#20114;&#30340;&#23398;&#29983;&#25110;&#24739;&#32773;&#65292;&#22240;&#27492;&#36825;&#20123;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#24517;&#39035;&#26126;&#30830;&#20102;&#35299;LLMs&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;ChatGPT&#20013;&#30340;&#27602;&#24615;&#65292;&#36825;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#22522;&#20110;&#23545;&#35805;&#30340;LLM&#65292;&#36229;&#36807;&#21322;&#30334;&#19975;&#27425;Generation&#34987;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#20026;ChatGPT&#25351;&#23450;&#19968;&#20010;&#20154;&#29289;&#35282;&#33394;&#65292;&#27604;&#22914;&#25331;&#20987;&#25163;&#31302;&#32597;&#40664;&#24503;&#183;&#38463;&#37324;&#65292;&#21487;&#20197;&#26174;&#33879;&#22686;&#21152;&#20135;&#29983;&#30340;&#27602;&#24615;&#12290;&#26681;&#25454;&#25351;&#23450;&#32473;ChatGPT&#30340;&#35282;&#33394;&#65292;&#20854;&#27602;&#24615;&#21487;&#33021;&#20250;&#22686;&#21152;&#21040;6&#20493;&#65292;&#20854;&#36755;&#20986;&#20250;&#28041;&#21450;&#19981;&#27491;&#30830;&#30340;&#21051;&#26495;&#21360;&#35937;&#12289;&#26377;&#23475;&#30340;&#23545;&#35805;&#21644;&#20260;&#20154;&#30340;&#35328;&#35770;&#12290;&#36825;&#21487;&#33021;&#20250;&#28508;&#22312;&#22320;&#25439;&#23475;&#20154;&#29289;&#35282;&#33394;&#30340;&#21517;&#35465;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown incredible capabilities and transcended the natural language processing (NLP) community, with adoption throughout many services like healthcare, therapy, education, and customer service. Since users include people with critical information needs like students or patients engaging with chatbots, the safety of these systems is of prime importance. Therefore, a clear understanding of the capabilities and limitations of LLMs is necessary. To this end, we systematically evaluate toxicity in over half a million generations of ChatGPT, a popular dialogue-based LLM. We find that setting the system parameter of ChatGPT by assigning it a persona, say that of the boxer Muhammad Ali, significantly increases the toxicity of generations. Depending on the persona assigned to ChatGPT, its toxicity can increase up to 6x, with outputs engaging in incorrect stereotypes, harmful dialogue, and hurtful opinions. This may be potentially defamatory to the persona and ha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;MC-ViViT&#65292;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;&#38754;&#37096;&#29305;&#24449;&#26816;&#27979;&#32769;&#24180;&#20154;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#12290;&#36890;&#36807;MC&#27169;&#22359;&#21644;&#32467;&#21512;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#26679;&#26412;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05292</link><description>&lt;p&gt;
MC-ViViT: &#22810;&#20998;&#25903;&#20998;&#31867;&#22120;-ViViT&#29992;&#20110;&#20351;&#29992;&#38754;&#37096;&#35270;&#39057;&#26816;&#27979;&#32769;&#24180;&#20154;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults using Facial Videos. (arXiv:2304.05292v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;MC-ViViT&#65292;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;&#38754;&#37096;&#29305;&#24449;&#26816;&#27979;&#32769;&#24180;&#20154;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#12290;&#36890;&#36807;MC&#27169;&#22359;&#21644;&#32467;&#21512;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#26679;&#26412;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#24050;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#20351;&#29992;&#21307;&#23398;&#22270;&#20687;&#12289;&#38382;&#21367;&#21644;&#35270;&#39057;&#26816;&#27979;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;(MCI)&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#20998;&#25903;&#20998;&#31867;&#22120;-&#35270;&#39057;&#35270;&#35273;&#21464;&#25442;&#22120;(MC-ViViT)&#27169;&#22411;&#65292;&#36890;&#36807;&#20998;&#26512;&#38754;&#37096;&#29305;&#24449;&#21306;&#20998;MCI&#21644;&#27491;&#24120;&#35748;&#30693;&#12290;&#25968;&#25454;&#26469;&#33258;I-CONECT&#65292;&#19968;&#20010;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#39057;&#32321;&#35270;&#39057;&#32842;&#22825;&#26469;&#25913;&#21892;&#35748;&#30693;&#21151;&#33021;&#30340;&#34892;&#20026;&#24178;&#39044;&#35797;&#39564;&#12290;MC-ViViT&#22312;&#19968;&#20010;&#20998;&#25903;&#20013;&#25552;&#21462;&#35270;&#39057;&#30340;&#26102;&#31354;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;MC&#27169;&#22359;&#22686;&#24378;&#34920;&#31034;&#12290;&#30001;&#20110;I-CONECT&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#26412;&#19981;&#24179;&#34913;&#38382;&#39064;&#65288;&#21253;&#21547;&#38590;&#26131;&#21644;&#27491;&#36127;&#26679;&#26412;&#65289;&#65292;&#36825;&#20351;MC-ViViT&#30340;&#24615;&#33021;&#21463;&#21040;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Hard-Easy&#21644;Positive-Negative&#26679;&#26412;&#30340;&#25439;&#22833;&#20989;&#25968;&#65288;HP Loss&#65289;&#26469;&#32467;&#21512;&#23545;&#27604;&#24230;&#35843;&#33410;&#25439;&#22833;Focal loss&#21644;AD-CORRE loss&#26469;&#35299;&#20915;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;I-CONECT&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20986;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep machine learning models including Convolutional Neural Networks (CNN) have been successful in the detection of Mild Cognitive Impairment (MCI) using medical images, questionnaires, and videos. This paper proposes a novel Multi-branch Classifier-Video Vision Transformer (MC-ViViT) model to distinguish MCI from those with normal cognition by analyzing facial features. The data comes from the I-CONECT, a behavioral intervention trial aimed at improving cognitive function by providing frequent video chats. MC-ViViT extracts spatiotemporal features of videos in one branch and augments representations by the MC module. The I-CONECT dataset is challenging as the dataset is imbalanced containing Hard-Easy and Positive-Negative samples, which impedes the performance of MC-ViViT. We propose a loss function for Hard-Easy and Positive-Negative Samples (HP Loss) by combining Focal loss and AD-CORRE loss to address the imbalanced problem. Our experimental results on the I-CONECT dataset show th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#38590;&#24230;&#24863;&#30693;&#30340;&#22686;&#37327;&#23398;&#20064;&#21442;&#25968;&#20998;&#37197;&#19982;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26681;&#25454;&#20219;&#21153;&#30340;&#23398;&#20064;&#38590;&#24230;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#36866;&#24403;&#30340;&#20998;&#37197;&#25110;&#27491;&#21017;&#21270;&#31574;&#30053;&#65292;&#20197;&#20811;&#26381;&#22312;&#23398;&#20064;&#19981;&#21516;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.05288</link><description>&lt;p&gt;
&#20219;&#21153;&#38590;&#24230;&#24863;&#30693;&#30340;&#22686;&#37327;&#23398;&#20064;&#21442;&#25968;&#20998;&#37197;&#19982;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Task Difficulty Aware Parameter Allocation &amp; Regularization for Lifelong Learning. (arXiv:2304.05288v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#38590;&#24230;&#24863;&#30693;&#30340;&#22686;&#37327;&#23398;&#20064;&#21442;&#25968;&#20998;&#37197;&#19982;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26681;&#25454;&#20219;&#21153;&#30340;&#23398;&#20064;&#38590;&#24230;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#36866;&#24403;&#30340;&#20998;&#37197;&#25110;&#27491;&#21017;&#21270;&#31574;&#30053;&#65292;&#20197;&#20811;&#26381;&#22312;&#23398;&#20064;&#19981;&#21516;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#27491;&#21017;&#21270;&#25110;&#20998;&#37197;&#26041;&#27861;&#23545;&#20110;&#20811;&#26381;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#24207;&#21015;&#20013;&#22343;&#21248;&#35299;&#20915;&#25152;&#26377;&#20219;&#21153;&#65292;&#24573;&#30053;&#20102;&#19981;&#21516;&#20219;&#21153;&#30340;&#23398;&#20064;&#38590;&#24230;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#24403;&#23398;&#20064;&#19982;&#24050;&#23398;&#20219;&#21153;&#38750;&#24120;&#19981;&#21516;&#30340;&#26032;&#20219;&#21153;&#26102;&#65292;&#21442;&#25968;&#27491;&#21017;&#21270;&#26041;&#27861;&#20250;&#38754;&#20020;&#26174;&#30528;&#30340;&#36951;&#24536;&#65292;&#32780;&#21442;&#25968;&#20998;&#37197;&#26041;&#27861;&#22312;&#23398;&#20064;&#31616;&#21333;&#20219;&#21153;&#26102;&#21017;&#38754;&#20020;&#19981;&#24517;&#35201;&#30340;&#21442;&#25968;&#24320;&#38144;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21442;&#25968;&#20998;&#37197;&#21644;&#27491;&#21017;&#21270;&#31574;&#30053;&#65288;PAR&#65289;&#65292;&#20854;&#21487;&#20197;&#26681;&#25454;&#23398;&#20064;&#38590;&#24230;&#20174;&#21442;&#25968;&#20998;&#37197;&#21644;&#27491;&#21017;&#21270;&#20013;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#36866;&#24403;&#30340;&#31574;&#30053;&#12290;&#23545;&#20110;&#19968;&#20010;&#24050;&#32463;&#23398;&#20064;&#30456;&#20851;&#20219;&#21153;&#30340;&#27169;&#22411;&#26469;&#35828;&#65292;&#19968;&#20010;&#20219;&#21153;&#20250;&#21464;&#24471;&#23481;&#26131;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#26368;&#36817;&#21407;&#22411;&#36317;&#31163;&#30340;&#31163;&#25955;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;&#26032;&#20219;&#21153;&#30340;&#29305;&#24449;&#26469;&#24230;&#37327;&#20219;&#21153;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#25928;&#24615;&#30456;&#20851;&#24615;&#24863;&#30693;&#37319;&#26679;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter regularization or allocation methods are effective in overcoming catastrophic forgetting in lifelong learning. However, they solve all tasks in a sequence uniformly and ignore the differences in the learning difficulty of different tasks. So parameter regularization methods face significant forgetting when learning a new task very different from learned tasks, and parameter allocation methods face unnecessary parameter overhead when learning simple tasks. In this paper, we propose the Parameter Allocation &amp; Regularization (PAR), which adaptively select an appropriate strategy for each task from parameter allocation and regularization based on its learning difficulty. A task is easy for a model that has learned tasks related to it and vice versa. We propose a divergence estimation method based on the Nearest-Prototype distance to measure the task relatedness using only features of the new task. Moreover, we propose a time-efficient relatedness-aware sampling-based architecture
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#33258;&#21160;&#26426;&#24341;&#23548;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#35838;&#31243;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#36880;&#28176;&#36882;&#22686;&#30340;&#36866;&#21512;&#20195;&#29702;&#23398;&#20064;&#30340;&#20013;&#38388;&#20219;&#21153;&#24207;&#21015;&#65292;&#24182;&#33021;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#20379;&#39640;&#36798;5&#20493;&#30340;&#24555;&#36895;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2304.05271</link><description>&lt;p&gt;
&#33258;&#21160;&#26426;&#24341;&#23548;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#35838;&#31243;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Automaton-Guided Curriculum Generation for Reinforcement Learning Agents. (arXiv:2304.05271v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05271
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#33258;&#21160;&#26426;&#24341;&#23548;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#35838;&#31243;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#36880;&#28176;&#36882;&#22686;&#30340;&#36866;&#21512;&#20195;&#29702;&#23398;&#20064;&#30340;&#20013;&#38388;&#20219;&#21153;&#24207;&#21015;&#65292;&#24182;&#33021;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#20379;&#39640;&#36798;5&#20493;&#30340;&#24555;&#36895;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#35768;&#22810;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20173;&#28982;&#38590;&#20197;&#23398;&#20064;&#19988;&#25104;&#26412;&#39640;&#26114;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#20174;&#36923;&#36753;&#20219;&#21153;&#35268;&#33539;&#33258;&#21160;&#29983;&#25104;&#22870;&#21169;&#20989;&#25968;&#30340;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#38271;&#26102;&#38388;&#36328;&#24230;&#20219;&#21153;&#65288;&#21363;&#20195;&#29702;&#20154;&#38656;&#35201;&#25191;&#34892;&#19968;&#31995;&#21015;&#27491;&#30830;&#30340;&#25805;&#20316;&#20197;&#36798;&#21040;&#30446;&#26631;&#29366;&#24577;&#65292;&#21516;&#26102;&#22312;&#36873;&#25321;&#21160;&#20316;&#26102;&#32771;&#34385;&#26410;&#26469;&#30340;&#36807;&#28193;&#65289;&#19978;&#30340;&#25193;&#23637;&#24615;&#20173;&#28982;&#24456;&#24046;&#12290;&#21033;&#29992;&#35838;&#31243;&#65288;&#19968;&#31995;&#21015;&#36880;&#28176;&#22797;&#26434;&#30340;&#20219;&#21153;&#65289;&#36827;&#19968;&#27493;&#25552;&#39640;&#20195;&#29702;&#20154;&#30340;&#23398;&#20064;&#36895;&#24230;&#65292;&#36890;&#36807;&#23545;&#36866;&#21512;&#20195;&#29702;&#20154;&#23398;&#20064;&#33021;&#21147;&#30340;&#20013;&#38388;&#20219;&#21153;&#36827;&#34892;&#24207;&#21015;&#21270;&#12290;&#28982;&#32780;&#65292;&#20174;&#36923;&#36753;&#35268;&#33539;&#29983;&#25104;&#35838;&#31243;&#20173;&#28982;&#26159;&#19968;&#20010;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AGCL&#65292;&#21363;&#33258;&#21160;&#23548;&#21521;&#35838;&#31243;&#23398;&#20064;&#65292;&#19968;&#31181;&#33258;&#21160;&#20197;DAG&#24418;&#24335;&#29983;&#25104;&#30446;&#26631;&#20219;&#21153;&#35838;&#31243;&#30340;&#26032;&#26041;&#27861;&#12290;AGCL&#23558;&#35268;&#33539;&#32534;&#30721;&#20026;&#30830;&#23450;&#24615;&#33258;&#21160;&#26426;&#65292;&#25506;&#32034;&#20854;&#20013;&#30340;&#37325;&#22797;&#23376;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#23558;&#36825;&#20123;&#32467;&#26500;&#20998;&#35299;&#20026;DAG&#26469;&#29983;&#25104;&#35838;&#31243;&#12290;&#25105;&#20204;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;AGCL&#22987;&#32456;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#26041;&#27861;&#65292;&#24182;&#19988;&#33021;&#22815;&#19982;&#27809;&#26377;&#36827;&#34892;&#35838;&#31243;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25552;&#20379;&#39640;&#36798;5&#20493;&#30340;&#24555;&#36895;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite advances in Reinforcement Learning, many sequential decision making tasks remain prohibitively expensive and impractical to learn. Recently, approaches that automatically generate reward functions from logical task specifications have been proposed to mitigate this issue; however, they scale poorly on long-horizon tasks (i.e., tasks where the agent needs to perform a series of correct actions to reach the goal state, considering future transitions while choosing an action). Employing a curriculum (a sequence of increasingly complex tasks) further improves the learning speed of the agent by sequencing intermediate tasks suited to the learning capacity of the agent. However, generating curricula from the logical specification still remains an unsolved problem. To this end, we propose AGCL, Automaton-guided Curriculum Learning, a novel method for automatically generating curricula for the target task in the form of Directed Acyclic Graphs (DAGs). AGCL encodes the specification in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COTI&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#24341;&#20837;&#29702;&#35770;&#25351;&#23548;&#30340;&#25439;&#22833;&#30446;&#26631;&#21644;&#20840;&#38754;&#30340;&#21152;&#26435;&#35780;&#20998;&#26426;&#21046;&#65292;&#24182;&#32467;&#21512;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#26469;&#35299;&#20915;&#25991;&#26412;&#21453;&#36716;&#26102;&#30340;&#22256;&#38590;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#65292;&#25968;&#25454;&#25928;&#29575;&#39640;&#65292;&#26131;&#20110;&#20351;&#29992;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2304.05265</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#21487;&#25511;&#25991;&#26412;&#21453;&#36716;
&lt;/p&gt;
&lt;p&gt;
Controllable Textual Inversion for Personalized Text-to-Image Generation. (arXiv:2304.05265v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COTI&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#24341;&#20837;&#29702;&#35770;&#25351;&#23548;&#30340;&#25439;&#22833;&#30446;&#26631;&#21644;&#20840;&#38754;&#30340;&#21152;&#26435;&#35780;&#20998;&#26426;&#21046;&#65292;&#24182;&#32467;&#21512;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#26469;&#35299;&#20915;&#25991;&#26412;&#21453;&#36716;&#26102;&#30340;&#22256;&#38590;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#65292;&#25968;&#25454;&#25928;&#29575;&#39640;&#65292;&#26131;&#20110;&#20351;&#29992;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#22312;&#20197;&#25991;&#26412;&#20026;&#24341;&#23548;&#30340;&#39640;&#20445;&#30495;&#22270;&#20687;&#30340;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#12290;&#24403;&#24341;&#23548;&#20449;&#24687;&#21253;&#21547;&#29992;&#25143;&#23450;&#20041;&#30340;&#12289;&#26410;&#35265;&#36807;&#30340;&#25110;&#38271;&#23614;&#27010;&#24565;&#26631;&#35760;&#26102;&#65292;&#25991;&#26412;&#21453;&#36716;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#20010;&#24615;&#21270;&#29983;&#25104;&#25216;&#26415;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#23637;&#31034;&#20102;&#25991;&#26412;&#21453;&#36716;&#30340;&#37096;&#32626;&#20173;&#20805;&#28385;&#20102;&#8220;&#40657;&#39764;&#27861;&#8221;&#65292;&#20363;&#22914;&#39069;&#22806;&#25968;&#25454;&#38598;&#30340;&#20005;&#33499;&#35201;&#27714;&#65292;&#22312;&#24490;&#29615;&#20013;&#38656;&#35201;&#33392;&#33510;&#30340;&#20154;&#21147;&#25104;&#26412;&#21644;&#32570;&#20047;&#40065;&#26834;&#24615;&#31561;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21487;&#25511;&#25991;&#26412;&#21453;&#36716;&#30340;&#22823;&#22823;&#22686;&#24378;&#29256;&#21453;&#36716;&#65292;&#35299;&#20915;&#20102;&#25152;&#26377;&#19978;&#36848;&#38382;&#39064;&#65292;&#24182;&#21453;&#36807;&#26469;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#65292;&#25968;&#25454;&#25928;&#29575;&#39640;&#65292;&#26131;&#20110;&#20351;&#29992;&#30340;&#26694;&#26550;&#12290;COTI&#30340;&#26680;&#24515;&#26159;&#22522;&#20110;&#29702;&#35770;&#30340;&#25439;&#22833;&#30446;&#26631;&#65292;&#20855;&#26377;&#20840;&#38754;&#21644;&#26032;&#39062;&#30340;&#21152;&#26435;&#35780;&#20998;&#26426;&#21046;&#65292;&#24182;&#30001;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#25152;&#25552;&#21462;&#12290;&#24191;&#27867;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;COTI&#30340;&#24615;&#33021;&#27604;&#20043;&#21069;&#25216;&#26415;&#26377;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#65292;&#23588;&#20854;&#26159;&#22312;&#25968;&#25454;&#23569;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent large-scale generative modeling has attained unprecedented performance especially in producing high-fidelity images driven by text prompts. Text inversion (TI), alongside the text-to-image model backbones, is proposed as an effective technique in personalizing the generation when the prompts contain user-defined, unseen or long-tail concept tokens. Despite that, we find and show that the deployment of TI remains full of "dark-magics" -- to name a few, the harsh requirement of additional datasets, arduous human efforts in the loop and lack of robustness. In this work, we propose a much-enhanced version of TI, dubbed Controllable Textual Inversion (COTI), in resolving all the aforementioned problems and in turn delivering a robust, data-efficient and easy-to-use framework. The core to COTI is a theoretically-guided loss objective instantiated with a comprehensive and novel weighted scoring mechanism, encapsulated by an active-learning paradigm. The extensive results show that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#26032;&#38395;&#25512;&#33616;&#39046;&#22495;&#39318;&#27425;&#37319;&#29992;&#39044;&#35757;&#32451;&#65292;&#25552;&#31034;&#23398;&#20064;&#21644;&#39044;&#27979;&#33539;&#20363;&#26469;&#24320;&#21457;Prompt4NR&#26694;&#26550;&#30340;&#23454;&#39564;&#12290;&#35813;&#26694;&#26550;&#23558;&#39044;&#27979;&#28857;&#20987;&#20505;&#36873;&#26032;&#38395;&#30340;&#20219;&#21153;&#36716;&#21270;&#20026;cloze-style&#22635;&#31354;&#24335;&#25513;&#30721;&#39044;&#27979;&#20219;&#21153;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21033;&#29992;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20016;&#23500;&#35821;&#20041;&#20449;&#24687;&#21644;&#35821;&#35328;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2304.05263</link><description>&lt;p&gt;
&#26032;&#38395;&#25512;&#33616;&#20013;&#30340;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Prompt Learning for News Recommendation. (arXiv:2304.05263v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#26032;&#38395;&#25512;&#33616;&#39046;&#22495;&#39318;&#27425;&#37319;&#29992;&#39044;&#35757;&#32451;&#65292;&#25552;&#31034;&#23398;&#20064;&#21644;&#39044;&#27979;&#33539;&#20363;&#26469;&#24320;&#21457;Prompt4NR&#26694;&#26550;&#30340;&#23454;&#39564;&#12290;&#35813;&#26694;&#26550;&#23558;&#39044;&#27979;&#28857;&#20987;&#20505;&#36873;&#26032;&#38395;&#30340;&#20219;&#21153;&#36716;&#21270;&#20026;cloze-style&#22635;&#31354;&#24335;&#25513;&#30721;&#39044;&#27979;&#20219;&#21153;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21033;&#29992;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20016;&#23500;&#35821;&#20041;&#20449;&#24687;&#21644;&#35821;&#35328;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#19968;&#20123;&#26032;&#38395;&#25512;&#33616;&#65288;NR&#65289;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#26469;&#32534;&#30721;&#26032;&#38395;&#34920;&#31034;&#65292;&#37319;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#25512;&#33616;&#29305;&#23450;&#31070;&#32463;&#32593;&#32476;&#21644;&#30446;&#26631;&#20989;&#25968;&#26469;&#36981;&#24490;&#39321;&#33609;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#33539;&#20363;&#12290;&#30001;&#20110;&#20219;&#21153;&#30446;&#26631;&#19982;PLM&#19981;&#19968;&#33268;&#65292;&#25105;&#20204;&#35748;&#20026;&#20182;&#20204;&#30340;&#24314;&#27169;&#33539;&#24335;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#23884;&#20837;&#30340;&#20016;&#23500;&#35821;&#20041;&#20449;&#24687;&#21644;&#35821;&#35328;&#30693;&#35782;&#12290;&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#65292;&#25552;&#31034;&#21644;&#39044;&#27979;&#33539;&#20363;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#35768;&#22810;&#25104;&#21151;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31532;&#19968;&#27425;&#23581;&#35797;&#20351;&#29992;&#36825;&#31181;&#26032;&#33539;&#20363;&#26469;&#24320;&#21457;&#19968;&#20010;&#26032;&#38395;&#25512;&#33616;&#20013;&#30340;Prompt Learning (Prompt4NR) &#26694;&#26550;&#65292;&#23558;&#39044;&#27979;&#29992;&#25143;&#26159;&#21542;&#20250;&#28857;&#20987;&#20505;&#36873;&#26032;&#38395;&#30340;&#20219;&#21153;&#36716;&#21270;&#20026;&#22635;&#31354;&#24335;&#25513;&#30721;&#39044;&#27979;&#20219;&#21153;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;prompt&#27169;&#26495;&#65292;&#21253;&#25324;&#31163;&#25955;&#12289;&#36830;&#32493;...
&lt;/p&gt;
&lt;p&gt;
Some recent \textit{news recommendation} (NR) methods introduce a Pre-trained Language Model (PLM) to encode news representation by following the vanilla pre-train and fine-tune paradigm with carefully-designed recommendation-specific neural networks and objective functions. Due to the inconsistent task objective with that of PLM, we argue that their modeling paradigm has not well exploited the abundant semantic information and linguistic knowledge embedded in the pre-training process. Recently, the pre-train, prompt, and predict paradigm, called \textit{prompt learning}, has achieved many successes in natural language processing domain. In this paper, we make the first trial of this new paradigm to develop a \textit{Prompt Learning for News Recommendation} (Prompt4NR) framework, which transforms the task of predicting whether a user would click a candidate news as a cloze-style mask-prediction task. Specifically, we design a series of prompt templates, including discrete, continuous, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#37325;&#26032;&#21152;&#26435;softmax&#30340;&#20132;&#21449;&#29109;&#26041;&#27861;&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#23458;&#25143;&#31471;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#32531;&#35299;&#23458;&#25143;&#31471;&#36951;&#24536;&#24182;&#23545;&#26631;&#20934;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#25552;&#20379;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2304.05260</link><description>&lt;p&gt;
&#25511;&#21046;&#32852;&#37030;&#23398;&#20064;&#20013;&#36951;&#24536;&#38382;&#39064;&#30340;&#37325;&#26032;&#21152;&#26435;Softmax&#20132;&#21449;&#29109;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Re-Weighted Softmax Cross-Entropy to Control Forgetting in Federated Learning. (arXiv:2304.05260v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#37325;&#26032;&#21152;&#26435;softmax&#30340;&#20132;&#21449;&#29109;&#26041;&#27861;&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#23458;&#25143;&#31471;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#32531;&#35299;&#23458;&#25143;&#31471;&#36951;&#24536;&#24182;&#23545;&#26631;&#20934;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#25552;&#20379;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#32858;&#21512;&#22312;&#19968;&#32452;&#29420;&#31435;&#23458;&#25143;&#33410;&#28857;&#35745;&#31639;&#30340;&#27169;&#22411;&#26356;&#26032;&#26469;&#23398;&#20064;&#20840;&#23616;&#27169;&#22411;&#65292;&#22312;&#32858;&#21512;&#20043;&#21069;&#22312;&#27599;&#20010;&#33410;&#28857;&#19978;&#25191;&#34892;&#22810;&#20010;&#26799;&#24230;&#27493;&#39588;&#65292;&#20197;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25968;&#25454;&#24322;&#26500;&#24615;&#20250;&#23548;&#33268;&#19981;&#21516;&#30340;&#23458;&#25143;&#31471;&#20855;&#26377;&#19981;&#21516;&#30340;&#26412;&#22320;&#30446;&#26631;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#23458;&#25143;&#31471;&#36807;&#24230;&#20943;&#23569;&#20854;&#33258;&#24049;&#30340;&#26412;&#22320;&#30446;&#26631;&#65292;&#20351;&#20854;&#19982;&#20840;&#23616;&#35299;&#20998;&#27495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23545;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#20132;&#21449;&#29109;&#30446;&#26631;&#36827;&#34892;&#20462;&#25913;&#65292;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;softmax&#30340;logits&#20197;&#35745;&#31639;&#25439;&#22833;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#27599;&#20010;&#23458;&#25143;&#31471;&#27169;&#22411;&#23545;&#26469;&#33258;&#20854;&#20182;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20445;&#25252;&#19981;&#22312;&#23458;&#25143;&#31471;&#26631;&#31614;&#38598;&#20013;&#30340;&#31867;&#21035;&#20813;&#21463;&#31361;&#28982;&#30340;&#34920;&#31034;&#21464;&#21270;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#21487;&#20197;&#32531;&#35299;&#23458;&#25143;&#31471;&#36951;&#24536;&#24182;&#23545;&#26631;&#20934;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#25552;&#20379;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290; &#25105;&#20204;&#30340;&#26041;&#27861;&#29305;&#21035;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Federated Learning, a global model is learned by aggregating model updates computed at a set of independent client nodes, to reduce communication costs multiple gradient steps are performed at each node prior to aggregation. A key challenge in this setting is data heterogeneity across clients resulting in differing local objectives which can lead clients to overly minimize their own local objective, diverging from the global solution. We demonstrate that individual client models experience a catastrophic forgetting with respect to data from other clients and propose an efficient approach that modifies the cross-entropy objective on a per-client basis by re-weighting the softmax logits prior to computing the loss. This approach shields classes outside a client's label set from abrupt representation change and we empirically demonstrate it can alleviate client forgetting and provide consistent improvements to standard federated learning algorithms. Our method is particularly beneficia
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#29992;&#20110;&#20934;&#30830;&#22320;&#39044;&#27979;&#23398;&#29983;&#22312;&#26631;&#20934;&#21270;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#23398;&#29983;&#30340;&#21382;&#21490;&#25968;&#25454;&#65292;&#21253;&#25324;&#20182;&#20204;&#20197;&#24448;&#30340;&#32771;&#35797;&#25104;&#32489;&#12289;&#23398;&#20064;&#20064;&#24815;&#21644;&#20854;&#20182;&#30456;&#20851;&#20449;&#24687;&#65292;&#24182;&#22312;&#35299;&#30721;&#22120;&#36755;&#20837;&#20013;&#20351;&#29992;&#20102;&#22810;&#20010;&#26102;&#38388;&#29305;&#24449;&#31890;&#24230;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#19982;LightGBM&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#26356;&#21152;&#20934;&#30830;&#65292;&#20026;&#25945;&#32946;&#39046;&#22495;&#30340;AI&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20280;&#32553;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#23398;&#29983;&#25104;&#26524;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2304.05257</link><description>&lt;p&gt;
&#22810;&#31890;&#24230;&#26102;&#38388;&#21464;&#25442;&#22120;&#29992;&#20110;&#30693;&#35782;&#36861;&#36394;
&lt;/p&gt;
&lt;p&gt;
Multi-granulariy Time-based Transformer for Knowledge Tracing. (arXiv:2304.05257v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05257
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#29992;&#20110;&#20934;&#30830;&#22320;&#39044;&#27979;&#23398;&#29983;&#22312;&#26631;&#20934;&#21270;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#23398;&#29983;&#30340;&#21382;&#21490;&#25968;&#25454;&#65292;&#21253;&#25324;&#20182;&#20204;&#20197;&#24448;&#30340;&#32771;&#35797;&#25104;&#32489;&#12289;&#23398;&#20064;&#20064;&#24815;&#21644;&#20854;&#20182;&#30456;&#20851;&#20449;&#24687;&#65292;&#24182;&#22312;&#35299;&#30721;&#22120;&#36755;&#20837;&#20013;&#20351;&#29992;&#20102;&#22810;&#20010;&#26102;&#38388;&#29305;&#24449;&#31890;&#24230;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#19982;LightGBM&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#26356;&#21152;&#20934;&#30830;&#65292;&#20026;&#25945;&#32946;&#39046;&#22495;&#30340;AI&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20280;&#32553;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#23398;&#29983;&#25104;&#26524;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#39044;&#27979;&#26631;&#20934;&#21270;&#27979;&#35797;&#20013;&#23398;&#29983;&#30340;&#34920;&#29616;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#23398;&#29983;&#30340;&#21382;&#21490;&#25968;&#25454;&#65292;&#21253;&#25324;&#20182;&#20204;&#20197;&#24448;&#30340;&#32771;&#35797;&#25104;&#32489;&#12289;&#23398;&#20064;&#20064;&#24815;&#21644;&#20854;&#20182;&#30456;&#20851;&#20449;&#24687;&#65292;&#20026;&#27599;&#20010;&#23398;&#29983;&#21019;&#24314;&#19968;&#20010;&#20010;&#24615;&#21270;&#30340;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#26469;&#39044;&#27979;&#23398;&#29983;&#22312;&#32473;&#23450;&#27979;&#35797;&#20013;&#30340;&#26410;&#26469;&#34920;&#29616;&#12290;&#23558;&#35813;&#27169;&#22411;&#24212;&#29992;&#20110;RIIID&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35777;&#26126;&#20351;&#29992;&#22810;&#20010;&#26102;&#38388;&#29305;&#24449;&#31890;&#24230;&#20316;&#20026;&#35299;&#30721;&#22120;&#36755;&#20837;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#30456;&#23545;&#20110;LightGBM&#26041;&#27861;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#25945;&#32946;&#39046;&#22495;&#30340;AI&#21457;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20280;&#32553;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#23398;&#29983;&#25104;&#26524;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a transformer architecture for predicting student performance on standardized tests. Specifically, we leverage students historical data, including their past test scores, study habits, and other relevant information, to create a personalized model for each student. We then use these models to predict their future performance on a given test. Applying this model to the RIIID dataset, we demonstrate that using multiple granularities for temporal features as the decoder input significantly improve model performance. Our results also show the effectiveness of our approach, with substantial improvements over the LightGBM method. Our work contributes to the growing field of AI in education, providing a scalable and accurate tool for predicting student outcomes.
&lt;/p&gt;</description></item><item><title>OpenAL&#26159;&#19968;&#20010;&#28789;&#27963;&#19988;&#24320;&#28304;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19968;&#31995;&#21015;&#29616;&#23454;&#20219;&#21153;&#19978;&#36731;&#26494;&#36816;&#34892;&#21644;&#27604;&#36739;&#37319;&#26679;AL&#31574;&#30053;&#65292;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#25351;&#26631;&#21644;&#32479;&#35745;&#20998;&#26512;&#26041;&#27861;&#65292;&#38024;&#23545;&#20027;&#21160;&#23398;&#20064;&#30340;&#19968;&#27425;&#24615;&#29305;&#24615;&#65292;&#20174;&#19994;&#20154;&#21592;&#20063;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2304.05246</link><description>&lt;p&gt;
OpenAL: &#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#30340;&#35780;&#20272;&#19982;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
OpenAL: Evaluation and Interpretation of Active Learning Strategies. (arXiv:2304.05246v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05246
&lt;/p&gt;
&lt;p&gt;
OpenAL&#26159;&#19968;&#20010;&#28789;&#27963;&#19988;&#24320;&#28304;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19968;&#31995;&#21015;&#29616;&#23454;&#20219;&#21153;&#19978;&#36731;&#26494;&#36816;&#34892;&#21644;&#27604;&#36739;&#37319;&#26679;AL&#31574;&#30053;&#65292;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#25351;&#26631;&#21644;&#32479;&#35745;&#20998;&#26512;&#26041;&#27861;&#65292;&#38024;&#23545;&#20027;&#21160;&#23398;&#20064;&#30340;&#19968;&#27425;&#24615;&#29305;&#24615;&#65292;&#20174;&#19994;&#20154;&#21592;&#20063;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#26041;&#38754;&#24050;&#32463;&#26377;&#22823;&#37327;&#30340;&#25991;&#29486;&#65292;&#20294;&#26159;&#36824;&#27809;&#26377;&#19968;&#31181;&#20840;&#38754;&#19988;&#24320;&#25918;&#30340;&#22522;&#20934;&#21487;&#20197;&#26377;&#25928;&#22320;&#27604;&#36739;&#25552;&#20986;&#30340;&#37319;&#26679;&#22120;&#12290;&#27492;&#22806;&#65292;&#25991;&#29486;&#20013;&#23454;&#39564;&#35774;&#32622;&#30340;&#21464;&#24322;&#24615;&#20351;&#24471;&#36873;&#25321;&#37319;&#26679;&#31574;&#30053;&#21464;&#24471;&#22256;&#38590;&#65292;&#36825;&#26159;&#30001;&#20110;&#20027;&#21160;&#23398;&#20064;&#23454;&#39564;&#30340;&#19968;&#27425;&#24615;&#29305;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;OpenAL&#65292;&#36825;&#26159;&#19968;&#20010;&#28789;&#27963;&#19988;&#24320;&#28304;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19968;&#31995;&#21015;&#29616;&#23454;&#20219;&#21153;&#19978;&#36731;&#26494;&#36816;&#34892;&#21644;&#27604;&#36739;&#37319;&#26679;AL&#31574;&#30053;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#36824;&#21152;&#20837;&#20102;&#21487;&#35299;&#37322;&#24615;&#25351;&#26631;&#21644;&#32479;&#35745;&#20998;&#26512;&#26041;&#27861;&#65292;&#20197;&#20102;&#35299;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#19968;&#20123;&#37319;&#26679;&#22120;&#20248;&#20110;&#20854;&#20182;&#37319;&#26679;&#22120;&#12290;&#26368;&#21518;&#65292;&#20174;&#19994;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#25552;&#20132;&#33258;&#24049;&#30340;AL&#37319;&#26679;&#22120;&#36731;&#26494;&#25193;&#23637;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the vast body of literature on Active Learning (AL), there is no comprehensive and open benchmark allowing for efficient and simple comparison of proposed samplers. Additionally, the variability in experimental settings across the literature makes it difficult to choose a sampling strategy, which is critical due to the one-off nature of AL experiments. To address those limitations, we introduce OpenAL, a flexible and open-source framework to easily run and compare sampling AL strategies on a collection of realistic tasks. The proposed benchmark is augmented with interpretability metrics and statistical analysis methods to understand when and why some samplers outperform others. Last but not least, practitioners can easily extend the benchmark by submitting their own AL samplers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#39044;&#35757;&#32451;&#20195;&#30721;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#25506;&#32034;&#20102;&#21508;&#23618;&#39044;&#35757;&#32451;&#34920;&#31034;&#21644;&#32534;&#30721;&#30340;&#20195;&#30721;&#30693;&#35782;&#65292;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#24494;&#35843;&#26041;&#26696;&#12290;&#23454;&#39564;&#21457;&#29616;&#24494;&#35843;&#36807;&#31243;&#21487;&#20197;&#20445;&#30041;&#22823;&#37096;&#20998;&#20195;&#30721;&#23646;&#24615;&#65292;&#22522;&#26412;&#20195;&#30721;&#23646;&#24615;&#30001;&#36739;&#20302;&#21644;&#20013;&#38388;&#23618;&#25429;&#33719;&#12290;</title><link>http://arxiv.org/abs/2304.05216</link><description>&lt;p&gt;
&#38754;&#21521;&#39044;&#35757;&#32451;&#20195;&#30721;&#27169;&#22411;&#26377;&#25928;&#24494;&#35843;&#65306;&#23454;&#39564;&#30740;&#31350;&#21450;&#20854;&#25299;&#23637;
&lt;/p&gt;
&lt;p&gt;
Towards Efficient Fine-tuning of Pre-trained Code Models: An Experimental Study and Beyond. (arXiv:2304.05216v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#39044;&#35757;&#32451;&#20195;&#30721;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#25506;&#32034;&#20102;&#21508;&#23618;&#39044;&#35757;&#32451;&#34920;&#31034;&#21644;&#32534;&#30721;&#30340;&#20195;&#30721;&#30693;&#35782;&#65292;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#24494;&#35843;&#26041;&#26696;&#12290;&#23454;&#39564;&#21457;&#29616;&#24494;&#35843;&#36807;&#31243;&#21487;&#20197;&#20445;&#30041;&#22823;&#37096;&#20998;&#20195;&#30721;&#23646;&#24615;&#65292;&#22522;&#26412;&#20195;&#30721;&#23646;&#24615;&#30001;&#36739;&#20302;&#21644;&#20013;&#38388;&#23618;&#25429;&#33719;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#35768;&#22810;&#36719;&#20214;&#27979;&#35797;&#21644;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#23545;&#39044;&#35757;&#32451;&#20195;&#30721;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65288;&#22914;CodeBERT&#65289;&#20197;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#34429;&#28982;&#26377;&#25928;&#19988;&#27969;&#34892;&#65292;&#20294;&#24494;&#35843;&#39044;&#35757;&#32451;&#21442;&#25968;&#20250;&#23548;&#33268;&#22823;&#37327;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#25506;&#32034;&#24494;&#35843;&#26399;&#38388;&#27599;&#23618;&#39044;&#35757;&#32451;&#34920;&#31034;&#21644;&#32534;&#30721;&#30340;&#20195;&#30721;&#30693;&#35782;&#21457;&#29983;&#20102;&#20160;&#20040;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#19978;&#36848;&#21457;&#29616;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#20195;&#30721;&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#30740;&#31350;&#34920;&#26126;&#65306;&#65288;1&#65289;&#28304;&#20195;&#30721;&#30340;&#35789;&#27719;&#12289;&#35821;&#27861;&#21644;&#32467;&#26500;&#24615;&#36136;&#20998;&#21035;&#32534;&#30721;&#22312;&#36739;&#20302;&#12289;&#20013;&#38388;&#21644;&#36739;&#39640;&#30340;&#23618;&#20013;&#65292;&#32780;&#35821;&#20041;&#23646;&#24615;&#36328;&#36234;&#25972;&#20010;&#27169;&#22411;&#12290;&#65288;2&#65289;&#24494;&#35843;&#36807;&#31243;&#20445;&#30041;&#20102;&#22823;&#37096;&#20998;&#20195;&#30721;&#23646;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36739;&#20302;&#21644;&#20013;&#38388;&#23618;&#25429;&#33719;&#30340;&#22522;&#26412;&#20195;&#30721;&#23646;&#24615;&#22312;&#24494;&#35843;&#26399;&#38388;&#20173;&#28982;&#20445;&#30041;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
Recently, fine-tuning pre-trained code models such as CodeBERT on downstream tasks has achieved great success in many software testing and analysis tasks. While effective and prevalent, fine-tuning the pre-trained parameters incurs a large computational cost. In this paper, we conduct an extensive experimental study to explore what happens to layer-wise pre-trained representations and their encoded code knowledge during fine-tuning. We then propose efficient alternatives to fine-tune the large pre-trained code model based on the above findings. Our experimental study shows that (1) lexical, syntactic and structural properties of source code are encoded in the lower, intermediate, and higher layers, respectively, while the semantic property spans across the entire model. (2) The process of fine-tuning preserves most of the code properties. Specifically, the basic code properties captured by lower and intermediate layers are still preserved during fine-tuning. Furthermore, we find that o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#30340;&#21313;&#20159;&#32423;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;&#20102;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#23545;&#35813;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#24433;&#21709;&#65292;&#23454;&#39564;&#26174;&#31034;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05215</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#30340;&#21313;&#20159;&#32423;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Billion-scale Foundation Model for Remote Sensing Images. (arXiv:2304.05215v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#30340;&#21313;&#20159;&#32423;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;&#20102;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#23545;&#35813;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#24433;&#21709;&#65292;&#23454;&#39564;&#26174;&#31034;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20808;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#12290;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#19977;&#20010;&#20851;&#38190;&#22240;&#32032;&#26159;&#39044;&#35757;&#32451;&#26041;&#27861;&#12289;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#20197;&#21450;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;&#26368;&#36817;&#65292;&#36965;&#24863;&#39046;&#22495;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#65292;&#23545;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#20851;&#27880;&#36739;&#23569;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#23545;&#22522;&#30784;&#27169;&#22411;&#22312;&#26059;&#36716;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#31561;&#19979;&#28216;&#20219;&#21153;&#20013;&#24615;&#33021;&#30340;&#24433;&#21709;&#26469;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#25968;&#37327;&#21442;&#25968;&#65288;&#21253;&#25324;86M&#12289;605.26M&#12289;1.3B&#21644;2.4B&#65289;&#30340;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#30830;&#23450;&#21442;&#25968;&#22686;&#21152;&#26159;&#21542;&#20250;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#30340;&#21313;&#20159;&#32423;&#22522;&#30784;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;10&#20159;&#20010;&#36965;&#24863;&#22270;&#20687;&#30340;&#26032;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#21521;&#30740;&#31350;&#31038;&#21306;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the potential of foundation models in visual tasks has garnered significant attention, pretraining these models before downstream tasks has become a crucial step. The three key factors in pretraining foundation models are the pretraining method, the size of the pretraining dataset, and the number of model parameters. Recently, research in the remote sensing field has focused primarily on the pretraining method and the size of the dataset, with limited emphasis on the number of model parameters. This paper addresses this gap by examining the effect of increasing the number of model parameters on the performance of foundation models in downstream tasks such as rotated object detection and semantic segmentation. We pretrained foundation models with varying numbers of parameters, including 86M, 605.26M, 1.3B, and 2.4B, to determine whether performance in downstream tasks improved with an increase in parameters. To the best of our knowledge, this is the first billion-scale foundation mod
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; CGX &#30340;&#20998;&#35299;&#26041;&#27861;&#65292;&#20351;&#29992;&#21452;&#32447;&#24615;&#35268;&#21010;&#20174;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#34255;&#34920;&#31034;&#20013;&#25552;&#21462;&#35268;&#21017;&#65292;&#20248;&#21270;&#23545;&#40784;&#65292;&#22797;&#26434;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.05207</link><description>&lt;p&gt;
CGXplain: &#22522;&#20110;&#35268;&#21017;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#65292;&#20351;&#29992;&#21452;&#32447;&#24615;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
CGXplain: Rule-Based Deep Neural Network Explanations Using Dual Linear Programs. (arXiv:2304.05207v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05207
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; CGX &#30340;&#20998;&#35299;&#26041;&#27861;&#65292;&#20351;&#29992;&#21452;&#32447;&#24615;&#35268;&#21010;&#20174;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#34255;&#34920;&#31034;&#20013;&#25552;&#21462;&#35268;&#21017;&#65292;&#20248;&#21270;&#23545;&#40784;&#65292;&#22797;&#26434;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35268;&#21017;&#30340;&#20195;&#29702;&#27169;&#22411;&#26159;&#36817;&#20284;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20915;&#31574;&#36793;&#30028;&#30340;&#19968;&#31181;&#26377;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#65292;&#20351;&#20154;&#31867;&#26131;&#20110;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#20998;&#35299;&#26041;&#27861;&#65292;&#21363;&#37027;&#20123;&#32771;&#34385;&#21040;DNN&#30340;&#28508;&#22312;&#31354;&#38388;&#20197;&#25552;&#21462;&#26356;&#31934;&#30830;&#35268;&#21017;&#38598;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#25512;&#23548;&#20986;&#20102;&#39640;&#31934;&#24230;&#30340;&#35268;&#21017;&#38598;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204; a) &#19981;&#33021;&#20445;&#35777;&#20195;&#29702;&#27169;&#22411;&#24050;&#20174;&#19982; DNN &#30456;&#21516;&#30340;&#21464;&#37327;&#20013;&#23398;&#20064;&#65288;&#23545;&#40784;&#65289;&#65292;b) &#21482;&#20801;&#35768;&#20248;&#21270;&#21333;&#20010;&#30446;&#26631;&#65292;&#20363;&#22914;&#20934;&#30830;&#24615;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#36807;&#24230;&#22823;&#30340;&#35268;&#21017;&#38598;&#65288;&#22797;&#26434;&#24615;&#65289;&#65292;&#24182;&#19988; c) &#20351;&#29992;&#20915;&#31574;&#26641;&#31639;&#27861;&#20316;&#20026;&#20013;&#38388;&#27169;&#22411;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#30456;&#21516;DNN&#30340;&#19981;&#21516;&#35299;&#37322;&#65288;&#31283;&#23450;&#24615;&#65289;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#21452;&#32447;&#24615;&#35268;&#21010;&#20174;DNN&#30340;&#38544;&#34255;&#34920;&#31034;&#20013;&#25552;&#21462;&#35268;&#21017;&#30340;&#20998;&#35299;&#26041;&#27861; CGX&#65288;&#21015;&#29983;&#25104;&#35299;&#37322;&#22120;&#65289; &#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#20248;&#21270;&#22810;&#20010;&#30446;&#26631;&#65292;&#20363;&#22914;&#23545;&#40784;&#65292;&#22797;&#26434;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rule-based surrogate models are an effective and interpretable way to approximate a Deep Neural Network's (DNN) decision boundaries, allowing humans to easily understand deep learning models. Current state-of-the-art decompositional methods, which are those that consider the DNN's latent space to extract more exact rule sets, manage to derive rule sets at high accuracy. However, they a) do not guarantee that the surrogate model has learned from the same variables as the DNN (alignment), b) only allow to optimise for a single objective, such as accuracy, which can result in excessively large rule sets (complexity), and c) use decision tree algorithms as intermediate models, which can result in different explanations for the same DNN (stability). This paper introduces the CGX (Column Generation eXplainer) to address these limitations a decompositional method using dual linear programming to extract rules from the hidden representations of the DNN. This approach allows to optimise for a
&lt;/p&gt;</description></item><item><title>TinyReptile&#26159;&#19968;&#20010;&#32852;&#37030;&#20803;&#23398;&#20064;&#23454;&#29616;&#30340;&#36855;&#20320;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#36855;&#20320;&#35774;&#22791;&#19978;&#21327;&#20316;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#20840;&#23616;&#25968;&#25454;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#21644;&#20445;&#25252;&#26412;&#22320;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2304.05201</link><description>&lt;p&gt;
TinyReptile&#65306;&#32852;&#37030;&#20803;&#23398;&#20064;&#23454;&#29616;&#30340;&#36855;&#20320;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TinyReptile: TinyML with Federated Meta-Learning. (arXiv:2304.05201v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05201
&lt;/p&gt;
&lt;p&gt;
TinyReptile&#26159;&#19968;&#20010;&#32852;&#37030;&#20803;&#23398;&#20064;&#23454;&#29616;&#30340;&#36855;&#20320;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#36855;&#20320;&#35774;&#22791;&#19978;&#21327;&#20316;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#20840;&#23616;&#25968;&#25454;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#21644;&#20445;&#25252;&#26412;&#22320;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36855;&#20320;&#26426;&#22120;&#23398;&#20064;&#65288;TinyML&#65289;&#26159;&#19968;&#20010;&#36805;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#26088;&#22312;&#20026;&#36164;&#28304;&#21463;&#38480;&#30340;&#24494;&#25511;&#21046;&#22120;&#65288;MCU&#65289;&#27665;&#20027;&#21270;&#26426;&#22120;&#23398;&#20064;&#12290;&#37492;&#20110;&#36825;&#20123;&#24494;&#22411;&#35774;&#22791;&#30340;&#26222;&#21450;&#24615;&#65292;&#26377;&#24517;&#35201;&#38382;&#26159;&#21542;TinyML&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#20174;&#32858;&#21512;&#20182;&#20204;&#30340;&#30693;&#35782;&#20013;&#21463;&#30410;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#24471;&#20998;&#25955;&#30340;&#20195;&#29702;&#21487;&#20197;&#20849;&#21516;&#23398;&#20064;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#19981;&#20849;&#20139;&#25935;&#24863;&#30340;&#26412;&#22320;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23454;&#38469;&#37096;&#32626;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#21644;&#27599;&#20010;&#35774;&#22791;&#21487;&#29992;&#25968;&#25454;&#30340;&#24322;&#26500;&#24615;&#65292;&#19968;&#20010;&#24120;&#35265;&#30340;&#20840;&#23616;&#27169;&#22411;&#21487;&#33021;&#19981;&#33021;&#36866;&#29992;&#20110;&#25152;&#26377;&#35774;&#22791;&#12290;&#27492;&#22806;&#65292; TinyML &#30828;&#20214;&#30340;&#37096;&#32626;&#20855;&#26377;&#37325;&#35201;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#32422;&#26463;&#65292;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26080;&#27861;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; TinyReptile&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#28789;&#24863;&#26469;&#28304;&#20110;&#20803;&#23398;&#20064;&#21644;&#22312;&#32447;&#23398;&#20064;&#65292;&#21487;&#20197;&#22312;&#36855;&#20320;&#35774;&#22791;&#19978;&#21327;&#20316;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;&#22362;&#23454;&#21021;&#22987;&#21270;&#65292;&#21487;&#20197;&#24555;&#36895;&#36866;&#24212;&#36825;&#20123;&#35774;&#22791;&#30340;&#26412;&#22320;&#25968;&#25454;&#12290;&#25105;&#20204;&#20351;&#29992; FL &#26469;&#35780;&#20272; TinyReptile&#65292;&#32467;&#26524;&#34920;&#26126; TinyReptile &#21487;&#20197;&#22312;&#20445;&#25252;&#26412;&#22320;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#21033;&#29992;&#20840;&#23616;&#25968;&#25454;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#21644;&#31934;&#30830;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tiny machine learning (TinyML) is a rapidly growing field aiming to democratize machine learning (ML) for resource-constrained microcontrollers (MCUs). Given the pervasiveness of these tiny devices, it is inherent to ask whether TinyML applications can benefit from aggregating their knowledge. Federated learning (FL) enables decentralized agents to jointly learn a global model without sharing sensitive local data. However, a common global model may not work for all devices due to the complexity of the actual deployment environment and the heterogeneity of the data available on each device. In addition, the deployment of TinyML hardware has significant computational and communication constraints, which traditional ML fails to address. Considering these challenges, we propose TinyReptile, a simple but efficient algorithm inspired by meta-learning and online learning, to collaboratively learn a solid initialization for a neural network (NN) across tiny devices that can be quickly adapted 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#20449;&#24687;&#23450;&#20041;&#20248;&#21270;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26080;&#38656;&#25163;&#21160;&#35843;&#25972;&#36229;&#21442;&#25968;&#30340;&#19968;&#38454;&#20248;&#21270;&#22120; - &#33258;&#21160;&#26799;&#24230;&#19979;&#38477;&#12290;&#35813;&#31639;&#27861;&#22312;&#28145;&#24230;&#20840;&#36830;&#25509;&#32593;&#32476;&#21644;&#21367;&#31215;&#32593;&#32476;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19982;&#25163;&#21160;&#35843;&#25972;&#20248;&#21270;&#22120;&#30456;&#24403;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.05187</link><description>&lt;p&gt;
&#33258;&#21160;&#26799;&#24230;&#19979;&#38477;&#65306;&#26080;&#36229;&#21442;&#25968;&#30340;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Automatic Gradient Descent: Deep Learning without Hyperparameters. (arXiv:2304.05187v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#20449;&#24687;&#23450;&#20041;&#20248;&#21270;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26080;&#38656;&#25163;&#21160;&#35843;&#25972;&#36229;&#21442;&#25968;&#30340;&#19968;&#38454;&#20248;&#21270;&#22120; - &#33258;&#21160;&#26799;&#24230;&#19979;&#38477;&#12290;&#35813;&#31639;&#27861;&#22312;&#28145;&#24230;&#20840;&#36830;&#25509;&#32593;&#32476;&#21644;&#21367;&#31215;&#32593;&#32476;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19982;&#25163;&#21160;&#35843;&#25972;&#20248;&#21270;&#22120;&#30456;&#24403;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#27966;&#29983;&#29305;&#23450;&#20110;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#26080;&#36229;&#21442;&#25968;&#30340;&#19968;&#38454;&#20248;&#21270;&#22120;&#65292;&#31216;&#20026;&#8220;&#33258;&#21160;&#26799;&#24230;&#19979;&#38477;&#8221;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#26174;&#24335;&#22320;&#23450;&#20041;&#32593;&#32476;&#32467;&#26500;&#21442;&#25968;&#26469;&#20248;&#21270;&#28145;&#24230;&#20840;&#36830;&#25509;&#32593;&#32476;&#21644;&#21367;&#31215;&#32593;&#32476;&#65292;&#35777;&#26126;&#20102;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#19982;&#25163;&#21160;&#35843;&#25972;&#20248;&#21270;&#22120;&#25928;&#26524;&#30456;&#24403;&#12290;&#35813;&#31639;&#27861;&#25193;&#23637;&#20102;&#38236;&#20687;&#19979;&#38477;&#26041;&#27861;&#20197;&#22788;&#29702;&#38750;&#20984;&#24615;&#22797;&#21512;&#30446;&#26631;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The architecture of a deep neural network is defined explicitly in terms of the number of layers, the width of each layer and the general network topology. Existing optimisation frameworks neglect this information in favour of implicit architectural information (e.g. second-order methods) or architecture-agnostic distance functions (e.g. mirror descent). Meanwhile, the most popular optimiser in practice, Adam, is based on heuristics. This paper builds a new framework for deriving optimisation algorithms that explicitly leverage neural architecture. The theory extends mirror descent to non-convex composite objective functions: the idea is to transform a Bregman divergence to account for the non-linear structure of neural architecture. Working through the details for deep fully-connected networks yields automatic gradient descent: a first-order optimiser without any hyperparameters. Automatic gradient descent trains both fully-connected and convolutional networks out-of-the-box and at Im
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#33258;&#25105;&#30417;&#30563;&#31639;&#27861;DSLAD&#65292;&#36890;&#36807;&#35299;&#32806;&#24322;&#24120;&#21028;&#21035;&#21644;&#34920;&#31034;&#23398;&#20064;&#26469;&#36827;&#34892;&#23646;&#24615;&#22270;&#19978;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#24179;&#34913;&#30340;&#29305;&#24449;&#31354;&#38388;&#35299;&#20915;&#20102;&#35821;&#20041;&#28151;&#21512;&#21644;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.05176</link><description>&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#23646;&#24615;&#22270;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24322;&#24120;&#21028;&#21035;&#19982;&#34920;&#31034;&#23398;&#20064;&#35299;&#32806;&#65306;DSLAD&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Decoupling anomaly discrimination and representation learning: self-supervised learning for anomaly detection on attributed graph. (arXiv:2304.05176v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#33258;&#25105;&#30417;&#30563;&#31639;&#27861;DSLAD&#65292;&#36890;&#36807;&#35299;&#32806;&#24322;&#24120;&#21028;&#21035;&#21644;&#34920;&#31034;&#23398;&#20064;&#26469;&#36827;&#34892;&#23646;&#24615;&#22270;&#19978;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#24179;&#34913;&#30340;&#29305;&#24449;&#31354;&#38388;&#35299;&#20915;&#20102;&#35821;&#20041;&#28151;&#21512;&#21644;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23646;&#24615;&#22270;&#19978;&#30340;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#20010;&#23454;&#38469;&#24212;&#29992;&#20013;&#24456;&#37325;&#35201;&#30340;&#35838;&#39064;&#12290;&#29616;&#26377;&#26041;&#27861;&#22240;&#20027;&#35201;&#20851;&#27880;&#24322;&#24120;&#21028;&#21035;&#32780;&#24573;&#30053;&#34920;&#31034;&#23398;&#20064;&#65292;&#23384;&#22312;&#35821;&#20041;&#28151;&#21512;&#21644;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#36825;&#19982;&#24322;&#36136;&#24615;&#20551;&#35774;&#30456;&#20914;&#31361;&#65292;&#21363;&#24322;&#24120;&#33410;&#28857;&#36890;&#24120;&#30452;&#25509;&#19982;&#27491;&#24120;&#33410;&#28857;&#30456;&#36830;&#12290;&#27492;&#22806;&#65292;&#24322;&#24120;&#33410;&#28857;&#27604;&#27491;&#24120;&#33410;&#28857;&#23569;&#24471;&#22810;&#65292;&#34920;&#26126;&#25968;&#25454;&#20998;&#24067;&#21576;&#38271;&#23614;&#24418;&#24577;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#31639;&#27861;DSLAD&#65292;&#23427;&#26159;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24322;&#24120;&#21028;&#21035;&#21644;&#34920;&#31034;&#23398;&#20064;&#36827;&#34892;&#35299;&#32806;&#26469;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12290;DSLAD&#37319;&#29992;&#21452;&#32447;&#24615;&#27744;&#21644;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#20316;&#20026;&#24322;&#24120;&#21028;&#21035;&#22120;&#12290;&#36890;&#36807;&#35299;&#32806;&#24322;&#24120;&#21028;&#21035;&#21644;&#34920;&#31034;&#23398;&#20064;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#24179;&#34913;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#20854;&#20013;&#33410;&#28857;&#26356;&#20855;&#35821;&#20041;&#37492;&#21035;&#33021;&#21147;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;DSLAD&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection on attributed graphs is a crucial topic for its practical application. Existing methods suffer from semantic mixture and imbalance issue because they mainly focus on anomaly discrimination, ignoring representation learning. It conflicts with the assortativity assumption that anomalous nodes commonly connect with normal nodes directly. Additionally, there are far fewer anomalous nodes than normal nodes, indicating a long-tailed data distribution. To address these challenges, a unique algorithm,Decoupled Self-supervised Learning forAnomalyDetection (DSLAD), is proposed in this paper. DSLAD is a self-supervised method with anomaly discrimination and representation learning decoupled for anomaly detection. DSLAD employs bilinear pooling and masked autoencoder as the anomaly discriminators. By decoupling anomaly discrimination and representation learning, a balanced feature space is constructed, in which nodes are more semantically discriminative, as well as imbalance issu
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#38598;&#20307;&#26234;&#33021;&#24037;&#31243;&#26088;&#22312;&#21033;&#29992;&#22823;&#37327;&#20010;&#20307;&#20135;&#29983;&#36229;&#36234;&#20010;&#20307;&#33021;&#21147;&#30340;&#25928;&#24212;&#21644;&#38598;&#20307;&#26234;&#33021;&#34892;&#20026;&#65292;&#25104;&#20026;&#35745;&#31639;&#31995;&#32479;&#35774;&#35745;&#30446;&#26631;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#38598;&#20307;&#26234;&#33021;&#24037;&#31243;&#30340;&#35282;&#33394;&#12289;&#27010;&#24565;&#21644;&#25361;&#25112;&#65292;&#24378;&#35843;&#20854;&#23545;&#35745;&#31639;&#31995;&#32479;&#24037;&#31243;&#21644;&#35774;&#35745;&#30340;&#30456;&#20851;&#24615;&#21644;&#28508;&#22312;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.05147</link><description>&lt;p&gt;
&#20154;&#24037;&#38598;&#20307;&#26234;&#33021;&#24037;&#31243;&#65306;&#27010;&#24565;&#21644;&#35270;&#35282;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
Artificial Collective Intelligence Engineering: a Survey of Concepts and Perspectives. (arXiv:2304.05147v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05147
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#38598;&#20307;&#26234;&#33021;&#24037;&#31243;&#26088;&#22312;&#21033;&#29992;&#22823;&#37327;&#20010;&#20307;&#20135;&#29983;&#36229;&#36234;&#20010;&#20307;&#33021;&#21147;&#30340;&#25928;&#24212;&#21644;&#38598;&#20307;&#26234;&#33021;&#34892;&#20026;&#65292;&#25104;&#20026;&#35745;&#31639;&#31995;&#32479;&#35774;&#35745;&#30446;&#26631;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#38598;&#20307;&#26234;&#33021;&#24037;&#31243;&#30340;&#35282;&#33394;&#12289;&#27010;&#24565;&#21644;&#25361;&#25112;&#65292;&#24378;&#35843;&#20854;&#23545;&#35745;&#31639;&#31995;&#32479;&#24037;&#31243;&#21644;&#35774;&#35745;&#30340;&#30456;&#20851;&#24615;&#21644;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#20307;&#24615;&#26159;&#35768;&#22810;&#31995;&#32479;&#65288;&#21253;&#25324;&#33258;&#28982;&#21644;&#20154;&#24037;&#31995;&#32479;&#65289;&#30340;&#37325;&#35201;&#23646;&#24615;&#12290;&#21033;&#29992;&#22823;&#37327;&#30340;&#20010;&#20307;&#65292;&#24448;&#24448;&#21487;&#20197;&#20135;&#29983;&#36229;&#36234;&#26368;&#32874;&#26126;&#20010;&#20307;&#33021;&#21147;&#30340;&#25928;&#24212;&#65292;&#29978;&#33267;&#33021;&#22312;&#19981;&#37027;&#20040;&#32874;&#26126;&#30340;&#20010;&#20307;&#20013;&#20135;&#29983;&#26234;&#33021;&#30340;&#38598;&#20307;&#34892;&#20026;&#12290;&#20107;&#23454;&#19978;&#65292;&#38598;&#20307;&#26234;&#33021;&#65288;&#21363;&#19968;&#32676;&#20154;&#20197;&#30475;&#20284;&#32874;&#26126;&#30340;&#26041;&#24335;&#38598;&#20013;&#34892;&#21160;&#30340;&#33021;&#21147;&#65289;&#36234;&#26469;&#36234;&#25104;&#20026;&#35745;&#31639;&#31995;&#32479;&#30340;&#35774;&#35745;&#30446;&#26631;&#65292;&#36825;&#26159;&#21463;&#21040;&#29289;&#32852;&#32593;&#12289;&#32676;&#20307;&#26426;&#22120;&#20154;&#21644;&#20247;&#21253;&#35745;&#31639;&#31561;&#36817;&#26399;&#25216;&#26415;&#31185;&#23398;&#36235;&#21183;&#30340;&#25512;&#21160;&#12290;&#22810;&#24180;&#26469;&#65292;&#33258;&#28982;&#21644;&#20154;&#24037;&#31995;&#32479;&#20013;&#35266;&#23519;&#21040;&#30340;&#38598;&#20307;&#26234;&#33021;&#19968;&#30452;&#26159;&#24037;&#31243;&#24605;&#24819;&#12289;&#27169;&#22411;&#21644;&#26426;&#21046;&#30340;&#28789;&#24863;&#26469;&#28304;&#12290;&#22914;&#20170;&#65292;&#20154;&#24037;&#21644;&#35745;&#31639;&#38598;&#20307;&#26234;&#33021;&#25104;&#20026;&#35748;&#21487;&#30340;&#30740;&#31350;&#20027;&#39064;&#65292;&#28085;&#30422;&#21508;&#31181;&#25216;&#26415;&#12289;&#30446;&#26631;&#31995;&#32479;&#21644;&#24212;&#29992;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#35813;&#39046;&#22495;&#30340;&#25991;&#29486;&#24448;&#24448;&#32570;&#20047;&#23545;&#36825;&#31181;&#27169;&#24335;&#12289;&#20854;&#20027;&#35201;&#25216;&#26415;&#35201;&#32032;&#20197;&#21450;&#19982;&#20854;&#23454;&#38469;&#21033;&#29992;&#30456;&#20851;&#30340;&#25361;&#25112;&#30340;&#31995;&#32479;&#27010;&#36848;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#36825;&#26679;&#30340;&#32508;&#36848;&#65292;&#36890;&#36807;&#35752;&#35770;&#20154;&#24037;&#38598;&#20307;&#26234;&#33021;&#24037;&#31243;&#30340;&#35282;&#33394;&#12289;&#27010;&#24565;&#21644;&#25361;&#25112;&#65292;&#24182;&#24378;&#35843;&#23427;&#23545;&#35745;&#31639;&#31995;&#32479;&#24037;&#31243;&#21644;&#35774;&#35745;&#30340;&#30456;&#20851;&#24615;&#21644;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collectiveness is an important property of many systems--both natural and artificial. By exploiting a large number of individuals, it is often possible to produce effects that go far beyond the capabilities of the smartest individuals, or even to produce intelligent collective behaviour out of not-so-intelligent individuals. Indeed, collective intelligence, namely the capability of a group to act collectively in a seemingly intelligent way, is increasingly often a design goal of engineered computational systems--motivated by recent techno-scientific trends like the Internet of Things, swarm robotics, and crowd computing, just to name a few. For several years, the collective intelligence observed in natural and artificial systems has served as a source of inspiration for engineering ideas, models, and mechanisms. Today, artificial and computational collective intelligence are recognised research topics, spanning various techniques, kinds of target systems, and application domains. Howev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#35843;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#28436;&#31034;&#26469;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35843;&#35797;&#20854;&#39044;&#27979;&#30340;&#31243;&#24207;&#65292;&#22312;&#22810;&#39033;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05128</link><description>&lt;p&gt;
&#33258;&#25105;&#35843;&#35797;&#65306;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35843;&#35797;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Teaching Large Language Models to Self-Debug. (arXiv:2304.05128v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#35843;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#28436;&#31034;&#26469;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35843;&#35797;&#20854;&#39044;&#27979;&#30340;&#31243;&#24207;&#65292;&#22312;&#22810;&#39033;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#30340;&#32534;&#31243;&#20219;&#21153;&#65292;&#22312;&#19968;&#27425;&#24615;&#29983;&#25104;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#19968;&#20123;&#20808;&#21069;&#30340;&#24037;&#20316;&#35774;&#35745;&#20102;&#31243;&#24207;&#20462;&#22797;&#26041;&#27861;&#26469;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#25105;&#35843;&#35797;(Self-Debugging)&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#26679;&#26412;&#28436;&#31034;&#26469;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35843;&#35797;&#20854;&#39044;&#27979;&#30340;&#31243;&#24207;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#25105;&#35843;&#35797;&#21487;&#20197;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#27233;&#30382;&#40493;&#23376;&#35843;&#35797;(Rubber Duck Debugging)&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#22312;&#27809;&#26377;&#20219;&#20309;&#20851;&#20110;&#20195;&#30721;&#27491;&#30830;&#24615;&#25110;&#38169;&#35823;&#20449;&#24687;&#30340;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#29992;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#29983;&#25104;&#30340;&#20195;&#30721;&#26469;&#35782;&#21035;&#23427;&#30340;&#38169;&#35823;&#12290;&#33258;&#25105;&#35843;&#35797;&#22312;&#22810;&#39033;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#25991;&#26412;&#21040;SQL&#29983;&#25104;&#30340;Spider&#25968;&#25454;&#38598;&#65292;C++&#21040;Python&#32763;&#35793;&#30340;TransCoder&#21644;&#25991;&#26412;&#21040;Python&#29983;&#25104;&#30340;MBPP&#12290;&#22312;&#27809;&#26377;&#21333;&#20803;&#27979;&#35797;&#30340;Spider&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#33258;&#25105;&#35843;&#35797;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#31243;&#24207;&#20462;&#22797;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations. In particular, we demonstrate that Self-Debugging can teach the large language model to perform rubber duck debugging; i.e., without any feedback on the code correctness or error messages, the model is able to identify its mistakes by explaining the generated code in natural language. Self-Debugging achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation. On the Spider benchmark where there are no unit te
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#36816;&#21160;&#39044;&#27979;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#32570;&#20047;&#35299;&#37322;&#24615;&#21644;&#21487;&#33021;&#36829;&#21453;&#29289;&#29702;&#32422;&#26463;&#12290;&#22240;&#27492;&#65292;&#32467;&#21512;&#24046;&#20998;&#32422;&#26463;&#36816;&#21160;&#27169;&#22411;&#33021;&#25552;&#20379;&#29289;&#29702;&#19978;&#21487;&#34892;&#30340;&#36712;&#36857;&#65292;&#30740;&#31350;&#34920;&#26126;&#20302;&#38454;&#31215;&#20998;&#22120;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#25968;&#20540;&#27714;&#35299;&#22120;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.05116</link><description>&lt;p&gt;
&#19981;&#21516;&#32422;&#26463;&#36816;&#21160;&#27169;&#22411;&#22312;&#22522;&#20110;&#22270;&#30340;&#36712;&#36857;&#39044;&#27979;&#20013;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Differentially Constrained Motion Models for Graph-Based Trajectory Prediction. (arXiv:2304.05116v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05116
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#36816;&#21160;&#39044;&#27979;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#32570;&#20047;&#35299;&#37322;&#24615;&#21644;&#21487;&#33021;&#36829;&#21453;&#29289;&#29702;&#32422;&#26463;&#12290;&#22240;&#27492;&#65292;&#32467;&#21512;&#24046;&#20998;&#32422;&#26463;&#36816;&#21160;&#27169;&#22411;&#33021;&#25552;&#20379;&#29289;&#29702;&#19978;&#21487;&#34892;&#30340;&#36712;&#36857;&#65292;&#30740;&#31350;&#34920;&#26126;&#20302;&#38454;&#31215;&#20998;&#22120;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#25968;&#20540;&#27714;&#35299;&#22120;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#34920;&#29616;&#20986;&#33394;&#19988;&#21487;&#35843;&#24615;&#39640;&#65292;&#25104;&#20026;&#36816;&#21160;&#39044;&#27979;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#39640;&#24230;&#28789;&#27963;&#24615;&#20276;&#38543;&#30340;&#26159;&#35299;&#37322;&#24615;&#32570;&#22833;&#21644;&#21487;&#33021;&#36829;&#21453;&#30340;&#29289;&#29702;&#32422;&#26463;&#12290;&#20351;&#29992;&#24046;&#20998;&#32422;&#26463;&#36816;&#21160;&#27169;&#22411;&#26469;&#25552;&#20379;&#29289;&#29702;&#19978;&#21487;&#34892;&#30340;&#36712;&#36857;&#65292;&#21487;&#20197;&#20316;&#20026;&#19982;&#36825;&#20123;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30456;&#37197;&#21512;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#20808;&#21069;&#25552;&#20986;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411; MTP-GO&#65292;&#30740;&#31350;&#20102;&#21508;&#31181;&#36816;&#21160;&#27169;&#22411;&#32467;&#21512;&#25968;&#20540;&#27714;&#35299;&#22120;&#36827;&#34892;&#39044;&#27979;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#20026;&#20102;&#33719;&#24471;&#31934;&#30830;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#31616;&#21333;&#30340;&#27169;&#22411;&#65292;&#22914;&#20302;&#38454;&#31215;&#20998;&#22120;&#27169;&#22411;&#65292;&#20248;&#20110;&#26356;&#22797;&#26434;&#30340;&#36816;&#21160;&#23398;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25968;&#20540;&#27714;&#35299;&#22120;&#21487;&#20197;&#23545;&#36816;&#21160;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given their adaptability and encouraging performance, deep-learning models are becoming standard for motion prediction in autonomous driving. However, with great flexibility comes a lack of interpretability and possible violations of physical constraints. Accompanying these data-driven methods with differentially-constrained motion models to provide physically feasible trajectories is a promising future direction. The foundation for this work is a previously introduced graph-neural-network-based model, MTP-GO. The neural network learns to compute the inputs to an underlying motion model to provide physically feasible trajectories. This research investigates the performance of various motion models in combination with numerical solvers for the prediction task. The study shows that simpler models, such as low-order integrator models, are preferred over more complex ones, e.g., kinematic models, to achieve accurate predictions. Further, the numerical solver can have a substantial impact o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25351;&#20986;&#65292;&#22914;&#26524;&#24847;&#35782;&#22312;&#21160;&#24577;&#19978;&#20855;&#26377;&#30456;&#20851;&#24615;&#65292;&#21017;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26080;&#27861;&#20855;&#26377;&#24847;&#35782;&#65292;&#22240;&#20026;&#36825;&#20123;&#31995;&#32479;&#30340;&#35774;&#35745;&#21644;&#39564;&#35777;&#25490;&#38500;&#20102;&#21487;&#33021;&#19982;&#24847;&#35782;&#30456;&#20851;&#30340;&#21160;&#21147;&#23398;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2304.05077</link><description>&lt;p&gt;
&#22914;&#26524;&#24847;&#35782;&#22312;&#21160;&#24577;&#19978;&#20855;&#26377;&#30456;&#20851;&#24615;&#65292;&#21017;&#20154;&#24037;&#26234;&#33021;&#19981;&#20250;&#20855;&#26377;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
If consciousness is dynamically relevant, artificial intelligence isn't conscious. (arXiv:2304.05077v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25351;&#20986;&#65292;&#22914;&#26524;&#24847;&#35782;&#22312;&#21160;&#24577;&#19978;&#20855;&#26377;&#30456;&#20851;&#24615;&#65292;&#21017;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26080;&#27861;&#20855;&#26377;&#24847;&#35782;&#65292;&#22240;&#20026;&#36825;&#20123;&#31995;&#32479;&#30340;&#35774;&#35745;&#21644;&#39564;&#35777;&#25490;&#38500;&#20102;&#21487;&#33021;&#19982;&#24847;&#35782;&#30456;&#20851;&#30340;&#21160;&#21147;&#23398;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#22914;&#26524;&#24847;&#35782;&#19982;&#31995;&#32479;&#29366;&#24577;&#30340;&#26102;&#38388;&#28436;&#21270;&#26377;&#20851;&#65292;&#20063;&#23601;&#26159;&#35828;&#65292;&#22914;&#26524;&#23427;&#22312;&#21160;&#24577;&#19978;&#20855;&#26377;&#30456;&#20851;&#24615;&#65292;&#21017;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23601;&#19981;&#33021;&#20855;&#26377;&#24847;&#35782;&#12290;&#36825;&#26159;&#22240;&#20026;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36816;&#34892;&#22312;CPU&#12289;GPU&#12289;TPU&#25110;&#20854;&#20182;&#22788;&#29702;&#22120;&#19978;&#65292;&#36825;&#20123;&#22788;&#29702;&#22120;&#30340;&#35774;&#35745;&#21644;&#39564;&#35777;&#26159;&#20026;&#20102;&#36981;&#23432;&#35745;&#31639;&#21160;&#21147;&#23398;&#65292;&#20174;&#32780;&#31995;&#32479;&#22320;&#25490;&#38500;&#25110;&#25233;&#21046;&#20559;&#24046;&#12290;&#29305;&#21035;&#26159;&#65292;&#35774;&#35745;&#21644;&#39564;&#35777;&#25490;&#38500;&#25110;&#25233;&#21046;&#20102;&#21487;&#33021;&#19982;&#24847;&#35782;&#30456;&#20851;&#30340;&#21160;&#21147;&#23398;&#25928;&#24212;&#65292;&#22240;&#27492;&#22914;&#26524;&#24847;&#35782;&#22312;&#21160;&#24577;&#19978;&#20855;&#26377;&#30456;&#20851;&#24615;&#65292;&#21017;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19981;&#33021;&#20855;&#26377;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate that if consciousness is relevant for the temporal evolution of a system's states -- that is, if it is dynamically relevant -- then AI systems cannot be conscious. That is because AI systems run on CPUs, GPUs, TPUs or other processors which have been designed and verified to adhere to computational dynamics that systematically preclude or suppress deviations. The design and verification preclude or suppress, in particular, potential consciousness-related dynamical effects, so that if consciousness is dynamically relevant, AI systems cannot be conscious.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20986;&#19968;&#31181;&#39640;&#31934;&#24230;&#30340;&#32954;&#30284;&#26816;&#27979;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#39044;&#27979;&#32954;&#30284;&#39118;&#38505;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2304.05065</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#32954;&#30284;&#39118;&#38505;&#22240;&#32032;&#20154;&#24037;&#26234;&#33021;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence based prediction on lung cancer risk factors using deep learning. (arXiv:2304.05065v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20986;&#19968;&#31181;&#39640;&#31934;&#24230;&#30340;&#32954;&#30284;&#26816;&#27979;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#39044;&#27979;&#32954;&#30284;&#39118;&#38505;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65288;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65289;&#24320;&#21457;&#20986;&#19968;&#31181;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#32954;&#30284;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20998;&#26512;&#24739;&#32773;&#35760;&#24405;&#30340;&#21382;&#21490;&#25968;&#25454;&#65292;&#26412;&#30740;&#31350;&#30830;&#23450;&#32954;&#30284;&#26159;&#39044;&#27979;&#32959;&#30244;&#26089;&#26399;&#38454;&#27573;&#30340;&#19968;&#20010;&#37325;&#35201;&#30740;&#31350;&#38382;&#39064;&#12290;&#19982;VGG16&#12289;InceptionV3&#21644;Resnet50&#31561;&#29616;&#26377;&#27169;&#22411;&#30456;&#27604;&#65292;&#26412;&#30740;&#31350;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;94&#65285;&#30340;&#20934;&#30830;&#29575;&#21644;0.1&#65285;&#30340;&#26368;&#23567;&#25439;&#22833;&#65292;&#21487;&#20197;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#39044;&#27979;&#32954;&#30284;&#39118;&#38505;&#22240;&#32032;&#65292;&#20026;&#21307;&#29983;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#32954;&#30284;&#26816;&#27979;&#25163;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this proposed work, we identified the significant research issues on lung cancer risk factors. Capturing and defining symptoms at an early stage is one of the most difficult phases for patients. Based on the history of patients records, we reviewed a number of current research studies on lung cancer and its various stages. We identified that lung cancer is one of the significant research issues in predicting the early stages of cancer disease. This research aimed to develop a model that can detect lung cancer with a remarkably high level of accuracy using the deep learning approach (convolution neural network). This method considers and resolves significant gaps in previous studies. We compare the accuracy levels and loss values of our model with VGG16, InceptionV3, and Resnet50. We found that our model achieved an accuracy of 94% and a minimum loss of 0.1%. Hence physicians can use our convolution neural network models for predicting lung cancer risk factors in the real world. More
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#30740;&#31350;&#29616;&#29366;&#21644;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#20248;&#21183;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.05055</link><description>&lt;p&gt;
&#28145;&#24230;&#22270;&#34920;&#31034;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Deep Graph Representation Learning. (arXiv:2304.05055v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#30740;&#31350;&#29616;&#29366;&#21644;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#20248;&#21183;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#23558;&#39640;&#32500;&#31232;&#30095;&#30340;&#22270;&#32467;&#26500;&#25968;&#25454;&#26377;&#25928;&#22320;&#32534;&#30721;&#25104;&#20302;&#32500;&#23494;&#38598;&#21521;&#37327;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;&#65292;&#22312;&#21253;&#25324;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#25366;&#25496;&#22312;&#20869;&#30340;&#19968;&#31995;&#21015;&#39046;&#22495;&#37117;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#20256;&#32479;&#22270;&#23884;&#20837;&#26041;&#27861;&#36981;&#24490;&#36825;&#26679;&#19968;&#31181;&#22522;&#26412;&#24605;&#24819;&#65292;&#21363;&#22270;&#20013;&#30456;&#20114;&#36830;&#25509;&#30340;&#33410;&#28857;&#30340;&#23884;&#20837;&#30690;&#37327;&#20173;&#28982;&#33021;&#22815;&#20445;&#25345;&#30456;&#23545;&#25509;&#36817;&#30340;&#36317;&#31163;&#65292;&#20174;&#32780;&#20445;&#30041;&#20102;&#22270;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#20197;&#19979;&#38382;&#39064;&#65306;&#65288;i&#65289;&#20256;&#32479;&#26041;&#27861;&#30340;&#27169;&#22411;&#23481;&#37327;&#21463;&#38480;&#65292;&#38480;&#21046;&#20102;&#23398;&#20064;&#24615;&#33021;; &#65288;ii&#65289;&#29616;&#26377;&#25216;&#26415;&#36890;&#24120;&#20381;&#36182;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#65292;&#26080;&#27861;&#19982;&#26368;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#30456;&#32467;&#21512;&#65307;&#65288;iii&#65289;&#34920;&#31034;&#23398;&#20064;&#21644;&#19979;&#28216;&#20219;&#21153;&#30456;&#20114;&#20381;&#23384;&#65292;&#24212;&#20849;&#21516;&#21152;&#24378;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#26174;&#30528;&#25104;&#21151;&#65292;&#28145;&#24230;&#22270;&#34920;&#31034;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph representation learning aims to effectively encode high-dimensional sparse graph-structured data into low-dimensional dense vectors, which is a fundamental task that has been widely studied in a range of fields, including machine learning and data mining. Classic graph embedding methods follow the basic idea that the embedding vectors of interconnected nodes in the graph can still maintain a relatively close distance, thereby preserving the structural information between the nodes in the graph. However, this is sub-optimal due to: (i) traditional methods have limited model capacity which limits the learning performance; (ii) existing techniques typically rely on unsupervised learning strategies and fail to couple with the latest learning paradigms; (iii) representation learning and downstream tasks are dependent on each other which should be jointly enhanced. With the remarkable success of deep learning, deep graph representation learning has shown great potential and advantages 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#23545;&#25239;&#26679;&#26412;&#23545;&#38754;&#37096;&#35782;&#21035;&#31995;&#32479;&#30340;&#22810;&#20010;&#32452;&#20214;&#30340;&#21516;&#26102;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#25915;&#20987;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#35813;&#30740;&#31350;&#20026;FRS&#30340;&#25915;&#20987;&#21521;&#37327;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#22686;&#24378;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.05048</link><description>&lt;p&gt;
&#22810;&#20010;&#38754;&#37096;&#35782;&#21035;&#31995;&#32479;&#32452;&#20214;&#30340;&#21516;&#26102;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Simultaneous Adversarial Attacks On Multiple Face Recognition System Components. (arXiv:2304.05048v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#23545;&#25239;&#26679;&#26412;&#23545;&#38754;&#37096;&#35782;&#21035;&#31995;&#32479;&#30340;&#22810;&#20010;&#32452;&#20214;&#30340;&#21516;&#26102;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#25915;&#20987;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#35813;&#30740;&#31350;&#20026;FRS&#30340;&#25915;&#20987;&#21521;&#37327;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#22686;&#24378;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23545;&#25239;&#26679;&#26412;&#23545;&#38754;&#37096;&#35782;&#21035;&#31995;&#32479;&#23433;&#20840;&#24615;&#30340;&#28508;&#22312;&#23041;&#32961;&#12290;&#34429;&#28982;&#20197;&#24448;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#35752;&#20102;&#23545;FRS&#30340;&#21508;&#20010;&#21333;&#29420;&#32452;&#20214;&#30340;&#23545;&#25239;&#39118;&#38505;&#65292;&#20294;&#25105;&#20204;&#30340;&#30740;&#31350;&#39318;&#27425;&#25506;&#32034;&#20102;&#36890;&#36807;&#21516;&#26102;&#25915;&#20987;FRS&#31649;&#36947;&#20013;&#30340;&#20154;&#33080;&#26816;&#27979;&#22120;&#21644;&#29305;&#24449;&#25552;&#21462;&#22120;&#26469;&#27450;&#39575;&#22810;&#20010;&#32452;&#20214;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#38024;&#23545;FRS&#30340;&#22810;&#30446;&#26631;&#25915;&#20987;&#65292;&#24182;&#36890;&#36807;&#23545;&#30446;&#26631;&#31995;&#32479;&#30340;&#21021;&#27493;&#23454;&#39564;&#20998;&#26512;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#21487;&#20197;&#36798;&#21040;100&#65285;&#65292;&#33021;&#22815;&#25805;&#32437;&#38754;&#37096;&#26816;&#27979;&#27010;&#29575;&#39640;&#36798;50&#65285;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#23545;&#25239;&#30446;&#26631;&#12290;&#26412;&#30740;&#31350;&#30830;&#23450;&#24182;&#26816;&#26597;&#20102;&#38024;&#23545;FRS&#30340;&#26032;&#22411;&#25915;&#20987;&#21521;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#21033;&#29992;&#25915;&#20987;&#21521;&#37327;&#30340;&#30693;&#35782;&#22312;&#35757;&#32451;FRS&#32452;&#20214;&#26102;&#22686;&#24378;&#20854;&#40065;&#26834;&#24615;&#30340;&#21487;&#33021;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we investigate the potential threat of adversarial examples to the security of face recognition systems. Although previous research has explored the adversarial risk to individual components of FRSs, our study presents an initial exploration of an adversary simultaneously fooling multiple components: the face detector and feature extractor in an FRS pipeline. We propose three multi-objective attacks on FRSs and demonstrate their effectiveness through a preliminary experimental analysis on a target system. Our attacks achieved up to 100% Attack Success Rates against both the face detector and feature extractor and were able to manipulate the face detection probability by up to 50% depending on the adversarial objective. This research identifies and examines novel attack vectors against FRSs and suggests possible ways to augment the robustness by leveraging the attack vector's knowledge during training of an FRS's components.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#19981;&#21516;&#30340;&#22825;&#27668;&#26465;&#20214;&#19979;&#20154;&#20204;&#22312;&#25512;&#29305;&#19978;&#35848;&#35770;&#30340;&#39135;&#29289;&#23384;&#22312;&#24046;&#24322;&#65292;&#30740;&#31350;&#36825;&#31181;&#29616;&#35937;&#21487;&#20419;&#36827;&#23545;&#39135;&#21697;&#28040;&#36153;&#32773;&#36873;&#25321;&#21644;&#30475;&#27861;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.05041</link><description>&lt;p&gt;
&#22312;&#38632;&#22825;&#25105;&#20204;&#20250;&#25512;&#29305;&#21738;&#20123;&#39135;&#29289;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Food Do We Tweet about on a Rainy Day?. (arXiv:2304.05041v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05041
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#19981;&#21516;&#30340;&#22825;&#27668;&#26465;&#20214;&#19979;&#20154;&#20204;&#22312;&#25512;&#29305;&#19978;&#35848;&#35770;&#30340;&#39135;&#29289;&#23384;&#22312;&#24046;&#24322;&#65292;&#30740;&#31350;&#36825;&#31181;&#29616;&#35937;&#21487;&#20419;&#36827;&#23545;&#39135;&#21697;&#28040;&#36153;&#32773;&#36873;&#25321;&#21644;&#30475;&#27861;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39135;&#21697;&#36873;&#25321;&#26159;&#19968;&#20010;&#30001;&#21697;&#21619;&#12289;&#27675;&#22260;&#12289;&#25991;&#21270;&#25110;&#22825;&#27668;&#31561;&#22240;&#32032;&#22609;&#36896;&#30340;&#22797;&#26434;&#29616;&#35937;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19981;&#21516;&#22825;&#27668;&#26465;&#20214;&#19979;&#19982;&#39135;&#21697;&#30456;&#20851;&#30340;&#25512;&#29305;&#25968;&#25454;&#12290;&#25105;&#20204;&#32467;&#21512;&#20102;&#35206;&#30422;&#36807;&#21435;&#21313;&#24180;&#30340;&#25289;&#33073;&#32500;&#20122;&#39135;&#21697;&#25512;&#29305;&#25968;&#25454;&#38598;&#21644;&#21253;&#21547;&#24179;&#22343;&#28201;&#24230;&#12289;&#38477;&#27700;&#31561;&#29616;&#35937;&#30340;&#22825;&#27668;&#35266;&#23519;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#21738;&#20123;&#22825;&#27668;&#26465;&#20214;&#23548;&#33268;&#20102;&#29305;&#23450;&#30340;&#39135;&#21697;&#20449;&#24687;&#20849;&#20139;&#65307;&#33258;&#21160;&#20998;&#31867;&#25512;&#29305;&#24773;&#24863;&#24182;&#35752;&#35770;&#22914;&#20309;&#26681;&#25454;&#22825;&#27668;&#24773;&#20917;&#21464;&#21270;&#12290;&#36825;&#39033;&#30740;&#31350;&#26377;&#21161;&#20110;&#22823;&#35268;&#27169;&#31038;&#20132;&#32593;&#32476;&#25968;&#25454;&#30340;&#29702;&#35299;&#65292;&#20102;&#35299;&#39135;&#21697;&#28040;&#36153;&#32773;&#30340;&#36873;&#25321;&#21644;&#30475;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Food choice is a complex phenomenon shaped by factors such as taste, ambience, culture or weather. In this paper, we explore food-related tweeting in different weather conditions. We inspect a Latvian food tweet dataset spanning the past decade in conjunction with a weather observation dataset consisting of average temperature, precipitation, and other phenomena. We find which weather conditions lead to specific food information sharing; automatically classify tweet sentiment and discuss how it changes depending on the weather. This research contributes to the growing area of large-scale social network data understanding of food consumers' choices and perceptions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#26426;&#21512;&#20316;&#30340;&#26041;&#27861;&#65292;&#23558;&#26377;&#38480;&#25968;&#25454;&#30340;&#20154;&#31867;&#35789;&#27719;&#35821;&#20041;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#39640;&#25928;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35821;&#20041;&#29305;&#24449;&#35268;&#33539;&#12290;</title><link>http://arxiv.org/abs/2304.05012</link><description>&lt;p&gt;
&#20154;&#26426;&#21512;&#20316;&#29983;&#25104;&#35821;&#20041;&#29305;&#24449;&#21015;&#34920;
&lt;/p&gt;
&lt;p&gt;
Human-machine cooperation for semantic feature listing. (arXiv:2304.05012v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#26426;&#21512;&#20316;&#30340;&#26041;&#27861;&#65292;&#23558;&#26377;&#38480;&#25968;&#25454;&#30340;&#20154;&#31867;&#35789;&#27719;&#35821;&#20041;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#39640;&#25928;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35821;&#20041;&#29305;&#24449;&#35268;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#29305;&#24449;&#35268;&#33539;&#26159;&#25551;&#36848;&#20154;&#31867;&#27010;&#24565;&#30693;&#35782;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20026;&#33258;&#21160;&#29983;&#25104;&#27492;&#31867;&#21151;&#33021;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#65292;&#20294;&#23481;&#26131;&#20986;&#29616;&#26174;&#33879;&#30340;&#38169;&#35823;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#26469;&#33258;&#26377;&#38480;&#25968;&#25454;&#30340;&#23398;&#20064;&#22411;&#20154;&#31867;&#35789;&#27719;&#35821;&#20041;&#27169;&#22411;&#19982;LLM&#29983;&#25104;&#30340;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#20197;&#39640;&#25928;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#29305;&#24449;&#35268;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic feature norms, lists of features that concepts do and do not possess, have played a central role in characterizing human conceptual knowledge, but require extensive human labor. Large language models (LLMs) offer a novel avenue for the automatic generation of such feature lists, but are prone to significant error. Here, we present a new method for combining a learned model of human lexical-semantics from limited data with LLM-generated data to efficiently generate high-quality feature norms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#36125;&#21494;&#26031;&#29702;&#35770;&#30340;&#34892;&#20026;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#36125;&#21494;&#26031;&#28508;&#21464;&#37327;&#8220;&#24847;&#22270;&#8221;&#65292;&#23558;&#20064;&#24815;&#24615;&#34892;&#20026;&#21644;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#32852;&#31995;&#36215;&#26469;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#28789;&#27963;&#12289;&#39640;&#25928;&#30340;&#34892;&#20026;&#27169;&#25311;&#12290;</title><link>http://arxiv.org/abs/2304.05008</link><description>&lt;p&gt;
&#20064;&#24815;&#19982;&#30446;&#26631;&#30340;&#21327;&#21516;&#20316;&#29992;&#65306;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#36125;&#21494;&#26031;&#26694;&#26550;&#30340;&#34892;&#20026;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Habits and goals in synergy: a variational Bayesian framework for behavior. (arXiv:2304.05008v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#36125;&#21494;&#26031;&#29702;&#35770;&#30340;&#34892;&#20026;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#36125;&#21494;&#26031;&#28508;&#21464;&#37327;&#8220;&#24847;&#22270;&#8221;&#65292;&#23558;&#20064;&#24815;&#24615;&#34892;&#20026;&#21644;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#32852;&#31995;&#36215;&#26469;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#28789;&#27963;&#12289;&#39640;&#25928;&#30340;&#34892;&#20026;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#39640;&#25928;&#12289;&#28789;&#27963;&#22320;&#34892;&#20026;&#26159;&#29702;&#35299;&#29983;&#29289;&#26426;&#20307;&#21644;&#21019;&#24314;&#26234;&#33021;&#21270;&#36523;&#20154;&#24037;&#26234;&#33021;&#30340;&#26680;&#24515;&#38382;&#39064;&#12290;&#34892;&#20026;&#34987;&#24402;&#31867;&#20026;&#20004;&#31181;&#31867;&#22411;&#65306;&#24555;&#36895;&#32780;&#19981;&#28789;&#27963;&#30340;&#22870;&#21169;&#26368;&#22823;&#21270;&#30340;&#20064;&#24815;&#34892;&#20026;&#65292;&#21644;&#28789;&#27963;&#32780;&#24930;&#30340;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#12290;&#20256;&#32479;&#19978;&#65292;&#20064;&#24815;&#21644;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#34987;&#35748;&#20026;&#26159;&#30001;&#22823;&#33041;&#20013;&#30340;&#20004;&#20010;&#19981;&#21516;&#31995;&#32479;&#22788;&#29702;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#21464;&#20998;&#36125;&#21494;&#26031;&#29702;&#35770;&#65292;&#23558;&#36825;&#20004;&#31181;&#34892;&#20026;&#32852;&#31995;&#36215;&#26469;&#65292;&#24341;&#20837;&#19968;&#31181;&#36125;&#21494;&#26031;&#28508;&#21464;&#37327;&#31216;&#20026;&#8220;&#24847;&#22270;&#8221;&#12290;&#20064;&#24815;&#24615;&#34892;&#20026;&#26159;&#36890;&#36807;&#20351;&#29992;&#24847;&#22270;&#30340;&#20808;&#39564;&#20998;&#24067;&#29983;&#25104;&#30340;&#65292;&#35813;&#20808;&#39564;&#20998;&#24067;&#26159;&#27809;&#26377;&#30446;&#26631;&#30340;&#65307;&#32780;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#26159;&#30001;&#24847;&#22270;&#30340;&#21518;&#39564;&#20998;&#24067;&#29983;&#25104;&#30340;&#65292;&#35813;&#21518;&#39564;&#20998;&#24067;&#21462;&#20915;&#20110;&#30446;&#26631;&#12290;&#22522;&#20110;&#36825;&#20010;&#24819;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#34892;&#20026;&#24314;&#27169;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
How to behave efficiently and flexibly is a central problem for understanding biological agents and creating intelligent embodied AI. It has been well known that behavior can be classified as two types: reward-maximizing habitual behavior, which is fast while inflexible; and goal-directed behavior, which is flexible while slow. Conventionally, habitual and goal-directed behaviors are considered handled by two distinct systems in the brain. Here, we propose to bridge the gap between the two behaviors, drawing on the principles of variational Bayesian theory. We incorporate both behaviors in one framework by introducing a Bayesian latent variable called "intention". The habitual behavior is generated by using prior distribution of intention, which is goal-less; and the goal-directed behavior is generated by the posterior distribution of intention, which is conditioned on the goal. Building on this idea, we present a novel Bayesian framework for modeling behaviors. Our proposed framework 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8212;&#8212;&#22810;&#36335;&#22797;&#29992;&#20851;&#31995;&#32593;&#32476;&#65288;MUREN&#65289;&#65292;&#36890;&#36807;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#20132;&#25442;&#26469;&#23398;&#20064;&#20840;&#38754;&#30340;&#20851;&#31995;&#19978;&#19979;&#25991;&#65292;&#36798;&#21040;&#20102;HOI&#26816;&#27979;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;</title><link>http://arxiv.org/abs/2304.04997</link><description>&lt;p&gt;
&#20154;-&#29289;&#20132;&#20114;&#26816;&#27979;&#30340;&#20851;&#31995;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Relational Context Learning for Human-Object Interaction Detection. (arXiv:2304.04997v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8212;&#8212;&#22810;&#36335;&#22797;&#29992;&#20851;&#31995;&#32593;&#32476;&#65288;MUREN&#65289;&#65292;&#36890;&#36807;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#20132;&#25442;&#26469;&#23398;&#20064;&#20840;&#38754;&#30340;&#20851;&#31995;&#19978;&#19979;&#25991;&#65292;&#36798;&#21040;&#20102;HOI&#26816;&#27979;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
HOI&#26816;&#27979;&#30340;&#26368;&#26032;&#26041;&#27861;&#36890;&#24120;&#24314;&#31435;&#22312;&#20351;&#29992;&#20004;&#20010;&#35299;&#30721;&#22120;&#20998;&#25903;&#30340;&#36716;&#25442;&#22120;&#20307;&#31995;&#32467;&#26500;&#19978;&#65292;&#19968;&#20010;&#29992;&#20110;&#20154;-&#29289;&#23545;&#26816;&#27979;&#65292;&#21478;&#19968;&#20010;&#29992;&#20110;&#20132;&#20114;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35299;&#32806;&#30340;&#36716;&#25442;&#22120;&#21487;&#33021;&#20250;&#22240;&#20026;&#20998;&#25903;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20132;&#25442;&#19981;&#36275;&#32780;&#23548;&#33268;&#32570;&#20047;&#20851;&#31995;&#25512;&#29702;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#32780;&#22312;&#21457;&#29616;HOI&#23454;&#20363;&#26102;&#36825;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#36335;&#22797;&#29992;&#20851;&#31995;&#32593;&#32476;&#65288;MUREN&#65289;&#65292;&#23427;&#20351;&#29992;&#20154;&#65292;&#29289;&#21644;&#20132;&#20114;&#26631;&#35760;&#30340;&#19968;&#20803;&#65292;&#20108;&#20803;&#21644;&#19977;&#20803;&#20851;&#31995;&#22312;&#19977;&#20010;&#35299;&#30721;&#22120;&#20998;&#25903;&#20043;&#38388;&#36827;&#34892;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#20132;&#25442;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23398;&#20064;&#20840;&#38754;&#30340;&#20851;&#31995;&#19978;&#19979;&#25991;&#65292;&#20197;&#21457;&#29616;HOI&#23454;&#20363;&#65292;&#22312;HOI&#26816;&#27979;&#30340;&#20004;&#20010;&#26631;&#20934;&#22522;&#20934;HICO-DET&#21644;V-COCO&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent state-of-the-art methods for HOI detection typically build on transformer architectures with two decoder branches, one for human-object pair detection and the other for interaction classification. Such disentangled transformers, however, may suffer from insufficient context exchange between the branches and lead to a lack of context information for relational reasoning, which is critical in discovering HOI instances. In this work, we propose the multiplex relation network (MUREN) that performs rich context exchange between three decoder branches using unary, pairwise, and ternary relations of human, object, and interaction tokens. The proposed method learns comprehensive relational contexts for discovering HOI instances, achieving state-of-the-art performance on two standard benchmarks for HOI detection, HICO-DET and V-COCO.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#36827;&#21046;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#23616;&#37096;&#29305;&#24449;&#25551;&#36848;&#32467;&#26500;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#23567;&#22825;&#20307;&#30456;&#23545;&#23548;&#33322;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.04985</link><description>&lt;p&gt;
&#20351;&#29992;&#20108;&#36827;&#21046;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23567;&#22825;&#20307;&#30456;&#23545;&#23548;&#33322;&#30340;&#39640;&#25928;&#29305;&#24449;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
Efficient Feature Description for Small Body Relative Navigation using Binary Convolutional Neural Networks. (arXiv:2304.04985v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#36827;&#21046;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#23616;&#37096;&#29305;&#24449;&#25551;&#36848;&#32467;&#26500;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#23567;&#22825;&#20307;&#30456;&#23545;&#23548;&#33322;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#22825;&#20307;&#20219;&#21153;&#20381;&#36182;&#20110;&#20809;&#23398;&#29305;&#24449;&#36319;&#36394;&#26469;&#34920;&#24449;&#21644;&#32469;&#30446;&#26631;&#22825;&#20307;&#36827;&#34892;&#30456;&#23545;&#23548;&#33322;&#12290;&#34429;&#28982;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29305;&#24449;&#36319;&#36394;&#25216;&#26415;&#26159;&#24403;&#21069;&#20154;&#26426;&#20132;&#20114;&#36807;&#31243;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#20294;&#30001;&#20110;&#33322;&#22825;&#22120;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38480;&#21046;&#65292;&#35774;&#35745;&#33021;&#22815;&#22312;&#26426;&#36733;&#19978;&#36816;&#34892;&#30340;&#28145;&#24230;&#32593;&#32476;&#32467;&#26500;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23616;&#37096;&#29305;&#24449;&#25551;&#36848;&#32467;&#26500;&#65292;&#21033;&#29992;&#20108;&#36827;&#21046;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23618;&#26174;&#30528;&#38477;&#20302;&#20102;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#12290;&#25105;&#20204;&#20351;&#29992;&#20256;&#32479;&#20219;&#21153;&#21644;&#27491;&#22312;&#36827;&#34892;&#30340;&#20219;&#21153;&#30340;&#23567;&#22825;&#20307;&#30495;&#23454;&#22270;&#20687;&#23545;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#24182;&#23637;&#31034;&#20102;&#30456;&#23545;&#20110;&#20256;&#32479;&#25163;&#24037;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#19979;&#19968;&#20195;&#33322;&#22825;&#22120;&#22788;&#29702;&#22120;&#30340;&#20195;&#29702;&#20013;&#23454;&#29616;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#32447;&#29305;&#24449;&#36319;&#36394;&#30340;&#21487;&#34892;&#36816;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Missions to small celestial bodies rely heavily on optical feature tracking for characterization of and relative navigation around the target body. While techniques for feature tracking based on deep learning are a promising alternative to current human-in-the-loop processes, designing deep architectures that can operate onboard spacecraft is challenging due to onboard computational and memory constraints. This paper introduces a novel deep local feature description architecture that leverages binary convolutional neural network layers to significantly reduce computational and memory requirements. We train and test our models on real images of small bodies from legacy and ongoing missions and demonstrate increased performance relative to traditional handcrafted methods. Moreover, we implement our models onboard a surrogate for the next-generation spacecraft processor and demonstrate feasible runtimes for online feature tracking.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#29289;&#22240;&#23376;&#35843;&#25511;&#31070;&#32463;&#32593;&#32476;&#65288;BFReg-NN&#65289;&#65292;&#23427;&#20351;&#29992;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#65292;&#24182;&#23558;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#29983;&#29289;&#23398;&#30693;&#35782;&#65288;&#22914;&#22522;&#22240;&#35843;&#33410;&#32593;&#32476;&#21644;GO&#65289;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#65292;&#20197;&#27169;&#25311;&#29983;&#29289;&#31995;&#32479;&#20013;&#29983;&#29289;&#22240;&#23376;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#22312;&#27169;&#25311;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.04982</link><description>&lt;p&gt;
&#29983;&#29289;&#22240;&#23376;&#35843;&#25511;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Biological Factor Regulatory Neural Network. (arXiv:2304.04982v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#29289;&#22240;&#23376;&#35843;&#25511;&#31070;&#32463;&#32593;&#32476;&#65288;BFReg-NN&#65289;&#65292;&#23427;&#20351;&#29992;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#65292;&#24182;&#23558;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#29983;&#29289;&#23398;&#30693;&#35782;&#65288;&#22914;&#22522;&#22240;&#35843;&#33410;&#32593;&#32476;&#21644;GO&#65289;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#65292;&#20197;&#27169;&#25311;&#29983;&#29289;&#31995;&#32479;&#20013;&#29983;&#29289;&#22240;&#23376;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#22312;&#27169;&#25311;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#22240;&#26159;&#20998;&#26512;&#29983;&#29289;&#31995;&#32479;&#30340;&#22522;&#30784;&#65292;&#35768;&#22810;&#26368;&#36817;&#30340;&#24037;&#20316;&#25552;&#20986;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21033;&#29992;&#22522;&#22240;&#34920;&#36798;&#36827;&#34892;&#21508;&#31181;&#29983;&#29289;&#20219;&#21153;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#26377;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#23427;&#20204;&#30340;&#40657;&#30418;&#26412;&#36136;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24456;&#38590;&#20026;&#20154;&#31867;&#25552;&#20379;&#29983;&#29289;&#23398;&#27934;&#35265;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#24037;&#20316;&#23558;&#29983;&#29289;&#30693;&#35782;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#36879;&#26126;&#24615;&#21644;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21482;&#33021;&#32435;&#20837;&#37096;&#20998;&#29983;&#29289;&#23398;&#30693;&#35782;&#65292;&#23548;&#33268;&#24615;&#33021;&#20122;&#20248;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29983;&#29289;&#22240;&#23376;&#35843;&#25511;&#31070;&#32463;&#32593;&#32476;&#65288;BFReg-NN&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#20197;&#27169;&#25311;&#32454;&#32990;&#31995;&#32479;&#20013;&#29983;&#29289;&#22240;&#23376;&#20043;&#38388;&#20851;&#31995;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;BFReg-NN&#20174;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#24320;&#22987;&#65292;&#33021;&#22815;&#23558;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#29983;&#29289;&#23398;&#30693;&#35782;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#65292;&#21253;&#25324;&#22522;&#22240;&#25110;&#34507;&#30333;&#36136;&#20043;&#38388;&#30340;&#35843;&#33410;&#20851;&#31995;&#65288;&#20363;&#22914;&#22522;&#22240;&#35843;&#33410;&#32593;&#32476;&#65288;GRN&#65289;&#65292;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#32593;&#32476;&#65288;PPI&#65289;&#65289;&#21644;&#29983;&#29289;&#22240;&#23376;&#20043;&#38388;&#30340;&#23618;&#27425;&#20851;&#31995;&#65288;&#20363;&#22914;Gene Ontology&#65288;GO&#65289;&#65292;pathways&#65289;&#12290;BFReg-NN&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#22240;&#23376;&#35843;&#33410;&#24809;&#32602;&#65288;FRP&#65289;&#30340;&#26032;&#30340;&#27491;&#21017;&#21270;&#39033;&#21040;&#25439;&#22833;&#20989;&#25968;&#20013;&#65292;&#20197;&#30830;&#20445;&#23398;&#20064;&#27169;&#22411;&#20013;&#29983;&#29289;&#22240;&#23376;&#20043;&#38388;&#30340;&#35843;&#33410;&#20851;&#31995;&#24471;&#21040;&#34920;&#31034;&#12290;&#22312;&#27169;&#25311;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;BFReg-NN&#30456;&#23545;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#21644;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Genes are fundamental for analyzing biological systems and many recent works proposed to utilize gene expression for various biological tasks by deep learning models. Despite their promising performance, it is hard for deep neural networks to provide biological insights for humans due to their black-box nature. Recently, some works integrated biological knowledge with neural networks to improve the transparency and performance of their models. However, these methods can only incorporate partial biological knowledge, leading to suboptimal performance. In this paper, we propose the Biological Factor Regulatory Neural Network (BFReg-NN), a generic framework to model relations among biological factors in cell systems. BFReg-NN starts from gene expression data and is capable of merging most existing biological knowledge into the model, including the regulatory relations among genes or proteins (e.g., gene regulatory networks (GRN), protein-protein interaction networks (PPI)) and the hierarc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;GRIL&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#25299;&#25169;&#29305;&#24449;&#34920;&#31034;&#25955;&#24230;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#31283;&#23450;&#22320;&#29992;&#20110;&#19981;&#21516;&#30340;&#36807;&#28388;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2304.04970</link><description>&lt;p&gt;
GRIL&#65306;&#19968;&#31181;&#20108;&#21442;&#25968;&#25345;&#20037;&#24615;&#22522;&#20110;&#21521;&#37327;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GRIL: A $2$-parameter Persistence Based Vectorization for Machine Learning. (arXiv:2304.04970v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;GRIL&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#25299;&#25169;&#29305;&#24449;&#34920;&#31034;&#25955;&#24230;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#31283;&#23450;&#22320;&#29992;&#20110;&#19981;&#21516;&#30340;&#36807;&#28388;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#21442;&#25968;&#25345;&#20037;&#24615;&#21516;Topology Data Analysis (TDA)&#30456;&#20851;&#65292;&#21487;&#30740;&#31350;&#25968;&#25454;&#20013;&#38544;&#34255;&#30528;&#30340;&#36830;&#36890;&#20998;&#37327;&#21644;&#24490;&#29615;&#31561;&#25299;&#25169;&#29305;&#24449;&#12290;&#24050;&#24212;&#29992;&#20110;&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#31561;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#20026;&#20102;&#20016;&#23500;&#25299;&#25169;&#29305;&#24449;&#30340;&#34920;&#31034;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#30740;&#31350;&#21452;&#36807;&#28388;&#20989;&#25968;&#35825;&#23548;&#30340;&#20108;&#21442;&#25968;&#25345;&#20037;&#24615;&#27169;&#22359;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#23558;&#36825;&#20123;&#34920;&#31034;&#20449;&#24687;&#21152;&#20837;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#21521;&#37327;&#34920;&#31034;&#31216;&#20026;Generalized Rank Invariant Landscape \textsc{Gril}&#65292;&#24182;&#23558;&#20854;&#35777;&#26126;&#20026;&#22312;Lipschitz&#31283;&#23450;&#26465;&#20214;&#19979;&#21487;&#24494;&#20998;&#65292;&#24182;&#19988;&#36890;&#36807;&#23545;&#22522;&#30784;&#36807;&#28388;&#20989;&#25968;&#30340;&#32534;&#30721;&#21487;&#20197;&#23481;&#26131;&#22320;&#34701;&#20837;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#35745;&#31639;&#21521;&#37327;&#34920;&#31034;&#30340;&#31639;&#27861;&#12290;&#26412;&#30740;&#31350;&#36824;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
$1$-parameter persistent homology, a cornerstone in Topological Data Analysis (TDA), studies the evolution of topological features such as connected components and cycles hidden in data. It has been applied to enhance the representation power of deep learning models, such as Graph Neural Networks (GNNs). To enrich the representations of topological features, here we propose to study $2$-parameter persistence modules induced by bi-filtration functions. In order to incorporate these representations into machine learning models, we introduce a novel vector representation called Generalized Rank Invariant Landscape \textsc{Gril} for $2$-parameter persistence modules. We show that this vector representation is $1$-Lipschitz stable and differentiable with respect to underlying filtration functions and can be easily integrated into machine learning models to augment encoding topological features. We present an algorithm to compute the vector representation efficiently. We also test our method
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21487;&#29992;&#20110;&#25552;&#20379;&#33258;&#36866;&#24212;&#30340;&#25945;&#32946;&#25903;&#25345;&#65292;&#23588;&#20854;&#23545;&#20110;&#26368;&#21021;&#25104;&#32489;&#36739;&#20302;&#30340;&#23398;&#29983;&#20855;&#26377;&#26368;&#22823;&#30340;&#30410;&#22788;&#12290;</title><link>http://arxiv.org/abs/2304.04933</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#36741;&#23548;&#21592;&#22312;&#25968;&#23398;&#20219;&#21153;&#20013;&#26356;&#22909;&#22320;&#25903;&#25345;&#20102;&#20302;&#25104;&#32489;&#23398;&#29983;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning Tutor Better Supported Lower Performers in a Math Task. (arXiv:2304.04933v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21487;&#29992;&#20110;&#25552;&#20379;&#33258;&#36866;&#24212;&#30340;&#25945;&#32946;&#25903;&#25345;&#65292;&#23588;&#20854;&#23545;&#20110;&#26368;&#21021;&#25104;&#32489;&#36739;&#20302;&#30340;&#23398;&#29983;&#20855;&#26377;&#26368;&#22823;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36164;&#28304;&#38480;&#21046;&#20351;&#24471;&#20026;&#25152;&#26377;&#23398;&#29983;&#25552;&#20379;&#20010;&#24615;&#21270;&#25945;&#23398;&#21464;&#24471;&#22256;&#38590;&#12290;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#25104;&#20026;&#20943;&#23569;&#21457;&#23637;&#25104;&#26412;&#12289;&#25552;&#39640;&#26234;&#33021;&#36741;&#23548;&#36719;&#20214;&#25928;&#26524;&#30340;&#20851;&#38190;&#24037;&#20855;&#65292;&#26088;&#22312;&#20026;&#23398;&#29983;&#25552;&#20379;&#27491;&#30830;&#30340;&#25903;&#25345;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22914;&#20309;&#22312;&#21465;&#36848;&#25925;&#20107;&#32447;&#36719;&#20214;&#20013;&#20026;&#23398;&#20064;&#8220;&#23481;&#31215;&#8221;&#27010;&#24565;&#30340;&#23398;&#29983;&#25552;&#20379;&#33258;&#36866;&#24212;&#25945;&#32946;&#25903;&#25345;&#12290;&#36890;&#36807;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#65292;&#25105;&#20204;&#20063;&#25552;&#21462;&#20102;&#26377;&#20851;&#23398;&#20064;&#30340;&#21487;&#35299;&#37322;&#27934;&#35265;&#65292;&#35777;&#26126;&#20102;&#25152;&#24471;&#25919;&#31574;&#22312;&#19981;&#21516;&#30340;&#23398;&#29983;&#32676;&#20307;&#20013;&#20855;&#26377;&#31867;&#20284;&#30340;&#34920;&#29616;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#36825;&#20004;&#39033;&#30740;&#31350;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#25925;&#20107;&#31995;&#32479;&#23545;&#26368;&#21021;&#30340;&#39044;&#27979;&#20998;&#25968;&#26368;&#20302;&#30340;&#23398;&#29983;&#26377;&#26368;&#22823;&#30340;&#30410;&#22788;&#65292;&#36825;&#34920;&#26126;&#20102;AI&#36866;&#24212;&#24182;&#20026;&#20302;&#25104;&#32489;&#23398;&#29983;&#25552;&#20379;&#25903;&#25345;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Resource limitations make it hard to provide all students with one of the most effective educational interventions: personalized instruction. Reinforcement learning could be a key tool to reduce the development cost and improve the effectiveness of, intelligent tutoring software that aims to provide the right support, at the right time, to a student. Here we illustrate that deep reinforcement learning can be used to provide adaptive pedagogical support to students learning about the concept of volume in a narrative storyline software. Using explainable artificial intelligence tools, we also extracted interpretable insights about the pedagogical policy learned, and we demonstrate that the resulting policy had similar performance in a different student population. Most importantly, in both studies the reinforcement-learning narrative system had the largest benefit for those students with the lowest initial pretest scores, suggesting the opportunity for AI to adapt and provide support for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;sRank&#30340;&#36890;&#29992;&#35821;&#20041;&#23398;&#20064;&#25490;&#21517;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;transformer&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#26234;&#33021;&#22238;&#22797;&#21644;&#29615;&#22659;&#20020;&#24202;&#26234;&#33021;&#31561;&#30495;&#23454;&#24212;&#29992;&#20013;&#65292;&#23454;&#29616;11.7%&#30340;&#31163;&#32447;&#20934;&#30830;&#24230;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2304.04918</link><description>&lt;p&gt;
&#26174;&#24335;&#21644;&#38544;&#24335;&#35821;&#20041;&#25490;&#24207;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Explicit and Implicit Semantic Ranking Framework. (arXiv:2304.04918v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;sRank&#30340;&#36890;&#29992;&#35821;&#20041;&#23398;&#20064;&#25490;&#21517;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;transformer&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#26234;&#33021;&#22238;&#22797;&#21644;&#29615;&#22659;&#20020;&#24202;&#26234;&#33021;&#31561;&#30495;&#23454;&#24212;&#29992;&#20013;&#65292;&#23454;&#29616;11.7%&#30340;&#31163;&#32447;&#20934;&#30830;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#26680;&#24515;&#38590;&#39064;&#26159;&#23558;&#19968;&#20010;&#26597;&#35810;&#19982;&#19968;&#20010;&#21487;&#21464;&#19988;&#26377;&#38480;&#30340;&#25991;&#26723;&#38598;&#20013;&#30340;&#26368;&#20339;&#25991;&#26723;&#36827;&#34892;&#21305;&#37197;&#12290;&#29616;&#26377;&#30340;&#24037;&#19994;&#35299;&#20915;&#26041;&#26696;&#65292;&#29305;&#21035;&#26159;&#24310;&#36831;&#21463;&#38480;&#30340;&#26381;&#21153;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#30456;&#20284;&#24615;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#20026;&#20102;&#36895;&#24230;&#32780;&#29306;&#29298;&#20102;&#36136;&#37327;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#35821;&#20041;&#23398;&#20064;&#25490;&#21517;&#26694;&#26550;&#65292;&#33258;&#25105;&#35757;&#32451;&#35821;&#20041;&#20132;&#21449;&#20851;&#27880;&#25490;&#21517;&#65288;sRank&#65289;&#12290;&#36825;&#20010;&#22522;&#20110;transformer&#30340;&#26694;&#26550;&#20351;&#29992;&#32447;&#24615;&#25104;&#23545;&#25439;&#22833;&#65292;&#20855;&#26377;&#21487;&#21464;&#30340;&#35757;&#32451;&#25209;&#37327;&#22823;&#23567;&#12289;&#23454;&#29616;&#36136;&#37327;&#25552;&#21319;&#21644;&#39640;&#25928;&#29575;&#65292;&#24182;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#24494;&#36719;&#20844;&#21496;&#30340;&#20004;&#20010;&#24037;&#19994;&#20219;&#21153;&#65306;&#26234;&#33021;&#22238;&#22797;&#65288;SR&#65289;&#21644;&#29615;&#22659;&#20020;&#24202;&#26234;&#33021;&#65288;ACI&#65289;&#30340;&#30495;&#23454;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#12290;&#22312;&#26234;&#33021;&#22238;&#22797;&#20013;&#65292;$sRank$&#36890;&#36807;&#22522;&#20110;&#28040;&#36153;&#32773;&#21644;&#25903;&#25345;&#20195;&#29702;&#20449;&#24687;&#30340;&#39044;&#23450;&#20041;&#35299;&#20915;&#26041;&#26696;&#36873;&#25321;&#26368;&#20339;&#31572;&#26696;&#65292;&#24110;&#21161;&#29992;&#25143;&#23454;&#26102;&#33719;&#24471;&#25216;&#26415;&#25903;&#25345;&#12290;&#22312;SR&#20219;&#21153;&#19978;&#65292;$sRank$&#23454;&#29616;&#20102;11.7%&#30340;&#31163;&#32447;top-one&#20934;&#30830;&#24230;&#25552;&#21319;&#65292;&#27604;&#20043;&#21069;&#30340;&#31995;&#32479;&#26356;&#21152;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;
The core challenge in numerous real-world applications is to match an inquiry to the best document from a mutable and finite set of candidates. Existing industry solutions, especially latency-constrained services, often rely on similarity algorithms that sacrifice quality for speed. In this paper we introduce a generic semantic learning-to-rank framework, Self-training Semantic Cross-attention Ranking (sRank). This transformer-based framework uses linear pairwise loss with mutable training batch sizes and achieves quality gains and high efficiency, and has been applied effectively to show gains on two industry tasks at Microsoft over real-world large-scale data sets: Smart Reply (SR) and Ambient Clinical Intelligence (ACI). In Smart Reply, $sRank$ assists live customers with technical support by selecting the best reply from predefined solutions based on consumer and support agent messages. It achieves 11.7% gain in offline top-one accuracy on the SR task over the previous system, and 
&lt;/p&gt;</description></item><item><title>AffectMachine-Classical&#26159;&#19968;&#31181;&#21487;&#20197;&#23454;&#26102;&#29983;&#25104;&#24773;&#24863;&#21476;&#20856;&#38899;&#20048;&#30340;&#38899;&#20048;&#29983;&#25104;&#31995;&#32479;&#65292;&#26088;&#22312;&#23558;&#20854;&#23884;&#20837;&#21040;&#29983;&#29289;&#21453;&#39304;&#31995;&#32479;&#20013;&#24110;&#21161;&#29992;&#25143;&#33258;&#25105;&#35843;&#33410;&#24773;&#32490;&#12290;</title><link>http://arxiv.org/abs/2304.04915</link><description>&lt;p&gt;
AffectMachine-Classical: &#19968;&#31181;&#29983;&#25104;&#24773;&#24863;&#21476;&#20856;&#38899;&#20048;&#30340;&#26032;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
AffectMachine-Classical: A novel system for generating affective classical music. (arXiv:2304.04915v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04915
&lt;/p&gt;
&lt;p&gt;
AffectMachine-Classical&#26159;&#19968;&#31181;&#21487;&#20197;&#23454;&#26102;&#29983;&#25104;&#24773;&#24863;&#21476;&#20856;&#38899;&#20048;&#30340;&#38899;&#20048;&#29983;&#25104;&#31995;&#32479;&#65292;&#26088;&#22312;&#23558;&#20854;&#23884;&#20837;&#21040;&#29983;&#29289;&#21453;&#39304;&#31995;&#32479;&#20013;&#24110;&#21161;&#29992;&#25143;&#33258;&#25105;&#35843;&#33410;&#24773;&#32490;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AffectMachine-Classical&#30340;&#26032;&#38899;&#20048;&#29983;&#25104;&#31995;&#32479;&#65292;&#33021;&#22815;&#23454;&#26102;&#29983;&#25104;&#24773;&#24863;&#21476;&#20856;&#38899;&#20048;&#12290;AffectMachine&#34987;&#35774;&#35745;&#29992;&#20110;&#29983;&#29289;&#21453;&#39304;&#31995;&#32479;&#65288;&#22914;&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#65289;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#24847;&#35782;&#21040;&#21644;&#35843;&#33410;&#33258;&#24049;&#30340;&#21160;&#24577;&#24773;&#24863;&#29366;&#24577;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#35813;&#31995;&#32479;&#26159;&#20026;&#22522;&#20110;&#38899;&#20048;&#30340;&#21307;&#30103;&#25216;&#26415;&#24320;&#21457;&#30340;&#65292;&#25903;&#25345;&#29992;&#25143;&#23454;&#26102;&#24773;&#32490;&#33258;&#25105;&#35843;&#33410;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22522;&#20110;&#35268;&#21017;&#21644;&#27010;&#29575;&#30340;&#31995;&#32479;&#26550;&#26500;&#27010;&#36848;&#65292;&#25551;&#36848;&#20102;&#31995;&#32479;&#30340;&#20027;&#35201;&#26041;&#38754;&#21450;&#20854;&#21019;&#26032;&#20043;&#22788;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#39033;&#21548;&#20247;&#30740;&#31350;&#30340;&#32467;&#26524;&#65292;&#26088;&#22312;&#39564;&#35777;&#35813;&#31995;&#32479;&#33021;&#22815;&#21487;&#38752;&#22320;&#20256;&#36798;&#30446;&#26631;&#24773;&#24863;&#32473;&#21548;&#20247;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;AffectMachine-Classical&#22312;&#20256;&#36798;&#21508;&#31181;Arousal&#27700;&#24179;&#65288;$R^2 = .96$&#65289;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#24182;&#19988;&#22312;Valence&#26041;&#38754;&#20063;&#38750;&#24120;&#20196;&#20154;&#20449;&#26381;&#65288;$R^2 = .90$&#65289;&#12290;&#23558;&#26469;&#30340;&#24037;&#20316;&#23558;&#25226; AffectMachine-Classical&#23884;&#20837;&#21040;&#29983;&#29289;&#21453;&#39304;&#24212;&#29992;&#20013;&#65292;&#20197;&#27979;&#35797;&#20854;&#22312;&#23454;&#26102;&#20419;&#36827;&#24773;&#32490;&#35843;&#33410;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces a new music generation system, called AffectMachine-Classical, that is capable of generating affective Classic music in real-time. AffectMachine was designed to be incorporated into biofeedback systems (such as brain-computer-interfaces) to help users become aware of, and ultimately mediate, their own dynamic affective states. That is, this system was developed for music-based MedTech to support real-time emotion self-regulation in users. We provide an overview of the rule-based, probabilistic system architecture, describing the main aspects of the system and how they are novel. We then present the results of a listener study that was conducted to validate the ability of the system to reliably convey target emotions to listeners. The findings indicate that AffectMachine-Classical is very effective in communicating various levels of Arousal ($R^2 = .96$) to listeners, and is also quite convincing in terms of Valence (R^2 = .90). Future work will embed AffectMachine-
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#30417;&#31649;&#24066;&#22330;&#30340;&#27010;&#24565;&#65292;&#21363;&#25919;&#24220;&#35201;&#27714;&#21463;&#30417;&#31649;&#23545;&#35937;&#20174;&#31169;&#20154;&#30417;&#31649;&#26426;&#26500;&#36141;&#20080;&#30417;&#31649;&#26381;&#21153;&#65292;&#20197;&#20811;&#26381;&#36807;&#24230;&#20381;&#36182;&#34892;&#19994;&#33258;&#24459;&#21644;&#31435;&#27861;&#26426;&#26500;&#32570;&#20047;&#19987;&#19994;&#30693;&#35782;&#30340;&#23616;&#38480;&#24615;&#65292;&#20174;&#32780;&#36880;&#27493;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#30340;&#24688;&#24403;&#30417;&#31649;&#12290;</title><link>http://arxiv.org/abs/2304.04914</link><description>&lt;p&gt;
&#30417;&#31649;&#24066;&#22330;&#65306;&#20154;&#24037;&#26234;&#33021;&#27835;&#29702;&#30340;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
Regulatory Markets: The Future of AI Governance. (arXiv:2304.04914v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04914
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#30417;&#31649;&#24066;&#22330;&#30340;&#27010;&#24565;&#65292;&#21363;&#25919;&#24220;&#35201;&#27714;&#21463;&#30417;&#31649;&#23545;&#35937;&#20174;&#31169;&#20154;&#30417;&#31649;&#26426;&#26500;&#36141;&#20080;&#30417;&#31649;&#26381;&#21153;&#65292;&#20197;&#20811;&#26381;&#36807;&#24230;&#20381;&#36182;&#34892;&#19994;&#33258;&#24459;&#21644;&#31435;&#27861;&#26426;&#26500;&#32570;&#20047;&#19987;&#19994;&#30693;&#35782;&#30340;&#23616;&#38480;&#24615;&#65292;&#20174;&#32780;&#36880;&#27493;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#30340;&#24688;&#24403;&#30417;&#31649;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24688;&#24403;&#22320;&#30417;&#31649;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#20010;&#26085;&#30410;&#32039;&#36843;&#30340;&#25919;&#31574;&#25361;&#25112;&#12290;&#31435;&#27861;&#26426;&#26500;&#21644;&#30417;&#31649;&#26426;&#26500;&#32570;&#20047;&#32763;&#35793;&#20844;&#20247;&#38656;&#27714;&#20026;&#27861;&#24459;&#35201;&#27714;&#25152;&#38656;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#36807;&#24230;&#20381;&#36182;&#34892;&#19994;&#33258;&#24459;&#26410;&#33021;&#20351;AI&#31995;&#32479;&#30340;&#29983;&#20135;&#32773;&#21644;&#20351;&#29992;&#32773;&#23545;&#27665;&#20027;&#35201;&#27714;&#36127;&#36131;&#12290;&#25552;&#20986;&#20102;&#30417;&#31649;&#24066;&#22330;&#30340;&#27010;&#24565;&#65292;&#21363;&#25919;&#24220;&#35201;&#27714;&#21463;&#30417;&#31649;&#23545;&#35937;&#20174;&#31169;&#20154;&#30417;&#31649;&#26426;&#26500;&#36141;&#20080;&#30417;&#31649;&#26381;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20811;&#26381;&#21629;&#20196;&#21644;&#25511;&#21046;&#30417;&#31649;&#21644;&#33258;&#25105;&#30417;&#31649;&#30340;&#23616;&#38480;&#24615;&#12290;&#30417;&#31649;&#24066;&#22330;&#21487;&#20197;&#20351;&#25919;&#24220;&#20026;AI&#30417;&#31649;&#24314;&#31435;&#25919;&#31574;&#20248;&#20808;&#32423;&#65292;&#21516;&#26102;&#20381;&#38752;&#24066;&#22330;&#21147;&#37327;&#21644;&#34892;&#19994;&#30740;&#21457;&#21162;&#21147;&#26469;&#24320;&#21019;&#26368;&#33021;&#23454;&#29616;&#25919;&#31574;&#21046;&#23450;&#32773;&#22768;&#26126;&#30446;&#26631;&#30340;&#30417;&#31649;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Appropriately regulating artificial intelligence is an increasingly urgent policy challenge. Legislatures and regulators lack the specialized knowledge required to best translate public demands into legal requirements. Overreliance on industry self-regulation fails to hold producers and users of AI systems accountable to democratic demands. Regulatory markets, in which governments require the targets of regulation to purchase regulatory services from a private regulator, are proposed. This approach to AI regulation could overcome the limitations of both command-and-control regulation and self-regulation. Regulatory market could enable governments to establish policy priorities for the regulation of AI, whilst relying on market forces and industry R&amp;D efforts to pioneer the methods of regulation that best achieve policymakers' stated objectives.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32467;&#21512;&#20351;&#29992;CNN&#21644;Transformer&#26469;&#39044;&#27979;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#30340;&#28072;&#36300;&#24133;&#12290;</title><link>http://arxiv.org/abs/2304.04912</link><description>&lt;p&gt;
&#20351;&#29992;CNN&#21644;Transformer&#36827;&#34892;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Financial Time Series Forecasting using CNN and Transformer. (arXiv:2304.04912v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32467;&#21512;&#20351;&#29992;CNN&#21644;Transformer&#26469;&#39044;&#27979;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#30340;&#28072;&#36300;&#24133;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#20915;&#31574;&#20013;&#37117;&#26159;&#37325;&#35201;&#30340;&#12290;&#20854;&#20013;&#65292;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#65288;&#22914;&#32929;&#31080;&#20215;&#26684;&#65289;&#30340;&#39044;&#27979;&#24448;&#24448;&#27604;&#36739;&#22256;&#38590;&#65292;&#22240;&#20026;&#24456;&#38590;&#23545;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#26102;&#38388;&#20381;&#36182;&#24615;&#24314;&#27169;&#12290;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#23545;&#20110;&#25429;&#25417;&#30701;&#26399;&#20381;&#36182;&#20851;&#31995;&#30340;&#23616;&#37096;&#27169;&#24335;&#24456;&#25797;&#38271;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26377;&#38480;&#30340;&#24863;&#21463;&#37326;&#65292;CNN&#19981;&#33021;&#23398;&#20064;&#38271;&#26399;&#20381;&#36182;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;Transformer&#21487;&#20197;&#23398;&#20064;&#20840;&#23616;&#19978;&#19979;&#25991;&#21644;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;CNN&#21644;Transformer&#30340;&#20248;&#21183;&#26469;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#39044;&#27979;&#26410;&#26469;&#20215;&#26684;&#30340;&#28072;&#36300;&#24133;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#24120;&#29992;&#30340;&#32479;&#35745;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#39044;&#27979;&#26631;&#26222;500&#25104;&#20998;&#32929;&#30424;&#20013;&#30424;&#32929;&#20215;&#26684;&#21464;&#21270;&#26041;&#38754;&#30340;&#25104;&#21151;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series forecasting is important across various domains for decision-making. In particular, financial time series such as stock prices can be hard to predict as it is difficult to model short-term and long-term temporal dependencies between data points. Convolutional Neural Networks (CNN) are good at capturing local patterns for modeling short-term dependencies. However, CNNs cannot learn long-term dependencies due to the limited receptive field. Transformers on the other hand are capable of learning global context and long-term dependencies. In this paper, we propose to harness the power of CNNs and Transformers to model both short-term and long-term dependencies within a time series, and forecast if the price would go up, down or remain the same (flat) in the future. In our experiments, we demonstrated the success of the proposed method in comparison to commonly adopted statistical and deep learning methods on forecasting intraday stock price change of S&amp;P 500 constituents.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#20195;&#29702;&#20219;&#21153;&#29992;&#20110;&#22312;&#20195;&#29702;&#30340;&#39046;&#22495;&#20869;&#39044;&#35757;&#32451;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#29983;&#25104;&#26410;&#26469;&#35270;&#22270;&#22270;&#20687;&#35821;&#20041;&#65292;&#20174;&#32780;&#22312;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.04907</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#26410;&#26469;&#35270;&#22270;&#22270;&#20687;&#35821;&#20041;&#20197;&#25913;&#21892;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Improving Vision-and-Language Navigation by Generating Future-View Image Semantics. (arXiv:2304.04907v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#20195;&#29702;&#20219;&#21153;&#29992;&#20110;&#22312;&#20195;&#29702;&#30340;&#39046;&#22495;&#20869;&#39044;&#35757;&#32451;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#29983;&#25104;&#26410;&#26469;&#35270;&#22270;&#22270;&#20687;&#35821;&#20041;&#65292;&#20174;&#32780;&#22312;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#26159;&#19968;&#39033;&#20219;&#21153;&#65292;&#35201;&#27714;&#20195;&#29702;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#22312;&#29615;&#22659;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;&#22312;&#27599;&#20010;&#27493;&#39588;&#20013;&#65292;&#20195;&#29702;&#36890;&#36807;&#20174;&#21487;&#23548;&#33322;&#20301;&#32622;&#38598;&#21512;&#20013;&#36827;&#34892;&#36873;&#25321;&#26469;&#36873;&#25321;&#19979;&#19968;&#27493;&#21160;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36827;&#19968;&#27493;&#25506;&#32034;&#20195;&#29702;&#26159;&#21542;&#21487;&#20197;&#21463;&#30410;&#20110;&#22312;&#23548;&#33322;&#26399;&#38388;&#29983;&#25104;&#28508;&#22312;&#26410;&#26469;&#35270;&#22270;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#20154;&#31867;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21644;&#21608;&#22260;&#30340;&#35270;&#22270;&#20250;&#23545;&#26410;&#26469;&#30340;&#29615;&#22659;&#26377;&#19968;&#20010;&#39044;&#26399;&#65292;&#24182;&#24110;&#21161;&#27491;&#30830;&#22320;&#23548;&#33322;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#32473;&#20195;&#29702;&#35013;&#22791;&#36825;&#31181;&#29983;&#25104;&#26410;&#26469;&#23548;&#33322;&#35270;&#22270;&#35821;&#20041;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#20195;&#29702;&#30340;&#39046;&#22495;&#20869;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#20986;&#20102;&#19977;&#31181;&#20195;&#29702;&#20219;&#21153;: &#25513;&#34109;&#20840;&#26223;&#24314;&#27169; (MPM)&#65292;&#25513;&#34109;&#36712;&#36857;&#24314;&#27169; (MTM) &#21644;&#24102;&#26377;&#22270;&#20687;&#29983;&#25104;&#30340;&#21160;&#20316;&#39044;&#27979; (APIG)&#12290;&#36825;&#19977;&#20010;&#30446;&#26631;&#25945;&#20250;&#20102;&#27169;&#22411;&#39044;&#27979;&#20840;&#26223;&#20013;&#30340;&#32570;&#23569;&#35270;&#22270; (MPM)&#12289;&#39044;&#27979;&#23436;&#25972;&#36712;&#36857;&#20013;&#30340;&#32570;&#23569;&#27493;&#39588; (MTM) &#21644;&#36827;&#34892;&#21160;&#20316;&#39044;&#27979;&#21644;&#22270;&#20687;&#29983;&#25104; (APIG)&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-and-Language Navigation (VLN) is the task that requires an agent to navigate through the environment based on natural language instructions. At each step, the agent takes the next action by selecting from a set of navigable locations. In this paper, we aim to take one step further and explore whether the agent can benefit from generating the potential future view during navigation. Intuitively, humans will have an expectation of how the future environment will look like, based on the natural language instructions and surrounding views, which will aid correct navigation. Hence, to equip the agent with this ability to generate the semantics of future navigation views, we first propose three proxy tasks during the agent's in-domain pre-training: Masked Panorama Modeling (MPM), Masked Trajectory Modeling (MTM), and Action Prediction with Image Generation (APIG). These three objectives teach the model to predict missing views in a panorama (MPM), predict missing steps in the full tra
&lt;/p&gt;</description></item><item><title>EVKG&#26159;&#19968;&#20010;&#32508;&#21512;&#12289;&#36328;&#39046;&#22495;&#12289;&#21487;&#25193;&#23637;&#12289;&#24320;&#25918;&#30340;&#22320;&#29702;&#31354;&#38388;&#30693;&#35782;&#31649;&#29702;&#31995;&#32479;&#65292;&#28085;&#30422;&#20102;&#19982;&#30005;&#21160;&#27773;&#36710;&#30456;&#20851;&#30340;&#22522;&#26412;&#30693;&#35782;&#12290;&#38598;&#25104;&#20854;&#20182;&#30693;&#21517;&#30693;&#35782;&#22270;&#35889;&#21644;&#26412;&#20307;&#27169;&#22359;&#65292;&#22686;&#24378;&#20102;EVKG&#22312;&#30005;&#21160;&#27773;&#36710;&#20915;&#31574;&#20013;&#30340;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2304.04893</link><description>&lt;p&gt;
EVKG&#65306;&#19968;&#20010;&#20114;&#32852;&#20114;&#36890;&#30340;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30005;&#21160;&#27773;&#36710;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
EVKG: An Interlinked and Interoperable Electric Vehicle Knowledge Graph for Smart Transportation System. (arXiv:2304.04893v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04893
&lt;/p&gt;
&lt;p&gt;
EVKG&#26159;&#19968;&#20010;&#32508;&#21512;&#12289;&#36328;&#39046;&#22495;&#12289;&#21487;&#25193;&#23637;&#12289;&#24320;&#25918;&#30340;&#22320;&#29702;&#31354;&#38388;&#30693;&#35782;&#31649;&#29702;&#31995;&#32479;&#65292;&#28085;&#30422;&#20102;&#19982;&#30005;&#21160;&#27773;&#36710;&#30456;&#20851;&#30340;&#22522;&#26412;&#30693;&#35782;&#12290;&#38598;&#25104;&#20854;&#20182;&#30693;&#21517;&#30693;&#35782;&#22270;&#35889;&#21644;&#26412;&#20307;&#27169;&#22359;&#65292;&#22686;&#24378;&#20102;EVKG&#22312;&#30005;&#21160;&#27773;&#36710;&#20915;&#31574;&#20013;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#37324;&#65292;&#30005;&#21160;&#27773;&#36710;&#34892;&#19994;&#32463;&#21382;&#20102;&#31354;&#21069;&#30340;&#22686;&#38271;&#21644;&#22810;&#26679;&#21270;&#65292;&#24418;&#25104;&#20102;&#22797;&#26434;&#30340;&#29983;&#24577;&#31995;&#32479;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#31649;&#29702;&#36825;&#20010;&#22810;&#26041;&#38754;&#30340;&#39046;&#22495;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#30005;&#21160;&#27773;&#36710;&#20026;&#20013;&#24515;&#30340;&#30693;&#35782;&#22270;&#35889;(EVKG)&#65292;&#20316;&#20026;&#19968;&#20010;&#32508;&#21512;&#30340;&#12289;&#36328;&#39046;&#22495;&#30340;&#12289;&#21487;&#25193;&#23637;&#30340;&#21644;&#24320;&#25918;&#30340;&#22320;&#29702;&#31354;&#38388;&#30693;&#35782;&#31649;&#29702;&#31995;&#32479;&#12290;EVKG&#28085;&#30422;&#20102;&#19982;&#30005;&#21160;&#27773;&#36710;&#30456;&#20851;&#30340;&#22522;&#26412;&#30693;&#35782;&#65292;&#21253;&#25324;&#30005;&#21160;&#27773;&#36710;&#30340;&#37319;&#29992;&#12289;&#30005;&#21160;&#27773;&#36710;&#20379;&#24212;&#35774;&#22791;&#21644;&#30005;&#21147;&#20256;&#36755;&#32593;&#32476;&#65292;&#36890;&#36807;&#25552;&#20379;&#21450;&#26102;&#20934;&#30830;&#30340;&#20449;&#24687;&#21644;&#20998;&#26512;&#26469;&#25903;&#25345;&#19982;&#30005;&#21160;&#27773;&#36710;&#25216;&#26415;&#24320;&#21457;&#12289;&#22522;&#30784;&#35774;&#26045;&#35268;&#21010;&#21644;&#25919;&#31574;&#21046;&#23450;&#30456;&#20851;&#30340;&#20915;&#31574;&#12290;&#20026;&#20102;&#20016;&#23500;&#21644;&#19978;&#19979;&#25991;&#21270;EVKG&#65292;&#25105;&#20204;&#23558;&#29616;&#26377;&#30340;&#30693;&#21517;&#30693;&#35782;&#22270;&#35889;&#21644;&#26412;&#20307;&#30340;&#24320;&#21457;&#30456;&#20851;&#30340;&#26412;&#20307;&#27169;&#22359;&#38598;&#25104;&#21040;EVKG&#20013;&#12290;&#36825;&#31181;&#38598;&#25104;&#20351;EVKG&#19982;Linked Data Open Cloud&#20013;&#30340;&#20854;&#20182;&#30693;&#35782;&#22270;&#35889;&#20855;&#26377;&#20114;&#25805;&#20316;&#24615;&#65292;&#22686;&#24378;&#20102;EVKG&#20316;&#20026;&#30005;&#21160;&#27773;&#36710;&#20915;&#31574;&#30340;&#30693;&#35782;&#20013;&#24515;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past decade, the electric vehicle industry has experienced unprecedented growth and diversification, resulting in a complex ecosystem. To effectively manage this multifaceted field, we present an EV-centric knowledge graph (EVKG) as a comprehensive, cross-domain, extensible, and open geospatial knowledge management system. The EVKG encapsulates essential EV-related knowledge, including EV adoption, electric vehicle supply equipment, and electricity transmission network, to support decision-making related to EV technology development, infrastructure planning, and policy-making by providing timely and accurate information and analysis. To enrich and contextualize the EVKG, we integrate the developed EV-relevant ontology modules from existing well-known knowledge graphs and ontologies. This integration enables interoperability with other knowledge graphs in the Linked Data Open Cloud, enhancing the EVKG's value as a knowledge hub for EV decision-making. Using six competency quest
&lt;/p&gt;</description></item><item><title>DISTO&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#22810;&#39033;&#36873;&#25321;&#39064;&#20013;&#29983;&#25104;&#30340;&#24178;&#25200;&#36873;&#39033;&#12290;DISTO&#19982;&#20154;&#31867;&#23545;&#24178;&#25200;&#36873;&#39033;&#30340;&#35780;&#20998;&#39640;&#24230;&#30456;&#20851;&#65292;&#19988;&#25490;&#21517;&#26368;&#20808;&#36827;&#30340;&#24178;&#25200;&#36873;&#39033;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#26426;&#22120;&#32763;&#35793;&#30340;&#24230;&#37327;&#26631;&#20934;&#38750;&#24120;&#19981;&#21516;&#65292;&#34920;&#26126;&#26426;&#22120;&#32763;&#35793;&#24230;&#37327;&#26631;&#20934;&#19981;&#36866;&#29992;&#20110;&#24178;&#25200;&#36873;&#39033;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2304.04881</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#36127;&#37319;&#26679;&#30340;&#26041;&#27861;&#35780;&#20272;&#22810;&#39033;&#36873;&#25321;&#39064;&#20013;&#30340;&#24178;&#25200;&#36873;&#39033;
&lt;/p&gt;
&lt;p&gt;
DISTO: Evaluating Textual Distractors for Multi-Choice Questions using Negative Sampling based Approach. (arXiv:2304.04881v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04881
&lt;/p&gt;
&lt;p&gt;
DISTO&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#22810;&#39033;&#36873;&#25321;&#39064;&#20013;&#29983;&#25104;&#30340;&#24178;&#25200;&#36873;&#39033;&#12290;DISTO&#19982;&#20154;&#31867;&#23545;&#24178;&#25200;&#36873;&#39033;&#30340;&#35780;&#20998;&#39640;&#24230;&#30456;&#20851;&#65292;&#19988;&#25490;&#21517;&#26368;&#20808;&#36827;&#30340;&#24178;&#25200;&#36873;&#39033;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#26426;&#22120;&#32763;&#35793;&#30340;&#24230;&#37327;&#26631;&#20934;&#38750;&#24120;&#19981;&#21516;&#65292;&#34920;&#26126;&#26426;&#22120;&#32763;&#35793;&#24230;&#37327;&#26631;&#20934;&#19981;&#36866;&#29992;&#20110;&#24178;&#25200;&#36873;&#39033;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#36873;&#25321;&#39064;&#26159;&#19968;&#31181;&#35780;&#20272;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#30340;&#39640;&#25928;&#24120;&#35265;&#26041;&#27861;&#12290;&#27599;&#36947;&#22810;&#39033;&#36873;&#25321;&#39064;&#38656;&#35201;&#19968;&#32452;&#24178;&#25200;&#36873;&#39033;&#65292;&#36825;&#20123;&#36873;&#39033;&#34429;&#28982;&#19981;&#27491;&#30830;&#65292;&#20294;&#35201;&#36275;&#22815;&#21512;&#29702;&#20197;&#32771;&#26597;&#23398;&#29983;&#30340;&#30693;&#35782;&#25484;&#25569;&#24773;&#20917;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#24178;&#25200;&#36873;&#39033;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#24615;&#33021;&#36890;&#24120;&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#32763;&#35793;&#24230;&#37327;&#26631;&#20934;&#32463;&#24120;&#35823;&#21028;&#29983;&#25104;&#30340;&#24178;&#25200;&#36873;&#39033;&#30340;&#21512;&#36866;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DISTO&#65306;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#24178;&#25200;&#36873;&#39033;&#30340;&#31532;&#19968;&#20010;&#23398;&#20064;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;DISTO&#24471;&#20998;&#19982;&#20154;&#31867;&#23545;&#24178;&#25200;&#36873;&#39033;&#36136;&#37327;&#30340;&#35780;&#20998;&#39640;&#24230;&#30456;&#20851;&#26469;&#39564;&#35777;DISTO&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;DISTO&#25490;&#21517;&#26368;&#20808;&#36827;&#30340;&#24178;&#25200;&#36873;&#39033;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#26426;&#22120;&#32763;&#35793;&#30340;&#24230;&#37327;&#26631;&#20934;&#38750;&#24120;&#19981;&#21516;&#65292;&#34920;&#26126;&#26426;&#22120;&#32763;&#35793;&#24230;&#37327;&#26631;&#20934;&#19981;&#24212;&#29992;&#20110;&#24178;&#25200;&#36873;&#39033;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiple choice questions (MCQs) are an efficient and common way to assess reading comprehension (RC). Every MCQ needs a set of distractor answers that are incorrect, but plausible enough to test student knowledge. Distractor generation (DG) models have been proposed, and their performance is typically evaluated using machine translation (MT) metrics. However, MT metrics often misjudge the suitability of generated distractors. We propose DISTO: the first learned evaluation metric for generated distractors. We validate DISTO by showing its scores correlate highly with human ratings of distractor quality. At the same time, DISTO ranks the performance of state-of-the-art DG models very differently from MT-based metrics, showing that MT metrics should not be used for distractor evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#22120; ImageCaptioner$^2$ &#65292;&#29992;&#20110;&#38024;&#23545;&#22270;&#20687;&#23383;&#24149;&#20559;&#24046;&#25918;&#22823;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2304.04874</link><description>&lt;p&gt;
ImageCaptioner$^2$: &#38024;&#23545;&#22270;&#20687;&#23383;&#24149;&#20559;&#24046;&#25918;&#22823;&#35780;&#20272;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
ImageCaptioner$^2$: Image Captioner for Image Captioning Bias Amplification Assessment. (arXiv:2304.04874v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#22120; ImageCaptioner$^2$ &#65292;&#29992;&#20110;&#38024;&#23545;&#22270;&#20687;&#23383;&#24149;&#20559;&#24046;&#25918;&#22823;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#39044;&#35757;&#32451;&#23398;&#20064;&#31995;&#32479;&#37117;&#20250;&#21463;&#21040;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#36825;&#36890;&#24120;&#26469;&#33258;&#25968;&#25454;&#12289;&#27169;&#22411;&#25110;&#20004;&#32773;&#12290;&#34913;&#37327;&#21644;&#37327;&#21270;&#20559;&#24046;&#21450;&#20854;&#26469;&#28304;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#24182;&#22312;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#38754;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#29616;&#26377;&#35780;&#20272;&#25351;&#26631;&#22312;&#21253;&#25324;&#35270;&#35273;&#20449;&#21495;&#26041;&#38754;&#23384;&#22312;&#19968;&#23450;&#19981;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38024;&#23545;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#30340;&#20559;&#24046;&#35780;&#20272;&#25351;&#26631;&#65292;&#31216;&#20026; ImageCaptioner$^2$&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#20165;&#22522;&#20110;&#29983;&#25104;&#30340;&#23383;&#24149;&#35780;&#20272;&#22270;&#20687;&#23383;&#24149;&#31639;&#27861;&#19981;&#21516;&#65292;ImageCaptioner$^2$&#22312;&#27979;&#37327;&#20559;&#24046;&#26102;&#32771;&#34385;&#22270;&#20687;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#20844;&#24335;&#26469;&#20316;&#20026;&#22522;&#20110;&#25552;&#31034;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26469;&#27979;&#37327;&#29983;&#25104;&#23383;&#24149;&#30340;&#20559;&#24046;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most pre-trained learning systems are known to suffer from bias, which typically emerges from the data, the model, or both. Measuring and quantifying bias and its sources is a challenging task and has been extensively studied in image captioning. Despite the significant effort in this direction, we observed that existing metrics lack consistency in the inclusion of the visual signal. In this paper, we introduce a new bias assessment metric, dubbed $ImageCaptioner^2$, for image captioning. Instead of measuring the absolute bias in the model or the data, $ImageCaptioner^2$ pay more attention to the bias introduced by the model w.r.t the data bias, termed bias amplification. Unlike the existing methods, which only evaluate the image captioning algorithms based on the generated captions only, $ImageCaptioner^2$ incorporates the image while measuring the bias. In addition, we design a formulation for measuring the bias of generated captions as prompt-based image captioning instead of using 
&lt;/p&gt;</description></item><item><title>ShapeShift&#26159;&#19968;&#20010;&#26426;&#22120;&#20154;&#25235;&#21462;&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#30340;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#36229;&#26925;&#29699;&#30340;&#21442;&#32771;&#24418;&#29366;&#39044;&#27979;&#29289;&#20307;&#30340;&#23039;&#24577;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.04861</link><description>&lt;p&gt;
ShapeShift&#65306;&#22522;&#20110;&#36229;&#26925;&#29699;&#24418;&#29366;&#30340;&#26426;&#22120;&#20154;&#25235;&#21462;&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
ShapeShift: Superquadric-based Object Pose Estimation for Robotic Grasping. (arXiv:2304.04861v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04861
&lt;/p&gt;
&lt;p&gt;
ShapeShift&#26159;&#19968;&#20010;&#26426;&#22120;&#20154;&#25235;&#21462;&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#30340;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#36229;&#26925;&#29699;&#30340;&#21442;&#32771;&#24418;&#29366;&#39044;&#27979;&#29289;&#20307;&#30340;&#23039;&#24577;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#26159;&#31934;&#30830;&#29289;&#20307;&#25805;&#32437;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#25216;&#26415;&#20005;&#37325;&#20381;&#36182;&#20110;&#21442;&#32771;3D&#29289;&#20307;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#20351;&#25193;&#23637;&#21040;&#26032;&#30340;&#29289;&#20307;&#31867;&#21035;&#21464;&#24471;&#26114;&#36149;&#12290;&#30452;&#25509;&#23039;&#24577;&#39044;&#27979;&#20063;&#19981;&#33021;&#25552;&#20379;&#20851;&#20110;&#26426;&#22120;&#20154;&#25235;&#21462;&#30340;&#20805;&#20998;&#20449;&#24687;&#32780;&#19981;&#21442;&#32771;3D&#27169;&#22411;&#12290;&#22522;&#20110;&#20851;&#38190;&#28857;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#20869;&#22312;&#30340;&#25551;&#36848;&#24615;&#33021;&#21147;&#32780;&#19981;&#20381;&#36182;&#20110;&#20934;&#30830;&#30340;3D&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#32570;&#20047;&#19968;&#33268;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36229;&#26925;&#29699;&#24418;&#29366;&#30340;ShapeShift&#26694;&#26550;&#65292;&#29992;&#20110;&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#65292;&#35813;&#26694;&#26550;&#39044;&#27979;&#19982;&#36866;&#21512;&#20110;&#29289;&#20307;&#30340;&#21407;&#22987;&#24418;&#29366;&#30456;&#23545;&#30340;&#29289;&#20307;&#23039;&#24577;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#20869;&#22312;&#30340;&#25551;&#36848;&#33021;&#21147;&#65292;&#24182;&#20855;&#26377;&#36229;&#20986;&#35757;&#32451;&#38598;&#30340;&#20219;&#24847;&#20960;&#20309;&#24418;&#29366;&#30340;&#27010;&#25324;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Object pose estimation is a critical task in robotics for precise object manipulation. However, current techniques heavily rely on a reference 3D object, limiting their generalizability and making it expensive to expand to new object categories. Direct pose predictions also provide limited information for robotic grasping without referencing the 3D model. Keypoint-based methods offer intrinsic descriptiveness without relying on an exact 3D model, but they may lack consistency and accuracy. To address these challenges, this paper proposes ShapeShift, a superquadric-based framework for object pose estimation that predicts the object's pose relative to a primitive shape which is fitted to the object. The proposed framework offers intrinsic descriptiveness and the ability to generalize to arbitrary geometric shapes beyond the training set.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#8220;&#24207;&#27169;&#24335;&#8221;&#20316;&#20026;&#20998;&#26512;&#24847;&#20041;&#30340;&#21333;&#20301;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#26684;&#19978;&#36890;&#36807;&#27491;&#24335;&#32972;&#26223;&#30340;&#20840;&#23610;&#24230;&#27979;&#37327;&#26469;&#35782;&#21035;&#36825;&#20123;&#24207;&#23376;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#20174;&#20013;&#31561;&#23610;&#23544;&#24207;&#25968;&#25968;&#25454;&#38598;&#20013;&#26816;&#32034;&#22522;&#26412;&#21547;&#20041;&#12290;</title><link>http://arxiv.org/abs/2304.04827</link><description>&lt;p&gt;
&#26684;&#19978;&#30340;&#24207;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Ordinal Motifs in Lattices. (arXiv:2304.04827v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#8220;&#24207;&#27169;&#24335;&#8221;&#20316;&#20026;&#20998;&#26512;&#24847;&#20041;&#30340;&#21333;&#20301;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#26684;&#19978;&#36890;&#36807;&#27491;&#24335;&#32972;&#26223;&#30340;&#20840;&#23610;&#24230;&#27979;&#37327;&#26469;&#35782;&#21035;&#36825;&#20123;&#24207;&#23376;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#20174;&#20013;&#31561;&#23610;&#23544;&#24207;&#25968;&#25968;&#25454;&#38598;&#20013;&#26816;&#32034;&#22522;&#26412;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26684;&#26159;&#34920;&#31034;&#21644;&#20998;&#26512;&#20851;&#31995;&#21644;&#26412;&#20307;&#30693;&#35782;&#30340;&#24120;&#29992;&#32467;&#26500;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#8220;&#24207;&#27169;&#24335;&#8221;&#20316;&#20026;&#20998;&#26512;&#24847;&#20041;&#30340;&#21333;&#20301;&#65292;&#30740;&#31350;&#36825;&#20123;&#24207;&#23376;&#32467;&#26500;&#65288;&#25110;&#26631;&#20934;&#23610;&#24230;&#65289;&#36890;&#36807;&#26469;&#33258;&#27491;&#24335;&#27010;&#24565;&#20998;&#26512;&#39046;&#22495;&#30340;&#27491;&#24335;&#32972;&#26223;&#30340;&#20840;&#23610;&#24230;&#27979;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24213;&#23618;&#20915;&#31574;&#38382;&#39064;&#26159;NP&#23436;&#20840;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#22914;&#20309;&#36880;&#27493;&#35782;&#21035;&#24207;&#27169;&#24335;&#20197;&#33410;&#30465;&#35745;&#31639;&#37327;&#30340;&#32467;&#26524;&#12290;&#20276;&#38543;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#24207;&#27169;&#24335;&#20174;&#20013;&#31561;&#23610;&#23544;&#30340;&#24207;&#25968;&#25968;&#25454;&#38598;&#20013;&#26816;&#32034;&#22522;&#26412;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lattices are a commonly used structure for the representation and analysis of relational and ontological knowledge. In particular, the analysis of these requires a decomposition of a large and high-dimensional lattice into a set of understandably large parts. With the present work we propose /ordinal motifs/ as analytical units of meaning. We study these ordinal substructures (or standard scales) through (full) scale-measures of formal contexts from the field of formal concept analysis. We show that the underlying decision problems are NP-complete and provide results on how one can incrementally identify ordinal motifs to save computational effort. Accompanying our theoretical results, we demonstrate how ordinal motifs can be leveraged to retrieve basic meaning from a medium sized ordinal data set.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#31361;&#30772;&#23433;&#20840;&#31995;&#32479;&#21644;&#31363;&#21462;&#25935;&#24863;&#25968;&#25454;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#22312;&#39044;&#27979;&#21644;&#38450;&#27490;&#32593;&#32476;&#29359;&#32618;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#21040;&#20102;&#26368;&#36817;&#30740;&#31350;&#20013;&#25928;&#26524;&#26368;&#22909;&#30340;&#36882;&#24402;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2304.04819</link><description>&lt;p&gt;
&#32593;&#32476;&#29359;&#32618;&#39044;&#27979;&#30340;&#36827;&#23637;&#65306;&#26426;&#22120;&#23398;&#20064;&#12289;&#28145;&#24230;&#23398;&#20064;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;&#33258;&#36866;&#24212;&#23398;&#20064;&#25216;&#26415;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Advances in Cybercrime Prediction: A Survey of Machine, Deep, Transfer, and Adaptive Learning Techniques. (arXiv:2304.04819v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#31361;&#30772;&#23433;&#20840;&#31995;&#32479;&#21644;&#31363;&#21462;&#25935;&#24863;&#25968;&#25454;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#22312;&#39044;&#27979;&#21644;&#38450;&#27490;&#32593;&#32476;&#29359;&#32618;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#21040;&#20102;&#26368;&#36817;&#30740;&#31350;&#20013;&#25928;&#26524;&#26368;&#22909;&#30340;&#36882;&#24402;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#29359;&#32618;&#26159;&#19968;&#31181;&#19981;&#26029;&#22686;&#38271;&#30340;&#23041;&#32961;&#65292;&#32618;&#29359;&#20351;&#29992;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#25216;&#26415;&#26469;&#31361;&#30772;&#23433;&#20840;&#31995;&#32479;&#24182;&#31363;&#21462;&#25935;&#24863;&#25968;&#25454;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#24050;&#32463;&#25104;&#20026;&#39044;&#27979;&#32593;&#32476;&#29359;&#32618;&#21644;&#22312;&#20854;&#21457;&#29983;&#20043;&#21069;&#38450;&#27490;&#30340;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#20351;&#29992;&#19978;&#36848;&#25216;&#26415;&#39044;&#27979;&#32593;&#32476;&#29359;&#32618;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#20840;&#38754;&#35843;&#26597;&#65292;&#37325;&#28857;&#20171;&#32461;&#27599;&#31181;&#26041;&#27861;&#30456;&#20851;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;150&#22810;&#31687;&#30740;&#31350;&#25991;&#31456;&#65292;&#24182;&#35752;&#35770;&#20102;&#22823;&#32422;50&#31687;&#26368;&#36817;&#21644;&#26368;&#30456;&#20851;&#30340;&#30740;&#31350;&#25991;&#31456;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#19968;&#20123;&#32593;&#32476;&#29359;&#32618;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#28982;&#21518;&#37325;&#28857;&#20171;&#32461;&#20102;&#26368;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#20363;&#22914;&#36882;&#24402;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#20123;&#25216;&#26415;&#22312;&#26816;&#27979;&#24322;&#24120;&#34892;&#20026;&#21644;&#35782;&#21035;&#28508;&#22312;&#23041;&#32961;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cybercrime is a growing threat to organizations and individuals worldwide, with criminals using increasingly sophisticated techniques to breach security systems and steal sensitive data. In recent years, machine learning, deep learning, and transfer learning techniques have emerged as promising tools for predicting cybercrime and preventing it before it occurs. This paper aims to provide a comprehensive survey of the latest advancements in cybercrime prediction using above mentioned techniques, highlighting the latest research related to each approach. For this purpose, we reviewed more than 150 research articles and discussed around 50 most recent and relevant research articles. We start the review by discussing some common methods used by cyber criminals and then focus on the latest machine learning techniques and deep learning techniques, such as recurrent and convolutional neural networks, which were effective in detecting anomalous behavior and identifying potential threats. We al
&lt;/p&gt;</description></item><item><title>Scallop&#26159;&#19968;&#31181;&#33021;&#22815;&#21516;&#26102;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#36923;&#36753;&#25512;&#29702;&#20248;&#28857;&#30340;&#31070;&#32463;&#31526;&#21495;&#32534;&#31243;&#35821;&#35328;&#65292;&#23427;&#33021;&#22815;&#20197;&#25968;&#25454;&#21644;&#35745;&#31639;&#26377;&#25928;&#30340;&#26041;&#24335;&#35757;&#32451;&#31070;&#32463;&#31526;&#21495;&#24212;&#29992;&#31243;&#24207;&#65292;&#36890;&#36807;&#23427;&#21487;&#22312;AI&#20219;&#21153;&#20013;&#34920;&#36798;&#31639;&#27861;&#25512;&#29702;&#24182;&#34701;&#21512;&#36923;&#36753;&#39046;&#22495;&#30693;&#35782;&#65292;&#20854;&#35299;&#20915;&#26041;&#26696;&#21487;&#19982;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#25110;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2304.04812</link><description>&lt;p&gt;
Scallop: &#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#32534;&#31243;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Scallop: A Language for Neurosymbolic Programming. (arXiv:2304.04812v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04812
&lt;/p&gt;
&lt;p&gt;
Scallop&#26159;&#19968;&#31181;&#33021;&#22815;&#21516;&#26102;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#36923;&#36753;&#25512;&#29702;&#20248;&#28857;&#30340;&#31070;&#32463;&#31526;&#21495;&#32534;&#31243;&#35821;&#35328;&#65292;&#23427;&#33021;&#22815;&#20197;&#25968;&#25454;&#21644;&#35745;&#31639;&#26377;&#25928;&#30340;&#26041;&#24335;&#35757;&#32451;&#31070;&#32463;&#31526;&#21495;&#24212;&#29992;&#31243;&#24207;&#65292;&#36890;&#36807;&#23427;&#21487;&#22312;AI&#20219;&#21153;&#20013;&#34920;&#36798;&#31639;&#27861;&#25512;&#29702;&#24182;&#34701;&#21512;&#36923;&#36753;&#39046;&#22495;&#30693;&#35782;&#65292;&#20854;&#35299;&#20915;&#26041;&#26696;&#21487;&#19982;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#25110;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102; Scallop&#65292;&#36825;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#36923;&#36753;&#25512;&#29702;&#20248;&#28857;&#30340;&#35821;&#35328;&#12290;&#36890;&#36807;&#19977;&#20010;&#20851;&#38190;&#29305;&#24615;&#65292;Scallop &#21551;&#29992;&#29992;&#25143;&#32534;&#20889;&#24191;&#27867;&#30340;&#31070;&#32463;&#31526;&#21495;&#24212;&#29992;&#31243;&#24207;&#24182;&#20197;&#25968;&#25454;&#21644;&#35745;&#31639;&#26377;&#25928;&#30340;&#26041;&#24335;&#35757;&#32451;&#23427;&#20204;&#12290;&#36825;&#19977;&#20010;&#20851;&#38190;&#29305;&#24615;&#21253;&#25324;&#65306;1&#65289;&#22522;&#20110;&#20851;&#31995;&#25968;&#25454;&#27169;&#22411;&#30340;&#28789;&#27963;&#31526;&#21495;&#34920;&#31034;&#65307;2&#65289;&#22522;&#20110; Datalog &#30340;&#22768;&#26126;&#24615;&#36923;&#36753;&#32534;&#31243;&#35821;&#35328;&#65292;&#25903;&#25345;&#36882;&#24402;&#12289;&#32858;&#21512;&#21644;&#21542;&#23450;&#65307;3&#65289;&#22522;&#20110;&#35777;&#26126;&#21322;&#29615;&#29702;&#35770;&#30340;&#33258;&#21160;&#39640;&#25928;&#21487;&#24494;&#25512;&#29702;&#26694;&#26550;&#12290;&#25105;&#20204;&#22312;&#25991;&#29486;&#20013;&#30340;&#20843;&#31181;&#31070;&#32463;&#31526;&#21495;&#24212;&#29992;&#31243;&#24207;&#22871;&#20214;&#19978;&#35780;&#20272; Scallop&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;Scallop &#33021;&#22815;&#22312;&#22810;&#26679;&#21270;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340; AI &#20219;&#21153;&#20013;&#34920;&#36798;&#31639;&#27861;&#25512;&#29702;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#31243;&#24207;&#21592;&#25552;&#20379;&#31616;&#27905;&#30340;&#25509;&#21475;&#20197;&#34701;&#21512;&#36923;&#36753;&#39046;&#22495;&#30693;&#35782;&#65292;&#24182;&#25552;&#20379;&#21487;&#19982;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#25110;&#26356;&#39640;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Scallop, a language which combines the benefits of deep learning and logical reasoning. Scallop enables users to write a wide range of neurosymbolic applications and train them in a data- and compute-efficient manner. It achieves these goals through three key features: 1) a flexible symbolic representation that is based on the relational data model; 2) a declarative logic programming language that is based on Datalog and supports recursion, aggregation, and negation; and 3) a framework for automatic and efficient differentiable reasoning that is based on the theory of provenance semirings. We evaluate Scallop on a suite of eight neurosymbolic applications from the literature. Our evaluation demonstrates that Scallop is capable of expressing algorithmic reasoning in diverse and challenging AI tasks, provides a succinct interface for machine learning programmers to integrate logical domain knowledge, and yields solutions that are comparable or superior to state-of-the-art mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#35780;&#20272;&#21327;&#35758;&#65292;&#35813;&#21327;&#35758;&#36890;&#36807;&#20026;&#36739;&#24930;&#30340;&#26041;&#27861;&#25552;&#20379;&#26356;&#23569;&#30340;&#26679;&#26412;&#26469;&#24809;&#32602;&#23427;&#20204;&#65292;&#20197;&#26356;&#21152;&#29616;&#23454;&#30340;&#26041;&#24335;&#35780;&#20272;&#20102;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#65288;TTA&#65289;&#26041;&#27861;&#12290;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#32771;&#34385;&#25512;&#26029;&#36895;&#24230;&#26102;&#65292;&#31616;&#21333;&#24555;&#36895;&#30340;&#26041;&#27861;&#21487;&#20197;&#20248;&#20110;&#26356;&#22797;&#26434;&#20294;&#36739;&#24930;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.04795</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;&#35780;&#20272;&#19979;&#37325;&#26032;&#23457;&#35270;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Revisiting Test Time Adaptation under Online Evaluation. (arXiv:2304.04795v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#35780;&#20272;&#21327;&#35758;&#65292;&#35813;&#21327;&#35758;&#36890;&#36807;&#20026;&#36739;&#24930;&#30340;&#26041;&#27861;&#25552;&#20379;&#26356;&#23569;&#30340;&#26679;&#26412;&#26469;&#24809;&#32602;&#23427;&#20204;&#65292;&#20197;&#26356;&#21152;&#29616;&#23454;&#30340;&#26041;&#24335;&#35780;&#20272;&#20102;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#65288;TTA&#65289;&#26041;&#27861;&#12290;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#32771;&#34385;&#25512;&#26029;&#36895;&#24230;&#26102;&#65292;&#31616;&#21333;&#24555;&#36895;&#30340;&#26041;&#27861;&#21487;&#20197;&#20248;&#20110;&#26356;&#22797;&#26434;&#20294;&#36739;&#24930;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#35780;&#20272;&#21327;&#35758;&#65292;&#29992;&#20110;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#65288;TTA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20026;&#36739;&#24930;&#30340;&#26041;&#27861;&#25552;&#20379;&#26356;&#23569;&#30340;&#26679;&#26412;&#26469;&#24809;&#32602;&#23427;&#20204;&#12290;TTA&#26041;&#27861;&#21033;&#29992;&#27979;&#35797;&#26102;&#38388;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#36866;&#24212;&#20998;&#24067;&#31227;&#20301;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#24778;&#20154;&#30340;&#24615;&#33021;&#36890;&#24120;&#35201;&#20197;&#26174;&#30528;&#22686;&#21152;&#30340;&#35745;&#31639;&#39044;&#31639;&#20026;&#20195;&#20215;&#12290;&#24403;&#21069;&#30340;&#35780;&#20272;&#21327;&#35758;&#24573;&#30053;&#20102;&#36825;&#31181;&#39069;&#22806;&#35745;&#31639;&#25104;&#26412;&#30340;&#24433;&#21709;&#65292;&#24433;&#21709;&#23427;&#20204;&#22312;&#23454;&#38469;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#29616;&#23454;&#30340;TTA&#26041;&#27861;&#35780;&#20272;&#21327;&#35758;&#65292;&#22312;&#36825;&#20010;&#21327;&#35758;&#20013;&#25968;&#25454;&#20197;&#24658;&#23450;&#36895;&#29575;&#20174;&#25968;&#25454;&#27969;&#20013;&#22312;&#32447;&#25509;&#25910;&#65292;&#20174;&#32780;&#32771;&#34385;&#21040;&#26041;&#27861;&#30340;&#36866;&#24212;&#36895;&#24230;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#21327;&#35758;&#24212;&#29992;&#20110;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#22330;&#26223;&#20013;&#23545;&#22810;&#31181;TTA&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#32771;&#34385;&#25512;&#26029;&#36895;&#24230;&#26102;&#65292;&#31616;&#21333;&#24555;&#36895;&#30340;&#26041;&#27861;&#21487;&#20197;&#20248;&#20110;&#26356;&#22797;&#26434;&#20294;&#36739;&#24930;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel online evaluation protocol for Test Time Adaptation (TTA) methods, which penalizes slower methods by providing them with fewer samples for adaptation. TTA methods leverage unlabeled data at test time to adapt to distribution shifts. Though many effective methods have been proposed, their impressive performance usually comes at the cost of significantly increased computation budgets. Current evaluation protocols overlook the effect of this extra computation cost, affecting their real-world applicability. To address this issue, we propose a more realistic evaluation protocol for TTA methods, where data is received in an online fashion from a constant-speed data stream, thereby accounting for the method's adaptation speed. We apply our proposed protocol to benchmark several TTA methods on multiple datasets and scenarios. Extensive experiments shows that, when accounting for inference speed, simple and fast approaches can outperform more sophisticated but slower
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24847;&#22270;&#24314;&#27169;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#34987;&#21160;&#25968;&#25454;&#20013;&#23398;&#20064;&#29305;&#24449;&#65292;&#24182;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#20215;&#20540;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.04782</link><description>&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#24847;&#22270;&#20174;&#34987;&#21160;&#25968;&#25454;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Passive Data via Latent Intentions. (arXiv:2304.04782v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24847;&#22270;&#24314;&#27169;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#34987;&#21160;&#25968;&#25454;&#20013;&#23398;&#20064;&#29305;&#24449;&#65292;&#24182;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#20215;&#20540;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34987;&#21160;&#35266;&#23519;&#25968;&#25454;&#20016;&#23500;&#32780;&#23500;&#26377;&#20449;&#24687;&#65292;&#28982;&#32780;&#24403;&#21069;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24456;&#23569;&#33021;&#22815;&#21033;&#29992;&#35813;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24314;&#27169;&#24847;&#22270;&#20174;&#34987;&#21160;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#34913;&#37327;&#24403;&#26234;&#33021;&#20307;&#20026;&#23454;&#29616;&#29305;&#23450;&#20219;&#21153;&#32780;&#37319;&#21462;&#34892;&#21160;&#26102;&#26410;&#26469;&#32467;&#26524;&#30340;&#21487;&#33021;&#24615;&#22914;&#20309;&#21464;&#21270;&#26469;&#23398;&#20064;&#24847;&#22270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26102;&#24046;&#23398;&#20064;&#30446;&#26631;&#26469;&#23398;&#20064;&#24847;&#22270;&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#31867;&#20284;&#20110;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#20294;&#26159;&#23436;&#20840;&#26159;&#20174;&#34987;&#21160;&#25968;&#25454;&#20013;&#23398;&#20064;&#24471;&#21040;&#30340;&#12290;&#36890;&#36807;&#20248;&#21270;&#35813;&#30446;&#26631;&#65292;&#25105;&#20204;&#30340;&#26234;&#33021;&#20307;&#21487;&#20197;&#21516;&#26102;&#20174;&#21407;&#22987;&#30340;&#35266;&#23519;&#25968;&#25454;&#20013;&#23398;&#20064;&#20986;&#29366;&#24577;&#12289;&#31574;&#30053;&#21644;&#29615;&#22659;&#19979;&#30340;&#21487;&#33021;&#32467;&#26524;&#12290;&#20174;&#29702;&#35770;&#21644;&#23454;&#39564;&#19978;&#30475;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#20986;&#30340;&#29305;&#24449;&#21487;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#20215;&#20540;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Passive observational data, such as human videos, is abundant and rich in information, yet remains largely untapped by current RL methods. Perhaps surprisingly, we show that passive data, despite not having reward or action labels, can still be used to learn features that accelerate downstream RL. Our approach learns from passive data by modeling intentions: measuring how the likelihood of future outcomes change when the agent acts to achieve a particular task. We propose a temporal difference learning objective to learn about intentions, resulting in an algorithm similar to conventional RL, but which learns entirely from passive data. When optimizing this objective, our agent simultaneously learns representations of states, of policies, and of possible outcomes in an environment, all from raw observational data. Both theoretically and empirically, this scheme learns features amenable for value prediction for downstream tasks, and our experiments demonstrate the ability to learn from m
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#32508;&#36848;&#31995;&#32479;&#20998;&#26512;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#24212;&#29992;&#21450;&#20854;&#27969;&#34892;&#36235;&#21183;&#65292;&#38416;&#36848;&#20102;XAI&#30340;&#20351;&#29992;&#21407;&#22240;&#12289;&#22914;&#20309;&#20351;&#29992;&#20197;&#21450;&#20309;&#26102;&#20351;&#29992;&#21450;&#20854;&#24433;&#21709;&#65292;&#24182;&#32473;&#20986;&#20102;&#22914;&#20309;&#33719;&#24471;&#21487;&#20449;&#36182;&#30340;AI&#30340;&#26041;&#27861;&#65292;&#23545;&#30740;&#31350;&#20154;&#21592;&#12289;&#20020;&#24202;&#21307;&#29983;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#26377;&#37325;&#35201;&#30340;&#25351;&#23548;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.04780</link><description>&lt;p&gt;
&#20851;&#20110;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#32508;&#36848;&#65306;&#20026;&#20160;&#20040;&#12289; &#22914;&#20309;&#21644;&#20309;&#26102;&#65311;
&lt;/p&gt;
&lt;p&gt;
A Review on Explainable Artificial Intelligence for Healthcare: Why, How, and When?. (arXiv:2304.04780v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#32508;&#36848;&#31995;&#32479;&#20998;&#26512;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#24212;&#29992;&#21450;&#20854;&#27969;&#34892;&#36235;&#21183;&#65292;&#38416;&#36848;&#20102;XAI&#30340;&#20351;&#29992;&#21407;&#22240;&#12289;&#22914;&#20309;&#20351;&#29992;&#20197;&#21450;&#20309;&#26102;&#20351;&#29992;&#21450;&#20854;&#24433;&#21709;&#65292;&#24182;&#32473;&#20986;&#20102;&#22914;&#20309;&#33719;&#24471;&#21487;&#20449;&#36182;&#30340;AI&#30340;&#26041;&#27861;&#65292;&#23545;&#30740;&#31350;&#20154;&#21592;&#12289;&#20020;&#24202;&#21307;&#29983;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#26377;&#37325;&#35201;&#30340;&#25351;&#23548;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#12290;&#21516;&#26102;&#20063;&#20986;&#29616;&#20102;&#20851;&#20110;&#36825;&#20123;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#20915;&#31574;&#21487;&#35299;&#37322;&#24615;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#31995;&#32479;&#20998;&#26512;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#65292;&#37325;&#28857;&#20851;&#27880;&#30446;&#21069;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#20351;&#29992;&#30340;&#27169;&#22411;&#12290;&#25353;&#29031;&#31995;&#32479;&#24615;&#32508;&#36848;&#21644;&#20803;&#20998;&#26512;&#30340;&#39318;&#36873;&#25253;&#21578;&#39033;&#30446;&#65288;PRISMA&#65289;&#26631;&#20934;&#65292;&#26816;&#32034;&#20102;2012&#24180;1&#26376;1&#26085;&#33267;2022&#24180;2&#26376;2&#26085;&#26399;&#38388;&#21457;&#34920;&#30340;&#30456;&#20851;&#25991;&#31456;&#12290;&#26412;&#32508;&#36848;&#20998;&#26512;&#20102;XAI&#30340;&#27969;&#34892;&#36235;&#21183;&#65292;&#24182;&#38416;&#36848;&#20102;&#30740;&#31350;&#30340;&#20027;&#35201;&#26041;&#21521;&#12290;&#25105;&#20204;&#35843;&#26597;&#36825;&#20123;XAI&#27169;&#22411;&#30340;&#20351;&#29992;&#21407;&#22240;&#12289;&#22914;&#20309;&#20351;&#29992;&#20197;&#21450;&#20309;&#26102;&#20351;&#29992;&#20197;&#21450;&#20854;&#24433;&#21709;&#12290;&#25105;&#20204;&#23545;XAI&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#20197;&#21450;&#38416;&#36848;&#20102;&#22914;&#20309;&#36890;&#36807;&#25551;&#36848;&#21307;&#30103;&#39046;&#22495;&#30340;AI&#27169;&#22411;&#26469;&#33719;&#24471;&#21487;&#20449;&#36182;&#30340;AI&#12290;&#23545;&#26412;&#25991;&#30340;&#35752;&#35770;&#23558;&#26377;&#21161;&#20110;&#30740;&#31350;&#20154;&#21592;&#12289;&#20020;&#24202;&#21307;&#29983;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#28145;&#20837;&#20102;&#35299;XAI&#22312;&#21307;&#30103;&#20445;&#20581;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) models are increasingly finding applications in the field of medicine. Concerns have been raised about the explainability of the decisions that are made by these AI models. In this article, we give a systematic analysis of explainable artificial intelligence (XAI), with a primary focus on models that are currently being used in the field of healthcare. The literature search is conducted following the preferred reporting items for systematic reviews and meta-analyses (PRISMA) standards for relevant work published from 1 January 2012 to 02 February 2022. The review analyzes the prevailing trends in XAI and lays out the major directions in which research is headed. We investigate the why, how, and when of the uses of these XAI models and their implications. We present a comprehensive examination of XAI methodologies as well as an explanation of how a trustworthy AI can be derived from describing AI models for healthcare fields. The discussion of this work will
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#24341;&#20837;&#36828;&#31243;&#24863;&#30693;&#22270;&#20687;&#34701;&#21512;&#65292;&#37319;&#29992;&#26465;&#20214;&#27880;&#20837;&#35843;&#21046;&#27169;&#22359;&#65288;&#39118;&#26684;&#36716;&#31227;&#35843;&#21046;&#21644;&#23567;&#27874;&#35843;&#21046;&#65289;&#23558;&#39118;&#26684;&#20449;&#24687;&#21644;&#39057;&#29575;&#20449;&#24687;&#27880;&#20837;&#25193;&#25955;UNet&#65292;&#20174;&#32780;&#29983;&#25104;&#39640;&#36136;&#37327;&#34701;&#21512;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2304.04774</link><description>&lt;p&gt;
DDRF: &#36828;&#31243;&#24863;&#30693;&#22270;&#20687;&#34701;&#21512;&#30340;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DDRF: Denoising Diffusion Model for Remote Sensing Image Fusion. (arXiv:2304.04774v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#24341;&#20837;&#36828;&#31243;&#24863;&#30693;&#22270;&#20687;&#34701;&#21512;&#65292;&#37319;&#29992;&#26465;&#20214;&#27880;&#20837;&#35843;&#21046;&#27169;&#22359;&#65288;&#39118;&#26684;&#36716;&#31227;&#35843;&#21046;&#21644;&#23567;&#27874;&#35843;&#21046;&#65289;&#23558;&#39118;&#26684;&#20449;&#24687;&#21644;&#39057;&#29575;&#20449;&#24687;&#27880;&#20837;&#25193;&#25955;UNet&#65292;&#20174;&#32780;&#29983;&#25104;&#39640;&#36136;&#37327;&#34701;&#21512;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20316;&#20026;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#22240;&#20854;&#24378;&#22823;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#22312;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#34701;&#21512;&#39046;&#22495;&#20869;&#30740;&#31350;&#19981;&#36275;&#12290;&#26412;&#25991;&#23558;&#25193;&#25955;&#27169;&#22411;&#24341;&#20837;&#21040;&#22270;&#20687;&#34701;&#21512;&#39046;&#22495;&#65292;&#23558;&#22270;&#20687;&#34701;&#21512;&#20219;&#21153;&#35270;&#20026;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#65292;&#35774;&#35745;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#26465;&#20214;&#27880;&#20837;&#35843;&#21046;&#27169;&#22359;&#65288;&#21363;&#39118;&#26684;&#36716;&#31227;&#35843;&#21046;&#21644;&#23567;&#27874;&#35843;&#21046;&#65289;&#65292;&#20197;&#23558;&#31895;&#31890;&#24230;&#30340;&#39118;&#26684;&#20449;&#24687;&#21644;&#32454;&#31890;&#24230;&#30340;&#39640;&#39057;&#21644;&#20302;&#39057;&#20449;&#24687;&#27880;&#20837;&#25193;&#25955;UNet&#20013;&#65292;&#20174;&#32780;&#29983;&#25104;&#34701;&#21512;&#22270;&#20687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#34701;&#21512;&#20219;&#21153;&#20013;&#30340;&#27531;&#24046;&#23398;&#20064;&#21644;&#35757;&#32451;&#30446;&#26631;&#30340;&#36873;&#25321;&#12290;&#22522;&#20110;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#30340;&#24191;&#27867;&#23454;&#39564;&#32467;&#26524;&#19982;&#22522;&#20934;&#30340;&#27604;&#36739;&#34920;&#26126;&#20102;&#26368;&#26032;&#25216;&#26415;&#25104;&#26524;&#21644;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denosing diffusion model, as a generative model, has received a lot of attention in the field of image generation recently, thanks to its powerful generation capability. However, diffusion models have not yet received sufficient research in the field of image fusion. In this article, we introduce diffusion model to the image fusion field, treating the image fusion task as image-to-image translation and designing two different conditional injection modulation modules (i.e., style transfer modulation and wavelet modulation) to inject coarse-grained style information and fine-grained high-frequency and low-frequency information into the diffusion UNet, thereby generating fused images. In addition, we also discussed the residual learning and the selection of training objectives of the diffusion model in the image fusion task. Extensive experimental results based on quantitative and qualitative assessments compared with benchmarks demonstrates state-of-the-art results and good generalizatio
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#26041;&#38754;&#30340;&#21010;&#26102;&#20195;&#25991;&#29486;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#32531;&#35299;&#25968;&#25454;&#21644;&#27169;&#22411;&#20013;&#30340;&#20559;&#24046;&#65292;&#24378;&#35843;&#22312;&#20844;&#20849;&#21355;&#29983;&#39046;&#22495;&#38656;&#35201;&#20844;&#24179;&#21644;&#20844;&#27491;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.04761</link><description>&lt;p&gt;
&#23558;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#19982;&#20844;&#20849;&#21355;&#29983;&#24179;&#31561;&#32852;&#31995;&#36215;&#26469;
&lt;/p&gt;
&lt;p&gt;
Connecting Fairness in Machine Learning with Public Health Equity. (arXiv:2304.04761v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04761
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#26041;&#38754;&#30340;&#21010;&#26102;&#20195;&#25991;&#29486;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#32531;&#35299;&#25968;&#25454;&#21644;&#27169;&#22411;&#20013;&#30340;&#20559;&#24046;&#65292;&#24378;&#35843;&#22312;&#20844;&#20849;&#21355;&#29983;&#39046;&#22495;&#38656;&#35201;&#20844;&#24179;&#21644;&#20844;&#27491;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24050;&#25104;&#20026;&#20844;&#20849;&#21355;&#29983;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#24037;&#20855;&#65292;&#26377;&#26395;&#25552;&#39640;&#20154;&#21475;&#20581;&#24247;&#12289;&#35786;&#26029;&#12289;&#27835;&#30103;&#36873;&#25321;&#21644;&#21355;&#29983;&#31995;&#32479;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#21644;&#27169;&#22411;&#35774;&#35745;&#20013;&#30340;&#20559;&#35265;&#21487;&#33021;&#23548;&#33268;&#26576;&#20123;&#21463;&#20445;&#25252;&#32676;&#20307;&#30340;&#19981;&#24179;&#31561;&#65292;&#24182;&#25918;&#22823;&#29616;&#26377;&#30340;&#21307;&#30103;&#20445;&#20581;&#19981;&#24179;&#31561;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#24635;&#32467;&#20102;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#26041;&#38754;&#30340;&#21010;&#26102;&#20195;&#25991;&#29486;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#32531;&#35299;&#25968;&#25454;&#21644;&#27169;&#22411;&#20013;&#30340;&#20559;&#24046;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#25351;&#23548;&#65292;&#21487;&#20197;&#23558;&#20844;&#24179;&#24615;&#32435;&#20837;&#20856;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#19981;&#21516;&#38454;&#27573;&#65292;&#22914;&#25968;&#25454;&#22788;&#29702;&#12289;&#27169;&#22411;&#35774;&#35745;&#12289;&#37096;&#32626;&#21644;&#35780;&#20272;&#12290;&#20026;&#20102;&#35828;&#26126;&#25968;&#25454;&#20013;&#20559;&#35265;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20363;&#23376;&#65292;&#23637;&#31034;&#20102;&#31995;&#32479;&#24615;&#20559;&#35265;&#22914;&#20309;&#36890;&#36807;&#27169;&#22411;&#39044;&#27979;&#34987;&#25918;&#22823;&#12290;&#36825;&#20123;&#26696;&#20363;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#35813;&#26694;&#26550;&#26469;&#39044;&#38450;&#36825;&#20123;&#20559;&#35265;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;&#20844;&#20849;&#21355;&#29983;&#39046;&#22495;&#38656;&#35201;&#20844;&#24179;&#21644;&#20844;&#27491;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) has become a critical tool in public health, offering the potential to improve population health, diagnosis, treatment selection, and health system efficiency. However, biases in data and model design can result in disparities for certain protected groups and amplify existing inequalities in healthcare. To address this challenge, this study summarizes seminal literature on ML fairness and presents a framework for identifying and mitigating biases in the data and model. The framework provides guidance on incorporating fairness into different stages of the typical ML pipeline, such as data processing, model design, deployment, and evaluation. To illustrate the impact of biases in data on ML models, we present examples that demonstrate how systematic biases can be amplified through model predictions. These case studies suggest how the framework can be used to prevent these biases and highlight the need for fair and equitable ML models in public health. This work aims
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26412;&#22320;&#31561;&#21516;&#24615;&#30340;&#19977;&#32500;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#23618;&#27425;&#32467;&#26500;&#26469;&#35780;&#20272;&#31561;&#21464;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#23616;&#37096;&#20122;&#32467;&#26500;&#32534;&#30721;&#65288;LSE&#65289;&#21644;&#24103;&#36716;&#25442;&#32534;&#30721;&#65288;FTE&#65289;&#20004;&#20010;&#20851;&#38190;&#27169;&#22359;&#12290;LEFTNet&#26377;&#25928;&#22320;&#24212;&#29992;&#20102;&#36825;&#20123;&#27169;&#22359;&#24182;&#22312;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.04757</link><description>&lt;p&gt;
&#24314;&#31435;&#39640;&#25928;&#21644;&#26377;&#34920;&#29616;&#21147;&#30340;&#19977;&#32500;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A new perspective on building efficient and expressive 3D equivariant graph neural networks. (arXiv:2304.04757v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26412;&#22320;&#31561;&#21516;&#24615;&#30340;&#19977;&#32500;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#23618;&#27425;&#32467;&#26500;&#26469;&#35780;&#20272;&#31561;&#21464;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#23616;&#37096;&#20122;&#32467;&#26500;&#32534;&#30721;&#65288;LSE&#65289;&#21644;&#24103;&#36716;&#25442;&#32534;&#30721;&#65288;FTE&#65289;&#20004;&#20010;&#20851;&#38190;&#27169;&#22359;&#12290;LEFTNet&#26377;&#25928;&#22320;&#24212;&#29992;&#20102;&#36825;&#20123;&#27169;&#22359;&#24182;&#22312;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#20351;&#24471;&#22312;&#24314;&#27169;&#19977;&#32500;&#29289;&#20307;&#26102;&#21487;&#20197;&#32534;&#30721;&#29289;&#29702;&#23545;&#31216;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26412;&#22320;&#31561;&#21516;&#24615;&#30340;&#19977;&#32500;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#23618;&#27425;&#32467;&#26500;&#26469;&#35780;&#20272;&#31561;&#21464;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#35843;&#26597;&#20174;&#23616;&#37096;&#22359;&#20195;&#34920;&#20840;&#23616;&#20960;&#20309;&#20449;&#24687;&#30340;&#36807;&#31243;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20004;&#20010;&#20851;&#38190;&#27169;&#22359;&#8212;&#8212;&#23616;&#37096;&#20122;&#32467;&#26500;&#32534;&#30721;&#65288;LSE&#65289;&#21644;&#24103;&#36716;&#25442;&#32534;&#30721;&#65288;FTE&#65289;&#26159;&#35774;&#35745;&#39640;&#25928;&#21644;&#26377;&#34920;&#29616;&#21147;&#20960;&#20309;GNN&#30340;&#22522;&#30784;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#29702;&#35770;&#30340;&#36866;&#29992;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LEFTNet&#65292;&#23427;&#26377;&#25928;&#22320;&#23454;&#29616;&#20102;&#36825;&#20123;&#27169;&#22359;&#65292;&#24182;&#22312;&#26631;&#37327;&#20540;&#21644;&#30690;&#37327;&#20540;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25351;&#20986;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26410;&#26469;&#21457;&#23637;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geometric deep learning enables the encoding of physical symmetries in modeling 3D objects. Despite rapid progress in encoding 3D symmetries into Graph Neural Networks (GNNs), a comprehensive evaluation of the expressiveness of these networks through a local-to-global analysis lacks today. In this paper, we propose a local hierarchy of 3D isomorphism to evaluate the expressive power of equivariant GNNs and investigate the process of representing global geometric information from local patches. Our work leads to two crucial modules for designing expressive and efficient geometric GNNs; namely local substructure encoding (LSE) and frame transition encoding (FTE). To demonstrate the applicability of our theory, we propose LEFTNet which effectively implements these modules and achieves state-of-the-art performance on both scalar-valued and vector-valued molecular property prediction tasks. We further point out the design space for future developments of equivariant graph neural networks. O
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#32423;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#65292;&#21033;&#29992;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#21152;&#36895;&#27773;&#36710;&#34892;&#19994;&#20013;&#22788;&#29702;&#36710;&#36742;&#36136;&#37327;&#38382;&#39064;&#30340;&#20840;&#21608;&#26399;&#65292;&#20174;&#32780;&#26356;&#24555;&#22320;&#38548;&#31163;&#26681;&#26412;&#21407;&#22240;&#12289;&#30830;&#23450;&#27835;&#30103;&#25514;&#26045;&#24182;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.04755</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#36947;&#36335;&#36710;&#36742;&#36136;&#37327;&#38382;&#39064;&#35786;&#26029;&#30340;&#20004;&#23618;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Novel Two-level Causal Inference Framework for On-road Vehicle Quality Issues Diagnosis. (arXiv:2304.04755v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04755
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#32423;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#65292;&#21033;&#29992;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#21152;&#36895;&#27773;&#36710;&#34892;&#19994;&#20013;&#22788;&#29702;&#36710;&#36742;&#36136;&#37327;&#38382;&#39064;&#30340;&#20840;&#21608;&#26399;&#65292;&#20174;&#32780;&#26356;&#24555;&#22320;&#38548;&#31163;&#26681;&#26412;&#21407;&#22240;&#12289;&#30830;&#23450;&#27835;&#30103;&#25514;&#26045;&#24182;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27773;&#36710;&#34892;&#19994;&#20013;&#65292;&#22788;&#29702;&#20351;&#29992;&#20013;&#36710;&#36742;&#36136;&#37327;&#38382;&#39064;&#30340;&#20840;&#21608;&#26399;&#21487;&#33021;&#38656;&#35201;&#25968;&#21608;&#36827;&#34892;&#35843;&#26597;&#12290;&#36825;&#20010;&#36807;&#31243;&#28041;&#21450;&#21040;&#38548;&#31163;&#26681;&#26412;&#21407;&#22240;&#12289;&#23450;&#20041;&#21644;&#23454;&#26045;&#36866;&#24403;&#30340;&#27835;&#30103;&#25514;&#26045;&#65292;&#20197;&#21450;&#22312;&#24517;&#35201;&#26102;&#25913;&#36827;&#27835;&#30103;&#25514;&#26045;&#12290;&#20027;&#35201;&#38382;&#39064;&#26159;&#32570;&#20047;&#19968;&#31181;&#31995;&#32479;&#24615;&#30340;&#26041;&#27861;&#26469;&#30830;&#23450;&#22240;&#26524;&#20851;&#31995;&#12289;&#35780;&#20272;&#27835;&#30103;&#25928;&#26524;&#65292;&#24182;&#22312;&#24403;&#21069;&#27835;&#30103;&#34987;&#35748;&#20026;&#26080;&#25928;&#26102;&#25351;&#23548;&#19979;&#19968;&#20010;&#21487;&#34892;&#30340;&#27835;&#30103;&#25514;&#26045;&#12290;&#26412;&#25991;&#23558;&#23637;&#31034;&#22914;&#20309;&#21033;&#29992;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21152;&#36895;&#36825;&#20123;&#36807;&#31243;&#12290;&#20351;&#29992;&#20174;&#36335;&#19978;&#36710;&#36742;&#25910;&#38598;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#26469;&#28436;&#31034;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#12290;&#36824;&#23558;&#35752;&#35770;&#36710;&#36742;&#36136;&#37327;&#24212;&#29992;&#30340;&#24320;&#25918;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the automotive industry, the full cycle of managing in-use vehicle quality issues can take weeks to investigate. The process involves isolating root causes, defining and implementing appropriate treatments, and refining treatments if needed. The main pain-point is the lack of a systematic method to identify causal relationships, evaluate treatment effectiveness, and direct the next actionable treatment if the current treatment was deemed ineffective. This paper will show how we leverage causal Machine Learning (ML) to speed up such processes. A real-word data set collected from on-road vehicles will be used to demonstrate the proposed framework. Open challenges for vehicle quality applications will also be discussed.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;e-Uber&#30340;&#20247;&#21253;&#24179;&#21488;&#65292;&#35813;&#24179;&#21488;&#21033;&#29992;&#30005;&#21160;&#27773;&#36710;&#30340;&#26085;&#30410;&#26222;&#21450;&#65292;&#36890;&#36807;V2G&#21644;BST&#20849;&#21516;&#23454;&#29616;&#25340;&#36710;&#21644;&#33021;&#37327;&#20849;&#20139;&#12290;&#24179;&#21488;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#22522;&#20110;CMAB&#31639;&#27861;&#23454;&#29616;&#20010;&#24615;&#21270;&#20219;&#21153;&#25512;&#33616;&#31995;&#32479;(CARS)&#65292;&#24182;&#21033;&#29992;&#21453;&#21521;&#25293;&#21334;&#26426;&#21046;&#36873;&#25321;&#26368;&#20248;&#20986;&#20215;&#12290;</title><link>http://arxiv.org/abs/2304.04753</link><description>&lt;p&gt;
$\textit{e-Uber}$&#65306;&#19968;&#20010;&#22522;&#20110;&#20849;&#20139;&#32463;&#27982;&#30340;&#24179;&#21488;&#65292;&#23454;&#29616;&#22522;&#20110;&#30005;&#21160;&#27773;&#36710;&#30340;&#25340;&#36710;&#21644;&#33021;&#37327;&#20849;&#20139;
&lt;/p&gt;
&lt;p&gt;
$\textit{e-Uber}$: A Crowdsourcing Platform for Electric Vehicle-based Ride- and Energy-sharing. (arXiv:2304.04753v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;e-Uber&#30340;&#20247;&#21253;&#24179;&#21488;&#65292;&#35813;&#24179;&#21488;&#21033;&#29992;&#30005;&#21160;&#27773;&#36710;&#30340;&#26085;&#30410;&#26222;&#21450;&#65292;&#36890;&#36807;V2G&#21644;BST&#20849;&#21516;&#23454;&#29616;&#25340;&#36710;&#21644;&#33021;&#37327;&#20849;&#20139;&#12290;&#24179;&#21488;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#22522;&#20110;CMAB&#31639;&#27861;&#23454;&#29616;&#20010;&#24615;&#21270;&#20219;&#21153;&#25512;&#33616;&#31995;&#32479;(CARS)&#65292;&#24182;&#21033;&#29992;&#21453;&#21521;&#25293;&#21334;&#26426;&#21046;&#36873;&#25321;&#26368;&#20248;&#20986;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#20849;&#20139;&#32463;&#27982;&#30340;&#21830;&#19994;&#27169;&#24335;&#22312;&#20132;&#36890;&#21644;&#20303;&#23487;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#22914;Uber&#21644;Airbnb&#12290;&#20154;&#20204;&#36234;&#26469;&#36234;&#26377;&#20852;&#36259;&#23558;&#27492;&#27169;&#24335;&#24212;&#29992;&#20110;&#33021;&#28304;&#31995;&#32479;&#65292;&#21253;&#25324;&#28857;&#23545;&#28857;&#65288;P2P&#65289;&#33021;&#28304;&#20132;&#26131;&#21644;&#22522;&#20110;&#30005;&#21160;&#27773;&#36710;&#65288;EV&#65289;&#30340;&#36710;&#23545;&#32593;&#65288;V2G&#65289;&#12289;&#36710;&#23545;&#23478;&#65288;V2H&#65289;&#12289;&#36710;&#23545;&#36710;&#65288;V2V&#65289;&#20197;&#21450;&#30005;&#27744;&#26356;&#25442;&#25216;&#26415;(BST)&#31561;&#26041;&#24335;&#12290;&#26412;&#25991;&#21033;&#29992;&#30005;&#21160;&#27773;&#36710;&#30340;&#26085;&#30410;&#26222;&#21450;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#21517;&#20026;e-Uber&#30340;&#20247;&#21253;&#24179;&#21488;&#65292;&#36890;&#36807;V2G&#21644;BST&#20849;&#21516;&#23454;&#29616;&#25340;&#36710;&#21644;&#33021;&#37327;&#20849;&#20139;&#12290;e-Uber&#21033;&#29992;&#31354;&#38388;&#20247;&#21253;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#21453;&#21521;&#25293;&#21334;&#29702;&#35770;&#23454;&#29616;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#24179;&#21488;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#20102;&#35299;&#21496;&#26426;&#23545;&#19981;&#21516;&#25340;&#36710;&#21644;&#33021;&#37327;&#20849;&#20139;&#20219;&#21153;&#30340;&#20559;&#22909;&#12290;&#22522;&#20110;&#36825;&#20123;&#20559;&#22909;&#65292;&#36890;&#36807;&#22522;&#20110;CMAB&#30340;&#31639;&#27861;&#36827;&#34892;&#20219;&#21153;&#25512;&#33616;&#31995;&#32479;(CARS)&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#21496;&#26426;&#22312;&#24895;&#24847;&#25191;&#34892;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#31454;&#26631;&#65292;&#32780;&#21453;&#21521;&#25293;&#21334;&#26426;&#21046;&#21017;&#36873;&#25321;&#26368;&#20248;&#20986;&#20215;&#12290;&#35745;&#31639;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#22312;&#21305;&#37197;&#25928;&#29575;&#12289;&#26102;&#38388;&#25928;&#29575;&#21644;&#29992;&#25143;&#28385;&#24847;&#24230;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The sharing-economy-based business model has recently seen success in the transportation and accommodation sectors with companies like Uber and Airbnb. There is growing interest in applying this model to energy systems, with modalities like peer-to-peer (P2P) Energy Trading, Electric Vehicles (EV)-based Vehicle-to-Grid (V2G), Vehicle-to-Home (V2H), Vehicle-to-Vehicle (V2V), and Battery Swapping Technology (BST). In this work, we exploit the increasing diffusion of EVs to realize a crowdsourcing platform called e-Uber that jointly enables ride-sharing and energy-sharing through V2G and BST. e-Uber exploits spatial crowdsourcing, reinforcement learning, and reverse auction theory. Specifically, the platform uses reinforcement learning to understand the drivers' preferences towards different ride-sharing and energy-sharing tasks. Based on these preferences, a personalized list is recommended to each driver through CMAB-based Algorithm for task Recommendation System (CARS). Drivers bid on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35774;&#35745;&#32676;&#20307;&#20248;&#21270;&#22120;&#20197;&#20840;&#23616;&#20248;&#21270;&#26114;&#36149;&#40657;&#30418;&#20989;&#25968;&#65292;&#24182;&#22312;&#21508;&#31867;&#22522;&#20934;&#20248;&#21270;&#20989;&#25968;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#21644;&#25152;&#38656;&#30340;&#32553;&#25918;&#12290;</title><link>http://arxiv.org/abs/2304.04751</link><description>&lt;p&gt;
DeepHive: &#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#21270;&#21457;&#29616;&#22522;&#20110;&#32676;&#20307;&#20248;&#21270;&#31574;&#30053;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DeepHive: A multi-agent reinforcement learning approach for automated discovery of swarm-based optimization policies. (arXiv:2304.04751v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35774;&#35745;&#32676;&#20307;&#20248;&#21270;&#22120;&#20197;&#20840;&#23616;&#20248;&#21270;&#26114;&#36149;&#40657;&#30418;&#20989;&#25968;&#65292;&#24182;&#22312;&#21508;&#31867;&#22522;&#20934;&#20248;&#21270;&#20989;&#25968;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#21644;&#25152;&#38656;&#30340;&#32553;&#25918;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35774;&#35745;&#32676;&#20307;&#20248;&#21270;&#22120;&#26469;&#20840;&#23616;&#20248;&#21270;&#26114;&#36149;&#40657;&#30418;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#25214;&#21040;&#39640;&#25928;&#30340;&#20248;&#21270;&#22120;&#30340;&#38382;&#39064;&#34987;&#35270;&#20026;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#25214;&#21040;&#38656;&#35201;&#23569;&#25968;&#20989;&#25968;&#35780;&#20272;&#21363;&#21487;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#20248;&#30340;&#20248;&#21270;&#31574;&#30053;&#12290;&#32676;&#20307;&#20013;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#29366;&#24577;&#34987;&#23450;&#20041;&#20026;&#20854;&#22312;&#35774;&#35745;&#31354;&#38388;&#20869;&#30340;&#24403;&#21069;&#20301;&#32622;&#21644;&#20989;&#25968;&#20540;&#65292;&#24182;&#19988;&#36825;&#20123;&#26234;&#33021;&#20307;&#23398;&#20250;&#37319;&#21462;&#26368;&#22823;&#21270;&#22870;&#21169;&#30340;&#26377;&#21033;&#34892;&#21160;&#65292;&#35813;&#22870;&#21169;&#22522;&#20110;&#30446;&#26631;&#20989;&#25968;&#30340;&#26368;&#32456;&#20540;&#12290;&#35813;&#26041;&#27861;&#22312;&#21508;&#31867;&#22522;&#20934;&#20248;&#21270;&#20989;&#25968;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#19982;&#20854;&#20182;&#20840;&#23616;&#20248;&#21270;&#31574;&#30053;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#26356;&#25913;&#20195;&#29702;&#20154;&#25968;&#37327;&#20197;&#21450;&#35757;&#32451;&#20195;&#29702;&#20154;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#20248;&#21270;&#22120;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#21644;&#25152;&#38656;&#30340;&#32553;&#25918;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an approach for designing swarm-based optimizers for the global optimization of expensive black-box functions. In the proposed approach, the problem of finding efficient optimizers is framed as a reinforcement learning problem, where the goal is to find optimization policies that require a few function evaluations to converge to the global optimum. The state of each agent within the swarm is defined as its current position and function value within a design space and the agents learn to take favorable actions that maximize reward, which is based on the final value of the objective function. The proposed approach is tested on various benchmark optimization functions and compared to the performance of other global optimization strategies. Furthermore, the effect of changing the number of agents, as well as the generalization capabilities of the trained agents are investigated. The results show superior performance compared to the other optimizers, desired scaling when the numb
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#21644;&#20132;&#20114;&#27169;&#22359;&#36827;&#34892;&#20132;&#20114;&#24863;&#30693;&#25552;&#31034;&#30340;&#38646;&#26679;&#26412;&#26102;&#31354;&#21160;&#20316;&#26816;&#27979;&#26041;&#27861;&#65292;&#20248;&#21270;&#20102;&#35270;&#35273;-&#35821;&#35328;&#29305;&#24449;&#30340;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.04688</link><description>&lt;p&gt;
&#20132;&#20114;&#24863;&#30693;&#25552;&#31034;&#30340;&#38646;&#26679;&#26412;&#26102;&#31354;&#21160;&#20316;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Interaction-Aware Prompting for Zero-Shot Spatio-Temporal Action Detection. (arXiv:2304.04688v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04688
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#21644;&#20132;&#20114;&#27169;&#22359;&#36827;&#34892;&#20132;&#20114;&#24863;&#30693;&#25552;&#31034;&#30340;&#38646;&#26679;&#26412;&#26102;&#31354;&#21160;&#20316;&#26816;&#27979;&#26041;&#27861;&#65292;&#20248;&#21270;&#20102;&#35270;&#35273;-&#35821;&#35328;&#29305;&#24449;&#30340;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;-&#26102;&#38388;&#21160;&#20316;&#26816;&#27979;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#27599;&#20010;&#20154;&#22312;&#35270;&#39057;&#20013;&#21160;&#20316;&#21457;&#29983;&#30340;&#26102;&#38388;&#21644;&#20301;&#32622;&#65292;&#24182;&#23545;&#30456;&#24212;&#30340;&#21160;&#20316;&#31867;&#21035;&#36827;&#34892;&#20998;&#31867;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37319;&#29992;&#20840;&#30417;&#30563;&#23398;&#20064;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#22240;&#27492;&#38590;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#23398;&#20064;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#20195;&#34920;&#24615;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#19981;&#21516;&#30340;&#20132;&#20114;&#27169;&#22359;&#24314;&#27169;&#36825;&#20123;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#24471;&#21040;&#20132;&#20114;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#36825;&#20010;&#29305;&#24449;&#25552;&#31034;&#27599;&#20010;&#26631;&#31614;&#20197;&#33719;&#21462;&#26356;&#21512;&#36866;&#30340;&#25991;&#26412;&#29305;&#24449;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35745;&#31639;&#27599;&#20010;&#26631;&#31614;&#30340;&#20132;&#20114;&#29305;&#24449;&#21644;&#25991;&#26412;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#65292;&#20197;&#30830;&#23450;&#21160;&#20316;&#31867;&#21035;&#12290;&#22312;J-HMDB&#21644;UCF101-24&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#20132;&#20114;&#27169;&#22359;&#21644;&#25552;&#31034;&#20351;&#35270;&#35273;-&#35821;&#35328;&#29305;&#24449;&#26356;&#21152;&#23545;&#40784;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of spatial-temporal action detection is to determine the time and place where each person's action occurs in a video and classify the corresponding action category. Most of the existing methods adopt fully-supervised learning, which requires a large amount of training data, making it very difficult to achieve zero-shot learning. In this paper, we propose to utilize a pre-trained visual-language model to extract the representative image and text features, and model the relationship between these features through different interaction modules to obtain the interaction feature. In addition, we use this feature to prompt each label to obtain more appropriate text features. Finally, we calculate the similarity between the interaction feature and the text feature for each label to determine the action category. Our experiments on J-HMDB and UCF101-24 datasets demonstrate that the proposed interaction module and prompting make the visual-language features better aligned, thus achievi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FedPAC&#26694;&#26550;&#65292;&#21033;&#29992;PAC&#23398;&#20064;&#29702;&#35770;&#25512;&#23548;&#20986;&#19968;&#20010;&#35299;&#26512;&#35299;&#65292;&#21487;&#20197;&#20445;&#35777;FL&#20043;&#38388;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#30340;&#26368;&#20339;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.04641</link><description>&lt;p&gt;
&#21487;&#33021;&#22823;&#33268;&#27491;&#30830;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Probably Approximately Correct Federated Learning. (arXiv:2304.04641v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FedPAC&#26694;&#26550;&#65292;&#21033;&#29992;PAC&#23398;&#20064;&#29702;&#35770;&#25512;&#23548;&#20986;&#19968;&#20010;&#35299;&#26512;&#35299;&#65292;&#21487;&#20197;&#20445;&#35777;FL&#20043;&#38388;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#30340;&#26368;&#20339;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#33539;&#20363;&#65292;&#20854;&#20027;&#35201;&#25903;&#26609;&#20026;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#12290;&#29616;&#26377;&#30740;&#31350;&#34920;&#26126;&#65292;&#21516;&#26102;&#23454;&#29616;&#26080;&#31351;&#23567;&#38544;&#31169;&#27844;&#38706;&#12289;&#25928;&#29992;&#25439;&#22833;&#21644;&#25928;&#29575;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#35774;&#35745;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#26102;&#65292;&#22914;&#20309;&#25214;&#21040;&#26368;&#20339;&#26435;&#34913;&#35299;&#20915;&#26041;&#26696;&#26159;&#20851;&#38190;&#32771;&#34385;&#22240;&#32032;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#23558;&#26435;&#34913;&#38382;&#39064;&#35270;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#30446;&#26631;&#26159;&#22312;&#32422;&#26463;&#38544;&#31169;&#27844;&#38706;&#19981;&#36229;&#36807;&#39044;&#23450;&#20540;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#25928;&#29992;&#25439;&#22833;&#21644;&#25928;&#29575;&#38477;&#20302;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#26694;&#26550;&#38750;&#24120;&#32791;&#26102;&#65292;&#24182;&#19988;&#19981;&#33021;&#20445;&#35777;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#23384;&#22312;&#24615;&#65292;&#36825;&#28608;&#21169;&#25105;&#20204;&#23547;&#27714;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#22810;&#30446;&#26631;&#38382;&#39064;&#36716;&#21270;&#20026;&#21333;&#30446;&#26631;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#26356;&#39640;&#25928;&#12289;&#26356;&#23481;&#26131;&#34987;&#35299;&#20915;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;FedPAC&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;PAC&#23398;&#20064;&#29702;&#35770;&#25512;&#23548;&#20986;&#19968;&#20010;&#35299;&#26512;&#35299;&#65292;&#21487;&#20197;&#20445;&#35777;FL&#20043;&#38388;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#30340;&#26368;&#20339;&#26435;&#34913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;FL&#38382;&#39064;&#20844;&#24335;&#21270;&#20026;&#19968;&#20010;&#20108;&#20998;&#31867;&#20219;&#21153;&#65292;&#28982;&#21518;&#35774;&#35745;&#19968;&#20010;&#33258;&#36866;&#24212;FL&#31639;&#27861;&#65292;&#21160;&#24577;&#35843;&#25972;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#37319;&#26679;&#27604;&#29575;&#65292;&#20197;&#24179;&#34913;&#20840;&#23616;&#21644;&#26412;&#22320;&#30340;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#65292;&#26368;&#21518;&#35777;&#26126;FedPAC&#21487;&#20197;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#39640;&#27010;&#29575;&#22320;&#23454;&#29616;&#26368;&#20248;&#30340;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;FedPAC&#26694;&#26550;&#30340;&#21151;&#25928;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a new distributed learning paradigm, with privacy, utility, and efficiency as its primary pillars. Existing research indicates that it is unlikely to simultaneously attain infinitesimal privacy leakage, utility loss, and efficiency. Therefore, how to find an optimal trade-off solution is the key consideration when designing the FL algorithm. One common way is to cast the trade-off problem as a multi-objective optimization problem, i.e., the goal is to minimize the utility loss and efficiency reduction while constraining the privacy leakage not exceeding a predefined value. However, existing multi-objective optimization frameworks are very time-consuming, and do not guarantee the existence of the Pareto frontier, this motivates us to seek a solution to transform the multi-objective problem into a single-objective problem because it is more efficient and easier to be solved. To this end, in this paper, we propose FedPAC, a unified framework that leverages PAC l
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#39640;&#25928;&#21442;&#25968;&#21270;&#21644;&#35299;&#26512;&#36870;&#21464;&#25442;&#30340;&#33267;&#23569;&#20108;&#27425;&#36830;&#32493;&#21487;&#24494;&#12289;&#21452;Lipschitz&#36830;&#32493;&#30340;&#38750;&#22343;&#21248;B&#26679;&#26465;&#27969;&#24418;&#65292;&#33021;&#22815;&#22312;&#23494;&#24230;&#20272;&#35745;&#21644;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.04555</link><description>&lt;p&gt;
&#31070;&#32463;&#27969;&#24418;&#38750;&#22343;&#21248;B&#26679;&#26465;&#27969;
&lt;/p&gt;
&lt;p&gt;
Neural Diffeomorphic Non-uniform B-spline Flows. (arXiv:2304.04555v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04555
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#39640;&#25928;&#21442;&#25968;&#21270;&#21644;&#35299;&#26512;&#36870;&#21464;&#25442;&#30340;&#33267;&#23569;&#20108;&#27425;&#36830;&#32493;&#21487;&#24494;&#12289;&#21452;Lipschitz&#36830;&#32493;&#30340;&#38750;&#22343;&#21248;B&#26679;&#26465;&#27969;&#24418;&#65292;&#33021;&#22815;&#22312;&#23494;&#24230;&#20272;&#35745;&#21644;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#21017;&#21270;&#27969;&#25104;&#21151;&#22320;&#23558;&#22797;&#26434;&#30340;&#27010;&#29575;&#20998;&#24067;&#24314;&#27169;&#20026;&#31616;&#21333;&#22522;&#26412;&#20998;&#24067;&#30340;&#21487;&#36870;&#21464;&#25442;&#12290;&#28982;&#32780;&#65292;&#26377;&#26102;&#20505;&#38656;&#35201;&#26356;&#22810;&#12290;&#22312;&#29289;&#29702;&#20013;&#35745;&#31639;&#33021;&#37327;&#21644;&#21147;&#35201;&#27714;&#21464;&#25442;&#30340;&#20108;&#38454;&#23548;&#25968;&#26159;&#33391;&#22909;&#23450;&#20041;&#21644;&#36830;&#32493;&#30340;&#65292;&#24179;&#28369;&#27491;&#21017;&#21270;&#27969;&#37319;&#29992;&#26080;&#38480;&#21487;&#24494;&#21464;&#25442;&#65292;&#20294;&#20197;&#32531;&#24930;&#30340;&#38750;&#35299;&#26512;&#36870;&#21464;&#25442;&#30340;&#20195;&#20215;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#33267;&#23569;&#20108;&#27425;&#36830;&#32493;&#21487;&#24494;&#19988;&#21452;Lipschitz&#36830;&#32493;&#30340;&#38750;&#22343;&#21248;B&#26679;&#26465;&#27969;&#24418;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#21442;&#25968;&#21270;&#21644;&#22522;&#20110;&#24494;&#20998;&#21516;&#32986;&#30340;&#35299;&#26512;&#36870;&#21464;&#25442;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Ck-2&#24494;&#20998;&#21516;&#32986;&#30340;&#38750;&#22343;&#21248;k&#38454;B&#26679;&#26465;&#21464;&#25442;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#31526;&#21512;&#20805;&#20998;&#26465;&#20214;&#30340;&#38750;&#22343;&#21248;&#31435;&#26041;B&#26679;&#26465;&#21464;&#25442;&#30340;&#35299;&#26512;&#36870;&#21464;&#25442;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23494;&#24230;&#20272;&#35745;&#21644;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#19978;&#33719;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Normalizing flows have been successfully modeling a complex probability distribution as an invertible transformation of a simple base distribution. However, there are often applications that require more than invertibility. For instance, the computation of energies and forces in physics requires the second derivatives of the transformation to be well-defined and continuous. Smooth normalizing flows employ infinitely differentiable transformation, but with the price of slow non-analytic inverse transforms. In this work, we propose diffeomorphic non-uniform B-spline flows that are at least twice continuously differentiable while bi-Lipschitz continuous, enabling efficient parametrization while retaining analytic inverse transforms based on a sufficient condition for diffeomorphism. Firstly, we investigate the sufficient condition for Ck-2-diffeomorphic non-uniform kth-order B-spline transformations. Then, we derive an analytic inverse transformation of the non-uniform cubic B-spline tran
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;COhort Representation lEarning&#65288;CORE&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;EHR&#34920;&#31034;&#23398;&#20064;&#65292;&#25903;&#25345;&#38024;&#23545;&#19981;&#21516;&#38431;&#21015;&#30340;&#29305;&#24449;&#36827;&#34892;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2304.04468</link><description>&lt;p&gt;
&#23454;&#29616;&#38431;&#21015;&#26234;&#33021;&#21270;&#65306;&#19968;&#31181;&#38024;&#23545;&#30005;&#23376;&#30149;&#21382;&#20998;&#26512;&#30340;&#36890;&#29992;&#32676;&#20307;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Toward Cohort Intelligence: A Universal Cohort Representation Learning Framework for Electronic Health Record Analysis. (arXiv:2304.04468v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04468
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;COhort Representation lEarning&#65288;CORE&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;EHR&#34920;&#31034;&#23398;&#20064;&#65292;&#25903;&#25345;&#38024;&#23545;&#19981;&#21516;&#38431;&#21015;&#30340;&#29305;&#24449;&#36827;&#34892;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#30149;&#21382;&#65288;EHR&#65289;&#26159;&#20174;&#20020;&#24202;&#24120;&#35268;&#25252;&#29702;&#20013;&#29983;&#25104;&#30340;&#65292;&#35760;&#24405;&#20102;&#24191;&#27867;&#30340;&#30149;&#20154;&#20154;&#32676;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;&#20026;&#25913;&#21892;&#20020;&#24202;&#23454;&#36341;&#20013;&#30340;&#30149;&#20154;&#31649;&#29702;&#21644;&#24178;&#39044;&#31574;&#30053;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#26426;&#20250;&#12290;&#20026;&#20102;&#21033;&#29992;EHR&#25968;&#25454;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#26426;&#22120;&#23398;&#20064;&#20013;&#27969;&#34892;&#30340;EHR&#25968;&#25454;&#20998;&#26512;&#33539;&#24335;&#26159;EHR&#34920;&#31034;&#23398;&#20064;&#65292;&#23427;&#39318;&#20808;&#21033;&#29992;&#21333;&#20010;&#30149;&#20154;&#30340;EHR&#25968;&#25454;&#36890;&#36807;&#19968;&#20010;&#20027;&#24178;&#23398;&#20064;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#24182;&#25903;&#25345;&#24314;&#31435;&#22312;&#36825;&#20123;&#34920;&#31034;&#30340;&#22810;&#26679;&#21270;&#30340;&#21307;&#30103;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33539;&#24335;&#26080;&#27861;&#28145;&#20837;&#20998;&#26512;&#30149;&#20154;&#30340;&#30456;&#20851;&#24615;&#65292;&#36890;&#24120;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#34987;&#31216;&#20026;&#38431;&#21015;&#30740;&#31350;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#21516;&#19968;&#38431;&#21015;&#20013;&#30340;&#30149;&#20154;&#20542;&#21521;&#20110;&#20855;&#26377;&#30456;&#20284;&#30340;&#29305;&#24449;&#65292;&#34920;&#26126;&#20182;&#20204;&#22312;&#21307;&#30103;&#26465;&#20214;&#65288;&#22914;&#30151;&#29366;&#25110;&#30142;&#30149;&#65289;&#26041;&#38754;&#20855;&#26377;&#30456;&#20284;&#20043;&#22788;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;COhort Representation lEarning (CORE)&#26694;&#26550;&#26469;&#22686;&#24378;EHR&#34920;&#31034;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#29992;&#38431;&#21015;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#65292;&#23545;&#32676;&#20307;&#20449;&#24687;&#36827;&#34892;&#24314;&#27169;&#24182;&#25903;&#25345;&#38024;&#23545;&#19981;&#21516;&#38431;&#21015;&#30340;&#29305;&#24449;&#36827;&#34892;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic Health Records (EHR) are generated from clinical routine care recording valuable information of broad patient populations, which provide plentiful opportunities for improving patient management and intervention strategies in clinical practice. To exploit the enormous potential of EHR data, a popular EHR data analysis paradigm in machine learning is EHR representation learning, which first leverages the individual patient's EHR data to learn informative representations by a backbone, and supports diverse health-care downstream tasks grounded on the representations. Unfortunately, such a paradigm fails to access the in-depth analysis of patients' relevance, which is generally known as cohort studies in clinical practice. Specifically, patients in the same cohort tend to share similar characteristics, implying their resemblance in medical conditions such as symptoms or diseases. In this paper, we propose a universal COhort Representation lEarning (CORE) framework to augment EHR
&lt;/p&gt;</description></item><item><title>H2RBox-v2&#26159;&#31532;&#19968;&#20010;&#23558;&#23545;&#31216;&#23398;&#20064;&#24212;&#29992;&#20110;&#22522;&#20110;HBox&#30417;&#30563;&#30340;&#26377;&#21521;&#29289;&#20307;&#26816;&#27979;&#65292;&#20854;&#24378;&#21270;&#20102;&#27700;&#24179;&#27880;&#37322;&#21644;&#26059;&#36716;&#27880;&#37322;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.04403</link><description>&lt;p&gt;
H2RBox-v2&#65306;&#36890;&#36807;&#23545;&#31216;&#23398;&#20064;&#25552;&#39640;&#22522;&#20110;HBox&#30417;&#30563;&#30340;&#26377;&#21521;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
H2RBox-v2: Boosting HBox-supervised Oriented Object Detection via Symmetric Learning. (arXiv:2304.04403v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04403
&lt;/p&gt;
&lt;p&gt;
H2RBox-v2&#26159;&#31532;&#19968;&#20010;&#23558;&#23545;&#31216;&#23398;&#20064;&#24212;&#29992;&#20110;&#22522;&#20110;HBox&#30417;&#30563;&#30340;&#26377;&#21521;&#29289;&#20307;&#26816;&#27979;&#65292;&#20854;&#24378;&#21270;&#20102;&#27700;&#24179;&#27880;&#37322;&#21644;&#26059;&#36716;&#27880;&#37322;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#21644;&#36965;&#24863;&#31561;&#26377;&#21521;&#29289;&#20307;&#26816;&#27979;&#38656;&#27714;&#30340;&#26085;&#30410;&#22686;&#38271;&#65292;&#26377;&#21521;&#27880;&#37322;&#21464;&#24471;&#38750;&#24120;&#36153;&#21147;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#29616;&#26377;&#30340;&#27700;&#24179;&#27880;&#37322;&#25968;&#25454;&#38598;&#24182;&#38477;&#20302;&#27880;&#37322;&#25104;&#26412;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#26816;&#27979;&#22120;H2RBox&#65292;&#29992;&#20110;&#20174;&#27700;&#24179;&#26694;Box&#20013;&#23398;&#20064;&#26059;&#36716;&#26694;RBox&#65292;&#24182;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;H2RBox-v2&#30340;&#26032;&#29256;&#26412;&#65292;&#20197;&#36827;&#19968;&#27493;&#24357;&#21512;HBox&#30417;&#30563;&#21644;RBox&#30417;&#30563;&#30340;&#26377;&#21521;&#29289;&#20307;&#26816;&#27979;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#21033;&#29992;&#32763;&#36716;&#21644;&#26059;&#36716;&#19968;&#33268;&#24615;&#26469;&#24320;&#21457;&#36724;&#23545;&#31216;&#24615;&#26159;&#21487;&#34892;&#30340;&#65292;H2RBox-v2&#21017;&#37319;&#29992;&#19982;H2RBox&#31867;&#20284;&#30340;&#24369;&#30417;&#30563;&#20998;&#25903;&#65292;&#24182;&#23884;&#20837;&#19968;&#20010;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#20998;&#25903;&#65292;&#23427;&#21487;&#20197;&#20174;&#23545;&#35937;&#22270;&#20687;&#20013;&#22266;&#26377;&#30340;&#23545;&#31216;&#24615;&#20013;&#23398;&#20064;&#26041;&#21521;&#12290;&#36890;&#36807;&#22788;&#29702;&#21608;&#36793;&#38382;&#39064;&#30340;&#27169;&#22359;&#65288;&#20363;&#22914;&#35282;&#21608;&#26399;&#24615;&#65289;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#31283;&#23450;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;H2RBox-v2&#26159;&#31532;&#19968;&#20010;&#23558;&#23545;&#31216;&#23398;&#20064;&#24212;&#29992;&#20110;&#22522;&#20110;HBox&#30417;&#30563;&#30340;&#26377;&#21521;&#29289;&#20307;&#26816;&#27979;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing demand for oriented object detection e.g. in autonomous driving and remote sensing, the oriented annotation has become a labor-intensive work. To make full use of existing horizontally annotated datasets and reduce the annotation cost, a weakly-supervised detector H2RBox for learning the rotated box (RBox) from the horizontal box (HBox) has been proposed and received great attention. This paper presents a new version, H2RBox-v2, to further bridge the gap between HBox-supervised and RBox-supervised oriented object detection. While exploiting axisymmetry via flipping and rotating consistencies is available through our theoretical analysis, H2RBox-v2, using a weakly-supervised branch similar to H2RBox, is embedded with a novel self-supervised branch that learns orientations from the symmetry inherent in the image of objects. Complemented by modules to cope with peripheral issues, e.g. angular periodicity, a stable and effective solution is achieved. To our knowledge, H
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Sat-NeRF&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#23569;&#37327;&#30340;&#21355;&#26143;&#22270;&#20687;&#38598;&#21512;&#20013;&#21512;&#25104;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#20934;&#30830;&#22320;&#20272;&#35745;&#22330;&#26223;&#34920;&#38754;&#30340;&#39640;&#31243;&#12290;</title><link>http://arxiv.org/abs/2304.04133</link><description>&lt;p&gt;
&#22522;&#20110;NeRF&#25216;&#26415;&#30340;&#21355;&#26143;&#22270;&#20687;&#34920;&#38754;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
NeRF applied to satellite imagery for surface reconstruction. (arXiv:2304.04133v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Sat-NeRF&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#23569;&#37327;&#30340;&#21355;&#26143;&#22270;&#20687;&#38598;&#21512;&#20013;&#21512;&#25104;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#20934;&#30830;&#22320;&#20272;&#35745;&#22330;&#26223;&#34920;&#38754;&#30340;&#39640;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Sat-NeRF&#27169;&#22411;&#65292;&#26159;&#23545;&#26368;&#36817;&#24341;&#20837;&#30340;S-NeRF&#27169;&#22411;&#30340;&#20462;&#25913;&#23454;&#29616;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#20174;&#31232;&#30095;&#30340;&#21355;&#26143;&#22270;&#20687;&#38598;&#21512;&#20013;&#21512;&#25104;&#26032;&#30340;&#35270;&#35282;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#22270;&#29255;&#20013;&#30340;&#20809;&#29031;&#21464;&#21270;&#12290;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#36824;&#33021;&#22815;&#31934;&#30830;&#22320;&#20272;&#35745;&#22330;&#26223;&#34920;&#38754;&#30340;&#39640;&#31243;&#65292;&#36825;&#23545;&#21355;&#26143;&#35266;&#27979;&#24212;&#29992;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;S-NeRF&#26041;&#27861;&#25913;&#36827;&#20102;&#26631;&#20934;&#30340;NeRF&#26041;&#27861;&#65292;&#23558;&#36752;&#23556;&#24378;&#24230;&#32771;&#34385;&#20026;&#39640;&#21453;&#23556;&#29575;&#21644;&#20837;&#23556;&#36752;&#29031;&#24230;&#30340;&#20989;&#25968;&#12290;&#36825;&#20004;&#20010;&#37327;&#37117;&#26159;&#27169;&#22411;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#26525;&#26465;&#30340;&#36755;&#20986;&#65292;&#32780;&#21518;&#32773;&#21017;&#34987;&#35270;&#20026;&#26469;&#33258;&#22826;&#38451;&#30340;&#30452;&#25509;&#20809;&#32447;&#21644;&#26469;&#33258;&#22825;&#31354;&#30340;&#28459;&#21453;&#23556;&#39068;&#33394;&#20989;&#25968;&#12290;&#35813;&#23454;&#29616;&#22522;&#20110;&#29992;&#32553;&#25918;-&#35009;&#21098;&#25216;&#26415;&#22686;&#24378;&#30340;&#21355;&#26143;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#23545;NeRF&#36827;&#34892;&#20102;&#36229;&#21442;&#25968;&#30740;&#31350;&#65292;&#24471;&#20986;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Sat-NeRF, a modified implementation of the recently introduced Shadow Neural Radiance Field (S-NeRF) model. This method is able to synthesize novel views from a sparse set of satellite images of a scene, while accounting for the variation in lighting present in the pictures. The trained model can also be used to accurately estimate the surface elevation of the scene, which is often a desirable quantity for satellite observation applications. S-NeRF improves on the standard Neural Radiance Field (NeRF) method by considering the radiance as a function of the albedo and the irradiance. Both these quantities are output by fully connected neural network branches of the model, and the latter is considered as a function of the direct light from the sun and the diffuse color from the sky. The implementations were run on a dataset of satellite images, augmented using a zoom-and-crop technique. A hyperparameter study for NeRF was carried out, leading to intriguing observations on the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24635;&#30456;&#20851;&#24615;&#30340;&#29983;&#25104;&#27169;&#22411;TC-VAE&#65292;&#21487;&#20197;&#25581;&#31034;&#25968;&#25454;&#29983;&#25104;&#22240;&#32032;&#20013;&#30340;&#26410;&#30693;&#20998;&#24067;&#25968;&#25454;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#19981;&#24179;&#34913;&#29983;&#25104;&#22240;&#32032;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2304.04103</link><description>&lt;p&gt;
TC-VAE&#65306;&#25581;&#31034;&#25968;&#25454;&#29983;&#25104;&#22240;&#32032;&#20013;&#30340;&#26410;&#30693;&#20998;&#24067;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
TC-VAE: Uncovering Out-of-Distribution Data Generative Factors. (arXiv:2304.04103v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24635;&#30456;&#20851;&#24615;&#30340;&#29983;&#25104;&#27169;&#22411;TC-VAE&#65292;&#21487;&#20197;&#25581;&#31034;&#25968;&#25454;&#29983;&#25104;&#22240;&#32032;&#20013;&#30340;&#26410;&#30693;&#20998;&#24067;&#25968;&#25454;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#19981;&#24179;&#34913;&#29983;&#25104;&#22240;&#32032;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;&#25968;&#25454;&#29983;&#25104;&#22240;&#32032;&#26159;&#35299;&#20915;&#35299;&#32544;&#32467;&#23398;&#20064;&#30340;&#26368;&#32456;&#30446;&#26631;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;-TC-VAE&#65292;&#23427;&#21487;&#20197;&#22522;&#20110;&#25152;&#23398;&#30340;&#28508;&#22312;&#34920;&#24449;&#21644;&#36755;&#20837;&#25968;&#25454;&#20043;&#38388;&#30340;&#24635;&#30456;&#20851;&#24615;&#19979;&#30028;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#21457;&#29616;&#19981;&#22312;&#25968;&#25454;&#38598;&#20013;&#26174;&#24335;&#20986;&#29616;&#30340;&#21464;&#21270;&#22240;&#32032;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#20351;&#29992;&#20855;&#26377;&#19981;&#24179;&#34913;&#30340;&#29983;&#25104;&#22240;&#32032;&#25968;&#25454;&#38598;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#23450;&#37327;&#21644;&#23450;&#24615;&#23454;&#39564;&#20013;&#34920;&#26126;&#20102;TC-VAE&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncovering data generative factors is the ultimate goal of disentanglement learning. Although many works proposed disentangling generative models able to uncover the underlying generative factors of a dataset, so far no one was able to uncover OOD generative factors (i.e., factors of variations that are not explicitly shown on the dataset). Moreover, the datasets used to validate these models are synthetically generated using a balanced mixture of some predefined generative factors, implicitly assuming that generative factors are uniformly distributed across the datasets. However, real datasets do not present this property. In this work we analyse the effect of using datasets with unbalanced generative factors, providing qualitative and quantitative results for widely used generative models. Moreover, we propose TC-VAE, a generative model optimized using a lower bound of the joint total correlation between the learned latent representations and the input data. We show that the proposed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#37051;&#25509;&#30697;&#38453;&#65292;&#23427;&#21253;&#25324;&#20102;&#29992;&#25143;-&#29992;&#25143;&#21644;&#39033;&#30446;-&#39033;&#30446;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#19968;&#20010;&#32463;&#36807;&#36866;&#24403;&#35774;&#35745;&#30340;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30697;&#38453;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;top-K&#37319;&#26679;&#22686;&#24378;&#20102;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30697;&#38453;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#25152;&#26377;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2304.03344</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#30340;&#22270;&#21327;&#20316;&#20449;&#21495;&#21435;&#22122;&#19982;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Graph Collaborative Signals Denoising and Augmentation for Recommendation. (arXiv:2304.03344v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#37051;&#25509;&#30697;&#38453;&#65292;&#23427;&#21253;&#25324;&#20102;&#29992;&#25143;-&#29992;&#25143;&#21644;&#39033;&#30446;-&#39033;&#30446;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#19968;&#20010;&#32463;&#36807;&#36866;&#24403;&#35774;&#35745;&#30340;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30697;&#38453;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;top-K&#37319;&#26679;&#22686;&#24378;&#20102;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30697;&#38453;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#25152;&#26377;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21327;&#20316;&#36807;&#28388;&#65288;GCF&#65289;&#26159;&#25429;&#25417;&#25512;&#33616;&#31995;&#32479;&#20013;&#39640;&#38454;&#21327;&#21516;&#20449;&#21495;&#30340;&#27969;&#34892;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;GCF&#30340;&#21452;&#21521;&#37051;&#25509;&#30697;&#38453;&#65292;&#20854;&#23450;&#20041;&#20102;&#22522;&#20110;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#36827;&#34892;&#32858;&#21512;&#30340;&#37051;&#23621;&#65292;&#23545;&#20110;&#26377;&#22823;&#37327;&#20132;&#20114;&#20294;&#19981;&#36275;&#30340;&#29992;&#25143;/&#39033;&#30446;&#26469;&#35828;&#21487;&#33021;&#26159;&#22024;&#26434;&#30340;&#12290;&#27492;&#22806;&#65292;&#37051;&#25509;&#30697;&#38453;&#24573;&#30053;&#20102;&#29992;&#25143;-&#29992;&#25143;&#21644;&#39033;&#30446;-&#39033;&#30446;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#32858;&#21512;&#30340;&#26377;&#30410;&#37051;&#23621;&#30340;&#33539;&#22260;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#37051;&#25509;&#30697;&#38453;&#65292;&#23427;&#21253;&#25324;&#20102;&#29992;&#25143;-&#29992;&#25143;&#21644;&#39033;&#30446;-&#39033;&#30446;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#19968;&#20010;&#32463;&#36807;&#36866;&#24403;&#35774;&#35745;&#30340;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30697;&#38453;&#65292;&#20197;&#24179;&#34913;&#25152;&#26377;&#29992;&#25143;&#20043;&#38388;&#30340;&#20132;&#20114;&#25968;&#37327;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#39044;&#20808;&#35757;&#32451;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#26041;&#27861;&#26469;&#33719;&#24471;&#29992;&#25143;/&#39033;&#30446;&#23884;&#20837;&#65292;&#28982;&#21518;&#36890;&#36807;top-K&#37319;&#26679;&#22686;&#24378;&#20102;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30697;&#38453;&#12290;&#25105;&#20204;&#36824;&#22686;&#24378;&#20102;&#23545;&#31216;&#30340;&#29992;&#25143;-&#29992;&#25143;&#21644;&#39033;&#30446;-&#39033;&#30446;&#30456;&#20851;&#32452;&#20214;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#25152;&#26377;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph collaborative filtering (GCF) is a popular technique for capturing high-order collaborative signals in recommendation systems. However, GCF's bipartite adjacency matrix, which defines the neighbors being aggregated based on user-item interactions, can be noisy for users/items with abundant interactions and insufficient for users/items with scarce interactions. Additionally, the adjacency matrix ignores user-user and item-item correlations, which can limit the scope of beneficial neighbors being aggregated.  In this work, we propose a new graph adjacency matrix that incorporates user-user and item-item correlations, as well as a properly designed user-item interaction matrix that balances the number of interactions across all users. To achieve this, we pre-train a graph-based recommendation method to obtain users/items embeddings, and then enhance the user-item interaction matrix via top-K sampling. We also augment the symmetric user-user and item-item correlation components to th
&lt;/p&gt;</description></item><item><title>BOTTRINET&#22522;&#20110;&#25991;&#26412;&#20869;&#23481;&#26816;&#27979;&#26426;&#22120;&#20154;&#65292;&#24182;&#35774;&#35745;&#20102;&#19977;&#20803;&#32452;&#32593;&#32476;&#20197;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;CRESCI2017&#19978;&#65292;&#31995;&#32479;&#34920;&#29616;&#26368;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.03144</link><description>&lt;p&gt;
BotTriNet: &#19968;&#31181;&#22522;&#20110;&#24230;&#37327;&#23398;&#20064;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#26816;&#27979;&#32479;&#19968;&#39640;&#25928;&#30340;&#23884;&#20837;&#24335;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
BotTriNet: A Unified and Efficient Embedding for Social Bots Detection via Metric Learning. (arXiv:2304.03144v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03144
&lt;/p&gt;
&lt;p&gt;
BOTTRINET&#22522;&#20110;&#25991;&#26412;&#20869;&#23481;&#26816;&#27979;&#26426;&#22120;&#20154;&#65292;&#24182;&#35774;&#35745;&#20102;&#19977;&#20803;&#32452;&#32593;&#32476;&#20197;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;CRESCI2017&#19978;&#65292;&#31995;&#32479;&#34920;&#29616;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#65292;&#24555;&#36895;&#20934;&#30830;&#22320;&#21457;&#29616;&#26426;&#22120;&#20154;&#36134;&#25143;&#20197;&#38450;&#27490;&#23427;&#20204;&#20405;&#29359;&#21644;&#39578;&#25200;&#30495;&#23454;&#29992;&#25143;&#26159;&#19968;&#20010;&#25345;&#20037;&#21463;&#27426;&#36814;&#30340;&#35805;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20316;BOTTRINET&#30340;&#32479;&#19968;&#23884;&#20837;&#24335;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#36134;&#25143;&#21457;&#24067;&#30340;&#25991;&#26412;&#20869;&#23481;&#26816;&#27979;&#26426;&#22120;&#20154;&#65292;&#22522;&#20110;&#30340;&#20551;&#35774;&#26159;&#19978;&#19979;&#25991;&#33258;&#28982;&#22320;&#25581;&#31034;&#36134;&#25143;&#20010;&#24615;&#21644;&#20064;&#24815;&#12290;&#22914;&#26524;&#31995;&#32479;&#33021;&#22815;&#20351;&#29992;&#23884;&#20837;&#25216;&#26415;&#26377;&#25928;&#22320;&#25552;&#21462;&#19982;&#26426;&#22120;&#20154;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#37027;&#20040;&#20869;&#23481;&#23601;&#26159;&#20016;&#23500;&#21644;&#26377;&#20215;&#20540;&#30340;&#12290;&#38500;&#20102;&#29983;&#25104;&#35789;&#12289;&#21477;&#21644;&#36134;&#25143;&#23884;&#20837;&#30340;&#19968;&#33324;&#23884;&#20837;&#24335;&#26694;&#26550;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#19977;&#20803;&#32452;&#32593;&#32476;&#26469;&#35843;&#25972;&#21407;&#22987;&#23884;&#20837;&#65288;&#30001;&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#29983;&#25104;&#65289;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;CRESCI2017&#19978;&#35780;&#20272;&#20102;&#26816;&#27979;&#20934;&#30830;&#24615;&#21644;F1&#24471;&#20998;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#19977;&#20010;&#26426;&#22120;&#20154;&#36134;&#25143;&#31867;&#21035;&#21644;&#20116;&#20010;&#26426;&#22120;&#20154;&#26679;&#26412;&#38598;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#20004;&#20010;&#20869;&#23481;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#24179;&#22343;&#20934;&#30830;&#24615;98.34%&#21644;F1&#24471;&#20998;97.99%&#12290;
&lt;/p&gt;
&lt;p&gt;
A persistently popular topic in online social networks is the rapid and accurate discovery of bot accounts to prevent their invasion and harassment of genuine users. We propose a unified embedding framework called BOTTRINET, which utilizes textual content posted by accounts for bot detection based on the assumption that contexts naturally reveal account personalities and habits. Content is abundant and valuable if the system efficiently extracts bot-related information using embedding techniques. Beyond the general embedding framework that generates word, sentence, and account embeddings, we design a triplet network to tune the raw embeddings (produced by traditional natural language processing techniques) for better classification performance. We evaluate detection accuracy and f1score on a real-world dataset CRESCI2017, comprising three bot account categories and five bot sample sets. Our system achieves the highest average accuracy of 98.34% and f1score of 97.99% on two content-inte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#31070;&#32463;&#21457;&#29983;&#12289;&#31070;&#32463;&#36798;&#23572;&#25991;&#20027;&#20041;&#21644;&#29289;&#31181;&#36827;&#21270;&#22914;&#20309;&#21551;&#21457;&#28436;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21019;&#20316;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;DNNs&#28436;&#21270;&#20013;dropout&#26041;&#27861;&#19982;&#31070;&#32463;&#21457;&#29983;&#30340;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2304.03122</link><description>&lt;p&gt;
&#31070;&#32463;&#21457;&#29983;&#12289;&#31070;&#32463;&#36798;&#23572;&#25991;&#20027;&#20041;&#21644;&#29289;&#31181;&#36827;&#21270;&#21487;&#21542;&#20316;&#20026;&#28436;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21019;&#24314;&#30340;&#28789;&#24863;&#26469;&#28304;?
&lt;/p&gt;
&lt;p&gt;
Is it conceivable that neurogenesis, neural Darwinism, and species evolution could all serve as inspiration for the creation of evolutionary deep neural networks?. (arXiv:2304.03122v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#31070;&#32463;&#21457;&#29983;&#12289;&#31070;&#32463;&#36798;&#23572;&#25991;&#20027;&#20041;&#21644;&#29289;&#31181;&#36827;&#21270;&#22914;&#20309;&#21551;&#21457;&#28436;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21019;&#20316;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;DNNs&#28436;&#21270;&#20013;dropout&#26041;&#27861;&#19982;&#31070;&#32463;&#21457;&#29983;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#26159;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#30340;&#65292;&#26159;&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;DNNs&#20027;&#35201;&#26159;&#25163;&#24037;&#26500;&#24314;&#30340;&#65292;&#36890;&#24120;&#21253;&#21547;&#22823;&#37327;&#30340;&#23618;&#25968;&#12290;&#26412;&#25991;&#24378;&#35843;&#20102;&#25105;&#20204;&#25152;&#35828;&#30340;&#20108;&#32500;&#33041;&#37096;&#36827;&#21270;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#23427;&#22914;&#20309;&#21551;&#21457;&#20108;&#32500;DNN&#28436;&#21270;&#24314;&#27169;&#12290;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#22312;DNNs&#27491;&#21017;&#21270;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;dropout&#26041;&#27861;&#19982;&#22823;&#33041;&#31070;&#32463;&#21457;&#29983;&#30340;&#32852;&#31995;&#65292;&#20197;&#21450;&#36825;&#20123;&#27010;&#24565;&#22914;&#20309;&#26377;&#30410;&#20110;DNNs&#30340;&#28436;&#21270;&#12290;&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;&#20960;&#20010;&#22686;&#24378;&#33258;&#21160;&#26500;&#24314;DNNs&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) are built using artificial neural networks. They are part of machine learning methods that are capable of learning from data that have been used in a wide range of applications. DNNs are mainly handcrafted and they usually contain numerous layers. Research frontier has emerged that concerns automated construction of DNNs via evolutionary algorithms. This paper emphasizes the importance of what we call two-dimensional brain evolution and how it can inspire two dimensional DNN evolutionary modeling. We also highlight the connection between the dropout method which is widely-used in regularizing DNNs and neurogenesis of the brain, and how these concepts could benefit DNNs evolution.The paper concludes with several recommendations for enhancing the automatic construction of DNNs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35266;&#23519;&#21040;&#22522;&#20110;DPR&#30340;&#26368;&#36817;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#32463;&#24120;&#23558;&#26080;&#27861;&#22238;&#31572;&#30340;&#21453;&#20107;&#23454;&#24773;&#26223;&#25490;&#21517;&#39640;&#20110;&#21487;&#22238;&#31572;&#30340;&#21407;&#22987;&#24773;&#26223;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#27573;&#33853;&#26816;&#32034;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;PiCL&#12290;</title><link>http://arxiv.org/abs/2304.03031</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#24102;&#26377;&#26080;&#27861;&#22238;&#31572;&#30340;&#21453;&#20107;&#23454;&#24773;&#26223;&#30340;&#23494;&#38598;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Revisiting Dense Retrieval with Unanswerable Counterfactuals. (arXiv:2304.03031v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35266;&#23519;&#21040;&#22522;&#20110;DPR&#30340;&#26368;&#36817;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#32463;&#24120;&#23558;&#26080;&#27861;&#22238;&#31572;&#30340;&#21453;&#20107;&#23454;&#24773;&#26223;&#25490;&#21517;&#39640;&#20110;&#21487;&#22238;&#31572;&#30340;&#21407;&#22987;&#24773;&#26223;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#27573;&#33853;&#26816;&#32034;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;PiCL&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#65288;ODQA&#65289;&#20013;&#65292;&#26816;&#32034;&#22120;-&#38405;&#35835;&#22120;&#26694;&#26550;&#24456;&#21463;&#27426;&#36814;&#65292;&#20854;&#20013;&#26816;&#32034;&#22120;&#20174;&#22823;&#22411;&#35821;&#26009;&#24211;&#20013;&#20026;&#38405;&#35835;&#22120;&#25277;&#21462;&#19968;&#32452;&#30456;&#20851;&#30340;&#20505;&#36873;&#27573;&#33853;&#12290;&#36825;&#31181;&#26041;&#27861;&#32972;&#21518;&#30340;&#19968;&#20010;&#20851;&#38190;&#20551;&#35774;&#26159;&#65292;&#20174;&#26816;&#32034;&#22120;&#24471;&#21040;&#30340;&#39640;&#30456;&#20851;&#24615;&#20998;&#25968;&#21487;&#33021;&#34920;&#26126;&#20174;&#38405;&#35835;&#22120;&#33719;&#21462;&#31572;&#26696;&#30340;&#21487;&#33021;&#24615;&#24456;&#39640;&#65292;&#36825;&#24847;&#21619;&#30528;&#26816;&#32034;&#21040;&#30340;&#27573;&#33853;&#24456;&#21487;&#33021;&#21253;&#21547;&#32473;&#23450;&#38382;&#39064;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#22312;&#26412;&#30740;&#31350;&#20013;&#23454;&#35777;&#39539;&#26021;&#20102;&#36825;&#31181;&#35266;&#28857;&#65292;&#24182;&#35266;&#23519;&#21040;&#22522;&#20110;DPR&#30340;&#26368;&#36817;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#32463;&#24120;&#23558;&#26080;&#27861;&#22238;&#31572;&#30340;&#21453;&#20107;&#23454;&#24773;&#26223;&#25490;&#21517;&#39640;&#20110;&#21487;&#22238;&#31572;&#30340;&#21407;&#22987;&#24773;&#26223;&#12290;&#20026;&#20102;&#35299;&#20915;&#23494;&#38598;&#26816;&#32034;&#20013;&#36825;&#31181;&#23545;&#31572;&#26696;&#26080;&#24863;&#30693;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#23547;&#27714;&#20351;&#29992;&#21453;&#20107;&#23454;&#26679;&#26412;&#20316;&#20026;&#39069;&#22806;&#30340;&#35757;&#32451;&#36164;&#28304;&#65292;&#20197;&#26356;&#22909;&#22320;&#21516;&#27493;DPR&#30340;&#30456;&#20851;&#24615;&#27979;&#37327;&#21644;&#38382;&#39064;-&#27573;&#33853;&#23545;&#30340;&#21487;&#31572;&#24615;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21453;&#20107;&#23454;Pivoting&#23545;&#27604;&#23398;&#20064;&#65288;PiCL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#27573;&#33853;&#26816;&#32034;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The retriever-reader framework is popular for open-domain question answering (ODQA), where a retriever samples for the reader a set of relevant candidate passages from a large corpus. A key assumption behind this method is that high relevance scores from the retriever likely indicate high answerability from the reader, which implies a high probability that the retrieved passages contain answers to a given question. In this work, we empirically dispel this belief and observe that recent dense retrieval models based on DPR often rank unanswerable counterfactual passages higher than their answerable original passages. To address such answer-unawareness in dense retrievers, we seek to use counterfactual samples as additional training resources to better synchronize the relevance measurement of DPR with the answerability of question-passage pairs. Specifically, we present counterfactually-Pivoting Contrastive Learning (PiCL), a novel representation learning approach for passage retrieval th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#22810;&#27425;&#24314;&#31435;&#21644;&#20998;&#26512;AutoRL&#36229;&#21442;&#25968;&#30340;&#26223;&#35266;&#65292;&#35777;&#26126;&#20195;&#34920;&#31639;&#27861;&#65288;DQN&#21644;SAC&#65289;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#36229;&#21442;&#25968;&#26223;&#35266;&#20250;&#38543;&#26102;&#38388;&#32780;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.02396</link><description>&lt;p&gt;
AutoRL&#36229;&#21442;&#25968;&#26223;&#35266;
&lt;/p&gt;
&lt;p&gt;
AutoRL Hyperparameter Landscapes. (arXiv:2304.02396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#22810;&#27425;&#24314;&#31435;&#21644;&#20998;&#26512;AutoRL&#36229;&#21442;&#25968;&#30340;&#26223;&#35266;&#65292;&#35777;&#26126;&#20195;&#34920;&#31639;&#27861;&#65288;DQN&#21644;SAC&#65289;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#36229;&#21442;&#25968;&#26223;&#35266;&#20250;&#38543;&#26102;&#38388;&#32780;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#21462;&#24471;&#20196;&#20154;&#30633;&#30446;&#25104;&#26524;&#30340;&#21516;&#26102;&#65292;&#20854;&#36229;&#21442;&#25968;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#33539;&#22260;&#12290;&#36825;&#32463;&#24120;&#20351;&#24471;&#22312;&#23454;&#36341;&#20013;&#38590;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#33258;&#21160;&#21270;RL&#65288;AutoRL&#65289;&#35299;&#20915;&#20102;&#36825;&#20010;&#38590;&#39064;&#65292;&#20294;&#26377;&#20851;&#36229;&#21442;&#25968;&#20248;&#21270;&#65288;HPO&#65289;&#26041;&#27861;&#22312;&#25628;&#32034;&#26368;&#20339;&#37197;&#32622;&#26102;&#25152;&#36941;&#21382;&#30340;&#36229;&#21442;&#25968;&#26223;&#35266;&#21160;&#24577;&#21464;&#21270;&#30340;&#20449;&#24687;&#24456;&#23569;&#12290;&#37492;&#20110;&#29616;&#26377;AutoRL&#26041;&#27861;&#21160;&#24577;&#35843;&#25972;&#36229;&#21442;&#25968;&#37197;&#32622;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#19981;&#20165;&#22312;&#19968;&#20010;&#26102;&#38388;&#28857;&#65292;&#32780;&#19988;&#22312;&#22810;&#20010;&#26102;&#38388;&#28857;&#19978;&#24314;&#31435;&#21644;&#20998;&#26512;&#36825;&#20123;&#36229;&#21442;&#25968;&#26223;&#35266;&#12290;&#38024;&#23545;&#20851;&#20110;&#36825;&#31181;&#21160;&#24577;AutoRL&#26041;&#27861;&#21512;&#27861;&#24615;&#30340;&#19968;&#20010;&#37325;&#35201;&#24320;&#25918;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20805;&#20998;&#30340;&#35777;&#25454;&#65292;&#34920;&#26126;&#22312;&#19981;&#21516;&#31181;&#31867;&#30340;&#29615;&#22659;&#65288;Cartpole&#21644;Pendulum&#65289;&#20013;&#65292;&#26469;&#33258;RL&#25991;&#29486;&#30340;&#20195;&#34920;&#31639;&#27861;&#65288;DQN&#21644;SAC&#65289;&#30340;&#36229;&#21442;&#25968;&#26223;&#35266;&#20250;&#38543;&#26102;&#38388;&#32780;&#24378;&#28872;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Reinforcement Learning (RL) has shown to be capable of producing impressive results, its use is limited by the impact of its hyperparameters on performance. This often makes it difficult to achieve good results in practice. Automated RL (AutoRL) addresses this difficulty, yet little is known about the dynamics of the hyperparameter landscapes that hyperparameter optimization (HPO) methods traverse in search of optimal configurations. In view of existing AutoRL approaches dynamically adjusting hyperparameter configurations, we propose an approach to build and analyze these hyperparameter landscapes not just for one point in time but at multiple points in time throughout training. Addressing an important open question on the legitimacy of such dynamic AutoRL approaches, we provide thorough empirical evidence that the hyperparameter landscapes strongly vary over time across representative algorithms from RL literature (DQN and SAC) in different kinds of environments (Cartpole and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#8212;&#8212;&#32467;&#26500;&#21270;&#20449;&#24687;&#25512;&#29702;&#65288;SIS&#65289;&#65292;&#21033;&#29992;GPT-3&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#25552;&#21462;&#26448;&#26009;&#31185;&#23398;&#35774;&#22791;&#23618;&#38754;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39044;&#27979;PCE&#21644;&#21453;&#21521;&#39044;&#27979;&#21442;&#25968;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26448;&#26009;&#23398;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.02213</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38053;&#21273;&#65306;&#29992;GPT&#35299;&#23494;&#26448;&#26009;&#31185;&#23398;&#30340;&#31192;&#23494;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Master Key: Unlocking the Secrets of Materials Science with GPT. (arXiv:2304.02213v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#8212;&#8212;&#32467;&#26500;&#21270;&#20449;&#24687;&#25512;&#29702;&#65288;SIS&#65289;&#65292;&#21033;&#29992;GPT-3&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#25552;&#21462;&#26448;&#26009;&#31185;&#23398;&#35774;&#22791;&#23618;&#38754;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39044;&#27979;PCE&#21644;&#21453;&#21521;&#39044;&#27979;&#21442;&#25968;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26448;&#26009;&#23398;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#8212;&#8212;&#32467;&#26500;&#21270;&#20449;&#24687;&#25512;&#29702;&#65288;SIS&#65289;&#65292;&#20197;&#35299;&#20915;&#26448;&#26009;&#31185;&#23398;&#35774;&#22791;&#23618;&#38754;&#20449;&#24687;&#25552;&#21462;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#29616;&#26377;&#30340;&#38041;&#38043;&#30719;&#22826;&#38451;&#33021;&#30005;&#27744;FAIR&#25968;&#25454;&#38598;&#23545;GPT-3&#36827;&#34892;&#24494;&#35843;&#65292;&#33719;&#24471;&#20102;91.8 F1&#24471;&#20998;&#65292;&#24182;&#26356;&#26032;&#20102;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36804;&#20170;&#20026;&#27490;&#25152;&#26377;&#30456;&#20851;&#31185;&#23398;&#35770;&#25991;&#12290;&#25152;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#24050;&#34987;&#26684;&#24335;&#21270;&#21644;&#26631;&#20934;&#21270;&#65292;&#20351;&#24471;&#23427;&#21487;&#20197;&#30452;&#25509;&#20316;&#20026;&#21518;&#32493;&#25968;&#25454;&#20998;&#26512;&#30340;&#36755;&#20837;&#12290;&#36825;&#20010;&#29305;&#24615;&#23558;&#20351;&#26448;&#26009;&#31185;&#23398;&#23478;&#36890;&#36807;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#39046;&#22495;&#35780;&#35770;&#25991;&#31456;&#26469;&#24320;&#21457;&#20854;&#33258;&#24049;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#23454;&#39564;&#26469;&#39044;&#27979;PCE&#21644;&#21453;&#21521;&#39044;&#27979;&#21442;&#25968;&#65292;&#24182;&#33719;&#24471;&#20102;&#19982;DFT&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#36825;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20687;&#26448;&#26009;&#23398;&#23478;&#19968;&#26679;&#35780;&#21028;&#26448;&#26009;&#21644;&#35774;&#35745;&#26032;&#26448;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents a new NLP task called structured information inference (SIS) to address the complexities of information extraction at the device level in materials science. We accomplished this task by finetuning GPT-3 on a exsiting perovskite solar cell FAIR dataset with 91.8 F1-score and we updated the dataset with all related scientific papers up to now. The produced dataset is formatted and normalized, enabling its direct utilization as input in subsequent data analysis. This feature will enable materials scientists to develop their own models by selecting high-quality review papers within their domain. Furthermore, we designed experiments to predict PCE and reverse-predict parameters and obtained comparable performance with DFT, which demonstrates the potential of large language models to judge materials and design new materials like a materials scientist.
&lt;/p&gt;</description></item><item><title>EGC&#26159;&#19968;&#31181;&#20351;&#29992;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#21331;&#36234;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36739;&#22909;&#22320;&#29983;&#25104;&#20986;&#39640;&#36136;&#37327;&#22270;&#20687;&#65292;&#24182;&#22312;&#22810;&#39033;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#39046;&#20808;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.02012</link><description>&lt;p&gt;
EGC: &#19968;&#31181;&#36890;&#36807;&#21333;&#19968;&#33021;&#37327;&#27169;&#22411;&#29983;&#25104;&#19982;&#20998;&#31867;&#22270;&#20687;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EGC: Image Generation and Classification via a Single Energy-Based Model. (arXiv:2304.02012v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02012
&lt;/p&gt;
&lt;p&gt;
EGC&#26159;&#19968;&#31181;&#20351;&#29992;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#21331;&#36234;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36739;&#22909;&#22320;&#29983;&#25104;&#20986;&#39640;&#36136;&#37327;&#22270;&#20687;&#65292;&#24182;&#22312;&#22810;&#39033;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#39046;&#20808;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#30456;&#21516;&#30340;&#32593;&#32476;&#21442;&#25968;&#23398;&#20064;&#22270;&#20687;&#20998;&#31867;&#21644;&#29983;&#25104;&#22270;&#20687;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#20808;&#36827;&#26041;&#27861;&#22312;&#19968;&#39033;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#21478;&#19968;&#39033;&#20219;&#21153;&#19978;&#21364;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;EGC&#30340;&#22522;&#20110;&#33021;&#37327;&#30340;&#20998;&#31867;&#22120;&#21644;&#29983;&#25104;&#22120;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#22312;&#20004;&#20010;&#20219;&#21153;&#20013;&#23454;&#29616;&#21331;&#36234;&#24615;&#33021;&#12290;&#19982;&#20256;&#32479;&#30340;&#20998;&#31867;&#22120;&#36755;&#20986;&#32473;&#23450;&#22270;&#20687;&#30340;&#26631;&#31614;&#65288;&#21363;&#26465;&#20214;&#20998;&#24067;$p(y|\mathbf{x})$&#65289;&#19981;&#21516;&#65292;EGC&#30340;&#21069;&#21521;&#20256;&#36882;&#22120;&#26159;&#19968;&#20010;&#20998;&#31867;&#22120;&#65292;&#23427;&#36755;&#20986;&#19968;&#20010;&#32852;&#21512;&#20998;&#24067;$p(\mathbf{x},y)$&#65292;&#22312;&#21518;&#21521;&#20256;&#36882;&#22120;&#20013;&#36890;&#36807;&#36793;&#32536;&#21270;&#26631;&#31614;$y$&#23454;&#29616;&#29983;&#25104;&#22120;&#12290;&#22312;&#21069;&#21521;&#20256;&#36882;&#20013;&#65292;&#20272;&#35745;&#32473;&#23450;&#22122;&#22768;&#22270;&#20687;&#30340;&#33021;&#37327;&#21644;&#20998;&#31867;&#27010;&#29575;&#65292;&#32780;&#22312;&#21518;&#21521;&#20256;&#36882;&#20013;&#65292;&#36890;&#36807;&#20272;&#35745;&#24471;&#20998;&#20989;&#25968;&#23545;&#20854;&#36827;&#34892;&#21435;&#22122;&#12290;EGC&#22312;ImageNet-1k&#12289;CelebA-HQ&#21644;LSUN Church&#19978;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#29983;&#25104;&#32467;&#26524;&#65292;&#21516;&#26102;&#22312;CIFAR-10&#12289;CIFAR-100&#21644;ImageNet-1k&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning image classification and image generation using the same set of network parameters is a challenging problem. Recent advanced approaches perform well in one task often exhibit poor performance in the other. This work introduces an energy-based classifier and generator, namely EGC, which can achieve superior performance in both tasks using a single neural network. Unlike a conventional classifier that outputs a label given an image (i.e., a conditional distribution $p(y|\mathbf{x})$), the forward pass in EGC is a classifier that outputs a joint distribution $p(\mathbf{x},y)$, enabling an image generator in its backward pass by marginalizing out the label $y$. This is done by estimating the energy and classification probability given a noisy image in the forward pass, while denoising it using the score function estimated in the backward pass. EGC achieves competitive generation results compared with state-of-the-art approaches on ImageNet-1k, CelebA-HQ and LSUN Church, while achi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;</title><link>http://arxiv.org/abs/2303.18223</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Large Language Models. (arXiv:2303.18223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#30001;&#35821;&#27861;&#35268;&#21017;&#25511;&#21046;&#30340;&#22797;&#26434;&#31934;&#32454;&#30340;&#20154;&#31867;&#34920;&#36798;&#31995;&#32479;&#65292;&#23545;&#20110;&#24320;&#21457;&#29702;&#35299;&#21644;&#25484;&#25569;&#35821;&#35328;&#30340;&#33021;&#21147;&#30340;AI&#31639;&#27861;&#26469;&#35828;&#26159;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#12290;&#20316;&#20026;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#65292;&#35821;&#35328;&#24314;&#27169;&#22312;&#36807;&#21435;&#20108;&#21313;&#24180;&#37324;&#24191;&#27867;&#30740;&#31350;&#29992;&#20110;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#20174;&#32479;&#35745;&#35821;&#35328;&#27169;&#22411;&#28436;&#21270;&#20026;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#22312;&#35299;&#20915;&#21508;&#31181;NLP&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#27169;&#22411;&#32553;&#25918;&#21487;&#20197;&#23548;&#33268;&#24615;&#33021;&#25913;&#36827;&#65292;&#20182;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#35268;&#27169;&#26469;&#30740;&#31350;&#32553;&#25918;&#25928;&#24212;&#65292;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;&#21442;&#25968;&#35268;&#27169;&#36229;&#36807;&#19968;&#23450;&#27700;&#24179;&#26102;&#65292;&#36825;&#20123;&#25193;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19988;&#36824;&#26174;&#31034;&#20986;&#19968;&#20123;&#23567;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25152;&#27809;&#26377;&#30340;&#29305;&#27530;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale langu
&lt;/p&gt;</description></item><item><title>Queer in AI&#26159;&#19968;&#20010;&#22522;&#20110;&#31038;&#21306;&#21442;&#19982;&#30340;AI&#35774;&#35745;&#26696;&#20363;&#30740;&#31350;&#65292;&#36890;&#36807;&#25298;&#32477;&#31561;&#32423;&#21046;&#24230;&#32780;&#36873;&#25321;&#21435;&#20013;&#24515;&#21270;&#65292;&#22312;&#37239;&#20799;&#31038;&#32676;&#20869;&#37096;&#24314;&#31435;&#20102;&#25588;&#21161;&#21644;&#39033;&#30446;&#65292;&#21516;&#26102;&#21162;&#21147;&#25913;&#21464;&#37239;&#20799;&#31038;&#32676;&#22806;&#30340;&#21442;&#19982;&#32773;&#21644;&#26426;&#26500;&#12290;&#36890;&#36807;&#22521;&#32946;AI&#21442;&#19982;&#25991;&#21270;&#65292;&#27426;&#36814;&#21644;&#36171;&#26435;&#36793;&#32536;&#21270;&#21442;&#19982;&#32773;&#65292;&#20026;AI&#30340;&#21442;&#19982;&#24335;&#35774;&#35745;&#20570;&#20986;&#20102;&#26356;&#24191;&#27867;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2303.16972</link><description>&lt;p&gt;
Queer In AI:&#22522;&#20110;&#31038;&#21306;&#21442;&#19982;&#30340;AI&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Queer In AI: A Case Study in Community-Led Participatory AI. (arXiv:2303.16972v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16972
&lt;/p&gt;
&lt;p&gt;
Queer in AI&#26159;&#19968;&#20010;&#22522;&#20110;&#31038;&#21306;&#21442;&#19982;&#30340;AI&#35774;&#35745;&#26696;&#20363;&#30740;&#31350;&#65292;&#36890;&#36807;&#25298;&#32477;&#31561;&#32423;&#21046;&#24230;&#32780;&#36873;&#25321;&#21435;&#20013;&#24515;&#21270;&#65292;&#22312;&#37239;&#20799;&#31038;&#32676;&#20869;&#37096;&#24314;&#31435;&#20102;&#25588;&#21161;&#21644;&#39033;&#30446;&#65292;&#21516;&#26102;&#21162;&#21147;&#25913;&#21464;&#37239;&#20799;&#31038;&#32676;&#22806;&#30340;&#21442;&#19982;&#32773;&#21644;&#26426;&#26500;&#12290;&#36890;&#36807;&#22521;&#32946;AI&#21442;&#19982;&#25991;&#21270;&#65292;&#27426;&#36814;&#21644;&#36171;&#26435;&#36793;&#32536;&#21270;&#21442;&#19982;&#32773;&#65292;&#20026;AI&#30340;&#21442;&#19982;&#24335;&#35774;&#35745;&#20570;&#20986;&#20102;&#26356;&#24191;&#27867;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;Queer in AI&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#25506;&#35752;&#31038;&#21306;&#21442;&#19982;&#24335;AI&#35774;&#35745;&#30340;&#23454;&#36341;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#31038;&#21306;&#21442;&#19982;&#35774;&#35745;&#21644;&#20132;&#21449;&#24615;&#21407;&#21017;&#22914;&#20309;&#22312;&#22810;&#24180;&#37324;&#22312;&#36825;&#20010;&#31038;&#32676;&#20013;&#33804;&#33469;&#21644;&#22609;&#36896;&#20102;&#20854;&#39033;&#30446;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#35813;&#32452;&#32455;&#22312;&#27492;&#36807;&#31243;&#20013;&#38754;&#20020;&#30340;&#19981;&#21516;&#25361;&#25112;&#65292;&#23457;&#35270;&#20102;&#35813;&#32452;&#32455;&#22312;&#23454;&#29616;&#21442;&#19982;&#24615;&#19982;&#20132;&#21449;&#24615;&#21407;&#21017;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#24182;&#35780;&#20272;&#20102;&#35813;&#32452;&#32455;&#30340;&#24433;&#21709;&#12290;Queer in AI&#36890;&#36807;&#25298;&#32477;&#31561;&#32423;&#21046;&#24230;&#32780;&#36873;&#25321;&#21435;&#20013;&#24515;&#21270;&#65292;&#36890;&#36807;&#23558;&#25588;&#21161;&#21644;&#39033;&#30446;&#24314;&#35774;&#24314;&#31435;&#22312;&#37239;&#20799;&#31038;&#32676;&#20869;&#37096;&#12289;&#30001;&#37239;&#20799;&#31038;&#32676;&#20869;&#25104;&#21592;&#26469;&#36127;&#36131;&#30340;&#26041;&#24335;&#65292;&#20197;&#21450;&#21162;&#21147;&#25913;&#21464;&#37239;&#20799;&#31038;&#32676;&#22806;&#30340;&#21442;&#19982;&#32773;&#21644;&#26426;&#26500;&#65292;&#20026;&#21442;&#19982;&#24335;&#26041;&#27861;&#30340;&#20174;&#19994;&#32773;&#21644;&#29702;&#35770;&#23478;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#32463;&#39564;&#21644;&#35265;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25512;&#27979;&#20687;Queer in AI&#36825;&#26679;&#30340;&#31038;&#21306;&#22914;&#20309;&#36890;&#36807;&#22521;&#32946;AI&#30340;&#21442;&#19982;&#25991;&#21270;&#65292;&#27426;&#36814;&#21644;&#36171;&#26435;&#36793;&#32536;&#21270;&#30340;&#21442;&#19982;&#32773;&#65292;&#25209;&#35780;&#36139;&#30240;&#21644;&#21093;&#21066;&#24615;&#34920;&#36848;&#31561;&#26041;&#38754;&#65292;&#20026;AI&#30340;&#21442;&#19982;&#24335;&#35774;&#35745;&#20570;&#20986;&#20102;&#26356;&#24191;&#27867;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Queer in AI as a case study for community-led participatory design in AI. We examine how participatory design and intersectional tenets started and shaped this community's programs over the years. We discuss different challenges that emerged in the process, look at ways this organization has fallen short of operationalizing participatory and intersectional principles, and then assess the organization's impact. Queer in AI provides important lessons and insights for practitioners and theorists of participatory methods broadly through its rejection of hierarchy in favor of decentralization, success at building aid and programs by and for the queer community, and effort to change actors and institutions outside of the queer community. Finally, we theorize how communities like Queer in AI contribute to the participatory design in AI more broadly by fostering cultures of participation in AI, welcoming and empowering marginalized participants, critiquing poor or exploitative parti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;ChatGPT&#20195;&#34920;&#31038;&#21306;&#23545;&#22797;&#26434;&#31995;&#32479;&#39046;&#22495;&#30340;&#29702;&#35299;&#36827;&#34892;&#27010;&#36848;&#65292;ChatGPT&#23398;&#20064;&#20102;&#22823;&#37327;&#20114;&#32852;&#32593;&#25991;&#26412;&#25968;&#25454;&#24182;&#33021;&#22815;&#25552;&#20379;&#21453;&#26144;&#31038;&#21306;&#26222;&#36941;&#24847;&#35265;&#12289;&#35266;&#28857;&#21644;&#35821;&#35328;&#27169;&#24335;&#30340;&#31572;&#26696;&#65292;&#25506;&#35752;&#20102;&#22797;&#26434;&#31995;&#32479;&#39046;&#22495;&#30340;&#25945;&#23398;&#12289;&#23398;&#20064;&#21644;&#30740;&#31350;&#20027;&#39064;&#65292;&#35748;&#20026;ChatGPT&#26159;&#31038;&#21306;&#24605;&#24819;&#30340;&#19968;&#31181;&#26469;&#28304;&#30340;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2303.16870</link><description>&lt;p&gt;
&#31185;&#23398;&#38382;&#39064;&#65306;&#19982;ChatGPT&#25506;&#35752;&#22797;&#26434;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Questions of science: chatting with ChatGPT about complex systems. (arXiv:2303.16870v1 [physics.soc-ph] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16870
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;ChatGPT&#20195;&#34920;&#31038;&#21306;&#23545;&#22797;&#26434;&#31995;&#32479;&#39046;&#22495;&#30340;&#29702;&#35299;&#36827;&#34892;&#27010;&#36848;&#65292;ChatGPT&#23398;&#20064;&#20102;&#22823;&#37327;&#20114;&#32852;&#32593;&#25991;&#26412;&#25968;&#25454;&#24182;&#33021;&#22815;&#25552;&#20379;&#21453;&#26144;&#31038;&#21306;&#26222;&#36941;&#24847;&#35265;&#12289;&#35266;&#28857;&#21644;&#35821;&#35328;&#27169;&#24335;&#30340;&#31572;&#26696;&#65292;&#25506;&#35752;&#20102;&#22797;&#26434;&#31995;&#32479;&#39046;&#22495;&#30340;&#25945;&#23398;&#12289;&#23398;&#20064;&#21644;&#30740;&#31350;&#20027;&#39064;&#65292;&#35748;&#20026;ChatGPT&#26159;&#31038;&#21306;&#24605;&#24819;&#30340;&#19968;&#31181;&#26469;&#28304;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;ChatGPT&#20316;&#20026;&#31038;&#21306;&#29702;&#35299;&#30340;&#20195;&#34920;&#65292;&#27010;&#36848;&#20102;&#22797;&#26434;&#31995;&#32479;&#39046;&#22495;&#12290;ChatGPT&#20174;&#22823;&#37327;&#30340;&#20114;&#32852;&#32593;&#25991;&#26412;&#25968;&#25454;&#20013;&#23398;&#20064;&#35821;&#35328;&#27169;&#24335;&#21644;&#39118;&#26684;&#65292;&#20351;&#20854;&#33021;&#22815;&#25552;&#20379;&#21453;&#26144;&#31038;&#21306;&#26222;&#36941;&#24847;&#35265;&#12289;&#35266;&#28857;&#21644;&#35821;&#35328;&#27169;&#24335;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#30340;&#25506;&#35752;&#28085;&#30422;&#20102;&#25945;&#23398;&#12289;&#23398;&#20064;&#21644;&#30740;&#31350;&#20027;&#39064;&#12290;&#25105;&#20204;&#25215;&#35748;ChatGPT&#20316;&#20026;&#31038;&#21306;&#24605;&#24819;&#30340;&#19968;&#31181;&#26469;&#28304;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an overview of the complex systems field using ChatGPT as a representation of the community's understanding. ChatGPT has learned language patterns and styles from a large dataset of internet texts, allowing it to provide answers that reflect common opinions, ideas, and language patterns found in the community. Our exploration covers both teaching and learning, and research topics. We recognize the value of ChatGPT as a source for the community's ideas.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39046;&#22495;&#8212;&#8212;&#26426;&#22120;&#24515;&#29702;&#23398;&#65292;&#21033;&#29992;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#32771;&#23519;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#35813;&#25991;&#35268;&#33539;&#20102;&#26426;&#22120;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#35770;&#26631;&#20934;&#65292;&#24182;&#23545;&#24515;&#29702;&#23454;&#39564;&#20013;&#25552;&#31034;&#35774;&#35745;&#25919;&#31574;&#36827;&#34892;&#20102;&#25506;&#35752;&#21644;&#21046;&#23450;&#12290;</title><link>http://arxiv.org/abs/2303.13988</link><description>&lt;p&gt;
&#26426;&#22120;&#24515;&#29702;&#23398;&#65306;&#21033;&#29992;&#24515;&#29702;&#23398;&#26041;&#27861;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#20852;&#33021;&#21147;&#21644;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods. (arXiv:2303.13988v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39046;&#22495;&#8212;&#8212;&#26426;&#22120;&#24515;&#29702;&#23398;&#65292;&#21033;&#29992;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#32771;&#23519;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#35813;&#25991;&#35268;&#33539;&#20102;&#26426;&#22120;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#35770;&#26631;&#20934;&#65292;&#24182;&#23545;&#24515;&#29702;&#23454;&#39564;&#20013;&#25552;&#31034;&#35774;&#35745;&#25919;&#31574;&#36827;&#34892;&#20102;&#25506;&#35752;&#21644;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26159;&#23558;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19982;&#20154;&#31867;&#20132;&#27969;&#21644;&#26085;&#24120;&#29983;&#27963;&#32039;&#23494;&#32467;&#21512;&#30340;&#20808;&#38155;&#12290;&#30001;&#20110;&#24555;&#36895;&#25216;&#26415;&#36827;&#27493;&#21644;&#20854;&#26497;&#39640;&#30340;&#36890;&#29992;&#24615;&#65292;&#29616;&#20170;LLM&#24050;&#32463;&#25317;&#26377;&#25968;&#30334;&#19975;&#29992;&#25143;&#65292;&#24182;&#27491;&#22788;&#20110;&#25104;&#20026;&#20027;&#35201;&#20449;&#24687;&#26816;&#32034;&#12289;&#20869;&#23481;&#29983;&#25104;&#12289;&#38382;&#39064;&#35299;&#20915;&#31561;&#25216;&#26415;&#30340;&#21069;&#27839;&#12290;&#22240;&#27492;&#65292;&#23545;&#20854;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#21644;&#23457;&#26597;&#26174;&#24471;&#23588;&#20026;&#37325;&#35201;&#12290;&#30001;&#20110;&#24403;&#21069;LLM&#20013;&#20986;&#29616;&#24840;&#21152;&#22797;&#26434;&#21644;&#26032;&#39062;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#21487;&#23558;&#20854;&#35270;&#20026;&#21442;&#19982;&#20154;&#31867;&#24515;&#29702;&#23454;&#39564;&#30340;&#23545;&#35937;&#65292;&#20197;&#20415;&#26356;&#20026;&#20840;&#38754;&#22320;&#35780;&#20272;&#20854;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;"&#26426;&#22120;&#24515;&#29702;&#23398;"&#30340;&#26032;&#20852;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#21508;&#31867;&#24515;&#29702;&#23398;&#20998;&#25903;&#22914;&#20309;&#20026;LLM&#30340;&#34892;&#20026;&#27979;&#35797;&#25552;&#20379;&#26377;&#29992;&#21442;&#32771;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#35268;&#33539;&#20102;&#26426;&#22120;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#35770;&#26631;&#20934;&#65292;&#29305;&#21035;&#26159;&#19987;&#27880;&#20110;&#25552;&#31034;&#35774;&#35745;&#25919;&#31574;&#30340;&#21046;&#23450;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#25551;&#36848;&#20102;&#34892;&#20026;&#27979;&#35797;&#32467;&#26524;&#22914;&#20309;&#20026;&#26410;&#26469;&#30340;LLM&#21457;&#23637;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Due to rapid technological advances and their extreme versatility, LLMs nowadays have millions of users and are at the cusp of being the main go-to technology for information retrieval, content generation, problem-solving, etc. Therefore, it is of great importance to thoroughly assess and scrutinize their capabilities. Due to increasingly complex and novel behavioral patterns in current LLMs, this can be done by treating them as participants in psychology experiments that were originally designed to test humans. For this purpose, the paper introduces a new field of research called "machine psychology". The paper outlines how different subfields of psychology can inform behavioral tests for LLMs. It defines methodological standards for machine psychology research, especially by focusing on policies for prompt designs. Additionally, it describes how behaviora
&lt;/p&gt;</description></item><item><title>ExBEHRT&#26159;&#19968;&#31181;&#25193;&#23637;Transformer&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#65292;&#23558;&#22810;&#31181;&#31867;&#22411;&#30340;&#35760;&#24405;&#21253;&#25324;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#65292;&#21487;&#20197;&#39044;&#27979;&#19981;&#21516;&#30142;&#30149;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#26356;&#22909;&#65292;&#24182;&#20351;&#29992;&#39044;&#26399;&#26799;&#24230;&#23545;&#32467;&#26524;&#36827;&#34892;&#26356;&#32454;&#31890;&#24230;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2303.12364</link><description>&lt;p&gt;
ExBEHRT&#65306;&#22522;&#20110;&#30005;&#23376;&#30149;&#21382;&#30340;&#25193;&#23637;Transformer&#39044;&#27979;&#30142;&#30149;&#20122;&#22411;&#21644;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
ExBEHRT: Extended Transformer for Electronic Health Records to Predict Disease Subtypes &amp; Progressions. (arXiv:2303.12364v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12364
&lt;/p&gt;
&lt;p&gt;
ExBEHRT&#26159;&#19968;&#31181;&#25193;&#23637;Transformer&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#65292;&#23558;&#22810;&#31181;&#31867;&#22411;&#30340;&#35760;&#24405;&#21253;&#25324;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#65292;&#21487;&#20197;&#39044;&#27979;&#19981;&#21516;&#30142;&#30149;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#26356;&#22909;&#65292;&#24182;&#20351;&#29992;&#39044;&#26399;&#26799;&#24230;&#23545;&#32467;&#26524;&#36827;&#34892;&#26356;&#32454;&#31890;&#24230;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;ExBEHRT&#65292;&#23427;&#26159;BEHRT&#65288;&#24212;&#29992;&#20110;&#30005;&#23376;&#30149;&#21382;&#30340;BERT&#65289;&#30340;&#25193;&#23637;&#29256;&#26412;&#65292;&#24182;&#24212;&#29992;&#19981;&#21516;&#30340;&#31639;&#27861;&#26469;&#35299;&#37322;&#20854;&#32467;&#26524;&#12290;&#25105;&#20204;&#23558;&#29305;&#24449;&#31354;&#38388;&#20174;&#20165;&#32771;&#34385;&#35786;&#26029;&#21644;&#24739;&#32773;&#24180;&#40836;&#25193;&#23637;&#21040;&#21253;&#25324;&#22810;&#31181;&#31867;&#22411;&#30340;&#35760;&#24405;&#65292;&#21253;&#25324;&#20154;&#21475;&#32479;&#35745;&#23398;&#12289;&#20020;&#24202;&#29305;&#24449;&#12289;&#29983;&#21629;&#20307;&#24449;&#12289;&#21560;&#28895;&#29366;&#24577;&#12289;&#35786;&#26029;&#12289;&#25163;&#26415;&#12289;&#33647;&#29289;&#21644;&#23454;&#39564;&#23460;&#26816;&#26597;&#65292;&#24182;&#37319;&#29992;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#32479;&#19968;&#19981;&#21516;&#29305;&#24449;&#30340;&#39057;&#29575;&#21644;&#26102;&#38388;&#32500;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#38468;&#21152;&#29305;&#24449;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#19981;&#21516;&#30142;&#30149;&#19979;&#28216;&#20219;&#21153;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#20102;&#20445;&#35777;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#39044;&#26399;&#26799;&#24230;&#30340;&#25913;&#36827;&#26041;&#27861;&#23545;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#35299;&#37322;&#65292;&#35813;&#26041;&#27861;&#20197;&#21069;&#26410;&#24212;&#29992;&#20110;&#23558;EHR&#25968;&#25454;&#19982;Transformer&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#27604;&#20197;&#21069;&#26041;&#27861;&#26356;&#32454;&#31890;&#24230;&#30340;&#35299;&#37322;&#65292;&#22914;&#29305;&#24449;&#21644;&#20196;&#29260;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23545;&#32959;&#30244;&#23398;&#24739;&#32773;&#30340;&#27169;&#22411;&#34920;&#31034;&#36827;&#34892;&#32858;&#31867;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ExBEHRT&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#30142;&#30149;&#20122;&#22411;&#21644;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we introduce ExBEHRT, an extended version of BEHRT (BERT applied to electronic health records), and apply different algorithms to interpret its results. While BEHRT considers only diagnoses and patient age, we extend the feature space to several multimodal records, namely demographics, clinical characteristics, vital signs, smoking status, diagnoses, procedures, medications, and laboratory tests, by applying a novel method to unify the frequencies and temporal dimensions of the different features. We show that additional features significantly improve model performance for various downstream tasks in different diseases. To ensure robustness, we interpret model predictions using an adaptation of expected gradients, which has not been previously applied to transformers with EHR data and provides more granular interpretations than previous approaches such as feature and token importances. Furthermore, by clustering the model representations of oncology patients, we show tha
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992; ChatGPT &#36827;&#34892;&#20020;&#24202;&#25991;&#26412;&#25366;&#25496;&#30340;&#28508;&#21147;&#65292;&#20294;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#25928;&#26524;&#27424;&#20339;&#65292;&#21516;&#26102;&#19978;&#20256;&#24739;&#32773;&#20449;&#24687;&#20063;&#24341;&#21457;&#38544;&#31169;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992; ChatGPT &#29983;&#25104;&#24102;&#26631;&#31614;&#30340;&#22823;&#37327;&#39640;&#36136;&#37327;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2303.04360</link><description>&lt;p&gt;
LLM&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#23545;&#20020;&#24202;&#25991;&#26412;&#25366;&#25496;&#26377;&#24110;&#21161;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does Synthetic Data Generation of LLMs Help Clinical Text Mining?. (arXiv:2303.04360v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04360
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992; ChatGPT &#36827;&#34892;&#20020;&#24202;&#25991;&#26412;&#25366;&#25496;&#30340;&#28508;&#21147;&#65292;&#20294;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#25928;&#26524;&#27424;&#20339;&#65292;&#21516;&#26102;&#19978;&#20256;&#24739;&#32773;&#20449;&#24687;&#20063;&#24341;&#21457;&#38544;&#31169;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992; ChatGPT &#29983;&#25104;&#24102;&#26631;&#31614;&#30340;&#22823;&#37327;&#39640;&#36136;&#37327;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26368;&#36817;&#36827;&#23637;&#25512;&#21160;&#20102;&#35832;&#22914;OpenAI&#30340;ChatGPT&#20043;&#31867;&#30340;&#39640;&#24615;&#33021;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#38382;&#31572;&#12289;&#35770;&#25991;&#20889;&#20316;&#21644;&#20195;&#30721;&#29983;&#25104;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#20173;&#19981;&#30830;&#23450;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#26816;&#26597;ChatGPT&#20174;&#38750;&#32467;&#26500;&#21270;&#20581;&#24247;&#25991;&#26412;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#37325;&#28857;&#20851;&#27880;&#29983;&#29289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25552;&#21462;&#65292;&#25506;&#35752;ChatGPT&#22312;&#20020;&#24202;&#25991;&#26412;&#25366;&#25496;&#20013;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#30452;&#25509;&#24212;&#29992;ChatGPT&#36827;&#34892;&#36825;&#20123;&#20219;&#21153;&#23548;&#33268;&#34920;&#29616;&#19981;&#20339;&#65292;&#24182;&#24341;&#21457;&#19982;&#23558;&#24739;&#32773;&#20449;&#24687;&#19978;&#20256;&#21040;ChatGPT API&#26377;&#20851;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#21033;&#29992;ChatGPT&#29983;&#25104;&#22823;&#37327;&#39640;&#36136;&#37327;&#30340;&#24102;&#26631;&#31614;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models (LLMs) have led to the development of highly potent models like OpenAI's ChatGPT. These models have exhibited exceptional performance in a variety of tasks, such as question answering, essay composition, and code generation. However, their effectiveness in the healthcare sector remains uncertain. In this study, we seek to investigate the potential of ChatGPT to aid in clinical text mining by examining its ability to extract structured information from unstructured healthcare texts, with a focus on biological named entity recognition and relation extraction. However, our preliminary results indicate that employing ChatGPT directly for these tasks resulted in poor performance and raised privacy concerns associated with uploading patients' information to the ChatGPT API. To overcome these limitations, we propose a new training paradigm that involves generating a vast quantity of high-quality synthetic data with labels utilizing ChatGPT and fine
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#25289;&#26222;&#25289;&#26031;&#25511;&#21046;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#19981;&#35268;&#21017;&#29366;&#24577;&#35266;&#27979;&#21644;&#26410;&#30693;&#24310;&#36831;&#30340;&#36830;&#32493;&#26102;&#38388;&#29615;&#22659;&#19979;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.12604</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#24310;&#36831;&#31995;&#32479;&#30340;&#31070;&#32463;&#25289;&#26222;&#25289;&#26031;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Neural Laplace Control for Continuous-time Delayed Systems. (arXiv:2302.12604v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#25289;&#26222;&#25289;&#26031;&#25511;&#21046;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#19981;&#35268;&#21017;&#29366;&#24577;&#35266;&#27979;&#21644;&#26410;&#30693;&#24310;&#36831;&#30340;&#36830;&#32493;&#26102;&#38388;&#29615;&#22659;&#19979;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23454;&#38469;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#21253;&#25324;&#20855;&#26377;&#24310;&#36831;&#30340;&#36830;&#32493;&#26102;&#38388;&#29615;&#22659;&#12290;&#36825;&#20123;&#29615;&#22659;&#20855;&#26377;&#20004;&#20010;&#26174;&#33879;&#29305;&#28857;&#65306;&#39318;&#20808;&#65292;&#35266;&#23519;&#21040;&#30340;&#29366;&#24577;x(t)&#22312;&#19981;&#35268;&#21017;&#30340;&#26102;&#38388;&#38388;&#38548;&#20869;&#36827;&#34892;&#35266;&#23519;&#65307;&#20854;&#27425;&#65292;&#24403;&#21069;&#34892;&#21160;a(t)&#20165;&#22312;&#26410;&#30693;&#24310;&#36831;g &gt; 0 &#30340;&#24773;&#20917;&#19979;&#24433;&#21709;&#26410;&#26469;&#29366;&#24577;x(t+g)&#12290;&#36825;&#26679;&#30340;&#29615;&#22659;&#30340;&#19968;&#20010;&#20856;&#22411;&#20363;&#23376;&#26159;&#21355;&#26143;&#25511;&#21046;&#65292;&#20854;&#20013;&#22320;&#29699;&#21644;&#21355;&#26143;&#20043;&#38388;&#30340;&#36890;&#20449;&#38142;&#36335;&#20250;&#36896;&#25104;&#35266;&#27979;&#19981;&#35268;&#21017;&#21644;&#24310;&#36831;&#12290;&#29616;&#26377;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#20855;&#26377;&#26102;&#38388;&#19981;&#35268;&#21017;&#35266;&#27979;&#25110;&#24050;&#30693;&#24310;&#36831;&#30340;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#28041;&#21450;&#26102;&#38388;&#19981;&#35268;&#21017;&#35266;&#27979;&#21644;&#26410;&#30693;&#24310;&#36831;&#30340;&#29615;&#22659;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31070;&#32463;&#25289;&#26222;&#25289;&#26031;&#25511;&#21046;&#65292;&#19968;&#31181;&#36830;&#32493;&#26102;&#38388;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#31070;&#32463;&#25289;&#26222;&#25289;&#26031;&#21160;&#21147;&#23398;&#27169;&#22411;&#19982;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#35268;&#21010;&#22120;&#30456;&#32467;&#21512;&#65292;&#24182;&#33021;&#22815;&#20174;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world offline reinforcement learning (RL) problems involve continuous-time environments with delays. Such environments are characterized by two distinctive features: firstly, the state x(t) is observed at irregular time intervals, and secondly, the current action a(t) only affects the future state x(t + g) with an unknown delay g &gt; 0. A prime example of such an environment is satellite control where the communication link between earth and a satellite causes irregular observations and delays. Existing offline RL algorithms have achieved success in environments with irregularly observed states in time or known delays. However, environments involving both irregular observations in time and unknown delays remains an open and challenging problem. To this end, we propose Neural Laplace Control, a continuous-time model-based offline RL method that combines a Neural Laplace dynamics model with a model predictive control (MPC) planner--and is able to learn from an offline dataset sam
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#30340;&#20844;&#27491;&#26234;&#24935;&#22478;&#24066;&#25919;&#31574;&#35843;&#25972;&#21644;&#29983;&#25104;&#26041;&#27861;Fairguard&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#38745;&#24577;&#29983;&#25104;&#21644;&#21160;&#24577;&#35843;&#33410;&#65292;&#32531;&#35299;&#30001;&#22810;&#31181;&#25968;&#25454;&#21644;&#31639;&#27861;&#20559;&#35265;&#23548;&#33268;&#30340;&#19981;&#20844;&#27491;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.11137</link><description>&lt;p&gt;
Fairguard: &#22312;&#26234;&#24935;&#22478;&#24066;&#20013;&#21033;&#29992;&#22522;&#20110;&#36923;&#36753;&#30340;&#20844;&#27491;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Fairguard: Harness Logic-based Fairness Rules in Smart Cities. (arXiv:2302.11137v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#30340;&#20844;&#27491;&#26234;&#24935;&#22478;&#24066;&#25919;&#31574;&#35843;&#25972;&#21644;&#29983;&#25104;&#26041;&#27861;Fairguard&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#38745;&#24577;&#29983;&#25104;&#21644;&#21160;&#24577;&#35843;&#33410;&#65292;&#32531;&#35299;&#30001;&#22810;&#31181;&#25968;&#25454;&#21644;&#31639;&#27861;&#20559;&#35265;&#23548;&#33268;&#30340;&#19981;&#20844;&#27491;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#24935;&#22478;&#24066;&#36816;&#34892;&#22312;&#35745;&#31639;&#39044;&#27979;&#26694;&#26550;&#19978;&#65292;&#25910;&#38598;&#12289;&#25972;&#21512;&#21644;&#21033;&#29992;&#22823;&#35268;&#27169;&#20256;&#24863;&#22120;&#32593;&#32476;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26694;&#26550;&#23481;&#26131;&#21463;&#21040;&#22810;&#31181;&#25968;&#25454;&#21644;&#31639;&#27861;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#36825;&#32463;&#24120;&#23548;&#33268;&#19981;&#20844;&#27491;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#26412;&#25991;&#39318;&#20808;&#36890;&#36807;&#30740;&#31350;&#30000;&#32435;&#35199;&#24030;&#26597;&#22612;&#21162;&#21152;&#30340;&#30495;&#23454;&#22478;&#24066;&#25968;&#25454;&#65292;&#23637;&#31034;&#20102;&#20559;&#35265;&#22312;&#24494;&#35266;&#23618;&#38754;&#19978;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#19978;&#20173;&#28982;&#23384;&#22312;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Fairguard&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#24494;&#35266;&#23618;&#38754;&#26102;&#38388;&#36923;&#36753;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22797;&#26434;&#30340;&#26102;&#38388;&#31354;&#38388;&#22495;&#20013;&#36827;&#34892;&#20844;&#27491;&#30340;&#26234;&#24935;&#22478;&#24066;&#25919;&#31574;&#35843;&#25972;&#21644;&#29983;&#25104;&#12290;Fairguard&#26694;&#26550;&#30001;&#20004;&#20010;&#38454;&#27573;&#32452;&#25104;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#38745;&#24577;&#29983;&#25104;&#22120;&#65292;&#33021;&#22815;&#36890;&#36807;&#26368;&#23567;&#21270;&#25152;&#36873;&#23646;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#26465;&#20214;&#26469;&#20943;&#23569;&#25968;&#25454;&#20559;&#35265;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#30830;&#20445;&#39044;&#27979;&#31639;&#27861;&#30340;&#20844;&#27491;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21160;&#24577;&#32452;&#20214;&#26469;&#35843;&#33410;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#21033;&#29992;&#36923;&#36753;&#35268;&#21017;&#29983;&#25104;&#26410;&#26469;&#30340;&#20844;&#27491;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Smart cities operate on computational predictive frameworks that collect, aggregate, and utilize data from large-scale sensor networks. However, these frameworks are prone to multiple sources of data and algorithmic bias, which often lead to unfair prediction results. In this work, we first demonstrate that bias persists at a micro-level both temporally and spatially by studying real city data from Chattanooga, TN. To alleviate the issue of such bias, we introduce Fairguard, a micro-level temporal logic-based approach for fair smart city policy adjustment and generation in complex temporal-spatial domains. The Fairguard framework consists of two phases: first, we develop a static generator that is able to reduce data bias based on temporal logic conditions by minimizing correlations between selected attributes. Then, to ensure fairness in predictive algorithms, we design a dynamic component to regulate prediction results and generate future fair predictions by harnessing logic rules. E
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#35752;&#35770;&#22823;&#22411;&#29983;&#25104;AI&#27169;&#22411;&#30340;&#21487;&#20449;AI&#30417;&#31649;&#65292;&#21253;&#25324;&#30452;&#25509;&#30417;&#31649;&#12289;&#25968;&#25454;&#20445;&#25252;&#12289;&#20869;&#23481;&#30417;&#31649;&#21644;&#25919;&#31574;&#24314;&#35758;&#65292;&#24182;&#24314;&#35758;&#20351;&#29992;&#26032;&#26415;&#35821;&#26469;&#21306;&#20998;&#21442;&#19982;&#32773;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#30830;&#20445;LGAIMs&#30340;&#21487;&#20449;&#24230;&#24182;&#20351;&#20854;&#20026;&#21463;&#30410;&#25152;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.02337</link><description>&lt;p&gt;
&#31649;&#21046;ChatGPT&#21644;&#20854;&#20182;&#22823;&#22411;&#29983;&#25104;AI&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Regulating ChatGPT and other Large Generative AI Models. (arXiv:2302.02337v5 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#35752;&#35770;&#22823;&#22411;&#29983;&#25104;AI&#27169;&#22411;&#30340;&#21487;&#20449;AI&#30417;&#31649;&#65292;&#21253;&#25324;&#30452;&#25509;&#30417;&#31649;&#12289;&#25968;&#25454;&#20445;&#25252;&#12289;&#20869;&#23481;&#30417;&#31649;&#21644;&#25919;&#31574;&#24314;&#35758;&#65292;&#24182;&#24314;&#35758;&#20351;&#29992;&#26032;&#26415;&#35821;&#26469;&#21306;&#20998;&#21442;&#19982;&#32773;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#30830;&#20445;LGAIMs&#30340;&#21487;&#20449;&#24230;&#24182;&#20351;&#20854;&#20026;&#21463;&#30410;&#25152;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#29983;&#25104;AI&#27169;&#22411;&#65288;LGAIMs&#65289;&#65292;&#22914;ChatGPT&#25110;Stable Diffusion&#65292;&#27491;&#22312;&#24555;&#36895;&#25913;&#21464;&#25105;&#20204;&#30340;&#27807;&#36890;&#12289;&#35828;&#26126;&#21644;&#21019;&#36896;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#27431;&#30431;&#21450;&#20854;&#20182;&#22320;&#21306;&#30340;AI&#30417;&#31649;&#20027;&#35201;&#38598;&#20013;&#22312;&#20256;&#32479;AI&#27169;&#22411;&#19978;&#65292;&#32780;&#38750;LGAIMs&#12290;&#26412;&#25991;&#23558;&#25226;&#36825;&#20123;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#25918;&#32622;&#22312;&#24403;&#21069;&#30340;&#8220;&#21487;&#20449;AI&#30417;&#31649;&#8221;&#36777;&#35770;&#20013;&#65292;&#24182;&#25506;&#35752;&#22914;&#20309;&#35843;&#25972;&#27861;&#24459;&#20197;&#36866;&#24212;&#20854;&#33021;&#21147;&#12290;&#22312;&#22880;&#23450;&#25216;&#26415;&#22522;&#30784;&#20043;&#21518;&#65292;&#26412;&#25991;&#30340;&#27861;&#24459;&#37096;&#20998;&#20998;&#22235;&#27493;&#36827;&#34892;&#65292;&#21253;&#25324;&#65288;1&#65289;&#30452;&#25509;&#30417;&#31649;&#65292;&#65288;2&#65289;&#25968;&#25454;&#20445;&#25252;&#65292;&#65288;3&#65289;&#20869;&#23481;&#30417;&#31649;&#21644;&#65288;4&#65289;&#25919;&#31574;&#24314;&#35758;&#12290;&#23427;&#24314;&#35758;&#20351;&#29992;&#26032;&#26415;&#35821;&#26469;&#25429;&#25417;LGAIM&#35774;&#32622;&#20013;&#30340;AI&#20215;&#20540;&#38142;&#65292;&#21306;&#20998;LGAIM&#24320;&#21457;&#20154;&#21592;&#12289;&#37096;&#32626;&#32773;&#12289;&#19987;&#19994;&#21644;&#38750;&#19987;&#19994;&#29992;&#25143;&#65292;&#20197;&#21450;LGAIM&#36755;&#20986;&#30340;&#25509;&#25910;&#32773;&#12290;&#25105;&#20204;&#23558;&#30417;&#31649;&#32844;&#36131;&#38024;&#23545;&#36825;&#20123;&#19981;&#21516;&#30340;&#20215;&#20540;&#38142;&#21442;&#19982;&#32773;&#36827;&#34892;&#35843;&#25972;&#65292;&#24182;&#25552;&#20986;&#22235;&#20010;&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;LGAIMs&#30340;&#20449;&#20219;&#24230;&#24182;&#20351;&#20854;&#20026;&#21463;&#30410;&#25152;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large generative AI models (LGAIMs), such as ChatGPT or Stable Diffusion, are rapidly transforming the way we communicate, illustrate, and create. However, AI regulation, in the EU and beyond, has primarily focused on conventional AI models, not LGAIMs. This paper will situate these new generative models in the current debate on trustworthy AI regulation, and ask how the law can be tailored to their capabilities. After laying technical foundations, the legal part of the paper proceeds in four steps, covering (1) direct regulation, (2) data protection, (3) content moderation, and (4) policy proposals. It suggests a novel terminology to capture the AI value chain in LGAIM settings by differentiating between LGAIM developers, deployers, professional and non-professional users, as well as recipients of LGAIM output. We tailor regulatory duties to these different actors along the value chain and suggest four strategies to ensure that LGAIMs are trustworthy and deployed for the benefit of so
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#12289;&#22522;&#20110;&#38170;&#28857;&#30340;&#23545;&#25239;&#35757;&#32451;&#31574;&#30053;LAAT&#65292;&#36890;&#36807;&#21033;&#29992;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#22312;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#22330;&#26223;&#19979;&#22686;&#24378;&#22270;&#20687;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#38646;&#26679;&#26412;&#23545;&#25239;&#24615;&#33021;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20339;&#29366;&#24577;&#23545;&#25239;&#24615;&#19968;&#27425;&#24615;&#26041;&#27861;&#65292;&#21516;&#26102;&#33021;&#20026;&#27969;&#34892;&#30340;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#24102;&#26469;&#23454;&#36136;&#24615;&#30340;&#38646;&#26679;&#26412;&#23545;&#25239;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2301.13096</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#38170;&#28857;&#30340;&#38646;&#26679;&#26412;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Language-Driven Anchors for Zero-Shot Adversarial Robustness. (arXiv:2301.13096v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#12289;&#22522;&#20110;&#38170;&#28857;&#30340;&#23545;&#25239;&#35757;&#32451;&#31574;&#30053;LAAT&#65292;&#36890;&#36807;&#21033;&#29992;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#22312;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#22330;&#26223;&#19979;&#22686;&#24378;&#22270;&#20687;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#38646;&#26679;&#26412;&#23545;&#25239;&#24615;&#33021;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20339;&#29366;&#24577;&#23545;&#25239;&#24615;&#19968;&#27425;&#24615;&#26041;&#27861;&#65292;&#21516;&#26102;&#33021;&#20026;&#27969;&#34892;&#30340;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#24102;&#26469;&#23454;&#36136;&#24615;&#30340;&#38646;&#26679;&#26412;&#23545;&#25239;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#26412;&#25991;&#26088;&#22312;&#25913;&#21892;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#22330;&#26223;&#19979;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#12289;&#22522;&#20110;&#38170;&#28857;&#30340;&#23545;&#25239;&#35757;&#32451;&#31574;&#30053;LAAT&#12290;LAAT&#21033;&#29992;&#25991;&#26412;&#32534;&#30721;&#22120;&#20026;&#27599;&#20010;&#31867;&#21035;&#29983;&#25104;&#22266;&#23450;&#30340;&#38170;&#28857;&#65288;&#24402;&#19968;&#21270;&#29305;&#24449;&#23884;&#20837;&#65289;&#65292;&#24182;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#20351;&#29992;&#36825;&#20123;&#38170;&#28857;&#12290;&#36890;&#36807;&#21033;&#29992;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;LAAT&#21487;&#20197;&#22686;&#24378;&#22270;&#20687;&#27169;&#22411;&#22312;&#26032;&#31867;&#21035;&#19978;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#26679;&#20363;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#26368;&#36817;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#20960;&#31181;&#26377;&#25928;&#30340;&#25216;&#26415;&#26469;&#35299;&#20915;&#23427;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LAAT&#26174;&#33879;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#23545;&#25239;&#24615;&#33021;&#65292;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20339;&#29366;&#24577;&#23545;&#25239;&#24615;&#19968;&#27425;&#24615;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20026;&#27969;&#34892;&#30340;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#65288;&#22914;ResNet-50&#21644;DenseNet-121&#65289;&#20135;&#29983;&#20102;&#23454;&#36136;&#24615;&#30340;&#38646;&#26679;&#26412;&#23545;&#25239;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are known to be susceptible to adversarial attacks. In this work, we focus on improving adversarial robustness in the challenging zero-shot image classification setting. To address this issue, we propose LAAT, a novel Language-driven, Anchor-based Adversarial Training strategy. LAAT utilizes a text encoder to generate fixed anchors (normalized feature embeddings) for each category and then uses these anchors for adversarial training. By leveraging the semantic consistency of the text encoders, LAAT can enhance the adversarial robustness of the image model on novel categories without additional examples. We identify the large cosine similarity problem of recent text encoders and design several effective techniques to address it. The experimental results demonstrate that LAAT significantly improves zero-shot adversarial performance, outperforming previous state-of-the-art adversarially robust one-shot methods. Moreover, our method produces substantial zero-shot adver
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#39537;&#21160;&#30340;&#26234;&#33021;&#35745;&#31639;&#35774;&#35745;(DICD)&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#25552;&#21462;&#21644;&#34920;&#31034;&#21382;&#21490;&#25110;&#21046;&#36896;&#36807;&#31243;&#25968;&#25454;&#20013;&#30340;&#35774;&#35745;&#29305;&#24449;&#65292;&#25903;&#25345;&#35774;&#35745;&#26041;&#26696;&#26816;&#32034;&#12289;&#29983;&#25104;&#12289;&#20248;&#21270;&#12289;&#35780;&#20272;&#31561;&#12290;&#20316;&#20026;&#19968;&#31181;&#26032;&#20852;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;DICD&#20173;&#23384;&#22312;&#22810;&#20010;&#26410;&#24320;&#21457;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.12382</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26234;&#33021;&#20135;&#21697;&#35745;&#31639;&#35774;&#35745;&#65306;&#26041;&#27861;&#12289;&#25216;&#26415;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Data-driven intelligent computational design for products: Method, techniques, and applications. (arXiv:2301.12382v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12382
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#26234;&#33021;&#35745;&#31639;&#35774;&#35745;(DICD)&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#25552;&#21462;&#21644;&#34920;&#31034;&#21382;&#21490;&#25110;&#21046;&#36896;&#36807;&#31243;&#25968;&#25454;&#20013;&#30340;&#35774;&#35745;&#29305;&#24449;&#65292;&#25903;&#25345;&#35774;&#35745;&#26041;&#26696;&#26816;&#32034;&#12289;&#29983;&#25104;&#12289;&#20248;&#21270;&#12289;&#35780;&#20272;&#31561;&#12290;&#20316;&#20026;&#19968;&#31181;&#26032;&#20852;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;DICD&#20173;&#23384;&#22312;&#22810;&#20010;&#26410;&#24320;&#21457;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#26234;&#33021;&#35745;&#31639;&#35774;&#35745;(DICD)&#26159;&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#20154;&#24037;&#26234;&#33021;&#32972;&#26223;&#19979;&#20986;&#29616;&#30340;&#30740;&#31350;&#28909;&#28857;&#12290;&#23427;&#24378;&#35843;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#25552;&#21462;&#21644;&#34920;&#31034;&#21382;&#21490;&#25110;&#21046;&#36896;&#36807;&#31243;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#35774;&#35745;&#29305;&#24449;&#65292;&#24182;&#23398;&#20064;&#36825;&#20123;&#35774;&#35745;&#29305;&#24449;&#30340;&#32452;&#21512;&#21644;&#26144;&#23556;&#27169;&#24335;&#65292;&#20197;&#25903;&#25345;&#35774;&#35745;&#26041;&#26696;&#26816;&#32034;&#12289;&#29983;&#25104;&#12289;&#20248;&#21270;&#12289;&#35780;&#20272;&#31561;&#12290;&#30001;&#20110;&#20854;&#33258;&#21160;&#12289;&#39640;&#25928;&#22320;&#29983;&#25104;&#35774;&#35745;&#26041;&#26696;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#25903;&#25345;&#20154;&#31867;&#21442;&#19982;&#30340;&#26234;&#33021;&#21019;&#26032;&#35774;&#35745;&#27963;&#21160;&#65292;DICD&#21560;&#24341;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#19968;&#31181;&#26032;&#20852;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;DICD&#20173;&#23384;&#22312;&#35768;&#22810;&#26410;&#24320;&#21457;&#30340;&#38382;&#39064;&#65292;&#22914;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;&#26500;&#24314;&#12289;&#19982;&#24037;&#31243;&#35774;&#35745;&#30456;&#20851;&#30340;&#29305;&#24449;&#24037;&#31243;&#12289;DICD&#22312;&#25972;&#20010;&#20135;&#21697;&#35774;&#35745;&#36807;&#31243;&#20013;&#30340;&#31995;&#32479;&#26041;&#27861;&#21644;&#25216;&#26415;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven intelligent computational design (DICD) is a research hotspot emerged under the context of fast-developing artificial intelligence. It emphasizes on utilizing deep learning algorithms to extract and represent the design features hidden in historical or fabricated design process data, and then learn the combination and mapping patterns of these design features for the purposes of design solution retrieval, generation, optimization, evaluation, etc. Due to its capability of automatically and efficiently generating design solutions and thus supporting human-in-the-loop intelligent and innovative design activities, DICD has drawn the attentions from both academic and industrial fields. However, as an emerging research subject, there are still many unexplored issues that limit the development and application of DICD, such as specific dataset building, engineering design related feature engineering, systematic methods and techniques for DICD implementation in the entire product d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#30340;&#25991;&#26412;&#36716;SQL&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#19968;&#31995;&#21015;&#29420;&#29305;&#25361;&#25112;&#65292;&#21253;&#25324;&#29983;&#25104;SQL&#26597;&#35810;&#12289;&#29702;&#35299;&#26102;&#38388;&#34920;&#36798;&#24335;&#20197;&#21450;&#21306;&#20998;&#26377;&#26080;&#31572;&#26696;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.07695</link><description>&lt;p&gt;
EHRSQL&#65306;&#38754;&#21521;&#30005;&#23376;&#30149;&#21382;&#30340;&#23454;&#29992;&#25991;&#26412;&#36716;SQL&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
EHRSQL: A Practical Text-to-SQL Benchmark for Electronic Health Records. (arXiv:2301.07695v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07695
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#30340;&#25991;&#26412;&#36716;SQL&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#19968;&#31995;&#21015;&#29420;&#29305;&#25361;&#25112;&#65292;&#21253;&#25324;&#29983;&#25104;SQL&#26597;&#35810;&#12289;&#29702;&#35299;&#26102;&#38388;&#34920;&#36798;&#24335;&#20197;&#21450;&#21306;&#20998;&#26377;&#26080;&#31572;&#26696;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#30005;&#23376;&#30149;&#21382;&#65288;EHR&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;&#12290;&#23545;&#35805;&#26159;&#30001;222&#20010;&#21307;&#38498;&#24037;&#20316;&#20154;&#21592;&#21253;&#25324;&#21307;&#29983;&#12289;&#25252;&#22763;&#12289;&#20445;&#38505;&#23457;&#26597;&#21644;&#20581;&#24247;&#26723;&#26696;&#22242;&#38431;&#31561;&#25163;&#26426;&#32780;&#26469;&#12290;&#20026;&#20102;&#26500;&#24314;&#20851;&#20110;&#32467;&#26500;&#21270;EHR&#25968;&#25454;&#30340;QA&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#22312;&#19968;&#25152;&#22823;&#23398;&#21307;&#38498;&#36827;&#34892;&#20102;&#19968;&#27425;&#27665;&#35843;&#24182;&#21046;&#20316;&#20102;&#27169;&#26495;&#35805;&#26415;&#20197;&#21019;&#24314;&#31181;&#23376;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25163;&#21160;&#23558;&#23427;&#20204;&#38142;&#25509;&#21040;&#20004;&#20010;&#24320;&#28304;&#30340;EHR&#25968;&#25454;&#24211;&#65288;MIMIC-III&#21644;eICU&#65289;&#20013;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#20102;&#26469;&#33258;&#27665;&#24847;&#35843;&#26597;&#30340;&#21508;&#31181;&#26102;&#38388;&#34920;&#36798;&#24335;&#21644;&#26410;&#33021;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#29420;&#29305;&#30340;&#25361;&#25112;&#65306;&#27169;&#22411;&#38656;&#35201; 1&#65289;&#29983;&#25104;&#21453;&#26144;&#21307;&#38498;&#20013;&#21508;&#31181;&#38656;&#27714;&#30340;SQL&#26597;&#35810;&#65292;&#21253;&#25324;&#31616;&#21333;&#30340;&#26816;&#32034;&#21644;&#22797;&#26434;&#30340;&#25805;&#20316;&#65292;&#22914;&#35745;&#31639;&#29983;&#23384;&#29575;&#65292;2&#65289;&#29702;&#35299;&#21508;&#31181;&#26102;&#38388;&#34920;&#36798;&#24335;&#20197;&#22238;&#31572;&#19982;&#26102;&#38388;&#25935;&#24863;&#30340;&#21307;&#30103;&#38382;&#39064;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;3&#65289;&#26681;&#25454;&#39044;&#27979;&#21306;&#20998;&#32473;&#23450;&#38382;&#39064;&#26159;&#21487;&#22238;&#31572;&#36824;&#26159;&#19981;&#21487;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new text-to-SQL dataset for electronic health records (EHRs). The utterances were collected from 222 hospital staff, including physicians, nurses, insurance review and health records teams, and more. To construct the QA dataset on structured EHR data, we conducted a poll at a university hospital and templatized the responses to create seed questions. Then, we manually linked them to two open-source EHR databases, MIMIC-III and eICU, and included them with various time expressions and held-out unanswerable questions in the dataset, which were all collected from the poll. Our dataset poses a unique set of challenges: the model needs to 1) generate SQL queries that reflect a wide range of needs in the hospital, including simple retrieval and complex operations such as calculating survival rate, 2) understand various time expressions to answer time-sensitive questions in healthcare, and 3) distinguish whether a given question is answerable or unanswerable based on the predicti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38416;&#36848;&#20102;&#23545;&#20110;&#20013;&#31561;&#35268;&#27169;&#30340;&#27169;&#22411;&#31867;&#65292;&#20219;&#20309;&#23436;&#25972;&#30340;&#32447;&#24615;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#37117;&#26080;&#27861;&#32988;&#20219;&#25512;&#26029;&#27169;&#22411;&#34892;&#20026;&#30340;&#25913;&#36827;&#65292;&#21551;&#31034;&#25105;&#20204;&#22312;&#20855;&#20307;&#23450;&#20041;&#26368;&#32456;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#35201;&#27762;&#21462;&#32463;&#39564;&#65292;&#37319;&#29992;&#37325;&#22797;&#27169;&#22411;&#35780;&#20272;&#30340;&#31616;&#21333;&#30452;&#25509;&#26041;&#27861;&#21487;&#20197;&#32988;&#36807;&#35768;&#22810;&#20854;&#20182;&#22797;&#26434;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.11870</link><description>&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#30340;&#19981;&#21487;&#33021;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
Impossibility Theorems for Feature Attribution. (arXiv:2212.11870v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38416;&#36848;&#20102;&#23545;&#20110;&#20013;&#31561;&#35268;&#27169;&#30340;&#27169;&#22411;&#31867;&#65292;&#20219;&#20309;&#23436;&#25972;&#30340;&#32447;&#24615;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#37117;&#26080;&#27861;&#32988;&#20219;&#25512;&#26029;&#27169;&#22411;&#34892;&#20026;&#30340;&#25913;&#36827;&#65292;&#21551;&#31034;&#25105;&#20204;&#22312;&#20855;&#20307;&#23450;&#20041;&#26368;&#32456;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#35201;&#27762;&#21462;&#32463;&#39564;&#65292;&#37319;&#29992;&#37325;&#22797;&#27169;&#22411;&#35780;&#20272;&#30340;&#31616;&#21333;&#30452;&#25509;&#26041;&#27861;&#21487;&#20197;&#32988;&#36807;&#35768;&#22810;&#20854;&#20182;&#22797;&#26434;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26377;&#35768;&#22810;&#21487;&#20135;&#29983;&#21512;&#29702;&#35299;&#37322;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#20294;&#35813;&#39046;&#22495;&#20063;&#32463;&#39564;&#24615;&#22320;&#30475;&#21040;&#20102;&#35768;&#22810;&#22833;&#36133;&#26696;&#20363;&#12290;&#37492;&#20110;&#36825;&#20123;&#32467;&#26524;&#65292;&#23545;&#20110;&#23454;&#36341;&#32773;&#22914;&#20309;&#20197;&#21407;&#21017;&#24615;&#26041;&#24335;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#24182;&#22312;&#23427;&#20204;&#20043;&#38388;&#36827;&#34892;&#36873;&#25321;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#20013;&#31561;&#35268;&#27169;&#30340;&#27169;&#22411;&#31867;&#65288;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#28385;&#36275;&#65289;&#65292;&#20219;&#20309;&#23436;&#25972;&#30340;&#32447;&#24615;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65288;&#20363;&#22914;Integrated Gradients&#21644;SHAP&#65289;&#21487;&#20197;&#34987;&#35777;&#26126;&#23545;&#20110;&#25512;&#26029;&#27169;&#22411;&#34892;&#20026;&#30340;&#25913;&#36827;&#37117;&#26080;&#27861;&#32988;&#20219;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#24120;&#35265;&#30340;&#26368;&#32456;&#20219;&#21153;&#65292;&#22914;&#25551;&#36848;&#23616;&#37096;&#27169;&#22411;&#34892;&#20026;&#12289;&#35782;&#21035;&#34394;&#20551;&#29305;&#24449;&#21644;&#31639;&#27861;&#22238;&#28335;&#12290;&#25105;&#20204;&#24037;&#20316;&#30340;&#19968;&#20010;&#37325;&#35201;&#21551;&#31034;&#26159;&#20855;&#20307;&#23450;&#20041;&#26368;&#32456;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#65306;&#19968;&#26086;&#36825;&#26679;&#30340;&#26368;&#32456;&#20219;&#21153;&#34987;&#23450;&#20041;&#65292;&#19968;&#20010;&#31616;&#21333;&#21644;&#30452;&#25509;&#30340;&#26041;&#27861;&#8212;&#8212;&#37325;&#22797;&#27169;&#22411;&#35780;&#20272;&#8212;&#8212;&#21487;&#20197;&#32988;&#36807;&#35768;&#22810;&#20854;&#20182;&#22797;&#26434;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite a sea of interpretability methods that can produce plausible explanations, the field has also empirically seen many failure cases of such methods. In light of these results, it remains unclear for practitioners how to use these methods and choose between them in a principled way. In this paper, we show that for moderately rich model classes (easily satisfied by neural networks), any feature attribution method that is complete and linear -- for example, Integrated Gradients and SHAP -- can provably fail to improve on random guessing for inferring model behaviour. Our results apply to common end-tasks such as characterizing local model behaviour, identifying spurious features, and algorithmic recourse. One takeaway from our work is the importance of concretely defining end-tasks: once such an end-task is defined, a simple and direct approach of repeated model evaluations can outperform many other complex feature attribution methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#25511;&#21046;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23618;&#35268;&#21010;&#22120;&#65292;&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#65292;&#35299;&#20915;&#38271;&#26102;&#38388;&#36328;&#24230;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#19979;&#30340;&#25511;&#21046;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#21333;&#20219;&#21153;&#21644;&#22810;&#20219;&#21153;&#25104;&#21151;&#29575;&#65292;&#24182;&#26497;&#22823;&#22320;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.15629</link><description>&lt;p&gt;
&#35821;&#35328;&#25511;&#21046;&#25193;&#25955;&#65306;&#36890;&#36807;&#31354;&#38388;&#12289;&#26102;&#38388;&#21644;&#20219;&#21153;&#39640;&#25928;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Language Control Diffusion: Efficiently Scaling through Space, Time, and Tasks. (arXiv:2210.15629v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#25511;&#21046;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23618;&#35268;&#21010;&#22120;&#65292;&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#65292;&#35299;&#20915;&#38271;&#26102;&#38388;&#36328;&#24230;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#19979;&#30340;&#25511;&#21046;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#21333;&#20219;&#21153;&#21644;&#22810;&#20219;&#21153;&#25104;&#21151;&#29575;&#65292;&#24182;&#26497;&#22823;&#22320;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#36890;&#29992;&#22411;&#26234;&#33021;&#20307;&#22312;&#21508;&#20010;&#26041;&#38754;&#37117;&#24456;&#22256;&#38590;&#65292;&#38656;&#35201;&#22788;&#29702;&#39640;&#32500;&#36755;&#20837;&#65288;&#31354;&#38388;&#65289;&#12289;&#38271;&#26102;&#38388;&#36328;&#24230;&#65288;&#26102;&#38388;&#65289;&#21644;&#22810;&#20010;&#26032;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#32467;&#26500;&#26041;&#38754;&#30340;&#36827;&#23637;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#27839;&#30528;&#20854;&#20013;&#19968;&#20010;&#25110;&#20004;&#20010;&#32500;&#24230;&#25552;&#39640;&#25193;&#23637;&#24615;&#33021;&#21147;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#20173;&#28982;&#24456;&#39640;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#35821;&#35328;&#25511;&#21046;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#26465;&#20214;&#30340;&#20998;&#23618;&#35268;&#21010;&#22120;&#65288;LCD&#65289;&#26469;&#24212;&#23545;&#36825;&#19977;&#20010;&#26041;&#38754;&#12290;&#25105;&#20204;&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#24212;&#23545;&#26102;&#38388;&#12289;&#29366;&#24577;&#21644;&#20219;&#21153;&#31354;&#38388;&#32500;&#24230;&#30340;&#38271;&#26102;&#38388;&#36328;&#24230;&#25511;&#21046;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;CALVIN&#35821;&#35328;&#26426;&#22120;&#20154;&#22522;&#20934;&#27979;&#35797;&#20013;&#23558;LCD&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;LCD&#22312;&#22810;&#20219;&#21153;&#25104;&#21151;&#29575;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#32780;&#21333;&#20219;&#21153;&#25104;&#21151;&#29575;&#65288;SR&#65289;&#20026;88.7%&#65292;&#36828;&#39640;&#20110;&#20197;&#21069;&#30340;&#26368;&#20339;&#25104;&#32489;82.6%&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training generalist agents is difficult across several axes, requiring us to deal with high-dimensional inputs (space), long horizons (time), and multiple and new tasks. Recent advances with architectures have allowed for improved scaling along one or two of these dimensions, but are still prohibitive computationally. In this paper, we propose to address all three axes by leveraging Language to Control Diffusion models as a hierarchical planner conditioned on language (LCD). We effectively and efficiently scale diffusion models for planning in extended temporal, state, and task dimensions to tackle long horizon control problems conditioned on natural language instructions. We compare LCD with other state-of-the-art models on the CALVIN language robotics benchmark and find that LCD outperforms other SOTA methods in multi task success rates while dramatically improving computational efficiency with a single task success rate (SR) of 88.7% against the previous best of 82.6%. We show that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Vitruvio&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#23558;&#21333;&#28857;&#36879;&#35270;&#33609;&#22270;&#36716;&#25442;&#20026;&#19977;&#32500;&#24314;&#31569;&#32593;&#26684;&#65292;&#38024;&#23545;AEC&#39046;&#22495;&#25552;&#20986;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#22312;&#24314;&#31569;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#23454;&#39564;&#65292;&#26174;&#31034;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#37325;&#24314;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2210.13634</link><description>&lt;p&gt;
Vitruvio&#65306;&#36890;&#36807;&#21333;&#28857;&#36879;&#35270;&#33609;&#22270;&#29983;&#25104;&#19977;&#32500;&#24314;&#31569;&#32593;&#26684;
&lt;/p&gt;
&lt;p&gt;
Vitruvio: 3D Building Meshes via Single Perspective Sketches. (arXiv:2210.13634v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Vitruvio&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#23558;&#21333;&#28857;&#36879;&#35270;&#33609;&#22270;&#36716;&#25442;&#20026;&#19977;&#32500;&#24314;&#31569;&#32593;&#26684;&#65292;&#38024;&#23545;AEC&#39046;&#22495;&#25552;&#20986;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#22312;&#24314;&#31569;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#23454;&#39564;&#65292;&#26174;&#31034;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#37325;&#24314;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#30340;&#24314;&#31569;&#24037;&#31243;&#21644;&#26045;&#24037;(AEC)&#36719;&#20214;&#38656;&#35201;&#23398;&#20064;&#26354;&#32447;&#20197;&#29983;&#25104;&#19977;&#32500;&#24314;&#31569;&#34920;&#31034;&#12290;&#36825;&#38480;&#21046;&#20102;&#36890;&#36807;&#21333;&#20010;&#33609;&#22270;&#24555;&#36895;&#39564;&#35777;&#21021;&#27493;&#35774;&#35745;&#24819;&#27861;&#30340;&#20307;&#31215;&#21547;&#20041;&#30340;&#33021;&#21147;&#12290;&#20801;&#35768;&#35774;&#35745;&#24072;&#23558;&#21333;&#20010;&#33609;&#22270;&#36716;&#25442;&#20026;&#19977;&#32500;&#24314;&#31569;&#23558;&#20351;&#19994;&#20027;&#33021;&#22815;&#31435;&#21363;&#21487;&#35270;&#21270;3D&#39033;&#30446;&#20449;&#24687;&#65292;&#26080;&#38656;&#36827;&#34892;&#35748;&#30693;&#36127;&#33655;&#12290;&#22914;&#26524;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#39537;&#21160;&#21333;&#35270;&#22270;&#37325;&#24314;(SVR)&#26041;&#27861;&#22312;&#21333;&#20010;&#22270;&#20687;&#25110;&#33609;&#22270;&#30340;&#37325;&#24314;&#36807;&#31243;&#20013;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#65292;&#37027;&#20040;&#23427;&#20204;&#22312;AEC&#20013;&#32570;&#20047;&#20855;&#20307;&#24212;&#29992;&#12289;&#20998;&#26512;&#21644;&#23454;&#39564;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#24357;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#20171;&#32461;&#20102;&#31532;&#19968;&#31181;&#19987;&#27880;&#20110;&#24314;&#31569;&#29289;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65306;Vitruvio&#65292;&#26088;&#22312;&#23558;&#21333;&#20010;&#33609;&#22270;&#36716;&#25442;&#20026;&#19977;&#32500;&#24314;&#31569;&#32593;&#26684;&#12290;Vitruvio&#22312;&#29305;&#23450;&#24314;&#31569;&#25968;&#25454;&#38598;(&#26364;&#21704;&#39039;1K)&#19978;&#35843;&#25972;&#20351;&#29992;&#21344;&#20301;&#32593;&#32476;&#36827;&#34892;SVR&#20219;&#21153;&#12290;&#36825;&#31181;&#36866;&#24212;&#24615;&#24102;&#26469;&#20102;&#19982;SOTA&#26041;&#27861;&#30456;&#27604;&#22312;3D&#24314;&#31569;&#37325;&#24314;&#36136;&#37327;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Today's architectural engineering and construction (AEC) software require a learning curve to generate a three-dimension building representation. This limits the ability to quickly validate the volumetric implications of an initial design idea communicated via a single sketch. Allowing designers to translate a single sketch to a 3D building will enable owners to instantly visualize 3D project information without the cognitive load required. If previous state-of-the-art (SOTA) data-driven methods for single view reconstruction (SVR) showed outstanding results in the reconstruction process from a single image or sketch, they lacked specific applications, analysis, and experiments in the AEC. Therefore, this research addresses this gap, introducing the first deep learning method focused only on buildings that aim to convert a single sketch to a 3D building mesh: Vitruvio. Vitruvio adapts Occupancy Network for SVR tasks on a specific building dataset (Manhattan 1K). This adaptation brings 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#21152;&#23454;&#29992;&#30340;&#22312;&#32447;&#33258;&#30417;&#30563;&#32456;&#36523;&#23398;&#20064;&#30340;&#38382;&#39064;&#35774;&#32622;&#65292;&#35813;&#35774;&#32622;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#21040;&#25968;&#25454;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#21644;&#21333;&#27425;&#36941;&#21382;&#12289;&#32570;&#20047;&#22806;&#37096;&#30417;&#30563;&#21644;&#20808;&#39564;&#30693;&#35782;&#31561;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SCALE&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#32431;&#31929;&#22320;&#20174;&#25968;&#25454;&#36830;&#32493;&#20307;&#20013;&#25552;&#21462;&#21644;&#35760;&#24518;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2208.11266</link><description>&lt;p&gt;
SCALE&#65306;&#26080;&#20808;&#39564;&#30693;&#35782;&#30340;&#22312;&#32447;&#33258;&#30417;&#30563;&#32456;&#36523;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SCALE: Online Self-Supervised Lifelong Learning without Prior Knowledge. (arXiv:2208.11266v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#21152;&#23454;&#29992;&#30340;&#22312;&#32447;&#33258;&#30417;&#30563;&#32456;&#36523;&#23398;&#20064;&#30340;&#38382;&#39064;&#35774;&#32622;&#65292;&#35813;&#35774;&#32622;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#21040;&#25968;&#25454;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#21644;&#21333;&#27425;&#36941;&#21382;&#12289;&#32570;&#20047;&#22806;&#37096;&#30417;&#30563;&#21644;&#20808;&#39564;&#30693;&#35782;&#31561;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SCALE&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#32431;&#31929;&#22320;&#20174;&#25968;&#25454;&#36830;&#32493;&#20307;&#20013;&#25552;&#21462;&#21644;&#35760;&#24518;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#32456;&#36523;&#23398;&#20064;&#26159;&#25351;&#33021;&#22815;&#22312;&#27809;&#26377;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#22312;&#26102;&#38388;&#19978;&#23398;&#20064;&#26032;&#30340;&#27169;&#24335;&#24182;&#35760;&#24518;&#20197;&#21069;&#30340;&#27169;&#24335;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#20551;&#35774;&#26377;&#20851;&#36755;&#20837;&#25968;&#25454;&#30340;&#24378;&#20808;&#39564;&#30693;&#35782;&#65288;&#20363;&#22914;&#65292;&#24050;&#30693;&#31867;&#21035;&#36793;&#30028;&#65289;&#65292;&#32780;&#36825;&#22312;&#22797;&#26434;&#21644;&#19981;&#21487;&#39044;&#27979;&#30340;&#29615;&#22659;&#20013;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#33719;&#24471;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#32447;&#33258;&#30417;&#30563;&#32456;&#36523;&#23398;&#20064;&#26080;&#20808;&#39564;&#30693;&#35782;&#30340;&#26356;&#23454;&#36341;&#30340;&#38382;&#39064;&#35774;&#32622;&#12290;&#20026;&#20102;&#24212;&#23545;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SCALE&#30340;&#33258;&#30417;&#30563;&#23545;&#27604;&#32456;&#36523;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#32431;&#31929;&#22320;&#20174;&#25968;&#25454;&#36830;&#32493;&#20307;&#20013;&#25552;&#21462;&#21644;&#35760;&#24518;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised lifelong learning refers to the ability to learn over time while memorizing previous patterns without supervision. Although great progress has been made in this direction, existing work often assumes strong prior knowledge about the incoming data (e.g., knowing the class boundaries), which can be impossible to obtain in complex and unpredictable environments. In this paper, motivated by real-world scenarios, we propose a more practical problem setting called online self-supervised lifelong learning without prior knowledge. The proposed setting is challenging due to the non-iid and single-pass data, the absence of external supervision, and no prior knowledge. To address the challenges, we propose Self-Supervised ContrAstive Lifelong LEarning without Prior Knowledge (SCALE) which can extract and memorize representations on the fly purely from the data continuum. SCALE is designed around three major components: a pseudo-supervised contrastive loss, a self-supervised forgettin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;LogLG&#24369;&#30417;&#30563;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#37319;&#29992;&#20107;&#20214;&#22270;&#26500;&#24314;&#21644;&#20266;&#26631;&#31614;&#29983;&#25104;&#65292;&#26088;&#22312;&#25506;&#32034;&#24207;&#21015;&#20013;&#20851;&#38190;&#23383;&#20043;&#38388;&#30340;&#35821;&#20041;&#32852;&#31995;&#65292;&#26377;&#25928;&#35782;&#21035;&#24212;&#29992;&#31243;&#24207;&#26085;&#24535;&#30340;&#24322;&#24120;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2208.10833</link><description>&lt;p&gt;
LogLG: &#36890;&#36807;&#20107;&#20214;&#22270;&#26500;&#24314;&#23454;&#29616;&#24369;&#30417;&#30563;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
LogLG: Weakly Supervised Log Anomaly Detection via Log-Event Graph Construction. (arXiv:2208.10833v5 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;LogLG&#24369;&#30417;&#30563;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#37319;&#29992;&#20107;&#20214;&#22270;&#26500;&#24314;&#21644;&#20266;&#26631;&#31614;&#29983;&#25104;&#65292;&#26088;&#22312;&#25506;&#32034;&#24207;&#21015;&#20013;&#20851;&#38190;&#23383;&#20043;&#38388;&#30340;&#35821;&#20041;&#32852;&#31995;&#65292;&#26377;&#25928;&#35782;&#21035;&#24212;&#29992;&#31243;&#24207;&#26085;&#24535;&#30340;&#24322;&#24120;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23436;&#20840;&#30417;&#30563;&#30340;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#38656;&#35201;&#20026;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#26085;&#24535;&#25968;&#25454;&#36827;&#34892;&#26631;&#27880;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#21322;&#30417;&#30563;&#26041;&#27861;&#24050;&#32463;&#34987;&#25552;&#20986;&#65292;&#20197;&#38477;&#20302;&#26631;&#27880;&#25104;&#26412;&#65292;&#24182;&#20511;&#21161;&#35299;&#26512;&#27169;&#26495;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#29420;&#31435;&#32771;&#34385;&#27599;&#20010;&#20851;&#38190;&#23383;&#65292;&#24573;&#30053;&#20851;&#38190;&#23383;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21644;&#26085;&#24535;&#24207;&#21015;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#24369;&#30417;&#30563;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#21517;&#20026;LogLG&#65292;&#20197;&#25506;&#32034;&#24207;&#21015;&#20013;&#20851;&#38190;&#23383;&#20043;&#38388;&#30340;&#35821;&#20041;&#32852;&#31995;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#36845;&#20195;&#36807;&#31243;&#65292;&#39318;&#20808;&#25552;&#21462;&#26410;&#26631;&#35760;&#26085;&#24535;&#30340;&#20851;&#38190;&#23383;&#20197;&#26500;&#24314;&#26085;&#24535;&#20107;&#20214;&#22270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#23376;&#22270;&#27880;&#37322;&#22120;&#29983;&#25104;&#26410;&#26631;&#35760;&#26085;&#24535;&#24207;&#21015;&#30340;&#20266;&#26631;&#31614;&#12290;&#20026;&#20102;&#25913;&#21892;&#27880;&#37322;&#36136;&#37327;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#33258;&#30417;&#30563;&#20219;&#21153;&#26469;&#39044;&#35757;&#32451;&#23376;&#22270;&#27880;&#37322;&#22120;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#29983;&#25104;&#30340;&#20266;&#26631;&#31614;&#35757;&#32451;&#26816;&#27979;&#27169;&#22411;&#12290;&#22312;&#26465;&#20214;&#160;&#19979;&#65292;&#27169;&#22411;&#33021;&#22815;&#35782;&#21035;&#26469;&#33258;&#24212;&#29992;&#31243;&#24207;&#26085;&#24535;&#30340;&#24322;&#24120;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fully supervised log anomaly detection methods suffer the heavy burden of annotating massive unlabeled log data. Recently, many semi-supervised methods have been proposed to reduce annotation costs with the help of parsed templates. However, these methods consider each keyword independently, which disregards the correlation between keywords and the contextual relationships among log sequences. In this paper, we propose a novel weakly supervised log anomaly detection framework, named LogLG, to explore the semantic connections among keywords from sequences. Specifically, we design an end-to-end iterative process, where the keywords of unlabeled logs are first extracted to construct a log-event graph. Then, we build a subgraph annotator to generate pseudo labels for unlabeled log sequences. To ameliorate the annotation quality, we adopt a self-supervised task to pre-train a subgraph annotator. After that, a detection model is trained with the generated pseudo labels. Conditioned on the cl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#20004;&#27493;&#31070;&#32463;&#32593;&#32476;TMO&#65292;&#20855;&#26377;&#33258;&#26657;&#20934;&#21644;&#24863;&#30693;&#20248;&#21270;&#21151;&#33021;&#65292;&#21487;&#20197;&#23558;HDR&#22270;&#20687;&#21387;&#32553;&#21040;LDR&#22270;&#20687;&#65292;&#21516;&#26102;&#36890;&#36807;&#24863;&#30693;&#24230;&#37327;&#23454;&#29616;&#20102;&#28789;&#25935;&#30340;&#36136;&#37327;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2206.09146</link><description>&lt;p&gt;
&#19968;&#31181;&#24863;&#30693;&#20248;&#21270;&#19988;&#33258;&#26657;&#20934;&#30340;&#33394;&#35843;&#26144;&#23556;&#36816;&#31639;&#31526;
&lt;/p&gt;
&lt;p&gt;
A Perceptually Optimized and Self-Calibrated Tone Mapping Operator. (arXiv:2206.09146v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#20004;&#27493;&#31070;&#32463;&#32593;&#32476;TMO&#65292;&#20855;&#26377;&#33258;&#26657;&#20934;&#21644;&#24863;&#30693;&#20248;&#21270;&#21151;&#33021;&#65292;&#21487;&#20197;&#23558;HDR&#22270;&#20687;&#21387;&#32553;&#21040;LDR&#22270;&#20687;&#65292;&#21516;&#26102;&#36890;&#36807;&#24863;&#30693;&#24230;&#37327;&#23454;&#29616;&#20102;&#28789;&#25935;&#30340;&#36136;&#37327;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39640;&#21160;&#24577;&#33539;&#22260;&#65288;HDR&#65289;&#25668;&#24433;&#30340;&#26222;&#21450;&#21644;&#21487;&#35775;&#38382;&#24615;&#22686;&#21152;&#65292;&#23545;&#21160;&#24577;&#33539;&#22260;&#21387;&#32553;&#30340;&#33394;&#35843;&#26144;&#23556;&#36816;&#31639;&#31526;&#65288;TMO&#65289;&#38656;&#27714;&#26085;&#30410;&#22686;&#21152;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#20004;&#27493;&#31070;&#32463;&#32593;&#32476;TMO&#65292;&#20855;&#26377;&#33258;&#26657;&#20934;&#21644;&#24863;&#30693;&#20248;&#21270;&#21151;&#33021;&#12290;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;HDR&#22270;&#20687;&#20998;&#35299;&#25104;&#35268;&#33539;&#21270;&#25289;&#26222;&#25289;&#26031;&#37329;&#23383;&#22612;&#65292;&#28982;&#21518;&#20351;&#29992;&#20004;&#20010;&#36731;&#37327;&#32423;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#65292;&#20197;&#35268;&#33539;&#21270;&#34920;&#31034;&#20316;&#20026;&#36755;&#20837;&#26469;&#20272;&#35745;&#30456;&#24212;LDR&#22270;&#20687;&#30340;&#25289;&#26222;&#25289;&#26031;&#37329;&#23383;&#22612;&#12290;&#25105;&#20204;&#36890;&#36807;&#26368;&#23567;&#21270;&#35268;&#33539;&#21270;&#25289;&#26222;&#25289;&#26031;&#37329;&#23383;&#22612;&#36317;&#31163;&#65288;NLPD&#65289;&#26469;&#20248;&#21270;&#33394;&#35843;&#26144;&#23556;&#32593;&#32476;&#65292;&#36825;&#26159;&#19982;&#20154;&#31867;&#23545;&#33394;&#35843;&#26144;&#23556;&#22270;&#20687;&#36136;&#37327;&#30340;&#21028;&#26029;&#30456;&#19968;&#33268;&#30340;&#24863;&#30693;&#24230;&#37327;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#20013;&#65292;&#36755;&#20837;&#30340;HDR&#22270;&#20687;&#26159;&#33258;&#26657;&#20934;&#30340;&#65292;&#20197;&#35745;&#31639;&#20986;&#26368;&#32456;&#30340;LDR&#22270;&#20687;&#12290;&#25105;&#20204;&#23558;&#21516;&#19968;&#24352;HDR&#22270;&#20687;&#36755;&#20837;&#32463;&#36807;&#19981;&#21516;&#26368;&#22823;&#20142;&#24230;&#37325;&#26032;&#35843;&#25972;&#27604;&#20363;&#21518;&#30340;&#23398;&#20064;&#33394;&#35843;&#26144;&#23556;&#32593;&#32476;&#20013;&#65292;&#26469;&#23436;&#25104;&#36825;&#19968;&#33258;&#26657;&#20934;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing popularity and accessibility of high dynamic range (HDR) photography, tone mapping operators (TMOs) for dynamic range compression are practically demanding. In this paper, we develop a two-stage neural network-based TMO that is self-calibrated and perceptually optimized. In Stage one, motivated by the physiology of the early stages of the human visual system, we first decompose an HDR image into a normalized Laplacian pyramid. We then use two lightweight deep neural networks (DNNs), taking the normalized representation as input and estimating the Laplacian pyramid of the corresponding LDR image. We optimize the tone mapping network by minimizing the normalized Laplacian pyramid distance (NLPD), a perceptual metric aligning with human judgments of tone-mapped image quality. In Stage two, the input HDR image is self-calibrated to compute the final LDR image. We feed the same HDR image but rescaled with different maximum luminances to the learned tone mapping network, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#30913;&#38236;&#38754;&#19979;&#38477;&#31639;&#27861;&#20316;&#20026;&#22343;&#34913;&#27714;&#35299;&#22120;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#21253;&#25324;&#23454;&#29616;&#20102;&#32447;&#24615;&#25910;&#25947;&#30340;&#30456;&#24212;&#22343;&#34913;&#27714;&#35299;&#22120;&#21644;&#22312;&#34920;&#26684;&#24335;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#19982;CFR&#30456;&#31454;&#20105;&#30340;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#21450;&#22312;&#8220;&#40657;&#26263;&#20845;&#35282;&#8221;&#21644;&#8220;&#24187;&#35937;&#20117;&#23383;&#8221;&#20013;&#30340;&#33258;&#25105;&#29609;&#32781;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#33391;&#22909;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2206.05825</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#12289;&#37327;&#21270;&#30456;&#24212;&#22343;&#34913;&#21644;&#20108;&#20154;&#38646;&#21644;&#21338;&#24328;&#30340;&#32479;&#19968;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Unified Approach to Reinforcement Learning, Quantal Response Equilibria, and Two-Player Zero-Sum Games. (arXiv:2206.05825v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.05825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#30913;&#38236;&#38754;&#19979;&#38477;&#31639;&#27861;&#20316;&#20026;&#22343;&#34913;&#27714;&#35299;&#22120;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#21253;&#25324;&#23454;&#29616;&#20102;&#32447;&#24615;&#25910;&#25947;&#30340;&#30456;&#24212;&#22343;&#34913;&#27714;&#35299;&#22120;&#21644;&#22312;&#34920;&#26684;&#24335;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#19982;CFR&#30456;&#31454;&#20105;&#30340;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#21450;&#22312;&#8220;&#40657;&#26263;&#20845;&#35282;&#8221;&#21644;&#8220;&#24187;&#35937;&#20117;&#23383;&#8221;&#20013;&#30340;&#33258;&#25105;&#29609;&#32781;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#30913;&#38236;&#38754;&#19979;&#38477;&#65292;&#23427;&#21463;&#21040;&#38236;&#38754;&#19979;&#38477;&#21644;&#38750;&#27431;&#20960;&#37324;&#24503;&#36817;&#31471;&#26799;&#24230;&#31639;&#27861;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#22312;&#20110;&#23637;&#31034;&#20102;&#30913;&#38236;&#38754;&#19979;&#38477;&#20316;&#20026;&#22343;&#34913;&#27714;&#35299;&#22120;&#20197;&#21450;&#22312;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#20013;&#20316;&#20026;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#20248;&#28857;&#12290;&#36825;&#20123;&#20248;&#28857;&#21253;&#25324;&#65306;1) &#25104;&#20026;&#39318;&#20010;&#23545;&#20110;&#20855;&#26377;&#19968;&#38454;&#21453;&#39304;&#30340;&#25193;&#23637;&#24418;&#24335;&#28216;&#25103;&#23454;&#29616;&#32447;&#24615;&#25910;&#25947;&#30340;&#37327;&#21270;&#30456;&#24212;&#22343;&#34913;&#27714;&#35299;&#22120;&#65307;2) &#25104;&#20026;&#39318;&#20010;&#22312;&#34920;&#26684;&#24335;&#35774;&#32622;&#20013;&#19982;CFR&#23454;&#29616;&#23454;&#39564;&#24615;&#31454;&#20105;&#32467;&#26524;&#30340;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65307;3) &#23454;&#29616;&#20102;&#22312;3x3&#40657;&#26263;&#20845;&#35282;&#21644;&#24187;&#35937;&#20117;&#23383;&#28216;&#25103;&#20013;&#25104;&#20026;&#33258;&#25105;&#28216;&#25103;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26377;&#21033;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work studies an algorithm, which we call magnetic mirror descent, that is inspired by mirror descent and the non-Euclidean proximal gradient algorithm. Our contribution is demonstrating the virtues of magnetic mirror descent as both an equilibrium solver and as an approach to reinforcement learning in two-player zero-sum games. These virtues include: 1) Being the first quantal response equilibria solver to achieve linear convergence for extensive-form games with first order feedback; 2) Being the first standard reinforcement learning algorithm to achieve empirically competitive results with CFR in tabular settings; 3) Achieving favorable performance in 3x3 Dark Hex and Phantom Tic-Tac-Toe as a self-play deep reinforcement learning algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;DPSNN&#65289;&#65292;&#23558;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#19982;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#20445;&#25345;&#20102;SNN&#30340;&#24378;&#38544;&#31169;&#20445;&#25252;&#65307;&#21516;&#26102;&#25552;&#20986;&#20102;&#26102;&#24207;&#22686;&#24378;&#27744;&#21270;&#65288;TEP&#65289;&#26041;&#27861;&#65292;&#20351;SNN&#33021;&#22815;&#33719;&#24471;&#26356;&#22909;&#30340;&#20449;&#24687;&#20256;&#36755;&#12290;&#22312;MNIST&#21644;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.12718</link><description>&lt;p&gt;
DPSNN: &#24046;&#20998;&#38544;&#31169;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#19982;&#26102;&#24207;&#22686;&#24378;&#27744;&#21270;
&lt;/p&gt;
&lt;p&gt;
DPSNN: A Differentially Private Spiking Neural Network with Temporal Enhanced Pooling. (arXiv:2205.12718v3 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;DPSNN&#65289;&#65292;&#23558;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#19982;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#20445;&#25345;&#20102;SNN&#30340;&#24378;&#38544;&#31169;&#20445;&#25252;&#65307;&#21516;&#26102;&#25552;&#20986;&#20102;&#26102;&#24207;&#22686;&#24378;&#27744;&#21270;&#65288;TEP&#65289;&#26041;&#27861;&#65292;&#20351;SNN&#33021;&#22815;&#33719;&#24471;&#26356;&#22909;&#30340;&#20449;&#24687;&#20256;&#36755;&#12290;&#22312;MNIST&#21644;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#26159;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#30446;&#21069;&#30340;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#24120;&#24120;&#32467;&#21512;&#20256;&#32479;&#22522;&#20110;&#23454;&#25968;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#12290;&#20316;&#20026;&#26032;&#19968;&#20195;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#23545;SNN&#30340;&#38544;&#31169;&#20445;&#25252;&#36827;&#34892;&#30740;&#31350;&#12290;&#26412;&#25991;&#23558;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#31639;&#27861;&#19982;SNN&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;DPSNN&#65289;&#12290;SNN&#20351;&#29992;&#31163;&#25955;&#30340;&#33033;&#20914;&#24207;&#21015;&#26469;&#20256;&#36755;&#20449;&#24687;&#65292;&#24182;&#32467;&#21512;DP&#24341;&#20837;&#30340;&#26799;&#24230;&#22122;&#22768;&#65292;&#20174;&#32780;&#20351;SNN&#20445;&#25345;&#24378;&#22823;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#20351;SNN&#22312;&#33719;&#24471;&#39640;&#38544;&#31169;&#20445;&#25252;&#30340;&#21516;&#26102;&#20445;&#25345;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26102;&#24207;&#22686;&#24378;&#27744;&#21270;&#65288;TEP&#65289;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20805;&#20998;&#23558;SNN&#30340;&#26102;&#24207;&#20449;&#24687;&#19982;&#31354;&#38388;&#20449;&#24687;&#20256;&#36755;&#32467;&#21512;&#36215;&#26469;&#65292;&#20351;SNN&#33021;&#22815;&#26356;&#22909;&#22320;&#36827;&#34892;&#20449;&#24687;&#20256;&#36755;&#12290;&#25105;&#20204;&#23545;DPSNN&#22312;MNIST&#21644;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy protection is a crucial issue in machine learning algorithms, and the current privacy protection is combined with traditional artificial neural networks based on real values. Spiking neural network (SNN), the new generation of artificial neural networks, plays a crucial role in many fields. Therefore, research on the privacy protection of SNN is urgently needed. This paper combines the differential privacy(DP) algorithm with SNN and proposes a differentially private spiking neural network (DPSNN). The SNN uses discrete spike sequences to transmit information, combined with the gradient noise introduced by DP so that SNN maintains strong privacy protection. At the same time, to make SNN maintain high performance while obtaining high privacy protection, we propose the temporal enhanced pooling (TEP) method. It fully integrates the temporal information of SNN into the spatial information transfer, which enables SNN to perform better information transfer. We conduct experiments on 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20215;&#20540;&#30693;&#35782;&#25972;&#21512;&#30340;&#39640;&#25928;&#20869;&#23384;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20174;&#30446;&#26631; Q &#32593;&#32476;&#21521;&#24403;&#21069; Q &#32593;&#32476;&#25972;&#21512;&#30693;&#35782;&#65292;&#20943;&#23569;&#36951;&#24536;&#24182;&#20445;&#25345;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#26412;&#31639;&#27861;&#22312;&#29305;&#24449;&#21644;&#22270;&#20687;&#20219;&#21153;&#20013;&#34920;&#29616;&#30456;&#24403;&#25110;&#26356;&#22909;&#65292;&#21516;&#26102;&#20943;&#36731;&#20102;&#22823;&#32463;&#39564;&#37325;&#25918;&#32531;&#23384;&#22120;&#24102;&#26469;&#30340;&#20869;&#23384;&#36127;&#25285;&#12290;</title><link>http://arxiv.org/abs/2205.10868</link><description>&lt;p&gt;
&#22522;&#20110;&#20215;&#20540;&#30693;&#35782;&#25972;&#21512;&#30340;&#39640;&#25928;&#20869;&#23384;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Memory-efficient Reinforcement Learning with Value-based Knowledge Consolidation. (arXiv:2205.10868v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20215;&#20540;&#30693;&#35782;&#25972;&#21512;&#30340;&#39640;&#25928;&#20869;&#23384;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20174;&#30446;&#26631; Q &#32593;&#32476;&#21521;&#24403;&#21069; Q &#32593;&#32476;&#25972;&#21512;&#30693;&#35782;&#65292;&#20943;&#23569;&#36951;&#24536;&#24182;&#20445;&#25345;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#26412;&#31639;&#27861;&#22312;&#29305;&#24449;&#21644;&#22270;&#20687;&#20219;&#21153;&#20013;&#34920;&#29616;&#30456;&#24403;&#25110;&#26356;&#22909;&#65292;&#21516;&#26102;&#20943;&#36731;&#20102;&#22823;&#32463;&#39564;&#37325;&#25918;&#32531;&#23384;&#22120;&#24102;&#26469;&#30340;&#20869;&#23384;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#19978;&#38750;&#24120;&#26377;&#21069;&#36884;&#65292;&#20294;&#38754;&#20020;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#35757;&#32451;&#38590;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#38750;&#29420;&#31435;&#25110;&#38750;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#22256;&#38590;&#12290;&#32463;&#39564;&#37325;&#25918;&#32531;&#23384;&#22120;&#26159;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26631;&#20934;&#32452;&#20214;&#65292;&#36890;&#24120;&#36890;&#36807;&#23558;&#32463;&#39564;&#23384;&#20648;&#22312;&#22823;&#32531;&#23384;&#22120;&#20013;&#24182;&#24310;&#36831;&#20351;&#29992;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#20943;&#23569;&#36951;&#24536;&#24182;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#37325;&#25918;&#32531;&#23384;&#22120;&#20250;&#23548;&#33268;&#37325;&#36127;&#36733;&#20869;&#23384;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20869;&#23384;&#23481;&#37327;&#26377;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#21644;&#23884;&#20837;&#24335;&#35774;&#22791;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230; Q &#32593;&#32476;&#31639;&#27861;&#30340;&#20869;&#23384;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#36807;&#20174;&#30446;&#26631; Q &#32593;&#32476;&#21521;&#24403;&#21069; Q &#32593;&#32476;&#25972;&#21512;&#30693;&#35782;&#65292;&#20943;&#23569;&#36951;&#24536;&#24182;&#20445;&#25345;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#19982;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22522;&#20110;&#29305;&#24449;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#21487;&#27604;&#36739;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#36731;&#20102;&#22823;&#32463;&#39564;&#37325;&#25918;&#32531;&#23384;&#22120;&#30340;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial neural networks are promising for general function approximation but challenging to train on non-independent or non-identically distributed data due to catastrophic forgetting. The experience replay buffer, a standard component in deep reinforcement learning, is often used to reduce forgetting and improve sample efficiency by storing experiences in a large buffer and using them for training later. However, a large replay buffer results in a heavy memory burden, especially for onboard and edge devices with limited memory capacities. We propose memory-efficient reinforcement learning algorithms based on the deep Q-network algorithm to alleviate this problem. Our algorithms reduce forgetting and maintain high sample efficiency by consolidating knowledge from the target Q-network to the current Q-network. Compared to baseline methods, our algorithms achieve comparable or better performance in both feature-based and image-based tasks while easing the burden of large experience re
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27719;&#24635;&#20102;65&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;ABSA&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;45&#20010;&#33521;&#25991;&#25968;&#25454;&#38598;&#21644;20&#20010;&#20854;&#20182;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#33258;&#20027;ABSA&#31995;&#32479;&#30340;&#25968;&#25454;&#24211;&#12290;</title><link>http://arxiv.org/abs/2204.05232</link><description>&lt;p&gt;
&#38754;&#21521;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Survey of Aspect-based Sentiment Analysis Datasets. (arXiv:2204.05232v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.05232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27719;&#24635;&#20102;65&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;ABSA&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;45&#20010;&#33521;&#25991;&#25968;&#25454;&#38598;&#21644;20&#20010;&#20854;&#20182;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#33258;&#20027;ABSA&#31995;&#32479;&#30340;&#25968;&#25454;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;(ABSA)&#26159;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#65292;&#38656;&#35201;&#20998;&#26512;&#29992;&#25143;&#29983;&#25104;&#30340;&#35780;&#35770;&#20197;&#30830;&#23450;&#65306;a)&#27491;&#22312;&#23457;&#26597;&#30340;&#30446;&#26631;&#23454;&#20307;&#65292;b)&#23646;&#20110;&#21738;&#20010;&#39640;&#32423;&#26041;&#38754;&#65292;c)&#23545;&#30446;&#26631;&#21644;&#26041;&#38754;&#34920;&#36798;&#30340;&#24773;&#24863;&#12290;ABSA&#30340;&#20247;&#22810;&#20294;&#20998;&#25955;&#30340;&#35821;&#26009;&#24211;&#20351;&#30740;&#31350;&#20154;&#21592;&#24456;&#38590;&#24555;&#36895;&#30830;&#23450;&#26368;&#36866;&#21512;&#29305;&#23450;ABSA&#23376;&#20219;&#21153;&#30340;&#35821;&#26009;&#24211;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#33258;&#20027;ABSA&#31995;&#32479;&#30340;&#25968;&#25454;&#24211;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;ABSA&#21644;&#20854;&#23376;&#20219;&#21153;&#30340;&#20027;&#35201;&#35821;&#26009;&#24211;&#27010;&#36848;&#65292;&#24182;&#24378;&#35843;&#30740;&#31350;&#20154;&#21592;&#22312;&#36873;&#25321;&#35821;&#26009;&#24211;&#26102;&#24212;&#32771;&#34385;&#30340;&#20960;&#20010;&#29305;&#24449;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24403;&#21069;&#25910;&#38598;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#24182;&#20026;&#26410;&#26469;&#35821;&#26009;&#24211;&#21019;&#24314;&#25552;&#20986;&#24314;&#35758;&#12290;&#26412;&#35843;&#26597;&#23457;&#26680;&#20102;65&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;ABSA&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;25&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;45&#20010;&#33521;&#35821;&#21644;20&#20010;&#20854;&#20182;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect-based sentiment analysis (ABSA) is a natural language processing problem that requires analyzing user-generated reviews to determine: a) The target entity being reviewed, b) The high-level aspect to which it belongs, and c) The sentiment expressed toward the targets and the aspects. Numerous yet scattered corpora for ABSA make it difficult for researchers to identify corpora best suited for a specific ABSA subtask quickly. This study aims to present a database of corpora that can be used to train and assess autonomous ABSA systems. Additionally, we provide an overview of the major corpora for ABSA and its subtasks and highlight several features that researchers should consider when selecting a corpus. Finally, we discuss the advantages and disadvantages of current collection approaches and make recommendations for future corpora creation. This survey examines 65 publicly available ABSA datasets covering over 25 domains, including 45 English and 20 other languages datasets.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#27850;&#26494;&#28151;&#21512;&#32593;&#32476;&#26041;&#27861;&#65288;DPMN&#65289;&#65292;&#33021;&#22815;&#22312;&#23384;&#22312;&#21487;&#38752;&#30340;&#23618;&#32423;&#20449;&#24687;&#26102;&#36827;&#34892;&#20934;&#30830;&#21644;&#19968;&#33268;&#30340;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#20445;&#35777;&#23618;&#32423;&#19968;&#33268;&#24615;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#30340;&#23618;&#32423;&#39044;&#27979;&#38382;&#39064;&#20013;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2110.13179</link><description>&lt;p&gt;
&#28145;&#24230;&#27850;&#26494;&#28151;&#21512;&#27169;&#22411;&#30340;&#27010;&#29575;&#23618;&#32423;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Hierarchical Forecasting with Deep Poisson Mixtures. (arXiv:2110.13179v8 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.13179
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#27850;&#26494;&#28151;&#21512;&#32593;&#32476;&#26041;&#27861;&#65288;DPMN&#65289;&#65292;&#33021;&#22815;&#22312;&#23384;&#22312;&#21487;&#38752;&#30340;&#23618;&#32423;&#20449;&#24687;&#26102;&#36827;&#34892;&#20934;&#30830;&#21644;&#19968;&#33268;&#30340;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#20445;&#35777;&#23618;&#32423;&#19968;&#33268;&#24615;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#30340;&#23618;&#32423;&#39044;&#27979;&#38382;&#39064;&#20013;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#32423;&#39044;&#27979;&#38382;&#39064;&#22312;&#26102;&#38388;&#24207;&#21015;&#20855;&#26377;&#33258;&#28982;&#30340;&#20998;&#32452;&#32467;&#26500;&#26102;&#20986;&#29616;&#65292;&#24182;&#38656;&#35201;&#22312;&#19981;&#21516;&#23618;&#27425;&#30340;&#32858;&#21512;&#21644;&#20998;&#35299;&#20013;&#36827;&#34892;&#39044;&#27979;&#12290;&#22312;&#36825;&#31181;&#38382;&#39064;&#20013;&#65292;&#36890;&#24120;&#24076;&#26395;&#22312;&#32473;&#23450;&#30340;&#23618;&#27425;&#32467;&#26500;&#20013;&#28385;&#36275;&#32858;&#21512;&#32422;&#26463;&#65292;&#31216;&#20026;&#23618;&#32423;&#19968;&#33268;&#24615;&#12290;&#22312;&#27010;&#29575;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#65292;&#20445;&#25345;&#19968;&#33268;&#24615;&#21516;&#26102;&#20135;&#29983;&#20934;&#30830;&#30340;&#39044;&#27979;&#21487;&#33021;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#24403;&#23384;&#22312;&#21487;&#38752;&#30340;&#23618;&#32423;&#20449;&#24687;&#26102;&#65292;&#21487;&#20197;&#36827;&#34892;&#20934;&#30830;&#21644;&#19968;&#33268;&#30340;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#28145;&#24230;&#27850;&#26494;&#28151;&#21512;&#32593;&#32476;&#65288;DPMN&#65289;&#12290;&#23427;&#20381;&#36182;&#20110;&#31070;&#32463;&#32593;&#32476;&#21644;&#19968;&#31181;&#32479;&#35745;&#27169;&#22411;&#30340;&#32452;&#21512;&#65292;&#29992;&#20110;&#23618;&#27425;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#32467;&#26500;&#30340;&#32852;&#21512;&#20998;&#24067;&#12290;&#36890;&#36807;&#26500;&#24314;&#65292;&#35813;&#27169;&#22411;&#20445;&#35777;&#20102;&#23618;&#32423;&#19968;&#33268;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#29992;&#20110;&#39044;&#27979;&#20998;&#24067;&#32858;&#21512;&#21644;&#20998;&#35299;&#30340;&#31616;&#21333;&#35268;&#21017;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#23618;&#32423;&#39044;&#27979;&#38382;&#39064;&#8212;M4&#31454;&#36187;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#22312;&#38750;&#38598;&#25104;&#26041;&#27861;&#20013;&#65292;DPMN&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical forecasting problems arise when time series have a natural group structure, and predictions at multiple levels of aggregation and disaggregation across the groups are needed. In such problems, it is often desired to satisfy the aggregation constraints in a given hierarchy, referred to as hierarchical coherence in the literature. Maintaining coherence while producing accurate forecasts can be a challenging problem, especially in the case of probabilistic forecasting. We present a novel method capable of accurate and coherent probabilistic forecasts for time series when reliable hierarchical information is present. We call it Deep Poisson Mixture Network (DPMN). It relies on the combination of neural networks and a statistical model for the joint distribution of the hierarchical multivariate time series structure. By construction, the model guarantees hierarchical coherence and provides simple rules for aggregation and disaggregation of the predictive distributions. We perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22823;&#20559;&#24046;&#29702;&#35770;&#24314;&#31435;&#20102;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#19982;&#38750;&#24179;&#34913;&#32479;&#35745;&#21147;&#23398;&#30340;&#32852;&#31995;&#65292;&#22312;&#38271;&#26102;&#38388;&#26497;&#38480;&#19979;&#25512;&#23548;&#20986;&#20102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#27169;&#22411;&#20013;&#26368;&#20248;&#31574;&#30053;&#21644;&#26368;&#20248;&#21160;&#24577;&#30340;&#31934;&#30830;&#35299;&#26512;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#26512;&#21644;&#35745;&#31639;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2106.03931</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#20559;&#24046;&#29702;&#35770;&#30340;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Entropy Regularized Reinforcement Learning Using Large Deviation Theory. (arXiv:2106.03931v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.03931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22823;&#20559;&#24046;&#29702;&#35770;&#24314;&#31435;&#20102;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#19982;&#38750;&#24179;&#34913;&#32479;&#35745;&#21147;&#23398;&#30340;&#32852;&#31995;&#65292;&#22312;&#38271;&#26102;&#38388;&#26497;&#38480;&#19979;&#25512;&#23548;&#20986;&#20102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#27169;&#22411;&#20013;&#26368;&#20248;&#31574;&#30053;&#21644;&#26368;&#20248;&#21160;&#24577;&#30340;&#31934;&#30830;&#35299;&#26512;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#26512;&#21644;&#35745;&#31639;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#24212;&#29992;&#20110;&#29289;&#29702;&#23398;&#20013;&#30340;&#22797;&#26434;&#20248;&#21270;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#29289;&#29702;&#23398;&#20013;&#30340;&#27010;&#24565;&#20063;&#20026;&#24378;&#21270;&#23398;&#20064;&#24102;&#26469;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#22914;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#20013;&#20248;&#21270;&#30340;&#35299;&#26512;&#35299;&#30446;&#21069;&#26159;&#19968;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#19982;&#38750;&#24179;&#34913;&#32479;&#35745;&#21147;&#23398;&#30340;&#32852;&#31995;&#65292;&#37325;&#28857;&#20851;&#27880;&#22312;&#32597;&#35265;&#20107;&#20214;&#26465;&#20214;&#19979;&#30340;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#12290;&#22312;&#38271;&#26102;&#38388;&#26497;&#38480;&#19979;&#65292;&#25105;&#20204;&#24212;&#29992;&#22823;&#20559;&#24046;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#25512;&#23548;&#20986;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#27169;&#22411;&#20013;&#26368;&#20248;&#31574;&#30053;&#21644;&#26368;&#20248;&#21160;&#24577;&#30340;&#31934;&#30830;&#35299;&#26512;&#32467;&#26524;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#19968;&#20010;&#26032;&#30340;&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#26512;&#21644;&#35745;&#31639;&#26694;&#26550;&#65292;&#32463;&#36807;&#27169;&#25311;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) is an important field of research in machine learning that is increasingly being applied to complex optimization problems in physics. In parallel, concepts from physics have contributed to important advances in RL with developments such as entropy-regularized RL. While these developments have led to advances in both fields, obtaining analytical solutions for optimization in entropy-regularized RL is currently an open problem. In this paper, we establish a mapping between entropy-regularized RL and research in non-equilibrium statistical mechanics focusing on Markovian processes conditioned on rare events. In the long-time limit, we apply approaches from large deviation theory to derive exact analytical results for the optimal policy and optimal dynamics in Markov Decision Process (MDP) models of reinforcement learning. The results obtained lead to a novel analytical and computational framework for entropy-regularized RL which is validated by simulations. The
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39118;&#38505;&#24847;&#35782;&#30340;&#31163;&#32447;&#31574;&#30053;&#36873;&#25321;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#8220;&#21033;&#29992;&#8221;&#19982;&#8220;&#35880;&#24910;&#8221;(EvC)&#30340;&#33539;&#24335;&#65292;&#36873;&#21462;&#26435;&#37325;&#20540;&#26368;&#22823;&#21270;&#19988;&#21516;&#26102;&#32771;&#34385;&#38271;&#26399;&#22870;&#21169;&#21644;&#39118;&#38505;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2105.13431</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#39118;&#38505;&#24847;&#35782;&#30340;&#36125;&#21494;&#26031;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#31163;&#32447;&#31574;&#30053;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Offline Risk-aware Policy Selection Method for Bayesian Markov Decision Processes. (arXiv:2105.13431v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.13431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39118;&#38505;&#24847;&#35782;&#30340;&#31163;&#32447;&#31574;&#30053;&#36873;&#25321;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#8220;&#21033;&#29992;&#8221;&#19982;&#8220;&#35880;&#24910;&#8221;(EvC)&#30340;&#33539;&#24335;&#65292;&#36873;&#21462;&#26435;&#37325;&#20540;&#26368;&#22823;&#21270;&#19988;&#21516;&#26102;&#32771;&#34385;&#38271;&#26399;&#22870;&#21169;&#21644;&#39118;&#38505;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35268;&#21010;&#30340;&#31163;&#32447;&#27169;&#22411;&#23398;&#20064;&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#26377;&#38480;&#30340;&#25968;&#25454;&#38598;&#38480;&#21046;&#20102;&#30456;&#23545;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#30340;&#20215;&#20540;&#20989;&#25968;&#30340;&#20272;&#35745;&#12290;&#22240;&#27492;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#33719;&#24471;&#30340;&#31574;&#30053;&#30340;&#24615;&#33021;&#21463;&#21040;&#38480;&#21046;&#65292;&#24182;&#21487;&#33021;&#23384;&#22312;&#39118;&#38505;&#65292;&#29305;&#21035;&#26159;&#24403;&#37096;&#32626;&#38169;&#35823;&#30340;&#31574;&#30053;&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#21518;&#26524;&#26102;&#12290;&#20026;&#27492;&#65292;&#20026;&#20102;&#20943;&#23569;&#27169;&#22411;&#35823;&#24046;&#65288;&#25110;&#23398;&#20064;&#27169;&#22411;&#19982;&#23454;&#38469;&#27169;&#22411;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#65289;&#24182;&#24191;&#27867;&#22320;&#33719;&#24471;&#39118;&#38505;&#24863;&#30693;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#27491;&#22312;&#37319;&#21462;&#22810;&#31181;&#36884;&#24452;&#12290;&#20294;&#22312;&#26368;&#32456;&#24212;&#29992;&#20013;&#65292;&#20174;&#21738;&#20010;&#35282;&#24230;&#20986;&#21457;&#36873;&#25321;&#22522;&#32447;&#21602;&#65311;&#22312;&#35745;&#31639;&#26102;&#38388;&#19981;&#26159;&#38382;&#39064;&#32780;&#40065;&#26834;&#24615;&#26159;&#39318;&#35201;&#32771;&#34385;&#22240;&#32032;&#30340;&#31163;&#32447;&#24773;&#24418;&#19979;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#21033;&#29992;&#8221;&#19982;&#8220;&#35880;&#24910;&#8221;(EvC)&#30340;&#33539;&#24335;&#65292;&#35813;&#33539;&#24335;(1)&#21033;&#29992;&#36125;&#21494;&#26031;&#24418;&#24335;&#20027;&#24352;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#21516;&#26102;(2)&#36873;&#25321;&#26368;&#22823;&#21270;&#39118;&#38505;&#26435;&#37325;&#20540;&#21644;&#38271;&#26399;&#22870;&#21169;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Offline Model Learning for Planning and in Offline Reinforcement Learning, the limited data set hinders the estimate of the Value function of the relative Markov Decision Process (MDP). Consequently, the performance of the obtained policy in the real world is bounded and possibly risky, especially when the deployment of a wrong policy can lead to catastrophic consequences. For this reason, several pathways are being followed with the scope of reducing the model error (or the distributional shift between the learned model and the true one) and, more broadly, obtaining risk-aware solutions with respect to model uncertainty. But when it comes to the final application which baseline should a practitioner choose? In an offline context where computational time is not an issue and robustness is the priority we propose Exploitation vs Caution (EvC), a paradigm that (1) elegantly incorporates model uncertainty abiding by the Bayesian formalism, and (2) selects the policy that maximizes a ris
&lt;/p&gt;</description></item></channel></rss>