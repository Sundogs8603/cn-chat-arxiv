<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;&#29992;&#25143;&#37027;&#37324;&#30452;&#25509;&#33719;&#21462;&#21453;&#39304;&#26469;&#35782;&#21035;&#20010;&#24615;&#21270;&#30340;&#26080;&#20851;&#32039;&#35201;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#24182;&#33719;&#24471;&#36866;&#24212;&#20010;&#24615;&#21270;&#29992;&#25143;&#30446;&#26631;&#30340;&#25919;&#31574;&#12290;</title><link>http://arxiv.org/abs/2307.06333</link><description>&lt;p&gt;
&#35786;&#26029;&#12289;&#21453;&#39304;&#12289;&#36866;&#24212;&#24615;: &#29992;&#20110;&#27979;&#35797;&#26102;&#25919;&#31574;&#35843;&#25972;&#30340;&#20154;-&#26426;&#29615;&#36335;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Diagnosis, Feedback, Adaptation: A Human-in-the-Loop Framework for Test-Time Policy Adaptation. (arXiv:2307.06333v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06333
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;&#29992;&#25143;&#37027;&#37324;&#30452;&#25509;&#33719;&#21462;&#21453;&#39304;&#26469;&#35782;&#21035;&#20010;&#24615;&#21270;&#30340;&#26080;&#20851;&#32039;&#35201;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#24182;&#33719;&#24471;&#36866;&#24212;&#20010;&#24615;&#21270;&#29992;&#25143;&#30446;&#26631;&#30340;&#25919;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25919;&#31574;&#24120;&#24120;&#30001;&#20110;&#20998;&#24067;&#20559;&#31227;&#32780;&#22833;&#25928;&#8212;&#8212;&#21363;&#24403;&#25919;&#31574;&#22312;&#26032;&#29615;&#22659;&#20013;&#37096;&#32626;&#26102;&#65292;&#29366;&#24577;&#21644;&#22870;&#21169;&#21457;&#29983;&#21464;&#21270;&#12290;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#36890;&#36807;&#20351;&#27169;&#22411;&#23545;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#21464;&#21270;&#20855;&#26377;&#19981;&#21464;&#24615;&#26469;&#22686;&#21152;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#32773;&#22312;&#20107;&#20808;&#24448;&#24448;&#19981;&#30693;&#36947;&#21738;&#20123;&#27010;&#24565;&#26159;&#26080;&#20851;&#32039;&#35201;&#30340;&#65292;&#23588;&#20854;&#26159;&#24403;&#19981;&#21516;&#30340;&#26368;&#32456;&#29992;&#25143;&#23545;&#20219;&#21153;&#25191;&#34892;&#26041;&#24335;&#26377;&#19981;&#21516;&#30340;&#20559;&#22909;&#26102;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20114;&#21160;&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#20174;&#29992;&#25143;&#37027;&#37324;&#33719;&#24471;&#21453;&#39304;&#26469;&#35782;&#21035;&#20010;&#24615;&#21270;&#30340;&#26080;&#20851;&#32039;&#35201;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#29983;&#25104;&#21453;&#20107;&#23454;&#28436;&#31034;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#24555;&#36895;&#30830;&#23450;&#21487;&#33021;&#19982;&#20219;&#21153;&#30456;&#20851;&#21644;&#26080;&#20851;&#30340;&#27010;&#24565;&#12290;&#28982;&#21518;&#21033;&#29992;&#26080;&#20851;&#32039;&#35201;&#30340;&#27010;&#24565;&#30340;&#30693;&#35782;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#20174;&#32780;&#33719;&#24471;&#36866;&#24212;&#20110;&#20010;&#24615;&#21270;&#29992;&#25143;&#30446;&#26631;&#30340;&#25919;&#31574;&#12290;&#25105;&#20204;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;(1)&#20351;&#29992;&#25143;&#33021;&#22815;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
Policies often fail due to distribution shift -- changes in the state and reward that occur when a policy is deployed in new environments. Data augmentation can increase robustness by making the model invariant to task-irrelevant changes in the agent's observation. However, designers don't know which concepts are irrelevant a priori, especially when different end users have different preferences about how the task is performed. We propose an interactive framework to leverage feedback directly from the user to identify personalized task-irrelevant concepts. Our key idea is to generate counterfactual demonstrations that allow users to quickly identify possible task-relevant and irrelevant concepts. The knowledge of task-irrelevant concepts is then used to perform data augmentation and thus obtain a policy adapted to personalized user objectives. We present experiments validating our framework on discrete and continuous control tasks with real human users. Our method (1) enables users to 
&lt;/p&gt;</description></item><item><title>&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#21160;&#24577;&#35268;&#21010;&#30340;&#26041;&#27861;&#38480;&#21046;&#36229;&#20986;&#20998;&#24067;&#21160;&#20316;&#30340;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.06328</link><description>&lt;p&gt;
Offline RL&#30340;&#39044;&#31639;&#21453;&#20107;&#23454;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Budgeting Counterfactual for Offline RL. (arXiv:2307.06328v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06328
&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#21160;&#24577;&#35268;&#21010;&#30340;&#26041;&#27861;&#38480;&#21046;&#36229;&#20986;&#20998;&#24067;&#21160;&#20316;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#28508;&#22312;&#21160;&#20316;&#39046;&#22495;&#20869;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;&#22256;&#22659;&#25152;&#24341;&#36215;&#65306;&#22914;&#26524;&#25105;&#20204;&#36873;&#25321;&#20102;&#19981;&#21516;&#30340;&#34892;&#21160;&#20250;&#24590;&#20040;&#26679;&#65311;&#36825;&#20123;&#24773;&#20917;&#36890;&#24120;&#20250;&#23548;&#33268;&#25351;&#25968;&#32423;&#32047;&#31215;&#30340;&#22806;&#25512;&#35823;&#24046;&#12290;&#22240;&#27492;&#65292;&#35748;&#35782;&#21040;&#24182;&#19981;&#26159;&#25152;&#26377;&#30340;&#20915;&#31574;&#27493;&#39588;&#23545;&#26368;&#32456;&#32467;&#26524;&#37117;&#21516;&#26679;&#37325;&#35201;&#65292;&#24182;&#22312;&#25919;&#31574;&#21046;&#23450;&#20013;&#39044;&#31639;&#21453;&#20107;&#23454;&#20915;&#31574;&#30340;&#25968;&#37327;&#20197;&#25511;&#21046;&#22806;&#25512;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#22312;&#25919;&#31574;&#25110;&#20540;&#20989;&#25968;&#19978;&#20351;&#29992;&#35268;&#21017;&#21270;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#26126;&#30830;&#38480;&#21046;&#35757;&#32451;&#26399;&#38388;&#30340;&#36229;&#20986;&#20998;&#24067;&#21160;&#20316;&#30340;&#25968;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#21160;&#24577;&#35268;&#21010;&#26469;&#20915;&#23450;&#22312;&#21738;&#37324;&#36827;&#34892;&#22806;&#25512;&#21644;&#22312;&#21738;&#37324;&#19981;&#36827;&#34892;&#22806;&#25512;&#65292;&#24182;&#19988;&#23545;&#20915;&#31574;&#30340;&#19978;&#38480;&#19981;&#21516;&#20110;&#34892;&#20026;&#31574;&#30053;&#12290;&#23427;&#22312;&#28508;&#22312;&#25913;&#36827;&#30340;&#28508;&#21147;&#21644;&#22806;&#25512;&#25511;&#21046;&#20043;&#38388;&#36827;&#34892;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The main challenge of offline reinforcement learning, where data is limited, arises from a sequence of counterfactual reasoning dilemmas within the realm of potential actions: What if we were to choose a different course of action? These circumstances frequently give rise to extrapolation errors, which tend to accumulate exponentially with the problem horizon. Hence, it becomes crucial to acknowledge that not all decision steps are equally important to the final outcome, and to budget the number of counterfactual decisions a policy make in order to control the extrapolation. Contrary to existing approaches that use regularization on either the policy or value function, we propose an approach to explicitly bound the amount of out-of-distribution actions during training. Specifically, our method utilizes dynamic programming to decide where to extrapolate and where not to, with an upper bound on the decisions different from behavior policy. It balances between the potential for improvemen
&lt;/p&gt;</description></item><item><title>NaViT&#26159;&#19968;&#20010;&#35270;&#35273;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#24207;&#21015;&#25171;&#21253;&#30340;&#26041;&#24335;&#22788;&#29702;&#20219;&#24847;&#20998;&#36776;&#29575;&#21644;&#32437;&#27178;&#27604;&#30340;&#36755;&#20837;&#22270;&#20687;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#21644;&#27169;&#22411;&#22312;&#26631;&#20934;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.06304</link><description>&lt;p&gt;
Patch n' Pack: NaViT,&#19968;&#20010;&#36866;&#29992;&#20110;&#20219;&#24847;&#32437;&#27178;&#27604;&#21644;&#20998;&#36776;&#29575;&#30340;&#35270;&#35273;Transformer
&lt;/p&gt;
&lt;p&gt;
Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution. (arXiv:2307.06304v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06304
&lt;/p&gt;
&lt;p&gt;
NaViT&#26159;&#19968;&#20010;&#35270;&#35273;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#24207;&#21015;&#25171;&#21253;&#30340;&#26041;&#24335;&#22788;&#29702;&#20219;&#24847;&#20998;&#36776;&#29575;&#21644;&#32437;&#27178;&#27604;&#30340;&#36755;&#20837;&#22270;&#20687;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#21644;&#27169;&#22411;&#22312;&#26631;&#20934;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#20013;&#65292;&#23558;&#22270;&#20687;&#35843;&#25972;&#20026;&#22266;&#23450;&#20998;&#36776;&#29575;&#21518;&#36827;&#34892;&#22788;&#29702;&#26159;&#26222;&#36941;&#19988;&#26126;&#26174;&#27425;&#20248;&#30340;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#20687;Vision Transformer&#65288;ViT&#65289;&#36825;&#26679;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#28789;&#27963;&#30340;&#22522;&#20110;&#24207;&#21015;&#30340;&#24314;&#27169;&#65292;&#22240;&#27492;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#38271;&#24230;&#30340;&#36755;&#20837;&#24207;&#21015;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#28857;&#65292;&#20351;&#29992;&#31216;&#20026;NaViT&#65288;Native Resolution ViT&#65289;&#30340;&#27169;&#22411;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36827;&#34892;&#24207;&#21015;&#25171;&#21253;&#65292;&#20197;&#22788;&#29702;&#20219;&#24847;&#20998;&#36776;&#29575;&#21644;&#32437;&#27178;&#27604;&#30340;&#36755;&#20837;&#12290;&#38500;&#20102;&#28789;&#27963;&#30340;&#27169;&#22411;&#20351;&#29992;&#26041;&#24335;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#22823;&#35268;&#27169;&#30417;&#30563;&#21644;&#23545;&#27604;&#24230;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#20013;&#30340;&#35757;&#32451;&#25928;&#29575;&#25552;&#21319;&#12290;NaViT&#21487;&#20197;&#39640;&#25928;&#22320;&#24212;&#29992;&#20110;&#22270;&#20687;&#21644;&#35270;&#39057;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#12289;&#35821;&#20041;&#20998;&#21106;&#31561;&#26631;&#20934;&#20219;&#21153;&#65292;&#24182;&#22312;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#25552;&#21319;&#30340;&#32467;&#26524;&#12290;&#22312;&#25512;&#29702;&#38454;&#27573;&#65292;&#36755;&#20837;&#20998;&#36776;&#29575;&#30340;&#28789;&#27963;&#24615;&#21487;&#20197;&#29992;&#20110;&#24179;&#31283;&#22320;&#22312;&#27979;&#35797;&#26102;&#38388;&#30340;&#25104;&#26412;&#21644;&#24615;&#33021;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#25105;&#20204;&#30456;&#20449;NaViT&#26631;&#24535;&#30528;&#19968;&#20010;&#31163;&#24320;&#20102;&#20197;&#24448;&#24605;&#32500;&#30340;&#26032;&#31687;&#31456;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ubiquitous and demonstrably suboptimal choice of resizing images to a fixed resolution before processing them with computer vision models has not yet been successfully challenged. However, models such as the Vision Transformer (ViT) offer flexible sequence-based modeling, and hence varying input sequence lengths. We take advantage of this with NaViT (Native Resolution ViT) which uses sequence packing during training to process inputs of arbitrary resolutions and aspect ratios. Alongside flexible model usage, we demonstrate improved training efficiency for large-scale supervised and contrastive image-text pretraining. NaViT can be efficiently transferred to standard tasks such as image and video classification, object detection, and semantic segmentation and leads to improved results on robustness and fairness benchmarks. At inference time, the input resolution flexibility can be used to smoothly navigate the test-time cost-performance trade-off. We believe that NaViT marks a depart
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;InstructMining&#65292;&#19968;&#31181;&#29992;&#20110;&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#30340;&#32447;&#24615;&#35268;&#21017;&#65292;&#20197;&#22686;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#21644;&#21709;&#24212;&#25351;&#20196;&#33021;&#21147;&#12290;&#36890;&#36807;&#29305;&#23450;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#26631;&#24314;&#27169;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#65292;&#35821;&#35328;&#27169;&#22411;&#20063;&#21487;&#20197;&#36827;&#34892;&#33391;&#22909;&#30340;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2307.06290</link><description>&lt;p&gt;
&#25351;&#20196;&#25366;&#25496;&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Instruction Mining: High-Quality Instruction Data Selection for Large Language Models. (arXiv:2307.06290v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;InstructMining&#65292;&#19968;&#31181;&#29992;&#20110;&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#30340;&#32447;&#24615;&#35268;&#21017;&#65292;&#20197;&#22686;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#21644;&#21709;&#24212;&#25351;&#20196;&#33021;&#21147;&#12290;&#36890;&#36807;&#29305;&#23450;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#26631;&#24314;&#27169;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#65292;&#35821;&#35328;&#27169;&#22411;&#20063;&#21487;&#20197;&#36827;&#34892;&#33391;&#22909;&#30340;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#32463;&#21382;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20004;&#20010;&#35757;&#32451;&#38454;&#27573;&#12290;&#23613;&#31649;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#36171;&#20104;&#27169;&#22411;&#24378;&#22823;&#30340;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#22238;&#24212;&#30340;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#26377;&#26102;&#20173;&#28982;&#26080;&#27861;&#29702;&#35299;&#20154;&#31867;&#25351;&#20196;&#12290;&#20026;&#20102;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#21644;&#21709;&#24212;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#25351;&#20196;&#24494;&#35843;&#24050;&#25104;&#20026;&#35813;&#39046;&#22495;&#30340;&#20851;&#38190;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20063;&#21487;&#20197;&#36827;&#34892;&#33391;&#22909;&#30340;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#29992;&#20110;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#20173;&#32570;&#20047;&#26126;&#30830;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InstructMining&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#36136;&#37327;&#30340;&#32447;&#24615;&#35268;&#21017;&#12290;&#25105;&#20204;&#20351;&#29992;&#20855;&#20307;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#26631;&#26469;&#36827;&#34892;InstructMining&#30340;&#24314;&#27169;&#12290;&#20026;&#20102;&#30740;&#31350;&#25968;&#25454;&#36136;&#37327;&#19982;&#36825;&#20123;&#25351;&#26631;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#32454;&#33268;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models typically undergo two training stages, pretraining and finetuning. Despite that large-scale pretraining endows the model with strong capabilities to generate natural language responses, these pretrained models can still fail to understand human instructions at times. To enhance language models' ability of interpreting and responding to instructions, instruction finetuning has emerged as a critical method in this area. Recent studies found that large language models can be finetuned to perform well even with a small amount of high-quality instruction-following data. However, the selection of high-quality datasets for finetuning language models still lacks clear guidelines to follow. In this paper, we propose InstructMining, a linear rule for evaluating instruction-following data quality. We formulate InstructMining using specific natural language indicators. To investigate the relationship between data quality and these indicators, we further conduct extensive fine
&lt;/p&gt;</description></item><item><title>DSSE&#26159;&#19968;&#20010;&#26080;&#20154;&#26426;&#32676;&#38598;&#25628;&#32034;&#29615;&#22659;&#65292;&#29992;&#20110;&#30740;&#31350;&#38656;&#35201;&#21160;&#24577;&#27010;&#29575;&#20316;&#20026;&#36755;&#20837;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.06240</link><description>&lt;p&gt;
DSSE: &#26080;&#20154;&#26426;&#32676;&#38598;&#25628;&#32034;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
DSSE: a drone swarm search environment. (arXiv:2307.06240v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06240
&lt;/p&gt;
&lt;p&gt;
DSSE&#26159;&#19968;&#20010;&#26080;&#20154;&#26426;&#32676;&#38598;&#25628;&#32034;&#29615;&#22659;&#65292;&#29992;&#20110;&#30740;&#31350;&#38656;&#35201;&#21160;&#24577;&#27010;&#29575;&#20316;&#20026;&#36755;&#20837;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20154;&#26426;&#32676;&#38598;&#25628;&#32034;&#39033;&#30446;&#26159;&#19968;&#20010;&#22522;&#20110;PettingZoo&#30340;&#29615;&#22659;&#65292;&#19982;&#22810;&#26234;&#33021;&#20307;&#65288;&#25110;&#21333;&#26234;&#33021;&#20307;&#65289;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#37197;&#21512;&#20351;&#29992;&#12290;&#35813;&#29615;&#22659;&#20013;&#30340;&#26234;&#33021;&#20307;&#65288;&#26080;&#20154;&#26426;&#65289;&#24517;&#39035;&#25214;&#21040;&#30446;&#26631;&#65288;&#36935;&#38505;&#20154;&#21592;&#65289;&#65292;&#20294;&#19981;&#30693;&#36947;&#30446;&#26631;&#30340;&#20301;&#32622;&#65292;&#24182;&#19988;&#19981;&#20250;&#26681;&#25454;&#33258;&#36523;&#19982;&#30446;&#26631;&#30340;&#36317;&#31163;&#24471;&#21040;&#22870;&#21169;&#12290;&#20294;&#26159;&#65292;&#26234;&#33021;&#20307;&#20250;&#25509;&#25910;&#21040;&#30446;&#26631;&#20986;&#29616;&#22312;&#22320;&#22270;&#26576;&#20010;&#21333;&#20803;&#26684;&#30340;&#27010;&#29575;&#12290;&#35813;&#39033;&#30446;&#30340;&#30446;&#26631;&#26159;&#24110;&#21161;&#30740;&#31350;&#38656;&#35201;&#21160;&#24577;&#27010;&#29575;&#20316;&#20026;&#36755;&#20837;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Drone Swarm Search project is an environment, based on PettingZoo, that is to be used in conjunction with multi-agent (or single-agent) reinforcement learning algorithms. It is an environment in which the agents (drones), have to find the targets (shipwrecked people). The agents do not know the position of the target and do not receive rewards related to their own distance to the target(s). However, the agents receive the probabilities of the target(s) being in a certain cell of the map. The aim of this project is to aid in the study of reinforcement learning algorithms that require dynamic probabilities as inputs.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#27979;&#35797;&#20102;&#22312;&#21521;&#37327;&#27169;&#22411;&#21152;&#26435;&#25216;&#26415;&#20013;&#20351;&#29992;&#19981;&#21516;&#23545;&#25968;&#24213;&#25968;&#30340;&#25928;&#26524;&#65292;&#20197;&#31361;&#20986;&#22312;&#19981;&#21516;&#21152;&#26435;&#25968;&#20540;&#19979;&#20102;&#35299;&#31995;&#32479;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06213</link><description>&lt;p&gt;
&#27979;&#35797;&#19981;&#21516;&#23545;&#25968;&#24213;&#25968;&#30340;&#21521;&#37327;&#27169;&#22411;&#21152;&#26435;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Testing different Log Bases For Vector Model Weighting Technique. (arXiv:2307.06213v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#27979;&#35797;&#20102;&#22312;&#21521;&#37327;&#27169;&#22411;&#21152;&#26435;&#25216;&#26415;&#20013;&#20351;&#29992;&#19981;&#21516;&#23545;&#25968;&#24213;&#25968;&#30340;&#25928;&#26524;&#65292;&#20197;&#31361;&#20986;&#22312;&#19981;&#21516;&#21152;&#26435;&#25968;&#20540;&#19979;&#20102;&#35299;&#31995;&#32479;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#26681;&#25454;&#29992;&#25143;&#25552;&#20132;&#30340;&#26597;&#35810;&#26816;&#32034;&#30456;&#20851;&#25991;&#26723;&#12290;&#25991;&#26723;&#39318;&#20808;&#34987;&#32034;&#24341;&#65292;&#25991;&#26723;&#20013;&#30340;&#35789;&#35821;&#20351;&#29992;&#31216;&#20026;TFIDF&#30340;&#21152;&#26435;&#25216;&#26415;&#34987;&#36171;&#20104;&#26435;&#37325;&#65292;TFIDF&#26159;&#35789;&#39057;&#65288;TF&#65289;&#21644;&#36870;&#25991;&#26723;&#39057;&#29575;&#65288;IDF&#65289;&#30340;&#20056;&#31215;&#12290;TF&#20195;&#34920;&#35789;&#39033;&#22312;&#25991;&#26723;&#20013;&#20986;&#29616;&#30340;&#27425;&#25968;&#12290;IDF&#34913;&#37327;&#35789;&#39033;&#22312;&#25152;&#26377;&#25991;&#26723;&#20013;&#30340;&#26222;&#36941;&#31243;&#24230;&#12290;&#23427;&#36890;&#36807;&#23558;&#31995;&#32479;&#20013;&#30340;&#24635;&#25991;&#26723;&#25968;&#38500;&#20197;&#21253;&#21547;&#35813;&#35789;&#39033;&#30340;&#25991;&#26723;&#25968;&#65292;&#28982;&#21518;&#35745;&#31639;&#21830;&#30340;&#23545;&#25968;&#26469;&#35745;&#31639;&#12290;&#40664;&#35748;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;&#20197;10&#20026;&#24213;&#30340;&#23545;&#25968;&#35745;&#31639;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#20351;&#29992;&#20174;0.1&#21040;100.0&#30340;&#19968;&#31995;&#21015;&#23545;&#25968;&#24213;&#25968;&#26469;&#35745;&#31639;IDF&#65292;&#20197;&#27979;&#35797;&#36825;&#31181;&#21152;&#26435;&#25216;&#26415;&#12290;&#27979;&#35797;&#19981;&#21516;&#23545;&#25968;&#24213;&#25968;&#30340;&#21521;&#37327;&#27169;&#22411;&#21152;&#26435;&#25216;&#26415;&#30340;&#30446;&#30340;&#26159;&#31361;&#20986;&#22312;&#19981;&#21516;&#21152;&#26435;&#25968;&#20540;&#19979;&#20102;&#35299;&#31995;&#32479;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;MED&#30340;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information retrieval systems retrieves relevant documents based on a query submitted by the user. The documents are initially indexed and the words in the documents are assigned weights using a weighting technique called TFIDF which is the product of Term Frequency (TF) and Inverse Document Frequency (IDF). TF represents the number of occurrences of a term in a document. IDF measures whether the term is common or rare across all documents. It is computed by dividing the total number of documents in the system by the number of documents containing the term and then computing the logarithm of the quotient. By default, we use base 10 to calculate the logarithm. In this paper, we are going to test this weighting technique by using a range of log bases from 0.1 to 100.0 to calculate the IDF. Testing different log bases for vector model weighting technique is to highlight the importance of understanding the performance of the system at different weighting values. We use the documents of MED
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38598;&#25104;&#21040;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#33258;&#36866;&#24212;&#31995;&#32479;&#21644;&#26377;&#25928;&#36890;&#20449;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#23545;&#22797;&#26434;&#29615;&#22659;&#21464;&#21270;&#30340;&#30417;&#27979;&#12289;&#20998;&#26512;&#12289;&#35745;&#21010;&#21644;&#25191;&#34892;&#31995;&#32479;&#33258;&#36866;&#24212;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2307.06187</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#36866;&#24212;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Self-Adaptive Large Language Model (LLM)-Based Multiagent Systems. (arXiv:2307.06187v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38598;&#25104;&#21040;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#33258;&#36866;&#24212;&#31995;&#32479;&#21644;&#26377;&#25928;&#36890;&#20449;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#23545;&#22797;&#26434;&#29615;&#22659;&#21464;&#21270;&#30340;&#30417;&#27979;&#12289;&#20998;&#26512;&#12289;&#35745;&#21010;&#21644;&#25191;&#34892;&#31995;&#32479;&#33258;&#36866;&#24212;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#20027;&#35745;&#31639;&#20013;&#65292;&#33258;&#36866;&#24212;&#34987;&#25552;&#20986;&#20316;&#20026;&#31649;&#29702;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#65288;MASs&#65289;&#22797;&#26434;&#24615;&#30340;&#22522;&#26412;&#33539;&#24335;&#12290;&#36890;&#36807;&#28155;&#21152;&#23545;&#31995;&#32479;&#30340;&#30417;&#27979;&#21644;&#33258;&#36866;&#24212;&#25903;&#25345;&#65292;&#20197;&#23454;&#29616;&#29305;&#23450;&#20851;&#27880;&#28857;&#30340;&#30446;&#26631;&#12290;&#22312;&#28041;&#21450;&#26234;&#33021;&#20307;&#20114;&#21160;&#30340;&#22330;&#26223;&#20013;&#65292;&#36890;&#20449;&#26159;&#20851;&#38190;&#65292;&#23427;&#36890;&#36807;&#30452;&#25509;&#12289;&#28165;&#26224;&#30340;&#20449;&#24687;&#20132;&#27969;&#22686;&#24378;&#21512;&#20316;&#24182;&#20943;&#23569;&#21327;&#35843;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#25552;&#39640;&#19982;MASs&#30340;&#20132;&#20114;&#36890;&#20449;&#30340;&#34920;&#36798;&#33021;&#21147;&#24182;&#38750;&#27809;&#26377;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#33258;&#36866;&#24212;&#31995;&#32479;&#23545;&#26377;&#25928;&#36890;&#20449;&#30340;&#30456;&#20114;&#20316;&#29992;&#23545;&#20110;&#26410;&#26469;MAS&#30340;&#36827;&#27493;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#22522;&#20110;GPT&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38598;&#25104;&#21040;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35770;&#22522;&#20110;MAPE-K&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20197;&#20854;&#22312;&#21709;&#24212;&#21160;&#24577;&#29615;&#22659;&#21464;&#21270;&#20013;&#30417;&#27979;&#12289;&#20998;&#26512;&#12289;&#35745;&#21010;&#21644;&#25191;&#34892;&#31995;&#32479;&#33258;&#36866;&#24212;&#30340;&#24378;&#22823;&#25903;&#25345;&#32780;&#38395;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
In autonomic computing, self-adaptation has been proposed as a fundamental paradigm to manage the complexity of multiagent systems (MASs). This achieved by extending a system with support to monitor and adapt itself to achieve specific concerns of interest. Communication in these systems is key given that in scenarios involving agent interaction, it enhances cooperation and reduces coordination challenges by enabling direct, clear information exchange. However, improving the expressiveness of the interaction communication with MASs is not without challenges. In this sense, the interplay between self-adaptive systems and effective communication is crucial for future MAS advancements. In this paper, we propose the integration of large language models (LLMs) such as GPT-based technologies into multiagent systems. We anchor our methodology on the MAPE-K model, which is renowned for its robust support in monitoring, analyzing, planning, and executing system adaptations in response to dynami
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;Vision-Language Models&#65288;VLMs&#65289;&#36827;&#34892;&#26102;&#38388;&#21644;&#20301;&#32622;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#35782;&#21035;&#21644;&#25512;&#29702;&#25506;&#27979;&#20219;&#21153;&#26469;&#30740;&#31350;VLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#21457;&#29616;&#65292;&#23613;&#31649;VLMs&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#26102;&#38388;&#21644;&#20301;&#32622;&#30456;&#20851;&#29305;&#24449;&#65292;&#20294;&#22312;&#25512;&#29702;&#26041;&#38754;&#20173;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2307.06166</link><description>&lt;p&gt;
Vision-Language Models&#33021;&#25104;&#20026;&#33391;&#22909;&#29468;&#27979;&#22120;&#21527;&#65311;&#25506;&#32034;VLMs&#29992;&#20110;&#26102;&#38388;&#21644;&#20301;&#32622;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Can Vision-Language Models be a Good Guesser? Exploring VLMs for Times and Location Reasoning. (arXiv:2307.06166v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;Vision-Language Models&#65288;VLMs&#65289;&#36827;&#34892;&#26102;&#38388;&#21644;&#20301;&#32622;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#35782;&#21035;&#21644;&#25512;&#29702;&#25506;&#27979;&#20219;&#21153;&#26469;&#30740;&#31350;VLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#21457;&#29616;&#65292;&#23613;&#31649;VLMs&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#26102;&#38388;&#21644;&#20301;&#32622;&#30456;&#20851;&#29305;&#24449;&#65292;&#20294;&#22312;&#25512;&#29702;&#26041;&#38754;&#20173;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26399;&#26395;Vision-Language Models&#65288;VLMs&#65289;&#33021;&#20687;&#20154;&#19968;&#26679;&#20855;&#22791;&#24120;&#35782;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#12290;&#19968;&#20010;&#20363;&#23376;&#26159;&#65292;&#20154;&#31867;&#21487;&#20197;&#26681;&#25454;&#20182;&#20204;&#30340;&#30693;&#35782;&#25512;&#26029;&#20986;&#19968;&#24352;&#22270;&#29255;&#30340;&#25293;&#25668;&#22320;&#28857;&#21644;&#26102;&#38388;&#12290;&#36825;&#35753;&#25105;&#20204;&#24819;&#30693;&#36947;&#65292;&#22522;&#20110;&#35270;&#35273;&#32447;&#32034;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#22270;&#20687;-&#25991;&#26412;&#36164;&#28304;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;Vision-Language Models&#26159;&#21542;&#33021;&#22815;&#36798;&#21040;&#29978;&#33267;&#36229;&#36807;&#20154;&#31867;&#22312;&#26102;&#38388;&#21644;&#20301;&#32622;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#35782;&#21035;&#21644;&#25512;&#29702;&#25506;&#27979;&#20219;&#21153;&#65292;&#24212;&#29992;&#20110;&#37492;&#21035;&#24615;&#21644;&#29983;&#25104;&#24615;&#30340;VLMs&#65292;&#20197;&#21457;&#29616;VLMs&#33021;&#21542;&#35782;&#21035;&#20986;&#19982;&#26102;&#38388;&#21644;&#20301;&#32622;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#24182;&#36827;&#19968;&#27493;&#36827;&#34892;&#25512;&#29702;&#12290;&#20026;&#20102;&#26041;&#20415;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;WikiTiLo&#65292;&#19968;&#20010;&#21253;&#21547;&#20016;&#23500;&#31038;&#20250;&#25991;&#21270;&#32447;&#32034;&#30340;&#31934;&#24515;&#31574;&#21010;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#22312;&#24191;&#27867;&#30340;&#23454;&#39564;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#34429;&#28982;VLMs&#33021;&#22815;&#26377;&#25928;&#22320;&#20445;&#30041;&#35270;&#35273;&#32534;&#30721;&#22120;&#20013;&#30340;&#30456;&#20851;&#29305;&#24449;&#65292;&#20294;&#22312;&#25512;&#29702;&#26041;&#38754;&#20173;&#23384;&#22312;&#19981;&#23436;&#21892;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#21457;&#24067;...
&lt;/p&gt;
&lt;p&gt;
Vision-Language Models (VLMs) are expected to be capable of reasoning with commonsense knowledge as human beings. One example is that humans can reason where and when an image is taken based on their knowledge. This makes us wonder if, based on visual cues, Vision-Language Models that are pre-trained with large-scale image-text resources can achieve and even outperform human's capability in reasoning times and location. To address this question, we propose a two-stage \recognition\space and \reasoning\space probing task, applied to discriminative and generative VLMs to uncover whether VLMs can recognize times and location-relevant features and further reason about it. To facilitate the investigation, we introduce WikiTiLo, a well-curated image dataset compromising images with rich socio-cultural cues. In the extensive experimental studies, we find that although VLMs can effectively retain relevant features in visual encoders, they still fail to make perfect reasoning. We will release o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#23545;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#29702;&#20449;&#21495;&#30740;&#31350;&#39046;&#22495;&#30340;&#31995;&#32479;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;&#26368;&#26032;&#26368;&#20808;&#36827;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#26377;&#21161;&#20110;&#20102;&#35299;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#29702;&#20449;&#21495;&#20013;&#30340;&#24212;&#29992;&#21644;&#25361;&#25112;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#30340;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2307.06162</link><description>&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#23545;&#29983;&#29702;&#20449;&#21495;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Deep Generative Models for Physiological Signals: A Systematic Literature Review. (arXiv:2307.06162v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#23545;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#29702;&#20449;&#21495;&#30740;&#31350;&#39046;&#22495;&#30340;&#31995;&#32479;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;&#26368;&#26032;&#26368;&#20808;&#36827;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#26377;&#21161;&#20110;&#20102;&#35299;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#29702;&#20449;&#21495;&#20013;&#30340;&#24212;&#29992;&#21644;&#25361;&#25112;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#29702;&#20449;&#21495;&#65292;&#29305;&#21035;&#26159;&#24515;&#30005;&#22270;&#12289;&#33041;&#30005;&#22270;&#12289;&#20809;&#30005;&#23481;&#25239;&#22270;&#21644;&#32908;&#30005;&#22270;&#39046;&#22495;&#30340;&#25991;&#29486;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#12290;&#19982;&#24050;&#26377;&#30340;&#32508;&#36848;&#25991;&#31456;&#30456;&#27604;&#65292;&#26412;&#25991;&#26159;&#31532;&#19968;&#31687;&#24635;&#32467;&#26368;&#26032;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#32508;&#36848;&#12290;&#36890;&#36807;&#20998;&#26512;&#19982;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30456;&#20851;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#20197;&#21450;&#36825;&#20123;&#27169;&#22411;&#30340;&#20027;&#35201;&#24212;&#29992;&#21644;&#25361;&#25112;&#65292;&#26412;&#32508;&#36848;&#20026;&#23545;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#29983;&#29702;&#20449;&#21495;&#30340;&#25972;&#20307;&#29702;&#35299;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#24378;&#35843;&#37319;&#29992;&#30340;&#35780;&#20272;&#21327;&#35758;&#21644;&#26368;&#24120;&#29992;&#30340;&#29983;&#29702;&#25968;&#25454;&#24211;&#65292;&#26412;&#32508;&#36848;&#26377;&#21161;&#20110;&#23545;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a systematic literature review on deep generative models for physiological signals, particularly electrocardiogram, electroencephalogram, photoplethysmogram and electromyogram. Compared to the existing review papers, we present the first review that summarizes the recent state-of-the-art deep generative models. By analysing the state-of-the-art research related to deep generative models along with their main applications and challenges, this review contributes to the overall understanding of these models applied to physiological signals. Additionally, by highlighting the employed evaluation protocol and the most used physiological databases, this review facilitates the assessment and benchmarking of deep generative models.
&lt;/p&gt;</description></item><item><title>&#26412;&#31456;&#20171;&#32461;&#20102;&#21453;&#24605;&#24615;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#26377;&#24847;&#20041;&#30340;&#20154;&#31867;&#25511;&#21046;&#12290;&#36890;&#36807;&#25972;&#21512;&#24515;&#29702;&#23398;&#21644;&#21746;&#23398;&#30693;&#35782;&#12289;&#24418;&#24335;&#25512;&#29702;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20197;&#21019;&#24314;&#33021;&#22815;&#21709;&#24212;&#20154;&#31867;&#20215;&#20540;&#35266;&#21644;&#31038;&#20250;&#35268;&#33539;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#35774;&#35745;&#21644;&#24320;&#21457;&#33021;&#21147;&#33258;&#21453;&#24605;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#30740;&#31350;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35748;&#20026;&#33258;&#21453;&#24605;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#26131;&#20110;&#29702;&#35299;&#30340;&#20449;&#24687;&#65292;&#22686;&#24378;&#20154;&#31867;&#36947;&#24503;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#22686;&#21152;&#26377;&#24847;&#20041;&#30340;&#20154;&#31867;&#25511;&#21046;&#21147;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.06159</link><description>&lt;p&gt;
&#21453;&#24605;&#24615;&#28151;&#21512;&#26234;&#33021;&#23545;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20013;&#26377;&#24847;&#20041;&#30340;&#20154;&#31867;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Reflective Hybrid Intelligence for Meaningful Human Control in Decision-Support Systems. (arXiv:2307.06159v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06159
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31456;&#20171;&#32461;&#20102;&#21453;&#24605;&#24615;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#26377;&#24847;&#20041;&#30340;&#20154;&#31867;&#25511;&#21046;&#12290;&#36890;&#36807;&#25972;&#21512;&#24515;&#29702;&#23398;&#21644;&#21746;&#23398;&#30693;&#35782;&#12289;&#24418;&#24335;&#25512;&#29702;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20197;&#21019;&#24314;&#33021;&#22815;&#21709;&#24212;&#20154;&#31867;&#20215;&#20540;&#35266;&#21644;&#31038;&#20250;&#35268;&#33539;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#35774;&#35745;&#21644;&#24320;&#21457;&#33021;&#21147;&#33258;&#21453;&#24605;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#30740;&#31350;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35748;&#20026;&#33258;&#21453;&#24605;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#26131;&#20110;&#29702;&#35299;&#30340;&#20449;&#24687;&#65292;&#22686;&#24378;&#20154;&#31867;&#36947;&#24503;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#22686;&#21152;&#26377;&#24847;&#20041;&#30340;&#20154;&#31867;&#25511;&#21046;&#21147;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#33021;&#21147;&#21644;&#26222;&#21450;&#31243;&#24230;&#30340;&#19981;&#26029;&#22686;&#38271;&#65292;&#31038;&#20250;&#24517;&#39035;&#20849;&#21516;&#36873;&#25321;&#22312;&#20943;&#23569;&#20154;&#31867;&#33258;&#27835;&#12289;&#21361;&#23475;&#27665;&#20027;&#21644;&#38480;&#21046;&#20154;&#26435;&#20043;&#38388;&#65292;&#36824;&#26159;&#36873;&#25321;&#19982;&#20154;&#31867;&#21644;&#31038;&#20250;&#20215;&#20540;&#30456;&#19968;&#33268;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#20419;&#36827;&#21512;&#20316;&#12289;&#38887;&#24615;&#12289;&#30693;&#35782;&#21644;&#36947;&#24503;&#34892;&#20026;&#30340;&#22521;&#20859;&#12290;&#22312;&#36825;&#19968;&#31456;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#33258;&#21453;&#24605;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#27010;&#24565;&#65292;&#20197;&#23454;&#29616;&#23545;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#26377;&#24847;&#20041;&#30340;&#20154;&#31867;&#25511;&#21046;&#12290;&#19987;&#27880;&#20110;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#24515;&#29702;&#23398;&#21644;&#21746;&#23398;&#30340;&#30693;&#35782;&#19982;&#24418;&#24335;&#25512;&#29702;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#21019;&#24314;&#33021;&#22815;&#21709;&#24212;&#20154;&#31867;&#20215;&#20540;&#35266;&#21644;&#31038;&#20250;&#35268;&#33539;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#33021;&#30340;&#30740;&#31350;&#26041;&#27861;&#65292;&#35774;&#35745;&#21644;&#24320;&#21457;&#33021;&#21147;&#33258;&#21453;&#24605;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35748;&#20026;&#33258;&#21453;&#24605;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#20197;&#23548;&#33268;&#33258;&#21453;&#24605;&#30340;&#28151;&#21512;&#31995;&#32479;&#65288;&#20154;&#31867;+&#20154;&#24037;&#26234;&#33021;&#65289;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#26377;&#24847;&#20041;&#30340;&#20154;&#31867;&#25511;&#21046;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#26131;&#20110;&#29702;&#35299;&#30340;&#20449;&#24687;&#20351;&#20154;&#31867;&#36947;&#24503;&#25512;&#29702;&#33021;&#21147;&#26356;&#24378;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growing capabilities and pervasiveness of AI systems, societies must collectively choose between reduced human autonomy, endangered democracies and limited human rights, and AI that is aligned to human and social values, nurturing collaboration, resilience, knowledge and ethical behaviour. In this chapter, we introduce the notion of self-reflective AI systems for meaningful human control over AI systems. Focusing on decision support systems, we propose a framework that integrates knowledge from psychology and philosophy with formal reasoning methods and machine learning approaches to create AI systems responsive to human values and social norms. We also propose a possible research approach to design and develop self-reflective capability in AI systems. Finally, we argue that self-reflective AI systems can lead to self-reflective hybrid systems (human + AI), thus increasing meaningful human control and empowering human moral reasoning by providing comprehensible information and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#25163;&#24037;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#30340;&#33258;&#21160;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#20174;&#38646;&#24320;&#22987;&#23398;&#20064;&#31354;&#25112;&#20013;&#30340;&#26377;&#25928;&#20915;&#31574;&#12290;&#36890;&#36807;&#36880;&#28176;&#23398;&#20064;&#19968;&#31995;&#21015;&#20174;&#31616;&#21333;&#21040;&#22256;&#38590;&#30340;&#23376;&#20219;&#21153;&#65292;&#20195;&#29702;&#33021;&#22815;&#22312;&#21508;&#31181;&#29366;&#24577;&#19979;&#20570;&#20986;&#26377;&#25928;&#30340;&#26426;&#21160;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2307.06152</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#38656;&#25163;&#24037;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#30340;&#33258;&#21160;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#26426;&#21160;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Maneuver Decision-Making Through Automatic Curriculum Reinforcement Learning Without Handcrafted Reward functions. (arXiv:2307.06152v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#25163;&#24037;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#30340;&#33258;&#21160;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#20174;&#38646;&#24320;&#22987;&#23398;&#20064;&#31354;&#25112;&#20013;&#30340;&#26377;&#25928;&#20915;&#31574;&#12290;&#36890;&#36807;&#36880;&#28176;&#23398;&#20064;&#19968;&#31995;&#21015;&#20174;&#31616;&#21333;&#21040;&#22256;&#38590;&#30340;&#23376;&#20219;&#21153;&#65292;&#20195;&#29702;&#33021;&#22815;&#22312;&#21508;&#31181;&#29366;&#24577;&#19979;&#20570;&#20986;&#26377;&#25928;&#30340;&#26426;&#21160;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#21160;&#20915;&#31574;&#26159;&#26080;&#20154;&#20316;&#25112;&#39134;&#34892;&#22120;&#33258;&#20027;&#31354;&#25112;&#30340;&#26680;&#24515;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#20174;&#38646;&#24320;&#22987;&#23398;&#20064;&#31354;&#25112;&#20013;&#30340;&#26377;&#25928;&#20915;&#31574;&#12290;&#36890;&#36807;&#21021;&#22987;&#29366;&#24577;&#33539;&#22260;&#26469;&#21306;&#20998;&#19981;&#21516;&#38590;&#24230;&#27700;&#24179;&#30340;&#35838;&#31243;&#65292;&#23558;&#26426;&#21160;&#20915;&#31574;&#20998;&#20026;&#19968;&#31995;&#21015;&#20174;&#31616;&#21333;&#21040;&#22256;&#38590;&#30340;&#23376;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#27979;&#35797;&#32467;&#26524;&#26469;&#25913;&#21464;&#23376;&#20219;&#21153;&#12290;&#38543;&#30528;&#23376;&#20219;&#21153;&#30340;&#21464;&#21270;&#65292;&#20195;&#29702;&#36880;&#28176;&#23398;&#20250;&#23436;&#25104;&#19968;&#31995;&#21015;&#20174;&#31616;&#21333;&#21040;&#22256;&#38590;&#30340;&#23376;&#20219;&#21153;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#26080;&#38656;&#36827;&#34892;&#22870;&#21169;&#20989;&#25968;&#35774;&#35745;&#30340;&#24773;&#20917;&#19979;&#20570;&#20986;&#26377;&#25928;&#30340;&#26426;&#21160;&#20915;&#31574;&#20197;&#24212;&#23545;&#21508;&#31181;&#29366;&#24577;&#12290;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#33258;&#21160;&#35838;&#31243;&#23398;&#20064;&#26159;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#21363;&#20195;&#29702;&#22312;&#27809;&#26377;&#35838;&#31243;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#23436;&#25104;&#26377;&#25928;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maneuver decision-making is the core of unmanned combat aerial vehicle for autonomous air combat. To solve this problem, we propose an automatic curriculum reinforcement learning method, which enables agents to learn effective decisions in air combat from scratch. The range of initial states are used for distinguishing curricula of different difficulty levels, thereby maneuver decision is divided into a series of sub-tasks from easy to difficult, and test results are used to change sub-tasks. As sub-tasks change, agents gradually learn to complete a series of sub-tasks from easy to difficult, enabling them to make effective maneuvering decisions to cope with various states without the need to spend effort designing reward functions. The ablation studied show that the automatic curriculum learning proposed in this article is an essential component for training through reinforcement learning, namely, agents cannot complete effective decisions without curriculum learning. Simulation exper
&lt;/p&gt;</description></item><item><title>SayPlan&#26159;&#19968;&#31181;&#21033;&#29992;3D&#22330;&#26223;&#22270;&#20026;&#22522;&#30784;&#30340;&#21487;&#25193;&#23637;&#30340;&#22823;&#35268;&#27169;&#20219;&#21153;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;3D&#22330;&#26223;&#22270;&#30340;&#23618;&#27425;&#32467;&#26500;&#12289;&#38598;&#25104;&#32463;&#20856;&#36335;&#24452;&#35268;&#21010;&#22120;&#20197;&#21450;&#24341;&#20837;&#36845;&#20195;&#30340;&#37325;&#35268;&#21010;&#27969;&#31243;&#65292;&#23454;&#29616;&#20102;&#22312;&#24222;&#22823;&#22797;&#26434;&#29615;&#22659;&#20013;&#36827;&#34892;&#35268;&#21010;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.06135</link><description>&lt;p&gt;
SayPlan: &#20351;&#29992;3D&#22330;&#26223;&#22270;&#20026;&#21487;&#25193;&#23637;&#20219;&#21153;&#35268;&#21010;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#30784;&#21270;
&lt;/p&gt;
&lt;p&gt;
SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Task Planning. (arXiv:2307.06135v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06135
&lt;/p&gt;
&lt;p&gt;
SayPlan&#26159;&#19968;&#31181;&#21033;&#29992;3D&#22330;&#26223;&#22270;&#20026;&#22522;&#30784;&#30340;&#21487;&#25193;&#23637;&#30340;&#22823;&#35268;&#27169;&#20219;&#21153;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;3D&#22330;&#26223;&#22270;&#30340;&#23618;&#27425;&#32467;&#26500;&#12289;&#38598;&#25104;&#32463;&#20856;&#36335;&#24452;&#35268;&#21010;&#22120;&#20197;&#21450;&#24341;&#20837;&#36845;&#20195;&#30340;&#37325;&#35268;&#21010;&#27969;&#31243;&#65292;&#23454;&#29616;&#20102;&#22312;&#24222;&#22823;&#22797;&#26434;&#29615;&#22659;&#20013;&#36827;&#34892;&#35268;&#21010;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#24320;&#21457;&#22810;&#26679;&#21270;&#20219;&#21153;&#30340;&#36890;&#29992;&#35268;&#21010;&#26234;&#33021;&#20307;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#35268;&#21010;&#24212;&#29992;&#20110;&#24222;&#22823;&#12289;&#22810;&#23618;&#27004;&#12289;&#22810;&#25151;&#38388;&#30340;&#29615;&#22659;&#20013;&#23545;&#26426;&#22120;&#20154;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SayPlan&#30340;&#21487;&#25193;&#23637;&#26041;&#27861;&#65292;&#21033;&#29992;3D&#22330;&#26223;&#22270;&#65288;3DSG&#65289;&#34920;&#31034;&#36827;&#34892;&#22522;&#20110;LLM&#30340;&#22823;&#35268;&#27169;&#20219;&#21153;&#35268;&#21010;&#12290;&#20026;&#20102;&#30830;&#20445;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#25105;&#20204;&#65306;&#65288;1&#65289;&#21033;&#29992;3DSG&#30340;&#23618;&#27425;&#32467;&#26500;&#20801;&#35768;LLMs&#20174;&#36739;&#23567;&#30340;&#12289;&#25240;&#21472;&#30340;&#23436;&#25972;&#22270;&#34920;&#31034;&#20013;&#36827;&#34892;&#35821;&#20041;&#25628;&#32034;&#65292;&#23547;&#25214;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#23376;&#22270;&#65307;&#65288;2&#65289;&#36890;&#36807;&#38598;&#25104;&#32463;&#20856;&#36335;&#24452;&#35268;&#21010;&#22120;&#20943;&#23569;LLM&#30340;&#35268;&#21010;&#35270;&#37326;&#65307;&#65288;3&#65289;&#24341;&#20837;&#19968;&#20010;&#36845;&#20195;&#30340;&#37325;&#35268;&#21010;&#27969;&#31243;&#65292;&#36890;&#36807;&#19982;&#22330;&#26223;&#22270;&#27169;&#25311;&#22120;&#30340;&#21453;&#39304;&#26469;&#20462;&#27491;&#19981;&#21487;&#34892;&#30340;&#21160;&#20316;&#24182;&#36991;&#20813;&#35268;&#21010;&#22833;&#36133;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#28085;&#30422;3&#23618;&#12289;36&#20010;&#25151;&#38388;&#21644;140&#20010;&#23545;&#35937;&#30340;&#22823;&#35268;&#27169;&#29615;&#22659;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated impressive results in developing generalist planning agents for diverse tasks. However, grounding these plans in expansive, multi-floor, and multi-room environments presents a significant challenge for robotics. We introduce SayPlan, a scalable approach to LLM-based, large-scale task planning for robotics using 3D scene graph (3DSG) representations. To ensure the scalability of our approach, we: (1) exploit the hierarchical nature of 3DSGs to allow LLMs to conduct a semantic search for task-relevant subgraphs from a smaller, collapsed representation of the full graph; (2) reduce the planning horizon for the LLM by integrating a classical path planner and (3) introduce an iterative replanning pipeline that refines the initial plan using feedback from a scene graph simulator, correcting infeasible actions and avoiding planning failures. We evaluate our approach on two large-scale environments spanning up to 3 floors, 36 rooms and 140 objects
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#25913;&#36827;&#32422;&#26463;&#33719;&#21462;&#31995;&#32479;&#25928;&#29575;&#30340;&#26032;&#26041;&#27861;&#65292;&#19968;&#31181;&#26159;&#33258;&#19979;&#32780;&#19978;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;GrowAcq&#65292;&#21487;&#20197;&#20943;&#23569;&#29992;&#25143;&#30340;&#31561;&#24453;&#26102;&#38388;&#21644;&#26597;&#35810;&#27425;&#25968;&#65292;&#25552;&#39640;&#20132;&#20114;&#24335;&#32422;&#26463;&#33719;&#21462;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.06126</link><description>&lt;p&gt;
Guided Bottom-Up Interactive Constraint Acquisition&#65288;&#25351;&#23548;&#30340;&#33258;&#19979;&#32780;&#19978;&#20132;&#20114;&#24335;&#32422;&#26463;&#33719;&#21462;&#65289;
&lt;/p&gt;
&lt;p&gt;
Guided Bottom-Up Interactive Constraint Acquisition. (arXiv:2307.06126v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06126
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#25913;&#36827;&#32422;&#26463;&#33719;&#21462;&#31995;&#32479;&#25928;&#29575;&#30340;&#26032;&#26041;&#27861;&#65292;&#19968;&#31181;&#26159;&#33258;&#19979;&#32780;&#19978;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;GrowAcq&#65292;&#21487;&#20197;&#20943;&#23569;&#29992;&#25143;&#30340;&#31561;&#24453;&#26102;&#38388;&#21644;&#26597;&#35810;&#27425;&#25968;&#65292;&#25552;&#39640;&#20132;&#20114;&#24335;&#32422;&#26463;&#33719;&#21462;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32422;&#26463;&#33719;&#21462;&#31995;&#32479;&#21487;&#20197;&#29992;&#20110;&#36741;&#21161;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#30340;&#24314;&#27169;&#12290;&#22312;&#20132;&#20114;&#24335;&#32422;&#26463;&#33719;&#21462;&#20013;&#65292;&#31995;&#32479;&#34987;&#32473;&#20104;&#19968;&#32452;&#20505;&#36873;&#32422;&#26463;&#65292;&#24182;&#21521;&#29992;&#25143;&#25552;&#38382;&#20197;&#25214;&#21040;&#20505;&#36873;&#32422;&#26463;&#20013;&#30340;&#27491;&#30830;&#32422;&#26463;&#12290;&#30446;&#21069;&#30340;&#20132;&#20114;&#24335;&#32422;&#26463;&#33719;&#21462;&#31639;&#27861;&#23384;&#22312;&#33267;&#23569;&#20004;&#20010;&#20027;&#35201;&#29942;&#39048;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#25910;&#25947;&#65292;&#23427;&#20204;&#38656;&#35201;&#21521;&#29992;&#25143;&#35810;&#38382;&#22823;&#37327;&#30340;&#26597;&#35810;&#12290;&#20854;&#27425;&#65292;&#23427;&#20204;&#26080;&#27861;&#22788;&#29702;&#22823;&#37327;&#30340;&#20505;&#36873;&#32422;&#26463;&#65292;&#22240;&#20026;&#36825;&#20250;&#23548;&#33268;&#29992;&#25143;&#31561;&#24453;&#26102;&#38388;&#36739;&#38271;&#12290;&#22240;&#27492;&#65292;&#29992;&#25143;&#24517;&#39035;&#23545;&#31995;&#32479;&#24212;&#32771;&#34385;&#30340;&#32422;&#26463;&#26377;&#30456;&#24403;&#31934;&#30830;&#30340;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#20004;&#31181;&#25913;&#36827;CA&#25928;&#29575;&#30340;&#26032;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20123;&#29942;&#39048;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;GrowAcq&#30340;&#33258;&#19979;&#32780;&#19978;&#26041;&#27861;&#65292;&#23427;&#20943;&#23569;&#20102;&#29992;&#25143;&#30340;&#26368;&#22823;&#31561;&#24453;&#26102;&#38388;&#65292;&#24182;&#20351;&#31995;&#32479;&#33021;&#22815;&#22788;&#29702;&#26356;&#22823;&#30340;&#20505;&#36873;&#32422;&#26463;&#38598;&#12290;&#23427;&#36824;&#20943;&#23569;&#20102;&#24635;&#30340;&#26597;&#35810;&#27425;&#25968;&#65292;&#25552;&#39640;&#20102;&#20132;&#20114;&#24335;&#32422;&#26463;&#33719;&#21462;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Constraint Acquisition (CA) systems can be used to assist in the modeling of constraint satisfaction problems. In (inter)active CA, the system is given a set of candidate constraints and posts queries to the user with the goal of finding the right constraints among the candidates. Current interactive CA algorithms suffer from at least two major bottlenecks. First, in order to converge, they require a large number of queries to be asked to the user. Second, they cannot handle large sets of candidate constraints, since these lead to large waiting times for the user. For this reason, the user must have fairly precise knowledge about what constraints the system should consider. In this paper, we alleviate these bottlenecks by presenting two novel methods that improve the efficiency of CA. First, we introduce a bottom-up approach named GrowAcq that reduces the maximum waiting time for the user and allows the system to handle much larger sets of candidate constraints. It also reduces the tot
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#38656;&#35201;&#21516;&#26102;&#36827;&#34892;&#25805;&#25511;&#21644;&#23548;&#33322;&#30340;&#20132;&#20114;&#24335;&#22810;&#30446;&#26631;&#25628;&#32034;&#20219;&#21153;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26032;&#29615;&#22659;&#20013;&#36827;&#34892;&#38646;&#26679;&#26412;&#36801;&#31227;&#65292;&#24182;&#23545;&#26410;&#35265;&#36807;&#30340;&#23376;&#20219;&#21153;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06125</link><description>&lt;p&gt;
&#23398;&#20064;&#23618;&#27425;&#20132;&#20114;&#24335;&#22810;&#30446;&#26631;&#25628;&#32034;&#20197;&#36827;&#34892;&#31227;&#21160;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Learning Hierarchical Interactive Multi-Object Search for Mobile Manipulation. (arXiv:2307.06125v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06125
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#38656;&#35201;&#21516;&#26102;&#36827;&#34892;&#25805;&#25511;&#21644;&#23548;&#33322;&#30340;&#20132;&#20114;&#24335;&#22810;&#30446;&#26631;&#25628;&#32034;&#20219;&#21153;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26032;&#29615;&#22659;&#20013;&#36827;&#34892;&#38646;&#26679;&#26412;&#36801;&#31227;&#65292;&#24182;&#23545;&#26410;&#35265;&#36807;&#30340;&#23376;&#20219;&#21153;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#30446;&#26631;&#25628;&#32034;&#26041;&#27861;&#20351;&#24471;&#26426;&#22120;&#20154;&#21487;&#20197;&#22312;&#33258;&#30001;&#36335;&#24452;&#19978;&#36827;&#34892;&#25628;&#32034;&#65292;&#28982;&#32780;&#65292;&#22312;&#38750;&#32467;&#26500;&#21270;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#29615;&#22659;&#20013;&#25805;&#20316;&#30340;&#26426;&#22120;&#20154;&#32463;&#24120;&#38656;&#35201;&#25805;&#25511;&#29615;&#22659;&#20197;&#28385;&#36275;&#20182;&#20204;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20132;&#20114;&#24335;&#22810;&#30446;&#26631;&#25628;&#32034;&#20219;&#21153;&#65292;&#26426;&#22120;&#20154;&#38656;&#35201;&#25171;&#24320;&#38376;&#20197;&#27983;&#35272;&#25151;&#38388;&#65292;&#24182;&#22312;&#27249;&#26588;&#21644;&#25277;&#23625;&#20869;&#25628;&#32034;&#30446;&#26631;&#29289;&#21697;&#12290;&#36825;&#20123;&#26032;&#25361;&#25112;&#38656;&#35201;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#32467;&#21512;&#25805;&#25511;&#21644;&#23548;&#33322;&#25216;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;HIMOS&#65292;&#19968;&#31181;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23398;&#20064;&#32452;&#21512;&#25506;&#32034;&#12289;&#23548;&#33322;&#21644;&#25805;&#25511;&#25216;&#33021;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22260;&#32469;&#35821;&#20041;&#22320;&#22270;&#35760;&#24518;&#30340;&#25277;&#35937;&#39640;&#32423;&#21160;&#20316;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#25506;&#32034;&#36807;&#30340;&#29615;&#22659;&#20316;&#20026;&#23454;&#20363;&#23548;&#33322;&#28857;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;HIMOS&#21487;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#26377;&#25928;&#22320;&#36801;&#31227;&#21040;&#26032;&#30340;&#29615;&#22659;&#20013;&#65292;&#24182;&#19988;&#23545;&#20110;&#26410;&#35265;&#36807;&#30340;&#23376;&#20219;&#21153;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing object-search approaches enable robots to search through free pathways, however, robots operating in unstructured human-centered environments frequently also have to manipulate the environment to their needs. In this work, we introduce a novel interactive multi-object search task in which a robot has to open doors to navigate rooms and search inside cabinets and drawers to find target objects. These new challenges require combining manipulation and navigation skills in unexplored environments. We present HIMOS, a hierarchical reinforcement learning approach that learns to compose exploration, navigation, and manipulation skills. To achieve this, we design an abstract high-level action space around a semantic map memory and leverage the explored environment as instance navigation points. We perform extensive experiments in simulation and the real-world that demonstrate that HIMOS effectively transfers to new environments in a zero-shot manner. It shows robustness to unseen subp
&lt;/p&gt;</description></item><item><title>TreeFormer&#26159;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;Transformer&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#21333;&#20010;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#20013;&#36827;&#34892;&#26641;&#26408;&#35745;&#25968;&#12290;&#23427;&#21033;&#29992;&#37329;&#23383;&#22612;&#26641;&#34920;&#31034;&#27169;&#22359;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#22238;&#24402;&#22120;&#36827;&#34892;&#26641;&#26408;&#23494;&#24230;&#20272;&#31639;&#12290;&#27492;&#22806;&#65292;&#37319;&#29992;&#23616;&#37096;&#26641;&#23494;&#24230;&#19968;&#33268;&#24615;&#21644;&#25490;&#21517;&#25439;&#22833;&#26469;&#21033;&#29992;&#26080;&#26631;&#31614;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#21644;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.06118</link><description>&lt;p&gt;
TreeFormer:&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;Transformer&#30340;&#21333;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#20013;&#26641;&#26408;&#35745;&#25968;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TreeFormer: a Semi-Supervised Transformer-based Framework for Tree Counting from a Single High Resolution Image. (arXiv:2307.06118v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06118
&lt;/p&gt;
&lt;p&gt;
TreeFormer&#26159;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;Transformer&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#21333;&#20010;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#20013;&#36827;&#34892;&#26641;&#26408;&#35745;&#25968;&#12290;&#23427;&#21033;&#29992;&#37329;&#23383;&#22612;&#26641;&#34920;&#31034;&#27169;&#22359;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#22238;&#24402;&#22120;&#36827;&#34892;&#26641;&#26408;&#23494;&#24230;&#20272;&#31639;&#12290;&#27492;&#22806;&#65292;&#37319;&#29992;&#23616;&#37096;&#26641;&#23494;&#24230;&#19968;&#33268;&#24615;&#21644;&#25490;&#21517;&#25439;&#22833;&#26469;&#21033;&#29992;&#26080;&#26631;&#31614;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#21644;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25668;&#24433;&#27979;&#37327;&#21644;&#36965;&#24863;&#20013;&#65292;&#21033;&#29992;&#21333;&#20010;&#33322;&#31354;&#21644;&#21355;&#26143;&#22270;&#20687;&#36827;&#34892;&#26641;&#26408;&#23494;&#24230;&#20272;&#31639;&#21644;&#35745;&#25968;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20294;&#22312;&#26862;&#26519;&#31649;&#29702;&#20013;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;Transformer&#30340;&#26641;&#26408;&#35745;&#25968;&#26694;&#26550;&#65292;&#36890;&#36807;&#20943;&#23569;&#36965;&#24863;&#22270;&#20687;&#19978;&#26114;&#36149;&#30340;&#26641;&#26408;&#26631;&#27880;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;TreeFormer&#65292;&#39318;&#20808;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#22359;&#30340;&#37329;&#23383;&#22612;&#26641;&#34920;&#31034;&#27169;&#22359;&#65292;&#29992;&#20110;&#22312;&#32534;&#30721;&#38454;&#27573;&#25552;&#21462;&#22810;&#23610;&#24230;&#29305;&#24449;&#12290;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#22522;&#20110;&#19978;&#19979;&#25991;&#27880;&#24847;&#21147;&#30340;&#29305;&#24449;&#34701;&#21512;&#21644;&#26641;&#23494;&#24230;&#22238;&#24402;&#22120;&#27169;&#22359;&#65292;&#21033;&#29992;&#32534;&#30721;&#22120;&#20013;&#30340;&#31283;&#20581;&#29305;&#24449;&#26469;&#20272;&#31639;&#35299;&#30721;&#22120;&#20013;&#30340;&#26641;&#23494;&#24230;&#22270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#37329;&#23383;&#22612;&#23398;&#20064;&#31574;&#30053;&#65292;&#21253;&#25324;&#23616;&#37096;&#26641;&#23494;&#24230;&#19968;&#33268;&#24615;&#21644;&#23616;&#37096;&#26641;&#35745;&#25968;&#25490;&#21517;&#25439;&#22833;&#65292;&#20197;&#21033;&#29992;&#26080;&#26631;&#31614;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#12290;&#26368;&#21518;&#65292;&#24341;&#20837;&#20102;&#26641;&#35745;&#25968;&#35760;&#21495;&#26469;&#35843;&#33410;&#32593;&#32476;&#65292;&#36890;&#36807;&#35745;&#31639;...
&lt;/p&gt;
&lt;p&gt;
Automatic tree density estimation and counting using single aerial and satellite images is a challenging task in photogrammetry and remote sensing, yet has an important role in forest management. In this paper, we propose the first semisupervised transformer-based framework for tree counting which reduces the expensive tree annotations for remote sensing images. Our method, termed as TreeFormer, first develops a pyramid tree representation module based on transformer blocks to extract multi-scale features during the encoding stage. Contextual attention-based feature fusion and tree density regressor modules are further designed to utilize the robust features from the encoder to estimate tree density maps in the decoder. Moreover, we propose a pyramid learning strategy that includes local tree density consistency and local tree count ranking losses to utilize unlabeled images into the training process. Finally, the tree counter token is introduced to regulate the network by computing th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#65292;&#24471;&#21040;&#20102;&#22312;&#22823;&#20294;&#26377;&#38480;&#30340; $n$ &#21644;&#20219;&#24847;&#22266;&#23450;&#32593;&#32476;&#28145;&#24230;&#19979;&#25104;&#31435;&#30340;&#27491;&#24577;&#36924;&#36817;&#30340;&#23450;&#37327;&#30028;&#38480;&#65292;&#35777;&#26126;&#20102;&#38543;&#26426;&#20840;&#36830;&#25509;&#32593;&#32476;&#19982;&#30456;&#24212;&#30340;&#26080;&#38480;&#23485;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#36317;&#31163;&#25353;&#29031; $n^{-\gamma}$ &#32553;&#25918;&#65292;&#30028;&#38480;&#22312;&#32593;&#32476;&#23485;&#24230;&#30340;&#20381;&#36182;&#24615;&#26041;&#38754;&#20248;&#20110;&#20197;&#21069;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.06092</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23450;&#37327;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
Quantitative CLTs in Deep Neural Networks. (arXiv:2307.06092v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#65292;&#24471;&#21040;&#20102;&#22312;&#22823;&#20294;&#26377;&#38480;&#30340; $n$ &#21644;&#20219;&#24847;&#22266;&#23450;&#32593;&#32476;&#28145;&#24230;&#19979;&#25104;&#31435;&#30340;&#27491;&#24577;&#36924;&#36817;&#30340;&#23450;&#37327;&#30028;&#38480;&#65292;&#35777;&#26126;&#20102;&#38543;&#26426;&#20840;&#36830;&#25509;&#32593;&#32476;&#19982;&#30456;&#24212;&#30340;&#26080;&#38480;&#23485;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#36317;&#31163;&#25353;&#29031; $n^{-\gamma}$ &#32553;&#25918;&#65292;&#30028;&#38480;&#22312;&#32593;&#32476;&#23485;&#24230;&#30340;&#20381;&#36182;&#24615;&#26041;&#38754;&#20248;&#20110;&#20197;&#21069;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#65292;&#20854;&#20013;&#38544;&#34255;&#23618;&#23485;&#24230;&#19982;&#22823;&#24120;&#25968; $n$ &#25104;&#27604;&#20363;&#12290;&#22312;&#38750;&#32447;&#24615;&#30340;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#22312;&#22823;&#20294;&#26377;&#38480;&#30340; $n$ &#21644;&#20219;&#24847;&#22266;&#23450;&#32593;&#32476;&#28145;&#24230;&#19979;&#25104;&#31435;&#30340;&#27491;&#24577;&#36924;&#36817;&#30340;&#23450;&#37327;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#23450;&#29702;&#34920;&#26126;&#65292;&#26080;&#35770;&#26159;&#23545;&#20110;&#26377;&#38480;&#32500;&#20998;&#24067;&#36824;&#26159;&#25972;&#20010;&#36807;&#31243;&#65292;&#38543;&#26426;&#20840;&#36830;&#25509;&#32593;&#32476;&#65288;&#21450;&#20854;&#23548;&#25968;&#65289;&#19982;&#30456;&#24212;&#30340;&#26080;&#38480;&#23485;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#36317;&#31163;&#37117;&#20250;&#25353;&#29031; $n^{-\gamma}$ &#32553;&#25918;&#65292;&#20854;&#20013; $\gamma&gt;0$&#65292;&#25351;&#25968;&#21462;&#20915;&#20110;&#29992;&#20110;&#24230;&#37327;&#24046;&#24322;&#30340;&#24230;&#37327;&#26041;&#24335;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#22312;&#32593;&#32476;&#23485;&#24230;&#30340;&#20381;&#36182;&#24615;&#26041;&#38754;&#27604;&#25991;&#29486;&#20013;&#20197;&#21069;&#25552;&#20379;&#30340;&#20219;&#20309;&#30028;&#38480;&#37117;&#35201;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the distribution of a fully connected neural network with random Gaussian weights and biases in which the hidden layer widths are proportional to a large constant $n$. Under mild assumptions on the non-linearity, we obtain quantitative bounds on normal approximations valid at large but finite $n$ and any fixed network depth. Our theorems show, both for the finite-dimensional distributions and the entire process, that the distance between a random fully connected network (and its derivatives) to the corresponding infinite width Gaussian process scales like $n^{-\gamma}$ for $\gamma&gt;0,$ with the exponent depending on the metric used to measure discrepancy. Our bounds are stronger in terms of their dependence on network width than any previously available in the literature.
&lt;/p&gt;</description></item><item><title>VELMA&#26159;&#19968;&#20010;&#21475;&#22836;&#21270;&#30340;LLM&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;&#20154;&#31867;&#20889;&#20316;&#30340;&#23548;&#33322;&#25351;&#20196;&#20013;&#30340;&#22320;&#26631;&#24182;&#32467;&#21512;CLIP&#26469;&#36827;&#34892;&#35270;&#35273;&#29615;&#22659;&#30340;&#29702;&#35299;&#65292;&#20197;&#23454;&#29616;&#22312;&#34903;&#26223;&#20013;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#12290;</title><link>http://arxiv.org/abs/2307.06082</link><description>&lt;p&gt;
VELMA: LLM&#26234;&#33021;&#20307;&#22312;&#34903;&#26223;&#20013;&#36827;&#34892;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#30340;&#21475;&#22836;&#21270;&#20307;&#29616;
&lt;/p&gt;
&lt;p&gt;
VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation in Street View. (arXiv:2307.06082v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06082
&lt;/p&gt;
&lt;p&gt;
VELMA&#26159;&#19968;&#20010;&#21475;&#22836;&#21270;&#30340;LLM&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;&#20154;&#31867;&#20889;&#20316;&#30340;&#23548;&#33322;&#25351;&#20196;&#20013;&#30340;&#22320;&#26631;&#24182;&#32467;&#21512;CLIP&#26469;&#36827;&#34892;&#35270;&#35273;&#29615;&#22659;&#30340;&#29702;&#35299;&#65292;&#20197;&#23454;&#29616;&#22312;&#34903;&#26223;&#20013;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#22686;&#37327;&#20915;&#31574;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20197;&#20307;&#29616;&#20154;&#24037;&#26234;&#33021;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#20854;&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20043;&#19968;&#26159;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;(VLN)&#65292;&#23427;&#38656;&#35201;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20197;&#21450;&#31354;&#38388;&#21644;&#26102;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#20010;&#20307;&#29616;&#26234;&#33021;&#20307;&#38656;&#35201;&#22312;&#34903;&#26223;&#31561;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#30340;&#35266;&#23519;&#22522;&#30784;&#19978;&#20934;&#30830;&#29702;&#35299;&#23548;&#33322;&#25351;&#20196;&#12290;&#23613;&#31649;LLM&#22312;&#20854;&#20182;&#30740;&#31350;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#22914;&#20309;&#26368;&#22909;&#22320;&#23558;&#23427;&#20204;&#19982;&#20132;&#20114;&#24335;&#35270;&#35273;&#29615;&#22659;&#36830;&#25509;&#36215;&#26469;&#20173;&#28982;&#26159;&#19968;&#20010;&#25345;&#32493;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VELMA&#65292;&#19968;&#31181;&#20351;&#29992;&#36712;&#36857;&#21644;&#35270;&#35273;&#29615;&#22659;&#35266;&#23519;&#30340;&#21475;&#22836;&#21270;&#20316;&#20026;&#19979;&#19968;&#27493;&#25805;&#20316;&#30340;&#19978;&#19979;&#25991;&#25552;&#31034;&#30340;LLM&#26234;&#33021;&#20307;&#12290;&#35270;&#35273;&#20449;&#24687;&#36890;&#36807;&#19968;&#20010;&#27969;&#31243;&#36827;&#34892;&#21475;&#22836;&#21270;&#65292;&#35813;&#27969;&#31243;&#20174;&#20154;&#31867;&#32534;&#20889;&#30340;&#23548;&#33322;&#25351;&#20196;&#20013;&#25552;&#21462;&#22320;&#26631;&#65292;&#24182;&#20351;&#29992;CLIP&#26469;&#30830;&#23450;&#23427;&#20204;&#22312;&#24403;&#21069;&#20840;&#26223;&#35270;&#22270;&#20013;&#30340;&#21487;&#35265;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incremental decision making in real-world environments is one of the most challenging tasks in embodied artificial intelligence. One particularly demanding scenario is Vision and Language Navigation~(VLN) which requires visual and natural language understanding as well as spatial and temporal reasoning capabilities. The embodied agent needs to ground its understanding of navigation instructions in observations of a real-world environment like Street View. Despite the impressive results of LLMs in other research areas, it is an ongoing problem of how to best connect them with an interactive visual environment. In this work, we propose VELMA, an embodied LLM agent that uses a verbalization of the trajectory and of visual environment observations as contextual prompt for the next action. Visual information is verbalized by a pipeline that extracts landmarks from the human written navigation instructions and uses CLIP to determine their visibility in the current panorama view. We show that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#30340;&#31616;&#21270;&#22810;&#20803;&#39640;&#26031;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#30333;&#21270;&#36716;&#25442;&#21487;&#29983;&#25104;&#21487;&#35270;&#21270;&#30340;&#28909;&#22270;&#26469;&#35299;&#37322;&#29305;&#24449;&#65292;&#24182;&#22312;MVTec-AD&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06052</link><description>&lt;p&gt;
&#22270;&#20687;&#20013;&#30340;&#22810;&#20803;&#39640;&#26031;&#24322;&#24120;&#26816;&#27979;&#30340;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Visualization for Multivariate Gaussian Anomaly Detection in Images. (arXiv:2307.06052v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#30340;&#31616;&#21270;&#22810;&#20803;&#39640;&#26031;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#30333;&#21270;&#36716;&#25442;&#21487;&#29983;&#25104;&#21487;&#35270;&#21270;&#30340;&#28909;&#22270;&#26469;&#35299;&#37322;&#29305;&#24449;&#65292;&#24182;&#22312;MVTec-AD&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;PaDiM&#65288;&#36890;&#36807;&#23454;&#20363;&#24314;&#27169;&#36827;&#34892;&#20687;&#32032;&#32423;&#24322;&#24120;&#26816;&#27979;&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22270;&#20687;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#23558;&#20174;&#39592;&#24178;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#25552;&#21462;&#30340;&#29305;&#24449;&#21521;&#37327;&#25311;&#21512;&#20026;&#21333;&#20010;&#22810;&#20803;&#39640;&#26031;&#65288;MVG&#65289;&#20998;&#24067;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#30340;&#39532;&#21704;&#25289;&#35834;&#27604;&#26031;&#36317;&#31163;&#20316;&#20026;&#24322;&#24120;&#24471;&#20998;&#12290;&#25105;&#20204;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#20013;&#38388;&#27493;&#39588;&#65292;&#36890;&#36807;&#23545;&#29305;&#24449;&#21521;&#37327;&#24212;&#29992;&#30333;&#21270;&#21464;&#25442;&#65292;&#33021;&#22815;&#29983;&#25104;&#33021;&#22815;&#21487;&#35270;&#21270;&#35299;&#37322;MVG&#25152;&#23398;&#21040;&#30340;&#29305;&#24449;&#30340;&#28909;&#22270;&#12290;&#25105;&#20204;&#22312;MVTec-AD&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25552;&#20986;&#30340;&#25216;&#26415;&#65292;&#24182;&#32467;&#26524;&#26174;&#31034;&#20102;&#35270;&#35273;&#27169;&#22411;&#39564;&#35777;&#30340;&#37325;&#35201;&#24615;&#65292;&#20026;&#36825;&#20010;&#26694;&#26550;&#20013;&#30340;&#19968;&#20123;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#30475;&#19981;&#35265;&#30340;&#38382;&#39064;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;&#26412;&#25991;&#29983;&#25104;&#30340;&#21487;&#35270;&#21270;&#32467;&#26524;&#21487;&#20197;&#22312;https://doi.org/10.5281/zenodo.7937978&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a simplified variation of the PaDiM (Pixel-Wise Anomaly Detection through Instance Modeling) method for anomaly detection in images, fitting a single multivariate Gaussian (MVG) distribution to the feature vectors extracted from a backbone convolutional neural network (CNN) and using their Mahalanobis distance as the anomaly score. We introduce an intermediate step in this framework by applying a whitening transformation to the feature vectors, which enables the generation of heatmaps capable of visually explaining the features learned by the MVG. The proposed technique is evaluated on the MVTec-AD dataset, and the results show the importance of visual model validation, providing insights into issues in this framework that were otherwise invisible. The visualizations generated for this paper are publicly available at https://doi.org/10.5281/zenodo.7937978.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;OOD&#27979;&#35797;&#22810;&#22270;&#20013;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21452;&#21487;&#20132;&#25442;&#24615;&#27010;&#24565;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#39044;&#27979;&#27169;&#24335;&#19988;&#21487;&#33021;&#20914;&#31361;&#30340;&#20851;&#31995;&#31867;&#22411;&#38598;&#21512;&#30340;&#23646;&#24615;&#22810;&#22270;&#12290;</title><link>http://arxiv.org/abs/2307.06046</link><description>&lt;p&gt;
&#20511;&#21161;&#26032;&#30340;&#20851;&#31995;&#31867;&#22411;&#21644;&#33410;&#28857;&#65292;&#20197;OOD&#22810;&#20219;&#21153;&#35270;&#35282;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
An OOD Multi-Task Perspective for Link Prediction with New Relation Types and Nodes. (arXiv:2307.06046v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;OOD&#27979;&#35797;&#22810;&#22270;&#20013;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21452;&#21487;&#20132;&#25442;&#24615;&#27010;&#24565;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#39044;&#27979;&#27169;&#24335;&#19988;&#21487;&#33021;&#20914;&#31361;&#30340;&#20851;&#31995;&#31867;&#22411;&#38598;&#21512;&#30340;&#23646;&#24615;&#22810;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24402;&#32435;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#25512;&#26029;&#20855;&#26377;&#23646;&#24615;&#30340;&#22810;&#22270;&#20013;&#26032;&#27979;&#35797;&#22810;&#22270;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#32570;&#22833;&#23646;&#24615;&#38142;&#25509;&#65288;&#20851;&#31995;&#65289;&#12290;&#20256;&#32479;&#30340;&#20851;&#31995;&#23398;&#20064;&#26041;&#27861;&#38754;&#20020;&#30528;&#23545;OOD&#27979;&#35797;&#22810;&#22270;&#30340;&#26377;&#38480;&#27867;&#21270;&#33021;&#21147;&#30340;&#25361;&#25112;&#65292;&#36825;&#20123;&#22810;&#22270;&#21253;&#21547;&#20102;&#35757;&#32451;&#20013;&#26410;&#35265;&#36807;&#30340;&#26032;&#33410;&#28857;&#21644;&#26032;&#20851;&#31995;&#31867;&#22411;&#12290;&#26368;&#36817;&#65292;&#39640;&#31561;&#20154;&#65288;2023&#65289;&#22312;&#25152;&#26377;&#20851;&#31995;&#31867;&#22411;&#20849;&#20139;&#30456;&#21516;&#32467;&#26500;&#39044;&#27979;&#27169;&#24335;&#65288;&#21333;&#20010;&#20219;&#21153;&#65289;&#30340;&#21807;&#19968;&#20551;&#35774;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21452;&#21487;&#20132;&#25442;&#24615;&#29702;&#35770;&#27010;&#24565;&#65288;&#29992;&#20110;&#33410;&#28857;&#21644;&#20851;&#31995;&#31867;&#22411;&#65289;&#26469;&#36827;&#34892;OOD&#38142;&#25509;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#19982;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#35774;&#35745;&#30340;&#65288;&#21333;&#20010;&#65289;&#21487;&#20132;&#25442;&#24615;&#65288;&#20165;&#29992;&#20110;&#33410;&#28857;&#65289;&#30456;&#21453;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#21452;&#21487;&#20132;&#25442;&#24615;&#27010;&#24565;&#25193;&#23637;&#21040;&#22810;&#20219;&#21153;&#21452;&#21487;&#20132;&#25442;&#24615;&#65292;&#20854;&#20013;&#25105;&#20204;&#23450;&#20041;&#20102;&#23646;&#24615;&#22810;&#22270;&#20013;&#30340;&#38142;&#25509;&#39044;&#27979;&#65292;&#36825;&#20123;&#22270;&#21487;&#33021;&#23545;&#19981;&#21516;&#30340;&#20851;&#31995;&#31867;&#22411;&#38598;&#21512;&#20855;&#26377;&#19981;&#21516;&#19988;&#21487;&#33021;&#20914;&#31361;&#30340;&#39044;&#27979;&#27169;&#24335;&#65288;&#22810;&#20010;&#20219;&#21153;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of inductive link prediction in (discrete) attributed multigraphs infers missing attributed links (relations) between nodes in new test multigraphs. Traditional relational learning methods face the challenge of limited generalization to OOD test multigraphs containing both novel nodes and novel relation types not seen in training. Recently, under the only assumption that all relation types share the same structural predictive patterns (single task), Gao et al. (2023) proposed an OOD link prediction method using the theoretical concept of double exchangeability (for nodes &amp; relation types), in contrast to the (single) exchangeability (only for nodes) used to design Graph Neural Networks (GNNs). In this work we further extend the double exchangeability concept to multi-task double exchangeability, where we define link prediction in attributed multigraphs that can have distinct and potentially conflicting predictive patterns for different sets of relation types (multiple tasks). 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#22914;&#20309;&#34987;&#35270;&#20026;&#33402;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21746;&#23398;&#26694;&#26550;&#65292;&#35748;&#20026;&#19968;&#20123;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#21487;&#20197;&#34987;&#35270;&#20026; "&#29616;&#25104;&#21697;" &#33402;&#26415;&#12290;</title><link>http://arxiv.org/abs/2307.06033</link><description>&lt;p&gt;
AI&#29983;&#25104;&#30340;&#22270;&#20687;&#65306;`&#29616;&#25104;&#21697;'&#30340;&#26032;&#26102;&#20195;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI-Generated Imagery: A New Era for the `Readymade'. (arXiv:2307.06033v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06033
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#22914;&#20309;&#34987;&#35270;&#20026;&#33402;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21746;&#23398;&#26694;&#26550;&#65292;&#35748;&#20026;&#19968;&#20123;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#21487;&#20197;&#34987;&#35270;&#20026; "&#29616;&#25104;&#21697;" &#33402;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26415;&#35821; "&#33402;&#26415;" &#27809;&#26377;&#20855;&#20307;&#30340;&#23450;&#20041;&#65292;&#20294;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#30001;&#29983;&#25104;AI&#31995;&#32479;&#65288;&#22914;Midjourney&#65289;&#20135;&#29983;&#30340;&#25968;&#23383;&#22270;&#20687;&#20026;&#20160;&#20040;&#32463;&#24120;&#34987;&#31216;&#20026;&#33402;&#26415;&#12290;&#30446;&#21069;&#65292;&#20851;&#20110;&#23558;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#20998;&#31867;&#20026;&#33402;&#26415;&#30340;&#35752;&#35770;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#26159;&#21516;&#36136;&#30340;&#65292;&#32570;&#20047;&#36866;&#29992;&#20110;&#26356;&#20256;&#32479;&#33402;&#26415;&#23186;&#20307;&#21046;&#20316;&#26041;&#24335;&#30340;&#32454;&#33268;&#26041;&#38754;&#12290;&#26412;&#25991;&#26088;&#22312;&#22312;&#33402;&#26415;&#32972;&#26223;&#19979;&#23558;&#37325;&#35201;&#30340;&#21746;&#23398;&#32771;&#34385;&#24102;&#21040;&#20851;&#20110;AI&#29983;&#25104;&#22270;&#20687;&#30340;&#35752;&#35770;&#30340;&#34920;&#38754;&#12290;&#25105;&#20204;&#20351;&#29992;&#29616;&#26377;&#30340;&#21746;&#23398;&#26694;&#26550;&#21644;&#35821;&#35328;&#29702;&#35770;&#65292;&#25552;&#20986;&#19968;&#20123;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#20973;&#20511;&#20854;&#22312;&#36825;&#20123;&#26694;&#26550;&#20869;&#30340;&#35270;&#35273;&#29305;&#24615;&#65292;&#21487;&#20197;&#34987;&#20316;&#20026; "&#29616;&#25104;&#21697;" &#32771;&#34385;&#20026;&#33402;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the term `art' defies any concrete definition, this paper aims to examine how digital images produced by generative AI systems, such as Midjourney, have come to be so regularly referred to as such. The discourse around the classification of AI-generated imagery as art is currently somewhat homogeneous, lacking the more nuanced aspects that would apply to more traditional modes of artistic media production. This paper aims to bring important philosophical considerations to the surface of the discussion around AI-generated imagery in the context of art. We employ existing philosophical frameworks and theories of language to suggest that some AI-generated imagery, by virtue of its visual properties within these frameworks, can be presented as `readymades' for consideration as art.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#38750;&#31070;&#32463;&#32593;&#32476;&#26102;&#38388;&#24863;&#30693;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550;&#65292;&#36890;&#36807;&#20004;&#26041;&#38754;&#19977;&#35270;&#22270;&#26631;&#31614;&#20256;&#25773;&#12289;&#31232;&#30095;&#30456;&#20284;&#24230;&#12289;Sinkhorn&#31639;&#23376;&#21644;&#26102;&#24577;&#36845;&#20195;&#23398;&#20064;&#26469;&#25552;&#39640;&#23545;&#40784;&#24615;&#33021;&#65292;&#24182;&#20943;&#23569;&#27169;&#22411;&#30340;&#26102;&#38388;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2307.06013</link><description>&lt;p&gt;
&#36890;&#36807;&#20004;&#26041;&#38754;&#19977;&#35270;&#22270;&#26631;&#31614;&#20256;&#25773;&#30340;&#39640;&#25928;&#26102;&#38388;&#24863;&#30693;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Effective and Efficient Time-aware Entity Alignment Framework via Two-aspect Three-view Label Propagation. (arXiv:2307.06013v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#38750;&#31070;&#32463;&#32593;&#32476;&#26102;&#38388;&#24863;&#30693;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550;&#65292;&#36890;&#36807;&#20004;&#26041;&#38754;&#19977;&#35270;&#22270;&#26631;&#31614;&#20256;&#25773;&#12289;&#31232;&#30095;&#30456;&#20284;&#24230;&#12289;Sinkhorn&#31639;&#23376;&#21644;&#26102;&#24577;&#36845;&#20195;&#23398;&#20064;&#26469;&#25552;&#39640;&#23545;&#40784;&#24615;&#33021;&#65292;&#24182;&#20943;&#23569;&#27169;&#22411;&#30340;&#26102;&#38388;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#23545;&#40784;&#26088;&#22312;&#23547;&#25214;&#19981;&#21516;&#30693;&#35782;&#22270;&#35889;&#20043;&#38388;&#30340;&#31561;&#20215;&#23454;&#20307;&#23545;&#65292;&#36825;&#23545;&#20110;&#25512;&#21160;&#30693;&#35782;&#34701;&#21512;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#26102;&#24577;&#30693;&#35782;&#22270;&#35889;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#26102;&#38388;&#24863;&#30693;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#20986;&#29616;&#20197;&#22686;&#24378;&#23545;&#40784;&#25928;&#26524;&#12290;&#29616;&#26377;&#30340;&#26102;&#38388;&#24863;&#30693;&#23454;&#20307;&#23545;&#40784;&#27169;&#22411;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;GNN&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#24456;&#38590;&#23558;&#20854;&#24212;&#29992;&#21040;&#22823;&#35268;&#27169;&#26102;&#24577;&#30693;&#35782;&#22270;&#35889;&#20013;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#38750;&#31070;&#32463;&#32593;&#32476;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550;LightTEA&#65292;&#23427;&#30001;&#22235;&#20010;&#20851;&#38190;&#32452;&#20214;&#32452;&#25104;&#65306;(1)&#20004;&#26041;&#38754;&#19977;&#35270;&#22270;&#26631;&#31614;&#20256;&#25773;&#65292;(2)&#24102;&#26377;&#26102;&#24577;&#32422;&#26463;&#30340;&#31232;&#30095;&#30456;&#20284;&#24230;&#65292;(3)Sinkhorn&#31639;&#23376;&#65292;&#20197;&#21450;(4)&#26102;&#24577;&#36845;&#20195;&#23398;&#20064;&#12290;&#25152;&#26377;&#36825;&#20123;&#27169;&#22359;&#20849;&#21516;&#20316;&#29992;&#65292;&#25552;&#39640;&#20102;&#23454;&#20307;&#23545;&#40784;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#27169;&#22411;&#30340;&#26102;&#38388;&#24320;&#38144;&#12290;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity alignment (EA) aims to find the equivalent entity pairs between different knowledge graphs (KGs), which is crucial to promote knowledge fusion. With the wide use of temporal knowledge graphs (TKGs), time-aware EA (TEA) methods appear to enhance EA. Existing TEA models are based on Graph Neural Networks (GNN) and achieve state-of-the-art (SOTA) performance, but it is difficult to transfer them to large-scale TKGs due to the scalability issue of GNN. In this paper, we propose an effective and efficient non-neural EA framework between TKGs, namely LightTEA, which consists of four essential components: (1) Two-aspect Three-view Label Propagation, (2) Sparse Similarity with Temporal Constraints, (3) Sinkhorn Operator, and (4) Temporal Iterative Learning. All of these modules work together to improve the performance of EA while reducing the time consumption of the model. Extensive experiments on public datasets indicate that our proposed model significantly outperforms the SOTA method
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#22522;&#20110;Transformer&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#24212;&#29992;&#12290;Transformer&#22312;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#19978;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#21253;&#25324;&#19981;&#31283;&#23450;&#35757;&#32451;&#12289;&#20449;&#29992;&#20998;&#37197;&#12289;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#31561;&#26041;&#38754;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#20854;&#22312;&#34920;&#31034;&#23398;&#20064;&#12289;&#29366;&#24577;&#36716;&#31227;&#21644;&#22870;&#21169;&#20989;&#25968;&#24314;&#27169;&#20197;&#21450;&#31574;&#30053;&#20248;&#21270;&#31561;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#36824;&#26377;&#19968;&#20123;&#26368;&#26032;&#30340;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;Transformer&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.05979</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#24378;&#21270;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Transformers in Reinforcement Learning: A Survey. (arXiv:2307.05979v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#22522;&#20110;Transformer&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#24212;&#29992;&#12290;Transformer&#22312;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#19978;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#21253;&#25324;&#19981;&#31283;&#23450;&#35757;&#32451;&#12289;&#20449;&#29992;&#20998;&#37197;&#12289;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#31561;&#26041;&#38754;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#20854;&#22312;&#34920;&#31034;&#23398;&#20064;&#12289;&#29366;&#24577;&#36716;&#31227;&#21644;&#22870;&#21169;&#20989;&#25968;&#24314;&#27169;&#20197;&#21450;&#31574;&#30053;&#20248;&#21270;&#31561;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#36824;&#26377;&#19968;&#20123;&#26368;&#26032;&#30340;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;Transformer&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#20154;&#31561;&#39046;&#22495;&#20855;&#26377;&#26174;&#33879;&#30340;&#24433;&#21709;&#65292;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#12290;&#26412;&#32508;&#36848;&#25506;&#35752;&#20102;Transformer&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#30340;&#24212;&#29992;&#65292;&#23427;&#34987;&#35270;&#20026;&#35299;&#20915;&#19981;&#31283;&#23450;&#35757;&#32451;&#12289;&#20449;&#29992;&#20998;&#37197;&#12289;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#31561;&#25361;&#25112;&#30340;&#26377;&#24076;&#26395;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#39318;&#20808;&#31616;&#35201;&#20171;&#32461;&#20102;RL&#39046;&#22495;&#30340;&#27010;&#36848;&#65292;&#28982;&#21518;&#35752;&#35770;&#20102;&#20256;&#32479;RL&#31639;&#27861;&#30340;&#25361;&#25112;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;Transformer&#21450;&#20854;&#21464;&#20307;&#30340;&#29305;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#20351;&#20854;&#36866;&#21512;&#24212;&#23545;RL&#20013;&#22266;&#26377;&#25361;&#25112;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;Transformer&#22312;RL&#20013;&#21508;&#20010;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#34920;&#31034;&#23398;&#20064;&#12289;&#29366;&#24577;&#36716;&#31227;&#21644;&#22870;&#21169;&#20989;&#25968;&#24314;&#27169;&#20197;&#21450;&#31574;&#30053;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26368;&#36817;&#30340;&#30740;&#31350;&#65292;&#26088;&#22312;&#25552;&#39640;Transformer&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have significantly impacted domains like natural language processing, computer vision, and robotics, where they improve performance compared to other neural networks. This survey explores how transformers are used in reinforcement learning (RL), where they are seen as a promising solution for addressing challenges such as unstable training, credit assignment, lack of interpretability, and partial observability. We begin by providing a brief domain overview of RL, followed by a discussion on the challenges of classical RL algorithms. Next, we delve into the properties of the transformer and its variants and discuss the characteristics that make them well-suited to address the challenges inherent in RL. We examine the application of transformers to various aspects of RL, including representation learning, transition and reward function modeling, and policy optimization. We also discuss recent research that aims to enhance the interpretability and efficiency of transformers i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SDD&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#23433;&#20840;&#33258;&#33976;&#39311;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#23548;&#22122;&#22768;&#20272;&#35745;&#19982;&#26080;&#26465;&#20214;&#27169;&#22411;&#21305;&#37197;&#65292;&#22312;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#28040;&#38500;&#26377;&#23475;&#20869;&#23481;&#30340;&#27604;&#20363;&#26356;&#22823;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#22270;&#20687;&#36136;&#37327;&#65292;&#24182;&#19988;&#20801;&#35768;&#19968;&#27425;&#31227;&#38500;&#22810;&#20010;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2307.05977</link><description>&lt;p&gt;
&#23454;&#29616;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#23433;&#20840;&#33258;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Towards Safe Self-Distillation of Internet-Scale Text-to-Image Diffusion Models. (arXiv:2307.05977v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SDD&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#23433;&#20840;&#33258;&#33976;&#39311;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#23548;&#22122;&#22768;&#20272;&#35745;&#19982;&#26080;&#26465;&#20214;&#27169;&#22411;&#21305;&#37197;&#65292;&#22312;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#28040;&#38500;&#26377;&#23475;&#20869;&#23481;&#30340;&#27604;&#20363;&#26356;&#22823;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#22270;&#20687;&#36136;&#37327;&#65292;&#24182;&#19988;&#20801;&#35768;&#19968;&#27425;&#31227;&#38500;&#22810;&#20010;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20511;&#21161;&#20114;&#32852;&#32593;&#19978;&#20016;&#23500;&#30340;&#25968;&#25454;&#20855;&#26377;&#21331;&#36234;&#30340;&#36136;&#37327;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#31038;&#20250;&#20851;&#20999;&#65292;&#25285;&#24515;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#29983;&#25104;&#26377;&#23475;&#25110;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20869;&#23481;&#12290;&#36825;&#20123;&#20559;&#35265;&#21644;&#26377;&#23475;&#24615;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#20135;&#29983;&#65292;&#24182;&#19988;&#24456;&#38590;&#23436;&#20840;&#28040;&#38500;&#65292;&#36825;&#24050;&#25104;&#20026;&#23433;&#20840;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#30340;&#37325;&#35201;&#38556;&#30861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SDD&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#38450;&#27490;&#38382;&#39064;&#20869;&#23481;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#33258;&#33976;&#39311;&#25193;&#25955;&#27169;&#22411;&#26469;&#24341;&#23548;&#22522;&#20110;&#30446;&#26631;&#31227;&#38500;&#27010;&#24565;&#30340;&#22122;&#22768;&#20272;&#35745;&#19982;&#26080;&#26465;&#20214;&#27169;&#22411;&#21305;&#37197;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#28040;&#38500;&#26356;&#22823;&#27604;&#20363;&#30340;&#26377;&#23475;&#20869;&#23481;&#65292;&#21516;&#26102;&#19981;&#38477;&#20302;&#25972;&#20307;&#22270;&#20687;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#20801;&#35768;&#19968;&#27425;&#31227;&#38500;&#22810;&#20010;&#27010;&#24565;&#65292;&#32780;&#20043;&#21069;&#30340;&#24037;&#20316;&#21482;&#33021;&#19968;&#27425;&#31227;&#38500;&#19968;&#20010;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale image generation models, with impressive quality made possible by the vast amount of data available on the Internet, raise social concerns that these models may generate harmful or copyrighted content. The biases and harmfulness arise throughout the entire training process and are hard to completely remove, which have become significant hurdles to the safe deployment of these models. In this paper, we propose a method called SDD to prevent problematic content generation in text-to-image diffusion models. We self-distill the diffusion model to guide the noise estimate conditioned on the target removal concept to match the unconditional one. Compared to the previous methods, our method eliminates a much greater proportion of harmful content from the generated images without degrading the overall image quality. Furthermore, our method allows the removal of multiple concepts at once, whereas previous works are limited to removing a single concept at a time.
&lt;/p&gt;</description></item><item><title>VoxPoser&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;3D&#20215;&#20540;&#26144;&#23556;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#22312;&#22810;&#31181;&#25805;&#20316;&#20219;&#21153;&#19979;&#26681;&#25454;&#33258;&#30001;&#24418;&#24335;&#30340;&#25351;&#20196;&#21644;&#23545;&#35937;&#21512;&#25104;&#26426;&#22120;&#20154;&#36712;&#36857;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.05973</link><description>&lt;p&gt;
VoxPoser: &#29992;&#20110;&#24102;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#21487;&#32452;&#21512;&#30340;3D&#20215;&#20540;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models. (arXiv:2307.05973v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05973
&lt;/p&gt;
&lt;p&gt;
VoxPoser&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;3D&#20215;&#20540;&#26144;&#23556;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#22312;&#22810;&#31181;&#25805;&#20316;&#20219;&#21153;&#19979;&#26681;&#25454;&#33258;&#30001;&#24418;&#24335;&#30340;&#25351;&#20196;&#21644;&#23545;&#35937;&#21512;&#25104;&#26426;&#22120;&#20154;&#36712;&#36857;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#20016;&#23500;&#30340;&#21487;&#34892;&#21160;&#30693;&#35782;&#65292;&#21487;&#20197;&#20197;&#25512;&#29702;&#21644;&#35268;&#21010;&#30340;&#24418;&#24335;&#25552;&#21462;&#20986;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#20449;&#24687;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#20173;&#28982;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#30340;&#36816;&#21160;&#21407;&#35821;&#26469;&#25191;&#34892;&#19982;&#29615;&#22659;&#30340;&#29289;&#29702;&#20132;&#20114;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#29942;&#39048;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#32473;&#23450;&#24320;&#38598;&#25351;&#20196;&#21644;&#24320;&#38598;&#23545;&#35937;&#30340;&#24773;&#20917;&#19979;&#65292;&#20026;&#21508;&#31181;&#25805;&#20316;&#20219;&#21153;&#21512;&#25104;&#26426;&#22120;&#20154;&#36712;&#36857;&#65292;&#21363;&#19968;&#31995;&#21015;&#23494;&#38598;&#30340;6-DoF&#26411;&#31471;&#25191;&#34892;&#22120;&#36335;&#24452;&#28857;&#12290;&#25105;&#20204;&#39318;&#20808;&#35266;&#23519;&#21040;LLMs&#22312;&#32473;&#23450;&#33258;&#30001;&#24418;&#24335;&#30340;&#35821;&#35328;&#25351;&#20196;&#26102;&#25797;&#38271;&#25512;&#26029;&#21487;&#34892;&#24615;&#21644;&#32422;&#26463;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#36890;&#36807;&#21033;&#29992;&#23427;&#20204;&#30340;&#20195;&#30721;&#32534;&#20889;&#33021;&#21147;&#65292;&#23427;&#20204;&#21487;&#20197;&#19982;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#20132;&#20114;&#65292;&#20197;&#32452;&#21512;3D&#20215;&#20540;&#26144;&#23556;&#23558;&#30693;&#35782;&#25509;&#22320;&#21040;Agent&#30340;&#35266;&#27979;&#31354;&#38388;&#20013;&#12290;&#28982;&#21518;&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#35268;&#21010;&#26694;&#26550;&#20013;&#20351;&#29992;&#32452;&#21512;&#30340;&#20215;&#20540;&#26144;&#23556;&#26469;&#38646;&#35797;&#21512;&#25104;&#38381;&#29615;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are shown to possess a wealth of actionable knowledge that can be extracted for robot manipulation in the form of reasoning and planning. Despite the progress, most still rely on pre-defined motion primitives to carry out the physical interactions with the environment, which remains a major bottleneck. In this work, we aim to synthesize robot trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of manipulation tasks given an open-set of instructions and an open-set of objects. We achieve this by first observing that LLMs excel at inferring affordances and constraints given a free-form language instruction. More importantly, by leveraging their code-writing capabilities, they can interact with a visual-language model (VLM) to compose 3D value maps to ground the knowledge into the observation space of the agent. The composed value maps are then used in a model-based planning framework to zero-shot synthesize closed-loop ro
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26080;&#26631;&#31614;&#30340;&#20154;&#31867;&#35270;&#39057;&#28436;&#31034;&#22686;&#24378;&#20102;&#30524;&#25163;&#21327;&#21516;&#30340;&#35270;&#35273;&#36816;&#21160;&#31574;&#30053;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#26114;&#36149;&#30340;&#19987;&#23478;&#28436;&#31034;&#25968;&#25454;&#25110;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.05959</link><description>&lt;p&gt;
&#32473;&#26426;&#22120;&#20154;&#20197;&#24110;&#21161;&#65306;&#36890;&#36807;&#30524;&#25163;&#21327;&#21516;&#30340;&#20154;&#31867;&#35270;&#39057;&#28436;&#31034;&#23398;&#20064;&#36890;&#29992;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Giving Robots a Hand: Learning Generalizable Manipulation with Eye-in-Hand Human Video Demonstrations. (arXiv:2307.05959v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05959
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26080;&#26631;&#31614;&#30340;&#20154;&#31867;&#35270;&#39057;&#28436;&#31034;&#22686;&#24378;&#20102;&#30524;&#25163;&#21327;&#21516;&#30340;&#35270;&#35273;&#36816;&#21160;&#31574;&#30053;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#26114;&#36149;&#30340;&#19987;&#23478;&#28436;&#31034;&#25968;&#25454;&#25110;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30524;&#25163;&#21327;&#21516;&#25668;&#20687;&#22836;&#22312;&#35270;&#35273;&#23548;&#21521;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#26174;&#31034;&#20986;&#20102;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#26426;&#22120;&#20154;&#27169;&#20223;&#20013;&#65292;&#35753;&#20154;&#31867;&#36828;&#31243;&#25805;&#20316;&#21592;&#25910;&#38598;&#22823;&#37327;&#19987;&#23478;&#28436;&#31034;&#23545;&#20110;&#30495;&#23454;&#26426;&#22120;&#20154;&#26469;&#35828;&#20173;&#28982;&#24456;&#26114;&#36149;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20154;&#31867;&#36827;&#34892;&#20219;&#21153;&#30340;&#35270;&#39057;&#35201;&#20415;&#23452;&#24471;&#22810;&#65292;&#22240;&#20026;&#23427;&#20204;&#28040;&#38500;&#20102;&#23545;&#26426;&#22120;&#20154;&#36965;&#25805;&#20316;&#19987;&#19994;&#30693;&#35782;&#30340;&#38656;&#27714;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#24555;&#36895;&#25429;&#25417;&#12290;&#22240;&#27492;&#65292;&#20154;&#31867;&#35270;&#39057;&#28436;&#31034;&#26159;&#23398;&#20064;&#22823;&#35268;&#27169;&#36890;&#29992;&#26426;&#22120;&#20154;&#25805;&#20316;&#31574;&#30053;&#30340;&#26377;&#24076;&#26395;&#30340;&#25968;&#25454;&#26469;&#28304;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24191;&#27867;&#30340;&#26080;&#26631;&#31614;&#20154;&#31867;&#35270;&#39057;&#28436;&#31034;&#26469;&#22686;&#24378;&#29421;&#31364;&#30340;&#26426;&#22120;&#20154;&#27169;&#20223;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#20102;&#30524;&#25163;&#21327;&#21516;&#35270;&#35273;&#36816;&#21160;&#31574;&#30053;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23613;&#31649;&#20154;&#31867;&#21644;&#26426;&#22120;&#20154;&#25968;&#25454;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#30340;&#35270;&#35273;&#39046;&#22495;&#24046;&#36317;&#65292;&#20294;&#25105;&#20204;&#30340;&#26694;&#26550;&#19981;&#38656;&#35201;&#20351;&#29992;&#20219;&#20309;&#26126;&#30830;&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#22240;&#20026;&#25105;&#20204;&#21033;&#29992;&#20102;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Eye-in-hand cameras have shown promise in enabling greater sample efficiency and generalization in vision-based robotic manipulation. However, for robotic imitation, it is still expensive to have a human teleoperator collect large amounts of expert demonstrations with a real robot. Videos of humans performing tasks, on the other hand, are much cheaper to collect since they eliminate the need for expertise in robotic teleoperation and can be quickly captured in a wide range of scenarios. Therefore, human video demonstrations are a promising data source for learning generalizable robotic manipulation policies at scale. In this work, we augment narrow robotic imitation datasets with broad unlabeled human video demonstrations to greatly enhance the generalization of eye-in-hand visuomotor policies. Although a clear visual domain gap exists between human and robot data, our framework does not need to employ any explicit domain adaptation method, as we leverage the partial observability of e
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#35268;&#33539;&#24615;&#19994;&#21153;&#27969;&#31243;&#30417;&#25511;&#20013;&#22914;&#20309;&#33258;&#21160;&#21327;&#35843;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#25552;&#21069;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#20043;&#21069;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#36739;&#20302;&#65292;&#20294;&#36739;&#26089;&#30340;&#35843;&#25972;&#23545;&#20110;&#39044;&#38450;&#19981;&#33391;&#27969;&#31243;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2307.05939</link><description>&lt;p&gt;
&#33258;&#21160;&#21327;&#35843;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#25552;&#21069;&#24615;&#30340;&#35268;&#33539;&#19994;&#21153;&#27969;&#31243;&#30417;&#25511;
&lt;/p&gt;
&lt;p&gt;
Automatically Reconciling the Trade-off between Prediction Accuracy and Earliness in Prescriptive Business Process Monitoring. (arXiv:2307.05939v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05939
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#35268;&#33539;&#24615;&#19994;&#21153;&#27969;&#31243;&#30417;&#25511;&#20013;&#22914;&#20309;&#33258;&#21160;&#21327;&#35843;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#25552;&#21069;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#20043;&#21069;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#36739;&#20302;&#65292;&#20294;&#36739;&#26089;&#30340;&#35843;&#25972;&#23545;&#20110;&#39044;&#38450;&#19981;&#33391;&#27969;&#31243;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35268;&#33539;&#24615;&#19994;&#21153;&#27969;&#31243;&#30417;&#25511;&#20026;&#27969;&#31243;&#32463;&#29702;&#25552;&#20379;&#20915;&#31574;&#25903;&#25345;&#65292;&#30830;&#23450;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#35843;&#25972;&#27491;&#22312;&#36827;&#34892;&#30340;&#19994;&#21153;&#27969;&#31243;&#20197;&#38450;&#27490;&#25110;&#20943;&#36731;&#19981;&#33391;&#27969;&#31243;&#32467;&#26524;&#12290;&#25105;&#20204;&#20851;&#27880;&#20110;&#33258;&#21160;&#21327;&#35843;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#25552;&#21069;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#65292;&#20197;&#30830;&#23450;&#20309;&#26102;&#36827;&#34892;&#35843;&#25972;&#12290;&#35843;&#25972;&#24212;&#23613;&#26089;&#21457;&#29983;&#65292;&#20197;&#25552;&#20379;&#36275;&#22815;&#30340;&#26102;&#38388;&#26469;&#26377;&#25928;&#36827;&#34892;&#35843;&#25972;&#12290;&#28982;&#32780;&#65292;&#36739;&#26089;&#30340;&#39044;&#27979;&#36890;&#24120;&#27604;&#36739;&#26202;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#36739;&#20302;&#12290;&#36825;&#24847;&#21619;&#30528;&#26681;&#25454;&#36739;&#19981;&#20934;&#30830;&#30340;&#39044;&#27979;&#37319;&#21462;&#34892;&#21160;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#24517;&#35201;&#30340;&#35843;&#25972;&#25110;&#38169;&#36807;&#35843;&#25972;&#12290;&#24050;&#22312;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#21327;&#35843;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#25552;&#21069;&#24615;&#30340;&#26435;&#34913;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36825;&#20123;&#26041;&#27861;&#19982;&#19981;&#21516;&#30340;&#22522;&#32447;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#29978;&#33267;&#26426;&#23494;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#36825;&#38480;&#21046;&#20102;&#26041;&#27861;&#30340;&#21487;&#27604;&#24615;&#21644;&#21487;&#22797;&#21046;&#24615;&#65292;&#20351;&#20854;&#21464;&#24471;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prescriptive business process monitoring provides decision support to process managers on when and how to adapt an ongoing business process to prevent or mitigate an undesired process outcome. We focus on the problem of automatically reconciling the trade-off between prediction accuracy and prediction earliness in determining when to adapt. Adaptations should happen sufficiently early to provide enough lead time for the adaptation to become effective. However, earlier predictions are typically less accurate than later predictions. This means that acting on less accurate predictions may lead to unnecessary adaptations or missed adaptations.  Different approaches were presented in the literature to reconcile the trade-off between prediction accuracy and earliness. So far, these approaches were compared with different baselines, and evaluated using different data sets or even confidential data sets. This limits the comparability and replicability of the approaches and makes it difficult t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30456;&#23545;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#20154;&#20307;&#28436;&#31034;&#20013;&#23398;&#20064;&#21452;&#33218;&#21327;&#35843;&#65292;&#23558;&#21452;&#33218;&#20219;&#21153;&#20998;&#20026;&#39046;&#23548;&#32773;-&#36861;&#38543;&#32773;&#21327;&#35843;&#21644;&#21327;&#21516;&#21327;&#35843;&#20004;&#31181;&#31867;&#22411;&#65292;&#24182;&#23558;&#21327;&#35843;&#34920;&#31034;&#20026;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#20197;&#25551;&#36848;&#21327;&#35843;&#30340;&#37325;&#35201;&#24615;&#21464;&#21270;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#26032;&#20219;&#21153;&#21442;&#25968;&#30340;&#24191;&#20041;&#21327;&#35843;&#12290;</title><link>http://arxiv.org/abs/2307.05933</link><description>&lt;p&gt;
BiRP&#65306;&#20351;&#29992;&#30456;&#23545;&#21442;&#25968;&#21270;&#26041;&#27861;&#23398;&#20064;&#26426;&#22120;&#20154;&#24191;&#20041;&#21452;&#33218;&#21327;&#35843;-&#22522;&#20110;&#20154;&#20307;&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
BiRP: Learning Robot Generalized Bimanual Coordination using Relative Parameterization Method on Human Demonstration. (arXiv:2307.05933v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30456;&#23545;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#20154;&#20307;&#28436;&#31034;&#20013;&#23398;&#20064;&#21452;&#33218;&#21327;&#35843;&#65292;&#23558;&#21452;&#33218;&#20219;&#21153;&#20998;&#20026;&#39046;&#23548;&#32773;-&#36861;&#38543;&#32773;&#21327;&#35843;&#21644;&#21327;&#21516;&#21327;&#35843;&#20004;&#31181;&#31867;&#22411;&#65292;&#24182;&#23558;&#21327;&#35843;&#34920;&#31034;&#20026;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#20197;&#25551;&#36848;&#21327;&#35843;&#30340;&#37325;&#35201;&#24615;&#21464;&#21270;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#26032;&#20219;&#21153;&#21442;&#25968;&#30340;&#24191;&#20041;&#21327;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#30340;&#21452;&#33218;&#25805;&#32437;&#21487;&#20197;&#25191;&#34892;&#27604;&#31616;&#21333;&#32452;&#21512;&#20004;&#21482;&#21333;&#33218;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#36825;&#35201;&#24402;&#21151;&#20110;&#21452;&#33218;&#20043;&#38388;&#30340;&#26102;&#31354;&#21327;&#35843;&#12290;&#28982;&#32780;&#65292;&#21452;&#33218;&#21327;&#35843;&#30340;&#25551;&#36848;&#20173;&#28982;&#26159;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#36825;&#20351;&#24471;&#24456;&#38590;&#25552;&#20379;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#21327;&#35843;&#33539;&#20363;&#65292;&#26356;&#19981;&#29992;&#35828;&#24212;&#29992;&#21040;&#26426;&#22120;&#20154;&#20013;&#20102;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#20154;&#31867;&#26085;&#24120;&#27963;&#21160;&#20013;&#30340;&#20027;&#35201;&#21452;&#33218;&#20219;&#21153;&#20998;&#20026;&#20004;&#31181;&#31867;&#22411;&#65306;&#39046;&#23548;&#32773;-&#36861;&#38543;&#32773;&#21327;&#35843;&#21644;&#21327;&#21516;&#21327;&#35843;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30456;&#23545;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#21452;&#33218;&#28436;&#31034;&#20013;&#23558;&#21327;&#35843;&#34920;&#31034;&#20026;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#20197;&#27010;&#29575;&#25551;&#36848;&#21327;&#35843;&#22312;&#36816;&#21160;&#36807;&#31243;&#20013;&#30340;&#37325;&#35201;&#24615;&#21464;&#21270;&#12290;&#23398;&#20064;&#21040;&#30340;&#21327;&#35843;&#34920;&#31034;&#21487;&#20197;&#25512;&#24191;&#21040;&#26032;&#30340;&#20219;&#21153;&#21442;&#25968;&#65292;&#21516;&#26102;&#30830;&#20445;&#26102;&#31354;&#21327;&#35843;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#21160;&#20316;&#21644;&#20154;&#20307;&#28436;&#31034;&#25968;&#25454;&#28436;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human bimanual manipulation can perform more complex tasks than a simple combination of two single arms, which is credited to the spatio-temporal coordination between the arms. However, the description of bimanual coordination is still an open topic in robotics. This makes it difficult to give an explainable coordination paradigm, let alone applied to robotics. In this work, we divide the main bimanual tasks in human daily activities into two types: leader-follower and synergistic coordination. Then we propose a relative parameterization method to learn these types of coordination from human demonstration. It represents coordination as Gaussian mixture models from bimanual demonstration to describe the change in the importance of coordination throughout the motions by probability. The learned coordinated representation can be generalized to new task parameters while ensuring spatio-temporal coordination. We demonstrate the method using synthetic motions and human demonstration data and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#34892;&#20102;&#29992;&#20110;&#34460;&#34411;&#31751;&#26816;&#27979;&#30340;&#27604;&#36739;&#30740;&#31350;&#12290;&#36890;&#36807;&#26816;&#27979;&#34460;&#34411;&#31751;&#65292;&#25105;&#20204;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;&#24863;&#26579;&#27700;&#24179;&#65292;&#20174;&#32780;&#23454;&#29616;&#31934;&#30830;&#30340;&#23616;&#37096;&#26432;&#34411;&#21058;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.05929</link><description>&lt;p&gt;
&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#29992;&#20110;&#34460;&#34411;&#31751;&#26816;&#27979;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A New Dataset and Comparative Study for Aphid Cluster Detection. (arXiv:2307.05929v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#34892;&#20102;&#29992;&#20110;&#34460;&#34411;&#31751;&#26816;&#27979;&#30340;&#27604;&#36739;&#30740;&#31350;&#12290;&#36890;&#36807;&#26816;&#27979;&#34460;&#34411;&#31751;&#65292;&#25105;&#20204;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;&#24863;&#26579;&#27700;&#24179;&#65292;&#20174;&#32780;&#23454;&#29616;&#31934;&#30830;&#30340;&#23616;&#37096;&#26432;&#34411;&#21058;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34460;&#34411;&#26159;&#23545;&#20892;&#20316;&#29289;&#12289;&#20892;&#26449;&#23478;&#24237;&#21644;&#20840;&#29699;&#31918;&#39135;&#23433;&#20840;&#26500;&#25104;&#30340;&#20027;&#35201;&#23041;&#32961;&#20043;&#19968;&#12290;&#21270;&#23398;&#38450;&#27835;&#26159;&#20892;&#20316;&#29289;&#29983;&#20135;&#20013;&#26368;&#22823;&#21270;&#20135;&#37327;&#30340;&#24517;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#28982;&#32780;&#32771;&#34385;&#21040;&#29615;&#22659;&#27745;&#26579;&#21644;&#25104;&#26412;&#65292;&#23558;&#21270;&#23398;&#26041;&#27861;&#24212;&#29992;&#20110;&#25972;&#20010;&#30000;&#22320;&#26159;&#19981;&#24517;&#35201;&#30340;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#22320;&#23450;&#20301;&#34460;&#34411;&#24182;&#20272;&#35745;&#24863;&#26579;&#27700;&#24179;&#23545;&#20110;&#31934;&#30830;&#22320;&#23616;&#37096;&#24212;&#29992;&#26432;&#34411;&#21058;&#33267;&#20851;&#37325;&#35201;&#12290;&#34460;&#34411;&#26816;&#27979;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#27599;&#21482;&#20010;&#20307;&#34460;&#34411;&#38750;&#24120;&#23567;&#65292;&#32780;&#19988;&#25152;&#26377;&#34460;&#34411;&#37117;&#32858;&#38598;&#22312;&#19968;&#36215;&#24418;&#25104;&#31751;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#26816;&#27979;&#34460;&#34411;&#31751;&#26469;&#20272;&#35745;&#24863;&#26579;&#27700;&#24179;&#12290;&#25105;&#20204;&#22312;&#39640;&#31921;&#30000;&#37319;&#38598;&#20102;&#25968;&#30334;&#19975;&#24352;&#22270;&#20687;&#65292;&#25163;&#21160;&#36873;&#25321;&#20102;5,447&#24352;&#21253;&#21547;&#34460;&#34411;&#30340;&#22270;&#20687;&#65292;&#24182;&#23545;&#27599;&#20010;&#34460;&#34411;&#31751;&#36827;&#34892;&#20102;&#27880;&#37322;&#12290;&#20026;&#20102;&#23558;&#36825;&#20123;&#22270;&#20687;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;&#22270;&#20687;&#35009;&#21098;&#20026;&#23567;&#22359;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#24102;&#26377;&#36229;&#36807;151,000&#20010;&#22270;&#20687;&#22359;&#30340;&#26631;&#31614;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23454;&#26045;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#34460;&#34411;&#31751;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aphids are one of the main threats to crops, rural families, and global food security. Chemical pest control is a necessary component of crop production for maximizing yields, however, it is unnecessary to apply the chemical approaches to the entire fields in consideration of the environmental pollution and the cost. Thus, accurately localizing the aphid and estimating the infestation level is crucial to the precise local application of pesticides. Aphid detection is very challenging as each individual aphid is really small and all aphids are crowded together as clusters. In this paper, we propose to estimate the infection level by detecting aphid clusters. We have taken millions of images in the sorghum fields, manually selected 5,447 images that contain aphids, and annotated each aphid cluster in the image. To use these images for machine learning models, we crop the images into patches and created a labeled dataset with over 151,000 image patches. Then, we implement and compare the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#20197;&#30142;&#30149;&#20026;&#23548;&#21521;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#33258;&#21160;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#20013;&#30340;&#32454;&#24494;&#24046;&#24322;&#20851;&#27880;&#12289;&#25968;&#25454;&#20559;&#24046;&#21644;&#38271;&#25991;&#26412;&#29983;&#25104;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.05921</link><description>&lt;p&gt;
&#38405;&#35835;&#25918;&#23556;&#23398;&#25104;&#20687;&#30340;&#26041;&#24335;&#65292;&#23601;&#20687;&#25918;&#23556;&#31185;&#21307;&#29983;&#19968;&#26679;
&lt;/p&gt;
&lt;p&gt;
Reading Radiology Imaging Like The Radiologist. (arXiv:2307.05921v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05921
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#20197;&#30142;&#30149;&#20026;&#23548;&#21521;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#33258;&#21160;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#20013;&#30340;&#32454;&#24494;&#24046;&#24322;&#20851;&#27880;&#12289;&#25968;&#25454;&#20559;&#24046;&#21644;&#38271;&#25991;&#26412;&#29983;&#25104;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#26088;&#22312;&#29983;&#25104;&#21253;&#21547;&#25918;&#23556;&#23398;&#25104;&#20687;&#30340;&#20016;&#23500;&#12289;&#31934;&#32454;&#25551;&#36848;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;&#19982;&#33258;&#28982;&#22270;&#20687;&#39046;&#22495;&#30340;&#22270;&#20687;&#25551;&#36848;&#30456;&#27604;&#65292;&#21307;&#23398;&#22270;&#20687;&#38750;&#24120;&#30456;&#20284;&#65292;&#20165;&#22312;&#30142;&#30149;&#21457;&#29983;&#30340;&#32454;&#24494;&#24046;&#24322;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;&#37492;&#20110;&#36825;&#20123;&#32454;&#24494;&#24046;&#24322;&#22312;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#40723;&#21169;&#27169;&#22411;&#26356;&#21152;&#20851;&#27880;&#30142;&#30149;&#21457;&#29983;&#30340;&#24494;&#22937;&#21306;&#22495;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#27425;&#65292;&#35270;&#35273;&#21644;&#25991;&#26412;&#25968;&#25454;&#20559;&#24046;&#30340;&#38382;&#39064;&#24456;&#20005;&#37325;&#12290;&#19981;&#20165;&#27491;&#24120;&#30149;&#20363;&#21344;&#25968;&#25454;&#38598;&#30340;&#22823;&#37096;&#20998;&#65292;&#36824;&#25551;&#32472;&#26377;&#30149;&#21464;&#21306;&#22495;&#30340;&#21477;&#23376;&#21482;&#21344;&#27573;&#33853;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#26368;&#21518;&#65292;&#29983;&#25104;&#21307;&#23398;&#22270;&#20687;&#25253;&#21578;&#28041;&#21450;&#21040;&#38271;&#25991;&#26412;&#30340;&#29983;&#25104;&#25361;&#25112;&#65292;&#36825;&#38656;&#35201;&#26356;&#22810;&#21307;&#23398;&#30693;&#35782;&#30340;&#19987;&#19994;&#24615;&#21644;&#32463;&#39564;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#29983;&#25104;&#27492;&#31867;&#25253;&#21578;&#30340;&#38590;&#24230;&#22686;&#21152;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#30142;&#30149;&#20026;&#23548;&#21521;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated radiology report generation aims to generate radiology reports that contain rich, fine-grained descriptions of radiology imaging. Compared with image captioning in the natural image domain, medical images are very similar to each other, with only minor differences in the occurrence of diseases. Given the importance of these minor differences in the radiology report, it is crucial to encourage the model to focus more on the subtle regions of disease occurrence. Secondly, the problem of visual and textual data biases is serious. Not only do normal cases make up the majority of the dataset, but sentences describing areas with pathological changes also constitute only a small part of the paragraph. Lastly, generating medical image reports involves the challenge of long text generation, which requires more expertise and empirical training in medical knowledge. As a result, the difficulty of generating such reports is increased. To address these challenges, we propose a disease-ori
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20809;&#27969;&#25554;&#20540;&#30340;&#26041;&#27861;&#23454;&#29616;&#36817;&#36317;&#31163;&#35270;&#35282;&#21512;&#25104;&#65292;&#24182;&#36890;&#36807;&#24039;&#22937;&#24212;&#29992;&#20809;&#27969;&#20540;&#65292;&#20811;&#26381;&#20102;&#35895;&#27468;&#34903;&#26223;&#31995;&#32479;&#20013;&#30340;&#35270;&#35273;&#22833;&#30495;&#21644;&#22270;&#20687;&#27169;&#31946;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.05913</link><description>&lt;p&gt;
&#36890;&#36807;&#25554;&#20540;&#20809;&#27969;&#23454;&#29616;&#36817;&#36317;&#31163;&#35270;&#35282;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Close-up View synthesis by Interpolating Optical Flow. (arXiv:2307.05913v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20809;&#27969;&#25554;&#20540;&#30340;&#26041;&#27861;&#23454;&#29616;&#36817;&#36317;&#31163;&#35270;&#35282;&#21512;&#25104;&#65292;&#24182;&#36890;&#36807;&#24039;&#22937;&#24212;&#29992;&#20809;&#27969;&#20540;&#65292;&#20811;&#26381;&#20102;&#35895;&#27468;&#34903;&#26223;&#31995;&#32479;&#20013;&#30340;&#35270;&#35273;&#22833;&#30495;&#21644;&#22270;&#20687;&#27169;&#31946;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#25311;&#35270;&#28857;&#34987;&#35270;&#20026;&#34394;&#25311;&#23548;&#33322;&#20013;&#30340;&#26032;&#25216;&#26415;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#28145;&#24230;&#20449;&#24687;&#21644;&#27169;&#31946;&#30340;&#30456;&#26426;&#21442;&#25968;&#32780;&#26410;&#24471;&#21040;&#25903;&#25345;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20809;&#27969;&#24314;&#31435;&#35270;&#24046;&#25928;&#26524;&#65292;&#23454;&#29616;&#20266;3D&#25237;&#24433;&#30340;&#36817;&#36317;&#31163;&#34394;&#25311;&#35270;&#35282;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#20351;&#29992;&#28145;&#24230;&#20256;&#24863;&#22120;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21452;&#21521;&#20809;&#27969;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#20363;&#25554;&#20540;&#20809;&#27969;&#26469;&#33719;&#24471;&#20219;&#24847;&#34394;&#25311;&#35270;&#28857;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#24039;&#22937;&#24212;&#29992;&#20809;&#27969;&#20540;&#65292;&#25105;&#20204;&#36890;&#36807;&#38236;&#22836;&#20280;&#32553;&#23454;&#29616;&#22312;&#20219;&#24847;&#35282;&#33853;&#30340;&#28165;&#26224;&#21644;&#35270;&#35273;&#20445;&#30495;&#30340;&#25918;&#22823;&#25928;&#26524;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#35895;&#27468;&#34903;&#26223;&#31995;&#32479;&#20013;&#36890;&#36807;&#35270;&#35282;&#25918;&#22823;&#21644;&#36807;&#28193;&#24102;&#26469;&#30340;&#35270;&#35273;&#22833;&#30495;&#21644;&#22270;&#20687;&#27169;&#31946;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The virtual viewpoint is perceived as a new technique in virtual navigation, as yet not supported due to the lack of depth information and obscure camera parameters. In this paper, a method for achieving close-up virtual view is proposed and it only uses optical flow to build parallax effects to realize pseudo 3D projection without using depth sensor. We develop a bidirectional optical flow method to obtain any virtual viewpoint by proportional interpolation of optical flow. Moreover, with the ingenious application of the optical-flow-value, we achieve clear and visual-fidelity magnified results through lens stretching in any corner, which overcomes the visual distortion and image blur through viewpoint magnification and transition in Google Street View system.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20056;&#27861;&#24179;&#28369;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#36890;&#36807;&#35777;&#26126;&#27169;&#22411;&#30340;Lipschitz&#24615;&#36136;&#65292;&#20445;&#35777;&#20102;&#20854;&#31283;&#23450;&#24615;&#65292;&#24182;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#26174;&#31034;&#20102;&#38750;&#24179;&#20961;&#30340;&#31283;&#23450;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2307.05902</link><description>&lt;p&gt;
&#24102;&#26377;&#20056;&#27861;&#24179;&#28369;&#30340;&#29305;&#24449;&#24402;&#22240;&#31283;&#23450;&#24615;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Stability Guarantees for Feature Attributions with Multiplicative Smoothing. (arXiv:2307.05902v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20056;&#27861;&#24179;&#28369;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#36890;&#36807;&#35777;&#26126;&#27169;&#22411;&#30340;Lipschitz&#24615;&#36136;&#65292;&#20445;&#35777;&#20102;&#20854;&#31283;&#23450;&#24615;&#65292;&#24182;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#26174;&#31034;&#20102;&#38750;&#24179;&#20961;&#30340;&#31283;&#23450;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#37322;&#26041;&#27861;&#24448;&#24448;&#19981;&#33021;&#25552;&#20379;&#20219;&#20309;&#24418;&#24335;&#30340;&#20445;&#35777;&#65292;&#20063;&#21487;&#33021;&#19981;&#21453;&#26144;&#24213;&#23618;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#31283;&#23450;&#24615;&#20316;&#20026;&#21487;&#38752;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#30340;&#19968;&#20010;&#23646;&#24615;&#36827;&#34892;&#20998;&#26512;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22914;&#26524;&#27169;&#22411;&#22312;&#29305;&#24449;&#23631;&#34109;&#26041;&#38754;&#20855;&#26377;&#36275;&#22815;&#30340;Lipschitz&#24615;&#36136;&#65292;&#21017;&#21487;&#20197;&#20445;&#35777;&#25918;&#26494;&#21464;&#20307;&#30340;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#26679;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31216;&#20026;&#20056;&#27861;&#24179;&#28369;&#65288;MuS&#65289;&#30340;&#24179;&#28369;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;MuS&#20811;&#26381;&#20102;&#26631;&#20934;&#24179;&#28369;&#25216;&#26415;&#30340;&#29702;&#35770;&#38480;&#21046;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#20219;&#20309;&#20998;&#31867;&#22120;&#21644;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65288;&#22914;LIME&#21644;SHAP&#65289;&#23545;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;MuS&#30340;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;MuS&#36171;&#20104;&#20102;&#29305;&#24449;&#24402;&#22240;&#20197;&#38750;&#24179;&#20961;&#30340;&#31283;&#23450;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explanation methods for machine learning models tend to not provide any formal guarantees and may not reflect the underlying decision-making process. In this work, we analyze stability as a property for reliable feature attribution methods. We prove that relaxed variants of stability are guaranteed if the model is sufficiently Lipschitz with respect to the masking of features. To achieve such a model, we develop a smoothing method called Multiplicative Smoothing (MuS). We show that MuS overcomes theoretical limitations of standard smoothing techniques and can be integrated with any classifier and feature attribution method. We evaluate MuS on vision and language models with a variety of feature attribution methods, such as LIME and SHAP, and demonstrate that MuS endows feature attributions with non-trivial stability guarantees.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21463;&#21040;PID&#25511;&#21046;&#22120;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#32534;&#30721;&#21382;&#21490;&#35760;&#24405;&#30340;&#26550;&#26500;&#65292;&#20197;&#24179;&#34913;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#30340;&#28789;&#27963;&#24615;&#19982;&#23545;&#29615;&#22659;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05891</link><description>&lt;p&gt;
&#21463;PID&#25511;&#21046;&#22120;&#21551;&#21457;&#30340;&#20559;&#24046;&#24402;&#32435;&#27861;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PID-Inspired Inductive Biases for Deep Reinforcement Learning in Partially Observable Control Tasks. (arXiv:2307.05891v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05891
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21463;&#21040;PID&#25511;&#21046;&#22120;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#32534;&#30721;&#21382;&#21490;&#35760;&#24405;&#30340;&#26550;&#26500;&#65292;&#20197;&#24179;&#34913;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#30340;&#28789;&#27963;&#24615;&#19982;&#23545;&#29615;&#22659;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#32463;&#23637;&#29616;&#20986;&#36890;&#36807;&#25968;&#25454;&#33258;&#24049;&#23398;&#20064;&#25511;&#21046;&#31995;&#32479;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;RL&#38754;&#20020;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#31995;&#32479;&#30340;&#23436;&#25972;&#29366;&#24577;&#36890;&#24120;&#19981;&#21487;&#35266;&#27979;&#12290;&#24403;&#20986;&#29616;&#36825;&#31181;&#24773;&#20917;&#26102;&#65292;&#31574;&#30053;&#38656;&#35201;&#21033;&#29992;&#35266;&#23519;&#21382;&#21490;&#26469;&#25512;&#26029;&#24403;&#21069;&#29366;&#24577;&#12290;&#21516;&#26102;&#65292;&#35757;&#32451;&#21644;&#27979;&#35797;&#29615;&#22659;&#20043;&#38388;&#30340;&#24046;&#24322;&#20351;&#24471;&#31574;&#30053;&#19981;&#20250;&#36807;&#24230;&#25311;&#21512;&#35757;&#32451;&#26102;&#35266;&#23519;&#21040;&#30340;&#24207;&#21015;&#12290;&#22240;&#27492;&#65292;&#22312;&#21382;&#21490;&#35760;&#24405;&#32534;&#30721;&#22120;&#28789;&#27963;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#30340;&#21516;&#26102;&#65292;&#35201;&#23545;&#29615;&#22659;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#24179;&#34913;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#24179;&#34913;&#65292;&#25105;&#20204;&#23547;&#27714;PID&#25511;&#21046;&#22120;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#26029;&#23450;PID&#25511;&#21046;&#22120;&#30340;&#25104;&#21151;&#34920;&#26126;&#65292;&#35768;&#22810;&#25511;&#21046;&#20219;&#21153;&#21482;&#38656;&#35201;&#27714;&#21644;&#21644;&#27714;&#24046;&#26469;&#32047;&#31215;&#20449;&#24687;&#12290;&#22522;&#20110;&#36825;&#20010;&#21407;&#21017;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#32534;&#30721;&#21382;&#21490;&#35760;&#24405;&#30340;&#26550;&#26500;&#65306;&#19968;&#31181;&#30452;&#25509;&#20351;&#29992;...
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (RL) has shown immense potential for learning to control systems through data alone. However, one challenge deep RL faces is that the full state of the system is often not observable. When this is the case, the policy needs to leverage the history of observations to infer the current state. At the same time, differences between the training and testing environments makes it critical for the policy not to overfit to the sequence of observations it sees at training time. As such, there is an important balancing act between having the history encoder be flexible enough to extract relevant information, yet be robust to changes in the environment. To strike this balance, we look to the PID controller for inspiration. We assert the PID controller's success shows that only summing and differencing are needed to accumulate information over time for many control tasks. Following this principle, we propose two architectures for encoding history: one that directly uses
&lt;/p&gt;</description></item><item><title>&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#23384;&#22312;&#31995;&#32479;&#24615;&#25925;&#38556;, &#21363;&#20351;&#21333;&#20010;&#27169;&#22411;&#22312;&#24635;&#20307;&#19978;&#30340;&#25913;&#21892;&#20063;&#19981;&#33021;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;</title><link>http://arxiv.org/abs/2307.05862</link><description>&lt;p&gt;
&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#30340;&#29983;&#24577;&#31995;&#32479;&#32423;&#20998;&#26512;&#25581;&#31034;&#20102;&#21516;&#36136;&#21270;&#30340;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Ecosystem-level Analysis of Deployed Machine Learning Reveals Homogeneous Outcomes. (arXiv:2307.05862v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05862
&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#23384;&#22312;&#31995;&#32479;&#24615;&#25925;&#38556;, &#21363;&#20351;&#21333;&#20010;&#27169;&#22411;&#22312;&#24635;&#20307;&#19978;&#30340;&#25913;&#21892;&#20063;&#19981;&#33021;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#26426;&#22120;&#23398;&#20064;&#36890;&#24120;&#22312;&#27169;&#22411;&#23618;&#38754;&#36827;&#34892;&#30740;&#31350;&#65306;&#30740;&#31350;&#20154;&#21592;&#34913;&#37327;&#21644;&#25913;&#36827;&#29305;&#23450;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#20559;&#35265;&#12289;&#25928;&#29575;&#21644;&#20854;&#20182;&#32500;&#24230;&#12290;&#23454;&#38469;&#19978;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#31038;&#20250;&#24433;&#21709;&#21462;&#20915;&#20110;&#26426;&#22120;&#23398;&#20064;&#37096;&#32626;&#30340;&#21608;&#22260;&#29615;&#22659;&#12290;&#20026;&#20102;&#25429;&#25417;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29983;&#24577;&#31995;&#32479;&#32423;&#20998;&#26512;&#65306;&#19981;&#26159;&#20998;&#26512;&#21333;&#20010;&#27169;&#22411;&#65292;&#32780;&#26159;&#32771;&#34385;&#22312;&#32473;&#23450;&#29615;&#22659;&#20013;&#37096;&#32626;&#30340;&#25152;&#26377;&#27169;&#22411;&#30340;&#38598;&#21512;&#12290;&#20363;&#22914;&#65292;&#22312;&#25307;&#32856;&#20013;&#36827;&#34892;&#29983;&#24577;&#31995;&#32479;&#32423;&#20998;&#26512;&#24847;&#21619;&#30528;&#35748;&#35782;&#21040;&#19968;&#20010;&#27714;&#32844;&#32773;&#30340;&#32467;&#26524;&#19981;&#20165;&#20165;&#21462;&#20915;&#20110;&#21333;&#20010;&#25307;&#32856;&#31639;&#27861;&#25110;&#20844;&#21496;&#65292;&#32780;&#26159;&#21462;&#20915;&#20110;&#20182;&#20204;&#30003;&#35831;&#30340;&#25152;&#26377;&#20844;&#21496;&#30340;&#38598;&#20307;&#20915;&#31574;&#12290;&#22312;&#19977;&#31181;&#27169;&#24335;&#65288;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35821;&#38899;&#65289;&#21644;11&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26126;&#26174;&#30340;&#36235;&#21183;&#65306;&#37096;&#32626;&#30340;&#26426;&#22120;&#23398;&#20064;&#23481;&#26131;&#20986;&#29616;&#31995;&#32479;&#24615;&#25925;&#38556;&#65292;&#36825;&#24847;&#21619;&#30528;&#19968;&#20123;&#29992;&#25143;&#34987;&#25152;&#26377;&#21487;&#29992;&#30340;&#27169;&#22411;&#38169;&#35823;&#20998;&#31867;&#12290;&#21363;&#20351;&#22312;&#20010;&#20307;&#27169;&#22411;&#38543;&#26102;&#38388;&#22312;&#24635;&#20307;&#27700;&#24179;&#19978;&#25913;&#21892;&#65292;&#25105;&#20204;&#20063;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Machine learning is traditionally studied at the model level: researchers measure and improve the accuracy, robustness, bias, efficiency, and other dimensions of specific models. In practice, the societal impact of machine learning is determined by the surrounding context of machine learning deployments. To capture this, we introduce ecosystem-level analysis: rather than analyzing a single model, we consider the collection of models that are deployed in a given context. For example, ecosystem-level analysis in hiring recognizes that a job candidate's outcomes are not only determined by a single hiring algorithm or firm but instead by the collective decisions of all the firms they applied to. Across three modalities (text, images, speech) and 11 datasets, we establish a clear trend: deployed machine learning is prone to systemic failure, meaning some users are exclusively misclassified by all models available. Even when individual models improve at the population level over time, we fin
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FAIRO&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#20154;&#26426;&#20132;&#20114;&#29615;&#22659;&#20013;&#23454;&#29616;&#20844;&#24179;&#30340;&#39034;&#24207;&#20915;&#31574;&#12290;&#35813;&#31639;&#27861;&#23558;&#20154;&#30340;&#34892;&#20026;&#21487;&#21464;&#24615;&#21644;&#20559;&#22909;&#21464;&#21270;&#32771;&#34385;&#22312;&#20869;&#65292;&#36890;&#36807;&#21033;&#29992;&#36873;&#39033;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#23558;&#22797;&#26434;&#30340;&#20844;&#24179;&#20219;&#21153;&#20998;&#35299;&#20026;&#33258;&#36866;&#24212;&#23376;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.05857</link><description>&lt;p&gt;
FAIRO: &#38754;&#21521;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#20013;&#39034;&#24207;&#20915;&#31574;&#30340;&#20844;&#24179;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
FAIRO: Fairness-aware Adaptation in Sequential-Decision Making for Human-in-the-Loop Systems. (arXiv:2307.05857v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05857
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FAIRO&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#20154;&#26426;&#20132;&#20114;&#29615;&#22659;&#20013;&#23454;&#29616;&#20844;&#24179;&#30340;&#39034;&#24207;&#20915;&#31574;&#12290;&#35813;&#31639;&#27861;&#23558;&#20154;&#30340;&#34892;&#20026;&#21487;&#21464;&#24615;&#21644;&#20559;&#22909;&#21464;&#21270;&#32771;&#34385;&#22312;&#20869;&#65292;&#36890;&#36807;&#21033;&#29992;&#36873;&#39033;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#23558;&#22797;&#26434;&#30340;&#20844;&#24179;&#20219;&#21153;&#20998;&#35299;&#20026;&#33258;&#36866;&#24212;&#23376;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#26426;&#20132;&#20114;&#29615;&#22659;&#20013;&#23454;&#29616;&#39034;&#24207;&#20915;&#31574;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#24403;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#34892;&#20026;&#21644;&#26399;&#26395;&#30340;&#20154;&#21463;&#21040;&#31995;&#32479;&#20013;&#30456;&#21516;&#36866;&#24212;&#20915;&#31574;&#30340;&#24433;&#21709;&#26102;&#12290;&#20154;&#30340;&#21487;&#21464;&#24615;&#22240;&#32032;&#22686;&#21152;&#20102;&#22797;&#26434;&#24615;&#65292;&#22240;&#20026;&#22312;&#26576;&#19968;&#26102;&#38388;&#28857;&#34987;&#35748;&#20026;&#26159;&#20844;&#24179;&#30340;&#25919;&#31574;&#21487;&#33021;&#20250;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#30001;&#20110;&#20154;&#30340;&#20559;&#22909;&#21464;&#21270;&#32780;&#25104;&#20026;&#27495;&#35270;&#24615;&#25919;&#31574;&#12290;&#26412;&#25991;&#20174;&#20844;&#24179;&#24615;&#35270;&#35282;&#32771;&#34385;&#20154;&#30340;&#34892;&#20026;&#21487;&#21464;&#24615;&#21644;&#20154;&#30340;&#20559;&#22909;&#21464;&#21270;&#65292;&#35299;&#20915;&#20102;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;FAIRO&#65292;&#29992;&#20110;&#38754;&#21521;&#20154;&#26426;&#20132;&#20114;&#36866;&#24212;&#30340;&#20844;&#24179;&#39034;&#24207;&#20915;&#31574;&#65292;&#23427;&#23558;&#36825;&#20123;&#27010;&#24565;&#32435;&#20837;&#20915;&#31574;&#36807;&#31243;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FAIRO&#23558;&#36825;&#20010;&#22797;&#26434;&#30340;&#20844;&#24179;&#20219;&#21153;&#22522;&#20110;&#20010;&#20307;&#20154;&#30340;&#20559;&#22909;&#36890;&#36807;&#21033;&#29992;&#36873;&#39033;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20998;&#35299;&#20026;&#33258;&#36866;&#24212;&#23376;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Achieving fairness in sequential-decision making systems within Human-in-the-Loop (HITL) environments is a critical concern, especially when multiple humans with different behavior and expectations are affected by the same adaptation decisions in the system. This human variability factor adds more complexity since policies deemed fair at one point in time may become discriminatory over time due to variations in human preferences resulting from inter- and intra-human variability. This paper addresses the fairness problem from an equity lens, considering human behavior variability, and the changes in human preferences over time. We propose FAIRO, a novel algorithm for fairness-aware sequential-decision making in HITL adaptation, which incorporates these notions into the decision-making process. In particular, FAIRO decomposes this complex fairness task into adaptive sub-tasks based on individual human preferences through leveraging the Options reinforcement learning framework. We design 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ISMnet&#30340;&#39640;&#38454;&#22270;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#20998;&#23618;&#20108;&#37096;&#22270;&#21644;&#39640;&#38454;&#20998;&#23618;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#65292;&#26469;&#35782;&#21035;&#31616;&#21333;&#22797;&#21512;&#20307;&#20013;&#30340;&#37325;&#35201;&#31616;&#21333;&#24418;&#20307;&#12290;</title><link>http://arxiv.org/abs/2307.05841</link><description>&lt;p&gt;
&#36890;&#36807;&#31616;&#21333;&#21367;&#31215;&#32593;&#32476;&#25366;&#25496;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#31616;&#21333;&#24418;&#20307;
&lt;/p&gt;
&lt;p&gt;
Influential Simplices Mining via Simplicial Convolutional Network. (arXiv:2307.05841v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ISMnet&#30340;&#39640;&#38454;&#22270;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#20998;&#23618;&#20108;&#37096;&#22270;&#21644;&#39640;&#38454;&#20998;&#23618;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#65292;&#26469;&#35782;&#21035;&#31616;&#21333;&#22797;&#21512;&#20307;&#20013;&#30340;&#37325;&#35201;&#31616;&#21333;&#24418;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31616;&#21333;&#22797;&#21512;&#20307;&#36817;&#24180;&#26469;&#22312;&#39640;&#38454;&#32593;&#32476;&#20998;&#26512;&#20013;&#22791;&#21463;&#20851;&#27880;&#65292;&#20854;&#20013;&#23569;&#25968;&#31616;&#21333;&#24418;&#20307;&#30001;&#20110;&#32593;&#32476;&#24322;&#36136;&#24615;&#22312;&#32467;&#26500;&#21644;&#21151;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#35782;&#21035;&#26377;&#24433;&#21709;&#21147;&#33410;&#28857;&#21644;&#31616;&#21333;&#24418;&#20307;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#19981;&#19968;&#33268;&#12290;&#22240;&#27492;&#65292;&#23613;&#31649;&#26377;&#20851;&#26377;&#24433;&#21709;&#21147;&#33410;&#28857;&#65288;0-&#31616;&#21333;&#24418;&#20307;&#65289;&#30340;&#30740;&#31350;&#30456;&#23545;&#25104;&#29087;&#65292;&#20294;&#22914;&#20309;&#34920;&#24449;&#31616;&#21333;&#24418;&#20307;&#30340;&#24433;&#21709;&#21147;&#24182;&#35782;&#21035;&#26377;&#24433;&#21709;&#21147;&#30340;&#31616;&#21333;&#24418;&#20307;&#20173;&#28982;&#38590;&#20197;&#35299;&#20915;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#21516;&#26102;&#21033;&#29992;&#32593;&#32476;&#25299;&#25169;&#21644;&#33410;&#28857;&#29305;&#24449;&#65292;&#20294;&#23427;&#20204;&#22312;&#22788;&#29702;&#39640;&#38454;&#20219;&#21153;&#26102;&#38754;&#20020;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26377;&#24433;&#21709;&#21147;&#31616;&#21333;&#24418;&#20307;&#25366;&#25496;&#31070;&#32463;&#32593;&#32476;&#65288;ISMnet&#65289;&#30340;&#39640;&#38454;&#22270;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#35782;&#21035;&#31616;&#21333;&#22797;&#21512;&#20307;&#20013;&#37325;&#35201;&#30340;h-&#31616;&#21333;&#24418;&#20307;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#26032;&#39062;&#30340;&#39640;&#38454;&#34920;&#36798;&#65306;&#20998;&#23618;&#20108;&#37096;&#22270;&#21644;&#39640;&#38454;&#20998;&#23618;&#65288;HoH&#65289;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#26469;&#22788;&#29702;&#39640;&#38454;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simplicial complexes have recently been in the limelight of higher-order network analysis, where a minority of simplices play crucial roles in structures and functions due to network heterogeneity. We find a significant inconsistency between identifying influential nodes and simplices. Therefore, it remains elusive how to characterize simplices' influence and identify influential simplices, despite the relative maturity of research on influential nodes (0-simplices) identification. Meanwhile, graph neural networks (GNNs) are potent tools that can exploit network topology and node features simultaneously, but they struggle to tackle higher-order tasks. In this paper, we propose a higher-order graph learning model, named influential simplices mining neural network (ISMnet), to identify vital h-simplices in simplicial complexes. It can tackle higher-order tasks by leveraging novel higher-order presentations: hierarchical bipartite graphs and higher-order hierarchical (HoH) Laplacians, whe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#32463;&#39564;&#20849;&#20139;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;DistMT-LSVI&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;DistMT-LSVI&#30340;&#21333;&#20010;&#20195;&#29702;&#21487;&#20197;&#23454;&#29616;&#25152;&#26377;&#20219;&#21153;&#30340;&#949;-&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.05834</link><description>&lt;p&gt;
&#29992;&#32463;&#39564;&#20849;&#20139;&#26469;&#25193;&#23637;&#20998;&#24067;&#24335;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Scaling Distributed Multi-task Reinforcement Learning with Experience Sharing. (arXiv:2307.05834v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05834
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#32463;&#39564;&#20849;&#20139;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;DistMT-LSVI&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;DistMT-LSVI&#30340;&#21333;&#20010;&#20195;&#29702;&#21487;&#20197;&#23454;&#29616;&#25152;&#26377;&#20219;&#21153;&#30340;&#949;-&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;DARPA&#25512;&#20986;&#20102;ShELL&#35745;&#21010;&#65292;&#26088;&#22312;&#25506;&#32034;&#32463;&#39564;&#20849;&#20139;&#22914;&#20309;&#20351;&#20998;&#24067;&#24335;&#32456;&#36523;&#23398;&#20064;&#20195;&#29702;&#22312;&#36866;&#24212;&#26032;&#25361;&#25112;&#26041;&#38754;&#21463;&#30410;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#20998;&#24067;&#24335;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36827;&#34892;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#65292;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#32452;N&#20010;&#20195;&#29702;&#21327;&#20316;&#35299;&#20915;&#20102;M&#20010;&#20219;&#21153;&#65292;&#32780;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#34920;&#36848;&#20026;&#32447;&#24615;&#21442;&#25968;&#21270;&#30340;&#19978;&#19979;&#25991;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#20854;&#20013;&#27599;&#20010;&#20219;&#21153;&#30001;&#25351;&#23450;&#36716;&#31227;&#21160;&#21147;&#23398;&#21644;&#22870;&#21169;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DistMT-LSVI&#30340;&#31639;&#27861;&#12290;&#39318;&#20808;&#65292;&#20195;&#29702;&#35782;&#21035;&#20219;&#21153;&#65292;&#28982;&#21518;&#36890;&#36807;&#20013;&#22830;&#26381;&#21153;&#22120;&#20132;&#25442;&#20449;&#24687;&#65292;&#20026;&#20219;&#21153;&#23548;&#20986;&#949;-&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#26126;&#65292;&#20026;&#20102;&#23454;&#29616;&#25152;&#26377;M&#20010;&#20219;&#21153;&#30340;&#949;-&#26368;&#20248;&#31574;&#30053;&#65292;&#20351;&#29992;DistMT-LSVI&#30340;&#21333;&#20010;&#20195;&#29702;&#38656;&#35201;&#36816;&#34892;&#19968;&#23450;&#25968;&#37327;&#30340;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, DARPA launched the ShELL program, which aims to explore how experience sharing can benefit distributed lifelong learning agents in adapting to new challenges. In this paper, we address this issue by conducting both theoretical and empirical research on distributed multi-task reinforcement learning (RL), where a group of $N$ agents collaboratively solves $M$ tasks without prior knowledge of their identities. We approach the problem by formulating it as linearly parameterized contextual Markov decision processes (MDPs), where each task is represented by a context that specifies the transition dynamics and rewards. To tackle this problem, we propose an algorithm called DistMT-LSVI. First, the agents identify the tasks, and then they exchange information through a central server to derive $\epsilon$-optimal policies for the tasks. Our research demonstrates that to achieve $\epsilon$-optimal policies for all $M$ tasks, a single agent using DistMT-LSVI needs to run a total number o
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22806;&#35266;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35270;&#22270;&#34955;&#65288;Bag-of-Views&#65289;&#27169;&#22411;&#26469;&#23545;&#25429;&#33719;&#30340;&#35270;&#22270;&#36827;&#34892;&#31163;&#32447;&#25968;&#25454;&#38598;&#32454;&#21270;&#21644;&#22312;&#32447;&#19979;&#19968;&#20010;&#26368;&#20339;&#35270;&#22270;&#65288;NBV&#65289;&#35268;&#21010;&#24212;&#29992;&#20998;&#37197;&#25928;&#29992;&#65292;&#20197;&#23454;&#29616;3D&#37325;&#24314;&#30340;&#20219;&#21153;&#12290;&#21516;&#26102;&#65292;&#36824;&#24320;&#21457;&#20102;&#35270;&#22270;&#35268;&#21010;&#24037;&#20855;&#31665;&#65288;VPT&#65289;&#12290;</title><link>http://arxiv.org/abs/2307.05832</link><description>&lt;p&gt;
&#35270;&#22270;&#34955;&#65306;&#19968;&#31181;&#22522;&#20110;&#22806;&#35266;&#30340;&#29992;&#20110;3D&#37325;&#24314;&#19979;&#19968;&#20010;&#26368;&#20339;&#35270;&#22270;&#35268;&#21010;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bag of Views: An Appearance-based Approach to Next-Best-View Planning for 3D Reconstruction. (arXiv:2307.05832v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05832
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22806;&#35266;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35270;&#22270;&#34955;&#65288;Bag-of-Views&#65289;&#27169;&#22411;&#26469;&#23545;&#25429;&#33719;&#30340;&#35270;&#22270;&#36827;&#34892;&#31163;&#32447;&#25968;&#25454;&#38598;&#32454;&#21270;&#21644;&#22312;&#32447;&#19979;&#19968;&#20010;&#26368;&#20339;&#35270;&#22270;&#65288;NBV&#65289;&#35268;&#21010;&#24212;&#29992;&#20998;&#37197;&#25928;&#29992;&#65292;&#20197;&#23454;&#29616;3D&#37325;&#24314;&#30340;&#20219;&#21153;&#12290;&#21516;&#26102;&#65292;&#36824;&#24320;&#21457;&#20102;&#35270;&#22270;&#35268;&#21010;&#24037;&#20855;&#31665;&#65288;VPT&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26080;&#20154;&#26426;&#30340;&#26234;&#33021;&#25968;&#25454;&#37319;&#38598;&#29992;&#20110;3D&#37325;&#24314;&#21644;&#22522;&#30784;&#35774;&#26045;&#30417;&#27979;&#65292;&#30001;&#20110;&#22270;&#20687;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#27491;&#32463;&#21382;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#12290;&#35270;&#22270;&#35268;&#21010;&#26159;&#36825;&#20010;&#20219;&#21153;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#23427;&#20915;&#23450;&#20102;&#20449;&#24687;&#25429;&#33719;&#31574;&#30053;&#65292;&#24182;&#19988;&#20005;&#37325;&#24433;&#21709;&#20174;&#25429;&#33719;&#30340;&#25968;&#25454;&#29983;&#25104;&#30340;3D&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#20351;&#29992;&#20808;&#21069;&#30340;&#30693;&#35782;&#25110;&#30446;&#26631;&#30340;&#37096;&#20998;&#37325;&#24314;&#26469;&#23454;&#29616;&#20027;&#21160;&#37325;&#24314;&#30340;&#35270;&#22270;&#35268;&#21010;&#65307;&#21069;&#19968;&#31181;&#26041;&#27861;&#23545;&#20110;&#22797;&#26434;&#25110;&#26032;&#35782;&#21035;&#30340;&#30446;&#26631;&#26500;&#25104;&#20102;&#25361;&#25112;&#65292;&#32780;&#21518;&#32773;&#35745;&#31639;&#24320;&#38144;&#24456;&#22823;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35270;&#22270;&#34955;&#65288;BoV&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#23436;&#20840;&#22522;&#20110;&#22806;&#35266;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20026;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#32454;&#21270;&#21644;&#22312;&#32447;&#19979;&#19968;&#20010;&#26368;&#20339;&#35270;&#22270;&#65288;NBV&#65289;&#35268;&#21010;&#24212;&#29992;&#20998;&#37197;&#25928;&#29992;&#65292;&#20197;&#23454;&#29616;3D&#37325;&#24314;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#36825;&#20010;&#24037;&#20316;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#35270;&#22270;&#35268;&#21010;&#24037;&#20855;&#31665;&#65288;VPT&#65289;&#65292;
&lt;/p&gt;
&lt;p&gt;
UAV-based intelligent data acquisition for 3D reconstruction and monitoring of infrastructure has been experiencing an increasing surge of interest due to the recent advancements in image processing and deep learning-based techniques. View planning is an essential part of this task that dictates the information capture strategy and heavily impacts the quality of the 3D model generated from the captured data. Recent methods have used prior knowledge or partial reconstruction of the target to accomplish view planning for active reconstruction; the former approach poses a challenge for complex or newly identified targets while the latter is computationally expensive. In this work, we present Bag-of-Views (BoV), a fully appearance-based model used to assign utility to the captured views for both offline dataset refinement and online next-best-view (NBV) planning applications targeting the task of 3D reconstruction. With this contribution, we also developed the View Planning Toolbox (VPT), 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#25439;&#22833;&#20989;&#25968;&#26354;&#29575;&#36827;&#34892;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#21516;&#26679;&#26412;&#19978;&#30340;&#27867;&#21270;&#19982;&#35760;&#24518;&#21270;&#29305;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#39640;&#26354;&#29575;&#30340;&#26679;&#26412;&#36890;&#24120;&#26159;&#20855;&#26377;&#26631;&#31614;&#38169;&#35823;&#25110;&#20914;&#31361;&#30340;&#38271;&#23614;&#26679;&#26412;&#65292;&#24182;&#22312;CIFAR100&#25968;&#25454;&#38598;&#19978;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#22833;&#36133;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;&#37096;&#20998;&#26679;&#26412;&#36827;&#34892;&#38543;&#26426;&#26631;&#31614;&#38169;&#35823;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26354;&#29575;&#25490;&#24207;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#20986;&#36825;&#20123;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2307.05831</link><description>&lt;p&gt;
&#36890;&#36807;&#25439;&#22833;&#20989;&#25968;&#26354;&#29575;&#35270;&#35282;&#25581;&#31034;&#35760;&#24518;&#21270;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Memorization Through the Lens of Curvature of Loss Function Around Samples. (arXiv:2307.05831v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#25439;&#22833;&#20989;&#25968;&#26354;&#29575;&#36827;&#34892;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#21516;&#26679;&#26412;&#19978;&#30340;&#27867;&#21270;&#19982;&#35760;&#24518;&#21270;&#29305;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#39640;&#26354;&#29575;&#30340;&#26679;&#26412;&#36890;&#24120;&#26159;&#20855;&#26377;&#26631;&#31614;&#38169;&#35823;&#25110;&#20914;&#31361;&#30340;&#38271;&#23614;&#26679;&#26412;&#65292;&#24182;&#22312;CIFAR100&#25968;&#25454;&#38598;&#19978;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#22833;&#36133;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;&#37096;&#20998;&#26679;&#26412;&#36827;&#34892;&#38543;&#26426;&#26631;&#31614;&#38169;&#35823;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26354;&#29575;&#25490;&#24207;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#20986;&#36825;&#20123;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#36807;&#22810;&#65292;&#24456;&#23481;&#26131;&#36807;&#25311;&#21512;&#35757;&#32451;&#25968;&#25454;&#12290;&#26497;&#31471;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#21487;&#20197;&#23436;&#20840;&#35760;&#24518;&#35757;&#32451;&#38598;&#65292;&#21363;&#20351;&#26631;&#31614;&#26159;&#38543;&#26426;&#30340;&#12290;&#25105;&#20204;&#25552;&#35758;&#20351;&#29992;&#35757;&#32451;&#26679;&#26412;&#21608;&#22260;&#30340;&#25439;&#22833;&#20989;&#25968;&#26354;&#29575;&#20316;&#20026;&#35760;&#24518;&#21270;&#31243;&#24230;&#30340;&#24230;&#37327;&#65292;&#23545;&#25152;&#26377;&#35757;&#32451;&#36718;&#27425;&#36827;&#34892;&#24179;&#22343;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#24230;&#37327;&#26469;&#30740;&#31350;&#24120;&#35265;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#19981;&#21516;&#26679;&#26412;&#30340;&#27867;&#21270;&#19982;&#35760;&#24518;&#21270;&#29305;&#24615;&#12290;&#25105;&#20204;&#21487;&#35270;&#21270;&#20855;&#26377;&#26368;&#39640;&#25439;&#22833;&#26354;&#29575;&#30340;&#26679;&#26412;&#65292;&#21457;&#29616;&#23427;&#20204;&#36890;&#24120;&#26159;&#38271;&#23614;&#26679;&#26412;&#12289;&#26631;&#31614;&#38169;&#35823;&#25110;&#20914;&#31361;&#26679;&#26412;&#12290;&#36825;&#31181;&#20998;&#26512;&#24110;&#21161;&#25105;&#20204;&#22312;CIFAR100&#25968;&#25454;&#38598;&#19978;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#22833;&#36133;&#27169;&#22411;&#65292;&#21363;&#20855;&#26377;&#19981;&#21516;&#26631;&#31614;&#30340;&#37325;&#22797;&#22270;&#20687;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#38543;&#26426;&#38169;&#35823;&#21270;&#23569;&#37327;&#26679;&#26412;&#30340;&#26631;&#31614;&#26469;&#20154;&#20026;&#22320;&#32473;&#25968;&#25454;&#38598;&#24341;&#20837;&#26631;&#31614;&#38169;&#35823;&#65292;&#24182;&#23637;&#31034;&#20102;&#25353;&#26354;&#29575;&#25490;&#24207;&#21487;&#20197;&#39640;&#25928;&#22320;&#35782;&#21035;&#20986;&#26631;&#31614;&#38169;&#35823;&#26679;&#26412;&#30340;&#39640;AUROC&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are overparametrized and easily overfit the datasets they train on. In the extreme case, it is shown that they can memorize a training set with fully randomized labels. We propose using the curvature of loss function around the training sample as a measure of its memorization, averaged over all training epochs. We use this to study the generalization versus memorization properties of different samples in popular image datasets. We visualize samples with the highest curvature of loss around them, and show that these visually correspond to long-tailed, mislabeled or conflicting samples. This analysis helps us find a, to the best of our knowledge, novel failure model on the CIFAR100 dataset, that of duplicated images with different labels. We also synthetically mislabel a proportion of the dataset by randomly corrupting the labels of a few samples, and show that sorting by curvature yields high AUROC values for identifying the mislabeled samples.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#21367;&#31215;&#21644;&#35760;&#24518;&#32593;&#32476;&#65292;&#22312;&#32500;&#22522;&#30334;&#31185;&#30340;&#34920;&#26684;&#25968;&#25454;&#20013;&#36827;&#34892;&#20851;&#31995;&#25277;&#21462;&#12290;&#35813;&#27169;&#22411;&#22312;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#32463;&#36807;&#20840;&#38754;&#30340;&#20998;&#26512;&#21644;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#21508;&#20010;&#27169;&#22411;&#32452;&#20214;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2307.05827</link><description>&lt;p&gt;
&#20351;&#29992;&#21367;&#31215;&#21644;&#35760;&#24518;&#32593;&#32476;&#22312;&#32500;&#22522;&#30334;&#31185;&#34920;&#26684;&#19978;&#36827;&#34892;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Relational Extraction on Wikipedia Tables using Convolutional and Memory Networks. (arXiv:2307.05827v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05827
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21367;&#31215;&#21644;&#35760;&#24518;&#32593;&#32476;&#65292;&#22312;&#32500;&#22522;&#30334;&#31185;&#30340;&#34920;&#26684;&#25968;&#25454;&#20013;&#36827;&#34892;&#20851;&#31995;&#25277;&#21462;&#12290;&#35813;&#27169;&#22411;&#22312;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#32463;&#36807;&#20840;&#38754;&#30340;&#20998;&#26512;&#21644;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#21508;&#20010;&#27169;&#22411;&#32452;&#20214;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25277;&#21462;&#26159;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#23454;&#20307;&#20043;&#38388;&#20851;&#31995;&#30340;&#20219;&#21153;&#12290;&#22823;&#37096;&#20998;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#20174;&#33258;&#30001;&#26684;&#24335;&#30340;&#36830;&#32493;&#25991;&#26412;&#20013;&#25552;&#21462;&#20851;&#31995;&#65292;&#32780;&#24573;&#30053;&#20102;&#20854;&#20182;&#20016;&#23500;&#30340;&#25968;&#25454;&#26469;&#28304;&#65292;&#27604;&#22914;&#34920;&#26684;&#12290;&#25105;&#20204;&#20174;&#24212;&#29992;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#22788;&#29702;&#34920;&#26684;&#21270;&#25968;&#25454;&#30340;&#35282;&#24230;&#25506;&#32034;&#20851;&#31995;&#25277;&#21462;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411;&#65292;&#30001;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;BiLSTM&#65289;&#32593;&#32476;&#32452;&#25104;&#65292;&#20998;&#21035;&#29992;&#20110;&#32534;&#30721;&#23454;&#20307;&#21644;&#23398;&#20064;&#23427;&#20204;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22823;&#35268;&#27169;&#19988;&#26368;&#26032;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#19982;&#20043;&#21069;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#22987;&#32456;&#20248;&#20110;&#20043;&#21069;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#38169;&#35823;&#20998;&#26512;&#21644;&#21093;&#31163;&#30740;&#31350;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#21508;&#20010;&#32452;&#25104;&#37096;&#20998;&#30340;&#36129;&#29486;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#21644;&#26435;&#34913;&#65292;&#24182;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction (RE) is the task of extracting relations between entities in text. Most RE methods extract relations from free-form running text and leave out other rich data sources, such as tables. We explore RE from the perspective of applying neural methods on tabularly organized data. We introduce a new model consisting of Convolutional Neural Network (CNN) and Bidirectional-Long Short Term Memory (BiLSTM) network to encode entities and learn dependencies among them, respectively. We evaluate our model on a large and recent dataset and compare results with previous neural methods. Experimental results show that our model consistently outperforms the previous model for the task of relation extraction on tabular data. We perform comprehensive error analyses and ablation study to show the contribution of various components of our model. Finally, we discuss the usefulness and trade-offs of our approach, and provide suggestions for fostering further research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#21551;&#21457;&#30340;&#22320;&#22270;&#26500;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#21106;&#21644;&#22238;&#28335;&#26469;&#35299;&#20915;&#22823;&#22411;&#29615;&#22659;&#19979;&#30340;&#25506;&#32034;&#38382;&#39064;&#65292;&#24182;&#22522;&#20110;&#24847;&#22806;&#24615;&#30340;&#31354;&#38388;&#32858;&#31867;&#35774;&#32622;&#25506;&#32034;&#23376;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2307.05793</link><description>&lt;p&gt;
&#31070;&#32463;&#21551;&#21457;&#30340;&#39640;&#25928;&#22320;&#22270;&#26500;&#24314;&#36890;&#36807;&#20998;&#21106;&#21644;&#22238;&#28335;
&lt;/p&gt;
&lt;p&gt;
Neuro-Inspired Efficient Map Building via Fragmentation and Recall. (arXiv:2307.05793v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#21551;&#21457;&#30340;&#22320;&#22270;&#26500;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#21106;&#21644;&#22238;&#28335;&#26469;&#35299;&#20915;&#22823;&#22411;&#29615;&#22659;&#19979;&#30340;&#25506;&#32034;&#38382;&#39064;&#65292;&#24182;&#22522;&#20110;&#24847;&#22806;&#24615;&#30340;&#31354;&#38388;&#32858;&#31867;&#35774;&#32622;&#25506;&#32034;&#23376;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#29289;&#21644;&#26426;&#22120;&#20154;&#36890;&#36807;&#26500;&#24314;&#21644;&#23436;&#21892;&#31354;&#38388;&#22320;&#22270;&#26469;&#23548;&#33322;&#29615;&#22659;&#12290;&#36825;&#20123;&#22320;&#22270;&#20351;&#24471;&#21253;&#25324;&#22238;&#23478;&#12289;&#35268;&#21010;&#12289;&#25628;&#32034;&#21644;&#35269;&#39135;&#22312;&#20869;&#30340;&#21151;&#33021;&#25104;&#20026;&#21487;&#33021;&#12290;&#22312;&#22823;&#22411;&#29615;&#22659;&#20013;&#65292;&#25506;&#32034;&#31354;&#38388;&#26159;&#19968;&#20010;&#38590;&#39064;&#65306;&#20195;&#29702;&#21487;&#33021;&#20250;&#38519;&#20837;&#23616;&#37096;&#21306;&#22495;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20174;&#31070;&#32463;&#31185;&#23398;&#20013;&#27762;&#21462;&#32463;&#39564;&#65292;&#25552;&#20986;&#24182;&#24212;&#29992;&#20102;&#20998;&#21106;&#21644;&#22238;&#28335;&#65288;FarMap&#65289;&#30340;&#27010;&#24565;&#12290;&#20195;&#29702;&#36890;&#36807;&#22522;&#20110;&#24847;&#22806;&#24615;&#30340;&#31354;&#38388;&#32858;&#31867;&#26469;&#35299;&#20915;&#22320;&#22270;&#26500;&#24314;&#38382;&#39064;&#65292;&#21516;&#26102;&#23558;&#20854;&#29992;&#20110;&#35774;&#32622;&#31354;&#38388;&#25506;&#32034;&#30340;&#23376;&#30446;&#26631;&#12290;&#20195;&#29702;&#26500;&#24314;&#21644;&#20351;&#29992;&#26412;&#22320;&#22320;&#22270;&#26469;&#39044;&#27979;&#20182;&#20204;&#30340;&#35266;&#27979;&#32467;&#26524;&#65307;&#39640;&#24847;&#22806;&#24615;&#20250;&#23548;&#33268;&#8220;&#20998;&#21106;&#20107;&#20214;&#8221;&#65292;&#20174;&#32780;&#25130;&#26029;&#26412;&#22320;&#22320;&#22270;&#12290;&#22312;&#36825;&#20123;&#20107;&#20214;&#20013;&#65292;&#26368;&#36817;&#30340;&#26412;&#22320;&#22320;&#22270;&#34987;&#25918;&#20837;&#38271;&#26399;&#35760;&#24518;&#65288;LTM&#65289;&#20013;&#65292;&#24182;&#21021;&#22987;&#21270;&#21478;&#19968;&#20010;&#26412;&#22320;&#22320;&#22270;&#12290;&#22914;&#26524;&#26029;&#35010;&#28857;&#30340;&#35266;&#23519;&#32467;&#26524;&#19982;&#23384;&#20648;&#30340;&#26576;&#20010;&#26412;&#22320;&#22320;&#22270;&#30340;&#35266;&#23519;&#32467;&#26524;&#30456;&#21305;&#37197;&#65292;&#37027;&#20040;&#35813;&#22320;&#22270;&#23601;&#20250;&#34987;&#22238;&#28335;&#65288;&#24182;&#37325;&#29992;&#65289;&#33258;LTM&#12290;&#20998;&#21106;&#28857;&#35825;&#23548;.
&lt;/p&gt;
&lt;p&gt;
Animals and robots navigate through environments by building and refining maps of the space. These maps enable functions including navigating back to home, planning, search, and foraging. In large environments, exploration of the space is a hard problem: agents can become stuck in local regions. Here, we use insights from neuroscience to propose and apply the concept of Fragmentation-and-Recall (FarMap), with agents solving the mapping problem by building local maps via a surprisal-based clustering of space, which they use to set subgoals for spatial exploration. Agents build and use a local map to predict their observations; high surprisal leads to a ``fragmentation event'' that truncates the local map. At these events, the recent local map is placed into long-term memory (LTM), and a different local map is initialized. If observations at a fracture point match observations in one of the stored local maps, that map is recalled (and thus reused) from LTM. The fragmentation points induc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#21512;&#24182;&#22810;&#20010;&#36755;&#20837;&#25551;&#36848;&#31526;&#21644;&#30417;&#30563;&#22120;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36807;&#28388;&#36947;&#36335;&#22270;&#25968;&#25454;&#12290;&#20316;&#32773;&#36890;&#36807;&#20351;&#29992;&#22235;&#31181;&#19981;&#21516;&#30340;&#36947;&#36335;&#22270;&#36807;&#28388;&#31574;&#30053;&#65292;&#24182;&#23558;&#23427;&#20204;&#30340;&#36755;&#20986;&#36827;&#34892;&#32452;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#36947;&#36335;&#32447;&#30340;&#20998;&#31867;&#12290;&#20316;&#32773;&#36824;&#21457;&#29616;&#65292;&#22312;&#27492;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#36947;&#36335;&#32447;&#30340;&#22352;&#26631;&#26159;&#26368;&#37325;&#35201;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2307.05786</link><description>&lt;p&gt;
&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#21512;&#24182;&#22810;&#20010;&#36755;&#20837;&#25551;&#36848;&#31526;&#21644;&#30417;&#30563;&#22120;&#29992;&#20110;&#36947;&#36335;&#22270;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
Merging multiple input descriptors and supervisors in a deep neural network for tractogram filtering. (arXiv:2307.05786v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#21512;&#24182;&#22810;&#20010;&#36755;&#20837;&#25551;&#36848;&#31526;&#21644;&#30417;&#30563;&#22120;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36807;&#28388;&#36947;&#36335;&#22270;&#25968;&#25454;&#12290;&#20316;&#32773;&#36890;&#36807;&#20351;&#29992;&#22235;&#31181;&#19981;&#21516;&#30340;&#36947;&#36335;&#22270;&#36807;&#28388;&#31574;&#30053;&#65292;&#24182;&#23558;&#23427;&#20204;&#30340;&#36755;&#20986;&#36827;&#34892;&#32452;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#36947;&#36335;&#32447;&#30340;&#20998;&#31867;&#12290;&#20316;&#32773;&#36824;&#21457;&#29616;&#65292;&#22312;&#27492;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#36947;&#36335;&#32447;&#30340;&#22352;&#26631;&#26159;&#26368;&#37325;&#35201;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#36947;&#36335;&#22270;&#26041;&#27861;&#30340;&#20027;&#35201;&#38382;&#39064;&#20043;&#19968;&#26159;&#35823;&#25253;&#29575;&#39640;&#12290;&#36947;&#36335;&#22270;&#36807;&#28388;&#26159;&#22312;&#21518;&#22788;&#29702;&#27493;&#39588;&#20013;&#20174;&#36947;&#36335;&#22270;&#25968;&#25454;&#20013;&#31227;&#38500;&#35823;&#25253;&#36947;&#36335;&#30340;&#36873;&#39033;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36807;&#28388;&#36947;&#36335;&#22270;&#25968;&#25454;&#65292;&#20854;&#20013;&#36947;&#36335;&#22270;&#30340;&#27599;&#26465;&#36947;&#36335;&#32447;&#34987;&#20998;&#31867;&#20026;"&#21512;&#29702;&#30340;"&#12289;"&#19981;&#21512;&#29702;&#30340;"&#25110;"&#19981;&#30830;&#23450;&#30340;"&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#36947;&#36335;&#22270;&#36807;&#28388;&#31574;&#30053;&#20316;&#20026;&#30417;&#30563;&#22120;&#65306;TractQuerier&#12289;RecobundlesX&#12289;TractSeg&#21644;&#19968;&#31181;&#22522;&#20110;&#35299;&#21078;&#23398;&#30340;&#36807;&#28388;&#22120;&#12290;&#23427;&#20204;&#30340;&#36755;&#20986;&#34987;&#32452;&#21512;&#36215;&#26469;&#20197;&#33719;&#24471;&#36947;&#36335;&#32447;&#30340;&#20998;&#31867;&#26631;&#31614;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#27839;&#36947;&#36335;&#32447;&#25191;&#34892;&#27492;&#20998;&#31867;&#20219;&#21153;&#25152;&#38656;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#65292;&#21253;&#25324;&#36947;&#36335;&#32447;&#30340;&#22352;&#26631;&#12289;&#25193;&#25955;&#25968;&#25454;&#12289;&#20851;&#38190;&#28857;&#12289;T1&#21152;&#26435;&#20449;&#24687;&#21644;&#33041;&#21306;&#20998;&#21106;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#36825;&#31181;&#29305;&#23450;&#24773;&#20917;&#19979;&#65292;&#36947;&#36335;&#32447;&#30340;&#22352;&#26631;&#26159;&#26368;&#30456;&#20851;&#30340;&#65292;&#20854;&#27425;&#26159;&#25193;&#25955;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the main issues of the current tractography methods is their high false-positive rate. Tractogram filtering is an option to remove false-positive streamlines from tractography data in a post-processing step. In this paper, we train a deep neural network for filtering tractography data in which every streamline of a tractogram is classified as {\em plausible, implausible}, or {\em inconclusive}. For this, we use four different tractogram filtering strategies as supervisors: TractQuerier, RecobundlesX, TractSeg, and an anatomy-inspired filter. Their outputs are combined to obtain the classification labels for the streamlines. We assessed the importance of different types of information along the streamlines for performing this classification task, including the coordinates of the streamlines, diffusion data, landmarks, T1-weighted information, and a brain parcellation. We found that the streamline coordinates are the most relevant followed by the diffusion data in this particular 
&lt;/p&gt;</description></item><item><title>EgoAdapt&#26159;&#19968;&#20010;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#33258;&#25105;&#20013;&#24515;&#21160;&#20316;&#35782;&#21035;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#33539;&#20363;&#24182;&#35299;&#20915;&#27969;&#25968;&#25454;&#20013;&#30340;&#20998;&#24067;&#36716;&#31227;&#25361;&#25112;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#26681;&#25454;&#29992;&#25143;&#30340;&#23454;&#38469;&#38656;&#27714;&#36827;&#34892;&#33258;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2307.05784</link><description>&lt;p&gt;
EgoAdapt&#65306;&#36866;&#24212;&#30495;&#23454;&#19990;&#30028;&#33258;&#25105;&#20013;&#24515;&#29992;&#25143;&#35270;&#39057;&#30340;&#22810;&#27969;&#35780;&#20272;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
EgoAdapt: A multi-stream evaluation study of adaptation to real-world egocentric user video. (arXiv:2307.05784v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05784
&lt;/p&gt;
&lt;p&gt;
EgoAdapt&#26159;&#19968;&#20010;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#33258;&#25105;&#20013;&#24515;&#21160;&#20316;&#35782;&#21035;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#33539;&#20363;&#24182;&#35299;&#20915;&#27969;&#25968;&#25454;&#20013;&#30340;&#20998;&#24067;&#36716;&#31227;&#25361;&#25112;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#26681;&#25454;&#29992;&#25143;&#30340;&#23454;&#38469;&#38656;&#27714;&#36827;&#34892;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#25105;&#20013;&#24515;&#21160;&#20316;&#35782;&#21035;&#20013;&#65292;&#36890;&#24120;&#20250;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#30340;&#31181;&#32676;&#27169;&#22411;&#65292;&#24182;&#22312;&#22836;&#25140;&#35774;&#22791;&#65288;&#22914;&#22686;&#24378;&#29616;&#23454;&#22836;&#30420;&#65289;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#27169;&#22411;&#23545;&#20110;&#26032;&#29992;&#25143;&#21644;&#29615;&#22659;&#26469;&#35828;&#26159;&#38745;&#24577;&#30340;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#30340;&#20004;&#38454;&#27573;&#33539;&#20363;&#65292;&#21363;&#22312;&#39044;&#35757;&#32451;&#31181;&#32676;&#27169;&#22411;&#21518;&#65292;&#27169;&#22411;&#26681;&#25454;&#29992;&#25143;&#30340;&#32463;&#39564;&#36827;&#34892;&#35774;&#22791;&#20869;&#21644;&#22312;&#32447;&#30340;&#36866;&#24212;&#12290;&#36825;&#20010;&#35774;&#32622;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20174;&#31181;&#32676;&#39046;&#22495;&#36716;&#21464;&#20026;&#29992;&#25143;&#39046;&#22495;&#65292;&#24182;&#19988;&#29992;&#25143;&#25968;&#25454;&#27969;&#20013;&#23384;&#22312;&#20998;&#24067;&#30340;&#36716;&#21464;&#12290;&#24212;&#23545;&#21518;&#32773;&#20013;&#22312;&#27969;&#37327;&#20013;&#30340;&#20998;&#24067;&#36716;&#31227;&#26159;&#25345;&#32493;&#23398;&#20064;&#30340;&#37325;&#28857;&#65292;&#32780;&#25345;&#32493;&#23398;&#20064;&#30340;&#36827;&#23637;&#20027;&#35201;&#26159;&#22312;&#21463;&#25511;&#22522;&#20934;&#19978;&#30340;&#65292;&#28982;&#32780;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#24448;&#24448;&#27809;&#26377;&#24471;&#21040;&#35299;&#20915;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;EgoAdapt&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#33258;&#25105;&#20013;&#24515;&#21160;&#20316;&#35782;&#21035;&#30340;&#22522;&#20934;&#65292;&#21487;&#20197;&#20419;&#36827;&#25105;&#20204;&#30340;&#20004;&#38454;&#27573;&#33258;&#36866;&#24212;&#33539;&#20363;&#65292;&#24182;&#19988;&#33258;&#25105;&#20013;&#24515;&#35270;&#39057;&#27969;&#20013;&#33258;&#28982;&#21457;&#29983;&#30340;&#30495;&#23454;&#19990;&#30028;&#25361;&#25112;&#65292;&#22914;&#38271;&#23614;&#21160;&#20316;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
In egocentric action recognition a single population model is typically trained and subsequently embodied on a head-mounted device, such as an augmented reality headset. While this model remains static for new users and environments, we introduce an adaptive paradigm of two phases, where after pretraining a population model, the model adapts on-device and online to the user's experience. This setting is highly challenging due to the change from population to user domain and the distribution shifts in the user's data stream. Coping with the latter in-stream distribution shifts is the focus of continual learning, where progress has been rooted in controlled benchmarks but challenges faced in real-world applications often remain unaddressed. We introduce EgoAdapt, a benchmark for real-world egocentric action recognition that facilitates our two-phased adaptive paradigm, and real-world challenges naturally occur in the egocentric video streams from Ego4d, such as long-tailed action distrib
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Rad-ReStruct&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#30340;&#26032;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;X&#20809;&#22270;&#20687;&#30340;&#32467;&#26500;&#21270;&#25253;&#21578;&#24418;&#24335;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#12289;&#25353;&#23618;&#27425;&#25490;&#24207;&#30340;&#27880;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;hi-VQA&#65292;&#23558;&#32467;&#26500;&#21270;&#25253;&#21578;&#20219;&#21153;&#24314;&#27169;&#20026;&#20998;&#23618;&#35270;&#35273;&#38382;&#31572;(VQA)&#65292;&#24182;&#32771;&#34385;&#20808;&#21069;&#25552;&#38382;&#21644;&#22238;&#31572;&#30340;&#19978;&#19979;&#25991;&#26469;&#22635;&#20805;&#32467;&#26500;&#21270;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;&#23454;&#39564;&#35777;&#26126;hi-VQA&#21462;&#24471;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05766</link><description>&lt;p&gt;
Rad-ReStruct: &#19968;&#31181;&#26032;&#39062;&#30340;&#32467;&#26500;&#21270;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;VQA&#22522;&#20934;&#21644;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rad-ReStruct: A Novel VQA Benchmark and Method for Structured Radiology Reporting. (arXiv:2307.05766v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Rad-ReStruct&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#30340;&#26032;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;X&#20809;&#22270;&#20687;&#30340;&#32467;&#26500;&#21270;&#25253;&#21578;&#24418;&#24335;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#12289;&#25353;&#23618;&#27425;&#25490;&#24207;&#30340;&#27880;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;hi-VQA&#65292;&#23558;&#32467;&#26500;&#21270;&#25253;&#21578;&#20219;&#21153;&#24314;&#27169;&#20026;&#20998;&#23618;&#35270;&#35273;&#38382;&#31572;(VQA)&#65292;&#24182;&#32771;&#34385;&#20808;&#21069;&#25552;&#38382;&#21644;&#22238;&#31572;&#30340;&#19978;&#19979;&#25991;&#26469;&#22635;&#20805;&#32467;&#26500;&#21270;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;&#23454;&#39564;&#35777;&#26126;hi-VQA&#21462;&#24471;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25918;&#23556;&#23398;&#25253;&#21578;&#26159;&#25918;&#23556;&#31185;&#21307;&#29983;&#19982;&#20854;&#20182;&#21307;&#21153;&#20154;&#21592;&#20043;&#38388;&#27807;&#36890;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#20294;&#20854;&#21487;&#33021;&#32791;&#26102;&#19988;&#23481;&#26131;&#20986;&#38169;&#12290;&#20854;&#20013;&#19968;&#31181;&#20943;&#36731;&#36825;&#31181;&#24773;&#20917;&#30340;&#26041;&#27861;&#26159;&#32467;&#26500;&#21270;&#25253;&#21578;&#65292;&#23427;&#27604;&#33258;&#30001;&#25991;&#26412;&#25253;&#21578;&#26356;&#33410;&#32422;&#26102;&#38388;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#33258;&#21160;&#21270;&#32467;&#26500;&#21270;&#25253;&#21578;&#30340;&#30740;&#31350;&#26377;&#38480;&#65292;&#24182;&#19988;&#30446;&#21069;&#27809;&#26377;&#20844;&#24320;&#30340;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Rad-ReStruct&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#30340;&#12289;&#25353;&#23618;&#27425;&#25490;&#24207;&#30340;X&#20809;&#22270;&#20687;&#30340;&#32467;&#26500;&#21270;&#25253;&#21578;&#24418;&#24335;&#30340;&#27880;&#37322;&#12290;&#25105;&#20204;&#23558;&#32467;&#26500;&#21270;&#25253;&#21578;&#20219;&#21153;&#24314;&#27169;&#20026;&#20998;&#23618;&#35270;&#35273;&#38382;&#31572;(VQA)&#65292;&#24182;&#25552;&#20986;&#20102;hi-VQA&#65292;&#19968;&#31181;&#32771;&#34385;&#20808;&#21069;&#25552;&#38382;&#21644;&#22238;&#31572;&#30340;&#19978;&#19979;&#25991;&#20197;&#22635;&#20805;&#32467;&#26500;&#21270;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;hi-VQA&#22312;&#21307;&#23398;VQA&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Radiology reporting is a crucial part of the communication between radiologists and other medical professionals, but it can be time-consuming and error-prone. One approach to alleviate this is structured reporting, which saves time and enables a more accurate evaluation than free-text reports. However, there is limited research on automating structured reporting, and no public benchmark is available for evaluating and comparing different methods. To close this gap, we introduce Rad-ReStruct, a new benchmark dataset that provides fine-grained, hierarchically ordered annotations in the form of structured reports for X-Ray images. We model the structured reporting task as hierarchical visual question answering (VQA) and propose hi-VQA, a novel method that considers prior context in the form of previously asked questions and answers for populating a structured radiology report. Our experiments show that hi-VQA achieves competitive performance to the state-of-the-art on the medical VQA benc
&lt;/p&gt;</description></item><item><title>&#23558;&#35838;&#31243;&#19982;&#22238;&#25918;&#26041;&#27861;&#30456;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#25345;&#32493;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#36890;&#36807;&#35843;&#25972;&#22238;&#25918;&#23454;&#20363;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#20132;&#26367;&#39057;&#29575;&#12289;&#22238;&#25918;&#23454;&#20363;&#30340;&#39034;&#24207;&#20197;&#21450;&#36873;&#25321;&#36827;&#20837;&#22238;&#25918;&#32531;&#20914;&#21306;&#30340;&#31574;&#30053;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.05747</link><description>&lt;p&gt;
&#23558;&#35838;&#31243;&#19982;&#22238;&#25918;&#30456;&#32467;&#21512;&#65306;&#23545;&#25345;&#32493;&#23398;&#20064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Integrating Curricula with Replays: Its Effects on Continual Learning. (arXiv:2307.05747v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05747
&lt;/p&gt;
&lt;p&gt;
&#23558;&#35838;&#31243;&#19982;&#22238;&#25918;&#26041;&#27861;&#30456;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#25345;&#32493;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#36890;&#36807;&#35843;&#25972;&#22238;&#25918;&#23454;&#20363;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#20132;&#26367;&#39057;&#29575;&#12289;&#22238;&#25918;&#23454;&#20363;&#30340;&#39034;&#24207;&#20197;&#21450;&#36873;&#25321;&#36827;&#20837;&#22238;&#25918;&#32531;&#20914;&#21306;&#30340;&#31574;&#30053;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22312;&#33719;&#21462;&#26032;&#25216;&#33021;&#25110;&#30693;&#35782;&#26102;&#65292;&#36890;&#36807;&#35838;&#31243;&#36827;&#34892;&#23398;&#20064;&#21644;&#22797;&#20064;&#12290;&#36825;&#31181;&#20154;&#31867;&#23398;&#20064;&#34892;&#20026;&#21551;&#21457;&#20102;&#22312;&#25345;&#32493;&#23398;&#20064;&#20195;&#29702;&#20013;&#23558;&#35838;&#31243;&#19982;&#22238;&#25918;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#30446;&#26631;&#26159;&#27169;&#25311;&#20154;&#31867;&#23398;&#20064;&#36807;&#31243;&#65292;&#20174;&#32780;&#25552;&#39640;&#30693;&#35782;&#20445;&#30041;&#21644;&#20419;&#36827;&#23398;&#20064;&#36716;&#31227;&#12290;&#29616;&#26377;&#30340;&#25345;&#32493;&#23398;&#20064;&#20195;&#29702;&#20013;&#30340;&#22238;&#25918;&#26041;&#27861;&#28041;&#21450;&#20174;&#20808;&#21069;&#20219;&#21153;&#20013;&#38543;&#26426;&#36873;&#25321;&#21644;&#25490;&#24207;&#25968;&#25454;&#65292;&#24050;&#32463;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#26377;&#38480;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#19981;&#21516;&#35838;&#31243;&#19982;&#22238;&#25918;&#26041;&#27861;&#30456;&#32467;&#21512;&#20197;&#22686;&#24378;&#25345;&#32493;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#39318;&#27425;&#32771;&#23519;&#20102;&#23558;&#35838;&#31243;&#19982;&#22238;&#25918;&#26041;&#27861;&#30456;&#32467;&#21512;&#23545;&#25345;&#32493;&#23398;&#20064;&#30340;&#24433;&#21709;&#30340;&#19977;&#20010;&#20855;&#20307;&#26041;&#38754;&#65306;&#22238;&#25918;&#23454;&#20363;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#20132;&#26367;&#39057;&#29575;&#65292;&#22238;&#25918;&#23454;&#20363;&#30340;&#39034;&#24207;&#65292;&#20197;&#21450;&#36873;&#25321;&#23454;&#20363;&#36827;&#20837;&#22238;&#25918;&#32531;&#20914;&#21306;&#30340;&#31574;&#30053;&#12290;&#36825;&#20123;&#35838;&#31243;&#35774;&#35745;&#30340;&#26041;&#38754;
&lt;/p&gt;
&lt;p&gt;
Humans engage in learning and reviewing processes with curricula when acquiring new skills or knowledge. This human learning behavior has inspired the integration of curricula with replay methods in continual learning agents. The goal is to emulate the human learning process, thereby improving knowledge retention and facilitating learning transfer. Existing replay methods in continual learning agents involve the random selection and ordering of data from previous tasks, which has shown to be effective. However, limited research has explored the integration of different curricula with replay methods to enhance continual learning. Our study takes initial steps in examining the impact of integrating curricula with replay methods on continual learning in three specific aspects: the interleaved frequency of replayed exemplars with training data, the sequence in which exemplars are replayed, and the strategy for selecting exemplars into the replay buffer. These aspects of curricula design al
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#38754;&#21521;&#32452;&#21512;&#20998;&#31867;&#20013;&#22810;&#32452;&#20844;&#24179;&#24615;&#25913;&#21892;&#30340;&#21487;&#25193;&#23637;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#24341;&#20837;&#20219;&#21153;&#36807;&#26465;&#20214;&#21270;&#21644;&#32452;&#38388;&#20132;&#26367;&#30340;&#25216;&#26415;&#65292;&#22312;&#22810;&#32452;&#22810;&#26631;&#31614;&#35774;&#23450;&#19979;&#23454;&#29616;&#20102;&#24658;&#23450;&#27604;&#20363;&#32553;&#25918;&#65292;&#24182;&#22312;&#23398;&#26415;&#21644;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05728</link><description>&lt;p&gt;
&#38754;&#21521;&#32452;&#21512;&#20998;&#31867;&#20013;&#22810;&#32452;&#20844;&#24179;&#24615;&#25913;&#21892;&#30340;&#21487;&#25193;&#23637;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Towards A Scalable Solution for Improving Multi-Group Fairness in Compositional Classification. (arXiv:2307.05728v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#38754;&#21521;&#32452;&#21512;&#20998;&#31867;&#20013;&#22810;&#32452;&#20844;&#24179;&#24615;&#25913;&#21892;&#30340;&#21487;&#25193;&#23637;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#24341;&#20837;&#20219;&#21153;&#36807;&#26465;&#20214;&#21270;&#21644;&#32452;&#38388;&#20132;&#26367;&#30340;&#25216;&#26415;&#65292;&#22312;&#22810;&#32452;&#22810;&#26631;&#31614;&#35774;&#23450;&#19979;&#23454;&#29616;&#20102;&#24658;&#23450;&#27604;&#20363;&#32553;&#25918;&#65292;&#24182;&#22312;&#23398;&#26415;&#21644;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#25991;&#29486;&#20016;&#23500;&#65292;&#20294;&#23545;&#20110;&#20462;&#22797;&#22797;&#26434;&#31995;&#32479;&#65288;&#20854;&#20013;&#26368;&#32456;&#39044;&#27979;&#26159;&#22810;&#20010;&#20998;&#31867;&#22120;&#30340;&#32452;&#21512;&#65292;&#23384;&#22312;&#22810;&#20010;&#32452;&#65289;&#30340;&#20851;&#27880;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#25991;&#39318;&#20808;&#23637;&#31034;&#20102;&#29992;&#20110;&#25913;&#21892;&#26426;&#20250;&#24179;&#31561;&#20844;&#24179;&#24615;&#30340;&#33258;&#28982;&#22522;&#32447;&#26041;&#27861;&#19982;&#34987;&#20462;&#22797;&#32452;&#25968;&#21644;&#34987;&#20462;&#22797;&#39044;&#27979;&#26631;&#31614;&#25968;&#30340;&#20056;&#31215;&#21576;&#32447;&#24615;&#20851;&#31995;&#65292;&#20351;&#20854;&#19981;&#23454;&#29992;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#31616;&#21333;&#30340;&#25216;&#26415;&#65292;&#31216;&#20026;&#8220;&#20219;&#21153;&#36807;&#26465;&#20214;&#21270;&#8221;&#21644;&#8220;&#32452;&#38388;&#20132;&#26367;&#8221;&#65292;&#20197;&#22312;&#22810;&#32452;&#22810;&#26631;&#31614;&#35774;&#23450;&#20013;&#23454;&#29616;&#24658;&#23450;&#27604;&#20363;&#32553;&#25918;&#12290;&#25105;&#20204;&#22312;&#23398;&#26415;&#21644;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#26696;&#22312;&#36825;&#20010;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the rich literature on machine learning fairness, relatively little attention has been paid to remediating complex systems, where the final prediction is the combination of multiple classifiers and where multiple groups are present. In this paper, we first show that natural baseline approaches for improving equal opportunity fairness scale linearly with the product of the number of remediated groups and the number of remediated prediction labels, rendering them impractical. We then introduce two simple techniques, called {\em task-overconditioning} and {\em group-interleaving}, to achieve a constant scaling in this multi-group multi-label setup. Our experimental results in academic and real-world environments demonstrate the effectiveness of our proposal at mitigation within this environment.
&lt;/p&gt;</description></item><item><title>PheKnowLator&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#30693;&#35782;&#22270;&#35889;&#29983;&#24577;&#31995;&#32479;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#26500;&#24314;&#21487;&#23450;&#21046;&#30340;FAIR&#26412;&#20307;&#21270;&#22522;&#30784;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#20197;&#35299;&#20915;&#29983;&#21629;&#31185;&#23398;&#20013;&#30340;&#25972;&#21512;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.05727</link><description>&lt;p&gt;
&#19968;&#20010;&#24320;&#28304;&#30340;&#30693;&#35782;&#22270;&#35889;&#29983;&#24577;&#31995;&#32479;&#29992;&#20110;&#29983;&#21629;&#31185;&#23398;
&lt;/p&gt;
&lt;p&gt;
An Open-Source Knowledge Graph Ecosystem for the Life Sciences. (arXiv:2307.05727v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05727
&lt;/p&gt;
&lt;p&gt;
PheKnowLator&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#30693;&#35782;&#22270;&#35889;&#29983;&#24577;&#31995;&#32479;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#26500;&#24314;&#21487;&#23450;&#21046;&#30340;FAIR&#26412;&#20307;&#21270;&#22522;&#30784;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#20197;&#35299;&#20915;&#29983;&#21629;&#31185;&#23398;&#20013;&#30340;&#25972;&#21512;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#21270;&#30740;&#31350;&#38656;&#35201;&#22810;&#20010;&#29983;&#29289;&#32452;&#32455;&#23610;&#24230;&#19978;&#30340;&#25968;&#25454;&#12290;&#27979;&#24207;&#21644;&#22810;&#32452;&#23398;&#25216;&#26415;&#30340;&#36827;&#27493;&#22686;&#21152;&#20102;&#36825;&#20123;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#20294;&#30740;&#31350;&#20154;&#21592;&#38754;&#20020;&#30528;&#37325;&#22823;&#30340;&#25972;&#21512;&#25361;&#25112;&#12290;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#29992;&#20110;&#24314;&#27169;&#22797;&#26434;&#29616;&#35937;&#65292;&#24050;&#32463;&#23384;&#22312;&#33258;&#21160;&#26500;&#24314;&#23427;&#20204;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;&#29983;&#29289;&#21307;&#23398;&#25972;&#21512;&#38382;&#39064;&#38656;&#35201;&#22312;&#30693;&#35782;&#24314;&#27169;&#26041;&#24335;&#19978;&#28789;&#27963;&#24615;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;KG&#26500;&#24314;&#26041;&#27861;&#22312;&#25552;&#20379;&#24378;&#22823;&#24037;&#20855;&#30340;&#21516;&#26102;&#65292;&#20063;&#20250;&#38480;&#21046;&#22312;&#30693;&#35782;&#34920;&#31034;&#27169;&#22411;&#20013;&#22266;&#23450;&#25110;&#26377;&#38480;&#30340;&#36873;&#25321;&#12290;PheKnowLator&#65288;Phenotype Knowledge Translator&#65289;&#26159;&#19968;&#20010;&#35821;&#20041;&#29983;&#24577;&#31995;&#32479;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#20855;&#26377;&#23436;&#20840;&#21487;&#23450;&#21046;&#30340;&#30693;&#35782;&#34920;&#31034;&#30340;FAIR&#65288;&#21487;&#25214;&#21040;&#65292;&#21487;&#35775;&#38382;&#65292;&#21487;&#20114;&#25805;&#20316;&#65292;&#21487;&#37325;&#22797;&#20351;&#29992;&#65289;&#26412;&#20307;&#21270;&#22522;&#30784;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#26500;&#24314;&#12290;&#35813;&#29983;&#24577;&#31995;&#32479;&#21253;&#25324;KG&#26500;&#24314;&#36164;&#28304;&#65288;&#20363;&#22914;&#65292;&#25968;&#25454;&#20934;&#22791;API&#65289;&#65292;&#20998;&#26512;&#24037;&#20855;&#65288;&#20363;&#22914;&#65292;SPARQL&#31471;&#28857;&#21644;&#25277;&#35937;&#31639;&#27861;&#65289;&#65292;&#36824;&#26377;
&lt;/p&gt;
&lt;p&gt;
Translational research requires data at multiple scales of biological organization. Advancements in sequencing and multi-omics technologies have increased the availability of these data but researchers face significant integration challenges. Knowledge graphs (KGs) are used to model complex phenomena, and methods exist to automatically construct them. However, tackling complex biomedical integration problems requires flexibility in the way knowledge is modeled. Moreover, existing KG construction methods provide robust tooling at the cost of fixed or limited choices among knowledge representation models. PheKnowLator (Phenotype Knowledge Translator) is a semantic ecosystem for automating the FAIR (Findable, Accessible, Interoperable, and Reusable) construction of ontologically grounded KGs with fully customizable knowledge representation. The ecosystem includes KG construction resources (e.g., data preparation APIs), analysis tools (e.g., SPARQL endpoints and abstraction algorithms), an
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22312;&#32447;&#32844;&#20301;&#25512;&#33616;&#20013;&#23545;&#22270;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;&#34892;&#20026;&#22270;&#65292;&#21457;&#29616;&#20854;&#20013;&#30340;&#28508;&#22312;&#27169;&#24335;&#21644;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2307.05722</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22312;&#32447;&#32844;&#20301;&#25512;&#33616;&#20013;&#23545;&#22270;&#25968;&#25454;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Exploring Large Language Model for Graph Data Understanding in Online Job Recommendations. (arXiv:2307.05722v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22312;&#32447;&#32844;&#20301;&#25512;&#33616;&#20013;&#23545;&#22270;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;&#34892;&#20026;&#22270;&#65292;&#21457;&#29616;&#20854;&#20013;&#30340;&#28508;&#22312;&#27169;&#24335;&#21644;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#20854;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#32844;&#20301;&#25512;&#33616;&#20013;&#23545;&#34892;&#20026;&#22270;&#30340;&#29702;&#35299;&#28508;&#21147;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#26088;&#22312;&#25581;&#31034;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#34892;&#20026;&#22270;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21033;&#29992;&#36825;&#31181;&#29702;&#35299;&#26469;&#25552;&#21319;&#22312;&#32447;&#25307;&#32856;&#20013;&#30340;&#25512;&#33616;&#65292;&#21253;&#25324;&#20419;&#36827;&#38750;&#20998;&#24067;&#24335;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#30340;&#20016;&#23500;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#35821;&#20041;&#34920;&#31034;&#26469;&#20998;&#26512;&#34892;&#20026;&#22270;&#24182;&#25581;&#31034;&#20854;&#20013;&#30340;&#28508;&#22312;&#27169;&#24335;&#21644;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#36335;&#24452;&#25552;&#31034;&#26500;&#36896;&#22120;&#65292;&#21033;&#29992;LLM&#25512;&#33616;&#22120;&#39318;&#27425;&#29702;&#35299;&#34892;&#20026;&#22270;&#65292;&#24182;&#35774;&#35745;&#20102;&#30456;&#24212;&#30340;&#36335;&#24452;&#22686;&#24378;&#27169;&#22359;&#26469;&#32531;&#35299;&#22522;&#20110;&#36335;&#24452;&#30340;&#24207;&#21015;&#36755;&#20837;&#24341;&#20837;&#30340;&#25552;&#31034;&#20559;&#24046;&#12290;&#36890;&#36807;&#21033;&#29992;&#23558;LM&#30340;&#29305;&#28857;&#24341;&#20837;&#21040;&#34892;&#20026;&#22270;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have revolutionized natural language processing tasks, demonstrating their exceptional capabilities in various domains. However, their potential for behavior graph understanding in job recommendations remains largely unexplored. This paper focuses on unveiling the capability of large language models in understanding behavior graphs and leveraging this understanding to enhance recommendations in online recruitment, including the promotion of out-of-distribution (OOD) application. We present a novel framework that harnesses the rich contextual information and semantic representations provided by large language models to analyze behavior graphs and uncover underlying patterns and relationships. Specifically, we propose a meta-path prompt constructor that leverages LLM recommender to understand behavior graphs for the first time and design a corresponding path augmentation module to alleviate the prompt bias introduced by path-based sequence input. By leveragin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#20855;&#26377;&#28508;&#22312;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65292;&#20197;&#21450;&#22522;&#20110;&#28508;&#22312;&#20998;&#24067;&#30340;&#28023;&#26862;&#30697;&#38453;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#40723;&#21169;&#28508;&#22312;&#31354;&#38388;&#36981;&#24490;&#22240;&#26524;&#25490;&#24207;&#12290;</title><link>http://arxiv.org/abs/2307.05704</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#22240;&#26524;&#25490;&#24207;&#20808;&#39564;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Causal Ordering Prior for Unsupervised Representation Learning. (arXiv:2307.05704v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05704
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#20855;&#26377;&#28508;&#22312;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65292;&#20197;&#21450;&#22522;&#20110;&#28508;&#22312;&#20998;&#24067;&#30340;&#28023;&#26862;&#30697;&#38453;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#40723;&#21169;&#28508;&#22312;&#31354;&#38388;&#36981;&#24490;&#22240;&#26524;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#20381;&#36182;&#20110;&#23545;&#28508;&#21464;&#37327;&#30340;&#29420;&#31435;&#24615;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65288;CRL&#65289;&#35748;&#20026;&#25968;&#25454;&#38598;&#20013;&#30340;&#21464;&#24322;&#22240;&#32032;&#23454;&#38469;&#19978;&#26159;&#22240;&#26524;&#30456;&#20851;&#30340;&#12290;&#20801;&#35768;&#28508;&#21464;&#37327;&#30001;&#20110;&#22240;&#26524;&#20851;&#31995;&#32780;&#30456;&#20851;&#24615;&#26356;&#21152;&#30495;&#23454;&#21644;&#21487;&#27867;&#21270;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#21487;&#35777;&#26126;&#21487;&#35782;&#21035;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#65306;&#36741;&#21161;&#20449;&#24687;&#12289;&#24369;&#26631;&#31614;&#65292;&#20197;&#21450;&#24178;&#39044;&#25110;&#29978;&#33267;&#23545;&#29031;&#25968;&#25454;&#12290;&#21463;&#21040;&#21151;&#33021;&#22240;&#26524;&#27169;&#22411;&#30340;&#22240;&#26524;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#20855;&#26377;&#28508;&#22312;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#65288;ANM&#65289;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#12290;&#36890;&#36807;&#22522;&#20110;&#28508;&#22312;&#20998;&#24067;&#30340;&#28023;&#26862;&#30697;&#38453;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#40723;&#21169;&#28508;&#22312;&#31354;&#38388;&#36981;&#24490;&#22240;&#26524;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised representation learning with variational inference relies heavily on independence assumptions over latent variables. Causal representation learning (CRL), however, argues that factors of variation in a dataset are, in fact, causally related. Allowing latent variables to be correlated, as a consequence of causal relationships, is more realistic and generalisable. So far, provably identifiable methods rely on: auxiliary information, weak labels, and interventional or even counterfactual data. Inspired by causal discovery with functional causal models, we propose a fully unsupervised representation learning method that considers a data generation process with a latent additive noise model (ANM). We encourage the latent space to follow a causal ordering via loss function based on the Hessian of the latent distribution.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#24378;&#21270;&#23398;&#20064;&#24635;&#32467;&#26381;&#21153;&#65292;&#36890;&#36807;&#20351;&#29992;&#23618;&#32423;&#20010;&#24615;&#21270;&#22522;&#20110;&#27010;&#24565;&#30340;&#24635;&#32467;&#26041;&#27861;&#65292;&#22312;&#25991;&#26412;&#25968;&#25454;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#30340;&#32972;&#26223;&#19979;&#65292;&#24110;&#21161;&#29992;&#25143;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.05696</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#24378;&#21270;&#23398;&#20064;&#24635;&#32467;&#26381;&#21153;&#65306;&#20174;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#23398;&#20064;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
A Personalized Reinforcement Learning Summarization Service for Learning Structure from Unstructured Data. (arXiv:2307.05696v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05696
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#24378;&#21270;&#23398;&#20064;&#24635;&#32467;&#26381;&#21153;&#65292;&#36890;&#36807;&#20351;&#29992;&#23618;&#32423;&#20010;&#24615;&#21270;&#22522;&#20110;&#27010;&#24565;&#30340;&#24635;&#32467;&#26041;&#27861;&#65292;&#22312;&#25991;&#26412;&#25968;&#25454;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#30340;&#32972;&#26223;&#19979;&#65292;&#24110;&#21161;&#29992;&#25143;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25968;&#25454;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#38656;&#35201;&#24037;&#20855;&#26469;&#24110;&#21161;&#29992;&#25143;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#35265;&#35299;&#12290;&#20256;&#32479;&#30340;&#25991;&#26723;&#25688;&#35201;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#28385;&#36275;&#20010;&#20154;&#29992;&#25143;&#38656;&#27714;&#65292;&#24182;&#19988;&#32570;&#20047;&#39640;&#25928;&#20449;&#24687;&#22788;&#29702;&#30340;&#32467;&#26500;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#32423;&#20010;&#24615;&#21270;&#22522;&#20110;&#27010;&#24565;&#30340;&#24635;&#32467;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#25991;&#26723;&#32508;&#21512;&#25104;&#31616;&#27905;&#30340;&#23618;&#32423;&#27010;&#24565;&#22270;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#21644;&#36866;&#24212;&#29992;&#25143;&#20559;&#22909;&#26469;&#31215;&#26497;&#21442;&#19982;&#29992;&#25143;&#12290;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#20026;&#29305;&#23450;&#20027;&#39064;&#30340;&#26410;&#35265;&#25991;&#26723;&#29983;&#25104;&#20010;&#24615;&#21270;&#25688;&#35201;&#12290;&#35813;&#26694;&#26550;&#25552;&#39640;&#20102;&#29702;&#35299;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#23548;&#33322;&#65292;&#24182;&#20351;&#29992;&#25143;&#33021;&#22815;&#26681;&#25454;&#33258;&#24049;&#29420;&#29305;&#30340;&#38656;&#27714;&#20174;&#22823;&#37327;&#25991;&#26723;&#38598;&#21512;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exponential growth of textual data has created a crucial need for tools that assist users in extracting meaningful insights. Traditional document summarization approaches often fail to meet individual user requirements and lack structure for efficient information processing. To address these limitations, we propose Summation, a hierarchical personalized concept-based summarization approach. It synthesizes documents into a concise hierarchical concept map and actively engages users by learning and adapting to their preferences. Using a Reinforcement Learning algorithm, Summation generates personalized summaries for unseen documents on specific topics. This framework enhances comprehension, enables effective navigation, and empowers users to extract meaningful insights from large document collections aligned with their unique requirements.
&lt;/p&gt;</description></item><item><title>Objaverse-XL&#26159;&#19968;&#20010;&#21253;&#21547;1&#21315;&#19975;+ 3D&#23545;&#35937;&#30340;&#26080;&#38480;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#65292;&#23427;&#35299;&#20915;&#20102;3D&#35270;&#35273;&#20219;&#21153;&#20013;&#33719;&#21462;&#39640;&#36136;&#37327;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#22823;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#25913;&#36827;&#20102;3D&#35270;&#35273;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.05663</link><description>&lt;p&gt;
Objaverse-XL&#65306;&#19968;&#20010;&#21253;&#21547;1&#21315;&#19975;+ 3D&#23545;&#35937;&#30340;&#26080;&#38480;&#19990;&#30028;
&lt;/p&gt;
&lt;p&gt;
Objaverse-XL: A Universe of 10M+ 3D Objects. (arXiv:2307.05663v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05663
&lt;/p&gt;
&lt;p&gt;
Objaverse-XL&#26159;&#19968;&#20010;&#21253;&#21547;1&#21315;&#19975;+ 3D&#23545;&#35937;&#30340;&#26080;&#38480;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#65292;&#23427;&#35299;&#20915;&#20102;3D&#35270;&#35273;&#20219;&#21153;&#20013;&#33719;&#21462;&#39640;&#36136;&#37327;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#22823;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#25913;&#36827;&#20102;3D&#35270;&#35273;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;2D&#35270;&#35273;&#27169;&#22411;&#36890;&#36807;&#25193;&#22823;&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#65292;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#32489;&#12290;&#28982;&#32780;&#65292;3D&#35270;&#35273;&#20219;&#21153;&#24182;&#27809;&#26377;&#35265;&#21040;&#21516;&#26679;&#30340;&#36827;&#23637;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;3D&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Objaverse-XL&#65292;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;1000&#19975;&#20010;3D&#23545;&#35937;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;&#26469;&#33258;&#22810;&#31181;&#26469;&#28304;&#30340;&#21435;&#37325;&#22797;&#30340;3D&#23545;&#35937;&#65292;&#21253;&#25324;&#25163;&#21160;&#35774;&#35745;&#30340;&#23545;&#35937;&#65292;&#22320;&#26631;&#21644;&#26085;&#24120;&#29289;&#21697;&#30340;&#25668;&#24433;&#27979;&#37327;&#25195;&#25551;&#65292;&#20197;&#21450;&#21382;&#21490;&#21644;&#21476;&#33891;&#25991;&#29289;&#30340;&#19987;&#19994;&#25195;&#25551;&#12290;Objaverse-XL&#20195;&#34920;&#20102;3D&#25968;&#25454;&#38598;&#39046;&#22495;&#20013;&#26368;&#22823;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#65292;&#20026;3D&#35270;&#35273;&#24102;&#26469;&#20102;&#37325;&#22823;&#30340;&#26032;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;Objaverse-XL&#25552;&#20379;&#30340;&#35268;&#27169;&#24102;&#26469;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#22312;&#26032;&#35270;&#35282;&#21512;&#25104;&#20013;&#35757;&#32451;Zero123&#65292;&#21033;&#29992;&#36229;&#36807;1&#20159;&#20010;&#22810;&#35270;&#35282;&#28210;&#26579;&#22270;&#20687;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language processing and 2D vision models have attained remarkable proficiency on many tasks primarily by escalating the scale of training data. However, 3D vision tasks have not seen the same progress, in part due to the challenges of acquiring high-quality 3D data. In this work, we present Objaverse-XL, a dataset of over 10 million 3D objects. Our dataset comprises deduplicated 3D objects from a diverse set of sources, including manually designed objects, photogrammetry scans of landmarks and everyday items, and professional scans of historic and antique artifacts. Representing the largest scale and diversity in the realm of 3D datasets, Objaverse-XL enables significant new possibilities for 3D vision. Our experiments demonstrate the improvements enabled with the scale provided by Objaverse-XL. We show that by training Zero123 on novel view synthesis, utilizing over 100 million multi-view rendered images, we achieve strong zero-shot generalization abilities. We hope that relea
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#20248;&#21270;&#22810;&#30446;&#26631;&#27700;&#30005;&#31449;&#27700;&#24211;&#35843;&#24230;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#30456;&#36739;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#36816;&#34892;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.05643</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#22810;&#30446;&#26631;&#27700;&#30005;&#31449;&#27700;&#24211;&#35843;&#24230;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multiobjective Hydropower Reservoir Operation Optimization with Transformer-Based Deep Reinforcement Learning. (arXiv:2307.05643v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05643
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#20248;&#21270;&#22810;&#30446;&#26631;&#27700;&#30005;&#31449;&#27700;&#24211;&#35843;&#24230;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#30456;&#36739;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#36816;&#34892;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#27700;&#36164;&#28304;&#30701;&#32570;&#21644;&#27700;&#38656;&#27714;&#22686;&#21152;&#65292;&#22810;&#27700;&#24211;&#31995;&#32479;&#30340;&#32852;&#21512;&#36816;&#34892;&#20197;&#24179;&#34913;&#21457;&#30005;&#12289;&#29983;&#24577;&#20445;&#25252;&#21644;&#23621;&#27665;&#29992;&#27700;&#20379;&#24212;&#24050;&#25104;&#20026;&#27700;&#30005;&#31649;&#29702;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22810;&#20010;&#27700;&#24211;&#30340;&#20247;&#22810;&#32422;&#26463;&#21644;&#38750;&#32447;&#24615;&#24615;&#20351;&#24471;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#38750;&#24120;&#32791;&#26102;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;Transformer&#26694;&#26550;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#32534;&#30721;&#22120;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#26377;&#25928;&#22320;&#20174;&#27700;&#24211;&#21644;&#23621;&#27665;&#21306;&#25552;&#21462;&#20449;&#24687;&#65292;&#35299;&#30721;&#22120;&#30340;&#22810;&#27700;&#24211;&#27880;&#24847;&#32593;&#32476;&#29983;&#25104;&#36866;&#24403;&#30340;&#36816;&#34892;&#20915;&#31574;&#12290;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#31185;&#32599;&#25289;&#22810;&#27827;&#27969;&#22495;&#30340;&#26757;&#24503;&#28246;&#21644;&#40077;&#23041;&#23572;&#28246;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#21512;&#36866;&#30340;&#36816;&#33829;&#32467;&#26524;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#30340;&#36816;&#34892;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to shortage of water resources and increasing water demands, the joint operation of multireservoir systems for balancing power generation, ecological protection, and the residential water supply has become a critical issue in hydropower management. However, the numerous constraints and nonlinearity of multiple reservoirs make solving this problem time-consuming. To address this challenge, a deep reinforcement learning approach that incorporates a transformer framework is proposed. The multihead attention mechanism of the encoder effectively extracts information from reservoirs and residential areas, and the multireservoir attention network of the decoder generates suitable operational decisions. The proposed method is applied to Lake Mead and Lake Powell in the Colorado River Basin. The experimental results demonstrate that the transformer-based deep reinforcement learning approach can produce appropriate operational outcomes. Compared to a state-of-the-art method, the operation st
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340;&#24452;&#21521;&#22522;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#31934;&#24230;&#30697;&#38453;&#65292;&#20174;&#35757;&#32451;&#23436;&#25104;&#21518;&#30340;&#27169;&#22411;&#20013;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#65292;&#21253;&#25324;&#27963;&#36291;&#23376;&#31354;&#38388;&#30340;&#26041;&#21521;&#21644;&#36755;&#20837;&#21464;&#37327;&#37325;&#35201;&#24615;&#30340;&#25490;&#24207;&#12290;</title><link>http://arxiv.org/abs/2307.05639</link><description>&lt;p&gt;
&#20351;&#29992;&#39640;&#26031;&#24452;&#21521;&#22522;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#27963;&#36291;&#23376;&#31354;&#38388;&#24182;&#21457;&#29616;&#37325;&#35201;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Learning Active Subspaces and Discovering Important Features with Gaussian Radial Basis Functions Neural Networks. (arXiv:2307.05639v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340;&#24452;&#21521;&#22522;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#31934;&#24230;&#30697;&#38453;&#65292;&#20174;&#35757;&#32451;&#23436;&#25104;&#21518;&#30340;&#27169;&#22411;&#20013;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#65292;&#21253;&#25324;&#27963;&#36291;&#23376;&#31354;&#38388;&#30340;&#26041;&#21521;&#21644;&#36755;&#20837;&#21464;&#37327;&#37325;&#35201;&#24615;&#30340;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#19968;&#20010;&#26082;&#33021;&#36798;&#21040;&#24378;&#22823;&#39044;&#27979;&#24615;&#33021;&#65292;&#21448;&#33021;&#34987;&#20154;&#31867;&#35299;&#37322;&#30340;&#27169;&#22411;&#26159;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#26368;&#22256;&#38590;&#30340;&#25361;&#25112;&#20043;&#19968;&#65292;&#30001;&#20110;&#36825;&#20004;&#20010;&#30446;&#26631;&#30340;&#20914;&#31361;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340;&#24452;&#21521;&#22522;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#20026;&#20854;&#39640;&#26031;&#26680;&#28155;&#21152;&#21487;&#23398;&#20064;&#30340;&#31934;&#24230;&#30697;&#38453;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35757;&#32451;&#23436;&#25104;&#21518;&#21487;&#20197;&#20174;&#31934;&#24230;&#30697;&#38453;&#30340;&#35889;&#20013;&#25552;&#21462;&#23453;&#36149;&#30340;&#20449;&#24687;&#12290;&#29305;&#21035;&#26159;&#65292;&#29305;&#24449;&#21521;&#37327;&#35299;&#37322;&#20102;&#27169;&#22411;&#26368;&#25935;&#24863;&#30340;&#26041;&#21521;&#65292;&#25581;&#31034;&#20102;&#27963;&#36291;&#23376;&#31354;&#38388;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#30417;&#30563;&#38477;&#32500;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;&#21516;&#26102;&#65292;&#29305;&#24449;&#21521;&#37327;&#20984;&#26174;&#20102;&#36755;&#20837;&#21644;&#28508;&#22312;&#21464;&#37327;&#20043;&#38388;&#30340;&#32477;&#23545;&#21464;&#21270;&#20851;&#31995;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#22522;&#20110;&#20854;&#23545;&#39044;&#27979;&#30340;&#37325;&#35201;&#24615;&#25552;&#21462;&#36755;&#20837;&#21464;&#37327;&#30340;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Providing a model that achieves a strong predictive performance and at the same time is interpretable by humans is one of the most difficult challenges in machine learning research due to the conflicting nature of these two objectives. To address this challenge, we propose a modification of the Radial Basis Function Neural Network model by equipping its Gaussian kernel with a learnable precision matrix. We show that precious information is contained in the spectrum of the precision matrix that can be extracted once the training of the model is completed. In particular, the eigenvectors explain the directions of maximum sensitivity of the model revealing the active subspace and suggesting potential applications for supervised dimensionality reduction. At the same time, the eigenvectors highlight the relationship in terms of absolute variation between the input and the latent variables, thereby allowing us to extract a ranking of the input variables based on their importance to the predi
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#22312;&#24037;&#19994;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#20351;&#29992;&#12290;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#20219;&#21153;&#30340;&#30693;&#35782;&#21644;&#32771;&#34385;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#65292;&#35299;&#20915;&#20102;&#20165;&#26377;&#23569;&#37327;&#25110;&#27809;&#26377;&#38468;&#21152;&#26631;&#35760;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#26032;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.05638</link><description>&lt;p&gt;
&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#22312;&#24037;&#19994;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#32508;&#21512;&#35843;&#26597;&#65306;&#26041;&#27861;&#12289;&#24212;&#29992;&#21644;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey of Deep Transfer Learning for Anomaly Detection in Industrial Time Series: Methods, Applications, and Directions. (arXiv:2307.05638v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#22312;&#24037;&#19994;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#20351;&#29992;&#12290;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#20219;&#21153;&#30340;&#30693;&#35782;&#21644;&#32771;&#34385;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#65292;&#35299;&#20915;&#20102;&#20165;&#26377;&#23569;&#37327;&#25110;&#27809;&#26377;&#38468;&#21152;&#26631;&#35760;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#30417;&#27979;&#24037;&#19994;&#36807;&#31243;&#26377;&#28508;&#21147;&#36890;&#36807;&#21450;&#26102;&#26816;&#27979;&#24322;&#24120;&#20107;&#20214;&#24182;&#20419;&#36827;&#21450;&#26102;&#24178;&#39044;&#26469;&#25552;&#39640;&#25928;&#29575;&#21644;&#20248;&#21270;&#36136;&#37327;&#12290;&#28145;&#24230;&#23398;&#20064;&#36890;&#36807;&#35782;&#21035;&#22823;&#25968;&#25454;&#38598;&#20013;&#30340;&#38750;&#24179;&#20961;&#27169;&#24335;&#65292;&#22312;&#36825;&#19968;&#36807;&#31243;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26631;&#20934;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36866;&#29992;&#20110;&#35299;&#20915;&#29305;&#23450;&#31867;&#22411;&#30340;&#25968;&#25454;&#32473;&#23450;&#29305;&#23450;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#36825;&#20123;&#31639;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24037;&#33402;&#21644;&#29615;&#22659;&#30340;&#21160;&#24577;&#24615;&#65292;&#20026;&#27599;&#20010;&#31245;&#26377;&#19981;&#21516;&#30340;&#24773;&#20917;&#37325;&#26032;&#33719;&#24471;&#25152;&#38656;&#25968;&#25454;&#36827;&#34892;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#25552;&#20379;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#20219;&#21153;&#30340;&#30693;&#35782;&#21644;&#32771;&#34385;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#65292;&#36825;&#20010;&#23398;&#20064;&#26694;&#26550;&#21487;&#20197;&#35299;&#20915;&#26032;&#20219;&#21153;&#65292;&#21363;&#20351;&#27809;&#26377;&#25110;&#21482;&#26377;&#24456;&#23569;&#30340;&#38468;&#21152;&#26631;&#35760;&#25968;&#25454;&#12290;&#36825;&#31181;&#26041;&#27861;&#36991;&#20813;&#20102;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automating the monitoring of industrial processes has the potential to enhance efficiency and optimize quality by promptly detecting abnormal events and thus facilitating timely interventions. Deep learning, with its capacity to discern non-trivial patterns within large datasets, plays a pivotal role in this process. Standard deep learning methods are suitable to solve a specific task given a specific type of data. During training, the algorithms demand large volumes of labeled training data. However, due to the dynamic nature of processes and the environment, it is impractical to acquire the needed data for standard deep learning training for every slightly different case anew. Deep transfer learning offers a solution to this problem. By leveraging knowledge from related tasks and accounting for variations in data distributions, this learning framework solves new tasks even with little or no additional labeled data. The approach bypasses the need to retrain a model from scratch for ev
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#30456;&#23545;&#38382;&#39064;&#12289;&#27010;&#29575;&#30340;&#20449;&#24565;&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#20449;&#24565;&#21160;&#24577;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#35813;&#27169;&#22411;&#25152;&#39564;&#35777;&#30340;&#21407;&#21017;&#27604;&#20256;&#32479;&#30340;&#20449;&#24565;&#20462;&#27491;&#29702;&#35770;&#35201;&#24369;&#65292;&#20294;&#27604;&#19982;&#39640;&#20027;&#35266;&#27010;&#29575;&#31561;&#21516;&#30340;Lockean&#20449;&#24565;&#29702;&#35770;&#35201;&#24378;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#20123;&#36866;&#29992;&#20110;&#29305;&#23450;&#31867;&#21035;&#27169;&#22411;&#30340;&#33258;&#28982;&#21407;&#21017;&#65292;&#24182;&#19982;&#20854;&#20182;&#27010;&#29575;&#20449;&#24565;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2307.05632</link><description>&lt;p&gt;
&#20174;&#27010;&#29575;&#35770;&#21040;&#32622;&#20449;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
Belief Revision from Probability. (arXiv:2307.05632v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05632
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#30456;&#23545;&#38382;&#39064;&#12289;&#27010;&#29575;&#30340;&#20449;&#24565;&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#20449;&#24565;&#21160;&#24577;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#35813;&#27169;&#22411;&#25152;&#39564;&#35777;&#30340;&#21407;&#21017;&#27604;&#20256;&#32479;&#30340;&#20449;&#24565;&#20462;&#27491;&#29702;&#35770;&#35201;&#24369;&#65292;&#20294;&#27604;&#19982;&#39640;&#20027;&#35266;&#27010;&#29575;&#31561;&#21516;&#30340;Lockean&#20449;&#24565;&#29702;&#35770;&#35201;&#24378;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#20123;&#36866;&#29992;&#20110;&#29305;&#23450;&#31867;&#21035;&#27169;&#22411;&#30340;&#33258;&#28982;&#21407;&#21017;&#65292;&#24182;&#19982;&#20854;&#20182;&#27010;&#29575;&#20449;&#24565;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20043;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#8220;&#26469;&#33258;&#27010;&#29575;&#30340;&#30693;&#35782;&#8221;&#65288;TARK 2021&#65289;&#24320;&#21457;&#20102;&#19968;&#20010;&#30456;&#23545;&#38382;&#39064;&#12289;&#27010;&#29575;&#30340;&#20449;&#24565;&#27169;&#22411;&#12290;&#26681;&#25454;&#36825;&#20010;&#27169;&#22411;&#65292;&#30456;&#23545;&#20110;&#32473;&#23450;&#38382;&#39064;&#65292;&#19968;&#20010;&#20154;&#30456;&#20449;&#30340;&#20869;&#23481;&#24212;&#35813;&#65288;i&#65289;&#22312;&#34164;&#28085;&#36816;&#31639;&#19979;&#20445;&#25345;&#31283;&#23450;&#65292;&#65288;ii&#65289;&#22312;&#20182;&#20204;&#30340;&#35777;&#25454;&#19979;&#20855;&#26377;&#36275;&#22815;&#30340;&#27010;&#29575;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#23545;&#38382;&#39064;&#30340;&#31572;&#26696;&#30340;&#30456;&#23545;&#27010;&#29575;&#25935;&#24863;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#20010;&#27169;&#22411;&#23545;&#20449;&#24565;&#21160;&#24577;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#25152;&#39564;&#35777;&#30340;&#21407;&#21017;&#35201;&#27604;AGM&#31561;&#20256;&#32479;&#20449;&#24565;&#20462;&#27491;&#29702;&#35770;&#30340;&#21407;&#21017;&#35201;&#24369;&#65292;&#20294;&#20173;&#28982;&#27604;&#19982;&#39640;&#20027;&#35266;&#27010;&#29575;&#31561;&#21516;&#30340;&#27969;&#34892;&#30340;Lockean&#20449;&#24565;&#29702;&#35770;&#35201;&#24378;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31867;&#36866;&#29992;&#20110;&#35768;&#22810;&#20294;&#19981;&#36866;&#29992;&#20110;&#25152;&#26377;&#24212;&#29992;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#30830;&#23450;&#20102;&#22312;&#36825;&#19968;&#31867;&#27169;&#22411;&#19978;&#36824;&#26377;&#25928;&#30340;&#19968;&#20123;&#33258;&#28982;&#21407;&#21017;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;Leitgeb&#21644;Lin&#21644;&#30340;&#27010;&#29575;&#20449;&#24565;&#27169;&#22411;&#65292;&#24471;&#20986;&#20102;&#36825;&#20010;&#29616;&#26377;&#26694;&#26550;&#20855;&#26377;&#27604;&#36739;&#20248;&#21183;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
In previous work ("Knowledge from Probability", TARK 2021) we develop a question-relative, probabilistic account of belief. On this account, what someone believes relative to a given question is (i) closed under entailment, (ii) sufficiently probable given their evidence, and (iii) sensitive to the relative probabilities of the answers to the question. Here we explore the implications of this account for the dynamics of belief. We show that the principles it validates are much weaker than those of orthodox theories of belief revision like AGM, but still stronger than those valid according to the popular Lockean theory of belief, which equates belief with high subjective probability. We then consider a restricted class of models, suitable for many but not all applications, and identify some further natural principles valid on this class. We conclude by arguing that the present framework compares favorably to the rival probabilistic accounts of belief developed by Leitgeb and by Lin and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;Halpern&#21644;Pearl&#30340;&#23454;&#38469;&#22240;&#26524;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#24102;&#26377;&#27169;&#24577;&#36816;&#31639;&#31526;&#30340;&#22240;&#26524;&#36923;&#36753;&#65292;&#21487;&#20197;&#25512;&#29702;&#28041;&#21450;&#22810;&#31181;&#21487;&#33021;&#24615;&#12289;&#26102;&#38388;&#24615;&#12289;&#30693;&#35782;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2307.05631</link><description>&lt;p&gt;
&#22240;&#26524;&#20811;&#37324;&#26222;&#20811;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Causal Kripke Models. (arXiv:2307.05631v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;Halpern&#21644;Pearl&#30340;&#23454;&#38469;&#22240;&#26524;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#24102;&#26377;&#27169;&#24577;&#36816;&#31639;&#31526;&#30340;&#22240;&#26524;&#36923;&#36753;&#65292;&#21487;&#20197;&#25512;&#29702;&#28041;&#21450;&#22810;&#31181;&#21487;&#33021;&#24615;&#12289;&#26102;&#38388;&#24615;&#12289;&#30693;&#35782;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;Halpern&#21644;Pearl&#30340;&#23454;&#38469;&#22240;&#26524;&#27169;&#22411;&#25193;&#23637;&#21040;&#21487;&#33021;&#19990;&#30028;&#30340;&#35821;&#20041;&#29615;&#22659;&#20013;&#12290;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24102;&#26377;&#27169;&#24577;&#36816;&#31639;&#31526;&#30340;&#23454;&#38469;&#22240;&#26524;&#36923;&#36753;&#65292;&#21487;&#20197;&#25512;&#29702;&#28041;&#21450;&#22810;&#31181;&#21487;&#33021;&#24615;&#12289;&#26102;&#38388;&#24615;&#12289;&#30693;&#35782;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20123;&#20363;&#23376;&#26469;&#35828;&#26126;&#36825;&#19968;&#28857;&#65292;&#24182;&#35752;&#35770;&#20102;&#19968;&#20123;&#20197;&#21518;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work extends Halpern and Pearl's causal models for actual causality to a possible world semantics environment. Using this framework we introduce a logic of actual causality with modal operators, which allows for reasoning about causality in scenarios involving multiple possibilities, temporality, knowledge and uncertainty. We illustrate this with a number of examples, and conclude by discussing some future directions for research.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;Kripke&#20449;&#24565;&#20851;&#31995;&#21644;Stalnaker-Lewis&#36873;&#25321;&#20989;&#25968;&#30340;&#26694;&#26550;&#65292;&#23545;AGM&#20449;&#24565;&#25910;&#32553;&#36827;&#34892;&#20102;&#35821;&#20041;&#34920;&#24449;&#65292;&#20854;&#20013;&#20851;&#38190;&#28857;&#26159;&#21021;&#22987;&#20449;&#24565;&#38598;&#21512;&#25910;&#32553;&#21518;&#30340;&#25104;&#21592;&#28385;&#36275;&#22312;&#23454;&#38469;&#29366;&#24577;&#19979;&#30340;&#30456;&#20449;&#21644;&#26465;&#20214;&#21028;&#26029;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2307.05629</link><description>&lt;p&gt;
AGM&#20449;&#24565;&#25910;&#32553;&#30340;&#26465;&#20214;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Characterization of AGM Belief Contraction in Terms of Conditionals. (arXiv:2307.05629v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05629
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;Kripke&#20449;&#24565;&#20851;&#31995;&#21644;Stalnaker-Lewis&#36873;&#25321;&#20989;&#25968;&#30340;&#26694;&#26550;&#65292;&#23545;AGM&#20449;&#24565;&#25910;&#32553;&#36827;&#34892;&#20102;&#35821;&#20041;&#34920;&#24449;&#65292;&#20854;&#20013;&#20851;&#38190;&#28857;&#26159;&#21021;&#22987;&#20449;&#24565;&#38598;&#21512;&#25910;&#32553;&#21518;&#30340;&#25104;&#21592;&#28385;&#36275;&#22312;&#23454;&#38469;&#29366;&#24577;&#19979;&#30340;&#30456;&#20449;&#21644;&#26465;&#20214;&#21028;&#26029;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22522;&#20110;&#30001;Kripke&#20449;&#24565;&#20851;&#31995;&#21644;Stalnaker-Lewis&#36873;&#25321;&#20989;&#25968;&#32452;&#25104;&#30340;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#23545;AGM&#20449;&#24565;&#25910;&#32553;&#30340;&#35821;&#20041;&#34920;&#24449;&#12290;&#26680;&#24515;&#24605;&#24819;&#26159;&#65306;&#35774;K&#20026;&#21021;&#22987;&#20449;&#24565;&#38598;&#21512;&#65292;K-A&#20026;&#36890;&#36807;&#20844;&#24335;A&#25910;&#32553;K&#24471;&#21040;&#30340;&#38598;&#21512;&#65307;&#21017;B&#23646;&#20110;&#38598;&#21512;K-A&#24403;&#19988;&#20165;&#24403;&#65292;&#22312;&#23454;&#38469;&#29366;&#24577;&#19979;&#65292;&#20195;&#29702;&#20154;&#30456;&#20449;B&#65292;&#24182;&#19988;&#30456;&#20449;&#22914;&#26524;&#38750;A&#26159;&#65288;&#23558;&#35201;&#65289;&#25104;&#31435;&#65292;&#21017;B&#26159;&#65288;&#23558;&#35201;&#65289;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a semantic characterization of AGM belief contraction based on frames consisting of a Kripke belief relation and a Stalnaker-Lewis selection function. The central idea is as follows. Let K be the initial belief set and K-A be the contraction of K by the formula A; then B belongs to the set K-A if and only if, at the actual state, the agent believes B and believes that if not-A is (were) the case then B is (would be) the case.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#21551;&#21457;&#23398;&#20064;&#26694;&#26550;&#65288;CILF&#65289;&#65292;&#36890;&#36807;&#26126;&#30830;&#23450;&#20041;&#25968;&#25454;&#30340;&#28508;&#22312;&#22240;&#26524;&#32467;&#26500;&#65292;&#24182;&#21033;&#29992;&#22240;&#26524;&#29305;&#24449;&#36827;&#34892;&#39044;&#27979;&#26469;&#35299;&#20915;&#36712;&#36857;&#39044;&#27979;&#20013;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.05624</link><description>&lt;p&gt;
CILF:&#22240;&#26524;&#21551;&#21457;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#36229;&#20986;&#20998;&#24067;&#30340;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
CILF:Causality Inspired Learning Framework for Out-of-Distribution Vehicle Trajectory Prediction. (arXiv:2307.05624v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05624
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#21551;&#21457;&#23398;&#20064;&#26694;&#26550;&#65288;CILF&#65289;&#65292;&#36890;&#36807;&#26126;&#30830;&#23450;&#20041;&#25968;&#25454;&#30340;&#28508;&#22312;&#22240;&#26524;&#32467;&#26500;&#65292;&#24182;&#21033;&#29992;&#22240;&#26524;&#29305;&#24449;&#36827;&#34892;&#39044;&#27979;&#26469;&#35299;&#20915;&#36712;&#36857;&#39044;&#27979;&#20013;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36712;&#36857;&#39044;&#27979;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#20542;&#21521;&#20110;&#24314;&#27169;&#21382;&#21490;&#36712;&#36857;&#65288;&#36755;&#20837;&#65289;&#19982;&#26410;&#26469;&#36712;&#36857;&#65288;&#36755;&#20986;&#65289;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#30001;&#20110;&#30456;&#20851;&#24615;&#21482;&#26159;&#23545;&#29616;&#23454;&#30340;&#19968;&#31181;&#34920;&#38754;&#25551;&#36848;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#20551;&#35774;&#65292;&#24182;&#23545;&#36229;&#20986;&#20998;&#24067;&#30340;&#25968;&#25454;&#34920;&#29616;&#20986;&#39640;&#24230;&#30340;&#25935;&#24863;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36229;&#20986;&#20998;&#24067;&#22240;&#26524;&#22270;&#65288;OOD-CG&#65289;&#65292;&#23427;&#26126;&#30830;&#22320;&#23450;&#20041;&#20102;&#25968;&#25454;&#30340;&#28508;&#22312;&#22240;&#26524;&#32467;&#26500;&#65292;&#21253;&#25324;&#19977;&#20010;&#32416;&#32544;&#30340;&#28508;&#22312;&#29305;&#24449;&#65306;1&#65289;&#39046;&#22495;&#19981;&#21464;&#30340;&#22240;&#26524;&#29305;&#24449;&#65288;IC&#65289;&#65292;2&#65289;&#39046;&#22495;&#21464;&#37327;&#30340;&#22240;&#26524;&#29305;&#24449;&#65288;VC&#65289;&#65292;3&#65289;&#39046;&#22495;&#21464;&#37327;&#30340;&#38750;&#22240;&#26524;&#29305;&#24449;&#65288;VN&#65289;&#12290;&#36825;&#20123;&#29305;&#24449;&#21463;&#21040;&#28151;&#28102;&#22240;&#23376;&#65288;C&#65289;&#21644;&#39046;&#22495;&#36873;&#25321;&#22120;&#65288;D&#65289;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#21033;&#29992;&#22240;&#26524;&#29305;&#24449;&#36827;&#34892;&#39044;&#27979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#21551;&#21457;&#23398;&#20064;&#26694;&#26550;&#65288;CILF&#65289;&#65292;&#23427;&#21253;&#25324;&#19977;&#20010;&#27493;&#39588;&#65306;1&#65289;&#36890;&#36807;&#19981;&#21464;&#24615;&#25439;&#22833;&#25552;&#21462;&#39046;&#22495;&#19981;&#21464;&#30340;&#22240;&#26524;&#29305;&#24449;&#65292;2&#65289;&#36890;&#36807;&#22240;&#26524;&#24615;&#25439;&#22833;&#25552;&#21462;&#39046;&#22495;&#21464;&#37327;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Trajectory prediction is critical for autonomous driving vehicles. Most existing methods tend to model the correlation between history trajectory (input) and future trajectory (output). Since correlation is just a superficial description of reality, these methods rely heavily on the i.i.d. assumption and evince a heightened susceptibility to out-of-distribution data. To address this problem, we propose an Out-of- Distribution Causal Graph (OOD-CG), which explicitly defines the underlying causal structure of the data with three entangled latent features: 1) domain-invariant causal feature (IC), 2) domain-variant causal feature (VC), and 3) domain-variant non-causal feature (VN ). While these features are confounded by confounder (C) and domain selector (D). To leverage causal features for prediction, we propose a Causal Inspired Learning Framework (CILF), which includes three steps: 1) extracting domain-invariant causal feature by means of an invariance loss, 2) extracting domain varian
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#26041;&#27861;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25512;&#26029;OD&#24207;&#21015;&#30340;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#32467;&#26500;&#32422;&#26463;&#25351;&#23548;&#20256;&#32479;&#30340;&#25968;&#20540;&#20248;&#21270;&#65292;&#35299;&#20915;&#20102;&#20132;&#36890;&#39046;&#22495;&#20013;&#38745;&#24577;&#21644;&#21160;&#24577;OD&#30697;&#38453;&#20272;&#35745;&#20013;&#30340;&#27424;&#23450;&#21644;&#28382;&#21518;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.05623</link><description>&lt;p&gt;
&#29992;&#20110;&#21160;&#24577;&#20272;&#35745;&#20986;&#21457;&#22320;-&#30446;&#30340;&#22320;&#24207;&#21015;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A DeepLearning Framework for Dynamic Estimation of Origin-Destination Sequence. (arXiv:2307.05623v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#26041;&#27861;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25512;&#26029;OD&#24207;&#21015;&#30340;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#32467;&#26500;&#32422;&#26463;&#25351;&#23548;&#20256;&#32479;&#30340;&#25968;&#20540;&#20248;&#21270;&#65292;&#35299;&#20915;&#20102;&#20132;&#36890;&#39046;&#22495;&#20013;&#38745;&#24577;&#21644;&#21160;&#24577;OD&#30697;&#38453;&#20272;&#35745;&#20013;&#30340;&#27424;&#23450;&#21644;&#28382;&#21518;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
OD&#30697;&#38453;&#20272;&#35745;&#26159;&#20132;&#36890;&#39046;&#22495;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#20027;&#35201;&#26041;&#27861;&#20351;&#29992;&#20132;&#36890;&#20256;&#24863;&#22120;&#27979;&#37327;&#20449;&#24687;&#65288;&#22914;&#20132;&#36890;&#27969;&#37327;&#65289;&#26469;&#20272;&#35745;&#30001;OD&#30697;&#38453;&#34920;&#31034;&#30340;&#20132;&#36890;&#38656;&#27714;&#12290;&#35813;&#38382;&#39064;&#20998;&#20026;&#38745;&#24577;OD&#30697;&#38453;&#20272;&#35745;&#21644;&#21160;&#24577;OD&#30697;&#38453;&#24207;&#21015;&#65288;&#31616;&#31216;OD&#24207;&#21015;&#65289;&#20272;&#35745;&#20004;&#31867;&#12290;&#19978;&#36848;&#20004;&#31181;&#26041;&#27861;&#38754;&#20020;&#30001;&#20110;&#22823;&#37327;&#20272;&#35745;&#21442;&#25968;&#21644;&#19981;&#36275;&#30340;&#32422;&#26463;&#20449;&#24687;&#36896;&#25104;&#30340;&#27424;&#23450;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;OD&#24207;&#21015;&#20272;&#35745;&#36824;&#38754;&#20020;&#28382;&#21518;&#25361;&#25112;&#65306;&#30001;&#20110;&#25317;&#22581;&#31561;&#19981;&#21516;&#20132;&#36890;&#26465;&#20214;&#65292;&#30456;&#21516;&#30340;&#36710;&#36742;&#22312;&#21516;&#19968;&#35266;&#27979;&#26102;&#27573;&#20869;&#20250;&#20986;&#29616;&#22312;&#19981;&#21516;&#30340;&#36335;&#27573;&#19978;&#65292;&#23548;&#33268;&#30456;&#21516;&#30340;OD&#38656;&#27714;&#23545;&#24212;&#19981;&#21516;&#30340;&#34892;&#31243;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25512;&#26029;OD&#24207;&#21015;&#30340;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#32467;&#26500;&#32422;&#26463;&#25351;&#23548;&#20256;&#32479;&#30340;&#25968;&#20540;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26174;&#31034;...
&lt;/p&gt;
&lt;p&gt;
OD matrix estimation is a critical problem in the transportation domain. The principle method uses the traffic sensor measured information such as traffic counts to estimate the traffic demand represented by the OD matrix. The problem is divided into two categories: static OD matrix estimation and dynamic OD matrices sequence(OD sequence for short) estimation. The above two face the underdetermination problem caused by abundant estimated parameters and insufficient constraint information. In addition, OD sequence estimation also faces the lag challenge: due to different traffic conditions such as congestion, identical vehicle will appear on different road sections during the same observation period, resulting in identical OD demands correspond to different trips. To this end, this paper proposes an integrated method, which uses deep learning methods to infer the structure of OD sequence and uses structural constraints to guide traditional numerical optimization. Our experiments show th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29305;&#24449;&#32534;&#30721;&#25216;&#26415;&#23545;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#30340;&#24433;&#21709;&#12290;&#20351;&#29992;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#27604;&#36739;&#20102;&#26631;&#31614;&#32534;&#30721;&#21644;&#29420;&#28909;&#32534;&#30721;&#30340;&#24615;&#33021;&#12290;&#21457;&#29616;&#34429;&#28982;&#29420;&#28909;&#32534;&#30721;&#20250;&#24102;&#26469;&#36731;&#24494;&#24615;&#33021;&#25439;&#22833;&#65292;&#20294;&#21487;&#20197;&#25552;&#20379;&#26356;&#35814;&#32454;&#30340;&#35299;&#37322;&#65292;&#24110;&#21161;&#26356;&#20840;&#38754;&#22320;&#29702;&#35299;&#12290;&#20351;&#29992;&#29420;&#28909;&#32534;&#30721;&#36824;&#21487;&#20197;&#20943;&#23567;&#35299;&#37322;&#25991;&#20214;&#22823;&#23567;&#65292;&#32553;&#30701;&#20154;&#24037;&#20998;&#26512;&#26102;&#38388;&#12290;&#36825;&#20123;&#32467;&#26524;&#24378;&#35843;&#20102;&#22312;XAI&#30740;&#31350;&#20013;&#37325;&#35270;&#29305;&#24449;&#32534;&#30721;&#25216;&#26415;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#25506;&#32034;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05614</link><description>&lt;p&gt;
&#29305;&#24449;&#32534;&#30721;&#23545;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#35299;&#37322;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of Feature Encoding on Malware Classification Explainability. (arXiv:2307.05614v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29305;&#24449;&#32534;&#30721;&#25216;&#26415;&#23545;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#30340;&#24433;&#21709;&#12290;&#20351;&#29992;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#27604;&#36739;&#20102;&#26631;&#31614;&#32534;&#30721;&#21644;&#29420;&#28909;&#32534;&#30721;&#30340;&#24615;&#33021;&#12290;&#21457;&#29616;&#34429;&#28982;&#29420;&#28909;&#32534;&#30721;&#20250;&#24102;&#26469;&#36731;&#24494;&#24615;&#33021;&#25439;&#22833;&#65292;&#20294;&#21487;&#20197;&#25552;&#20379;&#26356;&#35814;&#32454;&#30340;&#35299;&#37322;&#65292;&#24110;&#21161;&#26356;&#20840;&#38754;&#22320;&#29702;&#35299;&#12290;&#20351;&#29992;&#29420;&#28909;&#32534;&#30721;&#36824;&#21487;&#20197;&#20943;&#23567;&#35299;&#37322;&#25991;&#20214;&#22823;&#23567;&#65292;&#32553;&#30701;&#20154;&#24037;&#20998;&#26512;&#26102;&#38388;&#12290;&#36825;&#20123;&#32467;&#26524;&#24378;&#35843;&#20102;&#22312;XAI&#30740;&#31350;&#20013;&#37325;&#35270;&#29305;&#24449;&#32534;&#30721;&#25216;&#26415;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#25506;&#32034;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#29305;&#24449;&#32534;&#30721;&#25216;&#26415;&#23545;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#65288;XAI&#65289;&#30340;&#35299;&#37322;&#24615;&#30340;&#24433;&#21709;&#12290;&#20351;&#29992;&#19968;&#20010;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;XGBoost&#27169;&#22411;&#65292;&#24182;&#27604;&#36739;&#20102;&#20004;&#31181;&#29305;&#24449;&#32534;&#30721;&#26041;&#27861;&#65306;&#26631;&#31614;&#32534;&#30721;&#65288;LE&#65289;&#21644;&#29420;&#28909;&#32534;&#30721;&#65288;OHE&#65289;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#20351;&#29992;OHE&#30456;&#27604;LE&#20250;&#24102;&#26469;&#36731;&#24494;&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;&#28982;&#32780;&#65292;OHE&#25152;&#25552;&#20379;&#30340;&#26356;&#35814;&#32454;&#30340;&#35299;&#37322;&#24357;&#34917;&#20102;&#36825;&#31181;&#25439;&#22833;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;OHE&#33021;&#22815;&#26356;&#35814;&#32454;&#22320;&#25506;&#32034;&#20840;&#23616;&#21644;&#23616;&#37096;&#19978;&#30340;&#32454;&#33410;&#65292;&#25552;&#20379;&#26356;&#20840;&#38754;&#30340;&#31572;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#20351;&#29992;OHE&#20250;&#23548;&#33268;&#36739;&#23567;&#30340;&#35299;&#37322;&#25991;&#20214;&#21644;&#20943;&#23569;&#20154;&#24037;&#20998;&#26512;&#30340;&#26102;&#38388;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;XAI&#30740;&#31350;&#20013;&#32771;&#34385;&#29305;&#24449;&#32534;&#30721;&#25216;&#26415;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#24314;&#35758;&#36890;&#36807;&#32467;&#21512;&#20854;&#20182;&#32534;&#30721;&#26041;&#27861;&#21644;&#21019;&#26032;&#30340;&#21487;&#35270;&#21270;&#26041;&#27861;&#36827;&#34892;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the impact of feature encoding techniques on the explainability of XAI (Explainable Artificial Intelligence) algorithms. Using a malware classification dataset, we trained an XGBoost model and compared the performance of two feature encoding methods: Label Encoding (LE) and One Hot Encoding (OHE). Our findings reveal a marginal performance loss when using OHE instead of LE. However, the more detailed explanations provided by OHE compensated for this loss. We observed that OHE enables deeper exploration of details in both global and local contexts, facilitating more comprehensive answers. Additionally, we observed that using OHE resulted in smaller explanation files and reduced analysis time for human analysts. These findings emphasize the significance of considering feature encoding techniques in XAI research and suggest potential for further exploration by incorporating additional encoding methods and innovative visualization approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#20687;&#23884;&#20837;&#20013;&#30340;&#38750;&#35821;&#20041;&#20449;&#24687;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#31995;&#32479;&#30340;&#36716;&#25442;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616;&#20845;&#20010;&#23884;&#20837;&#21487;&#20197;&#35782;&#21035;&#20986;&#22810;&#20010;&#36716;&#25442;&#12290;&#36825;&#23545;&#20110;&#35757;&#32451;&#31639;&#27861;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.05610</link><description>&lt;p&gt;
&#29289;&#36136;&#36824;&#26159;&#39118;&#26684;&#65306;&#20320;&#30340;&#22270;&#20687;&#23884;&#20837;&#30693;&#36947;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
Substance or Style: What Does Your Image Embedding Know?. (arXiv:2307.05610v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#20687;&#23884;&#20837;&#20013;&#30340;&#38750;&#35821;&#20041;&#20449;&#24687;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#31995;&#32479;&#30340;&#36716;&#25442;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616;&#20845;&#20010;&#23884;&#20837;&#21487;&#20197;&#35782;&#21035;&#20986;&#22810;&#20010;&#36716;&#25442;&#12290;&#36825;&#23545;&#20110;&#35757;&#32451;&#31639;&#27861;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#38024;&#26159;&#19968;&#31181;&#20174;&#23884;&#20837;&#20013;&#39044;&#27979;&#24213;&#23618;&#25968;&#25454;&#23646;&#24615;&#30340;&#23567;&#22411;&#32593;&#32476;&#65292;&#23427;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#38024;&#23545;&#24615;&#21644;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#25581;&#31034;&#23884;&#20837;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#12290;&#34429;&#28982;&#36890;&#36807;&#20351;&#29992;&#25506;&#38024;&#36827;&#34892;&#20998;&#26512;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24050;&#32463;&#24456;&#24120;&#35265;&#20102;&#65292;&#20294;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#30340;&#25506;&#32034;&#21364;&#27604;&#36739;&#23569;&#12290;&#22270;&#20687;&#22522;&#30784;&#27169;&#22411;&#20027;&#35201;&#29992;&#20110;&#35780;&#20272;&#35821;&#20041;&#20869;&#23481;&#12290;&#26356;&#22909;&#22320;&#29702;&#35299;&#27969;&#34892;&#23884;&#20837;&#65288;&#22914;MAE&#65292;SimCLR&#25110;CLIP&#65289;&#20013;&#30340;&#38750;&#35821;&#20041;&#20449;&#24687;&#65292;&#23558;&#20026;&#35757;&#32451;&#31639;&#27861;&#21644;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#30340;&#29992;&#36884;&#25552;&#20379;&#26032;&#30340;&#35270;&#35282;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31995;&#32479;&#30340;&#36716;&#25442;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#32500;&#24230;&#19978;&#27979;&#37327;&#23884;&#20837;&#30340;&#35270;&#35273;&#20869;&#23481;&#65292;&#21253;&#25324;&#22270;&#20687;&#39118;&#26684;&#12289;&#36136;&#37327;&#20197;&#21450;&#21508;&#31181;&#33258;&#28982;&#21644;&#20154;&#24037;&#36716;&#25442;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#26377;&#20845;&#20010;&#23884;&#20837;&#65288;&#21253;&#25324;SimCLR&#65289;&#32534;&#30721;&#20102;&#36275;&#22815;&#30340;&#38750;&#35821;&#20041;&#20449;&#24687;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#25968;&#21313;&#20010;&#36716;&#25442;&#12290;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#19968;&#20010;&#27867;&#21270;&#20219;&#21153;&#65292;&#23558;&#30456;&#20284;&#30340;&#36716;&#25442;&#20998;&#32452;&#65292;&#24182;&#30041;&#20986;&#19968;&#37096;&#20998;&#25968;&#25454;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probes are small networks that predict properties of underlying data from embeddings, and they provide a targeted, effective way to illuminate the information contained in embeddings. While analysis through the use of probes has become standard in NLP, there has been much less exploration in vision. Image foundation models have primarily been evaluated for semantic content. Better understanding the non-semantic information in popular embeddings (e.g., MAE, SimCLR, or CLIP) will shed new light both on the training algorithms and on the uses for these foundation models. We design a systematic transformation prediction task and measure the visual content of embeddings along many axes, including image style, quality, and a range of natural and artificial transformations. Surprisingly, six embeddings (including SimCLR) encode enough non-semantic information to identify dozens of transformations. We also consider a generalization task, where we group similar transformations and hold out seve
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35270;&#39057;&#20998;&#31867;&#20013;&#30340;&#26631;&#27880;&#20943;&#36127;&#12290;&#36825;&#20010;&#26694;&#26550;&#36890;&#36807;&#33258;&#21160;&#35782;&#21035;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#26469;&#20943;&#23569;&#20154;&#24037;&#26631;&#27880;&#24037;&#20316;&#37327;&#21644;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2307.05587</link><description>&lt;p&gt;
&#20351;&#29992;&#24103;&#32423;&#21035;&#26597;&#35810;&#30340;&#20027;&#21160;&#23398;&#20064;&#35270;&#39057;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Active Learning for Video Classification with Frame Level Queries. (arXiv:2307.05587v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35270;&#39057;&#20998;&#31867;&#20013;&#30340;&#26631;&#27880;&#20943;&#36127;&#12290;&#36825;&#20010;&#26694;&#26550;&#36890;&#36807;&#33258;&#21160;&#35782;&#21035;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#26469;&#20943;&#23569;&#20154;&#24037;&#26631;&#27880;&#24037;&#20316;&#37327;&#21644;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#25512;&#21160;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#30340;&#36793;&#30028;&#65292;&#24182;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#19968;&#20010;&#24378;&#22823;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#33719;&#21462;&#36825;&#20123;&#25968;&#25454;&#38656;&#35201;&#22823;&#37327;&#26102;&#38388;&#21644;&#20154;&#21147;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#35270;&#39057;&#20998;&#31867;&#31561;&#24212;&#29992;&#20013;&#26356;&#20026;&#20005;&#37325;&#65292;&#22240;&#20026;&#20154;&#24037;&#26631;&#27880;&#32773;&#24517;&#39035;&#23436;&#25972;&#22320;&#35266;&#30475;&#25972;&#20010;&#35270;&#39057;&#20197;&#25552;&#20379;&#26631;&#31614;&#12290;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#65292;&#36825;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#20154;&#24037;&#27880;&#37322;&#30340;&#24037;&#20316;&#37327;&#65292;&#21482;&#38656;&#35201;&#25163;&#21160;&#26631;&#35760;&#31639;&#27861;&#35782;&#21035;&#20986;&#30340;&#23569;&#25968;&#26679;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#35270;&#39057;&#20998;&#31867;&#65292;&#26088;&#22312;&#36827;&#19968;&#27493;&#20943;&#36731;&#20154;&#24037;&#26631;&#27880;&#32773;&#30340;&#24037;&#20316;&#36127;&#25285;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#35782;&#21035;&#19968;&#25209;&#22270;&#20687;&#26679;&#26412;&#35270;&#39057;&#65292;
&lt;/p&gt;
&lt;p&gt;
Deep learning algorithms have pushed the boundaries of computer vision research and have depicted commendable performance in a variety of applications. However, training a robust deep neural network necessitates a large amount of labeled training data, acquiring which involves significant time and human effort. This problem is even more serious for an application like video classification, where a human annotator has to watch an entire video end-to-end to furnish a label. Active learning algorithms automatically identify the most informative samples from large amounts of unlabeled data; this tremendously reduces the human annotation effort in inducing a machine learning model, as only the few samples that are identified by the algorithm, need to be labeled manually. In this paper, we propose a novel active learning framework for video classification, with the goal of further reducing the labeling onus on the human annotators. Our framework identifies a batch of exemplar videos, togethe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#27169;&#22411;&#36716;&#25442;&#19982;SysML&#38598;&#25104;&#65292;&#33268;&#21147;&#20110;&#23454;&#29616;&#25968;&#25454;&#39537;&#21160;&#24037;&#31243;&#30340;&#20195;&#30721;&#29983;&#25104;&#65292;&#20197;&#25552;&#39640;&#24037;&#31243;&#31995;&#32479;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#32500;&#25252;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05584</link><description>&lt;p&gt;
&#20351;&#29992;&#27169;&#22411;&#39537;&#21160;&#24037;&#31243;&#21644;SysML&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#30340;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Code Generation for Machine Learning using Model-Driven Engineering and SysML. (arXiv:2307.05584v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05584
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#27169;&#22411;&#36716;&#25442;&#19982;SysML&#38598;&#25104;&#65292;&#33268;&#21147;&#20110;&#23454;&#29616;&#25968;&#25454;&#39537;&#21160;&#24037;&#31243;&#30340;&#20195;&#30721;&#29983;&#25104;&#65292;&#20197;&#25552;&#39640;&#24037;&#31243;&#31995;&#32479;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#32500;&#25252;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#24037;&#31243;&#25351;&#30340;&#26159;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#25913;&#36827;&#24037;&#31243;&#31995;&#32479;&#30340;&#31995;&#32479;&#24615;&#25968;&#25454;&#25910;&#38598;&#21644;&#22788;&#29702;&#12290;&#30446;&#21069;&#65292;&#25968;&#25454;&#39537;&#21160;&#24037;&#31243;&#30340;&#23454;&#29616;&#20381;&#36182;&#20110;&#22522;&#26412;&#30340;&#25968;&#25454;&#31185;&#23398;&#21644;&#36719;&#20214;&#24037;&#31243;&#25216;&#33021;&#12290;&#21516;&#26102;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;&#24037;&#31243;&#27491;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#29992;&#20110;&#22797;&#26434;&#31995;&#32479;&#30340;&#24037;&#31243;&#12290;&#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24037;&#31243;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#36890;&#29992;&#24314;&#27169;&#35821;&#35328;SysML&#26469;&#24418;&#24335;&#21270;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24418;&#24335;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20173;&#28982;&#38656;&#35201;&#22312;&#20687;Python&#36825;&#26679;&#30340;&#19987;&#38376;&#30340;&#32534;&#31243;&#35821;&#35328;&#20013;&#23454;&#29616;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25193;&#23637;&#20197;&#24448;&#30340;&#24418;&#24335;&#21270;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#24037;&#20316;&#65292;&#36890;&#36807;&#38598;&#25104;&#27169;&#22411;&#36716;&#25442;&#26469;&#29983;&#25104;&#21487;&#25191;&#34892;&#20195;&#30721;&#65292;&#20174;&#32780;&#20419;&#36827;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#25968;&#25454;&#39537;&#21160;&#24037;&#31243;&#30340;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#20391;&#37325;&#20110;&#27169;&#22411;&#36716;&#25442;&#30340;&#21487;&#20462;&#25913;&#24615;&#21644;&#21487;&#32500;&#25252;&#24615;&#65292;&#20197;&#20415;&#36827;&#34892;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven engineering refers to systematic data collection and processing using machine learning to improve engineering systems. Currently, the implementation of data-driven engineering relies on fundamental data science and software engineering skills. At the same time, model-based engineering is gaining relevance for the engineering of complex systems. In previous work, a model-based engineering approach integrating the formalization of machine learning tasks using the general-purpose modeling language SysML is presented. However, formalized machine learning tasks still require the implementation in a specialized programming languages like Python. Therefore, this work aims to facilitate the implementation of data-driven engineering in practice by extending the previous work of formalizing machine learning tasks by integrating model transformation to generate executable code. The method focuses on the modifiability and maintainability of the model transformation so that extensions a
&lt;/p&gt;</description></item><item><title>DBFed&#26159;&#19968;&#20010;&#22522;&#20110;&#39046;&#22495;&#26080;&#20851;&#24615;&#30340;&#21435;&#20559;&#24046;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#25968;&#25454;&#36136;&#37327;&#24046;&#24322;&#24341;&#36215;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.05582</link><description>&lt;p&gt;
DBFed: &#22522;&#20110;&#39046;&#22495;&#26080;&#20851;&#24615;&#30340;&#21435;&#20559;&#24046;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DBFed: Debiasing Federated Learning Framework based on Domain-Independent. (arXiv:2307.05582v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05582
&lt;/p&gt;
&lt;p&gt;
DBFed&#26159;&#19968;&#20010;&#22522;&#20110;&#39046;&#22495;&#26080;&#20851;&#24615;&#30340;&#21435;&#20559;&#24046;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#25968;&#25454;&#36136;&#37327;&#24046;&#24322;&#24341;&#36215;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#23383;&#21270;&#36716;&#22411;&#30340;&#25345;&#32493;&#36827;&#34892;&#65292;&#20225;&#19994;&#27491;&#22312;&#20135;&#29983;&#12289;&#31649;&#29702;&#21644;&#23384;&#20648;&#22823;&#37327;&#30340;&#25968;&#25454;&#65292;&#32780;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20063;&#22312;&#36805;&#36895;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20063;&#32473;&#20449;&#24687;&#23433;&#20840;&#21644;&#25968;&#25454;&#23433;&#20840;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#25968;&#25454;&#23433;&#20840;&#26159;&#25351;&#22312;&#20854;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#20869;&#65292;&#20445;&#25252;&#25968;&#23383;&#20449;&#24687;&#20813;&#21463;&#26410;&#32463;&#25480;&#26435;&#30340;&#35775;&#38382;&#12289;&#25439;&#22351;&#12289;&#30423;&#31363;&#31561;&#30340;&#25439;&#23475;&#12290;&#38543;&#30528;&#25968;&#25454;&#23433;&#20840;&#27861;&#30340;&#39041;&#24067;&#21644;&#25191;&#34892;&#20197;&#21450;&#32452;&#32455;&#21644;&#29992;&#25143;&#23545;&#25968;&#25454;&#23433;&#20840;&#21644;&#25968;&#25454;&#38544;&#31169;&#30340;&#37325;&#35270;&#65292;&#20197;&#32852;&#37030;&#23398;&#20064;&#20026;&#20195;&#34920;&#30340;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#22312;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#20013;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#35745;&#31639;&#26694;&#26550;&#65292;&#20801;&#35768;&#22810;&#20010;&#20027;&#20307;&#22312;&#19981;&#20849;&#20139;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20849;&#21516;&#27169;&#22411;&#35757;&#32451;&#65292;&#20197;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#24182;&#35299;&#20915;&#25968;&#25454;&#23396;&#23707;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22810;&#20010;&#20027;&#20307;&#20043;&#38388;&#30340;&#25968;&#25454;&#24444;&#27492;&#29420;&#31435;&#65292;&#32780;&#36136;&#37327;&#19978;&#30340;&#24046;&#24322;&#21487;&#33021;&#23548;&#33268;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
As digital transformation continues, enterprises are generating, managing, and storing vast amounts of data, while artificial intelligence technology is rapidly advancing. However, it brings challenges in information security and data security. Data security refers to the protection of digital information from unauthorized access, damage, theft, etc. throughout its entire life cycle. With the promulgation and implementation of data security laws and the emphasis on data security and data privacy by organizations and users, Privacy-preserving technology represented by federated learning has a wide range of application scenarios. Federated learning is a distributed machine learning computing framework that allows multiple subjects to train joint models without sharing data to protect data privacy and solve the problem of data islands. However, the data among multiple subjects are independent of each other, and the data differences in quality may cause fairness issues in federated learnin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#36890;&#36807;&#24212;&#23545;&#24694;&#24847;&#35328;&#35770;&#20013;&#30340;&#22797;&#26434;&#35821;&#20041;&#20449;&#24687;&#21644;&#24694;&#24847;&#35328;&#36766;&#30340;&#24178;&#25200;&#20197;&#21450;&#24694;&#24847;&#35328;&#35770;&#21644;&#38750;&#24694;&#24847;&#35328;&#35770;&#30340;&#19981;&#24179;&#34913;&#20998;&#24067;&#31561;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#36328;&#24230;&#32423;&#20449;&#24687;&#30340;&#25429;&#25417;&#12290;</title><link>http://arxiv.org/abs/2307.05578</link><description>&lt;p&gt;
&#24694;&#24847;&#35328;&#35770;&#36890;&#36807;&#21452;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Hate Speech Detection via Dual Contrastive Learning. (arXiv:2307.05578v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05578
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#36890;&#36807;&#24212;&#23545;&#24694;&#24847;&#35328;&#35770;&#20013;&#30340;&#22797;&#26434;&#35821;&#20041;&#20449;&#24687;&#21644;&#24694;&#24847;&#35328;&#36766;&#30340;&#24178;&#25200;&#20197;&#21450;&#24694;&#24847;&#35328;&#35770;&#21644;&#38750;&#24694;&#24847;&#35328;&#35770;&#30340;&#19981;&#24179;&#34913;&#20998;&#24067;&#31561;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#36328;&#24230;&#32423;&#20449;&#24687;&#30340;&#25429;&#25417;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24694;&#24847;&#35328;&#35770;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#24555;&#36895;&#20256;&#25773;&#24433;&#21709;&#30528;&#20114;&#32852;&#32593;&#29615;&#22659;&#21644;&#25105;&#20204;&#30340;&#31038;&#20250;&#65292;&#22686;&#21152;&#20102;&#20559;&#35265;&#24182;&#20260;&#23475;&#20102;&#20154;&#20204;&#12290;&#26816;&#27979;&#24694;&#24847;&#35328;&#35770;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#35299;&#20915;&#20102;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#20013;&#30340;&#19968;&#20123;&#38382;&#39064;&#65292;&#20294;&#36825;&#20010;&#20219;&#21153;&#20173;&#28982;&#38754;&#20020;&#30528;&#20004;&#20010;&#22266;&#26377;&#30340;&#26410;&#35299;&#20915;&#25361;&#25112;&#12290;&#31532;&#19968;&#20010;&#25361;&#25112;&#22312;&#20110;&#24694;&#24847;&#35328;&#35770;&#20013;&#20256;&#36798;&#30340;&#22797;&#26434;&#35821;&#20041;&#20449;&#24687;&#65292;&#29305;&#21035;&#26159;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#20013;&#20398;&#36785;&#24615;&#35328;&#36766;&#30340;&#24178;&#25200;&#12290;&#31532;&#20108;&#20010;&#25361;&#25112;&#26159;&#24694;&#24847;&#35328;&#35770;&#21644;&#38750;&#24694;&#24847;&#35328;&#35770;&#30340;&#19981;&#24179;&#34913;&#20998;&#24067;&#65292;&#21487;&#33021;&#20250;&#20005;&#37325;&#25439;&#23475;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#21452;&#23545;&#27604;&#23398;&#20064;&#65288;DCL&#65289;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#25439;&#22833;&#21644;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#25439;&#22833;&#65292;&#25429;&#25417;&#36229;&#20986;&#29616;&#26377;&#27169;&#22411;&#20013;&#20351;&#29992;&#30340;&#22522;&#20110;&#20196;&#29260;&#32423;&#24773;&#24863;&#35821;&#20041;&#30340;&#36328;&#24230;&#32423;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fast spread of hate speech on social media impacts the Internet environment and our society by increasing prejudice and hurting people. Detecting hate speech has aroused broad attention in the field of natural language processing. Although hate speech detection has been addressed in recent work, this task still faces two inherent unsolved challenges. The first challenge lies in the complex semantic information conveyed in hate speech, particularly the interference of insulting words in hate speech detection. The second challenge is the imbalanced distribution of hate speech and non-hate speech, which may significantly deteriorate the performance of models. To tackle these challenges, we propose a novel dual contrastive learning (DCL) framework for hate speech detection. Our framework jointly optimizes the self-supervised and the supervised contrastive learning loss for capturing span-level information beyond the token-level emotional semantics used in existing models, particularly 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#23547;&#20102;&#19968;&#31181;&#33021;&#22815;&#22788;&#29702;&#29616;&#23454;&#21644;&#34394;&#26500;&#24212;&#29992;&#39046;&#22495;&#20013;&#20986;&#29616;&#24773;&#20917;&#30340;&#24378;&#22823;&#36923;&#36753;&#65292;&#37319;&#29992;&#38750;&#24120;&#35268;&#25193;&#23637;&#65292;&#35797;&#22270;&#21246;&#21202;&#20986;&#19968;&#31181;&#26368;&#23567;&#22797;&#21512;&#36923;&#36753;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;ChatGPT AI&#20195;&#29702;&#20256;&#36882;&#29702;&#35770;&#27010;&#24565;&#32972;&#21518;&#30340;&#30452;&#35273;&#12290;</title><link>http://arxiv.org/abs/2307.05574</link><description>&lt;p&gt;
&#20851;&#20110;&#20803;&#23431;&#23449;&#36923;&#36753;&#30340;&#19968;&#20123;&#21021;&#27493;&#27493;&#39588;
&lt;/p&gt;
&lt;p&gt;
Some Preliminary Steps Towards Metaverse Logic. (arXiv:2307.05574v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#23547;&#20102;&#19968;&#31181;&#33021;&#22815;&#22788;&#29702;&#29616;&#23454;&#21644;&#34394;&#26500;&#24212;&#29992;&#39046;&#22495;&#20013;&#20986;&#29616;&#24773;&#20917;&#30340;&#24378;&#22823;&#36923;&#36753;&#65292;&#37319;&#29992;&#38750;&#24120;&#35268;&#25193;&#23637;&#65292;&#35797;&#22270;&#21246;&#21202;&#20986;&#19968;&#31181;&#26368;&#23567;&#22797;&#21512;&#36923;&#36753;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;ChatGPT AI&#20195;&#29702;&#20256;&#36882;&#29702;&#35770;&#27010;&#24565;&#32972;&#21518;&#30340;&#30452;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#35774;&#8220;&#20803;&#23431;&#23449;&#8221;&#19968;&#35789;&#21487;&#20197;&#29702;&#35299;&#20026;&#22810;&#20803;&#23431;&#23449;&#24212;&#29992;&#30340;&#22522;&#20110;&#35745;&#31639;&#26426;&#30340;&#23454;&#29616;&#65292;&#25105;&#20204;&#22312;&#26412;&#24037;&#20316;&#20013;&#24320;&#22987;&#23547;&#25214;&#19968;&#20010;&#36275;&#22815;&#24378;&#22823;&#30340;&#36923;&#36753;&#26469;&#22788;&#29702;&#29616;&#23454;&#21644;&#34394;&#26500;&#30340;&#22522;&#30784;&#24212;&#29992;&#39046;&#22495;&#20013;&#20986;&#29616;&#30340;&#24773;&#20917;&#12290;&#24847;&#35782;&#21040;&#19968;&#38454;&#36923;&#36753;&#26080;&#27861;&#35299;&#37322;&#29978;&#33267;&#26368;&#31616;&#21333;&#20449;&#24687;&#31995;&#32479;&#39046;&#22495;&#30340;&#19981;&#31283;&#23450;&#34892;&#20026;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#38750;&#24120;&#35268;&#25193;&#23637;&#65292;&#35797;&#22270;&#21246;&#21202;&#20986;&#19968;&#31181;&#26368;&#23567;&#22797;&#21512;&#36923;&#36753;&#31574;&#30053;&#12290;&#35752;&#35770;&#20445;&#25345;&#22312;&#30456;&#24403;&#38750;&#27491;&#24335;&#30340;&#27700;&#24179;&#19978;&#65292;&#22987;&#32456;&#35797;&#22270;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;&#26415;&#35821;&#20256;&#36798;&#29702;&#35770;&#27010;&#24565;&#32972;&#21518;&#30340;&#30452;&#35273;&#65292;&#24182;&#20511;&#21161;AI&#20195;&#29702;ChatGPT&#65292;&#24076;&#26395;&#31639;&#27861;&#21644;&#24120;&#35782;&#26041;&#27861;&#21487;&#20197;&#26377;&#29992;&#22320;&#32467;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Assuming that the term 'metaverse' could be understood as a computer-based implementation of multiverse applications, we started to look in the present work for a logic that would be powerful enough to handle the situations arising both in the real and in the fictional underlying application domains. Realizing that first-order logic fails to account for the unstable behavior of even the most simpleminded information system domains, we resorted to non-conventional extensions, in an attempt to sketch a minimal composite logic strategy. The discussion was kept at a rather informal level, always trying to convey the intuition behind the theoretical notions in natural language terms, and appealing to an AI agent, namely ChatGPT, in the hope that algorithmic and common-sense approaches can be usefully combined.
&lt;/p&gt;</description></item><item><title>RidgeBase&#26159;&#19968;&#20010;&#36328;&#20256;&#24863;&#22120;&#22810;&#25351;&#38750;&#25509;&#35302;&#25351;&#32441;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#20419;&#36827;&#19981;&#21516;&#21305;&#37197;&#22330;&#26223;&#19979;&#38750;&#25509;&#35302;&#25351;&#32441;&#21305;&#37197;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.05563</link><description>&lt;p&gt;
RidgeBase&#65306;&#19968;&#31181;&#36328;&#20256;&#24863;&#22120;&#22810;&#25351;&#38750;&#25509;&#35302;&#25351;&#32441;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
RidgeBase: A Cross-Sensor Multi-Finger Contactless Fingerprint Dataset. (arXiv:2307.05563v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05563
&lt;/p&gt;
&lt;p&gt;
RidgeBase&#26159;&#19968;&#20010;&#36328;&#20256;&#24863;&#22120;&#22810;&#25351;&#38750;&#25509;&#35302;&#25351;&#32441;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#20419;&#36827;&#19981;&#21516;&#21305;&#37197;&#22330;&#26223;&#19979;&#38750;&#25509;&#35302;&#25351;&#32441;&#21305;&#37197;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#30456;&#26426;&#36827;&#34892;&#38750;&#25509;&#35302;&#25351;&#32441;&#21305;&#37197;&#21487;&#20197;&#35299;&#20915;&#20256;&#32479;&#25351;&#32441;&#31995;&#32479;&#30340;&#19968;&#20123;&#20027;&#35201;&#25361;&#25112;&#65292;&#21253;&#25324;&#21355;&#29983;&#37319;&#38598;&#12289;&#20415;&#25658;&#24615;&#21644;&#25915;&#20987;&#38450;&#33539;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#21644;&#40065;&#26834;&#30340;&#38750;&#25509;&#35302;&#25351;&#32441;&#21305;&#37197;&#25216;&#26415;&#30340;&#21457;&#23637;&#21463;&#21040;&#20102;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#28608;&#21457;&#22312;&#19981;&#21516;&#20256;&#24863;&#22120;&#20043;&#38388;&#36827;&#19968;&#27493;&#21457;&#23637;&#38750;&#25509;&#35302;&#25351;&#32441;&#21305;&#37197;&#30340;&#36827;&#27493;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;RidgeBase&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;RidgeBase&#21253;&#21547;&#20102;&#26469;&#33258;88&#20010;&#20010;&#20307;&#65292;&#22312;&#19981;&#21516;&#32972;&#26223;&#21644;&#29031;&#26126;&#26465;&#20214;&#19979;&#20351;&#29992;&#20004;&#20010;&#26234;&#33021;&#25163;&#26426;&#30456;&#26426;&#21644;&#19968;&#20010;&#24179;&#26495;&#35302;&#25720;&#20256;&#24863;&#22120;&#33719;&#24471;&#30340;&#36229;&#36807;15,000&#20010;&#38750;&#25509;&#35302;&#21644;&#25509;&#35302;&#24335;&#25351;&#32441;&#22270;&#20687;&#23545;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;RidgeBase&#26088;&#22312;&#20419;&#36827;&#22312;&#19981;&#21516;&#21305;&#37197;&#22330;&#26223;&#19979;&#30340;&#30740;&#31350;&#65292;&#21253;&#25324;&#21333;&#25351;&#21305;&#37197;&#21644;&#22810;&#25351;&#21305;&#37197;&#65292;&#26082;&#21253;&#25324;&#38750;&#25509;&#35302;&#21040;&#38750;&#25509;&#35302;&#65288;CL2CL&#65289;&#30340;&#39564;&#35777;&#21644;&#35782;&#21035;&#65292;&#20063;&#21253;&#25324;&#25509;&#35302;&#21040;&#38750;&#25509;&#35302;&#65288;C2CL&#65289;&#30340;&#39564;&#35777;&#21644;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contactless fingerprint matching using smartphone cameras can alleviate major challenges of traditional fingerprint systems including hygienic acquisition, portability and presentation attacks. However, development of practical and robust contactless fingerprint matching techniques is constrained by the limited availability of large scale real-world datasets. To motivate further advances in contactless fingerprint matching across sensors, we introduce the RidgeBase benchmark dataset. RidgeBase consists of more than 15,000 contactless and contact-based fingerprint image pairs acquired from 88 individuals under different background and lighting conditions using two smartphone cameras and one flatbed contact sensor. Unlike existing datasets, RidgeBase is designed to promote research under different matching scenarios that include Single Finger Matching and Multi-Finger Matching for both contactless- to-contactless (CL2CL) and contact-to-contactless (C2CL) verification and identification. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#25913;&#36827;&#30340;6D&#29289;&#20307;&#20301;&#23039;&#20272;&#35745;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#20165;&#20351;&#29992;RGB&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#36731;&#37327;&#32423;&#28145;&#24230;&#20272;&#35745;&#32593;&#32476;&#21644;Transformer-based&#26816;&#27979;&#32593;&#32476;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#20301;&#23039;&#20272;&#35745;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#32454;&#21270;&#26041;&#27861;&#25552;&#39640;&#28145;&#24230;&#20272;&#35745;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.05561</link><description>&lt;p&gt;
TransPose:&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#32454;&#21270;&#30340;6D&#29289;&#20307;&#20301;&#23039;&#20272;&#35745;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
TransPose: A Transformer-based 6D Object Pose Estimation Network with Depth Refinement. (arXiv:2307.05561v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05561
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#25913;&#36827;&#30340;6D&#29289;&#20307;&#20301;&#23039;&#20272;&#35745;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#20165;&#20351;&#29992;RGB&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#36731;&#37327;&#32423;&#28145;&#24230;&#20272;&#35745;&#32593;&#32476;&#21644;Transformer-based&#26816;&#27979;&#32593;&#32476;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#20301;&#23039;&#20272;&#35745;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#32454;&#21270;&#26041;&#27861;&#25552;&#39640;&#28145;&#24230;&#20272;&#35745;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#26426;&#22120;&#20154;&#25805;&#20316;&#24212;&#29992;&#30340;&#38656;&#27714;&#22686;&#21152;&#65292;&#20934;&#30830;&#30340;&#22522;&#20110;&#35270;&#35273;&#30340;6D&#20301;&#23039;&#20272;&#35745;&#23545;&#20110;&#33258;&#20027;&#25805;&#20316;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20043;&#21069;&#24050;&#32463;&#24341;&#20837;&#20102;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#20301;&#23039;&#20272;&#35745;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20934;&#30830;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#65292;&#23545;&#26356;&#22909;&#30340;&#24615;&#33021;&#30340;&#36861;&#27714;&#20173;&#28982;&#23384;&#22312;&#12290;&#36825;&#31181;&#36861;&#27714;&#24310;&#20280;&#21040;&#20102;&#20892;&#19994;&#26426;&#22120;&#20154;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TransPose&#65292;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#25913;&#36827;&#30340;&#24102;&#28145;&#24230;&#32454;&#21270;&#27169;&#22359;&#30340;6D&#20301;&#23039;&#20272;&#35745;&#26041;&#27861;&#12290;&#35813;&#26550;&#26500;&#21482;&#25509;&#21463;RGB&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#65292;&#27809;&#26377;&#39069;&#22806;&#30340;&#34917;&#20805;&#27169;&#24577;&#65292;&#22914;&#28145;&#24230;&#25110;&#32418;&#22806;&#22270;&#20687;&#12290;&#35813;&#26550;&#26500;&#21253;&#21547;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#36731;&#37327;&#32423;&#28145;&#24230;&#20272;&#35745;&#32593;&#32476;&#65292;&#20351;&#29992;&#29305;&#24449;&#37329;&#23383;&#22612;&#21644;&#19978;&#37319;&#26679;&#26041;&#27861;&#20174;RGB&#22270;&#20687;&#20272;&#35745;&#28145;&#24230;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#26816;&#27979;&#32593;&#32476;&#65292;&#24102;&#26377;&#39069;&#22806;&#30340;&#39044;&#27979;&#22836;&#65292;&#21487;&#20197;&#30452;&#25509;&#22238;&#24402;&#29289;&#20307;&#30340;&#20013;&#24515;&#24182;&#39044;&#27979;&#30446;&#26631;&#30340;6D&#20301;&#23039;&#12290;&#19968;&#20010;&#26032;&#39062;&#30340;&#28145;&#24230;&#32454;&#21270;&#26041;&#27861;&#34987;&#25552;&#20986;&#65292;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#26469;&#25552;&#39640;&#28145;&#24230;&#20272;&#35745;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
As demand for robotics manipulation application increases, accurate vision-based 6D pose estimation becomes essential for autonomous operations. Convolutional Neural Networks (CNNs) based approaches for pose estimation have been previously introduced. However, the quest for better performance still persists especially for accurate robotics manipulation. This quest extends to the Agri-robotics domain. In this paper, we propose TransPose, an improved Transformer-based 6D pose estimation with a depth refinement module. The architecture takes in only an RGB image as input with no additional supplementing modalities such as depth or thermal images. The architecture encompasses an innovative lighter depth estimation network that estimates depth from an RGB image using feature pyramid with an up-sampling method. A transformer-based detection network with additional prediction heads is proposed to directly regress the object's centre and predict the 6D pose of the target. A novel depth refinem
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#33258;&#21160;&#21270;&#35770;&#25991;&#35780;&#20998;&#20013;&#30340;&#21453;&#39304;&#30740;&#31350;&#65292;&#21253;&#25324;&#19981;&#21516;&#31867;&#22411;&#30340;&#21453;&#39304;&#21644;&#35770;&#25991;&#29305;&#24449;&#65292;&#24182;&#22238;&#39038;&#20102;&#25552;&#20379;&#21453;&#39304;&#30340;&#26368;&#26032;&#26696;&#20363;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.05553</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#35770;&#25991;&#35780;&#20998;&#20013;&#30340;&#21453;&#39304;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Review of feedback in Automated Essay Scoring. (arXiv:2307.05553v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05553
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#33258;&#21160;&#21270;&#35770;&#25991;&#35780;&#20998;&#20013;&#30340;&#21453;&#39304;&#30740;&#31350;&#65292;&#21253;&#25324;&#19981;&#21516;&#31867;&#22411;&#30340;&#21453;&#39304;&#21644;&#35770;&#25991;&#29305;&#24449;&#65292;&#24182;&#22238;&#39038;&#20102;&#25552;&#20379;&#21453;&#39304;&#30340;&#26368;&#26032;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31532;&#19968;&#20010;&#33258;&#21160;&#21270;&#35770;&#25991;&#35780;&#20998;&#31995;&#32479;&#35806;&#29983;&#20110;50&#24180;&#21069;&#12290;&#33258;&#21160;&#21270;&#35770;&#25991;&#35780;&#20998;&#31995;&#32479;&#27491;&#22312;&#21457;&#23637;&#25104;&#20026;&#27604;&#20197;&#21069;&#31616;&#21333;&#35780;&#20998;&#31995;&#32479;&#26356;&#21152;&#21151;&#33021;&#20016;&#23500;&#30340;&#31995;&#32479;&#12290;&#23427;&#30340;&#30446;&#30340;&#19981;&#20165;&#20165;&#26159;&#35780;&#20998;&#65292;&#36824;&#20316;&#20026;&#19968;&#20010;&#23398;&#20064;&#24037;&#20855;&#26469;&#25552;&#39640;&#29992;&#25143;&#30340;&#20889;&#20316;&#33021;&#21147;&#12290;&#21453;&#39304;&#26159;&#20351;&#33258;&#21160;&#21270;&#35770;&#25991;&#35780;&#20998;&#31995;&#32479;&#22312;&#23454;&#38469;&#29983;&#27963;&#20013;&#26377;&#29992;&#30340;&#26368;&#37325;&#35201;&#26041;&#38754;&#12290;&#22312;&#31532;&#19968;&#20010;&#33258;&#21160;&#21270;&#35770;&#25991;&#35780;&#20998;&#31995;&#32479;&#20013;&#24050;&#32463;&#24378;&#35843;&#20102;&#21453;&#39304;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#20851;&#20110;&#33258;&#21160;&#21270;&#35770;&#25991;&#35780;&#20998;&#30340;&#21453;&#39304;&#30740;&#31350;&#65292;&#21253;&#25324;&#19981;&#21516;&#31867;&#22411;&#30340;&#21453;&#39304;&#21644;&#35770;&#25991;&#29305;&#24449;&#12290;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;&#25552;&#20379;&#21453;&#39304;&#30340;&#26368;&#26032;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The first automated essay scoring system was developed 50 years ago. Automated essay scoring systems are developing into systems with richer functions than the previous simple scoring systems. Its purpose is not only to score essays but also as a learning tool to improve the writing skill of users. Feedback is the most important aspect of making an automated essay scoring system useful in real life. The importance of feedback was already emphasized in the first AES system. This paper reviews research on feedback including different feedback types and essay traits on automated essay scoring. We also reviewed the latest case studies of the automated essay scoring system that provides feedback.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#29289;&#29702;&#39068;&#33394;&#26657;&#20934;&#35299;&#20915;&#20102;&#25968;&#23383;&#30149;&#29702;&#23398;&#25195;&#25551;&#20202;&#29983;&#20135;&#20013;&#30340;&#25216;&#26415;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#30284;&#30151;&#35786;&#26029;&#30340;&#31283;&#23450;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05519</link><description>&lt;p&gt;
&#29992;&#20110;&#31283;&#23450;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#30284;&#30151;&#35786;&#26029;&#30340;&#25968;&#23383;&#30149;&#29702;&#23398;&#25195;&#25551;&#20202;&#30340;&#29289;&#29702;&#39068;&#33394;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Physical Color Calibration of Digital Pathology Scanners for Robust Artificial Intelligence Assisted Cancer Diagnosis. (arXiv:2307.05519v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#29289;&#29702;&#39068;&#33394;&#26657;&#20934;&#35299;&#20915;&#20102;&#25968;&#23383;&#30149;&#29702;&#23398;&#25195;&#25551;&#20202;&#29983;&#20135;&#20013;&#30340;&#25216;&#26415;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#30284;&#30151;&#35786;&#26029;&#30340;&#31283;&#23450;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#30340;&#28508;&#21147;&#21463;&#21040;&#20840;&#20999;&#29255;&#22270;&#20687;&#65288;WSIs&#65289;&#29983;&#20135;&#20013;&#30340;&#25216;&#26415;&#19981;&#19968;&#33268;&#24615;&#30340;&#38480;&#21046;&#65292;&#23548;&#33268;AI&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#32473;&#24191;&#27867;&#20020;&#24202;&#24212;&#29992;&#24102;&#26469;&#25361;&#25112;&#12290;&#25195;&#25551;&#20202;&#30340;&#24037;&#20316;&#27969;&#26356;&#25913;&#36824;&#21487;&#33021;&#23548;&#33268;&#35786;&#26029;&#21463;&#25439;&#21644;&#24739;&#32773;&#23433;&#20840;&#39118;&#38505;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25195;&#25551;&#20202;&#30340;&#29289;&#29702;&#39068;&#33394;&#26657;&#20934;&#26159;&#21542;&#21487;&#20197;&#26631;&#20934;&#21270;WSI&#30340;&#22806;&#35266;&#65292;&#24182;&#23454;&#29616;&#31283;&#23450;&#30340;AI&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#23454;&#39564;&#23460;&#20013;&#20351;&#29992;&#20102;&#19968;&#20010;&#39068;&#33394;&#26657;&#20934;&#28369;&#29255;&#65292;&#24182;&#35780;&#20272;&#20854;&#23545;1,161&#20010;WSI&#19978;&#21069;&#21015;&#33146;&#30284;&#35786;&#26029;AI&#31995;&#32479;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#39068;&#33394;&#26631;&#20934;&#21270;&#23548;&#33268;AI&#27169;&#22411;&#26657;&#20934;&#25345;&#32493;&#25913;&#21892;&#65292;&#21516;&#26102;&#22312;Gleason&#20998;&#32423;&#24615;&#33021;&#26041;&#38754;&#26174;&#33879;&#25552;&#39640;&#12290;&#35813;&#30740;&#31350;&#35777;&#26126;&#65292;&#29289;&#29702;&#39068;&#33394;&#26657;&#20934;&#25552;&#20379;&#20102;&#35299;&#20915;&#30001;&#19981;&#21516;&#25195;&#25551;&#20202;&#24341;&#20837;&#30340;&#24046;&#24322;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22522;&#20110;AI&#30340;&#30284;&#30151;&#35786;&#26029;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The potential of artificial intelligence (AI) in digital pathology is limited by technical inconsistencies in the production of whole slide images (WSIs), leading to degraded AI performance and posing a challenge for widespread clinical application as fine-tuning algorithms for each new site is impractical. Changes in the imaging workflow can also lead to compromised diagnoses and patient safety risks. We evaluated whether physical color calibration of scanners can standardize WSI appearance and enable robust AI performance. We employed a color calibration slide in four different laboratories and evaluated its impact on the performance of an AI system for prostate cancer diagnosis on 1,161 WSIs. Color standardization resulted in consistently improved AI model calibration and significant improvements in Gleason grading performance. The study demonstrates that physical color calibration provides a potential solution to the variation introduced by different scanners, making AI-based cance
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#35268;&#21017;&#24182;&#23558;&#20854;&#20256;&#36798;&#32473;&#29609;&#23478;&#35843;&#25972;&#38590;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#21644;&#38590;&#24230;&#27979;&#37327;&#26469;&#25214;&#21040;&#30446;&#26631;&#35299;&#38598;&#30340;&#25968;&#37327;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21465;&#20107;&#32972;&#26223;&#19979;&#20256;&#36798;&#35268;&#21017;&#12290;&#35813;&#26041;&#27861;&#22312;&#27979;&#35797;&#20013;&#25104;&#21151;&#22320;&#25214;&#21040;&#20102;&#36924;&#36817;&#20219;&#20309;&#32473;&#23450;&#30446;&#26631;&#38590;&#24230;&#30340;&#35268;&#21017;&#12290;&#35813;&#26041;&#27861;&#36824;&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#21465;&#20107;&#30410;&#26234;&#28216;&#25103;&#65292;&#29609;&#23478;&#24517;&#39035;&#20030;&#21150;&#19968;&#27425;&#26202;&#23476;&#20026;&#26080;&#27861;&#21644;&#35856;&#20849;&#22788;&#30340;&#21160;&#29289;&#20204;&#12290;&#26410;&#26469;&#30340;&#24037;&#20316;&#23558;&#33268;&#21147;&#20110;&#25913;&#36827;&#35780;&#20272;&#65292;&#19987;&#38376;&#38024;&#23545;&#20799;&#31461;&#25991;&#23398;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#25910;&#38598;&#29609;&#23478;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#26469;&#25351;&#23548;&#38590;&#24230;&#30340;&#21160;&#24577;&#35843;&#25972;&#12290;</title><link>http://arxiv.org/abs/2307.05518</link><description>&lt;p&gt;
&#20026;&#21465;&#20107;&#30410;&#26234;&#28216;&#25103;&#30340;&#38590;&#24230;&#36827;&#34892;&#31243;&#24207;&#21270;&#29983;&#25104;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Procedurally generating rules to adapt difficulty for narrative puzzle games. (arXiv:2307.05518v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#35268;&#21017;&#24182;&#23558;&#20854;&#20256;&#36798;&#32473;&#29609;&#23478;&#35843;&#25972;&#38590;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#21644;&#38590;&#24230;&#27979;&#37327;&#26469;&#25214;&#21040;&#30446;&#26631;&#35299;&#38598;&#30340;&#25968;&#37327;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21465;&#20107;&#32972;&#26223;&#19979;&#20256;&#36798;&#35268;&#21017;&#12290;&#35813;&#26041;&#27861;&#22312;&#27979;&#35797;&#20013;&#25104;&#21151;&#22320;&#25214;&#21040;&#20102;&#36924;&#36817;&#20219;&#20309;&#32473;&#23450;&#30446;&#26631;&#38590;&#24230;&#30340;&#35268;&#21017;&#12290;&#35813;&#26041;&#27861;&#36824;&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#21465;&#20107;&#30410;&#26234;&#28216;&#25103;&#65292;&#29609;&#23478;&#24517;&#39035;&#20030;&#21150;&#19968;&#27425;&#26202;&#23476;&#20026;&#26080;&#27861;&#21644;&#35856;&#20849;&#22788;&#30340;&#21160;&#29289;&#20204;&#12290;&#26410;&#26469;&#30340;&#24037;&#20316;&#23558;&#33268;&#21147;&#20110;&#25913;&#36827;&#35780;&#20272;&#65292;&#19987;&#38376;&#38024;&#23545;&#20799;&#31461;&#25991;&#23398;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#25910;&#38598;&#29609;&#23478;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#26469;&#25351;&#23548;&#38590;&#24230;&#30340;&#21160;&#24577;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#20110;&#36890;&#36807;&#31243;&#24207;&#21270;&#29983;&#25104;&#35268;&#21017;&#24182;&#23558;&#20854;&#20256;&#36798;&#32473;&#29609;&#23478;&#20197;&#35843;&#25972;&#38590;&#24230;&#12290;&#36825;&#26159;&#19968;&#20010;&#26356;&#22823;&#30340;&#39033;&#30446;&#30340;&#19968;&#37096;&#20998;&#65292;&#26088;&#22312;&#20351;&#29992;&#26088;&#22312;&#24188;&#20799;&#22253;&#30340;&#25968;&#23383;&#30410;&#26234;&#28216;&#25103;&#22312;&#25945;&#32946;&#28216;&#25103;&#20013;&#25910;&#38598;&#21644;&#35843;&#25972;&#28216;&#25103;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#19982;&#38590;&#24230;&#27979;&#37327;&#19968;&#36215;&#65292;&#25214;&#21040;&#19968;&#32452;&#35299;&#30340;&#30446;&#26631;&#25968;&#37327;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21465;&#20107;&#32972;&#26223;&#19979;&#20256;&#36798;&#35268;&#21017;&#12290;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#24179;&#22343;&#20004;&#25171;&#20195;&#20043;&#20869;&#25214;&#21040;&#36924;&#36817;&#20219;&#20309;&#32473;&#23450;&#30446;&#26631;&#38590;&#24230;&#30340;&#35268;&#21017;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#21465;&#20107;&#30410;&#26234;&#28216;&#25103;&#65292;&#29609;&#23478;&#24517;&#39035;&#20026;&#19981;&#33021;&#30456;&#22788;&#30340;&#21160;&#29289;&#20030;&#21150;&#19968;&#27425;&#26202;&#23476;&#12290;&#26410;&#26469;&#30340;&#23454;&#39564;&#23558;&#23581;&#35797;&#25913;&#36827;&#35780;&#20272;&#65292;&#35753;&#35821;&#35328;&#27169;&#22411;&#19987;&#38376;&#38024;&#23545;&#20799;&#31461;&#25991;&#23398;&#65292;&#24182;&#25910;&#38598;&#29609;&#23478;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#26469;&#25351;&#23548;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on procedurally generating rules and communicating them to players to adjust the difficulty. This is part of a larger project to collect and adapt games in educational games for young children using a digital puzzle game designed for kindergarten. A genetic algorithm is used together with a difficulty measure to find a target number of solution sets and a large language model is used to communicate the rules in a narrative context. During testing the approach was able to find rules that approximate any given target difficulty within two dozen generations on average. The approach was combined with a large language model to create a narrative puzzle game where players have to host a dinner for animals that can't get along. Future experiments will try to improve evaluation, specialize the language model on children's literature, and collect multi-modal data from players to guide adaptation.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#22871;&#36866;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#31227;&#21160;&#24212;&#29992;&#22270;&#20687;&#20998;&#31867;&#30340;AIX&#21551;&#21457;&#21644;&#26816;&#26597;&#28165;&#21333;&#65292;&#20197;&#21450;&#30456;&#24212;&#30340;&#22312;&#32447;&#35838;&#31243;&#21644;Web&#24037;&#20855;&#65292;&#21487;&#29992;&#20110;&#25351;&#23548;&#24212;&#29992;&#30028;&#38754;&#35774;&#35745;&#21644;&#21551;&#21457;&#24335;&#35780;&#20272;&#65292;&#24110;&#21161;&#24320;&#21457;&#26131;&#20110;&#29702;&#35299;&#30340;&#22270;&#20687;&#20998;&#31867;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.05513</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#20687;&#20998;&#31867;&#30340;&#28145;&#24230;&#23398;&#20064;&#31227;&#21160;&#24212;&#29992;&#30340;&#29992;&#25143;&#20307;&#39564;&#21551;&#21457;&#21644;&#26816;&#26597;&#28165;&#21333;
&lt;/p&gt;
&lt;p&gt;
UX Heuristics and Checklist for Deep Learning powered Mobile Applications with Image Classification. (arXiv:2307.05513v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05513
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#22871;&#36866;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#31227;&#21160;&#24212;&#29992;&#22270;&#20687;&#20998;&#31867;&#30340;AIX&#21551;&#21457;&#21644;&#26816;&#26597;&#28165;&#21333;&#65292;&#20197;&#21450;&#30456;&#24212;&#30340;&#22312;&#32447;&#35838;&#31243;&#21644;Web&#24037;&#20855;&#65292;&#21487;&#29992;&#20110;&#25351;&#23548;&#24212;&#29992;&#30028;&#38754;&#35774;&#35745;&#21644;&#21551;&#21457;&#24335;&#35780;&#20272;&#65292;&#24110;&#21161;&#24320;&#21457;&#26131;&#20110;&#29702;&#35299;&#30340;&#22270;&#20687;&#20998;&#31867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#31227;&#21160;&#24212;&#29992;&#30340;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#30340;&#24212;&#29992;&#19981;&#26029;&#36827;&#27493;&#65292;&#38656;&#35201;&#21019;&#26032;&#30340;&#29992;&#25143;&#20307;&#39564;&#35299;&#20915;&#26041;&#26696;&#26469;&#30830;&#20445;&#29992;&#25143;&#30340;&#20805;&#20998;&#20351;&#29992;&#12290;&#20026;&#20102;&#24110;&#21161;&#35774;&#35745;&#36807;&#31243;&#65292;&#36890;&#24120;&#20250;&#20026;&#29305;&#23450;&#31867;&#22411;&#30340;&#24212;&#29992;&#23450;&#21046;&#21487;&#29992;&#24615;&#21551;&#21457;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#25991;&#29486;&#32508;&#36848;&#21644;&#20998;&#26512;&#29616;&#26377;&#30340;&#20855;&#22791;&#22270;&#20687;&#20998;&#31867;&#21151;&#33021;&#30340;&#31227;&#21160;&#24212;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#21021;&#22987;&#21270;&#30340;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#31227;&#21160;&#24212;&#29992;&#22270;&#20687;&#20998;&#31867;&#30340;AIX&#21551;&#21457;&#21644;&#26816;&#26597;&#28165;&#21333;&#12290;&#20026;&#20102;&#20415;&#20110;&#28165;&#21333;&#30340;&#20351;&#29992;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#22312;&#32447;&#35838;&#31243;&#26469;&#20171;&#32461;&#27010;&#24565;&#21644;&#21551;&#21457;&#65292;&#20197;&#21450;&#19968;&#20010;&#22522;&#20110;Web&#30340;&#24037;&#20855;&#26469;&#25903;&#25345;&#20351;&#29992;&#36825;&#20123;&#21551;&#21457;&#36827;&#34892;&#35780;&#20272;&#12290;&#35813;&#30740;&#31350;&#30340;&#32467;&#26524;&#21487;&#20197;&#29992;&#20110;&#25351;&#23548;&#27492;&#31867;&#24212;&#29992;&#30340;&#30028;&#38754;&#35774;&#35745;&#65292;&#24182;&#25903;&#25345;&#36827;&#34892;&#21551;&#21457;&#24335;&#35780;&#20272;&#65292;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#24320;&#21457;&#26131;&#20110;&#29702;&#35299;&#30340;&#22270;&#20687;&#20998;&#31867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in mobile applications providing image classification enabled by Deep Learning require innovative User Experience solutions in order to assure their adequate use by users. To aid the design process, usability heuristics are typically customized for a specific kind of application. Therefore, based on a literature review and analyzing existing mobile applications with image classification, we propose an initial set of AIX heuristics for Deep Learning powered mobile applications with image classification decomposed into a checklist. In order to facilitate the usage of the checklist we also developed an online course presenting the concepts and heuristics as well as a web-based tool in order to support an evaluation using these heuristics. These results of this research can be used to guide the design of the interfaces of such applications as well as support the conduction of heuristic evaluations supporting practitioners to develop image classification apps that people can unders
&lt;/p&gt;</description></item><item><title>&#20132;&#20114;&#20914;&#31361;&#12289;&#33258;&#21160;&#21270;&#27700;&#24179;&#21644;&#33258;&#21160;&#21270;&#39057;&#29575;&#23545;&#20154;&#31867;&#22312;&#26234;&#33021;&#23478;&#23621;&#29615;&#22659;&#20013;&#23545;&#33258;&#21160;&#21270;&#30340;&#20449;&#20219;&#21644;&#25509;&#21463;&#24230;&#20135;&#29983;&#24433;&#21709;&#65292;&#33258;&#21160;&#21270;&#27700;&#24179;&#21644;&#39057;&#29575;&#36234;&#39640;&#65292;&#29992;&#25143;&#23545;&#26234;&#33021;&#29615;&#22659;&#30340;&#20449;&#20219;&#24230;&#36234;&#39640;&#65292;&#20294;&#22312;&#20986;&#29616;&#33258;&#21160;&#21270;&#25925;&#38556;&#21644;&#20132;&#20114;&#20914;&#31361;&#26102;&#65292;&#29992;&#25143;&#23545;&#33258;&#21160;&#21270;&#26234;&#33021;&#29615;&#22659;&#30340;&#25509;&#21463;&#24230;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2307.05512</link><description>&lt;p&gt;
&#20132;&#20114;&#20914;&#31361;&#12289;&#33258;&#21160;&#21270;&#27700;&#24179;&#21644;&#33258;&#21160;&#21270;&#39057;&#29575;&#23545;&#20154;&#31867;&#23545;&#33258;&#21160;&#21270;&#20449;&#20219;&#21644;&#25509;&#21463;&#24230;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Effects of Interaction Conflicts, Levels of Automation, and Frequency of Automation on Human Automation Trust and Acceptance. (arXiv:2307.05512v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05512
&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#20914;&#31361;&#12289;&#33258;&#21160;&#21270;&#27700;&#24179;&#21644;&#33258;&#21160;&#21270;&#39057;&#29575;&#23545;&#20154;&#31867;&#22312;&#26234;&#33021;&#23478;&#23621;&#29615;&#22659;&#20013;&#23545;&#33258;&#21160;&#21270;&#30340;&#20449;&#20219;&#21644;&#25509;&#21463;&#24230;&#20135;&#29983;&#24433;&#21709;&#65292;&#33258;&#21160;&#21270;&#27700;&#24179;&#21644;&#39057;&#29575;&#36234;&#39640;&#65292;&#29992;&#25143;&#23545;&#26234;&#33021;&#29615;&#22659;&#30340;&#20449;&#20219;&#24230;&#36234;&#39640;&#65292;&#20294;&#22312;&#20986;&#29616;&#33258;&#21160;&#21270;&#25925;&#38556;&#21644;&#20132;&#20114;&#20914;&#31361;&#26102;&#65292;&#29992;&#25143;&#23545;&#33258;&#21160;&#21270;&#26234;&#33021;&#29615;&#22659;&#30340;&#25509;&#21463;&#24230;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20132;&#20114;&#20914;&#31361;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#29992;&#25143;&#23545;&#33258;&#21160;&#21270;&#30340;&#20449;&#20219;&#23545;&#20110;&#25509;&#21463;&#26234;&#33021;&#29615;&#22659;&#65288;&#22914;&#26234;&#33021;&#23478;&#23621;&#65289;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#37319;&#29992;&#19968;&#20010;&#22240;&#23376;&#30740;&#31350;&#35774;&#35745;&#65292;&#30740;&#31350;&#21644;&#27604;&#36739;&#33258;&#21160;&#21270;&#27700;&#24179;&#65288;LoA&#65289;&#12289;&#33258;&#21160;&#21270;&#21709;&#24212;&#39057;&#29575;&#65288;FoA&#65289;&#21644;&#20914;&#31361;&#24378;&#24230;&#65288;CI&#65289;&#23545;&#20154;&#31867;&#22312;&#26234;&#33021;&#23478;&#23621;&#29615;&#22659;&#20013;&#23545;&#33258;&#21160;&#21270;&#30340;&#20449;&#20219;&#21644;&#25509;&#21463;&#24230;&#30340;&#21333;&#29420;&#21644;&#32852;&#21512;&#24433;&#21709;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20123;&#24433;&#21709;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22522;&#20110;&#32593;&#32476;&#30340;&#23454;&#39564;&#65292;&#20174;324&#21517;&#22312;&#32447;&#21442;&#19982;&#32773;&#20013;&#25910;&#38598;&#20102;&#25968;&#25454;&#65292;&#20182;&#20204;&#36890;&#36807;&#19968;&#20010;&#26234;&#33021;&#23478;&#23621;&#30340;3D&#27169;&#25311;&#31995;&#32479;&#20307;&#39564;&#20102;&#35813;&#31995;&#32479;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#33258;&#21160;&#21270;&#27700;&#24179;&#21644;&#39057;&#29575;&#23545;&#29992;&#25143;&#22312;&#26234;&#33021;&#29615;&#22659;&#20013;&#30340;&#20449;&#20219;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#22312;&#20986;&#29616;&#33258;&#21160;&#21270;&#25925;&#38556;&#21644;&#20132;&#20114;&#20914;&#31361;&#26102;&#65292;&#29992;&#25143;&#23545;&#33258;&#21160;&#21270;&#26234;&#33021;&#29615;&#22659;&#30340;&#25509;&#21463;&#24230;&#20250;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the presence of interaction conflicts, user trust in automation plays an important role in accepting intelligent environments such as smart homes. In this paper, a factorial research design is employed to investigate and compare the single and joint effects of Level of Automation (LoA), Frequency of Automated responses (FoA), and Conflict Intensity (CI) on human trust and acceptance of automation in the context of smart homes. To study these effects, we conducted web-based experiments to gather data from 324 online participants who experienced the system through a 3D simulation of a smart home. The findings show that the level and frequency of automation had an impact on user trust in smart environments. Furthermore, the results demonstrate that the users' acceptance of automated smart environments decreased in the presence of automation failures and interaction conflicts.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;xAI&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#20154;&#22312;AI&#24490;&#29615;&#20013;&#36827;&#34892;&#35270;&#35273;&#26816;&#26597;&#30340;&#35770;&#25991;&#25506;&#35752;&#20102;&#24037;&#19994; 5.0 &#20013;&#20154;&#26426;&#21327;&#20316;&#30340;&#26032;&#26426;&#20250;&#65292;&#24182;&#20998;&#20139;&#20102;&#20851;&#20110;&#35270;&#35273;&#26816;&#26597;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#12289;&#20154;&#31867;&#25968;&#23383;&#23402;&#29983;&#21644;&#32593;&#32476;&#23433;&#20840;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.05508</link><description>&lt;p&gt;
&#36890;&#36807;xAI&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#20154;&#22312;AI&#24490;&#29615;&#20013;&#36827;&#34892;&#35270;&#35273;&#26816;&#26597;
&lt;/p&gt;
&lt;p&gt;
Human in the AI loop via xAI and Active Learning for Visual Inspection. (arXiv:2307.05508v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05508
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;xAI&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#20154;&#22312;AI&#24490;&#29615;&#20013;&#36827;&#34892;&#35270;&#35273;&#26816;&#26597;&#30340;&#35770;&#25991;&#25506;&#35752;&#20102;&#24037;&#19994; 5.0 &#20013;&#20154;&#26426;&#21327;&#20316;&#30340;&#26032;&#26426;&#20250;&#65292;&#24182;&#20998;&#20139;&#20102;&#20851;&#20110;&#35270;&#35273;&#26816;&#26597;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#12289;&#20154;&#31867;&#25968;&#23383;&#23402;&#29983;&#21644;&#32593;&#32476;&#23433;&#20840;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;&#38761;&#21629;&#36890;&#36807;&#24341;&#20837;&#33258;&#21160;&#21270;&#26469;&#25913;&#21464;&#21046;&#36896;&#19994;&#65292;&#22686;&#21152;&#30340;&#33258;&#21160;&#21270;&#25913;&#21464;&#20102;&#20154;&#24037;&#24037;&#20154;&#30340;&#35282;&#33394;&#12290;&#26426;&#22120;&#20154;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#27493;&#24320;&#36767;&#20102;&#20154;&#26426;&#21327;&#20316;&#30340;&#26032;&#39046;&#22495;&#12290;&#26412;&#31456;&#39318;&#20808;&#25551;&#36848;&#20102;&#24037;&#19994;5.0&#65292;&#20154;&#26426;&#21327;&#20316;&#20197;&#21450;&#20851;&#20110;&#36136;&#37327;&#26816;&#26597;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#37325;&#28857;&#26159;&#35270;&#35273;&#26816;&#26597;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#22312;&#35270;&#35273;&#26816;&#26597;&#20013;&#23454;&#29616;&#21644;&#22686;&#24378;&#20154;&#26426;&#21327;&#20316;&#30340;&#35266;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20998;&#20139;&#20102;&#22312;&#27431;&#30431;H2020 STAR&#39033;&#30446;&#20013;&#20851;&#20110;&#35270;&#35273;&#26816;&#26597;&#30340;&#19968;&#20123;&#32467;&#26524;&#65292;&#32771;&#34385;&#20102;&#20154;&#24037;&#26234;&#33021;&#65292;&#20154;&#31867;&#25968;&#23383;&#23402;&#29983;&#21644;&#32593;&#32476;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;
Industrial revolutions have historically disrupted manufacturing by introducing automation into production. Increasing automation reshapes the role of the human worker. Advances in robotics and artificial intelligence open new frontiers of human-machine collaboration. In this chapter, we first describe Industry 5.0, human-machine collaboration, and state-of-the-art regarding quality inspection, emphasizing visual inspection. We then provide our perspective on how human-machine collaboration could be realized and enhanced in visual inspection. Finally, we share some of the results obtained in the EU H2020 STAR project regarding visual inspection, considering artificial intelligence, human digital twins, and cybersecurity.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22320;&#29702;&#36127;&#36733;&#24179;&#34913;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#19981;&#21516;&#22320;&#21306;&#23545;&#29615;&#22659;&#30340;&#19981;&#24179;&#31561;&#24433;&#21709;&#65292;&#20174;&#32780;&#25512;&#36827;&#29615;&#22659;&#20844;&#24179;&#30340;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2307.05494</link><description>&lt;p&gt;
&#36890;&#36807;&#22320;&#29702;&#36127;&#36733;&#24179;&#34913;&#23454;&#29616;&#29615;&#22659;&#20844;&#24179;&#30340;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Towards Environmentally Equitable AI via Geographical Load Balancing. (arXiv:2307.05494v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22320;&#29702;&#36127;&#36733;&#24179;&#34913;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#19981;&#21516;&#22320;&#21306;&#23545;&#29615;&#22659;&#30340;&#19981;&#24179;&#31561;&#24433;&#21709;&#65292;&#20174;&#32780;&#25512;&#36827;&#29615;&#22659;&#20844;&#24179;&#30340;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#22823;&#22411;&#35821;&#35328;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;&#39129;&#21319;&#20154;&#27668;&#25512;&#21160;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#30340;&#24040;&#22823;&#29615;&#22659;&#36275;&#36857;&#30340;&#24555;&#36895;&#22686;&#38271;&#21463;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#34429;&#28982;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#20351;AI&#26356;&#33410;&#33021;&#29615;&#20445;&#65292;&#20294;&#29615;&#22659;&#19981;&#24179;&#31561;&#8212;&#8212;&#21363;AI&#30340;&#29615;&#22659;&#36275;&#36857;&#22312;&#26576;&#20123;&#22320;&#21306;&#21487;&#33021;&#19981;&#25104;&#27604;&#20363;&#22320;&#26356;&#39640;&#8212;&#8212;&#24050;&#32463;&#20986;&#29616;&#65292;&#24182;&#24341;&#21457;&#20102;&#31038;&#20250;&#29983;&#24577;&#27491;&#20041;&#30340;&#20851;&#20999;&#12290;&#26412;&#25991;&#36890;&#36807;&#24179;&#34913;AI&#30340;&#21306;&#22495;&#36127;&#38754;&#29615;&#22659;&#24433;&#21709;&#26469;&#39318;&#27425;&#35299;&#20915;AI&#30340;&#29615;&#22659;&#19981;&#24179;&#31561;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;AI&#27169;&#22411;&#25512;&#29702;&#30340;&#30899;&#36275;&#36857;&#21644;&#27700;&#36275;&#36857;&#65292;&#24182;&#25552;&#20986;&#20102;&#27880;&#37325;&#20844;&#24179;&#30340;&#22320;&#29702;&#36127;&#36733;&#24179;&#34913;&#65288;GLB&#65289;&#26469;&#26126;&#30830;&#35299;&#20915;AI&#23545;&#26368;&#24369;&#21183;&#22320;&#21306;&#30340;&#29615;&#22659;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#19968;&#32452;&#20998;&#24067;&#22312;&#22320;&#29702;&#19978;&#30340;10&#20010;&#25968;&#25454;&#20013;&#24515;&#26469;&#36816;&#34892;&#22522;&#20110;&#36319;&#36394;&#30340;&#20223;&#30495;&#65292;&#36825;&#20123;&#25968;&#25454;&#20013;&#24515;&#20026;&#22823;&#22411;L
&lt;/p&gt;
&lt;p&gt;
Fueled by the soaring popularity of large language and foundation models, the accelerated growth of artificial intelligence (AI) models' enormous environmental footprint has come under increased scrutiny. While many approaches have been proposed to make AI more energy-efficient and environmentally friendly, environmental inequity -- the fact that AI's environmental footprint can be disproportionately higher in certain regions than in others -- has emerged, raising social-ecological justice concerns. This paper takes a first step toward addressing AI's environmental inequity by balancing its regional negative environmental impact. Concretely, we focus on the carbon and water footprints of AI model inference and propose equity-aware geographical load balancing (GLB) to explicitly address AI's environmental impacts on the most disadvantaged regions. We run trace-based simulations by considering a set of 10 geographically-distributed data centers that serve inference requests for a large l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20013;&#23398;&#33521;&#35821;&#20316;&#20026;&#22806;&#35821;&#23398;&#29983;&#19982;ChatGPT&#21512;&#20316;&#23436;&#25104;&#20889;&#20316;&#20219;&#21153;&#30340;&#26696;&#20363;&#65292;&#24182;&#23545;&#25552;&#31034;&#30340;&#36136;&#37327;&#21644;&#25968;&#37327;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#38750;&#25216;&#26415;&#29992;&#25143;&#22312;&#20026;ChatGPT&#32534;&#20889;&#36866;&#24403;&#30340;&#25552;&#31034;&#19978;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#38656;&#35201;&#25552;&#20379;&#26356;&#22909;&#30340;&#25903;&#25345;&#21644;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2307.05493</link><description>&lt;p&gt;
&#20013;&#23398;&#33521;&#35821;&#20316;&#20026;&#22806;&#35821;&#23398;&#29983;&#19982;ChatGPT&#21512;&#20316;&#23436;&#25104;&#20889;&#20316;&#20219;&#21153;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Cases of EFL Secondary Students' Prompt Engineering Pathways to Complete a Writing Task with ChatGPT. (arXiv:2307.05493v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20013;&#23398;&#33521;&#35821;&#20316;&#20026;&#22806;&#35821;&#23398;&#29983;&#19982;ChatGPT&#21512;&#20316;&#23436;&#25104;&#20889;&#20316;&#20219;&#21153;&#30340;&#26696;&#20363;&#65292;&#24182;&#23545;&#25552;&#31034;&#30340;&#36136;&#37327;&#21644;&#25968;&#37327;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#38750;&#25216;&#26415;&#29992;&#25143;&#22312;&#20026;ChatGPT&#32534;&#20889;&#36866;&#24403;&#30340;&#25552;&#31034;&#19978;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#38656;&#35201;&#25552;&#20379;&#26356;&#22909;&#30340;&#25903;&#25345;&#21644;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#19968;&#20010;&#26368;&#20808;&#36827;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#23613;&#31649;&#23427;&#26377;&#28508;&#21147;&#25903;&#25345;&#33521;&#35821;&#20316;&#20026;&#22806;&#35821;&#23398;&#29983;&#30340;&#20889;&#20316;&#65292;&#20294;&#35201;&#26377;&#25928;&#22320;&#19982;&#20043;&#21512;&#20316;&#65292;&#23398;&#29983;&#24517;&#39035;&#23398;&#20250;&#35774;&#35745;&#25552;&#31034;&#65292;&#21363;&#21046;&#20316;&#36866;&#24403;&#30340;&#25351;&#20196;&#65292;&#20197;&#20351;ChatGPT&#20135;&#29983;&#26399;&#26395;&#30340;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#38750;&#25216;&#26415;&#29992;&#25143;&#26469;&#35828;&#65292;&#20026;ChatGPT&#32534;&#20889;&#36866;&#24403;&#30340;&#25552;&#31034;&#24182;&#38750;&#26131;&#20107;&#65292;&#20182;&#20204;&#32463;&#21382;&#20102;&#21453;&#22797;&#35797;&#38169;&#30340;&#36807;&#31243;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20013;&#23398;&#33521;&#35821;&#20316;&#20026;&#22806;&#35821;&#23398;&#29983;&#22312;&#23436;&#25104;&#20889;&#20316;&#20219;&#21153;&#26102;&#20351;&#29992;ChatGPT&#30340;&#25552;&#31034;&#20869;&#23481;&#65292;&#24182;&#25506;&#35752;&#20102;&#25552;&#31034;&#30340;&#36136;&#37327;&#21644;&#25968;&#37327;&#30340;&#27169;&#24335;&#12290;&#25968;&#25454;&#26469;&#33258;iPad&#23631;&#24149;&#24405;&#20687;&#65292;&#35760;&#24405;&#20102;&#39318;&#27425;&#20351;&#29992;ChatGPT&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#32842;&#22825;&#26426;&#22120;&#20154;&#23436;&#25104;&#30456;&#21516;&#20889;&#20316;&#20219;&#21153;&#30340;&#20013;&#23398;&#33521;&#35821;&#20316;&#20026;&#22806;&#35821;&#23398;&#29983;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#36890;&#36807;&#22235;&#20010;&#19981;&#21516;&#30340;&#36335;&#24452;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#35797;&#38169;&#36807;&#31243;&#21644;&#25552;&#31034;&#20869;&#23481;&#21644;&#25968;&#37327;&#30340;&#19981;&#21516;&#32452;&#21512;&#12290;&#36825;&#20123;&#26696;&#20363;&#20026;&#25552;&#20379;&#25903;&#25345;&#38750;&#25216;&#26415;&#29992;&#25143;&#26377;&#25928;&#20351;&#29992;ChatGPT&#30340;&#35777;&#25454;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a state-of-the-art (SOTA) chatbot. Although it has potential to support English as a foreign language (EFL) students' writing, to effectively collaborate with it, a student must learn to engineer prompts, that is, the skill of crafting appropriate instructions so that ChatGPT produces desired outputs. However, writing an appropriate prompt for ChatGPT is not straightforward for non-technical users who suffer a trial-and-error process. This paper examines the content of EFL students' ChatGPT prompts when completing a writing task and explores patterns in the quality and quantity of the prompts. The data come from iPad screen recordings of secondary school EFL students who used ChatGPT and other SOTA chatbots for the first time to complete the same writing task. The paper presents a case study of four distinct pathways that illustrate the trial-and-error process and show different combinations of prompt content and quantity. The cases contribute evidence for the need to provid
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27604;&#36739;GPT&#21644;&#20154;&#31867;&#23457;&#31295;&#20154;&#29983;&#25104;&#30340;&#35780;&#35770;&#65292;&#25105;&#20204;&#21457;&#29616;GPT&#22312;&#21516;&#34892;&#35780;&#23457;&#20013;&#20855;&#26377;&#19968;&#23450;&#30340;&#24110;&#21161;&#24615;&#65292;&#20026;&#35299;&#20915;&#21516;&#34892;&#35780;&#23457;&#36164;&#28304;&#38480;&#21046;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2307.05492</link><description>&lt;p&gt;
GPT4&#23545;&#21516;&#34892;&#35780;&#23457;&#23384;&#22312;&#19968;&#23450;&#24110;&#21161;&#65306;&#19968;&#39033;&#23454;&#39564;&#24615;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
GPT4 is Slightly Helpful for Peer-Review Assistance: A Pilot Study. (arXiv:2307.05492v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05492
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27604;&#36739;GPT&#21644;&#20154;&#31867;&#23457;&#31295;&#20154;&#29983;&#25104;&#30340;&#35780;&#35770;&#65292;&#25105;&#20204;&#21457;&#29616;GPT&#22312;&#21516;&#34892;&#35780;&#23457;&#20013;&#20855;&#26377;&#19968;&#23450;&#30340;&#24110;&#21161;&#24615;&#65292;&#20026;&#35299;&#20915;&#21516;&#34892;&#35780;&#23457;&#36164;&#28304;&#38480;&#21046;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#23454;&#39564;&#24615;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;GPT4&#22312;&#21516;&#34892;&#35780;&#23457;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#20551;&#35774;&#26159;&#65292;&#36890;&#36807;&#19982;&#20154;&#31867;&#23457;&#31295;&#20154;&#29983;&#25104;&#30340;&#35780;&#35770;&#36827;&#34892;&#27604;&#36739;&#65292;GPT&#25152;&#29983;&#25104;&#30340;&#35780;&#35770;&#21487;&#20197;&#36798;&#21040;&#30456;&#20284;&#30340;&#26377;&#29992;&#24615;&#12290;&#36890;&#36807;&#27604;&#36739;&#22312;&#19968;&#20010;&#37325;&#35201;&#30340;&#26426;&#22120;&#23398;&#20064;&#20250;&#35758;&#19978;&#25552;&#20132;&#30340;&#23398;&#26415;&#35770;&#25991;&#30340;&#20154;&#31867;&#23457;&#31295;&#20154;&#21644;GPT&#27169;&#22411;&#29983;&#25104;&#30340;&#35780;&#35770;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21021;&#27493;&#35777;&#25454;&#65292;&#34920;&#26126;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#26377;&#25928;&#22320;&#20026;&#21516;&#34892;&#35780;&#23457;&#36807;&#31243;&#20570;&#20986;&#36129;&#29486;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23545;&#25554;&#20837;&#20102;&#38169;&#35823;&#30340;&#40065;&#26834;&#24615;&#23454;&#39564;&#65292;&#20197;&#20102;&#35299;&#27169;&#22411;&#26356;&#20542;&#21521;&#20110;&#20851;&#27880;&#35770;&#25991;&#30340;&#21738;&#20123;&#37096;&#20998;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20026;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#35299;&#20915;&#21516;&#34892;&#35780;&#23457;&#36164;&#28304;&#38480;&#21046;&#24320;&#36767;&#20102;&#26032;&#36884;&#24452;&#12290;&#35813;&#32467;&#26524;&#36824;&#20026;&#25913;&#36827;&#35780;&#23457;&#36807;&#31243;&#25552;&#20379;&#20102;&#21551;&#31034;&#65292;&#24182;&#20026;&#22312;&#20154;&#31867;&#21453;&#39304;&#36164;&#28304;&#31232;&#32570;&#30340;&#39046;&#22495;&#20013;&#36827;&#19968;&#27493;&#30740;&#31350;&#25193;&#22823;&#30417;&#30563;&#25171;&#19979;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this pilot study, we investigate the use of GPT4 to assist in the peer-review process. Our key hypothesis was that GPT-generated reviews could achieve comparable helpfulness to human reviewers. By comparing reviews generated by both human reviewers and GPT models for academic papers submitted to a major machine learning conference, we provide initial evidence that artificial intelligence can contribute effectively to the peer-review process. We also perform robustness experiments with inserted errors to understand which parts of the paper the model tends to focus on. Our findings open new avenues for leveraging machine learning tools to address resource constraints in peer review. The results also shed light on potential enhancements to the review process and lay the groundwork for further research on scaling oversight in a domain where human-feedback is increasingly a scarce resource.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;VampNet&#30340;&#38899;&#20048;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#25513;&#30721;&#22768;&#23398;&#20196;&#29260;&#24314;&#27169;&#23454;&#29616;&#29983;&#25104;&#12289;&#21387;&#32553;&#12289;&#20462;&#22797;&#21644;&#21464;&#24322;&#12290;VampNet&#36890;&#36807;&#28789;&#27963;&#30340;&#25552;&#31034;&#33021;&#21147;&#65292;&#33021;&#22815;&#20445;&#25345;&#38899;&#20048;&#30340;&#39118;&#26684;&#12289;&#27969;&#27966;&#12289;&#20048;&#22120;&#21644;&#20854;&#20182;&#39640;&#23618;&#27425;&#26041;&#38754;&#65292;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#38899;&#20048;&#20849;&#21019;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2307.04686</link><description>&lt;p&gt;
VampNet&#65306;&#36890;&#36807;&#25513;&#30721;&#22768;&#23398;&#20196;&#29260;&#24314;&#27169;&#36827;&#34892;&#38899;&#20048;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
VampNet: Music Generation via Masked Acoustic Token Modeling. (arXiv:2307.04686v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;VampNet&#30340;&#38899;&#20048;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#25513;&#30721;&#22768;&#23398;&#20196;&#29260;&#24314;&#27169;&#23454;&#29616;&#29983;&#25104;&#12289;&#21387;&#32553;&#12289;&#20462;&#22797;&#21644;&#21464;&#24322;&#12290;VampNet&#36890;&#36807;&#28789;&#27963;&#30340;&#25552;&#31034;&#33021;&#21147;&#65292;&#33021;&#22815;&#20445;&#25345;&#38899;&#20048;&#30340;&#39118;&#26684;&#12289;&#27969;&#27966;&#12289;&#20048;&#22120;&#21644;&#20854;&#20182;&#39640;&#23618;&#27425;&#26041;&#38754;&#65292;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#38899;&#20048;&#20849;&#21019;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;VampNet&#65292;&#19968;&#31181;&#36890;&#36807;&#25513;&#30721;&#22768;&#23398;&#20196;&#29260;&#24314;&#27169;&#26041;&#27861;&#36827;&#34892;&#38899;&#20048;&#21512;&#25104;&#12289;&#21387;&#32553;&#12289;&#20462;&#22797;&#21644;&#21464;&#24322;&#12290;&#25105;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#21487;&#21464;&#30340;&#25513;&#30721;&#35745;&#21010;&#65292;&#36890;&#36807;&#24212;&#29992;&#21508;&#31181;&#25513;&#30721;&#26041;&#27861;&#65288;&#31216;&#20026;&#25552;&#31034;&#65289;&#36827;&#34892;&#25512;&#29702;&#26469;&#20174;&#27169;&#22411;&#20013;&#37319;&#26679;&#36830;&#36143;&#30340;&#38899;&#20048;&#12290;VampNet&#26159;&#38750;&#33258;&#22238;&#24402;&#30340;&#65292;&#21033;&#29992;&#21452;&#21521;Transformer&#26550;&#26500;&#65292;&#22312;&#27491;&#21521;&#20256;&#36882;&#20013;&#20851;&#27880;&#25152;&#26377;&#20196;&#29260;&#12290;&#32463;&#36807;36&#27425;&#37319;&#26679;&#20256;&#36882;&#65292;VampNet&#21487;&#20197;&#29983;&#25104;&#36830;&#36143;&#30340;&#39640;&#20445;&#30495;&#38899;&#20048;&#27874;&#24418;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20197;&#19981;&#21516;&#26041;&#24335;&#25552;&#31034;VampNet&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#20854;&#24212;&#29992;&#20110;&#38899;&#20048;&#21387;&#32553;&#12289;&#20462;&#22797;&#12289;&#25193;&#23637;&#12289;&#24310;&#32493;&#21644;&#24490;&#29615;&#21464;&#24322;&#65288;vamping&#65289;&#31561;&#20219;&#21153;&#12290;&#22312;&#36866;&#24403;&#30340;&#25552;&#31034;&#19979;&#65292;VampNet&#33021;&#22815;&#20445;&#25345;&#38899;&#20048;&#30340;&#39118;&#26684;&#12289;&#27969;&#27966;&#12289;&#20048;&#22120;&#21644;&#20854;&#20182;&#39640;&#23618;&#27425;&#26041;&#38754;&#12290;&#36825;&#31181;&#28789;&#27963;&#30340;&#25552;&#31034;&#33021;&#21147;&#20351;VampNet&#25104;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#38899;&#20048;&#20849;&#21019;&#24037;&#20855;&#12290;&#20195;&#30721;&#21644;&#38899;&#39057;&#26679;&#26412;&#21487;&#22312;&#32593;&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce VampNet, a masked acoustic token modeling approach to music synthesis, compression, inpainting, and variation. We use a variable masking schedule during training which allows us to sample coherent music from the model by applying a variety of masking approaches (called prompts) during inference. VampNet is non-autoregressive, leveraging a bidirectional transformer architecture that attends to all tokens in a forward pass. With just 36 sampling passes, VampNet can generate coherent high-fidelity musical waveforms. We show that by prompting VampNet in various ways, we can apply it to tasks like music compression, inpainting, outpainting, continuation, and looping with variation (vamping). Appropriately prompted, VampNet is capable of maintaining style, genre, instrumentation, and other high-level aspects of the music. This flexible prompting capability makes VampNet a powerful music co-creation tool. Code and audio samples are available online.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#23450;&#20301;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#21307;&#23398;&#24212;&#29992;&#20013;&#20351;&#29992;&#22823;&#37327;&#24369;&#26631;&#31614;&#22270;&#20687;&#36827;&#34892;&#32925;&#30828;&#21270;&#39044;&#27979;&#12290;&#36825;&#31181;&#31574;&#30053;&#23558;&#27599;&#20010;2D&#20999;&#29255;&#30340;&#31354;&#38388;&#19978;&#19979;&#25991;&#21644;&#24369;&#26631;&#31614;&#25972;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#20351;&#29992;&#36890;&#29992;&#22522;&#20110;&#26680;&#30340;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.04617</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#23450;&#20301;&#23545;&#27604;&#23398;&#20064;&#65306;&#32925;&#30828;&#21270;&#20998;&#31867;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Weakly-supervised positional contrastive learning: application to cirrhosis classification. (arXiv:2307.04617v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#23450;&#20301;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#21307;&#23398;&#24212;&#29992;&#20013;&#20351;&#29992;&#22823;&#37327;&#24369;&#26631;&#31614;&#22270;&#20687;&#36827;&#34892;&#32925;&#30828;&#21270;&#39044;&#27979;&#12290;&#36825;&#31181;&#31574;&#30053;&#23558;&#27599;&#20010;2D&#20999;&#29255;&#30340;&#31354;&#38388;&#19978;&#19979;&#25991;&#21644;&#24369;&#26631;&#31614;&#25972;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#20351;&#29992;&#36890;&#29992;&#22522;&#20110;&#26680;&#30340;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#21307;&#23398;&#24433;&#20687;&#25968;&#25454;&#38598;&#21487;&#20197;&#36890;&#36807;&#20302;&#32622;&#20449;&#24230;&#30340;&#24369;&#26631;&#31614;&#65288;&#20363;&#22914;&#25918;&#23556;&#23398;&#35780;&#20998;&#65289;&#36827;&#34892;&#24265;&#20215;&#24555;&#36895;&#30340;&#27880;&#37322;&#12290;&#32780;&#39640;&#32622;&#20449;&#24230;&#30340;&#26631;&#31614;&#65288;&#22914;&#22522;&#20110;&#32452;&#32455;&#23398;&#30340;&#35786;&#26029;&#65289;&#24456;&#23569;&#19988;&#26114;&#36149;&#12290;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#22914;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#26410;&#26631;&#35760;&#25110;&#24369;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#36739;&#22823;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#65292;&#36825;&#22312;&#22823;&#22411;3D&#22270;&#20687;&#30340;&#20840;&#20998;&#36776;&#29575;&#24773;&#20917;&#19979;&#23384;&#22312;&#22256;&#38590;&#65292;&#22240;&#20026;GPU&#20869;&#23384;&#26377;&#38480;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#20851;&#20110;&#27599;&#20010;2D&#20999;&#29255;&#30340;&#31354;&#38388;&#19978;&#19979;&#25991;&#30340;&#20307;&#31215;&#20449;&#24687;&#23545;&#20110;&#26576;&#20123;&#21307;&#23398;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#24369;&#30417;&#30563;&#23450;&#20301;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;&#29992;&#36890;&#29992;&#22522;&#20110;&#26680;&#30340;&#25439;&#22833;&#20989;&#25968;&#23558;&#27599;&#20010;2D&#20999;&#29255;&#30340;&#31354;&#38388;&#19978;&#19979;&#25991;&#21644;&#24369;&#26631;&#31614;&#36827;&#34892;&#25972;&#21512;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22823;&#37327;&#24369;&#26631;&#31614;&#22270;&#20687;&#65288;&#21363;&#25918;&#23556;&#23398;&#20302;&#32622;&#20449;&#24230;&#26631;&#27880;&#65289;&#26469;&#35828;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32925;&#30828;&#21270;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large medical imaging datasets can be cheaply and quickly annotated with low-confidence, weak labels (e.g., radiological scores). Access to high-confidence labels, such as histology-based diagnoses, is rare and costly. Pretraining strategies, like contrastive learning (CL) methods, can leverage unlabeled or weakly-annotated datasets. These methods typically require large batch sizes, which poses a difficulty in the case of large 3D images at full resolution, due to limited GPU memory. Nevertheless, volumetric positional information about the spatial context of each 2D slice can be very important for some medical applications. In this work, we propose an efficient weakly-supervised positional (WSP) contrastive learning strategy where we integrate both the spatial context of each 2D slice and a weak label via a generic kernel-based loss function. We illustrate our method on cirrhosis prediction using a large volume of weakly-labeled images, namely radiological low-confidence annotations,
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#24314;&#31569;&#26045;&#24037;&#39046;&#22495;&#20013;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#23454;&#29616;&#20154;&#26426;&#30452;&#35266;&#20132;&#27969;&#30340;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#26045;&#24037;&#29616;&#22330;&#22797;&#26434;&#24615;&#21644;&#20154;&#21147;&#30701;&#32570;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.04195</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#29992;&#20110;&#24314;&#31569;&#26045;&#24037;&#39046;&#22495;&#20013;&#20154;&#26426;&#20114;&#21160;&#30340;&#30452;&#35266;&#24615;&#20132;&#27969;
&lt;/p&gt;
&lt;p&gt;
Natural Language Instructions for Intuitive Human Interaction with Robotic Assistants in Field Construction Work. (arXiv:2307.04195v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04195
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#24314;&#31569;&#26045;&#24037;&#39046;&#22495;&#20013;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#23454;&#29616;&#20154;&#26426;&#30452;&#35266;&#20132;&#27969;&#30340;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#26045;&#24037;&#29616;&#22330;&#22797;&#26434;&#24615;&#21644;&#20154;&#21147;&#30701;&#32570;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#30340;&#24341;&#20837;&#34987;&#24191;&#27867;&#35748;&#20026;&#26377;&#26395;&#35299;&#20915;&#26045;&#24037;&#34892;&#19994;&#38754;&#20020;&#30340;&#21171;&#21160;&#21147;&#30701;&#32570;&#21644;&#29983;&#20135;&#21147;&#20572;&#28382;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#21644;&#26080;&#32467;&#26500;&#30340;&#26045;&#24037;&#29616;&#22330;&#20351;&#29992;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#26426;&#22120;&#20154;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20154;&#26426;&#21327;&#20316;&#22312;&#26045;&#24037;&#20013;&#26174;&#31034;&#20986;&#20102;&#32467;&#21512;&#20154;&#31867;&#24037;&#20154;&#30340;&#28789;&#27963;&#24615;&#21644;&#26426;&#22120;&#20154;&#21161;&#25163;&#30340;&#29289;&#29702;&#33021;&#21147;&#20849;&#21516;&#24212;&#23545;&#26045;&#24037;&#24037;&#20316;&#20013;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#28508;&#21147;&#12290;&#22312;&#24341;&#20837;&#20154;&#26426;&#21327;&#20316;&#26102;&#65292;&#35782;&#21035;&#22242;&#38431;&#21512;&#20316;&#21644;&#30417;&#30563;&#22312;&#23454;&#22320;&#26045;&#24037;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20026;&#20154;&#31867;&#24037;&#20154;&#21644;&#26426;&#22120;&#20154;&#21161;&#25163;&#24314;&#31435;&#19968;&#31181;&#33258;&#28982;&#30452;&#35266;&#30340;&#20132;&#27969;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#20132;&#20114;&#21487;&#20197;&#20351;&#38750;&#26426;&#22120;&#20154;&#32534;&#31243;&#19987;&#23478;&#30340;&#20154;&#31867;&#24037;&#20154;&#19982;&#26426;&#22120;&#20154;&#20043;&#38388;&#36827;&#34892;&#30452;&#35266;&#21644;&#29087;&#24713;&#30340;&#20132;&#27969;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22312;&#24314;&#31569;&#39046;&#22495;&#23545;&#36825;&#19968;&#20027;&#39064;&#30340;&#30740;&#31350;&#36824;&#24456;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#20801;&#35768;&#23454;&#29616;&#36825;&#31181;&#30452;&#35266;&#30340;&#20132;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;
The introduction of robots is widely considered to have significant potential of alleviating the issues of worker shortage and stagnant productivity that afflict the construction industry. However, it is challenging to use fully automated robots in complex and unstructured construction sites. Human-Robot Collaboration (HRC) has shown promise of combining human workers' flexibility and robot assistants' physical abilities to jointly address the uncertainties inherent in construction work. When introducing HRC in construction, it is critical to recognize the importance of teamwork and supervision in field construction and establish a natural and intuitive communication system for the human workers and robotic assistants. Natural language-based interaction can enable intuitive and familiar communication with robots for human workers who are non-experts in robot programming. However, limited research has been conducted on this topic in construction. This paper proposes a framework to allow
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28508;&#22312;&#22270;&#27880;&#24847;&#21147;&#65288;LGA&#65289;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#22270;&#20687;&#20013;&#30340;&#20840;&#23616;&#19978;&#19979;&#25991;&#12290;LGA&#22312;&#35745;&#31639;&#19978;&#31616;&#27905;&#19988;&#31283;&#23450;&#65292;&#33021;&#22815;&#22312;&#23567;&#35268;&#27169;&#20307;&#31995;&#32467;&#26500;&#20013;&#23454;&#29616;&#25509;&#36817;&#22823;&#35268;&#27169;&#20307;&#31995;&#32467;&#26500;&#30340;&#24615;&#33021;&#65292;&#20351;&#24471;&#36731;&#37327;&#32423;&#20307;&#31995;&#32467;&#26500;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#26356;&#21152;&#23454;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.04149</link><description>&lt;p&gt;
&#22686;&#24378;&#31354;&#38388;&#19978;&#19979;&#25991;&#30340;&#28508;&#22312;&#22270;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Latent Graph Attention for Enhanced Spatial Context. (arXiv:2307.04149v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28508;&#22312;&#22270;&#27880;&#24847;&#21147;&#65288;LGA&#65289;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#22270;&#20687;&#20013;&#30340;&#20840;&#23616;&#19978;&#19979;&#25991;&#12290;LGA&#22312;&#35745;&#31639;&#19978;&#31616;&#27905;&#19988;&#31283;&#23450;&#65292;&#33021;&#22815;&#22312;&#23567;&#35268;&#27169;&#20307;&#31995;&#32467;&#26500;&#20013;&#23454;&#29616;&#25509;&#36817;&#22823;&#35268;&#27169;&#20307;&#31995;&#32467;&#26500;&#30340;&#24615;&#33021;&#65292;&#20351;&#24471;&#36731;&#37327;&#32423;&#20307;&#31995;&#32467;&#26500;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#26356;&#21152;&#23454;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20013;&#30340;&#20840;&#23616;&#19978;&#19979;&#25991;&#22312;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#38382;&#39064;&#20013;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#21644;&#22270;&#27169;&#22411;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#25429;&#25417;&#21040;&#20102;&#20840;&#23616;&#19978;&#19979;&#25991;&#65292;&#20294;&#26159;&#36825;&#20123;&#26041;&#27861;&#35745;&#31639;&#19978;&#27604;&#36739;&#26114;&#36149;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#21482;&#33021;&#23398;&#20064;&#22270;&#20687;&#20013;&#20219;&#24847;&#20004;&#20010;&#28857;&#20043;&#38388;&#30340;&#37197;&#23545;&#35821;&#20041;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28508;&#22312;&#22270;&#27880;&#24847;&#21147;&#65288;LGA&#65289;&#30340;&#35745;&#31639;&#31616;&#27905;&#65288;&#19982;&#33410;&#28857;&#25968;&#37327;&#21576;&#32447;&#24615;&#20851;&#31995;&#65289;&#21644;&#31283;&#23450;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20840;&#23616;&#19978;&#19979;&#25991;&#32435;&#20837;&#29616;&#26377;&#20307;&#31995;&#32467;&#26500;&#20013;&#65292;&#29305;&#21035;&#26159;&#22686;&#24378;&#23567;&#35268;&#27169;&#20307;&#31995;&#32467;&#26500;&#30340;&#24615;&#33021;&#65292;&#20351;&#24471;&#36731;&#37327;&#32423;&#20307;&#31995;&#32467;&#26500;&#23545;&#20110;&#35745;&#31639;&#33021;&#21147;&#36739;&#20302;&#21644;&#33021;&#37327;&#38656;&#27714;&#36739;&#20302;&#30340;&#36793;&#32536;&#35774;&#22791;&#26356;&#21152;&#26377;&#29992;&#12290;LGA&#20351;&#29992;&#23616;&#37096;&#36830;&#25509;&#22270;&#32593;&#32476;&#26469;&#22312;&#31354;&#38388;&#19978;&#20256;&#25773;&#20449;&#24687;&#65292;&#20174;&#32780;&#26041;&#20415;&#22320;&#26500;&#24314;&#36828;&#36317;&#31163;&#30340;&#20004;&#20010;&#31354;&#38388;&#28857;&#20043;&#38388;&#30340;&#35821;&#20041;&#19968;&#33268;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Global contexts in images are quite valuable in image-to-image translation problems. Conventional attention-based and graph-based models capture the global context to a large extent, however, these are computationally expensive. Moreover, the existing approaches are limited to only learning the pairwise semantic relation between any two points on the image. In this paper, we present Latent Graph Attention (LGA) a computationally inexpensive (linear to the number of nodes) and stable, modular framework for incorporating the global context in the existing architectures, especially empowering small-scale architectures to give performance closer to large size architectures, thus making the light-weight architectures more useful for edge devices with lower compute power and lower energy needs. LGA propagates information spatially using a network of locally connected graphs, thereby facilitating to construct a semantically coherent relation between any two spatially distant points that also 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#35270;&#39057;&#29255;&#27573;&#20013;&#25552;&#21462;&#30340;&#29289;&#20307;&#34892;&#20026;&#36827;&#34892;&#25512;&#29702;&#26469;&#35782;&#21035;&#21103;&#35789;&#31867;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25928;&#26524;&#19978;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.04132</link><description>&lt;p&gt;
&#23545;&#35270;&#39057;&#29255;&#27573;&#20013;&#29289;&#20307;&#34892;&#20026;&#30340;&#25512;&#29702;&#29992;&#20110;&#21103;&#35789;&#31867;&#22411;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Reasoning over the Behaviour of Objects in Video-Clips for Adverb-Type Recognition. (arXiv:2307.04132v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#35270;&#39057;&#29255;&#27573;&#20013;&#25552;&#21462;&#30340;&#29289;&#20307;&#34892;&#20026;&#36827;&#34892;&#25512;&#29702;&#26469;&#35782;&#21035;&#21103;&#35789;&#31867;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25928;&#26524;&#19978;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26681;&#25454;&#23545;&#25551;&#36848;&#22330;&#26223;&#24207;&#21015;&#30340;&#21103;&#35789;&#26368;&#20339;&#35782;&#21035;&#26041;&#27861;&#20026;&#22522;&#30784;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#20174;&#21407;&#22987;&#35270;&#39057;&#29255;&#27573;&#20013;&#25552;&#21462;&#30340;&#29289;&#20307;&#34892;&#20026;&#36827;&#34892;&#25512;&#29702;&#26469;&#35782;&#21035;&#29255;&#27573;&#23545;&#24212;&#30340;&#21103;&#35789;&#31867;&#22411;&#12290;&#19982;&#20043;&#21069;&#38024;&#23545;&#24120;&#35268;&#22330;&#26223;&#21103;&#35789;&#35782;&#21035;&#30340;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#35270;&#39057;&#29255;&#27573;&#30340;&#34892;&#21160;&#31867;&#22411;&#26410;&#30693;&#30340;&#26356;&#19968;&#33324;&#30340;&#38382;&#39064;&#35774;&#32622;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27969;&#31243;&#65292;&#20174;&#21407;&#22987;&#35270;&#39057;&#29255;&#27573;&#20013;&#25552;&#21462;&#20102;&#21487;&#20197;&#20154;&#31867;&#29702;&#35299;&#30340;&#29289;&#20307;&#34892;&#20026;&#20107;&#23454;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#31526;&#21495;&#21644;&#36716;&#25442;&#22120;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#23545;&#36825;&#20123;&#25552;&#21462;&#20986;&#30340;&#20107;&#23454;&#36827;&#34892;&#25805;&#20316;&#20197;&#35782;&#21035;&#21103;&#35789;&#31867;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25903;&#25345;&#31526;&#21495;&#35270;&#39057;&#22788;&#29702;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#35270;&#39057;&#39046;&#22495;&#30456;&#20851;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, following the intuition that adverbs describing scene-sequences are best identified by reasoning over high-level concepts of object-behavior, we propose the design of a new framework that reasons over object-behaviours extracted from raw-video-clips to recognize the clip's corresponding adverb-types. Importantly, while previous works for general scene adverb-recognition assume knowledge of the clips underlying action-types, our method is directly applicable in the more general problem setting where the action-type of a video-clip is unknown. Specifically, we propose a novel pipeline that extracts human-interpretable object-behaviour-facts from raw video clips and propose novel symbolic and transformer based reasoning methods that operate over these extracted facts to identify adverb-types. Experiment results demonstrate that our proposed methods perform favourably against the previous state-of-the-art. Additionally, to support efforts in symbolic video-processing, we rele
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#30417;&#31649;&#38656;&#35201;&#26631;&#20934;&#21046;&#23450;&#12289;&#27880;&#20876;&#25253;&#21578;&#21644;&#23433;&#20840;&#21512;&#35268;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.03718</link><description>&lt;p&gt;
&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#30417;&#31649;&#65306;&#31649;&#29702;&#23545;&#20844;&#20849;&#23433;&#20840;&#30340;&#26032;&#20852;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Frontier AI Regulation: Managing Emerging Risks to Public Safety. (arXiv:2307.03718v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03718
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#30417;&#31649;&#38656;&#35201;&#26631;&#20934;&#21046;&#23450;&#12289;&#27880;&#20876;&#25253;&#21578;&#21644;&#23433;&#20840;&#21512;&#35268;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#20026;&#20154;&#31867;&#24102;&#26469;&#24040;&#22823;&#30340;&#22909;&#22788;&#65292;&#20294;&#31038;&#20250;&#38656;&#35201;&#20027;&#21160;&#31649;&#29702;&#30456;&#20851;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#20851;&#27880;&#25105;&#20204;&#25152;&#31216;&#30340;&#8220;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#8221;&#27169;&#22411;&#65306;&#39640;&#24230;&#33021;&#21147;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21487;&#33021;&#20855;&#22791;&#36275;&#20197;&#23545;&#20844;&#20849;&#23433;&#20840;&#36896;&#25104;&#20005;&#37325;&#39118;&#38505;&#30340;&#21361;&#38505;&#33021;&#21147;&#12290;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24102;&#26469;&#20102;&#29420;&#29305;&#30340;&#30417;&#31649;&#25361;&#25112;&#65306;&#21361;&#38505;&#33021;&#21147;&#21487;&#33021;&#20986;&#20046;&#24847;&#26009;&#65307;&#24456;&#38590;&#26377;&#25928;&#38450;&#27490;&#37096;&#32626;&#27169;&#22411;&#34987;&#28389;&#29992;&#65307;&#24182;&#19988;&#24456;&#38590;&#38459;&#27490;&#27169;&#22411;&#30340;&#33021;&#21147;&#24191;&#27867;&#25193;&#25955;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#36793;&#32536;&#27169;&#22411;&#30340;&#30417;&#31649;&#38656;&#35201;&#33267;&#23569;&#19977;&#20010;&#22522;&#26412;&#35201;&#32032;&#65306;(1) &#35774;&#23450;&#26631;&#20934;&#30340;&#36807;&#31243;&#65292;&#20197;&#30830;&#23450;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#32773;&#30340;&#36866;&#24403;&#35201;&#27714;&#65307;(2) &#27880;&#20876;&#21644;&#25253;&#21578;&#35201;&#27714;&#65292;&#20026;&#30417;&#31649;&#26426;&#26500;&#25552;&#20379;&#23545;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#36807;&#31243;&#30340;&#21487;&#35265;&#24615;&#65307;(3) &#20445;&#35777;&#24320;&#21457;&#21644;&#37096;&#32626;&#30340;&#23433;&#20840;&#26631;&#20934;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advanced AI models hold the promise of tremendous benefits for humanity, but society needs to proactively manage the accompanying risks. In this paper, we focus on what we term "frontier AI" models: highly capable foundation models that could possess dangerous capabilities sufficient to pose severe risks to public safety. Frontier AI models pose a distinct regulatory challenge: dangerous capabilities can arise unexpectedly; it is difficult to robustly prevent a deployed model from being misused; and, it is difficult to stop a model's capabilities from proliferating broadly. To address these challenges, at least three building blocks for the regulation of frontier models are needed: (1) standard-setting processes to identify appropriate requirements for frontier AI developers, (2) registration and reporting requirements to provide regulators with visibility into frontier AI development processes, and (3) mechanisms to ensure compliance with safety standards for the development and deplo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#35780;&#20272;&#20219;&#21153;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#20026;&#31038;&#20250;&#23618;&#38754;&#23545;LLMs&#28508;&#22312;&#39118;&#38505;&#30340;&#29702;&#35299;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2307.03109</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Evaluation of Large Language Models. (arXiv:2307.03109v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#35780;&#20272;&#20219;&#21153;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#20026;&#31038;&#20250;&#23618;&#38754;&#23545;LLMs&#28508;&#22312;&#39118;&#38505;&#30340;&#29702;&#35299;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#32780;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#38543;&#30528;LLMs&#22312;&#30740;&#31350;&#21644;&#26085;&#24120;&#20351;&#29992;&#20013;&#32487;&#32493;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#23427;&#20204;&#30340;&#35780;&#20272;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#65292;&#19981;&#20165;&#22312;&#20219;&#21153;&#27700;&#24179;&#19978;&#65292;&#32780;&#19988;&#22312;&#31038;&#20250;&#23618;&#38754;&#19978;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#23427;&#20204;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#37324;&#65292;&#24050;&#32463;&#20570;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#26469;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#26469;&#30740;&#31350;LLMs&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;LLMs&#30340;&#36825;&#20123;&#35780;&#20272;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#35780;&#20272;&#20219;&#21153;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;&#19968;&#33324;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#31185;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#21644;&#20854;&#20182;&#39046;&#22495;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#28145;&#20837;&#25506;&#35752;&#35780;&#20272;&#26041;&#27861;&#21644;&#22522;&#20934;&#31572;&#26696;&#26469;&#22238;&#31572;&#8220;&#22312;&#21738;&#37324;&#8221;&#21644;&#8220;&#22914;&#20309;&#8221;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and bench
&lt;/p&gt;</description></item><item><title>MOPO-LSI&#26159;&#19968;&#27454;&#24320;&#28304;&#30340;&#22810;&#30446;&#26631;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#24211;&#65292;&#20026;&#21487;&#25345;&#32493;&#25237;&#36164;&#25552;&#20379;&#29992;&#25143;&#25351;&#21335;&#65292;&#24182;&#20171;&#32461;&#20102;&#29256;&#26412;1.0&#30340;&#38382;&#39064;&#35774;&#32622;&#12289;&#24037;&#20316;&#27969;&#31243;&#21644;&#36229;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.01719</link><description>&lt;p&gt;
MOPO-LSI&#65306;&#29992;&#25143;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
MOPO-LSI: A User Guide. (arXiv:2307.01719v1 [q-fin.PM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01719
&lt;/p&gt;
&lt;p&gt;
MOPO-LSI&#26159;&#19968;&#27454;&#24320;&#28304;&#30340;&#22810;&#30446;&#26631;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#24211;&#65292;&#20026;&#21487;&#25345;&#32493;&#25237;&#36164;&#25552;&#20379;&#29992;&#25143;&#25351;&#21335;&#65292;&#24182;&#20171;&#32461;&#20102;&#29256;&#26412;1.0&#30340;&#38382;&#39064;&#35774;&#32622;&#12289;&#24037;&#20316;&#27969;&#31243;&#21644;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MOPO-LSI&#26159;&#19968;&#31181;&#24320;&#28304;&#30340;&#21487;&#25345;&#32493;&#25237;&#36164;&#22810;&#30446;&#26631;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#24211;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;MOPO-LSI&#29256;&#26412;1.0&#30340;&#29992;&#25143;&#25351;&#21335;&#65292;&#21253;&#25324;&#38382;&#39064;&#35774;&#32622;&#12289;&#24037;&#20316;&#27969;&#31243;&#21644;&#37197;&#32622;&#20013;&#30340;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
MOPO-LSI is an open-source Multi-Objective Portfolio Optimization Library for Sustainable Investments. This document provides a user guide for MOPO-LSI version 1.0, including problem setup, workflow and the hyper-parameters in configurations.
&lt;/p&gt;</description></item><item><title>ESGCN&#26159;&#19968;&#31181;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#36793;&#32536;&#21387;&#32553;&#27880;&#24847;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#24314;&#27169;&#26102;&#31354;&#21160;&#24577;&#21644;&#24341;&#20837;&#36793;&#32536;&#29305;&#24449;&#21644;&#36793;&#32536;&#27880;&#24847;&#26426;&#21046;&#26469;&#25552;&#39640;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01227</link><description>&lt;p&gt;
ESGCN: &#36793;&#32536;&#21387;&#32553;&#27880;&#24847;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
ESGCN: Edge Squeeze Attention Graph Convolutional Network for Traffic Flow Forecasting. (arXiv:2307.01227v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01227
&lt;/p&gt;
&lt;p&gt;
ESGCN&#26159;&#19968;&#31181;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#36793;&#32536;&#21387;&#32553;&#27880;&#24847;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#24314;&#27169;&#26102;&#31354;&#21160;&#24577;&#21644;&#24341;&#20837;&#36793;&#32536;&#29305;&#24449;&#21644;&#36793;&#32536;&#27880;&#24847;&#26426;&#21046;&#26469;&#25552;&#39640;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#26159;&#19968;&#20010;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#20132;&#36890;&#27969;&#30340;&#21160;&#24577;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30528;&#37325;&#20110;&#24314;&#27169;&#26102;&#31354;&#21160;&#24577;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Edge Squeeze Graph Convolutional Network (ESGCN)&#30340;&#32593;&#32476;&#26469;&#39044;&#27979;&#22810;&#20010;&#22320;&#21306;&#30340;&#20132;&#36890;&#27969;&#37327;&#12290;ESGCN&#30001;&#20004;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;W&#27169;&#22359;&#21644;ES&#27169;&#22359;&#12290;W&#27169;&#22359;&#26159;&#19968;&#20010;&#23436;&#20840;&#20197;&#33410;&#28857;&#20026;&#22522;&#30784;&#30340;&#21367;&#31215;&#32593;&#32476;&#12290;&#23427;&#20998;&#21035;&#23545;&#27599;&#20010;&#20132;&#36890;&#21306;&#22495;&#30340;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#22312;&#19981;&#21516;&#23610;&#24230;&#19978;&#20998;&#35299;&#26102;&#38388;&#24207;&#21015;&#20197;&#25429;&#25417;&#32454;&#31890;&#24230;&#21644;&#31895;&#31890;&#24230;&#30340;&#29305;&#24449;&#12290;ES&#27169;&#22359;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;(GCN)&#24314;&#27169;&#26102;&#31354;&#21160;&#24577;&#65292;&#24182;&#21033;&#29992;&#26102;&#24207;&#29305;&#24449;&#29983;&#25104;&#33258;&#36866;&#24212;&#37051;&#25509;&#30697;&#38453;(AAM)&#12290;&#20026;&#20102;&#25552;&#39640;AAM&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#20010;&#20851;&#38190;&#27010;&#24565;&#12290;1&#65289;&#20351;&#29992;&#36793;&#32536;&#29305;&#24449;&#30452;&#25509;&#25429;&#25417;&#21306;&#22495;&#20043;&#38388;&#30340;&#26102;&#31354;&#27969;&#21160;&#34920;&#31034;&#12290;2&#65289;&#23558;&#36793;&#32536;&#27880;&#24847;&#26426;&#21046;&#24212;&#29992;&#20110;GCN&#65292;&#20174;&#36793;&#32536;&#29305;&#24449;&#20013;&#25552;&#21462;AAM&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic forecasting is a highly challenging task owing to the dynamical spatio-temporal dependencies of traffic flows. To handle this, we focus on modeling the spatio-temporal dynamics and propose a network termed Edge Squeeze Graph Convolutional Network (ESGCN) to forecast traffic flow in multiple regions. ESGCN consists of two modules: W-module and ES module. W-module is a fully node-wise convolutional network. It encodes the time-series of each traffic region separately and decomposes the time-series at various scales to capture fine and coarse features. The ES module models the spatio-temporal dynamics using Graph Convolutional Network (GCN) and generates an Adaptive Adjacency Matrix (AAM) with temporal features. To improve the accuracy of AAM, we introduce three key concepts. 1) Using edge features to directly capture the spatiotemporal flow representation among regions. 2) Applying an edge attention mechanism to GCN to extract the AAM from the edge features. Here, the attention m
&lt;/p&gt;</description></item><item><title>PatternGPT&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#21462;&#33021;&#21147;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#30340;&#24605;&#24819;&#23454;&#29616;&#27169;&#24335;&#20849;&#20139;&#65292;&#26368;&#32456;&#36890;&#36807;&#25628;&#32034;&#39640;&#36136;&#37327;&#27169;&#24335;&#25351;&#23548;&#29983;&#25104;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#29983;&#25104;&#22810;&#26679;&#21270;&#27169;&#24335;&#12289;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12289;&#32467;&#21512;&#22806;&#37096;&#30693;&#35782;&#31561;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.00470</link><description>&lt;p&gt;
PatternGPT: &#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PatternGPT :A Pattern-Driven Framework for Large Language Model Text Generation. (arXiv:2307.00470v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00470
&lt;/p&gt;
&lt;p&gt;
PatternGPT&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#21462;&#33021;&#21147;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#30340;&#24605;&#24819;&#23454;&#29616;&#27169;&#24335;&#20849;&#20139;&#65292;&#26368;&#32456;&#36890;&#36807;&#25628;&#32034;&#39640;&#36136;&#37327;&#27169;&#24335;&#25351;&#23548;&#29983;&#25104;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#29983;&#25104;&#22810;&#26679;&#21270;&#27169;&#24335;&#12289;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12289;&#32467;&#21512;&#22806;&#37096;&#30693;&#35782;&#31561;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#65292;&#33021;&#22815;&#20026;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#29983;&#25104;&#27969;&#30021;&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#20851;&#38190;&#20219;&#21153;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#23481;&#26131;&#20986;&#29616;&#24187;&#35273;&#65292;&#24182;&#19988;&#26080;&#27861;&#30452;&#25509;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;PatternGPT&#65292;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#21462;&#33021;&#21147;&#29983;&#25104;&#20016;&#23500;&#22810;&#26679;&#30340;&#27169;&#24335;&#65292;&#28982;&#21518;&#20511;&#37492;&#32852;&#37030;&#23398;&#20064;&#30340;&#24605;&#24819;&#65292;&#20351;&#29992;&#22810;&#20010;&#20195;&#29702;&#23454;&#29616;&#20849;&#20139;&#20197;&#33719;&#21462;&#26356;&#22810;&#26679;&#30340;&#27169;&#24335;&#12290;&#26368;&#21518;&#65292;&#23427;&#20351;&#29992;&#21028;&#26029;&#26631;&#20934;&#21644;&#20248;&#21270;&#31639;&#27861;&#25628;&#32034;&#39640;&#36136;&#37327;&#30340;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#25628;&#32034;&#21040;&#30340;&#27169;&#24335;&#25351;&#23548;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#29983;&#25104;&#22810;&#26679;&#21270;&#27169;&#24335;&#12289;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12289;&#32467;&#21512;&#22806;&#37096;&#30693;&#35782;&#31561;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models(LLMS) have shown excellent text generation capabilities,capable of generating fluent responses for many downstream tasks. However,applying large language models to real-world critical tasks remains challenging due to their susceptibility to hallucinations and inability to directly use external knowledge. To address the above challenges,this paper proposes PatternGPT, a pattern-driven text generation framework for large language models. First,the framework utilizes the extraction capabilities of large language models to generate rich and diverse patterns and later draws on the idea of federated learning. Using multiple agents to achieve sharing to obtain more diverse patterns. Finally, it searches for high-quality patterns using judgment criteria and optimization algorithms and uses the searched patterns to guide the model for generation. This framework has the advantages of generating diversified patterns, protecting data privacy,combining external knowledge, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;C-K-S&#31639;&#27861;&#65292;&#36890;&#36807;&#20462;&#21098;&#28388;&#27874;&#22120;&#21644;&#36716;&#25442;&#31232;&#30095;&#24352;&#37327;&#20026;&#31264;&#23494;&#24352;&#37327;&#30340;&#26041;&#24335;&#65292;&#36339;&#36807;&#21367;&#31215;&#23618;&#20013;&#30340;0&#20803;&#32032;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;C-K-S&#30456;&#23545;&#20110;PyTorch&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.15951</link><description>&lt;p&gt;
&#36890;&#36807;&#36339;&#36807;&#38646;&#20803;&#32032;&#38477;&#20302;&#21367;&#31215;&#23618;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Reduce Computational Complexity for Convolutional Layers by Skipping Zeros. (arXiv:2306.15951v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;C-K-S&#31639;&#27861;&#65292;&#36890;&#36807;&#20462;&#21098;&#28388;&#27874;&#22120;&#21644;&#36716;&#25442;&#31232;&#30095;&#24352;&#37327;&#20026;&#31264;&#23494;&#24352;&#37327;&#30340;&#26041;&#24335;&#65292;&#36339;&#36807;&#21367;&#31215;&#23618;&#20013;&#30340;0&#20803;&#32032;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;C-K-S&#30456;&#23545;&#20110;PyTorch&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20381;&#36182;&#24182;&#34892;&#22788;&#29702;&#22120;&#36827;&#34892;&#21152;&#36895;&#12290;&#20026;&#20102;&#20026;&#20854;&#35774;&#35745;&#36816;&#31639;&#31526;&#65292;&#38656;&#35201;&#19981;&#20165;&#26377;&#20248;&#21270;&#31639;&#27861;&#20197;&#38477;&#20302;&#22797;&#26434;&#24230;&#65292;&#36824;&#38656;&#35201;&#20805;&#20998;&#21033;&#29992;&#30828;&#20214;&#36164;&#28304;&#12290;&#21367;&#31215;&#23618;&#20027;&#35201;&#21253;&#21547;&#19977;&#31181;&#36816;&#31639;&#31526;&#65306;&#21069;&#21521;&#20256;&#25773;&#30340;&#21367;&#31215;&#65292;&#21453;&#21521;&#20256;&#25773;&#30340;&#21453;&#21367;&#31215;&#21644;&#33192;&#32960;&#21367;&#31215;&#12290;&#24403;&#25191;&#34892;&#36825;&#20123;&#36816;&#31639;&#26102;&#65292;&#22987;&#32456;&#20250;&#21521;&#24352;&#37327;&#20013;&#28155;&#21152;0&#20803;&#32032;&#65292;&#23548;&#33268;&#20887;&#20313;&#35745;&#31639;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;C-K-S&#31639;&#27861;&#65288;ConvV2, KS-deconv, Sk-dilated&#65289;&#65292;&#20197;&#20004;&#31181;&#26041;&#24335;&#36339;&#36807;&#36825;&#20123;0&#20803;&#32032;&#65306;&#20462;&#21098;&#28388;&#27874;&#22120;&#20197;&#25490;&#38500;&#22635;&#20805;&#30340;0&#20803;&#32032;&#65307;&#23558;&#31232;&#30095;&#24352;&#37327;&#36716;&#25442;&#20026;&#31264;&#23494;&#24352;&#37327;&#65292;&#36991;&#20813;&#22312;&#21453;&#21367;&#31215;&#21644;&#33192;&#32960;&#21367;&#31215;&#20013;&#25554;&#20837;0&#20803;&#32032;&#12290;&#19982;&#26222;&#36890;&#21367;&#31215;&#30456;&#27604;&#65292;&#21453;&#21367;&#31215;&#30001;&#20110;&#20854;&#22797;&#26434;&#24615;&#32780;&#38590;&#20197;&#21152;&#36895;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;C-K-S&#30340;&#39640;&#24615;&#33021;GPU&#23454;&#29616;&#65292;&#24182;&#36890;&#36807;&#19982;PyTorch&#30340;&#27604;&#36739;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#26681;&#25454;&#23454;&#39564;&#32467;&#26524;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;C-K-S&#30456;&#23545;&#20110;PyTorch&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks rely on parallel processors for acceleration. To design operators for them, it requires not only good algorithm to reduce complexity, but also sufficient utilization of hardwares. Convolutional layers mainly contain 3 kinds of operators: convolution in forward propagation, deconvolution and dilated-convolution in backward propagation. When executing these operators, 0s are always added to tensors, causing redundant calculations. This paper gives C-K-S algorithm (ConvV2, KS-deconv, Sk-dilated), which skips these 0s in two ways: trim the filters to exclude padded 0s; transform sparse tensors to dense tensors, to avoid inserted 0s in deconvolution and dilated-convolution. In contrast to regular convolution, deconvolution is hard to accelerate due to its complicacy. This paper provides high-performance GPU implementations of C-K-S, and verifies their effectiveness with comparison to PyTorch. According to the experiments, C-K-S has advantages over PyTorch in certain cas
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;REFLECT&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#26426;&#22120;&#20154;&#22810;&#24863;&#23448;&#25968;&#25454;&#36716;&#21270;&#20026;&#20998;&#23618;&#24635;&#32467;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22833;&#36133;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#26377;&#30410;&#30340;&#22833;&#36133;&#35299;&#37322;&#65292;&#24110;&#21161;&#26426;&#22120;&#20154;&#23436;&#25104;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.15724</link><description>&lt;p&gt;
REFLECT:&#23545;&#26426;&#22120;&#20154;&#32463;&#21382;&#36827;&#34892;&#24635;&#32467;&#65292;&#20197;&#29992;&#20110;&#22833;&#36133;&#35299;&#37322;&#21644;&#32416;&#27491;
&lt;/p&gt;
&lt;p&gt;
REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction. (arXiv:2306.15724v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15724
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;REFLECT&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#26426;&#22120;&#20154;&#22810;&#24863;&#23448;&#25968;&#25454;&#36716;&#21270;&#20026;&#20998;&#23618;&#24635;&#32467;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22833;&#36133;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#26377;&#30410;&#30340;&#22833;&#36133;&#35299;&#37322;&#65292;&#24110;&#21161;&#26426;&#22120;&#20154;&#23436;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#26816;&#27979;&#21644;&#20998;&#26512;&#22833;&#36133;&#25191;&#34892;&#26159;&#23454;&#29616;&#21487;&#35299;&#37322;&#21644;&#31283;&#20581;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#20851;&#38190;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25991;&#26412;&#36755;&#20837;&#19978;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#12290;&#20026;&#20102;&#21033;&#29992;LLM&#30340;&#21147;&#37327;&#36827;&#34892;&#26426;&#22120;&#20154;&#22833;&#36133;&#35299;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;REFLECT&#65292;&#23558;&#22810;&#24863;&#23448;&#25968;&#25454;&#36716;&#21270;&#20026;&#26426;&#22120;&#20154;&#36807;&#21435;&#32463;&#39564;&#30340;&#20998;&#23618;&#24635;&#32467;&#65292;&#24182;&#20351;&#29992;&#36880;&#27493;&#22833;&#36133;&#35299;&#37322;&#31639;&#27861;&#26597;&#35810;LLM&#12290;&#22522;&#20110;&#35299;&#37322;&#65292;&#22833;&#36133;&#32416;&#27491;&#35268;&#21010;&#22120;&#29983;&#25104;&#19968;&#20010;&#21487;&#25191;&#34892;&#35745;&#21010;&#65292;&#20197;&#32416;&#27491;&#22833;&#36133;&#24182;&#23436;&#25104;&#20219;&#21153;&#12290;&#20026;&#20102;&#31995;&#32479;&#35780;&#20272;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;RoboFail&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#22522;&#20110;LLM&#30340;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#26377;&#30410;&#30340;&#22833;&#36133;&#35299;&#37322;&#65292;&#20174;&#32780;&#24110;&#21161;&#25104;&#21151;&#30340;&#32416;&#27491;&#35268;&#21010;&#12290;&#39033;&#30446;&#32593;&#31449;&#65306;https://roboreflect.github.io/
&lt;/p&gt;
&lt;p&gt;
The ability to detect and analyze failed executions automatically is crucial for an explainable and robust robotic system. Recently, Large Language Models (LLMs) have demonstrated strong common sense reasoning skills on textual inputs. To leverage the power of LLM for robot failure explanation, we propose a framework REFLECT, which converts multi-sensory data into a hierarchical summary of robot past experiences and queries LLM with a progressive failure explanation algorithm. Conditioned on the explanation, a failure correction planner generates an executable plan for the robot to correct the failure and complete the task. To systematically evaluate the framework, we create the RoboFail dataset and show that our LLM-based framework is able to generate informative failure explanations that assist successful correction planning. Project website: https://roboreflect.github.io/
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#22235;&#20010;&#20027;&#35201;&#26469;&#28304;&#65292;&#21253;&#25324;&#24694;&#24847;&#20351;&#29992;&#12289;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#12289;&#32452;&#32455;&#39118;&#38505;&#21644;&#27969;&#27667;&#20154;&#24037;&#26234;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12001</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
An Overview of Catastrophic AI Risks. (arXiv:2306.12001v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#22235;&#20010;&#20027;&#35201;&#26469;&#28304;&#65292;&#21253;&#25324;&#24694;&#24847;&#20351;&#29992;&#12289;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#12289;&#32452;&#32455;&#39118;&#38505;&#21644;&#27969;&#27667;&#20154;&#24037;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#36215;&#20102;&#19987;&#23478;&#12289;&#25919;&#31574;&#21046;&#23450;&#32773;&#21644;&#19990;&#30028;&#21508;&#22269;&#39046;&#23548;&#20154;&#23545;&#36234;&#26469;&#36234;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#33021;&#24102;&#26469;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#25285;&#24551;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#24456;&#22810;&#39118;&#38505;&#34987;&#21333;&#29420;&#35814;&#32454;&#20171;&#32461;&#36807;&#65292;&#20294;&#36843;&#20999;&#38656;&#35201;&#31995;&#32479;&#22320;&#35752;&#35770;&#21644;&#35828;&#26126;&#28508;&#22312;&#21361;&#38505;&#65292;&#20197;&#26356;&#22909;&#22320;&#25903;&#25345;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#30340;&#21162;&#21147;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#20027;&#35201;&#26469;&#28304;&#65292;&#25105;&#20204;&#23558;&#20854;&#20998;&#20026;&#22235;&#20010;&#31867;&#21035;&#65306;&#24694;&#24847;&#20351;&#29992;&#65292;&#21363;&#20010;&#20154;&#25110;&#22242;&#20307;&#26377;&#24847;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#36896;&#25104;&#20260;&#23475;&#65307;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#65292;&#21363;&#31454;&#20105;&#29615;&#22659;&#20419;&#20351;&#34892;&#21160;&#32773;&#37096;&#32626;&#19981;&#23433;&#20840;&#30340;&#20154;&#24037;&#26234;&#33021;&#25110;&#25918;&#24323;&#25511;&#21046;&#26435;&#20132;&#32473;&#20154;&#24037;&#26234;&#33021;&#65307;&#32452;&#32455;&#39118;&#38505;&#65292;&#31361;&#20986;&#20154;&#20026;&#21644;&#22797;&#26434;&#31995;&#32479;&#22914;&#20309;&#22686;&#21152;&#28798;&#38590;&#24615;&#20107;&#25925;&#21457;&#29983;&#30340;&#21487;&#33021;&#24615;&#65307;&#20197;&#21450;&#27969;&#27667;&#20154;&#24037;&#26234;&#33021;&#65292;&#25551;&#36848;&#20102;&#25511;&#21046;&#27604;&#20154;&#31867;&#26234;&#33021;&#26356;&#39640;&#30340;&#20195;&#29702;&#31243;&#24207;&#22256;&#38590;&#30340;&#22266;&#26377;&#38590;&#39064;&#12290;&#23545;&#20110;&#27599;&#20010;&#39118;&#38505;&#31867;&#21035;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20855;&#20307;&#30340;&#21361;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid advancements in artificial intelligence (AI) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced AI systems to pose catastrophic risks. Although numerous risks have been detailed separately, there is a pressing need for a systematic discussion and illustration of the potential dangers to better inform efforts to mitigate them. This paper provides an overview of the main sources of catastrophic AI risks, which we organize into four categories: malicious use, in which individuals or groups intentionally use AIs to cause harm; AI race, in which competitive environments compel actors to deploy unsafe AIs or cede control to AIs; organizational risks, highlighting how human factors and complex systems can increase the chances of catastrophic accidents; and rogue AIs, describing the inherent difficulty in controlling agents far more intelligent than humans. For each category of risk, we describe specific hazards,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;MARBLE&#65292;&#19968;&#20010;&#38899;&#20048;&#38899;&#39057;&#34920;&#24449;&#36890;&#29992;&#35780;&#20272;&#22522;&#20934;&#65292;&#23427;&#20026;&#38899;&#20048;&#29702;&#35299;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#21644;&#21487;&#25345;&#32493;&#24615;&#30340;&#22522;&#30784;&#65292;&#24182;&#25552;&#20379;&#21508;&#31181;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#65288;MIR&#65289;&#20219;&#21153;&#30340;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2306.10548</link><description>&lt;p&gt;
MARBLE&#65306;&#38899;&#20048;&#38899;&#39057;&#34920;&#24449;&#36890;&#29992;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MARBLE: Music Audio Representation Benchmark for Universal Evaluation. (arXiv:2306.10548v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;MARBLE&#65292;&#19968;&#20010;&#38899;&#20048;&#38899;&#39057;&#34920;&#24449;&#36890;&#29992;&#35780;&#20272;&#22522;&#20934;&#65292;&#23427;&#20026;&#38899;&#20048;&#29702;&#35299;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#21644;&#21487;&#25345;&#32493;&#24615;&#30340;&#22522;&#30784;&#65292;&#24182;&#25552;&#20379;&#21508;&#31181;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#65288;MIR&#65289;&#20219;&#21153;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33402;&#26415;&#19982;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20043;&#38388;&#20132;&#21449;&#30340;&#24191;&#27867;&#26102;&#20195;&#20013;&#65292;&#20363;&#22914;&#22270;&#20687;&#29983;&#25104;&#21644;&#34394;&#26500;&#20849;&#21019;&#65292;&#38899;&#20048;&#30340;AI&#20173;&#28982;&#30456;&#23545;&#21021;&#27493;&#65292;&#29305;&#21035;&#26159;&#22312;&#38899;&#20048;&#29702;&#35299;&#26041;&#38754;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#38899;&#20048;&#38899;&#39057;&#34920;&#24449;&#35780;&#20272;&#22522;&#20934;MARBLE&#65292;&#26088;&#22312;&#25552;&#20379;&#21508;&#31181;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#65288;MIR&#65289;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#23450;&#20041;&#21253;&#25324;&#22768;&#23398;&#65292;&#28436;&#22863;&#65292;&#20048;&#35889;&#21644;&#39640;&#32423;&#25551;&#36848;&#22312;&#20869;&#30340;&#22235;&#20010;&#23618;&#27425;&#30340;&#32508;&#21512;&#20998;&#31867;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;8&#20010;&#20844;&#20849;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#30340;14&#39033;&#20219;&#21153;&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21327;&#35758;&#65292;&#25552;&#20379;&#20102;&#25152;&#26377;&#22522;&#20110;&#38899;&#20048;&#24405;&#38899;&#24320;&#21457;&#30340;&#24320;&#25918;&#28304;&#20195;&#30721;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#34920;&#24449;&#30340;&#20844;&#24179;&#21644;&#26631;&#20934;&#30340;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;MARBLE&#25552;&#20379;&#20102;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#12289;&#21487;&#25193;&#23637;&#21644;&#21487;&#37325;&#29992;&#30340;&#24037;&#20855;&#24211;&#65292;&#20197;&#25903;&#25345;&#31038;&#21306;&#39537;&#21160;&#30340;&#23458;&#35266;&#22522;&#20934;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of extensive intersection between art and Artificial Intelligence (AI), such as image generation and fiction co-creation, AI for music remains relatively nascent, particularly in music understanding. This is evident in the limited work on deep music representations, the scarcity of large-scale datasets, and the absence of a universal and community-driven benchmark. To address this issue, we introduce the Music Audio Representation Benchmark for universaL Evaluation, termed MARBLE. It aims to provide a benchmark for various Music Information Retrieval (MIR) tasks by defining a comprehensive taxonomy with four hierarchy levels, including acoustic, performance, score, and high-level description. We then establish a unified protocol based on 14 tasks on 8 public-available datasets, providing a fair and standard assessment of representations of all open-sourced pre-trained models developed on music recordings as baselines. Besides, MARBLE offers an easy-to-use, extendable, and re
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#28145;&#24230;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#20915;&#31574;&#21644;&#25511;&#21046;&#38382;&#39064;&#20013;&#30340;&#32570;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#23558;&#24403;&#20195;&#29983;&#25104;&#24314;&#27169;&#24037;&#20855;&#20013;&#30340;&#25512;&#29702;&#25216;&#26415;&#37325;&#26032;&#35299;&#37322;&#20026;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#35268;&#21010;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.08810</link><description>&lt;p&gt;
Decision-Making and Control&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Deep Generative Models for Decision-Making and Control. (arXiv:2306.08810v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#28145;&#24230;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#20915;&#31574;&#21644;&#25511;&#21046;&#38382;&#39064;&#20013;&#30340;&#32570;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#23558;&#24403;&#20195;&#29983;&#25104;&#24314;&#27169;&#24037;&#20855;&#20013;&#30340;&#25512;&#29702;&#25216;&#26415;&#37325;&#26032;&#35299;&#37322;&#20026;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#35268;&#21010;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#27010;&#24565;&#19978;&#31616;&#21333;&#30340;&#20915;&#31574;&#21644;&#25511;&#21046;&#38382;&#39064;&#30340;&#26041;&#27861;&#65306;&#20351;&#29992;&#23398;&#20064;&#26469;&#20272;&#35745;&#36817;&#20284;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#24182;&#23558;&#21097;&#20313;&#24037;&#20316;&#22996;&#25176;&#32473;&#32463;&#20856;&#36712;&#36857;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#32452;&#21512;&#23384;&#22312;&#19968;&#20123;&#32463;&#39564;&#19978;&#30340;&#32570;&#28857;&#65292;&#38480;&#21046;&#20102;&#23454;&#38469;&#20013;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;&#26412;&#35770;&#25991;&#30340;&#21452;&#37325;&#30446;&#30340;&#26159;&#30740;&#31350;&#36825;&#20123;&#32570;&#28857;&#30340;&#21407;&#22240;&#24182;&#25552;&#20986;&#35299;&#20915;&#26041;&#27861;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#24403;&#20195;&#29983;&#25104;&#24314;&#27169;&#24037;&#20855;&#31665;&#20013;&#30340;&#25512;&#29702;&#25216;&#26415;&#65292;&#21253;&#25324;&#26463;&#25628;&#32034;&#12289;&#20998;&#31867;&#22120;&#24341;&#23548;&#37319;&#26679;&#21644;&#22270;&#20687;&#20462;&#22797;&#65292;&#22914;&#20309;&#37325;&#26032;&#35299;&#37322;&#20026;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#21487;&#34892;&#35268;&#21010;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep model-based reinforcement learning methods offer a conceptually simple approach to the decision-making and control problem: use learning for the purpose of estimating an approximate dynamics model, and offload the rest of the work to classical trajectory optimization. However, this combination has a number of empirical shortcomings, limiting the usefulness of model-based methods in practice. The dual purpose of this thesis is to study the reasons for these shortcomings and to propose solutions for the uncovered problems. Along the way, we highlight how inference techniques from the contemporary generative modeling toolbox, including beam search, classifier-guided sampling, and image inpainting, can be reinterpreted as viable planning strategies for reinforcement learning problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#39033;&#20351;&#29992;&#20247;&#21253;&#25216;&#26415;&#30340;&#35745;&#31639;&#26426;&#31185;&#23398;&#39640;&#31561;&#25945;&#32946;&#20316;&#19994;&#65292;&#40723;&#21169;&#23398;&#29983;&#20016;&#23500;&#38899;&#20048;&#20803;&#25968;&#25454;&#65292;&#24182;&#21019;&#24314;&#20102;&#21487;&#20379;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#38899;&#20048;&#26631;&#35760;&#30340;&#24320;&#25918;&#24615;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.07310</link><description>&lt;p&gt;
&#24212;&#29992;&#20247;&#21253;&#25216;&#26415;&#20016;&#23500;&#39640;&#31561;&#25945;&#32946;&#38899;&#20048;&#30693;&#35782;&#24211;&#30340;&#26041;&#27861;&#21644;&#32463;&#39564;
&lt;/p&gt;
&lt;p&gt;
Employing Crowdsourcing for Enriching a Music Knowledge Base in Higher Education. (arXiv:2306.07310v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#39033;&#20351;&#29992;&#20247;&#21253;&#25216;&#26415;&#30340;&#35745;&#31639;&#26426;&#31185;&#23398;&#39640;&#31561;&#25945;&#32946;&#20316;&#19994;&#65292;&#40723;&#21169;&#23398;&#29983;&#20016;&#23500;&#38899;&#20048;&#20803;&#25968;&#25454;&#65292;&#24182;&#21019;&#24314;&#20102;&#21487;&#20379;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#38899;&#20048;&#26631;&#35760;&#30340;&#24320;&#25918;&#24615;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#39033;&#20351;&#29992;&#20247;&#21253;&#25216;&#26415;&#20316;&#20026;&#35745;&#31639;&#26426;&#31185;&#23398;&#39640;&#31561;&#25945;&#32946;&#23398;&#29983;&#20316;&#19994;&#30340;&#26041;&#27861;&#21644;&#32463;&#39564;&#12290;&#21033;&#29992;&#25903;&#25345;&#25991;&#21270;&#36951;&#20135;&#39046;&#22495;&#20247;&#21253;&#30340;&#24179;&#21488;&#65292;&#40723;&#21169;&#23398;&#29983;&#20016;&#23500;&#19982;&#36873;&#23450;&#38899;&#20048;&#26354;&#30446;&#30456;&#20851;&#30340;&#20803;&#25968;&#25454;&#12290;&#35813;&#27963;&#21160;&#20849;&#26377;98&#21517;&#23398;&#29983;&#21442;&#21152;&#65292;&#20026;854&#39318;&#38899;&#20048;&#26354;&#30446;&#36129;&#29486;&#20102;6400&#22810;&#20010;&#27880;&#37322;&#12290;&#21516;&#26102;&#36824;&#21019;&#24314;&#20102;&#19968;&#20010;&#21487;&#20379;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#38899;&#20048;&#26631;&#35760;&#30340;&#24320;&#25918;&#24615;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#22312;&#32447;&#35843;&#26597;&#65292;&#35813;&#27963;&#21160;&#30340;&#32467;&#26524;&#21644;&#24847;&#35265;&#25910;&#38598;&#20351;&#25105;&#20204;&#33021;&#22815;&#24471;&#20986;&#19968;&#20123;&#26377;&#29992;&#30340;&#35265;&#35299;&#65292;&#20102;&#35299;&#23558;&#20247;&#21253;&#25972;&#21512;&#21040;&#35745;&#31639;&#26426;&#31185;&#23398;&#35838;&#31243;&#20013;&#30340;&#30410;&#22788;&#21644;&#25361;&#25112;&#20197;&#21450;&#22914;&#20309;&#25552;&#39640;&#23398;&#29983;&#30340;&#23398;&#20064;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes the methodology followed and the lessons learned from employing crowdsourcing techniques as part of a homework assignment involving higher education students of computer science. Making use of a platform that supports crowdsourcing in the cultural heritage domain students were solicited to enrich the metadata associated with a selection of music tracks. The results of the campaign were further analyzed and exploited by students through the use of semantic web technologies. In total, 98 students participated in the campaign, contributing more than 6400 annotations concerning 854 tracks. The process also led to the creation of an openly available annotated dataset, which can be useful for machine learning models for music tagging. The campaign's results and the comments gathered through an online survey enable us to draw some useful insights about the benefits and challenges of integrating crowdsourcing into computer science curricula and how this can enhance student
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20351;&#29992;&#24378;&#22823;&#30340;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#30830;&#35748;LLM&#35780;&#21028;&#21592;&#21644;&#20154;&#31867;&#20559;&#22909;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#21487;&#35843;&#25972;&#32842;&#22825;&#21161;&#25163;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05685</link><description>&lt;p&gt;
&#29992;MT-Bench&#21644;Chatbot Arena&#35780;&#20272;&#20197;LLM&#20026;&#22522;&#30784;&#30340;&#32842;&#22825;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. (arXiv:2306.05685v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05685
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20351;&#29992;&#24378;&#22823;&#30340;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#30830;&#35748;LLM&#35780;&#21028;&#21592;&#21644;&#20154;&#31867;&#20559;&#22909;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#21487;&#35843;&#25972;&#32842;&#22825;&#21161;&#25163;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#32842;&#22825;&#21161;&#25163;&#20250;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#24191;&#27867;&#30340;&#21151;&#33021;&#65292;&#32780;&#29616;&#26377;&#30340;&#22522;&#20934;&#26080;&#27861;&#34913;&#37327;&#20154;&#31867;&#20559;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20351;&#29992;&#24378;&#22823;&#30340;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#65292;&#22312;&#26356;&#21152;&#24320;&#25918;&#30340;&#38382;&#39064;&#19978;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#30340;&#20351;&#29992;&#21644;&#23616;&#38480;&#24615;&#65292;&#22914;&#20301;&#32622;&#21644;&#20887;&#20313;&#20559;&#35265;&#20197;&#21450;&#26377;&#38480;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#26469;&#36801;&#31227;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#65288;&#19968;&#20010;&#22810;&#36718;&#38382;&#31572;&#38598;&#21644;&#19968;&#20010;&#20247;&#21253;&#31454;&#25216;&#24179;&#21488;&#65289;&#26469;&#30830;&#35748;LLM&#35780;&#21028;&#21592;&#21644;&#20154;&#31867;&#20559;&#22909;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#24378;&#22823;LLM&#35780;&#21028;&#21592;&#21487;&#20197;&#24456;&#22909;&#22320;&#21305;&#37197;&#21463;&#25511;&#21644;&#20247;&#21253;&#20154;&#31867;&#20559;&#22909;&#65292;&#36798;&#21040;&#20102;80&#65285;&#20197;&#19978;&#30340;&#19968;&#33268;&#24615;&#65292;&#19982;&#20154;&#31867;&#19968;&#33268;&#24615;&#27700;&#24179;&#30456;&#21516;&#12290;&#22240;&#27492;&#65292;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#19988;&#21487;&#35299;&#37322;&#30340;&#36924;&#36817;&#20154;&#31867;&#20559;&#22909;&#30340;&#26041;&#24335;&#65292;&#32780;&#36825;&#20123;&#20559;&#22909;&#26159;&#38750;&#24120;&#26114;&#36149;&#33719;&#21462;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#65292;&#21487;&#20197;&#36890;&#36807;&#35843;&#25972;&#32842;&#22825;&#21161;&#25163;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, such as position and verbosity biases and limited reasoning ability, and propose solutions to migrate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80\% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#39564;&#35777;&#26041;&#27861;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#20197;&#39564;&#35777;&#23433;&#20840;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#24182;&#20195;&#34920;&#20102;&#36890;&#29992;&#12289;&#21487;&#20280;&#32553;&#21644;&#21487;&#39564;&#35777;&#30340;&#25511;&#21046;&#31995;&#32479;&#35774;&#35745;&#26694;&#26550;&#30340;&#31532;&#19968;&#27493;&#12290;</title><link>http://arxiv.org/abs/2306.04026</link><description>&lt;p&gt;
&#20215;&#20540;&#20989;&#25968;&#21363;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#65306;&#20351;&#29992;&#25511;&#21046;&#29702;&#35770;&#39564;&#35777;&#23398;&#20064;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Your Value Function is a Control Barrier Function: Verification of Learned Policies using Control Theory. (arXiv:2306.04026v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#39564;&#35777;&#26041;&#27861;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#20197;&#39564;&#35777;&#23433;&#20840;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#24182;&#20195;&#34920;&#20102;&#36890;&#29992;&#12289;&#21487;&#20280;&#32553;&#21644;&#21487;&#39564;&#35777;&#30340;&#25511;&#21046;&#31995;&#32479;&#35774;&#35745;&#26694;&#26550;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#20855;&#26377;&#39640;&#24230;&#30340;&#36890;&#29992;&#24615;&#21644;&#21487;&#20280;&#32553;&#24615;&#65292;&#20294;&#39564;&#35777;&#31574;&#30053;&#34892;&#20026;&#30340;&#38590;&#24230;&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#31243;&#24207;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#25511;&#21046;&#29702;&#35770;&#20013;&#20351;&#29992;&#30340;&#39564;&#35777;&#26041;&#27861;&#24212;&#29992;&#20110;&#23398;&#20064;&#30340;&#20215;&#20540;&#20989;&#25968;&#12290;&#36890;&#36807;&#20998;&#26512;&#23433;&#20840;&#32500;&#25252;&#30340;&#31616;&#21333;&#20219;&#21153;&#32467;&#26500;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#23558;&#20540;&#20989;&#25968;&#19982;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#30456;&#32852;&#31995;&#30340;&#21407;&#22987;&#23450;&#29702;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#20197;&#39564;&#35777;&#23433;&#20840;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#21892;&#23398;&#20064;&#30340;&#23454;&#38469;&#23454;&#26045;&#32454;&#33410;&#12290;&#38500;&#20102;&#25552;&#20986;&#35777;&#20070;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#22806;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;RL&#31574;&#30053;&#35299;&#38145;&#20102;&#20016;&#23500;&#30340;&#25511;&#21046;&#29702;&#35770;&#39564;&#35777;&#26041;&#27861;&#65292;&#24182;&#20195;&#34920;&#20102;&#36890;&#29992;&#12289;&#21487;&#20280;&#32553;&#21644;&#21487;&#39564;&#35777;&#30340;&#25511;&#21046;&#31995;&#32479;&#35774;&#35745;&#26694;&#26550;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although RL is highly general and scalable, the difficulty of verifying policy behaviours poses challenges for safety-critical applications. To remedy this, we propose to apply verification methods used in control theory to learned value functions. By analyzing a simple task structure for safety preservation, we derive original theorems linking value functions to control barrier functions. Inspired by this, we propose novel metrics for verification of value functions in safe control tasks, and practical implementation details that improve learning. Besides proposing a novel method for certificate learning, our work unlocks a wealth of verification methods in control theory for RL policies, and represents a first step towards a framework for general, scalable, and verifiable design of control systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#25551;&#36848;&#36923;&#36753;&#65288;DLs&#65289;&#20197;&#25903;&#25345;&#22810;&#20010;&#25277;&#35937;&#23618;&#27425;&#30340;&#30693;&#35782;&#34920;&#31034;&#65292;&#24182;&#35777;&#26126;&#20102;&#25512;&#29702;&#22312;&#35813;&#36923;&#36753;&#20013;&#26159;&#21487;&#21028;&#23450;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.03717</link><description>&lt;p&gt;
&#20855;&#26377;&#25277;&#35937;&#21644;&#32454;&#21270;&#30340;&#25551;&#36848;&#36923;&#36753;
&lt;/p&gt;
&lt;p&gt;
Description Logics with Abstraction and Refinement. (arXiv:2306.03717v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#25551;&#36848;&#36923;&#36753;&#65288;DLs&#65289;&#20197;&#25903;&#25345;&#22810;&#20010;&#25277;&#35937;&#23618;&#27425;&#30340;&#30693;&#35782;&#34920;&#31034;&#65292;&#24182;&#35777;&#26126;&#20102;&#25512;&#29702;&#22312;&#35813;&#36923;&#36753;&#20013;&#26159;&#21487;&#21028;&#23450;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#25551;&#36848;&#36923;&#36753;&#65288;DLs&#65289;&#65292;&#35813;DLs&#26377;&#21161;&#20110;&#25903;&#25345;&#22810;&#20010;&#25277;&#35937;&#23618;&#27425;&#30340;&#30693;&#35782;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#25193;&#23637;DLs&#23558;&#25277;&#35937;&#23618;&#32423;&#20316;&#20026;&#31532;&#19968;&#31867;&#20844;&#27665;&#65292;&#25552;&#20379;&#20102;&#26126;&#30830;&#30340;&#25805;&#20316;&#31526;&#65292;&#29992;&#20110;&#36328;&#22810;&#20010;&#25277;&#35937;&#23618;&#27425;&#36827;&#34892;&#27010;&#24565;&#21644;&#35282;&#33394;&#30340;&#25277;&#35937;&#21644;&#32454;&#21270;&#65292;&#22522;&#20110;&#21512;&#21462;&#26597;&#35810;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#24471;&#21040;&#30340;DLs&#26063;&#20013;&#36827;&#34892;&#25512;&#29702;&#26159;&#21487;&#21028;&#23450;&#30340;&#65292;&#32780;&#19968;&#20123;&#30475;&#20284;&#26080;&#23475;&#30340;&#21464;&#21270;&#23454;&#38469;&#19978;&#26159;&#19981;&#21487;&#21028;&#23450;&#30340;&#12290;&#25105;&#20204;&#36824;&#26126;&#30830;&#20102;&#25105;&#20204;&#36923;&#36753;&#21644;&#19968;&#20123;&#30456;&#20851;&#29255;&#27573;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ontologies often require knowledge representation on multiple levels of abstraction, but description logics (DLs) are not well-equipped for supporting this. We propose an extension of DLs in which abstraction levels are first-class citizens and which provides explicit operators for the abstraction and refinement of concepts and roles across multiple abstraction levels, based on conjunctive queries. We prove that reasoning in the resulting family of DLs is decidable while several seemingly harmless variations turn out to be undecidable. We also pinpoint the precise complexity of our logics and several relevant fragments.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#29983;&#21629;&#20307;&#24449;&#25968;&#25454;&#24320;&#21457;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#31639;&#27861;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#23376;&#32676;&#20043;&#38388;&#23384;&#22312;&#30528;&#19981;&#21516;&#30340;ICU&#21644;&#21307;&#38498;&#27515;&#20129;&#29575;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2306.02121</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#30495;&#23454;&#19990;&#30028;&#29983;&#21629;&#20307;&#24449;&#25968;&#25454;&#30340;&#31471;&#21040;&#31471;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#31639;&#27861;&#35782;&#21035;ICU&#24739;&#32773;&#23376;&#32676;
&lt;/p&gt;
&lt;p&gt;
Identifying Subgroups of ICU Patients Using End-to-End Multivariate Time-Series Clustering Algorithm Based on Real-World Vital Signs Data. (arXiv:2306.02121v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#29983;&#21629;&#20307;&#24449;&#25968;&#25454;&#24320;&#21457;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#31639;&#27861;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#23376;&#32676;&#20043;&#38388;&#23384;&#22312;&#30528;&#19981;&#21516;&#30340;ICU&#21644;&#21307;&#38498;&#27515;&#20129;&#29575;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;MIMIC-IV&#25968;&#25454;&#24211;&#20316;&#20026;&#25968;&#25454;&#28304;&#65292;&#30740;&#31350;&#20102;&#21160;&#24577;&#12289;&#39640;&#39057;&#29575;&#12289;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#29983;&#21629;&#20307;&#24449;&#25968;&#25454;&#22312;ICU&#36887;&#30041;&#26399;&#38388;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20307;&#28201;&#12289;&#24515;&#29575;&#12289;&#24179;&#22343;&#34880;&#21387;&#12289;&#21628;&#21560;&#39057;&#29575;&#21644;SpO2&#12290;&#27604;&#36739;&#20102;&#21508;&#31181;&#32858;&#31867;&#31639;&#27861;&#65292;&#24182;&#36873;&#25321;&#20102;&#19968;&#31181;&#21517;&#20026;Time2Feat&#30340;&#31471;&#21040;&#31471;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#31995;&#32479;&#32467;&#21512;K-Means&#20316;&#20026;&#26368;&#26377;&#25928;&#30340;ICU&#24739;&#32773;&#32858;&#31867;&#26041;&#27861;&#12290;&#22312;&#32858;&#31867;&#20998;&#26512;&#20013;&#65292;&#20351;&#29992;&#20102;2008&#24180;&#33267;2016&#24180;&#26399;&#38388;&#25910;&#27835;&#30340;8,080&#21517;&#24739;&#32773;&#30340;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#24320;&#21457;&#65292;&#20351;&#29992;&#20102;2017&#24180;&#33267;2019&#24180;&#26399;&#38388;&#25910;&#27835;&#30340;2,038&#21517;&#24739;&#32773;&#30340;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#39564;&#35777;&#12290;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#31867;&#21035;&#20043;&#38388;&#30340;&#20020;&#24202;&#27515;&#20129;&#39044;&#21518;&#24046;&#24322;&#65292;&#21457;&#29616;&#20102;&#19981;&#21516;&#20122;&#32452;&#20043;&#38388;ICU&#27515;&#20129;&#29575;&#21644;&#21307;&#38498;&#27515;&#20129;&#29575;&#30340;&#39118;&#38505;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#21487;&#35270;&#21270;&#20102;&#29983;&#21629;&#20307;&#24449;&#21464;&#21270;&#30340;&#36712;&#36857;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#21457;&#29616;&#20026;ICU&#24739;&#32773;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study employed the MIMIC-IV database as data source to investigate the use of dynamic, high-frequency, multivariate time-series vital signs data, including temperature, heart rate, mean blood pressure, respiratory rate, and SpO2, monitored first 8 hours data in the ICU stay. Various clustering algorithms were compared, and an end-to-end multivariate time series clustering system called Time2Feat, combined with K-Means, was chosen as the most effective method to cluster patients in the ICU. In clustering analysis, data of 8,080 patients admitted between 2008 and 2016 was used for model development and 2,038 patients admitted between 2017 and 2019 for model validation. By analyzing the differences in clinical mortality prognosis among different categories, varying risks of ICU mortality and hospital mortality were found between different subgroups. Furthermore, the study visualized the trajectory of vital signs changes. The findings of this study provide valuable insights into the p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#65292;&#21253;&#25324;&#21160;&#26426;&#12289;&#25361;&#25112;&#12289;&#26041;&#27861;&#35770;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#29616;&#26377;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#23450;&#21046;&#25216;&#26415;&#36827;&#34892;&#20102;&#35814;&#32454;&#27604;&#36739;&#65292;&#24182;&#24191;&#27867;&#35752;&#35770;&#20102;&#36825;&#19968;&#39046;&#22495;&#20013;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.18703</link><description>&lt;p&gt;
&#36229;&#36234;&#19968;&#20010;&#27169;&#22411;&#36866;&#29992;&#20110;&#25152;&#26377;&#39046;&#22495;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Beyond One-Model-Fits-All: A Survey of Domain Specialization for Large Language Models. (arXiv:2305.18703v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#65292;&#21253;&#25324;&#21160;&#26426;&#12289;&#25361;&#25112;&#12289;&#26041;&#27861;&#35770;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#29616;&#26377;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#23450;&#21046;&#25216;&#26415;&#36827;&#34892;&#20102;&#35814;&#32454;&#27604;&#36739;&#65292;&#24182;&#24191;&#27867;&#35752;&#35770;&#20102;&#36825;&#19968;&#39046;&#22495;&#20013;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#22823;&#22823;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#20026;&#24191;&#27867;&#24212;&#29992;&#25552;&#20379;&#20102;&#39640;&#24230;&#23454;&#29992;&#12289;&#20219;&#21153;&#26080;&#20851;&#30340;&#22522;&#30784;&#12290;LLMs &#20316;&#20026;&#36890;&#29992;&#20219;&#21153;&#27714;&#35299;&#22120;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20419;&#20351;&#20154;&#20204;&#23558;&#20854;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#12289;&#37329;&#34701;&#21644;&#25945;&#32946;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#21161;&#25163;&#29978;&#33267;&#26367;&#20195;&#29305;&#23450;&#39046;&#22495;&#30340;&#19987;&#23478;&#21644;&#24037;&#20855;&#12290;&#20294;&#26159;&#65292;&#23558;LLMs&#30452;&#25509;&#24212;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#20013;&#30340;&#22797;&#26434;&#38382;&#39064;&#20250;&#36935;&#21040;&#35768;&#22810;&#22256;&#38590;&#65292;&#21253;&#25324;&#39046;&#22495;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#12289;&#39046;&#22495;&#30693;&#35782;&#30340;&#22797;&#26434;&#24615;&#12289;&#39046;&#22495;&#30446;&#26631;&#30340;&#29420;&#29305;&#24615;&#20197;&#21450;&#32422;&#26463;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#31181;&#24046;&#36317;&#65292;&#26368;&#36817;&#20960;&#24180;&#36827;&#34892;&#20102;&#24613;&#21095;&#22686;&#21152;&#30340;&#30740;&#31350;&#21644;&#23454;&#36341;&#33268;&#21147;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#65292;&#28982;&#32780;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#23578;&#26410;&#34987;&#31995;&#32479;&#22320;&#24635;&#32467;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#23545;LLMs&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#36827;&#34892;&#20102;&#20840;&#38754;&#27010;&#36848;&#65292;&#21253;&#25324;&#21160;&#26426;&#12289;&#25361;&#25112;&#12289;&#26041;&#27861;&#35770;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#29616;&#26377;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#23450;&#21046;&#25216;&#26415;&#36827;&#34892;&#20102;&#35814;&#32454;&#27604;&#36739;&#65292;&#24182;&#24191;&#27867;&#35752;&#35770;&#20102;&#36825;&#19968;&#39046;&#22495;&#20013;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have significantly advanced the field of natural language processing (NLP), providing a highly useful, task-agnostic foundation for a wide range of applications. The great promise of LLMs as general task solvers motivated people to extend their functionality largely beyond just a ``chatbot'', and use it as an assistant or even replacement for domain experts and tools in specific domains such as healthcare, finance, and education. However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, the uniqueness of domain objectives, and the diversity of the constraints (e.g., various social norms, cultural conformity, religious beliefs, and ethical standards in the domain applications). To fill such a gap, explosively-increase research, and practices have been conducted in very recent years on the domain specialization of LLMs, which, howe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;2.5D&#22810;&#30446;&#26631;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#25214;&#21040;&#36317;&#31163;&#21644;&#33021;&#32791;&#36798;&#21040;&#33391;&#22909;&#26435;&#34913;&#30340;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2305.13783</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22320;&#38754;&#36710;&#36742;&#22312;&#36234;&#37326;&#22320;&#24418;&#29615;&#22659;&#19979;&#30340;&#22810;&#30446;&#26631;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning-based Multi-objective Path Planning on the Off-road Terrain Environment for Ground Vehicles. (arXiv:2305.13783v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;2.5D&#22810;&#30446;&#26631;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#25214;&#21040;&#36317;&#31163;&#21644;&#33021;&#32791;&#36798;&#21040;&#33391;&#22909;&#26435;&#34913;&#30340;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#19978;&#22369;&#21644;&#19979;&#22369;&#20043;&#38388;&#30340;&#33021;&#32791;&#25928;&#29575;&#24046;&#24322;&#24040;&#22823;&#65292;&#22312;&#22797;&#26434;&#30340;&#36234;&#37326;&#22320;&#24418;&#29615;&#22659;&#65288;2.5D&#22320;&#22270;&#65289;&#19978;&#65292;&#26368;&#30701;&#36335;&#24452;&#19981;&#19968;&#23450;&#26159;&#33021;&#32791;&#26368;&#23569;&#30340;&#36335;&#24452;&#12290;&#23545;&#20110;&#20219;&#20309;&#33021;&#28304;&#25935;&#24863;&#30340;&#36710;&#36742;&#26469;&#35828;&#65292;&#23454;&#29616;&#36317;&#31163;&#21644;&#33021;&#32791;&#22312;2.5D&#36335;&#24452;&#35268;&#21010;&#19978;&#33391;&#22909;&#30340;&#26435;&#34913;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;2.5D&#22810;&#30446;&#26631;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65288;DMOP&#65289;&#12290;DMOP&#21487;&#20197;&#36890;&#36807;&#19977;&#20010;&#27493;&#39588;&#39640;&#25928;&#22320;&#25214;&#21040;&#25152;&#38656;&#36335;&#24452;&#65306;(1)&#23558;&#39640;&#20998;&#36776;&#29575;&#30340;2.5D&#22320;&#22270;&#36716;&#25442;&#20026;&#23567;&#23610;&#23544;&#22320;&#22270;&#12290;(2)&#20351;&#29992;&#35757;&#32451;&#22909;&#30340;&#28145;&#24230;Q&#32593;&#32476;&#65288;DQN&#65289;&#22312;&#23567;&#23610;&#23544;&#22320;&#22270;&#19978;&#25214;&#21040;&#25152;&#38656;&#36335;&#24452;&#12290;(3)&#20351;&#29992;&#36335;&#24452;&#22686;&#24378;&#26041;&#27861;&#23558;&#35745;&#21010;&#36335;&#24452;&#26500;&#24314;&#21040;&#21407;&#22987;&#39640;&#20998;&#36776;&#29575;&#22320;&#22270;&#19978;&#12290;&#27492;&#22806;&#65292;&#36824;&#24212;&#29992;&#20102;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#21644;&#22870;&#21169;&#22609;&#36896;&#29702;&#35770;&#26469;&#35757;&#32451;DQN&#12290;&#22870;&#21169;&#20989;&#25968;&#32467;&#21512;&#20102;&#22320;&#24418;&#12289;&#36317;&#31163;&#21644;&#36793;&#30028;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the energy-consumption efficiency between up-slope and down-slope is hugely different, a path with the shortest length on a complex off-road terrain environment (2.5D map) is not always the path with the least energy consumption. For any energy-sensitive vehicles, realizing a good trade-off between distance and energy consumption on 2.5D path planning is significantly meaningful. In this paper, a deep reinforcement learning-based 2.5D multi-objective path planning method (DMOP) is proposed. The DMOP can efficiently find the desired path with three steps: (1) Transform the high-resolution 2.5D map into a small-size map. (2) Use a trained deep Q network (DQN) to find the desired path on the small-size map. (3) Build the planned path to the original high-resolution map using a path enhanced method. In addition, the imitation learning method and reward shaping theory are applied to train the DQN. The reward function is constructed with the information of terrain, distance, border. S
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36731;&#37327;&#32423;&#22270;&#20687;&#24674;&#22797;&#30340;&#20114;&#24800;&#24335;&#27880;&#24847;&#21147;&#28151;&#21512;Transformer&#65288;RAMiT&#65289;&#12290;&#36890;&#36807;&#20351;&#29992;&#21452;&#21521;&#27880;&#24847;&#21147;&#20197;&#21450;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#31867;&#22411;&#8212;&#8212;&#24378;&#24230;&#25513;&#30721;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#24674;&#22797;&#25928;&#26524;&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#20102;&#21442;&#25968;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.11474</link><description>&lt;p&gt;
RAMiT&#65306;&#36731;&#37327;&#32423;&#22270;&#20687;&#24674;&#22797;&#30340;&#20114;&#24800;&#24335;&#27880;&#24847;&#21147;&#28151;&#21512;Transformer
&lt;/p&gt;
&lt;p&gt;
RAMiT: Reciprocal Attention Mixing Transformer for Lightweight Image Restoration. (arXiv:2305.11474v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36731;&#37327;&#32423;&#22270;&#20687;&#24674;&#22797;&#30340;&#20114;&#24800;&#24335;&#27880;&#24847;&#21147;&#28151;&#21512;Transformer&#65288;RAMiT&#65289;&#12290;&#36890;&#36807;&#20351;&#29992;&#21452;&#21521;&#27880;&#24847;&#21147;&#20197;&#21450;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#31867;&#22411;&#8212;&#8212;&#24378;&#24230;&#25513;&#30721;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#24674;&#22797;&#25928;&#26524;&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#20102;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#24180;&#26469;&#35768;&#22810;&#24037;&#20316;&#22312;&#22270;&#20687;&#24674;&#22797;&#65288;IR&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#38754;&#20020;&#21442;&#25968;&#36807;&#22810;&#30340;&#38382;&#39064;&#12290;&#21478;&#19968;&#20010;&#38382;&#39064;&#26159;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;Transformer&#30340;IR&#26041;&#27861;&#21482;&#20381;&#38752;&#26412;&#22320;&#25110;&#20840;&#23616;&#29305;&#24449;&#65292;&#23548;&#33268;&#25509;&#21463;&#22495;&#26377;&#38480;&#25110;&#23384;&#22312;&#21442;&#25968;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;IR&#32593;&#32476;&#65306;&#20114;&#24800;&#27880;&#24847;&#21147;&#28151;&#21512;Transformer&#65288;RAMiT&#65289;&#12290;&#23427;&#37319;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#32500;&#24230;&#20114;&#24800;&#27880;&#24847;&#21147;&#28151;&#21512;Transformer&#65288;D-RAMiT&#65289;&#22359;&#65292;&#22312;&#20351;&#29992;&#19981;&#21516;&#25968;&#37327;&#30340;&#22810;&#22836;&#24182;&#34892;&#35745;&#31639;&#21452;&#21521;&#65288;&#31354;&#38388;&#21644;&#36890;&#36947;&#65289;&#33258;&#27880;&#24847;&#21147;&#30340;&#24773;&#20917;&#19979;&#12290;&#21452;&#21521;&#20851;&#27880;&#24110;&#21161;&#24444;&#27492;&#24357;&#34917;&#23545;&#26041;&#30340;&#32570;&#28857;&#65292;&#28982;&#21518;&#28151;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20998;&#23618;&#20114;&#24800;&#27880;&#24847;&#21147;&#28151;&#21512;&#65288;H-RAMi&#65289;&#23618;&#65292;&#23427;&#34917;&#20607;&#20687;&#32032;&#32423;&#20449;&#24687;&#20002;&#22833;&#24182;&#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#25928;&#30340;&#20998;&#23618;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#22312;IR&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#31867;&#22411;&#8212;&#8212;&#24378;&#24230;&#25513;&#30721;&#65292;&#20197;&#25552;&#39640;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;IR&#20219;&#21153;&#20013;&#65292;&#21253;&#25324;&#22270;&#20687;&#21435;&#22122;&#12289;&#22270;&#20687;&#21435;&#27169;&#31946;&#21644;JPEG&#22270;&#20687;&#21435;&#22359;&#65292;&#25105;&#20204;&#30340;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22823;&#22823;&#20943;&#23569;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although many recent works have made advancements in the image restoration (IR) field, they often suffer from an excessive number of parameters. Another issue is that most Transformer-based IR methods focus only on either local or global features, leading to limited receptive fields or deficient parameter issues. To address these problems, we propose a lightweight IR network, Reciprocal Attention Mixing Transformer (RAMiT). It employs our proposed dimensional reciprocal attention mixing Transformer (D-RAMiT) blocks, which compute bi-dimensional (spatial and channel) self-attentions in parallel with different numbers of multi-heads. The bi-dimensional attentions help each other to complement their counterpart's drawbacks and are then mixed. Additionally, we introduce a hierarchical reciprocal attention mixing (H-RAMi) layer that compensates for pixel-level information losses and utilizes semantic information while maintaining an efficient hierarchical structure. Furthermore, we revisit 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ANTONIO&#30340;Python&#24211;&#65292;&#23427;&#22522;&#20110;&#25277;&#35937;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#21644;&#21551;&#21457;&#24335;&#35268;&#21017;&#65292;&#20197;&#20415;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#29983;&#25104;&#24050;&#30693;&#39564;&#35777;&#26041;&#27861;&#30340;&#22522;&#20934;&#12290;&#22240;&#20026;&#20854;&#26222;&#36941;&#36866;&#29992;&#24615;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#20026;&#23558;NLP&#39564;&#35777;&#38382;&#39064;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#27604;&#36187;&#24320;&#36767;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;NLP&#38382;&#39064;&#20013;&#26222;&#21450;&#36825;&#19968;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.04003</link><description>&lt;p&gt;
ANTONIO:&#38754;&#21521;NLP&#39564;&#35777;&#30340;&#31995;&#32479;&#21270;&#22522;&#20934;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ANTONIO: Towards a Systematic Method of Generating NLP Benchmarks for Verification. (arXiv:2305.04003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ANTONIO&#30340;Python&#24211;&#65292;&#23427;&#22522;&#20110;&#25277;&#35937;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#21644;&#21551;&#21457;&#24335;&#35268;&#21017;&#65292;&#20197;&#20415;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#29983;&#25104;&#24050;&#30693;&#39564;&#35777;&#26041;&#27861;&#30340;&#22522;&#20934;&#12290;&#22240;&#20026;&#20854;&#26222;&#36941;&#36866;&#29992;&#24615;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#20026;&#23558;NLP&#39564;&#35777;&#38382;&#39064;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#27604;&#36187;&#24320;&#36767;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;NLP&#38382;&#39064;&#20013;&#26222;&#21450;&#36825;&#19968;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#20351;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39564;&#35777;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#38590;&#39064;&#12290;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#26041;&#27861;&#24120;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20854;&#20182;&#25968;&#23383;&#25968;&#25454;&#38598;&#65292;&#20294;&#24182;&#19981;&#36866;&#29992;&#20110;NLP&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36896;&#25104;&#36825;&#19968;&#38382;&#39064;&#30340;&#25216;&#26415;&#21407;&#22240;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#23454;&#29992;&#30340;&#26041;&#27861;&#21644;&#21551;&#21457;&#24335;&#35268;&#21017;&#65292;&#20197;&#20415;&#23558;NLP&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#20934;&#22791;&#20026;&#36866;&#21512;&#22522;&#20110;&#25277;&#35937;&#35299;&#37322;&#30340;&#24050;&#30693;&#39564;&#35777;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#23454;&#29616;&#20026;&#19968;&#20010;&#21517;&#20026;ANTONIO&#30340;Python&#24211;&#65292;&#35813;&#24211;&#36830;&#25509;&#21040;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#22120;ERAN&#21644;Marabou&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21517;&#20026;R-U-A-Robot&#30340;NLP&#25968;&#25454;&#38598;&#23545;&#24037;&#20855;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35813;&#25968;&#25454;&#38598;&#34987;&#25552;&#35758;&#20316;&#20026;&#39564;&#35777;&#20855;&#26377;&#27861;&#24459;&#37325;&#35201;&#24615;&#30340;NLP&#24212;&#29992;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#24076;&#26395;&#65292;&#30001;&#20110;&#20854;&#26222;&#36941;&#36866;&#29992;&#24615;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#20026;&#23558;NLP&#39564;&#35777;&#38382;&#39064;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#27604;&#36187;&#24320;&#36767;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;NLP&#38382;&#39064;&#20013;&#26222;&#21450;&#36825;&#19968;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Verification of machine learning models used in Natural Language Processing (NLP) is known to be a hard problem. In particular, many known neural network verification methods that work for computer vision and other numeric datasets do not work for NLP. Here, we study technical reasons that underlie this problem. Based on this analysis, we propose practical methods and heuristics for preparing NLP datasets and models in a way that renders them amenable to known verification methods based on abstract interpretation. We implement these methods as a Python library called ANTONIO that links to the neural network verifiers ERAN and Marabou. We perform evaluation of the tool using an NLP dataset R-U-A-Robot suggested as a benchmark for verifying legally critical NLP applications. We hope that, thanks to its general applicability, this work will open novel possibilities for including NLP verification problems into neural network verification competitions, and will popularise NLP problems withi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#20262;&#29702;&#25512;&#29702;&#26041;&#27861;&#65292;&#20351;&#29992;&#20551;&#24819;&#22238;&#39038;&#35770;&#35777;&#31243;&#24207;&#65292;&#20801;&#35768;&#20351;&#29992;&#21508;&#31181;&#21746;&#23398;&#29702;&#35770;&#36827;&#34892;&#20262;&#29702;&#25512;&#29702;&#65292;&#21487;&#20197;&#32771;&#34385;&#27010;&#29575;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#24212;&#29992;&#20110;&#19968;&#20010;&#33258;&#20027;&#22270;&#20070;&#39302;&#31995;&#32479;&#29992;&#20363;&#20013;&#36827;&#34892;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2305.01424</link><description>&lt;p&gt;
&#20351;&#29992;&#20551;&#24819;&#22238;&#39038;&#36827;&#34892;&#19981;&#30830;&#23450;&#26426;&#22120;&#20262;&#29702;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Uncertain Machine Ethical Decisions Using Hypothetical Retrospection. (arXiv:2305.01424v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#20262;&#29702;&#25512;&#29702;&#26041;&#27861;&#65292;&#20351;&#29992;&#20551;&#24819;&#22238;&#39038;&#35770;&#35777;&#31243;&#24207;&#65292;&#20801;&#35768;&#20351;&#29992;&#21508;&#31181;&#21746;&#23398;&#29702;&#35770;&#36827;&#34892;&#20262;&#29702;&#25512;&#29702;&#65292;&#21487;&#20197;&#32771;&#34385;&#27010;&#29575;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#24212;&#29992;&#20110;&#19968;&#20010;&#33258;&#20027;&#22270;&#20070;&#39302;&#31995;&#32479;&#29992;&#20363;&#20013;&#36827;&#34892;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#30001;Sven Hansson&#24320;&#21457;&#30340;&#20551;&#24819;&#22238;&#39038;&#35770;&#35777;&#31243;&#24207;&#65292;&#20197;&#32771;&#34385;&#27010;&#29575;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#21746;&#23398;&#30340;&#35282;&#24230;&#20986;&#21457;&#25913;&#36827;&#29616;&#26377;&#30340;&#26426;&#22120;&#20262;&#29702;&#25512;&#29702;&#26041;&#27861;&#65292;&#20197;&#19982;&#20154;&#31867;&#20135;&#29983;&#20849;&#40483;&#12290;&#35813;&#31243;&#24207;&#23558;&#34892;&#21160;&#34920;&#31034;&#20026;&#19968;&#32452;&#21487;&#33021;&#32467;&#26524;&#30340;&#20998;&#25903;&#65292;&#27599;&#20010;&#20998;&#25903;&#20855;&#26377;&#29366;&#24577;&#12289;&#25928;&#29992;&#21644;&#25968;&#20540;&#25110;&#35799;&#24847;&#27010;&#29575;&#20272;&#35745;&#12290;&#34892;&#21160;&#30340;&#36873;&#25321;&#22522;&#20110;&#20174;&#23427;&#20204;&#30340;&#20998;&#25903;&#30340;&#35282;&#24230;&#27604;&#36739;&#20542;&#21521;&#20110;&#34892;&#21160;&#30340;&#19968;&#32452;&#35770;&#35777;&#65292;&#21363;&#20351;&#36825;&#20123;&#20998;&#25903;&#23548;&#33268;&#20102;&#19981;&#33391;&#32467;&#26524;&#20063;&#19968;&#26679;&#12290;&#36825;&#31181;&#20351;&#29992;&#35770;&#35777;&#30340;&#26041;&#27861;&#20801;&#35768;&#20351;&#29992;&#21508;&#31181;&#21746;&#23398;&#29702;&#35770;&#36827;&#34892;&#20262;&#29702;&#25512;&#29702;&#65292;&#21487;&#33021;&#21487;&#20197;&#28789;&#27963;&#22320;&#32452;&#21512;&#20351;&#29992;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#19968;&#20010;&#33258;&#20027;&#22270;&#20070;&#39302;&#31995;&#32479;&#29992;&#20363;&#20013;&#65292;&#20998;&#21035;&#29420;&#31435;&#25110;&#21516;&#26102;&#24212;&#29992;&#32467;&#26524;&#20027;&#20041;&#21644;&#20041;&#21153;&#35770;&#30340;&#20262;&#29702;&#29702;&#35770;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#21021;&#27493;&#30340;&#26694;&#26550;&#65292;&#20284;&#20046;&#28385;&#36275;&#20102;&#21508;&#31181;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the use of the hypothetical retrospection argumentation procedure, developed by Sven Hansson, to improve existing approaches to machine ethical reasoning by accounting for probability and uncertainty from a position of Philosophy that resonates with humans. Actions are represented with a branching set of potential outcomes, each with a state, utility, and either a numeric or poetic probability estimate. Actions are chosen based on comparisons between sets of arguments favouring actions from the perspective of their branches, even those branches that led to an undesirable outcome. This use of arguments allows a variety of philosophical theories for ethical reasoning to be used, potentially in flexible combination with each other. We implement the procedure, applying consequentialist and deontological ethical theories, independently and concurrently, to an autonomous library system use case. We introduce a a preliminary framework that seems to meet the varied requirements of a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20803;&#22810;&#37325;&#22270;&#30340;&#26032;&#27010;&#24565;&#26469;&#20195;&#26367;&#20803;&#22270;&#21644;&#20803;&#36335;&#24452;&#29992;&#20110;&#22788;&#29702;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#20013;&#30340;&#20449;&#24687;&#32858;&#21512;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#31283;&#23450;&#30340;&#21487;&#24494;&#20998;&#25628;&#32034;&#26041;&#27861;&#26469;&#20248;&#21270;&#20803;&#22810;&#37325;&#22270;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;HIN&#65292;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#22797;&#26434;&#21040;&#31616;&#27905;&#30340;C2C&#20803;&#22810;&#37325;&#22270;&#26469;&#38477;&#20302;&#20887;&#20313;&#20449;&#24687;&#20256;&#25773;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.11574</link><description>&lt;p&gt;
&#20803;&#22810;&#37325;&#22270;&#25628;&#32034;&#65306;&#37325;&#26032;&#24605;&#32771;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#19978;&#30340;&#20803;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Meta-multigraph Search: Rethinking Meta-structure on Heterogeneous Information Networks. (arXiv:2304.11574v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20803;&#22810;&#37325;&#22270;&#30340;&#26032;&#27010;&#24565;&#26469;&#20195;&#26367;&#20803;&#22270;&#21644;&#20803;&#36335;&#24452;&#29992;&#20110;&#22788;&#29702;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#20013;&#30340;&#20449;&#24687;&#32858;&#21512;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#31283;&#23450;&#30340;&#21487;&#24494;&#20998;&#25628;&#32034;&#26041;&#27861;&#26469;&#20248;&#21270;&#20803;&#22810;&#37325;&#22270;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;HIN&#65292;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#22797;&#26434;&#21040;&#31616;&#27905;&#30340;C2C&#20803;&#22810;&#37325;&#22270;&#26469;&#38477;&#20302;&#20887;&#20313;&#20449;&#24687;&#20256;&#25773;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#32467;&#26500;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#23450;&#20041;&#22312;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#65288;HIN&#65289;&#20013;&#21738;&#20123;&#37051;&#23621;&#30340;&#23376;&#38598;&#26469;&#32858;&#21512;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#29616;&#26377;&#30340;&#20803;&#32467;&#26500;&#65292;&#21253;&#25324;&#20803;&#36335;&#24452;&#21644;&#20803;&#22270;&#65292;&#24182;&#35266;&#23519;&#21040;&#23427;&#20204;&#26368;&#21021;&#26159;&#25163;&#21160;&#35774;&#35745;&#30340;&#22266;&#23450;&#27169;&#24335;&#65292;&#22240;&#27492;&#19981;&#36275;&#20197;&#32534;&#30721;&#19981;&#21516;HIN&#19978;&#30340;&#21508;&#31181;&#20016;&#23500;&#35821;&#20041;&#20449;&#24687;&#12290;&#36890;&#36807;&#21453;&#24605;&#23427;&#20204;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#31216;&#20026;&#20803;&#22810;&#37325;&#22270;&#30340;&#26032;&#27010;&#24565;&#65292;&#20316;&#20026;&#20803;&#22270;&#30340;&#26356;&#20855;&#34920;&#29616;&#21147;&#21644;&#28789;&#27963;&#30340;&#27867;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#21487;&#24494;&#20998;&#30340;&#25628;&#32034;&#26041;&#27861;&#65292;&#20197;&#33258;&#21160;&#20248;&#21270;&#29305;&#23450;HIN&#21644;&#20219;&#21153;&#30340;&#20803;&#22810;&#37325;&#22270;&#12290;&#30001;&#20110;&#20803;&#22810;&#37325;&#22270;&#30340;&#28789;&#27963;&#24615;&#21487;&#33021;&#20250;&#20256;&#25773;&#20887;&#20313;&#30340;&#28040;&#24687;&#65292;&#22240;&#27492;&#25105;&#20204;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#22797;&#26434;&#21040;&#31616;&#27905;&#30340;C2C&#20803;&#22810;&#37325;&#22270;&#65292;&#23427;&#27839;&#30528;&#20803;&#22810;&#37325;&#22270;&#30340;&#28145;&#24230;&#20174;&#22797;&#26434;&#21040;&#31616;&#27905;&#22320;&#20256;&#25773;&#28040;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#21487;&#24494;&#20998;&#25628;&#32034;&#36890;&#24120;&#20250;&#36973;&#21463;&#19981;&#31283;&#23450;&#30340;&#25628;&#32034;&#21644;&#26174;&#30528;&#30340;g
&lt;/p&gt;
&lt;p&gt;
Meta-structures are widely used to define which subset of neighbors to aggregate information in heterogeneous information networks (HINs). In this work, we investigate existing meta-structures, including meta-path and meta-graph, and observe that they are initially designed manually with fixed patterns and hence are insufficient to encode various rich semantic information on diverse HINs. Through reflection on their limitation, we define a new concept called meta-multigraph as a more expressive and flexible generalization of meta-graph, and propose a stable differentiable search method to automatically optimize the meta-multigraph for specific HINs and tasks. As the flexibility of meta-multigraphs may propagate redundant messages, we further introduce a complex-to-concise (C2C) meta-multigraph that propagates messages from complex to concise along the depth of meta-multigraph. Moreover, we observe that the differentiable search typically suffers from unstable search and a significant g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#20851;&#31995;&#22312;&#32447;POMDP&#35268;&#21010;&#26041;&#27861;CAR-DESPOT&#65292;&#20351;&#29992;&#22240;&#26524;&#24314;&#27169;&#21644;&#25512;&#29702;&#26469;&#28040;&#38500;&#26410;&#27979;&#37327;&#28151;&#28102;&#21464;&#37327;&#24341;&#36215;&#30340;&#38169;&#35823;&#65292;&#24182;&#22312;&#28151;&#26434;&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2304.06848</link><description>&lt;p&gt;
CAR-DESPOT: &#38024;&#23545;&#28151;&#26434;&#29615;&#22659;&#19979;&#30340;&#26426;&#22120;&#20154;&#30340;&#22240;&#26524;&#20851;&#31995;&#22312;&#32447;POMDP&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
CAR-DESPOT: Causally-Informed Online POMDP Planning for Robots in Confounded Environments. (arXiv:2304.06848v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#20851;&#31995;&#22312;&#32447;POMDP&#35268;&#21010;&#26041;&#27861;CAR-DESPOT&#65292;&#20351;&#29992;&#22240;&#26524;&#24314;&#27169;&#21644;&#25512;&#29702;&#26469;&#28040;&#38500;&#26410;&#27979;&#37327;&#28151;&#28102;&#21464;&#37327;&#24341;&#36215;&#30340;&#38169;&#35823;&#65292;&#24182;&#22312;&#28151;&#26434;&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#24037;&#20316;&#30340;&#26426;&#22120;&#20154;&#24517;&#39035;&#32771;&#34385;&#38543;&#26426;&#34892;&#20026;&#30340;&#21487;&#33021;&#32467;&#26524;&#65292;&#24182;&#26681;&#25454;&#30495;&#23454;&#30340;&#19990;&#30028;&#29366;&#24577;&#30340;&#37096;&#20998;&#35266;&#23519;&#36827;&#34892;&#20915;&#31574;&#12290;&#22240;&#26524;&#28151;&#28102;&#30340;&#38382;&#39064;&#26159;&#36827;&#34892;&#20934;&#30830;&#21644;&#24378;&#20581;&#30340;&#34892;&#20026;&#39044;&#27979;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(POMDP)&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27169;&#25311;&#36825;&#20123;&#38543;&#26426;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#20915;&#31574;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#26126;&#30830;&#30340;&#22240;&#26524;&#35821;&#20041;&#65292;POMDP&#35268;&#21010;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#28151;&#28102;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#22312;&#26410;&#35266;&#23519;&#21040;&#28151;&#26434;&#21464;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#33021;&#20250;&#20135;&#29983;&#34920;&#29616;&#19981;&#20339;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#20851;&#31995;&#22312;&#32447;POMDP&#35268;&#21010;&#26041;&#27861;&#65292;&#20351;&#29992;&#22240;&#26524;&#24314;&#27169;&#21644;&#25512;&#29702;&#26469;&#28040;&#38500;&#26410;&#27979;&#37327;&#28151;&#28102;&#21464;&#37327;&#24341;&#36215;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#20351;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;CAR-DESPOT&#22312;&#28151;&#26434;&#29615;&#22659;&#20013;&#27604;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;POMDP&#35268;&#21010;&#31243;&#24207;&#34920;&#29616;&#26174;&#33879;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robots operating in real-world environments must reason about possible outcomes of stochastic actions and make decisions based on partial observations of the true world state. A major challenge for making accurate and robust action predictions is the problem of confounding, which if left untreated can lead to prediction errors. The partially observable Markov decision process (POMDP) is a widely-used framework to model these stochastic and partially-observable decision-making problems. However, due to a lack of explicit causal semantics, POMDP planning methods are prone to confounding bias and thus in the presence of unobserved confounders may produce underperforming policies. This paper presents a novel causally-informed extension of "anytime regularized determinized sparse partially observable tree" (AR-DESPOT), a modern anytime online POMDP planner, using causal modelling and inference to eliminate errors caused by unmeasured confounder variables. We further propose a method to lear
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;GPT&#26816;&#27979;&#22120;&#23545;&#38750;&#33521;&#35821;&#27597;&#35821;&#20316;&#32773;&#23384;&#22312;&#20559;&#35265;&#65292;&#23481;&#26131;&#23558;&#20854;&#20869;&#23481;&#38169;&#35823;&#22320;&#20998;&#31867;&#20026;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;&#27492;&#22806;&#65292;&#31616;&#21333;&#30340;&#25552;&#31034;&#31574;&#30053;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#65292;&#21516;&#26102;&#35268;&#36991;GPT&#26816;&#27979;&#22120;&#65292;&#36825;&#34920;&#26126;GPT&#26816;&#27979;&#22120;&#21487;&#33021;&#20250;&#24809;&#32602;&#20855;&#26377;&#21463;&#38480;&#35821;&#35328;&#34920;&#36798;&#33021;&#21147;&#30340;&#20316;&#32773;&#12290;</title><link>http://arxiv.org/abs/2304.02819</link><description>&lt;p&gt;
GPT&#26816;&#27979;&#22120;&#23545;&#38750;&#33521;&#35821;&#27597;&#35821;&#30340;&#20316;&#32773;&#23384;&#22312;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
GPT detectors are biased against non-native English writers. (arXiv:2304.02819v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02819
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;GPT&#26816;&#27979;&#22120;&#23545;&#38750;&#33521;&#35821;&#27597;&#35821;&#20316;&#32773;&#23384;&#22312;&#20559;&#35265;&#65292;&#23481;&#26131;&#23558;&#20854;&#20869;&#23481;&#38169;&#35823;&#22320;&#20998;&#31867;&#20026;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;&#27492;&#22806;&#65292;&#31616;&#21333;&#30340;&#25552;&#31034;&#31574;&#30053;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#65292;&#21516;&#26102;&#35268;&#36991;GPT&#26816;&#27979;&#22120;&#65292;&#36825;&#34920;&#26126;GPT&#26816;&#27979;&#22120;&#21487;&#33021;&#20250;&#24809;&#32602;&#20855;&#26377;&#21463;&#38480;&#35821;&#35328;&#34920;&#36798;&#33021;&#21147;&#30340;&#20316;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#25512;&#24191;&#24102;&#26469;&#20102;&#25968;&#23383;&#36890;&#20449;&#26041;&#38754;&#30340;&#23454;&#36136;&#24615;&#36827;&#23637;&#65292;&#21516;&#26102;&#20063;&#24341;&#21457;&#20102;AI&#29983;&#25104;&#20869;&#23481;&#28508;&#22312;&#35823;&#29992;&#30340;&#25285;&#24551;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26816;&#27979;&#26041;&#27861;&#26469;&#21306;&#20998;AI&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#20869;&#23481;&#65292;&#20294;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#33521;&#35821;&#27597;&#35821;&#21644;&#38750;&#33521;&#35821;&#27597;&#35821;&#20316;&#32773;&#30340;&#20889;&#20316;&#26679;&#26412;&#35780;&#20272;&#20102;&#20960;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;GPT&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#20123;&#26816;&#27979;&#22120;&#25345;&#32493;&#23558;&#38750;&#33521;&#35821;&#27597;&#35821;&#30340;&#20889;&#20316;&#26679;&#26412;&#38169;&#35823;&#22320;&#20998;&#31867;&#20026;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#65292;&#32780;&#21407;&#29983;&#20889;&#20316;&#26679;&#26412;&#21017;&#33021;&#22815;&#34987;&#20934;&#30830;&#35782;&#21035;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31616;&#21333;&#30340;&#25552;&#31034;&#31574;&#30053;&#19981;&#20165;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#26377;&#25928;&#22320;&#35268;&#36991;GPT&#26816;&#27979;&#22120;&#65292;&#36825;&#34920;&#26126;GPT&#26816;&#27979;&#22120;&#21487;&#33021;&#26080;&#24847;&#20013;&#24809;&#32602;&#20855;&#26377;&#21463;&#38480;&#35821;&#35328;&#34920;&#36798;&#33021;&#21147;&#30340;&#20316;&#32773;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#21628;&#21505;&#36827;&#34892;&#26356;&#24191;&#27867;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid adoption of generative language models has brought about substantial advancements in digital communication, while simultaneously raising concerns regarding the potential misuse of AI-generated content. Although numerous detection methods have been proposed to differentiate between AI and human-generated content, the fairness and robustness of these detectors remain underexplored. In this study, we evaluate the performance of several widely-used GPT detectors using writing samples from native and non-native English writers. Our findings reveal that these detectors consistently misclassify non-native English writing samples as AI-generated, whereas native writing samples are accurately identified. Furthermore, we demonstrate that simple prompting strategies can not only mitigate this bias but also effectively bypass GPT detectors, suggesting that GPT detectors may unintentionally penalize writers with constrained linguistic expressions. Our results call for a broader conversati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25805;&#32437;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#35328;&#34893;&#29983;&#30340;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#34920;&#31034;&#65292;&#25506;&#35752;&#20102;&#35821;&#35328;&#26159;&#21542;&#20250;&#22240;&#26524;&#25903;&#25345;&#24773;&#32490;&#25512;&#26029;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#23646;&#24615;&#29305;&#23450;&#30340;&#31070;&#32463;&#20803;&#25805;&#32437;&#23548;&#33268;&#24773;&#32490;&#25512;&#26029;&#20219;&#21153;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#36825;&#19982;&#20154;&#31867;&#24515;&#29702;&#31354;&#38388;&#20013;&#19981;&#21516;&#23646;&#24615;&#30340;&#37325;&#35201;&#24615;&#26377;&#20851;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#25903;&#25345;&#22522;&#20110;&#35821;&#35328;&#30340;&#24773;&#32490;&#25512;&#26029;&#26426;&#21046;&#25552;&#20379;&#20102;&#22240;&#26524;&#35777;&#25454;&#65292;&#24182;&#20984;&#26174;&#20102;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2302.09582</link><description>&lt;p&gt;
&#35821;&#35328;&#29305;&#23450;&#30340;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#34920;&#31034;&#23545;&#24773;&#32490;&#25512;&#26029;&#30340;&#22240;&#26524;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Language-Specific Representation of Emotion-Concept Knowledge Causally Supports Emotion Inference. (arXiv:2302.09582v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25805;&#32437;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#35328;&#34893;&#29983;&#30340;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#34920;&#31034;&#65292;&#25506;&#35752;&#20102;&#35821;&#35328;&#26159;&#21542;&#20250;&#22240;&#26524;&#25903;&#25345;&#24773;&#32490;&#25512;&#26029;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#23646;&#24615;&#29305;&#23450;&#30340;&#31070;&#32463;&#20803;&#25805;&#32437;&#23548;&#33268;&#24773;&#32490;&#25512;&#26029;&#20219;&#21153;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#36825;&#19982;&#20154;&#31867;&#24515;&#29702;&#31354;&#38388;&#20013;&#19981;&#21516;&#23646;&#24615;&#30340;&#37325;&#35201;&#24615;&#26377;&#20851;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#25903;&#25345;&#22522;&#20110;&#35821;&#35328;&#30340;&#24773;&#32490;&#25512;&#26029;&#26426;&#21046;&#25552;&#20379;&#20102;&#22240;&#26524;&#35777;&#25454;&#65292;&#24182;&#20984;&#26174;&#20102;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24773;&#32490;&#31185;&#23398;&#20013;&#65292;&#22914;&#20309;&#29702;&#35299;&#35821;&#35328;&#25903;&#25345;&#24773;&#32490;&#25512;&#26029;&#20173;&#28982;&#26159;&#19968;&#20010;&#20105;&#35758;&#30340;&#35805;&#39064;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25805;&#32437;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#35328;&#34893;&#29983;&#30340;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#34920;&#31034;&#65292;&#35843;&#26597;&#20102;&#35821;&#35328;&#26159;&#21542;&#20250;&#22240;&#26524;&#25903;&#25345;&#24773;&#32490;&#25512;&#26029;&#12290;&#20351;&#29992;&#25552;&#31034;&#25216;&#26415;&#65292;&#21457;&#29616;&#20102;14&#20010;&#24773;&#32490;&#27010;&#24565;&#30340;&#23646;&#24615;&#30001;&#19981;&#21516;&#30340;&#20154;&#24037;&#31070;&#32463;&#20803;&#32676;&#20307;&#34920;&#31034;&#12290;&#36890;&#36807;&#25805;&#32437;&#36825;&#20123;&#23646;&#24615;&#30456;&#20851;&#30340;&#31070;&#32463;&#20803;&#65292;&#19982;&#38543;&#26426;&#25805;&#32437;&#30456;&#27604;&#65292;&#22823;&#22810;&#25968;&#24773;&#32490;&#25512;&#26029;&#20219;&#21153;&#30340;&#34920;&#29616;&#20986;&#29616;&#20102;&#19979;&#38477;&#12290;&#23646;&#24615;&#29305;&#23450;&#30340;&#34920;&#29616;&#19979;&#38477;&#19982;&#20154;&#31867;&#24515;&#29702;&#31354;&#38388;&#20013;&#19981;&#21516;&#23646;&#24615;&#30340;&#37325;&#35201;&#24615;&#26377;&#20851;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25552;&#20379;&#20102;&#25903;&#25345;&#22522;&#20110;&#35821;&#35328;&#30340;&#24773;&#32490;&#25512;&#26029;&#26426;&#21046;&#30340;&#22240;&#26524;&#35777;&#25454;&#65292;&#24182;&#24378;&#35843;&#20102;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding how language supports emotion inference remains a topic of debate in emotion science. The present study investigated whether language-derived emotion-concept knowledge would causally support emotion inference by manipulating the language-specific knowledge representations in large language models. Using the prompt technique, 14 attributes of emotion concepts were found to be represented by distinct artificial neuron populations. By manipulating these attribute-related neurons, the majority of the emotion inference tasks showed performance deterioration compared to random manipulations. The attribute-specific performance deterioration was related to the importance of different attributes in human mental space. Our findings provide causal evidence in support of a language-based mechanism for emotion inference and highlight the contributions of emotion-concept knowledge.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;Transformer&#26550;&#26500;&#65292;&#29992;&#20110;&#34920;&#31034;&#20855;&#26377;&#26102;&#38388;&#30456;&#20851;&#30340;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#39057;&#29575;&#20989;&#25968;&#26469;&#34920;&#31034;&#25968;&#20540;&#29305;&#24449;&#65292;&#24182;&#37319;&#29992;&#21807;&#19968;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#32479;&#19968;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2302.06375</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;&#25152;&#26377;&#26102;&#38388;&#24207;&#21015;&#30340;Transformer&#65306;&#34920;&#31034;&#21644;&#35757;&#32451;&#20855;&#26377;&#26102;&#38388;&#30456;&#20851;&#30340;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
One Transformer for All Time Series: Representing and Training with Time-Dependent Heterogeneous Tabular Data. (arXiv:2302.06375v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;Transformer&#26550;&#26500;&#65292;&#29992;&#20110;&#34920;&#31034;&#20855;&#26377;&#26102;&#38388;&#30456;&#20851;&#30340;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#39057;&#29575;&#20989;&#25968;&#26469;&#34920;&#31034;&#25968;&#20540;&#29305;&#24449;&#65292;&#24182;&#37319;&#29992;&#21807;&#19968;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#32479;&#19968;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#65292;&#20197;&#22797;&#21046;&#20854;&#20182;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#22312;&#36825;&#19968;&#32467;&#26500;&#21270;&#39046;&#22495;&#30340;&#25104;&#21151;&#12290;&#29305;&#21035;&#26377;&#36259;&#30340;&#26159;&#65292;&#34920;&#26684;&#25968;&#25454;&#20855;&#26377;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#20363;&#22914;&#37329;&#34701;&#20132;&#26131;&#12290;&#28982;&#32780;&#65292;&#34920;&#26684;&#20540;&#30340;&#24322;&#36136;&#24615;&#65292;&#20854;&#20013;&#31867;&#21035;&#20803;&#32032;&#19982;&#25968;&#20540;&#39033;&#28151;&#21512;&#65292;&#20351;&#24471;&#36825;&#31181;&#36866;&#24212;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Transformer&#26550;&#26500;&#26469;&#34920;&#31034;&#24322;&#26500;&#30340;&#26102;&#38388;&#30456;&#20851;&#30340;&#34920;&#26684;&#25968;&#25454;&#65292;&#25968;&#20540;&#29305;&#24449;&#20351;&#29992;&#19968;&#32452;&#39057;&#29575;&#20989;&#25968;&#34920;&#31034;&#65292;&#24182;&#19988;&#25972;&#20010;&#32593;&#32476;&#20351;&#29992;&#21807;&#19968;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#32479;&#19968;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a recent growing interest in applying Deep Learning techniques to tabular data, in order to replicate the success of other Artificial Intelligence areas in this structured domain. Specifically interesting is the case in which tabular data have a time dependence, such as, for instance financial transactions. However, the heterogeneity of the tabular values, in which categorical elements are mixed with numerical items, makes this adaptation difficult. In this paper we propose a Transformer architecture to represent heterogeneous time-dependent tabular data, in which numerical features are represented using a set of frequency functions and the whole network is uniformly trained with a unique loss function.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#35299;&#21270;&#30340;&#26041;&#27861;&#65292;&#20197;&#25903;&#25345;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#36890;&#36807;&#22270;&#35299;&#22411;&#21644;&#20551;&#35774;&#24615;&#25512;&#29702;&#65292;&#32553;&#23567;&#21487;&#35299;&#37322;&#24615;&#24046;&#36317;&#12290;&#36890;&#36807;&#20020;&#24202;&#24212;&#29992;&#30740;&#31350;&#21644;&#24314;&#27169;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;DiagramNet&#19981;&#20165;&#33021;&#25552;&#20379;&#24544;&#23454;&#30340;&#26434;&#38899;&#24418;&#29366;&#35299;&#37322;&#65292;&#36824;&#20855;&#26377;&#36739;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#19988;&#22270;&#35299;&#22411;&#35299;&#37322;&#22312;&#20020;&#24202;&#30456;&#20851;&#30340;&#24773;&#20917;&#19979;&#26356;&#21463;&#25512;&#23815;&#12290;</title><link>http://arxiv.org/abs/2302.01241</link><description>&lt;p&gt;
&#22270;&#35299;&#21270;&#65306;&#21033;&#29992;&#22270;&#35299;&#22411;AI&#35299;&#37322;&#23545;&#20551;&#35774;&#24615;&#28436;&#32462;&#25512;&#29702;&#30340;&#29702;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
Diagrammatization: Rationalizing with diagrammatic AI explanations for abductive-deductive reasoning on hypotheses. (arXiv:2302.01241v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#35299;&#21270;&#30340;&#26041;&#27861;&#65292;&#20197;&#25903;&#25345;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#36890;&#36807;&#22270;&#35299;&#22411;&#21644;&#20551;&#35774;&#24615;&#25512;&#29702;&#65292;&#32553;&#23567;&#21487;&#35299;&#37322;&#24615;&#24046;&#36317;&#12290;&#36890;&#36807;&#20020;&#24202;&#24212;&#29992;&#30740;&#31350;&#21644;&#24314;&#27169;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;DiagramNet&#19981;&#20165;&#33021;&#25552;&#20379;&#24544;&#23454;&#30340;&#26434;&#38899;&#24418;&#29366;&#35299;&#37322;&#65292;&#36824;&#20855;&#26377;&#36739;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#19988;&#22270;&#35299;&#22411;&#35299;&#37322;&#22312;&#20020;&#24202;&#30456;&#20851;&#30340;&#24773;&#20917;&#19979;&#26356;&#21463;&#25512;&#23815;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#21487;&#35270;&#21270;&#24037;&#20855;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#29992;&#25143;&#36827;&#19968;&#27493;&#25512;&#29702;&#26469;&#35299;&#37322;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;XAI&#24212;&#35813;&#25903;&#25345;&#22270;&#35299;&#22411;&#21644;&#20551;&#35774;&#24615;&#25512;&#29702;&#65292;&#20197;&#20415;AI&#33021;&#22815;&#36827;&#34892;&#20551;&#35774;&#29983;&#25104;&#21644;&#35780;&#20272;&#65292;&#20174;&#32780;&#20943;&#23569;&#21487;&#35299;&#37322;&#24615;&#24046;&#36317;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22270;&#35299;&#21270;&#26041;&#27861;&#65292;&#20197;i)&#36827;&#34892;Peircean&#25512;&#23548;-&#28436;&#32462;&#25512;&#29702;&#65292;ii)&#36981;&#24490;&#39046;&#22495;&#24815;&#20363;&#65292;&#21644;iii)&#29992;&#22270;&#31034;&#25110;&#35821;&#35328;&#36827;&#34892;&#35299;&#37322;&#12290;&#25105;&#20204;&#22312;&#20020;&#24202;&#24212;&#29992;&#39046;&#22495;&#23454;&#29616;&#20102;DiagramNet&#65292;&#20197;&#39044;&#27979;&#24515;&#33039;&#21548;&#35786;&#20013;&#30340;&#24515;&#33039;&#35786;&#26029;&#65292;&#24182;&#29992;&#22522;&#20110;&#24418;&#29366;&#30340;&#26434;&#38899;&#22270;&#35299;&#36827;&#34892;&#35299;&#37322;&#12290;&#22312;&#24314;&#27169;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;DiagramNet&#19981;&#20165;&#25552;&#20379;&#20102;&#24544;&#23454;&#30340;&#26434;&#38899;&#24418;&#29366;&#35299;&#37322;&#65292;&#32780;&#19988;&#27604;&#22522;&#32447;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#21307;&#23398;&#29983;&#30340;&#23450;&#24615;&#29992;&#25143;&#30740;&#31350;&#23637;&#31034;&#20102;&#22270;&#35299;&#22411;&#35299;&#37322;&#30340;&#21487;&#29702;&#35299;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#24182;&#34920;&#26126;&#22312;&#20020;&#24202;&#30456;&#20851;&#30340;&#24773;&#20917;&#19979;&#65292;&#22270;&#35299;&#24335;&#35299;&#37322;&#27604;&#20854;&#20182;&#26041;&#24335;&#26356;&#21463;&#25512;&#23815;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many visualizations have been developed for explainable AI (XAI), but they often require further reasoning by users to interpret. We argue that XAI should support diagrammatic and abductive reasoning for the AI to perform hypothesis generation and evaluation to reduce the interpretability gap. We propose Diagrammatization to i) perform Peircean abductive-deductive reasoning, ii) follow domain conventions, and iii) explain with diagrams visually or verbally. We implemented DiagramNet for a clinical application to predict cardiac diagnoses from heart auscultation, and explain with shape-based murmur diagrams. In modeling studies, we found that DiagramNet not only provides faithful murmur shape explanations, but also has better prediction performance than baseline models. We further demonstrate the interpretability and trustworthiness of diagrammatic explanations in a qualitative user study with medical students, showing that clinically-relevant, diagrammatic explanations are preferred ov
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#20998;&#23376;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#32467;&#26500;&#21644;&#29983;&#21270;&#24615;&#36136;&#30340;&#27169;&#24577;&#32467;&#21512;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#23558;&#20998;&#23376;&#30340;&#32467;&#26500;&#21644;&#24615;&#36136;&#20043;&#38388;&#30340;&#21452;&#21521;&#20449;&#24687;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#22810;&#27169;&#24577;&#21644;&#21333;&#27169;&#24577;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2211.10590</link><description>&lt;p&gt;
&#21333;&#20010;&#20998;&#23376;&#22522;&#30784;&#27169;&#22411;&#30340;&#21452;&#21521;&#32467;&#26500;&#21644;&#24615;&#36136;&#30340;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Bidirectional Generation of Structure and Properties Through a Single Molecular Foundation Model. (arXiv:2211.10590v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#20998;&#23376;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#32467;&#26500;&#21644;&#29983;&#21270;&#24615;&#36136;&#30340;&#27169;&#24577;&#32467;&#21512;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#23558;&#20998;&#23376;&#30340;&#32467;&#26500;&#21644;&#24615;&#36136;&#20043;&#38388;&#30340;&#21452;&#21521;&#20449;&#24687;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#22810;&#27169;&#24577;&#21644;&#21333;&#27169;&#24577;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20013;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#25104;&#21151;&#20419;&#20351;&#20102;&#21270;&#23398;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20986;&#29616;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#23545;&#25552;&#20379;&#19979;&#28216;&#20219;&#21153;&#30340;&#20449;&#24687;&#34920;&#31034;&#30340;&#22823;&#20998;&#23376;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#65292;&#20294;&#20998;&#23376;&#39046;&#22495;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#26041;&#27861;&#23581;&#35797;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#20998;&#23376;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20174;&#22810;&#27169;&#24577;&#23398;&#20064;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#23558;&#32467;&#26500;&#21644;&#29983;&#21270;&#24615;&#36136;&#30340;&#27169;&#24577;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#31649;&#36947;&#22788;&#29702;&#25968;&#25454;&#24182;&#26681;&#25454;&#20849;&#21516;&#30340;&#23884;&#20837;&#31354;&#38388;&#23545;&#40784;&#32467;&#26500;/&#24615;&#36136;&#29305;&#24449;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#23558;&#20998;&#23376;&#30340;&#32467;&#26500;&#21644;&#24615;&#36136;&#20043;&#38388;&#30340;&#21452;&#21521;&#20449;&#24687;&#32852;&#31995;&#36215;&#26469;&#12290;&#36825;&#20123;&#36129;&#29486;&#20135;&#29983;&#20102;&#21327;&#21516;&#30340;&#30693;&#35782;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#21333;&#19968;&#27169;&#22411;&#22788;&#29702;&#22810;&#27169;&#24577;&#21644;&#21333;&#27169;&#24577;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36275;&#20197;&#29983;&#25104;&#20855;&#26377;&#21452;&#21521;&#32467;&#26500;&#21644;&#24615;&#36136;&#30340;&#20998;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent success of large foundation models in artificial intelligence has prompted the emergence of chemical pre-trained models. Despite the growing interest in large molecular pre-trained models that provide informative representations for downstream tasks, attempts for multimodal pre-training approaches on the molecule domain were limited. To address this, we present a novel multimodal molecular pre-trained model that incorporates the modalities of structure and biochemical properties, drawing inspiration from recent advances in multimodal learning techniques. Our proposed model pipeline of data handling and training objectives aligns the structure/property features in a common embedding space, which enables the model to regard bidirectional information between the molecules' structure and properties. These contributions emerge synergistic knowledge, allowing us to tackle both multimodal and unimodal downstream tasks through a single model. Through extensive experiments, we demons
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#27979;&#37327;&#21644;&#39044;&#27979;&#22270;&#20687;&#23545;&#26426;&#22120;&#30340;&#35760;&#24518;&#24615;&#65292;&#24182;&#21457;&#29616;&#8220;&#22797;&#26434;&#8221;&#22270;&#20687;&#36890;&#24120;&#23545;&#26426;&#22120;&#35760;&#24518;&#26356;&#21152;&#28145;&#21051;&#12290;&#36890;&#36807;&#35814;&#32454;&#20998;&#26512;&#21644;&#23454;&#39564;&#39564;&#35777;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26426;&#22120;&#35760;&#24518;&#24615;&#30340;&#27010;&#24565;&#65292;&#24182;&#20026;&#26426;&#22120;&#35760;&#24518;&#21644;&#35270;&#35273;&#25968;&#25454;&#30340;&#30740;&#31350;&#26041;&#21521;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.07625</link><description>&lt;p&gt;
&#22270;&#20687;&#23545;&#26426;&#22120;&#30340;&#35760;&#24518;&#24615;&#26377;&#20309;&#24433;&#21709;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Images are More Memorable to Machines?. (arXiv:2211.07625v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#27979;&#37327;&#21644;&#39044;&#27979;&#22270;&#20687;&#23545;&#26426;&#22120;&#30340;&#35760;&#24518;&#24615;&#65292;&#24182;&#21457;&#29616;&#8220;&#22797;&#26434;&#8221;&#22270;&#20687;&#36890;&#24120;&#23545;&#26426;&#22120;&#35760;&#24518;&#26356;&#21152;&#28145;&#21051;&#12290;&#36890;&#36807;&#35814;&#32454;&#20998;&#26512;&#21644;&#23454;&#39564;&#39564;&#35777;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26426;&#22120;&#35760;&#24518;&#24615;&#30340;&#27010;&#24565;&#65292;&#24182;&#20026;&#26426;&#22120;&#35760;&#24518;&#21644;&#35270;&#35273;&#25968;&#25454;&#30340;&#30740;&#31350;&#26041;&#21521;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#27979;&#37327;&#21644;&#39044;&#27979;&#22270;&#20687;&#23545;&#27169;&#24335;&#35782;&#21035;&#26426;&#22120;&#30340;&#35760;&#24518;&#24615;&#65292;&#20197;&#25506;&#32034;&#26426;&#22120;&#26234;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#30340;&#26426;&#22120;&#35760;&#24518;&#37327;&#21270;&#27969;&#31243;&#65292;&#31216;&#20026;&#8220;MachineMem measurer&#8221;&#65292;&#29992;&#20110;&#25910;&#38598;&#22270;&#20687;&#30340;&#26426;&#22120;&#35760;&#24518;&#24615;&#20998;&#25968;&#12290;&#19982;&#20154;&#31867;&#31867;&#20284;&#65292;&#26426;&#22120;&#20063;&#20542;&#21521;&#20110;&#35760;&#24518;&#26576;&#20123;&#31867;&#22411;&#30340;&#22270;&#20687;&#65292;&#20294;&#26426;&#22120;&#21644;&#20154;&#31867;&#35760;&#24518;&#30340;&#22270;&#20687;&#31867;&#22411;&#26159;&#19981;&#21516;&#30340;&#12290;&#36890;&#36807;&#28145;&#20837;&#20998;&#26512;&#21644;&#20840;&#38754;&#30340;&#21487;&#35270;&#21270;&#65292;&#25105;&#20204;&#36880;&#28176;&#25581;&#31034;&#20102;&#8220;&#22797;&#26434;&#8221;&#22270;&#20687;&#36890;&#24120;&#23545;&#26426;&#22120;&#35760;&#24518;&#26356;&#21152;&#28145;&#21051;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;11&#21488;&#19981;&#21516;&#30340;&#26426;&#22120;&#65288;&#20174;&#32447;&#24615;&#20998;&#31867;&#22120;&#21040;&#29616;&#20195;ViTs&#65289;&#21644;9&#31181;&#39044;&#35757;&#32451;&#26041;&#27861;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#20197;&#20998;&#26512;&#21644;&#29702;&#35299;&#26426;&#22120;&#35760;&#24518;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#26426;&#22120;&#35760;&#24518;&#24615;&#30340;&#27010;&#24565;&#65292;&#24182;&#22312;&#26426;&#22120;&#35760;&#24518;&#21644;&#35270;&#35273;&#25968;&#25454;&#30340;&#20132;&#21449;&#30028;&#38754;&#19978;&#24320;&#36767;&#20102;&#19968;&#26465;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the problem of measuring and predicting how memorable an image is to pattern recognition machines, as a path to explore machine intelligence. Firstly, we propose a self-supervised machine memory quantification pipeline, dubbed ``MachineMem measurer'', to collect machine memorability scores of images. Similar to humans, machines also tend to memorize certain kinds of images, whereas the types of images that machines and humans memorize are different. Through in-depth analysis and comprehensive visualizations, we gradually unveil that``complex" images are usually more memorable to machines. We further conduct extensive experiments across 11 different machines (from linear classifiers to modern ViTs) and 9 pre-training methods to analyze and understand machine memory. This work proposes the concept of machine memorability and opens a new research direction at the interface between machine memory and visual data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LIME&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35299;&#37322;&#26469;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24615;&#33021;&#12290;&#36890;&#36807;&#24341;&#20837;&#33258;&#23450;&#20041;&#21152;&#26435;&#25439;&#22833;&#65292;&#23558;&#30495;&#23454;LIME&#35299;&#37322;&#19982;&#27169;&#22411;&#39044;&#27979;&#30340;LIME&#35299;&#37322;&#20043;&#38388;&#30340;&#36317;&#31163;&#32771;&#34385;&#22312;&#20869;&#65292;&#24110;&#21161;&#27169;&#22411;&#26356;&#22909;&#22320;&#27867;&#21270;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#23454;&#38469;&#35757;&#32451;&#22330;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#39034;&#24207;&#23398;&#20064;&#32780;&#19981;&#20002;&#22833;&#20808;&#21069;&#25968;&#25454;&#20998;&#24067;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2211.01413</link><description>&lt;p&gt;
&#21033;&#29992;&#35299;&#37322;&#30340;&#21147;&#37327;&#36827;&#34892;&#22686;&#37327;&#35757;&#32451;&#65306;&#22522;&#20110;LIME&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Harnessing the Power of Explanations for Incremental Training: A LIME-Based Approach. (arXiv:2211.01413v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LIME&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35299;&#37322;&#26469;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24615;&#33021;&#12290;&#36890;&#36807;&#24341;&#20837;&#33258;&#23450;&#20041;&#21152;&#26435;&#25439;&#22833;&#65292;&#23558;&#30495;&#23454;LIME&#35299;&#37322;&#19982;&#27169;&#22411;&#39044;&#27979;&#30340;LIME&#35299;&#37322;&#20043;&#38388;&#30340;&#36317;&#31163;&#32771;&#34385;&#22312;&#20869;&#65292;&#24110;&#21161;&#27169;&#22411;&#26356;&#22909;&#22320;&#27867;&#21270;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#23454;&#38469;&#35757;&#32451;&#22330;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#39034;&#24207;&#23398;&#20064;&#32780;&#19981;&#20002;&#22833;&#20808;&#21069;&#25968;&#25454;&#20998;&#24067;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#29702;&#35299;&#29305;&#24449;&#37325;&#35201;&#24615;&#24182;&#33719;&#24471;&#21487;&#35299;&#37322;&#30340;&#27934;&#23519;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23545;&#31070;&#32463;&#32593;&#32476;&#32467;&#26524;&#30340;&#35299;&#37322;&#22823;&#22810;&#23616;&#38480;&#20110;&#21487;&#35270;&#21270;&#65292;&#24182;&#19988;&#24456;&#23569;&#26377;&#30740;&#31350;&#23558;&#36825;&#20123;&#35299;&#37322;&#29992;&#20316;&#25913;&#36827;&#27169;&#22411;&#24615;&#33021;&#30340;&#21453;&#39304;&#12290;&#26412;&#30740;&#31350;&#23558;&#27169;&#22411;&#30340;&#35299;&#37322;&#21453;&#39304;&#21040;&#21069;&#39304;&#35757;&#32451;&#20013;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#26356;&#22909;&#22320;&#27867;&#21270;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23450;&#20041;&#21152;&#26435;&#25439;&#22833;&#65292;&#20854;&#20013;&#26435;&#37325;&#26159;&#36890;&#36807;&#32771;&#34385;&#30495;&#23454;LIME&#65288;&#23616;&#37096;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;&#65289;&#35299;&#37322;&#21644;&#27169;&#22411;&#39044;&#27979;&#30340;LIME&#35299;&#37322;&#20043;&#38388;&#30340;&#27431;&#24335;&#36317;&#31163;&#26469;&#29983;&#25104;&#30340;&#12290;&#27492;&#22806;&#65292;&#22312;&#23454;&#38469;&#30340;&#35757;&#32451;&#22330;&#26223;&#20013;&#65292;&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#24110;&#21161;&#27169;&#22411;&#39034;&#24207;&#23398;&#20064;&#32780;&#19981;&#20002;&#22833;&#20808;&#21069;&#25968;&#25454;&#20998;&#24067;&#20449;&#24687;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#24517;&#35201;&#30340;&#65292;&#22240;&#20026;&#19981;&#20250;&#19968;&#27425;&#24615;&#25552;&#20379;&#25152;&#26377;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#35813;&#26694;&#26550;&#23558;&#33258;&#23450;&#20041;&#21152;&#26435;&#25439;&#22833;&#19982;&#24377;&#24615;&#35757;&#32451;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainability of neural network prediction is essential to understand feature importance and gain interpretable insight into neural network performance. However, explanations of neural network outcomes are mostly limited to visualization, and there is scarce work that looks to use these explanations as feedback to improve model performance. In this work, model explanations are fed back to the feed-forward training to help the model generalize better. To this extent, a custom weighted loss where the weights are generated by considering the Euclidean distances between true LIME (Local Interpretable Model-Agnostic Explanations) explanations and model-predicted LIME explanations is proposed. Also, in practical training scenarios, developing a solution that can help the model learn sequentially without losing information on previous data distribution is imperative due to the unavailability of all the training data at once. Thus, the framework incorporates the custom weighted loss with Elas
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#35299;&#30721;&#26159;&#19968;&#31181;&#21487;&#38752;&#30340;&#35299;&#30721;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20248;&#21270;&#23545;&#27604;&#30446;&#26631;&#26469;&#29983;&#25104;&#21512;&#29702;&#19988;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#65292;&#19982;&#20165;&#20351;&#29992;&#36739;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35299;&#30721;&#30456;&#27604;&#65292;&#23545;&#27604;&#35299;&#30721;&#33021;&#22815;&#36991;&#20813;&#30701;&#32780;&#37325;&#22797;&#30340;&#25991;&#26412;&#21644;&#19981;&#36830;&#36143;&#30340;&#25991;&#26412;&#65292;&#24182;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#27169;&#22411;&#35268;&#27169;&#12290;</title><link>http://arxiv.org/abs/2210.15097</link><description>&lt;p&gt;
&#23545;&#27604;&#35299;&#30721;&#65306;&#23558;&#24320;&#25918;&#24335;&#25991;&#26412;&#29983;&#25104;&#35270;&#20026;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Contrastive Decoding: Open-ended Text Generation as Optimization. (arXiv:2210.15097v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15097
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35299;&#30721;&#26159;&#19968;&#31181;&#21487;&#38752;&#30340;&#35299;&#30721;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20248;&#21270;&#23545;&#27604;&#30446;&#26631;&#26469;&#29983;&#25104;&#21512;&#29702;&#19988;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#65292;&#19982;&#20165;&#20351;&#29992;&#36739;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35299;&#30721;&#30456;&#27604;&#65292;&#23545;&#27604;&#35299;&#30721;&#33021;&#22815;&#36991;&#20813;&#30701;&#32780;&#37325;&#22797;&#30340;&#25991;&#26412;&#21644;&#19981;&#36830;&#36143;&#30340;&#25991;&#26412;&#65292;&#24182;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#27169;&#22411;&#35268;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#65292;&#26368;&#22823;&#27010;&#29575;&#26159;&#24320;&#25918;&#24335;&#29983;&#25104;&#30340;&#36739;&#24046;&#35299;&#30721;&#30446;&#26631;&#65292;&#22240;&#20026;&#23427;&#20250;&#20135;&#29983;&#30701;&#32780;&#37325;&#22797;&#30340;&#25991;&#26412;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#37319;&#26679;&#24448;&#24448;&#20250;&#20135;&#29983;&#19982;&#21407;&#22987;&#20027;&#39064;&#20559;&#31163;&#30340;&#19981;&#36830;&#36143;&#25991;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#35299;&#30721;&#65288;CD&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#38752;&#30340;&#35299;&#30721;&#26041;&#27861;&#65292;&#23427;&#22312;&#28385;&#36275;&#21512;&#29702;&#24615;&#32422;&#26463;&#26465;&#20214;&#30340;&#21069;&#25552;&#19979;&#20248;&#21270;&#23545;&#27604;&#30446;&#26631;&#12290;&#23545;&#27604;&#30446;&#26631;&#36820;&#22238;&#19968;&#20010;&#22823;&#22411;LM&#65288;&#34987;&#31216;&#20026;&#19987;&#23478;&#65292;&#20363;&#22914;OPT-13B&#65289;&#21644;&#19968;&#20010;&#23567;&#22411;LM&#65288;&#34987;&#31216;&#20026;&#19994;&#20313;&#32773;&#65292;&#20363;&#22914;OPT-125M&#65289;&#20043;&#38388;&#30340;&#20284;&#28982;&#24046;&#24322;&#65292;&#24182;&#19988;&#32422;&#26463;&#26465;&#20214;&#30830;&#20445;&#36755;&#20986;&#26159;&#21512;&#29702;&#30340;&#12290;CD&#30340;&#28789;&#24863;&#26469;&#33258;&#20110;&#36825;&#26679;&#19968;&#20010;&#20107;&#23454;&#65292;&#21363;&#36739;&#22823;&#30340;LM&#65288;&#20363;&#22914;&#37325;&#22797;&#12289;&#19981;&#36830;&#36143;&#65289;&#22312;&#36739;&#23567;&#30340;LM&#20013;&#26356;&#20026;&#26222;&#36941;&#65292;&#24182;&#19988;&#36825;&#31181;&#24046;&#24322;&#34920;&#26126;&#21738;&#20123;&#25991;&#26412;&#24212;&#20248;&#20808;&#32771;&#34385;&#12290;CD&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#22521;&#35757;&#65292;&#24182;&#19988;&#27604;&#20165;&#20174;&#36739;&#22823;&#30340;LM&#36827;&#34892;&#35299;&#30721;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#26356;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#12290;&#23427;&#36824;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#27169;&#22411;&#35268;&#27169;&#65288;OPT-13B&#21644;GPT2-1.5B&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a language model (LM), maximum probability is a poor decoding objective for open-ended generation, because it produces short and repetitive text. On the other hand, sampling can often produce incoherent text that drifts from the original topics. We propose contrastive decoding (CD), a reliable decoding approach that optimizes a contrastive objective subject to a plausibility constraint. The contrastive objective returns the difference between the likelihood under a large LM (called the expert, e.g. OPT-13B) and a small LM (called the amateur, e.g. OPT-125M), and the constraint ensures that the outputs are plausible. CD is inspired by the fact that the failures of larger LMs (e.g., repetition, incoherence) are even more prevalent in smaller LMs, and that this difference signals which texts should be preferred. CD requires zero additional training, and produces higher quality text than decoding from the larger LM alone. It also works across model scales (OPT-13B and GPT2-1.5B) and 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#30340;&#33258;&#36866;&#24212;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#22810;&#23454;&#20363;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#22312;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.09452</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#30340;&#33258;&#36866;&#24212;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#22810;&#23454;&#20363;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multiple Instance Learning via Iterative Self-Paced Supervised Contrastive Learning. (arXiv:2210.09452v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#30340;&#33258;&#36866;&#24212;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#22810;&#23454;&#20363;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#22312;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21482;&#26377;&#34955;&#32423;&#26631;&#31614;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#21333;&#20010;&#23454;&#20363;&#30340;&#34920;&#31034;&#26159;&#22810;&#23454;&#20363;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#23545;&#27604;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65288;CSSL&#65289;&#21487;&#20197;&#21462;&#24471;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;CSSL&#23398;&#20064;&#23558;&#23545;&#24212;&#20110;&#20004;&#20010;&#19981;&#21516;&#38543;&#26426;&#36873;&#25321;&#30340;&#23454;&#20363;&#30340;&#34920;&#31034;&#25512;&#24320;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#20363;&#22914;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#65292;&#36890;&#24120;&#23384;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#22240;&#27492;&#38543;&#26426;&#36873;&#25321;&#30340;&#23454;&#20363;&#22823;&#37096;&#20998;&#23646;&#20110;&#21516;&#19968;&#20010;&#22810;&#25968;&#31867;&#21035;&#65292;&#36825;&#20351;&#24471;CSSL&#26080;&#27861;&#23398;&#20064;&#31867;&#38388;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#36845;&#20195;&#33258;&#36866;&#24212;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#22810;&#23454;&#20363;&#34920;&#31034;&#65288;ItS2CLR&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#20174;&#21253;&#32423;&#26631;&#31614;&#27966;&#29983;&#30340;&#23454;&#20363;&#32423;&#20266;&#26631;&#31614;&#26469;&#25913;&#21892;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#22810;&#23454;&#20363;&#34920;&#31034;&#30340;&#31574;&#30053;&#26469;&#30830;&#20445;&#20266;&#26631;&#31614;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;ItS2CLR&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning representations for individual instances when only bag-level labels are available is a fundamental challenge in multiple instance learning (MIL). Recent works have shown promising results using contrastive self-supervised learning (CSSL), which learns to push apart representations corresponding to two different randomly-selected instances. Unfortunately, in real-world applications such as medical image classification, there is often class imbalance, so randomly-selected instances mostly belong to the same majority class, which precludes CSSL from learning inter-class differences. To address this issue, we propose a novel framework, Iterative Self-paced Supervised Contrastive Learning for MIL Representations (ItS2CLR), which improves the learned representation by exploiting instance-level pseudo labels derived from the bag-level labels. The framework employs a novel self-paced sampling strategy to ensure the accuracy of pseudo labels. We evaluate ItS2CLR on three medical datase
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#20998;&#24067;&#40065;&#26834;&#24615;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#27979;&#35797;&#20219;&#21153;&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#26102;&#34892;&#20026;&#36866;&#24212;&#65292;&#25552;&#20379;&#20102;&#23545;&#20998;&#24067;&#36716;&#21464;&#30340;&#25913;&#21892;&#36951;&#25022;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.03104</link><description>&lt;p&gt;
&#20998;&#24067;&#33258;&#36866;&#24212;&#20803;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distributionally Adaptive Meta Reinforcement Learning. (arXiv:2210.03104v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#20998;&#24067;&#40065;&#26834;&#24615;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#27979;&#35797;&#20219;&#21153;&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#26102;&#34892;&#20026;&#36866;&#24212;&#65292;&#25552;&#20379;&#20102;&#23545;&#20998;&#24067;&#36716;&#21464;&#30340;&#25913;&#21892;&#36951;&#25022;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#26469;&#33719;&#21462;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#22870;&#21169;&#25110;&#21160;&#21147;&#23398;&#20989;&#25968;&#30340;&#20219;&#21153;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#24471;&#21040;&#30340;&#20803;&#31574;&#30053;&#36890;&#24120;&#21482;&#22312;&#20854;&#35757;&#32451;&#26102;&#30340;&#31934;&#30830;&#20219;&#21153;&#20998;&#24067;&#19978;&#26377;&#25928;&#65292;&#22312;&#27979;&#35797;&#26102;&#30340;&#22870;&#21169;&#25110;&#36807;&#28193;&#21160;&#21147;&#23398;&#30340;&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#26102;&#20250;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#20803;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#20219;&#21153;&#31354;&#38388;&#20013;&#36866;&#24212;&#24615;&#22320;&#24212;&#23545;&#27979;&#35797;&#26102;&#30340;&#20998;&#24067;&#36716;&#21464;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20197;&#33258;&#36866;&#24212;&#30340;&#20998;&#24067;&#40065;&#26834;&#24615;&#26041;&#27861;&#20026;&#26680;&#24515;&#65292;&#35757;&#32451;&#19968;&#32676;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#20998;&#24067;&#36716;&#21464;&#40065;&#26834;&#24615;&#30340;&#20803;&#31574;&#30053;&#12290;&#24403;&#22312;&#21487;&#33021;&#21457;&#29983;&#20998;&#24067;&#36716;&#21464;&#30340;&#27979;&#35797;&#20219;&#21153;&#20998;&#24067;&#19978;&#35780;&#20272;&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#36873;&#25321;&#20855;&#26377;&#26368;&#21512;&#36866;&#40065;&#26834;&#24615;&#27700;&#24179;&#30340;&#20803;&#31574;&#30053;&#65292;&#24182;&#20351;&#29992;&#23427;&#36827;&#34892;&#24555;&#36895;&#36866;&#24212;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#20998;&#24067;&#36716;&#21464;&#19979;&#33021;&#22815;&#25913;&#21892;&#36951;&#25022;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-reinforcement learning algorithms provide a data-driven way to acquire policies that quickly adapt to many tasks with varying rewards or dynamics functions. However, learned meta-policies are often effective only on the exact task distribution on which they were trained and struggle in the presence of distribution shift of test-time rewards or transition dynamics. In this work, we develop a framework for meta-RL algorithms that are able to behave appropriately under test-time distribution shifts in the space of tasks. Our framework centers on an adaptive approach to distributional robustness that trains a population of meta-policies to be robust to varying levels of distribution shift. When evaluated on a potentially shifted test-time distribution of tasks, this allows us to choose the meta-policy with the most appropriate level of robustness, and use it to perform fast adaptation. We formally show how our framework allows for improved regret under distribution shift, and empirica
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#29305;&#24449;&#23481;&#37327;&#26469;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22810;&#20041;&#24615;&#29616;&#35937;&#65292;&#21457;&#29616;&#22312;&#26368;&#20248;&#30340;&#23481;&#37327;&#20998;&#37197;&#19979;&#65292;&#31070;&#32463;&#32593;&#32476;&#20542;&#21521;&#20110;&#21333;&#20041;&#22320;&#34920;&#31034;&#37325;&#35201;&#29305;&#24449;&#65292;&#22810;&#20041;&#22320;&#34920;&#31034;&#27425;&#37325;&#35201;&#29305;&#24449;&#65292;&#24182;&#24573;&#30053;&#26368;&#19981;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290;&#22810;&#20041;&#24615;&#29616;&#35937;&#22312;&#36755;&#20837;&#20855;&#26377;&#26356;&#39640;&#30340;&#23792;&#24230;&#25110;&#31232;&#30095;&#24615;&#26102;&#26356;&#20026;&#26222;&#36941;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#20307;&#31995;&#32467;&#26500;&#20013;&#27604;&#20854;&#20182;&#20307;&#31995;&#32467;&#26500;&#26356;&#20026;&#26222;&#36941;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#21457;&#29616;&#20102;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#20998;&#22359;&#21322;&#27491;&#20132;&#32467;&#26500;&#65292;&#19981;&#21516;&#27169;&#22411;&#20013;&#30340;&#20998;&#22359;&#22823;&#23567;&#19981;&#21516;&#65292;&#31361;&#20986;&#20102;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2210.01892</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22810;&#20041;&#24615;&#21644;&#23481;&#37327;
&lt;/p&gt;
&lt;p&gt;
Polysemanticity and Capacity in Neural Networks. (arXiv:2210.01892v3 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01892
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#29305;&#24449;&#23481;&#37327;&#26469;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22810;&#20041;&#24615;&#29616;&#35937;&#65292;&#21457;&#29616;&#22312;&#26368;&#20248;&#30340;&#23481;&#37327;&#20998;&#37197;&#19979;&#65292;&#31070;&#32463;&#32593;&#32476;&#20542;&#21521;&#20110;&#21333;&#20041;&#22320;&#34920;&#31034;&#37325;&#35201;&#29305;&#24449;&#65292;&#22810;&#20041;&#22320;&#34920;&#31034;&#27425;&#37325;&#35201;&#29305;&#24449;&#65292;&#24182;&#24573;&#30053;&#26368;&#19981;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290;&#22810;&#20041;&#24615;&#29616;&#35937;&#22312;&#36755;&#20837;&#20855;&#26377;&#26356;&#39640;&#30340;&#23792;&#24230;&#25110;&#31232;&#30095;&#24615;&#26102;&#26356;&#20026;&#26222;&#36941;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#20307;&#31995;&#32467;&#26500;&#20013;&#27604;&#20854;&#20182;&#20307;&#31995;&#32467;&#26500;&#26356;&#20026;&#26222;&#36941;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#21457;&#29616;&#20102;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#20998;&#22359;&#21322;&#27491;&#20132;&#32467;&#26500;&#65292;&#19981;&#21516;&#27169;&#22411;&#20013;&#30340;&#20998;&#22359;&#22823;&#23567;&#19981;&#21516;&#65292;&#31361;&#20986;&#20102;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21333;&#20010;&#31070;&#32463;&#20803;&#36890;&#24120;&#20195;&#34920;&#26080;&#20851;&#29305;&#24449;&#30340;&#28151;&#21512;&#12290;&#36825;&#31181;&#29616;&#35937;&#31216;&#20026;&#22810;&#20041;&#24615;&#65292;&#20351;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#29702;&#35299;&#20854;&#21407;&#22240;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#29305;&#24449;&#23481;&#37327;&#30340;&#35270;&#35282;&#26469;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#65292;&#29305;&#24449;&#23481;&#37327;&#26159;&#27599;&#20010;&#29305;&#24449;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#21344;&#29992;&#30340;&#20998;&#24418;&#32500;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19968;&#20010;&#29609;&#20855;&#27169;&#22411;&#20013;&#65292;&#26368;&#20248;&#30340;&#23481;&#37327;&#20998;&#37197;&#20542;&#21521;&#20110;&#21333;&#20041;&#22320;&#34920;&#31034;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#65292;&#22810;&#20041;&#22320;&#34920;&#31034;&#27425;&#37325;&#35201;&#29305;&#24449;&#65288;&#19982;&#20854;&#23545;&#25439;&#22833;&#30340;&#24433;&#21709;&#25104;&#27604;&#20363;&#65289;&#65292;&#24182;&#23436;&#20840;&#24573;&#30053;&#26368;&#19981;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290;&#24403;&#36755;&#20837;&#20855;&#26377;&#26356;&#39640;&#30340;&#23792;&#24230;&#25110;&#31232;&#30095;&#24615;&#26102;&#65292;&#22810;&#20041;&#24615;&#26356;&#20026;&#26222;&#36941;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#20307;&#31995;&#32467;&#26500;&#20013;&#27604;&#20854;&#20182;&#20307;&#31995;&#32467;&#26500;&#26356;&#20026;&#26222;&#36941;&#12290;&#22312;&#24471;&#21040;&#26368;&#20248;&#23481;&#37327;&#20998;&#37197;&#21518;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#23884;&#20837;&#31354;&#38388;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#20998;&#22359;&#21322;&#27491;&#20132;&#30340;&#32467;&#26500;&#65292;&#19981;&#21516;&#27169;&#22411;&#20013;&#30340;&#20998;&#22359;&#22823;&#23567;&#19981;&#21516;&#65292;&#31361;&#20986;&#20102;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Individual neurons in neural networks often represent a mixture of unrelated features. This phenomenon, called polysemanticity, can make interpreting neural networks more difficult and so we aim to understand its causes. We propose doing so through the lens of feature \emph{capacity}, which is the fractional dimension each feature consumes in the embedding space. We show that in a toy model the optimal capacity allocation tends to monosemantically represent the most important features, polysemantically represent less important features (in proportion to their impact on the loss), and entirely ignore the least important features. Polysemanticity is more prevalent when the inputs have higher kurtosis or sparsity and more prevalent in some architectures than others. Given an optimal allocation of capacity, we go on to study the geometry of the embedding space. We find a block-semi-orthogonal structure, with differing block sizes in different models, highlighting the impact of model archit
&lt;/p&gt;</description></item><item><title>GreenKGC&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#20010;&#27169;&#22359;&#65288;&#34920;&#31034;&#23398;&#20064;&#12289;&#29305;&#24449;&#20462;&#21098;&#21644;&#20915;&#31574;&#23398;&#20064;&#65289;&#25552;&#21462;&#21028;&#21035;&#24615;&#30340;&#30693;&#35782;&#22270;&#35889;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#20998;&#31867;&#22120;&#21644;&#36127;&#37319;&#26679;&#36827;&#34892;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#22312;&#20302;&#32500;&#24230;&#19979;&#65292;GreenKGC&#33021;&#22815;&#22312;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20854;&#20182;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#31454;&#20105;&#24615;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.09137</link><description>&lt;p&gt;
GreenKGC&#65306;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GreenKGC: A Lightweight Knowledge Graph Completion Method. (arXiv:2208.09137v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09137
&lt;/p&gt;
&lt;p&gt;
GreenKGC&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#20010;&#27169;&#22359;&#65288;&#34920;&#31034;&#23398;&#20064;&#12289;&#29305;&#24449;&#20462;&#21098;&#21644;&#20915;&#31574;&#23398;&#20064;&#65289;&#25552;&#21462;&#21028;&#21035;&#24615;&#30340;&#30693;&#35782;&#22270;&#35889;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#20998;&#31867;&#22120;&#21644;&#36127;&#37319;&#26679;&#36827;&#34892;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#22312;&#20302;&#32500;&#24230;&#19979;&#65292;GreenKGC&#33021;&#22815;&#22312;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20854;&#20182;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#31454;&#20105;&#24615;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#65288;KGC&#65289;&#26088;&#22312;&#21457;&#29616;&#30693;&#35782;&#22270;&#35889;&#20013;&#23454;&#20307;&#20043;&#38388;&#30340;&#32570;&#22833;&#20851;&#31995;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;KGC&#24037;&#20316;&#38598;&#20013;&#22312;&#36890;&#36807;&#31616;&#21333;&#30340;&#35780;&#20998;&#20989;&#25968;&#23398;&#20064;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#26356;&#39640;&#32500;&#24230;&#30340;&#23884;&#20837;&#31354;&#38388;&#36890;&#24120;&#38656;&#35201;&#26356;&#22909;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#36825;&#23548;&#33268;&#27169;&#22411;&#23610;&#23544;&#26356;&#22823;&#65292;&#38480;&#21046;&#20102;&#22312;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#65288;&#20363;&#22914;&#22823;&#35268;&#27169;&#30340;&#30693;&#35782;&#22270;&#35889;&#25110;&#31227;&#21160;/&#36793;&#32536;&#35745;&#31639;&#65289;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#27169;&#22359;&#21270;&#30340;KGC&#35299;&#20915;&#26041;&#26696;&#65292;&#31216;&#20026;GreenKGC&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;GreenKGC&#21253;&#25324;&#19977;&#20010;&#27169;&#22359;&#65306;&#34920;&#31034;&#23398;&#20064;&#65292;&#29305;&#24449;&#20462;&#21098;&#21644;&#20915;&#31574;&#23398;&#20064;&#65292;&#36890;&#36807;&#20998;&#31867;&#22120;&#21644;&#36127;&#37319;&#26679;&#25552;&#21462;&#21028;&#21035;&#24615;&#30340;&#30693;&#35782;&#22270;&#35889;&#29305;&#24449;&#65292;&#24182;&#23545;&#32570;&#22833;&#20851;&#31995;&#20570;&#20986;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20302;&#32500;&#24230;&#19979;&#65292;GreenKGC&#21487;&#20197;&#22312;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#20302;&#32500;&#30340;GreenKGC&#21487;&#20197;&#23454;&#29616;&#31454;&#20105;&#24615;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph completion (KGC) aims to discover missing relationships between entities in knowledge graphs (KGs). Most prior KGC work focuses on learning embeddings for entities and relations through a simple scoring function. Yet, a higher-dimensional embedding space is usually required for a better reasoning capability, which leads to a larger model size and hinders applicability to real-world problems (e.g., large-scale KGs or mobile/edge computing). A lightweight modularized KGC solution, called GreenKGC, is proposed in this work to address this issue. GreenKGC consists of three modules: representation learning, feature pruning, and decision learning, to extract discriminant KG features and make accurate predictions on missing relationships using classifiers and negative sampling. Experimental results demonstrate that, in low dimensions, GreenKGC can outperform SOTA methods in most datasets. In addition, low-dimensional GreenKGC can achieve competitive or even better performance 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#65292;&#22312;&#20801;&#35768;&#24494;&#23567;&#20559;&#35265;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#35774;&#35745;&#20986;&#26082;&#20960;&#20046;&#23436;&#20840;&#20844;&#24179;&#21448;&#20855;&#26377;&#24120;&#25968;&#20493;&#25928;&#30410;&#27604;&#29575;&#30340;&#26426;&#22120;&#35843;&#24230;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2204.06438</link><description>&lt;p&gt;
&#20844;&#24179;&#31639;&#27861;&#35774;&#35745;&#65306;&#20844;&#24179;&#19988;&#26377;&#25928;&#30340;&#26426;&#22120;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Fair Algorithm Design: Fair and Efficacious Machine Scheduling. (arXiv:2204.06438v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.06438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#65292;&#22312;&#20801;&#35768;&#24494;&#23567;&#20559;&#35265;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#35774;&#35745;&#20986;&#26082;&#20960;&#20046;&#23436;&#20840;&#20844;&#24179;&#21448;&#20855;&#26377;&#24120;&#25968;&#20493;&#25928;&#30410;&#27604;&#29575;&#30340;&#26426;&#22120;&#35843;&#24230;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#20363;&#23376;&#20013;&#65292;&#33258;&#21160;&#20915;&#31574;&#31639;&#27861;&#23548;&#33268;&#20102;&#20559;&#35265;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#20844;&#24179;&#31639;&#27861;&#35774;&#35745;&#30340;&#24378;&#28872;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20844;&#24179;&#21644;&#25928;&#30410;&#20043;&#38388;&#24448;&#24448;&#23384;&#22312;&#23545;&#31435;&#65306;&#20844;&#24179;&#31639;&#27861;&#21487;&#33021;&#25552;&#20379;&#36739;&#20302;&#30340;&#31038;&#20250;&#31119;&#21033;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#20248;&#21270;&#31119;&#21033;&#30340;&#31639;&#27861;&#21487;&#33021;&#38750;&#24120;&#19981;&#20844;&#24179;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#26426;&#22120;&#35843;&#24230;&#38382;&#39064;&#20013;&#24471;&#21040;&#20102;&#20307;&#29616;&#65292;&#23545;&#20110;n&#20010;&#20316;&#19994;&#65292;&#20219;&#20309;&#20844;&#24179;&#35299;&#30340;&#31038;&#20250;&#31119;&#21033;&#21487;&#33021;&#27604;&#26368;&#20248;&#31119;&#21033;&#24046;&#19968;&#20010;$\Omega(n)$&#30340;&#22240;&#23376;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#65292;&#22914;&#26524;&#25105;&#20204;&#20801;&#35768;&#23384;&#22312;&#24494;&#19981;&#36275;&#36947;&#30340;&#20559;&#35265;&#65292;&#20844;&#24179;&#21644;&#25928;&#33021;&#20043;&#38388;&#30340;&#23545;&#31435;&#26159;&#21487;&#20197;&#20811;&#26381;&#30340;&#65306;&#23384;&#22312;&#19968;&#20123;&#31639;&#27861;&#26082;&#26159;&#8220;&#20960;&#20046;&#23436;&#20840;&#20844;&#24179;&#30340;&#8221;&#65292;&#21448;&#20855;&#26377;&#24120;&#25968;&#20493;&#30340;&#25928;&#30410;&#27604;&#29575;&#65292;&#20063;&#23601;&#26159;&#35828;&#65292;&#23427;&#20204;&#33021;&#22815;&#20445;&#35777;&#36755;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#31038;&#20250;&#31119;&#21033;&#19978;&#19982;&#26368;&#20248;&#31119;&#21033;&#20043;&#38388;&#30456;&#24046;&#19968;&#20010;&#24120;&#25968;&#20493;&#30340;&#22240;&#23376;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23545;&#20110;&#20219;&#20309;$\epsilon&gt;0$&#65292;&#23384;&#22312;&#20855;&#26377;&#25928;&#30410;&#27604;&#29575;$\Theta(\frac{1}{\epsilon})$&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by a plethora of practical examples where bias is induced by automated-decision making algorithms, there has been strong recent interest in the design of fair algorithms. However, there is often a dichotomy between fairness and efficacy: fair algorithms may proffer low social welfare solutions whereas welfare optimizing algorithms may be very unfair. This issue is exemplified in the machine scheduling problem where, for $n$ jobs, the social welfare of any fair solution may be a factor $\Omega(n)$ worse than the optimal welfare. In this paper, we prove that this dichotomy between fairness and efficacy can be overcome if we allow for a negligible amount of bias: there exist algorithms that are both "almost perfectly fair" and have a constant factor efficacy ratio, that is, are guaranteed to output solutions that have social welfare within a constant factor of optimal welfare. Specifically, for any $\epsilon&gt;0$, there exist mechanisms with efficacy ratio $\Theta(\frac{1}{\epsilo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20449;&#24687;&#35770;&#21644;&#21160;&#24577;&#35268;&#21010;&#25216;&#26415;&#65292;&#20026;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#24314;&#31435;&#20102;&#24615;&#33021;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;&#36890;&#36807;&#35745;&#31639;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#30340;&#25968;&#37327;&#65292;&#21487;&#20197;&#33719;&#24471;&#19968;&#27493;&#20915;&#31574;&#20219;&#21153;&#30340;&#26368;&#39640;&#21487;&#23454;&#29616;&#39044;&#26399;&#25253;&#37228;&#30340;&#19978;&#30028;&#65292;&#24182;&#36890;&#36807;&#21160;&#24577;&#35268;&#21010;&#23558;&#36825;&#20010;&#19978;&#30028;&#25193;&#23637;&#21040;&#22810;&#27493;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#31034;&#20363;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#20915;&#31574;&#36807;&#31243;&#12289;&#33258;&#30001;&#33853;&#20307;&#29289;&#20307;&#30340;&#25429;&#25417;&#21644;&#20351;&#29992;&#28145;&#24230;&#20256;&#24863;&#22120;&#30340;&#36991;&#38556;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2202.00129</link><description>&lt;p&gt;
&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#30340;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Fundamental Limits for Sensor-Based Robot Control. (arXiv:2202.00129v5 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.00129
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20449;&#24687;&#35770;&#21644;&#21160;&#24577;&#35268;&#21010;&#25216;&#26415;&#65292;&#20026;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#24314;&#31435;&#20102;&#24615;&#33021;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;&#36890;&#36807;&#35745;&#31639;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#30340;&#25968;&#37327;&#65292;&#21487;&#20197;&#33719;&#24471;&#19968;&#27493;&#20915;&#31574;&#20219;&#21153;&#30340;&#26368;&#39640;&#21487;&#23454;&#29616;&#39044;&#26399;&#25253;&#37228;&#30340;&#19978;&#30028;&#65292;&#24182;&#36890;&#36807;&#21160;&#24577;&#35268;&#21010;&#23558;&#36825;&#20010;&#19978;&#30028;&#25193;&#23637;&#21040;&#22810;&#27493;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#31034;&#20363;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#20915;&#31574;&#36807;&#31243;&#12289;&#33258;&#30001;&#33853;&#20307;&#29289;&#20307;&#30340;&#25429;&#25417;&#21644;&#20351;&#29992;&#28145;&#24230;&#20256;&#24863;&#22120;&#30340;&#36991;&#38556;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20026;&#32473;&#23450;&#20219;&#21153;&#24314;&#31435;&#26426;&#22120;&#20154;&#20256;&#24863;&#22120;&#24615;&#33021;&#30340;&#22522;&#26412;&#38480;&#21046;&#30340;&#29702;&#35770;&#21644;&#31639;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#25429;&#25417;&#20256;&#24863;&#22120;&#25552;&#20379;&#30340;&#19982;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#25968;&#37327;&#30340;&#37327;&#12290;&#21033;&#29992;&#20449;&#24687;&#35770;&#20013;&#24191;&#20041;Fano&#19981;&#31561;&#24335;&#30340;&#26032;&#39062;&#29256;&#26412;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#37327;&#25552;&#20379;&#20102;&#19968;&#27493;&#20915;&#31574;&#20219;&#21153;&#30340;&#26368;&#39640;&#21487;&#23454;&#29616;&#39044;&#26399;&#25253;&#37228;&#30340;&#19978;&#30028;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#23558;&#36825;&#20010;&#19978;&#30028;&#25193;&#23637;&#21040;&#22810;&#27493;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#25968;&#20540;&#35745;&#31639;&#24471;&#20986;&#30340;&#19978;&#30028;&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;&#19977;&#20010;&#31034;&#20363;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;&#65288;i&#65289;&#26469;&#33258;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#25991;&#29486;&#30340;&#29076;&#23721;&#38382;&#39064;&#65292;&#65288;ii&#65289;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#35266;&#27979;&#31354;&#38388;&#30340;&#31034;&#20363;&#65292;&#23545;&#24212;&#20110;&#26426;&#22120;&#20154;&#25509;&#20303;&#33258;&#30001;&#33853;&#20307;&#29289;&#20307;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#20351;&#29992;&#38750;&#39640;&#26031;&#22122;&#22768;&#28145;&#24230;&#20256;&#24863;&#22120;&#30340;&#36991;&#38556;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31639;&#27861;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Our goal is to develop theory and algorithms for establishing fundamental limits on performance imposed by a robot's sensors for a given task. In order to achieve this, we define a quantity that captures the amount of task-relevant information provided by a sensor. Using a novel version of the generalized Fano inequality from information theory, we demonstrate that this quantity provides an upper bound on the highest achievable expected reward for one-step decision making tasks. We then extend this bound to multi-step problems via a dynamic programming approach. We present algorithms for numerically computing the resulting bounds, and demonstrate our approach on three examples: (i) the lava problem from the literature on partially observable Markov decision processes, (ii) an example with continuous state and observation spaces corresponding to a robot catching a freely-falling object, and (iii) obstacle avoidance using a depth sensor with non-Gaussian noise. We demonstrate the ability
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#25968;&#25454;&#23545;&#24378;&#21270;&#23398;&#20064;&#30340;&#36951;&#25022;&#65292;&#25552;&#20986;&#20102;&#31934;&#32454;&#30340;&#25910;&#25947;&#36895;&#29575;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25910;&#25947;&#36895;&#24230;&#36739;&#24555;&#30340;&#29616;&#35937;&#65292;&#24182;&#36890;&#36807;&#25351;&#25968;&#24418;&#24335;&#30340;&#21152;&#36895;&#26426;&#21046;&#21152;&#24555;&#20102;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2102.00479</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#36951;&#25022;&#24555;&#36895;&#25910;&#25947;&#36895;&#29575;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Fast Rates for the Regret of Offline Reinforcement Learning. (arXiv:2102.00479v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.00479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#25968;&#25454;&#23545;&#24378;&#21270;&#23398;&#20064;&#30340;&#36951;&#25022;&#65292;&#25552;&#20986;&#20102;&#31934;&#32454;&#30340;&#25910;&#25947;&#36895;&#29575;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25910;&#25947;&#36895;&#24230;&#36739;&#24555;&#30340;&#29616;&#35937;&#65292;&#24182;&#36890;&#36807;&#25351;&#25968;&#24418;&#24335;&#30340;&#21152;&#36895;&#26426;&#21046;&#21152;&#24555;&#20102;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22266;&#23450;&#34892;&#20026;&#31574;&#30053;&#22312;&#26080;&#38480;&#26102;&#38388;&#25240;&#29616;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#20013;&#29983;&#25104;&#30340;&#31163;&#32447;&#25968;&#25454;&#23545;&#24378;&#21270;&#23398;&#20064;&#30340;&#36951;&#25022;&#12290;&#29616;&#26377;&#26041;&#27861;&#65288;&#22914;&#25311;&#21512;Q-&#36845;&#20195;&#65289;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#23545;&#20110;&#36951;&#25022;&#30340;&#25910;&#25947;&#36895;&#29575;&#26159;O(1/&#8730;n)&#65292;&#20294;&#23454;&#35777;&#34892;&#20026;&#34920;&#29616;&#20986;&#38750;&#24120;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20379;&#36951;&#25022;&#25910;&#25947;&#36895;&#29575;&#30340;&#24555;&#36895;&#25910;&#25947;&#36827;&#34892;&#26356;&#31934;&#32454;&#30340;&#36951;&#25022;&#20998;&#26512;&#65292;&#20934;&#30830;&#22320;&#34920;&#24449;&#20102;&#36825;&#19968;&#29616;&#35937;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#32473;&#23450;&#26368;&#20248;&#36136;&#37327;&#20989;&#25968;Q*&#30340;&#20272;&#35745;&#30340;&#24773;&#20917;&#19979;&#65292;&#20854;&#23545;&#24212;&#30340;&#31574;&#30053;&#36951;&#25022;&#25353;&#29031;Q*&#20272;&#35745;&#30340;&#28857;&#23545;&#28857;&#25910;&#25947;&#36895;&#29575;&#30340;&#25351;&#25968;&#36827;&#34892;&#25910;&#25947;&#65292;&#20174;&#32780;&#21152;&#36895;&#20102;&#25910;&#25947;&#36895;&#24230;&#12290;&#25351;&#25968;&#30340;&#32423;&#21035;&#21462;&#20915;&#20110;&#8220;&#20915;&#31574;&#38382;&#39064;&#8221;&#20013;&#30340;&#22122;&#22768;&#27700;&#24179;&#65292;&#32780;&#19981;&#26159;&#20272;&#35745;&#38382;&#39064;&#12290;&#25105;&#20204;&#20197;&#32447;&#24615;&#21644;&#34920;&#26684;&#22411;MDP&#20316;&#20026;&#31034;&#20363;&#65292;&#24314;&#31435;&#20102;&#36825;&#26679;&#30340;&#22122;&#22768;&#27700;&#24179;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;&#25311;&#21512;Q-&#36845;&#20195;&#21644;Bellman&#27531;&#24046;&#36827;&#34892;&#20102;&#26032;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the regret of reinforcement learning from offline data generated by a fixed behavior policy in an infinite-horizon discounted Markov decision process (MDP). While existing analyses of common approaches, such as fitted $Q$-iteration (FQI), suggest a $O(1/\sqrt{n})$ convergence for regret, empirical behavior exhibits \emph{much} faster convergence. In this paper, we present a finer regret analysis that exactly characterizes this phenomenon by providing fast rates for the regret convergence. First, we show that given any estimate for the optimal quality function $Q^*$, the regret of the policy it defines converges at a rate given by the exponentiation of the $Q^*$-estimate's pointwise convergence rate, thus speeding it up. The level of exponentiation depends on the level of noise in the \emph{decision-making} problem, rather than the estimation problem. We establish such noise levels for linear and tabular MDPs as examples. Second, we provide new analyses of FQI and Bellman resid
&lt;/p&gt;</description></item><item><title>B-HAR&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#29992;&#20110;&#28145;&#20837;&#30740;&#31350;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#25968;&#25454;&#38598;&#21644;&#24037;&#20316;&#27969;&#31243;&#65292;&#35299;&#20915;&#20102;HAR&#26041;&#27861;&#32570;&#20047;&#26631;&#20934;&#24037;&#20316;&#27969;&#31243;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#37197;&#32622;&#30340;&#26694;&#26550;&#26469;&#35780;&#20272;&#27169;&#24335;&#35782;&#21035;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2101.10870</link><description>&lt;p&gt;
B-HAR:&#19968;&#20010;&#29992;&#20110;&#28145;&#20837;&#30740;&#31350;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#25968;&#25454;&#38598;&#21644;&#24037;&#20316;&#27969;&#31243;&#30340;&#24320;&#28304;&#22522;&#20934;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
B-HAR: an open-source baseline framework for in depth study of human activity recognition datasets and workflows. (arXiv:2101.10870v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.10870
&lt;/p&gt;
&lt;p&gt;
B-HAR&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#29992;&#20110;&#28145;&#20837;&#30740;&#31350;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#25968;&#25454;&#38598;&#21644;&#24037;&#20316;&#27969;&#31243;&#65292;&#35299;&#20915;&#20102;HAR&#26041;&#27861;&#32570;&#20047;&#26631;&#20934;&#24037;&#20316;&#27969;&#31243;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#37197;&#32622;&#30340;&#26694;&#26550;&#26469;&#35780;&#20272;&#27169;&#24335;&#35782;&#21035;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#34987;&#35748;&#20026;&#26159;&#26368;&#20855;&#28508;&#21147;&#30340;&#25216;&#26415;&#20043;&#19968;&#65292;&#29992;&#20110;&#30417;&#27979;&#19981;&#21516;&#20154;&#32676;&#65288;&#22914;&#36816;&#21160;&#21592;&#12289;&#32769;&#24180;&#20154;&#12289;&#20799;&#31461;&#12289;&#38599;&#20027;&#65289;&#30340;&#19987;&#19994;&#21644;&#26085;&#24120;&#27963;&#21160;&#65292;&#20197;&#25552;&#20379;&#21508;&#31181;&#19982;&#31119;&#31049;&#12289;&#25216;&#26415;&#24615;&#33021;&#22686;&#24378;&#12289;&#39118;&#38505;&#39044;&#38450;&#21644;&#25945;&#32946;&#30446;&#30340;&#30456;&#20851;&#30340;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;HAR&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#20998;&#26512;&#21463;&#21040;&#32570;&#20047;&#26631;&#20934;&#24037;&#20316;&#27969;&#31243;&#30340;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#25104;&#20026;&#35780;&#20272;&#24320;&#21457;&#30340;&#27169;&#24335;&#35782;&#21035;&#27169;&#22411;&#36136;&#37327;&#30340;&#22522;&#20934;&#12290;&#36825;&#20351;&#24471;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#30340;&#27604;&#36739;&#25104;&#20026;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#33021;&#20250;&#29359;&#38169;&#35823;&#65292;&#24403;&#36825;&#20123;&#38169;&#35823;&#27809;&#26377;&#34987;&#26816;&#27979;&#21040;&#26102;&#65292;&#20250;&#23545;&#36798;&#21040;&#30340;&#32467;&#26524;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;B-HAR&#30340;&#24320;&#28304;&#33258;&#21160;&#21270;&#21644;&#39640;&#24230;&#21487;&#37197;&#32622;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23450;&#20041;&#21644;&#26631;&#20934;&#21270;HAR&#23454;&#39564;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human Activity Recognition (HAR), based on machine and deep learning algorithms is considered one of the most promising technologies to monitor professional and daily life activities for different categories of people (e.g., athletes, elderly, kids, employers) in order to provide a variety of services related, for example to well-being, empowering of technical performances, prevention of risky situation, and educational purposes. However, the analysis of the effectiveness and the efficiency of HAR methodologies suffers from the lack of a standard workflow, which might represent the baseline for the estimation of the quality of the developed pattern recognition models. This makes the comparison among different approaches a challenging task. In addition, researchers can make mistakes that, when not detected, definitely affect the achieved results. To mitigate such issues, this paper proposes an open-source automatic and highly configurable framework, named B-HAR, for the definition, stan
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#23454;&#26102;&#25511;&#21046;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#20013;&#23454;&#29616;&#23433;&#20840;&#21644;&#39640;&#25928;&#30340;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2010.02613</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23454;&#26102;&#25511;&#21046;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Deep Learning based Uncertainty Decomposition for Real-time Control. (arXiv:2010.02613v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.02613
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#23454;&#26102;&#25511;&#21046;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#20013;&#23454;&#29616;&#23433;&#20840;&#21644;&#39640;&#25928;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#38656;&#35201;&#23545;&#28041;&#21450;&#30340;&#19981;&#30830;&#23450;&#24615;&#26377;&#28165;&#26224;&#30340;&#29702;&#35299;&#65292;&#20197;&#30830;&#20445;&#23433;&#20840;&#21644;&#39640;&#25928;&#30340;&#25506;&#32034;&#12290;&#34429;&#28982;&#30001;&#20110;&#27979;&#37327;&#35823;&#24046;&#32780;&#20135;&#29983;&#30340;&#38543;&#26426;&#19981;&#30830;&#23450;&#24615;&#21487;&#20197;&#22312;&#32473;&#23450;&#21442;&#25968;&#21270;&#25551;&#36848;&#30340;&#24773;&#20917;&#19979;&#26126;&#30830;&#22320;&#36827;&#34892;&#24314;&#27169;&#65292;&#20294;&#23545;&#20110;&#25551;&#36848;&#35757;&#32451;&#25968;&#25454;&#30340;&#23384;&#22312;&#25110;&#19981;&#23384;&#22312;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#65292;&#24314;&#27169;&#21487;&#33021;&#26356;&#21152;&#22256;&#38590;&#12290;&#24403;&#31995;&#32479;&#21160;&#24577;&#26410;&#30693;&#26102;&#65292;&#21518;&#32773;&#22312;&#23454;&#26045;&#25506;&#32034;&#24615;&#25511;&#21046;&#31574;&#30053;&#26102;&#29305;&#21035;&#26377;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#26816;&#27979;&#35757;&#32451;&#25968;&#25454;&#30340;&#32570;&#22833;&#65292;&#20854;&#36755;&#20986;&#19968;&#20010;&#20171;&#20110;0&#65288;&#34920;&#31034;&#20302;&#19981;&#30830;&#23450;&#24615;&#65289;&#21644;1&#65288;&#34920;&#31034;&#39640;&#19981;&#30830;&#23450;&#24615;&#65289;&#20043;&#38388;&#30340;&#36830;&#32493;&#20540;&#26631;&#37327;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26816;&#27979;&#22120;&#20316;&#20026;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#20195;&#29702;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30456;&#23545;&#29616;&#26377;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#30452;&#25509;&#19982;&#38543;&#26426;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30456;&#32467;&#21512;&#65292;&#24182;&#20801;&#35768;&#23454;&#26102;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven control in unknown environments requires a clear understanding of the involved uncertainties for ensuring safety and efficient exploration. While aleatoric uncertainty that arises from measurement noise can often be explicitly modeled given a parametric description, it can be harder to model epistemic uncertainty, which describes the presence or absence of training data. The latter can be particularly useful for implementing exploratory control strategies when system dynamics are unknown. We propose a novel method for detecting the absence of training data using deep learning, which gives a continuous valued scalar output between $0$ (indicating low uncertainty) and $1$ (indicating high uncertainty). We utilize this detector as a proxy for epistemic uncertainty and show its advantages over existing approaches on synthetic and real-world datasets. Our approach can be directly combined with aleatoric uncertainty estimates and allows for uncertainty estimation in real-time as 
&lt;/p&gt;</description></item></channel></rss>