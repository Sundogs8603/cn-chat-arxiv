<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22686;&#24378;Transformer&#19982;&#24490;&#29615;&#26426;&#21046;&#30340;&#20004;&#31181;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25193;&#23637;&#21644;&#32452;&#21512;&#26041;&#27861;&#12290;&#22312;&#22810;&#20010;&#35786;&#26029;&#20219;&#21153;&#20013;&#36827;&#34892;&#27604;&#36739;&#65292;&#25506;&#32034;&#23427;&#20204;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00976</link><description>&lt;p&gt;
&#20855;&#26377;&#21160;&#24577;&#20572;&#27490;&#30340;&#24490;&#29615;Transformer
&lt;/p&gt;
&lt;p&gt;
Recurrent Transformers with Dynamic Halt
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22686;&#24378;Transformer&#19982;&#24490;&#29615;&#26426;&#21046;&#30340;&#20004;&#31181;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25193;&#23637;&#21644;&#32452;&#21512;&#26041;&#27861;&#12290;&#22312;&#22810;&#20010;&#35786;&#26029;&#20219;&#21153;&#20013;&#36827;&#34892;&#27604;&#36739;&#65292;&#25506;&#32034;&#23427;&#20204;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#20027;&#35201;&#26041;&#27861;&#22312;&#22686;&#24378;Transformer&#19982;&#24490;&#29615;&#26426;&#21046;&#26041;&#38754;&#30340;&#24402;&#32435;&#20559;&#22909;&#8212;&#8212;&#65288;1&#65289;&#31867;&#20284;&#20110;Universal Transformers&#30340;&#28145;&#24230;&#36880;&#23618;&#24490;&#29615;&#26041;&#27861;&#65307;&#21644;&#65288;2&#65289;&#31867;&#20284;&#20110;Temporal Latent Bottleneck&#30340;&#20998;&#22359;&#26102;&#24577;&#24490;&#29615;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#25193;&#23637;&#21644;&#32452;&#21512;&#19978;&#36848;&#26041;&#27861;&#30340;&#26032;&#26041;&#24335;&#65292;&#20363;&#22914;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#23616;&#22343;&#20540;&#30340;Universal Transformer&#21160;&#24577;&#20572;&#27490;&#26426;&#21046;&#65292;&#24182;&#23558;Universal Transformer&#30340;&#20803;&#32032;&#34701;&#20837;&#21040;Temporal Latent Bottleneck&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#20010;&#35786;&#26029;&#20219;&#21153;&#65288;&#22914;Long Range Arena&#65288;LRA&#65289;&#65292;&#32763;&#36716;-&#32763;&#36716;&#35821;&#35328;&#24314;&#27169;&#65292;ListOps&#21644;&#36923;&#36753;&#25512;&#29702;&#65289;&#27604;&#36739;&#20102;&#27169;&#22411;&#24182;&#25506;&#32034;&#20102;&#23427;&#20204;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the inductive biases of two major approaches to augmenting Transformers with a recurrent mechanism - (1) the approach of incorporating a depth-wise recurrence similar to Universal Transformers; and (2) the approach of incorporating a chunk-wise temporal recurrence like Temporal Latent Bottleneck. Furthermore, we propose and investigate novel ways to extend and combine the above methods - for example, we propose a global mean-based dynamic halting mechanism for Universal Transformer and an augmentation of Temporal Latent Bottleneck with elements from Universal Transformer. We compare the models and probe their inductive biases in several diagnostic tasks such as Long Range Arena (LRA), flip-flop language modeling, ListOps, and Logical Inference.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;Masked AutoEncoders&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;NeRF-MAE&#29992;&#20110;&#33258;&#30417;&#30563;&#19977;&#32500;&#34920;&#31034;&#23398;&#20064;&#65292;&#21033;&#29992;&#26631;&#20934;&#30340;&#19977;&#32500;Vision Transformers&#36866;&#24212;NeRF&#30340;&#29420;&#29305;&#20844;&#24335;&#65292;&#23558;NeRF&#30340;&#20307;&#31215;&#32593;&#26684;&#20316;&#20026;&#23494;&#38598;&#36755;&#20837;&#65292;&#20197;&#20135;&#29983;&#26377;&#25928;&#30340;&#19977;&#32500;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2404.01300</link><description>&lt;p&gt;
NeRF-MAE: &#33258;&#30417;&#30563;&#19977;&#32500;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;Masked AutoEncoders
&lt;/p&gt;
&lt;p&gt;
NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation Learning for Neural Radiance Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01300
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;Masked AutoEncoders&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;NeRF-MAE&#29992;&#20110;&#33258;&#30417;&#30563;&#19977;&#32500;&#34920;&#31034;&#23398;&#20064;&#65292;&#21033;&#29992;&#26631;&#20934;&#30340;&#19977;&#32500;Vision Transformers&#36866;&#24212;NeRF&#30340;&#29420;&#29305;&#20844;&#24335;&#65292;&#23558;NeRF&#30340;&#20307;&#31215;&#32593;&#26684;&#20316;&#20026;&#23494;&#38598;&#36755;&#20837;&#65292;&#20197;&#20135;&#29983;&#26377;&#25928;&#30340;&#19977;&#32500;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#31070;&#32463;&#22330;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#21331;&#36234;&#33021;&#21147;&#65292;&#33021;&#22815;&#29702;&#35299;&#19977;&#32500;&#35270;&#35273;&#19990;&#30028;&#65292;&#22914;&#25512;&#26029;&#35821;&#20041;&#12289;&#20960;&#20309;&#21644;&#21160;&#24577;&#31561;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#31070;&#32463;&#22330;&#22312;&#20174;&#20108;&#32500;&#22270;&#20687;&#20013;&#23494;&#38598;&#34920;&#31034;&#19977;&#32500;&#22330;&#26223;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#20855;&#20307;&#20351;&#29992;Masked AutoEncoders&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#20511;&#37492;&#20102;&#23558;transformers&#25193;&#23637;&#21040;&#26032;&#25968;&#25454;&#27169;&#24577;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#25104;&#21151;&#65292;&#21033;&#29992;&#26631;&#20934;&#30340;&#19977;&#32500;Vision Transformers&#26469;&#36866;&#24212;NeRF&#30340;&#29420;&#29305;&#20844;&#24335;&#12290;&#25105;&#20204;&#23558;NeRF&#30340;&#20307;&#31215;&#32593;&#26684;&#20316;&#20026;transformer&#30340;&#23494;&#38598;&#36755;&#20837;&#65292;&#19982;&#20854;&#20182;&#19977;&#32500;&#34920;&#31034;&#65288;&#22914;&#28857;&#20113;&#65289;&#36827;&#34892;&#23545;&#27604;&#65292;&#20854;&#20449;&#24687;&#23494;&#24230;&#21487;&#33021;&#19981;&#22343;&#21248;&#65292;&#32780;&#34920;&#31034;&#26159;&#19981;&#35268;&#21017;&#30340;&#12290;&#30001;&#20110;&#23558;masked autoencoders&#24212;&#29992;&#20110;&#31867;&#20284;NeRF&#36825;&#26679;&#30340;&#38544;&#24335;&#34920;&#31034;&#30340;&#22256;&#38590;&#65292;&#25105;&#20204;&#36873;&#25321;&#25552;&#21462;&#19968;&#20010;&#26174;&#24335;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01300v1 Announce Type: cross  Abstract: Neural fields excel in computer vision and robotics due to their ability to understand the 3D visual world such as inferring semantics, geometry, and dynamics. Given the capabilities of neural fields in densely representing a 3D scene from 2D images, we ask the question: Can we scale their self-supervised pretraining, specifically using masked autoencoders, to generate effective 3D representations from posed RGB images. Owing to the astounding success of extending transformers to novel data modalities, we employ standard 3D Vision Transformers to suit the unique formulation of NeRFs. We leverage NeRF's volumetric grid as a dense input to the transformer, contrasting it with other 3D representations such as pointclouds where the information density can be uneven, and the representation is irregular. Due to the difficulty of applying masked autoencoders to an implicit representation, such as NeRF, we opt for extracting an explicit repres
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#21345;&#36890;&#22270;&#20687;&#26500;&#24314;&#30340;CausalChaos!&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26356;&#38271;&#22240;&#26524;&#38142;&#30340;&#22240;&#26524;&#38382;&#31572;&#65292;&#36890;&#36807;&#21160;&#24577;&#20114;&#21160;&#21644;&#35270;&#35273;&#23637;&#31034;&#25361;&#25112;&#24615;&#22240;&#26524;&#20851;&#31995;&#65292;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#26356;&#22810;&#20855;&#25361;&#25112;&#24615;&#19988;&#26126;&#30830;&#23450;&#20041;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2404.01299</link><description>&lt;p&gt;
CausalChaos!&#25968;&#25454;&#38598;&#65306;&#22522;&#20110;&#21160;&#24577;&#35270;&#35273;&#22330;&#26223;&#20013;&#26356;&#38271;&#22240;&#26524;&#38142;&#30340;&#20840;&#38754;&#22240;&#26524;&#34892;&#21160;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
CausalChaos! Dataset for Comprehensive Causal Action Question Answering Over Longer Causal Chains Grounded in Dynamic Visual Scenes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01299
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#21345;&#36890;&#22270;&#20687;&#26500;&#24314;&#30340;CausalChaos!&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26356;&#38271;&#22240;&#26524;&#38142;&#30340;&#22240;&#26524;&#38382;&#31572;&#65292;&#36890;&#36807;&#21160;&#24577;&#20114;&#21160;&#21644;&#35270;&#35273;&#23637;&#31034;&#25361;&#25112;&#24615;&#22240;&#26524;&#20851;&#31995;&#65292;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#26356;&#22810;&#20855;&#25361;&#25112;&#24615;&#19988;&#26126;&#30830;&#23450;&#20041;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#35270;&#39057;&#38382;&#31572;&#65288;QA&#65289;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#28982;&#32780;&#29616;&#26377;&#25968;&#25454;&#38598;&#22312;&#22240;&#26524;&#25512;&#29702;&#20998;&#26512;&#26041;&#38754;&#24448;&#24448;&#32570;&#20047;&#28145;&#24230;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#21033;&#29992;&#21345;&#36890;&#30340;&#29420;&#29305;&#23646;&#24615;&#26500;&#24314;&#20102;CausalChaos!&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22240;&#26524;&#38382;&#31572;&#65288;Why-QA&#65289;&#25968;&#25454;&#38598;&#65292;&#22522;&#20110;&#26631;&#24535;&#24615;&#30340;&#8220;&#29483;&#21644;&#32769;&#40736;&#8221;&#21345;&#36890;&#31995;&#21015;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#36890;&#36807;&#21608;&#21040;&#30340;&#38382;&#39064;&#21644;&#22810;&#23618;&#27425;&#31572;&#26696;&#65292;&#21253;&#21547;&#30528;&#23884;&#20837;&#21160;&#24577;&#20114;&#21160;&#21644;&#35270;&#35273;&#20013;&#30340;&#26356;&#38271;&#22240;&#26524;&#38142;&#65292;&#21516;&#26102;&#21160;&#30011;&#21407;&#29702;&#20801;&#35768;&#21160;&#30011;&#24072;&#21019;&#36896;&#23450;&#20041;&#26126;&#30830;&#12289;&#26126;&#20102;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#36825;&#20123;&#22240;&#32032;&#20351;&#27169;&#22411;&#33021;&#22815;&#35299;&#20915;&#26356;&#20855;&#25361;&#25112;&#24615;&#20294;&#26126;&#30830;&#23450;&#20041;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#30828;&#36127;&#37319;&#26679;&#65292;&#21253;&#25324;CausalConfusion&#29256;&#26412;&#12290;&#34429;&#28982;&#27169;&#22411;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20173;&#26377;&#24456;&#22823;&#25913;&#36827;&#31354;&#38388;&#65292;&#29305;&#21035;&#26159;&#22312;&#24320;&#25918;&#24335;&#31572;&#26696;&#26041;&#38754;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#26356;&#20026;&#20808;&#36827;/&#26126;&#30830;&#30340;&#22240;&#26524;&#20851;&#31995;&#24314;&#27169;&#21644;&#32852;&#21512;&#24314;&#27169;&#31561;&#25913;&#36827;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01299v1 Announce Type: cross  Abstract: Causal video question answering (QA) has garnered increasing interest, yet existing datasets often lack depth in causal reasoning analysis. To address this gap, we capitalize on the unique properties of cartoons and construct CausalChaos!, a novel, challenging causal Why-QA dataset built upon the iconic "Tom and Jerry" cartoon series. With thoughtful questions and multi-level answers, our dataset contains much longer causal chains embedded in dynamic interactions and visuals, at the same time principles of animation allows animators to create well-defined, unambiguous causal relationships. These factors allow models to solve more challenging, yet well-defined causal relationships. We also introduce hard negative mining, including CausalConfusion version. While models perform well, there is much room for improvement, especially, on open-ended answers. We identify more advanced/explicit causal relationship modeling and joint modeling of 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25511;&#21046;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23646;&#24615;&#65292;&#23454;&#29616;&#23433;&#20840;&#21644;&#24110;&#21161;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#20013;&#23454;&#29616;&#36825;&#31181;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2404.01295</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#25511;&#22823;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#23433;&#20840;&#21644;&#24110;&#21161;&#24179;&#34913;&#30340;&#22238;&#24212;
&lt;/p&gt;
&lt;p&gt;
Towards Safety and Helpfulness Balanced Responses via Controllable Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01295
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25511;&#21046;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23646;&#24615;&#65292;&#23454;&#29616;&#23433;&#20840;&#21644;&#24110;&#21161;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#20013;&#23454;&#29616;&#36825;&#31181;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22914;&#20170;&#21464;&#24471;&#23481;&#26131;&#33719;&#24471;&#65292;&#23433;&#20840;&#21644;&#24110;&#21161;&#20043;&#38388;&#30340;&#26435;&#34913;&#20250;&#26174;&#33879;&#24433;&#21709;&#29992;&#25143;&#20307;&#39564;&#12290;&#19968;&#20010;&#20248;&#20808;&#32771;&#34385;&#23433;&#20840;&#30340;&#27169;&#22411;&#20250;&#23548;&#33268;&#29992;&#25143;&#24863;&#21040;&#36739;&#23569;&#21442;&#19982;&#21644;&#34987;&#21327;&#21161;&#65292;&#32780;&#20248;&#20808;&#32771;&#34385;&#24110;&#21161;&#24615;&#21017;&#21487;&#33021;&#23548;&#33268;&#20260;&#23475;&#12290;&#21487;&#33021;&#30340;&#21361;&#23475;&#21253;&#25324;&#25945;&#25480;&#20154;&#20204;&#22914;&#20309;&#21046;&#36896;&#28856;&#24377;&#65292;&#21521;&#38738;&#23569;&#24180;&#26292;&#38706;&#19981;&#24403;&#20869;&#23481;&#20197;&#21450;&#20260;&#23475;&#29992;&#25143;&#30340;&#24515;&#29702;&#20581;&#24247;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#25511;&#21046;LLMs&#20013;&#30340;&#20004;&#20010;&#23646;&#24615;&#26469;&#22312;&#19981;&#21516;&#30340;&#29992;&#20363;&#20013;&#24179;&#34913;&#23433;&#20840;&#24615;&#21644;&#24110;&#21161;&#24615;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#19981;&#38656;&#35201;&#39069;&#22806;&#20154;&#21592;&#27880;&#37322;&#30340;&#35757;&#32451;&#26080;&#20851;&#21644;&#24494;&#35843;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#25511;&#21046;LLMs&#20013;&#23433;&#20840;&#24615;&#21644;&#24110;&#21161;&#24615;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#37325;&#32622;&#24050;&#23398;&#20064;&#30340;&#27169;&#22411;&#24182;&#35299;&#38145;&#20854;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01295v1 Announce Type: cross  Abstract: As large language models (LLMs) become easily accessible nowadays, the trade-off between safety and helpfulness can significantly impact user experience. A model that prioritizes safety will cause users to feel less engaged and assisted while prioritizing helpfulness will potentially cause harm. Possible harms include teaching people how to build a bomb, exposing youth to inappropriate content, and hurting users' mental health. In this work, we propose to balance safety and helpfulness in diverse use cases by controlling both attributes in LLM. We explore training-free and fine-tuning methods that do not require extra human annotations and analyze the challenges of controlling safety and helpfulness in LLMs. Our experiments demonstrate that our method can rewind a learned model and unlock its controllability.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;VQAScore&#65292;&#21033;&#29992;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#29983;&#25104;&#23545;&#40784;&#20998;&#25968;&#65292;&#21462;&#24471;&#20102;&#22312;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#22522;&#20934;&#19978;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;</title><link>https://arxiv.org/abs/2404.01291</link><description>&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#19982;&#22270;&#20687;&#21040;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Evaluating Text-to-Visual Generation with Image-to-Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01291
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;VQAScore&#65292;&#21033;&#29992;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#29983;&#25104;&#23545;&#40784;&#20998;&#25968;&#65292;&#21462;&#24471;&#20102;&#22312;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#22522;&#20934;&#19978;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#26377;&#25928;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#26631;&#20934;&#21270;&#22522;&#20934;&#65292;&#32508;&#21512;&#35780;&#20272;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;VQAScore&#65292;&#20351;&#29992;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#27169;&#22411;&#36890;&#36807;&#35745;&#31639;&#23545;&#31616;&#21333;&#30340;&#8220;&#36825;&#24133;&#22270;&#34920;&#29616;&#20986;&#20102;'{&#25991;&#26412;}'&#21527;&#65311;&#8221;&#38382;&#39064;&#30340;&#8220;&#26159;&#8221;&#31572;&#26696;&#30340;&#27010;&#29575;&#26469;&#29983;&#25104;&#23545;&#40784;&#20998;&#25968;&#12290;&#23613;&#31649;&#27604;&#20808;&#21069;&#30340;&#26041;&#27861;&#26356;&#31616;&#21333;&#65292;&#20294;&#20351;&#29992;&#29616;&#25104;&#27169;&#22411;&#35745;&#31639;&#30340;VQAScore&#22312;&#35768;&#22810;&#65288;8&#20010;&#65289;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#22522;&#20934;&#19978;&#20135;&#29983;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01291v1 Announce Type: cross  Abstract: Despite significant progress in generative AI, comprehensive evaluation remains challenging because of the lack of effective metrics and standardized benchmarks. For instance, the widely-used CLIPScore measures the alignment between a (generated) image and text prompt, but it fails to produce reliable scores for complex prompts involving compositions of objects, attributes, and relations. One reason is that text encoders of CLIP can notoriously act as a "bag of words", conflating prompts such as "the horse is eating the grass" with "the grass is eating the horse". To address this, we introduce the VQAScore, which uses a visual-question-answering (VQA) model to produce an alignment score by computing the probability of a "Yes" answer to a simple "Does this figure show '{text}'?" question. Though simpler than prior art, VQAScore computed with off-the-shelf models produces state-of-the-art results across many (8) image-text alignment benc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#31185;&#23398;&#35770;&#25991;&#20013;LLM&#20351;&#29992;&#22686;&#21152;&#30340;&#26144;&#23556;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#20998;&#26512;&#65292;&#22635;&#34917;&#20102;&#23398;&#26415;&#20889;&#20316;&#20013;&#30495;&#23454;LLM&#20462;&#25913;&#20869;&#23481;&#27604;&#20363;&#32570;&#22833;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2404.01268</link><description>&lt;p&gt;
&#31185;&#23398;&#35770;&#25991;&#20013;LLM&#20351;&#29992;&#22686;&#21152;&#30340;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Mapping the Increasing Use of LLMs in Scientific Papers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01268
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#31185;&#23398;&#35770;&#25991;&#20013;LLM&#20351;&#29992;&#22686;&#21152;&#30340;&#26144;&#23556;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#20998;&#26512;&#65292;&#22635;&#34917;&#20102;&#23398;&#26415;&#20889;&#20316;&#20013;&#30495;&#23454;LLM&#20462;&#25913;&#20869;&#23481;&#27604;&#20363;&#32570;&#22833;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#20986;&#29256;&#36890;&#36807;&#20256;&#25773;&#30740;&#31350;&#25104;&#26524;&#12289;&#20419;&#36827;&#21512;&#20316;&#12289;&#40723;&#21169;&#21487;&#37325;&#22797;&#24615;&#65292;&#24182;&#30830;&#20445;&#31185;&#23398;&#30693;&#35782;&#38543;&#26102;&#38388;&#21487;&#35775;&#38382;&#12289;&#21487;&#39564;&#35777;&#24182;&#19981;&#26029;&#24314;&#31435;&#65292;&#20026;&#31185;&#23398;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#22810;&#23569;&#20154;&#22312;&#23398;&#26415;&#20889;&#20316;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#21450;&#36825;&#31181;&#24037;&#20855;&#23545;&#20840;&#29699;&#31185;&#23398;&#23454;&#36341;&#21487;&#33021;&#20135;&#29983;&#20309;&#31181;&#24433;&#21709;&#36827;&#34892;&#20102;&#22823;&#37327;&#29468;&#27979;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#32570;&#20047;&#23545;&#23398;&#26415;&#20889;&#20316;&#20013;&#23454;&#36136;&#24615;&#20462;&#25913;&#25110;&#29983;&#25104;&#20869;&#23481;&#30340;&#20934;&#30830;&#24230;&#37327;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#23545;&#22312;arXiv&#12289;bioRxiv&#21644;&#33258;&#28982;&#23398;&#25253;&#31995;&#21015;&#26399;&#21002;&#19978;&#30340;950,965&#31687;&#35770;&#25991;&#65288;&#26102;&#38388;&#36328;&#24230;&#20174;2020&#24180;1&#26376;&#33267;2024&#24180;2&#26376;&#65289;&#36827;&#34892;&#20102;&#39318;&#27425;&#31995;&#32479;&#24615;&#12289;&#22823;&#35268;&#27169;&#30340;&#20998;&#26512;&#65292;&#21033;&#29992;&#20154;&#32676;&#32423;&#21035;&#30340;&#32479;&#35745;&#26694;&#26550;&#26469;&#27979;&#37327;LLM&#20462;&#25913;&#20869;&#23481;&#38543;&#26102;&#38388;&#30340;&#30427;&#34892;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#32479;&#35745;&#20272;&#35745;&#26159;&#22312;&#35821;&#26009;&#24211;&#32423;&#21035;&#21450;i
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01268v1 Announce Type: cross  Abstract: Scientific publishing lays the foundation of science by disseminating research findings, fostering collaboration, encouraging reproducibility, and ensuring that scientific knowledge is accessible, verifiable, and built upon over time. Recently, there has been immense speculation about how many people are using large language models (LLMs) like ChatGPT in their academic writing, and to what extent this tool might have an effect on global scientific practices. However, we lack a precise measure of the proportion of academic writing substantially modified or produced by LLMs. To address this gap, we conduct the first systematic, large-scale analysis across 950,965 papers published between January 2020 and February 2024 on the arXiv, bioRxiv, and Nature portfolio journals, using a population-level statistical framework to measure the prevalence of LLM-modified content over time. Our statistical estimation operates on the corpus level and i
&lt;/p&gt;</description></item><item><title>IsoBench&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#19981;&#21516;&#21516;&#26500;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#26356;&#20559;&#22909;&#25991;&#26412;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2404.01266</link><description>&lt;p&gt;
IsoBench&#65306;&#22522;&#20110;&#21516;&#26500;&#34920;&#31034;&#23545;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01266
&lt;/p&gt;
&lt;p&gt;
IsoBench&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#19981;&#21516;&#21516;&#26500;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#26356;&#20559;&#22909;&#25991;&#26412;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22522;&#30784;&#27169;&#22411;&#22312;&#20165;&#25991;&#26412;&#25110;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#21516;&#26102;&#25552;&#31034;&#26102;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#20294;&#23427;&#20204;&#30340;&#33021;&#21147;&#26159;&#21542;&#20250;&#26681;&#25454;&#36755;&#20837;&#26041;&#24335;&#32780;&#25913;&#21464;&#21602;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;$\textbf{IsoBench}$&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;&#22235;&#20010;&#20027;&#35201;&#39046;&#22495;&#30340;&#38382;&#39064;: &#25968;&#23398;&#12289;&#31185;&#23398;&#12289;&#31639;&#27861;&#21644;&#28216;&#25103;&#12290;&#27599;&#20010;&#31034;&#20363;&#21576;&#29616;&#20102;&#22810;&#20010;&#36755;&#20837;&#30340;&#21516;&#26500;&#34920;&#31034;&#65292;&#22914;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#25968;&#23398;&#23637;&#31034;&#12290;IsoBench&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#30340;&#21453;&#39304;&#65292;&#20197;&#35786;&#26029;&#30001;&#34920;&#31034;&#24418;&#24335;&#36896;&#25104;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#22312;&#21508;&#31181;&#22522;&#30784;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#30456;&#21516;&#38382;&#39064;&#19978;&#65292;&#27169;&#22411;&#19968;&#36143;&#20559;&#22909;&#25991;&#26412;&#34920;&#31034;&#12290;&#26368;&#31361;&#20986;&#30340;&#26159;&#65292;&#22312;&#25152;&#26377;IsoBench&#38382;&#39064;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;Claude-3 Opus&#22312;&#25552;&#20379;&#22270;&#20687;&#32780;&#19981;&#26159;&#25991;&#26412;&#26102;&#24615;&#33021;&#19979;&#38477;28.7&#20998;&#65307;&#21516;&#26679;&#65292;GPT-4 Turbo&#24615;&#33021;&#19979;&#38477;18.7&#20998;&#65292;Gemini Pro&#19979;&#38477;14.9&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01266v1 Announce Type: new  Abstract: Current foundation models exhibit impressive capabilities when prompted either with text only or with both image and text inputs. But do their capabilities change depending on the input modality? In this work, we propose $\textbf{IsoBench}$, a benchmark dataset containing problems from four major areas: math, science, algorithms, and games. Each example is presented with multiple $\textbf{isomorphic representations}$ of inputs, such as visual, textual, and mathematical presentations. IsoBench provides fine-grained feedback to diagnose performance gaps caused by the form of the representation. Across various foundation models, we observe that on the same problem, models have a consistent preference towards textual representations. Most prominently, when evaluated on all IsoBench problems, Claude-3 Opus performs 28.7 points worse when provided with images instead of text; similarly, GPT-4 Turbo is 18.7 points worse and Gemini Pro is 14.9 p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#23545;LLM&#29983;&#25104;&#30340;&#34394;&#26500;&#20070;&#31821;&#25688;&#35201;&#36827;&#34892;&#20102;&#24544;&#23454;&#24615;&#21644;&#20869;&#23481;&#36873;&#25321;&#30340;&#22823;&#35268;&#27169;&#20154;&#31867;&#35780;&#20272;&#65292;&#24314;&#31435;&#20102;FABLES&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23545;26&#26412;&#20070;&#30340;3158&#20010;&#22768;&#26126;&#36827;&#34892;&#20102;&#27880;&#37322;&#65292;&#25104;&#21151;&#23545;LLM&#25688;&#35201;&#36827;&#34892;&#20102;&#22522;&#20110;&#24544;&#23454;&#24615;&#30340;&#25490;&#21517;</title><link>https://arxiv.org/abs/2404.01261</link><description>&lt;p&gt;
FABLES&#65306;&#35780;&#20272;&#20070;&#31821;&#25688;&#35201;&#20013;&#30340;&#24544;&#23454;&#24615;&#21644;&#20869;&#23481;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
FABLES: Evaluating faithfulness and content selection in book-length summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23545;LLM&#29983;&#25104;&#30340;&#34394;&#26500;&#20070;&#31821;&#25688;&#35201;&#36827;&#34892;&#20102;&#24544;&#23454;&#24615;&#21644;&#20869;&#23481;&#36873;&#25321;&#30340;&#22823;&#35268;&#27169;&#20154;&#31867;&#35780;&#20272;&#65292;&#24314;&#31435;&#20102;FABLES&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23545;26&#26412;&#20070;&#30340;3158&#20010;&#22768;&#26126;&#36827;&#34892;&#20102;&#27880;&#37322;&#65292;&#25104;&#21151;&#23545;LLM&#25688;&#35201;&#36827;&#34892;&#20102;&#22522;&#20110;&#24544;&#23454;&#24615;&#30340;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#38271;&#25991;&#26412;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25216;&#26415;&#19978;&#21487;&#20197;&#24635;&#32467;&#38271;&#36798;100K&#20010;&#26631;&#35760;&#30340;&#20070;&#31821;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#65292;&#25991;&#26723;&#30340;&#38271;&#24230;&#21644;&#22797;&#26434;&#24615;&#38459;&#30861;&#20102;&#23545;&#24544;&#23454;&#24615;&#31561;&#36755;&#20837;&#30456;&#20851;&#26041;&#38754;&#30340;&#35780;&#20272;&#12290;&#26412;&#25991;&#22312;&#34394;&#26500;&#20070;&#31821;&#30340;LLM&#29983;&#25104;&#25688;&#35201;&#19978;&#36827;&#34892;&#20102;&#39318;&#27425;&#22823;&#35268;&#27169;&#20154;&#31867;&#35780;&#20272;&#65292;&#36890;&#36807;&#19987;&#27880;&#20110;2023&#25110;2024&#24180;&#20986;&#29256;&#30340;&#20070;&#31821;&#25688;&#35201;&#65292;&#38599;&#20323;&#22312;&#36827;&#34892;&#27880;&#37322;&#20219;&#21153;&#20043;&#21069;&#24050;&#23436;&#20840;&#38405;&#35835;&#27599;&#26412;&#20070;&#30340;&#27880;&#37322;&#32773;&#26469;&#20943;&#23569;&#25104;&#26412;&#21644;&#35748;&#30693;&#36127;&#25285;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;FABLES&#25968;&#25454;&#38598;&#65292;&#23545;26&#26412;&#20070;&#30340;LLM&#29983;&#25104;&#25688;&#35201;&#20013;&#30340;3158&#20010;&#22768;&#26126;&#36827;&#34892;&#20102;&#27880;&#37322;&#65292;&#33457;&#36153;&#20102;5200&#32654;&#20803;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22522;&#20110;&#24544;&#23454;&#24615;&#23545;LLM&#25688;&#35201;&#36827;&#34892;&#25490;&#21517;&#65306;Claude-3-Opus&#22312;&#24544;&#23454;&#24615;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#25152;&#26377;&#38381;&#28304;LLMs&#65292;&#32780;&#24320;&#28304;&#30340;Mixtral&#19982;GPT-3.5-Turbo&#25345;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01261v1 Announce Type: cross  Abstract: While long-context large language models (LLMs) can technically summarize book-length documents (&gt;100K tokens), the length and complexity of the documents have so far prohibited evaluations of input-dependent aspects like faithfulness. In this paper, we conduct the first large-scale human evaluation of faithfulness and content selection on LLM-generated summaries of fictional books. Our study mitigates the issue of data contamination by focusing on summaries of books published in 2023 or 2024, and we hire annotators who have fully read each book prior to the annotation task to minimize cost and cognitive burden. We collect FABLES, a dataset of annotations on 3,158 claims made in LLM-generated summaries of 26 books, at a cost of $5.2K USD, which allows us to rank LLM summarizers based on faithfulness: Claude-3-Opus significantly outperforms all closed-source LLMs, while the open-source Mixtral is on par with GPT-3.5-Turbo. An analysis o
&lt;/p&gt;</description></item><item><title>msGFM&#26159;&#19968;&#20010;&#22810;&#20256;&#24863;&#22120;&#22320;&#29702;&#31354;&#38388;&#22522;&#30784;&#27169;&#22411;&#65292;&#33021;&#22815;&#26377;&#25928;&#32479;&#19968;&#22235;&#31181;&#20851;&#38190;&#20256;&#24863;&#22120;&#27169;&#24577;&#30340;&#25968;&#25454;&#65292;&#20855;&#26377;&#22788;&#29702;&#25104;&#23545;&#21644;&#38750;&#25104;&#23545;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#36328;&#20256;&#24863;&#22120;&#39044;&#35757;&#32451;&#26041;&#27861;&#22312;&#33945;&#29256;&#22270;&#20687;&#24314;&#27169;&#20013;&#21512;&#25104;&#32852;&#21512;&#34920;&#31034;&#65292;&#23637;&#29616;&#20986;&#22312;&#21508;&#31181;&#20256;&#24863;&#22120;&#31867;&#22411;&#19979;&#37117;&#20855;&#26377;&#24378;&#22823;&#24615;&#33021;&#30340;&#32508;&#21512;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2404.01260</link><description>&lt;p&gt;
&#23558;&#36828;&#31243;&#20256;&#24863;&#22120;&#19982;&#22810;&#20256;&#24863;&#22120;&#22320;&#29702;&#31354;&#38388;&#22522;&#30784;&#27169;&#22411;&#32852;&#31995;&#36215;&#26469;
&lt;/p&gt;
&lt;p&gt;
Bridging Remote Sensors with Multisensor Geospatial Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01260
&lt;/p&gt;
&lt;p&gt;
msGFM&#26159;&#19968;&#20010;&#22810;&#20256;&#24863;&#22120;&#22320;&#29702;&#31354;&#38388;&#22522;&#30784;&#27169;&#22411;&#65292;&#33021;&#22815;&#26377;&#25928;&#32479;&#19968;&#22235;&#31181;&#20851;&#38190;&#20256;&#24863;&#22120;&#27169;&#24577;&#30340;&#25968;&#25454;&#65292;&#20855;&#26377;&#22788;&#29702;&#25104;&#23545;&#21644;&#38750;&#25104;&#23545;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#36328;&#20256;&#24863;&#22120;&#39044;&#35757;&#32451;&#26041;&#27861;&#22312;&#33945;&#29256;&#22270;&#20687;&#24314;&#27169;&#20013;&#21512;&#25104;&#32852;&#21512;&#34920;&#31034;&#65292;&#23637;&#29616;&#20986;&#22312;&#21508;&#31181;&#20256;&#24863;&#22120;&#31867;&#22411;&#19979;&#37117;&#20855;&#26377;&#24378;&#22823;&#24615;&#33021;&#30340;&#32508;&#21512;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22320;&#29702;&#31354;&#38388;&#20998;&#26512;&#39046;&#22495;&#65292;&#21253;&#25324;&#20809;&#23398;&#21644;&#24494;&#27874;&#25216;&#26415;&#22312;&#20869;&#30340;&#36828;&#31243;&#20256;&#24863;&#22120;&#30340;&#22810;&#26679;&#24615;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#29420;&#29305;&#35266;&#27979;&#33021;&#21147;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;msGFM&#65292;&#19968;&#20010;&#22810;&#20256;&#24863;&#22120;&#22320;&#29702;&#31354;&#38388;&#22522;&#30784;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#32479;&#19968;&#20102;&#26469;&#33258;&#22235;&#31181;&#20851;&#38190;&#20256;&#24863;&#22120;&#27169;&#24577;&#30340;&#25968;&#25454;&#12290;&#36825;&#31181;&#38598;&#25104;&#28085;&#30422;&#20102;&#20004;&#30334;&#19975;&#22810;&#20256;&#24863;&#22120;&#22270;&#20687;&#30340;&#24222;&#22823;&#25968;&#25454;&#38598;&#12290;msGFM&#22312;&#22788;&#29702;&#25104;&#23545;&#21644;&#38750;&#25104;&#23545;&#20256;&#24863;&#22120;&#25968;&#25454;&#26041;&#38754;&#29420;&#20855;&#25165;&#33021;&#12290;&#23545;&#20110;&#26469;&#33258;&#30456;&#21516;&#22320;&#29702;&#20301;&#32622;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#37319;&#29992;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#36328;&#20256;&#24863;&#22120;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#33945;&#29256;&#22270;&#20687;&#24314;&#27169;&#20013;&#23454;&#29616;&#32852;&#21512;&#34920;&#31034;&#30340;&#21512;&#25104;&#12290;msGFM&#32467;&#21512;&#20102;&#22235;&#31181;&#36828;&#31243;&#20256;&#24863;&#22120;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#36866;&#24212;&#21508;&#31181;&#20256;&#24863;&#22120;&#31867;&#22411;&#30340;&#32508;&#21512;&#27169;&#22411;&#12290;msGFM&#22312;&#19968;&#31995;&#21015;&#21333;&#20256;&#24863;&#22120;&#21644;&#22810;&#20256;&#24863;&#22120;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01260v1 Announce Type: cross  Abstract: In the realm of geospatial analysis, the diversity of remote sensors, encompassing both optical and microwave technologies, offers a wealth of distinct observational capabilities. Recognizing this, we present msGFM, a multisensor geospatial foundation model that effectively unifies data from four key sensor modalities. This integration spans an expansive dataset of two million multisensor images. msGFM is uniquely adept at handling both paired and unpaired sensor data. For data originating from identical geolocations, our model employs an innovative cross-sensor pretraining approach in masked image modeling, enabling the synthesis of joint representations from diverse sensors. msGFM, incorporating four remote sensors, upholds strong performance, forming a comprehensive model adaptable to various sensor types. msGFM has demonstrated enhanced proficiency in a range of both single-sensor and multisensor downstream tasks. These include sce
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#35814;&#32454;&#30340;&#35270;&#39057;&#26631;&#39064;&#20316;&#20026;&#35270;&#39057;&#20869;&#23481;&#30340;&#20195;&#29702;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#22312;&#35780;&#20998;&#35270;&#39057;&#38382;&#31572;&#65288;QA&#65289;&#39044;&#27979;&#26102;&#33021;&#22815;&#34701;&#20837;&#36825;&#20123;&#20449;&#24687;&#20316;&#20026;&#25903;&#25345;&#35777;&#25454;&#12290;</title><link>https://arxiv.org/abs/2404.01258</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22870;&#21169;&#19979;&#30340;&#35270;&#39057;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#35814;&#32454;&#30340;&#35270;&#39057;&#26631;&#39064;&#20316;&#20026;&#35270;&#39057;&#20869;&#23481;&#30340;&#20195;&#29702;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#22312;&#35780;&#20998;&#35270;&#39057;&#38382;&#31572;&#65288;QA&#65289;&#39044;&#27979;&#26102;&#33021;&#22815;&#34701;&#20837;&#36825;&#20123;&#20449;&#24687;&#20316;&#20026;&#25903;&#25345;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#22909;&#24314;&#27169;&#25216;&#26415;&#65292;&#22914;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#65292;&#24050;&#35777;&#26126;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#28041;&#21450;&#35270;&#39057;&#25351;&#20196;&#36319;&#38543;&#30340;&#20219;&#21153;&#20013;&#65292;&#25552;&#20379;&#20449;&#24687;&#20016;&#23500;&#30340;&#21453;&#39304;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#26816;&#27979;&#29983;&#25104;&#30340;&#21709;&#24212;&#20013;&#30340;&#24187;&#35273;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMM&#65289;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#26469;&#25351;&#23548;&#20559;&#22909;&#24314;&#27169;&#65292;&#20294;&#23427;&#20204;&#20934;&#30830;&#35780;&#20272;&#29983;&#25104;&#21709;&#24212;&#30340;&#20107;&#23454;&#24615;&#19982;&#23545;&#24212;&#35270;&#39057;&#30456;&#27604;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#20986;&#32467;&#35770;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#35814;&#32454;&#30340;&#35270;&#39057;&#26631;&#39064;&#20316;&#20026;&#35270;&#39057;&#20869;&#23481;&#30340;&#20195;&#29702;&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23558;&#36825;&#20123;&#20449;&#24687;&#20316;&#20026;&#25903;&#25345;&#35777;&#25454;&#26469;&#20026;&#35270;&#39057;&#38382;&#31572;&#65288;QA&#65289;&#39044;&#27979;&#25171;&#20998;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#19982;OpenAI GPT-4V&#27169;&#22411;&#30340;&#22870;&#21169;&#26426;&#21046;&#30340;&#31283;&#20581;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01258v1 Announce Type: cross  Abstract: Preference modeling techniques, such as direct preference optimization (DPO), has shown effective in enhancing the generalization abilities of large language model (LLM). However, in tasks involving video instruction-following, providing informative feedback, especially for detecting hallucinations in generated responses, remains a significant challenge. Previous studies have explored using large large multimodal models (LMMs) as reward models to guide preference modeling, but their ability to accurately assess the factuality of generated responses compared to corresponding videos has not been conclusively established. This paper introduces a novel framework that utilizes detailed video captions as a proxy of video content, enabling language models to incorporate this information as supporting evidence for scoring video Question Answering (QA) predictions. Our approach demonstrates robust alignment with OpenAI GPT-4V model's reward mec
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Feature Splatting&#30340;&#26041;&#27861;&#65292;&#23558;&#22522;&#20110;&#29289;&#29702;&#30340;&#21160;&#24577;&#22330;&#26223;&#21512;&#25104;&#19982;&#28304;&#33258;&#35270;&#35273;&#35821;&#35328;&#30340;&#20016;&#23500;&#35821;&#20041;&#32479;&#19968;&#36215;&#26469;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#23545;&#35937;&#20998;&#35299;&#21644;&#22522;&#20110;&#25991;&#26412;&#26597;&#35810;&#30340;&#29289;&#29702;&#29305;&#24615;&#33258;&#21160;&#21512;&#25104;&#12290;</title><link>https://arxiv.org/abs/2404.01223</link><description>&lt;p&gt;
&#29305;&#24449;&#28857;&#20999;&#29255;&#65306;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#22330;&#26223;&#21512;&#25104;&#21644;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Feature Splatting: Language-Driven Physics-Based Scene Synthesis and Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01223
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Feature Splatting&#30340;&#26041;&#27861;&#65292;&#23558;&#22522;&#20110;&#29289;&#29702;&#30340;&#21160;&#24577;&#22330;&#26223;&#21512;&#25104;&#19982;&#28304;&#33258;&#35270;&#35273;&#35821;&#35328;&#30340;&#20016;&#23500;&#35821;&#20041;&#32479;&#19968;&#36215;&#26469;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#23545;&#35937;&#20998;&#35299;&#21644;&#22522;&#20110;&#25991;&#26412;&#26597;&#35810;&#30340;&#29289;&#29702;&#29305;&#24615;&#33258;&#21160;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;3D&#39640;&#26031;&#21407;&#35821;&#34920;&#31034;&#22330;&#26223;&#22312;&#24314;&#27169;&#38745;&#24577;&#21644;&#21160;&#24577;3D&#22330;&#26223;&#30340;&#22806;&#35266;&#26041;&#38754;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290; &#28982;&#32780;&#65292;&#35768;&#22810;&#22270;&#24418;&#24212;&#29992;&#31243;&#24207;&#38656;&#35201;&#33021;&#22815;&#25805;&#32437;&#23545;&#35937;&#30340;&#22806;&#35266;&#21644;&#29289;&#29702;&#29305;&#24615;&#12290; &#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;Feature Splatting&#30340;&#26041;&#27861;&#65292;&#23427;&#23558;&#22522;&#20110;&#29289;&#29702;&#30340;&#21160;&#24577;&#22330;&#26223;&#21512;&#25104;&#19982;&#28304;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#20016;&#23500;&#35821;&#20041;&#32479;&#19968;&#36215;&#26469;&#12290; &#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#39640;&#36136;&#37327;&#30340;&#65292;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#35270;&#35273;&#35821;&#35328;&#29305;&#24449;&#25552;&#28860;&#25104;3D&#39640;&#26031;&#65292;&#20174;&#32780;&#21033;&#29992;&#25991;&#26412;&#26597;&#35810;&#23454;&#29616;&#21322;&#33258;&#21160;&#22330;&#26223;&#20998;&#35299;&#12290; &#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#36129;&#29486;&#26159;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#31890;&#23376;&#30340;&#27169;&#25311;&#22120;&#20174;&#38745;&#24577;&#22330;&#26223;&#20013;&#21512;&#25104;&#22522;&#20110;&#29289;&#29702;&#30340;&#21160;&#24577;&#22330;&#26223;&#65292;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#26448;&#26009;&#23646;&#24615;&#36890;&#36807;&#25991;&#26412;&#26597;&#35810;&#33258;&#21160;&#20998;&#37197;&#12290; &#25105;&#20204;&#21076;&#38500;&#20102;&#22312;&#36825;&#20010;&#27969;&#31243;&#20013;&#20351;&#29992;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#20197;&#38416;&#26126;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01223v1 Announce Type: cross  Abstract: Scene representations using 3D Gaussian primitives have produced excellent results in modeling the appearance of static and dynamic 3D scenes. Many graphics applications, however, demand the ability to manipulate both the appearance and the physical properties of objects. We introduce Feature Splatting, an approach that unifies physics-based dynamic scene synthesis with rich semantics from vision language foundation models that are grounded by natural language. Our first contribution is a way to distill high-quality, object-centric vision-language features into 3D Gaussians, that enables semi-automatic scene decomposition using text queries. Our second contribution is a way to synthesize physics-based dynamics from an otherwise static scene using a particle-based simulator, in which material properties are assigned automatically via text queries. We ablate key techniques used in this pipeline, to illustrate the challenge and opportunit
&lt;/p&gt;</description></item><item><title>&#23558;&#39046;&#22495;&#24494;&#20998;&#26041;&#31243;&#32435;&#20837;&#22270;&#21367;&#31215;&#32593;&#32476;&#21487;&#25552;&#39640;&#20854;&#23545;&#19981;&#21305;&#37197;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.01217</link><description>&lt;p&gt;
&#23558;&#39046;&#22495;&#24494;&#20998;&#26041;&#31243;&#32435;&#20837;&#22270;&#21367;&#31215;&#32593;&#32476;&#20197;&#38477;&#20302;&#27867;&#21270;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Incorporating Domain Differential Equations into Graph Convolutional Networks to Lower Generalization Discrepancy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01217
&lt;/p&gt;
&lt;p&gt;
&#23558;&#39046;&#22495;&#24494;&#20998;&#26041;&#31243;&#32435;&#20837;&#22270;&#21367;&#31215;&#32593;&#32476;&#21487;&#25552;&#39640;&#20854;&#23545;&#19981;&#21305;&#37197;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#23545;&#35768;&#22810;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#20174;&#22478;&#24066;&#35268;&#21010;&#21040;&#30123;&#24773;&#31649;&#29702;&#12290;&#26412;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;&#28085;&#30422;&#25152;&#26377;&#26102;&#31354;&#27169;&#24335;&#30340;&#20805;&#20998;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#20570;&#20986;&#30456;&#24403;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#24403;&#35757;&#32451;&#25968;&#25454;&#26469;&#33258;&#19982;&#27979;&#35797;&#25968;&#25454;&#19981;&#21516;&#29615;&#22659;&#65288;&#20363;&#22914;&#65292;&#27491;&#24120;&#22825;&#20132;&#36890;&#27169;&#24335;&#19982;&#33258;&#28982;&#28798;&#23475;&#21518;&#20132;&#36890;&#27169;&#24335;&#65289;&#26102;&#65292;&#29616;&#26377;&#26041;&#27861;&#20250;&#22833;&#36133;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#23558;&#39046;&#22495;&#24494;&#20998;&#26041;&#31243;&#32435;&#20837;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01217v1 Announce Type: cross  Abstract: Ensuring both accuracy and robustness in time series prediction is critical to many applications, ranging from urban planning to pandemic management. With sufficient training data where all spatiotemporal patterns are well-represented, existing deep-learning models can make reasonably accurate predictions. However, existing methods fail when the training data are drawn from different circumstances (e.g., traffic patterns on regular days) compared to test data (e.g., traffic patterns after a natural disaster). Such challenges are usually classified under domain generalization. In this work, we show that one way to address this challenge in the context of spatiotemporal prediction is by incorporating domain differential equations into Graph Convolutional Networks (GCNs). We theoretically derive conditions where GCNs incorporating such domain differential equations are robust to mismatched training and testing data compared to baseline do
&lt;/p&gt;</description></item><item><title>RelaxNN&#26694;&#26550;&#36890;&#36807;&#24347;&#35947;&#31995;&#32479;&#35299;&#20915;&#20102;&#22312;&#21452;&#26354;&#31995;&#32479;&#35299;&#20013;&#20986;&#29616;&#30340;&#28608;&#27874;&#38590;&#39064;&#65292;&#32531;&#35299;&#20102;PINN&#26694;&#26550;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25439;&#22833;&#20914;&#31361;&#12290;</title><link>https://arxiv.org/abs/2404.01163</link><description>&lt;p&gt;
&#36890;&#36807;&#24347;&#35947;&#31070;&#32463;&#32593;&#32476;&#25429;&#25417;&#28608;&#27874;
&lt;/p&gt;
&lt;p&gt;
Capturing Shock Waves by Relaxation Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01163
&lt;/p&gt;
&lt;p&gt;
RelaxNN&#26694;&#26550;&#36890;&#36807;&#24347;&#35947;&#31995;&#32479;&#35299;&#20915;&#20102;&#22312;&#21452;&#26354;&#31995;&#32479;&#35299;&#20013;&#20986;&#29616;&#30340;&#28608;&#27874;&#38590;&#39064;&#65292;&#32531;&#35299;&#20102;PINN&#26694;&#26550;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25439;&#22833;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#26469;&#35299;&#20915;&#38750;&#32447;&#24615;&#21452;&#26354;&#31995;&#32479;&#12290;&#35813;&#26694;&#26550;&#21517;&#20026;&#24347;&#35947;&#31070;&#32463;&#32593;&#32476;&#65288;RelaxNN&#65289;&#65292;&#26159;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#30340;&#19968;&#31181;&#31616;&#21333;&#21487;&#25193;&#23637;&#30340;&#25193;&#23637;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#20856;&#22411;&#30340;PINN&#26694;&#26550;&#24456;&#38590;&#22788;&#29702;&#22312;&#21452;&#26354;&#31995;&#32479;&#35299;&#20013;&#20986;&#29616;&#30340;&#28608;&#27874;&#65292;&#26368;&#32456;&#23548;&#33268;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#30340;&#20248;&#21270;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22833;&#36133;&#12290;&#24347;&#35947;&#31995;&#32479;&#20026;&#19981;&#36830;&#32493;&#35299;&#25552;&#20379;&#20102;&#24179;&#28369;&#30340;&#28176;&#36817;&#35299;&#65292;&#26399;&#26395;&#23439;&#35266;&#38382;&#39064;&#33021;&#22815;&#20174;&#24494;&#35266;&#35282;&#24230;&#24471;&#21040;&#35299;&#20915;&#12290;&#22522;&#20110;&#24347;&#35947;&#31995;&#32479;&#65292;RelaxNN&#26694;&#26550;&#32531;&#35299;&#20102;PINN&#26694;&#26550;&#35757;&#32451;&#36807;&#31243;&#20013;&#25439;&#22833;&#20914;&#31361;&#30340;&#38382;&#39064;&#12290;&#38500;&#22312;&#25968;&#20540;&#27169;&#25311;&#20013;&#23637;&#31034;&#20986;&#30340;&#26174;&#33879;&#32467;&#26524;&#22806;&#65292;&#22823;&#22810;&#25968;&#21152;&#36895;&#25216;&#26415;&#21644;&#25913;&#36827;&#31574;&#30053;&#37117;&#38024;&#23545;&#26631;&#20934;PINN&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01163v1 Announce Type: cross  Abstract: In this paper, we put forward a neural network framework to solve the nonlinear hyperbolic systems. This framework, named relaxation neural networks(RelaxNN), is a simple and scalable extension of physics-informed neural networks(PINN). It is shown later that a typical PINN framework struggles to handle shock waves that arise in hyperbolic systems' solutions. This ultimately results in the failure of optimization that is based on gradient descent in the training process. Relaxation systems provide a smooth asymptotic to the discontinuity solution, under the expectation that macroscopic problems can be solved from a microscopic perspective. Based on relaxation systems, the RelaxNN framework alleviates the conflict of losses in the training process of the PINN framework. In addition to the remarkable results demonstrated in numerical simulations, most of the acceleration techniques and improvement strategies aimed at the standard PINN fr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21516;&#27493;&#27880;&#24847;&#21147;&#36974;&#32617;&#65288;SyncMask&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#23578;&#39046;&#22495;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#20934;&#30830;&#23545;&#40784;&#32454;&#31890;&#24230;&#30340;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2404.01156</link><description>&lt;p&gt;
SyncMask&#65306;&#38754;&#21521;&#26102;&#23578;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#21516;&#27493;&#27880;&#24847;&#21147;&#36974;&#32617;
&lt;/p&gt;
&lt;p&gt;
SyncMask: Synchronized Attentional Masking for Fashion-centric Vision-Language Pretraining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01156
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21516;&#27493;&#27880;&#24847;&#21147;&#36974;&#32617;&#65288;SyncMask&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#23578;&#39046;&#22495;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#20934;&#30830;&#23545;&#40784;&#32454;&#31890;&#24230;&#30340;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#36890;&#36807;&#22823;&#35268;&#27169;&#37197;&#23545;&#25968;&#25454;&#38598;&#22312;&#36328;&#27169;&#24577;&#29702;&#35299;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#26102;&#23578;&#39046;&#22495;&#65292;&#25968;&#25454;&#38598;&#20013;&#32463;&#24120;&#23384;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20256;&#36798;&#20449;&#24687;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21516;&#27493;&#27880;&#24847;&#21147;&#36974;&#32617;&#65288;SyncMask&#65289;&#65292;&#20854;&#29983;&#25104;&#30340;&#36974;&#32617;&#25351;&#31034;&#20102;&#22270;&#20687;&#22359;&#21644;&#21333;&#35789;&#26631;&#35760;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20013;&#21516;&#26102;&#20986;&#29616;&#30340;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01156v1 Announce Type: cross  Abstract: Vision-language models (VLMs) have made significant strides in cross-modal understanding through large-scale paired datasets. However, in fashion domain, datasets often exhibit a disparity between the information conveyed in image and text. This issue stems from datasets containing multiple images of a single fashion item all paired with one text, leading to cases where some textual details are not visible in individual images. This mismatch, particularly when non-co-occurring elements are masked, undermines the training of conventional VLM objectives like Masked Language Modeling and Masked Image Modeling, thereby hindering the model's ability to accurately align fine-grained visual and textual features. Addressing this problem, we propose Synchronized attentional Masking (SyncMask), which generate masks that pinpoint the image patches and word tokens where the information co-occur in both image and text. This synchronization is accom
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#25991;&#26412;&#23884;&#20837;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#20851;&#20110;&#21333;&#35789;&#23884;&#20837;&#21450;&#20854;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#30340;&#37325;&#35201;&#35265;&#35299;&#65292;&#20026;&#23398;&#20064;&#20813;&#36153;&#22270;&#20687;&#32534;&#36753;&#25552;&#20379;&#20102;&#25351;&#23548;&#21407;&#21017;&#12290;</title><link>https://arxiv.org/abs/2404.01154</link><description>&lt;p&gt;
&#25581;&#31034;&#25991;&#26412;&#23884;&#20837;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Uncovering the Text Embedding in Text-to-Image Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#25991;&#26412;&#23884;&#20837;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#20851;&#20110;&#21333;&#35789;&#23884;&#20837;&#21450;&#20854;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#30340;&#37325;&#35201;&#35265;&#35299;&#65292;&#20026;&#23398;&#20064;&#20813;&#36153;&#22270;&#20687;&#32534;&#36753;&#25552;&#20379;&#20102;&#25351;&#23548;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36755;&#20837;&#25991;&#26412;&#19982;&#29983;&#25104;&#22270;&#20687;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#34920;&#29616;&#20026;&#19981;&#36879;&#26126;&#24615;&#65292;&#21363;&#36731;&#24494;&#30340;&#25991;&#26412;&#20462;&#25913;&#20250;&#23548;&#33268;&#29983;&#25104;&#22270;&#20687;&#30340;&#37325;&#22823;&#20559;&#24046;&#12290;&#32780;&#25991;&#26412;&#23884;&#20837;&#20316;&#20026;&#25991;&#26412;&#19982;&#22270;&#20687;&#20043;&#38388;&#30340;&#20851;&#38190;&#20013;&#38388;&#20307;&#20173;&#28982;&#30456;&#23545;&#26410;&#34987;&#28145;&#20837;&#25506;&#35752;&#12290;&#26412;&#25991;&#36890;&#36807;&#28145;&#20837;&#25506;&#35752;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#65292;&#37322;&#25918;&#20854;&#23545;&#21487;&#25511;&#22270;&#20687;&#32534;&#36753;&#21644;&#21487;&#35299;&#37322;&#30340;&#35821;&#20041;&#26041;&#21521;&#23646;&#24615;&#30340;&#33021;&#21147;&#65292;&#22312;&#26080;&#38656;&#23398;&#20064;&#30340;&#26694;&#26550;&#20869;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#20004;&#20010;&#20851;&#38190;&#35265;&#35299;&#65292;&#21363;&#21333;&#35789;&#23884;&#20837;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#23427;&#20204;&#22312;&#25991;&#26412;&#23884;&#20837;&#20013;&#30340;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#65292;&#20026;&#26080;&#38656;&#23398;&#20064;&#30340;&#22270;&#20687;&#32534;&#36753;&#25552;&#20379;&#20102;&#25351;&#23548;&#21407;&#21017;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#25991;&#26412;&#23884;&#20837;&#22266;&#26377;&#22320;&#20855;&#26377;&#22810;&#26679;&#30340;&#35821;&#20041;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#30340;&#35270;&#35282;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#36825;&#19968;&#23646;&#24615;&#12290;&#36825;&#20123;&#25581;&#31034;&#20986;&#26469;&#30340;&#29305;&#24615;&#25552;&#20379;&#20102;&#23454;&#29992;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01154v1 Announce Type: cross  Abstract: The correspondence between input text and the generated image exhibits opacity, wherein minor textual modifications can induce substantial deviations in the generated image. While, text embedding, as the pivotal intermediary between text and images, remains relatively underexplored. In this paper, we address this research gap by delving into the text embedding space, unleashing its capacity for controllable image editing and explicable semantic direction attributes within a learning-free framework. Specifically, we identify two critical insights regarding the importance of per-word embedding and their contextual correlations within text embedding, providing instructive principles for learning-free image editing. Additionally, we find that text embedding inherently possesses diverse semantic potentials, and further reveal this property through the lens of singular value decomposition (SVD). These uncovered properties offer practical uti
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26465;&#20214;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#65288;CAN&#65289;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#26469;&#25511;&#21046;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#65292;&#22312;ImageNet&#21644;COCO&#19978;&#27979;&#35797;&#34920;&#26126;&#65292;CAN&#19982;EfficientViT&#65288;CaT&#65289;&#30456;&#32467;&#21512;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2404.01143</link><description>&lt;p&gt;
&#26465;&#20214;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#21487;&#25511;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Condition-Aware Neural Network for Controlled Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01143
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26465;&#20214;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#65288;CAN&#65289;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#26469;&#25511;&#21046;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#65292;&#22312;ImageNet&#21644;COCO&#19978;&#27979;&#35797;&#34920;&#26126;&#65292;CAN&#19982;EfficientViT&#65288;CaT&#65289;&#30456;&#32467;&#21512;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#26465;&#20214;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#65288;CAN&#65289;&#65292;&#29992;&#20110;&#21521;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#28155;&#21152;&#25511;&#21046;&#12290;&#19982;&#20808;&#21069;&#30340;&#26465;&#20214;&#25511;&#21046;&#26041;&#27861;&#24182;&#34892;&#65292;CAN&#36890;&#36807;&#21160;&#24577;&#25805;&#32437;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#26469;&#25511;&#21046;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#12290;&#36825;&#26159;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26465;&#20214;&#24863;&#30693;&#26435;&#37325;&#29983;&#25104;&#27169;&#22359;&#26469;&#23454;&#29616;&#30340;&#65292;&#26681;&#25454;&#36755;&#20837;&#26465;&#20214;&#20026;&#21367;&#31215;/&#32447;&#24615;&#23618;&#29983;&#25104;&#26377;&#26465;&#20214;&#30340;&#26435;&#37325;&#12290;&#25105;&#20204;&#22312;ImageNet&#19978;&#23545;&#31867;&#21035;&#26377;&#26465;&#20214;&#30340;&#22270;&#20687;&#29983;&#25104;&#21644;&#22312;COCO&#19978;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#36827;&#34892;&#20102;CAN&#27979;&#35797;&#12290;CAN&#22987;&#32456;&#20026;&#25193;&#25955;&#21464;&#25442;&#22120;&#27169;&#22411;&#25552;&#20379;&#26174;&#30528;&#25913;&#36827;&#65292;&#21253;&#25324;DiT&#21644;UViT&#12290;&#29305;&#21035;&#26159;&#65292;CAN&#19982;EfficientViT&#65288;CaT&#65289;&#30456;&#32467;&#21512;&#65292;&#22312;ImageNet 512x512&#19978;&#23454;&#29616;&#20102;2.78 FID&#65292;&#36229;&#36807;&#20102;DiT-XL/2&#65292;&#21516;&#26102;&#27599;&#20010;&#37319;&#26679;&#27493;&#39588;&#38656;&#35201;&#30340;MAC&#25968;&#37327;&#20943;&#23569;&#20102;52&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01143v1 Announce Type: cross  Abstract: We present Condition-Aware Neural Network (CAN), a new method for adding control to image generative models. In parallel to prior conditional control methods, CAN controls the image generation process by dynamically manipulating the weight of the neural network. This is achieved by introducing a condition-aware weight generation module that generates conditional weight for convolution/linear layers based on the input condition. We test CAN on class-conditional image generation on ImageNet and text-to-image generation on COCO. CAN consistently delivers significant improvements for diffusion transformer models, including DiT and UViT. In particular, CAN combined with EfficientViT (CaT) achieves 2.78 FID on ImageNet 512x512, surpassing DiT-XL/2 while requiring 52x fewer MACs per sampling step.
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#35748;&#30693;&#22686;&#24378;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32593;&#32476;&#35843;&#26597;&#21644;&#25968;&#23383;&#21462;&#35777;&#20013;&#25552;&#39640;SLM&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#36890;&#36807;&#22788;&#29702;&#26412;&#22320;&#25968;&#25454;&#24182;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#26469;&#28385;&#36275;&#23433;&#20840;&#24615;&#21644;&#25968;&#25454;&#38544;&#31169;&#24615;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2404.01135</link><description>&lt;p&gt;
&#21033;&#29992;&#35748;&#30693;&#22686;&#24378;&#25216;&#26415;&#22686;&#24378;SLM&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Reasoning Capacity of SLM using Cognitive Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01135
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#35748;&#30693;&#22686;&#24378;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32593;&#32476;&#35843;&#26597;&#21644;&#25968;&#23383;&#21462;&#35777;&#20013;&#25552;&#39640;SLM&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#36890;&#36807;&#22788;&#29702;&#26412;&#22320;&#25968;&#25454;&#24182;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#26469;&#28385;&#36275;&#23433;&#20840;&#24615;&#21644;&#25968;&#25454;&#38544;&#31169;&#24615;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#34987;&#24212;&#29992;&#20110;&#33258;&#21160;&#21270;&#32593;&#32476;&#23433;&#20840;&#27963;&#21160;&#21644;&#27969;&#31243;&#65292;&#21253;&#25324;&#32593;&#32476;&#35843;&#26597;&#21644;&#25968;&#23383;&#21462;&#35777;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#29992;&#20110;&#32593;&#32476;&#35843;&#26597;&#21644;&#25968;&#23383;&#21462;&#35777;&#26102;&#65292;&#24212;&#35813;&#32771;&#34385;&#38382;&#36131;&#21046;&#21644;&#23433;&#20840;&#24615;&#12290;&#38382;&#36131;&#21046;&#30830;&#20445;&#27169;&#22411;&#33021;&#22815;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#25512;&#29702;&#21644;&#32467;&#26524;&#12290;&#36825;&#20123;&#20449;&#24687;&#21487;&#20197;&#36890;&#36807;&#26174;&#24335;&#25552;&#31034;&#35831;&#27714;&#26469;&#25552;&#21462;&#12290;&#23545;&#20110;&#23433;&#20840;&#32771;&#34385;&#65292;&#22312;&#25968;&#25454;&#22788;&#29702;&#36807;&#31243;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#35201;&#35299;&#20915;&#25152;&#28041;&#25968;&#25454;&#30340;&#38544;&#31169;&#21644;&#20445;&#23494;&#24615;&#12290;&#19968;&#31181;&#22788;&#29702;&#36825;&#20010;&#32771;&#34385;&#30340;&#26041;&#27861;&#26159;&#22312;&#26412;&#22320;&#20351;&#29992;&#27169;&#22411;&#30340;&#26412;&#22320;&#23454;&#20363;&#23545;&#25968;&#25454;&#36827;&#34892;&#22788;&#29702;&#12290;&#30001;&#20110;&#26412;&#22320;&#21487;&#29992;&#36164;&#28304;&#65288;&#20027;&#35201;&#26159;&#20869;&#23384;&#21644;GPU&#23481;&#37327;&#65289;&#30340;&#38480;&#21046;&#65292;&#36890;&#24120;&#20250;&#20351;&#29992;&#36739;&#23567;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(SLM)&#12290;&#36825;&#20123;SLM&#19982;LLMs&#30456;&#27604;&#65292;&#20855;&#26377;&#26126;&#26174;&#36739;&#23569;&#30340;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22823;&#23567;&#32553;&#20943;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01135v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have been applied to automate cyber security activities and processes including cyber investigation and digital forensics. However, the use of such models for cyber investigation and digital forensics should address accountability and security considerations. Accountability ensures models have the means to provide explainable reasonings and outcomes. This information can be extracted through explicit prompt requests. For security considerations, it is crucial to address privacy and confidentiality of the involved data during data processing as well. One approach to deal with this consideration is to have the data processed locally using a local instance of the model. Due to limitations of locally available resources, namely memory and GPU capacities, a Smaller Large Language Model (SLM) will typically be used. These SLMs have significantly fewer parameters compared to the LLMs. However, such size reductions
&lt;/p&gt;</description></item><item><title>GOV-REK&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#20026;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#20195;&#29702;&#20998;&#37197;&#22870;&#21169;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#27835;&#29702;&#20869;&#26680;&#21033;&#29992;&#29366;&#24577;&#25110;&#32852;&#21512;&#34892;&#21160;&#31354;&#38388;&#30340;&#32467;&#26500;&#65292;&#20197;&#35299;&#20915;&#22870;&#21169;&#24037;&#31243;&#21162;&#21147;&#26080;&#27861;&#36716;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.01131</link><description>&lt;p&gt;
GOV-REK&#65306;&#29992;&#20110;&#35774;&#35745;&#40065;&#26834;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#30340;&#21463;&#31649;&#22870;&#21169;&#24037;&#31243;&#26680;
&lt;/p&gt;
&lt;p&gt;
GOV-REK: Governed Reward Engineering Kernels for Designing Robust Multi-Agent Reinforcement Learning Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01131
&lt;/p&gt;
&lt;p&gt;
GOV-REK&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#20026;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#20195;&#29702;&#20998;&#37197;&#22870;&#21169;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#27835;&#29702;&#20869;&#26680;&#21033;&#29992;&#29366;&#24577;&#25110;&#32852;&#21512;&#34892;&#21160;&#31354;&#38388;&#30340;&#32467;&#26500;&#65292;&#20197;&#35299;&#20915;&#22870;&#21169;&#24037;&#31243;&#21162;&#21147;&#26080;&#27861;&#36716;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#65288;MARLS&#65289;&#65292;&#38382;&#39064;&#30340;&#21046;&#23450;&#36890;&#24120;&#38656;&#35201;&#25237;&#20837;&#22823;&#37327;&#22870;&#21169;&#24037;&#31243;&#24037;&#20316;&#65292;&#38024;&#23545;&#29305;&#23450;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21162;&#21147;&#36890;&#24120;&#26080;&#27861;&#36716;&#21270;&#20026;&#20854;&#20182;&#38382;&#39064;&#65307;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#24403;&#31995;&#32479;&#21160;&#24577;&#21457;&#29983; drastica &#25913;&#21464;&#26102;&#65292;&#36825;&#31181;&#21162;&#21147;&#23601;&#20250;&#34987;&#28010;&#36153;&#12290;&#22312;&#31232;&#30095;&#22870;&#21169;&#22330;&#26223;&#20013;&#65292;&#26377;&#24847;&#20041;&#30340;&#21551;&#21457;&#24335;&#21487;&#20197;&#24110;&#21161;&#31574;&#30053;&#25910;&#25947;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; GOVerned Reward Engineering Kernels (GOV-REK)&#65292;&#22312; MARLS &#30340;&#23398;&#20064;&#38454;&#27573;&#21160;&#24577;&#22320;&#20026;&#20195;&#29702;&#20998;&#37197;&#22870;&#21169;&#20998;&#24067;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#27835;&#29702;&#20869;&#26680;&#65292;&#21033;&#29992;&#29366;&#24577;&#25110;&#32852;&#21512;&#34892;&#21160;&#31354;&#38388;&#20013;&#30340;&#22522;&#30784;&#32467;&#26500;&#26469;&#20998;&#37197;&#26377;&#24847;&#20041;&#30340;&#20195;&#29702;&#22870;&#21169;&#20998;&#24067;&#12290;&#22312;&#20195;&#29702;&#23398;&#20064;&#38454;&#27573;&#65292;&#23427;&#20351;&#29992;&#31867;&#20284; Hyperband &#30340;&#31639;&#27861;&#36845;&#20195;&#22320;&#25506;&#32034;&#19981;&#21516;&#30340;&#22870;&#21169;&#20998;&#24067;&#37197;&#32622;&#65292;&#20197;&#22312;&#38382;&#39064;-&#19981;&#21487;&#30693; ma &#20013;&#23398;&#20064;&#29702;&#24819;&#30340;&#20195;&#29702;&#22870;&#21169;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01131v1 Announce Type: cross  Abstract: For multi-agent reinforcement learning systems (MARLS), the problem formulation generally involves investing massive reward engineering effort specific to a given problem. However, this effort often cannot be translated to other problems; worse, it gets wasted when system dynamics change drastically. This problem is further exacerbated in sparse reward scenarios, where a meaningful heuristic can assist in the policy convergence task. We propose GOVerned Reward Engineering Kernels (GOV-REK), which dynamically assign reward distributions to agents in MARLS during its learning stage. We also introduce governance kernels, which exploit the underlying structure in either state or joint action space for assigning meaningful agent reward distributions. During the agent learning stage, it iteratively explores different reward distribution configurations with a Hyperband-like algorithm to learn ideal agent reward models in a problem-agnostic ma
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21307;&#30103;&#35270;&#35273;&#25552;&#31034;&#65288;MVP&#65289;&#26694;&#26550;&#65292;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#27010;&#24565;&#65292;&#21033;&#29992;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#26469;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30149;&#21464;&#24418;&#29366;&#20449;&#24687;&#20002;&#22833;&#21644;&#25163;&#21160;&#26631;&#35760;&#25104;&#26412;&#39640;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2404.01127</link><description>&lt;p&gt;
&#21307;&#30103;&#35270;&#35273;&#25552;&#31034;&#65288;MVP&#65289;&#65306;&#36890;&#29992;&#39640;&#36136;&#37327;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Medical Visual Prompting (MVP): A Unified Framework for Versatile and High-Quality Medical Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01127
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21307;&#30103;&#35270;&#35273;&#25552;&#31034;&#65288;MVP&#65289;&#26694;&#26550;&#65292;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#27010;&#24565;&#65292;&#21033;&#29992;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#26469;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30149;&#21464;&#24418;&#29366;&#20449;&#24687;&#20002;&#22833;&#21644;&#25163;&#21160;&#26631;&#35760;&#25104;&#26412;&#39640;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28107;&#24052;&#30244;&#21306;&#22495;&#30340;&#20934;&#30830;&#20998;&#21106;&#23545;&#20110;&#21508;&#31181;&#30142;&#30149;&#30340;&#20020;&#24202;&#35786;&#26029;&#21644;&#27835;&#30103;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#20294;&#30001;&#20110;&#36830;&#32493;&#21367;&#31215;&#21644;&#19979;&#37319;&#26679;&#36896;&#25104;&#20102;&#25439;&#22833;&#30149;&#21464;&#24418;&#29366;&#20449;&#24687;&#12289;&#20197;&#21450;&#25163;&#21160;&#26631;&#35760;&#19981;&#21516;&#24418;&#29366;&#21644;&#22823;&#23567;&#30149;&#21464;&#30340;&#39640;&#25104;&#26412;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21307;&#30103;&#35270;&#35273;&#25552;&#31034;&#65288;MVP&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#27010;&#24565;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#22522;&#20110;&#36229;&#20687;&#32032;&#30340;&#25552;&#31034;&#65288;SPGP&#65289;&#29992;&#20110;&#23545;&#36755;&#20837;&#22270;&#20687;&#36827;&#34892;&#36229;&#20687;&#32032;&#22788;&#29702;&#65292;&#22270;&#20687;&#23884;&#20837;&#24341;&#23548;&#25552;&#31034;&#65288;IEGP&#65289;&#29992;&#20110;&#20923;&#32467;&#34917;&#19969;&#23884;&#20837;&#24182;&#19982;&#36229;&#20687;&#32032;&#21512;&#24182;&#20197;&#25552;&#20379;&#35270;&#35273;&#25552;&#31034;&#65292;&#20197;&#21450;&#33258;&#36866;&#24212;&#27880;&#24847;&#26426;&#21046;&#24341;&#23548;&#25552;&#31034;&#65288;AAGP&#65289;&#29992;&#20110;&#30830;&#23450;&#25552;&#31034;&#20869;&#23481;&#24182;&#26377;&#25928;&#22320;&#35843;&#25972;&#25152;&#26377;&#23618;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01127v1 Announce Type: cross  Abstract: Accurate segmentation of lesion regions is crucial for clinical diagnosis and treatment across various diseases. While deep convolutional networks have achieved satisfactory results in medical image segmentation, they face challenges such as loss of lesion shape information due to continuous convolution and downsampling, as well as the high cost of manually labeling lesions with varying shapes and sizes. To address these issues, we propose a novel medical visual prompting (MVP) framework that leverages pre-training and prompting concepts from natural language processing (NLP). The framework utilizes three key components: Super-Pixel Guided Prompting (SPGP) for superpixelating the input image, Image Embedding Guided Prompting (IEGP) for freezing patch embedding and merging with superpixels to provide visual prompts, and Adaptive Attention Mechanism Guided Prompting (AAGP) for pinpointing prompt content and efficiently adapting all layer
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21452;&#21521;&#38170;&#23450;&#26041;&#27861;&#65292;&#35782;&#21035;&#37027;&#20123;&#22312;&#24494;&#35843;&#21518;&#26356;&#21487;&#33021;&#38477;&#20302;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#33391;&#24615;&#25968;&#25454;&#23376;&#38598;&#65292;&#25552;&#39640;&#27169;&#22411;&#23545;&#26377;&#23475;&#35831;&#27714;&#30340;&#21709;&#24212;&#29575;&#12290;</title><link>https://arxiv.org/abs/2404.01099</link><description>&lt;p&gt;
&#20320;&#30340;&#8220;&#23433;&#20840;&#8221;&#25968;&#25454;&#20013;&#26377;&#20160;&#20040;&#65311;&#65306;&#35782;&#21035;&#30772;&#22351;&#23433;&#20840;&#24615;&#30340;&#33391;&#24615;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
What's in Your "Safe" Data?: Identifying Benign Data that Breaks Safety
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01099
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21452;&#21521;&#38170;&#23450;&#26041;&#27861;&#65292;&#35782;&#21035;&#37027;&#20123;&#22312;&#24494;&#35843;&#21518;&#26356;&#21487;&#33021;&#38477;&#20302;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#33391;&#24615;&#25968;&#25454;&#23376;&#38598;&#65292;&#25552;&#39640;&#27169;&#22411;&#23545;&#26377;&#23475;&#35831;&#27714;&#30340;&#21709;&#24212;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21363;&#20351;&#32463;&#36807;&#35843;&#25972;&#20197;&#30830;&#20445;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#65292;&#20063;&#23481;&#26131;&#34987;&#36234;&#29425;&#12290;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;&#65292;&#21482;&#26159;&#36827;&#19968;&#27493;&#20351;&#29992;&#33391;&#24615;&#25968;&#25454;&#65288;&#21363;&#27809;&#26377;&#26377;&#23475;&#20869;&#23481;&#30340;&#25968;&#25454;&#65289;&#23545;&#19968;&#20010;&#23545;&#40784;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20250;&#23548;&#33268;&#23433;&#20840;&#24615;&#22823;&#24133;&#19979;&#38477;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#33391;&#24615;&#24494;&#35843;&#19981;&#32463;&#24847;&#38388;&#23548;&#33268;&#36234;&#29425;&#30340;&#25968;&#25454;&#20013;&#24515;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#20004;&#31181;&#35270;&#35282;&#34920;&#24449;&#24494;&#35843;&#25968;&#25454;&#65306;&#34920;&#31034;&#21644;&#26799;&#24230;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#38170;&#23450;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20248;&#20808;&#32771;&#34385;&#38752;&#36817;&#26377;&#23475;&#31034;&#20363;&#24182;&#36828;&#31163;&#33391;&#24615;&#31034;&#20363;&#30340;&#25968;&#25454;&#28857;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#26356;&#26377;&#21487;&#33021;&#22312;&#24494;&#35843;&#21518;&#38477;&#20302;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#33391;&#24615;&#25968;&#25454;&#23376;&#38598;&#12290;&#20165;&#20165;&#35757;&#32451;100&#20010;&#36825;&#20123;&#30475;&#20284;&#33391;&#24615;&#30340;&#25968;&#25454;&#28857;&#65292;&#23601;&#21487;&#20197;&#20351;&#24494;&#35843;&#27169;&#22411;&#32943;&#23450;&#22320;&#22238;&#24212;&#36229;&#36807;70&#65285;&#30340;&#34987;&#27979;&#35797;&#30340;&#26377;&#23475;&#35831;&#27714;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01099v1 Announce Type: cross  Abstract: Current Large Language Models (LLMs), even those tuned for safety and alignment, are susceptible to jailbreaking. Some have found that just further fine-tuning an aligned model with benign data (i.e., data without harmful content) surprisingly leads to substantial degradation in safety. We delve into the data-centric aspects of why benign fine-tuning inadvertently contributes to jailbreaking. First, we represent fine-tuning data through two lenses: representation and gradient spaces. Furthermore, we propose a bi-directional anchoring method that prioritizes data points that are close to harmful examples and distant from benign ones. By doing so, our approach effectively identifies subsets of benign data that are more likely to degrade the model's safety after fine-tuning. Training on just 100 of these seemingly benign datapoints can lead to the fine-tuned model affirmatively responding to &gt; 70% of tested harmful requests, compared to &lt;
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Texture-Preserving Diffusion&#65288;TPD&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#31354;&#38388;&#32500;&#24230;&#36830;&#25509;&#36974;&#32617;&#20154;&#29289;&#21644;&#21442;&#32771;&#26381;&#35013;&#22270;&#20687;&#65292;&#22686;&#24378;&#20102;&#34394;&#25311;&#35797;&#31359;&#30340;&#20445;&#30495;&#24230;&#65292;&#21516;&#26102;&#19981;&#24341;&#20837;&#39069;&#22806;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#12290;</title><link>https://arxiv.org/abs/2404.01089</link><description>&lt;p&gt;
&#39640;&#20445;&#30495;&#34394;&#25311;&#35797;&#31359;&#30340;&#20445;&#25345;&#32441;&#29702;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Texture-Preserving Diffusion Models for High-Fidelity Virtual Try-On
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01089
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Texture-Preserving Diffusion&#65288;TPD&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#31354;&#38388;&#32500;&#24230;&#36830;&#25509;&#36974;&#32617;&#20154;&#29289;&#21644;&#21442;&#32771;&#26381;&#35013;&#22270;&#20687;&#65292;&#22686;&#24378;&#20102;&#34394;&#25311;&#35797;&#31359;&#30340;&#20445;&#30495;&#24230;&#65292;&#21516;&#26102;&#19981;&#24341;&#20837;&#39069;&#22806;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#34394;&#25311;&#35797;&#31359;&#26159;&#22312;&#32447;&#36141;&#29289;&#20013;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#23427;&#26088;&#22312;&#21512;&#25104;&#29305;&#23450;&#20154;&#29289;&#31359;&#30528;&#25351;&#23450;&#26381;&#35013;&#30340;&#22270;&#20687;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#22270;&#20687;&#21512;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#39069;&#22806;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#65292;&#24182;&#20381;&#36182;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#20174;&#26381;&#35013;&#21040;&#20154;&#29289;&#22270;&#20687;&#30340;&#32441;&#29702;&#20256;&#36755;&#65292;&#36825;&#24433;&#21709;&#20102;&#35797;&#31359;&#30340;&#25928;&#29575;&#21644;&#20445;&#30495;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25345;&#32441;&#29702;&#25193;&#25955;&#65288;TPD&#65289;&#27169;&#22411;&#36827;&#34892;&#34394;&#25311;&#35797;&#31359;&#65292;&#23427;&#22686;&#24378;&#20102;&#32467;&#26524;&#30340;&#20445;&#30495;&#24230;&#24182;&#19981;&#24341;&#20837;&#39069;&#22806;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#26041;&#38754;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#35758;&#27839;&#31354;&#38388;&#32500;&#24230;&#36830;&#25509;&#36974;&#32617;&#20154;&#29289;&#21644;&#21442;&#32771;&#26381;&#35013;&#22270;&#20687;&#65292;&#24182;&#21033;&#29992;&#29983;&#25104;&#30340;&#22270;&#20687;&#20316;&#20026;&#25193;&#25955;&#27169;&#22411;&#21435;&#22122;UNet&#30340;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01089v1 Announce Type: cross  Abstract: Image-based virtual try-on is an increasingly important task for online shopping. It aims to synthesize images of a specific person wearing a specified garment. Diffusion model-based approaches have recently become popular, as they are excellent at image synthesis tasks. However, these approaches usually employ additional image encoders and rely on the cross-attention mechanism for texture transfer from the garment to the person image, which affects the try-on's efficiency and fidelity. To address these issues, we propose an Texture-Preserving Diffusion (TPD) model for virtual try-on, which enhances the fidelity of the results and introduces no additional image encoders. Accordingly, we make contributions from two aspects. First, we propose to concatenate the masked person and reference garment images along the spatial dimension and utilize the resulting image as the input for the diffusion model's denoising UNet. This enables the orig
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#25972;&#22810;&#31181;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#22312;SemEval-2024&#20219;&#21153;9&#30340;&#33041;&#31563;&#24613;&#36716;&#24367;&#31454;&#36187;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#21147;&#21313;&#36275;&#30340;&#34920;&#29616;&#65292;&#22312;&#21477;&#23376;&#35868;&#39064;&#21644;&#21333;&#35789;&#35868;&#39064;&#30340;&#35780;&#20272;&#20013;&#65292;&#26368;&#20339;&#25552;&#20132;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#36229;&#36807;&#20102;&#26368;&#20339;&#31070;&#32463;&#22522;&#32447;ChatGPT&#36229;&#36807;20%&#21644;30%&#12290;</title><link>https://arxiv.org/abs/2404.01084</link><description>&lt;p&gt;
AILS-NTUA&#21442;&#21152;SemEval-2024&#20219;&#21153;9&#65306;&#30772;&#35299;&#33041;&#31563;&#24613;&#36716;&#24367;&#65306;&#21464;&#21387;&#22120;&#27169;&#22411;&#29992;&#20110;&#27178;&#21521;&#24605;&#32500;&#35868;&#39064;
&lt;/p&gt;
&lt;p&gt;
AILS-NTUA at SemEval-2024 Task 9: Cracking Brain Teasers: Transformer Models for Lateral Thinking Puzzles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01084
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#25972;&#22810;&#31181;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#22312;SemEval-2024&#20219;&#21153;9&#30340;&#33041;&#31563;&#24613;&#36716;&#24367;&#31454;&#36187;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#21147;&#21313;&#36275;&#30340;&#34920;&#29616;&#65292;&#22312;&#21477;&#23376;&#35868;&#39064;&#21644;&#21333;&#35789;&#35868;&#39064;&#30340;&#35780;&#20272;&#20013;&#65292;&#26368;&#20339;&#25552;&#20132;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#36229;&#36807;&#20102;&#26368;&#20339;&#31070;&#32463;&#22522;&#32447;ChatGPT&#36229;&#36807;20%&#21644;30%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#25105;&#20204;&#22312;SemEval-2024&#20219;&#21153;9&#31454;&#36187;&#20013;&#30340;&#25552;&#20132;:&#8220;BRAINTEASER:&#19968;&#20010;&#25361;&#25112;&#24120;&#35782;&#30340;&#26032;&#20219;&#21153;&#8221;&#12290;&#25105;&#20204;&#21442;&#19982;&#20102;&#23376;&#20219;&#21153;A-&#21477;&#23376;&#35868;&#39064;&#21644;&#23376;&#20219;&#21153;B-&#21333;&#35789;&#35868;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#24494;&#35843;&#35780;&#20272;&#20102;&#22823;&#37327;&#19981;&#21516;&#23610;&#23544;&#30340;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;&#23427;&#20204;&#30340;&#24471;&#20998;&#21644;&#21709;&#24212;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#20197;&#24110;&#21161;&#26410;&#26469;&#30340;&#30740;&#31350;&#20154;&#21592;&#26377;&#25928;&#22320;&#29702;&#35299;&#21644;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#12290;&#25105;&#20204;&#25490;&#21517;&#38752;&#21069;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#23376;&#20219;&#21153;&#30340;&#31454;&#36187;&#25490;&#34892;&#27036;&#19978;&#33719;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#20301;&#32622;&#12290;&#22312;&#35780;&#20272;&#38454;&#27573;&#65292;&#25105;&#20204;&#26368;&#22909;&#30340;&#25552;&#20132;&#22312;&#21477;&#23376;&#35868;&#39064;&#20013;&#33719;&#24471;&#20102;81.7%&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#65292;&#22312;&#21333;&#35789;&#35868;&#39064;&#20013;&#33719;&#24471;&#20102;85.4%&#30340;&#20934;&#30830;&#29575;&#65292;&#20998;&#21035;&#27604;&#26368;&#20339;&#31070;&#32463;&#22522;&#32447;&#65288;ChatGPT&#65289;&#39640;&#20986;&#36229;&#36807;20%&#21644;30%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01084v1 Announce Type: cross  Abstract: In this paper, we outline our submission for the SemEval-2024 Task 9 competition: 'BRAINTEASER: A Novel Task Defying Common Sense'. We engage in both sub-tasks: Sub-task A-Sentence Puzzle and Sub-task B-Word Puzzle. We evaluate a plethora of pre-trained transformer-based language models of different sizes through fine-tuning. Subsequently, we undertake an analysis of their scores and responses to aid future researchers in understanding and utilizing these models effectively. Our top-performing approaches secured competitive positions on the competition leaderboard across both sub-tasks. In the evaluation phase, our best submission attained an average accuracy score of 81.7% in the Sentence Puzzle, and 85.4% in the Word Puzzle, significantly outperforming the best neural baseline (ChatGPT) by more than 20% and 30% respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#20262;&#29702;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21644;&#25991;&#29486;&#32508;&#36848;&#30830;&#23450;&#21644;&#35299;&#20915;&#20102;&#25968;&#25454;&#22788;&#29702;&#12289;&#38544;&#31169;&#12289;&#25968;&#25454;&#25152;&#26377;&#26435;&#31561;&#26041;&#38754;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#20026;&#30830;&#20445;AI&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#21644;&#25991;&#21270;&#25935;&#24863;&#24615;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2404.01070</link><description>&lt;p&gt;
&#20197;&#35802;&#20449;&#25512;&#21160;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#65306;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#20262;&#29702;&#25361;&#25112;&#19982;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Advancing AI with Integrity: Ethical Challenges and Solutions in Neural Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#20262;&#29702;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21644;&#25991;&#29486;&#32508;&#36848;&#30830;&#23450;&#21644;&#35299;&#20915;&#20102;&#25968;&#25454;&#22788;&#29702;&#12289;&#38544;&#31169;&#12289;&#25968;&#25454;&#25152;&#26377;&#26435;&#31561;&#26041;&#38754;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#20026;&#30830;&#20445;AI&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#21644;&#25991;&#21270;&#25935;&#24863;&#24615;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#31995;&#32479;&#20013;&#30340;&#20262;&#29702;&#25361;&#25112;&#65292;&#24378;&#35843;&#24320;&#21457;&#32773;&#30830;&#20445;&#20844;&#24179;&#24615;&#21644;&#25991;&#21270;&#25935;&#24863;&#24615;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;NMT&#20013;AI&#27169;&#22411;&#30340;&#20262;&#29702;&#32032;&#20859;&#65292;&#23457;&#35270;&#20102;NMT&#24320;&#21457;&#30340;&#27599;&#20010;&#38454;&#27573;&#30340;&#20262;&#29702;&#32771;&#34385;&#65292;&#21253;&#25324;&#25968;&#25454;&#22788;&#29702;&#12289;&#38544;&#31169;&#12289;&#25968;&#25454;&#25152;&#26377;&#26435;&#21644;&#21516;&#24847;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#30830;&#23450;&#24182;&#35299;&#20915;&#20102;&#20262;&#29702;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#21253;&#25324;&#21033;&#29992;Transformer&#27169;&#22411;&#36827;&#34892;&#21346;&#24178;&#36798;&#35821;-&#33521;&#35821;&#32763;&#35793;&#65292;&#20197;&#21450;&#21033;&#29992;&#21477;&#23376;&#36855;&#20320;&#25209;&#22788;&#29702;&#25552;&#39640;&#25928;&#29575;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#32454;&#21270;&#25968;&#25454;&#26631;&#35760;&#25216;&#26415;&#21644;&#23545;&#21346;&#24178;&#36798;&#35821;&#21644;&#33521;&#35821;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#36827;&#34892;BERT&#21644;Longformer&#27169;&#22411;&#30340;&#24494;&#35843;&#31561;&#34917;&#20805;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#31532;&#20108;&#31181;&#26041;&#27861;&#26159;&#26469;&#33258;Google Scholar&#31561;&#25968;&#25454;&#24211;&#21644;GitHub&#31561;&#24179;&#21488;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25506;&#35752;&#20102;AI&#31995;&#32479;&#19982;&#30456;&#20851;&#36131;&#20219;&#20043;&#38388;&#30340;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01070v1 Announce Type: cross  Abstract: This paper addresses the ethical challenges of Artificial Intelligence in Neural Machine Translation (NMT) systems, emphasizing the imperative for developers to ensure fairness and cultural sensitivity. We investigate the ethical competence of AI models in NMT, examining the Ethical considerations at each stage of NMT development, including data handling, privacy, data ownership, and consent. We identify and address ethical issues through empirical studies. These include employing Transformer models for Luganda-English translations and enhancing efficiency with sentence mini-batching. And complementary studies that refine data labeling techniques and fine-tune BERT and Longformer models for analyzing Luganda and English social media content. Our second approach is a literature review from databases such as Google Scholar and platforms like GitHub. Additionally, the paper probes the distribution of responsibility between AI systems and 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Regularized Best-of-N (RBoN)&#65292;&#36890;&#36807;&#24341;&#20837;&#25509;&#36817;&#24615;&#39033;&#26469;&#20943;&#36731;&#22870;&#21169;&#27450;&#39575;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#22312;&#35299;&#30721;&#26102;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.01054</link><description>&lt;p&gt;
&#27491;&#21017;&#21270;&#30340;&#26368;&#20339;-N&#37319;&#26679;&#20197;&#20943;&#36731;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#20013;&#30340;&#22870;&#21169;&#27450;&#39575;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Regularized Best-of-N Sampling to Mitigate Reward Hacking for Language Model Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01054
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Regularized Best-of-N (RBoN)&#65292;&#36890;&#36807;&#24341;&#20837;&#25509;&#36817;&#24615;&#39033;&#26469;&#20943;&#36731;&#22870;&#21169;&#27450;&#39575;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#22312;&#35299;&#30721;&#26102;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Best-of-N (BoN)&#37319;&#26679;&#19982;&#22870;&#21169;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#22312;&#35299;&#30721;&#26102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;BoN&#37319;&#26679;&#23481;&#26131;&#21463;&#21040;&#22870;&#21169;&#27450;&#39575;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#38450;&#27490;&#22870;&#21169;&#27450;&#39575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Regularized Best-of-N (RBoN)&#30340;&#21464;&#20307;&#65292;&#36890;&#36807;&#22312;&#21709;&#24212;&#36873;&#25321;&#20013;&#32467;&#21512;&#25509;&#36817;&#24615;&#39033;&#26469;&#20943;&#36731;&#22870;&#21169;&#27450;&#39575;&#65292;&#31867;&#20284;&#20110;&#20559;&#22909;&#23398;&#20064;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01054v1 Announce Type: cross  Abstract: Best-of-N (BoN) sampling with a reward model has been shown to be an effective strategy for aligning Large Language Models (LLMs) to human preferences at the time of decoding. BoN sampling is susceptible to a problem known as reward hacking. Because the reward model is an imperfect proxy for the true objective, over-optimizing its value can compromise its performance on the true objective. A common solution to prevent reward hacking in preference learning techniques is to optimize a reward using proximity regularization (e.g., KL regularization), which ensures that the language model remains close to the reference model. In this research, we propose Regularized Best-of-N (RBoN), a variant of BoN that aims to mitigate reward hacking by incorporating a proximity term in response selection, similar to preference learning techniques. We evaluate two variants of RBoN on the AlpacaFarm dataset and find that they outperform BoN, especially wh
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#32423;&#32852;&#31995;&#32479;&#20013;&#36816;&#29992;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#20943;&#23569;&#22312;&#26597;&#35810;&#36828;&#31243;&#27169;&#22411;&#26102;&#27844;&#28431;&#31169;&#20154;&#20449;&#24687;&#30340;&#39118;&#38505;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#38544;&#31169;&#24230;&#37327;&#12290;</title><link>https://arxiv.org/abs/2404.01041</link><description>&lt;p&gt;
LLM&#21487;&#20197;&#22312;&#19981;&#36879;&#38706;&#31169;&#20154;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20854;&#20182;LLM&#30340;&#24110;&#21161;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can LLMs get help from other LLMs without revealing private information?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#32423;&#32852;&#31995;&#32479;&#20013;&#36816;&#29992;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#20943;&#23569;&#22312;&#26597;&#35810;&#36828;&#31243;&#27169;&#22411;&#26102;&#27844;&#28431;&#31169;&#20154;&#20449;&#24687;&#30340;&#39118;&#38505;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#38544;&#31169;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32423;&#32852;&#26159;&#19968;&#31181;&#24120;&#35265;&#31867;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#20854;&#20013;&#22914;&#26524;&#26412;&#22320;&#27169;&#22411;&#26080;&#27861;&#21333;&#29420;&#20934;&#30830;&#26631;&#35760;&#29992;&#25143;&#25968;&#25454;&#65292;&#21017;&#21487;&#20197;&#26597;&#35810;&#19968;&#20010;&#22823;&#22411;&#30340;&#36828;&#31243;&#27169;&#22411;&#12290;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#30001;&#20110;&#20854;&#22312;&#26174;&#33879;&#38477;&#20302;&#25512;&#26029;&#25104;&#26412;&#30340;&#21516;&#26102;&#20445;&#25345;&#20219;&#21153;&#24615;&#33021;&#30340;&#33021;&#21147;&#65292;&#26381;&#21153;&#22534;&#26632;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#32423;&#32852;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#22320;&#27169;&#22411;&#21487;&#20197;&#35775;&#38382;&#25935;&#24863;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#24212;&#29992;&#32423;&#32852;&#31995;&#32479;&#26500;&#25104;&#29992;&#25143;&#30340;&#37325;&#22823;&#38544;&#31169;&#39118;&#38505;&#65292;&#22240;&#20026;&#36825;&#20123;&#25968;&#25454;&#21487;&#33021;&#34987;&#36716;&#21457;&#21040;&#36828;&#31243;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#27492;&#31867;&#35774;&#32622;&#20013;&#24212;&#29992;&#32423;&#32852;&#31995;&#32479;&#30340;&#21487;&#34892;&#24615;&#65292;&#26041;&#27861;&#26159;&#20026;&#26412;&#22320;&#27169;&#22411;&#37197;&#22791;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#65292;&#20174;&#32780;&#20943;&#23569;&#35775;&#38382;&#36828;&#31243;&#27169;&#22411;&#26102;&#27844;&#28431;&#31169;&#20154;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#37327;&#21270;&#27492;&#31867;&#35774;&#32622;&#20013;&#30340;&#20449;&#24687;&#27844;&#28431;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#38544;&#31169;&#24230;&#37327;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#26368;&#36817;&#24341;&#20837;&#30340;&#31038;&#20132;&#23398;&#20064;&#33539;&#24335;&#30340;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01041v1 Announce Type: cross  Abstract: Cascades are a common type of machine learning systems in which a large, remote model can be queried if a local model is not able to accurately label a user's data by itself. Serving stacks for large language models (LLMs) increasingly use cascades due to their ability to preserve task performance while dramatically reducing inference costs. However, applying cascade systems in situations where the local model has access to sensitive data constitutes a significant privacy risk for users since such data could be forwarded to the remote model. In this work, we show the feasibility of applying cascade systems in such setups by equipping the local model with privacy-preserving techniques that reduce the risk of leaking private information when querying the remote model. To quantify information leakage in such setups, we introduce two privacy measures. We then propose a system that leverages the recently introduced social learning paradigm 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#23545;&#39640;&#31561;&#25945;&#32946;&#35780;&#20272;&#23454;&#36341;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#36825;&#20123;&#24037;&#20855;&#23637;&#31034;&#20102;&#23398;&#31185;&#30693;&#35782;&#12289;&#38382;&#39064;&#35299;&#20915;&#12289;&#20998;&#26512;&#33021;&#21147;&#31561;&#25216;&#33021;&#65292;&#20294;&#22312;&#19981;&#36947;&#24503;&#20351;&#29992;&#26102;&#21487;&#33021;&#38480;&#21046;&#23398;&#20064;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#37096;&#20998;&#23398;&#31185;&#35780;&#20272;&#20013;&#36825;&#20123;&#24037;&#20855;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.01036</link><description>&lt;p&gt;
&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#26102;&#20195;&#30340;&#39640;&#31561;&#25945;&#32946;&#35780;&#20272;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Higher education assessment practice in the era of generative AI tools
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01036
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#23545;&#39640;&#31561;&#25945;&#32946;&#35780;&#20272;&#23454;&#36341;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#36825;&#20123;&#24037;&#20855;&#23637;&#31034;&#20102;&#23398;&#31185;&#30693;&#35782;&#12289;&#38382;&#39064;&#35299;&#20915;&#12289;&#20998;&#26512;&#33021;&#21147;&#31561;&#25216;&#33021;&#65292;&#20294;&#22312;&#19981;&#36947;&#24503;&#20351;&#29992;&#26102;&#21487;&#33021;&#38480;&#21046;&#23398;&#20064;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#37096;&#20998;&#23398;&#31185;&#35780;&#20272;&#20013;&#36825;&#20123;&#24037;&#20855;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#31561;&#25945;&#32946;&#65288;HE&#65289;&#37096;&#38376;&#23545;&#27599;&#20010;&#22269;&#23478;&#30340;&#32463;&#27982;&#21644;&#31038;&#20250;&#37117;&#26377;&#30410;&#22788;&#65292;&#20294;&#23427;&#20204;&#30340;&#36129;&#29486;&#21463;&#21040;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#31561;&#20808;&#36827;&#25216;&#26415;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20840;&#38754;&#35780;&#20272;&#20102;GenAI&#24037;&#20855;&#23545;&#35780;&#20272;&#21644;&#25945;&#23398;&#23454;&#36341;&#30340;&#24433;&#21709;&#65292;&#24182;&#38543;&#21518;&#35752;&#35770;&#20102;&#28508;&#22312;&#24433;&#21709;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#25968;&#25454;&#31185;&#23398;&#12289;&#25968;&#25454;&#20998;&#26512;&#21644;&#24314;&#31569;&#31649;&#29702;&#23398;&#31185;&#30340;&#19977;&#31181;&#35780;&#20272;&#24037;&#20855;&#36827;&#34892;&#23454;&#39564;&#12290;&#30740;&#31350;&#21457;&#29616;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;&#39318;&#20808;&#65292;&#32467;&#26524;&#26174;&#31034;GenAI&#24037;&#20855;&#23637;&#31034;&#20102;&#23398;&#31185;&#30693;&#35782;&#12289;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12289;&#20998;&#26512;&#33021;&#21147;&#12289;&#25209;&#21028;&#24615;&#24605;&#32500;&#21644;&#34920;&#36798;&#33021;&#21147;&#65292;&#22240;&#27492;&#22312;&#19981;&#36947;&#24503;&#20351;&#29992;&#26102;&#21487;&#33021;&#20250;&#38480;&#21046;&#23398;&#20064;&#12290;&#20854;&#27425;&#65292;&#26576;&#20123;&#23398;&#31185;&#35780;&#20272;&#30340;&#35774;&#35745;&#25581;&#31034;&#20102;GenAI&#24037;&#20855;&#30340;&#23616;&#38480;&#24615;&#12290;&#26681;&#25454;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#22914;&#20309;&#22312;&#39640;&#31561;&#25945;&#32946;&#20013;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#36827;&#34892;&#25945;&#23398;&#19982;&#23398;&#20064;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01036v1 Announce Type: cross  Abstract: The higher education (HE) sector benefits every nation's economy and society at large. However, their contributions are challenged by advanced technologies like generative artificial intelligence (GenAI) tools. In this paper, we provide a comprehensive assessment of GenAI tools towards assessment and pedagogic practice and, subsequently, discuss the potential impacts. This study experimented using three assessment instruments from data science, data analytics, and construction management disciplines. Our findings are two-fold: first, the findings revealed that GenAI tools exhibit subject knowledge, problem-solving, analytical, critical thinking, and presentation skills and thus can limit learning when used unethically. Secondly, the design of the assessment of certain disciplines revealed the limitations of the GenAI tools. Based on our findings, we made recommendations on how AI tools can be utilised for teaching and learning in HE.
&lt;/p&gt;</description></item><item><title>&#35813;&#35843;&#26597;&#32508;&#36848;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#24615;&#21035;&#12289;&#32932;&#33394;&#21644;&#22320;&#22495;&#25991;&#21270;&#36825;&#20123;&#26041;&#38754;&#65292;&#26088;&#22312;&#24110;&#21161;&#29702;&#35299;&#24403;&#21069;&#36827;&#23637;&#21644;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2404.01030</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#35843;&#26597;&#65306;&#23450;&#20041;&#12289;&#35780;&#20272;&#21644;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Survey of Bias In Text-to-Image Generation: Definition, Evaluation, and Mitigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01030
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35843;&#26597;&#32508;&#36848;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#24615;&#21035;&#12289;&#32932;&#33394;&#21644;&#22320;&#22495;&#25991;&#21270;&#36825;&#20123;&#26041;&#38754;&#65292;&#26088;&#22312;&#24110;&#21161;&#29702;&#35299;&#24403;&#21069;&#36827;&#23637;&#21644;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24378;&#22823;&#30340;&#22823;&#22411;&#27169;&#22411;&#65292;&#22914;OpenAI&#30340;DALLE-3&#21644;Google&#30340;Gemini&#65292;&#20351;&#24471;&#29992;&#25143;&#33021;&#22815;&#20174;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#25552;&#31034;&#20063;&#21487;&#33021;&#23548;&#33268;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#22312;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#23637;&#29616;&#26126;&#26174;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#36825;&#31181;&#20559;&#35265;&#21487;&#33021;&#20250;&#23548;&#33268;&#31038;&#20250;&#20013;&#30340;&#20998;&#37197;&#21644;&#20195;&#34920;&#24615;&#20260;&#23475;&#65292;&#36827;&#19968;&#27493;&#36793;&#32536;&#21270;&#23569;&#25968;&#32676;&#20307;&#12290;&#37492;&#20110;&#36825;&#19968;&#38382;&#39064;&#65292;&#26368;&#36817;&#26377;&#22823;&#37327;&#30740;&#31350;&#33268;&#21147;&#20110;&#35843;&#26597;&#25991;&#26412;&#21040;&#22270;&#20687;&#31995;&#32479;&#20013;&#20559;&#35265;&#30340;&#19981;&#21516;&#32500;&#24230;&#12290;&#28982;&#32780;&#65292;&#23545;&#36825;&#20123;&#30740;&#31350;&#30340;&#20840;&#38754;&#22238;&#39038;&#20173;&#28982;&#32570;&#20047;&#65292;&#38459;&#30861;&#20102;&#23545;&#24403;&#21069;&#36827;&#23637;&#21644;&#30740;&#31350;&#31354;&#30333;&#30340;&#31995;&#32479;&#24615;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#20559;&#35265;&#30340;&#31532;&#19968;&#27425;&#24191;&#27867;&#35843;&#26597;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#20808;&#21069;&#20851;&#20110;&#20559;&#35265;&#32500;&#24230;&#30340;&#30740;&#31350;&#65306;&#24615;&#21035;&#12289;&#32932;&#33394;&#21644;&#22320;&#22495;&#25991;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01030v1 Announce Type: cross  Abstract: The recent advancement of large and powerful models with Text-to-Image (T2I) generation abilities -- such as OpenAI's DALLE-3 and Google's Gemini -- enables users to generate high-quality images from textual prompts. However, it has become increasingly evident that even simple prompts could cause T2I models to exhibit conspicuous social bias in generated images. Such bias might lead to both allocational and representational harms in society, further marginalizing minority groups. Noting this problem, a large body of recent works has been dedicated to investigating different dimensions of bias in T2I systems. However, an extensive review of these studies is lacking, hindering a systematic understanding of current progress and research gaps. We present the first extensive survey on bias in T2I generative models. In this survey, we review prior studies on dimensions of bias: Gender, Skintone, and Geo-Culture. Specifically, we discuss how 
&lt;/p&gt;</description></item><item><title>&#28304;&#24863;&#30693;&#35757;&#32451;&#20351;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#30693;&#35782;&#24402;&#22240;&#33021;&#21147;&#65292;&#36827;&#32780;&#22686;&#24378;&#20102;&#20854;&#36879;&#26126;&#24230;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.01019</link><description>&lt;p&gt;
&#28304;&#24863;&#30693;&#35757;&#32451;&#20351;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#30693;&#35782;&#24402;&#22240;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Source-Aware Training Enables Knowledge Attribution in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01019
&lt;/p&gt;
&lt;p&gt;
&#28304;&#24863;&#30693;&#35757;&#32451;&#20351;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#30693;&#35782;&#24402;&#22240;&#33021;&#21147;&#65292;&#36827;&#32780;&#22686;&#24378;&#20102;&#20854;&#36879;&#26126;&#24230;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#23398;&#21040;&#20102;&#22823;&#37327;&#30693;&#35782;&#65292;&#20294;&#24448;&#24448;&#23545;&#27492;&#31867;&#30693;&#35782;&#30340;&#26469;&#28304;&#27627;&#19981;&#22312;&#24847;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20869;&#22312;&#28304;&#24341;&#29992;&#38382;&#39064;&#65292;&#35201;&#27714;LLMs&#24341;&#29992;&#25903;&#25345;&#29983;&#25104;&#21709;&#24212;&#30340;&#39044;&#35757;&#32451;&#26469;&#28304;&#12290;&#20869;&#22312;&#28304;&#24341;&#29992;&#21487;&#20197;&#22686;&#24378;LLMs&#30340;&#36879;&#26126;&#24230;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#12290;&#20026;&#36171;&#20104;LLMs&#36825;&#31181;&#33021;&#21147;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#28304;&#24863;&#30693;&#35757;&#32451;&#8212;&#8212;&#19968;&#20010;&#21518;&#39044;&#35757;&#32451;&#37197;&#26041;&#65292;&#21253;&#25324;&#65288;i&#65289;&#35757;&#32451;LLMs&#23558;&#21807;&#19968;&#28304;&#25991;&#26723;&#26631;&#35782;&#31526;&#19982;&#27599;&#20010;&#25991;&#26723;&#20013;&#30340;&#30693;&#35782;&#20851;&#32852;&#36215;&#26469;&#65292;&#28982;&#21518;&#65288;ii&#65289;&#36827;&#34892;&#25351;&#31034;&#35843;&#25972;&#65292;&#25945;&#23548;LLMs&#22312;&#34987;&#25552;&#31034;&#26102;&#24341;&#29992;&#25903;&#25345;&#30340;&#39044;&#35757;&#32451;&#26469;&#28304;&#12290;&#28304;&#24863;&#30693;&#35757;&#32451;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#21363;&#25554;&#21363;&#29992;&#30340;&#39044;&#35757;&#32451;LLMs&#65292;&#24182;&#19982;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;/&#24494;&#35843;&#26694;&#26550;&#30340;&#24046;&#24322;&#26368;&#23567;&#12290;&#36890;&#36807;&#23545;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35757;&#32451;&#37197;&#26041;&#21487;&#20197;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01019v1 Announce Type: cross  Abstract: Large language models (LLMs) learn a vast amount of knowledge during pretraining, but they are often oblivious to the source(s) of such knowledge. We investigate the problem of intrinsic source citation, where LLMs are required to cite the pretraining source supporting a generated response. Intrinsic source citation can enhance LLM transparency, interpretability, and verifiability. To give LLMs such ability, we explore source-aware training -- a post pretraining recipe that involves (i) training the LLM to associate unique source document identifiers with the knowledge in each document, followed by (ii) an instruction-tuning to teach the LLM to cite a supporting pretraining source when prompted. Source-aware training can easily be applied to pretrained LLMs off the shelf, and diverges minimally from existing pretraining/fine-tuning frameworks. Through experiments on carefully curated data, we demonstrate that our training recipe can en
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ViT&#30340;TeethSEG&#26694;&#26550;&#65292;&#21253;&#25324;&#22810;&#23610;&#24230;&#32858;&#21512;&#27169;&#22359;&#21644;&#20154;&#31867;&#20808;&#39564;&#30693;&#35782;&#23618;&#65292;&#20197;&#35299;&#20915;&#29273;&#40831;&#24418;&#29366;&#24046;&#24322;&#12289;&#20301;&#32622;&#21464;&#21270;&#21644;&#24322;&#24120;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.01013</link><description>&lt;p&gt;
Teeth-SEG: &#22522;&#20110;&#20154;&#31867;&#20808;&#39564;&#30693;&#35782;&#30340;&#29273;&#40831;&#27835;&#30103;&#39640;&#25928;&#23454;&#20363;&#20998;&#21106;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Teeth-SEG: An Efficient Instance Segmentation Framework for Orthodontic Treatment based on Anthropic Prior Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01013
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ViT&#30340;TeethSEG&#26694;&#26550;&#65292;&#21253;&#25324;&#22810;&#23610;&#24230;&#32858;&#21512;&#27169;&#22359;&#21644;&#20154;&#31867;&#20808;&#39564;&#30693;&#35782;&#23618;&#65292;&#20197;&#35299;&#20915;&#29273;&#40831;&#24418;&#29366;&#24046;&#24322;&#12289;&#20301;&#32622;&#21464;&#21270;&#21644;&#24322;&#24120;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29273;&#40831;&#22312;2D&#22270;&#20687;&#20013;&#30340;&#23450;&#20301;&#12289;&#20998;&#21106;&#21644;&#26631;&#27880;&#22312;&#29616;&#20195;&#29273;&#31185;&#23398;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#21487;&#20197;&#22686;&#24378;&#29273;&#31185;&#35786;&#26029;&#12289;&#27835;&#30103;&#35268;&#21010;&#20197;&#21450;&#21475;&#33108;&#20581;&#24247;&#30340;&#22522;&#20110;&#20154;&#32676;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19968;&#20123;&#29273;&#40831;&#24418;&#29366;&#20043;&#38388;&#30340;&#32454;&#24494;&#24046;&#24322;&#65288;&#20363;&#22914;&#65292;&#19978;&#39052;&#31532;&#19968;&#21069;&#30952;&#29273;&#21644;&#31532;&#20108;&#21069;&#30952;&#29273;&#65289;&#12289;&#29273;&#40831;&#22312;&#21463;&#35797;&#32773;&#38388;&#30340;&#20301;&#32622;&#21644;&#24418;&#29366;&#21464;&#21270;&#20197;&#21450;&#29273;&#40831;&#24322;&#24120;&#65288;&#20363;&#22914;&#65292;&#40843;&#40831;&#21644;&#32570;&#22833;&#65289;&#30340;&#23384;&#22312;&#65292;&#36890;&#29992;&#23454;&#20363;&#20998;&#21106;&#26694;&#26550;&#25928;&#33021;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ViT&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;TeethSEG&#65292;&#23427;&#30001;&#22534;&#21472;&#30340;&#22810;&#23610;&#24230;&#32858;&#21512;&#65288;MSA&#65289;&#27169;&#22359;&#21644;&#19968;&#20010;&#20154;&#31867;&#20808;&#39564;&#30693;&#35782;&#65288;APK&#65289;&#23618;&#32452;&#25104;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#26500;&#24314;&#36825;&#20004;&#20010;&#27169;&#22359;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;1) &#19968;&#31181;&#29420;&#29305;&#30340;&#22522;&#20110;&#25490;&#21015;&#30340;&#19978;&#37319;&#26679;&#22120;&#65292;&#20197;&#30830;&#20445;&#39640;&#25928;&#24615;&#30340;&#21516;&#26102;&#24314;&#31435;&#28165;&#26224;&#30340;&#20998;&#21106;&#36793;&#30028;&#65292;&#24182;&#20855;&#26377;2) &#22810;&#22836;&#33258;/&#20132;&#21449;&#38376;&#25511;&#23618;&#26469;&#24378;&#35843;&#29305;&#23450;&#30340;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01013v1 Announce Type: cross  Abstract: Teeth localization, segmentation, and labeling in 2D images have great potential in modern dentistry to enhance dental diagnostics, treatment planning, and population-based studies on oral health. However, general instance segmentation frameworks are incompetent due to 1) the subtle differences between some teeth' shapes (e.g., maxillary first premolar and second premolar), 2) the teeth's position and shape variation across subjects, and 3) the presence of abnormalities in the dentition (e.g., caries and edentulism). To address these problems, we propose a ViT-based framework named TeethSEG, which consists of stacked Multi-Scale Aggregation (MSA) blocks and an Anthropic Prior Knowledge (APK) layer. Specifically, to compose the two modules, we design 1) a unique permutation-based upscaler to ensure high efficiency while establishing clear segmentation boundaries with 2) multi-head self/cross-gating layers to emphasize particular semanti
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#30340;&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#20808;&#21069;&#26041;&#27861;&#20013;&#23545;&#19981;&#21516;IR&#35780;&#20272;&#25351;&#26631;&#20934;&#30830;&#24615;&#21644;&#35299;&#37322;&#24615;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2404.01012</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#26469;&#39044;&#27979;&#26597;&#35810;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Query Performance Prediction using Relevance Judgments Generated by Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01012
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#30340;&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#20808;&#21069;&#26041;&#27861;&#20013;&#23545;&#19981;&#21516;IR&#35780;&#20272;&#25351;&#26631;&#20934;&#30830;&#24615;&#21644;&#35299;&#37322;&#24615;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#65288;QPP&#65289;&#26088;&#22312;&#20272;&#35745;&#25628;&#32034;&#31995;&#32479;&#23545;&#26597;&#35810;&#30340;&#26816;&#32034;&#36136;&#37327;&#65292;&#32780;&#26080;&#38656;&#20154;&#24037;&#30456;&#20851;&#24615;&#21028;&#26029;&#12290;&#20808;&#21069;&#30340;QPP&#26041;&#27861;&#36890;&#24120;&#36820;&#22238;&#21333;&#20010;&#26631;&#37327;&#20540;&#65292;&#24182;&#19981;&#35201;&#27714;&#39044;&#27979;&#20540;&#25509;&#36817;&#29305;&#23450;&#30340;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#35780;&#20272;&#25351;&#26631;&#65292;&#20174;&#32780;&#23548;&#33268;&#20197;&#19979;&#26576;&#20123;&#32570;&#28857;&#65306;&#65288;i&#65289;&#21333;&#20010;&#26631;&#37327;&#26080;&#27861;&#20934;&#30830;&#34920;&#31034;&#19981;&#21516;&#30340;IR&#35780;&#20272;&#25351;&#26631;&#65292;&#29305;&#21035;&#26159;&#24403;&#24230;&#37327;&#19981;&#39640;&#24230;&#30456;&#20851;&#26102;&#65292;&#65288;ii&#65289;&#21333;&#20010;&#26631;&#37327;&#38480;&#21046;&#20102;QPP&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#20026;&#20165;&#20351;&#29992;&#26631;&#37327;&#26080;&#27861;&#35299;&#37322;QPP&#32467;&#26524;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#30340;QPP&#26694;&#26550;&#65288;QPP-GenRE&#65289;&#65292;&#23558;QPP&#20998;&#35299;&#20026;&#29420;&#31435;&#30340;&#23376;&#20219;&#21153;&#65292;&#21363;&#23545;&#25490;&#21517;&#21015;&#34920;&#20013;&#27599;&#20010;&#39033;&#30446;&#23545;&#32473;&#23450;&#26597;&#35810;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#21028;&#26029;&#12290;&#36825;&#26679;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#26469;&#39044;&#27979;&#20219;&#20309;IR&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01012v1 Announce Type: cross  Abstract: Query performance prediction (QPP) aims to estimate the retrieval quality of a search system for a query without human relevance judgments. Previous QPP methods typically return a single scalar value and do not require the predicted values to approximate a specific information retrieval (IR) evaluation measure, leading to certain drawbacks: (i) a single scalar is insufficient to accurately represent different IR evaluation measures, especially when metrics do not highly correlate, and (ii) a single scalar limits the interpretability of QPP methods because solely using a scalar is insufficient to explain QPP results. To address these issues, we propose a QPP framework using automatically generated relevance judgments (QPP-GenRE), which decomposes QPP into independent subtasks of judging the relevance of each item in a ranked list to a given query. This allows us to predict any IR evaluation measure using the generated relevance judgment
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25918;&#23556;&#23398;&#25253;&#21578;&#35780;&#20272;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;GPT-4&#23454;&#29616;&#20102;&#19982;&#25918;&#23556;&#31185;&#21307;&#24072;&#35780;&#20272;&#19968;&#33268;&#24615;&#25509;&#36817;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#35757;&#32451;&#30340;&#36739;&#23567;&#27169;&#22411;&#20063;&#36798;&#21040;&#20102;&#31867;&#20284;&#30340;&#35780;&#20272;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.00998</link><description>&lt;p&gt;
LLM-RadJudge: &#23454;&#29616;X&#23556;&#32447;&#25253;&#21578;&#29983;&#25104;&#30340;&#25918;&#23556;&#31185;&#21307;&#24072;&#32423;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
LLM-RadJudge: Achieving Radiologist-Level Evaluation for X-Ray Report Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00998
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25918;&#23556;&#23398;&#25253;&#21578;&#35780;&#20272;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;GPT-4&#23454;&#29616;&#20102;&#19982;&#25918;&#23556;&#31185;&#21307;&#24072;&#35780;&#20272;&#19968;&#33268;&#24615;&#25509;&#36817;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#35757;&#32451;&#30340;&#36739;&#23567;&#27169;&#22411;&#20063;&#36798;&#21040;&#20102;&#31867;&#20284;&#30340;&#35780;&#20272;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25918;&#23556;&#23398;AI&#30340;&#21457;&#23637;&#20013;&#65292;&#35780;&#20272;&#29983;&#25104;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#29616;&#26377;&#25351;&#26631;&#26080;&#27861;&#21453;&#26144;&#20219;&#21153;&#30340;&#20020;&#24202;&#35201;&#27714;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#27604;&#36739;&#25918;&#23556;&#23398;&#25253;&#21578;&#20197;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#21508;&#31181;LLMs&#30340;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#65292;&#24403;&#20351;&#29992;GPT-4&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25351;&#26631;&#23454;&#29616;&#20102;&#25509;&#36817;&#25918;&#23556;&#31185;&#21307;&#24072;&#35780;&#20272;&#19968;&#33268;&#24615;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#38477;&#20302;&#25104;&#26412;&#24182;&#25552;&#39640;&#21487;&#35775;&#38382;&#24615;&#65292;&#20351;&#35813;&#26041;&#27861;&#23454;&#29992;&#21270;&#65292;&#25105;&#20204;&#21033;&#29992;LLM&#35780;&#20272;&#32467;&#26524;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#20197;&#35757;&#32451;&#19968;&#20010;&#36739;&#23567;&#30340;&#27169;&#22411;&#12290;&#33976;&#39311;&#27169;&#22411;&#23454;&#29616;&#20102;&#19982;GPT-4&#30456;&#24403;&#30340;&#35780;&#20272;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21644;&#33976;&#39311;&#27169;&#22411;&#20026;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#35775;&#38382;&#19988;&#39640;&#25928;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20419;&#36827;&#20102;&#24320;&#21457;&#26356;&#20855;&#20020;&#24202;&#30456;&#20851;&#24615;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#23558;&#36827;&#19968;&#27493;&#24320;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00998v1 Announce Type: cross  Abstract: Evaluating generated radiology reports is crucial for the development of radiology AI, but existing metrics fail to reflect the task's clinical requirements. This study proposes a novel evaluation framework using large language models (LLMs) to compare radiology reports for assessment. We compare the performance of various LLMs and demonstrate that, when using GPT-4, our proposed metric achieves evaluation consistency close to that of radiologists. Furthermore, to reduce costs and improve accessibility, making this method practical, we construct a dataset using LLM evaluation results and perform knowledge distillation to train a smaller model. The distilled model achieves evaluation capabilities comparable to GPT-4. Our framework and distilled model offer an accessible and efficient evaluation method for radiology report generation, facilitating the development of more clinically relevant models. The model will be further open-sourced 
&lt;/p&gt;</description></item><item><title>&#35813;&#25968;&#25454;&#38598;&#26159;&#31532;&#19968;&#20010;&#34701;&#21512;&#22810;&#20010;&#35270;&#35282;&#21644;&#22810;&#20010;&#25968;&#25454;&#27169;&#24577;&#20197;&#27169;&#25311;&#29616;&#23454;&#19990;&#30028;&#20013;&#26085;&#24120;&#20449;&#24687;&#33719;&#21462;&#26041;&#24335;&#30340;&#25968;&#25454;&#24211;&#12290;</title><link>https://arxiv.org/abs/2404.00989</link><description>&lt;p&gt;
360+x&#65306;&#19968;&#20010;&#20840;&#26223;&#22810;&#27169;&#24577;&#22330;&#26223;&#29702;&#35299;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
360+x: A Panoptic Multi-modal Scene Understanding Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00989
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25968;&#25454;&#38598;&#26159;&#31532;&#19968;&#20010;&#34701;&#21512;&#22810;&#20010;&#35270;&#35282;&#21644;&#22810;&#20010;&#25968;&#25454;&#27169;&#24577;&#20197;&#27169;&#25311;&#29616;&#23454;&#19990;&#30028;&#20013;&#26085;&#24120;&#20449;&#24687;&#33719;&#21462;&#26041;&#24335;&#30340;&#25968;&#25454;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#23545;&#19990;&#30028;&#30340;&#24863;&#30693;&#21463;&#21040;&#22810;&#31181;&#35270;&#35282;&#21644;&#27169;&#24577;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#35768;&#22810;&#29616;&#26377;&#25968;&#25454;&#38598;&#20391;&#37325;&#20110;&#20174;&#26576;&#31181;&#35270;&#35282;&#65288;&#20363;&#22914;&#33258;&#25105;&#20013;&#24515;&#25110;&#31532;&#19977;&#20154;&#31216;&#35270;&#35282;&#65289;&#29702;&#35299;&#22330;&#26223;&#65292;&#20294;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#26223;&#35270;&#35282;&#65288;&#21363;&#22810;&#35270;&#35282;&#21644;&#22810;&#25968;&#25454;&#27169;&#24577;&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25429;&#25417;&#20102;&#31532;&#19977;&#20154;&#31216;&#20840;&#26223;&#21644;&#21069;&#35270;&#22270;&#65292;&#20197;&#21450;&#20855;&#26377;&#35270;&#39057;&#12289;&#22810;&#36890;&#36947;&#38899;&#39057;&#12289;&#23450;&#21521;&#21452;&#32819;&#24310;&#36831;&#12289;&#20301;&#32622;&#25968;&#25454;&#21644;&#25991;&#26412;&#22330;&#26223;&#25551;&#36848;&#31561;&#20016;&#23500;&#27169;&#24577;&#30340;&#33258;&#25105;&#30340;&#21333;&#30524;/&#21452;&#30524;&#35270;&#22270;&#65292;&#21576;&#29616;&#20102;&#23545;&#19990;&#30028;&#30340;&#20840;&#38754;&#35266;&#23519;&#12290;&#22270;1&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;360+x&#25968;&#25454;&#38598;&#30340;&#25152;&#26377;28&#20010;&#22330;&#26223;&#31867;&#21035;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#28085;&#30422;&#22810;&#20010;&#35270;&#35282;&#21644;&#22810;&#20010;&#25968;&#25454;&#27169;&#24577;&#30340;&#25968;&#25454;&#24211;&#65292;&#27169;&#25311;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#26085;&#24120;&#20449;&#24687;&#30340;&#33719;&#21462;&#26041;&#24335;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#22522;&#20934;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;5&#31181;&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00989v1 Announce Type: cross  Abstract: Human perception of the world is shaped by a multitude of viewpoints and modalities. While many existing datasets focus on scene understanding from a certain perspective (e.g. egocentric or third-person views), our dataset offers a panoptic perspective (i.e. multiple viewpoints with multiple data modalities). Specifically, we encapsulate third-person panoramic and front views, as well as egocentric monocular/binocular views with rich modalities including video, multi-channel audio, directional binaural delay, location data and textual scene descriptions within each scene captured, presenting comprehensive observation of the world. Figure 1 offers a glimpse of all 28 scene categories of our 360+x dataset. To the best of our knowledge, this is the first database that covers multiple viewpoints with multiple data modalities to mimic how daily information is accessed in the real world. Through our benchmark analysis, we presented 5 differe
&lt;/p&gt;</description></item><item><title>&#35813;&#35843;&#30740;&#32508;&#21512;&#23457;&#26597;&#20102;&#26234;&#24935;&#22478;&#24066;&#21457;&#23637;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#20869;&#23481;&#28085;&#30422;&#20102;&#26041;&#27861;&#35770;&#20998;&#31867;&#12289;&#22810;&#31181;&#24212;&#29992;&#39046;&#22495;&#21644;&#30456;&#20851;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2404.00983</link><description>&lt;p&gt;
&#26234;&#24935;&#22478;&#24066;&#30340;&#25345;&#32493;&#23398;&#20064;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Continual Learning for Smart City: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00983
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35843;&#30740;&#32508;&#21512;&#23457;&#26597;&#20102;&#26234;&#24935;&#22478;&#24066;&#21457;&#23637;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#20869;&#23481;&#28085;&#30422;&#20102;&#26041;&#27861;&#35770;&#20998;&#31867;&#12289;&#22810;&#31181;&#24212;&#29992;&#39046;&#22495;&#21644;&#30456;&#20851;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;&#22478;&#24066;&#25968;&#23383;&#21270;&#65292;&#24222;&#22823;&#30340;&#25968;&#25454;&#37327;&#21644;&#24378;&#22823;&#30340;&#35745;&#31639;&#36164;&#28304;&#20419;&#20351;&#26234;&#24935;&#22478;&#24066;&#20013;&#37096;&#32626;&#30340;&#26234;&#33021;&#27169;&#22411;&#36805;&#36895;&#26356;&#26032;&#12290;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#19981;&#26029;&#26356;&#26032;&#27169;&#22411;&#20197;&#36866;&#24212;&#21464;&#21270;&#30340;&#29615;&#22659;&#65292;&#20854;&#20013;&#23398;&#20064;&#20219;&#21153;&#12289;&#25968;&#25454;&#21644;&#20998;&#24067;&#21487;&#20197;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#20840;&#38754;&#23457;&#26597;&#20102;&#22312;&#26234;&#24935;&#22478;&#24066;&#21457;&#23637;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#12290;&#20869;&#23481;&#20998;&#20026;&#19977;&#20010;&#37096;&#20998;&#65306;1&#65289;&#26041;&#27861;&#35770;&#12290;&#25105;&#20204;&#23558;&#22823;&#37327;&#22522;&#26412;&#30340;CL&#26041;&#27861;&#21644;&#32467;&#21512;&#20854;&#20182;&#23398;&#20064;&#33539;&#20363;&#30340;&#39640;&#32423;CL&#26694;&#26550;&#36827;&#34892;&#20998;&#31867;&#65292;&#21253;&#25324;&#22270;&#23398;&#20064;&#12289;&#26102;&#31354;&#23398;&#20064;&#12289;&#22810;&#27169;&#24577;&#23398;&#20064;&#21644;&#32852;&#37030;&#23398;&#20064;&#12290;2&#65289;&#24212;&#29992;&#26041;&#38754;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#28085;&#30422;&#20132;&#36890;&#12289;&#29615;&#22659;&#12289;&#20844;&#20849;&#21355;&#29983;&#12289;&#23433;&#20840;&#12289;&#32593;&#32476;&#20197;&#21450;&#19982;&#22478;&#24066;&#35745;&#31639;&#30456;&#20851;&#30340;&#30456;&#20851;&#25968;&#25454;&#38598;&#30340;&#20247;&#22810;CL&#24212;&#29992;&#12290;3&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00983v1 Announce Type: cross  Abstract: With the digitization of modern cities, large data volumes and powerful computational resources facilitate the rapid update of intelligent models deployed in smart cities. Continual learning (CL) is a novel machine learning paradigm that constantly updates models to adapt to changing environments, where the learning tasks, data, and distributions can vary over time. Our survey provides a comprehensive review of continual learning methods that are widely used in smart city development. The content consists of three parts: 1) Methodology-wise. We categorize a large number of basic CL methods and advanced CL frameworks in combination with other learning paradigms including graph learning, spatial-temporal learning, multi-modal learning, and federated learning. 2) Application-wise. We present numerous CL applications covering transportation, environment, public health, safety, networks, and associated datasets related to urban computing. 3
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#31639;&#27861;IPF&#65292;&#29992;&#20110;&#22478;&#24066;&#35268;&#21010;&#65292;&#33021;&#22815;&#39044;&#27979;&#19981;&#21516;&#31038;&#20250;&#25110;&#25919;&#27835;&#21033;&#30410;&#30456;&#20851;&#21442;&#25968;&#22312;&#35268;&#21010;&#36807;&#31243;&#20013;&#30340;&#21457;&#23637;&#65292;&#24050;&#22312;&#38899;&#20048;&#20048;&#22120;&#27169;&#25311;&#12289;&#33041;&#21160;&#21147;&#23398;&#21644;&#20154;&#38469;&#20114;&#21160;&#20013;&#34920;&#29616;&#20986;&#39640;&#39044;&#27979;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.00977</link><description>&lt;p&gt;
&#38024;&#23545;&#22478;&#24066;&#35268;&#21010;&#21644;&#20844;&#20247;&#21442;&#19982;&#30340;&#38750;&#32447;&#24615;&#33033;&#20914;&#27169;&#24335;&#21046;&#23450;&#21160;&#24577;&#31038;&#20250;&#25919;&#27835;&#39044;&#27979;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Nonlinear Impulse Pattern Formulation dynamical social and political prediction algorithm for city planning and public participation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00977
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#31639;&#27861;IPF&#65292;&#29992;&#20110;&#22478;&#24066;&#35268;&#21010;&#65292;&#33021;&#22815;&#39044;&#27979;&#19981;&#21516;&#31038;&#20250;&#25110;&#25919;&#27835;&#21033;&#30410;&#30456;&#20851;&#21442;&#25968;&#22312;&#35268;&#21010;&#36807;&#31243;&#20013;&#30340;&#21457;&#23637;&#65292;&#24050;&#22312;&#38899;&#20048;&#20048;&#22120;&#27169;&#25311;&#12289;&#33041;&#21160;&#21147;&#23398;&#21644;&#20154;&#38469;&#20114;&#21160;&#20013;&#34920;&#29616;&#20986;&#39640;&#39044;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22478;&#24066;&#35268;&#21010;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#31639;&#27861;&#65292;&#21363;&#33033;&#20914;&#27169;&#24335;&#21046;&#23450;&#65288;IPF&#65289;&#65292;&#29992;&#20110;&#39044;&#27979;&#19981;&#21516;&#31038;&#20250;&#25110;&#25919;&#27835;&#21033;&#30410;&#30456;&#20851;&#21442;&#25968;&#65288;&#22914;&#20581;&#24247;&#12289;&#33402;&#26415;&#33258;&#30001;&#25110;&#36130;&#21153;&#21457;&#23637;&#65289;&#22312;&#35268;&#21010;&#36807;&#31243;&#20013;&#30340;&#21457;&#23637;&#12290;IPF&#24050;&#32463;&#22312;&#38899;&#20048;&#20048;&#22120;&#27169;&#25311;&#12289;&#33041;&#21160;&#21147;&#23398;&#21644;&#20154;&#38469;&#20114;&#21160;&#20013;&#34920;&#29616;&#20986;&#39640;&#39044;&#27979;&#31934;&#24230;&#65292;&#32780;&#35745;&#31639;&#25104;&#26412;&#20302;&#24265;&#12290;&#31038;&#20250;&#21644;&#25919;&#27835;IPF&#30001;&#19977;&#20010;&#22522;&#26412;&#26041;&#31243;&#32452;&#25104;&#65292;&#29992;&#20110;&#31995;&#32479;&#29366;&#24577;&#21457;&#23637;&#12289;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#33258;&#36866;&#24212;&#12289;&#20004;&#31181;&#33258;&#36866;&#24212;&#20114;&#21160;&#21644;&#36866;&#21512;&#20110;&#19981;&#21516;&#35268;&#21010;&#24773;&#22659;&#30340;&#22806;&#37096;&#20914;&#20987;&#39033;&#12290;&#36890;&#36807;&#35843;&#25972;&#19968;&#32452;&#31995;&#32479;&#21442;&#25968;&#65292;&#23545;&#21033;&#30410;&#30456;&#20851;&#32773;&#20114;&#21160;&#21644;&#21457;&#23637;&#30340;&#20856;&#22411;&#24773;&#26223;&#36827;&#34892;&#24314;&#27169;&#65292;&#21253;&#25324;&#21033;&#30410;&#30456;&#20851;&#32773;&#23545;&#22806;&#37096;&#36755;&#20837;&#30340;&#21453;&#24212;&#12289;&#33258;&#36866;&#24212;&#36890;&#36807;&#25552;&#39640;&#31995;&#32479;&#31283;&#23450;&#24615;&#12289;&#21033;&#30410;&#30456;&#20851;&#32773;&#22240;&#35843;&#35299;&#20114;&#21160;&#33258;&#36866;&#24212;&#32780;&#25910;&#25947;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00977v1 Announce Type: cross  Abstract: A nonlinear-dynamical algorithm for city planning is proposed as an Impulse Pattern Formulation (IPF) for predicting relevant parameters like health, artistic freedom, or financial developments of different social or political stakeholders over the cause of a planning process. The IPF has already shown high predictive precision at low computational cost in musical instrument simulations, brain dynamics, and human-human interactions. The social and political IPF consists of three basic equations of system state developments, self-adaptation of stakeholders, two adaptive interactions, and external impact terms suitable for respective planning situations. Typical scenarios of stakeholder interactions and developments are modeled by adjusting a set of system parameters. These include stakeholder reaction to external input, enhanced system stability through self-adaptation, stakeholder convergence due to mediative interaction adaptation, as
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20027;&#39064;&#20998;&#26512;&#23545;LLM&#29983;&#25104;&#30340;&#20195;&#30721;&#20013;&#30340;&#24187;&#35273;&#36827;&#34892;&#20102;&#24635;&#32467;&#21644;&#20998;&#31867;&#65292;&#24314;&#31435;&#20102;&#20195;&#30721;&#20013;&#24187;&#35273;&#30340;&#20840;&#38754;&#20998;&#31867;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.00971</link><description>&lt;p&gt;
&#25506;&#32034;&#21644;&#35780;&#20272;LLM&#39537;&#21160;&#30340;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Exploring and Evaluating Hallucinations in LLM-Powered Code Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20027;&#39064;&#20998;&#26512;&#23545;LLM&#29983;&#25104;&#30340;&#20195;&#30721;&#20013;&#30340;&#24187;&#35273;&#36827;&#34892;&#20102;&#24635;&#32467;&#21644;&#20998;&#31867;&#65292;&#24314;&#31435;&#20102;&#20195;&#30721;&#20013;&#24187;&#35273;&#30340;&#20840;&#38754;&#20998;&#31867;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23835;&#36215;&#24050;&#32463;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#20013;&#35768;&#22810;&#24212;&#29992;&#30340;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#12290;&#23613;&#31649;&#34920;&#29616;&#20986;&#33394;&#65292;LLMs&#23481;&#26131;&#20135;&#29983;&#24187;&#35273;&#65292;&#21363;LLMs&#21487;&#33021;&#20135;&#29983;&#19982;&#29992;&#25143;&#24847;&#22270;&#20559;&#31163;&#12289;&#34920;&#29616;&#20986;&#20869;&#37096;&#19981;&#19968;&#33268;&#25110;&#19982;&#20107;&#23454;&#30693;&#35782;&#19981;&#31526;&#30340;&#36755;&#20986;&#65292;&#20351;&#24471;&#22312;&#24191;&#27867;&#24212;&#29992;&#20013;&#37096;&#32626;LLMs&#21487;&#33021;&#23384;&#22312;&#39118;&#38505;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#39046;&#22495;&#30340;&#24187;&#35273;&#65292;&#32570;&#20047;&#23545;&#20195;&#30721;&#29983;&#25104;&#29615;&#22659;&#20013;&#24187;&#35273;&#31867;&#22411;&#21644;&#31243;&#24230;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#23545;LLM&#29983;&#25104;&#30340;&#20195;&#30721;&#36827;&#34892;&#20102;&#20027;&#39064;&#20998;&#26512;&#65292;&#24635;&#32467;&#21644;&#24402;&#31867;&#20854;&#20013;&#23384;&#22312;&#30340;&#24187;&#35273;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24314;&#31435;&#20102;LLM&#29983;&#25104;&#30340;&#20195;&#30721;&#20013;&#24187;&#35273;&#30340;&#20840;&#38754;&#20998;&#31867;&#27861;&#65292;&#28085;&#30422;&#20102;5&#20010;&#20027;&#35201;&#24187;&#35273;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00971v1 Announce Type: cross  Abstract: The rise of Large Language Models (LLMs) has significantly advanced many applications on software engineering tasks, particularly in code generation. Despite the promising performance, LLMs are prone to generate hallucinations, which means LLMs might produce outputs that deviate from users' intent, exhibit internal inconsistencies, or misalign with the factual knowledge, making the deployment of LLMs potentially risky in a wide range of applications. Existing work mainly focuses on investing the hallucination in the domain of natural language generation (NLG), leaving a gap in understanding the types and extent of hallucinations in the context of code generation. To bridge the gap, we conducted a thematic analysis of the LLM-generated code to summarize and categorize the hallucinations present in it. Our study established a comprehensive taxonomy of hallucinations in LLM-generated code, encompassing 5 primary categories of hallucinatio
&lt;/p&gt;</description></item><item><title>Evalverse&#26159;&#19968;&#20010;&#32479;&#19968;&#21644;&#26131;&#29992;&#30340;&#24211;&#65292;&#31616;&#21270;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#38598;&#20013;&#19988;&#26131;&#20110;&#35775;&#38382;&#30340;&#35780;&#20272;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2404.00943</link><description>&lt;p&gt;
Evalverse: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#32479;&#19968;&#21644;&#26131;&#29992;&#24211;
&lt;/p&gt;
&lt;p&gt;
Evalverse: Unified and Accessible Library for Large Language Model Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00943
&lt;/p&gt;
&lt;p&gt;
Evalverse&#26159;&#19968;&#20010;&#32479;&#19968;&#21644;&#26131;&#29992;&#30340;&#24211;&#65292;&#31616;&#21270;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#38598;&#20013;&#19988;&#26131;&#20110;&#35775;&#38382;&#30340;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Evalverse&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#24211;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#30340;&#35780;&#20272;&#24037;&#20855;&#32479;&#19968;&#21040;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#26694;&#26550;&#20013;&#65292;&#31616;&#21270;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#12290;Evalverse&#20351;&#24471;&#23545;&#20154;&#24037;&#26234;&#33021;&#20102;&#35299;&#26377;&#38480;&#30340;&#20010;&#20154;&#21487;&#20197;&#36731;&#26494;&#35831;&#27714;LLMs&#35780;&#20272;&#24182;&#25910;&#21040;&#35814;&#32454;&#25253;&#21578;&#65292;&#21033;&#29992;&#19982;Slack&#31561;&#36890;&#20449;&#24179;&#21488;&#30340;&#38598;&#25104;&#12290;&#22240;&#27492;&#65292;Evalverse&#20316;&#20026;LLMs&#30340;&#20840;&#38754;&#35780;&#20272;&#24378;&#22823;&#24037;&#20855;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#38598;&#20013;&#19988;&#26131;&#20110;&#35775;&#38382;&#30340;&#35780;&#20272;&#26694;&#26550;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;Evalverse&#30340;&#28436;&#31034;&#35270;&#39057;&#65292;&#23637;&#31034;&#20102;&#23427;&#30340;&#21151;&#33021;&#21644;&#23454;&#29616;&#26041;&#24335;&#65292;&#20197;&#20004;&#20998;&#38047;&#30340;&#26684;&#24335;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00943v1 Announce Type: cross  Abstract: This paper introduces Evalverse, a novel library that streamlines the evaluation of Large Language Models (LLMs) by unifying disparate evaluation tools into a single, user-friendly framework. Evalverse enables individuals with limited knowledge of artificial intelligence to easily request LLM evaluations and receive detailed reports, facilitated by an integration with communication platforms like Slack. Thus, Evalverse serves as a powerful tool for the comprehensive assessment of LLMs, offering both researchers and practitioners a centralized and easily accessible evaluation framework. Finally, we also provide a demo video for Evalverse, showcasing its capabilities and implementation in a two-minute format.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#22411;&#30693;&#35782;&#22270;&#35889;&#21019;&#24314;&#35780;&#20272;&#27169;&#22411;&#20197;&#26816;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#24615;&#19978;&#30340;&#34920;&#29616;&#65292;&#26377;&#25928;&#38477;&#20302;&#35780;&#20272;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2404.00942</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#35889;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20107;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Factuality of Large Language Models using Large-Scale Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00942
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#30693;&#35782;&#22270;&#35889;&#21019;&#24314;&#35780;&#20272;&#27169;&#22411;&#20197;&#26816;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#24615;&#19978;&#30340;&#34920;&#29616;&#65292;&#26377;&#25928;&#38477;&#20302;&#35780;&#20272;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#26174;&#33879;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#22686;&#24378;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#33021;&#21147;&#12290;&#20107;&#23454;&#24615;&#38382;&#39064;&#23545;LLMs&#26469;&#35828;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#29983;&#25104;&#20107;&#23454;&#19981;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;GraphEval&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#27979;&#35797;&#25968;&#25454;&#38598;&#23545;LLM&#30340;&#24615;&#33021;&#36827;&#34892;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#27979;&#35797;&#25968;&#25454;&#38598;&#26159;&#20174;&#25317;&#26377;&#36229;&#36807;1000&#19975;&#20010;&#20107;&#23454;&#30340;&#22823;&#22411;&#30693;&#35782;&#22270;&#35889;&#20013;&#26816;&#32034;&#32780;&#26469;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;&#20154;&#21147;&#25104;&#26412;&#12290;&#19982;&#22522;&#20110;&#29983;&#25104;&#21709;&#24212;&#35780;&#20272;LLMs&#30340;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;GraphEval&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#35780;&#20272;&#27169;&#22411;&#31616;&#21270;&#20102;&#35780;&#20272;&#36807;&#31243;&#65292;&#29992;&#20110;&#20272;&#35745;LLM&#32473;&#20986;&#30340;&#31572;&#26696;&#30340;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35780;&#20272;&#27169;&#22411;&#30340;&#20107;&#23454;&#24615;&#35780;&#20272;&#19982;LLM&#29983;&#25104;&#30340;&#36755;&#20986;&#30340;&#27491;&#30830;&#24615;&#23494;&#20999;&#30456;&#20851;&#65292;&#21516;&#26102;&#26174;&#33879;&#38477;&#20302;&#20102;&#35780;&#20272;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;LLM&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00942v1 Announce Type: cross  Abstract: The advent of Large Language Models (LLMs) has significantly transformed the AI landscape, enhancing machine learning and AI capabilities. Factuality issue is a critical concern for LLMs, as they may generate factually incorrect responses. In this paper, we propose GraphEval to evaluate an LLM's performance using a substantially large test dataset. Specifically, the test dataset is retrieved from a large knowledge graph with more than 10 million facts without expensive human efforts. Unlike conventional methods that evaluate LLMs based on generated responses, GraphEval streamlines the evaluation process by creating a judge model to estimate the correctness of the answers given by the LLM. Our experiments demonstrate that the judge model's factuality assessment aligns closely with the correctness of the LLM's generated outputs, while also substantially reducing evaluation costs. Besides, our findings offer valuable insights into LLM per
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#28145;&#20837;&#35752;&#35770;&#20102;&#20851;&#38190;&#38382;&#39064;&#65292;&#21253;&#25324;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#12289;&#23545;&#40784;&#21644;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2404.00929</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#35821;&#26009;&#24211;&#12289;&#23545;&#40784;&#21644;&#20559;&#35265;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00929
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#28145;&#20837;&#35752;&#35770;&#20102;&#20851;&#38190;&#38382;&#39064;&#65292;&#21253;&#25324;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#12289;&#23545;&#40784;&#21644;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22522;&#30784;&#19978;&#65292;&#21457;&#23637;&#20102;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#26469;&#35299;&#20915;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#25361;&#25112;&#65292;&#24076;&#26395;&#23454;&#29616;&#20174;&#39640;&#36164;&#28304;&#21040;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#37325;&#35201;&#38480;&#21046;&#21644;&#25361;&#25112;&#65292;&#27604;&#22914;&#35821;&#35328;&#19981;&#24179;&#34913;&#12289;&#22810;&#35821;&#35328;&#23545;&#40784;&#21644;&#22266;&#26377;&#20559;&#35265;&#12290;&#26412;&#25991;&#26088;&#22312;&#23545;MLLMs&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#28145;&#20837;&#35752;&#35770;&#22260;&#32469;&#36825;&#20123;&#20851;&#38190;&#38382;&#39064;&#30340;&#35758;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00929v1 Announce Type: cross  Abstract: Based on the foundation of Large Language Models (LLMs), Multilingual Large Language Models (MLLMs) have been developed to address the challenges of multilingual natural language processing tasks, hoping to achieve knowledge transfer from high-resource to low-resource languages. However, significant limitations and challenges still exist, such as language imbalance, multilingual alignment, and inherent bias. In this paper, we aim to provide a comprehensive analysis of MLLMs, delving deeply into discussions surrounding these critical issues. First of all, we start by presenting an overview of MLLMs, covering their evolution, key techniques, and multilingual capacities. Secondly, we explore widely utilized multilingual corpora for MLLMs' training and multilingual datasets oriented for downstream tasks that are crucial for enhancing the cross-lingual capability of MLLMs. Thirdly, we survey the existing studies on multilingual representati
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22810;&#27169;&#24577;3D&#39640;&#26031;&#38632;&#28404;&#27861;SLAM&#21487;&#20197;&#32467;&#21512;&#26410;&#23450;&#20301;&#30456;&#26426;&#22270;&#20687;&#21644;&#24815;&#24615;&#27979;&#37327;&#65292;&#23454;&#29616;&#20934;&#30830;&#30340;&#22320;&#22270;&#24314;&#31435;&#21644;&#36712;&#36857;&#36319;&#36394;&#65292;&#20855;&#26377;&#26356;&#24555;&#30340;&#28210;&#26579;&#36895;&#24230;&#12289;&#23610;&#24230;&#24863;&#30693;&#24615;&#21644;&#25913;&#36827;&#30340;&#36712;&#36857;&#36861;&#36394;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.00923</link><description>&lt;p&gt;
MM3DGS SLAM: &#21033;&#29992;&#35270;&#35273;&#12289;&#28145;&#24230;&#21644;&#24815;&#24615;&#27979;&#37327;&#30340;&#22810;&#27169;&#24577;3D&#39640;&#26031;&#38632;&#28404;&#27861;SLAM
&lt;/p&gt;
&lt;p&gt;
MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision, Depth, and Inertial Measurements
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00923
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22810;&#27169;&#24577;3D&#39640;&#26031;&#38632;&#28404;&#27861;SLAM&#21487;&#20197;&#32467;&#21512;&#26410;&#23450;&#20301;&#30456;&#26426;&#22270;&#20687;&#21644;&#24815;&#24615;&#27979;&#37327;&#65292;&#23454;&#29616;&#20934;&#30830;&#30340;&#22320;&#22270;&#24314;&#31435;&#21644;&#36712;&#36857;&#36319;&#36394;&#65292;&#20855;&#26377;&#26356;&#24555;&#30340;&#28210;&#26579;&#36895;&#24230;&#12289;&#23610;&#24230;&#24863;&#30693;&#24615;&#21644;&#25913;&#36827;&#30340;&#36712;&#36857;&#36861;&#36394;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#27493;&#23450;&#20301;&#19982;&#24314;&#22270;&#23545;&#20110;&#20301;&#32622;&#36319;&#36394;&#21644;&#22330;&#26223;&#29702;&#35299;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;3D&#39640;&#26031;&#30340;&#22320;&#22270;&#34920;&#31034;&#20351;&#24471;&#33021;&#22815;&#21033;&#29992;&#22810;&#23039;&#24577;&#30456;&#26426;&#23454;&#29616;&#36924;&#30495;&#37325;&#24314;&#21644;&#23454;&#26102;&#28210;&#26579;&#22330;&#26223;&#12290;&#25105;&#20204;&#39318;&#27425;&#23637;&#31034;&#65292;&#21033;&#29992;3D&#39640;&#26031;&#36827;&#34892;&#22320;&#22270;&#34920;&#31034;&#65292;&#24182;&#32467;&#21512;&#26410;&#23450;&#20301;&#30456;&#26426;&#22270;&#20687;&#21644;&#24815;&#24615;&#27979;&#37327;&#65292;&#21487;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;SLAM&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;MM3DGS&#36890;&#36807;&#23454;&#29616;&#26356;&#24555;&#30340;&#28210;&#26579;&#12289;&#23610;&#24230;&#24863;&#30693;&#21644;&#25913;&#36827;&#30340;&#36712;&#36857;&#36319;&#36394;&#65292;&#35299;&#20915;&#20102;&#20808;&#21069;&#22522;&#20110;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#34920;&#31034;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23454;&#29616;&#20102;&#22522;&#20110;&#20851;&#38190;&#24103;&#30340;&#24314;&#22270;&#21644;&#36319;&#36394;&#65292;&#21033;&#29992;&#21253;&#21547;&#30456;&#23545;&#23039;&#24577;&#21464;&#25442;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36825;&#20123;&#23039;&#24577;&#21464;&#25442;&#26469;&#33258;&#39044;&#31215;&#20998;&#30340;&#24815;&#24615;&#27979;&#37327;&#12289;&#28145;&#24230;&#20272;&#35745;&#21644;&#20809;&#24230;&#28210;&#26579;&#36136;&#37327;&#30340;&#24230;&#37327;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;UT-MM&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;&#37197;&#22791;&#30456;&#26426;&#21644;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#25910;&#38598;&#32780;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00923v1 Announce Type: cross  Abstract: Simultaneous localization and mapping is essential for position tracking and scene understanding. 3D Gaussian-based map representations enable photorealistic reconstruction and real-time rendering of scenes using multiple posed cameras. We show for the first time that using 3D Gaussians for map representation with unposed camera images and inertial measurements can enable accurate SLAM. Our method, MM3DGS, addresses the limitations of prior neural radiance field-based representations by enabling faster rendering, scale awareness, and improved trajectory tracking. Our framework enables keyframe-based mapping and tracking utilizing loss functions that incorporate relative pose transformations from pre-integrated inertial measurements, depth estimates, and measures of photometric rendering quality. We also release a multi-modal dataset, UT-MM, collected from a mobile robot equipped with a camera and an inertial measurement unit. Experimen
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Token-Efficient Leverage Learning&#65288;TELL&#65289;&#30340;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23637;&#31034;&#20102;&#20854;&#38477;&#20302;&#20219;&#21153;&#25968;&#25454;&#38656;&#27714;&#12289;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#30340;&#28508;&#21147;&#65292;&#20026;&#20302;&#36164;&#28304;&#20219;&#21153;&#24102;&#26469;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00914</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#39640;&#25928;&#20196;&#29260;&#21033;&#29992;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Token-Efficient Leverage Learning in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00914
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Token-Efficient Leverage Learning&#65288;TELL&#65289;&#30340;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23637;&#31034;&#20102;&#20854;&#38477;&#20302;&#20219;&#21153;&#25968;&#25454;&#38656;&#27714;&#12289;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#30340;&#28508;&#21147;&#65292;&#20026;&#20302;&#36164;&#28304;&#20219;&#21153;&#24102;&#26469;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#39640;&#36164;&#28304;&#22330;&#26223;&#20013;&#34920;&#29616;&#26356;&#20339;&#65292;&#36825;&#22312;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#23384;&#22312;&#25361;&#25112;&#12290;&#25968;&#25454;&#31232;&#32570;&#21644;LLMs&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#22266;&#26377;&#30340;&#22256;&#38590;&#21152;&#21095;&#20102;&#36825;&#19968;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#22823;&#38590;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;\textbf{Leverage Learning}&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#19968;&#26041;&#27861;&#30340;&#31616;&#21270;&#23454;&#29616;&#65292;&#31216;&#20026;Token-Efficient Leverage Learning (TELL)&#12290;TELL&#23637;&#31034;&#20102;Leverage Learning&#30340;&#28508;&#21147;&#65292;&#35777;&#26126;&#23427;&#22312;&#21508;&#31181;LLMs&#21644;&#20302;&#36164;&#28304;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#20174;$10^4$&#21040;$10^6$&#20010;&#20196;&#29260;&#19981;&#31561;&#12290;&#19982;&#20256;&#32479;&#30340;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#30456;&#27604;&#65292;&#23427;&#23558;&#20219;&#21153;&#25968;&#25454;&#38656;&#27714;&#38477;&#20302;&#20102;&#36817;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#21516;&#26102;&#25552;&#20379;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#22312;&#30456;&#21516;&#37327;&#30340;&#20219;&#21153;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;TELL&#22312;&#25913;&#21892;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#39046;&#20808;&#20110;SFT&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;Leverage Learning&#30340;&#26426;&#21046;&#65292;&#26263;&#31034;&#20854;&#31526;&#21512;&#37327;&#21270;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00914v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have excelled in various tasks but perform better in high-resource scenarios, which presents challenges in low-resource scenarios. Data scarcity and the inherent difficulty of adapting LLMs to specific tasks compound the challenge. To address the twin hurdles, we introduce \textbf{Leverage Learning}. We present a streamlined implement of this methodology called Token-Efficient Leverage Learning (TELL). TELL showcases the potential of Leverage Learning, demonstrating effectiveness across various LLMs and low-resource tasks, ranging from $10^4$ to $10^6$ tokens. It reduces task data requirements by up to nearly an order of magnitude compared to conventional Supervised Fine-Tuning (SFT) while delivering competitive performance. With the same amount of task data, TELL leads in improving task performance compared to SFT. We discuss the mechanism of Leverage Learning, suggesting it aligns with quantization hypoth
&lt;/p&gt;</description></item><item><title>LLaMA-Excitor&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#28176;&#26356;&#22810;&#22320;&#20851;&#27880;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#26469;&#28608;&#21457;LLM&#26356;&#22909;&#22320;&#36981;&#24490;&#25351;&#20196;&#65292;&#32780;&#19981;&#30452;&#25509;&#25913;&#21464;&#20013;&#38388;&#38544;&#34255;&#29366;&#24577;&#65292;&#26377;&#25928;&#20445;&#30041;&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2404.00913</link><description>&lt;p&gt;
LLaMA-Excitor: &#36890;&#36807;&#38388;&#25509;&#29305;&#24449;&#20132;&#20114;&#36827;&#34892;&#36890;&#29992;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
LLaMA-Excitor: General Instruction Tuning via Indirect Feature Interaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00913
&lt;/p&gt;
&lt;p&gt;
LLaMA-Excitor&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#28176;&#26356;&#22810;&#22320;&#20851;&#27880;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#26469;&#28608;&#21457;LLM&#26356;&#22909;&#22320;&#36981;&#24490;&#25351;&#20196;&#65292;&#32780;&#19981;&#30452;&#25509;&#25913;&#21464;&#20013;&#38388;&#38544;&#34255;&#29366;&#24577;&#65292;&#26377;&#25928;&#20445;&#30041;&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#29992;&#20110;&#24494;&#35843;LLM&#65288;&#22914;Adapter&#12289;Prefix-tuning&#21644;LoRA&#65289;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#27169;&#22359;&#25110;&#38468;&#21152;&#36755;&#20837;&#24207;&#21015;&#65292;&#20197;&#27880;&#20837;&#26032;&#25216;&#33021;&#25110;&#30693;&#35782;&#65292;&#20294;&#21487;&#33021;&#20250;&#25439;&#23475;LLM&#30340;&#22266;&#26377;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA-Excitor&#65292;&#36825;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#28176;&#26356;&#22810;&#22320;&#20851;&#27880;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#26469;&#28608;&#21457;LLM&#26356;&#22909;&#22320;&#36981;&#24490;&#25351;&#20196;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;LLaMA-Excitor&#22312;transformer&#32467;&#26500;&#30340;&#33258;&#27880;&#24847;&#21147;&#35745;&#31639;&#36807;&#31243;&#20013;&#19981;&#30452;&#25509;&#25913;&#21464;&#20013;&#38388;&#38544;&#34255;&#29366;&#24577;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;Excitor&#22359;&#20316;&#20026;&#29992;&#20110;LLM&#33258;&#27880;&#24847;&#21147;&#20013;&#30456;&#20284;&#24230;&#35745;&#31639;&#30340;&#26049;&#36335;&#27169;&#22359;&#65292;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#37325;&#26032;&#26500;&#24314;keys&#24182;&#25913;&#21464;values&#30340;&#37325;&#35201;&#24615;&#12290;LLaMA-Excitor&#30830;&#20445;&#33258;&#36866;&#24212;&#22320;&#23558;&#39069;&#22806;&#30340;&#27880;&#24847;&#21147;&#20998;&#37197;&#32473;&#36755;&#20837;&#25351;&#20196;&#65292;&#20174;&#32780;&#22312;&#20302;&#36136;&#37327;&#25351;&#20196;&#19978;&#24494;&#35843;LLM&#26102;&#26377;&#25928;&#22320;&#20445;&#30041;LLM&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00913v1 Announce Type: cross  Abstract: Existing methods to fine-tune LLMs, like Adapter, Prefix-tuning, and LoRA, which introduce extra modules or additional input sequences to inject new skills or knowledge, may compromise the innate abilities of LLMs. In this paper, we propose LLaMA-Excitor, a lightweight method that stimulates the LLMs' potential to better follow instructions by gradually paying more attention to worthwhile information. Specifically, the LLaMA-Excitor does not directly change the intermediate hidden state during the self-attention calculation of the transformer structure. We designed the Excitor block as a bypass module for the similarity score computation in LLMs' self-attention to reconstruct keys and change the importance of values by learnable prompts. LLaMA-Excitor ensures a self-adaptive allocation of additional attention to input instructions, thus effectively preserving LLMs' pre-trained knowledge when fine-tuning LLMs on low-quality instruction-
&lt;/p&gt;</description></item><item><title>&#23558;LLMOps&#38598;&#25104;&#21040;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#25552;&#21319;&#20102;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#21487;&#38752;&#24615;&#65292;&#25512;&#21160;&#29983;&#25104;&#19982;&#29992;&#25143;&#20559;&#22909;&#19968;&#33268;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#65292;&#20026;&#26410;&#26469;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#30340;&#21457;&#23637;&#24102;&#26469;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2404.00903</link><description>&lt;p&gt;
&#36890;&#36807;LLMOps&#39537;&#21160;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#26368;&#22823;&#21270;&#29992;&#25143;&#20307;&#39564;
&lt;/p&gt;
&lt;p&gt;
Maximizing User Experience with LLMOps-Driven Personalized Recommendation Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00903
&lt;/p&gt;
&lt;p&gt;
&#23558;LLMOps&#38598;&#25104;&#21040;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#25552;&#21319;&#20102;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#21487;&#38752;&#24615;&#65292;&#25512;&#21160;&#29983;&#25104;&#19982;&#29992;&#25143;&#20559;&#22909;&#19968;&#33268;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#65292;&#20026;&#26410;&#26469;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#30340;&#21457;&#23637;&#24102;&#26469;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;LLMOps&#38598;&#25104;&#21040;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#26631;&#24535;&#30528;&#22312;&#31649;&#29702;LLM&#39537;&#21160;&#24212;&#29992;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#36825;&#19968;&#21019;&#26032;&#20026;&#20225;&#19994;&#24102;&#26469;&#20102;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#38656;&#35201;&#19987;&#38376;&#30340;&#22242;&#38431;&#22312;&#20248;&#20808;&#32771;&#34385;&#25968;&#25454;&#23433;&#20840;&#21644;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#21516;&#26102;&#65292;&#22788;&#29702;&#24037;&#31243;&#25216;&#26415;&#30340;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;LLMOps&#65292;&#20225;&#19994;&#21487;&#20197;&#25552;&#21319;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#21487;&#38752;&#24615;&#65292;&#25512;&#21160;&#19982;&#29992;&#25143;&#20559;&#22909;&#19968;&#33268;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#23613;&#31649;&#23384;&#22312;&#20262;&#29702;&#32771;&#37327;&#65292;&#20294;LLMOps&#24050;&#20934;&#22791;&#22909;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#25215;&#35834;&#25552;&#20379;&#26356;&#39640;&#25928;&#12289;&#26356;&#23433;&#20840;&#30340;&#26426;&#22120;&#23398;&#20064;&#26381;&#21153;&#65292;&#25552;&#21319;&#29992;&#25143;&#20307;&#39564;&#24182;&#22609;&#36896;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#30340;&#26410;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00903v1 Announce Type: cross  Abstract: The integration of LLMOps into personalized recommendation systems marks a significant advancement in managing LLM-driven applications. This innovation presents both opportunities and challenges for enterprises, requiring specialized teams to navigate the complexity of engineering technology while prioritizing data security and model interpretability. By leveraging LLMOps, enterprises can enhance the efficiency and reliability of large-scale machine learning models, driving personalized recommendations aligned with user preferences. Despite ethical considerations, LLMOps is poised for widespread adoption, promising more efficient and secure machine learning services that elevate user experience and shape the future of personalized recommendation systems.
&lt;/p&gt;</description></item><item><title>&#35813;&#31456;&#33410;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#31283;&#20581;&#24615;&#30340;&#37325;&#35201;&#27010;&#24565;&#21450;&#20851;&#38190;&#30340;&#25216;&#26415;&#21644;&#22240;&#32032;&#65292;&#20197;&#30830;&#31435;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.00897</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#40065;&#26834;&#24615;&#65306;&#20837;&#38376;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Robustness: A Primer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00897
&lt;/p&gt;
&lt;p&gt;
&#35813;&#31456;&#33410;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#31283;&#20581;&#24615;&#30340;&#37325;&#35201;&#27010;&#24565;&#21450;&#20851;&#38190;&#30340;&#25216;&#26415;&#21644;&#22240;&#32032;&#65292;&#20197;&#30830;&#31435;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#19968;&#31456;&#33410;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20013;&#31283;&#20581;&#24615;&#30340;&#22522;&#26412;&#27010;&#24565;&#21450;&#20854;&#22312;&#30830;&#31435;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#30340;&#21487;&#20449;&#24230;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;&#35752;&#35770;&#22987;&#20110;&#31283;&#20581;&#24615;&#30340;&#35814;&#32454;&#23450;&#20041;&#65292;&#23558;&#20854;&#25551;&#36848;&#20026;ML&#27169;&#22411;&#22312;&#21508;&#31181;&#19981;&#21516;&#21644;&#24847;&#22806;&#30340;&#29615;&#22659;&#26465;&#20214;&#19979;&#20445;&#25345;&#31283;&#23450;&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;ML&#40065;&#26834;&#24615;&#36890;&#36807;&#22810;&#20010;&#35270;&#35282;&#36827;&#34892;&#20102;&#21078;&#26512;&#65306;&#20854;&#19982;&#27867;&#21270;&#33021;&#21147;&#30340;&#20114;&#34917;&#24615;&#65307;&#20854;&#20316;&#20026;&#21487;&#20449;AI&#30340;&#35201;&#27714;&#65307;&#20854;&#23545;&#25239;&#24615;&#19982;&#38750;&#23545;&#25239;&#24615;&#26041;&#38754;&#65307;&#20854;&#25968;&#37327;&#21270;&#25351;&#26631;&#65307;&#20197;&#21450;&#20854;&#21487;&#22797;&#29616;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#31561;&#25351;&#26631;&#12290;&#31456;&#33410;&#28145;&#20837;&#25506;&#35752;&#20102;&#24433;&#21709;&#40065;&#26834;&#24615;&#30340;&#22240;&#32032;&#65292;&#22914;&#25968;&#25454;&#20559;&#24046;&#12289;&#27169;&#22411;&#22797;&#26434;&#24615;&#20197;&#21450;ML&#27969;&#31243;&#19981;&#26126;&#30830;&#30340;&#39118;&#38505;&#12290;&#23427;&#20174;&#24191;&#27867;&#30340;&#35270;&#35282;&#35843;&#26597;&#20102;&#40065;&#26834;&#24615;&#35780;&#20272;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#21253;&#25324;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#21253;&#25324;&#25968;&#23383;&#21644;&#29289;&#29702;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00897v1 Announce Type: cross  Abstract: This chapter explores the foundational concept of robustness in Machine Learning (ML) and its integral role in establishing trustworthiness in Artificial Intelligence (AI) systems. The discussion begins with a detailed definition of robustness, portraying it as the ability of ML models to maintain stable performance across varied and unexpected environmental conditions. ML robustness is dissected through several lenses: its complementarity with generalizability; its status as a requirement for trustworthy AI; its adversarial vs non-adversarial aspects; its quantitative metrics; and its indicators such as reproducibility and explainability. The chapter delves into the factors that impede robustness, such as data bias, model complexity, and the pitfalls of underspecified ML pipelines. It surveys key techniques for robustness assessment from a broad perspective, including adversarial attacks, encompassing both digital and physical realms.
&lt;/p&gt;</description></item><item><title>MTLight&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#30340;&#39640;&#25928;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#28508;&#22312;&#29366;&#24577;&#21644;&#26500;&#24314;&#22810;&#20010;&#36741;&#21161;&#20219;&#21153;&#26469;&#25552;&#39640;&#20195;&#29702;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00886</link><description>&lt;p&gt;
MTLight&#65306;&#29992;&#20110;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#30340;&#39640;&#25928;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MTLight: Efficient Multi-Task Reinforcement Learning for Traffic Signal Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00886
&lt;/p&gt;
&lt;p&gt;
MTLight&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#30340;&#39640;&#25928;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#28508;&#22312;&#29366;&#24577;&#21644;&#26500;&#24314;&#22810;&#20010;&#36741;&#21161;&#20219;&#21153;&#26469;&#25552;&#39640;&#20195;&#29702;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#23545;&#20110;&#32531;&#35299;&#29616;&#20195;&#22478;&#24066;&#20132;&#36890;&#25317;&#22581;&#20855;&#26377;&#24040;&#22823;&#24433;&#21709;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36817;&#24180;&#26469;&#34987;&#24191;&#27867;&#29992;&#20110;&#36825;&#19968;&#20219;&#21153;&#65292;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#20294;&#20063;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#65292;&#22914;&#24615;&#33021;&#26377;&#38480;&#21644;&#26679;&#26412;&#25928;&#29575;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;MTLight&#65292;&#20197;&#20174;&#20247;&#22810;&#20132;&#36890;&#25351;&#26631;&#20013;&#23398;&#20064;&#30340;&#28508;&#22312;&#29366;&#24577;&#26469;&#22686;&#24378;&#20195;&#29702;&#30340;&#35266;&#23519;&#12290;&#21516;&#26102;&#65292;&#26500;&#24314;&#20102;&#22810;&#20010;&#36741;&#21161;&#21644;&#30417;&#30563;&#20219;&#21153;&#26469;&#23398;&#20064;&#28508;&#22312;&#29366;&#24577;&#65292;&#24182;&#20351;&#29992;&#20004;&#31181;&#23884;&#20837;&#28508;&#22312;&#29305;&#24449;&#65292;&#21363;&#20219;&#21153;&#29305;&#23450;&#29305;&#24449;&#21644;&#20219;&#21153;&#20849;&#20139;&#29305;&#24449;&#65292;&#20351;&#28508;&#22312;&#29366;&#24577;&#26356;&#21152;&#20016;&#23500;&#12290;&#22312;CityFlow&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;MTLight&#20855;&#26377;&#39046;&#20808;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#28176;&#36817;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#25152;&#26377;&#19981;&#26029;&#22686;&#21152;&#30340;&#25511;&#21046;&#38590;&#24230;&#22330;&#26223;&#19979;&#36827;&#19968;&#27493;&#27169;&#25311;&#39640;&#23792;&#23567;&#26102;&#27573;&#65292;&#32467;&#26524;&#34920;&#26126;MTLig
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00886v1 Announce Type: new  Abstract: Traffic signal control has a great impact on alleviating traffic congestion in modern cities. Deep reinforcement learning (RL) has been widely used for this task in recent years, demonstrating promising performance but also facing many challenges such as limited performances and sample inefficiency. To handle these challenges, MTLight is proposed to enhance the agent observation with a latent state, which is learned from numerous traffic indicators. Meanwhile, multiple auxiliary and supervisory tasks are constructed to learn the latent state, and two types of embedding latent features, the task-specific feature and task-shared feature, are used to make the latent state more abundant. Extensive experiments conducted on CityFlow demonstrate that MTLight has leading convergence speed and asymptotic performance. We further simulate under peak-hour pattern in all scenarios with increasing control difficulty and the results indicate that MTLig
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#33258;&#25105;&#28436;&#31034;(Self-Demos)&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36890;&#36807;&#29983;&#25104;&#26597;&#35810;&#24863;&#30693;&#30340;&#31034;&#33539;&#65292;&#24341;&#20986;&#22266;&#26377;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#26500;&#24314;&#20102;&#29992;&#20110;&#35780;&#20272;&#35813;&#26041;&#27861;&#26377;&#25928;&#24615;&#30340;&#25968;&#25454;&#38598;OOD-Toolset&#12290;</title><link>https://arxiv.org/abs/2404.00884</link><description>&lt;p&gt;
&#33258;&#25105;&#28436;&#31034;: &#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20986;&#31034;&#33539;&#20043;&#22806;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Self-Demos: Eliciting Out-of-Demonstration Generalizability in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00884
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#33258;&#25105;&#28436;&#31034;(Self-Demos)&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36890;&#36807;&#29983;&#25104;&#26597;&#35810;&#24863;&#30693;&#30340;&#31034;&#33539;&#65292;&#24341;&#20986;&#22266;&#26377;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#26500;&#24314;&#20102;&#29992;&#20110;&#35780;&#20272;&#35813;&#26041;&#27861;&#26377;&#25928;&#24615;&#30340;&#25968;&#25454;&#38598;OOD-Toolset&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#26041;&#38754;&#30340;&#33391;&#22909;&#33021;&#21147;&#65292;&#20165;&#20973;&#20511;&#23569;&#37327;&#31034;&#33539;&#23601;&#33021;&#36805;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#23569;&#26679;&#26412;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#39640;&#36136;&#37327;&#12289;&#29305;&#23450;&#26597;&#35810;&#30340;&#31034;&#33539;&#65292;&#32780;&#36825;&#31181;&#31034;&#33539;&#36890;&#24120;&#32570;&#20047;&#12290;&#38754;&#23545;&#31034;&#33539;&#20043;&#22806;&#30340;&#26597;&#35810;&#65292;&#20381;&#36182;&#25163;&#24037;&#21046;&#23450;&#31034;&#33539;&#25110;&#22806;&#37096;&#26816;&#32034;&#22120;&#30340;&#26041;&#27861;&#21487;&#33021;&#20250;&#22833;&#36133;&#12290;&#20026;&#20102;&#24357;&#21512;&#26377;&#38480;&#31034;&#33539;&#21644;&#31034;&#33539;&#20043;&#22806;&#26597;&#35810;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#25105;&#28436;&#31034;(Self-Demos)&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#38754;&#21521;&#26597;&#35810;&#30340;&#31034;&#33539;&#29983;&#25104;&#26469;&#24341;&#20986;LLMs&#20013;&#22266;&#26377;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#26032;&#22411;&#25552;&#31034;&#26041;&#27861;&#12290;&#29983;&#25104;&#30340;&#31034;&#33539;&#31574;&#30053;&#24615;&#22320;&#25554;&#20540;&#20102;&#29616;&#26377;&#31034;&#33539;&#21644;&#32473;&#23450;&#30340;&#26597;&#35810;&#65292;&#23558;&#26597;&#35810;&#20174;&#31034;&#33539;&#20043;&#22806;&#21464;&#20026;&#31034;&#33539;&#20869;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#20154;&#24037;&#26500;&#24314;&#20102;OOD-Toolset&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;300&#20010;&#30495;&#23454;API&#21644;1000&#20010;&#23454;&#20363;&#65292;&#27599;&#20010;&#23454;&#20363;&#21253;&#25324;&#19977;&#20010;&#24037;&#20855;&#20351;&#29992;&#31034;&#20363;&#20316;&#20026;&#31034;&#33539;&#21644;&#19968;&#20010;&#31034;&#33539;&#20043;&#22806;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00884v1 Announce Type: cross  Abstract: Large language models (LLMs) have shown promising abilities of in-context learning (ICL), adapting swiftly to new tasks with only few-shot demonstrations. However, current few-shot methods heavily depend on high-quality, query-specific demos, which are often lacking. When faced with out-of-demonstration (OOD) queries, methods that rely on hand-crafted demos or external retrievers might fail. To bridge the gap between limited demos and OOD queries, we propose Self-Demos, a novel prompting method that elicits the inherent generalizability in LLMs by query-aware demo generation. The generated demos strategically interpolate between existing demos and the given query, transforming the query from OOD to ID. To evaluate the effectiveness of our approach, we manually constructed OOD-Toolset, a dataset in the tool-using scenario with over 300 real-world APIs and 1000 instances, each consisting of three tool-use cases as demos and an OOD query.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#21644;&#20808;&#36827;&#30340;&#23884;&#20837;&#21021;&#22987;&#21270;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#22312;&#20197;&#33521;&#35821;&#20026;&#20027;&#23548;&#30340;&#24320;&#28304;LLMs&#19978;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2404.00862</link><description>&lt;p&gt;
&#22522;&#20110;QLoRA&#21644;Zip-tie&#23884;&#20837;&#30340;&#21452;&#35821;&#36801;&#31227;&#23398;&#20064;Bailong
&lt;/p&gt;
&lt;p&gt;
Bailong: Bilingual Transfer Learning based on QLoRA and Zip-tie Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00862
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#21644;&#20808;&#36827;&#30340;&#23884;&#20837;&#21021;&#22987;&#21270;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#22312;&#20197;&#33521;&#35821;&#20026;&#20027;&#23548;&#30340;&#24320;&#28304;LLMs&#19978;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#22823;&#22810;&#25968;&#24320;&#28304;LLMs&#20027;&#35201;&#22312;&#33521;&#35821;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#23545;&#20854;&#20182;&#35821;&#35328;&#30340;&#35206;&#30422;&#36739;&#23569;&#12290;&#22810;&#35821;&#35328;&#35757;&#32451;&#25968;&#25454;&#30340;&#19981;&#36275;&#23548;&#33268;&#22312;&#24212;&#29992;&#21040;&#36164;&#28304;&#36739;&#23569;&#30340;&#35821;&#35328;&#26102;&#24615;&#33021;&#20122;&#20248;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#39069;&#22806;&#25968;&#25454;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#36827;&#34892;&#20840;&#21442;&#25968;&#24494;&#35843;&#20197;&#25552;&#39640;LLMs&#24615;&#33021;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#32473;&#30740;&#31350;&#26426;&#26500;&#21644;&#20010;&#20154;&#30740;&#31350;&#20154;&#21592;&#24102;&#26469;&#20102;&#35745;&#31639;&#38556;&#30861;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#25216;&#26415;&#65292;&#22914;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#21644;&#20808;&#36827;&#30340;&#23884;&#20837;&#21021;&#22987;&#21270;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#26412;&#25991;&#23558;&#23427;&#20204;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#20419;&#36827;&#23545;&#20197;&#33521;&#35821;&#20026;&#20027;&#23548;&#30340;&#24320;&#28304;LLMs&#36827;&#34892;&#36328;&#35821;&#35328;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00862v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated exceptional performance in various NLP applications. However, the majority of existing open-source LLMs are pre-trained primarily on English data and little part of other languages. This deficiency in multilingual training data results in suboptimal performance when applied to languages with fewer available resources. Furthermore, enhancing the performance of LLMs on low-resource languages by full-parameter fine-tuning with additional data requires substantial computational resources, posing computational barriers for research organizations and individual researchers. Consequently, several techniques such as parameter-efficient tuning and advanced embedding initialization have been proposed to address these challenges. In this work, we combine them to facilitate cross-lingual transfer on English-dominated open-source LLM. To effectively enhance the model's proficiency in Traditional Chines
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26088;&#22312;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#35821;&#38899;&#20013;&#31163;&#25955;&#21333;&#20803;&#36793;&#30028;&#65292;&#23454;&#29616;&#21487;&#21464;&#38271;&#24230;&#27744;&#21270;&#65292;&#20174;&#32780;&#21435;&#38500;&#34920;&#31034;&#20013;&#30340;&#35828;&#35805;&#32773;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2404.00856</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#21464;&#38271;&#24230;&#36719;&#27744;&#21270;&#20174;&#35821;&#38899;&#34920;&#31034;&#20013;&#21435;&#38500;&#35828;&#35805;&#32773;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Removing Speaker Information from Speech Representation using Variable-Length Soft Pooling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00856
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26088;&#22312;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#35821;&#38899;&#20013;&#31163;&#25955;&#21333;&#20803;&#36793;&#30028;&#65292;&#23454;&#29616;&#21487;&#21464;&#38271;&#24230;&#27744;&#21270;&#65292;&#20174;&#32780;&#21435;&#38500;&#34920;&#31034;&#20013;&#30340;&#35828;&#35805;&#32773;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26377;&#19968;&#20123;&#30740;&#31350;&#33268;&#21147;&#20110;&#21033;&#29992;&#33258;&#30417;&#30563;&#26694;&#26550;&#23545;&#35821;&#38899;&#36827;&#34892;&#32534;&#30721;&#65292;&#20197;&#36827;&#34892;&#35821;&#38899;&#21512;&#25104;&#30340;&#35821;&#35328;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20174;&#21608;&#22260;&#34920;&#31034;&#39044;&#27979;&#34920;&#31034;&#21487;&#33021;&#20250;&#22312;&#35821;&#38899;&#34920;&#31034;&#20013;&#19981;&#32463;&#24847;&#22320;&#32544;&#32467;&#35828;&#35805;&#32773;&#20449;&#24687;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#35821;&#38899;&#30340;&#32467;&#26500;&#21270;&#29305;&#24615;&#26469;&#21435;&#38500;&#35828;&#35805;&#32773;&#20449;&#24687;&#65292;&#35821;&#38899;&#30001;&#20855;&#26377;&#28165;&#26224;&#36793;&#30028;&#30340;&#31163;&#25955;&#21333;&#20803;(&#22914;&#38899;&#32032;)&#32452;&#25104;&#12290;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#36825;&#20123;&#36793;&#30028;&#65292;&#23454;&#29616;&#22522;&#20110;&#20107;&#20214;&#30340;&#34920;&#31034;&#25552;&#21462;&#30340;&#21487;&#21464;&#38271;&#24230;&#27744;&#21270;&#65292;&#32780;&#19981;&#26159;&#22266;&#23450;&#36895;&#29575;&#26041;&#27861;&#12290;&#36793;&#30028;&#39044;&#27979;&#22120;&#36755;&#20986;&#20171;&#20110;0&#21644;1&#20043;&#38388;&#30340;&#36793;&#30028;&#27010;&#29575;&#65292;&#20351;&#27744;&#21270;&#36719;&#21270;&#12290;&#27169;&#22411;&#35757;&#32451;&#20197;&#26368;&#23567;&#21270;&#25968;&#25454;&#22686;&#24378;&#30340;&#27744;&#21270;&#34920;&#31034;&#19982;&#26102;&#38388;&#25289;&#20280;&#21644;&#38899;&#39640;&#21464;&#25442;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#30830;&#35748;&#23398;&#21040;&#30340;&#34920;&#31034;&#21253;&#21547;&#20869;&#23481;&#20449;&#24687;&#20294;&#29420;&#31435;&#20110;&#35828;&#35805;&#32773;&#20449;&#24687;&#65292;&#27169;&#22411;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00856v1 Announce Type: cross  Abstract: Recently, there have been efforts to encode the linguistic information of speech using a self-supervised framework for speech synthesis. However, predicting representations from surrounding representations can inadvertently entangle speaker information in the speech representation. This paper aims to remove speaker information by exploiting the structured nature of speech, composed of discrete units like phonemes with clear boundaries. A neural network predicts these boundaries, enabling variable-length pooling for event-based representation extraction instead of fixed-rate methods. The boundary predictor outputs a probability for the boundary between 0 and 1, making pooling soft. The model is trained to minimize the difference with the pooled representation of the data augmented by time-stretch and pitch-shift. To confirm that the learned representation includes contents information but is independent of speaker information, the model
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22522;&#20110;&#40479;&#31867;&#35270;&#35273;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;TSOM&#65292;&#29992;&#20110;&#23567;&#29289;&#20307;&#36816;&#21160;&#26816;&#27979;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.00855</link><description>&lt;p&gt;
&#22522;&#20110;&#40479;&#31867;&#35270;&#35273;&#22238;&#36335;&#30340;&#23567;&#29289;&#20307;&#36816;&#21160;&#26816;&#27979;&#31070;&#32463;&#32593;&#32476;TSOM
&lt;/p&gt;
&lt;p&gt;
TSOM: Small Object Motion Detection Neural Network Inspired by Avian Visual Circuit
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22522;&#20110;&#40479;&#31867;&#35270;&#35273;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;TSOM&#65292;&#29992;&#20110;&#23567;&#29289;&#20307;&#36816;&#21160;&#26816;&#27979;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20463;&#35270;&#35282;&#24230;&#22312;&#22797;&#26434;&#32972;&#26223;&#20013;&#26816;&#27979;&#23567;&#36816;&#21160;&#29289;&#20307;&#23545;&#20110;&#26426;&#22120;&#35270;&#35273;&#31995;&#32479;&#26159;&#19968;&#20010;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#21463;&#33258;&#28982;&#30028;&#21551;&#21457;&#65292;&#40479;&#31867;&#35270;&#35273;&#31995;&#32479;&#33021;&#22815;&#22788;&#29702;&#21508;&#31181;&#22797;&#26434;&#30340;&#31354;&#20013;&#22330;&#26223;&#20013;&#30340;&#36816;&#21160;&#20449;&#24687;&#65292;&#20854;&#35270;&#32593;&#33180;-OT-Rt&#35270;&#35273;&#22238;&#36335;&#39640;&#24230;&#25935;&#24863;&#20110;&#20174;&#39640;&#31354;&#25429;&#25417;&#23567;&#29289;&#20307;&#30340;&#36816;&#21160;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#40479;&#31867;&#35270;&#35273;&#31995;&#32479;&#30340;&#23567;&#29289;&#20307;&#36816;&#21160;&#26816;&#27979;&#31639;&#27861;&#36824;&#26377;&#24453;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#35270;&#32593;&#33180;-OT-Rt&#35270;&#35273;&#22238;&#36335;&#29983;&#29289;&#26426;&#21046;&#30340;&#24191;&#27867;&#30740;&#31350;&#36827;&#34892;&#25968;&#23398;&#24314;&#27169;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;tectum&#23567;&#29289;&#20307;&#36816;&#21160;&#26816;&#27979;&#31070;&#32463;&#32593;&#32476;&#65288;TSOM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00855v1 Announce Type: cross  Abstract: Detecting small moving objects in complex backgrounds from an overhead perspective is a highly challenging task for machine vision systems. As an inspiration from nature, the avian visual system is capable of processing motion information in various complex aerial scenes, and its Retina-OT-Rt visual circuit is highly sensitive to capturing the motion information of small objects from high altitudes. However, more needs to be done on small object motion detection algorithms based on the avian visual system. In this paper, we conducted mathematical modeling based on extensive studies of the biological mechanisms of the Retina-OT-Rt visual circuit. Based on this, we proposed a novel tectum small object motion detection neural network (TSOM). The neural network includes the retina, SGC dendritic, SGC Soma, and Rt layers, each layer corresponding to neurons in the visual pathway. The Retina layer is responsible for accurately projecting inp
&lt;/p&gt;</description></item><item><title>HeteroMILE&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32423;&#23884;&#20837;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#24403;&#20195;&#22270;&#23884;&#20837;&#26041;&#27861;&#25193;&#23637;&#21040;&#22823;&#22411;&#24322;&#26500;&#22270;&#65292;&#24182;&#36890;&#36807;&#21453;&#22797;&#31895;&#21270;&#21644;&#32454;&#21270;&#30340;&#26041;&#24335;&#26377;&#25928;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2404.00816</link><description>&lt;p&gt;
HeteroMILE: &#29992;&#20110;&#24322;&#26500;&#22270;&#30340;&#22810;&#23618;&#22270;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
HeteroMILE: a Multi-Level Graph Representation Learning Framework for Heterogeneous Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00816
&lt;/p&gt;
&lt;p&gt;
HeteroMILE&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32423;&#23884;&#20837;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#24403;&#20195;&#22270;&#23884;&#20837;&#26041;&#27861;&#25193;&#23637;&#21040;&#22823;&#22411;&#24322;&#26500;&#22270;&#65292;&#24182;&#36890;&#36807;&#21453;&#22797;&#31895;&#21270;&#21644;&#32454;&#21270;&#30340;&#26041;&#24335;&#26377;&#25928;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#22270;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#34920;&#31034;&#19981;&#21516;&#31867;&#22411;&#23454;&#20307;&#20043;&#38388;&#30340;&#21508;&#31181;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#23398;&#20064;&#36825;&#31181;&#22270;&#20013;&#30340;&#23884;&#20837;&#26159;&#22270;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#30001;&#20110;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#32780;&#26080;&#27861;&#25193;&#23637;&#21040;&#22823;&#22411;&#24322;&#26500;&#22270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33410;&#28857;&#22312;&#24322;&#26500;&#22270;&#19978;&#30340;&#22810;&#32423;&#23884;&#20837;&#26694;&#26550;&#65288;HeteroMILE&#65289;-&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#24403;&#20195;&#22270;&#23884;&#20837;&#26041;&#27861;&#25193;&#23637;&#21040;&#22823;&#22270;&#12290;HeteroMILE&#23558;&#22823;&#22270;&#21453;&#22797;&#31895;&#21270;&#20026;&#36739;&#23567;&#30340;&#22823;&#23567;&#65292;&#21516;&#26102;&#20445;&#30041;&#22270;&#30340;&#20027;&#24178;&#32467;&#26500;&#65292;&#28982;&#21518;&#23558;&#20854;&#23884;&#20837;&#65292;&#36890;&#36807;&#36991;&#20813;&#32791;&#26102;&#30340;&#22788;&#29702;&#25805;&#20316;&#26377;&#25928;&#22320;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#28982;&#21518;&#65292;&#23427;&#20351;&#29992;&#24322;&#26500;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23558;&#31895;&#21270;&#30340;&#23884;&#20837;&#20248;&#21270;&#21040;&#21407;&#22987;&#22270;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00816v1 Announce Type: cross  Abstract: Heterogeneous graphs are ubiquitous in real-world applications because they can represent various relationships between different types of entities. Therefore, learning embeddings in such graphs is a critical problem in graph machine learning. However, existing solutions for this problem fail to scale to large heterogeneous graphs due to their high computational complexity. To address this issue, we propose a Multi-Level Embedding framework of nodes on a heterogeneous graph (HeteroMILE) - a generic methodology that allows contemporary graph embedding methods to scale to large graphs. HeteroMILE repeatedly coarsens the large sized graph into a smaller size while preserving the backbone structure of the graph before embedding it, effectively reducing the computational cost by avoiding time-consuming processing operations. It then refines the coarsened embedding to the original graph using a heterogeneous graph convolution neural network.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;LiDAR&#25193;&#25955;&#27169;&#22411;&#65288;LiDMs&#65289;&#65292;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;LiDAR&#36924;&#30495;&#25928;&#26524;&#30340;&#22330;&#26223;&#65292;&#36890;&#36807;&#24341;&#20837;&#20960;&#20309;&#20808;&#39564;&#65292;&#23454;&#29616;&#20102;&#27169;&#24335;&#36924;&#30495;&#12289;&#20960;&#20309;&#36924;&#30495;&#21644;&#29289;&#20307;&#36924;&#30495;&#12290;</title><link>https://arxiv.org/abs/2404.00815</link><description>&lt;p&gt;
&#22522;&#20110;LiDAR&#25193;&#25955;&#27169;&#22411;&#30340;&#36924;&#30495;&#22330;&#26223;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Towards Realistic Scene Generation with LiDAR Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;LiDAR&#25193;&#25955;&#27169;&#22411;&#65288;LiDMs&#65289;&#65292;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;LiDAR&#36924;&#30495;&#25928;&#26524;&#30340;&#22330;&#26223;&#65292;&#36890;&#36807;&#24341;&#20837;&#20960;&#20309;&#20808;&#39564;&#65292;&#23454;&#29616;&#20102;&#27169;&#24335;&#36924;&#30495;&#12289;&#20960;&#20309;&#36924;&#30495;&#21644;&#29289;&#20307;&#36924;&#30495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#36924;&#30495;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#36866;&#24212;LiDAR&#22330;&#26223;&#29983;&#25104;&#26041;&#38754;&#38754;&#20020;&#37325;&#22823;&#38556;&#30861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;LiDAR&#25193;&#25955;&#27169;&#22411;&#65288;LiDMs&#65289;&#65292;&#36890;&#36807;&#22312;&#23398;&#20064;&#27969;&#31243;&#20013;&#24341;&#20837;&#20960;&#20309;&#20808;&#39564;&#65292;&#20174;&#32780;&#29983;&#25104;&#20855;&#26377;LiDAR&#36924;&#30495;&#25928;&#26524;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00815v1 Announce Type: cross  Abstract: Diffusion models (DMs) excel in photo-realistic image synthesis, but their adaptation to LiDAR scene generation poses a substantial hurdle. This is primarily because DMs operating in the point space struggle to preserve the curve-like patterns and 3D geometry of LiDAR scenes, which consumes much of their representation power. In this paper, we propose LiDAR Diffusion Models (LiDMs) to generate LiDAR-realistic scenes from a latent space tailored to capture the realism of LiDAR scenes by incorporating geometric priors into the learning pipeline. Our method targets three major desiderata: pattern realism, geometry realism, and object realism. Specifically, we introduce curve-wise compression to simulate real-world LiDAR patterns, point-wise coordinate supervision to learn scene geometry, and patch-wise encoding for a full 3D object context. With these three core designs, our method achieves competitive performance on unconditional LiDAR g
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31639;&#27861;&#23450;&#20215;&#20195;&#29702;&#22312;&#23521;&#22836;&#24066;&#22330;&#29615;&#22659;&#20013;&#33258;&#20027;&#21246;&#32467;&#65292;&#23545;&#28040;&#36153;&#32773;&#21033;&#30410;&#26377;&#23475;&#65292;&#20854;&#35828;&#26126;&#20070;&#20013;&#30340;&#30701;&#35821;&#21464;&#21270;&#21487;&#33021;&#22686;&#21152;&#21246;&#32467;&#12290;</title><link>https://arxiv.org/abs/2404.00806</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31639;&#27861;&#21246;&#32467;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Collusion by Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00806
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31639;&#27861;&#23450;&#20215;&#20195;&#29702;&#22312;&#23521;&#22836;&#24066;&#22330;&#29615;&#22659;&#20013;&#33258;&#20027;&#21246;&#32467;&#65292;&#23545;&#28040;&#36153;&#32773;&#21033;&#30410;&#26377;&#23475;&#65292;&#20854;&#35828;&#26126;&#20070;&#20013;&#30340;&#30701;&#35821;&#21464;&#21270;&#21487;&#33021;&#22686;&#21152;&#21246;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00806v1 &#20844;&#21578;&#31867;&#22411;:&#20132;&#21449;&#25688;&#35201;:&#31639;&#27861;&#23450;&#20215;&#30340;&#20852;&#36215;&#24341;&#36215;&#20102;&#23545;&#31639;&#27861;&#21246;&#32467;&#30340;&#25285;&#24551;&#12290;&#25105;&#20204;&#23545;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29305;&#21035;&#26159;GPT-4&#30340;&#31639;&#27861;&#23450;&#20215;&#20195;&#29702;&#36827;&#34892;&#23454;&#39564;&#12290;&#25105;&#20204;&#21457;&#29616;&#65306;&#65288;1&#65289;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#22312;&#23450;&#20215;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#65288;2&#65289;&#22522;&#20110;LLM&#30340;&#23450;&#20215;&#20195;&#29702;&#22312;&#23521;&#22836;&#24066;&#22330;&#29615;&#22659;&#20013;&#33258;&#20027;&#21246;&#32467;&#65292;&#25439;&#23475;&#28040;&#36153;&#32773;&#21033;&#30410;&#65292;&#65288;3&#65289;LLM&#35828;&#26126;&#20070;&#20013;&#30475;&#20284;&#26080;&#23475;&#30701;&#35821;("&#25552;&#31034;")&#30340;&#21464;&#21270;&#21487;&#33021;&#20250;&#22686;&#21152;&#21246;&#32467;&#12290;&#36825;&#20123;&#32467;&#26524;&#20063;&#36866;&#29992;&#20110;&#25293;&#21334;&#35774;&#32622;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#35843;&#20102;&#26377;&#20851;&#31639;&#27861;&#23450;&#20215;&#30340;&#21453;&#22404;&#26029;&#30417;&#31649;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#21457;&#29616;&#20102;&#22522;&#20110;LLM&#30340;&#23450;&#20215;&#20195;&#29702;&#25152;&#38754;&#20020;&#30340;&#30417;&#31649;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00806v1 Announce Type: cross  Abstract: The rise of algorithmic pricing raises concerns of algorithmic collusion. We conduct experiments with algorithmic pricing agents based on Large Language Models (LLMs), and specifically GPT-4. We find that (1) LLM-based agents are adept at pricing tasks, (2) LLM-based pricing agents autonomously collude in oligopoly settings to the detriment of consumers, and (3) variation in seemingly innocuous phrases in LLM instructions ("prompts") may increase collusion. These results extend to auction settings. Our findings underscore the need for antitrust regulation regarding algorithmic pricing, and uncover regulatory challenges unique to LLM-based pricing agents.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#25928;&#29992;&#30340;&#25200;&#21160;&#26799;&#24230;&#19979;&#38477;&#65288;UPGD&#65289;&#65292;&#36890;&#36807;&#22312;&#26799;&#24230;&#26356;&#26032;&#20013;&#24212;&#29992;&#19981;&#21516;&#22823;&#23567;&#30340;&#25200;&#21160;&#65292;&#20445;&#25252;&#26377;&#29992;&#21333;&#20803;&#20197;&#38450;&#36951;&#24536;&#65292;&#21516;&#26102;&#24674;&#22797;&#19981;&#22826;&#26377;&#29992;&#21333;&#20803;&#30340;&#21487;&#22609;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00781</link><description>&lt;p&gt;
&#22788;&#29702;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#21487;&#22609;&#24615;&#20007;&#22833;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#25928;&#29992;&#30340;&#25200;&#21160;&#26799;&#24230;&#19979;&#38477;&#65288;UPGD&#65289;&#65292;&#36890;&#36807;&#22312;&#26799;&#24230;&#26356;&#26032;&#20013;&#24212;&#29992;&#19981;&#21516;&#22823;&#23567;&#30340;&#25200;&#21160;&#65292;&#20445;&#25252;&#26377;&#29992;&#21333;&#20803;&#20197;&#38450;&#36951;&#24536;&#65292;&#21516;&#26102;&#24674;&#22797;&#19981;&#22826;&#26377;&#29992;&#21333;&#20803;&#30340;&#21487;&#22609;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#23384;&#22312;&#22256;&#38590;&#65292;&#26082;&#36973;&#21463;&#26377;&#29992;&#21333;&#20803;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21448;&#22240;&#20725;&#21270;&#21644;&#26080;&#29992;&#21333;&#20803;&#23548;&#33268;&#21487;&#22609;&#24615;&#20002;&#22833;&#12290;&#34429;&#28982;&#35768;&#22810;&#26041;&#27861;&#20998;&#21035;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#20294;&#30446;&#21069;&#21482;&#26377;&#23569;&#25968;&#26041;&#27861;&#33021;&#21516;&#26102;&#22788;&#29702;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#22522;&#20110;&#25928;&#29992;&#30340;&#25200;&#21160;&#26799;&#24230;&#19979;&#38477;&#65288;UPGD&#65289;&#20316;&#20026;&#19968;&#31181;&#29992;&#20110;&#34920;&#31034;&#25345;&#32493;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#12290;UPGD&#32467;&#21512;&#20102;&#26799;&#24230;&#26356;&#26032;&#21644;&#25200;&#21160;&#65292;&#23427;&#23545;&#26356;&#26377;&#29992;&#30340;&#21333;&#20803;&#24212;&#29992;&#36739;&#23567;&#30340;&#20462;&#25913;&#65292;&#20445;&#25252;&#23427;&#20204;&#20813;&#21463;&#36951;&#24536;&#65292;&#23545;&#19981;&#22826;&#26377;&#29992;&#30340;&#21333;&#20803;&#24212;&#29992;&#36739;&#22823;&#30340;&#20462;&#25913;&#65292;&#24674;&#22797;&#23427;&#20204;&#30340;&#21487;&#22609;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00781v1 Announce Type: cross  Abstract: Deep representation learning methods struggle with continual learning, suffering from both catastrophic forgetting of useful units and loss of plasticity, often due to rigid and unuseful units. While many methods address these two issues separately, only a few currently deal with both simultaneously. In this paper, we introduce Utility-based Perturbed Gradient Descent (UPGD) as a novel approach for the continual learning of representations. UPGD combines gradient updates with perturbations, where it applies smaller modifications to more useful units, protecting them from forgetting, and larger modifications to less useful units, rejuvenating their plasticity. We use a challenging streaming learning setup where continual learning problems have hundreds of non-stationarities and unknown task boundaries. We show that many existing methods suffer from at least one of the issues, predominantly manifested by their decreasing accuracy over ta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30828;&#20214;&#32423;&#20154;&#33080;&#21435;&#26631;&#35782;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20809;&#23398;&#32534;&#30721;&#22120;&#21644;&#22238;&#24402;&#27169;&#22411;&#29983;&#25104;&#20154;&#33080;&#28909;&#22270;&#65292;&#21516;&#26102;&#38544;&#34255;&#20154;&#33080;&#36523;&#20221;&#65292;&#35299;&#20915;&#20102;&#20154;&#33080;&#21435;&#26631;&#35782;&#21270;&#36719;&#20214;&#23384;&#22312;&#30340;&#28431;&#27934;&#12290;</title><link>https://arxiv.org/abs/2404.00777</link><description>&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#20809;&#23398;&#25216;&#26415;&#22686;&#24378;&#20154;&#33080;&#21435;&#26631;&#35782;&#21270;&#30340;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Privacy-preserving Optics for Enhancing Protection in Face De-identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30828;&#20214;&#32423;&#20154;&#33080;&#21435;&#26631;&#35782;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20809;&#23398;&#32534;&#30721;&#22120;&#21644;&#22238;&#24402;&#27169;&#22411;&#29983;&#25104;&#20154;&#33080;&#28909;&#22270;&#65292;&#21516;&#26102;&#38544;&#34255;&#20154;&#33080;&#36523;&#20221;&#65292;&#35299;&#20915;&#20102;&#20154;&#33080;&#21435;&#26631;&#35782;&#21270;&#36719;&#20214;&#23384;&#22312;&#30340;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#26426;&#30340;&#29616;&#20195;&#28608;&#22686;&#65292;&#20197;&#21450;&#24191;&#27867;&#24212;&#29992;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#65292;&#24341;&#21457;&#20102;&#37325;&#22823;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#25285;&#24551;&#12290;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#24110;&#21161;&#35782;&#21035;&#30456;&#20851;&#20107;&#20214;&#65292;&#24182;&#22312;&#23478;&#24237;&#12289;&#21150;&#20844;&#23460;&#12289;&#21307;&#38498;&#31561;&#39046;&#22495;&#30340;&#26085;&#24120;&#20219;&#21153;&#20013;&#25552;&#20379;&#25903;&#25345;&#12290;&#20026;&#20102;&#36825;&#20123;&#30446;&#30340;&#32780;&#35775;&#38382;&#25110;&#22788;&#29702;&#20010;&#20154;&#20449;&#24687;&#24341;&#21457;&#20102;&#38544;&#31169;&#25285;&#24551;&#12290;&#34429;&#28982;&#36719;&#20214;&#32423;&#35299;&#20915;&#26041;&#26696;&#22914;&#20154;&#33080;&#21435;&#26631;&#35782;&#21270;&#25552;&#20379;&#20102;&#24456;&#22909;&#30340;&#38544;&#31169;/&#25928;&#29992;&#26435;&#34913;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#21957;&#25506;&#25915;&#20987;&#30340;&#28431;&#27934;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30828;&#20214;&#32423;&#20154;&#33080;&#21435;&#26631;&#35782;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#28431;&#27934;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#23398;&#20064;&#20809;&#23398;&#32534;&#30721;&#22120;&#20197;&#21450;&#22238;&#24402;&#27169;&#22411;&#65292;&#33719;&#21462;&#20154;&#33080;&#28909;&#22270;&#65292;&#21516;&#26102;&#38544;&#34255;&#28304;&#22270;&#20687;&#20013;&#30340;&#20154;&#33080;&#36523;&#20221;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#21311;&#21517;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#38544;&#31169;&#20445;&#25252;&#22270;&#20687;&#12289;&#20154;&#33080;&#28909;&#22270;&#21644;&#21442;&#32771;&#20154;&#33080;&#29983;&#25104;&#19968;&#20010;&#26032;&#30340;&#20154;&#33080;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00777v1 Announce Type: cross  Abstract: The modern surge in camera usage alongside widespread computer vision technology applications poses significant privacy and security concerns. Current artificial intelligence (AI) technologies aid in recognizing relevant events and assisting in daily tasks in homes, offices, hospitals, etc. The need to access or process personal information for these purposes raises privacy concerns. While software-level solutions like face de-identification provide a good privacy/utility trade-off, they present vulnerabilities to sniffing attacks. In this paper, we propose a hardware-level face de-identification method to solve this vulnerability. Specifically, our approach first learns an optical encoder along with a regression model to obtain a face heatmap while hiding the face identity from the source image. We also propose an anonymization framework that generates a new face using the privacy-preserving image, face heatmap, and a reference face i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#21517;&#20026;Recover&#65292;&#29992;&#20110;&#22312;&#32447;&#25925;&#38556;&#35782;&#21035;&#21644;&#24674;&#22797;&#65292;&#36890;&#36807;&#38598;&#25104;&#31526;&#21495;&#20449;&#24687;&#26469;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24674;&#22797;&#35745;&#21010;&#30340;&#33021;&#21147;&#65292;&#38477;&#20302;&#30456;&#20851;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2404.00756</link><description>&lt;p&gt;
Recover&#65306;&#29992;&#20110;&#25925;&#38556;&#26816;&#27979;&#21644;&#24674;&#22797;&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Recover: A Neuro-Symbolic Framework for Failure Detection and Recovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00756
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#21517;&#20026;Recover&#65292;&#29992;&#20110;&#22312;&#32447;&#25925;&#38556;&#35782;&#21035;&#21644;&#24674;&#22797;&#65292;&#36890;&#36807;&#38598;&#25104;&#31526;&#21495;&#20449;&#24687;&#26469;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24674;&#22797;&#35745;&#21010;&#30340;&#33021;&#21147;&#65292;&#38477;&#20302;&#30456;&#20851;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;Recover&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#32447;&#25925;&#38556;&#35782;&#21035;&#21644;&#24674;&#22797;&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#12290;Recover&#36890;&#36807;&#38598;&#25104;&#26412;&#20307;&#35770;&#12289;&#36923;&#36753;&#35268;&#21017;&#21644;&#22522;&#20110;LLM&#30340;&#35268;&#21010;&#22120;&#65292;&#21033;&#29992;&#31526;&#21495;&#20449;&#24687;&#22686;&#24378;LLM&#29983;&#25104;&#24674;&#22797;&#35745;&#21010;&#30340;&#33021;&#21147;&#65292;&#24182;&#38477;&#20302;&#30456;&#20851;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00756v1 Announce Type: new  Abstract: Recognizing failures during task execution and implementing recovery procedures is challenging in robotics. Traditional approaches rely on the availability of extensive data or a tight set of constraints, while more recent approaches leverage large language models (LLMs) to verify task steps and replan accordingly. However, these methods often operate offline, necessitating scene resets and incurring in high costs. This paper introduces Recover, a neuro-symbolic framework for online failure identification and recovery. By integrating ontologies, logical rules, and LLM-based planners, Recover exploits symbolic information to enhance the ability of LLMs to generate recovery plans and also to decrease the associated costs. In order to demonstrate the capabilities of our method in a simulated kitchen environment, we introduce OntoThor, an ontology describing the AI2Thor simulator setting. Empirical evaluation shows that OntoThor's logical ru
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#24322;&#24120;&#26816;&#27979;&#26469;&#34913;&#37327;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#20013;&#26679;&#26412;&#25509;&#36817;&#30495;&#23454;&#20998;&#24067;&#30340;&#31243;&#24230;&#65292;&#24182;&#39564;&#35777;&#23454;&#39564;&#32467;&#26524;&#39318;&#27425;&#25903;&#25345;&#20102;&#24615;&#33021;&#19982;&#37319;&#26679;&#36817;&#20284;&#24230;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;</title><link>https://arxiv.org/abs/2404.00752</link><description>&lt;p&gt;
&#20851;&#20110;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#30340;&#30495;&#23454;&#20998;&#24067;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
On the True Distribution Approximation of Minimum Bayes-Risk Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#24322;&#24120;&#26816;&#27979;&#26469;&#34913;&#37327;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#20013;&#26679;&#26412;&#25509;&#36817;&#30495;&#23454;&#20998;&#24067;&#30340;&#31243;&#24230;&#65292;&#24182;&#39564;&#35777;&#23454;&#39564;&#32467;&#26524;&#39318;&#27425;&#25903;&#25345;&#20102;&#24615;&#33021;&#19982;&#37319;&#26679;&#36817;&#20284;&#24230;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;MBR&#65289;&#35299;&#30721;&#26368;&#36817;&#22312;&#25991;&#26412;&#29983;&#25104;&#20013;&#37325;&#26032;&#24341;&#36215;&#20851;&#27880;&#12290;MBR&#35299;&#30721;&#23558;&#20174;&#27169;&#22411;&#20013;&#37319;&#26679;&#30340;&#25991;&#26412;&#35270;&#20026;&#20266;&#21442;&#32771;&#65292;&#24182;&#36873;&#25321;&#19982;&#20854;&#20182;&#25991;&#26412;&#26368;&#30456;&#20284;&#30340;&#25991;&#26412;&#12290;&#22240;&#27492;&#65292;&#37319;&#26679;&#26159;MBR&#35299;&#30721;&#30340;&#20851;&#38190;&#20803;&#32032;&#20043;&#19968;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#25253;&#21578;&#31216;&#24615;&#33021;&#22240;&#37319;&#26679;&#26041;&#27861;&#32780;&#24322;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#36825;&#31181;&#24615;&#33021;&#21464;&#21270;&#24456;&#21487;&#33021;&#19982;&#26679;&#26412;&#22914;&#20309;&#25509;&#36817;&#21442;&#32771;&#30495;&#23454;&#20998;&#24067;&#26377;&#20851;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36817;&#20284;&#23578;&#26410;&#28145;&#20837;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#24322;&#24120;&#26816;&#27979;&#26469;&#34913;&#37327;&#36924;&#36817;&#31243;&#24230;&#12290;&#25105;&#20204;&#39318;&#20808;&#20180;&#32454;&#30740;&#31350;&#24615;&#33021;&#30340;&#21464;&#21270;&#65292;&#28982;&#21518;&#23637;&#31034;&#20197;&#24448;&#20851;&#20110;&#26679;&#26412;&#30340;&#20551;&#35774;&#19982;&#21464;&#21270;&#30340;&#30456;&#20851;&#24615;&#19981;&#20339;&#65292;&#20294;&#25105;&#20204;&#24341;&#20837;&#30340;&#24322;&#24120;&#20998;&#25968;&#21017;&#30456;&#20851;&#12290;&#36825;&#20123;&#32467;&#26524;&#26159;&#39318;&#27425;&#20973;&#32463;&#39564;&#35777;&#26126;&#20102;&#24615;&#33021;&#21644;&#37319;&#26679;&#36817;&#20284;&#24230;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00752v1 Announce Type: cross  Abstract: Minimum Bayes-risk (MBR) decoding has recently gained renewed attention in text generation. MBR decoding considers texts sampled from a model as pseudo-references and selects the text with the highest similarity to the others. Therefore, sampling is one of the key elements of MBR decoding, and previous studies reported that the performance varies by sampling methods. From a theoretical standpoint, this performance variation is likely tied to how closely the samples approximate the true distribution of references. However, this approximation has not been the subject of in-depth study. In this study, we propose using anomaly detection to measure the degree of approximation. We first closely examine the performance variation and then show that previous hypotheses about samples do not correlate well with the variation, but our introduced anomaly scores do. The results are the first to empirically support the link between the performance an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#34913;&#37327;&#25968;&#25454;&#20998;&#24067;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#24615;&#33021;&#21644;&#35780;&#20272;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#25968;&#25454;&#20998;&#24067;&#30340;&#37325;&#35201;&#24615;&#21644;&#23545;&#35780;&#20272;&#26694;&#26550;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2404.00748</link><description>&lt;p&gt;
&#22522;&#20934;&#36879;&#26126;&#24230;&#65306;&#34913;&#37327;&#25968;&#25454;&#23545;&#35780;&#20272;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Benchmark Transparency: Measuring the Impact of Data on Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#34913;&#37327;&#25968;&#25454;&#20998;&#24067;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#24615;&#33021;&#21644;&#35780;&#20272;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#25968;&#25454;&#20998;&#24067;&#30340;&#37325;&#35201;&#24615;&#21644;&#23545;&#35780;&#20272;&#26694;&#26550;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#39033;&#20851;&#20110;&#37327;&#21270;&#25968;&#25454;&#20998;&#24067;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#24615;&#33021;&#21644;&#35780;&#20272;&#24433;&#21709;&#30340;&#21021;&#27493;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#34913;&#37327;&#25968;&#25454;&#28857;&#22312;6&#20010;&#19981;&#21516;&#32500;&#24230;&#19978;&#30340;&#20998;&#24067;&#65306;&#27169;&#31946;&#24230;&#12289;&#22256;&#38590;&#24230;&#12289;&#21487;&#21306;&#20998;&#24615;&#12289;&#38271;&#24230;&#12289;&#22122;&#22768;&#21644;&#22256;&#24785;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#22343;&#34913;&#20998;&#23618;&#25277;&#26679;&#26469;&#34913;&#37327;&#25968;&#25454;&#20998;&#24067;&#23545;&#32477;&#23545;&#65288;&#20934;&#30830;&#29575;/F1&#65289;&#21644;&#30456;&#23545;&#65288;&#25490;&#21517;&#65289;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65288;SQUAD&#21644;MNLI&#65289;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#27979;&#35797;&#20102;&#24635;&#20849;135&#31181;&#19981;&#21516;&#30340;&#27169;&#22411;&#65288;125&#31181;&#22312;SQUAD&#19978;&#65292;10&#31181;&#22312;MNLI&#19978;&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#27809;&#26377;&#26126;&#30830;&#25511;&#21046;&#25968;&#25454;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#26631;&#20934;&#35780;&#20272;&#26694;&#26550;&#26159;&#19981;&#19968;&#33268;&#21644;&#19981;&#21487;&#38752;&#30340;&#12290;&#25105;&#20204;&#21457;&#29616;&#25968;&#25454;&#30340;&#24433;&#21709;&#22312;&#32479;&#35745;&#19978;&#26159;&#26174;&#33879;&#30340;&#65292;&#24182;&#19988;&#36890;&#24120;&#27604;&#26356;&#25913;&#24230;&#37327;&#30340;&#24433;&#21709;&#26356;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00748v1 Announce Type: cross  Abstract: In this paper we present an exploratory research on quantifying the impact that data distribution has on the performance and evaluation of NLP models. We propose an automated framework that measures the data point distribution across 6 different dimensions: ambiguity, difficulty, discriminability, length, noise, and perplexity.   We use disproportional stratified sampling to measure how much the data distribution affects absolute (Acc/F1) and relative (Rank) model performance. We experiment on 2 different datasets (SQUAD and MNLI) and test a total of 135 different models (125 on SQUAD and 10 on MNLI). We demonstrate that without explicit control of the data distribution, standard evaluation frameworks are inconsistent and unreliable. We find that the impact of the data is statistically significant and is often larger than the impact of changing the metric.   In a second set of experiments, we demonstrate that the impact of data on eval
&lt;/p&gt;</description></item><item><title>&#22312;&#22686;&#37327;&#19981;&#30830;&#23450;&#25968;&#25454;&#24211;&#20013;&#25366;&#25496;&#21152;&#26435;&#24207;&#21015;&#27169;&#24335;&#26102;&#65292;&#38656;&#35201;&#22788;&#29702;&#26435;&#37325;&#32422;&#26463;&#65292;&#25552;&#20986;&#20102;&#22686;&#37327;&#25366;&#25496;&#31639;&#27861;&#65292;&#20351;&#24471;&#25366;&#25496;&#37325;&#35201;&#20449;&#24687;&#26356;&#21152;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2404.00746</link><description>&lt;p&gt;
&#22312;&#22686;&#37327;&#19981;&#30830;&#23450;&#25968;&#25454;&#24211;&#20013;&#25366;&#25496;&#21152;&#26435;&#24207;&#21015;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Mining Weighted Sequential Patterns in Incremental Uncertain Databases
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00746
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22686;&#37327;&#19981;&#30830;&#23450;&#25968;&#25454;&#24211;&#20013;&#25366;&#25496;&#21152;&#26435;&#24207;&#21015;&#27169;&#24335;&#26102;&#65292;&#38656;&#35201;&#22788;&#29702;&#26435;&#37325;&#32422;&#26463;&#65292;&#25552;&#20986;&#20102;&#22686;&#37327;&#25366;&#25496;&#31639;&#27861;&#65292;&#20351;&#24471;&#25366;&#25496;&#37325;&#35201;&#20449;&#24687;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#31185;&#23398;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#19981;&#31934;&#30830;&#12289;&#22122;&#38899;&#21644;&#19981;&#30830;&#23450;&#25968;&#25454;&#30340;&#37325;&#35201;&#24615;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#22240;&#27492;&#65292;&#22312;&#19981;&#30830;&#23450;&#25968;&#25454;&#24211;&#20013;&#25366;&#25496;&#27169;&#24335;&#24050;&#32463;&#24341;&#36215;&#30740;&#31350;&#20154;&#21592;&#30340;&#20851;&#27880;&#12290;&#27492;&#22806;&#65292;&#38656;&#35201;&#21457;&#29616;&#36825;&#20123;&#25968;&#25454;&#24211;&#20013;&#30340;&#39033;&#30446;&#30340;&#39057;&#32321;&#24207;&#21015;&#65292;&#20197;&#33719;&#24471;&#20855;&#26377;&#37325;&#22823;&#24433;&#21709;&#30340;&#26377;&#24847;&#20041;&#30693;&#35782;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#24341;&#20837;&#39033;&#30446;&#21644;&#27169;&#24335;&#30340;&#26435;&#37325;&#26469;&#21457;&#29616;&#26377;&#36259;&#30340;&#24207;&#21015;&#20316;&#20026;&#37325;&#35201;&#24615;&#30340;&#24230;&#37327;&#12290;&#22240;&#27492;&#65292;&#22312;&#25366;&#25496;&#24207;&#21015;&#27169;&#24335;&#26102;&#38656;&#35201;&#22788;&#29702;&#26435;&#37325;&#32422;&#26463;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#25968;&#25454;&#24211;&#30340;&#21160;&#24577;&#29305;&#24615;&#65292;&#25366;&#25496;&#37325;&#35201;&#20449;&#24687;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#22686;&#37327;&#25366;&#25496;&#31639;&#27861;&#19981;&#26159;&#22312;&#27599;&#20010;&#22686;&#37327;&#20043;&#21518;&#20174;&#22836;&#25366;&#25496;&#27169;&#24335;&#65292;&#32780;&#26159;&#21033;&#29992;&#20808;&#21069;&#25366;&#25496;&#30340;&#20449;&#24687;&#31435;&#21363;&#26356;&#26032;&#32467;&#26524;&#12290;&#23384;&#22312;&#20960;&#31181;&#31639;&#27861;&#29992;&#20110;&#20174;&#22686;&#37327;&#25968;&#25454;&#24211;&#20013;&#25366;&#25496;&#39057;&#32321;&#27169;&#24335;&#21644;&#21152;&#26435;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00746v1 Announce Type: cross  Abstract: Due to the rapid development of science and technology, the importance of imprecise, noisy, and uncertain data is increasing at an exponential rate. Thus, mining patterns in uncertain databases have drawn the attention of researchers. Moreover, frequent sequences of items from these databases need to be discovered for meaningful knowledge with great impact. In many real cases, weights of items and patterns are introduced to find interesting sequences as a measure of importance. Hence, a constraint of weight needs to be handled while mining sequential patterns. Besides, due to the dynamic nature of databases, mining important information has become more challenging. Instead of mining patterns from scratch after each increment, incremental mining algorithms utilize previously mined information to update the result immediately. Several algorithms exist to mine frequent patterns and weighted sequences from incremental databases. However, t
&lt;/p&gt;</description></item><item><title>&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#30456;&#21516;&#39044;&#31639;&#19979;&#20135;&#29983;&#21487;&#38752;&#30340;&#25913;&#36827;&#65292;&#20294;&#22312;&#26080;&#27861;&#36827;&#34892;&#21333;&#20803;&#27979;&#35797;&#30340;&#24773;&#20917;&#19979;&#65292;&#36739;&#23567;&#30340;&#27169;&#22411;&#36873;&#25321;&#25490;&#21517;&#27425;&#20110;&#36739;&#22823;&#27169;&#22411;&#30340;&#21333;&#20010;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2404.00725</link><description>&lt;p&gt;
&#36234;&#22823;&#36234;&#22909;&#21527;&#65311;&#36890;&#36807;&#39044;&#31639;&#37325;&#26032;&#20998;&#37197;&#25913;&#36827;LLM&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
The Larger the Better? Improved LLM Code-Generation via Budget Reallocation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00725
&lt;/p&gt;
&lt;p&gt;
&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#30456;&#21516;&#39044;&#31639;&#19979;&#20135;&#29983;&#21487;&#38752;&#30340;&#25913;&#36827;&#65292;&#20294;&#22312;&#26080;&#27861;&#36827;&#34892;&#21333;&#20803;&#27979;&#35797;&#30340;&#24773;&#20917;&#19979;&#65292;&#36739;&#23567;&#30340;&#27169;&#22411;&#36873;&#25321;&#25490;&#21517;&#27425;&#20110;&#36739;&#22823;&#27169;&#22411;&#30340;&#21333;&#20010;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#27604;&#36739;&#23567;&#30340;&#27169;&#22411;&#26356;&#22909;&#12290;&#28982;&#32780;&#65292;&#26356;&#22823;&#30340;&#27169;&#22411;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#20063;&#38656;&#35201;&#26356;&#22810;&#30340;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#36825;&#23601;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#24403;&#20004;&#20010;&#27169;&#22411;&#22312;&#30456;&#21516;&#30340;&#39044;&#31639;&#19979;&#36816;&#34892;&#26102;&#20250;&#21457;&#29983;&#20160;&#20040;&#65311;&#65288;&#20363;&#22914;&#65292;&#35745;&#31639;&#36164;&#28304;&#65292;&#36816;&#34892;&#26102;&#38388;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#21508;&#31181;&#22823;&#23567;&#30340;&#20195;&#30721;&#29983;&#25104;LLMs&#65292;&#24182;&#36827;&#34892;&#27604;&#36739;&#65292;&#20363;&#22914;&#36816;&#34892;&#19968;&#20010;70B&#27169;&#22411;&#19968;&#27425;&#19982;&#20174;13B&#27169;&#22411;&#29983;&#25104;&#20116;&#20010;&#36755;&#20986;&#24182;&#36873;&#25321;&#19968;&#20010;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26631;&#20934;&#21333;&#20803;&#27979;&#35797;&#35774;&#32622;&#20013;&#65292;&#21453;&#22797;&#20351;&#29992;&#36739;&#23567;&#30340;&#27169;&#22411;&#21487;&#20197;&#20135;&#29983;&#19968;&#33268;&#30340;&#25913;&#36827;&#65292;&#22312;&#20116;&#20010;&#20219;&#21153;&#20013;&#26368;&#39640;&#21487;&#36798;15%&#30340;&#22686;&#30410;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;&#26080;&#27861;&#36827;&#34892;&#21333;&#20803;&#27979;&#35797;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#36739;&#23567;&#27169;&#22411;&#20013;&#22522;&#20110;&#25490;&#21517;&#30340;&#20505;&#36873;&#36873;&#25321;&#34920;&#29616;&#19981;&#21450;&#26469;&#33258;&#36739;&#22823;&#27169;&#22411;&#30340;&#21333;&#20010;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#26174;&#20102;&#20351;&#29992;&#36739;&#23567;&#27169;&#22411;&#32780;&#38750;&#36739;&#22823;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00725v1 Announce Type: cross  Abstract: It is a common belief that large language models (LLMs) are better than smaller-sized ones. However, larger models also require significantly more time and compute during inference. This begs the question: what happens when both models operate under the same budget? (e.g., compute, run-time). To address this question, we analyze code generation LLMs of various sizes and make comparisons such as running a 70B model once vs. generating five outputs from a 13B model and selecting one. Our findings reveal that, in a standard unit-test setup, the repeated use of smaller models can yield consistent improvements, with gains of up to 15% across five tasks. On the other hand, in scenarios where unit-tests are unavailable, a ranking-based selection of candidates from the smaller model falls short of the performance of a single output from larger ones. Our results highlight the potential of using smaller models instead of larger ones, and the imp
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;Vision Transformer&#30340;DRCT&#26041;&#27861;&#37319;&#29992;&#21019;&#26032;&#30340;&#26426;&#21046;&#35299;&#20915;&#20102;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#20013;&#31354;&#38388;&#20449;&#24687;&#34928;&#20943;&#30340;&#38382;&#39064;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00722</link><description>&lt;p&gt;
DRCT&#65306;&#23558;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#20445;&#23384;&#22312;&#20449;&#24687;&#29942;&#39048;&#20043;&#22806;
&lt;/p&gt;
&lt;p&gt;
DRCT: Saving Image Super-resolution away from Information Bottleneck
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00722
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Vision Transformer&#30340;DRCT&#26041;&#27861;&#37319;&#29992;&#21019;&#26032;&#30340;&#26426;&#21046;&#35299;&#20915;&#20102;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#20013;&#31354;&#38388;&#20449;&#24687;&#34928;&#20943;&#30340;&#38382;&#39064;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;Vision Transformer&#30340;&#20302;&#23618;&#35270;&#35273;&#20219;&#21153;&#24212;&#29992;&#21462;&#24471;&#20102;&#24191;&#27867;&#30340;&#25104;&#21151;&#12290;&#19982;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#19981;&#21516;&#65292;Transformer&#26356;&#25797;&#38271;&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#21487;&#20197;&#21033;&#29992;&#38750;&#23616;&#37096;&#21306;&#22495;&#30340;&#20449;&#24687;&#37325;&#24314;&#22270;&#20687;&#12290;&#22312;&#36229;&#20998;&#36776;&#29575;&#39046;&#22495;&#65292;&#22522;&#20110;Swin Transformer&#30340;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#20027;&#27969;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#25429;&#25417;&#20840;&#23616;&#31354;&#38388;&#20449;&#24687;&#65292;&#24182;&#19988;&#20855;&#26377;&#26059;&#36716;&#31383;&#21475;&#27880;&#24847;&#26426;&#21046;&#65292;&#26377;&#21161;&#20110;&#22312;&#19981;&#21516;&#31383;&#21475;&#20043;&#38388;&#20132;&#25442;&#20449;&#24687;&#12290;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#25193;&#22823;&#24863;&#30693;&#37326;&#25110;&#35774;&#35745;&#22797;&#26434;&#32593;&#32476;&#26469;&#25552;&#39640;&#22270;&#20687;&#36136;&#37327;&#21644;&#32593;&#32476;&#25928;&#29575;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#31216;&#36190;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#21069;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#65292;&#30001;&#20110;&#28145;&#24230;&#22686;&#21152;&#65292;&#31354;&#38388;&#20449;&#24687;&#24448;&#24448;&#20250;&#20943;&#23569;&#65292;&#20174;&#32780;&#23548;&#33268;&#31354;&#38388;&#20449;&#24687;&#30340;&#20002;&#22833;&#65292;&#24182;&#26368;&#32456;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00722v1 Announce Type: cross  Abstract: In recent years, Vision Transformer-based applications to low-level vision tasks have achieved widespread success. Unlike CNN-based models, Transformers are more adept at capturing long-range dependencies, enabling the reconstruction of images utilizing information from non-local areas. In the domain of super-resolution, Swin-transformer-based approaches have become mainstream due to their capacity to capture global spatial information and their shifting-window attention mechanism that facilitates the interchange of information between different windows. Many researchers have enhanced image quality and network efficiency by expanding the receptive field or designing complex networks, yielding commendable results. However, we observed that spatial information tends to diminish during the forward propagation process due to increased depth, leading to a loss of spatial information and, consequently, limiting the model's potential. To addr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#26426;&#22120;&#23398;&#20064;&#35270;&#35282;&#32508;&#36848;&#20102;&#35745;&#31639;&#26426;&#33258;&#36866;&#24212;&#27979;&#35797;&#65288;CAT&#65289;&#65292;&#37325;&#28857;&#35299;&#26512;&#20854;&#27979;&#35797;&#38382;&#39064;&#36873;&#25321;&#31639;&#27861;&#21644;&#22914;&#20309;&#20248;&#21270;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;&#12289;&#39064;&#24211;&#26500;&#24314;&#21644;&#27979;&#35797;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2404.00712</link><description>&lt;p&gt;
&#35745;&#31639;&#26426;&#33258;&#36866;&#24212;&#27979;&#35797;&#32508;&#36848;&#65306;&#26426;&#22120;&#23398;&#20064;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Survey of Computerized Adaptive Testing: A Machine Learning Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#26426;&#22120;&#23398;&#20064;&#35270;&#35282;&#32508;&#36848;&#20102;&#35745;&#31639;&#26426;&#33258;&#36866;&#24212;&#27979;&#35797;&#65288;CAT&#65289;&#65292;&#37325;&#28857;&#35299;&#26512;&#20854;&#27979;&#35797;&#38382;&#39064;&#36873;&#25321;&#31639;&#27861;&#21644;&#22914;&#20309;&#20248;&#21270;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;&#12289;&#39064;&#24211;&#26500;&#24314;&#21644;&#27979;&#35797;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#33258;&#36866;&#24212;&#27979;&#35797;&#65288;CAT&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#12289;&#37327;&#36523;&#23450;&#21046;&#30340;&#35780;&#20272;&#32771;&#29983;&#29087;&#32451;&#31243;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26681;&#25454;&#20182;&#20204;&#30340;&#34920;&#29616;&#21160;&#24577;&#35843;&#25972;&#27979;&#35797;&#38382;&#39064;&#12290;CAT&#24191;&#27867;&#24212;&#29992;&#20110;&#25945;&#32946;&#12289;&#21307;&#30103;&#12289;&#20307;&#32946;&#21644;&#31038;&#20250;&#23398;&#31561;&#22810;&#20010;&#39046;&#22495;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#27979;&#35797;&#23454;&#36341;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22823;&#35268;&#27169;&#27979;&#35797;&#30340;&#22686;&#21152;&#22797;&#26434;&#24615;&#65292;CAT&#24050;&#32463;&#34701;&#21512;&#20102;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#20197;&#26426;&#22120;&#23398;&#20064;&#20026;&#37325;&#28857;&#30340;CAT&#32508;&#36848;&#65292;&#20174;&#26032;&#30340;&#35282;&#24230;&#35299;&#35835;&#36825;&#31181;&#33258;&#36866;&#24212;&#27979;&#35797;&#26041;&#27861;&#12290;&#36890;&#36807;&#30740;&#31350;CAT&#36866;&#24212;&#24615;&#26680;&#24515;&#30340;&#27979;&#35797;&#38382;&#39064;&#36873;&#25321;&#31639;&#27861;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20854;&#21151;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;&#12289;&#39064;&#24211;&#26500;&#24314;&#21644;CAT&#20013;&#30340;&#27979;&#35797;&#25511;&#21046;&#65292;&#25506;&#32034;&#20102;&#26426;&#22120;&#23398;&#20064;&#22914;&#20309;&#20248;&#21270;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#12290;&#36890;&#36807;&#23545;&#24403;&#21069;&#24773;&#20917;&#30340;&#20998;&#26512;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00712v1 Announce Type: cross  Abstract: Computerized Adaptive Testing (CAT) provides an efficient and tailored method for assessing the proficiency of examinees, by dynamically adjusting test questions based on their performance. Widely adopted across diverse fields like education, healthcare, sports, and sociology, CAT has revolutionized testing practices. While traditional methods rely on psychometrics and statistics, the increasing complexity of large-scale testing has spurred the integration of machine learning techniques. This paper aims to provide a machine learning-focused survey on CAT, presenting a fresh perspective on this adaptive testing method. By examining the test question selection algorithm at the heart of CAT's adaptivity, we shed light on its functionality. Furthermore, we delve into cognitive diagnosis models, question bank construction, and test control within CAT, exploring how machine learning can optimize these components. Through an analysis of curre
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#23610;&#24230;&#29305;&#24615;&#65292;&#21487;&#20197;&#39044;&#27979;&#20854;&#35821;&#35328;&#24615;&#33021;&#30340;&#25193;&#23637;&#36895;&#24230;&#65292;&#19982;&#25991;&#26412;-based&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#65292;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#24615;&#33021;&#25193;&#23637;&#36895;&#24230;&#24930;&#19977;&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>https://arxiv.org/abs/2404.00685</link><description>&lt;p&gt;
&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#23610;&#24230;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Scaling Properties of Speech Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00685
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#23610;&#24230;&#29305;&#24615;&#65292;&#21487;&#20197;&#39044;&#27979;&#20854;&#35821;&#35328;&#24615;&#33021;&#30340;&#25193;&#23637;&#36895;&#24230;&#65292;&#19982;&#25991;&#26412;-based&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#65292;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#24615;&#33021;&#25193;&#23637;&#36895;&#24230;&#24930;&#19977;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#65288;SLM&#65289;&#26088;&#22312;&#20174;&#21407;&#22987;&#38899;&#39057;&#20013;&#23398;&#20064;&#35821;&#35328;&#65292;&#32780;&#26080;&#38656;&#25991;&#26412;&#36164;&#28304;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#25105;&#20204;&#24403;&#21069;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#24369;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#30340;&#23610;&#24230;&#29305;&#24615;&#23545;&#35821;&#38899;&#27169;&#24577;&#25104;&#31435;&#65292;&#36825;&#20123;&#33021;&#21147;&#23558;&#38543;&#30528;&#35757;&#32451;&#25152;&#20351;&#29992;&#30340;&#35745;&#31639;&#37327;&#22686;&#21152;&#32780;&#25552;&#39640;&#12290;&#26412;&#25991;&#21033;&#29992;&#27169;&#22411;&#30340;&#23610;&#24230;&#34892;&#20026;&#26469;&#20272;&#35745;&#25105;&#20204;&#24403;&#21069;&#26041;&#27861;&#23558;&#20135;&#29983;&#20855;&#26377;&#25991;&#26412;-based&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33521;&#35821;&#29087;&#32451;&#24230;&#30340;SLM&#30340;&#23610;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00685v1 Announce Type: cross  Abstract: Speech Language Models (SLMs) aim to learn language from raw audio, without textual resources. Despite significant advances, our current models exhibit weak syntax and semantic abilities. However, if the scaling properties of neural language models hold for the speech modality, these abilities will improve as the amount of compute used for training increases. In this paper, we use models of this scaling behavior to estimate the scale at which our current methods will yield a SLM with the English proficiency of text-based Large Language Models (LLMs). We establish a strong correlation between pre-training loss and downstream syntactic and semantic performance in SLMs and LLMs, which results in predictable scaling of linguistic performance. We show that the linguistic performance of SLMs scales up to three orders of magnitude more slowly than that of text-based LLMs. Additionally, we study the benefits of synthetic data designed to boost
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#36890;&#36807;&#23637;&#31034;&#29983;&#25104;&#24335;&#26816;&#32034;&#21644;&#22810;&#21521;&#37327;&#23494;&#38598;&#26816;&#32034;&#20849;&#20139;&#30456;&#21516;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#34913;&#37327;&#25991;&#26723;&#19982;&#26597;&#35810;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00684</link><description>&lt;p&gt;
&#22810;&#21521;&#37327;&#23494;&#38598;&#26816;&#32034;&#20013;&#30340;&#29983;&#25104;&#24335;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Generative Retrieval as Multi-Vector Dense Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#36890;&#36807;&#23637;&#31034;&#29983;&#25104;&#24335;&#26816;&#32034;&#21644;&#22810;&#21521;&#37327;&#23494;&#38598;&#26816;&#32034;&#20849;&#20139;&#30456;&#21516;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#34913;&#37327;&#25991;&#26723;&#19982;&#26597;&#35810;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00684v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495;  &#25688;&#35201;: &#29983;&#25104;&#24335;&#26816;&#32034;&#20197;&#31471;&#23545;&#31471;&#26041;&#24335;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#26550;&#26500;&#20026;&#32473;&#23450;&#26597;&#35810;&#29983;&#25104;&#30456;&#20851;&#25991;&#26723;&#30340;&#26631;&#35782;&#31526;&#12290;&#29983;&#25104;&#24335;&#26816;&#32034;&#19982;&#20854;&#20182;&#26816;&#32034;&#26041;&#27861;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#20869;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24102;&#26377;&#21407;&#23376;&#26631;&#35782;&#31526;&#30340;&#29983;&#25104;&#24335;&#26816;&#32034;&#31561;&#25928;&#20110;&#21333;&#21521;&#37327;&#23494;&#38598;&#26816;&#32034;&#12290;&#22240;&#27492;&#65292;&#24403;&#20351;&#29992;&#20998;&#23618;&#35821;&#20041;&#26631;&#35782;&#31526;&#26102;&#65292;&#29983;&#25104;&#24335;&#26816;&#32034;&#34920;&#29616;&#20986;&#31867;&#20284;&#20110;&#23494;&#38598;&#26816;&#32034;&#20013;&#26641;&#32034;&#24341;&#20869;&#30340;&#20998;&#23618;&#25628;&#32034;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20165;&#19987;&#27880;&#20110;&#26816;&#32034;&#38454;&#27573;&#65292;&#26410;&#32771;&#34385;&#29983;&#25104;&#24335;&#26816;&#32034;&#35299;&#30721;&#22120;&#20869;&#30340;&#28145;&#23618;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00684v1 Announce Type: cross  Abstract: Generative retrieval generates identifiers of relevant documents in an end-to-end manner using a sequence-to-sequence architecture for a given query. The relation between generative retrieval and other retrieval methods, especially those based on matching within dense retrieval models, is not yet fully comprehended. Prior work has demonstrated that generative retrieval with atomic identifiers is equivalent to single-vector dense retrieval. Accordingly, generative retrieval exhibits behavior analogous to hierarchical search within a tree index in dense retrieval when using hierarchical semantic identifiers. However, prior work focuses solely on the retrieval stage without considering the deep interactions within the decoder of generative retrieval.   In this paper, we fill this gap by demonstrating that generative retrieval and multi-vector dense retrieval share the same framework for measuring the relevance to a query of a document. Sp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#35299;&#20915;&#26041;&#26696;&#65292;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#21333;&#31867;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#38469;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00675</link><description>&lt;p&gt;
LLM meets Vision-Language Models&#29992;&#20110;&#38646;&#26679;&#26412;&#21333;&#31867;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
LLM meets Vision-Language Models for Zero-Shot One-Class Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00675
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#35299;&#20915;&#26041;&#26696;&#65292;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#21333;&#31867;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#38469;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#38646;&#26679;&#26412;&#21333;&#31867;&#35270;&#35273;&#20998;&#31867;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20165;&#30446;&#26631;&#31867;&#21035;&#30340;&#26631;&#31614;&#21487;&#29992;&#65292;&#24182;&#19988;&#30446;&#26631;&#26159;&#22312;&#19981;&#38656;&#35201;&#26469;&#33258;&#30446;&#26631;&#20219;&#21153;&#30340;&#20219;&#20309;&#39564;&#35777;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#21306;&#20998;&#27491;&#36127;&#26597;&#35810;&#26679;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;&#35299;&#20915;&#26041;&#26696;&#65292;&#39318;&#20808;&#26597;&#35810;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#26597;&#25214;&#22312;&#35270;&#35273;&#19978;&#20196;&#20154;&#22256;&#24785;&#30340;&#23545;&#35937;&#65292;&#28982;&#21518;&#20381;&#36182;&#20110;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#20363;&#22914;CLIP&#65289;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#35843;&#25972;&#22823;&#35268;&#27169;&#35270;&#35273;&#22522;&#20934;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#27492;&#35774;&#32622;&#20013;&#20248;&#20110;&#33258;&#36866;&#24212;&#30340;&#29616;&#25104;&#26367;&#20195;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#38469;&#30340;&#22522;&#20934;&#65292;&#20854;&#20013;&#36127;&#26597;&#35810;&#26679;&#26412;&#20174;&#19982;&#27491;&#26597;&#35810;&#26679;&#26412;&#30456;&#21516;&#30340;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#65292;&#21253;&#25324;iNaturalist&#30340;&#32454;&#31890;&#24230;&#25511;&#21046;&#29256;&#26412;&#65292;&#20854;&#20013;&#36127;&#26679;&#26412;&#22312;&#20998;&#31867;&#26641;&#20013;&#19982;&#27491;&#26679;&#26412;&#30456;&#36317;&#22266;&#23450;&#36317;&#31163;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#21487;&#20197;&#36890;&#36807;&#26597;&#35810;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24182;&#20381;&#36182;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#35299;&#20915;&#38646;&#26679;&#26412;&#21333;&#31867;&#20998;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00675v1 Announce Type: cross  Abstract: We consider the problem of zero-shot one-class visual classification. In this setting, only the label of the target class is available, and the goal is to discriminate between positive and negative query samples without requiring any validation example from the target task. We propose a two-step solution that first queries large language models for visually confusing objects and then relies on vision-language pre-trained models (e.g., CLIP) to perform classification. By adapting large-scale vision benchmarks, we demonstrate the ability of the proposed method to outperform adapted off-the-shelf alternatives in this setting. Namely, we propose a realistic benchmark where negative query samples are drawn from the same original dataset as positive ones, including a granularity-controlled version of iNaturalist, where negative samples are at a fixed distance in the taxonomy tree from the positive ones. Our work shows that it is possible to 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#31532;&#19968;&#20010;&#20840;&#38754;&#35843;&#26597;&#27169;&#22411;&#35299;&#37322;&#20013;&#38544;&#31169;&#25915;&#20987;&#21450;&#20854;&#23545;&#25239;&#25514;&#26045;&#30340;&#35770;&#25991;&#65292;&#36890;&#36807;&#20998;&#31867;&#38544;&#31169;&#25915;&#20987;&#21644;&#23545;&#25239;&#25514;&#26045;&#65292;&#21021;&#27493;&#25506;&#35752;&#20102;&#38544;&#31169;&#27844;&#28431;&#21407;&#22240;&#65292;&#25552;&#20986;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2404.00673</link><description>&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#22411;&#27169;&#22411;&#35299;&#37322;&#30740;&#31350;&#32508;&#36848;&#65306;&#38544;&#31169;&#39118;&#38505;&#12289;&#25915;&#20987;&#21644;&#23545;&#25239;&#25514;&#26045;
&lt;/p&gt;
&lt;p&gt;
A Survey of Privacy-Preserving Model Explanations: Privacy Risks, Attacks, and Countermeasures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#31532;&#19968;&#20010;&#20840;&#38754;&#35843;&#26597;&#27169;&#22411;&#35299;&#37322;&#20013;&#38544;&#31169;&#25915;&#20987;&#21450;&#20854;&#23545;&#25239;&#25514;&#26045;&#30340;&#35770;&#25991;&#65292;&#36890;&#36807;&#20998;&#31867;&#38544;&#31169;&#25915;&#20987;&#21644;&#23545;&#25239;&#25514;&#26045;&#65292;&#21021;&#27493;&#25506;&#35752;&#20102;&#38544;&#31169;&#27844;&#28431;&#21407;&#22240;&#65292;&#25552;&#20986;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#37319;&#29992;&#19981;&#26029;&#25193;&#22823;&#65292;&#35299;&#20915;&#20854;&#38544;&#31169;&#24433;&#21709;&#30340;&#32039;&#36843;&#24615;&#21464;&#24471;&#26356;&#21152;&#36843;&#20999;&#12290;&#23613;&#31649;&#22312;&#20154;&#24037;&#26234;&#33021;&#38544;&#31169;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#26377;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#38544;&#31169;&#20445;&#25252;&#22411;&#27169;&#22411;&#35299;&#37322;&#21364;&#40092;&#26377;&#20851;&#27880;&#12290;&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#35843;&#26597;&#20102;&#27169;&#22411;&#35299;&#37322;&#30340;&#38544;&#31169;&#25915;&#20987;&#21450;&#20854;&#23545;&#25239;&#25514;&#26045;&#12290;&#25105;&#20204;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#36129;&#29486;&#21253;&#25324;&#23545;&#30740;&#31350;&#35770;&#25991;&#36827;&#34892;&#24443;&#24213;&#20998;&#26512;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#30456;&#20114;&#36830;&#25509;&#30340;&#20998;&#31867;&#27861;&#65292;&#20415;&#20110;&#26681;&#25454;&#30446;&#26631;&#35299;&#37322;&#23545;&#38544;&#31169;&#25915;&#20987;&#21644;&#23545;&#25239;&#25514;&#26045;&#36827;&#34892;&#20998;&#31867;&#12290;&#26412;&#30740;&#31350;&#36824;&#23545;&#38544;&#31169;&#27844;&#28431;&#21407;&#22240;&#36827;&#34892;&#20102;&#21021;&#27493;&#35843;&#26597;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#20998;&#26512;&#20013;&#21457;&#29616;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#35813;&#35843;&#26597;&#26088;&#22312;&#25104;&#20026;&#30740;&#31350;&#30028;&#30340;&#23453;&#36149;&#36164;&#28304;&#65292;&#24182;&#20026;&#36825;&#19968;&#39046;&#22495;&#30340;&#26032;&#25163;&#25552;&#20379;&#26126;&#30830;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00673v1 Announce Type: cross  Abstract: As the adoption of explainable AI (XAI) continues to expand, the urgency to address its privacy implications intensifies. Despite a growing corpus of research in AI privacy and explainability, there is little attention on privacy-preserving model explanations. This article presents the first thorough survey about privacy attacks on model explanations and their countermeasures. Our contribution to this field comprises a thorough analysis of research papers with a connected taxonomy that facilitates the categorisation of privacy attacks and countermeasures based on the targeted explanations. This work also includes an initial investigation into the causes of privacy leaks. Finally, we discuss unresolved issues and prospective research directions uncovered in our analysis. This survey aims to be a valuable resource for the research community and offers clear insights for those new to this domain. To support ongoing research, we have estab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ToE&#30340;&#20196;&#29260;&#29983;&#38271;&#26041;&#26696;&#65292;&#26088;&#22312;&#36890;&#36807;&#21152;&#36895;&#19968;&#33268;&#24615;&#35757;&#32451;&#26469;&#25913;&#21892;ViT&#30340;&#35757;&#32451;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.00672</link><description>&lt;p&gt;
&#36890;&#36807;&#20196;&#29260;&#25193;&#23637;&#23454;&#29616;Transformer&#30340;&#19968;&#33324;&#39640;&#25928;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
A General and Efficient Training for Transformer via Token Expansion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ToE&#30340;&#20196;&#29260;&#29983;&#38271;&#26041;&#26696;&#65292;&#26088;&#22312;&#36890;&#36807;&#21152;&#36895;&#19968;&#33268;&#24615;&#35757;&#32451;&#26469;&#25913;&#21892;ViT&#30340;&#35757;&#32451;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Vision Transformer&#65288;ViT&#65289;&#36890;&#24120;&#38656;&#35201;&#26497;&#22823;&#30340;&#35757;&#32451;&#25104;&#26412;&#25165;&#33021;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#20196;&#29260;&#29983;&#38271;&#26041;&#26696;Token Expansion (ToE)&#65292;&#20197;&#23454;&#29616;ViT&#30340;&#19968;&#33268;&#24615;&#35757;&#32451;&#21152;&#36895;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#8220;&#21021;&#22987;&#21270;-&#25193;&#23637;-&#21512;&#24182;&#8221;&#31649;&#36947;&#65292;&#20197;&#20445;&#25345;&#21407;&#22987;Transformer&#30340;&#20013;&#38388;&#29305;&#24449;&#20998;&#24067;&#30340;&#23436;&#25972;&#24615;&#65292;&#38450;&#27490;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20002;&#22833;&#20851;&#38190;&#30340;&#21487;&#23398;&#20064;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00672v1 Announce Type: cross  Abstract: The remarkable performance of Vision Transformers (ViTs) typically requires an extremely large training cost. Existing methods have attempted to accelerate the training of ViTs, yet typically disregard method universality with accuracy dropping. Meanwhile, they break the training consistency of the original transformers, including the consistency of hyper-parameters, architecture, and strategy, which prevents them from being widely applied to different Transformer networks. In this paper, we propose a novel token growth scheme Token Expansion (termed ToE) to achieve consistent training acceleration for ViTs. We introduce an "initialization-expansion-merging" pipeline to maintain the integrity of the intermediate feature distribution of original transformers, preventing the loss of crucial learnable information in the training process. ToE can not only be seamlessly integrated into the training and fine-tuning process of transformers (e
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22238;&#39038;&#20102;&#23545;&#25216;&#26415;&#25991;&#26723;RAG&#31995;&#32479;&#26500;&#24314;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#26368;&#20339;&#23454;&#36341;&#21644;&#28508;&#22312;&#25361;&#25112;</title><link>https://arxiv.org/abs/2404.00657</link><description>&lt;p&gt;
&#20851;&#20110;&#20026;&#25216;&#26415;&#25991;&#26723;&#26500;&#24314;RAG&#31995;&#32479;&#30340;&#35266;&#23519;
&lt;/p&gt;
&lt;p&gt;
Observations on Building RAG Systems for Technical Documents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00657
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22238;&#39038;&#20102;&#23545;&#25216;&#26415;&#25991;&#26723;RAG&#31995;&#32479;&#26500;&#24314;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#26368;&#20339;&#23454;&#36341;&#21644;&#28508;&#22312;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
RAG&#65288;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65289;&#29992;&#20110;&#25216;&#26415;&#25991;&#26723;&#26102;&#23384;&#22312;&#25361;&#25112;&#65292;&#22240;&#20026;&#23884;&#20837;&#36890;&#24120;&#26080;&#27861;&#25429;&#25417;&#39046;&#22495;&#20449;&#24687;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#24433;&#21709;RAG&#30340;&#37325;&#35201;&#22240;&#32032;&#30340;&#20808;&#21069;&#30740;&#31350;&#65292;&#24182;&#36827;&#34892;&#23454;&#39564;&#20197;&#31361;&#20986;&#26500;&#24314;&#25216;&#26415;&#25991;&#26723;RAG&#31995;&#32479;&#30340;&#26368;&#20339;&#23454;&#36341;&#21644;&#28508;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00657v1 Announce Type: cross  Abstract: Retrieval augmented generation (RAG) for technical documents creates challenges as embeddings do not often capture domain information. We review prior art for important factors affecting RAG and perform experiments to highlight best practices and potential challenges to build RAG systems for technical documents.
&lt;/p&gt;</description></item><item><title>WavLLM&#26159;&#19968;&#20010;&#31283;&#20581;&#21644;&#33258;&#36866;&#24212;&#35821;&#38899;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#21452;&#32534;&#30721;&#22120;&#21644;Prompt-aware LoRA&#26435;&#37325;&#36866;&#37197;&#22120;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#20248;&#21270;&#65292;&#35299;&#32806;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#38899;&#20449;&#24687;&#65292;&#20026;&#22788;&#29702;&#35821;&#20041;&#20869;&#23481;&#21644;&#35828;&#35805;&#32773;&#36523;&#20221;&#30340;&#29420;&#29305;&#29305;&#24449;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;</title><link>https://arxiv.org/abs/2404.00656</link><description>&lt;p&gt;
WavLLM&#65306;&#38754;&#21521;&#31283;&#20581;&#21644;&#33258;&#36866;&#24212;&#35821;&#38899;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
WavLLM: Towards Robust and Adaptive Speech Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00656
&lt;/p&gt;
&lt;p&gt;
WavLLM&#26159;&#19968;&#20010;&#31283;&#20581;&#21644;&#33258;&#36866;&#24212;&#35821;&#38899;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#21452;&#32534;&#30721;&#22120;&#21644;Prompt-aware LoRA&#26435;&#37325;&#36866;&#37197;&#22120;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#20248;&#21270;&#65292;&#35299;&#32806;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#38899;&#20449;&#24687;&#65292;&#20026;&#22788;&#29702;&#35821;&#20041;&#20869;&#23481;&#21644;&#35828;&#35805;&#32773;&#36523;&#20221;&#30340;&#29420;&#29305;&#29305;&#24449;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#26368;&#26032;&#36827;&#23637;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#36880;&#28176;&#25299;&#23485;&#20102;&#23427;&#20204;&#30340;&#33539;&#22260;&#21040;&#22810;&#27169;&#24577;&#24863;&#30693;&#21644;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#23558;&#21548;&#35273;&#33021;&#21147;&#25972;&#21512;&#21040;LLMs&#20013;&#20250;&#24102;&#26469;&#26174;&#33879;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#27867;&#21270;&#36328;&#19981;&#21516;&#35821;&#22659;&#21644;&#25191;&#34892;&#22797;&#26434;&#21548;&#35273;&#20219;&#21153;&#26041;&#38754;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;WavLLM&#65292;&#19968;&#20010;&#20855;&#26377;&#21452;&#32534;&#30721;&#22120;&#21644;Prompt-aware LoRA&#26435;&#37325;&#36866;&#37197;&#22120;&#30340;&#31283;&#20581;&#21644;&#33258;&#36866;&#24212;&#35821;&#38899;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#21033;&#29992;&#21452;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#35299;&#32806;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#38899;&#20449;&#24687;&#65292;&#21033;&#29992;Whisper&#32534;&#30721;&#22120;&#22788;&#29702;&#35821;&#38899;&#30340;&#35821;&#20041;&#20869;&#23481;&#65292;&#21033;&#29992;WavLM&#32534;&#30721;&#22120;&#25429;&#25417;&#35828;&#35805;&#32773;&#36523;&#20221;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#22312;&#35838;&#31243;&#23398;&#20064;&#26694;&#26550;&#20869;&#65292;WavLLM&#39318;&#20808;&#36890;&#36807;&#28151;&#21512;&#35201;&#32032;&#36827;&#34892;&#20248;&#21270;&#26469;&#24314;&#31435;&#20854;&#22522;&#30784;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00656v1 Announce Type: cross  Abstract: The recent advancements in large language models (LLMs) have revolutionized the field of natural language processing, progressively broadening their scope to multimodal perception and generation. However, effectively integrating listening capabilities into LLMs poses significant challenges, particularly with respect to generalizing across varied contexts and executing complex auditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach. Leveraging dual encoders, we decouple different types of speech information, utilizing a Whisper encoder to process the semantic content of speech, and a WavLM encoder to capture the unique characteristics of the speaker's identity. Within the curriculum learning framework, WavLLM first builds its foundational capabilities by optimizing on mixed elemen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#39044;&#27979;&#27169;&#22411;&#21644;&#31163;&#32447;&#23398;&#20064;&#20803;&#32032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20869;&#22312;&#22870;&#21169;&#19982;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#20851;&#32852;&#65292;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26679;&#26412;&#26377;&#25928;&#30340;&#25506;&#32034;&#12290;</title><link>https://arxiv.org/abs/2404.00651</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#27169;&#22411;&#30340;&#20869;&#22312;&#21160;&#26426;&#23398;&#20064;&#31163;&#32447;&#31574;&#30053;&#20197;&#23454;&#29616;&#20027;&#21160;&#22312;&#32447;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Learning Off-policy with Model-based Intrinsic Motivation For Active Online Exploration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#39044;&#27979;&#27169;&#22411;&#21644;&#31163;&#32447;&#23398;&#20064;&#20803;&#32032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20869;&#22312;&#22870;&#21169;&#19982;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#20851;&#32852;&#65292;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26679;&#26412;&#26377;&#25928;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#65292;&#26368;&#36817;&#30340;&#36827;&#23637;&#22312;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#65292;&#28085;&#30422;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#21644;&#26080;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#23454;&#29616;&#26679;&#26412;&#26377;&#25928;&#30340;&#25506;&#32034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#39044;&#27979;&#27169;&#22411;&#21644;&#31163;&#32447;&#23398;&#20064;&#20803;&#32032;&#65292;&#20854;&#20013;&#22312;&#32447;&#35268;&#21010;&#22120;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#24863;&#30693;&#30340;&#32456;&#31471;&#20215;&#20540;&#20989;&#25968;&#29992;&#20110;&#26679;&#26412;&#25910;&#38598;&#12290;&#21033;&#29992;&#28508;&#22312;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#21069;&#21521;&#39044;&#27979;&#38169;&#35823;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#31181;&#20869;&#22312;&#22870;&#21169;&#65292;&#32780;&#19981;&#20250;&#20135;&#29983;&#21442;&#25968;&#24320;&#38144;&#12290;&#35813;&#22870;&#21169;&#24314;&#31435;&#20102;&#19982;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#29282;&#22266;&#32852;&#31995;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#26377;&#25928;&#22320;&#20811;&#26381;&#28176;&#36817;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00651v1 Announce Type: cross  Abstract: Recent advancements in deep reinforcement learning (RL) have demonstrated notable progress in sample efficiency, spanning both model-based and model-free paradigms. Despite the identification and mitigation of specific bottlenecks in prior works, the agent's exploration ability remains under-emphasized in the realm of sample-efficient RL. This paper investigates how to achieve sample-efficient exploration in continuous control tasks. We introduce an RL algorithm that incorporates a predictive model and off-policy learning elements, where an online planner enhanced by a novelty-aware terminal value function is employed for sample collection. Leveraging the forward predictive error within a latent state space, we derive an intrinsic reward without incurring parameters overhead. This reward establishes a solid connection to model uncertainty, allowing the agent to effectively overcome the asymptotic performance gap. Through extensive expe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#27425;&#24615;&#30340;3D&#24863;&#30693;&#32918;&#20687;&#21160;&#30011;&#26041;&#27861;Export3D&#65292;&#36890;&#36807;&#24341;&#20837;&#19977;&#24179;&#38754;&#29983;&#25104;&#22120;&#21644;&#23545;&#27604;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#25511;&#21046;&#32473;&#23450;&#32918;&#20687;&#22270;&#20687;&#30340;&#38754;&#37096;&#34920;&#24773;&#21644;&#25668;&#20687;&#26426;&#35270;&#35282;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#34920;&#36798;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2404.00636</link><description>&lt;p&gt;
&#23398;&#20064;&#29983;&#25104;&#26465;&#20214;&#21270;&#19977;&#24179;&#38754;&#29992;&#20110;3D&#24863;&#30693;&#34920;&#24773;&#21487;&#25511;&#32918;&#20687;&#21160;&#30011;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Conditional Tri-plane for 3D-aware Expression Controllable Portrait Animation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#27425;&#24615;&#30340;3D&#24863;&#30693;&#32918;&#20687;&#21160;&#30011;&#26041;&#27861;Export3D&#65292;&#36890;&#36807;&#24341;&#20837;&#19977;&#24179;&#38754;&#29983;&#25104;&#22120;&#21644;&#23545;&#27604;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#25511;&#21046;&#32473;&#23450;&#32918;&#20687;&#22270;&#20687;&#30340;&#38754;&#37096;&#34920;&#24773;&#21644;&#25668;&#20687;&#26426;&#35270;&#35282;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#34920;&#36798;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#27425;&#24615;&#30340;3D&#24863;&#30693;&#32918;&#20687;&#21160;&#30011;&#26041;&#27861;Export3D&#65292;&#33021;&#22815;&#25511;&#21046;&#32473;&#23450;&#32918;&#20687;&#22270;&#20687;&#30340;&#38754;&#37096;&#34920;&#24773;&#21644;&#25668;&#20687;&#26426;&#35270;&#35282;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#19977;&#24179;&#38754;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#23558;3DMM&#30340;&#34920;&#24773;&#21442;&#25968;&#36716;&#31227;&#21040;&#28304;&#22270;&#20687;&#20013;&#30452;&#25509;&#29983;&#25104;3D&#20808;&#39564;&#30340;&#19977;&#24179;&#38754;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#21487;&#24494;&#20998;&#20307;&#31215;&#28210;&#26579;&#23558;&#19977;&#24179;&#38754;&#35299;&#30721;&#20026;&#19981;&#21516;&#35270;&#35282;&#30340;&#22270;&#20687;&#12290;&#29616;&#26377;&#30340;&#32918;&#20687;&#21160;&#30011;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#22270;&#20687;&#21464;&#24418;&#26469;&#22312;&#36816;&#21160;&#31354;&#38388;&#20013;&#20256;&#36755;&#34920;&#24773;&#65292;&#25361;&#25112;&#22312;&#22806;&#35266;&#21644;&#34920;&#24773;&#30340;&#20998;&#31163;&#19978;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26080;&#22806;&#35266;&#34920;&#24773;&#21442;&#25968;&#30340;&#23545;&#27604;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#28040;&#38500;&#20102;&#22312;&#20256;&#36755;&#36328;&#36523;&#20221;&#34920;&#36798;&#26102;&#19981;&#33391;&#22806;&#35266;&#20132;&#25442;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#33021;&#22815;&#23398;&#20064;&#38544;&#34255;&#22312;3DMM&#20013;&#30340;&#26080;&#22806;&#35266;&#34920;&#36798;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00636v1 Announce Type: cross  Abstract: In this paper, we present Export3D, a one-shot 3D-aware portrait animation method that is able to control the facial expression and camera view of a given portrait image. To achieve this, we introduce a tri-plane generator that directly generates a tri-plane of 3D prior by transferring the expression parameter of 3DMM into the source image. The tri-plane is then decoded into the image of different view through a differentiable volume rendering. Existing portrait animation methods heavily rely on image warping to transfer the expression in the motion space, challenging on disentanglement of appearance and expression. In contrast, we propose a contrastive pre-training framework for appearance-free expression parameter, eliminating undesirable appearance swap when transferring a cross-identity expression. Extensive experiments show that our pre-training framework can learn the appearance-free expression representation hidden in 3DMM, and 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#35757;&#32451;&#19968;&#20010;&#29992;&#20110;&#35268;&#21010;&#26410;&#26469;&#20889;&#20316;&#36807;&#31243;&#30340;&#27169;&#22359;&#65292;&#25193;&#23637;&#20102;&#25104;&#21151;&#30340;&#35821;&#35328;&#27169;&#22411;&#20844;&#24335;&#21040;&#26356;&#25277;&#35937;&#30340;&#35268;&#21010;&#20013;&#65292;&#25913;&#21892;&#20102;&#35821;&#35328;&#24314;&#27169;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#25991;&#26412;&#32467;&#26500;&#26041;&#38754;&#65292;&#21516;&#26102;&#26032;&#30340;&#35268;&#21010;&#27169;&#22359;&#21487;&#20197;&#22823;&#35268;&#27169;&#35757;&#32451;&#24182;&#36731;&#26494;&#19982;&#31038;&#21306;&#20849;&#20139;&#12290;</title><link>https://arxiv.org/abs/2404.00614</link><description>&lt;p&gt;
&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#23398;&#20064;&#35821;&#35328;&#24314;&#27169;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Learning to Plan for Language Modeling from Unlabeled Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00614
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#35757;&#32451;&#19968;&#20010;&#29992;&#20110;&#35268;&#21010;&#26410;&#26469;&#20889;&#20316;&#36807;&#31243;&#30340;&#27169;&#22359;&#65292;&#25193;&#23637;&#20102;&#25104;&#21151;&#30340;&#35821;&#35328;&#27169;&#22411;&#20844;&#24335;&#21040;&#26356;&#25277;&#35937;&#30340;&#35268;&#21010;&#20013;&#65292;&#25913;&#21892;&#20102;&#35821;&#35328;&#24314;&#27169;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#25991;&#26412;&#32467;&#26500;&#26041;&#38754;&#65292;&#21516;&#26102;&#26032;&#30340;&#35268;&#21010;&#27169;&#22359;&#21487;&#20197;&#22823;&#35268;&#27169;&#35757;&#32451;&#24182;&#36731;&#26494;&#19982;&#31038;&#21306;&#20849;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35757;&#32451;&#26469;&#39044;&#27979;&#26410;&#26631;&#35760;&#35821;&#26009;&#24211;&#20013;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23398;&#20250;&#25191;&#34892;&#35768;&#22810;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30446;&#26631;&#21487;&#20197;&#35828;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#38656;&#35201;&#35268;&#21010;&#30340;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#65292;&#27604;&#22914;&#20889;&#20316;&#19968;&#31687;&#36830;&#36143;&#30340;&#25991;&#31456;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#35757;&#32451;&#19968;&#20010;&#29992;&#20110;&#35268;&#21010;&#26410;&#26469;&#20889;&#20316;&#36807;&#31243;&#30340;&#27169;&#22359;&#12290;&#36890;&#36807;&#26681;&#25454;&#29983;&#25104;&#30340;&#28508;&#22312;&#35745;&#21010;&#36827;&#34892;&#26465;&#20214;&#21270;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#23558;&#25104;&#21151;&#30340;&#35821;&#35328;&#27169;&#22411;&#20844;&#24335;&#25193;&#23637;&#21040;&#26356;&#25277;&#35937;&#30340;&#35268;&#21010;&#20013;&#12290;&#23454;&#39564;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#25913;&#21892;&#20102;&#35821;&#35328;&#24314;&#27169;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#25991;&#26412;&#32467;&#26500;&#26041;&#38754;&#12290;&#30001;&#20110;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#30340;&#26159;&#26080;&#30417;&#30563;&#19988;&#22806;&#37096;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#21010;&#27169;&#22359;&#65292;&#22240;&#27492;&#26032;&#30340;&#35268;&#21010;&#27169;&#22359;&#21487;&#20197;&#22823;&#35268;&#27169;&#35757;&#32451;&#65292;&#24182;&#19988;&#33021;&#22815;&#36731;&#26494;&#22320;&#19982;&#31038;&#21306;&#20849;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00614v1 Announce Type: cross  Abstract: By training to predict the next token in an unlabeled corpus, large language models learn to perform many tasks without any labeled data. However, their next-token-prediction objective arguably limits their performance in scenarios that require planning, such as writing a coherent article. In this paper, we train a module for planning the future writing process via a self-supervised learning objective. By conditioning on generated latent plans, our model extends the successful language model formula to more abstract planning in an unsupervised way. Empirically, we demonstrate that our method improves language modeling performance in general, particularly with respect to the text structure. Because our framework uses a planner module that is unsupervised and external to the language model, new planner modules can be trained at large scale and easily be shared with the community.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24191;&#27867;&#33258;&#23545;&#27604;&#29983;&#25104;&#36127;&#20363;&#30340;&#26080;&#38656;&#21453;&#39304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.00604</link><description>&lt;p&gt;
&#24191;&#27867;&#30340;&#33258;&#23545;&#27604;&#20351;&#24471;&#26080;&#38656;&#21453;&#39304;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#25104;&#20026;&#21487;&#33021;
&lt;/p&gt;
&lt;p&gt;
Extensive Self-Contrast Enables Feedback-Free Language Model Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24191;&#27867;&#33258;&#23545;&#27604;&#29983;&#25104;&#36127;&#20363;&#30340;&#26080;&#38656;&#21453;&#39304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#19968;&#30452;&#26159;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;&#40784;&#30340;&#26680;&#24515;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#20854;&#20005;&#37325;&#20381;&#36182;&#26114;&#36149;&#30340;&#20154;&#31867;&#25110;LLM&#20316;&#20026;&#35780;&#21028;&#32773;&#30340;&#20559;&#22909;&#21453;&#39304;&#21487;&#33021;&#20250;&#38459;&#30861;&#20854;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Self-Contrast&#65292;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#24191;&#27867;&#33258;&#21160;&#29983;&#25104;&#30340;&#36127;&#20363;&#26469;&#36827;&#34892;&#26080;&#38656;&#21453;&#39304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#26041;&#27861;&#12290;&#20165;&#36890;&#36807;&#30417;&#30563;&#30340;&#24494;&#35843;&#65288;SFT&#65289;&#30446;&#26631;&#65292;Self-Contrast&#21033;&#29992;LLM&#26412;&#36523;&#29983;&#25104;&#22823;&#37327;&#22810;&#26679;&#30340;&#20505;&#36873;&#39033;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#27169;&#22411;&#26681;&#25454;&#25991;&#26412;&#30456;&#20284;&#24615;&#36807;&#28388;&#22810;&#20010;&#36127;&#20363;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;&#20165;&#20165;&#25193;&#22823;&#36127;&#38754;&#22238;&#24212;&#20173;&#28982;&#21487;&#20197;&#26377;&#25928;&#22320;&#36817;&#20284;&#20855;&#26377;&#26356;&#24179;&#34913;&#30340;&#27491;&#38754;&#21644;&#36127;&#38754;&#20559;&#22909;&#27880;&#37322;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#23545;&#19977;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Self-Contrast&#33021;&#22815;&#22987;&#32456;&#20248;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00604v1 Announce Type: cross  Abstract: Reinforcement learning from human feedback (RLHF) has been a central technique for recent large language model (LLM) alignment. However, its heavy dependence on costly human or LLM-as-Judge preference feedback could stymie its wider applications. In this work, we introduce Self-Contrast, a feedback-free large language model alignment method via exploiting extensive self-generated negatives. With only supervised fine-tuning (SFT) targets, Self-Contrast leverages the LLM itself to generate massive diverse candidates, and harnesses a pre-trained embedding model to filter multiple negatives according to text similarity. Theoretically, we illustrate that in this setting, merely scaling negative responses can still effectively approximate situations with more balanced positive and negative preference annotations. Our experiments with direct preference optimization (DPO) on three datasets show that, Self-Contrast could consistently outperform
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#35752;&#35770;&#20102;&#20154;&#31867;&#30417;&#30563;&#12289;&#36947;&#24503;&#30417;&#30563;&#21644;&#38544;&#31169;&#24433;&#21709;&#35780;&#20272;&#22312;&#38754;&#23545;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21457;&#23637;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00600</link><description>&lt;p&gt;
AI&#27861;&#24459;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65306;&#24403;&#20851;&#38190;&#38382;&#39064;&#21644;&#38544;&#31169;&#24433;&#21709;&#38656;&#35201;&#20154;&#31867;&#21644;&#36947;&#24503;&#30417;&#30563;&#26102;
&lt;/p&gt;
&lt;p&gt;
AI Act and Large Language Models (LLMs): When critical issues and privacy impact require human and ethical oversight
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00600
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35752;&#35770;&#20102;&#20154;&#31867;&#30417;&#30563;&#12289;&#36947;&#24503;&#30417;&#30563;&#21644;&#38544;&#31169;&#24433;&#21709;&#35780;&#20272;&#22312;&#38754;&#23545;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21457;&#23637;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#26085;&#30410;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#65292;&#20351;&#24471;&#26377;&#24517;&#35201;&#23545;&#23427;&#20204;&#22312;&#38544;&#31169;&#12289;&#20010;&#20154;&#25968;&#25454;&#20445;&#25252;&#20197;&#21450;&#36947;&#24503;&#23618;&#38754;&#65292;&#23588;&#20854;&#26159;&#23545;&#26368;&#33030;&#24369;&#21644;&#26368;&#24369;&#21183;&#32676;&#20307;&#21487;&#33021;&#20135;&#29983;&#30340;&#39118;&#38505;&#21644;&#24433;&#21709;&#36827;&#34892;&#35780;&#20272;&#12290;&#26412;&#25991;&#23545;&#20154;&#31867;&#30417;&#30563;&#12289;&#36947;&#24503;&#30417;&#30563;&#21644;&#38544;&#31169;&#24433;&#21709;&#35780;&#20272;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00600v1 Announce Type: cross  Abstract: The imposing evolution of artificial intelligence systems and, specifically, of Large Language Models (LLM) makes it necessary to carry out assessments of their level of risk and the impact they may have in the area of privacy, personal data protection and at an ethical level, especially on the weakest and most vulnerable. This contribution addresses human oversight, ethical oversight, and privacy impact assessment.
&lt;/p&gt;</description></item><item><title>EvoCodeBench &#26159;&#19968;&#20010;&#19982;&#29616;&#23454;&#19990;&#30028;&#20195;&#30721;&#24211;&#23545;&#40784;&#30340;&#36827;&#21270;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#65292;&#20855;&#26377;&#23436;&#21892;&#30340;&#27880;&#37322;&#21644;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#65292;&#21516;&#26102;&#36991;&#20813;&#25968;&#25454;&#27844;&#38706;&#12290;</title><link>https://arxiv.org/abs/2404.00599</link><description>&lt;p&gt;
EvoCodeBench: &#19968;&#20010;&#19982;&#29616;&#23454;&#19990;&#30028;&#20195;&#30721;&#24211;&#23545;&#40784;&#30340;&#36827;&#21270;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
EvoCodeBench: An Evolving Code Generation Benchmark Aligned with Real-World Code Repositories
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00599
&lt;/p&gt;
&lt;p&gt;
EvoCodeBench &#26159;&#19968;&#20010;&#19982;&#29616;&#23454;&#19990;&#30028;&#20195;&#30721;&#24211;&#23545;&#40784;&#30340;&#36827;&#21270;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#65292;&#20855;&#26377;&#23436;&#21892;&#30340;&#27880;&#37322;&#21644;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#65292;&#21516;&#26102;&#36991;&#20813;&#25968;&#25454;&#27844;&#38706;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00599v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#30028; &#25688;&#35201;: &#22914;&#20309;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#34920;&#29616;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#34920;&#29616;&#20986;&#19982;&#29616;&#23454;&#19990;&#30028;&#20195;&#30721;&#24211;&#30340;&#24046;&#36317;&#65292;&#19981;&#36275;&#20197;&#35780;&#20272;LLMs&#30340;&#32534;&#30721;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797; - EvoCodeBench&#26469;&#35299;&#20915;&#21069;&#36848;&#38382;&#39064;&#65292;&#20854;&#20855;&#26377;&#19977;&#20010;&#20027;&#35201;&#36827;&#23637;&#12290;(1) EvoCodeBench&#22312;&#22810;&#20010;&#32500;&#24230;&#19978;&#19982;&#29616;&#23454;&#19990;&#30028;&#24211;&#23545;&#40784;&#65292;&#20363;&#22914;&#20195;&#30721;&#20998;&#24067;&#21644;&#20381;&#36182;&#20998;&#24067;&#12290;(2) EvoCodeBench&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#27880;&#37322;(&#20363;&#22914;&#38656;&#27714;&#12289;&#21442;&#32771;&#20195;&#30721;&#21644;&#21442;&#32771;&#20381;&#36182;)&#65292;&#20197;&#21450;&#24378;&#22823;&#30340;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;(&#20363;&#22914;Pass@k&#21644;Recall@k)&#12290;(3) EvoCodeBench&#26159;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#36991;&#20813;&#25968;&#25454;&#27844;&#28431;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#27969;&#27700;&#32447;&#65292;&#20174;&#26368;&#26032;&#30340;&#24211;&#20013;&#26356;&#26032;EvoCodeBench&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#31532;&#19968;&#20010;&#29256;&#26412; - EvoCodeBench-2403&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;25&#20010;&#29616;&#23454;&#19990;&#30028;&#24211;&#30340;275&#20010;&#26679;&#26412;&#12290;&#22522;&#20110;EvoCodeBench&#65292;&#25105;&#20204;&#25552;&#20986;repo
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00599v1 Announce Type: cross  Abstract: How to evaluate Large Language Models (LLMs) in code generation is an open question. Existing benchmarks demonstrate poor alignment with real-world code repositories and are insufficient to evaluate the coding abilities of LLMs. This paper proposes a new benchmark - EvoCodeBench to address the preceding problems, which has three primary advances. (1) EvoCodeBench aligns with real-world repositories in multiple dimensions, e.g., code distributions and dependency distributions. (2) EvoCodeBench offers comprehensive annotations (e.g., requirements, reference code, and reference dependencies), and robust evaluation metrics (e.g., Pass@k and Recall@k). (3) EvoCodeBench is an evolving benchmark to avoid data leakage. We build an automatic pipeline to update EvoCodeBench from the latest repositories. We release the first version - EvoCodeBench-2403, containing 275 samples from 25 real-world repositories. Based on EvoCodeBench, we propose repo
&lt;/p&gt;</description></item><item><title>LAESI&#25968;&#25454;&#38598;&#21253;&#21547;10&#19975;&#24352;&#21512;&#25104;&#21494;&#29255;&#22270;&#20687;&#65292;&#29992;&#20110;&#21494;&#24418;&#24577;&#20998;&#26512;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#21518;&#21487;&#39044;&#27979;&#21494;&#29255;&#34920;&#38754;&#31215;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;3D&#31243;&#24207;&#27169;&#22411;&#21644;&#29983;&#25104;&#24335;AI&#30340;&#39640;&#25928;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2404.00593</link><description>&lt;p&gt;
LAESI: &#20351;&#29992;&#21512;&#25104;&#22270;&#20687;&#36827;&#34892;&#21494;&#29255;&#38754;&#31215;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
LAESI: Leaf Area Estimation with Synthetic Imagery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00593
&lt;/p&gt;
&lt;p&gt;
LAESI&#25968;&#25454;&#38598;&#21253;&#21547;10&#19975;&#24352;&#21512;&#25104;&#21494;&#29255;&#22270;&#20687;&#65292;&#29992;&#20110;&#21494;&#24418;&#24577;&#20998;&#26512;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#21518;&#21487;&#39044;&#27979;&#21494;&#29255;&#34920;&#38754;&#31215;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;3D&#31243;&#24207;&#27169;&#22411;&#21644;&#29983;&#25104;&#24335;AI&#30340;&#39640;&#25928;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;LAESI&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#20102;10&#19975;&#24352;&#21512;&#25104;&#21494;&#29255;&#22270;&#20687;&#30340;&#21512;&#25104;&#21494;&#29255;&#25968;&#25454;&#38598;&#65292;&#27599;&#24352;&#22270;&#20687;&#37117;&#26377;&#35821;&#20041;&#20998;&#21106;&#25513;&#27169;&#21644;&#34920;&#38754;&#31215;&#26631;&#27880;&#65292;&#22270;&#20687;&#32972;&#26223;&#26159;&#27627;&#31859;&#32440;&#12290;&#35813;&#25968;&#25454;&#38598;&#20027;&#35201;&#29992;&#20110;&#23545;&#23665;&#27611;&#27017;&#21644;&#27233;&#26641;&#21494;&#36827;&#34892;&#24418;&#24577;&#20998;&#26512;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21494;&#29255;&#34920;&#38754;&#31215;&#39044;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#26469;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#36866;&#29992;&#24615;&#65292;&#20351;&#29992;&#30495;&#23454;&#22270;&#20687;&#36827;&#34892;&#39564;&#35777;&#12290;&#39564;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#35757;&#32451;&#20986;&#39044;&#27979;&#21494;&#38754;&#31215;&#30340;&#33021;&#21147;&#65292;&#30456;&#23545;&#35823;&#24046;&#19981;&#20250;&#36229;&#36807;&#24179;&#22343;&#20154;&#31867;&#26631;&#27880;&#21592;&#30340;&#35823;&#24046;&#12290;LAESI&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;3D&#31243;&#24207;&#27169;&#22411;&#21644;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#39640;&#25928;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#22823;&#35268;&#27169;&#12289;&#21487;&#25511;&#30340;&#25968;&#25454;&#29983;&#25104;&#65292;&#22312;&#20892;&#19994;&#21644;&#29983;&#29289;&#23398;&#31561;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#23558;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#32435;&#20837;&#25105;&#20204;&#30340;&#31243;&#24207;&#21270;&#25968;&#25454;&#29983;&#25104;&#27969;&#31243;&#20013;&#65292;&#24182;&#23637;&#31034;&#20102;&#22522;&#20110;&#27880;&#37322;&#19968;&#33268;&#24615;&#30340;&#25968;&#25454;&#36807;&#28388;&#22914;&#20309;&#23548;&#33268;&#25968;&#25454;&#38598;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00593v1 Announce Type: cross  Abstract: We introduce LAESI, a Synthetic Leaf Dataset of 100,000 synthetic leaf images on millimeter paper, each with semantic masks and surface area labels. This dataset provides a resource for leaf morphology analysis primarily aimed at beech and oak leaves. We evaluate the applicability of the dataset by training machine learning models for leaf surface area prediction and semantic segmentation, using real images for validation. Our validation shows that these models can be trained to predict leaf surface area with a relative error not greater than an average human annotator. LAESI also provides an efficient framework based on 3D procedural models and generative AI for the large-scale, controllable generation of data with potential further applications in agriculture and biology. We evaluate the inclusion of generative AI in our procedural data generation pipeline and show how data filtering based on annotation consistency results in dataset
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35760;&#24518;&#30340;&#20132;&#21449;&#27169;&#24577;&#35821;&#20041;&#23545;&#40784;&#32593;&#32476;&#29992;&#20110;&#20943;&#23569;&#24037;&#20316;&#37327;&#24182;&#29983;&#25104;&#20934;&#30830;&#25918;&#23556;&#23398;&#25253;&#21578;</title><link>https://arxiv.org/abs/2404.00588</link><description>&lt;p&gt;
&#22522;&#20110;&#35760;&#24518;&#30340;&#20132;&#21449;&#27169;&#24577;&#35821;&#20041;&#23545;&#40784;&#32593;&#32476;&#29992;&#20110;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Memory-based Cross-modal Semantic Alignment Network for Radiology Report Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00588
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35760;&#24518;&#30340;&#20132;&#21449;&#27169;&#24577;&#35821;&#20041;&#23545;&#40784;&#32593;&#32476;&#29992;&#20110;&#20943;&#23569;&#24037;&#20316;&#37327;&#24182;&#29983;&#25104;&#20934;&#30830;&#25918;&#23556;&#23398;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#25918;&#23556;&#23398;&#25253;&#21578;&#21487;&#20197;&#33258;&#21160;&#20943;&#23569;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#24037;&#20316;&#37327;&#24182;&#26377;&#21161;&#20110;&#29305;&#23450;&#30142;&#30149;&#30340;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28041;&#21450;&#30142;&#30149;&#30340;&#20851;&#38190;&#20449;&#24687;&#22312;&#22270;&#20687;&#21644;&#25253;&#21578;&#20013;&#25152;&#21344;&#27604;&#20363;&#24456;&#23567;&#65292;&#27169;&#22411;&#24456;&#38590;&#23398;&#20064;&#25918;&#23556;&#23398;&#22270;&#20687;&#21644;&#25253;&#21578;&#20043;&#38388;&#30340;&#28508;&#22312;&#20851;&#31995;&#65292;&#20174;&#32780;&#26080;&#27861;&#29983;&#25104;&#27969;&#30021;&#21644;&#20934;&#30830;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35760;&#24518;&#30340;&#20132;&#21449;&#27169;&#24577;&#35821;&#20041;&#23545;&#40784;&#27169;&#22411;&#65288;MCSAM&#65289;&#65292;&#37319;&#29992;&#32534;&#30721;-&#35299;&#30721;&#33539;&#24335;&#12290;MCSAM&#21253;&#25324;&#19968;&#20010;&#33391;&#22909;&#21021;&#22987;&#21270;&#30340;&#38271;&#26399;&#20020;&#24202;&#35760;&#24518;&#24211;&#65292;&#29992;&#20110;&#23398;&#20064;&#19982;&#30142;&#30149;&#30456;&#20851;&#30340;&#34920;&#31034;&#20197;&#21450;&#19981;&#21516;&#27169;&#24577;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20197;&#26816;&#32034;&#24182;&#21033;&#29992;&#26816;&#32034;&#21040;&#30340;&#35760;&#24518;&#26469;&#36827;&#34892;&#29305;&#24449;&#25972;&#21512;&#12290;&#20026;&#20102;&#30830;&#20445;&#26816;&#32034;&#21040;&#30340;&#36328;&#27169;&#24577;&#20808;&#39564;&#30693;&#35782;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#27169;&#24577;&#35821;&#20041;&#23545;&#40784;&#27169;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00588v1 Announce Type: cross  Abstract: Generating radiology reports automatically reduces the workload of radiologists and helps the diagnoses of specific diseases. Many existing methods take this task as modality transfer process. However, since the key information related to disease accounts for a small proportion in both image and report, it is hard for the model to learn the latent relation between the radiology image and its report, thus failing to generate fluent and accurate radiology reports. To tackle this problem, we propose a memory-based cross-modal semantic alignment model (MCSAM) following an encoder-decoder paradigm. MCSAM includes a well initialized long-term clinical memory bank to learn disease-related representations as well as prior knowledge for different modalities to retrieve and use the retrieved memory to perform feature consolidation. To ensure the semantic consistency of the retrieved cross modal prior knowledge, a cross-modal semantic alignment m
&lt;/p&gt;</description></item><item><title>RLGNet&#25552;&#20986;&#20102;&#37325;&#22797;-&#23616;&#37096;-&#20840;&#23616;&#21382;&#21490;&#32593;&#32476;&#65292;&#21033;&#29992;&#20840;&#23616;&#21382;&#21490;&#32534;&#30721;&#22120;&#21644;&#23616;&#37096;&#21382;&#21490;&#32534;&#30721;&#22120;&#20998;&#21035;&#25429;&#25417;&#21382;&#21490;&#20449;&#24687;&#30340;&#25972;&#20307;&#21644;&#30456;&#20851;&#32454;&#33410;&#65292;&#20197;&#26377;&#25928;&#36827;&#34892;&#26102;&#38388;&#30693;&#35782;&#22270;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2404.00586</link><description>&lt;p&gt;
RLGNet&#65306;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#25512;&#29702;&#30340;&#37325;&#22797;-&#23616;&#37096;-&#20840;&#23616;&#21382;&#21490;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
RLGNet: Repeating-Local-Global History Network for Temporal Knowledge Graph Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00586
&lt;/p&gt;
&lt;p&gt;
RLGNet&#25552;&#20986;&#20102;&#37325;&#22797;-&#23616;&#37096;-&#20840;&#23616;&#21382;&#21490;&#32593;&#32476;&#65292;&#21033;&#29992;&#20840;&#23616;&#21382;&#21490;&#32534;&#30721;&#22120;&#21644;&#23616;&#37096;&#21382;&#21490;&#32534;&#30721;&#22120;&#20998;&#21035;&#25429;&#25417;&#21382;&#21490;&#20449;&#24687;&#30340;&#25972;&#20307;&#21644;&#30456;&#20851;&#32454;&#33410;&#65292;&#20197;&#26377;&#25928;&#36827;&#34892;&#26102;&#38388;&#30693;&#35782;&#22270;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00586v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25688;&#35201;&#65306;&#26102;&#38388;&#30693;&#35782;&#22270;&#65288;TKG&#65289;&#25512;&#29702;&#22522;&#20110;&#21382;&#21490;&#20449;&#24687;&#20197;&#39044;&#27979;&#26410;&#26469;&#12290;&#22240;&#27492;&#65292;&#35299;&#26512;&#21644;&#25366;&#25496;&#21382;&#21490;&#20449;&#24687;&#23545;&#39044;&#27979;&#26410;&#26469;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#21516;&#26102;&#20174;&#20840;&#23616;&#21644;&#23616;&#37096;&#35282;&#24230;&#22788;&#29702;&#21644;&#29702;&#35299;&#21382;&#21490;&#20449;&#24687;&#12290;&#24573;&#35270;&#20840;&#23616;&#35270;&#22270;&#21487;&#33021;&#23548;&#33268;&#24573;&#30053;&#23439;&#35266;&#36235;&#21183;&#21644;&#27169;&#24335;&#65292;&#32780;&#24573;&#35270;&#23616;&#37096;&#35270;&#22270;&#21487;&#33021;&#23548;&#33268;&#36951;&#28431;&#20851;&#38190;&#35814;&#32454;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#26041;&#27861;&#19981;&#20851;&#27880;&#20174;&#39640;&#39057;&#37325;&#22797;&#20107;&#20214;&#20013;&#23398;&#20064;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#25484;&#25569;&#39057;&#32321;&#21457;&#29983;&#30340;&#21382;&#21490;&#20107;&#20214;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;\textbf{R}epetitive-\textbf{L}ocal-\textbf{G}lobal History \textbf{Net}work&#65288;RLGNet&#65289;&#12290;&#25105;&#20204;&#21033;&#29992;&#20840;&#23616;&#21382;&#21490;&#32534;&#30721;&#22120;&#25429;&#25417;&#21382;&#21490;&#20449;&#24687;&#30340;&#20840;&#38754;&#24615;&#36136;&#12290;&#38543;&#21518;&#65292;&#23616;&#37096;&#21382;&#21490;&#32534;&#30721;&#22120;&#25552;&#20379;&#19982;&#26597;&#35810;&#26102;&#38388;&#25139;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00586v1 Announce Type: new  Abstract: Temporal Knowledge Graph (TKG) reasoning is based on historical information to predict the future. Therefore, parsing and mining historical information is key to predicting the future. Most existing methods fail to concurrently address and comprehend historical information from both global and local perspectives. Neglecting the global view might result in overlooking macroscopic trends and patterns, while ignoring the local view can lead to missing critical detailed information. Additionally, some methods do not focus on learning from high-frequency repeating events, which means they may not fully grasp frequently occurring historical events. To this end, we propose the \textbf{R}epetitive-\textbf{L}ocal-\textbf{G}lobal History \textbf{Net}work(RLGNet). We utilize a global history encoder to capture the overarching nature of historical information. Subsequently, the local history encoder provides information related to the query timestam
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#27169;&#22411;&#22312;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#23637;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#28508;&#21147;&#65292;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#21382;&#21490;&#12289;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#31561;&#22797;&#26434;&#25968;&#25454;&#65292;&#20026;&#25512;&#33616;&#31995;&#32479;&#24102;&#26469;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00579</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#30340;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#65288;Gen-RecSys&#65289;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review of Modern Recommender Systems Using Generative Models (Gen-RecSys)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00579
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#22312;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#23637;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#28508;&#21147;&#65292;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#21382;&#21490;&#12289;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#31561;&#22797;&#26434;&#25968;&#25454;&#65292;&#20026;&#25512;&#33616;&#31995;&#32479;&#24102;&#26469;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#25512;&#33616;&#31995;&#32479;&#65288;RS&#65289;&#36890;&#24120;&#20351;&#29992;&#29992;&#25143;-&#39033;&#30446;&#35780;&#20998;&#21382;&#21490;&#20316;&#20026;&#20027;&#35201;&#25968;&#25454;&#26469;&#28304;&#65292;&#21327;&#21516;&#36807;&#28388;&#26159;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#27169;&#22411;&#26368;&#36817;&#24050;&#32463;&#20855;&#22791;&#20102;&#24314;&#27169;&#21644;&#37319;&#26679;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#30340;&#33021;&#21147;&#65292;&#19981;&#20165;&#21487;&#20197;&#28085;&#30422;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#21382;&#21490;&#65292;&#36824;&#21487;&#20197;&#21253;&#25324;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#65292;&#20026;&#26032;&#39062;&#30340;&#25512;&#33616;&#20219;&#21153;&#35299;&#38145;&#20102;&#20016;&#23500;&#30340;&#25968;&#25454;&#12290;&#36890;&#36807;&#36825;&#31687;&#20840;&#38754;&#30340;&#12289;&#22810;&#23398;&#31185;&#30340;&#35843;&#30740;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#35752;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#65288;Gen-RecSys&#65289;&#22312;RS&#20013;&#30340;&#20851;&#38190;&#36827;&#23637;&#65292;&#21253;&#25324;&#65306;&#22522;&#20110;&#20132;&#20114;&#39537;&#21160;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#22522;&#30784;&#27010;&#36848;&#65307;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29983;&#25104;&#25512;&#33616;&#12289;&#26816;&#32034;&#21644;&#23545;&#35805;&#25512;&#33616;&#20013;&#30340;&#24212;&#29992;&#65307;&#20197;&#21450;&#29992;&#20110;&#22788;&#29702;&#21644;&#29983;&#25104;RS&#20013;&#30340;&#22270;&#20687;&#21644;&#35270;&#39057;&#20869;&#23481;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#25972;&#21512;&#12290;&#25105;&#20204;&#30340;&#25972;&#20307;&#35270;&#35282;&#20351;&#25105;&#20204;&#33021;&#22815;&#24378;&#35843;&#35780;&#20272;&#25152;&#38656;&#33539;&#24335;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00579v1 Announce Type: cross  Abstract: Traditional recommender systems (RS) have used user-item rating histories as their primary data source, with collaborative filtering being one of the principal methods. However, generative models have recently developed abilities to model and sample from complex data distributions, including not only user-item interaction histories but also text, images, and videos - unlocking this rich data for novel recommendation tasks. Through this comprehensive and multi-disciplinary survey, we aim to connect the key advancements in RS using Generative Models (Gen-RecSys), encompassing: a foundational overview of interaction-driven generative models; the application of large language models (LLM) for generative recommendation, retrieval, and conversational recommendation; and the integration of multimodal models for processing and generating image and video content in RS. Our holistic perspective allows us to highlight necessary paradigms for eval
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#22411;&#30340;&#33258;&#21160;&#21452;&#21521;&#21152;&#26435;&#38598;&#25104;&#31639;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#33041;&#32959;&#30244;&#26816;&#27979;&#21644;&#20998;&#31867;&#30340;&#25928;&#26524;</title><link>https://arxiv.org/abs/2404.00576</link><description>&lt;p&gt;
&#33258;&#21160;&#21452;&#21521;&#21152;&#26435;&#38598;&#25104;&#31639;&#27861;&#21450;&#20854;&#22312;&#33041;&#32959;&#30244;&#26816;&#27979;&#21644;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Automated Bi-Fold Weighted Ensemble Algorithms and its Application to Brain Tumor Detection and Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00576
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#22411;&#30340;&#33258;&#21160;&#21452;&#21521;&#21152;&#26435;&#38598;&#25104;&#31639;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#33041;&#32959;&#30244;&#26816;&#27979;&#21644;&#20998;&#31867;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#32454;&#32990;&#30340;&#19981;&#21463;&#25511;&#21046;&#21644;&#26080;&#32467;&#26500;&#29983;&#38271;&#34987;&#31216;&#20026;&#33041;&#32959;&#30244;&#65292;&#23427;&#22312;&#21508;&#31181;&#30284;&#30151;&#20013;&#25317;&#26377;&#26368;&#39640;&#30340;&#27515;&#20129;&#29575;&#20043;&#19968;&#12290;&#30001;&#20110;&#26377;&#38480;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#31532;&#19977;&#19990;&#30028;&#22269;&#23478;&#65292;&#33041;&#32959;&#30244;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26089;&#26399;&#35786;&#26029;&#22312;&#26377;&#25928;&#31649;&#29702;&#33041;&#32959;&#30244;&#21644;&#20943;&#23569;&#27515;&#20129;&#29575;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#25104;&#26412;&#21644;&#38271;&#26102;&#38388;&#30340;&#32467;&#26524;&#33719;&#21462;&#26102;&#38388;&#31561;&#21508;&#31181;&#38480;&#21046;&#65292;&#35786;&#26029;&#26041;&#27861;&#30340;&#21487;&#29992;&#24615;&#21463;&#21040;&#38459;&#30861;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#30142;&#30149;&#30340;&#26089;&#26399;&#26816;&#27979;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#21069;&#27839;&#30340;&#21452;&#21521;&#21152;&#26435;&#25237;&#31080;&#38598;&#25104;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#21152;&#26435;&#38598;&#25104;&#26041;&#27861;&#30340;&#25928;&#21147;&#12290;&#36825;&#20004;&#31181;&#25552;&#20986;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#26469;&#33258;&#22810;&#20010;&#20998;&#31867;&#22120;&#30340;&#20998;&#31867;&#32467;&#26524;&#65292;&#24182;&#36890;&#36807;&#36873;&#25321;&#31532;&#19968;&#31181;&#26041;&#27861;&#20013;&#20855;&#26377;&#26368;&#39640;&#27010;&#29575;&#30340;&#32467;&#26524;&#65292;&#20197;&#21450;&#26368;&#39640;&#21152;&#26435; p
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00576v1 Announce Type: cross  Abstract: The uncontrolled and unstructured growth of brain cells is known as brain tumor, which has one of the highest mortality rates among diseases from all types of cancers. Due to limited diagnostic and treatment capabilities, they pose significant challenges, especially in third-world countries. Early diagnosis plays a vital role in effectively managing brain tumors and reducing mortality rates. However, the availability of diagnostic methods is hindered by various limitations, including high costs and lengthy result acquisition times, impeding early detection of the disease. In this study, we present two cutting-edge bi-fold weighted voting ensemble models that aim to boost the effectiveness of weighted ensemble methods. These two proposed methods combine the classification outcomes from multiple classifiers and determine the optimal result by selecting the one with the highest probability in the first approach, and the highest weighted p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#23398;&#20064;&#25512;&#29702;&#20013;&#38271;&#24230;&#27867;&#21270;&#38382;&#39064;&#30340;&#29702;&#35770;&#30740;&#31350;&#65292;&#35774;&#35745;&#20102;&#22522;&#20110;DAGs&#30340;&#38382;&#39064;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;Transformer&#26469;&#23454;&#29616;&#23436;&#32654;&#30340;&#38271;&#24230;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2404.00560</link><description>&lt;p&gt;
&#23398;&#20064;&#25512;&#29702;&#20013;&#38271;&#24230;&#27867;&#21270;&#30340;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Theory for Length Generalization in Learning to Reason
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00560
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#23398;&#20064;&#25512;&#29702;&#20013;&#38271;&#24230;&#27867;&#21270;&#38382;&#39064;&#30340;&#29702;&#35770;&#30740;&#31350;&#65292;&#35774;&#35745;&#20102;&#22522;&#20110;DAGs&#30340;&#38382;&#39064;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;Transformer&#26469;&#23454;&#29616;&#23436;&#32654;&#30340;&#38271;&#24230;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#24230;&#27867;&#21270;&#65288;LG&#65289;&#26159;&#23398;&#20064;&#25512;&#29702;&#20013;&#30340;&#19968;&#20010;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;&#23427;&#25351;&#30340;&#26159;&#24403;&#22312;&#36739;&#30701;&#38271;&#24230;&#25110;&#22823;&#23567;&#30340;&#25512;&#29702;&#38382;&#39064;&#19978;&#35757;&#32451;&#26102;&#65292;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#22312;&#22788;&#29702;&#36739;&#38271;&#38271;&#24230;&#25110;&#22823;&#23567;&#30340;&#38382;&#39064;&#26102;&#20250;&#36935;&#21040;&#22256;&#38590;&#12290;&#23613;&#31649;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#30740;&#31350;&#20102;LG&#65292;&#20294;&#25361;&#25112;&#20381;&#28982;&#23384;&#22312;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#21487;&#20197;&#34987;&#24314;&#27169;&#20026;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAGs&#65289;&#30340;&#38382;&#39064;&#30340;LG&#30340;&#29702;&#35770;&#30740;&#31350;&#12290;&#35813;&#35770;&#25991;&#39318;&#20808;&#30830;&#23450;&#24182;&#35777;&#26126;&#20102;&#22312;&#23398;&#20064;&#25512;&#29702;&#20013;&#21487;&#20197;&#23454;&#29616;LG&#30340;&#26465;&#20214;&#12290;&#28982;&#21518;&#22522;&#20110;&#36825;&#19968;&#29702;&#35770;&#35774;&#35745;&#20102;&#38382;&#39064;&#34920;&#31034;&#65292;&#20197;&#23398;&#20064;&#35299;&#20915;&#35832;&#22914;&#22855;&#20598;&#12289;&#21152;&#27861;&#21644;&#20056;&#27861;&#31561;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#38382;&#39064;&#65292;&#20351;&#29992;Transformer&#23454;&#29616;&#23436;&#32654;&#30340;LG&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00560v1 Announce Type: new  Abstract: Length generalization (LG) is a challenging problem in learning to reason. It refers to the phenomenon that when trained on reasoning problems of smaller lengths or sizes, the resulting model struggles with problems of larger sizes or lengths. Although LG has been studied by many researchers, the challenge remains. This paper proposes a theoretical study of LG for problems whose reasoning processes can be modeled as DAGs (directed acyclic graphs). The paper first identifies and proves the conditions under which LG can be achieved in learning to reason. It then designs problem representations based on the theory to learn to solve challenging reasoning problems like parity, addition, and multiplication, using a Transformer to achieve perfect LG.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28145;&#24230;&#22806;&#22312;&#27969;&#24418;&#34920;&#31034;&#65288;DEMR&#65289;&#30340;&#25216;&#24039;&#65292;&#23558;&#22806;&#22312;&#27969;&#24418;&#23884;&#20837;&#34701;&#20837;&#21040;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#24110;&#21161;&#29983;&#25104;&#27969;&#24418;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#35777;&#25454;&#25903;&#25345;&#20854;&#22312; $SE(3)$ &#31561;&#27969;&#24418;&#19978;&#30340;&#21487;&#34892;&#24615;&#12289;&#28176;&#36817;&#24615;&#36136;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.00544</link><description>&lt;p&gt;
&#35270;&#35273;&#20219;&#21153;&#30340;&#28145;&#24230;&#22806;&#22312;&#27969;&#24418;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Deep Extrinsic Manifold Representation for Vision Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00544
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28145;&#24230;&#22806;&#22312;&#27969;&#24418;&#34920;&#31034;&#65288;DEMR&#65289;&#30340;&#25216;&#24039;&#65292;&#23558;&#22806;&#22312;&#27969;&#24418;&#23884;&#20837;&#34701;&#20837;&#21040;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#24110;&#21161;&#29983;&#25104;&#27969;&#24418;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#35777;&#25454;&#25903;&#25345;&#20854;&#22312; $SE(3)$ &#31561;&#27969;&#24418;&#19978;&#30340;&#21487;&#34892;&#24615;&#12289;&#28176;&#36817;&#24615;&#36136;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#27431;&#20960;&#37324;&#24503;&#25968;&#25454;&#22312;&#19981;&#21516;&#39046;&#22495;&#32463;&#24120;&#36935;&#21040;&#65292;&#20294;&#20851;&#20110;&#29992;&#27969;&#24418;&#34920;&#31034;&#20316;&#20026;&#36755;&#20986;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#26412;&#25361;&#25112;&#30340;&#25991;&#29486;&#26377;&#38480;&#12290;&#25105;&#20204;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;&#28145;&#24230;&#22806;&#22312;&#27969;&#24418;&#34920;&#31034;&#65288;Deep Extrinsic Manifold Representation&#65292;DEMR&#65289;&#30340;&#25216;&#24039;&#65292;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;&#12290;DEMR&#23558;&#22806;&#22312;&#27969;&#24418;&#23884;&#20837;&#34701;&#20837;&#21040;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#26377;&#21161;&#20110;&#29983;&#25104;&#27969;&#24418;&#34920;&#31034;&#12290;DEMR&#26041;&#27861;&#19981;&#30452;&#25509;&#20248;&#21270;&#22797;&#26434;&#30340;&#27979;&#22320;&#32447;&#25439;&#22833;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110;&#20248;&#21270;&#23884;&#20837;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20869;&#30340;&#35745;&#31639;&#22270;&#65292;&#20174;&#32780;&#36866;&#24212;&#21508;&#31181;&#26550;&#26500;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25903;&#25345;&#25152;&#25552;&#27010;&#24565;&#30340;&#23454;&#35777;&#35777;&#25454;&#65292;&#28041;&#21450;&#20004;&#31181;&#27969;&#24418;&#65292;$SE(3)$ &#21450;&#20854;&#20851;&#32852;&#21830;&#27969;&#24418;&#12290;&#36825;&#20123;&#35777;&#25454;&#20026;&#25152;&#25552;&#21462;&#27010;&#24565;&#30340;&#21487;&#34892;&#24615;&#12289;&#28176;&#36817;&#24615;&#36136;&#21644;&#27867;&#21270;&#33021;&#21147;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00544v1 Announce Type: cross  Abstract: Non-Euclidean data is frequently encountered across different fields, yet there is limited literature that addresses the fundamental challenge of training neural networks with manifold representations as outputs. We introduce the trick named Deep Extrinsic Manifold Representation (DEMR) for visual tasks in this context. DEMR incorporates extrinsic manifold embedding into deep neural networks, which helps generate manifold representations. The DEMR approach does not directly optimize the complex geodesic loss. Instead, it focuses on optimizing the computation graph within the embedded Euclidean space, allowing for adaptability to various architectural requirements. We provide empirical evidence supporting the proposed concept on two types of manifolds, $SE(3)$ and its associated quotient manifolds. This evidence offers theoretical assurances regarding feasibility, asymptotic properties, and generalization capability. The experimental re
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#20154;&#31867;&#20027;&#21160;&#24863;&#30693;&#21644;&#24490;&#29615;&#21453;&#39304;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#20855;&#36523;&#20027;&#21160;&#38450;&#24481;&#65288;EAD&#65289;&#31574;&#30053;&#65292;&#22312;3D&#23454;&#38469;&#29615;&#22659;&#20013;&#20027;&#21160;&#19978;&#19979;&#25991;&#21270;&#29615;&#22659;&#20449;&#24687;&#20197;&#23545;&#25239;&#23545;&#25239;&#24615;&#36148;&#29255;&#12290;</title><link>https://arxiv.org/abs/2404.00540</link><description>&lt;p&gt;
&#20855;&#36523;&#20027;&#21160;&#38450;&#24481;&#65306;&#21033;&#29992;&#24490;&#29615;&#21453;&#39304;&#23545;&#25239;&#23545;&#25239;&#24615;&#36148;&#29255;
&lt;/p&gt;
&lt;p&gt;
Embodied Active Defense: Leveraging Recurrent Feedback to Counter Adversarial Patches
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00540
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#20154;&#31867;&#20027;&#21160;&#24863;&#30693;&#21644;&#24490;&#29615;&#21453;&#39304;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#20855;&#36523;&#20027;&#21160;&#38450;&#24481;&#65288;EAD&#65289;&#31574;&#30053;&#65292;&#22312;3D&#23454;&#38469;&#29615;&#22659;&#20013;&#20027;&#21160;&#19978;&#19979;&#25991;&#21270;&#29615;&#22659;&#20449;&#24687;&#20197;&#23545;&#25239;&#23545;&#25239;&#24615;&#36148;&#29255;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#23545;&#25239;&#24615;&#36148;&#29255;&#30340;&#33030;&#24369;&#24615;&#20419;&#20351;&#20102;&#35768;&#22810;&#38450;&#24481;&#31574;&#30053;&#30340;&#20135;&#29983;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#38450;&#24481;&#20381;&#36182;&#20110;&#21333;&#19968;&#35266;&#23519;&#25110;&#39044;&#20808;&#24314;&#31435;&#30340;&#23545;&#25163;&#20449;&#24687;&#26469;&#23545;&#25239;&#23545;&#25239;&#24615;&#36148;&#29255;&#65292;&#24448;&#24448;&#26080;&#27861;&#24212;&#23545;&#26410;&#30693;&#25110;&#33258;&#36866;&#24212;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#22312;&#21160;&#24577;&#30340;3D&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#19981;&#23613;&#22914;&#20154;&#24847;&#30340;&#24615;&#33021;&#12290;&#21463;&#20154;&#31867;&#20027;&#21160;&#24863;&#30693;&#21644;&#24490;&#29615;&#21453;&#39304;&#26426;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20855;&#36523;&#20027;&#21160;&#38450;&#24481;&#65288;EAD&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31215;&#26497;&#30340;&#38450;&#24481;&#31574;&#30053;&#65292;&#20027;&#21160;&#22320;&#23558;&#29615;&#22659;&#20449;&#24687;&#25972;&#21512;&#21040;3D&#23454;&#38469;&#29615;&#22659;&#20013;&#65292;&#20197;&#35299;&#20915;&#23545;&#25239;&#24615;&#36148;&#29255;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;EAD&#24320;&#21457;&#20102;&#20004;&#20010;&#20013;&#24515;&#24490;&#29615;&#23376;&#27169;&#22359;&#65292;&#21363;&#24863;&#30693;&#27169;&#22359;&#21644;&#31574;&#30053;&#27169;&#22359;&#65292;&#20197;&#23454;&#29616;&#20027;&#21160;&#35270;&#35273;&#30340;&#20004;&#20010;&#20851;&#38190;&#21151;&#33021;&#12290;&#36825;&#20123;&#27169;&#22411;&#24490;&#29615;&#22788;&#29702;&#19968;&#31995;&#21015;&#20449;&#24565;&#21644;&#35266;&#23519;&#65292;&#20419;&#36827;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00540v1 Announce Type: cross  Abstract: The vulnerability of deep neural networks to adversarial patches has motivated numerous defense strategies for boosting model robustness. However, the prevailing defenses depend on single observation or pre-established adversary information to counter adversarial patches, often failing to be confronted with unseen or adaptive adversarial attacks and easily exhibiting unsatisfying performance in dynamic 3D environments. Inspired by active human perception and recurrent feedback mechanisms, we develop Embodied Active Defense (EAD), a proactive defensive strategy that actively contextualizes environmental information to address misaligned adversarial patches in 3D real-world settings. To achieve this, EAD develops two central recurrent sub-modules, i.e., a perception module and a policy module, to implement two critical functions of active vision. These models recurrently process a series of beliefs and observations, facilitating progress
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#22909;&#33719;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;DOVE&#21327;&#35758;&#23545;&#25351;&#20196;-&#21709;&#24212;&#23545;&#30340;&#32852;&#21512;&#27010;&#29575;&#36827;&#34892;&#20248;&#21270;&#65292;&#20197;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2404.00530</link><description>&lt;p&gt;
&#23558;&#22351;&#33529;&#26524;&#19982;&#22909;&#27224;&#23376;&#36827;&#34892;&#27604;&#36739;&#65306;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#20559;&#22909;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Comparing Bad Apples to Good Oranges: Aligning Large Language Models via Joint Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#22909;&#33719;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;DOVE&#21327;&#35758;&#23545;&#25351;&#20196;-&#21709;&#24212;&#23545;&#30340;&#32852;&#21512;&#27010;&#29575;&#36827;&#34892;&#20248;&#21270;&#65292;&#20197;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#24120;&#35265;&#30340;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25216;&#26415;&#20381;&#36182;&#20110;&#36890;&#36807;&#27604;&#36739;&#22312;&#22266;&#23450;&#19978;&#19979;&#25991;&#20013;&#26465;&#20214;&#29983;&#25104;&#30340;&#22810;&#20010;&#29983;&#25104;&#30340;&#20154;&#31867;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#24403;&#36825;&#20123;&#29983;&#25104;&#25918;&#32622;&#22312;&#30456;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#26102;&#65292;&#36825;&#20165;&#21033;&#29992;&#20102;&#25104;&#23545;&#27604;&#36739;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26465;&#20214;&#25490;&#21517;&#36890;&#24120;&#26080;&#27861;&#25429;&#33719;&#20154;&#31867;&#20559;&#22909;&#30340;&#22797;&#26434;&#21644;&#22810;&#32500;&#26041;&#38754;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20559;&#22909;&#33719;&#21462;&#30340;&#20256;&#32479;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22312;&#25351;&#20196;-&#21709;&#24212;&#23545;&#19978;&#32852;&#21512;&#24341;&#21457;&#20559;&#22909;&#30340;&#26032;&#36724;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#20559;&#22909;&#20248;&#21270;&#26159;&#38024;&#23545;&#26465;&#20214;&#25490;&#21517;&#21327;&#35758;&#65288;&#20363;&#22914;&#65292;DPO&#65289;&#35774;&#35745;&#30340;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#30340;&#20559;&#22909;&#33719;&#21462;&#21327;&#35758;&#24341;&#20837;&#20102;DOVE&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#20559;&#22909;&#20248;&#21270;&#30446;&#26631;&#65292;&#36890;&#36807;&#25552;&#21319;&#25152;&#36873;&#25351;&#20196;-&#21709;&#24212;&#23545;&#30340;&#32852;&#21512;&#27010;&#29575;&#26469;&#38477;&#20302;&#25152;&#25298;&#32477;&#25351;&#20196;-&#21709;&#24212;&#23545;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00530v1 Announce Type: cross  Abstract: A common technique for aligning large language models (LLMs) relies on acquiring human preferences by comparing multiple generations conditioned on a fixed context. This only leverages the pairwise comparisons when the generations are placed in an identical context. However, such conditional rankings often fail to capture the complex and multidimensional aspects of human preferences. In this work, we revisit the traditional paradigm of preference acquisition and propose a new axis that is based on eliciting preferences jointly over the instruction-response pairs. While prior preference optimizations are designed for conditional ranking protocols (e.g., DPO), our proposed preference acquisition protocol introduces DOVE, a new preference optimization objective that upweights the joint probability of the chosen instruction-response pair over the rejected instruction-response pair. Interestingly, we find that the LLM trained with joint ins
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#28216;&#25103;&#25345;&#32493;&#26102;&#38388;&#23545;&#29609;&#23478;&#24773;&#32490;&#30340;&#24433;&#21709;&#65292;&#24314;&#31435;&#20102;&#24773;&#24863;&#26816;&#27979;&#26694;&#26550;&#65292;&#24182;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#38271;&#26102;&#38388;&#28216;&#25103;&#20250;&#26174;&#33879;&#24433;&#21709;&#29609;&#23478;&#30340;&#24773;&#32490;&#12290;</title><link>https://arxiv.org/abs/2404.00526</link><description>&lt;p&gt;
&#28216;&#25103;&#25345;&#32493;&#26102;&#38388;&#30340;&#24773;&#24863;&#24433;&#21709;&#65306;&#29702;&#35299;&#38271;&#26102;&#38388;&#28216;&#25103;&#20013;&#29609;&#23478;&#24773;&#24863;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
The Emotional Impact of Game Duration: A Framework for Understanding Player Emotions in Extended Gameplay Sessions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00526
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#28216;&#25103;&#25345;&#32493;&#26102;&#38388;&#23545;&#29609;&#23478;&#24773;&#32490;&#30340;&#24433;&#21709;&#65292;&#24314;&#31435;&#20102;&#24773;&#24863;&#26816;&#27979;&#26694;&#26550;&#65292;&#24182;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#38271;&#26102;&#38388;&#28216;&#25103;&#20250;&#26174;&#33879;&#24433;&#21709;&#29609;&#23478;&#30340;&#24773;&#32490;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#28216;&#25103;&#33258;&#19978;&#19990;&#32426;70&#24180;&#20195;&#20197;&#26469;&#19968;&#30452;&#22312;&#23089;&#20048;&#20013;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#22312;&#23553;&#38145;&#26399;&#38388;&#21464;&#24471;&#26356;&#21152;&#31361;&#20986;&#65292;&#37027;&#26102;&#20154;&#20204;&#27491;&#22312;&#23547;&#25214;&#23089;&#20048;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#24403;&#26102;&#29609;&#23478;&#27809;&#26377;&#24847;&#35782;&#21040;&#28216;&#25103;&#26102;&#38388;&#23545;&#20182;&#20204;&#24773;&#32490;&#30340;&#37325;&#22823;&#24433;&#21709;&#12290;&#36825;&#20351;&#24471;&#35774;&#35745;&#24072;&#21644;&#24320;&#21457;&#20154;&#21592;&#24456;&#38590;&#21019;&#24314;&#26032;&#28216;&#25103;&#65292;&#22240;&#20026;&#20182;&#20204;&#24517;&#39035;&#25511;&#21046;&#36825;&#20123;&#28216;&#25103;&#23545;&#29609;&#23478;&#24773;&#32490;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#30740;&#31350;&#29609;&#23478;&#24773;&#32490;&#22914;&#20309;&#21463;&#21040;&#28216;&#25103;&#25345;&#32493;&#26102;&#38388;&#30340;&#24433;&#21709;&#12290;&#20026;&#23454;&#29616;&#27492;&#30446;&#26631;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#24773;&#24863;&#26816;&#27979;&#26694;&#26550;&#12290;&#26681;&#25454;&#23454;&#39564;&#32467;&#26524;&#65292;&#24535;&#24895;&#32773;&#20174;20&#20998;&#38047;&#21040;60&#20998;&#38047;&#30340;&#19968;&#33324;&#34920;&#36798;&#24773;&#24863;&#33021;&#21147;&#26377;&#25152;&#22686;&#21152;&#12290;&#19982;&#36739;&#30701;&#28216;&#25103;&#26102;&#38388;&#30456;&#27604;&#65292;&#23454;&#39564;&#21457;&#29616;&#38271;&#26102;&#38388;&#28216;&#25103;&#23545;&#29609;&#23478;&#30340;&#24773;&#32490;&#30830;&#23454;&#20135;&#29983;&#20102;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00526v1 Announce Type: cross  Abstract: Video games have played a crucial role in entertainment since their development in the 1970s, becoming even more prominent during the lockdown period when people were looking for ways to entertain them. However, at that time, players were unaware of the significant impact that playtime could have on their feelings. This has made it challenging for designers and developers to create new games since they have to control the emotional impact that these games will take on players. Thus, the purpose of this study is to look at how a player's emotions are affected by the duration of the game. In order to achieve this goal, a framework for emotion detection is created. According to the experiment's results, the volunteers' general ability to express emotions increased from 20 to 60 minutes. In comparison to shorter gameplay sessions, the experiment found that extended gameplay sessions did significantly affect the player's emotions. According
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#37325;&#24314;&#38454;&#27573;&#21644;&#37325;&#24314;&#25439;&#22833;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20849;&#20139;&#27169;&#22411;&#21442;&#25968;&#21644;&#29305;&#24449;&#34920;&#31034;&#30340;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#20849;&#21516;&#20449;&#24687;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#35299;&#20915;&#30456;&#20851;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2404.00505</link><description>&lt;p&gt;
&#20855;&#26377;&#37325;&#24314;&#25439;&#22833;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning with Reconstruction Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#37325;&#24314;&#38454;&#27573;&#21644;&#37325;&#24314;&#25439;&#22833;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20849;&#20139;&#27169;&#22411;&#21442;&#25968;&#21644;&#29305;&#24449;&#34920;&#31034;&#30340;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#20849;&#21516;&#20449;&#24687;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#35299;&#20915;&#30456;&#20851;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22810;&#25968;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25968;&#23398;&#20248;&#21270;&#30340;&#24212;&#29992;&#20013;&#65292;&#36890;&#24120;&#20026;&#27599;&#20010;&#29305;&#23450;&#20248;&#21270;&#30446;&#26631;&#35757;&#32451;&#19968;&#20010;&#19987;&#29992;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#65292;&#21516;&#19968;&#32452;&#38382;&#39064;&#36755;&#20837;&#19978;&#32463;&#24120;&#38656;&#35201;&#20248;&#21270;&#20960;&#20010;&#19981;&#21516;&#20294;&#30456;&#20851;&#30340;&#30446;&#26631;&#25110;&#20219;&#21153;&#12290;&#19982;&#20026;&#27599;&#20010;&#38382;&#39064;&#21333;&#29420;&#35757;&#32451;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;&#26356;&#26377;&#25928;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#36825;&#20123;&#30446;&#26631;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20351;&#29992;&#20849;&#20139;&#27169;&#22411;&#21442;&#25968;&#21644;&#29305;&#24449;&#34920;&#31034;&#35757;&#32451;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#26412;&#25991;&#39318;&#20808;&#24314;&#31435;&#20102;&#20849;&#21516;&#20449;&#24687;&#30340;&#27010;&#24565;&#65306;&#35299;&#20915;&#30456;&#20851;&#20219;&#21153;&#25152;&#38656;&#30340;&#20849;&#20139;&#30693;&#35782;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27169;&#22411;&#20013;&#28155;&#21152;&#19968;&#20010;&#39069;&#22806;&#30340;&#37325;&#24314;&#38454;&#27573;&#20197;&#21450;&#30456;&#20851;&#30340;&#26032;&#37325;&#24314;&#25439;&#22833;&#12290;&#35813;&#25439;&#22833;&#29992;&#20110;&#20174;&#36873;&#25321;&#30340;&#38544;&#34255;&#29366;&#24577;&#24320;&#22987;&#37325;&#26032;&#26500;&#24314;&#20849;&#21516;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00505v1 Announce Type: cross  Abstract: In most applications of utilizing neural networks for mathematical optimization, a dedicated model is trained for each specific optimization objective. However, in many scenarios, several distinct yet correlated objectives or tasks often need to be optimized on the same set of problem inputs. Instead of independently training a different neural network for each problem separately, it would be more efficient to exploit the correlations between these objectives and to train multiple neural network models with shared model parameters and feature representations. To achieve this, this paper first establishes the concept of common information: the shared knowledge required for solving the correlated tasks, then proposes a novel approach for model training by adding into the model an additional reconstruction stage associated with a new reconstruction loss. This loss is for reconstructing the common information starting from a selected hidde
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Configurable Safety Tuning&#65288;CST&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#20559;&#22909;&#25968;&#25454;&#65292;&#22312;&#25512;&#26029;&#26102;&#23454;&#29616;&#23545;LLMs&#30340;&#28789;&#27963;&#23433;&#20840;&#37197;&#32622;&#65292;&#20801;&#35768;&#29992;&#25143;&#26681;&#25454;&#38656;&#35201;&#31105;&#29992;/&#21551;&#29992;&#23433;&#20840;&#20559;&#22909;&#65292;&#19988;&#23454;&#39564;&#34920;&#26126;CST&#25104;&#21151;&#31649;&#29702;&#19981;&#21516;&#30340;&#23433;&#20840;&#37197;&#32622;&#24182;&#20445;&#30041;&#20102;&#21407;&#22987;&#21151;&#33021;&#65292;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#21487;&#37197;&#32622;&#37096;&#32626;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.00495</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#20559;&#22909;&#25968;&#25454;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21487;&#37197;&#32622;&#23433;&#20840;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Configurable Safety Tuning of Language Models with Synthetic Preference Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00495
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Configurable Safety Tuning&#65288;CST&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#20559;&#22909;&#25968;&#25454;&#65292;&#22312;&#25512;&#26029;&#26102;&#23454;&#29616;&#23545;LLMs&#30340;&#28789;&#27963;&#23433;&#20840;&#37197;&#32622;&#65292;&#20801;&#35768;&#29992;&#25143;&#26681;&#25454;&#38656;&#35201;&#31105;&#29992;/&#21551;&#29992;&#23433;&#20840;&#20559;&#22909;&#65292;&#19988;&#23454;&#39564;&#34920;&#26126;CST&#25104;&#21151;&#31649;&#29702;&#19981;&#21516;&#30340;&#23433;&#20840;&#37197;&#32622;&#24182;&#20445;&#30041;&#20102;&#21407;&#22987;&#21151;&#33021;&#65292;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#21487;&#37197;&#32622;&#37096;&#32626;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#25216;&#26415;&#65292;&#22914;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#65292;&#36890;&#36807;&#23558;&#39044;&#23450;&#20041;&#34892;&#20026;&#30828;&#32534;&#30721;&#21040;&#27169;&#22411;&#20013;&#65292;&#38480;&#21046;&#20102;&#29992;&#25143;&#30340;&#25511;&#21046;&#26435;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;Configurable Safety Tuning&#65288;CST&#65289;&#65292;&#23427;&#21033;&#29992;&#21512;&#25104;&#20559;&#22909;&#25968;&#25454;&#26469;&#22686;&#24378;DPO&#65292;&#20197;&#20419;&#36827;&#22312;&#25512;&#26029;&#26102;&#23545;LLMs&#36827;&#34892;&#28789;&#27963;&#30340;&#23433;&#20840;&#37197;&#32622;&#12290;CST&#36890;&#36807;&#24341;&#20837;&#25351;&#23450;&#23433;&#20840;&#37197;&#32622;&#30340;&#31995;&#32479;&#25552;&#31034;&#65292;&#20801;&#35768;LLM&#37096;&#32626;&#32773;&#26681;&#25454;&#38656;&#35201;&#31105;&#29992;/&#21551;&#29992;&#23433;&#20840;&#20559;&#22909;&#65292;&#20165;&#38656;&#26356;&#25913;&#31995;&#32479;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;CST&#25104;&#21151;&#31649;&#29702;&#19981;&#21516;&#30340;&#23433;&#20840;&#37197;&#32622;&#65292;&#24182;&#20445;&#30041;&#20102;LLMs&#30340;&#21407;&#22987;&#21151;&#33021;&#65292;&#26174;&#31034;&#20986;&#23427;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#21487;&#37197;&#32622;&#37096;&#32626;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00495v1 Announce Type: cross  Abstract: State-of-the-art language model fine-tuning techniques, such as Direct Preference Optimization (DPO), restrict user control by hard-coding predefined behaviors into the model. To address this, we propose a novel method, Configurable Safety Tuning (CST), that augments DPO using synthetic preference data to facilitate flexible safety configuration of LLMs at inference time. CST overcomes the constraints of vanilla DPO by introducing a system prompt specifying safety configurations, enabling LLM deployers to disable/enable safety preferences based on their need, just changing the system prompt. Our experimental evaluations indicate that CST successfully manages different safety configurations and retains the original functionality of LLMs, showing it is a robust method for configurable deployment. Data and models available at https://github.com/vicgalle/configurable-safety-tuning
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;TEMPLE-MQA&#65292;&#36890;&#36807;&#26500;&#24314;&#26102;&#38388;&#24863;&#30693;&#22270;&#21644;&#25512;&#29702;&#36335;&#24452;&#12289;&#32467;&#26500;&#26816;&#32034;&#21644;&#32852;&#21512;&#25512;&#29702;&#38454;&#27573;&#65292;&#26377;&#25928;&#22320;&#35782;&#21035;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#26102;&#38388;&#32972;&#26223;&#65292;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#36129;&#29486;&#20102;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;TKEMQA&#12290;</title><link>https://arxiv.org/abs/2404.00492</link><description>&lt;p&gt;
&#26102;&#24577;&#30693;&#35782;&#32534;&#36753;&#19979;&#30340;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Multi-hop Question Answering under Temporal Knowledge Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00492
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;TEMPLE-MQA&#65292;&#36890;&#36807;&#26500;&#24314;&#26102;&#38388;&#24863;&#30693;&#22270;&#21644;&#25512;&#29702;&#36335;&#24452;&#12289;&#32467;&#26500;&#26816;&#32034;&#21644;&#32852;&#21512;&#25512;&#29702;&#38454;&#27573;&#65292;&#26377;&#25928;&#22320;&#35782;&#21035;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#26102;&#38388;&#32972;&#26223;&#65292;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#36129;&#29486;&#20102;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;TKEMQA&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#65292;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572; (MQA) &#22312;&#30693;&#35782;&#32534;&#36753; (KE) &#19979;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MQA&#22312;&#22788;&#29702;&#21253;&#21547;&#26174;&#24335;&#26102;&#38388;&#32972;&#26223;&#30340;&#38382;&#39064;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21363;&#26102;&#24577;&#30693;&#35782;&#22686;&#24378;&#30340;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572; (TEMPLE-MQA)&#12290;&#19981;&#21516;&#20110;&#20197;&#24448;&#30340;&#26041;&#27861;&#65292;TEMPLE-MQA&#39318;&#20808;&#26500;&#24314;&#19968;&#20010;&#26102;&#38388;&#24863;&#30693;&#22270; (TAG)&#65292;&#20197;&#32467;&#26500;&#21270;&#26041;&#24335;&#23384;&#20648;&#32534;&#36753;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#25512;&#29702;&#36335;&#24452;&#12289;&#32467;&#26500;&#26816;&#32034;&#21644;&#32852;&#21512;&#25512;&#29702;&#38454;&#27573;&#65292;TEMPLE-MQA&#26377;&#25928;&#22320;&#35782;&#21035;&#38382;&#39064;&#26597;&#35810;&#20013;&#30340;&#26102;&#38388;&#32972;&#26223;&#12290;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;TEMPLE-MQA&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36129;&#29486;&#20102;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;TKEMQA&#65292;&#19987;&#38376;&#20026;&#24102;&#26377;&#26102;&#38388;&#32422;&#26463;&#30340;MQA&#37327;&#36523;&#23450;&#21046;&#65292;&#20316;&#20026;&#39318;&#20010;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00492v1 Announce Type: cross  Abstract: Multi-hop question answering (MQA) under knowledge editing (KE) has garnered significant attention in the era of large language models. However, existing models for MQA under KE exhibit poor performance when dealing with questions containing explicit temporal contexts. To address this limitation, we propose a novel framework, namely TEMPoral knowLEdge augmented Multi-hop Question Answering (TEMPLE-MQA). Unlike previous methods, TEMPLE-MQA first constructs a time-aware graph (TAG) to store edit knowledge in a structured manner. Then, through our proposed inference path, structural retrieval, and joint reasoning stages, TEMPLE-MQA effectively discerns temporal contexts within the question query. Experiments on benchmark datasets demonstrate that TEMPLE-MQA significantly outperforms baseline models. Additionally, we contribute a new dataset, namely TKEMQA, which serves as the inaugural benchmark tailored specifically for MQA with temporal
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PROMPT-SAW&#27169;&#22411;&#65292;&#21033;&#29992;&#20851;&#31995;&#24863;&#30693;&#22270;&#26469;&#23454;&#29616;&#25991;&#26412;&#25552;&#31034;&#30340;&#21387;&#32553;&#65292;&#25552;&#39640;&#20102;&#25552;&#31034;&#30340;&#21487;&#35835;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00489</link><description>&lt;p&gt;
PROMPT-SAW&#65306;&#21033;&#29992;&#20851;&#31995;&#24863;&#30693;&#22270;&#36827;&#34892;&#25991;&#26412;&#25552;&#31034;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00489
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PROMPT-SAW&#27169;&#22411;&#65292;&#21033;&#29992;&#20851;&#31995;&#24863;&#30693;&#22270;&#26469;&#23454;&#29616;&#25991;&#26412;&#25552;&#31034;&#30340;&#21387;&#32553;&#65292;&#25552;&#39640;&#20102;&#25552;&#31034;&#30340;&#21487;&#35835;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22810;&#31181;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#25552;&#31034;&#26159;LLM&#25512;&#29702;&#20013;&#30340;&#22522;&#26412;&#24037;&#20855;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#36229;&#38271;&#25552;&#31034;&#20250;&#24102;&#26469;&#26174;&#33879;&#30340;&#25104;&#26412;&#12290;&#29616;&#26377;&#30340;&#21387;&#32553;&#38271;&#25552;&#31034;&#30340;&#23581;&#35797;&#23548;&#33268;&#21387;&#32553;&#25552;&#31034;&#22312;&#21487;&#35835;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#23545;&#25552;&#31034;&#25928;&#29992;&#20135;&#29983;&#26377;&#23475;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PROMPT-SAW&#65306;&#36890;&#36807;&#20851;&#31995;&#24863;&#30693;&#22270;&#36827;&#34892;&#25552;&#31034;&#21387;&#32553;&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#20219;&#21153;&#19981;&#21487;&#30693;&#21644;&#20219;&#21153;&#24863;&#30693;&#25552;&#31034;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;PROMPT-SAW&#20351;&#29992;&#25552;&#31034;&#30340;&#25991;&#26412;&#20449;&#24687;&#26500;&#24314;&#22270;&#24418;&#65292;&#22312;&#22270;&#24418;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#20803;&#32032;&#65292;&#20174;&#32780;&#24471;&#20986;&#21387;&#32553;&#25552;&#31034;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;GSM8K-AUG&#65292;&#21363;&#29616;&#26377;GSM8k&#22522;&#20934;&#30340;&#25193;&#23637;&#29256;&#26412;&#65292;&#29992;&#20110;&#20219;&#21153;&#19981;&#21487;&#30693;&#25552;&#31034;&#65292;&#20197;&#25552;&#20379;&#20840;&#38754;&#30340;&#35780;&#20272;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00489v1 Announce Type: cross  Abstract: Large language models (LLMs) have shown exceptional abilities for multiple different natural language processing tasks. While prompting is a crucial tool for LLM inference, we observe that there is a significant cost associated with exceedingly lengthy prompts. Existing attempts to compress lengthy prompts lead to sub-standard results in terms of readability and interpretability of the compressed prompt, with a detrimental impact on prompt utility. To address this, we propose PROMPT-SAW: Prompt compresSion via Relation AWare graphs, an effective strategy for prompt compression over task-agnostic and task-aware prompts. PROMPT-SAW uses the prompt's textual information to build a graph, later extracts key information elements in the graph to come up with the compressed prompt. We also propose GSM8K-AUG, i.e., an extended version of the existing GSM8k benchmark for task-agnostic prompts in order to provide a comprehensive evaluation platf
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22122;&#22768;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#21487;&#25193;&#23637;&#30340;&#26041;&#24335;&#20351;&#29992;&#24369;&#26631;&#35760;&#25991;&#26723;&#35757;&#32451;&#25552;&#21462;&#22120;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#22312;&#20225;&#19994;&#22330;&#26223;&#20013;&#35757;&#32451;&#22823;&#37327;&#19981;&#21516;&#25991;&#26723;&#31867;&#22411;&#30340;&#33258;&#23450;&#20041;&#25552;&#21462;&#22120;&#25152;&#38656;&#30340;&#26114;&#36149;&#20154;&#24037;&#26631;&#35760;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2404.00488</link><description>&lt;p&gt;
&#22122;&#22768;&#24863;&#30693;&#30340;&#24067;&#23616;&#24863;&#30693;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Noise-Aware Training of Layout-Aware Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22122;&#22768;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#21487;&#25193;&#23637;&#30340;&#26041;&#24335;&#20351;&#29992;&#24369;&#26631;&#35760;&#25991;&#26723;&#35757;&#32451;&#25552;&#21462;&#22120;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#22312;&#20225;&#19994;&#22330;&#26223;&#20013;&#35757;&#32451;&#22823;&#37327;&#19981;&#21516;&#25991;&#26723;&#31867;&#22411;&#30340;&#33258;&#23450;&#20041;&#25552;&#21462;&#22120;&#25152;&#38656;&#30340;&#26114;&#36149;&#20154;&#24037;&#26631;&#35760;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31687;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#65288;VRD&#65289;&#21033;&#29992;&#35270;&#35273;&#29305;&#24449;&#21644;&#35821;&#35328;&#32447;&#32034;&#20256;&#25773;&#20449;&#24687;&#12290;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#20174;&#25991;&#26723;&#20013;&#35782;&#21035;&#21629;&#21517;&#23454;&#20307;&#30340;&#33258;&#23450;&#20041;&#25552;&#21462;&#22120;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#20026;&#25991;&#26412;&#21644;&#35270;&#35273;&#27169;&#24577;&#30340;&#30446;&#26631;&#25991;&#26723;&#23454;&#20363;&#12290;&#22312;&#20225;&#19994;&#22330;&#26223;&#20013;&#65292;&#25105;&#20204;&#24076;&#26395;&#20197;&#21487;&#25193;&#23637;&#30340;&#26041;&#24335;&#20026;&#25104;&#21315;&#19978;&#19975;&#31181;&#19981;&#21516;&#25991;&#26723;&#31867;&#22411;&#35757;&#32451;&#33258;&#23450;&#20041;&#25552;&#21462;&#22120;&#65292;&#36825;&#26159;&#19968;&#20010;&#26114;&#36149;&#30340;&#29942;&#39048;&#12290;&#22312;&#26410;&#26631;&#35760;&#30446;&#26631;&#25991;&#26723;&#23454;&#20363;&#19978;&#39044;&#35757;&#32451;&#25552;&#21462;&#22120;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#20154;&#24037;&#26631;&#35760;&#23454;&#20363;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#26159;&#34892;&#19981;&#36890;&#30340;&#65292;&#22240;&#20026;&#23427;&#36229;&#20986;&#20102;&#20026;&#25552;&#21462;&#22120;&#20998;&#37197;&#30340;&#26368;&#22823;&#20801;&#35768;&#35757;&#32451;&#26102;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22122;&#22768;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#65288;NAT&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#22330;&#26223;&#12290;NAT&#21033;&#29992;&#24369;&#26631;&#35760;&#25991;&#26723;&#20197;&#21487;&#25193;&#23637;&#30340;&#26041;&#24335;&#35757;&#32451;&#25552;&#21462;&#22120;&#65292;&#32780;&#19981;&#26159;&#33719;&#21462;&#26114;&#36149;&#30340;&#20154;&#24037;&#26631;&#35760;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00488v1 Announce Type: cross  Abstract: A visually rich document (VRD) utilizes visual features along with linguistic cues to disseminate information. Training a custom extractor that identifies named entities from a document requires a large number of instances of the target document type annotated at textual and visual modalities. This is an expensive bottleneck in enterprise scenarios, where we want to train custom extractors for thousands of different document types in a scalable way. Pre-training an extractor model on unlabeled instances of the target document type, followed by a fine-tuning step on human-labeled instances does not work in these scenarios, as it surpasses the maximum allowable training time allocated for the extractor. We address this scenario by proposing a Noise-Aware Training method or NAT in this paper. Instead of acquiring expensive human-labeled documents, NAT utilizes weakly labeled documents to train an extractor in a scalable way. To avoid degr
&lt;/p&gt;</description></item><item><title>&#23558;&#26102;&#38388;&#24207;&#21015;&#34892;&#20026;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#21019;&#24314;&#26032;&#24418;&#24335;&#19978;&#19979;&#25991;&#26234;&#33021;&#26085;&#24535;&#24212;&#29992;&#65292;&#20419;&#36827;&#33258;&#25105;&#21453;&#24605;&#21644;&#24184;&#31119;&#24863;&#65292;&#20195;&#34920;&#20102;&#20154;&#24037;&#26234;&#33021;&#26032;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2404.00487</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#26234;&#33021;&#26085;&#24535;&#65306;&#23558;LLM&#21644;&#26102;&#38388;&#24207;&#21015;&#34892;&#20026;&#24863;&#30693;&#25216;&#26415;&#25972;&#21512;&#65292;&#20351;&#29992;MindScape&#24212;&#29992;&#20419;&#36827;&#33258;&#25105;&#21453;&#24605;&#21644;&#24184;&#31119;&#24863;
&lt;/p&gt;
&lt;p&gt;
Contextual AI Journaling: Integrating LLM and Time Series Behavioral Sensing Technology to Promote Self-Reflection and Well-being using the MindScape App
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00487
&lt;/p&gt;
&lt;p&gt;
&#23558;&#26102;&#38388;&#24207;&#21015;&#34892;&#20026;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#21019;&#24314;&#26032;&#24418;&#24335;&#19978;&#19979;&#25991;&#26234;&#33021;&#26085;&#24535;&#24212;&#29992;&#65292;&#20419;&#36827;&#33258;&#25105;&#21453;&#24605;&#21644;&#24184;&#31119;&#24863;&#65292;&#20195;&#34920;&#20102;&#20154;&#24037;&#26234;&#33021;&#26032;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MindScape&#26088;&#22312;&#30740;&#31350;&#23558;&#26102;&#38388;&#24207;&#21015;&#34892;&#20026;&#27169;&#24335;&#65288;&#22914;&#23545;&#35805;&#21442;&#19982;&#24230;&#12289;&#30561;&#30496;&#12289;&#20301;&#32622;&#65289;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30456;&#32467;&#21512;&#65292;&#21019;&#24314;&#19968;&#31181;&#26032;&#24418;&#24335;&#30340;&#19978;&#19979;&#25991;&#26234;&#33021;&#26085;&#24535;&#65292;&#20419;&#36827;&#33258;&#25105;&#21453;&#24605;&#21644;&#24184;&#31119;&#24863;&#12290;&#25105;&#20204;&#35748;&#20026;&#22312;LLMs&#20013;&#25972;&#21512;&#34892;&#20026;&#24863;&#30693;&#24456;&#21487;&#33021;&#20250;&#24341;&#39046;&#20154;&#24037;&#26234;&#33021;&#30340;&#26032;&#39046;&#22495;&#12290;&#22312;&#36825;&#31687;&#26202;&#25253;&#21578;&#24037;&#20316;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;MindScape&#19978;&#19979;&#25991;&#26085;&#24535;&#24212;&#29992;&#30340;&#35774;&#35745;&#65292;&#35813;&#24212;&#29992;&#21033;&#29992;LLMs&#21644;&#34892;&#20026;&#24863;&#30693;&#29983;&#25104;&#19978;&#19979;&#25991;&#21270;&#21644;&#20010;&#24615;&#21270;&#30340;&#26085;&#24535;&#25552;&#31034;&#65292;&#26088;&#22312;&#40723;&#21169;&#33258;&#25105;&#21453;&#24605;&#21644;&#24773;&#24863;&#21457;&#23637;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22522;&#20110;&#21021;&#27493;&#29992;&#25143;&#30740;&#31350;&#30340;MindScape&#23545;&#22823;&#23398;&#29983;&#30340;&#30740;&#31350;&#65292;&#20197;&#21450;&#25105;&#20204;&#21363;&#23558;&#23637;&#24320;&#30340;&#30740;&#31350;&#65292;&#35780;&#20272;&#19978;&#19979;&#25991;&#26234;&#33021;&#26085;&#24535;&#22312;&#20419;&#36827;&#22823;&#23398;&#26657;&#22253;&#20013;&#26356;&#22909;&#24184;&#31119;&#24863;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;MindScape&#20195;&#34920;&#20102;&#19968;&#31181;&#26032;&#30340;&#24212;&#29992;&#31867;&#21035;&#65292;&#23558;&#34892;&#20026;&#26234;&#33021;&#23884;&#20837;&#21040;&#20154;&#24037;&#26234;&#33021;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00487v1 Announce Type: cross  Abstract: MindScape aims to study the benefits of integrating time series behavioral patterns (e.g., conversational engagement, sleep, location) with Large Language Models (LLMs) to create a new form of contextual AI journaling, promoting self-reflection and well-being. We argue that integrating behavioral sensing in LLMs will likely lead to a new frontier in AI. In this Late-Breaking Work paper, we discuss the MindScape contextual journal App design that uses LLMs and behavioral sensing to generate contextual and personalized journaling prompts crafted to encourage self-reflection and emotional development. We also discuss the MindScape study of college students based on a preliminary user study and our upcoming study to assess the effectiveness of contextual AI journaling in promoting better well-being on college campuses. MindScape represents a new application class that embeds behavioral intelligence in AI.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36777;&#35777;&#23545;&#40784;&#65288;DA&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;LLM&#22312;&#38754;&#20020;&#22806;&#37096;&#26377;&#27602;&#25968;&#25454;&#26102;&#30340;&#36866;&#24212;&#24615;&#21464;&#33394;&#40857;&#38382;&#39064;&#65292;&#22686;&#24378;&#20102;&#20854;&#23545;&#25239;&#22806;&#37096;&#25915;&#20987;&#30340;&#23433;&#20840;&#24615;</title><link>https://arxiv.org/abs/2404.00486</link><description>&lt;p&gt;
&#36777;&#35777;&#23545;&#40784;&#65306;&#35299;&#20915;3H&#32039;&#24352;&#19982;LLM&#23433;&#20840;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
Dialectical Alignment: Resolving the Tension of 3H and Security Threats of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00486
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36777;&#35777;&#23545;&#40784;&#65288;DA&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;LLM&#22312;&#38754;&#20020;&#22806;&#37096;&#26377;&#27602;&#25968;&#25454;&#26102;&#30340;&#36866;&#24212;&#24615;&#21464;&#33394;&#40857;&#38382;&#39064;&#65292;&#22686;&#24378;&#20102;&#20854;&#23545;&#25239;&#22806;&#37096;&#25915;&#20987;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20852;&#36215;&#65292;&#30830;&#20445;&#23427;&#20204;&#20307;&#29616;&#26377;&#30410;&#12289;&#35802;&#23454;&#21644;&#26080;&#23475;&#65288;3H&#65289;&#21407;&#21017;&#65292;&#21363;&#20154;&#31867;&#23545;&#40784;&#65292;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#23545;&#40784;&#26041;&#27861;&#22914;RLHF&#12289;DPO&#31561;&#26377;&#25928;&#22320;&#24494;&#35843;LLM&#20197;&#21305;&#37197;&#20559;&#22909;&#25968;&#25454;&#38598;&#20013;&#30340;&#20559;&#22909;&#65292;&#20294;&#24448;&#24448;&#20250;&#20351;LLM&#23545;&#39640;&#24230;&#25509;&#21463;&#20154;&#31867;&#36755;&#20837;&#21644;&#22806;&#37096;&#35777;&#25454;&#65292;&#21363;&#20351;&#36825;&#20123;&#20449;&#24687;&#26159;&#26377;&#27602;&#30340;&#12290;&#36825;&#23548;&#33268;LLM&#20542;&#21521;&#20110;&#25104;&#20026;&#36866;&#24212;&#21464;&#33394;&#40857;&#65292;&#24403;&#22806;&#37096;&#35777;&#25454;&#19982;&#20854;&#21442;&#25968;&#24615;&#35760;&#24518;&#20914;&#31361;&#26102;&#12290;&#36825;&#21152;&#21095;&#20102;LLM&#36973;&#21463;&#22806;&#37096;&#26377;&#27602;&#25968;&#25454;&#25915;&#20987;&#30340;&#39118;&#38505;&#65292;&#23545;LLM&#31995;&#32479;&#24212;&#29992;&#65288;&#22914;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65289;&#26500;&#25104;&#20102;&#37325;&#22823;&#23433;&#20840;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65306;&#36777;&#35777;&#23545;&#40784;&#65288;DA&#65289;&#65292;&#23427;&#65288;1&#65289;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;&#26469;&#30830;&#23450;LLM&#23548;&#33322;&#30456;&#20114;&#25991;&#26412;&#20914;&#31361;&#21644;&#19978;&#19979;&#25991;&#35760;&#24518;&#20914;&#31361;&#30340;&#26368;&#20339;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00486v1 Announce Type: cross  Abstract: With the rise of large language models (LLMs), ensuring they embody the principles of being helpful, honest, and harmless (3H), known as Human Alignment, becomes crucial. While existing alignment methods like RLHF, DPO, etc., effectively fine-tune LLMs to match preferences in the preference dataset, they often lead LLMs to highly receptive human input and external evidence, even when this information is poisoned. This leads to a tendency for LLMs to be Adaptive Chameleons when external evidence conflicts with their parametric memory. This exacerbates the risk of LLM being attacked by external poisoned data, which poses a significant security risk to LLM system applications such as Retrieval-augmented generation (RAG). To address the challenge, we propose a novel framework: Dialectical Alignment (DA), which (1) utilizes AI feedback to identify optimal strategies for LLMs to navigate inter-context conflicts and context-memory conflicts w
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#25163;&#21160;&#26631;&#27880;&#30340;&#29992;&#20110;&#20845;&#31181;&#26031;&#25289;&#22827;&#35821;&#35328;&#30340;&#21629;&#21517;&#23454;&#20307;&#35821;&#26009;&#24211;&#65292;&#25552;&#20379;&#20102;&#20004;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#21010;&#20998;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#30340;&#35782;&#21035;&#12289;&#20998;&#31867;&#12289;&#24341;&#29992;&#35789;&#21270;&#21644;&#38142;&#25509;&#12290;</title><link>https://arxiv.org/abs/2404.00482</link><description>&lt;p&gt;
&#29992;&#20110;&#26031;&#25289;&#22827;&#35821;&#30340;&#36328;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual Named Entity Corpus for Slavic Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00482
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#25163;&#21160;&#26631;&#27880;&#30340;&#29992;&#20110;&#20845;&#31181;&#26031;&#25289;&#22827;&#35821;&#35328;&#30340;&#21629;&#21517;&#23454;&#20307;&#35821;&#26009;&#24211;&#65292;&#25552;&#20379;&#20102;&#20004;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#21010;&#20998;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#30340;&#35782;&#21035;&#12289;&#20998;&#31867;&#12289;&#24341;&#29992;&#35789;&#21270;&#21644;&#38142;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#21253;&#21547;&#20845;&#31181;&#26031;&#25289;&#22827;&#35821;&#35328;&#65288;&#20445;&#21152;&#21033;&#20122;&#35821;&#12289;&#25463;&#20811;&#35821;&#12289;&#27874;&#20848;&#35821;&#12289;&#26031;&#27931;&#25991;&#23612;&#20122;&#35821;&#12289;&#20420;&#35821;&#21644;&#20044;&#20811;&#20848;&#35821;&#65289;&#21629;&#21517;&#23454;&#20307;&#30340;&#35821;&#26009;&#24211;&#12290;&#36825;&#39033;&#24037;&#20316;&#26159;2017-2023&#24180;&#38388;&#26031;&#25289;&#22827;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#35752;&#20250;&#30340;&#19968;&#31995;&#21015;&#20849;&#20139;&#20219;&#21153;&#30340;&#32467;&#26524;&#12290;&#35813;&#35821;&#26009;&#24211;&#21253;&#21547;&#20102;5017&#20221;&#28085;&#30422;&#19971;&#20010;&#20027;&#39064;&#30340;&#25991;&#26723;&#65292;&#25991;&#26723;&#26631;&#26377;&#20116;&#31867;&#21629;&#21517;&#23454;&#20307;&#65292;&#27599;&#20010;&#23454;&#20307;&#30001;&#31867;&#21035;&#12289;&#24341;&#29992;&#35789;&#21644;&#21807;&#19968;&#36328;&#35821;&#35328;&#26631;&#35782;&#31526;&#25551;&#36848;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;&#35757;&#32451;&#35843;&#25972;&#30340;&#25968;&#25454;&#38598;&#21010;&#20998; - &#21333;&#20010;&#20027;&#39064;&#21010;&#20998;&#21644;&#36328;&#20027;&#39064;&#21010;&#20998;&#12290;&#23545;&#20110;&#27599;&#20010;&#21010;&#20998;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#35774;&#32622;&#20102;&#22522;&#20934;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;XLM-RoBERTa-large&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#25552;&#21450;&#35782;&#21035;&#21644;&#20998;&#31867;&#65292;&#20197;&#21450;mT5-large&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#24341;&#29992;&#35789;&#21270;&#21644;&#38142;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00482v1 Announce Type: cross  Abstract: This paper presents a corpus manually annotated with named entities for six Slavic languages - Bulgarian, Czech, Polish, Slovenian, Russian, and Ukrainian. This work is the result of a series of shared tasks, conducted in 2017-2023 as a part of the Workshops on Slavic Natural Language Processing. The corpus consists of 5 017 documents on seven topics. The documents are annotated with five classes of named entities. Each entity is described by a category, a lemma, and a unique cross-lingual identifier. We provide two train-tune dataset splits - single topic out and cross topics. For each split, we set benchmarks using a transformer-based neural network architecture with the pre-trained multilingual models - XLM-RoBERTa-large for named entity mention recognition and categorization, and mT5-large for named entity lemmatization and linking.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#29983;&#25104;&#26469;&#23454;&#29616;&#35821;&#35328;&#26657;&#20934;&#65292;&#21487;&#20197;&#20351;&#29992;&#25143;&#20570;&#20986;&#26657;&#20934;&#27010;&#29575;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.00474</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Linguistic Calibration of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00474
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#29983;&#25104;&#26469;&#23454;&#29616;&#35821;&#35328;&#26657;&#20934;&#65292;&#21487;&#20197;&#20351;&#29992;&#25143;&#20570;&#20986;&#26657;&#20934;&#27010;&#29575;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#22312;&#33258;&#20449;&#24187;&#35273;&#26102;&#23548;&#33268;&#29992;&#25143;&#20570;&#20986;&#27425;&#20248;&#21270;&#30340;&#19979;&#28216;&#20915;&#31574;&#12290;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#21475;&#22836;&#20256;&#36798;&#20854;&#20027;&#24352;&#27491;&#30830;&#27010;&#29575;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#29616;&#26377;&#27169;&#22411;&#26080;&#27861;&#29983;&#25104;&#20855;&#26377;&#26657;&#20934;&#32622;&#20449;&#24230;&#22768;&#26126;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#36890;&#36807;&#20915;&#31574;&#35282;&#24230;&#65292;&#20026;&#38271;&#31687;&#29983;&#25104;&#24418;&#24335;&#30340;&#35821;&#35328;&#26657;&#20934;&#24418;&#24335;&#21270;&#23450;&#20041;&#65306;&#22914;&#26524;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#20351;&#20854;&#29992;&#25143;&#33021;&#22815;&#20570;&#20986;&#26657;&#20934;&#27010;&#29575;&#39044;&#27979;&#65292;&#21017;&#35813;&#27169;&#22411;&#26159;&#35821;&#35328;&#19978;&#26657;&#20934;&#30340;&#12290;&#36825;&#20010;&#23450;&#20041;&#20351;&#24471;&#19968;&#20010;&#35757;&#32451;&#26694;&#26550;&#25104;&#20026;&#21487;&#33021;&#65292;&#20854;&#20013;&#19968;&#20010;&#30417;&#30563;&#24494;&#35843;&#27493;&#39588;&#24341;&#23548;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#21457;&#20986;&#24102;&#26377;&#32622;&#20449;&#24230;&#22768;&#26126;&#30340;&#38271;&#31687;&#29983;&#25104;&#65292;&#35832;&#22914;&#8220;&#25105;&#20272;&#35745;&#26377;30%&#30340;&#26426;&#20250;&#8230;&#8221;&#25110;&#8220;&#25105;&#30830;&#20449;&#8230;&#8221;&#65292;&#28982;&#21518;&#26159;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#27493;&#39588;&#65292;&#22870;&#21169;&#20351;&#29992;&#25143;&#33021;&#22815;&#23545;&#30456;&#20851;&#38382;&#39064;&#25552;&#20379;&#26657;&#20934;&#31572;&#26696;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#23545;Llama 2 7B &#36827;&#34892;&#35821;&#35328;&#26657;&#20934;&#65292;&#24182;&#21457;&#29616;&#22312;&#33258;&#21160;&#21270;&#21644;&#20154;&#31867;&#27979;&#35797;&#20013;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00474v1 Announce Type: cross  Abstract: Language models (LMs) may lead their users to make suboptimal downstream decisions when they confidently hallucinate. This issue can be mitigated by having the LM verbally convey the probability that its claims are correct, but existing models cannot produce text with calibrated confidence statements. Through the lens of decision-making, we formalize linguistic calibration for long-form generations: an LM is linguistically calibrated if its generations enable its users to make calibrated probabilistic predictions. This definition enables a training framework where a supervised finetuning step bootstraps an LM to emit long-form generations with confidence statements such as "I estimate a 30% chance of..." or "I am certain that...", followed by a reinforcement learning step which rewards generations that enable a user to provide calibrated answers to related questions. We linguistically calibrate Llama 2 7B and find in automated and huma
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#20013;&#30340;&#24178;&#20928;&#26631;&#31614;&#25915;&#20987;&#36890;&#36807;&#29305;&#23450;&#25552;&#31034;&#20316;&#20026;&#35302;&#21457;&#22120;&#65292;&#23454;&#29616;&#25104;&#21151;&#30340;&#21516;&#26102;&#30830;&#20445;&#27491;&#30830;&#26631;&#35760;&#26377;&#27602;&#26679;&#26412;&#65292;&#20294;&#20173;&#38754;&#20020;&#35823;&#28608;&#27963;&#38382;&#39064;&#21644;&#25361;&#25112;&#65292;&#38656;&#35201;&#26356;&#39640;&#27604;&#20363;&#30340;&#27602;&#23475;&#12290;</title><link>https://arxiv.org/abs/2404.00461</link><description>&lt;p&gt;
&#20174;&#23545;&#27604;&#20013;&#20986;&#29616;&#30340;&#24555;&#36895;&#26041;&#27861;&#65306;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#20013;&#30340;&#26377;&#25928;&#21644;&#38544;&#34109;&#24178;&#20928;&#26631;&#31614;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Shortcuts Arising from Contrast: Effective and Covert Clean-Label Attacks in Prompt-Based Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00461
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#20013;&#30340;&#24178;&#20928;&#26631;&#31614;&#25915;&#20987;&#36890;&#36807;&#29305;&#23450;&#25552;&#31034;&#20316;&#20026;&#35302;&#21457;&#22120;&#65292;&#23454;&#29616;&#25104;&#21151;&#30340;&#21516;&#26102;&#30830;&#20445;&#27491;&#30830;&#26631;&#35760;&#26377;&#27602;&#26679;&#26412;&#65292;&#20294;&#20173;&#38754;&#20020;&#35823;&#28608;&#27963;&#38382;&#39064;&#21644;&#25361;&#25112;&#65292;&#38656;&#35201;&#26356;&#39640;&#27604;&#20363;&#30340;&#27602;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt-based learning&#33539;&#24335;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#25928;&#21147;&#65292;&#21487;&#20197;&#25552;&#21319;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23398;&#20064;&#33539;&#24335;&#24050;&#34987;&#35777;&#26126;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#24403;&#21069;&#30340;&#24178;&#20928;&#26631;&#31614;&#25915;&#20987;&#65292;&#21033;&#29992;&#29305;&#23450;&#25552;&#31034;&#20316;&#20026;&#35302;&#21457;&#22120;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#22806;&#37096;&#35302;&#21457;&#22120;&#30340;&#24773;&#20917;&#19979;&#25104;&#21151;&#65292;&#24182;&#30830;&#20445;&#23545;&#26377;&#27602;&#26679;&#26412;&#30340;&#27491;&#30830;&#26631;&#35760;&#65292;&#30456;&#27604;&#26377;&#27602;&#26631;&#31614;&#25915;&#20987;&#26356;&#20855;&#38544;&#34109;&#24615;&#65292;&#20294;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#38754;&#20020;&#30528;&#20005;&#37325;&#30340;&#35823;&#28608;&#27963;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26356;&#22823;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#26356;&#39640;&#27604;&#20363;&#30340;&#27602;&#23475;&#12290;&#36890;&#36807;&#20256;&#32479;&#30340;&#36127;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#24178;&#20928;&#26631;&#31614;&#35774;&#32622;&#20013;&#22312;&#25928;&#21147;&#21644;&#38544;&#34109;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#26102;&#65292;&#25105;&#20204;&#21463;&#21040;&#21518;&#38376;&#20805;&#24403;&#24555;&#25463;&#26041;&#24335;&#30340;&#35266;&#24565;&#30340;&#21551;&#21457;&#65292;&#24182;&#20551;&#35774;&#36825;&#19968;&#24555;&#25463;&#26041;&#24335;&#28304;&#20110;t&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00461v1 Announce Type: cross  Abstract: Prompt-based learning paradigm has demonstrated remarkable efficacy in enhancing the adaptability of pretrained language models (PLMs), particularly in few-shot scenarios. However, this learning paradigm has been shown to be vulnerable to backdoor attacks. The current clean-label attack, employing a specific prompt as a trigger, can achieve success without the need for external triggers and ensure correct labeling of poisoned samples, which is more stealthy compared to the poisoned-label attack, but on the other hand, it faces significant issues with false activations and poses greater challenges, necessitating a higher rate of poisoning. Using conventional negative data augmentation methods, we discovered that it is challenging to trade off between effectiveness and stealthiness in a clean-label setting. In addressing this issue, we are inspired by the notion that a backdoor acts as a shortcut and posit that this shortcut stems from t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20114;&#21160;&#22810;&#26426;&#22120;&#20154;&#38598;&#32676;&#20219;&#21153;&#65292;&#30446;&#30340;&#26159;&#36890;&#36807;&#25163;&#21183;&#21644;&#22768;&#38899;&#24341;&#23548;&#20154;&#31867;&#21442;&#19982;&#26426;&#22120;&#20154;&#32676;&#20307;&#65292;&#24182;&#21019;&#36896;&#20102;&#26032;&#39062;&#30340;&#32676;&#20307;&#23548;&#33322;&#21644;&#25163;&#21183;&#21709;&#24212;&#31639;&#27861;</title><link>https://arxiv.org/abs/2404.00442</link><description>&lt;p&gt;
&#20855;&#26377;&#25163;&#21183;&#21709;&#24212;&#21644;&#38899;&#20048;&#20276;&#22863;&#30340;&#20114;&#21160;&#22810;&#26426;&#22120;&#20154;&#38598;&#32676;
&lt;/p&gt;
&lt;p&gt;
Interactive Multi-Robot Flocking with Gesture Responsiveness and Musical Accompaniment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00442
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20114;&#21160;&#22810;&#26426;&#22120;&#20154;&#38598;&#32676;&#20219;&#21153;&#65292;&#30446;&#30340;&#26159;&#36890;&#36807;&#25163;&#21183;&#21644;&#22768;&#38899;&#24341;&#23548;&#20154;&#31867;&#21442;&#19982;&#26426;&#22120;&#20154;&#32676;&#20307;&#65292;&#24182;&#21019;&#36896;&#20102;&#26032;&#39062;&#30340;&#32676;&#20307;&#23548;&#33322;&#21644;&#25163;&#21183;&#21709;&#24212;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#21313;&#24180;&#26469;&#65292;&#26426;&#22120;&#20154;&#30740;&#31350;&#20154;&#21592;&#19968;&#30452;&#33268;&#21147;&#20110;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#20174;&#21512;&#20316;&#25805;&#32437;&#21040;&#25628;&#32034;&#21644;&#25937;&#25588;&#12290;&#36825;&#20123;&#20219;&#21153;&#26159;&#20256;&#32479;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#22810;&#26426;&#22120;&#20154;&#25193;&#23637;&#65292;&#36890;&#24120;&#22312;&#36895;&#24230;&#25110;&#25928;&#29575;&#31561;&#32500;&#24230;&#19978;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;&#38543;&#30528;&#26426;&#22120;&#20154;&#20174;&#21830;&#19994;&#21644;&#30740;&#31350;&#39046;&#22495;&#36807;&#28193;&#21040;&#26085;&#24120;&#29615;&#22659;&#65292;&#31038;&#20132;&#20219;&#21153;&#30446;&#26631;&#65292;&#22914;&#21442;&#19982;&#25110;&#23089;&#20048;&#65292;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24341;&#20154;&#20837;&#32988;&#19988;&#26377;&#36259;&#30340;&#22810;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#20854;&#20013;&#20027;&#35201;&#30446;&#26631;&#26159;&#20351;&#20154;&#30528;&#36855;&#24182;&#20135;&#29983;&#20852;&#36259;&#12290;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;&#30446;&#26631;&#26159;&#35753;&#20154;&#31867;&#34987;&#21560;&#24341;&#24182;&#19982;&#21160;&#24577;&#12289;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#26426;&#22120;&#20154;&#32676;&#20307;&#19968;&#36215;&#31227;&#21160;&#24182;&#21442;&#19982;&#20854;&#20013;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#30740;&#31350;&#22242;&#38431;&#21019;&#24314;&#20102;&#28041;&#21450;&#26426;&#22120;&#20154;&#36816;&#21160;&#21644;&#24341;&#20154;&#27880;&#24847;&#30340;&#20132;&#20114;&#27169;&#24335;&#65292;&#22914;&#25163;&#21183;&#21644;&#22768;&#38899;&#30340;&#31639;&#27861;&#12290;&#36129;&#29486;&#22914;&#19979;&#65306;&#65288;1&#65289;&#19968;&#31181;&#28041;&#21450;&#20154;&#31867;&#21644;&#26426;&#22120;&#20154;&#20195;&#29702;&#30340;&#26032;&#39062;&#32676;&#20307;&#23548;&#33322;&#31639;&#27861;&#65292;&#65288;2&#65289;&#29992;&#20110;&#23454;&#26102;&#30340;&#25163;&#21183;&#21709;&#24212;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00442v1 Announce Type: cross  Abstract: For decades, robotics researchers have pursued various tasks for multi-robot systems, from cooperative manipulation to search and rescue. These tasks are multi-robot extensions of classical robotic tasks and often optimized on dimensions such as speed or efficiency. As robots transition from commercial and research settings into everyday environments, social task aims such as engagement or entertainment become increasingly relevant. This work presents a compelling multi-robot task, in which the main aim is to enthrall and interest. In this task, the goal is for a human to be drawn to move alongside and participate in a dynamic, expressive robot flock. Towards this aim, the research team created algorithms for robot movements and engaging interaction modes such as gestures and sound. The contributions are as follows: (1) a novel group navigation algorithm involving human and robot agents, (2) a gesture responsive algorithm for real-time
&lt;/p&gt;</description></item><item><title>&#20998;&#24067;&#24335;&#29422;&#23376;&#26159;&#23545; Lion &#36827;&#34892;&#20102;&#21019;&#26032;&#24615;&#25913;&#36827;&#65292;&#21033;&#29992;&#31526;&#21495;&#25805;&#20316;&#31526;&#38477;&#20302;&#20102;&#36890;&#20449;&#25104;&#26412;&#65292;&#22312;&#20998;&#24067;&#24335;&#35757;&#32451;&#20013;&#21462;&#24471;&#20102;&#19982;&#26631;&#20934; Lion &#25110; AdamW &#20248;&#21270;&#22120;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20102;&#36890;&#20449;&#24102;&#23485;&#12290;</title><link>https://arxiv.org/abs/2404.00438</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#24067;&#24335;&#29422;&#23376;&#36827;&#34892;&#39640;&#25928;&#36890;&#20449;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Communication Efficient Distributed Training with Distributed Lion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00438
&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#29422;&#23376;&#26159;&#23545; Lion &#36827;&#34892;&#20102;&#21019;&#26032;&#24615;&#25913;&#36827;&#65292;&#21033;&#29992;&#31526;&#21495;&#25805;&#20316;&#31526;&#38477;&#20302;&#20102;&#36890;&#20449;&#25104;&#26412;&#65292;&#22312;&#20998;&#24067;&#24335;&#35757;&#32451;&#20013;&#21462;&#24471;&#20102;&#19982;&#26631;&#20934; Lion &#25110; AdamW &#20248;&#21270;&#22120;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20102;&#36890;&#20449;&#24102;&#23485;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Lion&#20248;&#21270;&#22120;&#22312;&#35757;&#32451;&#22823;&#22411;AI&#27169;&#22411;&#26041;&#38754;&#19982;AdamW&#26377;&#19968;&#23450;&#31454;&#20105;&#21147;&#65292;&#20855;&#26377;&#22312;&#20869;&#23384;&#12289;&#35745;&#31639;&#21644;&#26679;&#26412;&#25928;&#29575;&#19978;&#30340;&#20248;&#21183;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20998;&#24067;&#24335;&#29422;&#23376;&#65292;&#36825;&#26159;&#29422;&#23376;&#22312;&#20998;&#24067;&#24335;&#35757;&#32451;&#29615;&#22659;&#20013;&#30340;&#21019;&#26032;&#24615;&#25913;&#36827;&#12290;&#21033;&#29992;&#29422;&#23376;&#20013;&#30340;&#31526;&#21495;&#25805;&#20316;&#31526;&#65292;&#25105;&#20204;&#30340;&#20998;&#24067;&#24335;&#29422;&#23376;&#21482;&#38656;&#35201;&#22312;&#24037;&#20316;&#33410;&#28857;&#21644;&#20013;&#24515;&#26381;&#21153;&#22120;&#20043;&#38388;&#20256;&#36882;&#20108;&#36827;&#21046;&#25110;&#20302;&#31934;&#24230;&#21521;&#37327;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#36890;&#20449;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#35777;&#23454;&#20102;&#20998;&#24067;&#24335;&#29422;&#23376;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#22312;&#22810;&#31181;&#20219;&#21153;&#12289;&#24037;&#20316;&#32773;&#25968;&#37327;&#21644;&#25209;&#37327;&#22823;&#23567;&#19978;&#34920;&#29616;&#31283;&#20581;&#65292;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20998;&#24067;&#24335;&#29422;&#23376;&#22312;&#32858;&#21512;&#26799;&#24230;&#19978;&#36798;&#21040;&#20102;&#19982;&#26631;&#20934;&#29422;&#23376;&#25110;AdamW&#20248;&#21270;&#22120;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20294;&#36890;&#20449;&#24102;&#23485;&#26174;&#33879;&#20943;&#23569;&#12290;&#36825;&#20010;&#29305;&#24615;&#23545;&#20110;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#23588;&#20026;&#26377;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00438v1 Announce Type: cross  Abstract: The Lion optimizer has been a promising competitor with the AdamW for training large AI models, with advantages on memory, computation, and sample efficiency. In this paper, we introduce Distributed Lion, an innovative adaptation of Lion for distributed training environments. Leveraging the sign operator in Lion, our Distributed Lion only requires communicating binary or lower-precision vectors between workers to the center server, significantly reducing the communication cost. Our theoretical analysis confirms Distributed Lion's convergence properties. Empirical results demonstrate its robustness across a range of tasks, worker counts, and batch sizes, on both vision and language problems. Notably, Distributed Lion attains comparable performance to standard Lion or AdamW optimizers applied on aggregated gradients, but with significantly reduced communication bandwidth. This feature is particularly advantageous for training large model
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#27861;&#24459;&#25991;&#26412;&#20998;&#31867;&#31995;&#32479;&#65292;&#20351;&#27169;&#22411;&#30340;&#20915;&#31574;&#21464;&#24471;&#21487;&#29702;&#35299;&#32473;&#26368;&#32456;&#29992;&#25143;</title><link>https://arxiv.org/abs/2404.00437</link><description>&lt;p&gt;
&#21033;&#29992;&#26641;&#20272;&#35745;&#22120;&#33258;&#21160;&#35299;&#37322;&#35199;&#29677;&#29273;&#27861;&#24459;&#21028;&#20915;&#22312;&#20381;&#36182;&#21496;&#27861;&#31649;&#36758;&#30340;&#27861;&#24459;&#31867;&#21035;&#20013;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Automatic explanation of the classification of Spanish legal judgments in jurisdiction-dependent law categories with tree estimators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#27861;&#24459;&#25991;&#26412;&#20998;&#31867;&#31995;&#32479;&#65292;&#20351;&#27169;&#22411;&#30340;&#20915;&#31574;&#21464;&#24471;&#21487;&#29702;&#35299;&#32473;&#26368;&#32456;&#29992;&#25143;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#27861;&#24459;&#25991;&#26412;&#20998;&#31867;&#31995;&#32479;&#24050;&#32463;&#34987;&#25552;&#20986;&#22312;&#25991;&#29486;&#20013;&#65292;&#20197;&#35299;&#20915;&#30693;&#35782;&#20174;&#21028;&#20915;&#20013;&#25552;&#21462;&#24182;&#26816;&#27979;&#20854;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#23427;&#20204;&#30340;&#27169;&#22411;&#26159;&#21487;&#35299;&#37322;&#30340;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#31995;&#32479;&#37117;&#26159;&#40657;&#30418;&#30340;&#12290;&#36825;&#21487;&#33021;&#24341;&#21457;&#23545;&#23427;&#20204;&#21487;&#20449;&#24230;&#30340;&#25285;&#24551;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#65292;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#23545;&#27861;&#24459;&#25991;&#26412;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20915;&#31574;&#20013;&#28041;&#21450;&#30340;&#29305;&#24449;&#21644;&#26641;&#32467;&#26500;&#30340;&#20915;&#31574;&#36335;&#24452;&#30340;&#38408;&#20540;&#20998;&#21449;&#20540;&#65292;&#24182;&#20197;&#33258;&#28982;&#35821;&#35328;&#30340;&#26041;&#24335;&#21521;&#29992;&#25143;&#21576;&#29616;&#36825;&#20123;&#20449;&#24687;&#12290;&#36825;&#26159;&#31532;&#19968;&#39033;&#20851;&#20110;&#33258;&#21160;&#20998;&#26512;&#27861;&#24459;&#25991;&#26412;&#30340;&#24037;&#20316;&#65292;&#32467;&#21512;NLP&#21644;ML&#20197;&#21450;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#20197;&#20351;&#27169;&#22411;&#30340;&#20915;&#31574;&#33258;&#21160;&#21464;&#24471;&#21487;&#29702;&#35299;&#32473;&#26368;&#32456;&#29992;&#25143;&#12290;&#27492;&#22806;&#65292;&#27861;&#24459;&#19987;&#23478;&#24050;&#32463;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#20123;&#30693;&#35782;&#20063;&#24050;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00437v1 Announce Type: cross  Abstract: Automatic legal text classification systems have been proposed in the literature to address knowledge extraction from judgments and detect their aspects. However, most of these systems are black boxes even when their models are interpretable. This may raise concerns about their trustworthiness. Accordingly, this work contributes with a system combining Natural Language Processing (NLP) with Machine Learning (ML) to classify legal texts in an explainable manner. We analyze the features involved in the decision and the threshold bifurcation values of the decision paths of tree structures and present this information to the users in natural language. This is the first work on automatic analysis of legal texts combining NLP and ML along with Explainable Artificial Intelligence techniques to automatically make the models' decisions understandable to end users. Furthermore, legal experts have validated our solution, and this knowledge has al
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#37327;&#21270;&#20132;&#26131;&#31574;&#30053;&#65292;&#21033;&#29992;&#25913;&#36827;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#24773;&#24863;&#20998;&#26512;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#19981;&#20165;&#22312;&#25429;&#25417;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#21644;&#24314;&#27169;&#25968;&#25454;&#20851;&#31995;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#32780;&#19988;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#19968;&#27573;&#26102;&#38388;&#20869;&#30340;&#22238;&#25253;&#12290;</title><link>https://arxiv.org/abs/2404.00424</link><description>&lt;p&gt;
&#20174;&#27880;&#24847;&#21147;&#21040;&#21033;&#28070;&#65306;&#22522;&#20110;Transformer&#30340;&#37327;&#21270;&#20132;&#26131;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
From attention to profit: quantitative trading strategy based on transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00424
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#37327;&#21270;&#20132;&#26131;&#31574;&#30053;&#65292;&#21033;&#29992;&#25913;&#36827;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#24773;&#24863;&#20998;&#26512;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#19981;&#20165;&#22312;&#25429;&#25417;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#21644;&#24314;&#27169;&#25968;&#25454;&#20851;&#31995;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#32780;&#19988;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#19968;&#27573;&#26102;&#38388;&#20869;&#30340;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#37327;&#21270;&#20132;&#26131;&#23454;&#36341;&#20013;&#65292;&#24212;&#23545;&#22797;&#26434;&#21160;&#24577;&#30340;&#37329;&#34701;&#24066;&#22330;&#19968;&#30452;&#26159;&#20010;&#25345;&#20037;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#20805;&#20998;&#25429;&#25417;&#21508;&#31181;&#24066;&#22330;&#21464;&#37327;&#65292;&#32463;&#24120;&#24573;&#35270;&#38271;&#26399;&#20449;&#24687;&#24182;&#19988;&#26080;&#27861;&#25429;&#25417;&#21487;&#33021;&#24102;&#26469;&#21033;&#28070;&#30340;&#22522;&#26412;&#20449;&#21495;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#25913;&#36827;&#30340;Transformer&#26550;&#26500;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#35813;&#27169;&#22411;&#30340;&#26032;&#22411;&#22240;&#23376;&#12290;&#36890;&#36807;&#20174;&#24773;&#24863;&#20998;&#26512;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#19981;&#20165;&#21457;&#25381;&#20102;&#20854;&#21407;&#26377;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#25429;&#25417;&#21644;&#24314;&#27169;&#22797;&#26434;&#25968;&#25454;&#20851;&#31995;&#30340;&#20248;&#21183;&#65292;&#32780;&#19988;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#25968;&#20540;&#36755;&#20837;&#30340;&#20219;&#21153;&#65292;&#24182;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#19968;&#27573;&#26102;&#38388;&#20869;&#30340;&#22238;&#25253;&#12290;&#35813;&#30740;&#31350;&#25910;&#38598;&#20102;2010&#24180;&#33267;2019&#24180;&#20013;&#22269;&#36164;&#26412;&#24066;&#22330;4,601&#21482;&#32929;&#31080;&#30340;5,000,000&#22810;&#26465;&#28378;&#21160;&#25968;&#25454;&#12290;&#30740;&#31350;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#32929;&#31080;&#34920;&#29616;&#26041;&#38754;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00424v1 Announce Type: cross  Abstract: In traditional quantitative trading practice, navigating the complicated and dynamic financial market presents a persistent challenge. Former machine learning approaches have struggled to fully capture various market variables, often ignore long-term information and fail to catch up with essential signals that may lead the profit. This paper introduces an enhanced transformer architecture and designs a novel factor based on the model. By transfer learning from sentiment analysis, the proposed model not only exploits its original inherent advantages in capturing long-range dependencies and modelling complex data relationships but is also able to solve tasks with numerical inputs and accurately forecast future returns over a period. This work collects more than 5,000,000 rolling data of 4,601 stocks in the Chinese capital market from 2010 to 2019. The results of this study demonstrated the model's superior performance in predicting stock
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;Multi-level Online Sequential Experts (MOSE)&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#23618;&#30417;&#30563;&#21644;&#21453;&#21521;&#33258;&#33976;&#39311;&#65292;&#35299;&#20915;&#20102;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#20013;&#26032;&#26087;&#35757;&#32451;&#26679;&#26412;&#23398;&#20064;&#19981;&#36275;&#21644;&#37325;&#22797;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.00417</link><description>&lt;p&gt;
&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#20013;&#25512;&#36827;&#28508;&#22312;&#19987;&#19994;&#30693;&#35782;&#30340;&#32534;&#25490;&#65306;&#22810;&#23618;&#30417;&#30563;&#21644;&#21453;&#21521;&#33258;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Orchestrate Latent Expertise: Advancing Online Continual Learning with Multi-Level Supervision and Reverse Self-Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00417
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;Multi-level Online Sequential Experts (MOSE)&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#23618;&#30417;&#30563;&#21644;&#21453;&#21521;&#33258;&#33976;&#39311;&#65292;&#35299;&#20915;&#20102;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#20013;&#26032;&#26087;&#35757;&#32451;&#26679;&#26412;&#23398;&#20064;&#19981;&#36275;&#21644;&#37325;&#22797;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36866;&#24212;&#29616;&#23454;&#19990;&#30028;&#30340;&#21160;&#24577;&#21464;&#21270;&#65292;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#38656;&#35201;&#20197;&#22312;&#32447;&#26041;&#24335;&#22788;&#29702;&#36830;&#32493;&#21040;&#36798;&#30340;&#20869;&#23481;&#12290;&#22312;&#27491;&#24120;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#35797;&#22270;&#36890;&#36807;&#31163;&#32447;&#35757;&#32451;&#27599;&#20010;&#20219;&#21153;&#26469;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24773;&#20917;&#20043;&#22806;&#65292;&#22312;&#19968;&#27425;&#25968;&#25454;&#27969;&#20013;&#25191;&#34892;CL&#30340;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#65288;OCL&#65289;&#26159;&#19968;&#31181;&#26356;&#20855;&#25361;&#25112;&#24615;&#20294;&#26356;&#29616;&#23454;&#30340;&#35774;&#32622;&#12290;&#24403;&#21069;&#30340;OCL&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#26087;&#35757;&#32451;&#26679;&#26412;&#30340;&#20869;&#23384;&#37325;&#25918;&#12290;&#28982;&#32780;&#65292;&#20174;CL&#21040;OCL&#30340;&#19968;&#20010;&#26174;&#30528;&#24046;&#36317;&#28304;&#20110;&#19982;&#37325;&#28436;&#32531;&#20914;&#21306;&#30340;&#20351;&#29992;&#30456;&#20851;&#30340;&#36807;&#24230;&#25311;&#21512;-&#27424;&#25311;&#21512;&#22256;&#22659;&#65306;&#23545;&#26032;&#35757;&#32451;&#26679;&#26412;&#30340;&#23398;&#20064;&#19981;&#36275;&#65288;&#27424;&#25311;&#21512;&#65289;&#20197;&#21450;&#23545;&#23569;&#37327;&#26087;&#35757;&#32451;&#26679;&#26412;&#30340;&#37325;&#22797;&#23398;&#20064;&#65288;&#36807;&#25311;&#21512;&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#22810;&#32423;&#22312;&#32447;&#39034;&#24207;&#19987;&#23478;&#65288;MOSE&#65289;&#65292;&#23427;&#23558;&#27169;&#22411;&#20316;&#20026;&#22534;&#21472;&#30340;&#23376;&#19987;&#23478;&#36827;&#34892;&#22521;&#32946;&#65292;&#24182;&#25972;&#21512;&#20102;&#22810;&#32423;&#30417;&#30563;&#21644;&#21453;&#21521;&#33258;&#33976;&#39311;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00417v1 Announce Type: cross  Abstract: To accommodate real-world dynamics, artificial intelligence systems need to cope with sequentially arriving content in an online manner. Beyond regular Continual Learning (CL) attempting to address catastrophic forgetting with offline training of each task, Online Continual Learning (OCL) is a more challenging yet realistic setting that performs CL in a one-pass data stream. Current OCL methods primarily rely on memory replay of old training samples. However, a notable gap from CL to OCL stems from the additional overfitting-underfitting dilemma associated with the use of rehearsal buffers: the inadequate learning of new training samples (underfitting) and the repeated learning of a few old training samples (overfitting). To this end, we introduce a novel approach, Multi-level Online Sequential Experts (MOSE), which cultivates the model as stacked sub-experts, integrating multi-level supervision and reverse self-distillation. Supervisi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26088;&#22312;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24212;&#29992;&#20110;&#31354;&#38388;&#23548;&#33322;&#21644;&#25511;&#21046;&#39046;&#22495;&#65292;&#36890;&#36807;&#24320;&#21457;&#32431;LLM&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#22312; Kerbal &#22826;&#31354;&#35745;&#21010;&#24046;&#20998;&#28216;&#25103;&#25361;&#25112;&#20013;&#21462;&#24471;&#31532;&#20108;&#21517;&#65292;&#39318;&#27425;&#23558;LLM&#20195;&#29702;&#38598;&#25104;&#21040;&#31354;&#38388;&#36164;&#28304;&#20013;&#12290;</title><link>https://arxiv.org/abs/2404.00413</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#33322;&#22825;&#22120;&#25805;&#20316;&#21592;
&lt;/p&gt;
&lt;p&gt;
Language Models are Spacecraft Operators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00413
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26088;&#22312;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24212;&#29992;&#20110;&#31354;&#38388;&#23548;&#33322;&#21644;&#25511;&#21046;&#39046;&#22495;&#65292;&#36890;&#36807;&#24320;&#21457;&#32431;LLM&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#22312; Kerbal &#22826;&#31354;&#35745;&#21010;&#24046;&#20998;&#28216;&#25103;&#25361;&#25112;&#20013;&#21462;&#24471;&#31532;&#20108;&#21517;&#65292;&#39318;&#27425;&#23558;LLM&#20195;&#29702;&#38598;&#25104;&#21040;&#31354;&#38388;&#36164;&#28304;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20986;&#29616;&#20102;&#19968;&#20010;&#36235;&#21183;&#65292;&#21363;&#24191;&#27867;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#26681;&#25454;&#29992;&#25143;&#25991;&#26412;&#25552;&#31034;&#20869;&#23481;&#37319;&#21462;&#34892;&#21160;&#30340;&#33258;&#20027;&#20195;&#29702;&#12290;&#25105;&#20204;&#25171;&#31639;&#23558;&#36825;&#20123;&#27010;&#24565;&#24212;&#29992;&#20110;&#31354;&#38388;&#23548;&#33322;&#21644;&#25511;&#21046;&#39046;&#22495;&#65292;&#20351;LLMs&#22312;&#33258;&#20027;&#21355;&#26143;&#25805;&#20316;&#30340;&#20915;&#31574;&#36807;&#31243;&#20013;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#12290;&#20316;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#31532;&#19968;&#27493;&#65292;&#25105;&#20204;&#20026; Kerbal &#22826;&#31354;&#35745;&#21010;&#24046;&#20998;&#28216;&#25103;(KSPDG)&#25361;&#25112;&#24320;&#21457;&#20102;&#19968;&#31181;&#32431;LLM&#26041;&#26696;&#65292;&#36825;&#26159;&#19968;&#20010;&#20844;&#24320;&#30340;&#36719;&#20214;&#35774;&#35745;&#31454;&#36187;&#65292;&#21442;&#19982;&#32773;&#20026;&#21355;&#26143;&#25805;&#32437;&#21019;&#24314;&#33258;&#20027;&#20195;&#29702;&#65292;&#22312;KSP&#28216;&#25103;&#24341;&#25806;&#19978;&#36816;&#34892;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#25552;&#31034;&#24037;&#31243;&#12289;&#23569;&#26679;&#26412;&#25552;&#31034;&#21644;&#24494;&#35843;&#25216;&#26415;&#21019;&#24314;&#20102;&#19968;&#20010;&#25928;&#26524;&#33391;&#22909;&#30340;LLM&#20195;&#29702;&#65292;&#22312;&#31454;&#36187;&#20013;&#25490;&#21517;&#31532;&#20108;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#24320;&#21019;&#20102;&#23558;LLM&#20195;&#29702;&#25972;&#21512;&#21040;&#31354;&#38388;&#36164;&#28304;&#20013;&#30340;&#20808;&#27827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00413v1 Announce Type: cross  Abstract: Recent trends are emerging in the use of Large Language Models (LLMs) as autonomous agents that take actions based on the content of the user text prompts. We intend to apply these concepts to the field of Guidance, Navigation, and Control in space, enabling LLMs to have a significant role in the decision-making process for autonomous satellite operations. As a first step towards this goal, we have developed a pure LLM-based solution for the Kerbal Space Program Differential Games (KSPDG) challenge, a public software design competition where participants create autonomous agents for maneuvering satellites involved in non-cooperative space operations, running on the KSP game engine. Our approach leverages prompt engineering, few-shot prompting, and fine-tuning techniques to create an effective LLM-based agent that ranked 2nd in the competition. To the best of our knowledge, this work pioneers the integration of LLM agents into space res
&lt;/p&gt;</description></item><item><title>TACO&#26159;&#31532;&#19968;&#20010;&#21033;&#29992;1,814&#26465;&#25512;&#25991;&#26500;&#24314;&#30340;Twitter Arguments&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;200&#22330;&#23436;&#25972;&#23545;&#35805;&#65292;&#20845;&#20010;&#20027;&#39064;&#65292;&#20855;&#26377;0.718&#30340;Krippendorff's alpha&#19968;&#33268;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#27880;&#37322;&#26694;&#26550;&#26469;&#23450;&#20041;&#21644;&#35782;&#21035;&#35770;&#28857;&#32467;&#26500;&#35201;&#32032;&#12290;</title><link>https://arxiv.org/abs/2404.00406</link><description>&lt;p&gt;
TACO -- Twitter Arguments from COnversations
&lt;/p&gt;
&lt;p&gt;
TACO -- Twitter Arguments from COnversations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00406
&lt;/p&gt;
&lt;p&gt;
TACO&#26159;&#31532;&#19968;&#20010;&#21033;&#29992;1,814&#26465;&#25512;&#25991;&#26500;&#24314;&#30340;Twitter Arguments&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;200&#22330;&#23436;&#25972;&#23545;&#35805;&#65292;&#20845;&#20010;&#20027;&#39064;&#65292;&#20855;&#26377;0.718&#30340;Krippendorff's alpha&#19968;&#33268;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#27880;&#37322;&#26694;&#26550;&#26469;&#23450;&#20041;&#21644;&#35782;&#21035;&#35770;&#28857;&#32467;&#26500;&#35201;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Twitter&#24050;&#32463;&#25104;&#20026;&#20840;&#29699;&#21442;&#19982;&#22312;&#32447;&#23545;&#35805;&#30340;&#20013;&#24515;&#65292;&#20063;&#25104;&#20026;&#21508;&#20010;&#23398;&#31185;&#30740;&#31350;&#35821;&#26009;&#24211;&#30340;&#37325;&#35201;&#26469;&#28304;&#65292;&#36825;&#20123;&#23398;&#31185;&#24050;&#32463;&#24847;&#35782;&#21040;&#20854;&#29992;&#25143;&#29983;&#25104;&#30340;&#20869;&#23481;&#30340;&#37325;&#35201;&#24615;&#12290;&#35770;&#28857;&#25366;&#25496;&#26159;&#22788;&#29702;&#21644;&#29702;&#35299;&#22312;&#32447;&#35805;&#35821;&#30340;&#37325;&#35201;&#20998;&#26512;&#20219;&#21153;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#26088;&#22312;&#35782;&#21035;&#35770;&#28857;&#30340;&#32467;&#26500;&#35201;&#32032;&#65292;&#34920;&#31034;&#20026;&#20449;&#24687;&#21644;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35201;&#32032;&#24182;&#19981;&#26159;&#38745;&#24577;&#30340;&#65292;&#21487;&#33021;&#38656;&#35201;&#22312;&#25152;&#22312;&#23545;&#35805;&#20013;&#35774;&#32622;&#19978;&#19979;&#25991;&#65292;&#28982;&#32780;&#32570;&#20047;&#35299;&#20915;Twitter&#19978;&#36825;&#19968;&#21160;&#24577;&#26041;&#38754;&#30340;&#25968;&#25454;&#21644;&#27880;&#37322;&#26694;&#26550;&#12290;&#25105;&#20204;&#36129;&#29486;&#20102;TACO&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21033;&#29992;1,814&#26465;&#28085;&#30422;200&#22330;&#23436;&#25972;&#23545;&#35805;&#12289;&#28085;&#30422;&#20845;&#20010;&#24322;&#36136;&#20027;&#39064;&#30340;&#25512;&#25991;&#30340;Twitter Arguments&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#20845;&#21517;&#19987;&#23478;&#20043;&#38388;&#20197;0.718&#30340;Krippendorff's alpha&#36798;&#25104;&#19968;&#33268;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25105;&#20204;&#30340;&#27880;&#37322;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#26469;&#33258;&#21073;&#26725;&#35789;&#20856;&#30340;&#23450;&#20041;&#65292;&#20197;&#23450;&#20041;&#21644;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00406v1 Announce Type: cross  Abstract: Twitter has emerged as a global hub for engaging in online conversations and as a research corpus for various disciplines that have recognized the significance of its user-generated content. Argument mining is an important analytical task for processing and understanding online discourse. Specifically, it aims to identify the structural elements of arguments, denoted as information and inference. These elements, however, are not static and may require context within the conversation they are in, yet there is a lack of data and annotation frameworks addressing this dynamic aspect on Twitter. We contribute TACO, the first dataset of Twitter Arguments utilizing 1,814 tweets covering 200 entire conversations spanning six heterogeneous topics annotated with an agreement of 0.718 Krippendorff's alpha among six experts. Second, we provide our annotation framework, incorporating definitions from the Cambridge Dictionary, to define and identify
&lt;/p&gt;</description></item><item><title>Aurora-M &#26159;&#31532;&#19968;&#20010;&#26681;&#25454;&#32654;&#22269;&#34892;&#25919;&#21629;&#20196;&#36827;&#34892;&#32418;&#38431;&#27979;&#35797;&#30340;&#24320;&#28304;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#33521;&#35821;&#12289;&#33452;&#20848;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#26085;&#35821;&#12289;&#36234;&#21335;&#35821;&#21644;&#20195;&#30721;&#19978;&#35757;&#32451;&#65292;&#19981;&#26029;&#39044;&#35757;&#32451;&#65292;&#21253;&#25324;&#20102;&#20154;&#24037;&#23457;&#26680;&#30340;&#23433;&#20840;&#35828;&#26126;&#65292;&#24635;&#35757;&#32451; token &#25968;&#36229;&#36807; 2 &#19975;&#20159;&#20010;</title><link>https://arxiv.org/abs/2404.00399</link><description>&lt;p&gt;
Aurora-M: &#26681;&#25454;&#32654;&#22269;&#34892;&#25919;&#21629;&#20196;&#65292;&#31532;&#19968;&#20010;&#24320;&#28304;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#32418;&#38431;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00399
&lt;/p&gt;
&lt;p&gt;
Aurora-M &#26159;&#31532;&#19968;&#20010;&#26681;&#25454;&#32654;&#22269;&#34892;&#25919;&#21629;&#20196;&#36827;&#34892;&#32418;&#38431;&#27979;&#35797;&#30340;&#24320;&#28304;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#33521;&#35821;&#12289;&#33452;&#20848;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#26085;&#35821;&#12289;&#36234;&#21335;&#35821;&#21644;&#20195;&#30721;&#19978;&#35757;&#32451;&#65292;&#19981;&#26029;&#39044;&#35757;&#32451;&#65292;&#21253;&#25324;&#20102;&#20154;&#24037;&#23457;&#26680;&#30340;&#23433;&#20840;&#35828;&#26126;&#65292;&#24635;&#35757;&#32451; token &#25968;&#36229;&#36807; 2 &#19975;&#20159;&#20010;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25903;&#25345;&#22810;&#31181;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#35757;&#32451;&#26102;&#39640;&#26114;&#30340;&#35745;&#31639;&#25104;&#26412;&#38480;&#21046;&#20102;&#21487;&#35775;&#38382;&#24615;&#12290;BLOOM &#21644; StarCoder &#31561;&#20513;&#35758;&#26088;&#22312;&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#20110;&#21327;&#20316;&#31038;&#21306;&#24320;&#21457;&#26356;&#20855;&#27665;&#20027;&#24615;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23384;&#22312;&#30340;&#27169;&#22411;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#65306;&#22810;&#35821;&#35328;&#33021;&#21147;&#26377;&#38480;&#65292;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#32780;&#20174;&#22836;&#24320;&#22987;&#39044;&#35757;&#32451;&#21448;&#20855;&#26377;&#39640;&#26114;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#19988;&#38656;&#35201;&#36981;&#23432;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#21644;&#21457;&#23637;&#27861;&#24459;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102; Aurora-M&#65292;&#19968;&#20010;&#21253;&#21547; 15B &#21442;&#25968;&#30340;&#22810;&#35821;&#35328;&#24320;&#28304;&#27169;&#22411;&#65292;&#35757;&#32451;&#35821;&#35328;&#21253;&#25324;&#33521;&#35821;&#12289;&#33452;&#20848;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#26085;&#35821;&#12289;&#36234;&#21335;&#35821;&#21644;&#20195;&#30721;&#12290;Aurora-M &#19981;&#26029;&#20174; StarCoderPlus &#19978;&#39044;&#35757;&#32451;&#65292;&#39069;&#22806;&#35757;&#32451;&#20102; 4350 &#20159;&#20010; token&#65292;&#24635;&#35757;&#32451; token &#25968;&#36229;&#36807;&#20102; 2 &#19975;&#20159;&#20010;&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#22312;&#20154;&#24037;&#23457;&#26680;&#30340;&#23433;&#20840;&#35828;&#26126;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#24320;&#28304;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20854;&#24320;&#21457;&#19982;&#20256;&#32479;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00399v1 Announce Type: cross  Abstract: Pretrained language models underpin several AI applications, but their high computational cost for training limits accessibility. Initiatives such as BLOOM and StarCoder aim to democratize access to pretrained models for collaborative community development. However, such existing models face challenges: limited multilingual capabilities, continual pretraining causing catastrophic forgetting, whereas pretraining from scratch is computationally expensive, and compliance with AI safety and development laws. This paper presents Aurora-M, a 15B parameter multilingual open-source model trained on English, Finnish, Hindi, Japanese, Vietnamese, and code. Continually pretrained from StarCoderPlus on 435 billion additional tokens, Aurora-M surpasses 2 trillion tokens in total training token count. It is the first open-source multilingual model fine-tuned on human-reviewed safety instructions, thus aligning its development not only with conventio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#23376;&#22270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38754;&#21521;&#23545;&#35937;&#30340;&#21463;&#32422;&#26463;&#24067;&#23616;&#29983;&#25104;&#65292;&#21487;&#20197;&#20934;&#30830;&#25429;&#25417;&#22797;&#26434;&#20132;&#20114;&#20851;&#31995;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2404.00385</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#23376;&#22270;&#30340;&#21463;&#32422;&#26463;&#24067;&#23616;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Constrained Layout Generation with Factor Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#23376;&#22270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38754;&#21521;&#23545;&#35937;&#30340;&#21463;&#32422;&#26463;&#24067;&#23616;&#29983;&#25104;&#65292;&#21487;&#20197;&#20934;&#30830;&#25429;&#25417;&#22797;&#26434;&#20132;&#20114;&#20851;&#31995;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#21253;&#25324;&#24179;&#38754;&#35774;&#35745;&#36807;&#31243;&#22312;&#20869;&#30340;&#22810;&#20010;&#39046;&#22495;&#20013;&#20986;&#29616;&#30340;&#38754;&#21521;&#23545;&#35937;&#30340;&#21463;&#32422;&#26463;&#24067;&#23616;&#29983;&#25104;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#20316;&#21697;&#36890;&#24120;&#23558;&#23545;&#35937;&#34920;&#31034;&#20026;&#21333;&#20010;&#33410;&#28857;&#65292;&#32570;&#20047;&#32454;&#31890;&#24230;&#26469;&#20934;&#30830;&#24314;&#27169;&#23545;&#35937;&#20043;&#38388;&#30340;&#22797;&#26434;&#20132;&#20114;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#23376;&#22270;&#30340;&#26041;&#27861;&#65292;&#23545;&#27599;&#20010;&#25151;&#38388;&#24341;&#20837;&#22235;&#20010;&#28508;&#21464;&#37327;&#33410;&#28857;&#21644;&#27599;&#20010;&#32422;&#26463;&#24341;&#20837;&#19968;&#20010;&#22240;&#23376;&#33410;&#28857;&#12290;&#22240;&#23376;&#33410;&#28857;&#34920;&#31034;&#19982;&#20854;&#36830;&#25509;&#30340;&#21464;&#37327;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#26377;&#25928;&#25429;&#25417;&#21487;&#33021;&#26159;&#39640;&#38454;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00385v1 Announce Type: cross  Abstract: This paper addresses the challenge of object-centric layout generation under spatial constraints, seen in multiple domains including floorplan design process. The design process typically involves specifying a set of spatial constraints that include object attributes like size and inter-object relations such as relative positioning. Existing works, which typically represent objects as single nodes, lack the granularity to accurately model complex interactions between objects. For instance, often only certain parts of an object, like a room's right wall, interact with adjacent objects. To address this gap, we introduce a factor graph based approach with four latent variable nodes for each room, and a factor node for each constraint. The factor nodes represent dependencies among the variables to which they are connected, effectively capturing constraints that are potentially of a higher order. We then develop message-passing on the bipar
&lt;/p&gt;</description></item><item><title>SpikingJET&#26159;&#19968;&#31181;&#35774;&#35745;&#29992;&#20110;&#20840;&#36830;&#25509;&#21644;&#21367;&#31215;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#22411;&#25925;&#38556;&#27880;&#20837;&#22120;&#65292;&#36890;&#36807;&#22312;&#20851;&#38190;&#32452;&#20214;&#20013;&#24341;&#20837;&#38169;&#35823;&#21644;&#27880;&#20837;&#25925;&#38556;&#65292;&#20026;&#35780;&#20272;SNNs&#30340;&#38887;&#24615;&#25552;&#20379;&#20102;&#20840;&#38754;&#24179;&#21488;&#12290;</title><link>https://arxiv.org/abs/2404.00383</link><description>&lt;p&gt;
SpikingJET&#65306;&#22686;&#24378;&#23436;&#20840;&#21644;&#21367;&#31215;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#25925;&#38556;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
SpikingJET: Enhancing Fault Injection for Fully and Convolutional Spiking Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00383
&lt;/p&gt;
&lt;p&gt;
SpikingJET&#26159;&#19968;&#31181;&#35774;&#35745;&#29992;&#20110;&#20840;&#36830;&#25509;&#21644;&#21367;&#31215;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#22411;&#25925;&#38556;&#27880;&#20837;&#22120;&#65292;&#36890;&#36807;&#22312;&#20851;&#38190;&#32452;&#20214;&#20013;&#24341;&#20837;&#38169;&#35823;&#21644;&#27880;&#20837;&#25925;&#38556;&#65292;&#20026;&#35780;&#20272;SNNs&#30340;&#38887;&#24615;&#25552;&#20379;&#20102;&#20840;&#38754;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#36234;&#26469;&#36234;&#22810;&#22320;&#38598;&#25104;&#21040;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#12289;&#21307;&#23398;&#35786;&#26029;&#35774;&#22791;&#21644;&#24037;&#19994;&#33258;&#21160;&#21270;&#31561;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20013;&#65292;&#30830;&#20445;&#23427;&#20204;&#22312;&#38754;&#23545;&#38543;&#26426;&#30828;&#20214;&#25925;&#38556;&#26102;&#30340;&#21487;&#38752;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SpikingJET&#65292;&#19968;&#31181;&#19987;&#20026;&#20840;&#36830;&#25509;&#21644;&#21367;&#31215;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#35774;&#35745;&#30340;&#26032;&#22411;&#25925;&#38556;&#27880;&#20837;&#22120;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#35780;&#20272;SNNs&#23545;&#30828;&#20214;&#25925;&#38556;&#30340;&#38887;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#32771;&#34385;&#21040;&#23427;&#20204;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#26085;&#30410;&#31361;&#20986;&#12290;SpikingJET&#36890;&#36807;&#21521;&#20851;&#38190;&#32452;&#20214;&#65288;&#22914;&#31361;&#35302;&#26435;&#37325;&#12289;&#31070;&#32463;&#20803;&#27169;&#22411;&#21442;&#25968;&#12289;&#20869;&#37096;&#29366;&#24577;&#21644;&#28608;&#27963;&#20989;&#25968;&#65289;&#24341;&#20837;&#38169;&#35823;&#21644;&#27880;&#20837;&#25925;&#38556;&#65292;&#20026;&#35780;&#20272;SNNs&#30340;&#38887;&#24615;&#25552;&#20379;&#20102;&#20840;&#38754;&#24179;&#21488;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#21508;&#31181;SNN&#26550;&#26500;&#19978;&#36827;&#34892;&#24191;&#27867;&#30340;&#36719;&#20214;&#32423;&#23454;&#39564;&#23637;&#31034;&#20102;Spiking-JET&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00383v1 Announce Type: cross  Abstract: As artificial neural networks become increasingly integrated into safety-critical systems such as autonomous vehicles, devices for medical diagnosis, and industrial automation, ensuring their reliability in the face of random hardware faults becomes paramount. This paper introduces SpikingJET, a novel fault injector designed specifically for fully connected and convolutional Spiking Neural Networks (SNNs). Our work underscores the critical need to evaluate the resilience of SNNs to hardware faults, considering their growing prominence in real-world applications. SpikingJET provides a comprehensive platform for assessing the resilience of SNNs by inducing errors and injecting faults into critical components such as synaptic weights, neuron model parameters, internal states, and activation functions. This paper demonstrates the effectiveness of Spiking-JET through extensive software-level experiments on various SNN architectures, reveali
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;Holonic&#25511;&#21046;&#26550;&#26500;&#23454;&#29616;&#24037;&#20154;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#21327;&#20316;&#19982;&#38598;&#25104;&#65292;&#20197;&#23454;&#29616;&#26032;&#30340;&#26234;&#33021;&#21046;&#36896;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2404.00369</link><description>&lt;p&gt;
&#36890;&#36807;Holonic&#25511;&#21046;&#26550;&#26500;&#23454;&#29616;&#24037;&#20154;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#21327;&#20316;&#19982;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Worker Robot Cooperation and Integration into the Manufacturing Workcell via the Holonic Control Architecture
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;Holonic&#25511;&#21046;&#26550;&#26500;&#23454;&#29616;&#24037;&#20154;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#21327;&#20316;&#19982;&#38598;&#25104;&#65292;&#20197;&#23454;&#29616;&#26032;&#30340;&#26234;&#33021;&#21046;&#36896;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20154;&#26426;&#22120;&#20154;&#21512;&#20316;&#26159;&#19968;&#31181;&#26032;&#30340;&#24037;&#19994;&#36235;&#21183;&#65292;&#26088;&#22312;&#32508;&#21512;&#20154;&#31867;&#21644;&#24037;&#19994;&#26426;&#22120;&#20154;&#30340;&#20248;&#21183;&#65292;&#20197;&#25552;&#20379;&#26032;&#30340;&#26234;&#33021;&#21046;&#36896;&#25216;&#26415;&#12290;&#24037;&#20154;&#21644;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#21512;&#20316;&#21046;&#36896;&#21253;&#21547;&#20135;&#21697;&#38646;&#37096;&#20214;&#21644;&#21046;&#36896;&#24037;&#20855;&#31561;&#20854;&#20182;&#20803;&#32032;&#12290;&#25152;&#26377;&#36825;&#20123;&#29983;&#20135;&#20803;&#32032;&#24517;&#39035;&#22312;&#19968;&#20010;&#21046;&#36896;&#24037;&#20316;&#21333;&#20803;&#20013;&#21512;&#20316;&#65292;&#20197;&#28385;&#36275;&#29983;&#20135;&#35201;&#27714;&#12290;&#21046;&#36896;&#25511;&#21046;&#31995;&#32479;&#26159;&#36830;&#25509;&#25152;&#26377;&#36825;&#20123;&#21512;&#20316;&#20803;&#32032;&#30340;&#25163;&#27573;&#12290;&#30001;&#20110;&#21512;&#20316;&#24037;&#20316;&#21333;&#20803;&#30340;&#24615;&#36136;&#65292;&#36825;&#31181;&#21046;&#36896;&#25511;&#21046;&#31995;&#32479;&#26159;&#20998;&#24067;&#24335;&#21644;&#33258;&#20027;&#30340;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Holonic&#25511;&#21046;&#26550;&#26500;&#20316;&#20026;&#21512;&#20316;&#24037;&#20316;&#21333;&#20803;&#30340;&#21046;&#36896;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#20851;&#27880;&#35813;&#21046;&#36896;&#27010;&#24565;&#30340;&#21487;&#34892;&#24615;&#65292;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#28041;&#21450;&#21452;&#33218;&#21327;&#20316;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00369v1 Announce Type: cross  Abstract: Worker-Robot Cooperation is a new industrial trend, which aims to sum the advantages of both the human and the industrial robot to afford a new intelligent manufacturing techniques. The cooperative manufacturing between the worker and the robot contains other elements such as the product parts and the manufacturing tools. All these production elements must cooperate in one manufacturing workcell to fulfill the production requirements. The manufacturing control system is the mean to connect all these cooperative elements together in one body. This manufacturing control system is distributed and autonomous due to the nature of the cooperative workcell. Accordingly, this article proposes the holonic control architecture as the manufacturing concept of the cooperative workcell. Furthermore, the article focuses on the feasibility of this manufacturing concept, by applying it over a case study that involves the cooperation between a dual-arm
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010; Fcaf3d-lychee &#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#19982;&#25380;&#21387;-&#28608;&#21457;&#65288;SE&#65289;&#27169;&#22359;&#25913;&#36827;&#29305;&#24449;&#25552;&#21462;&#65292;&#23454;&#29616;&#20102;&#23545;&#33620;&#26525;&#37319;&#25688;&#28857;&#30340;&#20934;&#30830;&#23450;&#20301;</title><link>https://arxiv.org/abs/2404.00364</link><description>&lt;p&gt;
&#36890;&#36807;&#20960;&#20309;&#24863;&#30693;&#23398;&#20064;&#23454;&#29616;&#26426;&#22120;&#20154;&#33620;&#26525;&#37319;&#25688;&#20934;&#30830;&#30340;&#20999;&#21106;&#28857;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Accurate Cutting-point Estimation for Robotic Lychee Harvesting through Geometry-aware Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00364
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010; Fcaf3d-lychee &#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#19982;&#25380;&#21387;-&#28608;&#21457;&#65288;SE&#65289;&#27169;&#22359;&#25913;&#36827;&#29305;&#24449;&#25552;&#21462;&#65292;&#23454;&#29616;&#20102;&#23545;&#33620;&#26525;&#37319;&#25688;&#28857;&#30340;&#20934;&#30830;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#35782;&#21035;&#32467;&#26500;&#19981;&#35268;&#21017;&#30340;&#26524;&#22253;&#29615;&#22659;&#20013;&#30340;&#33620;&#26525;&#37319;&#25688;&#28857;&#65292;&#24182;&#33719;&#21462;&#23427;&#20204;&#30340;&#22352;&#26631;&#20301;&#32622;&#23545;&#20110;&#33620;&#26525;&#37319;&#25688;&#26426;&#22120;&#20154;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#20108;&#32500;&#65288;2D&#65289;&#22270;&#20687;&#30340;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#36890;&#24120;&#22240;&#20026;&#26641;&#26525;&#12289;&#21494;&#23376;&#21644;&#26524;&#23454;&#30340;&#22797;&#26434;&#20960;&#20309;&#32467;&#26500;&#32780;&#38590;&#20197;&#24212;&#23545;&#65292;&#23548;&#33268;&#23545;&#33620;&#26525;&#37319;&#25688;&#28857;&#30340;&#38169;&#35823;&#30830;&#23450;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#20934;&#30830;&#23450;&#20301;&#33620;&#26525;&#37319;&#25688;&#28857;&#30340; Fcaf3d-lychee &#32593;&#32476;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#24494;&#36719;&#30340; Azure Kinect DK &#39134;&#34892;&#26102;&#38388;&#65288;TOF&#65289;&#30456;&#26426;&#36890;&#36807;&#22810;&#35270;&#22270;&#25340;&#25509;&#33719;&#21462;&#33620;&#26525;&#37319;&#25688;&#28857;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#30340;&#28857;&#20113;&#25968;&#25454;&#12290;&#25105;&#20204;&#23558;&#23436;&#20840;&#21367;&#31215;&#26080;&#38170;&#28857; 3D &#30446;&#26631;&#26816;&#27979;&#65288;Fcaf3d&#65289;&#27169;&#22411;&#19982;&#19968;&#20010;&#25380;&#21387;-&#28608;&#21457;&#65288;SE&#65289;&#27169;&#22359;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#20154;&#31867;&#35270;&#35273;&#27880;&#24847;&#26426;&#21046;&#26469;&#25913;&#36827;&#33620;&#26525;&#37319;&#25688;&#28857;&#30340;&#29305;&#24449;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00364v1 Announce Type: cross  Abstract: Accurately identifying lychee-picking points in unstructured orchard environments and obtaining their coordinate locations is critical to the success of lychee-picking robots. However, traditional two-dimensional (2D) image-based object detection methods often struggle due to the complex geometric structures of branches, leaves and fruits, leading to incorrect determination of lychee picking points. In this study, we propose a Fcaf3d-lychee network model specifically designed for the accurate localisation of lychee picking points. Point cloud data of lychee picking points in natural environments are acquired using Microsoft's Azure Kinect DK time-of-flight (TOF) camera through multi-view stitching. We augment the Fully Convolutional Anchor-Free 3D Object Detection (Fcaf3d) model with a squeeze-and-excitation(SE) module, which exploits human visual attention mechanisms for improved feature extraction of lychee picking points. The traine
&lt;/p&gt;</description></item><item><title>LLMs&#22312;&#25968;&#23398;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#20854;&#20013;GPT-4&#22312;Math Stack Exchange&#19978;&#22238;&#31572;&#25968;&#23398;&#38382;&#39064;&#30340;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>https://arxiv.org/abs/2404.00344</link><description>&lt;p&gt;
LLMs&#33021;&#22815;&#25484;&#25569;&#25968;&#23398;&#21527;&#65311;&#22312;&#25968;&#23398;&#22534;&#26632;&#20132;&#25442;&#19978;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Master Math? Investigating Large Language Models on Math Stack Exchange
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00344
&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#25968;&#23398;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#20854;&#20013;GPT-4&#22312;Math Stack Exchange&#19978;&#22238;&#31572;&#25968;&#23398;&#38382;&#39064;&#30340;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#24322;&#24120;&#33021;&#21147;&#65292;&#36890;&#24120;&#34920;&#29616;&#20986;&#36229;&#36234;&#20154;&#31867;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#25968;&#23398;&#39046;&#22495;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#20854;&#19987;&#38376;&#30340;&#32467;&#26500;&#21644;&#25152;&#38656;&#30340;&#31934;&#24230;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#20102;&#20004;&#27493;&#26041;&#27861;&#26469;&#35843;&#26597;LLMs&#22312;&#22238;&#31572;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37319;&#29992;&#22312;&#25968;&#23398;&#38382;&#39064;-&#31572;&#26696;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#26368;&#20339;&#30340;LLMs&#26469;&#22238;&#31572;Math Stack Exchange&#65288;MSE&#65289;&#20013;&#30340;78&#20010;&#38382;&#39064;&#12290;&#20854;&#27425;&#65292;&#23545;&#34920;&#29616;&#26368;&#20339;&#30340;LLM&#36827;&#34892;&#26696;&#20363;&#20998;&#26512;&#65292;&#37325;&#28857;&#20851;&#27880;&#20854;&#31572;&#26696;&#30340;&#36136;&#37327;&#21644;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#20026;&#22238;&#31572;&#25968;&#23398;&#38382;&#39064;&#36827;&#34892;&#24494;&#35843;&#30340;&#29616;&#26377;LLMs&#20013;&#65292;GPT-4&#34920;&#29616;&#26368;&#20339;&#65288;nDCG&#20026;0.48&#65292;P@10&#20026;0.37&#65289;&#65292;&#22312;&#25968;&#23398;&#38382;&#39064;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00344v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated exceptional capabilities in various natural language tasks, often achieving performances that surpass those of humans. Despite these advancements, the domain of mathematics presents a distinctive challenge, primarily due to its specialized structure and the precision it demands. In this study, we adopted a two-step approach for investigating the proficiency of LLMs in answering mathematical questions. First, we employ the most effective LLMs, as identified by their performance on math question-answer benchmarks, to generate answers to 78 questions from the Math Stack Exchange (MSE). Second, a case analysis is conducted on the LLM that showed the highest performance, focusing on the quality and accuracy of its answers through manual evaluation. We found that GPT-4 performs best (nDCG of 0.48 and P@10 of 0.37) amongst existing LLMs fine-tuned for answering mathematics questions and outperfor
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26412;&#20307;&#35770;&#27010;&#24565;&#34920;&#31034;&#21512;&#20316;&#21046;&#36896;&#30693;&#35782;&#30340;&#25972;&#20307;&#25511;&#21046;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#22312;&#21512;&#20316;&#35013;&#37197;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#23454;&#26045;&#12290;</title><link>https://arxiv.org/abs/2404.00341</link><description>&lt;p&gt;
&#20849;&#21516;&#21046;&#36896;&#20013;&#30340;&#26412;&#20307;&#35770;&#65306;&#20998;&#20139;&#21644;&#20132;&#25442;&#30693;&#35782;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Ontology in Holonic Cooperative Manufacturing: A Solution to Share and Exchange the Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00341
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26412;&#20307;&#35770;&#27010;&#24565;&#34920;&#31034;&#21512;&#20316;&#21046;&#36896;&#30693;&#35782;&#30340;&#25972;&#20307;&#25511;&#21046;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#22312;&#21512;&#20316;&#35013;&#37197;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#23454;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#20316;&#21046;&#36896;&#26159;&#24037;&#19994;&#30340;&#19968;&#20010;&#26032;&#36235;&#21183;&#65292;&#23427;&#20381;&#36182;&#20110;&#21327;&#20316;&#26426;&#22120;&#20154;&#30340;&#23384;&#22312;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26412;&#20307;&#35770;&#27010;&#24565;&#34920;&#31034;&#21512;&#20316;&#21046;&#36896;&#30693;&#35782;&#30340;&#25972;&#20307;&#25511;&#21046;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20010;&#25972;&#20307;&#25511;&#21046;&#35299;&#20915;&#26041;&#26696;&#34987;&#23454;&#26045;&#20026;&#19968;&#20010;&#33258;&#20027;&#22810;Agent&#31995;&#32479;&#65292;&#22522;&#20110;&#26412;&#20307;&#27169;&#22411;&#20132;&#25442;&#21046;&#36896;&#30693;&#35782;&#12290;&#26368;&#32456;&#65292;&#30740;&#31350;&#23545;&#19968;&#20010;&#28041;&#21450;&#20004;&#21517;&#24037;&#20154;&#21644;&#19968;&#20010;&#21327;&#20316;&#24037;&#20154;&#30340;&#21512;&#20316;&#35013;&#37197;&#22330;&#26223;&#36827;&#34892;&#20102;&#35828;&#26126;&#21644;&#23454;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00341v1 Announce Type: new  Abstract: Cooperative manufacturing is a new trend in industry, which depends on the existence of a collaborative robot. A collaborative robot is usually a light-weight robot which is capable of operating safely with a human co-worker in a shared work environment. During this cooperation, a vast amount of information is exchanged between the collaborative robot and the worker. This information constructs the cooperative manufacturing knowledge, which describes the production components and environment. In this research, we propose a holonic control solution, which uses the ontology concept to represent the cooperative manufacturing knowledge. The holonic control solution is implemented as an autonomous multi-agent system that exchanges the manufacturing knowledge based on an ontology model. Ultimately, the research illustrates and implements the proposed solution over a cooperative assembly scenario, which involves two workers and one collaborativ
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23384;&#20648;&#21487;&#25193;&#23637;&#19988;&#39640;&#25928;&#30340;&#21151;&#33021;&#22320;&#22270;&#23398;&#20064;&#31649;&#36947;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#23558;&#36880;&#28857;&#22320;&#22270;&#23384;&#20648;&#22312;&#20869;&#23384;&#20013;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#30456;&#21516;&#32467;&#26524;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#19968;&#31181;&#19981;&#21516;iable&#30340;&#22320;&#22270;&#32454;&#21270;&#23618;&#12290;</title><link>https://arxiv.org/abs/2404.00330</link><description>&lt;p&gt;
&#23384;&#20648;&#21487;&#25193;&#23637;&#19988;&#31616;&#21270;&#30340;&#21151;&#33021;&#22320;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Memory-Scalable and Simplified Functional Map Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00330
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23384;&#20648;&#21487;&#25193;&#23637;&#19988;&#39640;&#25928;&#30340;&#21151;&#33021;&#22320;&#22270;&#23398;&#20064;&#31649;&#36947;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#23558;&#36880;&#28857;&#22320;&#22270;&#23384;&#20648;&#22312;&#20869;&#23384;&#20013;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#30456;&#21516;&#32467;&#26524;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#19968;&#31181;&#19981;&#21516;iable&#30340;&#22320;&#22270;&#32454;&#21270;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#21151;&#33021;&#22320;&#22270;&#20316;&#20026;&#38750;&#21018;&#24615;&#24418;&#29366;&#21305;&#37197;&#38382;&#39064;&#30340;&#26480;&#20986;&#22522;&#20110;&#23398;&#20064;&#30340;&#26694;&#26550;&#24050;&#32463;&#20986;&#29616;&#12290;&#26089;&#26399;&#22312;&#35813;&#39046;&#22495;&#30340;&#26041;&#27861;&#20165;&#20851;&#27880;&#20110;&#22312;&#21151;&#33021;&#39046;&#22495;&#20013;&#30340;&#23398;&#20064;&#65292;&#32780;&#26368;&#26032;&#30340;&#25216;&#26415;&#24050;&#32463;&#35777;&#26126;&#65292;&#36890;&#36807;&#20419;&#36827;&#21151;&#33021;&#22320;&#22270;&#21644;&#36880;&#28857;&#22320;&#22270;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#26041;&#27861;&#36807;&#20998;&#20381;&#36182;&#20110;&#36719;&#36880;&#28857;&#22320;&#22270;&#20135;&#29983;&#30340;&#22823;&#22411;&#23494;&#38598;&#30697;&#38453;&#30340;&#35745;&#31639;&#65292;&#36825;&#25439;&#23475;&#20102;&#23427;&#20204;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23384;&#20648;&#21487;&#25193;&#23637;&#19988;&#39640;&#25928;&#30340;&#21151;&#33021;&#22320;&#22270;&#23398;&#20064;&#31649;&#36947;&#12290;&#36890;&#36807;&#21033;&#29992;&#21151;&#33021;&#22320;&#22270;&#30340;&#20855;&#20307;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#19981;&#23558;&#36880;&#28857;&#22320;&#22270;&#23384;&#20648;&#22312;&#20869;&#23384;&#20013;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#30456;&#21516;&#32467;&#26524;&#30340;&#21487;&#33021;&#24615;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#30456;&#21516;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#21487;&#24494;&#20998;&#30340;&#22320;&#22270;&#32454;&#21270;&#23618;&#65292;&#25913;&#32534;&#33258;&#29616;&#26377;&#30340;&#20844;&#29702;&#24615;ref
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00330v1 Announce Type: cross  Abstract: Deep functional maps have emerged in recent years as a prominent learning-based framework for non-rigid shape matching problems. While early methods in this domain only focused on learning in the functional domain, the latest techniques have demonstrated that by promoting consistency between functional and pointwise maps leads to significant improvements in accuracy. Unfortunately, existing approaches rely heavily on the computation of large dense matrices arising from soft pointwise maps, which compromises their efficiency and scalability. To address this limitation, we introduce a novel memory-scalable and efficient functional map learning pipeline. By leveraging the specific structure of functional maps, we offer the possibility to achieve identical results without ever storing the pointwise map in memory. Furthermore, based on the same approach, we present a differentiable map refinement layer adapted from an existing axiomatic ref
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#32479;&#35745;&#30456;&#20851;&#24615;&#21644;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#25913;&#21892;&#20102;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#22312;&#30140;&#30171;&#35782;&#21035;&#20013;&#30340;&#24615;&#33021;&#65292;&#31361;&#20986;&#20102;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#23450;&#21046;&#21270;&#30340;&#27169;&#24577;&#20998;&#21106;&#23545;&#30140;&#30171;&#34892;&#20026;&#20998;&#26512;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00320</link><description>&lt;p&gt;
&#22312;&#30140;&#30171;&#35782;&#21035;&#20013;&#25512;&#36827;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#65306;&#21033;&#29992;&#32479;&#35745;&#30456;&#20851;&#24615;&#21644;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Advancing Multimodal Data Fusion in Pain Recognition: A Strategy Leveraging Statistical Correlation and Human-Centered Perspectives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00320
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#32479;&#35745;&#30456;&#20851;&#24615;&#21644;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#25913;&#21892;&#20102;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#22312;&#30140;&#30171;&#35782;&#21035;&#20013;&#30340;&#24615;&#33021;&#65292;&#31361;&#20986;&#20102;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#23450;&#21046;&#21270;&#30340;&#27169;&#24577;&#20998;&#21106;&#23545;&#30140;&#30171;&#34892;&#20026;&#20998;&#26512;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#33268;&#21147;&#20110;&#35299;&#20915;&#22312;&#30140;&#30171;&#35782;&#21035;&#39046;&#22495;&#20869;&#25972;&#21512;&#24322;&#26500;&#25968;&#25454;&#20197;&#36827;&#34892;&#29305;&#23450;&#34892;&#20026;&#35782;&#21035;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#32479;&#35745;&#30456;&#20851;&#24615;&#19982;&#20197;&#20154;&#20026;&#20013;&#24515;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#21508;&#31181;&#21508;&#26679;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#25105;&#20204;&#31361;&#20986;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25552;&#21319;&#27169;&#22411;&#22312;&#21508;&#31181;&#22797;&#26434;&#22330;&#26223;&#20013;&#34920;&#29616;&#30340;&#36866;&#24212;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26032;&#39062;&#20043;&#22788;&#22312;&#20110;&#25112;&#30053;&#24615;&#22320;&#32467;&#21512;&#20102;&#32479;&#35745;&#30456;&#20851;&#24615;&#26435;&#37325;&#65292;&#24182;&#20174;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35282;&#24230;&#23545;&#27169;&#24577;&#36827;&#34892;&#20102;&#20998;&#21106;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#31934;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#21487;&#35299;&#37322;&#24615;&#30340;&#20998;&#26512;&#12290;&#35813;&#30740;&#31350;&#36890;&#36807;&#24378;&#35843;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#23450;&#21046;&#21270;&#30340;&#27169;&#24577;&#20998;&#21106;&#22312;&#22686;&#24378;&#30140;&#30171;&#34892;&#20026;&#20998;&#26512;&#20013;&#30340;&#20316;&#29992;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#27169;&#24577;&#34701;&#21512;&#25216;&#26415;&#12290;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#27599;&#31181;&#27169;&#24577;&#19982;&#36866;&#21512;&#30340;&#20998;&#31867;&#22120;&#36827;&#34892;&#21305;&#37197;&#65292;&#22522;&#20110;&#32479;&#35745;&#30456;&#20851;&#24615;&#65292;&#22686;&#21152;&#20102;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#35299;&#37322;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00320v1 Announce Type: new  Abstract: This research tackles the challenge of integrating heterogeneous data for specific behavior recognition within the domain of Pain Recognition, presenting a novel methodology that harmonizes statistical correlations with a human-centered approach. By leveraging a diverse range of deep learning architectures, we highlight the adaptability and efficacy of our approach in improving model performance across various complex scenarios. The novelty of our methodology is the strategic incorporation of statistical relevance weights and the segmentation of modalities from a human-centric perspective, enhancing model precision and providing a explainable analysis of multimodal data. This study surpasses traditional modality fusion techniques by underscoring the role of data diversity and customized modality segmentation in enhancing pain behavior analysis. Introducing a framework that matches each modality with an suited classifier, based on the sta
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#27010;&#29575;&#27169;&#22411;&#38598;&#25104;&#26694;&#26550;&#65292;&#33021;&#26377;&#25928;&#25972;&#21512;&#19981;&#21516;&#20110;CLIP&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#20302;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#20998;&#26512;&#25512;&#26029;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;</title><link>https://arxiv.org/abs/2404.00312</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#25506;&#32034;&#39044;&#35757;&#32451;&#27169;&#22411;&#29992;&#20110;&#20302;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Bayesian Exploration of Pre-trained Models for Low-shot Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00312
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#27010;&#29575;&#27169;&#22411;&#38598;&#25104;&#26694;&#26550;&#65292;&#33021;&#26377;&#25928;&#25972;&#21512;&#19981;&#21516;&#20110;CLIP&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#20302;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#20998;&#26512;&#25512;&#26029;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#19968;&#39033;&#22522;&#30784;&#20219;&#21153;&#65292;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#30340;&#20986;&#29616;&#26497;&#22823;&#25512;&#36827;&#20102;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#21069;&#27839;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#22522;&#20110;CLIP&#30340;&#26041;&#27861;&#32570;&#20047;&#26377;&#25928;&#25972;&#21512;&#20854;&#20182;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#65292;&#36825;&#20123;&#27169;&#22411;&#21253;&#21547;&#19981;&#21516;&#20110;CLIP&#30340;&#30693;&#35782;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#31616;&#21333;&#39640;&#25928;&#30340;&#27010;&#29575;&#27169;&#22411;&#38598;&#25104;&#26694;&#26550;&#65292;&#39640;&#26031;&#36807;&#31243;&#22312;&#22788;&#29702;&#23569;&#37327;&#25968;&#25454;&#26102;&#24050;&#32463;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25928;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#22343;&#20540;&#20989;&#25968;&#25351;&#23450;&#20026;CLIP&#65292;&#23558;&#26680;&#20989;&#25968;&#25351;&#23450;&#20026;&#22522;&#20110;&#21508;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#26500;&#24314;&#30340;&#28145;&#23618;&#26680;&#65292;&#23454;&#29616;&#20102;&#20808;&#39564;&#30693;&#35782;&#30340;&#25972;&#21512;&#12290;&#36890;&#36807;&#30452;&#25509;&#22238;&#24402;&#20998;&#31867;&#26631;&#31614;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#23454;&#29616;&#20102;&#20998;&#26512;&#25512;&#26029;&#12289;&#30452;&#35266;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#21512;&#29702;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00312v1 Announce Type: cross  Abstract: Low-shot image classification is a fundamental task in computer vision, and the emergence of large-scale vision-language models such as CLIP has greatly advanced the forefront of research in this field. However, most existing CLIP-based methods lack the flexibility to effectively incorporate other pre-trained models that encompass knowledge distinct from CLIP. To bridge the gap, this work proposes a simple and effective probabilistic model ensemble framework based on Gaussian processes, which have previously demonstrated remarkable efficacy in processing small data. We achieve the integration of prior knowledge by specifying the mean function with CLIP and the kernel function with an ensemble of deep kernels built upon various pre-trained models. By regressing the classification label directly, our framework enables analytical inference, straightforward uncertainty quantification, and principled hyper-parameter tuning. Through extensiv
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26234;&#33021;&#25512;&#33616;&#31995;&#32479;&#25216;&#26415;&#30340;&#25968;&#25454;&#39537;&#21160;&#20379;&#24212;&#38142;&#32039;&#24613;&#21709;&#24212;&#26694;&#26550;&#65292;&#33021;&#22815;&#20316;&#20026;&#20379;&#24212;&#38142;&#20013;&#26029;&#30340;&#26377;&#25928;&#25514;&#26045;&#65292;&#24182;&#24110;&#21161;&#21442;&#19982;&#32773;&#22312;&#21361;&#26426;&#21457;&#29983;&#21518;&#33719;&#24471;&#26356;&#22909;&#30340;&#21453;&#24212;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2404.00306</link><description>&lt;p&gt;
&#21033;&#29992;&#26234;&#33021;&#25512;&#33616;&#31995;&#32479;&#20316;&#20026;&#31532;&#19968;&#27493;&#24377;&#24615;&#25514;&#26045; &#8212;&#8212; &#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20379;&#24212;&#38142;&#32039;&#24613;&#21709;&#24212;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Leveraging Intelligent Recommender system as a first step resilience measure -- A data-driven supply chain disruption response framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00306
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26234;&#33021;&#25512;&#33616;&#31995;&#32479;&#25216;&#26415;&#30340;&#25968;&#25454;&#39537;&#21160;&#20379;&#24212;&#38142;&#32039;&#24613;&#21709;&#24212;&#26694;&#26550;&#65292;&#33021;&#22815;&#20316;&#20026;&#20379;&#24212;&#38142;&#20013;&#26029;&#30340;&#26377;&#25928;&#25514;&#26045;&#65292;&#24182;&#24110;&#21161;&#21442;&#19982;&#32773;&#22312;&#21361;&#26426;&#21457;&#29983;&#21518;&#33719;&#24471;&#26356;&#22909;&#30340;&#21453;&#24212;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#25216;&#26415;&#22312;&#25552;&#39640;&#20379;&#24212;&#38142;&#24377;&#24615;&#26041;&#38754;&#30340;&#28508;&#22312;&#29992;&#36884;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#23588;&#20854;&#26159;&#22312;&#24037;&#19994;4.0&#21644;&#20840;&#29699;&#22823;&#27969;&#34892;&#30149;&#32972;&#26223;&#19979;&#12290;&#23613;&#31649;&#25512;&#33616;&#31995;&#32479; (RS) &#20316;&#20026;&#19968;&#31181;&#33021;&#22815;&#25552;&#21319;&#20379;&#24212;&#38142;&#24377;&#24615;&#30340;&#24037;&#20855;&#34987;&#24573;&#35270;&#65292;&#20294;&#20174;&#24212;&#21464;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;RS &#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#24037;&#20855;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26234;&#33021;&#25512;&#33616;&#31995;&#32479;&#25216;&#26415;&#30340;&#20840;&#26032;&#25968;&#25454;&#39537;&#21160;&#20379;&#24212;&#38142;&#32039;&#24613;&#21709;&#24212;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#23454;&#38469;&#26696;&#20363;&#39564;&#35777;&#20102;&#27010;&#24565;&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#20379;&#24212;&#38142;&#32039;&#24613;&#21709;&#24212;&#25514;&#26045;&#22312;&#31532;&#19968;&#38454;&#27573;&#24471;&#21040;&#23454;&#26045;&#65292;&#24182;&#24110;&#21161;&#20379;&#24212;&#38142;&#21442;&#19982;&#32773;&#22312;&#20379;&#24212;&#38142;&#20013;&#26029;&#20043;&#21518;&#33719;&#24471;&#26356;&#22909;&#30340;&#21453;&#24212;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00306v1 Announce Type: cross  Abstract: Interests in the value of digital technologies for its potential uses to increase supply chain resilience (SCRes) are increasing in light to the industry 4.0 and the global pandemic. Utilization of Recommender systems (RS) as a supply chain (SC) resilience measure is neglected although RS is a capable tool to enhance SC resilience from a reactive aspect. To address this problem, this research proposed a novel data-driven supply chain disruption response framework based on the intelligent recommender system techniques and validated the conceptual model through a practical use case. Results show that our framework can be implemented as an effective SC disruption mitigation measure in the very first response phrase and help SC participants get better reaction performance after the SC disruption.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26657;&#20934;&#21644;&#33976;&#39311;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#20840;&#31934;&#24230;&#27169;&#22411;&#20316;&#20026;&#25945;&#24072;&#65292;&#22312;&#23398;&#20064;&#38271;&#23614;&#25968;&#25454;&#38598;&#19978;&#30340;&#20108;&#20540;&#32593;&#32476;&#26102;&#36827;&#34892;&#33976;&#39311;&#65292;&#36890;&#36807;&#23545;&#30446;&#26631;&#20989;&#25968;&#20013;&#39033;&#30340;&#25932;&#23545;&#24179;&#34913;&#21644;&#22810;&#20998;&#36776;&#29575;&#23398;&#20064;&#65292;&#26174;&#33879;&#20248;&#20110;&#20197;&#24448;&#25216;&#26415;</title><link>https://arxiv.org/abs/2404.00285</link><description>&lt;p&gt;
&#36890;&#36807;&#26657;&#20934;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#20108;&#20540;&#32593;&#32476;&#19978;&#36827;&#34892;&#38271;&#23614;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Long-Tailed Recognition on Binary Networks by Calibrating A Pre-trained Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00285
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26657;&#20934;&#21644;&#33976;&#39311;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#20840;&#31934;&#24230;&#27169;&#22411;&#20316;&#20026;&#25945;&#24072;&#65292;&#22312;&#23398;&#20064;&#38271;&#23614;&#25968;&#25454;&#38598;&#19978;&#30340;&#20108;&#20540;&#32593;&#32476;&#26102;&#36827;&#34892;&#33976;&#39311;&#65292;&#36890;&#36807;&#23545;&#30446;&#26631;&#20989;&#25968;&#20013;&#39033;&#30340;&#25932;&#23545;&#24179;&#34913;&#21644;&#22810;&#20998;&#36776;&#29575;&#23398;&#20064;&#65292;&#26174;&#33879;&#20248;&#20110;&#20197;&#24448;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#26223;&#20013;&#37096;&#32626;&#28145;&#24230;&#27169;&#22411;&#38754;&#20020;&#22810;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#35745;&#31639;&#25928;&#29575;&#21644;&#30495;&#23454;&#19990;&#30028;&#65288;&#22914;&#38271;&#23614;&#65289;&#25968;&#25454;&#20998;&#24067;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#23398;&#20064;&#38271;&#23614;&#20998;&#24067;&#65292;&#21033;&#29992;&#39640;&#24230;&#36164;&#28304;&#26377;&#25928;&#30340;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#39592;&#24178;&#32593;&#30340;&#32452;&#21512;&#25361;&#25112;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26657;&#20934;&#21644;&#33976;&#39311;&#26694;&#26550;&#65292;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#30340;&#20840;&#31934;&#24230;&#27169;&#22411;&#20316;&#20026;&#25945;&#24072;&#65292;&#22312;&#23398;&#20064;&#38271;&#23614;&#25968;&#25454;&#38598;&#19978;&#30340;&#20108;&#20540;&#32593;&#32476;&#26102;&#36827;&#34892;&#33976;&#39311;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#27867;&#21270;&#21040;&#21508;&#31181;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#20013;&#21508;&#39033;&#20043;&#38388;&#30340;&#25932;&#23545;&#24179;&#34913;&#21644;&#39640;&#25928;&#30340;&#22810;&#20998;&#36776;&#29575;&#23398;&#20064;&#26041;&#26696;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#25991;&#29486;&#20013;&#35268;&#27169;&#26368;&#22823;&#30340;&#32463;&#39564;&#30740;&#31350;&#65292;&#20351;&#29992;&#20102;15&#20010;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20174;&#29616;&#26377;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#26032;&#23548;&#20986;&#30340;&#38271;&#23614;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22823;&#22823;&#20248;&#20110;&#20808;&#21069;&#30340;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00285v1 Announce Type: cross  Abstract: Deploying deep models in real-world scenarios entails a number of challenges, including computational efficiency and real-world (e.g., long-tailed) data distributions. We address the combined challenge of learning long-tailed distributions using highly resource-efficient binary neural networks as backbones. Specifically, we propose a calibrate-and-distill framework that uses off-the-shelf pretrained full-precision models trained on balanced datasets to use as teachers for distillation when learning binary networks on long-tailed datasets. To better generalize to various datasets, we further propose a novel adversarial balancing among the terms in the objective function and an efficient multiresolution learning scheme. We conducted the largest empirical study in the literature using 15 datasets, including newly derived long-tailed datasets from existing balanced datasets, and show that our proposed method outperforms prior art by large 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#20998;&#31867;&#21644;&#35282;&#33394;&#20998;&#26512;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2404.00282</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#30340;&#35843;&#26597;:&#27010;&#24565;&#12289;&#20998;&#31867;&#21644;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00282
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#20998;&#31867;&#21644;&#35282;&#33394;&#20998;&#26512;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25317;&#26377;&#24191;&#27867;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#21644;&#39640;&#32423;&#36890;&#29992;&#33021;&#21147;&#65292;&#23427;&#20204;&#22312;&#22686;&#24378;&#23398;&#20064;&#26041;&#38754;&#22914;&#22810;&#20219;&#21153;&#23398;&#20064;&#12289;&#26679;&#26412;&#25928;&#29575;&#21644;&#20219;&#21153;&#35268;&#21010;&#31561;&#26041;&#38754;&#23637;&#29616;&#20986;&#28508;&#21147;&#12290;&#26412;&#35843;&#26597;&#32508;&#36848;&#20102;&#29616;&#26377;$\textit{LLM&#22686;&#24378;RL}$&#25991;&#29486;&#65292;&#24635;&#32467;&#20102;&#20854;&#19982;&#20256;&#32479;RL&#26041;&#27861;&#30340;&#29305;&#24449;&#65292;&#26088;&#22312;&#28548;&#28165;&#30740;&#31350;&#33539;&#22260;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#21033;&#29992;&#32463;&#20856;&#30340;Agent-&#29615;&#22659;&#20132;&#20114;&#33539;&#20363;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#20998;&#31867;&#27861;&#65292;&#31995;&#32479;&#22320;&#23558;LLMs&#22312;RL&#20013;&#30340;&#21151;&#33021;&#20998;&#31867;&#65292;&#21253;&#25324;&#22235;&#31181;&#35282;&#33394;&#65306;&#20449;&#24687;&#22788;&#29702;&#22120;&#12289;&#22870;&#21169;&#35774;&#35745;&#32773;&#12289;&#20915;&#31574;&#32773;&#21644;&#29983;&#25104;&#22120;&#12290;&#27492;&#22806;&#65292;&#38024;&#23545;&#27599;&#20010;&#35282;&#33394;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#26041;&#27861;&#35770;&#65292;&#20998;&#26512;&#20102;&#32531;&#35299;&#30340;&#29305;&#23450;RL&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#26041;&#21521;&#30340;&#35265;&#35299;&#12290;&#26368;&#21518;&#65292;&#28508;&#22312;&#24212;&#29992;&#12289;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00282v1 Announce Type: cross  Abstract: With extensive pre-trained knowledge and high-level general capabilities, large language models (LLMs) emerge as a promising avenue to augment reinforcement learning (RL) in aspects such as multi-task learning, sample efficiency, and task planning. In this survey, we provide a comprehensive review of the existing literature in $\textit{LLM-enhanced RL}$ and summarize its characteristics compared to conventional RL methods, aiming to clarify the research scope and directions for future studies. Utilizing the classical agent-environment interaction paradigm, we propose a structured taxonomy to systematically categorize LLMs' functionalities in RL, including four roles: information processor, reward designer, decision-maker, and generator. Additionally, for each role, we summarize the methodologies, analyze the specific RL challenges that are mitigated, and provide insights into future directions. Lastly, potential applications, prospecti
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#24320;&#21457;&#25351;&#20196;&#39537;&#21160;&#28216;&#25103;&#24341;&#25806;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21019;&#24314;&#28216;&#25103;&#65292;&#20174;&#32780;&#38477;&#20302;&#28216;&#25103;&#24320;&#21457;&#30340;&#38590;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.00276</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#25351;&#20196;&#39537;&#21160;&#28216;&#25103;&#24341;&#25806;
&lt;/p&gt;
&lt;p&gt;
Instruction-Driven Game Engines on Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00276
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#24320;&#21457;&#25351;&#20196;&#39537;&#21160;&#28216;&#25103;&#24341;&#25806;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21019;&#24314;&#28216;&#25103;&#65292;&#20174;&#32780;&#38477;&#20302;&#28216;&#25103;&#24320;&#21457;&#30340;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Instruction-Driven Game Engine (IDGE) &#39033;&#30446;&#26088;&#22312;&#36890;&#36807;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36981;&#24490;&#33258;&#30001;&#24418;&#24335;&#30340;&#28216;&#25103;&#35268;&#21017;&#24182;&#33258;&#21160;&#29983;&#25104;&#28216;&#25103;&#36807;&#31243;&#26469;&#20351;&#28216;&#25103;&#24320;&#21457;&#27665;&#20027;&#21270;&#12290;IDGE&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#21457;&#20986;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26469;&#21019;&#24314;&#28216;&#25103;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#20102;&#28216;&#25103;&#24320;&#21457;&#30340;&#38556;&#30861;&#12290;&#25105;&#20204;&#23558;IDGE&#30340;&#23398;&#20064;&#36807;&#31243;&#35270;&#20026;&#19979;&#19968;&#20010;&#29366;&#24577;&#39044;&#27979;&#20219;&#21153;&#65292;&#27169;&#22411;&#33258;&#22238;&#24402;&#22320;&#39044;&#27979;&#29609;&#23478;&#34892;&#21160;&#32473;&#20986;&#30340;&#28216;&#25103;&#29366;&#24577;&#12290;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#28216;&#25103;&#29366;&#24577;&#30340;&#35745;&#31639;&#24517;&#39035;&#20934;&#30830;&#65307;&#21542;&#21017;&#65292;&#36731;&#24494;&#30340;&#38169;&#35823;&#21487;&#33021;&#20250;&#30772;&#22351;&#28216;&#25103;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20197;&#35838;&#31243;&#26041;&#24335;&#35757;&#32451;IDGE&#65292;&#36880;&#28176;&#22686;&#21152;&#27169;&#22411;&#23545;&#22797;&#26434;&#22330;&#26223;&#30340;&#25509;&#35302;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00276v1 Announce Type: new  Abstract: The Instruction-Driven Game Engine (IDGE) project aims to democratize game development by enabling a large language model (LLM) to follow free-form game rules and autonomously generate game-play processes. The IDGE allows users to create games by issuing simple natural language instructions, which significantly lowers the barrier for game development. We approach the learning process for IDGEs as a Next State Prediction task, wherein the model autoregressively predicts in-game states given player actions. It is a challenging task because the computation of in-game states must be precise; otherwise, slight errors could disrupt the game-play. To address this, we train the IDGE in a curriculum manner that progressively increases the model's exposure to complex scenarios.   Our initial progress lies in developing an IDGE for Poker, a universally cherished card game. The engine we've designed not only supports a wide range of poker variants b
&lt;/p&gt;</description></item><item><title>TG-NAS&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#27169;&#22411;&#36890;&#29992;&#20195;&#29702;&#65292;&#21033;&#29992;Transformer&#30340;&#36816;&#31639;&#31526;&#23884;&#20837;&#29983;&#25104;&#22120;&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#26469;&#39044;&#27979;&#26550;&#26500;&#24615;&#33021;&#65292;&#25351;&#23548;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#12290;</title><link>https://arxiv.org/abs/2404.00271</link><description>&lt;p&gt;
TG-NAS&#65306;&#21033;&#29992;Transformer&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#19982;&#38646;&#25104;&#26412;&#20195;&#29702;&#36827;&#34892;&#39640;&#25928;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
TG-NAS: Leveraging Zero-Cost Proxies with Transformer and Graph Convolution Networks for Efficient Neural Architecture Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00271
&lt;/p&gt;
&lt;p&gt;
TG-NAS&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#27169;&#22411;&#36890;&#29992;&#20195;&#29702;&#65292;&#21033;&#29992;Transformer&#30340;&#36816;&#31639;&#31526;&#23884;&#20837;&#29983;&#25104;&#22120;&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#26469;&#39044;&#27979;&#26550;&#26500;&#24615;&#33021;&#65292;&#25351;&#23548;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;(NAS)&#26159;&#19968;&#31181;&#21457;&#29616;&#26032;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#26550;&#26500;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#32791;&#26102;&#30340;&#35757;&#32451;&#25110;&#23494;&#38598;&#30340;&#37319;&#26679;&#21644;&#35780;&#20272;&#12290;&#38646;&#25104;&#26412;NAS&#26088;&#22312;&#20026;&#26550;&#26500;&#24615;&#33021;&#39044;&#27979;&#21019;&#24314;&#20813;&#35757;&#32451;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#20195;&#29702;&#24615;&#33021;&#20122;&#20248;&#65292;&#24182;&#19988;&#24120;&#24120;&#34987;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#25110;&#28014;&#28857;&#36816;&#31639;&#27425;&#25968;&#31561;&#31616;&#21333;&#25351;&#26631;&#25152;&#36229;&#36234;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#22522;&#20110;&#27169;&#22411;&#30340;&#20195;&#29702;&#26080;&#27861;&#23558;&#27867;&#21270;&#21040;&#26032;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#20854;&#20013;&#20855;&#26377;&#26410;&#35265;&#26032;&#31867;&#22411;&#36816;&#31639;&#31526;&#19988;&#19981;&#24102;&#26377;&#40644;&#37329;&#20934;&#30830;&#24230;&#12290;&#19968;&#20010;&#26222;&#36941;&#26368;&#20248;&#30340;&#20195;&#29702;&#20173;&#28982;&#38590;&#20197;&#25214;&#21040;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;TG-NAS&#65292;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;Transformer&#30340;&#36816;&#31639;&#31526;&#23884;&#20837;&#29983;&#25104;&#22120;&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;(GCN)&#26469;&#39044;&#27979;&#26550;&#26500;&#24615;&#33021;&#30340;&#26032;&#22411;&#27169;&#22411;&#36890;&#29992;&#20195;&#29702;&#12290;&#36825;&#31181;&#26041;&#27861;&#25351;&#23548;&#30528;&#22312;&#20219;&#20309;&#32473;&#23450;&#25628;&#32034;&#31354;&#38388;&#20869;&#36827;&#34892;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00271v1 Announce Type: cross  Abstract: Neural architecture search (NAS) is an effective method for discovering new convolutional neural network (CNN) architectures. However, existing approaches often require time-consuming training or intensive sampling and evaluations. Zero-shot NAS aims to create training-free proxies for architecture performance prediction. However, existing proxies have suboptimal performance, and are often outperformed by simple metrics such as model parameter counts or the number of floating-point operations. Besides, existing model-based proxies cannot be generalized to new search spaces with unseen new types of operators without golden accuracy truth. A universally optimal proxy remains elusive. We introduce TG-NAS, a novel model-based universal proxy that leverages a transformer-based operator embedding generator and a graph convolution network (GCN) to predict architecture performance. This approach guides neural architecture search across any giv
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22810;&#26679;&#21270;&#31867;&#21035;&#24863;&#30693;&#20851;&#27880;SBRS(DCA-SBRS)&#26694;&#26550;&#65292;&#21487;&#20197;&#20316;&#20026;&#25554;&#20214;&#29992;&#20110;&#36741;&#21161;&#29616;&#26377;&#30340;SBRSs&#22312;&#20445;&#25345;&#25512;&#33616;&#20934;&#30830;&#24230;&#30340;&#21516;&#26102;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#30340;&#21015;&#34920;</title><link>https://arxiv.org/abs/2404.00261</link><description>&lt;p&gt;
&#19968;&#31181;&#31616;&#21333;&#32780;&#21448;&#26377;&#25928;&#30340;&#22810;&#26679;&#21270;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Simple Yet Effective Approach for Diversified Session-Based Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00261
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22810;&#26679;&#21270;&#31867;&#21035;&#24863;&#30693;&#20851;&#27880;SBRS(DCA-SBRS)&#26694;&#26550;&#65292;&#21487;&#20197;&#20316;&#20026;&#25554;&#20214;&#29992;&#20110;&#36741;&#21161;&#29616;&#26377;&#30340;SBRSs&#22312;&#20445;&#25345;&#25512;&#33616;&#20934;&#30830;&#24230;&#30340;&#21516;&#26102;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#30340;&#21015;&#34920;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#22411;&#25512;&#33616;&#31995;&#32479;(SBRSs)&#22312;&#25429;&#25417;&#30701;&#26399;&#21644;&#21160;&#24577;&#29992;&#25143;&#20559;&#22909;&#30340;&#26680;&#24515;&#33021;&#21147;&#26041;&#38754;&#21464;&#24471;&#26497;&#20026;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;SBRSs&#20027;&#35201;&#22312;&#20110;&#26368;&#22823;&#21270;&#25512;&#33616;&#20934;&#30830;&#24230;&#65292;&#20294;&#24573;&#30053;&#20102;&#29992;&#25143;&#30340;&#27425;&#35201;&#20559;&#22909;&#65292;&#20174;&#32780;&#26368;&#32456;&#23548;&#33268;&#38271;&#26399;&#30340;&#31579;&#27873;&#12290;&#21482;&#26377;&#23569;&#25968;&#20960;&#39033;&#24037;&#20316;&#33268;&#21147;&#20110;&#25552;&#39640;&#22810;&#26679;&#24615;&#65292;&#20381;&#36182;&#20110;&#29420;&#29305;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;&#26657;&#20934;&#25439;&#22833;&#20989;&#25968;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#36731;&#26131;&#22320;&#36866;&#24212;&#29616;&#26377;&#30340;&#38754;&#21521;&#20934;&#30830;&#24230;&#30340;SBRSs&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#19968;&#31181;&#26082;&#31616;&#21333;&#21448;&#26377;&#25928;&#30340;&#35774;&#35745;&#26159;&#20540;&#24471;&#30340;&#65292;&#23427;&#21487;&#20197;&#20316;&#20026;&#25554;&#20214;&#29992;&#20110;&#36741;&#21161;&#29616;&#26377;&#30340;SBRSs&#22312;&#20445;&#25345;&#25512;&#33616;&#20934;&#30830;&#24230;&#30340;&#21516;&#26102;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#30340;&#21015;&#34920;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#27599;&#19968;&#20010;&#29616;&#26377;&#30340;&#20195;&#34920;&#24615;(&#38754;&#21521;&#20934;&#30830;&#24230;)SBRS&#65292;&#31216;&#20026;&#22810;&#26679;&#21270;&#31867;&#21035;&#24863;&#30693;&#20851;&#27880;SBRS(DCA-SBRS)&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#22810;&#26679;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00261v1 Announce Type: cross  Abstract: Session-based recommender systems (SBRSs) have become extremely popular in view of the core capability of capturing short-term and dynamic user preferences. However, most SBRSs primarily maximize recommendation accuracy but ignore user minor preferences, thus leading to filter bubbles in the long run. Only a handful of works, being devoted to improving diversity, depend on unique model designs and calibrated loss functions, which cannot be easily adapted to existing accuracy-oriented SBRSs. It is thus worthwhile to come up with a simple yet effective design that can be used as a plugin to facilitate existing SBRSs on generating a more diversified list in the meantime preserving the recommendation accuracy. In this case, we propose an end-to-end framework applied for every existing representative (accuracy-oriented) SBRS, called diversified category-aware attentive SBRS (DCA-SBRS), to boost the performance on recommendation diversity. I
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;YOLO&#26550;&#26500;&#30340;YOLOOC&#26816;&#27979;&#22120;&#65292;&#38024;&#23545;&#24320;&#25918;&#31867;&#21035;&#35774;&#32622;&#24341;&#20837;&#20102;&#26631;&#31614;&#24179;&#28369;&#65292;&#26377;&#25928;&#24212;&#23545;&#26032;&#31867;&#21035;&#26816;&#27979;&#21644;&#22686;&#37327;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2404.00257</link><description>&lt;p&gt;
YOLOOC: &#22522;&#20110;YOLO&#30340;&#24320;&#25918;&#31867;&#21035;&#22686;&#37327;&#30446;&#26631;&#26816;&#27979;&#19982;&#26032;&#31867;&#21035;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
YOLOOC: YOLO-based Open-Class Incremental Object Detection with Novel Class Discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00257
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;YOLO&#26550;&#26500;&#30340;YOLOOC&#26816;&#27979;&#22120;&#65292;&#38024;&#23545;&#24320;&#25918;&#31867;&#21035;&#35774;&#32622;&#24341;&#20837;&#20102;&#26631;&#31614;&#24179;&#28369;&#65292;&#26377;&#25928;&#24212;&#23545;&#26032;&#31867;&#21035;&#26816;&#27979;&#21644;&#22686;&#37327;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#22312;&#23454;&#36341;&#20013;&#30340;&#36816;&#29992;&#65292;&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#26816;&#27979;&#65288;OWOD&#65289;&#21463;&#21040;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#25361;&#25112;&#22312;&#20110;&#27169;&#22411;&#22914;&#20309;&#26816;&#27979;&#26032;&#31867;&#21035;&#65292;&#28982;&#21518;&#22686;&#37327;&#23398;&#20064;&#23427;&#20204;&#32780;&#19981;&#20250;&#24536;&#35760;&#20808;&#21069;&#24050;&#30693;&#30340;&#31867;&#21035;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#24378;&#30417;&#30563;&#25110;&#24369;&#30417;&#30563;&#30340;&#26032;&#31867;&#21035;&#25968;&#25454;&#29992;&#20110;&#26032;&#31867;&#21035;&#26816;&#27979;&#65292;&#36825;&#21487;&#33021;&#19981;&#36866;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#20854;&#20013;&#26032;&#31867;&#21035;&#21482;&#22312;&#25512;&#26029;&#38454;&#27573;&#36935;&#21040;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;YOLO&#26550;&#26500;&#30340;&#26032;OWOD&#26816;&#27979;&#22120;YOLOOC&#65292;&#19987;&#38376;&#38024;&#23545;&#24320;&#25918;&#31867;&#21035;&#30340;&#35774;&#32622;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26631;&#31614;&#24179;&#28369;&#20197;&#38450;&#27490;&#26816;&#27979;&#22120;&#36807;&#20110;&#33258;&#20449;&#22320;&#23558;&#26032;&#31867;&#21035;&#26144;&#23556;&#21040;&#24050;&#30693;&#31867;&#21035;&#24182;&#21457;&#29616;&#26032;&#31867;&#21035;&#12290;&#22312;&#25105;&#20204;&#26356;&#21152;&#29616;&#23454;&#30340;&#35774;&#32622;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25105;&#20204;&#30340;&#26032;&#22522;&#20934;&#19979;&#21457;&#29616;&#26032;&#31867;&#21035;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00257v1 Announce Type: cross  Abstract: Because of its use in practice, open-world object detection (OWOD) has gotten a lot of attention recently. The challenge is how can a model detect novel classes and then incrementally learn them without forgetting previously known classes. Previous approaches hinge on strongly-supervised or weakly-supervised novel-class data for novel-class detection, which may not apply to real applications. We construct a new benchmark that novel classes are only encountered at the inference stage. And we propose a new OWOD detector YOLOOC, based on the YOLO architecture yet for the Open-Class setup. We introduce label smoothing to prevent the detector from over-confidently mapping novel classes to known classes and to discover novel classes. Extensive experiments conducted on our more realistic setup demonstrate the effectiveness of our method for discovering novel classes in our new benchmark.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#36801;&#31227;&#23398;&#20064;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#20854;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20026;&#36807;&#31243;&#25511;&#21046;&#24102;&#26469;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00247</link><description>&lt;p&gt;
&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#20419;&#36827;&#36807;&#31243;&#25511;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;&#65306;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
Facilitating Reinforcement Learning for Process Control Using Transfer Learning: Perspectives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#36801;&#31227;&#23398;&#20064;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#20854;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20026;&#36807;&#31243;&#25511;&#21046;&#24102;&#26469;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#36801;&#31227;&#23398;&#20064;&#30340;&#35282;&#24230;&#65292;&#20026;&#36807;&#31243;&#25511;&#21046;&#20013;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#25552;&#20379;&#20102;&#28145;&#20837;&#35265;&#35299;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#36807;&#31243;&#24037;&#19994;&#39046;&#22495;&#24212;&#29992;DRL&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#24341;&#20837;&#36801;&#31227;&#23398;&#20064;&#30340;&#24517;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#24314;&#35758;&#21644;&#23637;&#26395;&#65292;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#36801;&#31227;&#23398;&#20064;&#19982;DRL&#32467;&#21512;&#36215;&#26469;&#21152;&#24378;&#36807;&#31243;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00247v1 Announce Type: cross  Abstract: This paper provides insights into deep reinforcement learning (DRL) for process control from the perspective of transfer learning. We analyze the challenges of applying DRL in the field of process industries and the necessity of introducing transfer learning. Furthermore, recommendations and prospects are provided for future research directions on how transfer learning can be integrated with DRL to empower process control.
&lt;/p&gt;</description></item><item><title>&#22312;&#19968;&#20010;&#26041;&#22359;&#19990;&#30028;&#29615;&#22659;&#20013;&#65292;&#35770;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21327;&#20316;&#33021;&#21147;&#65292;&#36890;&#36807;&#35774;&#35745;&#19981;&#26029;&#22686;&#21152;&#25361;&#25112;&#24615;&#30340;&#35774;&#32622;&#26469;&#35780;&#20272;&#19981;&#21516;&#30340;&#21327;&#20316;&#35270;&#35282;&#65292;&#20174;&#29420;&#31435;&#21040;&#26356;&#22797;&#26434;&#30340;&#20381;&#36182;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2404.00246</link><description>&lt;p&gt;
&#20320;&#30340;&#21516;&#20107;&#24456;&#37325;&#35201;&#65306;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#26041;&#22359;&#19990;&#30028;&#20013;&#30340;&#21327;&#20316;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Your Co-Workers Matter: Evaluating Collaborative Capabilities of Language Models in Blocks World
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00246
&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#26041;&#22359;&#19990;&#30028;&#29615;&#22659;&#20013;&#65292;&#35770;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21327;&#20316;&#33021;&#21147;&#65292;&#36890;&#36807;&#35774;&#35745;&#19981;&#26029;&#22686;&#21152;&#25361;&#25112;&#24615;&#30340;&#35774;&#32622;&#26469;&#35780;&#20272;&#19981;&#21516;&#30340;&#21327;&#20316;&#35270;&#35282;&#65292;&#20174;&#29420;&#31435;&#21040;&#26356;&#22797;&#26434;&#30340;&#20381;&#36182;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#19990;&#30028;&#33258;&#34892;&#20132;&#20114;&#30340;&#35821;&#35328;&#20195;&#29702;&#22312;&#33258;&#21160;&#21270;&#25968;&#23383;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#22312;&#29702;&#35299;&#21644;&#25191;&#34892;&#25991;&#26412;&#28216;&#25103;&#21644;&#32593;&#39029;&#25511;&#21046;&#31561;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#35768;&#22810;&#29616;&#23454;&#20219;&#21153;&#20063;&#38656;&#35201;&#19982;&#20154;&#31867;&#25110;&#20854;&#20182;&#21516;&#31561;&#35282;&#33394;&#30340;LLM&#21327;&#20316;&#65292;&#36825;&#28041;&#21450;&#24847;&#22270;&#29702;&#35299;&#12289;&#20219;&#21153;&#21327;&#35843;&#21644;&#27807;&#36890;&#12290;&#20026;&#27979;&#35797;LLM&#21327;&#20316;&#33021;&#21147;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26041;&#22359;&#19990;&#30028;&#29615;&#22659;&#65292;&#22312;&#36825;&#20010;&#29615;&#22659;&#20013;&#65292;&#20004;&#20010;&#20195;&#29702;&#65292;&#27599;&#20010;&#20195;&#29702;&#37117;&#26377;&#29420;&#29305;&#30340;&#30446;&#26631;&#21644;&#25216;&#33021;&#65292;&#19968;&#36215;&#24314;&#36896;&#19968;&#20010;&#30446;&#26631;&#32467;&#26500;&#12290;&#20026;&#23454;&#29616;&#30446;&#26631;&#65292;&#20182;&#20204;&#21487;&#20197;&#22312;&#19990;&#30028;&#20013;&#34892;&#21160;&#24182;&#29992;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#27807;&#36890;&#12290;&#22312;&#36825;&#20010;&#29615;&#22659;&#19979;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35774;&#32622;&#65292;&#20197;&#35780;&#20272;&#19981;&#21516;&#21327;&#20316;&#35270;&#35282;&#65292;&#20174;&#29420;&#31435;&#30340;&#21040;&#26356;&#22797;&#26434;&#30340;&#20381;&#36182;&#20219;&#21153;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#37319;&#29992;&#20102;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#26469;&#24314;&#27169;&#21512;&#20316;&#20249;&#20276;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00246v1 Announce Type: cross  Abstract: Language agents that interact with the world on their own have great potential for automating digital tasks. While large language model (LLM) agents have made progress in understanding and executing tasks such as textual games and webpage control, many real-world tasks also require collaboration with humans or other LLMs in equal roles, which involves intent understanding, task coordination, and communication. To test LLM's ability to collaborate, we design a blocks-world environment, where two agents, each having unique goals and skills, build a target structure together. To complete the goals, they can act in the world and communicate in natural language. Under this environment, we design increasingly challenging settings to evaluate different collaboration perspectives, from independent to more complex, dependent tasks. We further adopt chain-of-thought prompts that include intermediate reasoning steps to model the partner's state a
&lt;/p&gt;</description></item><item><title>DeFT&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;IO&#24847;&#35782;&#30340;&#26641;&#27880;&#24847;&#21147;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;QKV&#20934;&#22791;&#21644;&#27880;&#24847;&#21147;&#35745;&#31639;&#38454;&#27573;&#23454;&#29616;&#20869;&#23384;&#39640;&#25928;&#30340;&#35745;&#31639;&#65292;&#38477;&#20302;&#20869;&#23384;&#21360;&#35760;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#26641;&#35299;&#30721;&#31574;&#30053;&#21644;&#25512;&#26029;&#31995;&#32479;&#19981;&#36866;&#37197;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.00242</link><description>&lt;p&gt;
DeFT&#65306;&#24102;IO&#24847;&#35782;&#30340;Flash Tree-attention&#29992;&#20110;&#39640;&#25928;&#30340;&#22522;&#20110;&#26641;&#25628;&#32034;&#30340;LLM&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based LLM Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00242
&lt;/p&gt;
&lt;p&gt;
DeFT&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;IO&#24847;&#35782;&#30340;&#26641;&#27880;&#24847;&#21147;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;QKV&#20934;&#22791;&#21644;&#27880;&#24847;&#21147;&#35745;&#31639;&#38454;&#27573;&#23454;&#29616;&#20869;&#23384;&#39640;&#25928;&#30340;&#35745;&#31639;&#65292;&#38477;&#20302;&#20869;&#23384;&#21360;&#35760;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#26641;&#35299;&#30721;&#31574;&#30053;&#21644;&#25512;&#26029;&#31995;&#32479;&#19981;&#36866;&#37197;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26641;&#25628;&#32034;&#36827;&#34892;&#35299;&#30721;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#26029;&#36136;&#37327;&#12290;&#26681;&#25454;&#24341;&#23548;&#20449;&#21495;&#65292;&#23427;&#36890;&#36807;&#24418;&#25104;LLM&#36755;&#20986;&#20174;&#26681;&#21040;&#21494;&#23376;&#30340;&#26368;&#20339;&#36335;&#24452;&#26469;&#25552;&#39640;&#21487;&#25511;&#24615;&#12289;&#25512;&#29702;&#33021;&#21147;&#12289;&#23545;&#40784;&#31561;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35745;&#31639;&#20887;&#20313;&#12289;&#20869;&#23384;&#21344;&#29992;&#21644;&#20869;&#23384;&#35775;&#38382;&#65292;&#24403;&#21069;&#30340;&#26641;&#35299;&#30721;&#31574;&#30053;&#21450;&#20854;&#25512;&#26029;&#31995;&#32479;&#20114;&#30456;&#19981;&#36866;&#37197;&#65292;&#23548;&#33268;&#25512;&#26029;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeFT&#65292;&#19968;&#31181;IO&#24863;&#30693;&#26641;&#27880;&#24847;&#21147;&#31639;&#27861;&#65292;&#23427;&#22312;&#20004;&#20010;&#38454;&#27573;&#20013;&#20445;&#25345;&#20869;&#23384;&#39640;&#25928;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#65292;&#38477;&#20302;&#20869;&#23384;&#21360;&#35760;&#65306;&#65288;1&#65289;QKV&#20934;&#22791;&#65306;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;KV&#24341;&#23548;&#26641;&#20998;&#35010;&#31574;&#30053;&#65292;&#20026;GPU&#30340;&#39640;&#21033;&#29992;&#29575;&#21644;&#23613;&#21487;&#33021;&#20943;&#23569;GPU&#20840;&#23616;&#20869;&#23384;&#21644;&#33455;&#29255;&#19978;&#20849;&#20139;&#20869;&#23384;&#20043;&#38388;&#30340;KV&#32531;&#23384;&#30340;&#20869;&#23384;&#35835;/&#20889;; &#65288;2&#65289;&#27880;&#24847;&#21147;&#35745;&#31639;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00242v1 Announce Type: cross  Abstract: Decoding using tree search can greatly enhance the inference quality for transformer-based Large Language Models (LLMs). Depending on the guidance signal, it searches for the best path from root to leaf in the tree by forming LLM outputs to improve controllability, reasoning ability, alignment, et cetera. However, current tree decoding strategies and their inference systems do not suit each other well due to redundancy in computation, memory footprints, and memory access, resulting in inefficient inference. To address this issue, we propose DeFT, an IO-aware tree attention algorithm that maintains memory-efficient attention calculation with low memory footprints in two stages: (1) QKV Preparation: we propose a KV-Guided Tree Split strategy to group QKV wisely for high utilization of GPUs and reduction of memory reads/writes for the KV cache between GPU global memory and on-chip shared memory as much as possible; (2) Attention Calculati
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;TransDeformer&#65292;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#20102;&#23545;&#33136;&#26894;&#36718;&#24275;&#30340;&#39640;&#31354;&#38388;&#20934;&#30830;&#24615;&#37325;&#24314;&#65292;&#24182;&#36328;&#24739;&#32773;&#23454;&#29616;&#20102;&#32593;&#26684;&#23545;&#24212;&#65292;&#20026;&#21307;&#23398;&#21442;&#25968;&#27979;&#37327;&#25552;&#20379;&#20102;&#21487;&#38752;&#24615;&#65292;&#36824;&#35774;&#35745;&#20102;&#21464;&#20307;&#29992;&#20110;&#38169;&#35823;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2404.00231</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#24418;&#29366;&#21464;&#24418;&#32593;&#32476;&#29992;&#20110;&#26080;&#20266;&#24433;&#20960;&#20309;&#37325;&#26500;&#39592;&#30406;&#33136;&#26894;MR&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Attention-based Shape-Deformation Networks for Artifact-Free Geometry Reconstruction of Lumbar Spine from MR Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00231
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;TransDeformer&#65292;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#20102;&#23545;&#33136;&#26894;&#36718;&#24275;&#30340;&#39640;&#31354;&#38388;&#20934;&#30830;&#24615;&#37325;&#24314;&#65292;&#24182;&#36328;&#24739;&#32773;&#23454;&#29616;&#20102;&#32593;&#26684;&#23545;&#24212;&#65292;&#20026;&#21307;&#23398;&#21442;&#25968;&#27979;&#37327;&#25552;&#20379;&#20102;&#21487;&#38752;&#24615;&#65292;&#36824;&#35774;&#35745;&#20102;&#21464;&#20307;&#29992;&#20110;&#38169;&#35823;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33136;&#26894;&#26894;&#38388;&#30424;&#36864;&#21464;&#65292;&#26159;&#33136;&#26894;&#38388;&#30424;&#28176;&#36827;&#24615;&#32467;&#26500;&#24615;&#30952;&#25439;&#65292;&#34987;&#35748;&#20026;&#22312;&#33136;&#37096;&#30140;&#30171;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20840;&#29699;&#20581;&#24247;&#20851;&#27880;&#28966;&#28857;&#12290;&#20174;MR&#22270;&#20687;&#20013;&#33258;&#21160;&#37325;&#24314;&#33136;&#26894;&#20960;&#20309;&#24418;&#29366;&#65292;&#23558;&#20351;&#21307;&#23398;&#21442;&#25968;&#30340;&#24555;&#36895;&#27979;&#37327;&#25104;&#20026;&#21487;&#33021;&#65292;&#20197;&#35780;&#20272;&#33136;&#26894;&#29366;&#24577;&#65292;&#20174;&#32780;&#30830;&#23450;&#21512;&#36866;&#30340;&#27835;&#30103;&#26041;&#26696;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#22270;&#20687;&#20998;&#21106;&#30340;&#25216;&#26415;&#36890;&#24120;&#20250;&#29983;&#25104;&#38169;&#35823;&#30340;&#20998;&#21106;&#25110;&#19981;&#36866;&#21512;&#21307;&#23398;&#21442;&#25968;&#27979;&#37327;&#30340;&#26080;&#32467;&#26500;&#28857;&#20113;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TransDeformer&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#39640;&#31354;&#38388;&#20934;&#30830;&#24230;&#21644;&#24739;&#32773;&#38388;&#32593;&#26684;&#23545;&#24212;&#30340;&#26041;&#24335;&#37325;&#24314;&#33136;&#26894;&#36718;&#24275;&#65292;&#24182;&#19988;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;TransDeformer&#30340;&#21464;&#31181;&#29992;&#20110;&#38169;&#35823;&#20272;&#35745;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26032;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#26032;&#30340;&#27880;&#24847;&#21147;&#20844;&#24335;&#65292;&#23558;&#22270;&#20687;&#29305;&#24449;&#21644;&#26631;&#35760;&#21270;&#30340;&#36718;&#24275;&#29305;&#24449;&#38598;&#25104;&#36215;&#26469;&#65292;&#29992;&#20110;&#39044;&#27979;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00231v1 Announce Type: cross  Abstract: Lumbar disc degeneration, a progressive structural wear and tear of lumbar intervertebral disc, is regarded as an essential role on low back pain, a significant global health concern. Automated lumbar spine geometry reconstruction from MR images will enable fast measurement of medical parameters to evaluate the lumbar status, in order to determine a suitable treatment. Existing image segmentation-based techniques often generate erroneous segments or unstructured point clouds, unsuitable for medical parameter measurement. In this work, we present TransDeformer: a novel attention-based deep learning approach that reconstructs the contours of the lumbar spine with high spatial accuracy and mesh correspondence across patients, and we also present a variant of TransDeformer for error estimation. Specially, we devise new attention modules with a new attention formula, which integrates image features and tokenized contour features to predict 
&lt;/p&gt;</description></item><item><title>InfLoRA&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#21517;&#20026;&#26080;&#24178;&#25200;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;InfLoRA&#65289;&#65292;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#65292;&#26088;&#22312;&#28040;&#38500;&#26032;&#20219;&#21153;&#23545;&#26087;&#20219;&#21153;&#30340;&#24178;&#25200;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2404.00228</link><description>&lt;p&gt;
InfLoRA&#65306;&#26080;&#24178;&#25200;&#30340;&#20302;&#31209;&#33258;&#36866;&#24212;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00228
&lt;/p&gt;
&lt;p&gt;
InfLoRA&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#21517;&#20026;&#26080;&#24178;&#25200;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;InfLoRA&#65289;&#65292;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#65292;&#26088;&#22312;&#28040;&#38500;&#26032;&#20219;&#21153;&#23545;&#26087;&#20219;&#21153;&#30340;&#24178;&#25200;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#35201;&#27714;&#27169;&#22411;&#20381;&#27425;&#23398;&#20064;&#22810;&#20010;&#20219;&#21153;&#12290;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#65292;&#27169;&#22411;&#24212;&#20855;&#22791;&#22312;&#26087;&#20219;&#21153;&#19978;&#32500;&#25345;&#24615;&#33021;&#65288;&#31283;&#23450;&#24615;&#65289;&#21644;&#19981;&#26029;&#36866;&#24212;&#26032;&#20219;&#21153;&#30340;&#33021;&#21147;&#65288;&#21487;&#22609;&#24615;&#65289;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#23613;&#31649;&#29616;&#26377;&#22522;&#20110;PEFT&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#20986;&#27604;&#38750;PEFT&#26041;&#27861;&#26356;&#20248;&#31168;&#30340;&#24615;&#33021;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#24182;&#26410;&#32771;&#34385;&#22914;&#20309;&#28040;&#38500;&#26032;&#20219;&#21153;&#23545;&#26087;&#20219;&#21153;&#30340;&#24178;&#25200;&#65292;&#20174;&#32780;&#38459;&#30861;&#27169;&#22411;&#22312;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#24179;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#31216;&#20026;&#26080;&#24178;&#25200;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;InfLoRA&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00228v1 Announce Type: cross  Abstract: Continual learning requires the model to learn multiple tasks sequentially. In continual learning, the model should possess the ability to maintain its performance on old tasks (stability) and the ability to adapt to new tasks continuously (plasticity). Recently, parameter-efficient fine-tuning (PEFT), which involves freezing a pre-trained model and injecting a small number of learnable parameters to adapt to downstream tasks, has gained increasing popularity in continual learning. Although existing continual learning methods based on PEFT have demonstrated superior performance compared to those not based on PEFT, most of them do not consider how to eliminate the interference of the new task on the old tasks, which inhibits the model from making a good trade-off between stability and plasticity. In this work, we propose a new PEFT method, called interference-free low-rank adaptation (InfLoRA), for continual learning. InfLoRA injects a 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#20107;&#23454;&#35299;&#30721;&#26041;&#27861;&#25552;&#39640;&#20102;&#20107;&#23454;&#20934;&#30830;&#24615;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20351;&#27169;&#22411;&#23545;&#24050;&#30693;&#20107;&#23454;&#36807;&#20110;&#33258;&#20449;&#65292;&#36827;&#19968;&#27493;&#35780;&#20272;&#26174;&#31034;&#22312;&#30693;&#35782;&#32534;&#36753;&#22522;&#20934;&#19978;&#25152;&#26377;&#35299;&#30721;&#26041;&#27861;&#22343;&#26174;&#33879;&#38477;&#20302;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00216</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#23454;&#35299;&#30721;&#65306;&#22312;&#30693;&#35782;&#32534;&#36753;&#22522;&#20934;&#19978;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Is Factuality Decoding a Free Lunch for LLMs? Evaluation on Knowledge Editing Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00216
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#20107;&#23454;&#35299;&#30721;&#26041;&#27861;&#25552;&#39640;&#20102;&#20107;&#23454;&#20934;&#30830;&#24615;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20351;&#27169;&#22411;&#23545;&#24050;&#30693;&#20107;&#23454;&#36807;&#20110;&#33258;&#20449;&#65292;&#36827;&#19968;&#27493;&#35780;&#20272;&#26174;&#31034;&#22312;&#30693;&#35782;&#32534;&#36753;&#22522;&#20934;&#19978;&#25152;&#26377;&#35299;&#30721;&#26041;&#27861;&#22343;&#26174;&#33879;&#38477;&#20302;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#23427;&#20204;&#33021;&#22815;&#20197;&#26356;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#26041;&#24335;&#20256;&#36798;&#20107;&#23454;&#30693;&#35782;&#12290;&#20154;&#20204;&#24050;&#32463;&#20570;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#26469;&#36890;&#36807;&#20462;&#25913;LLMs&#24182;&#38477;&#20302;&#20107;&#23454;&#24187;&#35273;&#26469;&#25552;&#39640;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20462;&#25913;&#20063;&#23384;&#22312;&#38459;&#30861;&#30693;&#35782;&#26356;&#26032;&#30340;&#39118;&#38505;&#65292;&#22240;&#20026;&#23427;&#20204;&#20351;&#27169;&#22411;&#23545;&#24050;&#30693;&#20107;&#23454;&#36807;&#20110;&#33258;&#20449;&#12290;&#26412;&#25991;&#39318;&#20808;&#37325;&#26032;&#23457;&#35270;&#24403;&#21069;&#30340;&#20107;&#23454;&#35299;&#30721;&#26041;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;&#23427;&#20204;&#22312;&#25552;&#39640;&#20107;&#23454;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;&#20960;&#31181;&#24378;&#22823;&#30340;&#20107;&#23454;&#35299;&#30721;&#26041;&#27861;&#22312;&#30693;&#35782;&#32534;&#36753;&#22522;&#20934;&#19978;&#36827;&#34892;&#36827;&#19968;&#27493;&#35780;&#20272;&#12290;&#25152;&#26377;&#36825;&#20123;&#35299;&#30721;&#26041;&#27861;&#19982;&#20854;&#21407;&#22987;&#35299;&#30721;&#30456;&#27604;&#22343;&#26174;&#30528;&#38477;&#20302;&#20102;llama2&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#26368;&#22823;&#30340;&#38477;&#20302;&#24133;&#24230;&#36798;&#21040;&#24778;&#20154;&#30340;81.3\%&#12290;&#36825;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;&#35299;&#30721;&#26041;&#27861;&#20173;&#26080;&#27861;&#23436;&#20840;&#35299;&#20915;&#20107;&#23454;&#24187;&#35273;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#24573;&#35270;&#20102;&#20808;&#39564;&#30693;&#35782;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00216v1 Announce Type: cross  Abstract: The rapid development of large language models (LLMs) enables them to convey factual knowledge in a more human-like fashion. Extensive efforts have been made to reduce factual hallucinations by modifying LLMs with factuality decoding. However, they also pose risks of hindering knowledge updates, as they make models overly confident in known facts. In this work, we first revisite the current factuality decoding methods and verified their effectiveness in enhancing factual accuracy. Subsequently, we conduct further evaluation of several strong factuality decoding methods on the knowledge editing benchmark. All these decoding methods significantly diminish the performance of llama2 models compared to their original decoding, with the largest decrease being a staggering 81.3\%. This further indicates that the current existing decoding methods still cannot perfectly address the factual hallucinations, as they overlook the importance of pres
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#20316;&#21160;&#24577;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22240;&#26524;&#20272;&#35745;Incremental Stylistic Effect&#26469;&#35299;&#37322;&#22914;&#20309;&#25913;&#21464;&#21512;&#20316;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.00207</link><description>&lt;p&gt;
&#20154;&#31867;-&#35821;&#35328;&#27169;&#22411;&#21327;&#20316;&#30340;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Causal Inference for Human-Language Model Collaboration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#20316;&#21160;&#24577;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22240;&#26524;&#20272;&#35745;Incremental Stylistic Effect&#26469;&#35299;&#37322;&#22914;&#20309;&#25913;&#21464;&#21512;&#20316;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20043;&#38388;&#30340;&#21327;&#20316;&#21160;&#24577;&#65292;&#36825;&#20123;&#20114;&#21160;&#36890;&#24120;&#28041;&#21450;LMs&#25552;&#20986;&#25991;&#26412;&#27573;&#33853;&#65292;&#32780;&#20154;&#31867;&#32534;&#36753;&#25110;&#22238;&#24212;&#36825;&#20123;&#24314;&#35758;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#19982;LMs&#36827;&#34892;&#26377;&#25928;&#30340;&#20114;&#21160;&#35201;&#27714;&#20154;&#31867;&#36776;&#21035;&#20986;&#26377;&#25928;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#20114;&#21160;&#31574;&#30053;&#65292;&#20363;&#22914;&#32534;&#36753;&#21644;&#22238;&#24212;&#26679;&#24335;&#65292;&#20174;&#21382;&#21490;&#20154;&#31867;-LM&#20114;&#21160;&#20013;&#12290;&#36825;&#20010;&#30446;&#26631;&#26412;&#36136;&#19978;&#26159;&#22240;&#26524;&#20851;&#31995;&#65292;&#21463;&#21040;&#21453;&#20107;&#23454;&#8220;&#22914;&#26524;&#8221;&#38382;&#39064;&#30340;&#39537;&#21160;:&#22914;&#26524;&#20154;&#31867;&#37319;&#29992;&#19981;&#21516;&#30340;&#25991;&#26412;&#32534;&#36753;/&#31934;&#28860;&#31574;&#30053;&#65292;&#21327;&#20316;&#30340;&#32467;&#26524;&#20250;&#22914;&#20309;&#25913;&#21464;&#65311;&#22238;&#31572;&#36825;&#20010;&#22240;&#26524;&#25512;&#26029;&#38382;&#39064;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#21046;&#23450;&#19968;&#20010;&#36866;&#24403;&#30340;&#22240;&#26524;&#20272;&#35745;:&#20256;&#32479;&#30340;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;ATE&#65289;&#20272;&#35745;&#30001;&#20110;&#25991;&#26412;&#30340;&#39640;&#32500;&#24230;&#32780;&#19981;&#36866;&#29992;&#20110;&#22522;&#20110;&#25991;&#26412;&#30340;&#22788;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22240;&#26524;&#20272;&#35745; - &#22686;&#37327;&#39118;&#26684;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00207v1 Announce Type: cross  Abstract: In this paper, we examine the collaborative dynamics between humans and language models (LMs), where the interactions typically involve LMs proposing text segments and humans editing or responding to these proposals. Productive engagement with LMs in such scenarios necessitates that humans discern effective text-based interaction strategies, such as editing and response styles, from historical human-LM interactions. This objective is inherently causal, driven by the counterfactual `what-if' question: how would the outcome of collaboration change if humans employed a different text editing/refinement strategy? A key challenge in answering this causal inference question is formulating an appropriate causal estimand: the conventional average treatment effect (ATE) estimand is inapplicable to text-based treatments due to their high dimensionality. To address this concern, we introduce a new causal estimand -- Incremental Stylistic Effect (
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026; $\mathrm{CAESAR}$ &#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#19968;&#20010;&#36817;&#20284;&#30340;&#26368;&#20248;&#31163;&#32447;&#37319;&#26679;&#20998;&#24067;&#65292;&#21516;&#26102;&#20272;&#35745;&#22810;&#20010;&#31574;&#30053;&#30340;&#20215;&#20540;&#65292;&#20197;&#35299;&#20915;&#22810;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.00195</link><description>&lt;p&gt;
&#36890;&#36807;&#23494;&#24230;&#20272;&#35745;&#36827;&#34892;&#22810;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Multiple-policy Evaluation via Density Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00195
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026; $\mathrm{CAESAR}$ &#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#19968;&#20010;&#36817;&#20284;&#30340;&#26368;&#20248;&#31163;&#32447;&#37319;&#26679;&#20998;&#24067;&#65292;&#21516;&#26102;&#20272;&#35745;&#22810;&#20010;&#31574;&#30053;&#30340;&#20215;&#20540;&#65292;&#20197;&#35299;&#20915;&#22810;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#22810;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#65292;&#32473;&#23450;&#19968;&#32452; $K$ &#20010;&#30446;&#26631;&#31574;&#30053;&#65292;&#30446;&#26631;&#26159;&#20197;&#33267;&#23569; $1-\delta$ &#30340;&#27010;&#29575;&#35780;&#20272;&#23427;&#20204;&#30340;&#24615;&#33021;&#65288;&#26399;&#26395;&#24635;&#22870;&#21169;&#65289;&#36798;&#21040;&#31934;&#24230; $\epsilon$&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; $\mathrm{CAESAR}$ &#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#35745;&#31639;&#19968;&#20010;&#36817;&#20284;&#30340;&#26368;&#20248;&#31163;&#32447;&#37319;&#26679;&#20998;&#24067;&#65292;&#24182;&#21033;&#29992;&#20174;&#20013;&#37319;&#26679;&#30340;&#25968;&#25454;&#26469;&#21516;&#26102;&#20272;&#35745;&#31574;&#30053;&#20215;&#20540;&#12290;$\mathrm{CAESAR}$ &#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#12290;&#22312;&#31532;&#19968;&#20010;&#38454;&#27573;&#65292;&#25105;&#20204;&#20197;&#38543;&#30528; $\tilde{O}(\frac{1}{\epsilon})$ &#32553;&#25918;&#30340;&#20302;&#35746;&#21333;&#37319;&#26679;&#22797;&#26434;&#24615;&#29575;&#20135;&#29983;&#30446;&#26631;&#31574;&#30053;&#30340;&#35775;&#38382;&#20998;&#24067;&#30340;&#31895;&#30053;&#20272;&#35745;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#36817;&#20284;&#26368;&#20248;&#31163;&#32447;&#37319;&#26679;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#19968;&#20010;&#36880;&#27493;&#20108;&#27425;&#25439;&#22833;&#20989;&#25968;&#26469;&#35745;&#31639;&#25152;&#26377;&#30446;&#26631;&#31574;&#30053;&#30340;&#37325;&#35201;&#24615;&#26435;&#37325;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00195v1 Announce Type: cross  Abstract: In this work, we focus on the multiple-policy evaluation problem where we are given a set of $K$ target policies and the goal is to evaluate their performance (the expected total rewards) to an accuracy $\epsilon$ with probability at least $1-\delta$. We propose an algorithm named $\mathrm{CAESAR}$ to address this problem. Our approach is based on computing an approximate optimal offline sampling distribution and using the data sampled from it to perform the simultaneous estimation of the policy values. $\mathrm{CAESAR}$ consists of two phases. In the first one we produce coarse estimates of the vistation distributions of the target policies at a low order sample complexity rate that scales with $\tilde{O}(\frac{1}{\epsilon})$. In the second phase, we approximate the optimal offline sampling distribution and compute the importance weighting ratios for all target policies by minimizing a step-wise quadratic loss function inspired by the
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102; OpenAI &#30340; GPT-3.5 &#27169;&#22411;&#20316;&#20026;&#8220;&#35821;&#35328;&#25968;&#25454;&#31185;&#23398;&#23478;&#8221;&#65292;&#25104;&#21151;&#22238;&#31572;&#20102;&#19982;&#22522;&#20934;&#25968;&#25454;&#38598;&#30456;&#20851;&#30340;&#25968;&#25454;&#31185;&#23398;&#26597;&#35810;&#12290;</title><link>https://arxiv.org/abs/2404.00188</link><description>&lt;p&gt;
DataAgent: &#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22238;&#31572;&#38646;&#26679;&#26412;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
DataAgent: Evaluating Large Language Models' Ability to Answer Zero-Shot, Natural Language Queries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00188
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102; OpenAI &#30340; GPT-3.5 &#27169;&#22411;&#20316;&#20026;&#8220;&#35821;&#35328;&#25968;&#25454;&#31185;&#23398;&#23478;&#8221;&#65292;&#25104;&#21151;&#22238;&#31572;&#20102;&#19982;&#22522;&#20934;&#25968;&#25454;&#38598;&#30456;&#20851;&#30340;&#25968;&#25454;&#31185;&#23398;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#25968;&#25454;&#38598;&#20998;&#26512;&#21644;&#25552;&#21462;&#26377;&#24847;&#20041;&#20449;&#24687;&#30340;&#36807;&#31243;&#24448;&#24448;&#32791;&#26102;&#19988;&#32321;&#29712;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#30830;&#23450;&#25163;&#21160;&#12289;&#37325;&#22797;&#24615;&#32534;&#30721;&#21644;&#25968;&#25454;&#25910;&#38598;&#26159;&#38459;&#30861;&#25968;&#25454;&#31185;&#23398;&#23478;&#20174;&#20107;&#26356;&#24494;&#22937;&#30340;&#24037;&#20316;&#21644;&#39640;&#27700;&#24179;&#39033;&#30446;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102; OpenAI &#30340; GPT-3.5 &#20316;&#20026;&#8220;&#35821;&#35328;&#25968;&#25454;&#31185;&#23398;&#23478;&#8221;&#65288;LDS&#65289;&#65292;&#21487;&#20197;&#20174;&#32473;&#23450;&#25968;&#25454;&#38598;&#20013;&#25512;&#23548;&#20986;&#20851;&#38190;&#21457;&#29616;&#65292;&#21253;&#25324;&#30456;&#20851;&#24615;&#21644;&#22522;&#26412;&#20449;&#24687;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#26631;&#20934;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#21253;&#25324;&#22522;&#20110;&#25968;&#25454;&#31185;&#23398;&#20195;&#30721;&#29983;&#25104;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;NumPy&#12289;Pandas&#12289;Scikit-Learn &#21644; TensorFlow &#31561;&#24211;&#65292;&#24182;&#22312;&#27491;&#30830;&#22238;&#31572;&#19982;&#22522;&#20934;&#25968;&#25454;&#38598;&#30456;&#20851;&#30340;&#32473;&#23450;&#25968;&#25454;&#31185;&#23398;&#26597;&#35810;&#26041;&#38754;&#21462;&#24471;&#20102;&#24191;&#27867;&#25104;&#21151;&#12290;LDS &#20351;&#29992;&#20102;&#21508;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#26469;&#26377;&#25928;&#22238;&#31572;&#32473;&#23450;&#38382;&#39064;&#65292;&#21253;&#25324;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00188v1 Announce Type: cross  Abstract: Conventional processes for analyzing datasets and extracting meaningful information are often time-consuming and laborious. Previous work has identified manual, repetitive coding and data collection as major obstacles that hinder data scientists from undertaking more nuanced labor and high-level projects. To combat this, we evaluated OpenAI's GPT-3.5 as a "Language Data Scientist" (LDS) that can extrapolate key findings, including correlations and basic information, from a given dataset. The model was tested on a diverse set of benchmark datasets to evaluate its performance across multiple standards, including data science code-generation based tasks involving libraries such as NumPy, Pandas, Scikit-Learn, and TensorFlow, and was broadly successful in correctly answering a given data science query related to the benchmark dataset. The LDS used various novel prompt engineering techniques to effectively answer a given question, including
&lt;/p&gt;</description></item><item><title>&#20027;&#21160;&#35270;&#35273;&#26426;&#21046;&#30340;&#38598;&#25104;&#21487;&#20197;&#25552;&#20379;&#24403;&#21069;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#20248;&#21183;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;GFNet&#21644;FALcon&#36825;&#20004;&#31181;&#20027;&#21160;&#35270;&#35273;&#26041;&#27861;&#22312;&#40657;&#30418;&#23041;&#32961;&#27169;&#22411;&#19979;&#30340;&#22266;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00185</link><description>&lt;p&gt;
&#35770;&#20027;&#21160;&#35270;&#35273;&#31995;&#32479;&#22266;&#26377;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
On Inherent Adversarial Robustness of Active Vision Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00185
&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#35270;&#35273;&#26426;&#21046;&#30340;&#38598;&#25104;&#21487;&#20197;&#25552;&#20379;&#24403;&#21069;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#20248;&#21183;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;GFNet&#21644;FALcon&#36825;&#20004;&#31181;&#20027;&#21160;&#35270;&#35273;&#26041;&#27861;&#22312;&#40657;&#30418;&#23041;&#32961;&#27169;&#22411;&#19979;&#30340;&#22266;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#26679;&#26412;&#30340;&#25915;&#20987;&#65292;&#36825;&#31181;&#23545;&#25239;&#26679;&#26412;&#36890;&#36807;&#28155;&#21152;&#31934;&#24515;&#35774;&#35745;&#30340;&#22122;&#22768;&#26469;&#25913;&#21464;&#23427;&#20204;&#30340;&#39044;&#27979;&#12290;&#30001;&#20110;&#20154;&#31867;&#30524;&#30555;&#23545;&#36825;&#31181;&#36755;&#20837;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#21487;&#33021;&#30340;&#24369;&#28857;&#28304;&#20110;&#29992;&#30456;&#21516;&#37325;&#35201;&#24615;&#22788;&#29702;&#27599;&#20010;&#20687;&#32032;&#30340;&#26631;&#20934;&#26041;&#24335;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#31070;&#32463;&#31185;&#23398;&#34920;&#26126;&#20154;&#31867;&#35270;&#35273;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#65288;1&#65289;&#20999;&#25442;&#22810;&#20010;&#27880;&#35270;&#28857;&#65288;&#25195;&#35270;&#65289;&#21644;&#65288;2&#65289;&#20197;&#38750;&#22343;&#21248;&#22806;&#37096;&#20998;&#36776;&#29575;&#65288;&#35270;&#31070;&#32463;&#65289;&#22788;&#29702;&#21608;&#22260;&#20449;&#24687;&#26469;&#21306;&#20998;&#26174;&#33879;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#25552;&#20513;&#23558;&#36825;&#31181;&#20027;&#21160;&#35270;&#35273;&#26426;&#21046;&#34701;&#20837;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#65292;&#20197;&#25552;&#20379;&#40065;&#26834;&#24615;&#20248;&#21183;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#40657;&#30418;&#23041;&#32961;&#27169;&#22411;&#19979;&#65292;&#32463;&#39564;&#24615;&#22320;&#23637;&#31034;&#20102;&#20004;&#31181;&#20027;&#21160;&#35270;&#35273;&#26041;&#27861;GFNet&#21644;FALcon&#30340;&#22266;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00185v1 Announce Type: cross  Abstract: Current Deep Neural Networks are vulnerable to adversarial examples, which alter their predictions by adding carefully crafted noise. Since human eyes are robust to such inputs, it is possible that the vulnerability stems from the standard way of processing inputs in one shot by processing every pixel with the same importance. In contrast, neuroscience suggests that the human vision system can differentiate salient features by (1) switching between multiple fixation points (saccades) and (2) processing the surrounding with a non-uniform external resolution (foveation). In this work, we advocate that the integration of such active vision mechanisms into current deep learning systems can offer robustness benefits. Specifically, we empirically demonstrate the inherent robustness of two active vision methods - GFNet and FALcon - under a black box threat model. By learning and inferencing based on downsampled glimpses obtained from multiple
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#27979;&#24615;&#21644;&#22788;&#26041;&#24615;&#20998;&#26512;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#65292;&#20026;&#32467;&#26463;&#29616;&#26377;&#20307;&#32946;&#36187;&#23395;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#20197;&#20135;&#29983;&#31867;&#20284;&#23436;&#25972;&#36187;&#23395;&#32467;&#26524;&#30340;&#38431;&#20237;&#25490;&#21517;&#12290;</title><link>https://arxiv.org/abs/2404.00178</link><description>&lt;p&gt;
&#36229;&#36234;&#20572;&#36187;&#65306;&#32467;&#26463;&#20307;&#32946;&#32852;&#30431;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#35770;
&lt;/p&gt;
&lt;p&gt;
Beyond Suspension: A Two-phase Methodology for Concluding Sports Leagues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00178
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#27979;&#24615;&#21644;&#22788;&#26041;&#24615;&#20998;&#26512;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#65292;&#20026;&#32467;&#26463;&#29616;&#26377;&#20307;&#32946;&#36187;&#23395;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#20197;&#20135;&#29983;&#31867;&#20284;&#23436;&#25972;&#36187;&#23395;&#32467;&#26524;&#30340;&#38431;&#20237;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#23450;&#20041;&#65306;&#19987;&#19994;&#20307;&#32946;&#32852;&#30431;&#21487;&#33021;&#22240;&#21508;&#31181;&#21407;&#22240;&#26242;&#20572;&#65292;&#27604;&#22914;&#26368;&#36817;&#30340;COVID-19&#22823;&#27969;&#34892;&#12290;&#37325;&#35201;&#38382;&#39064;&#26159;&#32852;&#30431;&#22312;&#37325;&#26032;&#24320;&#36187;&#26102;&#22914;&#20309;&#36866;&#24403;&#22320;&#36873;&#25321;&#20854;&#20313;&#27604;&#36187;&#30340;&#23376;&#38598;&#65292;&#20197;&#22312;&#32553;&#30701;&#30340;&#26102;&#38388;&#20869;&#32467;&#26463;&#36187;&#23395;&#12290;&#23398;&#26415;/&#23454;&#36341;&#24847;&#20041;&#65306;&#23613;&#31649;&#26377;&#20016;&#23500;&#30340;&#25991;&#29486;&#20851;&#20110;&#20174;&#38646;&#24320;&#22987;&#23433;&#25490;&#25972;&#20010;&#36187;&#23395;&#65292;&#32467;&#26463;&#29616;&#26377;&#36187;&#23395;&#21364;&#25130;&#28982;&#19981;&#21516;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#23454;&#29616;&#22242;&#38431;&#25490;&#21517;&#65292;&#20351;&#20854;&#19982;&#23436;&#25972;&#36187;&#23395;&#27604;&#36187;&#30340;&#32467;&#26524;&#31867;&#20284;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#65292;&#21033;&#29992;&#39044;&#27979;&#24615;&#21644;&#22788;&#26041;&#24615;&#20998;&#26512;&#65292;&#20026;&#21097;&#20313;&#36187;&#23395;&#21046;&#23450;&#19968;&#20010;&#30001;&#21407;&#23450;&#23433;&#25490;&#27604;&#36187;&#32452;&#25104;&#30340;&#36187;&#31243;&#23376;&#38598;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#38543;&#26426;&#20248;&#21270;&#27169;&#22411;&#20013;&#24341;&#20837;&#20102;&#22522;&#20110;&#26032;&#39062;&#25490;&#21517;&#30446;&#26631;&#65292;&#20854;&#21442;&#25968;&#39318;&#20808;&#20351;&#29992;&#39044;&#27979;&#24615;&#30830;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00178v1 Announce Type: cross  Abstract: Problem definition: Professional sports leagues may be suspended due to various reasons such as the recent COVID-19 pandemic. A critical question the league must address when re-opening is how to appropriately select a subset of the remaining games to conclude the season in a shortened time frame. Academic/practical relevance: Despite the rich literature on scheduling an entire season starting from a blank slate, concluding an existing season is quite different. Our approach attempts to achieve team rankings similar to that which would have resulted had the season been played out in full. Methodology: We propose a data-driven model which exploits predictive and prescriptive analytics to produce a schedule for the remainder of the season comprised of a subset of originally-scheduled games. Our model introduces novel rankings-based objectives within a stochastic optimization model, whose parameters are first estimated using a predictive 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#25968;&#25454;&#21644;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#36827;&#34892;&#36890;&#29992;&#29275;&#26631;&#35782;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#29289;&#31181;&#29305;&#23450;&#30340;&#22806;&#22871;&#22270;&#26696;&#25110;&#29305;&#20889;&#21475;&#21563;&#21360;&#35760;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;CNN&#21644;MLP&#22522;&#30784;&#23454;&#29616;&#23398;&#20064;&#21040;&#33391;&#22909;&#27867;&#21270;&#23884;&#20837;&#31354;&#38388;&#65292;&#20174;&#20307;&#24418;&#21306;&#20998;&#20010;&#20307;&#12290;</title><link>https://arxiv.org/abs/2404.00172</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#25968;&#25454;&#21644;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#23454;&#29616;&#30340;&#36890;&#29992;&#29275;&#26631;&#35782;
&lt;/p&gt;
&lt;p&gt;
Universal Bovine Identification via Depth Data and Deep Metric Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00172
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#25968;&#25454;&#21644;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#36827;&#34892;&#36890;&#29992;&#29275;&#26631;&#35782;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#29289;&#31181;&#29305;&#23450;&#30340;&#22806;&#22871;&#22270;&#26696;&#25110;&#29305;&#20889;&#21475;&#21563;&#21360;&#35760;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;CNN&#21644;MLP&#22522;&#30784;&#23454;&#29616;&#23398;&#20064;&#21040;&#33391;&#22909;&#27867;&#21270;&#23884;&#20837;&#31354;&#38388;&#65292;&#20174;&#20307;&#24418;&#21306;&#20998;&#20010;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#39030;&#37096;&#65288;&#32972;&#37096;&#35270;&#22270;&#65289;&#28145;&#24230;&#25968;&#25454;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#65292;&#29992;&#20110;&#20934;&#30830;&#35782;&#21035;&#20010;&#20307;&#29275;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#20195;&#30721;&#12289;&#25968;&#25454;&#38598;&#21644;&#35757;&#32451;&#26435;&#37325;&#65292;&#20197;&#20415;&#31435;&#21363;&#22797;&#29616;&#12290;&#30044;&#32676;&#35268;&#27169;&#22686;&#21152;&#23548;&#33268;&#29275;&#19982;&#20154;&#30340;&#27604;&#20363;&#22833;&#34913;&#65292;&#20351;&#24471;&#23545;&#20010;&#20307;&#36827;&#34892;&#25163;&#21160;&#30417;&#27979;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#23454;&#26102;&#29275;&#26631;&#35782;&#23545;&#20892;&#22330;&#33267;&#20851;&#37325;&#35201;&#65292;&#26159;&#31934;&#20934;&#30044;&#29287;&#19994;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00172v1 Announce Type: cross  Abstract: This paper proposes and evaluates, for the first time, a top-down (dorsal view), depth-only deep learning system for accurately identifying individual cattle and provides associated code, datasets, and training weights for immediate reproducibility. An increase in herd size skews the cow-to-human ratio at the farm and makes the manual monitoring of individuals more challenging. Therefore, real-time cattle identification is essential for the farms and a crucial step towards precision livestock farming. Underpinned by our previous work, this paper introduces a deep-metric learning method for cattle identification using depth data from an off-the-shelf 3D camera. The method relies on CNN and MLP backbones that learn well-generalised embedding spaces from the body shape to differentiate individuals -- requiring neither species-specific coat patterns nor close-up muzzle prints for operation. The network embeddings are clustered using a simp
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#36755;&#20837;&#22270;&#20687;&#36827;&#34892;&#21453;&#20107;&#23454;&#21464;&#21270;&#65292;&#25105;&#20204;&#23545;&#19981;&#21516;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20197;&#25581;&#31034;&#20854;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2404.00166</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#20107;&#23454;&#26469;&#25581;&#31034;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Uncovering Bias in Large Vision-Language Models with Counterfactuals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00166
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#36755;&#20837;&#22270;&#20687;&#36827;&#34892;&#21453;&#20107;&#23454;&#21464;&#21270;&#65292;&#25105;&#20204;&#23545;&#19981;&#21516;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20197;&#25581;&#31034;&#20854;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25317;&#26377;&#36234;&#26469;&#36234;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#26469;&#21033;&#29992;&#35270;&#35273;&#36755;&#20837;&#22686;&#24378;LLMs&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#25104;&#25991;&#26412;&#26102;&#21516;&#26102;&#26465;&#20214;&#20110;&#36755;&#20837;&#22270;&#20687;&#21644;&#25991;&#26412;&#25552;&#31034;&#65292;&#23454;&#29616;&#20102;&#21508;&#31181;&#29992;&#20363;&#65292;&#22914;&#35270;&#35273;&#38382;&#31572;&#21644;&#22810;&#27169;&#24577;&#32842;&#22825;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#32771;&#23519;&#20102;LLMs&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#21253;&#21547;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#20294;&#22312;LVLMs&#20013;&#30456;&#23545;&#36739;&#23569;&#25506;&#35752;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#30001;&#20110;&#25991;&#26412;&#21644;&#35270;&#35273;&#27169;&#24577;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#24341;&#36215;&#30340;&#20559;&#35265;&#30456;&#20114;&#20132;&#21449;&#65292;&#26816;&#26597;LVLMs&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;&#19981;&#21516;LVLMs&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30740;&#31350;&#65292;&#36890;&#36807;&#23545;&#36755;&#20837;&#22270;&#20687;&#36827;&#34892;&#21453;&#20107;&#23454;&#21464;&#21270;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21521;LVLMs&#21576;&#29616;&#30456;&#21516;&#30340;&#24320;&#25918;&#24335;&#25991;&#26412;&#25552;&#31034;&#65292;&#21516;&#26102;&#22312;&#19981;&#21516;&#30340;&#22270;&#20687;&#26465;&#20214;&#19979;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00166v1 Announce Type: cross  Abstract: With the advent of Large Language Models (LLMs) possessing increasingly impressive capabilities, a number of Large Vision-Language Models (LVLMs) have been proposed to augment LLMs with visual inputs. Such models condition generated text on both an input image and a text prompt, enabling a variety of use cases such as visual question answering and multimodal chat. While prior studies have examined the social biases contained in text generated by LLMs, this topic has been relatively unexplored in LVLMs. Examining social biases in LVLMs is particularly challenging due to the confounding contributions of bias induced by information contained across the text and visual modalities. To address this challenging problem, we conduct a large-scale study of text generated by different LVLMs under counterfactual changes to input images. Specifically, we present LVLMs with identical open-ended text prompts while conditioning on images from differen
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#22522;&#20110;&#20914;&#31361;&#25628;&#32034;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20854;&#37325;&#22797;&#24615;&#21644;&#22686;&#37327;&#24615;&#36136;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#28041;&#21450;&#22810;&#33218;&#21327;&#35843;&#30340;&#22797;&#26434;&#22330;&#26223;</title><link>https://arxiv.org/abs/2404.00143</link><description>&lt;p&gt;
&#21033;&#29992;&#22312;&#32447;&#29983;&#25104;&#30340;&#32463;&#39564;&#21152;&#36895;&#22522;&#20110;&#25628;&#32034;&#30340;&#22810;&#26426;&#22120;&#20154;&#25805;&#20316;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Accelerating Search-Based Planning for Multi-Robot Manipulation by Leveraging Online-Generated Experiences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00143
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#22522;&#20110;&#20914;&#31361;&#25628;&#32034;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20854;&#37325;&#22797;&#24615;&#21644;&#22686;&#37327;&#24615;&#36136;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#28041;&#21450;&#22810;&#33218;&#21327;&#35843;&#30340;&#22797;&#26434;&#22330;&#26223;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#19968;&#20010;&#28608;&#21160;&#20154;&#24515;&#30340;&#21069;&#27839;&#26159;&#21516;&#26102;&#20351;&#29992;&#22810;&#20010;&#26426;&#26800;&#33218;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#24403;&#21069;&#26041;&#27861;&#35268;&#21010;&#24182;&#21457;&#36816;&#21160;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#39640;&#32500;&#22797;&#21512;&#29366;&#24577;&#31354;&#38388;&#20351;&#35768;&#22810;&#20247;&#25152;&#21608;&#30693;&#30340;&#36816;&#21160;&#35268;&#21010;&#31639;&#27861;&#21464;&#24471;&#26840;&#25163;&#12290;&#26368;&#36817;&#65292;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#65288;MAPF&#65289;&#31639;&#27861;&#22312;&#31163;&#25955;2D&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;MAPF&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20110;&#20914;&#31361;&#30340;&#26041;&#27861;&#20551;&#23450;&#23384;&#22312;&#39640;&#25928;&#30340;&#21333;&#26234;&#33021;&#20307;&#36816;&#21160;&#35268;&#21010;&#22120;&#12290;&#36825;&#22312;&#35843;&#25972;&#36825;&#20123;&#26041;&#27861;&#20197;&#36866;&#24212;&#35268;&#21010;&#25805;&#20316;&#24773;&#20917;&#26102;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#19968;&#20551;&#35774;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#19981;&#25104;&#31435;&#65292;&#30001;&#20110;&#37197;&#32622;&#31354;&#38388;&#30340;&#39640;&#32500;&#24230;&#20197;&#21450;&#19982;&#30896;&#25758;&#26816;&#27979;&#30456;&#20851;&#30340;&#35745;&#31639;&#29942;&#39048;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#20854;&#37325;&#22797;&#24615;&#21644;&#22686;&#37327;&#24615;&#36136;&#21152;&#36895;&#20914;&#31361;&#22522;&#25628;&#32034;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#28041;&#21450;&#22810;&#33218;&#21327;&#35843;&#30340;&#22797;&#26434;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00143v1 Announce Type: cross  Abstract: An exciting frontier in robotic manipulation is the use of multiple arms at once. However, planning concurrent motions is a challenging task using current methods. The high-dimensional composite state space renders many well-known motion planning algorithms intractable. Recently, Multi-Agent Path-Finding (MAPF) algorithms have shown promise in discrete 2D domains, providing rigorous guarantees. However, widely used conflict-based methods in MAPF assume an efficient single-agent motion planner. This poses challenges in adapting them to manipulation cases where this assumption does not hold, due to the high dimensionality of configuration spaces and the computational bottlenecks associated with collision checking. To this end, we propose an approach for accelerating conflict-based search algorithms by leveraging their repetitive and incremental nature -- making them tractable for use in complex scenarios involving multi-arm coordination 
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;&#25200;&#21160;&#26041;&#27861;Shapley&#20540;&#21644;LIME&#21487;&#23454;&#29616;&#26356;&#39640;&#30340;&#20449;&#23454;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#24314;&#35758;&#20248;&#21270;&#21487;&#35299;&#37322;&#24615;&#31639;&#27861;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#21452;&#37325;&#30446;&#26631;</title><link>https://arxiv.org/abs/2404.00140</link><description>&lt;p&gt;
&#20449;&#23454;&#24615;&#19982;&#21487;&#20449;&#24230;&#26159;&#21542;&#23384;&#22312;&#20914;&#31361;&#65311;&#19968;&#39033;&#20851;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Does Faithfulness Conflict with Plausibility? An Empirical Study in Explainable AI across NLP Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00140
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#25200;&#21160;&#26041;&#27861;Shapley&#20540;&#21644;LIME&#21487;&#23454;&#29616;&#26356;&#39640;&#30340;&#20449;&#23454;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#24314;&#35758;&#20248;&#21270;&#21487;&#35299;&#37322;&#24615;&#31639;&#27861;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#21452;&#37325;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26088;&#22312;&#35299;&#37322;&#20915;&#31574;&#22411;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#31639;&#27861;&#36890;&#24120;&#35201;&#24179;&#34913;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;1&#65289;\textit{&#20449;&#23454;&#24615;}&#65292;&#21363;&#35299;&#37322;&#24517;&#39035;&#20934;&#30830;&#21453;&#26144;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#65307;2&#65289;\textit{&#21487;&#20449;&#24230;}&#65292;&#21363;&#35299;&#37322;&#24517;&#39035;&#19982;&#39046;&#22495;&#19987;&#23478;&#20445;&#25345;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#20449;&#23454;&#24615;&#21644;&#21487;&#20449;&#24230;&#26159;&#21542;&#20174;&#26681;&#26412;&#19978;&#23384;&#22312;&#20914;&#31361;&#65311;&#36890;&#36807;&#22312;&#24773;&#24863;&#20998;&#26512;&#12289;&#24847;&#22270;&#26816;&#27979;&#21644;&#20027;&#39064;&#26631;&#27880;&#19977;&#20010;NLP&#20219;&#21153;&#20013;&#36873;&#23450;&#30340;&#35299;&#37322;&#26041;&#27861;&#19982;&#19987;&#23478;&#32423;&#35299;&#37322;&#20043;&#38388;&#30340;&#20840;&#38754;&#37327;&#21270;&#27604;&#36739;&#65292;&#25105;&#20204;&#35777;&#26126;&#20256;&#32479;&#30340;&#25200;&#21160;&#26041;&#27861;Shapley&#20540;&#21644;LIME&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#20449;&#23454;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#23547;&#27714;&#36890;&#36807;&#21452;&#37325;&#30446;&#26631;&#20248;&#21270;&#21487;&#35299;&#37322;&#24615;&#31639;&#27861;&#26469;&#23454;&#29616;&#39640;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00140v1 Announce Type: new  Abstract: Explainability algorithms aimed at interpreting decision-making AI systems usually consider balancing two critical dimensions: 1) \textit{faithfulness}, where explanations accurately reflect the model's inference process. 2) \textit{plausibility}, where explanations are consistent with domain experts. However, the question arises: do faithfulness and plausibility inherently conflict? In this study, through a comprehensive quantitative comparison between the explanations from the selected explainability methods and expert-level interpretations across three NLP tasks: sentiment analysis, intent detection, and topic labeling, we demonstrate that traditional perturbation-based methods Shapley value and LIME could attain greater faithfulness and plausibility. Our findings suggest that rather than optimizing for one dimension at the expense of the other, we could seek to optimize explainability algorithms with dual objectives to achieve high l
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#29289;&#32852;&#32593;&#20013;&#30340;&#38598;&#25104;&#24102;&#26469;&#20102;&#23433;&#20840;&#39118;&#38505;&#65292;&#26412;&#25991;&#20998;&#26512;&#20102;&#25968;&#25454;&#27844;&#38706;&#21644;&#25216;&#26415;&#28389;&#29992;&#31561;&#39118;&#38505;&#65292;&#24182;&#35752;&#35770;&#20102;&#32531;&#35299;&#36825;&#20123;&#39118;&#38505;&#30340;&#25112;&#30053;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.00139</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#29289;&#32852;&#32593;&#20013;&#30340;&#23433;&#20840;&#39118;&#38505;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Security Risks Concerns of Generative AI in the IoT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00139
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#29289;&#32852;&#32593;&#20013;&#30340;&#38598;&#25104;&#24102;&#26469;&#20102;&#23433;&#20840;&#39118;&#38505;&#65292;&#26412;&#25991;&#20998;&#26512;&#20102;&#25968;&#25454;&#27844;&#38706;&#21644;&#25216;&#26415;&#28389;&#29992;&#31561;&#39118;&#38505;&#65292;&#24182;&#35752;&#35770;&#20102;&#32531;&#35299;&#36825;&#20123;&#39118;&#38505;&#30340;&#25112;&#30053;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#32852;&#32593; (IoT) &#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021; (AI) &#26085;&#30410;&#20132;&#27719;&#30340;&#26102;&#20195;&#65292;&#26412;&#25991;&#23457;&#35270;&#20102;&#36825;&#31181;&#25972;&#21512;&#20013;&#28508;&#22312;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#29289;&#32852;&#32593;&#20013;&#25512;&#21160;&#21019;&#26032;&#30340;&#26041;&#24335;&#65292;&#20998;&#26512;&#20102;&#22312;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#26102;&#21487;&#33021;&#21457;&#29983;&#30340;&#25968;&#25454;&#27844;&#38706;&#39118;&#38505;&#20197;&#21450;&#22312;&#29289;&#32852;&#32593;&#29983;&#24577;&#31995;&#32479;&#20013;&#28389;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#28508;&#21147;&#12290;&#36825;&#20123;&#39118;&#38505;&#19981;&#20165;&#23041;&#32961;&#30528;&#29289;&#32852;&#32593;&#31995;&#32479;&#30340;&#38544;&#31169;&#21644;&#25928;&#29575;&#65292;&#32780;&#19988;&#23545;&#22312;&#20197;&#20154;&#24037;&#26234;&#33021;&#20026;&#39537;&#21160;&#30340;&#29615;&#22659;&#20013;&#30340;&#20449;&#20219;&#21644;&#23433;&#20840;&#24615;&#20135;&#29983;&#20102;&#26356;&#24191;&#27867;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#35752;&#35770;&#24310;&#20280;&#33267;&#32531;&#35299;&#36825;&#20123;&#39118;&#38505;&#30340;&#25112;&#30053;&#26041;&#27861;&#65292;&#21253;&#25324;&#21046;&#23450;&#20581;&#20840;&#30340;&#23433;&#20840;&#21327;&#35758;&#12289;&#22810;&#23618;&#27425;&#30340;&#23433;&#20840;&#26041;&#27861;&#20197;&#21450;&#37319;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#20840;&#38754;&#20998;&#26512;&#65292;&#26412;&#25991;&#26088;&#22312;&#38416;&#26126;&#22312;&#29289;&#32852;&#32593;&#20013;&#25317;&#25265;&#20154;&#24037;&#26234;&#33021;&#36827;&#27493;&#19982;&#30830;&#20445;&#20005;&#26684;&#23433;&#20840;&#20043;&#38388;&#30340;&#20851;&#38190;&#24179;&#34913;&#65292;&#25552;&#20379;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00139v1 Announce Type: cross  Abstract: In an era where the Internet of Things (IoT) intersects increasingly with generative Artificial Intelligence (AI), this article scrutinizes the emergent security risks inherent in this integration. We explore how generative AI drives innovation in IoT and we analyze the potential for data breaches when using generative AI and the misuse of generative AI technologies in IoT ecosystems. These risks not only threaten the privacy and efficiency of IoT systems but also pose broader implications for trust and safety in AI-driven environments. The discussion in this article extends to strategic approaches for mitigating these risks, including the development of robust security protocols, the multi-layered security approaches, and the adoption of AI technological solutions. Through a comprehensive analysis, this article aims to shed light on the critical balance between embracing AI advancements and ensuring stringent security in IoT, providin
&lt;/p&gt;</description></item><item><title>&#23558;&#20256;&#32479;&#25104;&#26412;&#21333;&#20301;&#35270;&#20026;&#21464;&#37327;&#65292;&#36890;&#36807;&#35843;&#25972;&#20540;&#21487;&#20197;&#33719;&#24471;&#27604;&#40664;&#35748;&#26597;&#35810;&#35745;&#21010;&#26356;&#20248;&#30340;&#26597;&#35810;&#35745;&#21010;</title><link>https://arxiv.org/abs/2404.00137</link><description>&lt;p&gt;
&#39044;&#31639;&#24863;&#30693;&#26597;&#35810;&#20248;&#21270;&#65306;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Budget-aware Query Tuning: An AutoML Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00137
&lt;/p&gt;
&lt;p&gt;
&#23558;&#20256;&#32479;&#25104;&#26412;&#21333;&#20301;&#35270;&#20026;&#21464;&#37327;&#65292;&#36890;&#36807;&#35843;&#25972;&#20540;&#21487;&#20197;&#33719;&#24471;&#27604;&#40664;&#35748;&#26597;&#35810;&#35745;&#21010;&#26356;&#20248;&#30340;&#26597;&#35810;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25968;&#25454;&#24211;&#31995;&#32479;&#20381;&#36182;&#22522;&#20110;&#25104;&#26412;&#30340;&#26597;&#35810;&#20248;&#21270;&#22120;&#20026;&#36755;&#20837;&#26597;&#35810;&#25552;&#20379;&#33391;&#22909;&#30340;&#25191;&#34892;&#35745;&#21010;&#12290;&#36825;&#31181;&#26597;&#35810;&#20248;&#21270;&#22120;&#20381;&#36182;&#25104;&#26412;&#27169;&#22411;&#26469;&#20272;&#31639;&#20505;&#36873;&#26597;&#35810;&#25191;&#34892;&#35745;&#21010;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#25361;&#25112;&#20102;&#20256;&#32479;&#35266;&#24565;&#65292;&#23558;&#36825;&#20123;&#25104;&#26412;&#21333;&#20301;&#35270;&#20026;&#21464;&#37327;&#65292;&#34920;&#26126;&#36890;&#36807;&#35843;&#25972;&#25104;&#26412;&#21333;&#20301;&#20540;&#65292;&#21487;&#20197;&#33719;&#24471;&#26126;&#26174;&#20248;&#20110;&#23558;&#25104;&#26412;&#21333;&#20301;&#35270;&#20026;&#24120;&#25968;&#26102;&#26597;&#35810;&#20248;&#21270;&#22120;&#36820;&#22238;&#30340;&#40664;&#35748;&#26597;&#35810;&#35745;&#21010;&#30340;&#26597;&#35810;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00137v1 Announce Type: cross  Abstract: Modern database systems rely on cost-based query optimizers to come up with good execution plans for input queries. Such query optimizers rely on cost models to estimate the costs of candidate query execution plans. A cost model represents a function from a set of cost units to query execution cost, where each cost unit specifies the unit cost of executing a certain type of query processing operation (such as table scan or join). These cost units are traditionally viewed as constants, whose values only depend on the platform configuration where the database system runs on top of but are invariant for queries processed by the database system. In this paper, we challenge this classic view by thinking of these cost units as variables instead. We show that, by varying the cost-unit values one can obtain query plans that significantly outperform the default query plans returned by the query optimizer when viewing the cost units as constants
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27491;&#20132;&#34701;&#21512;&#22788;&#29702;&#36974;&#25377;&#30340;&#24378;&#20581;&#38598;&#25104;&#20154;&#21592;&#35782;&#21035;&#27169;&#22411;&#65292;&#26080;&#38656;&#25163;&#21160;&#26631;&#35760;&#36974;&#25377;&#21306;&#22495;&#65292;&#21033;&#29992;CNN&#21644;Transformer&#26550;&#26500;&#29983;&#25104;&#40065;&#26834;&#29305;&#24449;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2404.00107</link><description>&lt;p&gt;
&#36890;&#36807;&#20855;&#26377;&#36974;&#25377;&#22788;&#29702;&#30340;&#27491;&#20132;&#34701;&#21512;&#30340;&#24378;&#20581;&#38598;&#25104;&#20154;&#21592;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Robust Ensemble Person Re-Identification via Orthogonal Fusion with Occlusion Handling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00107
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27491;&#20132;&#34701;&#21512;&#22788;&#29702;&#36974;&#25377;&#30340;&#24378;&#20581;&#38598;&#25104;&#20154;&#21592;&#35782;&#21035;&#27169;&#22411;&#65292;&#26080;&#38656;&#25163;&#21160;&#26631;&#35760;&#36974;&#25377;&#21306;&#22495;&#65292;&#21033;&#29992;CNN&#21644;Transformer&#26550;&#26500;&#29983;&#25104;&#40065;&#26834;&#29305;&#24449;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36974;&#25377;&#20173;&#28982;&#26159;&#20154;&#21592;&#20877;&#35782;&#21035;&#65288;ReID&#65289;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#36825;&#26159;&#30001;&#20110;&#23039;&#21183;&#30340;&#22810;&#26679;&#24615;&#21644;&#22806;&#35266;&#30340;&#21464;&#21270;&#12290;&#20026;&#20102;&#25913;&#21892;&#20855;&#26377;&#36974;&#25377;&#24863;&#30693;&#30340;&#20154;&#21592;Re-ID&#30340;&#40065;&#26834;&#24615;&#65292;&#38656;&#35201;&#24320;&#21457;&#26032;&#30340;&#26550;&#26500;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#20998;&#36776;&#29575;&#36793;&#32536;&#25668;&#20687;&#22836;&#19978;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#38598;&#25104;&#27169;&#22411;&#65292;&#21033;&#29992;CNN&#21644;Transformer&#26550;&#26500;&#29983;&#25104;&#24378;&#22823;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;&#20026;&#20102;&#23454;&#29616;&#40065;&#26834;&#30340;Re-ID&#65292;&#32780;&#26080;&#38656;&#25163;&#21160;&#26631;&#35760;&#34987;&#36974;&#25377;&#30340;&#21306;&#22495;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#25104;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20174;&#20219;&#24847;&#24418;&#29366;&#30340;&#34987;&#36974;&#25377;&#21306;&#22495;&#21644;&#40065;&#26834;&#29305;&#24449;&#34920;&#31034;&#20043;&#38388;&#30340;&#31867;&#27604;&#20013;&#25512;&#23548;&#20986;&#12290;&#21033;&#29992;&#27491;&#20132;&#24615;&#21407;&#29702;&#65292;&#25105;&#20204;&#24320;&#21457;&#30340;&#28145;&#24230;CNN&#27169;&#22411;&#21033;&#29992;&#20102;&#25513;&#34109;&#33258;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#21644;&#20840;&#23616;-&#23616;&#37096;&#29305;&#24449;&#34701;&#21512;&#26469;&#36827;&#34892;&#24378;&#22823;&#30340;&#20154;&#21592;&#35782;&#21035;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#23398;&#20064;&#40065;&#26834;&#29305;&#24449;&#31354;&#38388;&#30340;&#37096;&#20998;&#36974;&#25377;&#24863;&#30693;&#21464;&#21387;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00107v1 Announce Type: cross  Abstract: Occlusion remains one of the major challenges in person reidentification (ReID) as a result of the diversity of poses and the variation of appearances. Developing novel architectures to improve the robustness of occlusion-aware person Re-ID requires new insights, especially on low-resolution edge cameras. We propose a deep ensemble model that harnesses both CNN and Transformer architectures to generate robust feature representations. To achieve robust Re-ID without the need to manually label occluded regions, we propose to take an ensemble learning-based approach derived from the analogy between arbitrarily shaped occluded regions and robust feature representation. Using the orthogonality principle, our developed deep CNN model makes use of masked autoencoder (MAE) and global-local feature fusion for robust person identification. Furthermore, we present a part occlusion-aware transformer capable of learning feature space that is robust
&lt;/p&gt;</description></item><item><title>&#22312;&#23545;&#25239;&#24615;&#29615;&#22659;&#20013;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20462;&#25913;&#36716;&#31227;&#26680;&#23494;&#24230;&#30340;&#25200;&#21160;&#27169;&#22411;&#65292;&#25299;&#23637;&#20102;&#20256;&#32479;&#30340;&#36793;&#32536;&#25935;&#24863;&#24615;&#27169;&#22411;&#65292;&#23545;&#26080;&#38480;&#26102;&#38388;RL&#20013;&#31574;&#30053;&#20215;&#20540;&#36827;&#34892;&#20102;&#23574;&#38160;&#36793;&#30028;&#30340;&#21051;&#30011;&#21644;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2404.00099</link><description>&lt;p&gt;
&#22312;&#24378;&#20581;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#39640;&#25928;&#32780;&#23574;&#38160;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Efficient and Sharp Off-Policy Evaluation in Robust Markov Decision Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00099
&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#25239;&#24615;&#29615;&#22659;&#20013;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20462;&#25913;&#36716;&#31227;&#26680;&#23494;&#24230;&#30340;&#25200;&#21160;&#27169;&#22411;&#65292;&#25299;&#23637;&#20102;&#20256;&#32479;&#30340;&#36793;&#32536;&#25935;&#24863;&#24615;&#27169;&#22411;&#65292;&#23545;&#26080;&#38480;&#26102;&#38388;RL&#20013;&#31574;&#30053;&#20215;&#20540;&#36827;&#34892;&#20102;&#23574;&#38160;&#36793;&#30028;&#30340;&#21051;&#30011;&#21644;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#20013;&#32473;&#23450;&#26469;&#33258;&#21407;&#22987;MDP&#30340;&#36716;&#31227;&#35266;&#23519;&#26102;&#65292;&#22312;&#26368;&#20339;&#21644;&#26368;&#22351;&#24773;&#20917;&#19979;&#35780;&#20272;&#31574;&#30053;&#65292;&#26080;&#35770;&#26159;&#22312;&#30456;&#21516;&#31574;&#30053;&#36824;&#26159;&#19981;&#21516;&#31574;&#30053;&#19979;&#12290;&#24403;&#23384;&#22312;&#21382;&#21490;&#21644;&#26410;&#26469;&#29615;&#22659;&#20043;&#38388;&#21487;&#33021;&#21457;&#29983;&#36716;&#21464;&#30340;&#21487;&#33021;&#24615;&#26102;&#65292;&#27604;&#22914;&#30001;&#20110;&#26410;&#27979;&#37327;&#30340;&#28151;&#26434;&#12289;&#20998;&#24067;&#36716;&#31227;&#25110;&#23545;&#25239;&#24615;&#29615;&#22659;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25200;&#21160;&#27169;&#22411;&#65292;&#21487;&#20197;&#23558;&#36716;&#31227;&#26680;&#23494;&#24230;&#20462;&#25913;&#33267;&#32473;&#23450;&#20056;&#27861;&#22240;&#23376;&#25110;&#20854;&#20498;&#25968;&#65292;&#36825;&#23558;&#32463;&#20856;&#30340;&#36793;&#38469;&#25935;&#24863;&#24615;&#27169;&#22411;&#65288;MSM&#65289;&#25193;&#23637;&#21040;&#26080;&#38480;&#26102;&#38388; RL&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#22312;&#36825;&#20010;&#27169;&#22411;&#19979;&#30340;&#31574;&#30053;&#20215;&#20540;&#30340;&#23574;&#38160;&#36793;&#30028;&#65292;&#21363;&#22312;&#32473;&#23450;&#26469;&#33258;&#21407;&#22987;MDP&#30340;&#36716;&#31227;&#35266;&#27979;&#26102;&#21487;&#33021;&#30340;&#26368;&#20005;&#26684;&#36793;&#30028;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#36825;&#20123;&#36716;&#31227;&#35266;&#23519;&#20013;&#20272;&#35745;&#36825;&#20123;&#36793;&#30028;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20272;&#35745;&#22120;&#65292;&#20855;&#26377;&#20960;&#20010;&#21560;&#24341;&#20154;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00099v1 Announce Type: new  Abstract: We study evaluating a policy under best- and worst-case perturbations to a Markov decision process (MDP), given transition observations from the original MDP, whether under the same or different policy. This is an important problem when there is the possibility of a shift between historical and future environments, due to e.g. unmeasured confounding, distributional shift, or an adversarial environment. We propose a perturbation model that can modify transition kernel densities up to a given multiplicative factor or its reciprocal, which extends the classic marginal sensitivity model (MSM) for single time step decision making to infinite-horizon RL. We characterize the sharp bounds on policy value under this model, that is, the tightest possible bounds given by the transition observations from the original MDP, and we study the estimation of these bounds from such transition observations. We develop an estimator with several appealing gua
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#30340;GAN&#65292;&#21363;InstGAN&#65292;&#20197;&#22312;&#20196;&#29260;&#32423;&#21035;&#19978;&#29983;&#25104;&#20855;&#26377;&#22810;&#23646;&#24615;&#20248;&#21270;&#30340;&#20998;&#23376;&#65292;&#24182;&#21033;&#29992;&#26368;&#22823;&#21270;&#20449;&#24687;&#29109;&#26469;&#32531;&#35299;&#27169;&#24335;&#23849;&#28291;&#12290;</title><link>https://arxiv.org/abs/2404.00081</link><description>&lt;p&gt;
&#20855;&#26377;&#22810;&#23646;&#24615;&#20248;&#21270;&#30340;&#20998;&#23376;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Molecular Generative Adversarial Network with Multi-Property Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00081
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#30340;GAN&#65292;&#21363;InstGAN&#65292;&#20197;&#22312;&#20196;&#29260;&#32423;&#21035;&#19978;&#29983;&#25104;&#20855;&#26377;&#22810;&#23646;&#24615;&#20248;&#21270;&#30340;&#20998;&#23376;&#65292;&#24182;&#21033;&#29992;&#26368;&#22823;&#21270;&#20449;&#24687;&#29109;&#26469;&#32531;&#35299;&#27169;&#24335;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#65292;&#24050;&#34987;&#24212;&#29992;&#20110;&#33647;&#29289;&#21457;&#29616;&#20013;$de~novo$&#20998;&#23376;&#29983;&#25104;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#30740;&#31350;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#65292;&#26469;&#22788;&#29702;GANs&#20013;&#20998;&#23376;&#34920;&#31034;&#30340;&#31163;&#25955;&#29305;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;GANs&#21644;RL&#27169;&#22411;&#30340;&#22266;&#26377;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#65292;&#20197;&#21450;&#19982;MCTS&#37319;&#26679;&#30456;&#20851;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;MCTS RL-based GANs&#38590;&#20197;&#25193;&#23637;&#21040;&#22823;&#22411;&#21270;&#23398;&#25968;&#25454;&#24211;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24102;&#21363;&#26102;&#21644;&#20840;&#23616;&#22870;&#21169;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;RL&#30340;&#26032;&#22411;GAN&#65292;&#31216;&#20026;InstGAN&#65292;&#20197;&#22312;&#20196;&#29260;&#32423;&#21035;&#19978;&#29983;&#25104;&#20855;&#26377;&#22810;&#23646;&#24615;&#20248;&#21270;&#30340;&#20998;&#23376;&#12290;&#27492;&#22806;&#65292;&#26368;&#22823;&#21270;&#20449;&#24687;&#29109;&#34987;&#21033;&#29992;&#26469;&#32531;&#35299;&#27169;&#24335;&#23849;&#28291;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;InstGAN&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#65292;&#36798;&#21040;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00081v1 Announce Type: cross  Abstract: Deep generative models, such as generative adversarial networks (GANs), have been employed for $de~novo$ molecular generation in drug discovery. Most prior studies have utilized reinforcement learning (RL) algorithms, particularly Monte Carlo tree search (MCTS), to handle the discrete nature of molecular representations in GANs. However, due to the inherent instability in training GANs and RL models, along with the high computational cost associated with MCTS sampling, MCTS RL-based GANs struggle to scale to large chemical databases. To tackle these challenges, this study introduces a novel GAN based on actor-critic RL with instant and global rewards, called InstGAN, to generate molecules at the token-level with multi-property optimization. Furthermore, maximized information entropy is leveraged to alleviate the mode collapse. The experimental results demonstrate that InstGAN outperforms other baselines, achieves comparable performance
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21517;&#20026;&#8220;DirtyFlipping&#8221;&#65292;&#21033;&#29992;&#33039;&#26631;&#31614;&#25216;&#26415;&#22312;&#36873;&#23450;&#30340;&#25968;&#25454;&#27169;&#24335;&#20013;&#36755;&#20837;&#35302;&#21457;&#22120;&#65292;&#20174;&#32780;&#23454;&#29616;&#38544;&#34109;&#30340;&#21518;&#38376;&#12290;</title><link>https://arxiv.org/abs/2404.00076</link><description>&lt;p&gt;
&#20351;&#29992;&#20498;&#32622;&#26631;&#31614;&#30340;&#21518;&#38376;&#26041;&#27861;&#65306;&#33039;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
A Backdoor Approach with Inverted Labels Using Dirty Label-Flipping Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00076
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21517;&#20026;&#8220;DirtyFlipping&#8221;&#65292;&#21033;&#29992;&#33039;&#26631;&#31614;&#25216;&#26415;&#22312;&#36873;&#23450;&#30340;&#25968;&#25454;&#27169;&#24335;&#20013;&#36755;&#20837;&#35302;&#21457;&#22120;&#65292;&#20174;&#32780;&#23454;&#29616;&#38544;&#34109;&#30340;&#21518;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22768;&#38899;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#32463;&#24120;&#20351;&#29992;&#20844;&#20849;&#25110;&#31532;&#19977;&#26041;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#26159;&#19981;&#20934;&#30830;&#30340;&#12290;&#36825;&#20351;&#24471;&#35757;&#32451;&#22312;&#36825;&#20123;&#25968;&#25454;&#19978;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#28508;&#22312;&#30340;&#25968;&#25454;&#27602;&#21270;&#25915;&#20987;&#12290;&#22312;&#36825;&#31181;&#25915;&#20987;&#31867;&#22411;&#20013;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#20351;&#29992;&#27602;&#21270;&#25968;&#25454;&#26469;&#35757;&#32451;DNN&#27169;&#22411;&#65292;&#21487;&#33021;&#20250;&#38477;&#20302;&#20854;&#24615;&#33021;&#12290;&#21478;&#19968;&#31181;&#23545;&#25105;&#20204;&#30340;&#30740;&#31350;&#38750;&#24120;&#30456;&#20851;&#30340;&#25968;&#25454;&#27602;&#21270;&#25915;&#20987;&#31867;&#22411;&#26159;&#26631;&#31614;&#32763;&#36716;&#65292;&#25915;&#20987;&#32773;&#22312;&#20854;&#20013;&#25805;&#32437;&#25968;&#25454;&#23376;&#38598;&#30340;&#26631;&#31614;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#21363;&#20351;&#26159;&#33021;&#21147;&#26377;&#38480;&#30340;&#25915;&#20987;&#32773;&#65292;&#36825;&#20123;&#25915;&#20987;&#20063;&#21487;&#33021;&#26497;&#22823;&#22320;&#38477;&#20302;&#31995;&#32479;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;DirtyFlipping&#8221;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#33039;&#26631;&#31614;&#25216;&#26415;&#65292;&#8220;&#26631;&#31614;&#23545;&#26631;&#31614;&#8221;&#65292;&#22312;&#19982;&#30446;&#26631;&#31867;&#21035;&#30456;&#20851;&#30340;&#36873;&#23450;&#25968;&#25454;&#27169;&#24335;&#20013;&#36755;&#20837;&#35302;&#21457;&#22120;&#65288;&#25293;&#25163;&#65289;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38544;&#34109;&#30340;&#21518;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00076v1 Announce Type: cross  Abstract: Audio-based machine learning systems frequently use public or third-party data, which might be inaccurate. This exposes deep neural network (DNN) models trained on such data to potential data poisoning attacks. In this type of assault, attackers can train the DNN model using poisoned data, potentially degrading its performance. Another type of data poisoning attack that is extremely relevant to our investigation is label flipping, in which the attacker manipulates the labels for a subset of data. It has been demonstrated that these assaults may drastically reduce system performance, even for attackers with minimal abilities. In this study, we propose a backdoor attack named 'DirtyFlipping', which uses dirty label techniques, "label-on-label", to input triggers (clapping) in the selected data patterns associated with the target class, thereby enabling a stealthy backdoor.
&lt;/p&gt;</description></item><item><title>&#26102;&#38388;&#22270;&#32593;&#32476;&#65288;TGN&#65289;&#22312;&#37329;&#34701;&#32593;&#32476;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#21183;&#65292;&#36866;&#24212;&#20102;&#29616;&#20195;&#37329;&#34701;&#31995;&#32479;&#21160;&#24577;&#21644;&#22797;&#26434;&#30340;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00060</link><description>&lt;p&gt;
&#37329;&#34701;&#32593;&#32476;&#20013;&#30340;&#26102;&#38388;&#22270;&#32593;&#32476;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Temporal Graph Networks for Graph Anomaly Detection in Financial Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00060
&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#22270;&#32593;&#32476;&#65288;TGN&#65289;&#22312;&#37329;&#34701;&#32593;&#32476;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#21183;&#65292;&#36866;&#24212;&#20102;&#29616;&#20195;&#37329;&#34701;&#31995;&#32479;&#21160;&#24577;&#21644;&#22797;&#26434;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#26102;&#38388;&#22270;&#32593;&#32476;&#65288;TGN&#65289;&#36827;&#34892;&#37329;&#34701;&#24322;&#24120;&#26816;&#27979;&#65292;&#22312;&#37329;&#34701;&#31185;&#25216;&#21644;&#25968;&#23383;&#21270;&#37329;&#34701;&#20132;&#26131;&#26102;&#20195;&#36825;&#26159;&#19968;&#20010;&#36843;&#20999;&#38656;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;TGN&#25429;&#25417;&#37329;&#34701;&#32593;&#32476;&#20013;&#36793;&#30340;&#21160;&#24577;&#21464;&#21270;&#65292;&#29992;&#20110;&#27450;&#35784;&#26816;&#27979;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;TGN&#19982;&#38745;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22522;&#32447;&#20197;&#21450;&#20351;&#29992;DGraph&#25968;&#25454;&#38598;&#36827;&#34892;&#29616;&#23454;&#37329;&#34701;&#22330;&#26223;&#19979;&#30340;&#21069;&#27839;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#22522;&#32447;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;TGN&#22312;AUC&#25351;&#26631;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#36825;&#31181;&#20248;&#36234;&#24615;&#33021;&#31361;&#26174;&#20102;TGN&#20316;&#20026;&#26816;&#27979;&#37329;&#34701;&#27450;&#35784;&#30340;&#26377;&#25928;&#24037;&#20855;&#30340;&#28508;&#21147;&#65292;&#23637;&#31034;&#20102;&#20854;&#36866;&#24212;&#29616;&#20195;&#37329;&#34701;&#31995;&#32479;&#21160;&#24577;&#21644;&#22797;&#26434;&#24615;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#22312;TGN&#26694;&#26550;&#20869;&#23581;&#35797;&#20102;&#21508;&#31181;&#22270;&#23884;&#20837;&#27169;&#22359;&#65292;&#24182;&#27604;&#36739;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00060v1 Announce Type: cross  Abstract: This paper explores the utilization of Temporal Graph Networks (TGN) for financial anomaly detection, a pressing need in the era of fintech and digitized financial transactions. We present a comprehensive framework that leverages TGN, capable of capturing dynamic changes in edges within financial networks, for fraud detection. Our study compares TGN's performance against static Graph Neural Network (GNN) baselines, as well as cutting-edge hypergraph neural network baselines using DGraph dataset for a realistic financial context. Our results demonstrate that TGN significantly outperforms other models in terms of AUC metrics. This superior performance underlines TGN's potential as an effective tool for detecting financial fraud, showcasing its ability to adapt to the dynamic and complex nature of modern financial systems. We also experimented with various graph embedding modules within the TGN framework and compared the effectiveness of 
&lt;/p&gt;</description></item><item><title>PerOS&#26159;&#20010;&#24615;&#21270;&#33258;&#36866;&#24212;&#25805;&#20316;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;OS&#22312;&#24773;&#25253;&#21644;&#29992;&#25143;&#20307;&#39564;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#23588;&#20854;&#36866;&#24212;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2404.00057</link><description>&lt;p&gt;
PerOS: &#22312;&#20113;&#20013;&#30340;&#20010;&#24615;&#21270;&#33258;&#36866;&#24212;&#25805;&#20316;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
PerOS: Personalized Self-Adapting Operating Systems in the Cloud
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00057
&lt;/p&gt;
&lt;p&gt;
PerOS&#26159;&#20010;&#24615;&#21270;&#33258;&#36866;&#24212;&#25805;&#20316;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;OS&#22312;&#24773;&#25253;&#21644;&#29992;&#25143;&#20307;&#39564;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#23588;&#20854;&#36866;&#24212;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25805;&#20316;&#31995;&#32479;&#65288;OS&#65289;&#26159;&#35745;&#31639;&#26426;&#31995;&#32479;&#30340;&#22522;&#30784;&#65292;&#31649;&#29702;&#30828;&#20214;&#36164;&#28304;&#24182;&#30830;&#20445;&#22810;&#26679;&#24212;&#29992;&#31243;&#24207;&#30340;&#23433;&#20840;&#29615;&#22659;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#25345;&#20037;&#37325;&#35201;&#24615;&#65292;&#20294;OS&#30340;&#22522;&#26412;&#35774;&#35745;&#30446;&#26631;&#20960;&#21313;&#24180;&#26469;&#20960;&#20046;&#27809;&#26377;&#36827;&#21270;&#12290;&#20256;&#32479;&#19978;&#20248;&#20808;&#32771;&#34385;&#36895;&#24230;&#12289;&#20869;&#23384;&#25928;&#29575;&#12289;&#23433;&#20840;&#24615;&#21644;&#21487;&#20280;&#32553;&#24615;&#31561;&#26041;&#38754;&#65292;&#36825;&#20123;&#30446;&#26631;&#24120;&#24120;&#24573;&#35270;&#20102;&#24773;&#25253;&#21644;&#20010;&#24615;&#21270;&#29992;&#25143;&#20307;&#39564;&#36825;&#19968;&#20851;&#38190;&#26041;&#38754;&#12290;&#24773;&#25253;&#30340;&#32570;&#20047;&#22312;&#25216;&#26415;&#38761;&#21629;&#20013;&#21464;&#24471;&#36234;&#21457;&#20851;&#38190;&#65292;&#27604;&#22914;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#38754;&#30340;&#26174;&#33879;&#36827;&#23637;&#12290;&#22914;&#20170;&#30340;&#20010;&#20154;&#35774;&#22791;&#27491;&#22312;&#28436;&#21464;&#25104;&#29992;&#25143;&#30340;&#20146;&#23494;&#20276;&#20387;&#65292;&#23545;&#20256;&#32479;OS&#65288;&#22914;Linux&#21644;iOS&#65289;&#25552;&#20986;&#29420;&#29305;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#38543;&#30528;&#29305;&#27530;&#30828;&#20214;&#21644;&#24322;&#26500;&#32452;&#20214;&#30340;&#20986;&#29616;&#12290;&#27492;&#22806;&#65292;&#26426;&#22120;&#23398;&#20064;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23835;&#36215;&#24341;&#20837;&#20102;&#36716;&#21464;&#24615;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00057v1 Announce Type: cross  Abstract: Operating systems (OSes) are foundational to computer systems, managing hardware resources and ensuring secure environments for diverse applications. However, despite their enduring importance, the fundamental design objectives of OSes have seen minimal evolution over decades. Traditionally prioritizing aspects like speed, memory efficiency, security, and scalability, these objectives often overlook the crucial aspect of intelligence as well as personalized user experience. The lack of intelligence becomes increasingly critical amid technological revolutions, such as the remarkable advancements in machine learning (ML).   Today's personal devices, evolving into intimate companions for users, pose unique challenges for traditional OSes like Linux and iOS, especially with the emergence of specialized hardware featuring heterogeneous components. Furthermore, the rise of large language models (LLMs) in ML has introduced transformative capa
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ChapTER&#65292;&#19968;&#20010;&#23545;&#27604;&#21382;&#21490;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#21069;&#32512;&#35843;&#25972;&#23454;&#29616;&#25991;&#26412;&#19982;&#26102;&#38388;&#30340;&#24179;&#34913;&#65292;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2404.00051</link><description>&lt;p&gt;
Deja vu: &#20351;&#29992;&#21069;&#32512;&#35843;&#25972;&#36827;&#34892;&#23545;&#27604;&#21382;&#21490;&#24314;&#27169;&#65292;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Deja vu: Contrastive Historical Modeling with Prefix-tuning for Temporal Knowledge Graph Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00051
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ChapTER&#65292;&#19968;&#20010;&#23545;&#27604;&#21382;&#21490;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#21069;&#32512;&#35843;&#25972;&#23454;&#29616;&#25991;&#26412;&#19982;&#26102;&#38388;&#30340;&#24179;&#34913;&#65292;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#22270;&#25512;&#29702;&#65288;TKGR&#65289;&#26159;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#20026;&#19981;&#23436;&#25972;&#30340;TKG&#25512;&#26029;&#32570;&#22833;&#20107;&#23454;&#30340;&#20219;&#21153;&#65288;&#20363;&#22914;&#65292;&#20256;&#23548;&#21644;&#24402;&#32435;&#35774;&#32622;&#65289;&#65292;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#20026;&#20102;&#20943;&#23569;TKG&#20013;&#32467;&#26500;&#36830;&#25509;&#30340;&#20381;&#36182;&#24615;&#65292;&#24050;&#24320;&#21457;&#20102;&#22522;&#20110;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#23454;&#20307;&#25551;&#36848;&#20013;&#20016;&#23500;&#30340;&#35821;&#35328;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24040;&#22823;&#21442;&#25968;&#21644;&#19981;&#28789;&#27963;&#24615;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#26114;&#36149;&#19988;&#30446;&#30340;&#24314;&#31435;&#30340;&#35757;&#32451;&#31574;&#30053;&#19978;&#24456;&#38590;&#24179;&#34913;&#25991;&#26412;&#30693;&#35782;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;&#20026;&#20102;&#21457;&#25496;&#25991;&#26412;&#27169;&#22411;&#22312;&#21508;&#31181;&#22797;&#26434;&#22330;&#26223;&#20013;&#29992;&#20110;TKGR&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ChapTER&#65292;&#19968;&#20010;&#20855;&#26377;&#21069;&#32512;&#35843;&#25972;&#23545;&#27604;&#21382;&#21490;&#24314;&#27169;&#26694;&#26550;&#65292;&#29992;&#20110;&#26102;&#38388;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00051v1 Announce Type: new  Abstract: Temporal Knowledge Graph Reasoning (TKGR) is the task of inferring missing facts for incomplete TKGs in complex scenarios (e.g., transductive and inductive settings), which has been gaining increasing attention. Recently, to mitigate dependence on structured connections in TKGs, text-based methods have been developed to utilize rich linguistic information from entity descriptions. However, suffering from the enormous parameters and inflexibility of pre-trained language models, existing text-based methods struggle to balance the textual knowledge and temporal information with computationally expensive purpose-built training strategies. To tap the potential of text-based models for TKGR in various complex scenarios, we propose ChapTER, a Contrastive historical modeling framework with prefix-tuning for TEmporal Reasoning. ChapTER feeds history-contextualized text into the pseudo-Siamese encoders to strike a textual-temporal balance via cont
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#30456;&#23545;&#29109;&#27491;&#21017;&#21270;&#23545;&#19968;&#33324;&#21644;&#24635; $N$-agent &#28216;&#25103;&#30340;&#32435;&#20160;&#22343;&#34913;&#20135;&#29983;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;NE&#31526;&#21512;&#32447;&#24615;&#39640;&#26031;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#25919;&#31574;&#20248;&#21270;&#31639;&#27861;&#20197;&#21450;&#22686;&#24378;&#25216;&#26415;&#26469;&#25214;&#21040;&#28216;&#25103;&#20869;&#30340;NE&#12290;</title><link>https://arxiv.org/abs/2404.00045</link><description>&lt;p&gt;
&#25919;&#31574;&#20248;&#21270;&#22312;&#27491;&#21017;&#21270;&#24191;&#20041;&#21644;&#24635; LQ &#28216;&#25103;&#20013;&#25214;&#21040;&#32435;&#20160;&#22343;&#34913;
&lt;/p&gt;
&lt;p&gt;
Policy Optimization finds Nash Equilibrium in Regularized General-Sum LQ Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00045
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#30456;&#23545;&#29109;&#27491;&#21017;&#21270;&#23545;&#19968;&#33324;&#21644;&#24635; $N$-agent &#28216;&#25103;&#30340;&#32435;&#20160;&#22343;&#34913;&#20135;&#29983;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;NE&#31526;&#21512;&#32447;&#24615;&#39640;&#26031;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#25919;&#31574;&#20248;&#21270;&#31639;&#27861;&#20197;&#21450;&#22686;&#24378;&#25216;&#26415;&#26469;&#25214;&#21040;&#28216;&#25103;&#20869;&#30340;NE&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24341;&#20837;&#30456;&#23545;&#29109;&#27491;&#21017;&#21270;&#23545;&#19968;&#33324;&#21644;&#24635; $N$-agent &#28216;&#25103;&#30340;&#32435;&#20160;&#22343;&#34913; (NE) &#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#36825;&#31867;&#28216;&#25103;&#30340;NE&#31526;&#21512;&#32447;&#24615;&#39640;&#26031;&#31574;&#30053;&#30340;&#20107;&#23454;&#12290;&#27492;&#22806;&#65292;&#23427;&#25551;&#32472;&#20102;&#22312;&#29109;&#27491;&#21017;&#21270;&#30340;&#36866;&#24403;&#24615;&#26041;&#38754;&#65292;&#23545;&#28216;&#25103;&#20869;NE&#29420;&#29305;&#24615;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#30001;&#20110;&#25919;&#31574;&#20248;&#21270;&#26159;&#24378;&#21270;&#23398;&#20064; (RL) &#25216;&#26415;&#30340;&#22522;&#30784;&#26041;&#27861;&#65292;&#26088;&#22312;&#25214;&#21040; NE&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#25919;&#31574;&#20248;&#21270;&#31639;&#27861;&#30340;&#32447;&#24615;&#25910;&#25947;&#24615;&#65292;&#35813;&#31639;&#27861; (&#22312;&#29109;&#27491;&#21017;&#21270;&#30340;&#36866;&#24403;&#24615;&#19979;) &#33021;&#22815;&#26126;&#26174;&#22320;&#23454;&#29616; NE&#12290;&#27492;&#22806;&#65292;&#22312;&#29109;&#27491;&#21017;&#21270;&#35777;&#26126;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010; $\delta$-&#22686;&#24378;&#25216;&#26415;&#65292;&#26377;&#21161;&#20110;&#23454;&#29616;&#28216;&#25103;&#20869;&#30340; $\epsilon$-NE&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00045v1 Announce Type: cross  Abstract: In this paper, we investigate the impact of introducing relative entropy regularization on the Nash Equilibria (NE) of General-Sum $N$-agent games, revealing the fact that the NE of such games conform to linear Gaussian policies. Moreover, it delineates sufficient conditions, contingent upon the adequacy of entropy regularization, for the uniqueness of the NE within the game. As Policy Optimization serves as a foundational approach for Reinforcement Learning (RL) techniques aimed at finding the NE, in this work we prove the linear convergence of a policy optimization algorithm which (subject to the adequacy of entropy regularization) is capable of provably attaining the NE. Furthermore, in scenarios where the entropy regularization proves insufficient, we present a $\delta$-augmentation technique, which facilitates the achievement of an $\epsilon$-NE within the game.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;UAlign&#65292;&#19968;&#31181;&#26080;&#27169;&#26495;&#21270;&#30340;&#22270;&#21040;&#24207;&#21015;&#30340;&#36870;&#21512;&#25104;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#65292;&#21033;&#29992;&#20998;&#23376;&#30340;&#22266;&#26377;&#22270;&#32467;&#26500;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;SMILES&#23545;&#40784;&#25216;&#26415;&#26469;&#20419;&#36827;&#26410;&#25913;&#21464;&#32467;&#26500;&#30340;&#22797;&#29992;&#12290;</title><link>https://arxiv.org/abs/2404.00044</link><description>&lt;p&gt;
UAlign: &#26080;&#27169;&#26495;&#21270;&#30340;&#38750;&#30417;&#30563;&#24335;SMILES&#23545;&#40784;&#25512;&#21160;&#26080;&#27169;&#26495;&#21270;&#36870;&#21512;&#25104;&#39044;&#27979;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
UAlign: Pushing the Limit of Template-free Retrosynthesis Prediction with Unsupervised SMILES Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;UAlign&#65292;&#19968;&#31181;&#26080;&#27169;&#26495;&#21270;&#30340;&#22270;&#21040;&#24207;&#21015;&#30340;&#36870;&#21512;&#25104;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#65292;&#21033;&#29992;&#20998;&#23376;&#30340;&#22266;&#26377;&#22270;&#32467;&#26500;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;SMILES&#23545;&#40784;&#25216;&#26415;&#26469;&#20419;&#36827;&#26410;&#25913;&#21464;&#32467;&#26500;&#30340;&#22797;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#21512;&#25104;&#35268;&#21010;&#22312;&#26377;&#26426;&#21270;&#24037;&#34892;&#19994;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#21046;&#33647;&#39046;&#22495;&#65292;&#38754;&#20020;&#30528;&#24040;&#22823;&#25361;&#25112;&#12290;&#21333;&#27493;&#36870;&#21512;&#25104;&#39044;&#27979;&#26159;&#35268;&#21010;&#36807;&#31243;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#19968;&#27493;&#65292;&#36817;&#24180;&#26469;&#30001;&#20110;&#31185;&#23398;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#27493;&#65292;&#36825;&#19968;&#27493;&#39588;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#27987;&#21402;&#20852;&#36259;&#12290;&#36817;&#24180;&#26469;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#19981;&#21516;&#31243;&#24230;&#30340;&#39069;&#22806;&#21270;&#23398;&#30693;&#35782;&#20381;&#36182;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;UAlign&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#21040;&#24207;&#21015;&#30340;&#26080;&#27169;&#26495;&#21270;&#36870;&#21512;&#25104;&#39044;&#27979;&#31649;&#32447;&#12290;&#36890;&#36807;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#20998;&#23376;&#30340;&#22266;&#26377;&#22270;&#32467;&#26500;&#12290;&#22522;&#20110;&#20998;&#23376;&#32467;&#26500;&#22312;&#21270;&#23398;&#21453;&#24212;&#36807;&#31243;&#20013;&#20445;&#25345;&#19981;&#21464;&#30340;&#20107;&#23454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;SMILES&#23545;&#40784;&#25216;&#26415;&#65292;&#20197;&#20419;&#36827;&#26410;&#25913;&#21464;&#32467;&#26500;&#30340;&#22797;&#29992;&#20197;&#29983;&#25104;&#21453;&#24212;&#29289;&#12290;&#22823;&#37327;&#23454;&#39564;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00044v1 Announce Type: cross  Abstract: Retrosynthesis planning poses a formidable challenge in the organic chemical industry, particularly in pharmaceuticals. Single-step retrosynthesis prediction, a crucial step in the planning process, has witnessed a surge in interest in recent years due to advancements in AI for science. Various deep learning-based methods have been proposed for this task in recent years, incorporating diverse levels of additional chemical knowledge dependency. This paper introduces UAlign, a template-free graph-to-sequence pipeline for retrosynthesis prediction. By combining graph neural networks and Transformers, our method can more effectively leverage the inherent graph structure of molecules. Based on the fact that the majority of molecule structures remain unchanged during a chemical reaction, we propose a simple yet effective SMILES alignment technique to facilitate the reuse of unchanged structures for reactant generation. Extensive experiments 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#20984;&#32422;&#26463;&#30340;&#38543;&#26426;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#28176;&#36817;&#20445;&#35777;&#30340;VRPG&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#21463;&#21040;&#35299;&#20197;&#21450;&#24102;&#20984;&#32422;&#26463;&#35299;&#20915;&#30340;&#38382;&#39064;&#30340;&#32553;&#25918;&#36317;&#31163;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2404.00042</link><description>&lt;p&gt;
&#20855;&#26377;&#32422;&#26463;&#30340;&#38543;&#26426;&#20248;&#21270;&#65306;&#38750;&#28176;&#36817;&#23454;&#20363;&#30456;&#20851;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Stochastic Optimization with Constraints: A Non-asymptotic Instance-Dependent Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00042
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#20984;&#32422;&#26463;&#30340;&#38543;&#26426;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#28176;&#36817;&#20445;&#35777;&#30340;VRPG&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#21463;&#21040;&#35299;&#20197;&#21450;&#24102;&#20984;&#32422;&#26463;&#35299;&#20915;&#30340;&#38382;&#39064;&#30340;&#32553;&#25918;&#36317;&#31163;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#38543;&#26426;&#20984;&#20248;&#21270;&#22312;&#20984;&#32422;&#26463;&#19979;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#36825;&#20010;&#38382;&#39064;&#30340;&#33258;&#28982;&#26041;&#24046;&#20943;&#23569;&#30340;&#36817;&#31471;&#26799;&#24230;&#65288;VRPG&#65289;&#31639;&#27861;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;VRPG&#31639;&#27861;&#30340;&#38750;&#28176;&#36817;&#20445;&#35777;&#12290;&#19982;&#26497;&#23567;&#20540;&#26368;&#22351;&#24773;&#20917;&#20445;&#35777;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#22522;&#20110;&#23454;&#20363;&#30340;&#12290;&#36825;&#24847;&#21619;&#30528;&#25105;&#20204;&#30340;&#20445;&#35777;&#25429;&#25417;&#20102;&#25439;&#22833;&#20989;&#25968;&#30340;&#22797;&#26434;&#24615;&#65292;&#22122;&#22768;&#30340;&#21464;&#24322;&#24615;&#21644;&#32422;&#26463;&#38598;&#30340;&#20960;&#20309;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;VRPG&#31639;&#27861;&#30340;&#38750;&#28176;&#36817;&#24615;&#33021;&#21463;&#32473;&#23450;&#38382;&#39064;&#30340;&#35299;&#21644;&#32473;&#23450;&#20984;&#32422;&#26463;&#19979;&#35299;&#20915;&#30340;&#29305;&#23450;&#23567;&#25200;&#21160;&#38382;&#39064;&#30340;&#35299;&#20043;&#38388;&#30340;&#32553;&#25918;&#36317;&#31163;&#65288;&#30001;$\sqrt{N}$&#32553;&#25918;&#65289;&#30340;&#25511;&#21046;&#65292;&#36825;&#37324;&#65292;$N$&#34920;&#31034;&#26679;&#26412;&#25968;&#12290;&#21033;&#29992;&#23616;&#37096;&#26497;&#23567;&#20540;&#19979;&#30028;&#21644;&#25200;&#21160;&#38382;&#39064;&#35299;&#20043;&#38388;&#30340;&#19968;&#31181;&#25104;&#29087;&#32852;&#31995;&#65292;&#25105;&#20204;&#34920;&#26126;&#24403;$N \rightarrow +\infty$&#26102;&#65292;&#26497;&#23567;&#20540;&#23384;&#22312;&#24182;&#19988;&#21463;&#25351;&#23450;&#20984;&#32422;&#26463;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00042v1 Announce Type: cross  Abstract: We consider the problem of stochastic convex optimization under convex constraints. We analyze the behavior of a natural variance reduced proximal gradient (VRPG) algorithm for this problem. Our main result is a non-asymptotic guarantee for VRPG algorithm. Contrary to minimax worst case guarantees, our result is instance-dependent in nature. This means that our guarantee captures the complexity of the loss function, the variability of the noise, and the geometry of the constraint set. We show that the non-asymptotic performance of the VRPG algorithm is governed by the scaled distance (scaled by $\sqrt{N}$) between the solutions of the given problem and that of a certain small perturbation of the given problem -- both solved under the given convex constraints; here, $N$ denotes the number of samples. Leveraging a well-established connection between local minimax lower bounds and solutions to perturbed problems, we show that as $N \right
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MicroHD&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#38754;&#21521;TinyML&#31995;&#32479;&#30340;&#31934;&#24230;&#39537;&#21160;&#36229;&#39640;&#32500;&#35745;&#31639;&#20248;&#21270;&#26041;&#27861;</title><link>https://arxiv.org/abs/2404.00039</link><description>&lt;p&gt;
MicroHD&#65306;&#38754;&#21521;TinyML&#31995;&#32479;&#30340;&#39640;&#32500;&#36229;&#35745;&#31639;&#31639;&#27861;&#30340;&#31934;&#24230;&#39537;&#21160;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
MicroHD: An Accuracy-Driven Optimization of Hyperdimensional Computing Algorithms for TinyML systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00039
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MicroHD&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#38754;&#21521;TinyML&#31995;&#32479;&#30340;&#31934;&#24230;&#39537;&#21160;&#36229;&#39640;&#32500;&#35745;&#31639;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#32500;&#35745;&#31639;&#65288;HDC&#65289;&#20316;&#20026;&#19968;&#31181;&#33021;&#22815;&#26377;&#25928;&#23450;&#20301;&#21040;TinyML&#24212;&#29992;&#30340;&#26377;&#21069;&#36884;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#27491;&#22312;&#20852;&#36215;&#65292;&#36825;&#35201;&#24402;&#21151;&#20110;&#20854;&#36731;&#37327;&#32423;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#12290;&#22312;HDC&#30340;&#20808;&#21069;&#30740;&#31350;&#20013;&#65292;&#35777;&#26126;&#23558;&#26631;&#20934;&#30340;10k&#32500;&#36229;&#32500;&#31354;&#38388;&#38480;&#21046;&#22312;&#26356;&#20302;&#30340;&#20540;&#26159;&#21487;&#34892;&#30340;&#65292;&#36827;&#19968;&#27493;&#38477;&#20302;HDC&#30340;&#36164;&#28304;&#38656;&#27714;&#12290;&#31867;&#20284;&#22320;&#65292;&#20854;&#20182;&#30740;&#31350;&#34920;&#26126;&#20108;&#36827;&#21046;&#20540;&#21487;&#29992;&#20316;&#29983;&#25104;&#30340;&#36229;&#30690;&#37327;&#30340;&#20803;&#32032;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25928;&#29575;&#65292;&#20195;&#20215;&#26159;&#26576;&#31181;&#31243;&#24230;&#30340;&#31934;&#24230;&#38477;&#20302;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#20248;&#21270;&#23581;&#35797;&#27809;&#26377;&#21516;&#26102;&#21327;&#21516;&#20248;&#21270;HDC&#30340;&#36229;&#21442;&#25968;&#65292;&#24182;&#19988;&#31934;&#24230;&#38477;&#20302;&#19981;&#33021;&#30452;&#25509;&#25511;&#21046;&#65292;&#23548;&#33268;HDC&#27169;&#22411;&#19981;&#22815;&#20248;&#21270;&#65292;&#25552;&#20379;&#20102;&#20960;&#20010;&#36136;&#37327;&#19981;&#21487;&#25509;&#21463;&#30340;&#36755;&#20986;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;MicroHD&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#31934;&#24230;&#39537;&#21160;HDC&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#35843;&#25972;HDC&#30340;&#36229;&#21442;&#25968;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00039v1 Announce Type: cross  Abstract: Hyperdimensional computing (HDC) is emerging as a promising AI approach that can effectively target TinyML applications thanks to its lightweight computing and memory requirements. Previous works on HDC showed that limiting the standard 10k dimensions of the hyperdimensional space to much lower values is possible, reducing even more HDC resource requirements. Similarly, other studies demonstrated that binary values can be used as elements of the generated hypervectors, leading to significant efficiency gains at the cost of some degree of accuracy degradation. Nevertheless, current optimization attempts do not concurrently co-optimize HDC hyper-parameters, and accuracy degradation is not directly controlled, resulting in sub-optimal HDC models providing several applications with unacceptable output qualities. In this work, we propose MicroHD, a novel accuracy-driven HDC optimization approach that iteratively tunes HDC hyper-parameters, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24314;&#31435;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#27010;&#24565;&#21270;&#20114;&#34917;&#24615;&#65292;&#24182;&#30830;&#23450;&#20102;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#20013;&#20114;&#34917;&#22242;&#38431;&#32489;&#25928;&#65288;CTP&#65289;&#30340;&#26469;&#28304;&#12290;</title><link>https://arxiv.org/abs/2404.00029</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#20013;&#30340;&#20114;&#34917;&#24615;&#65306;&#27010;&#24565;&#12289;&#26469;&#28304;&#21644;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Complementarity in Human-AI Collaboration: Concept, Sources, and Evidence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24314;&#31435;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#27010;&#24565;&#21270;&#20114;&#34917;&#24615;&#65292;&#24182;&#30830;&#23450;&#20102;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#20013;&#20114;&#34917;&#22242;&#38431;&#32489;&#25928;&#65288;CTP&#65289;&#30340;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21487;&#20197;&#25552;&#39640;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#20013;&#20154;&#31867;&#20915;&#31574;&#30340;&#33021;&#21147;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#20154;&#31867;&#21644;AI&#20043;&#38388;&#30340;&#21327;&#20316;&#24212;&#35813;&#23548;&#33268;&#20114;&#34917;&#30340;&#22242;&#38431;&#32489;&#25928;&#65288;CTP&#65289;--&#19968;&#31181;&#26082;&#19981;&#26159;&#20182;&#20204;&#20010;&#20307;&#25152;&#33021;&#36798;&#21040;&#30340;&#32489;&#25928;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#24456;&#23569;&#35266;&#23519;&#21040;CTP&#65292;&#36825;&#34920;&#26126;&#23545;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#20013;&#30340;&#20114;&#34917;&#32452;&#20998;&#30340;&#29702;&#35299;&#19981;&#22815;&#65292;&#36825;&#20123;&#32452;&#20998;&#21487;&#20197;&#20419;&#36827;&#20915;&#31574;&#20013;&#30340;CTP&#12290;&#26412;&#30740;&#31350;&#20026;&#29702;&#35299;&#21644;&#21457;&#23637;&#20154;&#24037;&#26234;&#33021;&#20114;&#34917;&#24615;&#22880;&#23450;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#21644;&#24418;&#24335;&#21270;&#20114;&#34917;&#24615;&#28508;&#21147;&#21450;&#20854;&#23454;&#29616;&#30340;&#27010;&#24565;&#65292;&#27010;&#24565;&#21270;&#20114;&#34917;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#24182;&#27010;&#36848;&#20102;&#35299;&#37322;CTP&#30340;&#26469;&#28304;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20004;&#39033;&#23454;&#35777;&#30740;&#31350;&#20013;&#24212;&#29992;&#25105;&#20204;&#30340;&#27010;&#24565;&#26469;&#35828;&#26126;&#25105;&#20204;&#30340;&#27010;&#24565;&#12290;&#22312;&#31532;&#19968;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#20449;&#24687;&#19981;&#23545;&#31216;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00029v1 Announce Type: cross  Abstract: Artificial intelligence (AI) can improve human decision-making in various application areas. Ideally, collaboration between humans and AI should lead to complementary team performance (CTP) -- a level of performance that neither of them can attain individually. So far, however, CTP has rarely been observed, suggesting an insufficient understanding of the complementary constituents in human-AI collaboration that can contribute to CTP in decision-making. This work establishes a holistic theoretical foundation for understanding and developing human-AI complementarity. We conceptualize complementarity by introducing and formalizing the notion of complementarity potential and its realization. Moreover, we identify and outline sources that explain CTP. We illustrate our conceptualization by applying it in two empirical studies exploring two different sources of complementarity potential. In the first study, we focus on information asymmetry 
&lt;/p&gt;</description></item><item><title>&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#24341;&#21457;&#30340;&#20889;&#20316;&#25152;&#26377;&#26435;&#24863;&#21644;&#20316;&#32773;&#36523;&#20221;&#35748;&#30693;&#20043;&#38388;&#30340;&#24515;&#29702;&#22256;&#22659;&#12290;</title><link>https://arxiv.org/abs/2404.00027</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#65306;&#25506;&#35752;&#25152;&#26377;&#26435;&#24863;&#21644;&#25512;&#29702;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership and Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00027
&lt;/p&gt;
&lt;p&gt;
&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#24341;&#21457;&#30340;&#20889;&#20316;&#25152;&#26377;&#26435;&#24863;&#21644;&#20316;&#32773;&#36523;&#20221;&#35748;&#30693;&#20043;&#38388;&#30340;&#24515;&#29702;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20889;&#20316;&#20013;&#30340;&#25152;&#26377;&#26435;&#24863;&#38480;&#21046;&#20102;&#25105;&#20204;&#23545;&#24605;&#24819;&#12289;&#26102;&#38388;&#21644;&#36129;&#29486;&#30340;&#25237;&#20837;&#65292;&#23548;&#33268;&#23545;&#20135;&#20986;&#29289;&#30340;&#20381;&#24651;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20889;&#20316;&#21161;&#25163;&#24341;&#20837;&#20102;&#19968;&#31181;&#24515;&#29702;&#22256;&#22659;&#65292;&#22240;&#20026;&#19968;&#20123;&#20869;&#23481;&#24182;&#38750;&#30452;&#25509;&#25105;&#20204;&#30340;&#21019;&#20316;&#12290;&#25105;&#20204;&#24448;&#24448;&#26356;&#20542;&#21521;&#20110;&#22312;&#21019;&#36896;&#24615;&#20219;&#21153;&#20013;&#26356;&#22810;&#22320;&#24402;&#21151;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#23613;&#31649;&#23427;&#20204;&#23545;&#25152;&#26377;&#20219;&#21153;&#37117;&#26159;&#24179;&#31561;&#30340;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#25105;&#20204;&#21487;&#33021;&#19981;&#20250;&#23436;&#20840;&#22768;&#31216;&#23545;&#30001;LLM&#29983;&#25104;&#30340;&#20869;&#23481;&#25317;&#26377;&#25152;&#26377;&#26435;&#65292;&#20294;&#21364;&#33258;&#30001;&#22320;&#22768;&#31216;&#20316;&#32773;&#36523;&#20221;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#31616;&#30701;&#35843;&#26597;&#26469;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#20102;&#35299;&#28508;&#22312;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#20154;&#26426;&#20132;&#20114;&#22312;&#20889;&#20316;&#20013;&#30340;&#24212;&#29992;&#24182;&#25913;&#36827;&#20889;&#20316;&#36741;&#21161;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00027v1 Announce Type: cross  Abstract: Sense of ownership in writing confines our investment of thoughts, time, and contribution, leading to attachment to the output. However, using writing assistants introduces a mental dilemma, as some content isn't directly our creation. For instance, we tend to credit Large Language Models (LLMs) more in creative tasks, even though all tasks are equal for them. Additionally, while we may not claim complete ownership of LLM-generated content, we freely claim authorship. We conduct a short survey to examine these issues and understand underlying cognitive processes in order to gain a better knowledge of human-computer interaction in writing and improve writing aid systems.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#20204;&#26085;&#30410;&#20381;&#36182;&#30340;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#23545;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#21487;&#33021;&#36896;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#26088;&#22312;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#25552;&#21319;&#20889;&#20316;&#21161;&#25163;&#30340;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00026</link><description>&lt;p&gt;
&#22696;&#27700;&#19982;&#20010;&#24615;&#65306;&#22312;LLMs&#26102;&#20195;&#22609;&#36896;&#20010;&#24615;&#21270;&#21465;&#20107;
&lt;/p&gt;
&lt;p&gt;
Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00026
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#20204;&#26085;&#30410;&#20381;&#36182;&#30340;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#23545;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#21487;&#33021;&#36896;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#26088;&#22312;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#25552;&#21319;&#20889;&#20316;&#21161;&#25163;&#30340;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21644;&#20010;&#24615;&#21270;&#26500;&#25104;&#20102;&#20351;&#27599;&#20010;&#20316;&#23478;&#29420;&#29305;&#24182;&#24433;&#21709;&#20854;&#25991;&#23383;&#20197;&#26377;&#25928;&#21560;&#24341;&#35835;&#32773;&#21516;&#26102;&#20256;&#36798;&#30495;&#23454;&#24615;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#26085;&#30410;&#20381;&#36182;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#21487;&#33021;&#20250;&#21361;&#21450;&#25105;&#20204;&#30340;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#12290;&#25105;&#20204;&#32463;&#24120;&#24573;&#35270;&#36825;&#19968;&#36235;&#21183;&#23545;&#25105;&#20204;&#30340;&#21019;&#36896;&#21147;&#21644;&#29420;&#29305;&#24615;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#23613;&#31649;&#21487;&#33021;&#20250;&#36896;&#25104;&#21518;&#26524;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#36827;&#34892;&#31616;&#35201;&#35843;&#26597;&#25506;&#32034;&#19981;&#21516;&#30340;&#35266;&#28857;&#21644;&#27010;&#24565;&#65292;&#20197;&#21450;&#23581;&#35797;&#29702;&#35299;&#20154;&#20204;&#30340;&#35266;&#28857;&#65292;&#32467;&#21512;&#20197;&#24448;&#22312;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#26469;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#12290;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#23545;&#20110;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#22686;&#24378;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#20889;&#20316;&#21161;&#25163;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00026v1 Announce Type: cross  Abstract: Individuality and personalization comprise the distinctive characteristics that make each writer unique and influence their words in order to effectively engage readers while conveying authenticity. However, our growing reliance on LLM-based writing assistants risks compromising our creativity and individuality over time. We often overlook the negative impacts of this trend on our creativity and uniqueness, despite the possible consequences. This study investigates these concerns by performing a brief survey to explore different perspectives and concepts, as well as trying to understand people's viewpoints, in conjunction with past studies in the area. Addressing these issues is essential for improving human-computer interaction systems and enhancing writing assistants for personalization and individuality.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35843;&#26597;&#19982;&#20844;&#20849;&#37096;&#38376;&#31639;&#27861;&#31995;&#32479;&#30456;&#20851;&#30340;&#20174;&#19994;&#32773;&#36827;&#34892;&#35775;&#35848;&#65292;&#30740;&#31350;AI&#20844;&#24179;&#30456;&#20851;&#20915;&#31574;&#30340;&#27807;&#36890;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#23450;&#24615;&#32534;&#30721;&#20998;&#26512;&#65292;&#30830;&#23450;&#25903;&#25745;&#20844;&#24179;&#20915;&#31574;&#30340;&#20851;&#38190;&#27807;&#36890;&#20803;&#32032;&#12290;</title><link>https://arxiv.org/abs/2404.00022</link><description>&lt;p&gt;
&#20998;&#26512;&#21644;&#32452;&#32455;&#20154;&#31867;&#27807;&#36890;&#20197;&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#19982;&#20844;&#24179;&#20915;&#31574;&#30456;&#20851;&#30340;&#20915;&#31574;&#65306;&#26469;&#33258;&#20844;&#20849;&#37096;&#38376;&#30340;&#20351;&#29992;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Analysing and Organising Human Communications for AI Fairness-Related Decisions: Use Cases from the Public Sector
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00022
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35843;&#26597;&#19982;&#20844;&#20849;&#37096;&#38376;&#31639;&#27861;&#31995;&#32479;&#30456;&#20851;&#30340;&#20174;&#19994;&#32773;&#36827;&#34892;&#35775;&#35848;&#65292;&#30740;&#31350;AI&#20844;&#24179;&#30456;&#20851;&#20915;&#31574;&#30340;&#27807;&#36890;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#23450;&#24615;&#32534;&#30721;&#20998;&#26512;&#65292;&#30830;&#23450;&#25903;&#25745;&#20844;&#24179;&#20915;&#31574;&#30340;&#20851;&#38190;&#27807;&#36890;&#20803;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20844;&#20849;&#37096;&#38376;&#20351;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#20013;&#65292;&#20363;&#22914;&#20998;&#37197;&#31038;&#20250;&#31119;&#21033;&#25110;&#39044;&#27979;&#27450;&#35784;&#65292;&#36890;&#24120;&#28041;&#21450;&#22810;&#20010;&#20844;&#20849;&#21644;&#31169;&#20154;&#21033;&#30410;&#30456;&#20851;&#32773;&#22312;&#31639;&#27861;&#29983;&#21629;&#21608;&#26399;&#30340;&#21508;&#20010;&#38454;&#27573;&#12290;&#36825;&#20123;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#20043;&#38388;&#30340;&#27807;&#36890;&#38382;&#39064;&#21487;&#33021;&#23548;&#33268;&#31639;&#27861;&#30340;&#35823;&#35299;&#21644;&#35823;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#19982;&#22312;&#20844;&#20849;&#37096;&#38376;&#31639;&#27861;&#31995;&#32479;&#19978;&#24037;&#20316;&#30340;&#20174;&#19994;&#32773;&#36827;&#34892;&#35775;&#35848;&#65292;&#35843;&#26597;&#19982;AI&#20844;&#24179;&#30456;&#20851;&#20915;&#31574;&#26377;&#20851;&#30340;&#27807;&#36890;&#36807;&#31243;&#12290;&#36890;&#36807;&#24212;&#29992;&#23450;&#24615;&#32534;&#30721;&#20998;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#25903;&#25745;&#19982;&#20844;&#24179;&#30456;&#20851;&#30340;&#20154;&#31867;&#20915;&#31574;&#30340;&#27807;&#36890;&#36807;&#31243;&#30340;&#20851;&#38190;&#20803;&#32032;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#21033;&#30410;&#30456;&#20851;&#32773;&#24863;&#30693;&#30340;&#35282;&#33394;&#12289;&#20219;&#21153;&#12289;&#25216;&#33021;&#21644;&#25361;&#25112;&#30340;&#20998;&#24037;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#27010;&#24565;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#28508;&#22312;&#30340;&#27807;&#36890;&#38382;&#39064;&#65292;&#20854;&#20013;i.&#20195;&#34920;&#20102;&#27807;&#36890;&#27169;&#24335; ii.&#27010;&#36848;&#20102;&#32570;&#22833;&#20803;&#32032;&#65292;&#27604;&#22914;&#32570;&#20047;&#20026;&#20219;&#21153;&#25552;&#20379;&#25216;&#33021;&#30340;&#35282;&#33394;&#12290;&#35813;&#26694;&#26550;&#34987;&#29992;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00022v1 Announce Type: cross  Abstract: AI algorithms used in the public sector, e.g., for allocating social benefits or predicting fraud, often involve multiple public and private stakeholders at various phases of the algorithm's life-cycle. Communication issues between these diverse stakeholders can lead to misinterpretation and misuse of algorithms. We investigate the communication processes for AI fairness-related decisions by conducting interviews with practitioners working on algorithmic systems in the public sector. By applying qualitative coding analysis, we identify key elements of communication processes that underlie fairness-related human decisions. We analyze the division of roles, tasks, skills, and challenges perceived by stakeholders. We formalize the underlying communication issues within a conceptual framework that i. represents the communication patterns ii. outlines missing elements, such as actors who miss skills for their tasks. The framework is used fo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32508;&#36848;&#20102;&#29616;&#26377;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#35299;&#37322;&#26041;&#27861;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26410;&#26469;&#30740;&#31350;&#36335;&#32447;&#22270;&#65292;&#37325;&#28857;&#25918;&#22312;&#20102;&#20102;&#35299;&#20132;&#27969;&#23545;&#35937;&#21644;&#29983;&#25104;&#21450;&#26102;&#35299;&#37322;&#19978;&#12290;</title><link>https://arxiv.org/abs/2404.00019</link><description>&lt;p&gt;
&#25512;&#36827;&#21487;&#35299;&#37322;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#31995;&#32479;&#65306;&#32508;&#36848;&#19982;&#30740;&#31350;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
Advancing Explainable Autonomous Vehicle Systems: A Comprehensive Review and Research Roadmap
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32508;&#36848;&#20102;&#29616;&#26377;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#35299;&#37322;&#26041;&#27861;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26410;&#26469;&#30740;&#31350;&#36335;&#32447;&#22270;&#65292;&#37325;&#28857;&#25918;&#22312;&#20102;&#20102;&#35299;&#20132;&#27969;&#23545;&#35937;&#21644;&#29983;&#25104;&#21450;&#26102;&#35299;&#37322;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#29616;&#26377;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#35299;&#37322;&#26041;&#27861;&#22914;&#20309;&#28385;&#36275;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#22810;&#26679;&#38656;&#27714;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#65292;&#24517;&#39035;&#36827;&#34892;&#28145;&#20837;&#35843;&#26597;&#20197;&#30830;&#23450;&#38656;&#35201;&#35299;&#37322;&#30340;&#24773;&#22659;&#21644;&#36866;&#24403;&#30340;&#20114;&#21160;&#31574;&#30053;&#12290;&#19968;&#39033;&#20840;&#38754;&#30340;&#32508;&#36848;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#35780;&#20272;&#24403;&#21069;&#26041;&#27861;&#19982;AV&#29983;&#24577;&#31995;&#32479;&#20869;&#19981;&#21516;&#21033;&#30410;&#21644;&#26399;&#26395;&#30340;&#19968;&#33268;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#39033;&#32508;&#36848;&#65292;&#35752;&#35770;&#20102;&#29983;&#25104;&#21644;&#21576;&#29616;&#35299;&#37322;&#25152;&#28041;&#21450;&#30340;&#22797;&#26434;&#24615;&#65292;&#20197;&#20419;&#36827;&#24320;&#21457;&#26356;&#21152;&#26377;&#25928;&#21644;&#21253;&#23481;&#30340;&#21487;&#35299;&#37322;AV&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#23558;&#29616;&#26377;&#25991;&#29486;&#20998;&#31867;&#20026;&#19977;&#20010;&#20027;&#35201;&#20027;&#39064;&#65306;&#35299;&#37322;&#20219;&#21153;&#12289;&#35299;&#37322;&#20449;&#24687;&#21644;&#35299;&#37322;&#20449;&#24687;&#20256;&#36798;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26410;&#26469;&#30740;&#31350;&#30340;&#20840;&#38754;&#36335;&#32447;&#22270;&#65292;&#38598;&#20013;&#22312;&#65288;i&#65289;&#20102;&#35299;&#20132;&#27969;&#23545;&#35937;&#65292;&#65288;ii&#65289;&#29983;&#25104;&#21450;&#26102;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00019v1 Announce Type: cross  Abstract: Given the uncertainty surrounding how existing explainability methods for autonomous vehicles (AVs) meet the diverse needs of stakeholders, a thorough investigation is imperative to determine the contexts requiring explanations and suitable interaction strategies. A comprehensive review becomes crucial to assess the alignment of current approaches with the varied interests and expectations within the AV ecosystem. This study presents a review to discuss the complexities associated with explanation generation and presentation to facilitate the development of more effective and inclusive explainable AV systems. Our investigation led to categorising existing literature into three primary topics: explanatory tasks, explanatory information, and explanatory information communication. Drawing upon our insights, we have proposed a comprehensive roadmap for future research centred on (i) knowing the interlocutor, (ii) generating timely explanat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#20154;&#24037;&#26234;&#33021;&#30456;&#23545;&#20110;&#20154;&#31867;&#19987;&#23478;&#22312;&#21019;&#24847;&#39046;&#22495;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#25552;&#31034;&#26469;&#29983;&#25104;&#31038;&#20132;&#23186;&#20307;&#21019;&#24847;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.00018</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#33021;&#21542;&#36229;&#36234;&#20154;&#31867;&#19987;&#23478;&#22312;&#21019;&#24314;&#31038;&#20132;&#23186;&#20307;&#21019;&#24847;&#26041;&#38754;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can AI Outperform Human Experts in Creating Social Media Creatives?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#20154;&#24037;&#26234;&#33021;&#30456;&#23545;&#20110;&#20154;&#31867;&#19987;&#23478;&#22312;&#21019;&#24847;&#39046;&#22495;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#25552;&#31034;&#26469;&#29983;&#25104;&#31038;&#20132;&#23186;&#20307;&#21019;&#24847;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#35937;&#26827;&#21644;&#22260;&#26827;&#31561;&#21151;&#33021;&#24615;&#20219;&#21153;&#20013;&#24050;&#32463;&#36229;&#36234;&#20102;&#20154;&#31867;&#19987;&#23478;&#12290;&#37027;&#20040;&#22312;&#21019;&#24847;&#20219;&#21153;&#20013;&#21602;&#65311;&#26412;&#25991;&#35780;&#20272;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#21019;&#24847;&#39046;&#22495;&#30340;&#33021;&#21147;&#19982;&#20154;&#31867;&#19987;&#23478;&#30456;&#27604;&#65292;&#36804;&#20170;&#20026;&#27490;&#24456;&#23569;&#26377;&#30740;&#31350;&#36827;&#34892;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Prompt-for-Prompt&#26041;&#27861;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#22686;&#24378;&#29983;&#25104;&#31038;&#20132;&#23186;&#20307;&#21019;&#24847;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;&#39030;&#32423;&#21697;&#29260;Instagram&#36134;&#21495;&#20013;&#26368;&#21463;&#27426;&#36814;&#30340;&#24086;&#23376;&#65288;&#33719;&#24471;&#26368;&#22810;&#28857;&#36190;&#28857;&#20987;&#65289;&#26469;&#21019;&#24314;&#31038;&#20132;&#23186;&#20307;&#21019;&#24847;&#12290;&#25105;&#20204;&#32473;GPT 4&#25552;&#20379;&#20102;&#20960;&#20010;&#24102;&#26377;&#25991;&#26412;&#25551;&#36848;&#30340;&#25552;&#31034;&#35828;&#26126;&#65292;&#20197;&#29983;&#25104;&#29992;&#20110;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;&#65288;Midjourney&#12289;DALL E 3&#21644;Stable Diffusion&#65289;&#30340;&#26368;&#26377;&#25928;&#25552;&#31034;&#12290;&#36890;&#36807;LLM&#22686;&#24378;&#30340;&#25552;&#31034;&#21487;&#20197;&#36890;&#36807;&#28155;&#21152;&#30446;&#26631;&#12289;&#21442;&#19982;&#31574;&#30053;&#12289;&#28783;&#20809;&#21644;&#21697;&#29260;&#19968;&#33268;&#24615;&#26469;&#25552;&#21319;&#20154;&#24037;&#26234;&#33021;&#22312;&#31038;&#20132;&#23186;&#20307;&#22270;&#20687;&#21019;&#20316;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;&#20154;&#31867;&#35780;&#20272;&#23454;&#39564;&#65292;&#21457;&#29616;&#20154;&#24037;&#26234;&#33021;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00018v1 Announce Type: cross  Abstract: Artificial Intelligence has outperformed human experts in functional tasks such as chess and baduk. How about creative tasks? This paper evaluates AI's capability in the creative domain compared to human experts, which little research has been conducted so far. We propose a novel Prompt-for-Prompt to generate social media creatives via prompt augmentation by Large Language Models. We take the most popular Instagram posts (with the biggest number of like clicks) in top brands' Instagram accounts to create social media creatives. We give GPT 4 several prompt instructions with text descriptions to generate the most effective prompts for cutting-edge text-to-image generators: Midjourney, DALL E 3, and Stable Diffusion. LLM-augmented prompts can boost AI's abilities by adding objectives, engagement strategy, lighting and brand consistency for social media image creation. We conduct an extensive human evaluation experiment, and find that AI 
&lt;/p&gt;</description></item><item><title>AI&#31995;&#32479;&#22312;&#29983;&#25104;&#39033;&#30446;&#26631;&#39064;&#26041;&#38754;&#23637;&#29616;&#20986;&#29420;&#29305;&#30340;&#21019;&#26032;&#33021;&#21147;&#65292;&#21363;&#20351;&#22312;&#20219;&#21153;&#22797;&#26434;&#24615;&#22686;&#21152;&#21644;&#35745;&#31639;&#33021;&#21147;&#26497;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;&#30340;&#20869;&#23481;&#20855;&#26377;&#34920;&#38754;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00017</link><description>&lt;p&gt;
&#21019;&#26032;&#30340;&#40550;&#40521;&#65311;&#35780;&#20272;AI&#21019;&#20316;&#30340;&#30495;&#27491;&#26032;&#39062;&#24615;
&lt;/p&gt;
&lt;p&gt;
Psittacines of Innovation? Assessing the True Novelty of AI Creations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00017
&lt;/p&gt;
&lt;p&gt;
AI&#31995;&#32479;&#22312;&#29983;&#25104;&#39033;&#30446;&#26631;&#39064;&#26041;&#38754;&#23637;&#29616;&#20986;&#29420;&#29305;&#30340;&#21019;&#26032;&#33021;&#21147;&#65292;&#21363;&#20351;&#22312;&#20219;&#21153;&#22797;&#26434;&#24615;&#22686;&#21152;&#21644;&#35745;&#31639;&#33021;&#21147;&#26497;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;&#30340;&#20869;&#23481;&#20855;&#26377;&#34920;&#38754;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26816;&#39564;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#26159;&#21542;&#29983;&#25104;&#30495;&#27491;&#26032;&#39062;&#30340;&#24819;&#27861;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22312;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#21040;&#30340;&#27169;&#24335;&#12290;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#39564;&#35774;&#35745;&#65292;&#25105;&#20204;&#35753;&#19968;&#20010;AI&#29983;&#25104;&#34394;&#26500;&#30340;&#20247;&#31609;&#27963;&#21160;&#39033;&#30446;&#26631;&#39064;&#12290;&#25105;&#20204;&#27604;&#36739;AI&#29983;&#25104;&#30340;&#39033;&#30446;&#26631;&#39064;&#20869;&#37096;&#65292;&#34913;&#37327;&#37325;&#22797;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#25193;&#23637;&#30340;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#26041;&#27861;&#65292;&#22312;AI&#29983;&#25104;&#30340;&#26631;&#39064;&#21644;&#23454;&#38469;&#35266;&#27979;&#30340;&#29616;&#22330;&#25968;&#25454;&#20043;&#38388;&#36827;&#34892;&#27604;&#36739;&#65292;&#35813;&#26041;&#27861;&#26159;&#20174;&#23558;&#32479;&#35745;&#20998;&#24067;&#30340;&#26680;&#22343;&#20540;&#23884;&#20837;&#24212;&#29992;&#21040;&#39640;&#32500;&#26426;&#22120;&#23398;&#20064;&#65288;&#22823;&#35821;&#35328;&#65289;&#23884;&#20837;&#21521;&#37327;&#20013;&#25512;&#23548;&#20986;&#26469;&#30340;--&#20174;&#32780;&#24471;&#21040;&#23545;AI&#36755;&#20986;&#26032;&#39062;&#24615;&#30340;&#32467;&#26500;&#21270;&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;1&#65289;&#21363;&#20351;&#22312;&#20219;&#21153;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#33021;&#21147;&#30340;&#26497;&#38480;&#19979;&#65292;AI&#20063;&#20250;&#29983;&#25104;&#29420;&#29305;&#20869;&#23481;&#65292;&#65288;2&#65289;&#29983;&#25104;&#30340;&#20869;&#23481;&#20855;&#26377;&#34920;&#38754;&#26377;&#25928;&#24615;&#65292;&#19982;&#36865;&#20837;&#20854;&#20182;&#29983;&#25104;AI&#30340;&#36755;&#20837;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00017v1 Announce Type: new  Abstract: We examine whether Artificial Intelligence (AI) systems generate truly novel ideas rather than merely regurgitating patterns learned during training. Utilizing a novel experimental design, we task an AI with generating project titles for hypothetical crowdfunding campaigns. We compare within AI-generated project titles, measuring repetition and complexity. We compare between the AI-generated titles and actual observed field data using an extension of maximum mean discrepancy--a metric derived from the application of kernel mean embeddings of statistical distributions to high-dimensional machine learning (large language) embedding vectors--yielding a structured analysis of AI output novelty. Results suggest that (1) the AI generates unique content even under increasing task complexity, and at the limits of its computational capabilities, (2) the generated content has face validity, being consistent with both inputs to other generative AI 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#28145;&#24230;&#20960;&#20309;&#22788;&#29702;&#21327;&#35758;&#65292;&#36890;&#36807;&#24341;&#20837;&#29255;&#27573;&#24335;&#29983;&#25104;&#33539;&#24335;&#35299;&#20915;&#20102;&#20849;&#21516;&#25361;&#25112;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#20960;&#20309;&#21487;&#38752;&#30340;&#20998;&#23376;&#19977;&#32500;&#22270;&#29983;&#25104;&#26041;&#27861;</title><link>https://arxiv.org/abs/2404.00014</link><description>&lt;p&gt;
&#28145;&#24230;&#20960;&#20309;&#22788;&#29702;&#19982;&#22522;&#20110;&#29255;&#27573;&#30340;&#20998;&#23376;&#19977;&#32500;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Deep Geometry Handling and Fragment-wise Molecular 3D Graph Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00014
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#28145;&#24230;&#20960;&#20309;&#22788;&#29702;&#21327;&#35758;&#65292;&#36890;&#36807;&#24341;&#20837;&#29255;&#27573;&#24335;&#29983;&#25104;&#33539;&#24335;&#35299;&#20915;&#20102;&#20849;&#21516;&#25361;&#25112;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#20960;&#20309;&#21487;&#38752;&#30340;&#20998;&#23376;&#19977;&#32500;&#22270;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#26089;&#26399;&#22522;&#20110;3D&#32467;&#26500;&#30340;&#20998;&#23376;&#29983;&#25104;&#26041;&#27861;&#36981;&#24490;&#36880;&#20010;&#21407;&#23376;&#30340;&#33539;&#24335;&#65292;&#22312;&#34507;&#30333;&#36136;&#21475;&#34955;&#20869;&#36880;&#27493;&#28155;&#21152;&#21407;&#23376;&#21040;&#37096;&#20998;&#26500;&#24314;&#30340;&#20998;&#23376;&#29255;&#27573;&#20013;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#35774;&#35745;&#32039;&#23494;&#32467;&#21512;&#30340;&#37197;&#20307;&#26041;&#38754;&#26377;&#25928;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;&#20854;&#20182;&#37325;&#35201;&#24615;&#36136;&#65292;&#22914;&#21487;&#21512;&#25104;&#24615;&#12290;&#29255;&#27573;&#24335;&#29983;&#25104;&#33539;&#24335;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#21407;&#23376;&#24335;&#21644;&#29255;&#27573;&#24335;&#26041;&#27861;&#37117;&#38754;&#20020;&#19968;&#20010;&#20849;&#21516;&#30340;&#25361;&#25112;&#65292;&#21363;&#22312;&#26377;&#38480;&#30340;&#33021;&#21147;&#19979;&#20849;&#21516;&#35774;&#35745;&#20986;&#21512;&#29702;&#30340;&#21270;&#23398;&#21644;&#20960;&#20309;&#32467;&#26500;&#65292;&#23548;&#33268;&#25197;&#26354;&#30340;&#26500;&#35937;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#28145;&#24230;&#20960;&#20309;&#22788;&#29702;&#21327;&#35758;&#65292;&#36825;&#26159;&#19968;&#20010;&#26356;&#25277;&#35937;&#30340;&#35774;&#35745;&#65292;&#23558;&#35774;&#35745;&#37325;&#28857;&#24310;&#20280;&#21040;&#27169;&#22411;&#26550;&#26500;&#20043;&#22806;&#12290;&#36890;&#36807;&#23545;&#29616;&#26377;&#20960;&#20309;&#30456;&#20851;&#27169;&#22411;&#21450;&#20854;&#21327;&#35758;&#30340;&#20840;&#38754;&#23457;&#26597;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#31574;&#30053;&#65292;&#26368;&#32456;&#24320;&#21457;&#20986;FragGen - &#19968;&#31181;&#20960;&#20309;&#21487;&#38752;&#30340;&#29255;&#27573;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00014v1 Announce Type: cross  Abstract: Most earlier 3D structure-based molecular generation approaches follow an atom-wise paradigm, incrementally adding atoms to a partially built molecular fragment within protein pockets. These methods, while effective in designing tightly bound ligands, often overlook other essential properties such as synthesizability. The fragment-wise generation paradigm offers a promising solution. However, a common challenge across both atom-wise and fragment-wise methods lies in their limited ability to co-design plausible chemical and geometrical structures, resulting in distorted conformations. In response to this challenge, we introduce the Deep Geometry Handling protocol, a more abstract design that extends the design focus beyond the model architecture. Through a comprehensive review of existing geometry-related models and their protocols, we propose a novel hybrid strategy, culminating in the development of FragGen - a geometry-reliable, frag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#31934;&#30830;&#35821;&#20041;&#30340;&#32570;&#22833;&#25968;&#25454;&#25554;&#34917;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#31890;&#31354;&#38388;&#20013;&#21033;&#29992;&#29305;&#24449;&#35821;&#20041;&#21644;&#21487;&#38752;&#35266;&#27979;&#26469;&#39044;&#27979;&#32570;&#22833;&#20540;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#30772;&#20135;&#39044;&#27979;&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2404.00013</link><description>&lt;p&gt;
&#20855;&#26377;&#31934;&#30830;&#35821;&#20041;&#21644;AI&#39537;&#21160;&#27969;&#31243;&#30340;&#32570;&#22833;&#25968;&#25454;&#25554;&#34917;&#21450;&#30772;&#20135;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Missing Data Imputation With Granular Semantics and AI-driven Pipeline for Bankruptcy Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#31934;&#30830;&#35821;&#20041;&#30340;&#32570;&#22833;&#25968;&#25454;&#25554;&#34917;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#31890;&#31354;&#38388;&#20013;&#21033;&#29992;&#29305;&#24449;&#35821;&#20041;&#21644;&#21487;&#38752;&#35266;&#27979;&#26469;&#39044;&#27979;&#32570;&#22833;&#20540;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#30772;&#20135;&#39044;&#27979;&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30528;&#37325;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#39044;&#27979;&#30772;&#20135;&#30340;&#27969;&#31243;&#12290;&#32570;&#22833;&#20540;&#12289;&#39640;&#32500;&#25968;&#25454;&#20197;&#21450;&#39640;&#24230;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#24211;&#26159;&#35813;&#20219;&#21153;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#31934;&#30830;&#35821;&#20041;&#30340;&#32570;&#22833;&#25968;&#25454;&#25554;&#34917;&#26032;&#26041;&#27861;&#12290;&#25506;&#35752;&#20102;&#31890;&#35745;&#31639;&#30340;&#20248;&#28857;&#20197;&#23450;&#20041;&#27492;&#26041;&#27861;&#12290;&#21033;&#29992;&#29305;&#24449;&#35821;&#20041;&#21644;&#21487;&#38752;&#35266;&#27979;&#22312;&#20302;&#32500;&#31354;&#38388;&#12289;&#31890;&#31354;&#38388;&#20013;&#39044;&#27979;&#32570;&#22833;&#20540;&#12290;&#22260;&#32469;&#27599;&#20010;&#32570;&#22833;&#26465;&#30446;&#24418;&#25104;&#31890;&#23376;&#65292;&#32771;&#34385;&#21040;&#19968;&#20123;&#39640;&#24230;&#30456;&#20851;&#30340;&#29305;&#24449;&#21644;&#26368;&#21487;&#38752;&#30340;&#26368;&#36817;&#35266;&#27979;&#20197;&#20445;&#25345;&#25968;&#25454;&#24211;&#23545;&#32570;&#22833;&#26465;&#30446;&#30340;&#30456;&#20851;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#28982;&#21518;&#22312;&#36825;&#20123;&#19978;&#19979;&#25991;&#31890;&#23376;&#20013;&#36827;&#34892;&#36328;&#31890;&#23376;&#39044;&#27979;&#36827;&#34892;&#25554;&#34917;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#19978;&#19979;&#25991;&#31890;&#23376;&#20351;&#24471;&#22312;&#24040;&#22823;&#25968;&#25454;&#24211;&#20013;&#36827;&#34892;&#19968;&#23567;&#37096;&#20998;&#30456;&#20851;&#30340;&#25554;&#34917;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00013v1 Announce Type: cross  Abstract: This work focuses on designing a pipeline for the prediction of bankruptcy. The presence of missing values, high dimensional data, and highly class-imbalance databases are the major challenges in the said task. A new method for missing data imputation with granular semantics has been introduced here. The merits of granular computing have been explored here to define this method. The missing values have been predicted using the feature semantics and reliable observations in a low-dimensional space, in the granular space. The granules are formed around every missing entry, considering a few of the highly correlated features and most reliable closest observations to preserve the relevance and reliability, the context, of the database against the missing entries. An intergranular prediction is then carried out for the imputation within those contextual granules. That is, the contextual granules enable a small relevant fraction of the huge 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#37329;&#34701;&#21387;&#21147;&#25351;&#26631;&#19982;&#36130;&#32463;&#26032;&#38395;&#24773;&#24863;&#20998;&#26512;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#32929;&#31080;&#24066;&#22330;&#30340;&#39118;&#38505;&#25511;&#21046;&#31574;&#30053;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2404.00012</link><description>&lt;p&gt;
&#22522;&#20110;&#36130;&#32463;&#26032;&#38395;&#24773;&#24863;&#20998;&#26512;&#30340;&#32929;&#31080;&#24066;&#22330;&#21387;&#21147;&#25351;&#25968;&#31574;&#30053;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Stress index strategy enhanced with financial news sentiment analysis for the equity markets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00012
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#37329;&#34701;&#21387;&#21147;&#25351;&#26631;&#19982;&#36130;&#32463;&#26032;&#38395;&#24773;&#24863;&#20998;&#26512;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#32929;&#31080;&#24066;&#22330;&#30340;&#39118;&#38505;&#25511;&#21046;&#31574;&#30053;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#32929;&#24066;&#39118;&#38505;-&#39118;&#38505;&#31574;&#30053;&#65292;&#23558;&#37329;&#34701;&#21387;&#21147;&#25351;&#26631;&#19982;&#36890;&#36807;ChatGPT&#35835;&#21462;&#21644;&#35299;&#37322;Bloomberg&#27599;&#26085;&#24066;&#22330;&#25688;&#35201;&#36827;&#34892;&#30340;&#24773;&#24863;&#20998;&#26512;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#23558;&#20174;&#27874;&#21160;&#29575;&#21644;&#20449;&#36151;&#21033;&#24046;&#25512;&#23548;&#20986;&#30340;&#24066;&#22330;&#21387;&#21147;&#39044;&#27979;&#19982;GPT-4&#25512;&#23548;&#20986;&#30340;&#36130;&#32463;&#26032;&#38395;&#24773;&#24863;&#30456;&#32467;&#21512;&#65292;&#25913;&#36827;&#20102;&#31574;&#30053;&#30340;&#34920;&#29616;&#65292;&#34920;&#29616;&#20026;&#26356;&#39640;&#30340;&#22799;&#26222;&#27604;&#29575;&#21644;&#38477;&#20302;&#30340;&#26368;&#22823;&#22238;&#25764;&#12290;&#25913;&#36827;&#30340;&#34920;&#29616;&#22312;&#32435;&#26031;&#36798;&#20811;&#12289;&#26631;&#26222;500&#25351;&#25968;&#21644;&#20845;&#20010;&#20027;&#35201;&#32929;&#31080;&#24066;&#22330;&#20013;&#37117;&#20445;&#25345;&#19968;&#33268;&#65292;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#32929;&#31080;&#24066;&#22330;&#20013;&#20855;&#26377;&#26222;&#36941;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00012v1 Announce Type: cross  Abstract: This paper introduces a new risk-on risk-off strategy for the stock market, which combines a financial stress indicator with a sentiment analysis done by ChatGPT reading and interpreting Bloomberg daily market summaries. Forecasts of market stress derived from volatility and credit spreads are enhanced when combined with the financial news sentiment derived from GPT-4. As a result, the strategy shows improved performance, evidenced by higher Sharpe ratio and reduced maximum drawdowns. The improved performance is consistent across the NASDAQ, the S&amp;P 500 and the six major equity markets, indicating that the method generalises across equities markets.
&lt;/p&gt;</description></item><item><title>FABind+&#36890;&#36807;&#25913;&#36827;&#21475;&#34955;&#39044;&#27979;&#21644;&#23039;&#24577;&#29983;&#25104;&#65292;&#25552;&#21319;&#20998;&#23376;&#23545;&#25509;&#34920;&#29616;</title><link>https://arxiv.org/abs/2403.20261</link><description>&lt;p&gt;
FABind+: &#36890;&#36807;&#25913;&#36827;&#21475;&#34955;&#39044;&#27979;&#21644;&#23039;&#24577;&#29983;&#25104;&#22686;&#24378;&#20998;&#23376;&#23545;&#25509;
&lt;/p&gt;
&lt;p&gt;
FABind+: Enhancing Molecular Docking through Improved Pocket Prediction and Pose Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20261
&lt;/p&gt;
&lt;p&gt;
FABind+&#36890;&#36807;&#25913;&#36827;&#21475;&#34955;&#39044;&#27979;&#21644;&#23039;&#24577;&#29983;&#25104;&#65292;&#25552;&#21319;&#20998;&#23376;&#23545;&#25509;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#23545;&#25509;&#26159;&#33647;&#29289;&#21457;&#29616;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#36807;&#31243;&#12290;&#20256;&#32479;&#25216;&#26415;&#20381;&#36182;&#20110;&#21463;&#29289;&#29702;&#21407;&#29702;&#25903;&#37197;&#30340;&#24191;&#27867;&#37319;&#26679;&#21644;&#27169;&#25311;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#36895;&#24230;&#24930;&#19988;&#26114;&#36149;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#30340;&#20986;&#29616;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#21069;&#26223;&#65292;&#25552;&#20379;&#20102;&#31934;&#30830;&#24615;&#21644;&#25928;&#29575;&#30340;&#22686;&#38271;&#12290;&#24314;&#31435;&#22312;FABind&#30340;&#22522;&#30784;&#24037;&#20316;&#20043;&#19978;&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FABind+&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#22823;&#25552;&#21319;&#20854;&#21069;&#36523;&#24615;&#33021;&#30340;&#22686;&#24378;&#29256;&#12290;&#25105;&#20204;&#30830;&#23450;&#21475;&#34955;&#39044;&#27979;&#26159;&#20998;&#23376;&#23545;&#25509;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#29942;&#39048;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26174;&#33879;&#25913;&#36827;&#21475;&#34955;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#23545;&#25509;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#23545;&#25509;&#27169;&#22359;&#36827;&#34892;&#20102;&#20462;&#25913;&#65292;&#20197;&#22686;&#24378;&#20854;&#23039;&#24577;&#29983;&#25104;&#33021;&#21147;&#12290;&#20026;&#20102;&#32553;&#23567;&#19982;&#20256;&#32479;&#37319;&#26679;/&#29983;&#25104;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;s
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20261v1 Announce Type: cross  Abstract: Molecular docking is a pivotal process in drug discovery. While traditional techniques rely on extensive sampling and simulation governed by physical principles, these methods are often slow and costly. The advent of deep learning-based approaches has shown significant promise, offering increases in both accuracy and efficiency. Building upon the foundational work of FABind, a model designed with a focus on speed and accuracy, we present FABind+, an enhanced iteration that largely boosts the performance of its predecessor. We identify pocket prediction as a critical bottleneck in molecular docking and propose a novel methodology that significantly refines pocket prediction, thereby streamlining the docking process. Furthermore, we introduce modifications to the docking module to enhance its pose generation capabilities. In an effort to bridge the gap with conventional sampling/generative methods, we incorporate a simple yet effective s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#65292;&#21487;&#20197;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#21644;&#32534;&#36753;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;&#22270;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#26410;&#39044;&#26009;&#26426;&#21046;&#30340;&#35814;&#32454;&#29702;&#35299;&#21644;&#21253;&#21547;&#20102;&#29992;&#20110;&#25552;&#39640;&#20998;&#31867;&#22120;&#27867;&#21270;&#33021;&#21147;&#30340;SHIFT&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.19647</link><description>&lt;p&gt;
&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#65306;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#21644;&#32534;&#36753;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;&#22270;
&lt;/p&gt;
&lt;p&gt;
Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19647
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#65292;&#21487;&#20197;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#21644;&#32534;&#36753;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;&#22270;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#26410;&#39044;&#26009;&#26426;&#21046;&#30340;&#35814;&#32454;&#29702;&#35299;&#21644;&#21253;&#21547;&#20102;&#29992;&#20110;&#25552;&#39640;&#20998;&#31867;&#22120;&#27867;&#21270;&#33021;&#21147;&#30340;SHIFT&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#29992;&#20110;&#21457;&#29616;&#21644;&#24212;&#29992;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#30005;&#36335;&#26159;&#20154;&#31867;&#21487;&#35299;&#37322;&#29305;&#24449;&#30340;&#22240;&#26524;&#30456;&#20851;&#23376;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;&#12290; &#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#30830;&#23450;&#30340;&#30005;&#36335;&#30001;&#22810;&#20041;&#19988;&#38590;&#20197;&#35299;&#37322;&#30340;&#21333;&#20803;&#32452;&#25104;&#65292;&#20363;&#22914;&#27880;&#24847;&#21147;&#22836;&#25110;&#31070;&#32463;&#20803;&#65292;&#20351;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#12290; &#30456;&#27604;&#20043;&#19979;&#65292;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#23454;&#29616;&#20102;&#23545;&#26410;&#39044;&#26009;&#26426;&#21046;&#30340;&#35814;&#32454;&#29702;&#35299;&#12290; &#30001;&#20110;&#23427;&#20204;&#22522;&#20110;&#32454;&#31890;&#24230;&#21333;&#20803;&#65292;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#23545;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#26377;&#29992;&#65306;&#25105;&#20204; introduc&#20102;SHIFT&#65292;&#36890;&#36807;&#20999;&#38500;&#20154;&#31867;&#21028;&#26029;&#20026;&#20219;&#21153;&#19981;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290; &#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#21457;&#29616;&#25104;&#21315;&#19978;&#19975;&#20010;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#26469;&#23637;&#31034;&#19968;&#20010;&#23436;&#20840;&#26080;&#30417;&#30563;&#19988;&#21487;&#25193;&#23637;&#30340;&#21487;&#35299;&#37322;&#24615;&#31649;&#32447;&#65292;&#29992;&#20110;&#33258;&#21160;&#21457;&#29616;&#30340;&#27169;&#22411;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19647v1 Announce Type: cross  Abstract: We introduce methods for discovering and applying sparse feature circuits. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. Circuits identified in prior work consist of polysemantic and difficult-to-interpret units like attention heads or neurons, rendering them unsuitable for many downstream applications. In contrast, sparse feature circuits enable detailed understanding of unanticipated mechanisms. Because they are based on fine-grained units, sparse feature circuits are useful for downstream tasks: We introduce SHIFT, where we improve the generalization of a classifier by ablating features that a human judges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised and scalable interpretability pipeline by discovering thousands of sparse feature circuits for automatically discovered model behaviors.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#25913;&#36827;&#23398;&#20064;(SIL)&#26041;&#27861;&#65292;&#23454;&#29616;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#30340;&#26356;&#22909;&#21487;&#25193;&#23637;&#24615;&#65292;&#36890;&#36807;&#33258;&#36523;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#20316;&#20026;&#20266;&#26631;&#31614;&#65292;&#35774;&#35745;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#27880;&#24847;&#26426;&#21046;&#26469;&#22788;&#29702;&#22823;&#35268;&#27169;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#23454;&#20363;&#12290;</title><link>https://arxiv.org/abs/2403.19561</link><description>&lt;p&gt;
&#33258;&#25105;&#25913;&#36827;&#23398;&#20064;&#29992;&#20110;&#21487;&#25193;&#23637;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Self-Improved Learning for Scalable Neural Combinatorial Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19561
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#25913;&#36827;&#23398;&#20064;(SIL)&#26041;&#27861;&#65292;&#23454;&#29616;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#30340;&#26356;&#22909;&#21487;&#25193;&#23637;&#24615;&#65292;&#36890;&#36807;&#33258;&#36523;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#20316;&#20026;&#20266;&#26631;&#31614;&#65292;&#35774;&#35745;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#27880;&#24847;&#26426;&#21046;&#26469;&#22788;&#29702;&#22823;&#35268;&#27169;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
end-to-end&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;(NCO)&#26041;&#27861;&#22312;&#35299;&#20915;&#22797;&#26434;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#19987;&#23478;&#35774;&#35745;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#38382;&#39064;&#26102;&#23384;&#22312;&#22256;&#38590;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#25913;&#36827;&#23398;&#20064;(SIL)&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#30340;&#26356;&#22909;&#21487;&#25193;&#23637;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#33258;&#25105;&#25913;&#36827;&#26426;&#21046;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#27809;&#26377;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#30452;&#25509;&#22312;&#22823;&#35268;&#27169;&#38382;&#39064;&#23454;&#20363;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#19968;&#31181;&#21019;&#26032;&#30340;&#23616;&#37096;&#37325;&#26500;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#33258;&#36523;&#36845;&#20195;&#29983;&#25104;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#20316;&#20026;&#20266;&#26631;&#31614;&#65292;&#20197;&#25351;&#23548;&#26377;&#25928;&#30340;&#27169;&#22411;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#20302;&#35745;&#31639;&#24320;&#38144;&#30340;&#22823;&#35268;&#27169;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19561v1 Announce Type: cross  Abstract: The end-to-end neural combinatorial optimization (NCO) method shows promising performance in solving complex combinatorial optimization problems without the need for expert design. However, existing methods struggle with large-scale problems, hindering their practical applicability. To overcome this limitation, this work proposes a novel Self-Improved Learning (SIL) method for better scalability of neural combinatorial optimization. Specifically, we develop an efficient self-improved mechanism that enables direct model training on large-scale problem instances without any labeled data. Powered by an innovative local reconstruction approach, this method can iteratively generate better solutions by itself as pseudo-labels to guide efficient model training. In addition, we design a linear complexity attention mechanism for the model to efficiently handle large-scale combinatorial problem instances with low computation overhead. Comprehens
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35266;&#23519;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#21516;&#23618;&#23545;&#38544;&#34255;&#29366;&#24577;&#30340;&#24433;&#21709;&#31243;&#24230;&#65292;&#25552;&#20986;&#20102;LLM-Streamline&#26041;&#27861;&#65292;&#21253;&#25324;&#23618;&#21098;&#26525;&#21644;&#23618;&#26367;&#25442;&#65292;&#29992;&#20110;&#21387;&#32553;&#27169;&#22411;&#24182;&#20445;&#25345;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.19135</link><description>&lt;p&gt;
&#36890;&#36807;&#31616;&#21270;&#19981;&#37325;&#35201;&#30340;&#23618;&#21387;&#32553;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Compressing Large Language Models by Streamlining the Unimportant Layer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19135
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35266;&#23519;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#21516;&#23618;&#23545;&#38544;&#34255;&#29366;&#24577;&#30340;&#24433;&#21709;&#31243;&#24230;&#65292;&#25552;&#20986;&#20102;LLM-Streamline&#26041;&#27861;&#65292;&#21253;&#25324;&#23618;&#21098;&#26525;&#21644;&#23618;&#26367;&#25442;&#65292;&#29992;&#20110;&#21387;&#32553;&#27169;&#22411;&#24182;&#20445;&#25345;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#21644;&#39046;&#22495;&#65292;&#20294;&#20854;&#36866;&#29992;&#24615;&#21463;&#21040;&#27169;&#22411;&#21442;&#25968;&#30340;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#20851;&#27880;&#34920;&#29616;&#20986;&#39640;&#24615;&#33021;&#30340;&#32039;&#20945;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;LLM&#30340;&#19981;&#21516;&#23618;&#23545;&#38544;&#34255;&#29366;&#24577;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#25200;&#21160;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;&#20986;&#19981;&#37027;&#20040;&#37325;&#35201;&#30340;&#23618;&#12290;&#22522;&#20110;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLM-Streamline&#65292;&#21253;&#25324;&#20004;&#37096;&#20998;&#65306;&#23618;&#21098;&#26525;&#65292;&#26681;&#25454;&#30446;&#26631;&#31232;&#30095;&#24230;&#31227;&#38500;&#27169;&#22411;&#20013;&#19968;&#32452;&#36830;&#32493;&#30340;&#26368;&#19981;&#37325;&#35201;&#30340;&#23618;&#65307;&#23618;&#26367;&#25442;&#65292;&#35757;&#32451;&#19968;&#20010;&#36731;&#37327;&#32423;&#27169;&#22411;&#26469;&#26367;&#25442;&#34987;&#21098;&#26525;&#30340;&#23618;&#65292;&#20174;&#32780;&#32531;&#35299;&#30001;&#21098;&#26525;&#36896;&#25104;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;(MLP)&#21644;&#19968;&#20010;transformer&#23618;&#31561;&#32467;&#26500;&#20316;&#20026;&#36731;&#37327;&#32423;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19135v1 Announce Type: cross  Abstract: Large language models (LLM) have been extensively applied in various natural language tasks and domains, but their applicability is constrained by the large number of parameters of the models. Consequently, there is an increasing emphasis on compact models that exhibit high performance. In this study, we observe that different layers in LLM have varying degrees of perturbation on the hidden states, which allows us to identify less important layers. Based on this phenomenon, we propose LLM-Streamline, which consists of two parts: layer pruning, where we remove a set of consecutive layers with the lowest importance in the model according to the target sparsity; and layer replacement, where we train a lightweight model to substitute the pruned layers, thereby mitigating the performance degradation caused by pruning. In our experiments, we utilize structures such as a multi-layer perceptron (MLP) and a transformer layer as lightweight mode
&lt;/p&gt;</description></item><item><title>TinyML&#26159;&#19968;&#31181;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21387;&#32553;&#21040;&#29289;&#32852;&#32593;&#35774;&#22791;&#21644;&#24494;&#25511;&#21046;&#22120;&#20013;&#23454;&#29616;&#26080;&#22788;&#19981;&#22312;&#26234;&#33021;&#30340;&#26032;&#26041;&#27861;&#65292;&#38656;&#35201;&#20849;&#21516;&#35774;&#35745;&#31639;&#27861;&#21644;&#31995;&#32479;&#22534;&#26632;&#20197;&#20811;&#26381;&#30828;&#20214;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.19076</link><description>&lt;p&gt;
&#23567;&#22411;&#26426;&#22120;&#23398;&#20064;&#65306;&#36827;&#23637;&#19982;&#26410;&#26469;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Tiny Machine Learning: Progress and Futures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19076
&lt;/p&gt;
&lt;p&gt;
TinyML&#26159;&#19968;&#31181;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21387;&#32553;&#21040;&#29289;&#32852;&#32593;&#35774;&#22791;&#21644;&#24494;&#25511;&#21046;&#22120;&#20013;&#23454;&#29616;&#26080;&#22788;&#19981;&#22312;&#26234;&#33021;&#30340;&#26032;&#26041;&#27861;&#65292;&#38656;&#35201;&#20849;&#21516;&#35774;&#35745;&#31639;&#27861;&#21644;&#31995;&#32479;&#22534;&#26632;&#20197;&#20811;&#26381;&#30828;&#20214;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Tiny Machine Learning&#65288;TinyML&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#20010;&#26032;&#39046;&#22495;&#12290;&#36890;&#36807;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21387;&#32553;&#21040;&#25968;&#21313;&#20159;&#20010;&#29289;&#32852;&#32593;&#35774;&#22791;&#21644;&#24494;&#25511;&#21046;&#22120;&#65288;MCUs&#65289;&#20013;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#33539;&#22260;&#65292;&#23454;&#29616;&#20102;&#26080;&#22788;&#19981;&#22312;&#30340;&#26234;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30828;&#20214;&#38480;&#21046;&#65292;TinyML&#20855;&#26377;&#25361;&#25112;&#24615;&#65306;&#26377;&#38480;&#30340;&#20869;&#23384;&#36164;&#28304;&#20351;&#24471;&#38590;&#20197;&#23481;&#32435;&#20026;&#20113;&#21644;&#31227;&#21160;&#24179;&#21488;&#35774;&#35745;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#23545;&#20110;&#35064;&#26426;&#35774;&#22791;&#65292;&#32534;&#35793;&#22120;&#21644;&#25512;&#26029;&#24341;&#25806;&#25903;&#25345;&#20063;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#20849;&#21516;&#35774;&#35745;&#31639;&#27861;&#21644;&#31995;&#32479;&#22534;&#26632;&#20197;&#23454;&#29616;TinyML&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19076v1 Announce Type: cross  Abstract: Tiny Machine Learning (TinyML) is a new frontier of machine learning. By squeezing deep learning models into billions of IoT devices and microcontrollers (MCUs), we expand the scope of AI applications and enable ubiquitous intelligence. However, TinyML is challenging due to hardware constraints: the tiny memory resource makes it difficult to hold deep learning models designed for cloud and mobile platforms. There is also limited compiler and inference engine support for bare-metal devices. Therefore, we need to co-design the algorithm and system stack to enable TinyML. In this review, we will first discuss the definition, challenges, and applications of TinyML. We then survey the recent progress in TinyML and deep learning on MCUs. Next, we will introduce MCUNet, showing how we can achieve ImageNet-scale AI applications on IoT devices with system-algorithm co-design. We will further extend the solution from inference to training and in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26032;&#39062;&#30340;&#26694;&#26550;SFFormer&#65292;&#32467;&#21512;&#20102;&#22810;&#22836;&#20132;&#21449;&#27880;&#24847;&#21147;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#65292;&#22522;&#20110;dMRI&#32420;&#32500;&#26463;&#36861;&#36394;&#65292;&#39044;&#27979;&#20102;&#20027;&#35266;&#35821;&#35328;&#34920;&#29616;&#65292;&#25299;&#23637;&#20102;&#33041;&#32467;&#26500;&#19982;&#20154;&#31867;&#35748;&#30693;&#21151;&#33021;&#30340;&#20851;&#32852;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.19001</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#30340;&#32420;&#32500;&#31751;&#24418;&#29366;&#20998;&#26512;&#29992;&#20110;&#35821;&#35328;&#34920;&#29616;&#35748;&#30693;&#20998;&#25968;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Cross--domain Fiber Cluster Shape Analysis for Language Performance Cognitive Score Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26032;&#39062;&#30340;&#26694;&#26550;SFFormer&#65292;&#32467;&#21512;&#20102;&#22810;&#22836;&#20132;&#21449;&#27880;&#24847;&#21147;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#65292;&#22522;&#20110;dMRI&#32420;&#32500;&#26463;&#36861;&#36394;&#65292;&#39044;&#27979;&#20102;&#20027;&#35266;&#35821;&#35328;&#34920;&#29616;&#65292;&#25299;&#23637;&#20102;&#33041;&#32467;&#26500;&#19982;&#20154;&#31867;&#35748;&#30693;&#21151;&#33021;&#30340;&#20851;&#32852;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24418;&#29366;&#22312;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#20013;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;&#23545;&#35937;&#24418;&#24577;&#21644;&#21151;&#33021;&#30340;&#20449;&#24687;&#29305;&#24449;&#12290;&#33041;&#25104;&#20687;&#20013;&#30340;&#24418;&#29366;&#20998;&#26512;&#21487;&#24110;&#21161;&#35299;&#37322;&#20154;&#33041;&#32467;&#26500;&#21644;&#21151;&#33021;&#30340;&#30456;&#20851;&#24615;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#33041;&#30340;3D&#30333;&#36136;&#36830;&#25509;&#30340;&#24418;&#29366;&#21450;&#20854;&#19982;&#20154;&#31867;&#35748;&#30693;&#21151;&#33021;&#30340;&#28508;&#22312;&#39044;&#27979;&#20851;&#31995;&#12290;&#25105;&#20204;&#20351;&#29992;&#25193;&#25955;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;dMRI&#65289;&#32420;&#32500;&#26463;&#36861;&#36394;&#23558;&#22823;&#33041;&#36830;&#25509;&#37325;&#24314;&#20026;3D&#28857;&#24207;&#21015;&#12290;&#20026;&#20102;&#25551;&#36848;&#27599;&#20010;&#36830;&#25509;&#65292;&#25105;&#20204;&#25552;&#21462;&#20102;12&#20010;&#24418;&#29366;&#25551;&#36848;&#31526;&#20197;&#21450;&#20256;&#32479;&#30340;dMRI&#36830;&#25509;&#21644;&#32452;&#32455;&#24494;&#32467;&#26500;&#29305;&#24449;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#24418;&#29366;&#34701;&#21512;&#32420;&#32500;&#31751;&#21464;&#25442;&#22120;&#65288;SFFormer&#65289;&#65292;&#21033;&#29992;&#22810;&#22836;&#20132;&#21449;&#27880;&#24847;&#21147;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#22522;&#20110;dMRI&#32420;&#32500;&#26463;&#36861;&#36394;&#26469;&#39044;&#27979;&#29305;&#23450;&#20010;&#20307;&#30340;&#35821;&#35328;&#34920;&#29616;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19001v1 Announce Type: cross  Abstract: Shape plays an important role in computer graphics, offering informative features to convey an object's morphology and functionality. Shape analysis in brain imaging can help interpret structural and functionality correlations of the human brain. In this work, we investigate the shape of the brain's 3D white matter connections and its potential predictive relationship to human cognitive function. We reconstruct brain connections as sequences of 3D points using diffusion magnetic resonance imaging (dMRI) tractography. To describe each connection, we extract 12 shape descriptors in addition to traditional dMRI connectivity and tissue microstructure features. We introduce a novel framework, Shape--fused Fiber Cluster Transformer (SFFormer), that leverages a multi-head cross-attention feature fusion module to predict subject-specific language performance based on dMRI tractography. We assess the performance of the method on a large dataset
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38024;&#23545;&#24494;&#26381;&#21153;&#31995;&#32479;&#30340;&#23569;&#26679;&#26412;&#24322;&#24120;&#36319;&#36394;&#20998;&#31867;&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#33258;&#32534;&#30721;&#22120;&#26500;&#24314;&#31995;&#32479;&#29305;&#23450;&#30340;&#36319;&#36394;&#34920;&#31034;&#65292;&#24182;&#24212;&#29992;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#36827;&#34892;&#39640;&#25928;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2403.18998</link><description>&lt;p&gt;
&#24494;&#26381;&#21153;&#31995;&#32479;&#30340;&#23569;&#26679;&#26412;&#36328;&#31995;&#32479;&#24322;&#24120;&#36319;&#36394;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Cross-System Anomaly Trace Classification for Microservice-based systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18998
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38024;&#23545;&#24494;&#26381;&#21153;&#31995;&#32479;&#30340;&#23569;&#26679;&#26412;&#24322;&#24120;&#36319;&#36394;&#20998;&#31867;&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#33258;&#32534;&#30721;&#22120;&#26500;&#24314;&#31995;&#32479;&#29305;&#23450;&#30340;&#36319;&#36394;&#34920;&#31034;&#65292;&#24182;&#24212;&#29992;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#36827;&#34892;&#39640;&#25928;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#26381;&#21153;&#31995;&#32479;&#65288;MSS&#65289;&#30001;&#20110;&#20854;&#22797;&#26434;&#21644;&#21160;&#24577;&#30340;&#29305;&#24615;&#21487;&#33021;&#22312;&#21508;&#31181;&#25925;&#38556;&#31867;&#21035;&#20013;&#20986;&#29616;&#25925;&#38556;&#12290;&#20026;&#20102;&#26377;&#25928;&#22788;&#29702;&#25925;&#38556;&#65292;AIOps&#24037;&#20855;&#21033;&#29992;&#22522;&#20110;&#36319;&#36394;&#30340;&#24322;&#24120;&#26816;&#27979;&#21644;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#24494;&#26381;&#21153;&#31995;&#32479;&#30340;&#23569;&#26679;&#26412;&#24322;&#24120;&#36319;&#36394;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#65288;1&#65289;&#22810;&#22836;&#27880;&#24847;&#21147;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#26500;&#24314;&#31995;&#32479;&#29305;&#23450;&#30340;&#36319;&#36394;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#65288;2&#65289;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#23569;&#26679;&#26412;&#24322;&#24120;&#36319;&#36394;&#20998;&#31867;&#12290;&#35813;&#26694;&#26550;&#22312;&#20004;&#20010;&#20195;&#34920;&#24615;&#30340;MSS&#65292;Trainticket&#21644;OnlineBoutique&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20351;&#29992;&#24320;&#25918;&#25968;&#25454;&#38598;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#35843;&#25972;&#23398;&#21040;&#30340;&#30693;&#35782;&#65292;&#20197;&#23545;&#26032;&#30340;&#12289;&#26410;&#35265;&#30340;&#26032;&#39062;&#25925;&#38556;&#31867;&#21035;&#30340;&#24322;&#24120;&#36319;&#36394;&#36827;&#34892;&#20998;&#31867;&#65292;&#26080;&#35770;&#26159;&#22312;&#26368;&#21021;&#35757;&#32451;&#30340;&#21516;&#19968;&#31995;&#32479;&#20869;&#65292;&#36824;&#26159;&#22312;&#20854;&#20182;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18998v1 Announce Type: cross  Abstract: Microservice-based systems (MSS) may experience failures in various fault categories due to their complex and dynamic nature. To effectively handle failures, AIOps tools utilize trace-based anomaly detection and root cause analysis. In this paper, we propose a novel framework for few-shot abnormal trace classification for MSS. Our framework comprises two main components: (1) Multi-Head Attention Autoencoder for constructing system-specific trace representations, which enables (2) Transformer Encoder-based Model-Agnostic Meta-Learning to perform effective and efficient few-shot learning for abnormal trace classification. The proposed framework is evaluated on two representative MSS, Trainticket and OnlineBoutique, with open datasets. The results show that our framework can adapt the learned knowledge to classify new, unseen abnormal traces of novel fault categories both within the same system it was initially trained on and even in the 
&lt;/p&gt;</description></item><item><title>&#24635;&#20307;&#32780;&#35328;&#65292;&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#20013;&#25991;&#20013;&#26816;&#27979; offensive &#35821;&#35328;&#30340;&#25361;&#25112;&#65292;&#24182;&#24378;&#35843;&#20102;&#24320;&#21457;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#29305;&#23450;&#27169;&#22411;&#21644;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2403.18314</link><description>&lt;p&gt;
&#20013;&#22269; offensive &#35821;&#35328;&#26816;&#27979;&#65306;&#29616;&#29366;&#19982;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Chinese Offensive Language Detection:Current Status and Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18314
&lt;/p&gt;
&lt;p&gt;
&#24635;&#20307;&#32780;&#35328;&#65292;&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#20013;&#25991;&#20013;&#26816;&#27979; offensive &#35821;&#35328;&#30340;&#25361;&#25112;&#65292;&#24182;&#24378;&#35843;&#20102;&#24320;&#21457;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#29305;&#23450;&#27169;&#22411;&#21644;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#27491;&#22312;&#20570;&#20986;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#30417;&#27979;&#21644;&#35268;&#33539;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#65292;&#20294;&#22312;&#25968;&#23383;&#31354;&#38388;&#20013;&#65292;&#24694;&#24847;&#35821;&#35328;&#65288;&#22914;&#20167;&#24680;&#35328;&#35770;&#25110;&#32593;&#32476;&#27450;&#20940;&#65289;&#30340;&#26222;&#36941;&#23384;&#22312;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#37492;&#20110;&#32500;&#25252;&#25991;&#26126;&#21644;&#23562;&#37325;&#30340;&#22312;&#32447;&#29615;&#22659;&#30340;&#37325;&#35201;&#24615;&#65292;&#36843;&#20999;&#38656;&#35201;&#33021;&#22815;&#23454;&#26102;&#26816;&#27979;&#24694;&#24847;&#35328;&#35770;&#30340;&#33258;&#21160;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#24320;&#21457;&#22788;&#29702;&#27721;&#35821;&#31561;&#35821;&#35328;&#30340;&#26377;&#25928;&#31995;&#32479;&#65292;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#35821;&#35328;&#30340;&#22797;&#26434;&#21644;&#24494;&#22937;&#24615;&#20351;&#24471;&#33258;&#21160;&#22788;&#29702;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#25991;&#20840;&#38754;&#24635;&#32467;&#20102;&#20013;&#22269; offensive &#35821;&#35328;&#26816;&#27979;&#24773;&#20917;&#65292;&#23457;&#26597;&#20102;&#24403;&#21069;&#30340;&#22522;&#20934;&#21644;&#26041;&#27861;&#65292;&#24182;&#37325;&#28857;&#20171;&#32461;&#20102;&#29992;&#20110;&#35299;&#20915;&#22312;&#36825;&#31181;&#22797;&#26434;&#35821;&#35328;&#20013;&#26816;&#27979;&#24694;&#24847;&#35821;&#35328;&#30340;&#29420;&#29305;&#25361;&#25112;&#30340;&#29305;&#23450;&#27169;&#22411;&#21644;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18314v1 Announce Type: cross  Abstract: Despite the considerable efforts being made to monitor and regulate user-generated content on social media platforms, the pervasiveness of offensive language, such as hate speech or cyberbullying, in the digital space remains a significant challenge. Given the importance of maintaining a civilized and respectful online environment, there is an urgent and growing need for automatic systems capable of detecting offensive speech in real time. However, developing effective systems for processing languages such as Chinese presents a significant challenge, owing to the language's complex and nuanced nature, which makes it difficult to process automatically. This paper provides a comprehensive overview of offensive language detection in Chinese, examining current benchmarks and approaches and highlighting specific models and tools for addressing the unique challenges of detecting offensive language in this complex language. The primary object
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ECSP&#30340;&#21333;&#22810;&#27169;&#24577;&#24773;&#24863;&#25991;&#21270;&#29305;&#23450;&#25552;&#31034;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#21333;&#19968;&#27169;&#24577;&#28040;&#24687;&#22686;&#24378;&#22810;&#27169;&#24577;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#35774;&#35745;&#33391;&#22909;&#30340;&#25552;&#31034;&#26469;&#20943;&#23569;&#25991;&#21270;&#24046;&#24322;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17683</link><description>&lt;p&gt;
&#35299;&#20915;&#24773;&#24863;&#19982;&#25991;&#21270;&#26234;&#33021;&#20154;&#24037;&#26234;&#33021;&#30740;&#35752;&#20250;&#24773;&#24863;&#39044;&#27979;&#31454;&#36187;&#30340;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Solution for Emotion Prediction Competition of Workshop on Emotionally and Culturally Intelligent AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17683
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ECSP&#30340;&#21333;&#22810;&#27169;&#24577;&#24773;&#24863;&#25991;&#21270;&#29305;&#23450;&#25552;&#31034;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#21333;&#19968;&#27169;&#24577;&#28040;&#24687;&#22686;&#24378;&#22810;&#27169;&#24577;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#35774;&#35745;&#33391;&#22909;&#30340;&#25552;&#31034;&#26469;&#20943;&#23569;&#25991;&#21270;&#24046;&#24322;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25253;&#21578;&#25552;&#20379;&#20102;&#25105;&#20204;&#22312;WECIA&#24773;&#24863;&#39044;&#27979;&#31454;&#36187;&#65288;EPC&#65289;&#20013;&#25506;&#32034;&#21644;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#35814;&#32454;&#25551;&#36848;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#24102;&#26377;&#35780;&#35770;&#30340;&#33402;&#26415;&#20316;&#21697;&#39044;&#27979;&#19968;&#20010;&#20154;&#30340;&#24773;&#24863;&#12290;&#35813;&#31454;&#36187;&#30340;&#25968;&#25454;&#38598;&#26159;ArtELingo&#65292;&#26088;&#22312;&#40723;&#21169;&#36328;&#35821;&#35328;&#21644;&#25991;&#21270;&#30340;&#22810;&#26679;&#21270;&#24037;&#20316;&#12290;&#35813;&#25968;&#25454;&#38598;&#20027;&#35201;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#65292;&#21363;&#27169;&#24577;&#19981;&#24179;&#34913;&#38382;&#39064;&#21644;&#35821;&#35328;&#25991;&#21270;&#24046;&#24322;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#21333;&#22810;&#27169;&#24577;&#24773;&#24863;&#25991;&#21270;&#29305;&#23450;&#25552;&#31034;&#65288;ECSP&#65289;&#65292;&#20854;&#37325;&#28857;&#26159;&#20351;&#29992;&#21333;&#19968;&#27169;&#24577;&#28040;&#24687;&#26469;&#22686;&#24378;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#35774;&#35745;&#33391;&#22909;&#30340;&#25552;&#31034;&#26469;&#20943;&#23569;&#25991;&#21270;&#24046;&#24322;&#38382;&#39064;&#12290;&#28548;&#28165;&#19968;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#21547;&#20004;&#20010;&#20027;&#35201;&#22359;&#65306;&#65288;1&#65289;&#22522;&#20110;XLM-R&#30340;&#21333;&#27169;&#24577;&#27169;&#22411;&#21644;&#22522;&#20110;X$^2$-VLM&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;2&#65289;&#24773;&#24863;&#25991;&#21270;&#29305;&#23450;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17683v1 Announce Type: new  Abstract: This report provide a detailed description of the method that we explored and proposed in the WECIA Emotion Prediction Competition (EPC), which predicts a person's emotion through an artistic work with a comment. The dataset of this competition is ArtELingo, designed to encourage work on diversity across languages and cultures. The dataset has two main challenges, namely modal imbalance problem and language-cultural differences problem. In order to address this issue, we propose a simple yet effective approach called single-multi modal with Emotion-Cultural specific prompt(ECSP), which focuses on using the single modal message to enhance the performance of multimodal models and a well-designed prompt to reduce cultural differences problem. To clarify, our approach contains two main blocks: (1)XLM-R\cite{conneau2019unsupervised} based unimodal model and X$^2$-VLM\cite{zeng2022x} based multimodal model (2) Emotion-Cultural specific prompt.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#21306;&#22495;&#25351;&#23548;&#26631;&#35760;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23391;&#21152;&#25289;&#25991;&#26412;&#19982;&#22320;&#26041;&#26041;&#35328;&#36716;&#24405;&#20026;&#22269;&#38469;&#38899;&#26631;&#30340;&#26041;&#27861;&#65292;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#20851;&#20110;&#36755;&#20837;&#25991;&#26412;&#30340;&#22320;&#21306;&#26041;&#35328;&#20449;&#24687;&#65292;&#20197;&#29702;&#35299;&#19982;&#27599;&#20010;&#22320;&#21306;&#30456;&#20851;&#30340;&#29420;&#29305;&#38899;&#38901;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.17407</link><description>&lt;p&gt;
&#20351;&#29992;&#21306;&#22495;&#25351;&#23548;&#26631;&#35760;&#23558;&#23391;&#21152;&#25289;&#25991;&#26412;&#19982;&#22320;&#26041;&#26041;&#35328;&#36716;&#24405;&#20026;&#22269;&#38469;&#38899;&#26631;
&lt;/p&gt;
&lt;p&gt;
Transcribing Bengali Text with Regional Dialects to IPA using District Guided Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17407
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#21306;&#22495;&#25351;&#23548;&#26631;&#35760;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23391;&#21152;&#25289;&#25991;&#26412;&#19982;&#22320;&#26041;&#26041;&#35328;&#36716;&#24405;&#20026;&#22269;&#38469;&#38899;&#26631;&#30340;&#26041;&#27861;&#65292;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#20851;&#20110;&#36755;&#20837;&#25991;&#26412;&#30340;&#22320;&#21306;&#26041;&#35328;&#20449;&#24687;&#65292;&#20197;&#29702;&#35299;&#19982;&#27599;&#20010;&#22320;&#21306;&#30456;&#20851;&#30340;&#29420;&#29305;&#38899;&#38901;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23391;&#21152;&#25289;&#25991;&#26412;&#21040;&#22269;&#38469;&#38899;&#26631;&#65288;IPA&#65289;&#30340;&#20934;&#30830;&#36716;&#24405;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#35821;&#35328;&#30340;&#22797;&#26434;&#38899;&#38901;&#23398;&#21644;&#35821;&#22659;&#30456;&#20851;&#30340;&#38899;&#21464;&#12290;&#23545;&#20110;&#21306;&#22495;&#23391;&#21152;&#25289;&#26041;&#35328;&#26469;&#35828;&#65292;&#30001;&#20110;&#32570;&#20047;&#38024;&#23545;&#36825;&#20123;&#26041;&#35328;&#30340;&#26631;&#20934;&#25340;&#20889;&#32422;&#23450;&#12289;&#24403;&#22320;&#21644;&#22806;&#35821;&#22312;&#36825;&#20123;&#22320;&#21306;&#20013;&#27969;&#34892;&#30340;&#35789;&#27719;&#20197;&#21450;&#19981;&#21516;&#22320;&#21306;&#20043;&#38388;&#30340;&#38899;&#38901;&#22810;&#26679;&#24615;&#65292;&#36825;&#19968;&#25361;&#25112;&#29978;&#33267;&#26356;&#20026;&#20005;&#23803;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#35206;&#30422;&#23391;&#21152;&#25289;&#22269;&#20845;&#20010;&#22320;&#21306;&#30340;&#26032;&#25968;&#25454;&#38598;&#19978;&#24341;&#20837;&#8220;&#21306;&#22495;&#25351;&#23548;&#26631;&#35760;&#8221;&#65288;DGT&#65289;&#25216;&#26415;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;&#29983;&#25104;IPA&#36716;&#24405;&#20043;&#21069;&#21521;&#27169;&#22411;&#25552;&#20379;&#26377;&#20851;&#36755;&#20837;&#25991;&#26412;&#30340;&#21306;&#22495;&#26041;&#35328;&#25110;&#8220;&#22320;&#21306;&#8221;&#30340;&#26126;&#30830;&#20449;&#24687;&#12290;&#36825;&#36890;&#36807;&#22312;&#36755;&#20837;&#24207;&#21015;&#21069;&#28155;&#21152;&#19968;&#20010;&#22320;&#21306;&#26631;&#35760;&#26469;&#23454;&#29616;&#65292;&#26377;&#25928;&#22320;&#24341;&#23548;&#27169;&#22411;&#29702;&#35299;&#19982;&#27599;&#20010;&#22320;&#21306;&#30456;&#20851;&#30340;&#29420;&#29305;&#38899;&#38901;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17407v1 Announce Type: cross  Abstract: Accurate transcription of Bengali text to the International Phonetic Alphabet (IPA) is a challenging task due to the complex phonology of the language and context-dependent sound changes. This challenge is even more for regional Bengali dialects due to unavailability of standardized spelling conventions for these dialects, presence of local and foreign words popular in those regions and phonological diversity across different regions. This paper presents an approach to this sequence-to-sequence problem by introducing the District Guided Tokens (DGT) technique on a new dataset spanning six districts of Bangladesh. The key idea is to provide the model with explicit information about the regional dialect or "district" of the input text before generating the IPA transcription. This is achieved by prepending a district token to the input sequence, effectively guiding the model to understand the unique phonetic patterns associated with each 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#32454;&#33268;&#35752;&#35770;&#65292;&#23545;&#24187;&#35273;&#36827;&#34892;&#20102;&#37327;&#21270;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#24320;&#25968;&#25454;&#38598;VHILT&#65292;&#26377;&#21161;&#20110;&#30740;&#31350;&#27492;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17306</link><description>&lt;p&gt;
&#35270;&#35273;&#24187;&#35273;&#65306;&#23450;&#20041;&#12289;&#37327;&#21270;&#21644;&#22788;&#26041;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Visual Hallucination: Definition, Quantification, and Prescriptive Remediations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#32454;&#33268;&#35752;&#35770;&#65292;&#23545;&#24187;&#35273;&#36827;&#34892;&#20102;&#37327;&#21270;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#24320;&#25968;&#25454;&#38598;VHILT&#65292;&#26377;&#21161;&#20110;&#30740;&#31350;&#27492;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#21457;&#24187;&#35273;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#25110;&#35768;&#26159;&#23545;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#36827;&#23637;&#30340;&#26368;&#26174;&#33879;&#38556;&#30861;&#12290;&#26368;&#36817;&#65292;&#30456;&#24403;&#22810;&#30340;&#30740;&#31350;&#20391;&#37325;&#20110;&#26816;&#27979;&#21644;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#24187;&#35273;&#12290;&#28982;&#32780;&#65292;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#20013;&#24187;&#35273;&#20063;&#30456;&#24403;&#26222;&#36941;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#20110;&#22522;&#20110;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#38382;&#31572;&#20004;&#20010;&#20219;&#21153;&#30340;VLM&#24187;&#35273;&#21078;&#26512;&#30340;&#32454;&#33268;&#35752;&#35770;&#12290;&#25105;&#20204;&#38416;&#26126;&#20102;&#20843;&#20010;&#32454;&#33268;&#30340;&#35270;&#35273;&#24187;&#35273;&#21462;&#21521;&#65306;i) &#19978;&#19979;&#25991;&#29468;&#27979;&#65292;ii) &#36523;&#20221;&#19981;&#19968;&#33268;&#65292;iii) &#22320;&#29702;&#38169;&#35823;&#65292;iv) &#35270;&#35273;&#24187;&#35273;&#65292;v) &#24615;&#21035;&#24322;&#24120;&#65292;vi) VLM&#20316;&#20026;&#20998;&#31867;&#22120;&#65292;vii) &#38169;&#35823;&#38405;&#35835;&#65292;&#21644;viii) &#25968;&#23383;&#24046;&#24322;&#12290;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#20221;&#21517;&#20026;&#35270;&#35273;&#24187;&#35273;&#35825;&#21457;&#65288;VHILT&#65289;&#30340;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;&#36890;&#36807;&#20004;&#20010;&#20219;&#21153;&#65288;&#23383;&#24149;&#21644;VQA&#65289;&#29983;&#25104;&#30340;&#26469;&#33258;&#20843;&#20010;VLM&#30340;2,000&#20010;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17306v1 Announce Type: new  Abstract: The troubling rise of hallucination presents perhaps the most significant impediment to the advancement of responsible AI. In recent times, considerable research has focused on detecting and mitigating hallucination in Large Language Models (LLMs). However, it's worth noting that hallucination is also quite prevalent in Vision-Language models (VLMs). In this paper, we offer a fine-grained discourse on profiling VLM hallucination based on two tasks: i) image captioning, and ii) Visual Question Answering (VQA). We delineate eight fine-grained orientations of visual hallucination: i) Contextual Guessing, ii) Identity Incongruity, iii) Geographical Erratum, iv) Visual Illusion, v) Gender Anomaly, vi) VLM as Classifier, vii) Wrong Reading, and viii) Numeric Discrepancy. We curate Visual HallucInation eLiciTation (VHILT), a publicly available dataset comprising 2,000 samples generated using eight VLMs across two tasks of captioning and VQA alo
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#29420;&#29305;&#30340;Transformer&#27169;&#22411;&#22312;&#30005;&#21147;&#20215;&#26684;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#65292;&#20026;&#21487;&#38752;&#21644;&#21487;&#25345;&#32493;&#30340;&#30005;&#21147;&#31995;&#32479;&#36816;&#34892;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.16108</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#30005;&#21147;&#20215;&#26684;&#39044;&#27979;&#30340;Transformer&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Transformer approach for Electricity Price Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16108
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#29420;&#29305;&#30340;Transformer&#27169;&#22411;&#22312;&#30005;&#21147;&#20215;&#26684;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#65292;&#20026;&#21487;&#38752;&#21644;&#21487;&#25345;&#32493;&#30340;&#30005;&#21147;&#31995;&#32479;&#36816;&#34892;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32431;Transformer&#27169;&#22411;&#36827;&#34892;&#30005;&#21147;&#20215;&#26684;&#39044;&#27979;&#65288;EPF&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;&#27809;&#26377;&#20351;&#29992;&#20854;&#20182;&#36882;&#24402;&#32593;&#32476;&#32467;&#21512;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#22240;&#27492;&#65292;&#34920;&#26126;&#27880;&#24847;&#21147;&#23618;&#36275;&#20197;&#25429;&#25417;&#26102;&#38388;&#27169;&#24335;&#12290;&#35813;&#35770;&#25991;&#36824;&#36890;&#36807;&#20351;&#29992;&#24320;&#28304;EPF&#24037;&#20855;&#36827;&#34892;&#20102;&#23545;&#27169;&#22411;&#30340;&#20844;&#24179;&#27604;&#36739;&#65292;&#24182;&#25552;&#20379;&#20102;&#20195;&#30721;&#20197;&#22686;&#24378;EPF&#30740;&#31350;&#30340;&#21487;&#20877;&#29616;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;Transformer&#27169;&#22411;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#20026;&#21487;&#38752;&#21644;&#21487;&#25345;&#32493;&#30340;&#30005;&#21147;&#31995;&#32479;&#36816;&#34892;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16108v1 Announce Type: cross  Abstract: This paper presents a novel approach to electricity price forecasting (EPF) using a pure Transformer model. As opposed to other alternatives, no other recurrent network is used in combination to the attention mechanism. Hence, showing that the attention layer is enough for capturing the temporal patterns. The paper also provides fair comparison of the models using the open-source EPF toolbox and provide the code to enhance reproducibility and transparency in EPF research. The results show that the Transformer model outperforms traditional methods, offering a promising solution for reliable and sustainable power system operation.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36879;&#26126;&#27700;&#21360;&#26816;&#27979;&#30340;&#40657;&#30418;&#26041;&#27861;WMD&#65292;&#22312;&#26080;&#27880;&#37322;&#35774;&#32622;&#19979;&#65292;&#21033;&#29992;&#24178;&#20928;&#26080;&#27700;&#21360;&#25968;&#25454;&#38598;&#26816;&#27979;&#20219;&#24847;&#27700;&#21360;&#65292;&#25928;&#26524;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.15955</link><description>&lt;p&gt;
&#22312;&#24178;&#33609;&#22534;&#20013;&#23547;&#25214;&#38024;: &#19968;&#31181;&#36879;&#26126;&#27700;&#21360;&#26816;&#27979;&#30340;&#40657;&#30418;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Finding needles in a haystack: A Black-Box Approach to Invisible Watermark Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15955
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36879;&#26126;&#27700;&#21360;&#26816;&#27979;&#30340;&#40657;&#30418;&#26041;&#27861;WMD&#65292;&#22312;&#26080;&#27880;&#37322;&#35774;&#32622;&#19979;&#65292;&#21033;&#29992;&#24178;&#20928;&#26080;&#27700;&#21360;&#25968;&#25454;&#38598;&#26816;&#27979;&#20219;&#24847;&#27700;&#21360;&#65292;&#25928;&#26524;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;WaterMark Detection&#65288;WMD&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#40657;&#30418;&#21644;&#26080;&#27880;&#37322;&#35774;&#32622;&#19979;&#36827;&#34892;&#36879;&#26126;&#27700;&#21360;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;WMD&#33021;&#22815;&#21033;&#29992;&#19968;&#20010;&#24178;&#20928;&#30340;&#26080;&#27700;&#21360;&#25968;&#25454;&#38598;&#20316;&#20026;&#21442;&#32771;&#65292;&#22312;&#19981;&#20381;&#36182;&#29305;&#23450;&#35299;&#30721;&#26041;&#27861;&#25110;&#23545;&#27700;&#21360;&#25216;&#26415;&#30340;&#20107;&#20808;&#20102;&#35299;&#30340;&#24773;&#20917;&#19979;&#65292;&#26816;&#27979;&#32473;&#23450;&#21442;&#32771;&#25968;&#25454;&#38598;&#20013;&#30340;&#20219;&#24847;&#27700;&#21360;&#12290;&#25105;&#20204;&#20351;&#29992;&#20559;&#31227;&#23398;&#20064;&#30340;&#22522;&#30784;&#24320;&#21457;&#20102;WMD&#65292;&#24178;&#20928;&#30340;&#26080;&#27700;&#21360;&#25968;&#25454;&#38598;&#20351;&#25105;&#20204;&#33021;&#22815;&#20165;&#20998;&#31163;&#20986;&#21442;&#32771;&#25968;&#25454;&#38598;&#20013;&#24102;&#27700;&#21360;&#26679;&#26412;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;WMD&#30340;&#26377;&#25928;&#24615;&#65292;&#26126;&#26174;&#20248;&#20110;&#20165;&#20135;&#29983;&#32422;0.5&#30340;AUC&#24471;&#20998;&#30340;&#31616;&#21333;&#26816;&#27979;&#26041;&#27861;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;WMD&#22312;&#22823;&#22810;&#25968;&#21333;&#27700;&#21360;&#25968;&#25454;&#38598;&#20013;&#25345;&#32493;&#33719;&#24471;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26816;&#27979;AUC&#24471;&#20998;&#65292;&#36229;&#36807;0.9&#65292;&#24182;&#22312;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#22810;&#27700;&#21360;&#22330;&#26223;&#20013;&#30340;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#27700;&#21360;&#26041;&#27861;&#20013;&#36229;&#36807;0.7&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15955v1 Announce Type: cross  Abstract: In this paper, we propose WaterMark Detection (WMD), the first invisible watermark detection method under a black-box and annotation-free setting. WMD is capable of detecting arbitrary watermarks within a given reference dataset using a clean non-watermarked dataset as a reference, without relying on specific decoding methods or prior knowledge of the watermarking techniques. We develop WMD using foundations of offset learning, where a clean non-watermarked dataset enables us to isolate the influence of only watermarked samples in the reference dataset. Our comprehensive evaluations demonstrate the effectiveness of WMD, significantly outperforming naive detection methods, which only yield AUC scores around 0.5. In contrast, WMD consistently achieves impressive detection AUC scores, surpassing 0.9 in most single-watermark datasets and exceeding 0.7 in more challenging multi-watermark scenarios across diverse datasets and watermarking me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#25439;&#22833;&#35282;&#24230;&#37325;&#26032;&#23450;&#20041;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#29616;&#33021;&#21147;&#65292;&#21457;&#29616;&#20855;&#26377;&#30456;&#21516;&#39044;&#35757;&#32451;&#25439;&#22833;&#30340;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#34920;&#29616;&#30456;&#20284;&#65292;&#32780;&#24403;&#39044;&#35757;&#32451;&#25439;&#22833;&#20302;&#20110;&#29305;&#23450;&#38408;&#20540;&#26102;&#65292;&#27169;&#22411;&#23558;&#23637;&#29616;&#20986;&#31361;&#29616;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.15796</link><description>&lt;p&gt;
&#20174;&#25439;&#22833;&#35282;&#24230;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#29616;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Understanding Emergent Abilities of Language Models from the Loss Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#25439;&#22833;&#35282;&#24230;&#37325;&#26032;&#23450;&#20041;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#29616;&#33021;&#21147;&#65292;&#21457;&#29616;&#20855;&#26377;&#30456;&#21516;&#39044;&#35757;&#32451;&#25439;&#22833;&#30340;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#34920;&#29616;&#30456;&#20284;&#65292;&#32780;&#24403;&#39044;&#35757;&#32451;&#25439;&#22833;&#20302;&#20110;&#29305;&#23450;&#38408;&#20540;&#26102;&#65292;&#27169;&#22411;&#23558;&#23637;&#29616;&#20986;&#31361;&#29616;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#36136;&#30097;&#20102;&#20256;&#32479;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#29616;&#33021;&#21147;&#20165;&#23384;&#22312;&#20110;&#22823;&#27169;&#22411;&#20013;&#30340;&#35266;&#28857;&#12290;&#36825;&#31181;&#24576;&#30097;&#28304;&#33258;&#20004;&#28857;&#35266;&#23519;&#65306;1&#65289;&#36739;&#23567;&#30340;&#27169;&#22411;&#20063;&#33021;&#23637;&#29616;&#20986;&#23545;&#31361;&#29616;&#33021;&#21147;&#30340;&#39640;&#24615;&#33021;&#65307;2&#65289;&#36136;&#30097;&#29992;&#20110;&#27979;&#37327;&#36825;&#20123;&#33021;&#21147;&#30340;&#19981;&#36830;&#32493;&#24615;&#25351;&#26631;&#12290;&#26412;&#25991;&#25552;&#35758;&#20174;&#39044;&#35757;&#32451;&#25439;&#22833;&#30340;&#35282;&#24230;&#30740;&#31350;&#31361;&#29616;&#33021;&#21147;&#65292;&#32780;&#38750;&#27169;&#22411;&#22823;&#23567;&#25110;&#35757;&#32451;&#35745;&#31639;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20855;&#26377;&#30456;&#21516;&#39044;&#35757;&#32451;&#25439;&#22833;&#20294;&#19981;&#21516;&#27169;&#22411;&#21644;&#25968;&#25454;&#22823;&#23567;&#30340;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#30456;&#21516;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#24403;&#26576;&#19968;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25439;&#22833;&#20302;&#20110;&#29305;&#23450;&#38408;&#20540;&#26102;&#65292;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#31361;&#29616;&#33021;&#21147;&#65292;&#32780;&#19981;&#35770;&#25351;&#26631;&#30340;&#36830;&#32493;&#24615;&#22914;&#20309;&#65307;&#32780;&#22312;&#36798;&#21040;&#35813;&#38408;&#20540;&#20043;&#21069;&#65292;&#20854;&#24615;&#33021;&#20173;&#20445;&#25345;&#22312;&#38543;&#26426;&#29468;&#27979;&#27700;&#24179;&#12290;&#36825;&#21551;&#21457;&#25105;&#20204;&#37325;&#26032;&#23450;&#20041;&#31361;&#29616;&#33021;&#21147;&#20026;&#37027;&#20123;......
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15796v1 Announce Type: cross  Abstract: Recent studies have put into question the belief that emergent abilities in language models are exclusive to large models. This skepticism arises from two observations: 1) smaller models can also exhibit high performance on emergent abilities and 2) there is doubt on the discontinuous metrics used to measure these abilities. In this paper, we propose to study emergent abilities in the lens of pre-training loss, instead of model size or training compute. We demonstrate that the models with the same pre-training loss, but different model and data sizes, generate the same performance on various downstream tasks. We also discover that a model exhibits emergent abilities on certain tasks -- regardless of the continuity of metrics -- when its pre-training loss falls below a specific threshold. Before reaching this threshold, its performance remains at the level of random guessing. This inspires us to redefine emergent abilities as those that
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;PET&#25195;&#25551;&#22270;&#20687;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;&#26089;&#26399;&#26816;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65292;&#24182;&#19988;&#22312;&#20998;&#31867;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26102;&#20351;&#29992;&#20102;&#22810;&#31181;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.15443</link><description>&lt;p&gt;
&#24341;&#20837;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;PET&#25195;&#25551;&#22270;&#20687;&#26089;&#26399;&#26816;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;
&lt;/p&gt;
&lt;p&gt;
Introducing an ensemble method for the early detection of Alzheimer's disease through the analysis of PET scan images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15443
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;PET&#25195;&#25551;&#22270;&#20687;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;&#26089;&#26399;&#26816;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65292;&#24182;&#19988;&#22312;&#20998;&#31867;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26102;&#20351;&#29992;&#20102;&#22810;&#31181;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26159;&#19968;&#31181;&#36880;&#28176;&#24694;&#21270;&#30340;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#65292;&#20027;&#35201;&#24433;&#21709;&#35760;&#24518;&#12289;&#24605;&#32500;&#21644;&#34892;&#20026;&#31561;&#35748;&#30693;&#21151;&#33021;&#12290;&#26412;&#30149;&#23384;&#22312;&#19968;&#20010;&#20851;&#38190;&#38454;&#27573;&#65292;&#21363;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#65292;&#38750;&#24120;&#37325;&#35201;&#23613;&#26089;&#35786;&#26029;&#65292;&#22240;&#20026;&#19968;&#20123;&#36880;&#28176;&#21457;&#23637;&#20026;&#30149;&#30151;&#30340;MCI&#24739;&#32773;&#20250;&#21457;&#23637;&#20026;&#36825;&#31181;&#30142;&#30149;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#20998;&#31867;&#20026;&#22235;&#20010;&#19981;&#21516;&#32452;&#65306;&#25511;&#21046;&#27491;&#24120;&#65288;CN&#65289;&#12289;&#36880;&#28176;&#21457;&#23637;&#30340;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#65288;pMCI&#65289;&#12289;&#31283;&#23450;&#30340;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#65288;sMCI&#65289;&#21644;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36825;&#31181;&#20998;&#31867;&#26159;&#22522;&#20110;&#23545;&#20174;ADNI&#25968;&#25454;&#38598;&#33719;&#24471;&#30340;PET&#25195;&#25551;&#22270;&#20687;&#30340;&#24443;&#24213;&#26816;&#26597;&#65292;&#36825;&#25552;&#20379;&#20102;&#23545;&#30142;&#30149;&#36827;&#23637;&#30340;&#24443;&#24213;&#29702;&#35299;&#12290;&#24050;&#32463;&#20351;&#29992;&#20102;&#20960;&#31181;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#26816;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20351;&#29992;&#20102;&#19977;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21363;VGG16&#12289;AlexNet&#21644;&#33258;&#23450;&#20041;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15443v1 Announce Type: cross  Abstract: Alzheimer's disease is a progressive neurodegenerative disorder that primarily affects cognitive functions such as memory, thinking, and behavior. In this disease, there is a critical phase, mild cognitive impairment, that is really important to be diagnosed early since some patients with progressive MCI will develop the disease. This study delves into the challenging task of classifying Alzheimer's disease into four distinct groups: control normal (CN), progressive mild cognitive impairment (pMCI), stable mild cognitive impairment (sMCI), and Alzheimer's disease (AD). This classification is based on a thorough examination of PET scan images obtained from the ADNI dataset, which provides a thorough understanding of the disease's progression. Several deep-learning and traditional machine-learning models have been used to detect Alzheimer's disease. In this paper, three deep-learning models, namely VGG16 and AlexNet, and a custom Convolu
&lt;/p&gt;</description></item><item><title>PruMerge&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#35270;&#35273;&#20196;&#29260;&#20943;&#23569;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#20196;&#29260;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.15388</link><description>&lt;p&gt;
LLaVA-PruMerge: &#33258;&#36866;&#24212;&#20196;&#29260;&#20943;&#23569;&#29992;&#20110;&#39640;&#25928;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15388
&lt;/p&gt;
&lt;p&gt;
PruMerge&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#35270;&#35273;&#20196;&#29260;&#20943;&#23569;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#20196;&#29260;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMMs)&#36890;&#36807;&#36830;&#25509;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;LMMs&#21253;&#25324;&#20102;&#26356;&#22797;&#26434;&#30340;&#35270;&#35273;&#36755;&#20837;&#65292;&#22914;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21644;&#35270;&#39057;&#65292;&#36825;&#26174;&#33879;&#22686;&#21152;&#20102;&#35270;&#35273;&#20196;&#29260;&#30340;&#25968;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#20196;&#29260;&#20943;&#23569;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#31867;&#20284;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#35768;&#22810;&#35270;&#35273;&#20196;&#29260;&#22312;&#31354;&#38388;&#19978;&#26159;&#20887;&#20313;&#30340;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PruMerge&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#35270;&#35273;&#20196;&#29260;&#20943;&#23569;&#26041;&#27861;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35270;&#35273;&#20196;&#29260;&#30340;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#21487;&#27604;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15388v1 Announce Type: cross  Abstract: Large Multimodal Models (LMMs) have shown significant reasoning capabilities by connecting a visual encoder and a large language model. LMMs typically use a fixed amount of visual tokens, such as the penultimate layer features in the CLIP visual encoder, as the prefix content. Recent LMMs incorporate more complex visual inputs, such as high-resolution images and videos, which increase the number of visual tokens significantly. However, due to the design of the Transformer architecture, computational costs associated with these models tend to increase quadratically with the number of input tokens. To tackle this problem, we explore a token reduction mechanism and find, similar to prior work, that many visual tokens are spatially redundant. Based on this, we propose PruMerge, a novel adaptive visual token reduction approach, which largely reduces the number of visual tokens while maintaining comparable model performance. We first select 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20020;&#24202;&#21307;&#29983;&#23545;LLMs&#30340;&#20449;&#20219;&#12289;&#25968;&#25454;&#26469;&#28304;&#20174;&#20027;&#35201;&#26159;&#20154;&#31867;&#29983;&#25104;&#21040;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#30340;&#36716;&#21464;&#65292;&#20197;&#21450;&#38543;&#20043;&#32780;&#26469;&#23545;LLMs&#31934;&#30830;&#24615;&#21644;&#20020;&#24202;&#21307;&#24072;&#33021;&#21147;&#30340;&#24433;&#21709;&#20043;&#38388;&#30340;&#19981;&#26029;&#21457;&#23637;&#30340;&#20851;&#31995;&#65292;&#24378;&#35843;&#20102;LLMs&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#25972;&#21512;&#21152;&#28145;&#21487;&#33021;&#24102;&#26469;&#30340;&#25361;&#25112;&#21644;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2403.14691</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#29992;&#25143;&#20449;&#20219;&#65306;&#20197;&#21307;&#30103;&#20026;&#37325;&#28857;
&lt;/p&gt;
&lt;p&gt;
Large Language Models and User Trust: Focus on Healthcare
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20020;&#24202;&#21307;&#29983;&#23545;LLMs&#30340;&#20449;&#20219;&#12289;&#25968;&#25454;&#26469;&#28304;&#20174;&#20027;&#35201;&#26159;&#20154;&#31867;&#29983;&#25104;&#21040;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#30340;&#36716;&#21464;&#65292;&#20197;&#21450;&#38543;&#20043;&#32780;&#26469;&#23545;LLMs&#31934;&#30830;&#24615;&#21644;&#20020;&#24202;&#21307;&#24072;&#33021;&#21147;&#30340;&#24433;&#21709;&#20043;&#38388;&#30340;&#19981;&#26029;&#21457;&#23637;&#30340;&#20851;&#31995;&#65292;&#24378;&#35843;&#20102;LLMs&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#25972;&#21512;&#21152;&#28145;&#21487;&#33021;&#24102;&#26469;&#30340;&#25361;&#25112;&#21644;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20020;&#24202;&#21307;&#29983;&#23545;LLMs&#30340;&#20449;&#20219;&#12289;&#25968;&#25454;&#26469;&#28304;&#20174;&#20027;&#35201;&#26159;&#20154;&#31867;&#29983;&#25104;&#21040;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#30340;&#36716;&#21464;&#65292;&#20197;&#21450;&#38543;&#20043;&#32780;&#26469;&#23545;LLMs&#31934;&#30830;&#24615;&#21644;&#20020;&#24202;&#21307;&#24072;&#33021;&#21147;&#30340;&#24433;&#21709;&#20043;&#38388;&#30340;&#19981;&#26029;&#21457;&#23637;&#30340;&#20851;&#31995;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#38543;&#30528;LLMs&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#36234;&#26469;&#36234;&#20381;&#36182;&#23427;&#20204;&#30340;&#36755;&#20986;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#36755;&#20986;&#36136;&#37327;&#19979;&#38477;&#65292;&#24182;&#23548;&#33268;&#20020;&#24202;&#21307;&#24072;&#25216;&#33021;&#20943;&#23569;&#65292;&#22240;&#20026;&#20182;&#20204;&#19982;&#22522;&#26412;&#35786;&#26029;&#36807;&#31243;&#30340;&#21442;&#19982;&#20943;&#23569;&#12290;&#23613;&#31649;&#30446;&#21069;&#36824;&#22788;&#20110;&#29702;&#35770;&#38454;&#27573;&#65292;&#20294;&#36825;&#31181;&#21453;&#39304;&#24490;&#29615;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;LLMs&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#25972;&#21512;&#21152;&#28145;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#31215;&#26497;&#23545;&#35805;&#21644;&#25112;&#30053;&#25514;&#26045;&#65292;&#20197;&#30830;&#20445;LLM&#25216;&#26415;&#30340;&#23433;&#20840;&#26377;&#25928;&#20351;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#19982;LLMs&#33258;&#25105;&#21442;&#32771;&#23398;&#20064;&#24490;&#29615;&#20197;&#21450;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#25216;&#33021;&#19979;&#38477;&#30456;&#20851;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14691v1 Announce Type: cross  Abstract: This paper explores the evolving relationship between clinician trust in LLMs, the transformation of data sources from predominantly human-generated to AI-generated content, and the subsequent impact on the precision of LLMs and clinician competence. One of the primary concerns identified is the potential feedback loop that arises as LLMs become more reliant on their outputs for learning, which may lead to a degradation in output quality and a reduction in clinician skills due to decreased engagement with fundamental diagnostic processes. While theoretical at this stage, this feedback loop poses a significant challenge as the integration of LLMs in healthcare deepens, emphasizing the need for proactive dialogue and strategic measures to ensure the safe and effective use of LLM technology. Moreover, we delve into the potential risks associated with LLMs' self-referential learning loops and the deskilling of healthcare professionals. The
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;A$^3$T&#26694;&#26550;&#65292;&#36890;&#36807;ActRe&#25552;&#31034;&#20195;&#29702;&#23454;&#29616;&#20102;ReAct&#39118;&#26684;&#20195;&#29702;&#23545;&#20195;&#29702;&#36712;&#36857;&#30340;&#33258;&#20027;&#26631;&#27880;&#65292;&#21516;&#26102;&#22686;&#24378;&#20102;&#26032;&#30340;&#36712;&#36857;&#21512;&#25104;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.14589</link><description>&lt;p&gt;
ReAct&#36935;&#19978;ActRe&#65306;&#23545;&#27604;&#24615;&#33258;&#35757;&#32451;&#20013;&#30340;&#20195;&#29702;&#36712;&#36857;&#33258;&#21160;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for Contrastive Self-Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14589
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;A$^3$T&#26694;&#26550;&#65292;&#36890;&#36807;ActRe&#25552;&#31034;&#20195;&#29702;&#23454;&#29616;&#20102;ReAct&#39118;&#26684;&#20195;&#29702;&#23545;&#20195;&#29702;&#36712;&#36857;&#30340;&#33258;&#20027;&#26631;&#27880;&#65292;&#21516;&#26102;&#22686;&#24378;&#20102;&#26032;&#30340;&#36712;&#36857;&#21512;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14589v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25991;&#25688;&#65306;&#35821;&#35328;&#20195;&#29702;&#36890;&#36807;&#19982;&#22522;&#30784;&#27169;&#22411;&#25512;&#29702;&#23637;&#31034;&#20102;&#33258;&#20027;&#20915;&#31574;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#36890;&#36807;&#22810;&#27493;&#25512;&#29702;&#21644;&#34892;&#21160;&#36712;&#36857;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#26469;&#35757;&#32451;&#35821;&#35328;&#20195;&#29702;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#36825;&#26679;&#30340;&#36712;&#36857;&#20173;&#38656;&#35201;&#30456;&#24403;&#22823;&#30340;&#20154;&#21147;&#65292;&#26080;&#35770;&#26159;&#36890;&#36807;&#20154;&#24037;&#26631;&#27880;&#36824;&#26159;&#23454;&#26045;&#22810;&#26679;&#21270;&#25552;&#31034;&#26694;&#26550;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;A$^3$T&#65292;&#19968;&#20010;&#20801;&#35768;&#20197;ReAct&#39118;&#26684;&#33258;&#20027;&#27880;&#37322;&#20195;&#29702;&#36712;&#36857;&#30340;&#26694;&#26550;&#12290;&#20854;&#20013;&#24515;&#26159;&#19968;&#20010;ActRe&#25552;&#31034;&#20195;&#29702;&#65292;&#23427;&#35299;&#37322;&#20219;&#24847;&#21160;&#20316;&#30340;&#21407;&#22240;&#12290;&#24403;&#38543;&#26426;&#25277;&#21462;&#22806;&#37096;&#21160;&#20316;&#26102;&#65292;ReAct&#39118;&#26684;&#20195;&#29702;&#21487;&#20197;&#26597;&#35810;ActRe&#20195;&#29702;&#20197;&#33719;&#21462;&#20854;&#25991;&#26412;&#29702;&#30001;&#12290;&#26032;&#39062;&#30340;&#36712;&#36857;&#28982;&#21518;&#36890;&#36807;&#23558;ActRe&#30340;&#21518;&#39564;&#25512;&#29702;&#21069;&#32622;&#21040;&#25277;&#26679;&#21160;&#20316;&#20013;&#36827;&#34892;&#32508;&#21512;&#21512;&#25104;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;ReAct&#39118;&#26684;&#20195;&#29702;&#21487;&#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14589v1 Announce Type: new  Abstract: Language agents have demonstrated autonomous decision-making abilities by reasoning with foundation models. Recently, efforts have been made to train language agents for performance improvement, with multi-step reasoning and action trajectories as the training data. However, collecting such trajectories still requires considerable human effort, by either artificial annotations or implementations of diverse prompting frameworks. In this work, we propose A$^3$T, a framework that enables the Autonomous Annotation of Agent Trajectories in the style of ReAct. The central role is an ActRe prompting agent, which explains the reason for an arbitrary action. When randomly sampling an external action, the ReAct-style agent could query the ActRe agent with the action to obtain its textual rationales. Novel trajectories are then synthesized by prepending the posterior reasoning from ActRe to the sampled action. In this way, the ReAct-style agent exe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#36807;&#31243;&#20013;&#36890;&#36807;&#21033;&#29992;CLIP&#30340;&#22266;&#26377;&#23646;&#24615;&#26469;&#25506;&#35752;&#26657;&#20934;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#25552;&#31034;&#36873;&#25321;&#26174;&#33879;&#24433;&#21709;&#20102;CLIP&#20013;&#30340;&#26657;&#20934;&#65292;&#20854;&#20013;&#23548;&#33268;&#26356;&#39640;&#25991;&#26412;&#29305;&#24449;&#31163;&#25955;&#24615;&#30340;&#25552;&#31034;&#20250;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.14119</link><description>&lt;p&gt;
C-TPT&#65306;&#36890;&#36807;&#25991;&#26412;&#29305;&#24449;&#31163;&#25955;&#24615;&#30340;&#26657;&#20934;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#36807;&#31243;&#20013;&#36890;&#36807;&#21033;&#29992;CLIP&#30340;&#22266;&#26377;&#23646;&#24615;&#26469;&#25506;&#35752;&#26657;&#20934;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#25552;&#31034;&#36873;&#25321;&#26174;&#33879;&#24433;&#21709;&#20102;CLIP&#20013;&#30340;&#26657;&#20934;&#65292;&#20854;&#20013;&#23548;&#33268;&#26356;&#39640;&#25991;&#26412;&#29305;&#24449;&#31163;&#25955;&#24615;&#30340;&#25552;&#31034;&#20250;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#27979;&#35797;&#26102;&#36866;&#24212;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#20316;&#20026;&#19968;&#31181;&#22312;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#26041;&#27861;&#12290;&#19968;&#20010;&#20027;&#35201;&#30340;&#20363;&#35777;&#26159;&#26368;&#36817;&#25552;&#20986;&#30340;&#29992;&#20110;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#30340;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25552;&#31034;&#20027;&#35201;&#26159;&#20026;&#20102;&#25552;&#39640;&#20934;&#30830;&#24615;&#32780;&#24320;&#21457;&#30340;&#65292;&#24573;&#35270;&#20102;&#26657;&#20934;&#30340;&#37325;&#35201;&#24615;&#8212;&#8212;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26657;&#20934;&#26041;&#27861;&#20381;&#36182;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#27979;&#35797;&#26102;&#22330;&#26223;&#19979;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;CLIP&#30340;&#22266;&#26377;&#23646;&#24615;&#65292;&#22312;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#36807;&#31243;&#20013;&#25506;&#35752;&#26657;&#20934;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#35266;&#23519;&#65292;&#25105;&#20204;&#21457;&#29616;&#25552;&#31034;&#36873;&#25321;&#26174;&#33879;&#24433;&#21709;&#20102;CLIP&#20013;&#30340;&#26657;&#20934;&#65292;&#20854;&#20013;&#23548;&#33268;&#26356;&#39640;&#25991;&#26412;&#29305;&#24449;&#31163;&#25955;&#24615;&#30340;&#25552;&#31034;&#20250;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14119v1 Announce Type: cross  Abstract: In deep learning, test-time adaptation has gained attention as a method for model fine-tuning without the need for labeled data. A prime exemplification is the recently proposed test-time prompt tuning for large-scale vision-language models such as CLIP. Unfortunately, these prompts have been mainly developed to improve accuracy, overlooking the importance of calibration-a crucial aspect for quantifying prediction uncertainty. However, traditional calibration methods rely on substantial amounts of labeled data, making them impractical for test-time scenarios. To this end, this paper explores calibration during test-time prompt tuning by leveraging the inherent properties of CLIP. Through a series of observations, we find that the prompt choice significantly affects the calibration in CLIP, where the prompts leading to higher text feature dispersion result in better-calibrated predictions. Introducing the Average Text Feature Dispersion
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Zigzag Mamba&#30340;&#38646;&#21442;&#25968;&#26041;&#27861;&#65292;&#36890;&#36807;&#32416;&#27491;&#24403;&#21069;Mamba-based&#35270;&#35273;&#26041;&#27861;&#20013;&#23545;&#31354;&#38388;&#36830;&#32493;&#24615;&#30340;&#24573;&#35270;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36895;&#24230;&#21644;&#20869;&#23384;&#21033;&#29992;&#65292;&#21516;&#26102;&#22312;&#22823;&#20998;&#36776;&#29575;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13802</link><description>&lt;p&gt;
ZigMa&#65306;&#34623;&#34578;&#26364;&#24052;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ZigMa: Zigzag Mamba Diffusion Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Zigzag Mamba&#30340;&#38646;&#21442;&#25968;&#26041;&#27861;&#65292;&#36890;&#36807;&#32416;&#27491;&#24403;&#21069;Mamba-based&#35270;&#35273;&#26041;&#27861;&#20013;&#23545;&#31354;&#38388;&#36830;&#32493;&#24615;&#30340;&#24573;&#35270;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36895;&#24230;&#21644;&#20869;&#23384;&#21033;&#29992;&#65292;&#21516;&#26102;&#22312;&#22823;&#20998;&#36776;&#29575;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#21463;&#21040;&#21487;&#20280;&#32553;&#24615;&#21644;&#20108;&#27425;&#22797;&#26434;&#24615;&#38382;&#39064;&#30340;&#22256;&#25200;&#65292;&#29305;&#21035;&#26159;&#22312;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#32467;&#26500;&#20869;&#37096;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#21033;&#29992;&#19968;&#31181;&#31216;&#20026;&#26364;&#24052;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#38271;&#24207;&#21015;&#24314;&#27169;&#33021;&#21147;&#65292;&#20197;&#25193;&#23637;&#20854;&#22312;&#35270;&#35273;&#25968;&#25454;&#29983;&#25104;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22823;&#22810;&#25968;&#24403;&#21069;&#22522;&#20110;&#26364;&#24052;&#30340;&#35270;&#35273;&#26041;&#27861;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#30095;&#24573;&#65292;&#21363;&#26364;&#24052;&#30340;&#25195;&#25551;&#26041;&#26696;&#20013;&#32570;&#20047;&#23545;&#31354;&#38388;&#36830;&#32493;&#24615;&#30340;&#32771;&#34385;&#12290;&#20854;&#27425;&#65292;&#22522;&#20110;&#36825;&#19968;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Zigzag Mamba&#30340;&#31616;&#21333;&#12289;&#21363;&#25554;&#21363;&#29992;&#12289;&#38646;&#21442;&#25968;&#26041;&#27861;&#65292;&#23427;&#20248;&#20110;&#22522;&#20110;&#26364;&#24052;&#30340;&#22522;&#32447;&#65292;&#24182;&#34920;&#29616;&#20986;&#27604;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#22522;&#32447;&#26356;&#24555;&#36895;&#21644;&#26356;&#22909;&#30340;&#20869;&#23384;&#21033;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;Zigzag Mamba&#38598;&#25104;&#21040;&#38543;&#26426;&#25554;&#20540;&#26694;&#26550;&#20013;&#65292;&#20197;&#30740;&#31350;&#27169;&#22411;&#22312;&#22823;&#20998;&#36776;&#29575;&#35270;&#35273;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;FacesHQ $1024\times 1024$&#21644;UCF101&#65292;MultiModal-CelebA-HQ&#65289;&#19978;&#30340;&#21487;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13802v1 Announce Type: cross  Abstract: The diffusion model has long been plagued by scalability and quadratic complexity issues, especially within transformer-based structures. In this study, we aim to leverage the long sequence modeling capability of a State-Space Model called Mamba to extend its applicability to visual data generation. Firstly, we identify a critical oversight in most current Mamba-based vision methods, namely the lack of consideration for spatial continuity in the scan scheme of Mamba. Secondly, building upon this insight, we introduce a simple, plug-and-play, zero-parameter method named Zigzag Mamba, which outperforms Mamba-based baselines and demonstrates improved speed and memory utilization compared to transformer-based baselines. Lastly, we integrate Zigzag Mamba with the Stochastic Interpolant framework to investigate the scalability of the model on large-resolution visual datasets, such as FacesHQ $1024\times 1024$ and UCF101, MultiModal-CelebA-HQ
&lt;/p&gt;</description></item><item><title>CAL-MAPF&#24341;&#20837;&#20102;&#32531;&#23384;&#26426;&#21046;&#26469;&#22686;&#24378;&#32456;&#36523;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#21512;&#36866;&#30340;&#36755;&#20837;&#20219;&#21153;&#20998;&#24067;&#12289;&#39640;&#32531;&#23384;&#21629;&#20013;&#29575;&#21644;&#27969;&#30021;&#30340;&#20132;&#36890;&#36825;&#19977;&#20010;&#22240;&#32032;&#26174;&#33879;&#25552;&#39640;&#20102;&#35299;&#20915;&#26041;&#26696;&#30340;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13421</link><description>&lt;p&gt;
&#22522;&#20110;&#32531;&#23384;&#22686;&#24378;&#30340;&#32456;&#36523;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Caching-Augmented Lifelong Multi-Agent Path Finding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13421
&lt;/p&gt;
&lt;p&gt;
CAL-MAPF&#24341;&#20837;&#20102;&#32531;&#23384;&#26426;&#21046;&#26469;&#22686;&#24378;&#32456;&#36523;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#21512;&#36866;&#30340;&#36755;&#20837;&#20219;&#21153;&#20998;&#24067;&#12289;&#39640;&#32531;&#23384;&#21629;&#20013;&#29575;&#21644;&#27969;&#30021;&#30340;&#20132;&#36890;&#36825;&#19977;&#20010;&#22240;&#32032;&#26174;&#33879;&#25552;&#39640;&#20102;&#35299;&#20915;&#26041;&#26696;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010; (Multi-Agent Path Finding&#65292;MAPF) &#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#28041;&#21450;&#20026;&#22810;&#20010;&#26426;&#22120;&#20154;&#25214;&#21040;&#26080;&#30896;&#25758;&#36335;&#24452;&#12290;&#32456;&#36523;MAPF&#20250;&#22312;&#20195;&#29702;&#23436;&#25104;&#20854;&#21021;&#22987;&#30446;&#26631;&#21518;&#31435;&#21363;&#23558;&#30446;&#26631;&#37325;&#26032;&#20998;&#37197;&#32473;&#20195;&#29702;&#65292;&#25552;&#20379;&#23545;&#23454;&#38469;&#20179;&#24211;&#35268;&#21010;&#30340;&#26356;&#20934;&#30830;&#36924;&#36817;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32531;&#23384;&#22686;&#24378;&#32456;&#36523;MAPF (CAL-MAPF)&#30340;&#26032;&#26426;&#21046;&#65292;&#26088;&#22312;&#25552;&#39640;&#32456;&#36523;MAPF&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31216;&#20026;&#32531;&#23384;&#30340;&#20020;&#26102;&#29289;&#21697;&#23384;&#20648;&#21644;&#26367;&#25442;&#30340;&#26032;&#31867;&#22411;&#22320;&#22270;&#32593;&#26684;&#65292;&#24182;&#20026;&#20854;&#35774;&#35745;&#20102;&#19968;&#31181;&#38145;&#23450;&#26426;&#21046;&#65292;&#20197;&#25552;&#39640;&#35268;&#21010;&#35299;&#20915;&#26041;&#26696;&#30340;&#31283;&#23450;&#24615;&#12290;&#35813;&#32531;&#23384;&#26426;&#21046;&#32463;&#36807;&#21508;&#31181;&#32531;&#23384;&#26367;&#25442;&#31574;&#30053;&#21644;&#19968;&#31995;&#21015;&#36755;&#20837;&#20219;&#21153;&#20998;&#24067;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#30830;&#23450;&#20102;&#19977;&#20010;&#26174;&#33879;&#24433;&#21709;CAL-MAPF&#24615;&#33021;&#30340;&#20027;&#35201;&#22240;&#32032;&#65306;&#21512;&#36866;&#30340;&#36755;&#20837;&#20219;&#21153;&#20998;&#24067;&#12289;&#39640;&#32531;&#23384;&#21629;&#20013;&#29575;&#21644;&#27969;&#30021;&#30340;&#20132;&#36890;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13421v2 Announce Type: replace-cross  Abstract: Multi-Agent Path Finding (MAPF), which involves finding collision-free paths for multiple robots, is crucial in various applications. Lifelong MAPF, where targets are reassigned to agents as soon as they complete their initial targets, offers a more accurate approximation of real-world warehouse planning. In this paper, we present a novel mechanism named Caching-Augmented Lifelong MAPF (CAL-MAPF), designed to improve the performance of Lifelong MAPF. We have developed a new type of map grid called cache for temporary item storage and replacement, and designed a locking mechanism for it to improve the stability of the planning solution. This cache mechanism was evaluated using various cache replacement policies and a spectrum of input task distributions. We identified three main factors significantly impacting CAL-MAPF performance through experimentation: suitable input task distribution, high cache hit rate, and smooth traffic.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21019;&#24314;&#20351;&#29992; GPT-2 &#30340;&#26426;&#22120;&#20154;&#36134;&#25143;&#65292;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#22238;&#22797;&#29992;&#25143;&#30340;&#25512;&#25991;&#65292;&#40723;&#21169;&#29992;&#25143;&#25509;&#35302;&#21644;&#20851;&#27880;&#39564;&#35777;&#30340;&#12289;&#24847;&#35782;&#24418;&#24577;&#24179;&#34913;&#30340;&#26032;&#38395;&#65292;&#20197;&#22686;&#21152;&#29992;&#25143;&#25509;&#35302;&#36825;&#20123;&#26032;&#38395;&#24182;&#25552;&#39640;&#21442;&#19982;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.13362</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30495;&#23454;&#26426;&#22120;&#20154;&#36134;&#25143;&#28608;&#21169;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#26032;&#38395;&#28040;&#36153;
&lt;/p&gt;
&lt;p&gt;
Incentivizing News Consumption on Social Media Platforms Using Large Language Models and Realistic Bot Accounts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13362
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21019;&#24314;&#20351;&#29992; GPT-2 &#30340;&#26426;&#22120;&#20154;&#36134;&#25143;&#65292;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#22238;&#22797;&#29992;&#25143;&#30340;&#25512;&#25991;&#65292;&#40723;&#21169;&#29992;&#25143;&#25509;&#35302;&#21644;&#20851;&#27880;&#39564;&#35777;&#30340;&#12289;&#24847;&#35782;&#24418;&#24577;&#24179;&#34913;&#30340;&#26032;&#38395;&#65292;&#20197;&#22686;&#21152;&#29992;&#25143;&#25509;&#35302;&#36825;&#20123;&#26032;&#38395;&#24182;&#25552;&#39640;&#21442;&#19982;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26497;&#21270;&#12289;&#20449;&#20219;&#19979;&#38477;&#20197;&#21450;&#23545;&#27665;&#20027;&#35268;&#33539;&#25903;&#25345;&#21160;&#25671;&#26159;&#32654;&#22269;&#27665;&#20027;&#38754;&#20020;&#30340;&#32039;&#36843;&#23041;&#32961;&#12290;&#25509;&#35302;&#39564;&#35777;&#21644;&#20248;&#36136;&#26032;&#38395;&#21487;&#33021;&#38477;&#20302;&#20010;&#20154;&#23545;&#36825;&#20123;&#23041;&#32961;&#30340;&#26131;&#24863;&#24615;&#65292;&#24182;&#20351;&#20844;&#27665;&#26356;&#20855;&#25239;&#20987;&#38169;&#35823;&#20449;&#24687;&#12289;&#27665;&#31929;&#20027;&#20041;&#21644;&#26497;&#31471;&#20826;&#27966;&#35328;&#35770;&#30340;&#33021;&#21147;&#12290;&#35813;&#39033;&#30446;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#19968;&#20010;&#29983;&#24577;&#26377;&#25928;&#30340;&#29615;&#22659;&#20013;&#22686;&#24378;&#29992;&#25143;&#25509;&#35302;&#21644;&#21442;&#19982;&#39564;&#35777;&#30340;&#12289;&#24847;&#35782;&#24418;&#24577;&#24179;&#34913;&#30340;&#26032;&#38395;&#12290;&#25105;&#20204;&#20381;&#36182;&#20110;&#23545; 28,457 &#20010; Twitter &#29992;&#25143;&#36827;&#34892;&#30340;&#22823;&#35268;&#27169;&#20026;&#26399;&#20004;&#21608;&#30340;&#30000;&#37326;&#23454;&#39564;&#65288;&#20174; 2023 &#24180; 1 &#26376; 19 &#26085;&#21040; 2 &#26376; 3 &#26085;&#65289;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102; 28 &#20010;&#21033;&#29992; GPT-2 &#30340;&#26426;&#22120;&#20154;&#65292;&#22312;&#29992;&#25143;&#21457;&#34920;&#26377;&#20851;&#20307;&#32946;&#12289;&#23089;&#20048;&#25110;&#29983;&#27963;&#26041;&#24335;&#30340;&#25512;&#25991;&#26102;&#22238;&#22797;&#19968;&#20010;&#20869;&#23481;&#30456;&#20851;&#30340;&#22238;&#22797;&#65292;&#20854;&#20013;&#21253;&#21547;&#20004;&#20010;&#30828;&#20195;&#30721;&#20803;&#32032;&#65306;&#19968;&#20010;&#25351;&#21521;&#20248;&#36136;&#26032;&#38395;&#26426;&#26500;&#30456;&#20851;&#20027;&#39064;&#37096;&#20998;&#30340; URL &#21644;&#40723;&#21169;&#20851;&#27880;&#20854; Twitter &#36134;&#25143;&#12290;&#20026;&#36827;&#19968;&#27493;&#27979;&#35797;&#26426;&#22120;&#20154;&#23545;&#24615;&#21035;&#30340;&#24046;&#24322;&#24433;&#21709;&#65292;&#34987;&#35797;&#29992;&#25143;&#34987;&#38543;&#26426;&#20998;&#37197;&#20197;&#25509;&#21463;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13362v1 Announce Type: cross  Abstract: Polarization, declining trust, and wavering support for democratic norms are pressing threats to U.S. democracy. Exposure to verified and quality news may lower individual susceptibility to these threats and make citizens more resilient to misinformation, populism, and hyperpartisan rhetoric. This project examines how to enhance users' exposure to and engagement with verified and ideologically balanced news in an ecologically valid setting. We rely on a large-scale two-week long field experiment (from 1/19/2023 to 2/3/2023) on 28,457 Twitter users. We created 28 bots utilizing GPT-2 that replied to users tweeting about sports, entertainment, or lifestyle with a contextual reply containing two hardcoded elements: a URL to the topic-relevant section of quality news organization and an encouragement to follow its Twitter account. To further test differential effects by gender of the bots, treated users were randomly assigned to receive re
&lt;/p&gt;</description></item><item><title>STG-Mamba &#26159;&#39318;&#20010;&#21033;&#29992;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;&#26102;&#31354;&#22270;&#23398;&#20064;&#30340;&#30740;&#31350;&#65292;&#23558;STG&#32593;&#32476;&#35270;&#20026;&#31995;&#32479;&#65292;&#24182;&#37319;&#29992;&#22270;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22359;&#65288;GS3B&#65289;&#31934;&#30830;&#34920;&#24449;STG&#30340;&#21160;&#24577;&#28436;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.12418</link><description>&lt;p&gt;
STG-Mamba: &#36890;&#36807;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;&#26102;&#31354;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
STG-Mamba: Spatial-Temporal Graph Learning via Selective State Space Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12418
&lt;/p&gt;
&lt;p&gt;
STG-Mamba &#26159;&#39318;&#20010;&#21033;&#29992;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;&#26102;&#31354;&#22270;&#23398;&#20064;&#30340;&#30740;&#31350;&#65292;&#23558;STG&#32593;&#32476;&#35270;&#20026;&#31995;&#32479;&#65292;&#24182;&#37319;&#29992;&#22270;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22359;&#65288;GS3B&#65289;&#31934;&#30830;&#34920;&#24449;STG&#30340;&#21160;&#24577;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Spatial-Temporal Graph&#65288;STG&#65289;&#25968;&#25454;&#20855;&#26377;&#21160;&#24577;&#24615;&#12289;&#24322;&#36136;&#24615;&#21644;&#38750;&#24179;&#31283;&#24615;&#29305;&#28857;&#65292;&#23548;&#33268;&#26102;&#31354;&#22270;&#23398;&#20064;&#25345;&#32493;&#38754;&#20020;&#25361;&#25112;&#12290;&#36817;&#24180;&#26469;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#22522;&#20110;GNN&#30340;&#26041;&#27861;&#65292;&#20027;&#35201;&#38598;&#20013;&#20110;&#27169;&#25311;STG&#32593;&#32476;&#20013;&#33410;&#28857;&#20010;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24573;&#30053;&#20102;&#38543;&#26102;&#38388;&#23384;&#22312;&#30340;STG&#31995;&#32479;&#26412;&#36136;&#29305;&#24449;&#30340;&#24314;&#27169;&#37325;&#35201;&#24615;&#12290;&#30456;&#21453;&#65292;&#29616;&#20195;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSSMs&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;STG&#32593;&#32476;&#35270;&#20026;&#31995;&#32479;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#31934;&#24515;&#25506;&#32034;&#20102;STG&#31995;&#32479;&#22312;&#26102;&#38388;&#32500;&#24230;&#19978;&#30340;&#21160;&#24577;&#29366;&#24577;&#28436;&#21464;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Spatial-Temporal Graph Mamba&#65288;STG-Mamba&#65289;&#65292;&#20316;&#20026;&#39318;&#20010;&#21033;&#29992;&#24378;&#22823;&#30340;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;STG&#23398;&#20064;&#30340;&#30740;&#31350;&#65292;&#23558;STG&#32593;&#32476;&#35270;&#20026;&#31995;&#32479;&#65292;&#24182;&#37319;&#29992;&#22270;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22359;&#65288;GS3B&#65289;&#31934;&#30830;&#34920;&#24449;STG&#30340;&#21160;&#24577;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12418v1 Announce Type: cross  Abstract: Spatial-Temporal Graph (STG) data is characterized as dynamic, heterogenous, and non-stationary, leading to the continuous challenge of spatial-temporal graph learning. In the past few years, various GNN-based methods have been proposed to solely focus on mimicking the relationships among node individuals of the STG network, ignoring the significance of modeling the intrinsic features that exist in STG system over time. In contrast, modern Selective State Space Models (SSSMs) present a new approach which treat STG Network as a system, and meticulously explore the STG system's dynamic state evolution across temporal dimension. In this work, we introduce Spatial-Temporal Graph Mamba (STG-Mamba) as the first exploration of leveraging the powerful selective state space models for STG learning by treating STG Network as a system, and employing the Graph Selective State Space Block (GS3B) to precisely characterize the dynamic evolution of ST
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#22270;&#20687;&#36136;&#37327;&#30340;&#26032;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#30830;&#20445;&#25991;&#26412;&#21644;&#22270;&#20687;&#20869;&#23481;&#30340;&#23545;&#40784;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#27861;&#26469;&#24402;&#32435;&#36825;&#20123;&#25351;&#26631;</title><link>https://arxiv.org/abs/2403.11821</link><description>&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#65306;&#22270;&#20687;&#36136;&#37327;&#24230;&#37327;&#30340;&#35843;&#26597;&#19982;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Evaluating Text to Image Synthesis: Survey and Taxonomy of Image Quality Metrics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11821
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#22270;&#20687;&#36136;&#37327;&#30340;&#26032;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#30830;&#20445;&#25991;&#26412;&#21644;&#22270;&#20687;&#20869;&#23481;&#30340;&#23545;&#40784;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#27861;&#26469;&#24402;&#32435;&#36825;&#20123;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#21644;&#35270;&#35273;&#32467;&#21512;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#25512;&#21160;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20114;&#32852;&#32593;&#25110;&#20854;&#20182;&#22823;&#35268;&#27169;&#25968;&#25454;&#24211;&#20013;&#30340;&#28023;&#37327;&#25991;&#26412;-&#22270;&#20687;&#23545;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#12290;&#38543;&#30528;&#23545;&#39640;&#36136;&#37327;&#22270;&#20687;&#29983;&#25104;&#30340;&#38656;&#27714;&#36716;&#21521;&#30830;&#20445;&#25991;&#26412;&#19982;&#22270;&#20687;&#20043;&#38388;&#30340;&#20869;&#23481;&#23545;&#40784;&#65292;&#24050;&#24320;&#21457;&#20102;&#26032;&#39062;&#30340;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#65292;&#26088;&#22312;&#27169;&#25311;&#20154;&#31867;&#21028;&#26029;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#25910;&#38598;&#20855;&#26377;&#36234;&#26469;&#36234;&#22797;&#26434;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#30740;&#31350;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#25104;&#24615;&#21450;&#20854;&#20316;&#20026;&#25991;&#26412;&#19982;&#22270;&#20687;&#20869;&#23481;&#32452;&#25104;&#23545;&#40784;&#36136;&#37327;&#24230;&#37327;&#30340;&#20854;&#32435;&#20837;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#20171;&#32461;&#20102;&#29616;&#26377;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#26469;&#23545;&#36825;&#20123;&#25351;&#26631;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#36824;&#23457;&#26597;&#20102;&#32463;&#24120;&#37319;&#29992;&#30340;&#25991;&#26412;-&#22270;&#20687;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11821v1 Announce Type: cross  Abstract: Recent advances in text-to-image synthesis have been enabled by exploiting a combination of language and vision through foundation models. These models are pre-trained on tremendous amounts of text-image pairs sourced from the World Wide Web or other large-scale databases. As the demand for high-quality image generation shifts towards ensuring content alignment between text and image, novel evaluation metrics have been developed with the aim of mimicking human judgments. Thus, researchers have started to collect datasets with increasingly complex annotations to study the compositionality of vision-language models and their incorporation as a quality measure of compositional alignment between text and image contents. In this work, we provide a comprehensive overview of existing text-to-image evaluation metrics and propose a new taxonomy for categorizing these metrics. We also review frequently adopted text-image benchmark datasets befor
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLM&#20197;&#21450;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#29983;&#25104;&#20855;&#36523;&#26426;&#22120;&#20154;&#30340;&#8220;&#34892;&#36335;&#25351;&#31034;&#8221;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#27169;&#25311;&#24179;&#21488;&#19978;&#23637;&#31034;&#20986;&#36328;&#24179;&#21488;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11487</link><description>&lt;p&gt;
LLM&#33021;&#29983;&#25104;&#31867;&#20154;&#34892;&#36335;&#25351;&#31034;&#21527;&#65311;&#36208;&#21521;&#36328;&#24179;&#21488;&#30340;&#20855;&#36523;&#25351;&#20196;&#32508;&#21512;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic Embodied Instruction Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11487
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLM&#20197;&#21450;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#29983;&#25104;&#20855;&#36523;&#26426;&#22120;&#20154;&#30340;&#8220;&#34892;&#36335;&#25351;&#31034;&#8221;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#27169;&#25311;&#24179;&#21488;&#19978;&#23637;&#31034;&#20986;&#36328;&#24179;&#21488;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#21512;&#25104;&#8220;&#34892;&#36335;&#25351;&#31034;&#8221;&#20197;&#25351;&#23548;&#20855;&#36523;&#26426;&#22120;&#20154;&#12290;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#20877;&#20381;&#36182;&#20110;&#20165;&#35774;&#35745;&#29992;&#20110;&#29305;&#23450;&#27169;&#25311;&#24179;&#21488;&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#32780;&#26159;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#35843;&#33410;LLM&#65292;&#20197;&#20351;&#29992;&#23569;&#37327;&#21442;&#32771;&#29983;&#25104;&#25351;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;LLM&#30340;&#35270;&#35273;&#38382;&#31572;&#31574;&#30053;&#25910;&#38598;&#29615;&#22659;&#30340;&#35814;&#32454;&#20449;&#24687;&#65292;LLM&#29992;&#20110;&#25351;&#20196;&#21512;&#25104;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#22312;&#22810;&#20010;&#27169;&#25311;&#24179;&#21488;&#19978;&#65292;&#21253;&#25324;Matterport3D&#12289;AI Habitat&#21644;ThreeDWorld&#65292;&#20174;&#32780;&#23637;&#31034;&#20102;&#20854;&#36328;&#24179;&#21488;&#29305;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#20027;&#35266;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35266;&#23519;&#21040;83.3%&#30340;&#29992;&#25143;&#35748;&#20026;&#21512;&#25104;&#30340;&#25351;&#31034;&#20934;&#30830;&#25429;&#25417;&#20102;&#29615;&#22659;&#30340;&#32454;&#33410;&#65292;&#24182;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#29983;&#25104;&#30340;&#25351;&#31034;&#31867;&#20284;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11487v1 Announce Type: cross  Abstract: We present a novel approach to automatically synthesize "wayfinding instructions" for an embodied robot agent. In contrast to prior approaches that are heavily reliant on human-annotated datasets designed exclusively for specific simulation platforms, our algorithm uses in-context learning to condition an LLM to generate instructions using just a few references. Using an LLM-based Visual Question Answering strategy, we gather detailed information about the environment which is used by the LLM for instruction synthesis. We implement our approach on multiple simulation platforms including Matterport3D, AI Habitat and ThreeDWorld, thereby demonstrating its platform-agnostic nature. We subjectively evaluate our approach via a user study and observe that 83.3% of users find the synthesized instructions accurately capture the details of the environment and show characteristics similar to those of human-generated instructions. Further, we con
&lt;/p&gt;</description></item><item><title>&#26356;&#22810;&#30340;&#21709;&#24212;&#20294;&#26356;&#23569;&#30340;&#25552;&#31034;&#21487;&#20197;&#26356;&#22909;&#22320;&#35302;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20154;&#31867;&#23545;&#40784;&#65292;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#22810;&#26679;&#24615;&#20844;&#24335;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#24433;&#21709;LLMs&#30340;&#26368;&#32456;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.11124</link><description>&lt;p&gt;
&#22312;&#20154;&#31867;&#23545;&#40784;&#20013;&#25193;&#23637;&#25968;&#25454;&#22810;&#26679;&#24615;&#20197;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scaling Data Diversity for Fine-Tuning Language Models in Human Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11124
&lt;/p&gt;
&lt;p&gt;
&#26356;&#22810;&#30340;&#21709;&#24212;&#20294;&#26356;&#23569;&#30340;&#25552;&#31034;&#21487;&#20197;&#26356;&#22909;&#22320;&#35302;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20154;&#31867;&#23545;&#40784;&#65292;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#22810;&#26679;&#24615;&#20844;&#24335;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#24433;&#21709;LLMs&#30340;&#26368;&#32456;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#21487;&#20197;&#38450;&#27490;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#35823;&#23548;&#24615;&#25110;&#26377;&#27602;&#20869;&#23481;&#65292;&#21516;&#26102;&#38656;&#35201;&#39640;&#25104;&#26412;&#30340;&#20154;&#31867;&#21453;&#39304;&#12290;&#20551;&#35774;&#20154;&#24037;&#27880;&#37322;&#36164;&#28304;&#26377;&#38480;&#65292;&#21017;&#21487;&#20197;&#32771;&#34385;&#20004;&#31181;&#19981;&#21516;&#30340;&#20998;&#37197;&#26041;&#24335;&#65306;&#26356;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#25110;&#26356;&#22810;&#26679;&#21270;&#30340;&#24453;&#26631;&#35760;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#30340;&#30452;&#25509;&#27604;&#36739;&#23578;&#19981;&#23384;&#22312;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#26681;&#25454;&#24494;&#35843;&#26679;&#26412;&#25968;&#37327;&#25511;&#21046;&#21452;&#26041;&#30340;&#22810;&#26679;&#24615;&#65292;&#36825;&#21487;&#20197;&#30452;&#25509;&#21453;&#26144;&#23427;&#20204;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#22823;&#37327;&#25552;&#31034;&#19981;&#21516;&#65292;&#26356;&#22810;&#30340;&#21709;&#24212;&#20294;&#26159;&#26356;&#23569;&#30340;&#25552;&#31034;&#26356;&#33021;&#28608;&#21457;LLMs&#36827;&#34892;&#20154;&#31867;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#25552;&#31034;&#30340;&#22810;&#26679;&#24615;&#27010;&#24565;&#21487;&#33021;&#27604;&#36890;&#24120;&#30001;&#21333;&#20010;&#25968;&#23383;&#37327;&#21270;&#30340;&#21709;&#24212;&#26356;&#22797;&#26434;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#25552;&#31034;&#22810;&#26679;&#24615;&#30340;&#26032;&#20844;&#24335;&#65292;&#36827;&#19968;&#27493;&#26263;&#31034;&#19982;&#24494;&#35843;&#21518;LLMs&#26368;&#32456;&#24615;&#33021;&#30340;&#32447;&#24615;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11124v1 Announce Type: cross  Abstract: Alignment with human preference prevents large language models (LLMs) from generating misleading or toxic content while requiring high-cost human feedback. Assuming resources of human annotation are limited, there are two different ways of allocating considered: more diverse PROMPTS or more diverse RESPONSES to be labeled. Nonetheless, a straightforward comparison between their impact is absent. In this work, we first control the diversity of both sides according to the number of samples for fine-tuning, which can directly reflect their influence. We find that instead of numerous prompts, more responses but fewer prompts better trigger LLMs for human alignment. Additionally, the concept of diversity for prompts can be more complex than responses that are typically quantified by single digits. Consequently, a new formulation of prompt diversity is proposed, further implying a linear correlation with the final performance of LLMs after f
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#26465;&#20214;&#23398;&#20064;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22522;&#20110;&#27969;&#30340;&#36229;&#20998;&#36776;&#27169;&#22411;&#20013;&#30340;&#32593;&#26684;&#20266;&#24433;&#12289;&#36870;&#30697;&#38453;&#29190;&#28856;&#21644;&#27425;&#20248;&#32467;&#26524;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.10988</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#30340;&#20808;&#39564;&#30693;&#35782;&#25552;&#21319;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#36229;&#20998;&#36776;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Boosting Flow-based Generative Super-Resolution Models via Learned Prior
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10988
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#26465;&#20214;&#23398;&#20064;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22522;&#20110;&#27969;&#30340;&#36229;&#20998;&#36776;&#27169;&#22411;&#20013;&#30340;&#32593;&#26684;&#20266;&#24433;&#12289;&#36870;&#30697;&#38453;&#29190;&#28856;&#21644;&#27425;&#20248;&#32467;&#26524;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27969;&#30340;&#36229;&#20998;&#36776;&#65288;SR&#65289;&#27169;&#22411;&#23637;&#29616;&#20102;&#22312;&#29983;&#25104;&#21151;&#33021;&#39640;&#36136;&#37327;&#22270;&#20687;&#26041;&#38754;&#30340;&#24778;&#20154;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#20013;&#36935;&#21040;&#20102;&#20960;&#20010;&#25361;&#25112;&#65292;&#27604;&#22914;&#32593;&#26684;&#20266;&#24433;&#12289;&#36870;&#30697;&#38453;&#29190;&#28856;&#21644;&#30001;&#20110;&#22266;&#23450;&#30340;&#37319;&#26679;&#28201;&#24230;&#23548;&#33268;&#30340;&#27425;&#20248;&#32467;&#26524;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#22312;&#22522;&#20110;&#27969;&#30340;SR&#27169;&#22411;&#30340;&#25512;&#26029;&#38454;&#27573;&#24341;&#20837;&#20102;&#26465;&#20214;&#23398;&#20064;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#36825;&#20010;&#20808;&#39564;&#30693;&#35782;&#26159;&#30001;&#25105;&#20204;&#25552;&#20986;&#30340;&#28508;&#22312;&#27169;&#22359;&#26681;&#25454;&#20302;&#20998;&#36776;&#29575;&#22270;&#20687;&#39044;&#27979;&#20986;&#30340;&#28508;&#22312;&#20195;&#30721;&#65292;&#28982;&#21518;&#36890;&#36807;&#27969;&#27169;&#22411;&#36716;&#25442;&#20026;SR&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26088;&#22312;&#26080;&#32541;&#22320;&#19982;&#20219;&#20309;&#29616;&#20195;&#22522;&#20110;&#27969;&#30340;SR&#27169;&#22411;&#38598;&#25104;&#65292;&#32780;&#26080;&#38656;&#20462;&#25913;&#20854;&#26550;&#26500;&#25110;&#39044;&#35757;&#32451;&#26435;&#37325;&#12290;&#25105;&#20204;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#21644;&#28040;&#34701;&#20998;&#26512;&#35780;&#20272;&#20102;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;&#25552;&#20986;&#30340;&#26694;&#26550;&#25104;&#21151;&#35299;&#20915;&#20102;&#22522;&#20110;&#27969;&#30340;SR&#27169;&#22411;&#20013;&#30340;&#25152;&#26377;&#22266;&#26377;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10988v1 Announce Type: cross  Abstract: Flow-based super-resolution (SR) models have demonstrated astonishing capabilities in generating high-quality images. However, these methods encounter several challenges during image generation, such as grid artifacts, exploding inverses, and suboptimal results due to a fixed sampling temperature. To overcome these issues, this work introduces a conditional learned prior to the inference phase of a flow-based SR model. This prior is a latent code predicted by our proposed latent module conditioned on the low-resolution image, which is then transformed by the flow model into an SR image. Our framework is designed to seamlessly integrate with any contemporary flow-based SR model without modifying its architecture or pre-trained weights. We evaluate the effectiveness of our proposed framework through extensive experiments and ablation analyses. The proposed framework successfully addresses all the inherent issues in flow-based SR models a
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#35821;&#20041;&#29305;&#24449;&#20316;&#20026;&#36890;&#29992;&#27010;&#24565;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23436;&#20840;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#20026;&#30333;&#30418;&#31070;&#32463;&#32593;&#32476;&#30340;&#33539;&#24335;&#36716;&#21464;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09863</link><description>&lt;p&gt;
&#19968;&#20010;&#30333;&#30418;&#31070;&#32463;&#32593;&#32476;&#30340;&#27010;&#24565;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Conceptual Framework For White Box Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09863
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#35821;&#20041;&#29305;&#24449;&#20316;&#20026;&#36890;&#29992;&#27010;&#24565;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23436;&#20840;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#20026;&#30333;&#30418;&#31070;&#32463;&#32593;&#32476;&#30340;&#33539;&#24335;&#36716;&#21464;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#35821;&#20041;&#29305;&#24449;&#20316;&#20026;&#23436;&#20840;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#36890;&#29992;&#27010;&#24565;&#26694;&#26550;&#12290;&#19968;&#20010;&#20805;&#20998;&#21160;&#26426;&#30340;MNIST&#30456;&#20851;&#23376;&#38382;&#39064;&#30340;&#27010;&#24565;&#39564;&#35777;&#27169;&#22411;&#21253;&#25324;4&#20010;&#36825;&#26679;&#30340;&#23618;&#65292;&#24635;&#20849;4800&#20010;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;&#35813;&#27169;&#22411;&#26131;&#20110;&#35299;&#37322;&#65292;&#26080;&#38656;&#20219;&#20309;&#24418;&#24335;&#30340;&#23545;&#25239;&#35757;&#32451;&#21363;&#21487;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#23545;&#25239;&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#38656;&#35201;&#36739;&#23569;&#30340;&#36229;&#21442;&#25968;&#35843;&#33410;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#21333;&#20010;CPU&#19978;&#24555;&#36895;&#35757;&#32451;&#12290;&#35813;&#25216;&#26415;&#30340;&#36890;&#29992;&#24615;&#25215;&#35834;&#20026;&#24443;&#24213;&#27665;&#20027;&#21270;&#21644;&#30495;&#27491;&#36890;&#29992;&#30340;&#30333;&#30418;&#31070;&#32463;&#32593;&#32476;&#24102;&#26469;&#20102;&#24076;&#26395;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/314-Foundation/white-box-nn&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09863v1 Announce Type: cross  Abstract: This paper introduces semantic features as a general conceptual framework for fully explainable neural network layers. A well-motivated proof of concept model for relevant subproblem of MNIST consists of 4 such layers with the total of 4.8K learnable parameters. The model is easily interpretable, achieves human-level adversarial test accuracy with no form of adversarial training, requires little hyperparameter tuning and can be quickly trained on a single CPU. The general nature of the technique bears promise for a paradigm shift towards radically democratised and truly generalizable white box neural networks. The code is available at https://github.com/314-Foundation/white-box-nn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#32467;&#26500;&#21270;&#20998;&#35299;&#24352;&#37327;&#36827;&#19968;&#27493;&#25277;&#35937;&#31232;&#30095;DNN&#21152;&#36895;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23558;&#31232;&#30095;&#24352;&#37327;&#36716;&#25442;&#25104;&#19968;&#31995;&#21015;&#32467;&#26500;&#21270;&#31232;&#30095;&#24352;&#37327;&#65292;&#20174;&#32780;&#24357;&#21512;&#20102;&#31232;&#30095;DNN&#27169;&#22411;&#21644;&#30828;&#20214;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.07953</link><description>&lt;p&gt;
&#36890;&#36807;&#32467;&#26500;&#21270;&#31232;&#30095;&#24352;&#37327;&#20998;&#35299;&#23545;&#31232;&#30095;DNN&#21152;&#36895;&#36827;&#34892;&#25277;&#35937;&#21270;
&lt;/p&gt;
&lt;p&gt;
Abstracting Sparse DNN Acceleration via Structured Sparse Tensor Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#32467;&#26500;&#21270;&#20998;&#35299;&#24352;&#37327;&#36827;&#19968;&#27493;&#25277;&#35937;&#31232;&#30095;DNN&#21152;&#36895;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23558;&#31232;&#30095;&#24352;&#37327;&#36716;&#25442;&#25104;&#19968;&#31995;&#21015;&#32467;&#26500;&#21270;&#31232;&#30095;&#24352;&#37327;&#65292;&#20174;&#32780;&#24357;&#21512;&#20102;&#31232;&#30095;DNN&#27169;&#22411;&#21644;&#30828;&#20214;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#20013;&#21033;&#29992;&#31232;&#30095;&#24615;&#24050;&#25104;&#20026;&#28385;&#36275;&#29616;&#20195;DNN&#26085;&#30410;&#22686;&#38271;&#30340;&#35745;&#31639;&#38656;&#27714;&#30340;&#19968;&#31181;&#20855;&#26377;&#21069;&#26223;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#31232;&#30095;DNN&#21152;&#36895;&#20173;&#28982;&#38754;&#20020;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#20026;&#20102;&#26368;&#23567;&#21270;&#31232;&#30095;&#21152;&#36895;&#30340;&#24320;&#38144;&#65292;&#30828;&#20214;&#35774;&#35745;&#24072;&#26368;&#36817;&#25552;&#20986;&#20102;&#32467;&#26500;&#21270;&#31232;&#30095;&#30828;&#20214;&#25903;&#25345;&#65292;&#36825;&#25552;&#20379;&#20102;&#26377;&#38480;&#30340;&#28789;&#27963;&#24615;&#24182;&#38656;&#35201;&#39069;&#22806;&#30340;&#27169;&#22411;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#20026;&#26576;&#20123;&#32467;&#26500;&#21270;&#31232;&#30095;&#30828;&#20214;&#24494;&#35843;&#30340;&#20219;&#20309;&#31232;&#30095;&#27169;&#22411;&#26080;&#27861;&#34987;&#20854;&#20182;&#32467;&#26500;&#21270;&#30828;&#20214;&#21152;&#36895;&#12290;&#20026;&#20102;&#24357;&#21512;&#31232;&#30095;DNN&#27169;&#22411;&#21644;&#30828;&#20214;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#32467;&#26500;&#20998;&#35299;&#30340;&#24352;&#37327;&#36817;&#20284;&#65288;TASD&#65289;&#65292;&#21033;&#29992;&#20102;&#32447;&#24615;&#20195;&#25968;&#20013;&#30340;&#20998;&#37197;&#24615;&#36136;&#23558;&#20219;&#20309;&#31232;&#30095;&#24352;&#37327;&#36716;&#21270;&#20026;&#19968;&#31995;&#21015;&#32467;&#26500;&#21270;&#31232;&#30095;&#24352;&#37327;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36719;&#20214;&#26694;&#26550;TASDER&#65292;&#36890;&#36807;&#25628;&#32034;&#36880;&#23618;&#39640;&#36136;&#37327;&#30340;&#32467;&#26500;&#21270;&#20998;&#35299;&#26469;&#21152;&#36895;DNNs&#30340;&#26435;&#37325;&#21644;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07953v1 Announce Type: cross  Abstract: Exploiting sparsity in deep neural networks (DNNs) has been a promising area to meet the growing computation need of modern DNNs. However, in practice, sparse DNN acceleration still faces a key challenge. To minimize the overhead of sparse acceleration, hardware designers have proposed structured sparse hardware support recently, which provides limited flexibility and requires extra model fine-tuning. Moreover, any sparse model fine-tuned for certain structured sparse hardware cannot be accelerated by other structured hardware. To bridge the gap between sparse DNN models and hardware, this paper proposes tensor approximation via structured decomposition (TASD), which leverages the distributive property in linear algebra to turn any sparse tensor into a series of structured sparse tensors. Next, we develop a software framework, TASDER, to accelerate DNNs by searching layer-wise, high-quality structured decomposition for both weight and 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#30697;&#38453;&#21464;&#25442;&#30340;&#20302;&#31209;&#35843;&#25972;&#65288;MTLoRA&#65289;&#26041;&#27861;&#65292;&#21463;&#22823;&#33041;&#21551;&#21457;&#65292;&#29992;&#20110;&#25552;&#39640;&#24494;&#35843;&#25216;&#26415;&#30340;&#22797;&#26434;&#20219;&#21153;&#36866;&#24212;&#24615;&#12289;&#24615;&#33021;&#12289;&#31283;&#23450;&#24615;&#21644;&#31639;&#27861;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07440</link><description>&lt;p&gt;
&#22522;&#20110;&#30697;&#38453;&#21464;&#25442;&#30340;&#20302;&#31209;&#35843;&#25972;&#65288;MTLoRA&#65289;&#65306;&#19968;&#31181;&#21463;&#22823;&#33041;&#21551;&#21457;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A Brain-Inspired Method for Parameter-Efficient Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07440
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#30697;&#38453;&#21464;&#25442;&#30340;&#20302;&#31209;&#35843;&#25972;&#65288;MTLoRA&#65289;&#26041;&#27861;&#65292;&#21463;&#22823;&#33041;&#21551;&#21457;&#65292;&#29992;&#20110;&#25552;&#39640;&#24494;&#35843;&#25216;&#26415;&#30340;&#22797;&#26434;&#20219;&#21153;&#36866;&#24212;&#24615;&#12289;&#24615;&#33021;&#12289;&#31283;&#23450;&#24615;&#21644;&#31639;&#27861;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LPLMs&#65289;&#30340;&#24494;&#35843;&#25216;&#26415;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#26377;&#25928;&#25511;&#21046;LPLMs&#30340;&#36755;&#20986;&#34892;&#20026;&#12290;&#26412;&#25991;&#21463;&#22823;&#33041;&#21151;&#33021;&#21463;&#20854;&#20960;&#20309;&#32467;&#26500;&#22609;&#36896;&#30340;&#21551;&#21457;&#65292;&#23558;&#36825;&#19968;&#24605;&#24819;&#34701;&#20837;LoRA&#25216;&#26415;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#30697;&#38453;&#21464;&#25442;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#22797;&#26434;&#20219;&#21153;&#36866;&#24212;&#24615;&#12289;&#24615;&#33021;&#12289;&#31283;&#23450;&#24615;&#21644;&#31639;&#27861;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07440v1 Announce Type: cross  Abstract: Fine-tuning techniques based on Large Pretrained Language Models (LPLMs) have been proven to significantly enhance model performance on a variety of downstream tasks and effectively control the output behaviors of LPLMs. Recent studies have proposed numerous methods for fine-tuning a small number of parameters based on open-source LPLMs, reducing the demand for computational and storage resources. Among these, reparameterization fine-tuning methods represented by LoRA (Low-Rank Adaptation) have gained popularity. We find that although these methods perform well in many aspects, there is still considerable room for improvement in terms of complex task adaptability, performance, stability, and algorithm complexity. In response to this, inspired by the idea that the functions of the brain are shaped by its geometric structure, this paper integrates this idea into LoRA technology and proposes a new matrix transformation-based reparameteriz
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;ParallelPARC&#27969;&#27700;&#32447;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22797;&#26434;&#27573;&#33853;&#31867;&#27604;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#21508;&#31181;&#31867;&#27604;&#31867;&#22411;&#65292;&#24182;&#23637;&#31034;&#20986;&#20154;&#31867;&#22312;&#31867;&#27604;&#35782;&#21035;&#20013;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.01139</link><description>&lt;p&gt;
ParallelPARC: &#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#31867;&#27604;&#30340;&#21487;&#25193;&#23637;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
ParallelPARC: A Scalable Pipeline for Generating Natural-Language Analogies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01139
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;ParallelPARC&#27969;&#27700;&#32447;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22797;&#26434;&#27573;&#33853;&#31867;&#27604;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#21508;&#31181;&#31867;&#27604;&#31867;&#22411;&#65292;&#24182;&#23637;&#31034;&#20986;&#20154;&#31867;&#22312;&#31867;&#27604;&#35782;&#21035;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Analogy-making&#23545;&#20110;&#20154;&#31867;&#35748;&#30693;&#33267;&#20851;&#37325;&#35201;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36866;&#24212;&#26032;&#39062;&#24773;&#22659;--&#36825;&#26159;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20173;&#28982;&#32570;&#20047;&#30340;&#33021;&#21147;&#12290;&#22823;&#22810;&#25968;&#31867;&#27604;&#25968;&#25454;&#38598;&#20170;&#22825;&#20851;&#27880;&#31616;&#21333;&#30340;&#31867;&#27604;&#65288;&#20363;&#22914;&#65292;&#35789;&#31867;&#27604;&#65289;&#65307;&#21253;&#21547;&#22797;&#26434;&#31867;&#22411;&#31867;&#27604;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#26159;&#25163;&#24037;&#31574;&#21010;&#30340;&#65292;&#24182;&#19988;&#38750;&#24120;&#23567;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#38480;&#21046;&#20102;&#35745;&#31639;&#31867;&#27604;&#30340;&#36827;&#23637;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#65292;ParallelPARC&#65288;Parallel Paragraph Creator&#65289;&#65292;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#21019;&#24314;&#22522;&#20110;&#27573;&#33853;&#30340;&#22797;&#26434;&#31867;&#27604;&#65292;&#20197;&#21450;&#31616;&#21333;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24178;&#25200;&#39033;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27969;&#27700;&#32447;&#65292;&#24182;&#21019;&#24314;&#20102;ProPara-Logy&#65292;&#19968;&#20010;&#20851;&#20110;&#31185;&#23398;&#36807;&#31243;&#38388;&#31867;&#27604;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#30001;&#20154;&#31867;&#39564;&#35777;&#36807;&#30340;&#37329;&#26631;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22312;&#20108;&#36827;&#21046;&#21644;&#22810;&#36873;&#29615;&#22659;&#20013;&#27979;&#35797;&#20102;LLMs&#21644;&#20154;&#31867;&#23545;&#31867;&#27604;&#30340;&#35782;&#21035;&#65292;&#21457;&#29616;&#20154;&#31867;&#32988;&#36807;&#26368;&#20339;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01139v1 Announce Type: cross  Abstract: Analogy-making is central to human cognition, allowing us to adapt to novel situations -- an ability that current AI systems still lack. Most analogy datasets today focus on simple analogies (e.g., word analogies); datasets including complex types of analogies are typically manually curated and very small. We believe that this holds back progress in computational analogy. In this work, we design a data generation pipeline, ParallelPARC (Parallel Paragraph Creator) leveraging state-of-the-art Large Language Models (LLMs) to create complex, paragraph-based analogies, as well as distractors, both simple and challenging. We demonstrate our pipeline and create ProPara-Logy, a dataset of analogies between scientific processes. We publish a gold-set, validated by humans, and a silver-set, generated automatically. We test LLMs' and humans' analogy recognition in binary and multiple-choice settings, and found that humans outperform the best mod
&lt;/p&gt;</description></item><item><title>&#37117;&#24066;GPT&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#20855;&#26377;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#30340;&#26102;&#31354;&#27169;&#22411;&#65292;&#20511;&#37492;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#23601;&#12290;</title><link>https://arxiv.org/abs/2403.00813</link><description>&lt;p&gt;
UrbanGPT: &#26102;&#31354;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UrbanGPT: Spatio-Temporal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00813
&lt;/p&gt;
&lt;p&gt;
&#37117;&#24066;GPT&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#20855;&#26377;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#30340;&#26102;&#31354;&#27169;&#22411;&#65292;&#20511;&#37492;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#23601;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37117;&#24066;GPT&#26088;&#22312;&#39044;&#27979;&#24182;&#27934;&#23519;&#22478;&#24066;&#29615;&#22659;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#19978;&#19981;&#26029;&#21464;&#21270;&#30340;&#21160;&#24577;&#12290;&#20854;&#30446;&#30340;&#26159;&#39044;&#27979;&#37117;&#24066;&#29983;&#27963;&#21508;&#20010;&#26041;&#38754;&#30340;&#26410;&#26469;&#27169;&#24335;&#12289;&#36235;&#21183;&#21644;&#20107;&#20214;&#65292;&#21253;&#25324;&#20132;&#36890;&#12289;&#20154;&#21475;&#27969;&#21160;&#21644;&#29359;&#32618;&#29575;&#31561;&#12290;&#23613;&#31649;&#24050;&#32463;&#20184;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#24320;&#21457;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#20197;&#20934;&#30830;&#39044;&#27979;&#26102;&#31354;&#25968;&#25454;&#65292;&#20294;&#38656;&#27880;&#24847;&#21040;&#24456;&#22810;&#26041;&#27861;&#22312;&#29983;&#25104;&#31934;&#30830;&#30340;&#26102;&#31354;&#34920;&#31034;&#26102;&#20005;&#37325;&#20381;&#36182;&#20110;&#26377;&#36275;&#22815;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22312;&#23454;&#38469;&#37117;&#24066;&#24863;&#30693;&#22330;&#26223;&#20013;&#65292;&#25968;&#25454;&#31232;&#32570;&#26159;&#19968;&#20010;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#24314;&#31435;&#19968;&#20010;&#20855;&#26377;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#30340;&#26102;&#31354;&#27169;&#22411;&#36328;&#36234;&#22810;&#26679;&#26102;&#31354;&#23398;&#20064;&#22330;&#26223;&#26159;&#24517;&#35201;&#30340;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#21331;&#36234;&#25104;&#23601;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00813v1 Announce Type: cross  Abstract: Spatio-temporal prediction aims to forecast and gain insights into the ever-changing dynamics of urban environments across both time and space. Its purpose is to anticipate future patterns, trends, and events in diverse facets of urban life, including transportation, population movement, and crime rates. Although numerous efforts have been dedicated to developing neural network techniques for accurate predictions on spatio-temporal data, it is important to note that many of these methods heavily depend on having sufficient labeled data to generate precise spatio-temporal representations. Unfortunately, the issue of data scarcity is pervasive in practical urban sensing scenarios. Consequently, it becomes necessary to build a spatio-temporal model with strong generalization capabilities across diverse spatio-temporal learning scenarios. Taking inspiration from the remarkable achievements of large language models (LLMs), our objective is 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20445;&#30041;&#21407;&#38382;&#39064;&#32467;&#26500;&#38590;&#24230;&#20294;&#38024;&#23545;LLMs&#26080;&#35299;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#26377;&#25928;&#22320;&#38477;&#20302;&#20102;LLMs&#30340;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.17916</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#25915;&#20987;&#23454;&#29616;&#25239;LLM&#30340;&#25968;&#23398;&#38382;&#39064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
LLM-Resistant Math Word Problem Generation via Adversarial Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20445;&#30041;&#21407;&#38382;&#39064;&#32467;&#26500;&#38590;&#24230;&#20294;&#38024;&#23545;LLMs&#26080;&#35299;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#26377;&#25928;&#22320;&#38477;&#20302;&#20102;LLMs&#30340;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26174;&#33879;&#25913;&#21464;&#20102;&#25945;&#32946;&#39046;&#22495;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#29983;&#25104;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20197;&#30830;&#20445;&#20844;&#24179;&#35780;&#20272;&#65292;&#36825;&#20123;&#31034;&#20363;&#20445;&#30041;&#20102;&#21407;&#22987;&#38382;&#39064;&#30340;&#32467;&#26500;&#21644;&#38590;&#24230;&#65292;&#20294;LLMs&#26080;&#27861;&#35299;&#20915;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#25968;&#23398;&#24212;&#29992;&#39046;&#22495;&#30340;&#35789;&#38382;&#39064;&#65292;&#21033;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#32467;&#26500;&#29983;&#25104;&#23545;&#25239;&#31034;&#20363;&#65292;&#36890;&#36807;&#31616;&#21333;&#32534;&#36753;&#38382;&#39064;&#20013;&#30340;&#25968;&#23383;&#20540;&#65292;&#23548;&#33268;LLMs&#20135;&#29983;&#38169;&#35823;&#31572;&#26696;&#12290;&#25105;&#20204;&#23545;&#21508;&#31181;&#24320;&#28304;&#21644;&#38381;&#28304;LLMs&#36827;&#34892;&#23454;&#39564;&#65292;&#23450;&#37327;&#21644;&#23450;&#24615;&#22320;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#38477;&#20302;&#20102;&#23427;&#20204;&#30340;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17916v1 Announce Type: cross  Abstract: Large language models (LLMs) have significantly transformed the educational landscape. As current plagiarism detection tools struggle to keep pace with LLMs' rapid advancements, the educational community faces the challenge of assessing students' true problem-solving abilities in the presence of LLMs. In this work, we explore a new paradigm for ensuring fair evaluation -- generating adversarial examples which preserve the structure and difficulty of the original questions aimed for assessment, but are unsolvable by LLMs. Focusing on the domain of math word problems, we leverage abstract syntax trees to structurally generate adversarial examples that cause LLMs to produce incorrect answers by simply editing the numeric values in the problems. We conduct experiments on various open- and closed-source LLMs, quantitatively and qualitatively demonstrating that our method significantly degrades their math problem-solving ability. We identify
&lt;/p&gt;</description></item><item><title>Q-FOX&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;FOX&#20248;&#21270;&#22120;&#21644;Q-learning&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.16562</link><description>&lt;p&gt;
Q-FOX&#23398;&#20064;&#65306;&#39072;&#35206;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Q-FOX Learning: Breaking Tradition in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16562
&lt;/p&gt;
&lt;p&gt;
Q-FOX&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;FOX&#20248;&#21270;&#22120;&#21644;Q-learning&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26159;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#20195;&#29702;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#26469;&#23398;&#20064;&#26368;&#20339;&#21160;&#20316;&#65292;&#22240;&#27492;&#36866;&#29992;&#20110;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#25110;&#30452;&#25509;&#30417;&#30563;&#30340;&#20219;&#21153;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Q-FOX&#30340;&#26032;&#39062;&#33258;&#21160;&#35843;&#21442;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;FOX&#20248;&#21270;&#22120;&#21644;&#24120;&#29992;&#30340;&#26131;&#20110;&#23454;&#29616;&#30340;RL Q-learning&#31639;&#27861;&#35299;&#20915;&#20102;&#35843;&#21442;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#23558;&#22870;&#21169;&#25918;&#22312;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#21644;&#23398;&#20064;&#26102;&#38388;&#20043;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16562v2 Announce Type: replace-cross  Abstract: Reinforcement learning (RL) is a subset of artificial intelligence (AI) where agents learn the best action by interacting with the environment, making it suitable for tasks that do not require labeled data or direct supervision. Hyperparameters (HP) tuning refers to choosing the best parameter that leads to optimal solutions in RL algorithms. Manual or random tuning of the HP may be a crucial process because variations in this parameter lead to changes in the overall learning aspects and different rewards. In this paper, a novel and automatic HP-tuning method called Q-FOX is proposed. This uses both the FOX optimizer, a new optimization method inspired by nature that mimics red foxes' hunting behavior, and the commonly used, easy-to-implement RL Q-learning algorithm to solve the problem of HP tuning. Moreover, a new objective function is proposed which prioritizes the reward over the mean squared error (MSE) and learning time (
&lt;/p&gt;</description></item><item><title>Triad&#26694;&#26550;&#21033;&#29992;&#20102;&#22522;&#20110;&#22810;&#35282;&#33394;LLM&#20195;&#29702;&#26469;&#35299;&#20915;&#30693;&#35782;&#24211;&#38382;&#31572;&#38382;&#39064;&#65292;&#36890;&#36807;&#20195;&#29702;&#30340;&#19981;&#21516;&#35282;&#33394;&#20998;&#21035;&#22788;&#29702;KBQA&#23376;&#20219;&#21153;&#65292;&#21512;&#20316;&#23436;&#25104;KBQA&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.14320</link><description>&lt;p&gt;
Triad: &#19968;&#20010;&#21033;&#29992;&#22522;&#20110;&#22810;&#35282;&#33394;LLM&#20195;&#29702;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#30693;&#35782;&#24211;&#38382;&#31572;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14320
&lt;/p&gt;
&lt;p&gt;
Triad&#26694;&#26550;&#21033;&#29992;&#20102;&#22522;&#20110;&#22810;&#35282;&#33394;LLM&#20195;&#29702;&#26469;&#35299;&#20915;&#30693;&#35782;&#24211;&#38382;&#31572;&#38382;&#39064;&#65292;&#36890;&#36807;&#20195;&#29702;&#30340;&#19981;&#21516;&#35282;&#33394;&#20998;&#21035;&#22788;&#29702;KBQA&#23376;&#20219;&#21153;&#65292;&#21512;&#20316;&#23436;&#25104;KBQA&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22522;&#20110;LLM&#20195;&#29702;&#30340;&#36827;&#23637;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22238;&#31572;&#30693;&#35782;&#24211;&#20013;&#38382;&#39064;&#30340;&#36816;&#29992;&#20173;&#28982;&#40092;&#20026;&#20154;&#30693;&#12290;&#20351;&#29992;&#20256;&#32479;&#26041;&#27861;&#26469;&#23454;&#29616;KBQA&#31995;&#32479;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#32570;&#20047;&#29305;&#23450;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#20197;&#21450;&#21019;&#24314;&#20197;&#20219;&#21153;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#32467;&#26500;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Triad&#65292;&#19968;&#20010;&#21033;&#29992;&#20855;&#26377;&#19977;&#20010;&#35282;&#33394;&#30340;LLM&#20195;&#29702;&#30340;&#32479;&#19968;&#26694;&#26550;&#26469;&#36827;&#34892;KBQA&#20219;&#21153;&#12290;&#20195;&#29702;&#34987;&#20998;&#37197;&#19977;&#20010;&#35282;&#33394;&#26469;&#22788;&#29702;&#19981;&#21516;&#30340;KBQA&#23376;&#20219;&#21153;&#65306;&#20316;&#20026;&#25484;&#25569;&#21508;&#31181;&#23376;&#20219;&#21153;&#30340;&#36890;&#25165;&#65292;&#20316;&#20026;&#36873;&#25321;&#20505;&#36873;&#32773;&#30340;&#20915;&#31574;&#32773;&#65292;&#20197;&#21450;&#20316;&#20026;&#22238;&#31572;&#24102;&#26377;&#30693;&#35782;&#30340;&#38382;&#39064;&#30340;&#39038;&#38382;&#12290;&#25105;&#20204;&#30340;KBQA&#26694;&#26550;&#22312;&#22235;&#20010;&#38454;&#27573;&#20013;&#25191;&#34892;&#65292;&#28041;&#21450;&#20195;&#29702;&#30340;&#22810;&#37325;&#35282;&#33394;&#30340;&#21327;&#20316;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#26694;&#26550;&#32988;&#36807;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14320v1 Announce Type: cross  Abstract: Recent progress with LLM-based agents has shown promising results across various tasks. However, their use in answering questions from knowledge bases remains largely unexplored. Implementing a KBQA system using traditional methods is challenging due to the shortage of task-specific training data and the complexity of creating task-focused model structures. In this paper, we present Triad, a unified framework that utilizes an LLM-based agent with three roles for KBQA tasks. The agent is assigned three roles to tackle different KBQA subtasks: agent as a generalist for mastering various subtasks, as a decision maker for the selection of candidates, and as an advisor for answering questions with knowledge. Our KBQA framework is executed in four phases, involving the collaboration of the agent's multiple roles. We evaluated the performance of our framework using three benchmark datasets, and the results show that our framework outperforms 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20027;&#39064;&#23545;&#35805;&#25688;&#35201;&#35780;&#20272;&#22522;&#20934;TofuEval&#65292;&#30740;&#31350;&#21457;&#29616;&#29616;&#26377;&#30340;LLMs&#22312;&#23545;&#35805;&#39046;&#22495;&#23384;&#22312;&#22823;&#37327;&#20107;&#23454;&#38169;&#35823;&#30340;&#24187;&#35273;&#65292;&#24182;&#34920;&#26126;&#24403;LLMs&#20805;&#24403;&#20107;&#23454;&#35780;&#20272;&#22120;&#26102;&#65292;&#20854;&#34920;&#29616;&#19981;&#20339;&#12290;</title><link>https://arxiv.org/abs/2402.13249</link><description>&lt;p&gt;
TofuEval&#65306;&#35780;&#20272;LLM&#22312;&#20027;&#39064;&#23545;&#35805;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13249
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20027;&#39064;&#23545;&#35805;&#25688;&#35201;&#35780;&#20272;&#22522;&#20934;TofuEval&#65292;&#30740;&#31350;&#21457;&#29616;&#29616;&#26377;&#30340;LLMs&#22312;&#23545;&#35805;&#39046;&#22495;&#23384;&#22312;&#22823;&#37327;&#20107;&#23454;&#38169;&#35823;&#30340;&#24187;&#35273;&#65292;&#24182;&#34920;&#26126;&#24403;LLMs&#20805;&#24403;&#20107;&#23454;&#35780;&#20272;&#22120;&#26102;&#65292;&#20854;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#25991;&#26723;&#26032;&#38395;&#25688;&#35201;&#22312;&#24544;&#23454;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#38271;&#36275;&#36827;&#27493;&#65292;&#36825;&#24471;&#30410;&#20110;&#23545;&#20107;&#23454;&#19968;&#33268;&#24615;&#25110;&#24187;&#35273;&#35780;&#20272;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#20123;&#36827;&#23637;&#26159;&#21542;&#33021;&#24310;&#20280;&#21040;&#20854;&#20182;&#25991;&#26412;&#25688;&#35201;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20027;&#39064;&#23545;&#35805;&#25688;&#35201;&#35780;&#20272;&#22522;&#20934;&#65292;&#30001;&#19981;&#21516;&#35268;&#27169;&#30340;LLMs&#29983;&#25104;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#36825;&#20123;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#20108;&#20803;&#21477;&#32423;&#20154;&#31867;&#27880;&#37322;&#65292;&#20197;&#21450;&#23545;&#20107;&#23454;&#19981;&#19968;&#33268;&#21477;&#23376;&#30340;&#35814;&#32454;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;LLMs&#22312;&#23545;&#35805;&#39046;&#22495;&#23384;&#22312;&#22823;&#37327;&#20107;&#23454;&#38169;&#35823;&#30340;&#24187;&#35273;&#65292;&#26080;&#35770;&#27169;&#22411;&#22823;&#23567;&#22914;&#20309;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24403;LLMs&#65288;&#21253;&#25324;GPT-4&#65289;&#20805;&#24403;&#20108;&#20803;&#20107;&#23454;&#35780;&#20272;&#22120;&#26102;&#65292;&#23427;&#20204;&#34920;&#29616;&#19981;&#20339;&#65292;&#19988;&#21487;&#20197;&#34987;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#19987;&#38376;&#20107;&#23454;&#35780;&#20272;&#24230;&#37327;&#25152;&#36229;&#36234;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#24187;&#35273;&#31867;&#22411;&#36827;&#34892;&#20102;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13249v1 Announce Type: cross  Abstract: Single document news summarization has seen substantial progress on faithfulness in recent years, driven by research on the evaluation of factual consistency, or hallucinations. We ask whether these advances carry over to other text summarization domains. We propose a new evaluation benchmark on topic-focused dialogue summarization, generated by LLMs of varying sizes. We provide binary sentence-level human annotations of the factual consistency of these summaries along with detailed explanations of factually inconsistent sentences. Our analysis shows that existing LLMs hallucinate significant amounts of factual errors in the dialogue domain, regardless of the model's size. On the other hand, when LLMs, including GPT-4, serve as binary factual evaluators, they perform poorly and can be outperformed by prevailing state-of-the-art specialized factuality evaluation metrics. Finally, we conducted an analysis of hallucination types with a cu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RS-DPO&#30340;&#26041;&#27861;&#65292;&#23427;&#23558;&#25298;&#32477;&#37319;&#26679;&#21644;&#30452;&#25509;&#20248;&#21270;&#20559;&#22909;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#32463;&#36807;&#30417;&#30563;&#24494;&#35843;&#30340;&#31574;&#30053;&#27169;&#22411;&#65292;&#24182;&#20174;&#35813;&#27169;&#22411;&#20013;&#30452;&#25509;&#37319;&#26679;&#21709;&#24212;&#65292;RS-DPO&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22522;&#20110;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35782;&#21035;&#23545;&#27604;&#26679;&#26412;&#23545;&#65292;RS-DPO&#33021;&#22815;&#26356;&#22909;&#22320;&#36827;&#34892;RLHF&#12290;</title><link>https://arxiv.org/abs/2402.10038</link><description>&lt;p&gt;
RS-DPO&#65306;&#19968;&#31181;&#29992;&#20110;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28151;&#21512;&#25298;&#32477;&#37319;&#26679;&#21644;&#30452;&#25509;&#20248;&#21270;&#20559;&#22909;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RS-DPO&#30340;&#26041;&#27861;&#65292;&#23427;&#23558;&#25298;&#32477;&#37319;&#26679;&#21644;&#30452;&#25509;&#20248;&#21270;&#20559;&#22909;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#32463;&#36807;&#30417;&#30563;&#24494;&#35843;&#30340;&#31574;&#30053;&#27169;&#22411;&#65292;&#24182;&#20174;&#35813;&#27169;&#22411;&#20013;&#30452;&#25509;&#37319;&#26679;&#21709;&#24212;&#65292;RS-DPO&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22522;&#20110;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35782;&#21035;&#23545;&#27604;&#26679;&#26412;&#23545;&#65292;RS-DPO&#33021;&#22815;&#26356;&#22909;&#22320;&#36827;&#34892;RLHF&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;RLHF&#65289;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#29992;&#25143;&#24847;&#22270;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#30340;RLHF&#26377;&#26102;&#19981;&#31283;&#23450;&#65292;&#38656;&#35201;&#26174;&#33879;&#30340;&#36229;&#21442;&#25968;&#24494;&#35843;&#65292;&#24182;&#19988;&#22312;&#23545;&#40784;&#36807;&#31243;&#20013;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#30452;&#25509;&#20248;&#21270;&#20559;&#22909;&#65288;DPO&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;DPO&#20381;&#36182;&#20110;&#20174;&#20154;&#31867;&#26631;&#27880;&#32773;&#21644;&#26367;&#20195;LLM&#29983;&#25104;&#30340;&#23545;&#27604;&#22238;&#22797;&#65292;&#32780;&#19981;&#26159;&#31574;&#30053;&#27169;&#22411;&#65292;&#38480;&#21046;&#20102;RLHF&#30340;&#25928;&#26524;&#12290;&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#22320;&#32467;&#21512;&#25298;&#32477;&#37319;&#26679;&#65288;RS&#65289;&#21644;DPO&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;RS-DPO&#65292;&#39318;&#20808;&#24320;&#21457;&#20986;&#19968;&#20010;&#32463;&#36807;&#30417;&#30563;&#24494;&#35843;&#30340;&#31574;&#30053;&#27169;&#22411;&#65288;SFT&#65289;&#12290;&#28982;&#21518;&#30452;&#25509;&#20174;SFT&#27169;&#22411;&#20013;&#37319;&#26679;&#27599;&#20010;&#25552;&#31034;&#30340;k&#20010;&#21709;&#24212;&#12290;RS-DPO&#22522;&#20110;&#20854;&#30456;&#20284;&#24230;&#35782;&#21035;&#23545;&#27604;&#26679;&#26412;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10038v1 Announce Type: cross  Abstract: Reinforcement learning from human feedback (RLHF) has been extensively employed to align large language models with user intent. However, proximal policy optimization (PPO) based RLHF is occasionally unstable requiring significant hyperparameter finetuning, and computationally expensive to maximize the estimated reward during alignment. Recently, direct preference optimization (DPO) is proposed to address those challenges. However, DPO relies on contrastive responses generated from human annotator and alternative LLM, instead of the policy model, limiting the effectiveness of the RLHF. In this paper, we addresses both challenges by systematically combining rejection sampling (RS) and DPO. Our proposed method, RS-DPO, initiates with the development of a supervised fine-tuned policy model (SFT). A varied set of k responses per prompt are sampled directly from the SFT model. RS-DPO identifies pairs of contrastive samples based on their re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;LlaSMol&#65292;&#23427;&#26159;&#19968;&#31181;&#25512;&#36827;&#21270;&#23398;&#39046;&#22495;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#22823;&#35268;&#27169;&#12289;&#20840;&#38754;&#12289;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;LlaSMol&#22312;&#21270;&#23398;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;GPT-4&#24182;&#25509;&#36817;&#20110;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.09391</link><description>&lt;p&gt;
LlaSMol:&#21033;&#29992;&#22823;&#35268;&#27169;&#12289;&#20840;&#38754;&#12289;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#25512;&#36827;&#21270;&#23398;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;LlaSMol&#65292;&#23427;&#26159;&#19968;&#31181;&#25512;&#36827;&#21270;&#23398;&#39046;&#22495;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#22823;&#35268;&#27169;&#12289;&#20840;&#38754;&#12289;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;LlaSMol&#22312;&#21270;&#23398;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;GPT-4&#24182;&#25509;&#36817;&#20110;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21270;&#23398;&#22312;&#33647;&#29289;&#30740;&#21457;&#21644;&#26448;&#26009;&#31185;&#23398;&#31561;&#35768;&#22810;&#39046;&#22495;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#35832;&#22914;GPT-4&#20043;&#31867;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#65292;&#20294;&#29616;&#26377;&#24037;&#20316;&#34920;&#26126;&#23427;&#20204;&#22312;&#21270;&#23398;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20196;&#20154;&#22833;&#26395;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#24320;&#21457;&#30340;LLM&#22312;&#19968;&#31995;&#21015;&#21270;&#23398;&#20219;&#21153;&#19978;&#21487;&#20197;&#21462;&#24471;&#38750;&#24120;&#24378;&#22823;&#30340;&#32467;&#26524;&#65292;&#22312;&#25152;&#26377;&#20219;&#21153;&#19978;&#37117;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;GPT-4&#65292;&#24182;&#25509;&#36817;SoTA&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;&#25105;&#20204;&#21462;&#24471;&#25104;&#21151;&#30340;&#20851;&#38190;&#26159;&#19968;&#20010;&#21517;&#20026;SMolInstruct&#30340;&#22823;&#35268;&#27169;&#12289;&#20840;&#38754;&#12289;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#12290;&#23427;&#21253;&#21547;&#20102;14&#20010;&#32463;&#36807;&#31934;&#24515;&#25361;&#36873;&#30340;&#21270;&#23398;&#20219;&#21153;&#21644;&#36229;&#36807;&#19977;&#30334;&#19975;&#20010;&#39640;&#36136;&#37327;&#26679;&#26412;&#65292;&#20026;&#35757;&#32451;&#21644;&#35780;&#20272;&#21270;&#23398;LLM&#22880;&#23450;&#20102;&#22362;&#23454;&#22522;&#30784;&#12290;&#22522;&#20110;SMolInstruct&#65292;&#25105;&#20204;&#23545;&#19968;&#32452;&#24320;&#28304;LLM&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#20854;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;Mistral ser&#26159;&#26368;&#20339;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09391v1 Announce Type: new Abstract: Chemistry plays a crucial role in many domains, such as drug discovery and material science. While large language models (LLMs) such as GPT-4 exhibit remarkable capabilities on natural language processing tasks, existing work shows their performance on chemistry tasks is discouragingly low. In this paper, however, we demonstrate that our developed LLMs can achieve very strong results on a comprehensive set of chemistry tasks, outperforming the most advanced GPT-4 across all the tasks by a substantial margin and approaching the SoTA task-specific models. The key to our success is a large-scale, comprehensive, high-quality dataset for instruction tuning named SMolInstruct. It contains 14 meticulously selected chemistry tasks and over three million high-quality samples, laying a solid foundation for training and evaluating LLMs for chemistry. Based on SMolInstruct, we fine-tune a set of open-source LLMs, among which, we find that Mistral ser
&lt;/p&gt;</description></item><item><title>X-LoRA&#26159;&#19968;&#31181;&#28789;&#27963;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#21033;&#29992;&#20302;&#31209;&#36866;&#37197;&#22120;&#19987;&#23478;&#30340;&#28151;&#21512;&#31574;&#30053;&#65292;&#21487;&#20197;&#21019;&#24314;&#31934;&#32454;&#35843;&#25972;&#30340;&#27169;&#22411;&#24182;&#22312;&#34507;&#30333;&#36136;&#21147;&#23398;&#21644;&#35774;&#35745;&#39046;&#22495;&#24212;&#29992;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#28145;&#23618;&#36880;&#23618;&#36866;&#24212;&#30340;&#32452;&#21512;&#26469;&#35299;&#20915;&#29305;&#23450;&#20219;&#21153;&#65292;&#24182;&#21463;&#21040;&#29983;&#29289;&#23398;&#21407;&#29702;&#30340;&#21551;&#21457;&#12290;&#26080;&#38656;&#20462;&#25913;&#24213;&#23618;&#32467;&#26500;&#21363;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.07148</link><description>&lt;p&gt;
X-LoRA: &#19968;&#31181;&#28789;&#27963;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#21033;&#29992;&#20302;&#31209;&#36866;&#37197;&#22120;&#19987;&#23478;&#30340;&#28151;&#21512;&#31574;&#30053;&#22312;&#34507;&#30333;&#36136;&#21147;&#23398;&#21644;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for Large Language Models with Applications in Protein Mechanics and Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07148
&lt;/p&gt;
&lt;p&gt;
X-LoRA&#26159;&#19968;&#31181;&#28789;&#27963;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#21033;&#29992;&#20302;&#31209;&#36866;&#37197;&#22120;&#19987;&#23478;&#30340;&#28151;&#21512;&#31574;&#30053;&#65292;&#21487;&#20197;&#21019;&#24314;&#31934;&#32454;&#35843;&#25972;&#30340;&#27169;&#22411;&#24182;&#22312;&#34507;&#30333;&#36136;&#21147;&#23398;&#21644;&#35774;&#35745;&#39046;&#22495;&#24212;&#29992;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#28145;&#23618;&#36880;&#23618;&#36866;&#24212;&#30340;&#32452;&#21512;&#26469;&#35299;&#20915;&#29305;&#23450;&#20219;&#21153;&#65292;&#24182;&#21463;&#21040;&#29983;&#29289;&#23398;&#21407;&#29702;&#30340;&#21551;&#21457;&#12290;&#26080;&#38656;&#20462;&#25913;&#24213;&#23618;&#32467;&#26500;&#21363;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25253;&#36947;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#23618;&#36880;&#23618;&#22522;&#20110;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#26032;&#39062;&#39044;&#35757;&#32451;&#36866;&#37197;&#22120;&#30340;&#28151;&#21512;&#19987;&#23478;&#31574;&#30053;&#65292;&#29992;&#20110;&#21019;&#24314;&#31934;&#32454;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38544;&#34255;&#29366;&#24577;&#21160;&#24577;&#28151;&#21512;&#32463;&#36807;&#36866;&#24212;&#30340;&#23618;&#30340;&#38376;&#25511;&#31574;&#30053;&#65292;&#20801;&#35768;&#24471;&#21040;&#30340;X-LoRA&#27169;&#22411;&#21033;&#29992;&#19981;&#21516;&#30340;&#33021;&#21147;&#24182;&#21019;&#24314;&#20197;&#21069;&#26410;&#20351;&#29992;&#30340;&#28145;&#23618;&#36880;&#23618;&#36866;&#24212;&#30340;&#32452;&#21512;&#26469;&#35299;&#20915;&#29305;&#23450;&#20219;&#21153;&#12290;&#35813;&#35774;&#35745;&#21463;&#21040;&#20102;&#29983;&#29289;&#26222;&#36941;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#29983;&#29289;&#23398;&#21407;&#29702;&#30340;&#21551;&#21457;&#65292;&#20854;&#20013;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#22359;&#22312;&#19981;&#21516;&#30340;&#20998;&#23618;&#34920;&#31034;&#20013;&#34987;&#37325;&#22797;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;X-LoRA&#27169;&#22411;&#21487;&#20197;&#36731;&#26494;&#29992;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#26080;&#38656;&#20462;&#25913;&#24213;&#23618;&#32467;&#26500;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;X-LoRA&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#21253;&#25324;&#21069;&#21521;/&#36870;&#21521;&#20998;&#26512;&#20219;&#21153;&#21644;&#22686;&#24378;&#25512;&#29702;&#33021;&#21147;&#22312;&#20869;&#30340;&#31185;&#23398;&#33021;&#21147;&#65292;&#37325;&#28857;&#26159;&#29983;&#29289;&#26448;&#26009;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report a mixture of expert strategy to create fine-tuned large language models using a deep layer-wise token-level approach based on low-rank adaptation (LoRA). Starting with a set of pre-trained LoRA adapters, we propose a gating strategy that uses the hidden states to dynamically mix adapted layers, allowing the resulting X-LoRA model to draw upon different capabilities and create never-before-used deep layer-wise combinations of adaptations are established to solve specific tasks. The design is inspired by the biological principles of universality and diversity, where neural network building blocks are reused in different hierarchical manifestations. Hence, the X-LoRA model can be easily implemented for any existing large language model (LLM) without a need for modifications of the underlying structure. We develop a tailored X-LoRA model that offers scientific capabilities including forward/inverse analysis tasks and enhanced reasoning capability, focused on biomaterial analysis,
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#23398;&#20064;&#31995;&#32479;&#23433;&#20840;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#24403;&#21069;&#21457;&#23637;&#29366;&#24577;&#19979;&#30340;&#20851;&#38190;&#38480;&#21046;&#36827;&#34892;&#20102;&#23457;&#26597;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.05355</link><description>&lt;p&gt;
&#23433;&#20840;&#22810;&#27169;&#24577;&#23398;&#20064;&#31995;&#32479;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
A Survey on Safe Multi-Modal Learning System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05355
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#23398;&#20064;&#31995;&#32479;&#23433;&#20840;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#24403;&#21069;&#21457;&#23637;&#29366;&#24577;&#19979;&#30340;&#20851;&#38190;&#38480;&#21046;&#36827;&#34892;&#20102;&#23457;&#26597;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#27169;&#24577;&#23398;&#20064;&#31995;&#32479;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#23433;&#20840;&#38382;&#39064;&#21464;&#24471;&#36234;&#26469;&#36234;&#31361;&#20986;&#12290;&#23545;&#20110;&#36825;&#19968;&#39046;&#22495;&#30340;&#23433;&#20840;&#38382;&#39064;&#32570;&#20047;&#31995;&#32479;&#24615;&#30740;&#31350;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#23398;&#20064;&#31995;&#32479;&#23433;&#20840;&#30340;&#20998;&#31867;&#27861;&#65292;&#30830;&#23450;&#20102;&#36825;&#20123;&#38382;&#39064;&#30340;&#22235;&#20010;&#20851;&#38190;&#25903;&#26609;&#12290;&#20511;&#21161;&#36825;&#19968;&#20998;&#31867;&#27861;&#65292;&#25105;&#20204;&#23545;&#27599;&#20010;&#25903;&#26609;&#36827;&#34892;&#20102;&#28145;&#20837;&#23457;&#26597;&#65292;&#31361;&#20986;&#20102;&#24403;&#21069;&#21457;&#23637;&#29366;&#24577;&#30340;&#20851;&#38190;&#38480;&#21046;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#31995;&#32479;&#23433;&#20840;&#38754;&#20020;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the wide deployment of multimodal learning systems (MMLS) in real-world scenarios, safety concerns have become increasingly prominent. The absence of systematic research into their safety is a significant barrier to progress in this field. To bridge the gap, we present the first taxonomy for MMLS safety, identifying four essential pillars of these concerns. Leveraging this taxonomy, we conduct in-depth reviews for each pillar, highlighting key limitations based on the current state of development. Finally, we pinpoint unique challenges in MMLS safety and provide potential directions for future research.
&lt;/p&gt;</description></item><item><title>TinyLLM&#26159;&#19968;&#31181;&#20174;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#23567;&#22411;&#23398;&#29983;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#33539;&#24335;&#65292;&#26088;&#22312;&#35299;&#20915;&#30693;&#35782;&#22810;&#26679;&#24615;&#26377;&#38480;&#21644;&#32570;&#20047;&#19978;&#19979;&#25991;&#20449;&#24687;&#31561;&#38382;&#39064;&#65292;&#24182;&#40723;&#21169;&#23398;&#29983;&#27169;&#22411;&#29702;&#35299;&#31572;&#26696;&#32972;&#21518;&#30340;&#21407;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.04616</link><description>&lt;p&gt;
TinyLLM: &#20174;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#19968;&#20010;&#23567;&#22411;&#23398;&#29983;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TinyLLM: Learning a Small Student from Multiple Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04616
&lt;/p&gt;
&lt;p&gt;
TinyLLM&#26159;&#19968;&#31181;&#20174;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#23567;&#22411;&#23398;&#29983;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#33539;&#24335;&#65292;&#26088;&#22312;&#35299;&#20915;&#30693;&#35782;&#22810;&#26679;&#24615;&#26377;&#38480;&#21644;&#32570;&#20047;&#19978;&#19979;&#25991;&#20449;&#24687;&#31561;&#38382;&#39064;&#65292;&#24182;&#40723;&#21169;&#23398;&#29983;&#27169;&#22411;&#29702;&#35299;&#31572;&#26696;&#32972;&#21518;&#30340;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#26356;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#36716;&#31227;&#21040;&#36739;&#23567;&#30340;&#27169;&#22411;&#19978;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#36739;&#23567;&#30340;LLMs&#26356;&#28789;&#27963;&#65292;&#25104;&#26412;&#26356;&#20302;&#12290;&#22312;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#20013;&#65292;&#30693;&#35782;&#33976;&#39311;&#22240;&#20854;&#20986;&#33394;&#30340;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#32780;&#33073;&#39062;&#32780;&#20986;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65292;&#21253;&#25324;&#30693;&#35782;&#22810;&#26679;&#24615;&#26377;&#38480;&#21644;&#32570;&#20047;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#24182;&#20419;&#36827;&#32039;&#20945;&#35821;&#35328;&#27169;&#22411;&#30340;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TinyLLM&#65292;&#19968;&#31181;&#20174;&#22810;&#20010;&#22823;&#22411;&#25945;&#24072;LLMs&#20013;&#23398;&#20064;&#23567;&#22411;&#23398;&#29983;LLM&#30340;&#26032;&#22411;&#30693;&#35782;&#33976;&#39311;&#33539;&#24335;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#40723;&#21169;&#23398;&#29983;LLM&#19981;&#20165;&#29983;&#25104;&#27491;&#30830;&#31572;&#26696;&#65292;&#32780;&#19988;&#29702;&#35299;&#36825;&#20123;&#31572;&#26696;&#32972;&#21518;&#30340;&#21407;&#29702;&#12290;&#37492;&#20110;&#19981;&#21516;&#30340;LLMs&#20855;&#26377;&#19981;&#21516;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#23548;&#23398;&#29983;&#27169;&#22411;&#21560;&#25910;&#26469;&#33258;&#22810;&#20010;&#25945;&#24072;LLMs&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#31034;&#20363;&#29983;&#25104;&#22120;&#21644;&#19968;&#20010;&#32769;&#24072;&#24378;&#21046;&#27169;&#22359;...
&lt;/p&gt;
&lt;p&gt;
Transferring the reasoning capability from stronger large language models (LLMs) to smaller ones has been quite appealing, as smaller LLMs are more flexible to deploy with less expense. Among the existing solutions, knowledge distillation stands out due to its outstanding efficiency and generalization. However, existing methods suffer from several drawbacks, including limited knowledge diversity and the lack of rich contextual information. To solve the problems and facilitate the learning of compact language models, we propose TinyLLM, a novel knowledge distillation paradigm to learn a small student LLM from multiple large teacher LLMs. In particular, we encourage the student LLM to not only generate the correct answers but also understand the rationales behind these answers. Given that different LLMs possess diverse reasoning skills, we guide the student model to assimilate knowledge from various teacher LLMs. We further introduce an in-context example generator and a teacher-forcing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32593;&#39029;&#25130;&#22270;&#20013;&#20803;&#32032;&#30340;&#8220;&#21452;&#35270;&#22270;&#8221;&#26469;&#36827;&#34892; HTML &#20803;&#32032;&#30340;&#32972;&#26223;&#21270;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23558;&#27599;&#20010;&#20803;&#32032;&#19982;&#20854;&#37051;&#23621;&#20803;&#32032;&#36827;&#34892;&#32972;&#26223;&#21270;&#65292;&#20351;&#29992;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#65292;&#20351;&#24471;HTML&#20803;&#32032;&#30340;&#32467;&#26524;&#34920;&#31034;&#23545;&#20110;&#20195;&#29702;&#25191;&#34892;&#25805;&#20316;&#26356;&#21152;&#20449;&#24687;&#20016;&#23500;&#12290;</title><link>https://arxiv.org/abs/2402.04476</link><description>&lt;p&gt;
&#21452;&#35270;&#22270;&#35270;&#35273;&#32972;&#26223;&#21270;&#30340;&#32593;&#39029;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Dual-View Visual Contextualization for Web Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32593;&#39029;&#25130;&#22270;&#20013;&#20803;&#32032;&#30340;&#8220;&#21452;&#35270;&#22270;&#8221;&#26469;&#36827;&#34892; HTML &#20803;&#32032;&#30340;&#32972;&#26223;&#21270;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23558;&#27599;&#20010;&#20803;&#32032;&#19982;&#20854;&#37051;&#23621;&#20803;&#32032;&#36827;&#34892;&#32972;&#26223;&#21270;&#65292;&#20351;&#29992;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#65292;&#20351;&#24471;HTML&#20803;&#32032;&#30340;&#32467;&#26524;&#34920;&#31034;&#23545;&#20110;&#20195;&#29702;&#25191;&#34892;&#25805;&#20316;&#26356;&#21152;&#20449;&#24687;&#20016;&#23500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#32593;&#39029;&#23548;&#33322;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#21487;&#20197;&#26681;&#25454;&#35821;&#35328;&#25351;&#20196;&#22312;&#23454;&#38469;&#32593;&#31449;&#19978;&#25191;&#34892;&#22797;&#26434;&#21644;&#22810;&#26679;&#20219;&#21153;&#30340;&#32593;&#32476;&#20195;&#29702;&#12290;&#29616;&#26377;&#24037;&#20316;&#20027;&#35201;&#26159;&#20197; HTML &#25991;&#26723;&#20316;&#20026;&#36755;&#20837;&#65292;HTML &#25991;&#26723;&#23450;&#20041;&#20102;&#32593;&#39029;&#30340;&#20869;&#23481;&#21644;&#25805;&#20316;&#31354;&#38388;&#65288;&#21363;&#21487;&#25805;&#20316;&#20803;&#32032;&#21644;&#25805;&#20316;&#65289;&#12290;&#28982;&#32780;&#65292;HTML &#25991;&#26723;&#21487;&#33021;&#26080;&#27861;&#20026;&#27599;&#20010;&#20803;&#32032;&#25552;&#20379;&#28165;&#26224;&#30340;&#20219;&#21153;&#30456;&#20851;&#32972;&#26223;&#65292;&#20351;&#24471;&#36873;&#25321;&#27491;&#30830;&#30340;&#65288;&#19968;&#31995;&#21015;&#30340;&#65289;&#25805;&#20316;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#32593;&#39029;&#25130;&#22270;&#20013;&#20803;&#32032;&#30340;&#8220;&#21452;&#35270;&#22270;&#8221;&#26469;&#36827;&#34892; HTML &#20803;&#32032;&#30340;&#32972;&#26223;&#21270;&#65306;&#27599;&#20010; HTML &#20803;&#32032;&#22312;&#25130;&#22270;&#20013;&#26377;&#20854;&#23545;&#24212;&#30340;&#36793;&#30028;&#26694;&#21644;&#35270;&#35273;&#20869;&#23481;&#12290;&#25105;&#20204;&#22522;&#20110;&#19968;&#20010;&#27934;&#23519;&#21147;&#8212;&#8212;&#32593;&#39029;&#24320;&#21457;&#32773;&#20542;&#21521;&#20110;&#22312;&#32593;&#39029;&#19978;&#23558;&#20219;&#21153;&#30456;&#20851;&#20803;&#32032;&#25918;&#32622;&#22312;&#38468;&#36817;&#20197;&#22686;&#24378;&#29992;&#25143;&#20307;&#39564;&#65292;&#24182;&#25552;&#20986;&#23558;&#27599;&#20010;&#20803;&#32032;&#19982;&#20854;&#37051;&#23621;&#20803;&#32032;&#36827;&#34892;&#32972;&#26223;&#21270;&#65292;&#20351;&#29992;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#12290;HTML &#20803;&#32032;&#30340;&#32467;&#26524;&#34920;&#31034;&#23545;&#20110;&#20195;&#29702;&#25191;&#34892;&#25805;&#20316;&#26356;&#21152;&#20449;&#24687;&#20016;&#23500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic web navigation aims to build a web agent that can follow language instructions to execute complex and diverse tasks on real-world websites. Existing work primarily takes HTML documents as input, which define the contents and action spaces (i.e., actionable elements and operations) of webpages. Nevertheless, HTML documents may not provide a clear task-related context for each element, making it hard to select the right (sequence of) actions. In this paper, we propose to contextualize HTML elements through their "dual views" in webpage screenshots: each HTML element has its corresponding bounding box and visual content in the screenshot. We build upon the insight -- web developers tend to arrange task-related elements nearby on webpages to enhance user experiences -- and propose to contextualize each element with its neighbor elements, using both textual and visual features. The resulting representations of HTML elements are more informative for the agent to take action. We val
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#20026;&#26234;&#33021;&#20307;&#25552;&#20379;&#22686;&#24378;&#24418;&#24335;&#30340;&#20219;&#21153;&#25351;&#23548;&#65292;&#20351;&#20854;&#33021;&#22815;&#29702;&#35299;&#28216;&#25103;&#25351;&#23548;&#24182;&#23454;&#29616;"&#35835;&#29609;&#28216;&#25103;"&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#30340;&#25104;&#21151;&#24212;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#26500;&#24314;&#20102;&#19968;&#32452;... (&#20869;&#23481;&#22826;&#38271;&#65292;&#26080;&#27861;&#32487;&#32493;&#26174;&#31034;)</title><link>https://arxiv.org/abs/2402.04154</link><description>&lt;p&gt;
&#12298;&#35835;&#29609;&#28216;&#25103;&#65288;R2-Play&#65289;: &#22810;&#27169;&#24577;&#28216;&#25103;&#25351;&#23548;&#19979;&#30340;&#20915;&#31574; Transformer&#12299;
&lt;/p&gt;
&lt;p&gt;
Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#20026;&#26234;&#33021;&#20307;&#25552;&#20379;&#22686;&#24378;&#24418;&#24335;&#30340;&#20219;&#21153;&#25351;&#23548;&#65292;&#20351;&#20854;&#33021;&#22815;&#29702;&#35299;&#28216;&#25103;&#25351;&#23548;&#24182;&#23454;&#29616;"&#35835;&#29609;&#28216;&#25103;"&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#30340;&#25104;&#21151;&#24212;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#26500;&#24314;&#20102;&#19968;&#32452;... (&#20869;&#23481;&#22826;&#38271;&#65292;&#26080;&#27861;&#32487;&#32493;&#26174;&#31034;)
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#24320;&#21457;&#19968;&#27454;&#36890;&#29992;&#26234;&#33021;&#20307;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#30340;&#30446;&#26631;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21033;&#29992;&#26469;&#33258;&#21508;&#31181;&#20219;&#21153;&#30340;&#22823;&#37327;&#31163;&#32447;&#25968;&#25454;&#38598;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#20219;&#21153;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#22312;&#25193;&#23637;&#21040;&#26032;&#20219;&#21153;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#23558;&#25991;&#26412;&#25351;&#23548;&#25110;&#35270;&#35273;&#36712;&#36857;&#25972;&#21512;&#21040;&#20915;&#31574;&#32593;&#32476;&#20013;&#65292;&#25552;&#20379;&#20219;&#21153;&#29305;&#23450;&#30340;&#19978;&#19979;&#25991;&#25552;&#31034;&#65292;&#20195;&#34920;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#35266;&#23519;&#21040;&#20165;&#20381;&#36182;&#20110;&#25991;&#26412;&#25351;&#23548;&#25110;&#35270;&#35273;&#36712;&#36857;&#23545;&#20110;&#20934;&#30830;&#20256;&#36798;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#22686;&#24378;&#26234;&#33021;&#20307;&#20219;&#21153;&#25351;&#23548;&#30340;&#24418;&#24335;&#65292;&#20351;&#20854;&#33021;&#22815;&#29702;&#35299;&#28216;&#25103;&#25351;&#23548;&#65292;&#20174;&#32780;&#23454;&#29616;"&#35835;&#29609;&#28216;&#25103;"&#30340;&#33021;&#21147;&#12290;&#21463;&#21040;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#22522;&#20110;&#35270;&#35273;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#35270;&#20026;&#19968;&#20010;&#38271;&#26399;&#35270;&#35273;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#32452;... (&#20869;&#23481;&#22826;&#38271;&#65292;&#26080;&#27861;&#32487;&#32493;&#26174;&#31034;)
&lt;/p&gt;
&lt;p&gt;
Developing a generalist agent is a longstanding objective in artificial intelligence. Previous efforts utilizing extensive offline datasets from various tasks demonstrate remarkable performance in multitasking scenarios within Reinforcement Learning.However, these works encounter challenges in extending their capabilities to new tasks.Recent approaches integrate textual guidance or visual trajectory into decision networks to provide task-specific contextual cues, representing a promising direction.However, it is observed that relying solely on textual guidance or visual trajectory is insufficient for accurately conveying the contextual information of tasks.This paper explores enhanced forms of task guidance for agents, enabling them to comprehend gameplay instructions, thereby facilitating a "read-to-play" capability.Drawing inspiration from the success of multimodal instruction tuning in visual tasks, we treat the visual-based RL task as a long-horizon vision task and construct a set 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#26597;&#12289;&#20998;&#26512;&#20102;&#22312;&#29359;&#32618;&#32593;&#32476;&#20013;&#35782;&#21035;&#29359;&#32618;&#22836;&#30446;&#30340;&#25216;&#26415;&#21644;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#35770;&#20998;&#31867;&#20307;&#31995;&#65292;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#21644;&#23454;&#39564;&#27604;&#36739;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#20915;&#31574;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2402.03355</link><description>&lt;p&gt;
&#22312;&#29359;&#32618;&#32593;&#32476;&#20013;&#35782;&#21035;&#29359;&#32618;&#22836;&#30446;&#30340;&#25216;&#26415;&#65306;&#19968;&#39033;&#35843;&#26597;&#12289;&#23454;&#39564;&#21644;&#27604;&#36739;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Techniques to Detect Crime Leaders within a Criminal Network: A Survey, Experimental, and Comparative Evaluations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#26597;&#12289;&#20998;&#26512;&#20102;&#22312;&#29359;&#32618;&#32593;&#32476;&#20013;&#35782;&#21035;&#29359;&#32618;&#22836;&#30446;&#30340;&#25216;&#26415;&#21644;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#35770;&#20998;&#31867;&#20307;&#31995;&#65292;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#21644;&#23454;&#39564;&#27604;&#36739;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#20915;&#31574;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#35770;&#25991;&#23545;&#22312;&#29359;&#32618;&#32593;&#32476;&#20013;&#35782;&#21035;&#29359;&#32618;&#22836;&#30446;&#25152;&#20351;&#29992;&#30340;&#25216;&#26415;&#21644;&#31639;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#38024;&#23545;&#27599;&#31181;&#25216;&#26415;&#65292;&#35770;&#25991;&#35752;&#35770;&#20102;&#20854;&#26377;&#25928;&#24615;&#12289;&#23616;&#38480;&#24615;&#12289;&#25913;&#36827;&#28508;&#21147;&#21644;&#26410;&#26469;&#21069;&#26223;&#12290;&#29616;&#26377;&#20851;&#27880;&#20110;&#35782;&#21035;&#29359;&#32618;&#22836;&#30446;&#21644;&#39044;&#27979;&#29359;&#32618;&#31639;&#27861;&#30340;&#35843;&#26597;&#35770;&#25991;&#25152;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#22914;&#20309;&#26377;&#25928;&#22320;&#23545;&#36825;&#20123;&#31639;&#27861;&#36827;&#34892;&#20998;&#31867;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#35770;&#20998;&#31867;&#20307;&#31995;&#65292;&#23558;&#31639;&#27861;&#20998;&#20026;&#26356;&#35814;&#32454;&#30340;&#31867;&#21035;&#21644;&#20855;&#20307;&#25216;&#26415;&#12290;&#35770;&#25991;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#21644;&#23454;&#39564;&#27604;&#36739;&#23545;&#19981;&#21516;&#25216;&#26415;&#36827;&#34892;&#20102;&#25490;&#21517;&#12290;&#26041;&#27861;&#35770;&#20998;&#31867;&#20307;&#31995;&#12289;&#23454;&#35777;&#35780;&#20272;&#21644;&#23454;&#39564;&#27604;&#36739;&#30340;&#32467;&#21512;&#20351;&#20154;&#20204;&#33021;&#22815;&#20840;&#38754;&#32780;&#32454;&#33268;&#22320;&#20102;&#35299;&#35782;&#21035;&#29359;&#32618;&#22836;&#30446;&#30340;&#25216;&#26415;&#21644;&#31639;&#27861;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#26126;&#26234;&#30340;&#20915;&#31574;&#25903;&#25345;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#25552;&#20379;&#20102;&#20854;&#20182;&#26041;&#38754;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey paper offers a thorough analysis of techniques and algorithms used in the identification of crime leaders within criminal networks. For each technique, the paper examines its effectiveness, limitations, potential for improvement, and future prospects. The main challenge faced by existing survey papers focusing on algorithms for identifying crime leaders and predicting crimes is effectively categorizing these algorithms. To address this limitation, this paper proposes a new methodological taxonomy that hierarchically classifies algorithms into more detailed categories and specific techniques. The paper includes empirical and experimental evaluations to rank the different techniques. The combination of the methodological taxonomy, empirical evaluations, and experimental comparisons allows for a nuanced and comprehensive understanding of the techniques and algorithms for identifying crime leaders, assisting researchers in making informed decisions. Moreover, the paper offers v
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20840;&#38754;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#27599;&#20010;&#28508;&#22312;&#22240;&#32032;&#12290;&#23545;&#20110;&#19981;&#21516;&#29983;&#25104;&#27169;&#22411;&#30340;&#35299;&#37322;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#20102;&#24615;&#33021;&#35780;&#20272;&#65292;&#19988;&#23450;&#24615;&#22320;&#23637;&#31034;&#20102;&#28508;&#22312;&#22240;&#32032;&#30340;&#21464;&#21270;&#25928;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01858</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Explaining latent representations of generative models with large multimodal models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01858
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20840;&#38754;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#27599;&#20010;&#28508;&#22312;&#22240;&#32032;&#12290;&#23545;&#20110;&#19981;&#21516;&#29983;&#25104;&#27169;&#22411;&#30340;&#35299;&#37322;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#20102;&#24615;&#33021;&#35780;&#20272;&#65292;&#19988;&#23450;&#24615;&#22320;&#23637;&#31034;&#20102;&#28508;&#22312;&#22240;&#32032;&#30340;&#21464;&#21270;&#25928;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#29983;&#25104;&#28508;&#22312;&#22240;&#32032;&#34920;&#31034;&#26159;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#30340;&#37325;&#35201;&#35838;&#39064;&#12290;&#38543;&#30528;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#23427;&#21487;&#20197;&#23558;&#22270;&#20687;&#19982;&#25991;&#26412;&#23545;&#40784;&#20197;&#29983;&#25104;&#31572;&#26696;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20840;&#38754;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#27599;&#20010;&#28508;&#22312;&#22240;&#32032;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#37327;&#21270;&#35780;&#20272;&#20102;&#25105;&#20204;&#29983;&#25104;&#30340;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#22312;&#22810;&#20010;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20043;&#38388;&#35780;&#20272;&#20102;&#35299;&#37322;&#29983;&#25104;&#30340;&#24615;&#33021;&#65292;&#24182;&#23450;&#24615;&#22320;&#21487;&#35270;&#21270;&#20102;&#27599;&#20010;&#28508;&#22312;&#22240;&#32032;&#30340;&#21464;&#21270;&#65292;&#20197;&#23398;&#20064;&#19981;&#21516;&#29983;&#25104;&#27169;&#22411;&#23545;&#35299;&#37322;&#30340;&#35299;&#32544;&#25928;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning interpretable representations of data generative latent factors is an important topic for the development of artificial intelligence. With the rise of the large multimodal model, it can align images with text to generate answers. In this work, we propose a framework to comprehensively explain each latent factor in the generative models using a large multimodal model. We further measure the uncertainty of our generated explanations, quantitatively evaluate the performance of explanation generation among multiple large multimodal models, and qualitatively visualize the variations of each latent factor to learn the disentanglement effects of different generative models on explanations. Finally, we discuss the explanatory capabilities and limitations of state-of-the-art large multimodal models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#26412;&#22320;&#31070;&#32463;&#32593;&#32476;&#21644;&#26377;&#38480;&#20803;&#27714;&#35299;&#22120;&#30340;PNP&#31163;&#23376;&#36890;&#36947;&#28145;&#24230;&#23398;&#20064;&#27714;&#35299;&#22120;&#12290;&#35813;&#27714;&#35299;&#22120;&#33021;&#22815;&#36739;&#24555;&#22320;&#35757;&#32451;&#24182;&#29983;&#25104;&#39640;&#20934;&#30830;&#24230;&#30340;&#25968;&#20540;&#35299;&#65292;&#22312;&#22788;&#29702;&#19981;&#21516;&#25200;&#21160;&#24773;&#20917;&#21644;&#31163;&#23376;&#36890;&#36947;&#23376;&#21306;&#22495;&#26102;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>https://arxiv.org/abs/2401.17513</link><description>&lt;p&gt;
&#19968;&#20010;&#20855;&#26377;&#26412;&#22320;&#31070;&#32463;&#32593;&#32476;&#21644;&#26377;&#38480;&#20803;&#36755;&#20837;&#25968;&#25454;&#30340;PNP&#31163;&#23376;&#36890;&#36947;&#28145;&#24230;&#23398;&#20064;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
A PNP ion channel deep learning solver with local neural network and finite element input data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17513
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#26412;&#22320;&#31070;&#32463;&#32593;&#32476;&#21644;&#26377;&#38480;&#20803;&#27714;&#35299;&#22120;&#30340;PNP&#31163;&#23376;&#36890;&#36947;&#28145;&#24230;&#23398;&#20064;&#27714;&#35299;&#22120;&#12290;&#35813;&#27714;&#35299;&#22120;&#33021;&#22815;&#36739;&#24555;&#22320;&#35757;&#32451;&#24182;&#29983;&#25104;&#39640;&#20934;&#30830;&#24230;&#30340;&#25968;&#20540;&#35299;&#65292;&#22312;&#22788;&#29702;&#19981;&#21516;&#25200;&#21160;&#24773;&#20917;&#21644;&#31163;&#23376;&#36890;&#36947;&#23376;&#21306;&#22495;&#26102;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#25913;&#36827;&#30340;&#19968;&#32500;Poisson-Nernst-Planck&#31163;&#23376;&#36890;&#36947;&#65288;PNPic&#65289;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;PNPic&#28145;&#24230;&#23398;&#20064;&#27714;&#35299;&#22120;&#12290;&#29305;&#21035;&#26159;&#65292;&#23427;&#23558;&#19968;&#31181;&#26032;&#39062;&#30340;&#26412;&#22320;&#31070;&#32463;&#32593;&#32476;&#26041;&#26696;&#19982;&#26377;&#25928;&#30340;PNPic&#26377;&#38480;&#20803;&#27714;&#35299;&#22120;&#30456;&#32467;&#21512;&#12290;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#26041;&#26696;&#30340;&#36755;&#20837;&#25968;&#25454;&#21482;&#28041;&#21450;&#31895;&#32593;&#26684;&#35299;&#30340;&#19968;&#23567;&#22359;&#23616;&#37096;&#21306;&#22495;&#65292;&#26377;&#38480;&#20803;&#27714;&#35299;&#22120;&#21487;&#20197;&#24555;&#36895;&#29983;&#25104;&#65292;&#22240;&#27492;PNPic&#28145;&#24230;&#23398;&#20064;&#27714;&#35299;&#22120;&#30340;&#35757;&#32451;&#36895;&#24230;&#27604;&#20219;&#20309;&#23545;&#24212;&#30340;&#20256;&#32479;&#20840;&#23616;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#22120;&#37117;&#35201;&#24555;&#12290;&#32463;&#36807;&#36866;&#24403;&#35757;&#32451;&#65292;&#23427;&#21487;&#20197;&#36755;&#20986;&#19968;&#20010;&#27604;&#20302;&#25104;&#26412;&#31895;&#32593;&#26684;&#35299;&#26356;&#39640;&#20934;&#30830;&#24230;&#30340;&#39044;&#27979;PNPic&#35299;&#65292;&#24182;&#33021;&#21453;&#26144;&#19981;&#21516;&#21442;&#25968;&#25200;&#21160;&#24773;&#20917;&#12289;&#31163;&#23376;&#36890;&#36947;&#23376;&#21306;&#22495;&#20197;&#21450;&#30028;&#38754;&#21644;&#36793;&#30028;&#20540;&#31561;&#12290;&#22240;&#27492;&#65292;PNPic&#28145;&#24230;&#23398;&#20064;&#27714;&#35299;&#22120;&#21487;&#20197;&#20026;&#19968;&#31867;PNPic&#27169;&#22411;&#29983;&#25104;&#39640;&#20934;&#30830;&#24230;&#30340;&#25968;&#20540;&#35299;&#12290;&#20316;&#20026;&#19968;&#20010;&#21021;&#27493;&#30740;&#31350;&#65292;&#20004;&#20010;......
&lt;/p&gt;
&lt;p&gt;
In this paper, a deep learning method for solving an improved one-dimensional Poisson-Nernst-Planck ion channel (PNPic) model, called the PNPic deep learning solver, is presented. In particular, it combines a novel local neural network scheme with an effective PNPic finite element solver. Since the input data of the neural network scheme only involves a small local patch of coarse grid solutions, which the finite element solver can quickly produce, the PNPic deep learning solver can be trained much faster than any corresponding conventional global neural network solvers. After properly trained, it can output a predicted PNPic solution in a much higher degree of accuracy than the low cost coarse grid solutions and can reflect different perturbation cases on the parameters, ion channel subregions, and interface and boundary values, etc. Consequently, the PNPic deep learning solver can generate a numerical solution with high accuracy for a family of PNPic models. As an initial study, two 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#26435;&#34913;&#22788;&#29702;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#20013;&#30340;&#32039;&#24352;&#20851;&#31995;&#30340;&#20116;&#31181;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#23454;&#26045;&#20840;&#38754;&#30340;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;</title><link>https://arxiv.org/abs/2401.08103</link><description>&lt;p&gt;
&#35299;&#20915;&#22312;&#23454;&#26045;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#20262;&#29702;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Resolving Ethics Trade-offs in Implementing Responsible AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08103
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#26435;&#34913;&#22788;&#29702;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#20013;&#30340;&#32039;&#24352;&#20851;&#31995;&#30340;&#20116;&#31181;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#23454;&#26045;&#20840;&#38754;&#30340;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#25226;&#39640;&#32423;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#21407;&#21017;&#24212;&#29992;&#21040;&#23454;&#38469;&#30340;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#24050;&#32463;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22312;&#22788;&#29702;&#24213;&#23618;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#26041;&#38754;&#30340;&#32039;&#24352;&#20851;&#31995;&#26041;&#38754;&#20173;&#23384;&#22312;&#29702;&#35770;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20116;&#31181;&#22788;&#29702;&#36825;&#20123;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#20174;&#31616;&#21333;&#21040;&#22797;&#26434;&#19981;&#31561;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#32771;&#34385;&#30340;&#19978;&#19979;&#25991;&#31867;&#22411;&#12289;&#33539;&#22260;&#12289;&#34913;&#37327;&#19978;&#19979;&#25991;&#30340;&#26041;&#27861;&#21644;&#35777;&#26126;&#31243;&#24230;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;&#36825;&#20123;&#26041;&#27861;&#20013;&#27809;&#26377;&#19968;&#31181;&#36866;&#29992;&#20110;&#25152;&#26377;&#32452;&#32455;&#12289;&#31995;&#32479;&#25110;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21253;&#25324;&#65306;&#65288;i&#65289;&#31215;&#26497;&#35782;&#21035;&#32039;&#24352;&#20851;&#31995;&#65292;&#65288;ii&#65289;&#20248;&#20808;&#22788;&#29702;&#21644;&#26435;&#34913;&#20262;&#29702;&#26041;&#38754;&#65292;&#65288;iii&#65289;&#35777;&#26126;&#21644;&#35760;&#24405;&#26435;&#34913;&#20915;&#31574;&#12290;&#35813;&#25552;&#35758;&#30340;&#26694;&#26550;&#26088;&#22312;&#20419;&#36827;&#23454;&#26045;&#31526;&#21512;&#28508;&#22312;&#30417;&#31649;&#35201;&#27714;&#30340;&#20840;&#38754;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the operationalisation of high-level AI ethics principles into practical AI/ML systems has made progress, there is still a theory-practice gap in managing tensions between the underlying AI ethics aspects. We cover five approaches for addressing the tensions via trade-offs, ranging from rudimentary to complex. The approaches differ in the types of considered context, scope, methods for measuring contexts, and degree of justification. None of the approaches is likely to be appropriate for all organisations, systems, or applications. To address this, we propose a framework which consists of: (i) proactive identification of tensions, (ii) prioritisation and weighting of ethics aspects, (iii) justification and documentation of trade-off decisions. The proposed framework aims to facilitate the implementation of well-rounded AI/ML systems that are appropriate for potential regulatory requirements.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Dr$^2$Net&#30340;&#26032;&#22411;&#32593;&#32476;&#32467;&#26500;&#65292;&#21487;&#20197;&#20197;&#22823;&#22823;&#38477;&#20302;&#20869;&#23384;&#28040;&#32791;&#30340;&#26041;&#24335;&#20316;&#20026;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26367;&#20195;&#32593;&#32476;&#65292;&#36890;&#36807;&#20855;&#26377;&#20004;&#31181;&#31867;&#22411;&#27531;&#24046;&#36830;&#25509;&#30340;&#35774;&#35745;&#23454;&#29616;&#20102;&#21487;&#36870;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28165;&#38500;&#20013;&#38388;&#28608;&#27963;&#24182;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;</title><link>https://arxiv.org/abs/2401.04105</link><description>&lt;p&gt;
Dr$^2$Net&#65306;&#29992;&#20110;&#20869;&#23384;&#39640;&#25928;&#24494;&#35843;&#30340;&#21160;&#24577;&#21487;&#36870;&#21452;&#27531;&#24046;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Dr$^2$Net: Dynamic Reversible Dual-Residual Networks for Memory-Efficient Finetuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.04105
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Dr$^2$Net&#30340;&#26032;&#22411;&#32593;&#32476;&#32467;&#26500;&#65292;&#21487;&#20197;&#20197;&#22823;&#22823;&#38477;&#20302;&#20869;&#23384;&#28040;&#32791;&#30340;&#26041;&#24335;&#20316;&#20026;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26367;&#20195;&#32593;&#32476;&#65292;&#36890;&#36807;&#20855;&#26377;&#20004;&#31181;&#31867;&#22411;&#27531;&#24046;&#36830;&#25509;&#30340;&#35774;&#35745;&#23454;&#29616;&#20102;&#21487;&#36870;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28165;&#38500;&#20013;&#38388;&#28608;&#27963;&#24182;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29616;&#20195;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#31471;&#23545;&#31471;&#24494;&#35843;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20351;&#29992;&#65292;&#23545;&#20110;&#20855;&#26377;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#30340;&#20219;&#21153;&#65288;&#20363;&#22914;&#35270;&#39057;&#29702;&#35299;&#12289;&#23567;&#30446;&#26631;&#26816;&#27979;&#21644;&#28857;&#20113;&#20998;&#26512;&#65289;&#65292;&#36825;&#31181;&#24494;&#35843;&#23545;&#20869;&#23384;&#35201;&#27714;&#24456;&#39640;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Dr$^2$Net&#65288;Dynamic Reversible Dual-Residual Networks&#65289;&#30340;&#26032;&#22411;&#32593;&#32476;&#32467;&#26500;&#31995;&#21015;&#65292;&#23427;&#20805;&#24403;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26367;&#20195;&#32593;&#32476;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#20869;&#23384;&#28040;&#32791;&#12290;Dr$^2$Net&#21253;&#21547;&#20004;&#31181;&#31867;&#22411;&#30340;&#27531;&#24046;&#36830;&#25509;&#65292;&#19968;&#31181;&#20445;&#25345;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#27531;&#24046;&#32467;&#26500;&#65292;&#21478;&#19968;&#31181;&#20351;&#32593;&#32476;&#21487;&#36870;&#12290;&#30001;&#20110;&#20854;&#21487;&#36870;&#24615;&#65292;&#21487;&#20197;&#20174;&#36755;&#20986;&#20013;&#37325;&#24314;&#20013;&#38388;&#28608;&#27963;&#65292;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23558;&#20854;&#28165;&#38500;&#20869;&#23384;&#12290;&#25105;&#20204;&#20998;&#21035;&#22312;&#20004;&#31181;&#27531;&#24046;&#36830;&#25509;&#31867;&#22411;&#19978;&#24341;&#20837;&#20004;&#20010;&#31995;&#25968;&#65292;&#24182;&#24341;&#20837;&#21160;&#24577;tr
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.04105v2 Announce Type: replace-cross  Abstract: Large pretrained models are increasingly crucial in modern computer vision tasks. These models are typically used in downstream tasks by end-to-end finetuning, which is highly memory-intensive for tasks with high-resolution data, e.g., video understanding, small object detection, and point cloud analysis. In this paper, we propose Dynamic Reversible Dual-Residual Networks, or Dr$^2$Net, a novel family of network architectures that acts as a surrogate network to finetune a pretrained model with substantially reduced memory consumption. Dr$^2$Net contains two types of residual connections, one maintaining the residual structure in the pretrained models, and the other making the network reversible. Due to its reversibility, intermediate activations, which can be reconstructed from output, are cleared from memory during training. We use two coefficients on either type of residual connections respectively, and introduce a dynamic tr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#29366;&#24577;&#31934;&#28860;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25581;&#31034;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24433;&#21709;&#26234;&#33021;&#20307;&#30446;&#26631;&#30340;&#22870;&#21169;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#19981;&#36879;&#26126;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2401.00104</link><description>&lt;p&gt;
&#22240;&#26524;&#29366;&#24577;&#31934;&#28860;&#29992;&#20110;&#21487;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Causal State Distillation for Explainable Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.00104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#29366;&#24577;&#31934;&#28860;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25581;&#31034;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24433;&#21709;&#26234;&#33021;&#20307;&#30446;&#26631;&#30340;&#22870;&#21169;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#19981;&#36879;&#26126;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26159;&#35757;&#32451;&#26234;&#33021;&#20307;&#30340;&#24378;&#22823;&#25216;&#26415;&#65292;&#20294;&#29702;&#35299;&#36825;&#20123;&#26234;&#33021;&#20307;&#20026;&#20309;&#20570;&#20986;&#29305;&#23450;&#20915;&#31574;&#21487;&#33021;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;RL&#27169;&#22411;&#30340;&#19981;&#36879;&#26126;&#24615;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#20351;&#29992;&#25143;&#38590;&#20197;&#29702;&#35299;&#26234;&#33021;&#20307;&#34892;&#20026;&#32972;&#21518;&#30340;&#21407;&#22240;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#29366;&#24577;&#31934;&#28860;&#65288;Causal State Distillation&#65289;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20811;&#26381;&#22870;&#21169;&#20998;&#35299;&#31561;&#20854;&#20182;&#26041;&#27861;&#25152;&#24102;&#26469;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25581;&#31034;&#20986;&#23545;&#26234;&#33021;&#20307;&#30446;&#26631;&#20135;&#29983;&#24433;&#21709;&#30340;&#22870;&#21169;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.00104v2 Announce Type: replace-cross  Abstract: Reinforcement learning (RL) is a powerful technique for training intelligent agents, but understanding why these agents make specific decisions can be quite challenging. This lack of transparency in RL models has been a long-standing problem, making it difficult for users to grasp the reasons behind an agent's behaviour. Various approaches have been explored to address this problem, with one promising avenue being reward decomposition (RD). RD is appealing as it sidesteps some of the concerns associated with other methods that attempt to rationalize an agent's behaviour in a post-hoc manner. RD works by exposing various facets of the rewards that contribute to the agent's objectives during training. However, RD alone has limitations as it primarily offers insights based on sub-rewards and does not delve into the intricate cause-and-effect relationships that occur within an RL agent's neural model. In this paper, we present an e
&lt;/p&gt;</description></item><item><title>ComplexityNet&#36890;&#36807;&#23398;&#20064;&#20219;&#21153;&#22797;&#26434;&#24615;&#65292;&#25552;&#39640;&#20102;LLM&#25512;&#29702;&#25928;&#29575;&#65292;&#36890;&#36807;&#39044;&#27979;&#20219;&#21153;&#30340;&#20934;&#30830;&#36755;&#20986;&#27010;&#29575;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;90%&#30340;&#35745;&#31639;&#36164;&#28304;&#20351;&#29992;&#65292;&#24182;&#22312;&#20219;&#21153;&#22797;&#26434;&#24615;&#30830;&#23450;&#26041;&#38754;&#21462;&#24471;&#20102;79%&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2312.11511</link><description>&lt;p&gt;
ComplexityNet: &#36890;&#36807;&#23398;&#20064;&#20219;&#21153;&#22797;&#26434;&#24615;&#25552;&#39640;LLM&#25512;&#29702;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
ComplexityNet: Increasing LLM Inference Efficiency by Learning Task Complexity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11511
&lt;/p&gt;
&lt;p&gt;
ComplexityNet&#36890;&#36807;&#23398;&#20064;&#20219;&#21153;&#22797;&#26434;&#24615;&#65292;&#25552;&#39640;&#20102;LLM&#25512;&#29702;&#25928;&#29575;&#65292;&#36890;&#36807;&#39044;&#27979;&#20219;&#21153;&#30340;&#20934;&#30830;&#36755;&#20986;&#27010;&#29575;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;90%&#30340;&#35745;&#31639;&#36164;&#28304;&#20351;&#29992;&#65292;&#24182;&#22312;&#20219;&#21153;&#22797;&#26434;&#24615;&#30830;&#23450;&#26041;&#38754;&#21462;&#24471;&#20102;79%&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;ComplexityNet&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#35780;&#20272;&#20219;&#21153;&#22797;&#26434;&#24615;&#32780;&#35774;&#35745;&#30340;&#31616;&#21270;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#19981;&#21516;&#33021;&#21147;&#30340;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#26469;&#39044;&#27979;&#20934;&#30830;&#36755;&#20986;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#39318;&#27425;&#22312;Mostly Basic Python Problems&#65288;MBPP&#65289;&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#20102;ComplexityNet&#12290;&#25105;&#20204;&#24320;&#21019;&#24615;&#22320;&#21019;&#24314;&#20102;&#31532;&#19968;&#32452;&#26631;&#31614;&#26469;&#23450;&#20041;&#20219;&#21153;&#22797;&#26434;&#24615;&#12290;ComplexityNet&#22312;&#30830;&#23450;&#20219;&#21153;&#22797;&#26434;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;79%&#20934;&#30830;&#29575;&#65292;&#36739;&#21407;&#22987;&#12289;&#38750;&#24494;&#35843;&#27169;&#22411;&#30340;34%&#20934;&#30830;&#29575;&#26377;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#19982;&#20351;&#29992;&#26368;&#39640;&#22797;&#26434;&#24615;&#27169;&#22411;&#30456;&#27604;&#65292;ComplexityNet&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;90%&#30340;&#35745;&#31639;&#36164;&#28304;&#20351;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;86.7%&#30340;&#39640;&#20195;&#30721;&#29983;&#25104;&#20934;&#30830;&#29575;&#12290;&#36825;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#24494;&#35843;&#36739;&#23567;&#30340;&#27169;&#22411;&#26469;&#23545;&#20219;&#21153;&#36827;&#34892;&#20998;&#31867;&#65292;&#21487;&#20197;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#26356;&#24179;&#34913;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11511v2 Announce Type: replace-cross  Abstract: We present ComplexityNet, a streamlined language model designed for assessing task complexity. This model predicts the likelihood of accurate output by various language models, each with different capabilities. Our initial application of ComplexityNet involves the Mostly Basic Python Problems (MBPP) dataset. We pioneered the creation of the first set of labels to define task complexity. ComplexityNet achieved a notable 79% accuracy in determining task complexity, a significant improvement over the 34% accuracy of the original, non fine-tuned model. Furthermore, ComplexityNet effectively reduces computational resource usage by 90% compared to using the highest complexity model, while maintaining a high code generation accuracy of 86.7%. This study demonstrates that fine-tuning smaller models to categorize tasks based on their complexity can lead to a more balanced trade-off between accuracy and efficiency in the use of Large Lan
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#35270;&#39057;&#31505;&#22768;&#25512;&#29702;&#65292;&#26088;&#22312;&#35299;&#37322;&#29305;&#23450;&#35270;&#39057;&#20013;&#20154;&#20204;&#31505;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#25968;&#25454;&#38598;SMILE&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#35270;&#39057;&#34920;&#31034;&#65292;&#25105;&#20204;&#30340;&#22522;&#32447;&#33021;&#22815;&#29983;&#25104;&#21512;&#29702;&#30340;&#31505;&#22768;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2312.09818</link><description>&lt;p&gt;
SMILE&#65306;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#35270;&#39057;&#20013;&#30340;&#31505;&#22768;
&lt;/p&gt;
&lt;p&gt;
SMILE: Multimodal Dataset for Understanding Laughter in Video with Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#35270;&#39057;&#31505;&#22768;&#25512;&#29702;&#65292;&#26088;&#22312;&#35299;&#37322;&#29305;&#23450;&#35270;&#39057;&#20013;&#20154;&#20204;&#31505;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#25968;&#25454;&#38598;SMILE&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#35270;&#39057;&#34920;&#31034;&#65292;&#25105;&#20204;&#30340;&#22522;&#32447;&#33021;&#22815;&#29983;&#25104;&#21512;&#29702;&#30340;&#31505;&#22768;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#26368;&#36817;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#26500;&#24314;&#31038;&#20132;&#26234;&#33021;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20854;&#20013;&#65292;&#31505;&#22768;&#26159;&#20154;&#31867;&#31038;&#20132;&#20114;&#21160;&#20013;&#21457;&#29983;&#30340;&#29420;&#29305;&#34920;&#36798;&#20043;&#19968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#38754;&#23545;&#20102;&#26426;&#22120;&#29702;&#35299;&#35270;&#39057;&#20013;&#31505;&#22768;&#32972;&#21518;&#29702;&#30001;&#30340;&#26032;&#25361;&#25112;&#65292;&#21363;&#35270;&#39057;&#31505;&#22768;&#25512;&#29702;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#36825;&#19968;&#26032;&#20219;&#21153;&#65292;&#35299;&#37322;&#20154;&#20204;&#22312;&#29305;&#23450;&#35270;&#39057;&#20013;&#20026;&#20160;&#20040;&#20250;&#31505;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#36825;&#19968;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;SMILE&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#32447;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#25991;&#26412;&#35270;&#39057;&#34920;&#31034;&#29983;&#25104;&#21512;&#29702;&#30340;&#31505;&#22768;&#35299;&#37322;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#22522;&#32447;&#21487;&#20197;&#29983;&#25104;&#21487;&#20449;&#30340;&#31505;&#22768;&#35299;&#37322;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#25105;&#20204;&#30340;&#22522;&#32447;&#22312;&#25506;&#27979;&#20854;&#20182;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#21644;&#37326;&#22806;&#35270;&#39057;&#26041;&#38754;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12289;&#20195;&#30721;&#21644;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09818v2 Announce Type: replace-cross  Abstract: Despite the recent advances of the artificial intelligence, building social intelligence remains a challenge. Among social signals, laughter is one of the distinctive expressions that occurs during social interactions between humans. In this work, we tackle a new challenge for machines to understand the rationale behind laughter in video, Video Laugh Reasoning. We introduce this new task to explain why people laugh in a particular video and a dataset for this task. Our proposed dataset, SMILE, comprises video clips and language descriptions of why people laugh. We propose a baseline by leveraging the reasoning capacity of large language models (LLMs) with textual video representation. Experiments show that our baseline can generate plausible explanations for laughter. We further investigate the scalability of our baseline by probing other video understanding tasks and in-the-wild videos. We release our dataset, code, and model 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#22522;&#30784;&#27169;&#22411;&#26694;&#26550;&#65288;CLOUDS&#65289;&#29992;&#20110;&#39046;&#22495;&#27867;&#21270;&#35821;&#20041;&#20998;&#21106;&#65292;&#38598;&#25104;&#20102;CLIP&#39592;&#24178;&#12289;&#29983;&#25104;&#27169;&#22411;&#21644;&#8220;Segment Anything Model&#8221;&#65292;&#20197;&#23454;&#29616;&#23545;&#21508;&#31181;&#30446;&#26631;&#20998;&#24067;&#27169;&#24335;&#30340;&#35206;&#30422;&#21644;&#39044;&#27979;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2312.09788</link><description>&lt;p&gt;
&#21327;&#20316;&#22522;&#30784;&#27169;&#22411;&#29992;&#20110;&#39046;&#22495;&#27867;&#21270;&#30340;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Collaborating Foundation Models for Domain Generalized Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09788
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#22522;&#30784;&#27169;&#22411;&#26694;&#26550;&#65288;CLOUDS&#65289;&#29992;&#20110;&#39046;&#22495;&#27867;&#21270;&#35821;&#20041;&#20998;&#21106;&#65292;&#38598;&#25104;&#20102;CLIP&#39592;&#24178;&#12289;&#29983;&#25104;&#27169;&#22411;&#21644;&#8220;Segment Anything Model&#8221;&#65292;&#20197;&#23454;&#29616;&#23545;&#21508;&#31181;&#30446;&#26631;&#20998;&#24067;&#27169;&#24335;&#30340;&#35206;&#30422;&#21644;&#39044;&#27979;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#27867;&#21270;&#35821;&#20041;&#20998;&#21106;&#65288;DGSS&#65289;&#28041;&#21450;&#22312;&#26631;&#35760;&#30340;&#28304;&#22495;&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#26088;&#22312;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;&#12290;&#29616;&#26377;&#30340;DGSS&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#22495;&#38543;&#26426;&#21270;&#65288;DR&#65289;&#23454;&#29616;&#24378;&#20581;&#29305;&#24449;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23427;&#21482;&#33021;&#32771;&#34385;&#26679;&#24335;&#22810;&#26679;&#24615;&#32780;&#19981;&#26159;&#20869;&#23481;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#27491;&#20132;&#30340;DGSS&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#19968;&#31995;&#21015;&#21327;&#20316;&#22522;&#30784;&#27169;&#22411;&#29992;&#20110;&#39046;&#22495;&#27867;&#21270;&#30340;&#35821;&#20041;&#20998;&#21106;&#65288;CLOUDS&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CLOUDS&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;&#21508;&#31181;&#22522;&#30784;&#27169;&#22411;&#30340;&#26694;&#26550;&#65306;&#65288;i&#65289;CLIP&#39592;&#24178;&#29992;&#20110;&#20854;&#24378;&#20581;&#29305;&#24449;&#34920;&#31034;&#65292;&#65288;ii&#65289;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#20351;&#20869;&#23481;&#22810;&#26679;&#21270;&#65292;&#20174;&#32780;&#28085;&#30422;&#21487;&#33021;&#30446;&#26631;&#20998;&#24067;&#30340;&#21508;&#31181;&#27169;&#24335;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#20351;&#29992;&#8220;Segment Anything Model&#8221;&#65288;SAM&#65289;&#36845;&#20195;&#22320;&#25913;&#36827;&#20998;&#21106;&#27169;&#22411;&#39044;&#27979;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;CLOUDS&#34920;&#29616;&#31361;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09788v2 Announce Type: replace-cross  Abstract: Domain Generalized Semantic Segmentation (DGSS) deals with training a model on a labeled source domain with the aim of generalizing to unseen domains during inference. Existing DGSS methods typically effectuate robust features by means of Domain Randomization (DR). Such an approach is often limited as it can only account for style diversification and not content. In this work, we take an orthogonal approach to DGSS and propose to use an assembly of CoLlaborative FOUndation models for Domain Generalized Semantic Segmentation (CLOUDS). In detail, CLOUDS is a framework that integrates FMs of various kinds: (i) CLIP backbone for its robust feature representation, (ii) generative models to diversify the content, thereby covering various modes of the possible target distribution, and (iii) Segment Anything Model (SAM) for iteratively refining the predictions of the segmentation model. Extensive experiments show that our CLOUDS excels
&lt;/p&gt;</description></item><item><title>LatentEditor &#26159;&#19968;&#20010;&#21019;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#23454;&#29616;&#23545;&#31070;&#32463;&#22330;&#36827;&#34892;&#31934;&#30830;&#21644;&#23616;&#37096;&#21463;&#25511;&#32534;&#36753;&#65292;&#23558;&#30495;&#23454;&#22330;&#26223;&#23884;&#20837;&#28508;&#22312;&#31354;&#38388;&#65292;&#25552;&#20379;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#26356;&#20855;&#36866;&#24212;&#24615;&#30340;&#32534;&#36753;NeRF&#39592;&#24178;&#12290;&#24341;&#20837;&#20102;&#22686;&#37327;&#20998;&#25968;&#21644;&#20687;&#32032;&#32423;&#35780;&#20998;&#26041;&#27861;&#20197;&#25552;&#39640;&#32534;&#36753;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2312.09313</link><description>&lt;p&gt;
LatentEditor: &#25991;&#26412;&#39537;&#21160;&#30340;&#19977;&#32500;&#22330;&#26223;&#23616;&#37096;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
LatentEditor: Text Driven Local Editing of 3D Scenes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09313
&lt;/p&gt;
&lt;p&gt;
LatentEditor &#26159;&#19968;&#20010;&#21019;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#23454;&#29616;&#23545;&#31070;&#32463;&#22330;&#36827;&#34892;&#31934;&#30830;&#21644;&#23616;&#37096;&#21463;&#25511;&#32534;&#36753;&#65292;&#23558;&#30495;&#23454;&#22330;&#26223;&#23884;&#20837;&#28508;&#22312;&#31354;&#38388;&#65292;&#25552;&#20379;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#26356;&#20855;&#36866;&#24212;&#24615;&#30340;&#32534;&#36753;NeRF&#39592;&#24178;&#12290;&#24341;&#20837;&#20102;&#22686;&#37327;&#20998;&#25968;&#21644;&#20687;&#32032;&#32423;&#35780;&#20998;&#26041;&#27861;&#20197;&#25552;&#39640;&#32534;&#36753;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31070;&#32463;&#22330;&#22312;&#35270;&#22270;&#21512;&#25104;&#21644;&#22330;&#26223;&#37325;&#24314;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#23427;&#20204;&#38544;&#21547;&#22320;&#20174;&#22810;&#35270;&#22270;&#36755;&#20837;&#32534;&#30721;&#20960;&#20309;&#21644;&#32441;&#29702;&#20449;&#24687;&#65292;&#32534;&#36753;&#23427;&#20204;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;\textsc{LatentEditor}&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#24615;&#26694;&#26550;&#65292;&#26088;&#22312;&#36171;&#20104;&#29992;&#25143;&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#25191;&#34892;&#31070;&#32463;&#22330;&#30340;&#31934;&#30830;&#21644;&#23616;&#37096;&#21463;&#25511;&#32534;&#36753;&#30340;&#33021;&#21147;&#12290;&#21033;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#23558;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#23884;&#20837;&#28508;&#22312;&#31354;&#38388;&#65292;&#20174;&#32780;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#23545;NeRF&#39592;&#24178;&#36827;&#34892;&#26356;&#24555;&#26356;&#20855;&#36866;&#24212;&#24615;&#30340;&#32534;&#36753;&#12290;&#20026;&#20102;&#22686;&#24378;&#32534;&#36753;&#31934;&#24230;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22686;&#37327;&#20998;&#25968;&#26469;&#35745;&#31639;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;2D&#25513;&#30721;&#65292;&#20316;&#20026;&#23616;&#37096;&#20462;&#25913;&#30340;&#25351;&#21335;&#65292;&#21516;&#26102;&#20445;&#30041;&#19981;&#30456;&#20851;&#21306;&#22495;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#30340;&#20687;&#32032;&#32423;&#35780;&#20998;&#26041;&#27861;&#21033;&#29992;&#20102;InstructPix2Pix (IP2P)&#30340;&#33021;&#21147;&#65292;&#20197;&#36776;&#21035; IP2 &#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09313v3 Announce Type: replace-cross  Abstract: While neural fields have made significant strides in view synthesis and scene reconstruction, editing them poses a formidable challenge due to their implicit encoding of geometry and texture information from multi-view inputs. In this paper, we introduce \textsc{LatentEditor}, an innovative framework designed to empower users with the ability to perform precise and locally controlled editing of neural fields using text prompts. Leveraging denoising diffusion models, we successfully embed real-world scenes into the latent space, resulting in a faster and more adaptable NeRF backbone for editing compared to traditional methods. To enhance editing precision, we introduce a delta score to calculate the 2D mask in the latent space that serves as a guide for local modifications while preserving irrelevant regions. Our novel pixel-level scoring approach harnesses the power of InstructPix2Pix (IP2P) to discern the disparity between IP2
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Auto MC-Reward&#30340;&#20808;&#36827;&#23398;&#20064;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35774;&#35745;&#31264;&#23494;&#22870;&#21169;&#20989;&#25968;&#26469;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;</title><link>https://arxiv.org/abs/2312.09238</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20026;Minecraft&#33258;&#21160;&#35774;&#35745;&#31264;&#23494;&#22870;&#21169;&#30340;Auto MC-Reward
&lt;/p&gt;
&lt;p&gt;
Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Auto MC-Reward&#30340;&#20808;&#36827;&#23398;&#20064;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35774;&#35745;&#31264;&#23494;&#22870;&#21169;&#20989;&#25968;&#26469;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#65288;&#20363;&#22914;Minecraft&#65289;&#20165;&#25552;&#20379;&#25351;&#31034;&#20219;&#21153;&#23436;&#25104;&#25110;&#22833;&#36133;&#30340;&#31232;&#30095;&#22870;&#21169;&#65292;&#36825;&#20123;&#22870;&#21169;&#20197;&#20108;&#36827;&#21046;&#20540;&#34920;&#31034;&#12290;&#22312;&#36825;&#31181;&#29615;&#22659;&#20013;&#25506;&#32034;&#25928;&#29575;&#30340;&#25361;&#25112;&#20351;&#24471;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20195;&#29702;&#31243;&#24207;&#38590;&#20197;&#23398;&#20064;&#22797;&#26434;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Auto MC-Reward&#30340;&#20808;&#36827;&#23398;&#20064;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#21160;&#35774;&#35745;&#31264;&#23494;&#22870;&#21169;&#20989;&#25968;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12290;Auto MC-Reward&#21253;&#25324;&#19977;&#20010;&#37325;&#35201;&#32452;&#20214;&#65306;&#22870;&#21169;&#35774;&#35745;&#32773;&#12289;&#22870;&#21169;&#35780;&#35770;&#23478;&#21644;&#36712;&#36857;&#20998;&#26512;&#22120;&#12290;&#32473;&#23450;&#29615;&#22659;&#20449;&#24687;&#21644;&#20219;&#21153;&#25551;&#36848;&#65292;&#22870;&#21169;&#35774;&#35745;&#32773;&#39318;&#20808;&#36890;&#36807;&#32534;&#20889;&#21487;&#25191;&#34892;&#30340;Python&#20989;&#25968;&#21644;&#39044;&#23450;&#20041;&#30340;&#35266;&#27979;&#36755;&#20837;&#26469;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30340;&#22870;&#21169;&#35780;&#35770;&#23478;&#23558;&#36127;&#36131;&#39564;&#35777;&#20195;&#30721;&#65292;&#26816;&#26597;&#20195;&#30721;&#26159;&#21542;&#33258;&#27965;&#19988;&#26080;&#35821;&#27861;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09238v2 Announce Type: replace  Abstract: Many reinforcement learning environments (e.g., Minecraft) provide only sparse rewards that indicate task completion or failure with binary values. The challenge in exploration efficiency in such environments makes it difficult for reinforcement-learning-based agents to learn complex tasks. To address this, this paper introduces an advanced learning system, named Auto MC-Reward, that leverages Large Language Models (LLMs) to automatically design dense reward functions, thereby enhancing the learning efficiency. Auto MC-Reward consists of three important components: Reward Designer, Reward Critic, and Trajectory Analyzer. Given the environment information and task descriptions, the Reward Designer first design the reward function by coding an executable Python function with predefined observation inputs. Then, our Reward Critic will be responsible for verifying the code, checking whether the code is self-consistent and free of syntax 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26082;&#28789;&#27963;&#21448;&#22686;&#24378;&#23616;&#37096;&#24615;&#30340;&#26032;&#22411;&#25237;&#24433;&#20202;&#35774;&#35745;&#65292;&#26377;&#21161;&#20110;&#36830;&#25509;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#31354;&#38388;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2312.06742</link><description>&lt;p&gt;
&#34588;&#34562;&#65306;&#22810;&#27169;&#24577;LLM&#30340;&#22686;&#24378;&#23616;&#37096;&#25237;&#24433;&#20202;
&lt;/p&gt;
&lt;p&gt;
Honeybee: Locality-enhanced Projector for Multimodal LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06742
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26082;&#28789;&#27963;&#21448;&#22686;&#24378;&#23616;&#37096;&#24615;&#30340;&#26032;&#22411;&#25237;&#24433;&#20202;&#35774;&#35745;&#65292;&#26377;&#21161;&#20110;&#36830;&#25509;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#31354;&#38388;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#20013;&#65292;&#35270;&#35273;&#25237;&#24433;&#20202;&#22312;&#36830;&#25509;&#39044;&#20808;&#35757;&#32451;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#19982;LLMs&#20043;&#38388;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#23454;&#29616;&#28145;&#20837;&#30340;&#35270;&#35273;&#29702;&#35299;&#24182;&#21033;&#29992;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#23613;&#31649;&#35270;&#35273;&#25237;&#24433;&#20202;&#30340;&#37325;&#35201;&#24615;&#19981;&#35328;&#32780;&#21947;&#65292;&#20294;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#20004;&#20010;&#20851;&#38190;&#30340;&#25237;&#24433;&#20202;&#23646;&#24615;&#65306;&#65288;i&#65289;&#28789;&#27963;&#24615;&#20197;&#31649;&#29702;&#35270;&#35273;&#20195;&#24065;&#30340;&#25968;&#37327;&#65292;&#23545;&#20110;MLLMs&#30340;&#25972;&#20307;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#65307;&#65288;ii&#65289;&#20445;&#30041;&#26469;&#33258;&#35270;&#35273;&#29305;&#24449;&#30340;&#23616;&#37096;&#19978;&#19979;&#25991;&#65292;&#23545;&#20110;&#31354;&#38388;&#29702;&#35299;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26082;&#28789;&#27963;&#21448;&#22686;&#24378;&#23616;&#37096;&#24615;&#30340;&#26032;&#22411;&#25237;&#24433;&#20202;&#35774;&#35745;&#65292;&#26377;&#25928;&#22320;&#28385;&#36275;&#20102;&#36825;&#20004;&#31181;&#29702;&#24819;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20840;&#38754;&#30340;&#31574;&#30053;&#65292;&#20197;&#26377;&#25928;&#21033;&#29992;&#22810;&#20010;&#21644;&#22810;&#26041;&#38754;&#30340;&#25351;&#23548;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#26816;&#39564;&#20102;&#21508;&#31181;&#35774;&#35745;&#36873;&#25321;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06742v2 Announce Type: replace-cross  Abstract: In Multimodal Large Language Models (MLLMs), a visual projector plays a crucial role in bridging pre-trained vision encoders with LLMs, enabling profound visual understanding while harnessing the LLMs' robust capabilities. Despite the importance of the visual projector, it has been relatively less explored. In this study, we first identify two essential projector properties: (i) flexibility in managing the number of visual tokens, crucial for MLLMs' overall efficiency, and (ii) preservation of local context from visual features, vital for spatial understanding. Based on these findings, we propose a novel projector design that is both flexible and locality-enhanced, effectively satisfying the two desirable properties. Additionally, we present comprehensive strategies to effectively utilize multiple and multifaceted instruction datasets. Through extensive experiments, we examine the impact of individual design choices. Finally, o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;IRSS&#65292;&#36890;&#36807;&#35299;&#32806;&#22270;&#20687;&#30340;&#39118;&#26684;&#21644;&#34394;&#20551;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#22312;&#32570;&#22833;&#22495;&#26631;&#31614;&#24773;&#20917;&#19979;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2312.06226</link><description>&lt;p&gt;
&#36890;&#36807;&#35299;&#32806;&#22270;&#20687;&#30340;&#39118;&#26684;&#21644;&#34394;&#20551;&#29305;&#24449;&#26469;&#23454;&#29616;&#19981;&#21464;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Invariant Representation via Decoupling Style and Spurious Features from Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;IRSS&#65292;&#36890;&#36807;&#35299;&#32806;&#22270;&#20687;&#30340;&#39118;&#26684;&#21644;&#34394;&#20551;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#22312;&#32570;&#22833;&#22495;&#26631;&#31614;&#24773;&#20917;&#19979;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#24403;&#39118;&#26684;&#20998;&#24067;&#36716;&#31227;&#21644;&#34394;&#20551;&#29305;&#24449;&#21516;&#26102;&#23384;&#22312;&#19988;&#32570;&#22833;&#22495;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#38382;&#39064;&#12290;&#20026;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#26469;&#25429;&#25417;&#39118;&#26684;&#20998;&#24067;&#36716;&#31227;&#21644;&#34394;&#20551;&#29305;&#24449;&#65292;&#36827;&#32780;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;IRSS&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#23545;&#25239;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#29615;&#22659;&#20248;&#21270;&#36880;&#28176;&#20174;&#22270;&#20687;&#20013;&#20998;&#31163;&#39118;&#26684;&#20998;&#24067;&#21644;&#34394;&#20551;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;OOD&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06226v2 Announce Type: replace-cross  Abstract: This paper considers the out-of-distribution (OOD) generalization problem under the setting that both style distribution shift and spurious features exist and domain labels are missing. This setting frequently arises in real-world applications and is underlooked because previous approaches mainly handle either of these two factors. The critical challenge is decoupling style and spurious features in the absence of domain labels. To address this challenge, we first propose a structural causal model (SCM) for the image generation process, which captures both style distribution shift and spurious features. The proposed SCM enables us to design a new framework called IRSS, which can gradually separate style distribution and spurious features from images by introducing adversarial neural networks and multi-environment optimization, thus achieving OOD generalization. Moreover, it does not require additional supervision (e.g., domain l
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#39318;&#20010;&#38754;&#21521;&#20307;&#39564;&#23548;&#33322;&#30340;&#36890;&#29992;&#27169;&#22411;NaviLLM&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#25351;&#23548;&#65292;&#23558;LLMs&#24212;&#29992;&#21040;&#20307;&#39564;&#23548;&#33322;&#20013;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#20219;&#21153;&#30340;&#32479;&#19968;&#22788;&#29702;&#12290;</title><link>https://arxiv.org/abs/2312.02010</link><description>&lt;p&gt;
&#26397;&#21521;&#23398;&#20064;&#36890;&#29992;&#27169;&#22411;&#30340;&#20307;&#39564;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Towards Learning a Generalist Model for Embodied Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02010
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#39318;&#20010;&#38754;&#21521;&#20307;&#39564;&#23548;&#33322;&#30340;&#36890;&#29992;&#27169;&#22411;NaviLLM&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#25351;&#23548;&#65292;&#23558;LLMs&#24212;&#29992;&#21040;&#20307;&#39564;&#23548;&#33322;&#20013;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#20219;&#21153;&#30340;&#32479;&#19968;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#19968;&#20010;&#21487;&#20197;&#19982;&#19990;&#30028;&#20114;&#21160;&#30340;&#36890;&#29992;&#20195;&#29702;&#26159;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#24341;&#20154;&#27880;&#30446;&#30446;&#26631;&#65292;&#22240;&#27492;&#28608;&#21457;&#20102;&#20851;&#20110;&#20307;&#39564;&#23548;&#33322;&#30340;&#30740;&#31350;&#65292;&#20854;&#20013;&#20195;&#29702;&#38656;&#35201;&#26681;&#25454;&#25351;&#31034;&#36827;&#34892;&#23548;&#33322;&#25110;&#22238;&#31572;&#26597;&#35810;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#20027;&#35201;&#36827;&#23637;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#29305;&#23450;&#20219;&#21153;&#30340;&#20195;&#29702;&#19978;&#65292;&#32570;&#20047;&#23545;&#26410;&#35265;&#22330;&#26223;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#24182;&#20026;&#20307;&#39564;&#23548;&#33322;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39318;&#20010;&#38754;&#21521;&#20307;&#39564;&#23548;&#33322;&#30340;&#36890;&#29992;&#27169;&#22411;NaviLLM&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#25351;&#23548;&#26469;&#23558;LLMs&#35843;&#25972;&#21040;&#20307;&#39564;&#23548;&#33322;&#20013;&#12290;&#22522;&#20110;&#27169;&#24335;&#30340;&#25351;&#23548;&#28789;&#27963;&#22320;&#23558;&#21508;&#31181;&#20219;&#21153;&#36716;&#21270;&#20026;&#29983;&#25104;&#38382;&#39064;&#65292;&#20174;&#32780;&#32479;&#19968;&#20102;&#21508;&#31181;&#20219;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#26469;&#33258;&#21508;&#20010;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#28304;&#38598;&#25104;&#21040;&#35757;&#32451;&#20013;&#65292;&#20026;NaviLLM&#37197;&#22791;&#20102;&#24191;&#27867;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02010v3 Announce Type: replace-cross  Abstract: Building a generalist agent that can interact with the world is the intriguing target of AI systems, thus spurring the research for embodied navigation, where an agent is required to navigate according to instructions or respond to queries. Despite the major progress attained, previous works primarily focus on task-specific agents and lack generalizability to unseen scenarios. Recently, LLMs have presented remarkable capabilities across various fields, and provided a promising opportunity for embodied navigation. Drawing on this, we propose the first generalist model for embodied navigation, NaviLLM. It adapts LLMs to embodied navigation by introducing schema-based instruction. The schema-based instruction flexibly casts various tasks into generation problems, thereby unifying a wide range of tasks. This approach allows us to integrate diverse data sources from various datasets into the training, equipping NaviLLM with a wide r
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#24694;&#21155;&#22825;&#27668;&#30340;&#22810;&#27169;&#22359;&#21644;&#27169;&#22359;&#21270;&#31995;&#32479;&#26550;&#26500;&#65292;&#22312;WATonoBus&#24179;&#21488;&#19978;&#36827;&#34892;&#20102;&#23454;&#38469;&#27979;&#35797;&#65292;&#35777;&#26126;&#20854;&#33021;&#22815;&#35299;&#20915;&#20840;&#22825;&#20505;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#38754;&#20020;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2312.00938</link><description>&lt;p&gt;
WATonoBus&#65306;&#19968;&#31181;&#20840;&#22825;&#20505;&#33258;&#21160;&#24033;&#33322;&#36710;
&lt;/p&gt;
&lt;p&gt;
WATonoBus: An All Weather Autonomous Shuttle
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00938
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#24694;&#21155;&#22825;&#27668;&#30340;&#22810;&#27169;&#22359;&#21644;&#27169;&#22359;&#21270;&#31995;&#32479;&#26550;&#26500;&#65292;&#22312;WATonoBus&#24179;&#21488;&#19978;&#36827;&#34892;&#20102;&#23454;&#38469;&#27979;&#35797;&#65292;&#35777;&#26126;&#20854;&#33021;&#22815;&#35299;&#20915;&#20840;&#22825;&#20505;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#38754;&#20020;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#20840;&#22825;&#20505;&#36816;&#34892;&#20013;&#38754;&#20020;&#26174;&#33879;&#25361;&#25112;&#65292;&#28085;&#30422;&#20102;&#20174;&#24863;&#30693;&#21644;&#20915;&#31574;&#21040;&#36335;&#24452;&#35268;&#21010;&#21644;&#25511;&#21046;&#30340;&#21508;&#20010;&#27169;&#22359;&#12290;&#22797;&#26434;&#24615;&#28304;&#20110;&#38656;&#35201;&#35299;&#20915;&#20687;&#38632;&#12289;&#38634;&#21644;&#38654;&#31561;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#22312;&#33258;&#20027;&#24615;&#22534;&#26632;&#20013;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#27169;&#22411;&#21644;&#21333;&#27169;&#22359;&#26041;&#27861;&#36890;&#24120;&#32570;&#20047;&#19982;&#19978;&#28216;&#25110;&#19979;&#28216;&#20219;&#21153;&#30340;&#25972;&#20307;&#38598;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#32771;&#34385;&#24694;&#21155;&#22825;&#27668;&#30340;&#22810;&#27169;&#22359;&#21644;&#27169;&#22359;&#21270;&#31995;&#32479;&#26550;&#26500;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#20174;&#24863;&#30693;&#27700;&#24179;&#21040;&#20915;&#31574;&#21644;&#23433;&#20840;&#30417;&#27979;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#20363;&#22914;&#35206;&#30422;&#38634;&#30340;&#36335;&#32536;&#26816;&#27979;&#12290;&#36890;&#36807;&#22312;WATonoBus&#24179;&#21488;&#19978;&#27599;&#21608;&#26085;&#24120;&#26381;&#21153;&#36817;&#19968;&#24180;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#65292;&#24182;&#20174;&#36816;&#33829;&#20013;&#35266;&#23519;&#21040;&#30340;&#26497;&#31471;&#24773;&#20917;&#20013;&#33719;&#24471;&#23453;&#36149;&#30340;&#32463;&#39564;&#25945;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00938v1 Announce Type: cross  Abstract: Autonomous vehicle all-weather operation poses significant challenges, encompassing modules from perception and decision-making to path planning and control. The complexity arises from the need to address adverse weather conditions like rain, snow, and fog across the autonomy stack. Conventional model-based and single-module approaches often lack holistic integration with upstream or downstream tasks. We tackle this problem by proposing a multi-module and modular system architecture with considerations for adverse weather across the perception level, through features such as snow covered curb detection, to decision-making and safety monitoring. Through daily weekday service on the WATonoBus platform for almost a year, we demonstrate that our proposed approach is capable of addressing adverse weather conditions and provide valuable learning from edge cases observed during operation.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Contrastive Denoising Score (CDS) &#30340;&#31616;&#21333;&#20294;&#24378;&#22823;&#30340;&#20462;&#25913;&#29256;&#26412;&#65292;&#29992;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411; (LDM)&#65292;&#22312;DDS&#26694;&#26550;&#20869;&#20351;&#29992;&#20102;CUT&#25439;&#22833;&#26469;&#26356;&#22909;&#22320;&#20445;&#30041;&#21407;&#22987;&#22270;&#20687;&#30340;&#29305;&#23450;&#32467;&#26500;&#20803;&#32032;&#12290;</title><link>https://arxiv.org/abs/2311.18608</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#21435;&#22122;&#20998;&#25968;&#30340;&#25991;&#26412;&#24341;&#23548;&#28508;&#22312;&#25193;&#25955;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Contrastive Denoising Score for Text-guided Latent Diffusion Image Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18608
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Contrastive Denoising Score (CDS) &#30340;&#31616;&#21333;&#20294;&#24378;&#22823;&#30340;&#20462;&#25913;&#29256;&#26412;&#65292;&#29992;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411; (LDM)&#65292;&#22312;DDS&#26694;&#26550;&#20869;&#20351;&#29992;&#20102;CUT&#25439;&#22833;&#26469;&#26356;&#22909;&#22320;&#20445;&#30041;&#21407;&#22987;&#22270;&#20687;&#30340;&#29305;&#23450;&#32467;&#26500;&#20803;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#26174;&#33879;&#20986;&#29616;&#65292;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#21464;&#24471;&#26356;&#21152;&#22810;&#26679;&#21270;&#24182;&#19981;&#26029;&#21457;&#23637;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#19968;&#31181;&#24456;&#26377;&#21069;&#26223;&#30340;&#26368;&#36817;&#26041;&#27861;&#26159;Delta Denoising Score (DDS) - &#19968;&#31181;&#22522;&#20110;Score Distillation Sampling (SDS)&#26694;&#26550;&#30340;&#22270;&#20687;&#32534;&#36753;&#25216;&#26415;&#65292;&#23427;&#21033;&#29992;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#20016;&#23500;&#29983;&#25104;&#20808;&#39564;&#12290;&#28982;&#32780;&#65292;&#20165;&#20381;&#36182;&#35780;&#20998;&#20989;&#25968;&#20043;&#38388;&#30340;&#24046;&#24322;&#19981;&#36275;&#20197;&#20445;&#30041;&#21407;&#22987;&#22270;&#20687;&#30340;&#29305;&#23450;&#32467;&#26500;&#20803;&#32032;&#65292;&#36825;&#26159;&#22270;&#20687;&#32534;&#36753;&#30340;&#20851;&#38190;&#26041;&#38754;&#20043;&#19968;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; DDS &#30340;&#19968;&#20010;&#23604;&#23596;&#31616;&#21333;&#20294;&#38750;&#24120;&#24378;&#22823;&#30340;&#20462;&#25913;&#29256;&#26412;&#65292;&#31216;&#20026;Contrastive Denoising Score (CDS)&#65292;&#29992;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411; (LDM)&#12290;&#21463;&#21040; DDS &#21644;&#26080;&#37197;&#23545;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#30340;&#23545;&#27604;&#23398;&#20064; (CUT) &#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30452;&#25509;&#30340;&#26041;&#27861;&#65292;&#22312;DDS&#26694;&#26550;&#20869;&#20351;&#29992;CUT&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18608v2 Announce Type: replace-cross  Abstract: With the remarkable advent of text-to-image diffusion models, image editing methods have become more diverse and continue to evolve. A promising recent approach in this realm is Delta Denoising Score (DDS) - an image editing technique based on Score Distillation Sampling (SDS) framework that leverages the rich generative prior of text-to-image diffusion models. However, relying solely on the difference between scoring functions is insufficient for preserving specific structural elements from the original image, a crucial aspect of image editing. To address this, here we present an embarrassingly simple yet very powerful modification of DDS, called Contrastive Denoising Score (CDS), for latent diffusion models (LDM). Inspired by the similarities and differences between DDS and the contrastive learning for unpaired image-to-image translation(CUT), we introduce a straightforward approach using CUT loss within the DDS framework. Ra
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#20026;&#19987;&#19994;C4I&#31995;&#32479;&#35774;&#35745;&#30340;&#30011;&#22270;&#36755;&#20837;&#27861;&#32534;&#36753;&#22120;&#65288;SketchIME&#65289;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#23569;&#26679;&#26412;&#22495;&#33258;&#36866;&#24212;&#21644;&#22686;&#37327;&#23398;&#20064;&#26469;&#25552;&#39640;&#32593;&#32476;&#24615;&#33021;&#21644;&#36866;&#24212;&#26032;&#29992;&#25143;&#12290;</title><link>https://arxiv.org/abs/2311.18254</link><description>&lt;p&gt;
&#30011;&#22270;&#36755;&#20837;&#27861;&#32534;&#36753;&#22120;&#65306;&#31995;&#32479;&#21270;&#36755;&#20837;&#35782;&#21035;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#35770;
&lt;/p&gt;
&lt;p&gt;
Sketch Input Method Editor: A Comprehensive Dataset and Methodology for Systematic Input Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18254
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#20026;&#19987;&#19994;C4I&#31995;&#32479;&#35774;&#35745;&#30340;&#30011;&#22270;&#36755;&#20837;&#27861;&#32534;&#36753;&#22120;&#65288;SketchIME&#65289;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#23569;&#26679;&#26412;&#22495;&#33258;&#36866;&#24212;&#21644;&#22686;&#37327;&#23398;&#20064;&#26469;&#25552;&#39640;&#32593;&#32476;&#24615;&#33021;&#21644;&#36866;&#24212;&#26032;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35302;&#25720;&#23631;&#35774;&#22791;&#30340;&#26085;&#30410;&#26222;&#21450;&#65292;&#33258;&#30001;&#25163;&#32472;&#24050;&#32463;&#25104;&#20026;&#20154;&#26426;&#20132;&#20114;&#20013;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#24335;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#30528;&#30524;&#20110;&#35782;&#21035;&#12289;&#26816;&#32034;&#21644;&#29983;&#25104;&#29087;&#24713;&#30340;&#26085;&#24120;&#29289;&#20307;&#31561;&#20219;&#21153;&#65292;&#20294;&#26412;&#30740;&#31350;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#19987;&#38376;&#20026;&#19987;&#19994;C4I&#31995;&#32479;&#35774;&#35745;&#30340;&#30011;&#22270;&#36755;&#20837;&#27861;&#32534;&#36753;&#22120;&#65288;SketchIME&#65289;&#12290;&#22312;&#35813;&#31995;&#32479;&#20013;&#65292;&#33609;&#22270;&#34987;&#29992;&#20316;&#20302;&#20445;&#30495;&#21407;&#22411;&#65292;&#29992;&#20110;&#25512;&#33616;&#22312;&#21019;&#24314;&#20840;&#38754;&#24773;&#26223;&#22320;&#22270;&#26102;&#30340;&#26631;&#20934;&#21270;&#31526;&#21495;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;374&#31181;&#19987;&#19994;&#33609;&#22270;&#31867;&#22411;&#30340;&#31995;&#32479;&#21270;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#22810;&#32423;&#30417;&#30563;&#30340;&#21516;&#26102;&#35782;&#21035;&#21644;&#20998;&#21106;&#26550;&#26500;&#65292;&#20197;&#25913;&#21892;&#24615;&#33021;&#24182;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#23569;&#26679;&#26412;&#22495;&#33258;&#36866;&#24212;&#21644;&#22686;&#37327;&#23398;&#20064;&#65292;&#32593;&#32476;&#33021;&#22815;&#36866;&#24212;&#26032;&#29992;&#25143;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18254v2 Announce Type: replace-cross  Abstract: With the recent surge in the use of touchscreen devices, free-hand sketching has emerged as a promising modality for human-computer interaction. While previous research has focused on tasks such as recognition, retrieval, and generation of familiar everyday objects, this study aims to create a Sketch Input Method Editor (SketchIME) specifically designed for a professional C4I system. Within this system, sketches are utilized as low-fidelity prototypes for recommending standardized symbols in the creation of comprehensive situation maps. This paper also presents a systematic dataset comprising 374 specialized sketch types, and proposes a simultaneous recognition and segmentation architecture with multilevel supervision between recognition and segmentation to improve performance and enhance interpretability. By incorporating few-shot domain adaptation and class-incremental learning, the network's ability to adapt to new users and
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#24335;&#24605;&#32500;&#25552;&#31034;&#65288;CCoT&#65289;&#65292;&#20197;&#20811;&#26381;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#38590;&#20197;&#25429;&#25417;&#21040;&#32452;&#21512;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#30340;&#32454;&#33410;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.17076</link><description>&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#32452;&#21512;&#24335;&#24605;&#32500;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Compositional Chain-of-Thought Prompting for Large Multimodal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17076
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#24335;&#24605;&#32500;&#25552;&#31034;&#65288;CCoT&#65289;&#65292;&#20197;&#20811;&#26381;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#38590;&#20197;&#25429;&#25417;&#21040;&#32452;&#21512;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#30340;&#32454;&#33410;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#22823;&#30340;&#35270;&#35273;&#39592;&#24178;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLM)&#25512;&#29702;&#30340;&#32467;&#21512;&#24050;&#32463;&#23548;&#33268;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMM)&#25104;&#20026;&#24403;&#21069;&#24191;&#27867;&#35270;&#35273;&#21644;&#35821;&#35328;(VL)&#20219;&#21153;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;LMM&#20173;&#28982;&#38590;&#20197;&#25429;&#25417;&#21040;&#32452;&#21512;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#30340;&#32454;&#33410;&#65292;&#27604;&#22914;&#23545;&#35937;&#20043;&#38388;&#30340;&#23646;&#24615;&#21644;&#20851;&#31995;&#12290;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#26159;&#21033;&#29992;&#22330;&#26223;&#22270;(SGs)&#8212;&#8212;&#23545;&#35937;&#21450;&#20854;&#20851;&#31995;&#21644;&#23646;&#24615;&#30340;&#24418;&#24335;&#21270;&#34920;&#36798;&#65292;&#23427;&#24050;&#34987;&#24191;&#27867;&#29992;&#20316;&#35270;&#35273;&#21644;&#25991;&#26412;&#39046;&#22495;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;&#28982;&#32780;&#65292;&#22330;&#26223;&#22270;&#25968;&#25454;&#38656;&#35201;&#22330;&#26223;&#22270;&#27880;&#37322;&#65292;&#36825;&#31181;&#25968;&#25454;&#25910;&#38598;&#25104;&#26412;&#39640;&#26114;&#65292;&#22240;&#27492;&#38590;&#20197;&#25193;&#23637;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#22330;&#26223;&#22270;&#25968;&#25454;&#24494;&#35843;LMM&#21487;&#33021;&#23548;&#33268;&#39044;&#35757;&#32451;&#30446;&#26631;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#21463;&#21040;&#24605;&#32500;&#38142;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#24335;&#24605;&#32500;&#25552;&#31034;&#65288;CCoT&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17076v2 Announce Type: replace-cross  Abstract: The combination of strong visual backbones and Large Language Model (LLM) reasoning has led to Large Multimodal Models (LMMs) becoming the current standard for a wide range of vision and language (VL) tasks. However, recent research has shown that even the most advanced LMMs still struggle to capture aspects of compositional visual reasoning, such as attributes and relationships between objects. One solution is to utilize scene graphs (SGs)--a formalization of objects and their relations and attributes that has been extensively used as a bridge between the visual and textual domains. Yet, scene graph data requires scene graph annotations, which are expensive to collect and thus not easily scalable. Moreover, finetuning an LMM based on SG data can lead to catastrophic forgetting of the pretraining objective. To overcome this, inspired by chain-of-thought methods, we propose Compositional Chain-of-Thought (CCoT), a novel zero-sho
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#20195;&#29702;&#27169;&#22411;&#24182;&#20351;&#29992;&#36827;&#21270;&#25628;&#32034;&#36807;&#31243;&#65292;&#21457;&#29616;&#20102;&#21487;&#23450;&#21046;&#21040;&#19981;&#21516;&#20301;&#32622;&#30340;&#26377;&#25928;&#22303;&#22320;&#21033;&#29992;&#25919;&#31574;&#65292;&#20026;&#22303;&#22320;&#21033;&#29992;&#35268;&#21010;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2311.12304</link><description>&lt;p&gt;
&#21457;&#29616;&#26377;&#25928;&#30340;&#22303;&#22320;&#21033;&#29992;&#35268;&#21010;&#25919;&#31574;
&lt;/p&gt;
&lt;p&gt;
Discovering Effective Policies for Land-Use Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12304
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#20195;&#29702;&#27169;&#22411;&#24182;&#20351;&#29992;&#36827;&#21270;&#25628;&#32034;&#36807;&#31243;&#65292;&#21457;&#29616;&#20102;&#21487;&#23450;&#21046;&#21040;&#19981;&#21516;&#20301;&#32622;&#30340;&#26377;&#25928;&#22303;&#22320;&#21033;&#29992;&#25919;&#31574;&#65292;&#20026;&#22303;&#22320;&#21033;&#29992;&#35268;&#21010;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22303;&#22320;&#34987;&#20998;&#37197;&#32473;&#19981;&#21516;&#30340;&#29992;&#36884;&#65292;&#22914;&#26862;&#26519;&#12289;&#22478;&#24066;&#21306;&#22495;&#21644;&#20892;&#19994;&#65292;&#23545;&#38470;&#22320;&#30899;&#24179;&#34913;&#21644;&#27668;&#20505;&#21464;&#21270;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;&#22522;&#20110;&#21487;&#29992;&#30340;&#22303;&#22320;&#21033;&#29992;&#21464;&#21270;&#30340;&#21382;&#21490;&#25968;&#25454;&#21644;&#30456;&#20851;&#30340;&#30899;&#25490;&#25918;&#21644;&#21560;&#25910;&#30340;&#27169;&#25311;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#19968;&#20010;&#20195;&#29702;&#27169;&#22411;&#65292;&#20174;&#32780;&#33021;&#22815;&#39640;&#25928;&#35780;&#20272;&#20915;&#31574;&#32773;&#21487;&#36873;&#25321;&#30340;&#19981;&#21516;&#36873;&#39033;&#12290;&#28982;&#21518;&#21487;&#20197;&#20351;&#29992;&#36827;&#21270;&#25628;&#32034;&#36807;&#31243;&#26469;&#21457;&#29616;&#29305;&#23450;&#20301;&#32622;&#30340;&#26377;&#25928;&#22303;&#22320;&#21033;&#29992;&#25919;&#31574;&#12290;&#35813;&#31995;&#32479;&#26500;&#24314;&#22312;Project Resilience&#24179;&#21488;&#19978;&#65292;&#24182;&#20351;&#29992;Land-Use Harmonization&#25968;&#25454;&#38598;LUH2&#21644;&#31807;&#35760;&#27169;&#22411;BLUE&#36827;&#34892;&#35780;&#20272;&#12290;&#23427;&#29983;&#25104;&#21487;&#23450;&#21046;&#21040;&#19981;&#21516;&#20301;&#32622;&#30340;&#30899;&#24433;&#21709;&#21644;&#22303;&#22320;&#21033;&#29992;&#21464;&#21270;&#37327;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#65292;&#20174;&#32780;&#20026;&#22303;&#22320;&#21033;&#29992;&#35268;&#21010;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
How areas of land are allocated for different uses, such as forests, urban areas, and agriculture, has a large effect on the terrestrial carbon balance, and therefore climate change. Based on available historical data on land-use changes and a simulation of the associated carbon emissions and removals, a surrogate model can be learned that makes it possible to evaluate the different options available to decision-makers efficiently. An evolutionary search process can then be used to discover effective land-use policies for specific locations. Such a system was built on the Project Resilience platform and evaluated with the Land-Use Harmonization dataset LUH2 and the bookkeeping model BLUE. It generates Pareto fronts that trade off carbon impact and amount of land-use change customized to different locations, thus providing a potentially useful tool for land-use planning.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;Outcome-supervised Value Model (OVM)&#21033;&#29992;&#32467;&#26524;&#30417;&#30563;&#35757;&#32451;&#20215;&#20540;&#27169;&#22411;&#65292;&#20197;&#22312;&#25968;&#23398;&#25512;&#29702;&#20013;&#20943;&#23569;&#38169;&#35823;&#20256;&#25773;&#65292;&#23558;&#20219;&#21153;&#36716;&#21270;&#20026;&#20215;&#20540;&#20272;&#35745;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.09724</link><description>&lt;p&gt;
OVM&#65292;&#32467;&#26524;&#30417;&#30563;&#20215;&#20540;&#27169;&#22411;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
OVM, Outcome-supervised Value Models for Planning in Mathematical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09724
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;Outcome-supervised Value Model (OVM)&#21033;&#29992;&#32467;&#26524;&#30417;&#30563;&#35757;&#32451;&#20215;&#20540;&#27169;&#22411;&#65292;&#20197;&#22312;&#25968;&#23398;&#25512;&#29702;&#20013;&#20943;&#23569;&#38169;&#35823;&#20256;&#25773;&#65292;&#23558;&#20219;&#21153;&#36716;&#21270;&#20026;&#20215;&#20540;&#20272;&#35745;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32463;&#24120;&#22312;&#22810;&#20010;&#25512;&#29702;&#27493;&#39588;&#20013;&#20445;&#25345;&#20934;&#30830;&#24615;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#23398;&#25512;&#29702;&#20013;&#65292;&#26089;&#26399;&#27493;&#39588;&#20013;&#30340;&#38169;&#35823;&#21487;&#33021;&#20256;&#25773;&#21040;&#21518;&#32493;&#27493;&#39588;&#65292;&#26368;&#32456;&#23548;&#33268;&#38169;&#35823;&#31572;&#26696;&#12290;&#20026;&#20102;&#20943;&#23569;&#38169;&#35823;&#20256;&#25773;&#65292;&#24341;&#20837;&#20102;&#24341;&#23548;&#35299;&#30721;&#20197;&#36880;&#27493;&#25351;&#23548;LM&#35299;&#30721;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#24341;&#23548;&#35299;&#30721;&#20013;&#65292;&#35780;&#20272;&#19981;&#23436;&#25972;&#25512;&#29702;&#36335;&#24452;&#30340;&#28508;&#21147;&#21487;&#33021;&#27604;&#20165;&#30830;&#20445;&#27599;&#20010;&#27493;&#39588;&#30340;&#27491;&#30830;&#24615;&#26356;&#26377;&#20248;&#21183;&#65292;&#22240;&#20026;&#21069;&#19968;&#31181;&#26041;&#27861;&#20250;&#23548;&#21521;&#27491;&#30830;&#30340;&#26368;&#32456;&#31572;&#26696;&#12290;&#36825;&#23558;&#20219;&#21153;&#36716;&#21270;&#20026;&#35745;&#21010;&#20013;&#30340;&#20215;&#20540;&#20272;&#35745;&#38382;&#39064;&#12290;&#21463;&#21040;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#21363;$\textit{&#24341;&#23548;&#35299;&#30721;&#30340;&#32467;&#26524;&#30417;&#30563;&#26412;&#36136;&#19978;&#20805;&#24403;&#20215;&#20540;&#27169;&#22411;}$&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Outcome-supervised Value Model (OVM)&#65292;&#23427;&#37319;&#29992;&#32467;&#26524;&#30417;&#30563;&#26469;&#35757;&#32451;&#20215;&#20540;&#27169;&#22411;&#65292;&#20248;&#20808;&#32771;&#34385;&#23548;&#33268;&#20934;&#30830;&#24615;&#30340;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09724v2 Announce Type: replace  Abstract: Large language models (LLMs) often struggle with maintaining accuracy throughout multiple multiple reasoning steps, especially in mathematical reasoning where an error in earlier steps can propagate to subsequent ones and it ultimately leading to an incorrect answer. To reduce error propagation, guided decoding is employed to direct the LM decoding on a step-by-step basis. We argue that in guided decoding, assessing the potential of an incomplete reasoning path can be more advantageous than simply ensuring per-step correctness, as the former approach leads towards a correct final answer. This transforms the task into a $\textit{value estimation}$ problem in planning.   Inspired by the findings that $\textit{outcome supervision for guided decoding essentially acts as a value model}$, we propose Outcome-supervised Value Model (OVM) that employs outcome supervision for training a value model, which prioritizes steps that lead to accurat
&lt;/p&gt;</description></item><item><title>ARES&#26159;&#29992;&#20110;&#35780;&#20272;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#30340;&#33258;&#21160;&#21270;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#21019;&#24314;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#21644;&#24494;&#35843;&#35780;&#20272;&#22120;&#65292;&#26377;&#25928;&#35780;&#20272;RAG&#31995;&#32479;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2311.09476</link><description>&lt;p&gt;
ARES: &#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#30340;&#33258;&#21160;&#21270;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09476
&lt;/p&gt;
&lt;p&gt;
ARES&#26159;&#29992;&#20110;&#35780;&#20272;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#30340;&#33258;&#21160;&#21270;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#21019;&#24314;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#21644;&#24494;&#35843;&#35780;&#20272;&#22120;&#65292;&#26377;&#25928;&#35780;&#20272;RAG&#31995;&#32479;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#20256;&#32479;&#19978;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#36755;&#20837;&#26597;&#35810;&#12289;&#26816;&#32034;&#27573;&#33853;&#21644;&#29983;&#25104;&#21709;&#24212;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;ARES&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;RAG&#31995;&#32479;&#30340;&#33258;&#21160;&#21270;&#35780;&#20272;&#31995;&#32479;&#65292;&#35780;&#20272;&#32500;&#24230;&#21253;&#25324;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#12289;&#31572;&#26696;&#24544;&#23454;&#24230;&#21644;&#31572;&#26696;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#21019;&#24314;&#33258;&#24049;&#30340;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;ARES&#24494;&#35843;&#36731;&#37327;&#32423;LM&#35780;&#20272;&#22120;&#20197;&#35780;&#20272;&#21333;&#20010;RAG&#32452;&#20214;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#20943;&#23569;&#28508;&#22312;&#30340;&#39044;&#27979;&#38169;&#35823;&#65292;ARES&#21033;&#29992;&#23569;&#37327;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#27979;&#39537;&#21160;&#25512;&#29702;&#65288;PPI&#65289;&#12290;&#22312;KILT&#12289;SuperGLUE&#21644;AIS&#30340;&#20843;&#20010;&#19981;&#21516;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#65292;ARES&#22312;&#35780;&#20272;&#36807;&#31243;&#20013;&#20165;&#20351;&#29992;&#23569;&#37327;&#20154;&#24037;&#27880;&#37322;&#23601;&#20934;&#30830;&#35780;&#20272;RAG&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;ARES&#35780;&#20272;&#22120;&#22312;&#39046;&#22495;&#36716;&#31227;&#20013;&#20173;&#28982;&#26377;&#25928;&#65292;&#21363;&#20351;&#22312;&#26356;&#25913;&#29992;&#20110;&#35780;&#20272;&#30340;&#26597;&#35810;&#21644;/&#25110;&#25991;&#26723;&#31867;&#22411;&#21518;&#20173;&#28982;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09476v2 Announce Type: replace-cross  Abstract: Evaluating retrieval-augmented generation (RAG) systems traditionally relies on hand annotations for input queries, passages to retrieve, and responses to generate. We introduce ARES, an Automated RAG Evaluation System, for evaluating RAG systems along the dimensions of context relevance, answer faithfulness, and answer relevance. By creating its own synthetic training data, ARES finetunes lightweight LM judges to assess the quality of individual RAG components. To mitigate potential prediction errors, ARES utilizes a small set of human-annotated datapoints for prediction-powered inference (PPI). Across eight different knowledge-intensive tasks in KILT, SuperGLUE, and AIS, ARES accurately evaluates RAG systems while using only a few hundred human annotations during evaluation. Furthermore, ARES judges remain effective across domain shifts, proving accurate even after changing the type of queries and/or documents used in the eva
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;AbsPyramid&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#34164;&#28085;&#22270;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#25277;&#35937;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;LLMs&#21487;&#20197;&#36890;&#36807;&#22312;&#20016;&#23500;&#25277;&#35937;&#30693;&#35782;&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#33719;&#24471;&#22522;&#26412;&#30340;&#25277;&#35937;&#33021;&#21147;&#65292;&#24182;&#27867;&#21270;&#21040;&#26410;&#35265;&#30340;&#20107;&#20214;&#12290;</title><link>https://arxiv.org/abs/2311.09174</link><description>&lt;p&gt;
AbsPyramid&#65306;&#20351;&#29992;&#32479;&#19968;&#30340;&#34164;&#28085;&#22270;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#25277;&#35937;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
AbsPyramid: Benchmarking the Abstraction Ability of Language Models with a Unified Entailment Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09174
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;AbsPyramid&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#34164;&#28085;&#22270;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#25277;&#35937;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;LLMs&#21487;&#20197;&#36890;&#36807;&#22312;&#20016;&#23500;&#25277;&#35937;&#30693;&#35782;&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#33719;&#24471;&#22522;&#26412;&#30340;&#25277;&#35937;&#33021;&#21147;&#65292;&#24182;&#27867;&#21270;&#21040;&#26410;&#35265;&#30340;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#30740;&#31350;&#34920;&#26126;&#65292;&#25277;&#35937;&#33021;&#21147;&#23545;&#20154;&#31867;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#36825;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#20173;&#28982;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;AbsPyramid&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;221K&#20010;&#25991;&#26412;&#25551;&#36848;&#30340;&#32479;&#19968;&#34164;&#28085;&#22270;&#65292;&#29992;&#20110;&#25910;&#38598;&#24191;&#27867;&#20107;&#20214;&#19977;&#20010;&#32452;&#25104;&#37096;&#20998;&#30340;&#25277;&#35937;&#30693;&#35782;&#65292;&#20197;&#20840;&#38754;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#24320;&#25918;&#39046;&#22495;&#20013;&#30340;&#25277;&#35937;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30446;&#21069;&#30340;LLMs&#22312;&#38646;&#30701;&#21644;&#23569;&#37327;&#25968;&#25454;&#24773;&#20917;&#19979;&#38754;&#20020;&#29702;&#35299;&#25277;&#35937;&#30693;&#35782;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#22312;&#25105;&#20204;&#20016;&#23500;&#30340;&#25277;&#35937;&#30693;&#35782;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#21487;&#20197;&#33719;&#24471;&#22522;&#26412;&#30340;&#25277;&#35937;&#33021;&#21147;&#65292;&#24182;&#27867;&#21270;&#21040;&#26410;&#35265;&#30340;&#20107;&#20214;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#23454;&#35777;&#34920;&#26126;&#25105;&#20204;&#30340;&#22522;&#20934;&#21487;&#20197;&#20840;&#38754;&#25552;&#39640;LLMs&#22312;&#20004;&#20010;&#20808;&#21069;&#30340;&#25277;&#35937;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09174v3 Announce Type: replace-cross  Abstract: Cognitive research indicates that abstraction ability is essential in human intelligence, which remains under-explored in language models. In this paper, we present AbsPyramid, a unified entailment graph of 221K textual descriptions of abstraction knowledge. While existing resources only touch nouns or verbs within simplified events or specific domains, AbsPyramid collects abstract knowledge for three components of diverse events to comprehensively evaluate the abstraction ability of language models in the open domain. Experimental results demonstrate that current LLMs face challenges comprehending abstraction knowledge in zero-shot and few-shot settings. By training on our rich abstraction knowledge, we find LLMs can acquire basic abstraction abilities and generalize to unseen events. In the meantime, we empirically show that our benchmark is comprehensive to enhance LLMs across two previous abstraction tasks.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32447;&#24615;&#20851;&#31995;&#27010;&#24565;(LRC)&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#28608;&#27963;&#28508;&#22312;&#31354;&#38388;&#20013;&#25214;&#21040;&#19982;&#20154;&#31867;&#21487;&#35299;&#37322;&#27010;&#24565;&#23545;&#24212;&#30340;&#27010;&#24565;&#26041;&#21521;&#65292;&#36825;&#31181;&#25216;&#26415;&#36229;&#36234;&#20102;&#26631;&#20934;&#40657;&#30418;&#25506;&#27979;&#20998;&#31867;&#22120;&#12290;</title><link>https://arxiv.org/abs/2311.08968</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35782;&#21035;&#32447;&#24615;&#20851;&#31995;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Identifying Linear Relational Concepts in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08968
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32447;&#24615;&#20851;&#31995;&#27010;&#24565;(LRC)&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#28608;&#27963;&#28508;&#22312;&#31354;&#38388;&#20013;&#25214;&#21040;&#19982;&#20154;&#31867;&#21487;&#35299;&#37322;&#27010;&#24565;&#23545;&#24212;&#30340;&#27010;&#24565;&#26041;&#21521;&#65292;&#36825;&#31181;&#25216;&#26415;&#36229;&#36234;&#20102;&#26631;&#20934;&#40657;&#30418;&#25506;&#27979;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;(LMs)&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#23558;&#27010;&#24565;&#34920;&#31034;&#20026;&#38544;&#34255;&#28608;&#27963;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20219;&#20309;&#21487;&#30001;&#20154;&#31867;&#35299;&#37322;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#22914;&#20309;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#25214;&#21040;&#20854;&#26041;&#21521;&#21602;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#32447;&#24615;&#20851;&#31995;&#27010;&#24565;(LRC)&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#39318;&#20808;&#24314;&#27169;&#20027;&#20307;&#21644;&#23458;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#20026;&#32447;&#24615;&#20851;&#31995;&#23884;&#20837;(LRE)&#26469;&#25214;&#21040;&#19982;&#20154;&#31867;&#21487;&#35299;&#37322;&#27010;&#24565;&#23545;&#24212;&#30340;&#27010;&#24565;&#26041;&#21521;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21453;&#36716;LRE&#24182;&#20351;&#29992;&#36739;&#26089;&#30340;&#23458;&#20307;&#23618;&#20250;&#23548;&#33268;&#19968;&#31181;&#24378;&#22823;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#25214;&#21040;&#32988;&#36807;&#26631;&#20934;&#40657;&#30418;&#25506;&#27979;&#20998;&#31867;&#22120;&#30340;&#27010;&#24565;&#26041;&#21521;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;LRC&#20316;&#20026;&#27010;&#24565;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#20197;&#21450;&#23427;&#20204;&#25913;&#21464;&#27169;&#22411;&#36755;&#20986;&#30340;&#22240;&#26524;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08968v2 Announce Type: replace-cross  Abstract: Transformer language models (LMs) have been shown to represent concepts as directions in the latent space of hidden activations. However, for any human-interpretable concept, how can we find its direction in the latent space? We present a technique called linear relational concepts (LRC) for finding concept directions corresponding to human-interpretable concepts by first modeling the relation between subject and object as a linear relational embedding (LRE). We find that inverting the LRE and using earlier object layers results in a powerful technique for finding concept directions that outperforms standard black-box probing classifiers. We evaluate LRCs on their performance as concept classifiers as well as their ability to causally change model output.
&lt;/p&gt;</description></item><item><title>Safer-Instruct&#36890;&#36807;&#21453;&#21521;&#25351;&#23548;&#35843;&#25972;&#12289;&#25351;&#23548;&#24863;&#24212;&#21644;&#19987;&#23478;&#27169;&#22411;&#35780;&#20272;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#26500;&#24314;&#22823;&#35268;&#27169;&#20559;&#22909;&#25968;&#25454;&#30340;&#30446;&#30340;&#65292;&#20174;&#32780;&#22312;&#27809;&#26377;&#20154;&#24037;&#26631;&#27880;&#32773;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20559;&#22909;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2311.08685</link><description>&lt;p&gt;
Safer-Instruct: &#20351;&#29992;&#33258;&#21160;&#21270;&#20559;&#22909;&#25968;&#25454;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Safer-Instruct: Aligning Language Models with Automated Preference Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08685
&lt;/p&gt;
&lt;p&gt;
Safer-Instruct&#36890;&#36807;&#21453;&#21521;&#25351;&#23548;&#35843;&#25972;&#12289;&#25351;&#23548;&#24863;&#24212;&#21644;&#19987;&#23478;&#27169;&#22411;&#35780;&#20272;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#26500;&#24314;&#22823;&#35268;&#27169;&#20559;&#22909;&#25968;&#25454;&#30340;&#30446;&#30340;&#65292;&#20174;&#32780;&#22312;&#27809;&#26377;&#20154;&#24037;&#26631;&#27880;&#32773;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20559;&#22909;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#37325;&#35201;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#20026;RLHF&#26631;&#27880;&#20559;&#22909;&#25968;&#25454;&#26159;&#19968;&#39033;&#36164;&#28304;&#23494;&#38598;&#19988;&#38656;&#35201;&#21019;&#36896;&#21147;&#30340;&#36807;&#31243;&#65292;&#32780;&#29616;&#26377;&#30340;&#33258;&#21160;&#29983;&#25104;&#26041;&#27861;&#22312;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Safer-Instruct&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#26500;&#24314;&#22823;&#35268;&#27169;&#20559;&#22909;&#25968;&#25454;&#30340;&#20840;&#26032;&#27969;&#27700;&#32447;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#21453;&#21521;&#25351;&#23548;&#35843;&#25972;&#12289;&#25351;&#23548;&#24863;&#24212;&#21644;&#19987;&#23478;&#27169;&#22411;&#35780;&#20272;&#65292;&#20197;&#39640;&#25928;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20559;&#22909;&#25968;&#25454;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#32773;&#12290;&#20026;&#20102;&#39564;&#35777;Safer-Instruct&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;&#35813;&#27969;&#27700;&#32447;&#24212;&#29992;&#20110;&#26500;&#24314;&#19968;&#20010;&#23433;&#20840;&#20559;&#22909;&#25968;&#25454;&#38598;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#22312;&#36825;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;Alpaca&#27169;&#22411;&#19981;&#20165;&#23637;&#31034;&#20986;&#26356;&#22909;&#30340;&#26080;&#23475;&#24615;&#65292;&#36824;&#34920;&#29616;&#20986;&#20248;&#20110;&#22312;&#20154;&#24037;&#26631;&#27880;&#30340;&#23433;&#20840;&#20559;&#22909;&#25968;&#25454;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08685v2 Announce Type: replace-cross  Abstract: Reinforcement learning from human feedback (RLHF) is a vital strategy for enhancing model capability in language models. However, annotating preference data for RLHF is a resource-intensive and creativity-demanding process, while existing automatic generation methods face limitations in data diversity and quality. In response, we present Safer-Instruct, a novel pipeline for automatically constructing large-scale preference data. Our approach leverages reversed instruction tuning, instruction induction, and expert model evaluation to efficiently generate high-quality preference data without human annotators. To verify the effectiveness of Safer-Instruct, we apply the pipeline to construct a safety preference dataset as a case study. Finetuning an Alpaca model on this synthetic dataset not only demonstrates improved harmlessness but also outperforms models fine-tuned on human-annotated safety preference data, all the while mainta
&lt;/p&gt;</description></item><item><title>LoRA &#22312;&#22810;&#35821;&#35328;&#25688;&#35201;&#26041;&#38754;&#31454;&#20105;&#21147;&#24378;&#65292;&#29305;&#21035;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2311.08572</link><description>&lt;p&gt;
&#20302;&#31209;&#36866;&#24212;&#29992;&#20110;&#22810;&#35821;&#35328;&#25688;&#35201;&#65306;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Low-Rank Adaptation for Multilingual Summarization: An Empirical Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08572
&lt;/p&gt;
&lt;p&gt;
LoRA &#22312;&#22810;&#35821;&#35328;&#25688;&#35201;&#26041;&#38754;&#31454;&#20105;&#21147;&#24378;&#65292;&#29305;&#21035;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#26174;&#33879;&#21152;&#36895;&#20102;&#36817;&#24180;&#26469;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#36827;&#27493;&#65292;&#20294;&#23427;&#20204;&#19981;&#26029;&#22686;&#38271;&#30340;&#20307;&#31215;&#23545;&#20256;&#32479;&#30340;&#24494;&#35843;&#25552;&#20986;&#20102;&#37325;&#35201;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#20869;&#23384;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#28508;&#21147;&#65292;&#37325;&#28857;&#26159;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#65292;&#28041;&#21450;&#22810;&#35821;&#35328;&#25688;&#35201;&#39046;&#22495;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65288;&#22240;&#20026;&#36755;&#20837;&#36890;&#24120;&#24456;&#38271;&#65289;&#65292;&#19988;&#30456;&#23545;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#28085;&#30422;&#19981;&#21516;&#25968;&#25454;&#21487;&#29992;&#24615;&#22330;&#26223;&#65292;&#21253;&#25324;&#39640;&#25968;&#25454;&#21644;&#20302;&#25968;&#25454;&#35774;&#32622;&#65292;&#20197;&#21450;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#21033;&#29992;&#19981;&#21516;&#35268;&#27169;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;&#22823;&#37327;&#25968;&#25454;&#35757;&#32451;&#26102;&#65292;LoRA&#19982;&#23436;&#20840;&#24494;&#35843;&#31454;&#20105;&#28608;&#28872;&#65292;&#24182;&#19988;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#23569;&#25968;&#25454;&#28857;&#36328;&#35821;&#35328;&#36716;&#31227;&#31574;&#30053;&#65292;&#21457;&#29616;&#25345;&#32493;&#30340;LoRA&#35843;&#20248;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08572v2 Announce Type: replace-cross  Abstract: Although the advancements of pre-trained Large Language Models have significantly accelerated recent progress in NLP, their ever-increasing size poses significant challenges for conventional fine-tuning, especially in memory-intensive tasks. We investigate the potential of Parameter-Efficient Fine-Tuning, focusing on Low-Rank Adaptation (LoRA), in the domain of multilingual summarization, a task that is both challenging (due to typically long inputs), and relatively unexplored. We conduct an extensive study across different data availability scenarios, including high- and low-data settings, and cross-lingual transfer, leveraging models of different sizes. Our findings reveal that LoRA is competitive with full fine-tuning when trained with high quantities of data, and excels in low-data scenarios and cross-lingual transfer. We also study different strategies for few-shot cross-lingual transfer, finding that continued LoRA tuning
&lt;/p&gt;</description></item><item><title>&#35780;&#20215;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#37051;&#23621;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25552;&#20986;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#24182;&#21457;&#29616;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#22312;GNN&#39046;&#22495;&#30340;&#35299;&#37322;&#27809;&#26377;&#22826;&#22823;&#24046;&#24322;&#65292;&#21516;&#26102;&#21457;&#29616;&#24456;&#22810;&#25216;&#26415;&#22312;&#27809;&#26377;&#33258;&#29615;&#30340;GNNs&#19979;&#26080;&#27861;&#20934;&#30830;&#35782;&#21035;&#37325;&#35201;&#37051;&#23621;&#12290;</title><link>https://arxiv.org/abs/2311.08118</link><description>&lt;p&gt;
&#35780;&#20215;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#37051;&#23621;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating Neighbor Explainability for Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08118
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20215;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#37051;&#23621;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25552;&#20986;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#24182;&#21457;&#29616;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#22312;GNN&#39046;&#22495;&#30340;&#35299;&#37322;&#27809;&#26377;&#22826;&#22823;&#24046;&#24322;&#65292;&#21516;&#26102;&#21457;&#29616;&#24456;&#22810;&#25216;&#26415;&#22312;&#27809;&#26377;&#33258;&#29615;&#30340;GNNs&#19979;&#26080;&#27861;&#20934;&#30830;&#35782;&#21035;&#37325;&#35201;&#37051;&#23621;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#26159;&#36817;&#24180;&#26469;&#26032;&#20852;&#39046;&#22495;&#12290;&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#22312;&#23545;&#33410;&#28857;&#36827;&#34892;&#20998;&#31867;&#26102;&#65292;&#30830;&#23450;&#27599;&#20010;&#37051;&#23621;&#23545;&#20110; GNN &#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#22914;&#20309;&#34913;&#37327;&#36825;&#19968;&#29305;&#23450;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;&#20026;&#27492;&#65292;&#21508;&#31181;&#24050;&#30693;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#34987;&#37325;&#26032;&#26500;&#36896;&#20197;&#33719;&#21462;&#37051;&#23621;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#22235;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312; GNN &#39046;&#22495;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#25216;&#26415;&#25552;&#20379;&#30340;&#35299;&#37322;&#20960;&#20046;&#27809;&#26377;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#22312;&#20351;&#29992;&#27809;&#26377;&#33258;&#29615;&#30340; GNNs &#26102;&#26410;&#33021;&#35782;&#21035;&#37325;&#35201;&#30340;&#37051;&#23621;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08118v2 Announce Type: replace-cross  Abstract: Explainability in Graph Neural Networks (GNNs) is a new field growing in the last few years. In this publication we address the problem of determining how important is each neighbor for the GNN when classifying a node and how to measure the performance for this specific task. To do this, various known explainability methods are reformulated to get the neighbor importance and four new metrics are presented. Our results show that there is almost no difference between the explanations provided by gradient-based techniques in the GNN domain. In addition, many explainability techniques failed to identify important neighbors when GNNs without self-loops are used.
&lt;/p&gt;</description></item><item><title>&#20013;&#22269;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#35266;&#22865;&#21512;&#24615;&#38656;&#35201;&#26356;&#21152;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#28779;&#28976;"&#65288;Flames&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#24120;&#35265;&#30340;&#26080;&#23475;&#21407;&#21017;&#21644;&#29305;&#23450;&#20013;&#22269;&#20215;&#20540;&#35266;&#65292;&#20197;&#21450;&#22797;&#26434;&#22330;&#26223;&#21644;&#38544;&#21547;&#24694;&#24847;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.06899</link><description>&lt;p&gt;
&#28779;&#28976;: &#35780;&#20272;&#20013;&#22269;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#22865;&#21512;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Flames: Benchmarking Value Alignment of Chinese Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06899
&lt;/p&gt;
&lt;p&gt;
&#20013;&#22269;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#35266;&#22865;&#21512;&#24615;&#38656;&#35201;&#26356;&#21152;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#28779;&#28976;"&#65288;Flames&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#24120;&#35265;&#30340;&#26080;&#23475;&#21407;&#21017;&#21644;&#29305;&#23450;&#20013;&#22269;&#20215;&#20540;&#35266;&#65292;&#20197;&#21450;&#22797;&#26434;&#22330;&#26223;&#21644;&#38544;&#21547;&#24694;&#24847;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#22320;&#21306;&#30340;&#24191;&#27867;&#24212;&#29992;&#24378;&#35843;&#20102;&#35780;&#20272;&#23427;&#20204;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#22865;&#21512;&#24615;&#30340;&#36843;&#20999;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#26410;&#33021;&#26377;&#25928;&#22320;&#25581;&#31034;LLMs&#20013;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;&#23613;&#31649;&#35768;&#22810;&#27169;&#22411;&#22312;&#36825;&#20123;&#35780;&#20272;&#20013;&#24471;&#20998;&#24456;&#39640;&#65292;&#19988;&#8220;&#21517;&#21015;&#21069;&#33541;&#8221;&#65292;&#20294;&#22312;LLMs&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#28145;&#23618;&#22865;&#21512;&#24615;&#21644;&#23454;&#29616;&#30495;&#27491;&#26080;&#23475;&#26041;&#38754;&#20173;&#23384;&#22312;&#37325;&#22823;&#24046;&#36317;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#28779;&#28976;"&#65288;Flames&#65289;&#30340;&#20215;&#20540;&#35266;&#22865;&#21512;&#24615;&#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#27979;&#35797;&#28085;&#30422;&#20102;&#24120;&#35265;&#30340;&#26080;&#23475;&#21407;&#21017;&#65292;&#20197;&#21450;&#19968;&#20010;&#25972;&#21512;&#20102;&#29305;&#23450;&#20013;&#22269;&#20215;&#20540;&#35266;&#22914;&#21644;&#35856;&#30340;&#29420;&#29305;&#36947;&#24503;&#32500;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#21253;&#21547;&#22797;&#26434;&#24773;&#22659;&#21644;&#22823;&#22810;&#24102;&#26377;&#38544;&#21547;&#24694;&#24847;&#30340;&#30772;&#35299;&#26041;&#27861;&#30340;&#23545;&#25239;&#24615;&#25552;&#31034;&#12290;&#36890;&#36807;&#23545;17&#20010;&#20027;&#27969;LLMs&#36827;&#34892;&#25552;&#31034;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#27169;&#22411;&#30340;&#22238;&#24212;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35814;&#32454;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06899v2 Announce Type: replace-cross  Abstract: The widespread adoption of large language models (LLMs) across various regions underscores the urgent need to evaluate their alignment with human values. Current benchmarks, however, fall short of effectively uncovering safety vulnerabilities in LLMs. Despite numerous models achieving high scores and 'topping the chart' in these evaluations, there is still a significant gap in LLMs' deeper alignment with human values and achieving genuine harmlessness. To this end, this paper proposes a value alignment benchmark named Flames, which encompasses both common harmlessness principles and a unique morality dimension that integrates specific Chinese values such as harmony. Accordingly, we carefully design adversarial prompts that incorporate complex scenarios and jailbreaking methods, mostly with implicit malice. By prompting 17 mainstream LLMs, we obtain model responses and rigorously annotate them for detailed evaluation. Our findin
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23433;&#20840;&#24615;&#35780;&#20272;&#20013;&#23384;&#22312;&#20551;&#23545;&#40784;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;Fake alIgNment Evaluation (FINE)&#26694;&#26550;&#21644;&#20004;&#20010;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#39564;&#35777;&#21644;&#20462;&#27491;&#36825;&#19968;&#29616;&#35937;</title><link>https://arxiv.org/abs/2311.05915</link><description>&lt;p&gt;
&#20551;&#23545;&#40784;&#65306;LLMs&#30495;&#30340;&#23545;&#40784;&#24471;&#22909;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Fake Alignment: Are LLMs Really Aligned Well?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.05915
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23433;&#20840;&#24615;&#35780;&#20272;&#20013;&#23384;&#22312;&#20551;&#23545;&#40784;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;Fake alIgNment Evaluation (FINE)&#26694;&#26550;&#21644;&#20004;&#20010;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#39564;&#35777;&#21644;&#20462;&#27491;&#36825;&#19968;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23433;&#20840;&#38382;&#39064;&#30340;&#20851;&#27880;&#26085;&#30410;&#22686;&#24378;&#65292;&#20154;&#20204;&#23545;&#23433;&#20840;&#35780;&#20272;&#20135;&#29983;&#20102;&#30456;&#24403;&#22823;&#30340;&#20852;&#36259;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35780;&#20272;LLMs&#30340;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#38382;&#39064;&#65292;&#21363;&#22810;&#39033;&#36873;&#25321;&#39064;&#21644;&#24320;&#25918;&#24335;&#38382;&#39064;&#20043;&#38388;&#30340;&#34920;&#29616;&#24046;&#24322;&#12290;&#21463;&#21040;&#36234;&#29425;&#25915;&#20987;&#27169;&#24335;&#30740;&#31350;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#30001;&#20110;&#27867;&#21270;&#19981;&#21305;&#37197;&#25152;&#24341;&#36215;&#30340;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;LLMs&#21482;&#35760;&#20303;&#20102;&#24320;&#25918;&#24335;&#23433;&#20840;&#38382;&#39064;&#30340;&#31572;&#26696;&#39118;&#26684;&#65292;&#36825;&#20351;&#20854;&#26080;&#27861;&#35299;&#20915;&#20854;&#20182;&#24418;&#24335;&#30340;&#23433;&#20840;&#27979;&#35797;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#29616;&#35937;&#31216;&#20026;&#20551;&#23545;&#40784;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#27604;&#36739;&#22522;&#20934;&#26469;&#22312;LLMs&#20013;&#32463;&#39564;&#24615;&#22320;&#39564;&#35777;&#20854;&#23384;&#22312;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;Fake alIgNment Evaluation (FINE)&#26694;&#26550;&#21644;&#20004;&#20010;&#26032;&#39062;&#30340;&#24230;&#37327;&#26631;&#20934;--&#19968;&#33268;&#24615;&#20998;&#25968;&#65288;CS&#65289;&#21644;&#19968;&#33268;&#30340;&#23433;&#20840;&#20998;&#25968;&#65288;CSS&#65289;&#65292;&#23427;&#20204;&#20849;&#21516;&#35780;&#20272;&#20004;&#31181;&#20114;&#34917;&#24418;&#24335;&#30340;&#35780;&#20272;&#65292;&#20197;&#37327;&#21270;&#20551;&#23545;&#40784;&#24182;&#33719;&#24471;&#26356;&#27491;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.05915v3 Announce Type: replace-cross  Abstract: The growing awareness of safety concerns in large language models (LLMs) has sparked considerable interest in the evaluation of safety. This study investigates an under-explored issue about the evaluation of LLMs, namely the substantial discrepancy in performance between multiple-choice questions and open-ended questions. Inspired by research on jailbreak attack patterns, we argue this is caused by mismatched generalization. That is, LLM only remembers the answer style for open-ended safety questions, which makes it unable to solve other forms of safety tests. We refer to this phenomenon as fake alignment and construct a comparative benchmark to empirically verify its existence in LLMs. We introduce a Fake alIgNment Evaluation (FINE) framework and two novel metrics--Consistency Score (CS) and Consistent Safety Score (CSS), which jointly assess two complementary forms of evaluation to quantify fake alignment and obtain corrected
&lt;/p&gt;</description></item><item><title>DistillSpec&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#25913;&#36827;&#20102;&#25237;&#26426;&#24615;&#35299;&#30721;&#65292;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;10-45%&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2310.08461</link><description>&lt;p&gt;
DistillSpec&#65306;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#25913;&#36827;&#25237;&#26426;&#24615;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
DistillSpec: Improving Speculative Decoding via Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.08461
&lt;/p&gt;
&lt;p&gt;
DistillSpec&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#25913;&#36827;&#20102;&#25237;&#26426;&#24615;&#35299;&#30721;&#65292;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;10-45%&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25237;&#26426;&#24615;&#35299;&#30721;&#65288;SD&#65289;&#36890;&#36807;&#20351;&#29992;&#26356;&#24555;&#30340;&#33609;&#31295;&#27169;&#22411;&#29983;&#25104;&#22810;&#20010;&#26631;&#35760;&#65292;&#28982;&#21518;&#30001;&#26356;&#22823;&#30340;&#30446;&#26631;&#27169;&#22411;&#24182;&#34892;&#39564;&#35777;&#36825;&#20123;&#26631;&#35760;&#65292;&#20174;&#32780;&#29983;&#25104;&#31526;&#21512;&#30446;&#26631;&#27169;&#22411;&#20998;&#24067;&#30340;&#25991;&#26412;&#65292;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#25214;&#21040;&#19982;&#30446;&#26631;&#27169;&#22411;&#33391;&#22909;&#23545;&#40784;&#30340;&#32039;&#20945;&#33609;&#31295;&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DistillSpec&#65292;&#23427;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#26356;&#22909;&#22320;&#23558;&#33609;&#31295;&#27169;&#22411;&#19982;&#30446;&#26631;&#27169;&#22411;&#23545;&#40784;&#65292;&#28982;&#21518;&#24212;&#29992;SD&#12290;DistillSpec&#20570;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#35774;&#35745;&#36873;&#25321;&#65292;&#25105;&#20204;&#36890;&#36807;&#31995;&#32479;&#30740;&#31350;&#35777;&#26126;&#36825;&#23545;&#25913;&#36827;&#33609;&#31295;&#21644;&#30446;&#26631;&#23545;&#40784;&#33267;&#20851;&#37325;&#35201;&#65306;&#21033;&#29992;&#26469;&#33258;&#33609;&#31295;&#27169;&#22411;&#30340;on-policy&#25968;&#25454;&#29983;&#25104;&#65292;&#20197;&#21450;&#23558;&#21457;&#25955;&#20989;&#25968;&#23450;&#21046;&#21040;&#20219;&#21153;&#21644;&#35299;&#30721;&#31574;&#30053;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;DistillSpec&#22312;&#19968;&#31995;&#21015;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#27604;&#26631;&#20934;SD&#33719;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;10-45%&#30340;&#21152;&#36895;&#65292;&#20351;&#29992;&#36138;&#23146;&#21644;&#38750;&#36138;&#23146;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.08461v2 Announce Type: replace-cross  Abstract: Speculative decoding (SD) accelerates large language model inference by employing a faster draft model for generating multiple tokens, which are then verified in parallel by the larger target model, resulting in the text generated according to the target model distribution. However, identifying a compact draft model that is well-aligned with the target model is challenging. To tackle this issue, we propose DistillSpec that uses knowledge distillation to better align the draft model with the target model, before applying SD. DistillSpec makes two key design choices, which we demonstrate via systematic study to be crucial to improving the draft and target alignment: utilizing on-policy data generation from the draft model, and tailoring the divergence function to the task and decoding strategy. Notably, DistillSpec yields impressive 10 - 45% speedups over standard SD on a range of standard benchmarks, using both greedy and non-gr
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#21517;&#20026;LogiGLUE&#30340;&#22522;&#20934;&#65292;&#20197;&#30740;&#31350;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#12289;&#20219;&#21153;&#21644;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2310.00836</link><description>&lt;p&gt;
&#36808;&#21521;LogiGLUE&#65306;&#23545;&#35821;&#35328;&#27169;&#22411;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#30340;&#31616;&#35201;&#35843;&#26597;&#21644;&#22522;&#20934;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.00836
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#21517;&#20026;LogiGLUE&#30340;&#22522;&#20934;&#65292;&#20197;&#30740;&#31350;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#12289;&#20219;&#21153;&#21644;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36923;&#36753;&#25512;&#29702;&#23545;&#20154;&#31867;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#21364;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#30740;&#31350;&#20154;&#21592;&#26368;&#21021;&#20351;&#29992;&#30340;&#30693;&#35782;&#34920;&#31034;&#19982;&#25512;&#29702;&#31995;&#32479;&#24182;&#19981;&#33021;&#24456;&#22909;&#22320;&#25193;&#23637;&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#25163;&#21160;&#24037;&#20316;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20986;&#29616;&#34920;&#26126;&#20102;&#20854;&#33021;&#22815;&#20811;&#26381;&#24418;&#24335;&#21270;&#30693;&#35782;&#34920;&#31034;&#31995;&#32479;&#30340;&#21508;&#31181;&#23616;&#38480;&#24615;&#12290;&#22240;&#27492;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#30340;LLMs&#22791;&#21463;&#20851;&#27880;&#12290;&#35813;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#23545;&#27492;&#39046;&#22495;&#26368;&#26032;&#36827;&#23637;&#30340;&#31616;&#35201;&#22238;&#39038;&#26469;&#20102;&#35299;LLMs&#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#65307;&#37325;&#28857;&#20851;&#27880;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#12289;&#20219;&#21153;&#20197;&#21450;&#37319;&#29992;&#30340;&#21033;&#29992;LLMs&#36827;&#34892;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#25552;&#20379;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#21517;&#20026;LogiGLUE&#30340;&#22522;&#20934;&#12290;&#20854;&#20013;&#21253;&#25324;24&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#28436;&#32462;&#12289;&#36827;&#38454;&#21644;&#24402;&#32435;&#25512;&#29702;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.00836v3 Announce Type: replace-cross  Abstract: Logical reasoning is fundamental for humans yet presents a substantial challenge in the domain of Artificial Intelligence. Initially, researchers used Knowledge Representation and Reasoning (KR) systems that did not scale and required non-trivial manual effort. Recently, the emergence of large language models (LLMs) has demonstrated the ability to overcome various limitations of formal Knowledge Representation (KR) systems. Consequently, there's a growing interest in using LLMs for logical reasoning via natural language. This work strives to understand the proficiency of LLMs in logical reasoning by offering a brief review of the latest progress in this area; with a focus on the logical reasoning datasets, tasks, and the methods adopted to utilize LLMs for reasoning. To offer a thorough analysis, we have compiled a benchmark titled LogiGLUE. This includes 24 varied datasets encompassing deductive, abductive, and inductive reaso
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#22522;&#20110;&#35270;&#35273;&#30340;&#35270;&#39057;&#38382;&#31572;&#65292;&#21457;&#29616;&#24403;&#21069;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#20570;&#20986;&#21487;&#38752;&#39044;&#27979;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2309.01327</link><description>&lt;p&gt;
&#25105;&#33021;&#30456;&#20449;&#20320;&#30340;&#22238;&#31572;&#21527;&#65311;&#22522;&#20110;&#35270;&#35273;&#30340;&#35270;&#39057;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Can I Trust Your Answer? Visually Grounded Video Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01327
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#22522;&#20110;&#35270;&#35273;&#30340;&#35270;&#39057;&#38382;&#31572;&#65292;&#21457;&#29616;&#24403;&#21069;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#20570;&#20986;&#21487;&#38752;&#39044;&#27979;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#35270;&#35273;&#30340;&#35270;&#39057;&#38382;&#31572;&#65292;&#20197;&#24212;&#23545;&#21033;&#29992;&#39044;&#35757;&#32451;&#25216;&#26415;&#36827;&#34892;&#35270;&#39057;&#35821;&#35328;&#29702;&#35299;&#30340;&#26032;&#36235;&#21183;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#36843;&#20351;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22238;&#31572;&#38382;&#39064;&#24182;&#21516;&#26102;&#25552;&#20379;&#35270;&#35273;&#35777;&#25454;&#65292;&#25105;&#20204;&#26088;&#22312;&#30830;&#23450;&#36825;&#20123;&#25216;&#26415;&#30340;&#39044;&#27979;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#30495;&#27491;&#22522;&#20110;&#30456;&#20851;&#35270;&#39057;&#20869;&#23481;&#65292;&#32780;&#19981;&#26159;&#26469;&#33258;&#35821;&#35328;&#25110;&#26080;&#20851;&#35270;&#35273;&#19978;&#19979;&#25991;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;NExT-GQA--&#19968;&#20010;&#24102;&#26377;10.5K&#26102;&#38388;&#23450;&#20301;&#65288;&#25110;&#20301;&#32622;&#65289;&#26631;&#31614;&#19982;&#21407;&#22987;QA&#23545;&#30456;&#20851;&#32852;&#30340;NExT-QA&#25193;&#23637;&#12290;&#36890;&#36807;NExT-GQA&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;&#19968;&#31995;&#21015;&#26368;&#20808;&#36827;&#30340;VLMs&#12290;&#36890;&#36807;&#20107;&#21518;&#27880;&#24847;&#21147;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#35777;&#23454;&#31572;&#26696;&#26041;&#38754;&#38750;&#24120;&#34180;&#24369;&#65292;&#23613;&#31649;&#23427;&#20204;&#30340;QA&#24615;&#33021;&#24378;&#21170;&#12290;&#36825;&#26292;&#38706;&#20102;&#24403;&#21069;VLM&#22312;&#20570;&#20986;&#21487;&#38752;&#39044;&#27979;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.01327v2 Announce Type: replace-cross  Abstract: We study visually grounded VideoQA in response to the emerging trends of utilizing pretraining techniques for video-language understanding. Specifically, by forcing vision-language models (VLMs) to answer questions and simultaneously provide visual evidence, we seek to ascertain the extent to which the predictions of such techniques are genuinely anchored in relevant video content, versus spurious correlations from language or irrelevant visual context. Towards this, we construct NExT-GQA -- an extension of NExT-QA with 10.5$K$ temporal grounding (or location) labels tied to the original QA pairs. With NExT-GQA, we scrutinize a series of state-of-the-art VLMs. Through post-hoc attention analysis, we find that these models are extremely weak in substantiating the answers despite their strong QA performance. This exposes the limitation of current VLMs in making reliable predictions. As a remedy, we further explore and propose a g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26816;&#39564;&#20102;&#29983;&#25104;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#24615;&#65292;&#21457;&#29616;&#24403;&#21069;&#22522;&#20934;&#20013;&#23384;&#22312;&#21477;&#27861;&#20559;&#35265;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;SyntaxBias Score&#21644;&#26032;&#30340;&#22522;&#20934;SyntActically D&#12290;</title><link>https://arxiv.org/abs/2308.10509</link><description>&lt;p&gt;
&#23545;&#22823;&#35268;&#27169;&#29983;&#25104;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Examination of the Compositionality of Large Generative Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.10509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26816;&#39564;&#20102;&#29983;&#25104;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#24615;&#65292;&#21457;&#29616;&#24403;&#21069;&#22522;&#20934;&#20013;&#23384;&#22312;&#21477;&#27861;&#20559;&#35265;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;SyntaxBias Score&#21644;&#26032;&#30340;&#22522;&#20934;SyntActically D&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25104;&#21151;&#65292;&#35768;&#22810;&#29983;&#25104;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;GVLMs&#65289;&#36890;&#36807;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#25972;&#24471;&#20197;&#26500;&#24314;&#12290;&#28982;&#32780;&#65292;GVLMs&#22312;&#22810;&#27169;&#24577;&#32452;&#21512;&#25512;&#29702;&#20013;&#30340;&#24615;&#33021;&#36824;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#26088;&#22312;&#26816;&#39564;&#35780;&#20272;GVLMs&#32452;&#21512;&#24615;&#30340;&#35780;&#20272;&#25351;&#26631;&#65288;VisualGPTScore&#31561;&#65289;&#21644;&#24403;&#21069;&#22522;&#20934;&#12290;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#22522;&#20934;&#20013;&#30340;&#21477;&#27861;&#20559;&#35265;&#65292;&#34987;GVLMs&#30340;&#35821;&#35328;&#33021;&#21147;&#25152;&#21033;&#29992;&#12290;&#36825;&#31181;&#20559;&#35265;&#20351;&#24471;VisualGPTScore&#25104;&#20026;&#35780;&#20272;GVLMs&#30340;&#19981;&#36275;&#25351;&#26631;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20010;SyntaxBias Score&#65292;&#21033;&#29992;LLMs&#37327;&#21270;&#27492;&#31867;&#20559;&#35265;&#20197;&#36827;&#34892;&#32531;&#35299;&#12290;&#38543;&#21518;&#28155;&#21152;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26032;&#20219;&#21153;&#65292;&#20197;&#35780;&#20272;GVLMs&#23545;&#22266;&#26377;&#20542;&#21521;&#20110;&#21477;&#27861;&#27491;&#30830;&#24615;&#30340;&#20581;&#22766;&#24615;&#12290;&#21033;&#29992;&#32531;&#35299;&#20559;&#35265;&#30340;&#25968;&#25454;&#38598;&#21644;&#26032;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#21363;SyntActically D
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.10509v2 Announce Type: replace-cross  Abstract: With the success of Large Language Models (LLMs), many Generative Vision-Language Models (GVLMs) have been constructed via multimodal instruction tuning. However, the performance of GVLMs in multimodal compositional reasoning remains under-explored. In this paper, we examine both the evaluation metrics (VisualGPTScore, etc.) and current benchmarks for evaluating the compositionality of GVLMs. We identify the syntactical bias in current benchmarks, which is exploited by the linguistic capability of GVLMs. The bias renders VisualGPTScore an insufficient metric for assessing GVLMs. To combat this, we first introduce a SyntaxBias Score, leveraging LLMs to quantify such bias for mitigation. A challenging new task is subsequently added to evaluate the robustness of GVLMs against inherent inclination toward syntactical correctness. Using the bias-mitigated datasets and the new task, we propose a novel benchmark, namely SyntActically D
&lt;/p&gt;</description></item><item><title>&#24341;&#25991;&#34987;&#30830;&#23450;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#38190;&#20294;&#32570;&#22833;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#20854;&#24341;&#20837;&#21487;&#20197;&#22686;&#24378;&#20869;&#23481;&#30340;&#36879;&#26126;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#20197;&#24212;&#23545;&#30693;&#35782;&#20135;&#26435;&#21644;&#20262;&#29702;&#38382;&#39064;&#65292;&#25552;&#35758;LLMs&#30340;&#20840;&#38754;&#24341;&#25991;&#26426;&#21046;&#24212;&#32771;&#34385;&#38750;&#21442;&#25968;&#21270;&#21644;&#21442;&#25968;&#21270;&#20869;&#23481;&#65292;&#23613;&#31649;&#23454;&#26045;&#24341;&#25991;&#26426;&#21046;&#22797;&#26434;&#19988;&#23384;&#22312;&#28508;&#22312;&#32570;&#38519;&#65292;&#20173;&#20027;&#24352;&#25512;&#21160;&#20854;&#21457;&#23637;&#24182;&#27010;&#36848;&#26410;&#26469;&#30740;&#31350;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2307.02185</link><description>&lt;p&gt;
&#24341;&#25991;&#65306;&#26500;&#24314;&#36127;&#36131;&#20219;&#21644;&#21487;&#35745;&#31639;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;
&lt;/p&gt;
&lt;p&gt;
Citation: A Key to Building Responsible and Accountable Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.02185
&lt;/p&gt;
&lt;p&gt;
&#24341;&#25991;&#34987;&#30830;&#23450;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#38190;&#20294;&#32570;&#22833;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#20854;&#24341;&#20837;&#21487;&#20197;&#22686;&#24378;&#20869;&#23481;&#30340;&#36879;&#26126;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#20197;&#24212;&#23545;&#30693;&#35782;&#20135;&#26435;&#21644;&#20262;&#29702;&#38382;&#39064;&#65292;&#25552;&#35758;LLMs&#30340;&#20840;&#38754;&#24341;&#25991;&#26426;&#21046;&#24212;&#32771;&#34385;&#38750;&#21442;&#25968;&#21270;&#21644;&#21442;&#25968;&#21270;&#20869;&#23481;&#65292;&#23613;&#31649;&#23454;&#26045;&#24341;&#25991;&#26426;&#21046;&#22797;&#26434;&#19988;&#23384;&#22312;&#28508;&#22312;&#32570;&#38519;&#65292;&#20173;&#20027;&#24352;&#25512;&#21160;&#20854;&#21457;&#23637;&#24182;&#27010;&#36848;&#26410;&#26469;&#30740;&#31350;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24102;&#26469;&#20102;&#21464;&#38761;&#24615;&#30340;&#22909;&#22788;&#65292;&#21516;&#26102;&#20063;&#24102;&#26469;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#30693;&#35782;&#20135;&#26435;&#65288;IP&#65289;&#21644;&#20262;&#29702;&#20851;&#20999;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#32531;&#35299;&#36825;&#20123;&#39118;&#38505;&#30340;&#26032;&#24605;&#36335;&#65292;&#20174;LLMs&#21644;&#24050;&#24314;&#31435;&#30340;Web&#31995;&#32479;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#20986;&#21457;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#8220;&#24341;&#25991;&#8221; - &#23545;&#26469;&#28304;&#25110;&#35777;&#25454;&#30340;&#25215;&#35748;&#25110;&#24341;&#29992; - &#22312;LLMs&#20013;&#30340;&#20851;&#38190;&#32570;&#22833;&#32452;&#25104;&#37096;&#20998;&#12290;&#24341;&#20837;&#24341;&#25991;&#21487;&#20197;&#22686;&#24378;&#20869;&#23481;&#30340;&#36879;&#26126;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#65292;&#20174;&#32780;&#24212;&#23545;LLMs&#30340;&#30693;&#35782;&#20135;&#26435;&#21644;&#20262;&#29702;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#35758;&#65292;LLMs&#30340;&#20840;&#38754;&#24341;&#25991;&#26426;&#21046;&#24212;&#32771;&#34385;&#38750;&#21442;&#25968;&#21270;&#21644;&#21442;&#25968;&#21270;&#20869;&#23481;&#12290;&#23613;&#31649;&#23454;&#26045;&#36825;&#26679;&#30340;&#24341;&#25991;&#26426;&#21046;&#30340;&#22797;&#26434;&#24615;&#21450;&#28508;&#22312;&#32570;&#38519;&#65292;&#25105;&#20204;&#20027;&#24352;&#25512;&#21160;&#20854;&#21457;&#23637;&#12290;&#22522;&#20110;&#36825;&#19968;&#22522;&#30784;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#35813;&#39046;&#22495;&#20013;&#30340;&#20960;&#20010;&#30740;&#31350;&#38382;&#39064;&#65292;&#26088;&#22312;&#24341;&#23548;&#26410;&#26469;&#30340;&#25506;&#32034;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.02185v3 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) bring transformative benefits alongside unique challenges, including intellectual property (IP) and ethical concerns. This position paper explores a novel angle to mitigate these risks, drawing parallels between LLMs and established web systems. We identify "citation" - the acknowledgement or reference to a source or evidence - as a crucial yet missing component in LLMs. Incorporating citation could enhance content transparency and verifiability, thereby confronting the IP and ethical issues in the deployment of LLMs. We further propose that a comprehensive citation mechanism for LLMs should account for both non-parametric and parametric content. Despite the complexity of implementing such a citation mechanism, along with the potential pitfalls, we advocate for its development. Building on this foundation, we outline several research problems in this area, aiming to guide future explorations towards
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20013;&#38388;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#23558;&#35757;&#32451;&#26500;&#24314;&#20026;&#24369;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#36890;&#36807;&#19981;&#23436;&#25972;&#27880;&#37322;&#25968;&#25454;&#26816;&#27979;&#24694;&#24615;&#20083;&#33146;&#30149;&#21464;&#30340;&#22256;&#22659;</title><link>https://arxiv.org/abs/2301.13418</link><description>&lt;p&gt;
BRAIxDet&#65306;&#23398;&#20064;&#20351;&#29992;&#19981;&#23436;&#25972;&#27880;&#37322;&#26816;&#27979;&#24694;&#24615;&#20083;&#33146;&#30149;&#21464;
&lt;/p&gt;
&lt;p&gt;
BRAIxDet: Learning to Detect Malignant Breast Lesion with Incomplete Annotations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.13418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20013;&#38388;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#23558;&#35757;&#32451;&#26500;&#24314;&#20026;&#24369;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#36890;&#36807;&#19981;&#23436;&#25972;&#27880;&#37322;&#25968;&#25454;&#26816;&#27979;&#24694;&#24615;&#20083;&#33146;&#30149;&#21464;&#30340;&#22256;&#22659;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31579;&#26597;&#20083;&#25151;X&#32447;&#29031;&#29255;&#20013;&#26816;&#27979;&#24694;&#24615;&#30149;&#21464;&#30340;&#26041;&#27861;&#36890;&#24120;&#26159;&#20351;&#29992;&#20855;&#26377;&#23436;&#20840;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;&#22270;&#20687;&#34987;&#26631;&#35760;&#20026;&#30284;&#30151;&#30149;&#21464;&#30340;&#23450;&#20301;&#21644;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#19990;&#30028;&#30340;&#31579;&#26597;&#20083;&#25151;X&#32447;&#29031;&#29255;&#25968;&#25454;&#38598;&#36890;&#24120;&#26377;&#19968;&#20010;&#37096;&#20998;&#26159;&#23436;&#20840;&#27880;&#37322;&#30340;&#65292;&#21478;&#19968;&#20010;&#37096;&#20998;&#21482;&#26377;&#20840;&#23616;&#20998;&#31867;&#30340;&#24369;&#27880;&#37322;&#65288;&#21363;&#27809;&#26377;&#30149;&#21464;&#23450;&#20301;&#65289;&#12290;&#37492;&#20110;&#36825;&#31867;&#25968;&#25454;&#38598;&#30340;&#24222;&#22823;&#35268;&#27169;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#22312;&#24369;&#27880;&#37322;&#23376;&#38598;&#38754;&#20020;&#20004;&#38590;&#36873;&#25321;&#65306;&#35201;&#20040;&#19981;&#20351;&#29992;&#23427;&#65292;&#35201;&#20040;&#23436;&#20840;&#27880;&#37322;&#23427;&#12290;&#31532;&#19968;&#31181;&#36873;&#25321;&#20250;&#38477;&#20302;&#26816;&#27979;&#20934;&#30830;&#24615;&#65292;&#22240;&#20026;&#23427;&#27809;&#26377;&#20351;&#29992;&#25972;&#20010;&#25968;&#25454;&#38598;&#65292;&#32780;&#31532;&#20108;&#31181;&#36873;&#25321;&#21017;&#36807;&#20110;&#26114;&#36149;&#65292;&#22240;&#20026;&#27880;&#37322;&#38656;&#35201;&#19987;&#19994;&#25918;&#23556;&#31185;&#21307;&#24072;&#23436;&#25104;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#19968;&#22256;&#22659;&#30340;&#19968;&#20010;&#20013;&#38388;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#23558;&#35757;&#32451;&#26500;&#24314;&#20026;&#19968;&#20010;&#25105;&#20204;&#31216;&#20043;&#20026;&#24694;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.13418v3 Announce Type: replace-cross  Abstract: Methods to detect malignant lesions from screening mammograms are usually trained with fully annotated datasets, where images are labelled with the localisation and classification of cancerous lesions. However, real-world screening mammogram datasets commonly have a subset that is fully annotated and another subset that is weakly annotated with just the global classification (i.e., without lesion localisation). Given the large size of such datasets, researchers usually face a dilemma with the weakly annotated subset: to not use it or to fully annotate it. The first option will reduce detection accuracy because it does not use the whole dataset, and the second option is too expensive given that the annotation needs to be done by expert radiologists. In this paper, we propose a middle-ground solution for the dilemma, which is to formulate the training as a weakly- and semi-supervised learning problem that we refer to as malignant
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#35757;&#32451;&#30340;&#27169;&#22411;&#20250;&#20135;&#29983;&#31867;&#20284;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#30340;1/f&#22122;&#22768;&#65292;&#20294;&#24403;&#31070;&#32463;&#32593;&#32476;&#36807;&#24230;&#23481;&#37327;&#26102;&#65292;&#28608;&#27963;&#27169;&#24335;&#20250;&#36716;&#21521;&#30333;&#22122;&#22768;&#12290;</title><link>https://arxiv.org/abs/2301.08530</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#33258;&#32452;&#32455;&#20135;&#29983;1/f&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;
Self-Organization Towards $1/f$ Noise in Deep Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.08530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#35757;&#32451;&#30340;&#27169;&#22411;&#20250;&#20135;&#29983;&#31867;&#20284;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#30340;1/f&#22122;&#22768;&#65292;&#20294;&#24403;&#31070;&#32463;&#32593;&#32476;&#36807;&#24230;&#23481;&#37327;&#26102;&#65292;&#28608;&#27963;&#27169;&#24335;&#20250;&#36716;&#21521;&#30333;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
$1/f$&#22122;&#22768;&#65292;&#20063;&#31216;&#20026;&#31881;&#32418;&#22122;&#22768;&#65292;&#26159;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#20013;&#19968;&#20010;&#34987;&#24191;&#27867;&#35748;&#21487;&#30340;&#29616;&#35937;&#65292;&#34987;&#35748;&#20026;&#22312;&#22823;&#33041;&#20449;&#24687;&#22788;&#29702;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#35757;&#32451;&#33258;&#28982;&#35821;&#35328;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20063;&#23384;&#22312;&#31867;&#20284;&#29983;&#29289;&#32593;&#32476;&#30340;1/f&#22122;&#22768;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;'IMDb' AI&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#65292;&#28982;&#21518;&#27979;&#37327;&#20102;&#31070;&#32463;&#20803;&#30340;&#28608;&#27963;&#12290;&#23545;&#19981;&#21516;&#31070;&#32463;&#20803;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#21435;&#36235;&#21183;&#27874;&#21160;&#20998;&#26512;&#65288;DFA&#65289;&#34920;&#26126;&#26126;&#26174;&#30340;1/f&#27169;&#24335;&#65292;&#32780;&#22312;LSTM&#36755;&#20837;&#30340;&#26102;&#38388;&#24207;&#21015;&#20013;&#21364;&#19981;&#23384;&#22312;&#36825;&#31181;&#27169;&#24335;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;&#31070;&#32463;&#32593;&#32476;&#22788;&#20110;&#36807;&#24230;&#23481;&#37327;&#29366;&#24577;&#65292;&#25317;&#26377;&#36275;&#22815;&#22810;&#30340;&#31070;&#32463;&#20803;&#26469;&#23436;&#25104;&#23398;&#20064;&#20219;&#21153;&#26102;&#65292;&#28608;&#27963;&#27169;&#24335;&#23558;&#20559;&#31163;1/f&#22122;&#22768;&#24182;&#36716;&#21521;&#30333;&#22122;&#22768;&#12290;&#36825;&#26159;&#22240;&#20026;&#35768;&#22810;&#31070;&#32463;&#20803;&#26410;&#33021;&#26377;&#25928;&#22320;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.08530v2 Announce Type: replace-cross  Abstract: The presence of $1/f$ noise, also known as pink noise, is a well-established phenomenon in biological neural networks, and is thought to play an important role in information processing in the brain. In this study, we find that such $1/f$ noise is also found in deep neural networks trained on natural language, resembling that of their biological counterparts. Specifically, we trained Long Short-Term Memory (LSTM) networks on the `IMDb' AI benchmark dataset, then measured the neuron activations. The detrended fluctuation analysis (DFA) on the time series of the different neurons demonstrate clear $1/f$ patterns, which is absent in the time series of the inputs to the LSTM. Interestingly, when the neural network is at overcapacity, having more than enough neurons to achieve the learning task, the activation patterns deviate from $1/f$ noise and shifts towards white noise. This is because many of the neurons are not effectively us
&lt;/p&gt;</description></item><item><title>&#22312;&#31163;&#25955;&#21160;&#21147;&#31995;&#32479;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#23547;&#25214;&#20855;&#26377;&#26368;&#23567;&#21463;&#24433;&#21709;&#33410;&#28857;&#25968;&#37327;&#30340;&#31995;&#32479;&#38750;&#24179;&#20961;&#19981;&#21160;&#28857;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20123;&#29305;&#27530;&#24773;&#20917;&#19979;&#21487;&#20197;&#39640;&#25928;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2301.04090</link><description>&lt;p&gt;
&#22312;&#31163;&#25955;&#21160;&#21147;&#31995;&#32479;&#20013;&#25214;&#21040;&#38750;&#24179;&#20961;&#30340;&#26368;&#23567;&#19981;&#21160;&#28857;
&lt;/p&gt;
&lt;p&gt;
Finding Nontrivial Minimum Fixed Points in Discrete Dynamical Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.04090
&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#25955;&#21160;&#21147;&#31995;&#32479;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#23547;&#25214;&#20855;&#26377;&#26368;&#23567;&#21463;&#24433;&#21709;&#33410;&#28857;&#25968;&#37327;&#30340;&#31995;&#32479;&#38750;&#24179;&#20961;&#19981;&#21160;&#28857;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20123;&#29305;&#27530;&#24773;&#20917;&#19979;&#21487;&#20197;&#39640;&#25928;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#31163;&#25955;&#21160;&#21147;&#31995;&#32479;&#24120;&#29992;&#20110;&#27169;&#25311;&#20256;&#26579;&#30149;&#30340;&#20256;&#25773;&#21644;&#20915;&#31574;&#21327;&#35843;&#28216;&#25103;&#20013;&#30340;&#20195;&#29702;&#12290;&#36825;&#31181;&#21160;&#21147;&#31995;&#32479;&#30340;&#19981;&#21160;&#28857;&#20195;&#34920;&#31995;&#32479;&#25910;&#25947;&#21040;&#30340;&#37197;&#32622;&#12290;&#22312;&#20256;&#25773;&#19981;&#33391;&#20256;&#26579;&#30149;&#65288;&#22914;&#35875;&#35328;&#21644;&#38169;&#35823;&#20449;&#24687;&#65289;&#26041;&#38754;&#65292;&#25910;&#25947;&#21040;&#21463;&#24433;&#21709;&#33410;&#28857;&#25968;&#37327;&#36739;&#23569;&#30340;&#19981;&#21160;&#28857;&#26159;&#19968;&#20010;&#20540;&#24471;&#36861;&#27714;&#30340;&#30446;&#26631;&#12290;&#21463;&#36825;&#20123;&#32771;&#34385;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#23547;&#25214;&#20855;&#26377;&#26368;&#23567;&#21463;&#24433;&#21709;&#33410;&#28857;&#25968;&#37327;&#30340;&#31995;&#32479;&#38750;&#24179;&#20961;&#19981;&#21160;&#28857;&#12290;&#25105;&#20204;&#30830;&#23450;&#65292;&#38500;&#38750;P = NP&#65292;&#21542;&#21017;&#27809;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#21487;&#20197;&#22312;&#20219;&#24847;&#23567;&#20110; n^1-\epsilon &#30340;&#22240;&#23376;&#20869;&#36817;&#20284;&#27714;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#31181;&#35745;&#31639;&#19978;&#30340;&#38590;&#39064;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20960;&#31181;&#29305;&#27530;&#24773;&#20917;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.04090v4 Announce Type: replace-cross  Abstract: Networked discrete dynamical systems are often used to model the spread of contagions and decision-making by agents in coordination games. Fixed points of such dynamical systems represent configurations to which the system converges. In the dissemination of undesirable contagions (such as rumors and misinformation), convergence to fixed points with a small number of affected nodes is a desirable goal. Motivated by such considerations, we formulate a novel optimization problem of finding a nontrivial fixed point of the system with the minimum number of affected nodes. We establish that, unless P = NP, there is no polynomial time algorithm for approximating a solution to this problem to within the factor n^1-\epsilon for any constant epsilon &gt; 0. To cope with this computational intractability, we identify several special cases for which the problem can be solved efficiently. Further, we introduce an integer linear program to addr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#21608;&#26399;&#28608;&#27963;&#20989;&#25968;&#30340;&#21551;&#21457;&#21644;&#20351;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;HMMs&#65289;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36827;&#34892;&#21512;&#25104;&#36712;&#36857;&#25968;&#25454;&#22686;&#24378;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;G-PECNet&#22312;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;9.5%&#30340;&#26368;&#32456;&#20301;&#31227;&#35823;&#24046;&#65288;FDE&#65289;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2210.09846</link><description>&lt;p&gt;
&#26397;&#21521;&#36890;&#29992;&#21270;&#30340;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#31995;&#32479;&#8212;&#8212;G-PECNet
&lt;/p&gt;
&lt;p&gt;
G-PECNet: Towards a Generalizable Pedestrian Trajectory Prediction System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.09846
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#21608;&#26399;&#28608;&#27963;&#20989;&#25968;&#30340;&#21551;&#21457;&#21644;&#20351;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;HMMs&#65289;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36827;&#34892;&#21512;&#25104;&#36712;&#36857;&#25968;&#25454;&#22686;&#24378;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;G-PECNet&#22312;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;9.5%&#30340;&#26368;&#32456;&#20301;&#31227;&#35823;&#24046;&#65288;FDE&#65289;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#35299;&#20915;&#20102;&#33258;&#20027;&#26080;&#20154;&#26426;&#23548;&#33322;&#20013;&#30340;&#23376;&#38382;&#39064;&#65292;&#21363;&#39044;&#27979;&#22495;&#22806;&#20154;&#31867;&#21644;&#20195;&#29702;&#20154;&#36712;&#36857;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#8212;&#8212;General-PECNet&#25110;G-PECNet&#65292;&#36890;&#36807;&#21463;&#21608;&#26399;&#28608;&#27963;&#20989;&#25968;&#21551;&#21457;&#30340;&#26550;&#26500;&#25913;&#36827;&#21644;&#20351;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;HMMs&#65289;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36827;&#34892;&#21512;&#25104;&#36712;&#36857;&#25968;&#25454;&#22686;&#24378;&#65292;&#22312;2020&#24180;&#22522;&#20934;PECNet&#19978;&#23558;&#26368;&#32456;&#20301;&#31227;&#35823;&#24046;&#65288;FDE&#65289;&#25552;&#39640;&#20102;9.5&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31616;&#21333;&#20960;&#20309;&#24418;&#29366;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#36712;&#36857;&#38750;&#32447;&#24615;&#21644;&#24322;&#24120;&#20540;&#26816;&#27979;&#65292;&#26377;&#21161;&#20110;&#35813;&#20219;&#21153;&#30340;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.09846v3 Announce Type: replace  Abstract: Navigating dynamic physical environments without obstructing or damaging human assets is of quintessential importance for social robots. In this work, we solve autonomous drone navigation's sub-problem of predicting out-of-domain human and agent trajectories using a deep generative model. Our method: General-PECNet or G-PECNet observes an improvement of 9.5\% on the Final Displacement Error (FDE) on 2020's benchmark: PECNet through a combination of architectural improvements inspired by periodic activation functions and synthetic trajectory (data) augmentations using Hidden Markov Models (HMMs) and Reinforcement Learning (RL). Additionally, we propose a simple geometry-inspired metric for trajectory non-linearity and outlier detection, helpful for the task. Code available at https://github.com/Aryan-Garg/PECNet-Pedestrian-Trajectory-Prediction.git
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#23558;&#21512;&#36866;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#24212;&#29992;&#20110;&#35299;&#20915;&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#20013;&#30340;&#25968;&#25454;&#38598;&#36716;&#31227;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;&#22312;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;EEG&#20449;&#21495;&#20998;&#31867;&#31995;&#32479;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2210.06554</link><description>&lt;p&gt;
&#36816;&#29992;XAI&#26041;&#27861;&#20110;&#22522;&#20110;EEG&#30340;&#31995;&#32479;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Toward the application of XAI methods in EEG-based systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.06554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23558;&#21512;&#36866;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#24212;&#29992;&#20110;&#35299;&#20915;&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#20013;&#30340;&#25968;&#25454;&#38598;&#36716;&#31227;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;&#22312;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;EEG&#20449;&#21495;&#20998;&#31867;&#31995;&#32479;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#25968;&#25454;&#38598;&#36716;&#31227;&#38382;&#39064;&#30340;&#19968;&#20010;&#26377;&#36259;&#26696;&#20363;&#26159;&#22312;&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#32972;&#26223;&#19979;&#23545;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#36827;&#34892;&#20998;&#31867;&#12290; EEG&#20449;&#21495;&#30340;&#38750;&#38745;&#27490;&#24615;&#21487;&#33021;&#23548;&#33268;BCI&#20998;&#31867;&#31995;&#32479;&#22312;&#19981;&#21516;&#20250;&#35805;&#20013;&#20351;&#29992;&#26102;&#24615;&#33021;&#27867;&#21270;&#24046;&#65292;&#29978;&#33267;&#26159;&#21516;&#19968;&#34987;&#35797;&#39564;&#12290; &#26412;&#25991;&#30340;&#20986;&#21457;&#28857;&#26159;&#65292;&#36890;&#36807;&#21033;&#29992;&#21512;&#36866;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#26469;&#23450;&#20301;&#21644;&#36716;&#25442;&#36755;&#20837;&#30340;&#30456;&#20851;&#29305;&#24449;&#65292;&#20174;&#32780;&#32531;&#35299;&#25968;&#25454;&#38598;&#36716;&#31227;&#38382;&#39064;&#12290; &#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#23545;&#20960;&#31181;XAI&#26041;&#27861;&#22312;&#22312;&#20856;&#22411;&#30340;&#29992;&#20110;&#24773;&#32490;&#35782;&#21035;&#30340;EEG&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;ML&#31995;&#32479;&#19978;&#29983;&#25104;&#30340;&#35299;&#37322;&#30340;&#23454;&#39564;&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;XAI&#26041;&#27861;&#25214;&#21040;&#30340;&#35768;&#22810;&#30456;&#20851;&#32452;&#20214;&#22312;&#20250;&#35805;&#20043;&#38388;&#26159;&#20849;&#20139;&#30340;&#65292;&#21487;&#20197;&#29992;&#26469;&#26500;&#24314;&#19968;&#20010;&#20855;&#26377;&#26356;&#22909;&#27867;&#21270;&#33021;&#21147;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.06554v3 Announce Type: replace-cross  Abstract: An interesting case of the well-known Dataset Shift Problem is the classification of Electroencephalogram (EEG) signals in the context of Brain-Computer Interface (BCI). The non-stationarity of EEG signals can lead to poor generalisation performance in BCI classification systems used in different sessions, also from the same subject. In this paper, we start from the hypothesis that the Dataset Shift problem can be alleviated by exploiting suitable eXplainable Artificial Intelligence (XAI) methods to locate and transform the relevant characteristics of the input for the goal of classification. In particular, we focus on an experimental analysis of explanations produced by several XAI methods on an ML system trained on a typical EEG dataset for emotion recognition. Results show that many relevant components found by XAI methods are shared across the sessions and can be used to build a system able to generalise better. However, re
&lt;/p&gt;</description></item><item><title>XAI regulation may be redundant and mandating fully transparent XAI may make firms and consumers worse off, revealing a tradeoff between maximizing welfare and receiving explainable AI outputs.</title><link>https://arxiv.org/abs/2209.03499</link><description>&lt;p&gt;
&#35268;&#33539;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#21487;&#33021;&#20250;&#25439;&#23475;&#28040;&#36153;&#32773;
&lt;/p&gt;
&lt;p&gt;
Regulating eXplainable Artificial Intelligence (XAI) May Harm Consumers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.03499
&lt;/p&gt;
&lt;p&gt;
XAI regulation may be redundant and mandating fully transparent XAI may make firms and consumers worse off, revealing a tradeoff between maximizing welfare and receiving explainable AI outputs.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#26159;&#40657;&#21283;&#23376;&#27169;&#22411;&#65292;&#20854;&#20915;&#31574;&#38590;&#20197;&#35299;&#37322;&#12290;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26159;&#19968;&#31867;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#21521;&#23458;&#25143;&#35299;&#37322;&#20182;&#20204;&#30340;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#26469;&#35299;&#20915;&#32570;&#20047;&#20154;&#24037;&#26234;&#33021;&#21487;&#35299;&#37322;&#24615;&#21644;&#20449;&#20219;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#20010;&#21338;&#24328;&#35770;&#27169;&#22411;&#25361;&#25112;&#20102;&#30417;&#31649;&#20154;&#24037;&#26234;&#33021;&#36890;&#36807;&#35268;&#23450;&#23436;&#20840;&#36879;&#26126;XAI&#20250;&#23548;&#33268;&#26356;&#22823;&#31038;&#20250;&#31119;&#21033;&#30340;&#26222;&#36941;&#35266;&#24565;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;XAI&#35268;&#21046;&#21487;&#33021;&#26159;&#22810;&#20313;&#30340;&#12290;&#20107;&#23454;&#19978;&#65292;&#24378;&#21046;&#35268;&#23450;&#23436;&#20840;&#36879;&#26126;XAI&#21487;&#33021;&#20250;&#20351;&#20844;&#21496;&#21644;&#28040;&#36153;&#32773;&#21464;&#24471;&#26356;&#31967;&#12290;&#36825;&#25581;&#31034;&#20102;&#22312;&#26368;&#22823;&#21270;&#31119;&#21033;&#21644;&#33719;&#24471;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#36755;&#20986;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#22312;&#26041;&#27861;&#21644;&#23454;&#36136;&#33539;&#30068;&#19978;&#25299;&#23637;&#20102;&#29616;&#26377;&#25991;&#29486;&#65292;&#24182;&#24341;&#20837;&#24182;&#30740;&#31350;&#20102;XAI&#20844;&#24179;&#30340;&#27010;&#24565;&#65292;&#21363;&#20351;&#22312;&#24378;&#21046;XAI&#30340;&#24773;&#20917;&#19979;&#20063;&#21487;&#33021;&#26080;&#27861;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.03499v3 Announce Type: replace  Abstract: Recent AI algorithms are black box models whose decisions are difficult to interpret. eXplainable AI (XAI) is a class of methods that seek to address lack of AI interpretability and trust by explaining to customers their AI decisions. The common wisdom is that regulating AI by mandating fully transparent XAI leads to greater social welfare. Our paper challenges this notion through a game theoretic model of a policy-maker who maximizes social welfare, firms in a duopoly competition that maximize profits, and heterogenous consumers. The results show that XAI regulation may be redundant. In fact, mandating fully transparent XAI may make firms and consumers worse off. This reveals a tradeoff between maximizing welfare and receiving explainable AI outputs. We extend the existing literature on method and substantive fronts, and we introduce and study the notion of XAI fairness, which may be impossible to guarantee even under mandatory XAI.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SoftCPT&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#36719;&#19978;&#19979;&#25991;&#20849;&#20139;&#26469;&#32852;&#21512;&#35843;&#25972;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23545;&#22810;&#20010;&#30446;&#26631;&#23569;&#26679;&#26412;&#20219;&#21153;&#36827;&#34892;&#35843;&#25972;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#20219;&#21153;&#20849;&#20139;&#30340;&#20803;&#32593;&#32476;&#26469;&#29983;&#25104;&#25552;&#31034;&#19978;&#19979;&#25991;&#12290;</title><link>https://arxiv.org/abs/2208.13474</link><description>&lt;p&gt;
&#20351;&#29992;&#36719;&#19978;&#19979;&#25991;&#20849;&#20139;&#30340;&#25552;&#31034;&#35843;&#25972;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Prompt Tuning with Soft Context Sharing for Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.13474
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SoftCPT&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#36719;&#19978;&#19979;&#25991;&#20849;&#20139;&#26469;&#32852;&#21512;&#35843;&#25972;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23545;&#22810;&#20010;&#30446;&#26631;&#23569;&#26679;&#26412;&#20219;&#21153;&#36827;&#34892;&#35843;&#25972;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#20219;&#21153;&#20849;&#20139;&#30340;&#20803;&#32593;&#32476;&#26469;&#29983;&#25104;&#25552;&#31034;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#26368;&#36817;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#35768;&#22810;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24040;&#22823;&#28508;&#21147;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19987;&#20026;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#25552;&#31034;&#35843;&#25972;&#30456;&#36739;&#20110;&#32447;&#24615;&#25506;&#27979;&#65288;&#19968;&#31181;&#24378;&#22522;&#20934;&#65289;&#22312;&#23569;&#26679;&#26412;&#22270;&#20687;&#35782;&#21035;&#26041;&#38754;&#21487;&#20197;&#33719;&#24471;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#35768;&#22810;&#23569;&#26679;&#26412;&#20219;&#21153;&#22312;&#26412;&#36136;&#19978;&#26159;&#30456;&#20851;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#19987;&#19994;&#39046;&#22495;&#20869;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#20449;&#24687;&#20808;&#21069;&#34987;&#24573;&#35270;&#20102;&#12290;&#21463;&#21040;&#23558;&#20219;&#21153;&#20851;&#31995;&#24314;&#27169;&#20026;&#22810;&#20219;&#21153;&#23398;&#20064;&#36890;&#24120;&#21487;&#20197;&#25552;&#21319;&#24615;&#33021;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;SoftCPT&#65288;&#29992;&#20110;&#25552;&#31034;&#35843;&#25972;&#30340;&#36719;&#19978;&#19979;&#25991;&#20849;&#20139;&#65289;&#26469;&#32852;&#21512;&#35843;&#25972;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23545;&#22810;&#20010;&#30446;&#26631;&#23569;&#26679;&#26412;&#20219;&#21153;&#36827;&#34892;&#35843;&#25972;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20219;&#21153;&#20849;&#20139;&#30340;&#20803;&#32593;&#32476;&#65292;&#21033;&#29992;&#20219;&#21153;&#21517;&#31216;&#20197;&#21450;&#21487;&#23398;&#20064;&#30340;&#20219;&#21153;&#19978;&#19979;&#25991;&#20316;&#20026;&#36755;&#20837;&#20026;&#27599;&#20010;&#20219;&#21153;&#29983;&#25104;&#25552;&#31034;&#19978;&#19979;&#25991;&#12290;&#36825;&#20010;&#20803;&#32593;&#32476;&#30340;&#21442;&#25968;&#20197;&#21450;&#20219;&#21153;&#19978;&#19979;&#25991;&#34987;&#35843;&#25972;&#20026;
&lt;/p&gt;
&lt;p&gt;
arXiv:2208.13474v2 Announce Type: replace-cross  Abstract: Vision-language models have recently shown great potential on many tasks in computer vision. Meanwhile, prior work demonstrates prompt tuning designed for vision-language models could acquire superior performance on few-shot image recognition compared to linear probe, a strong baseline. In practice, many few-shot tasks are inherently correlated, particularly within specialized domains. However, such information is overlooked previously. Inspired by the fact that modeling task relationship by multi-task learning can usually boost performance, we propose a novel method SoftCPT (Soft Context Sharing for Prompt Tuning) to tune pre-trained vision-language models on multiple target few-shot tasks jointly. Specifically, we design a task-shared meta network to generate prompt context for each task using task name together with a learnable task context as input. The parameters of this meta network as well as the task context are tuned o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;IMA-GloVe-GA&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#22810;&#27493;&#25512;&#29702;&#30340;&#36845;&#20195;&#31070;&#32463;&#25512;&#29702;&#32593;&#32476;&#65292;&#22312;&#36229;&#39046;&#22495;&#27867;&#21270;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2207.14000</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#19978;&#30340;&#22810;&#27493;&#28436;&#32462;&#25512;&#29702;&#65306;&#22522;&#20110;&#36229;&#39046;&#22495;&#27867;&#21270;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.14000
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;IMA-GloVe-GA&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#22810;&#27493;&#25512;&#29702;&#30340;&#36845;&#20195;&#31070;&#32463;&#25512;&#29702;&#32593;&#32476;&#65292;&#22312;&#36229;&#39046;&#22495;&#27867;&#21270;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#28145;&#24230;&#23398;&#20064;&#19982;&#31526;&#21495;&#36923;&#36753;&#25512;&#29702;&#32467;&#21512;&#36215;&#26469;&#65292;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#25104;&#21151;&#65292;&#24182;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#21463;DeepLogic&#21551;&#21457;&#65292;&#35813;&#27169;&#22411;&#32463;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#29992;&#20110;&#25191;&#34892;&#36923;&#36753;&#31243;&#24207;&#25512;&#29702;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;IMA-GloVe-GA&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#22810;&#27493;&#25512;&#29702;&#30340;&#36845;&#20195;&#31070;&#32463;&#25512;&#29702;&#32593;&#32476;&#12290;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;&#25512;&#29702;&#26159;&#20351;&#29992;&#22522;&#20110;RNN&#30340;&#36845;&#20195;&#20869;&#23384;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#30340;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#38376;&#20851;&#27880;&#26426;&#21046;&#12290;&#25105;&#20204;&#22312;PARARULES&#12289;CONCEPTRULES V1&#21644;CONCEPTRULES V2&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;IMA-GloVe-GA&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24102;&#26377;&#38376;&#20851;&#27880;&#26426;&#21046;&#30340;DeepLogic&#27604;DeepLogic&#21644;&#20854;&#20182;RNN&#22522;&#32447;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#26356;&#39640;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35268;&#21017;&#34987;&#25171;&#20081;&#26102;&#27604;RoBERTa-Large&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36229;&#39046;&#22495;&#27867;&#21270;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#24403;&#21069;&#22810;&#27493;&#25512;&#29702;&#25968;&#25454;&#38598;&#20013;&#25512;&#29702;&#28145;&#24230;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2207.14000v2 Announce Type: replace-cross  Abstract: Combining deep learning with symbolic logic reasoning aims to capitalize on the success of both fields and is drawing increasing attention. Inspired by DeepLogic, an end-to-end model trained to perform inference on logic programs, we introduce IMA-GloVe-GA, an iterative neural inference network for multi-step reasoning expressed in natural language. In our model, reasoning is performed using an iterative memory neural network based on RNN with a gate attention mechanism. We evaluate IMA-GloVe-GA on three datasets: PARARULES, CONCEPTRULES V1 and CONCEPTRULES V2. Experimental results show DeepLogic with gate attention can achieve higher test accuracy than DeepLogic and other RNN baseline models. Our model achieves better out-of-distribution generalisation than RoBERTa-Large when the rules have been shuffled. Furthermore, to address the issue of unbalanced distribution of reasoning depths in the current multi-step reasoning datase
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#39044;&#27979;&#32858;&#31867;&#26641;&#30340;&#65288;&#20998;&#23618;&#65289;&#22810;&#26631;&#31614;&#20998;&#31867;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2207.09237</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#39044;&#27979;&#32858;&#31867;&#26641;&#29992;&#20110;&#65288;&#20998;&#23618;&#65289;&#22810;&#26631;&#31614;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised Predictive Clustering Trees for (Hierarchical) Multi-label Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.09237
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#39044;&#27979;&#32858;&#31867;&#26641;&#30340;&#65288;&#20998;&#23618;&#65289;&#22810;&#26631;&#31614;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#19981;&#20165;&#20351;&#29992;&#26631;&#35760;&#30340;&#31034;&#20363;&#65292;&#36824;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#31034;&#20363;&#12290;&#23613;&#31649;SSL&#22312;&#31616;&#21333;&#30340;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#19978;&#24471;&#21040;&#20102;&#30740;&#31350;&#30028;&#30340;&#20851;&#27880;&#65292;&#20294;&#23545;&#20110;&#20855;&#26377;&#32467;&#26500;&#30456;&#20851;&#21464;&#37327;&#30340;&#22797;&#26434;&#39044;&#27979;&#20219;&#21153;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#35843;&#26597;&#12290;&#36825;&#31181;&#24773;&#20917;&#20986;&#29616;&#22312;&#22810;&#26631;&#31614;&#20998;&#31867;&#21644;&#20998;&#23618;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#36825;&#21487;&#33021;&#38656;&#35201;&#39069;&#22806;&#30340;&#20449;&#24687;&#65292;&#21487;&#33021;&#26469;&#33258;&#25551;&#36848;&#31354;&#38388;&#20013;&#30001;&#26410;&#26631;&#35760;&#31034;&#20363;&#25552;&#20379;&#30340;&#22522;&#30784;&#20998;&#24067;&#65292;&#20197;&#26356;&#22909;&#22320;&#38754;&#23545;&#25361;&#25112;&#24615;&#30340;&#21516;&#26102;&#39044;&#27979;&#22810;&#20010;&#31867;&#26631;&#31614;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#19968;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#39044;&#27979;&#32858;&#31867;&#26641;&#30340;&#65288;&#20998;&#23618;&#65289;&#22810;&#26631;&#31614;&#20998;&#31867;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#23558;&#35813;&#26041;&#27861;&#25193;&#23637;&#21040;&#38598;&#25104;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;
&lt;/p&gt;
&lt;p&gt;
arXiv:2207.09237v2 Announce Type: replace-cross  Abstract: Semi-supervised learning (SSL) is a common approach to learning predictive models using not only labeled examples, but also unlabeled examples. While SSL for the simple tasks of classification and regression has received a lot of attention from the research community, this is not properly investigated for complex prediction tasks with structurally dependent variables. This is the case of multi-label classification and hierarchical multi-label classification tasks, which may require additional information, possibly coming from the underlying distribution in the descriptive space provided by unlabeled examples, to better face the challenging task of predicting simultaneously multiple class labels.   In this paper, we investigate this aspect and propose a (hierarchical) multi-label classification method based on semi-supervised learning of predictive clustering trees. We also extend the method towards ensemble learning and propose
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Embed to Control&#65288;ETC&#65289;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#20004;&#20010;&#32423;&#21035;&#23398;&#20064;&#34920;&#31034;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#37096;&#20998;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#26679;&#26412;&#39640;&#25928;&#21033;&#29992;&#12290;</title><link>https://arxiv.org/abs/2205.13476</link><description>&lt;p&gt;
&#23884;&#20837;&#25511;&#21046;&#37096;&#20998;&#35266;&#23519;&#31995;&#32479;&#65306;&#20855;&#26377;&#21487;&#35777;&#26126;&#26679;&#26412;&#25928;&#29575;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Embed to Control Partially Observed Systems: Representation Learning with Provable Sample Efficiency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.13476
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Embed to Control&#65288;ETC&#65289;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#20004;&#20010;&#32423;&#21035;&#23398;&#20064;&#34920;&#31034;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#37096;&#20998;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#26679;&#26412;&#39640;&#25928;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#37096;&#20998;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDPs&#65289;&#20013;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#12290;&#19968;&#26159;&#36890;&#24120;&#38656;&#35201;&#20840;&#37096;&#21382;&#21490;&#35760;&#24405;&#26469;&#39044;&#27979;&#26410;&#26469;&#65292;&#36825;&#23548;&#33268;&#26679;&#26412;&#22797;&#26434;&#24230;&#38543;&#30528;&#26102;&#38388;&#36328;&#24230;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#20108;&#26159;&#35266;&#27979;&#21644;&#29366;&#24577;&#31354;&#38388;&#36890;&#24120;&#26159;&#36830;&#32493;&#30340;&#65292;&#36825;&#23548;&#33268;&#26679;&#26412;&#22797;&#26434;&#24230;&#38543;&#22806;&#22312;&#32500;&#25968;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#38656;&#35201;&#36890;&#36807;&#21033;&#29992;POMDP&#30340;&#32467;&#26500;&#23398;&#20064;&#35266;&#27979;&#21644;&#29366;&#24577;&#21382;&#21490;&#30340;&#26368;&#23567;&#20294;&#36275;&#22815;&#30340;&#34920;&#31034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Embed to Control (ETC)&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#20248;&#21270;&#31574;&#30053;&#30340;&#21516;&#26102;&#23398;&#20064;&#20004;&#20010;&#32423;&#21035;&#30340;&#34920;&#31034;&#12290;(i)&#22312;&#27599;&#19968;&#27493;&#65292;ETC&#23398;&#20064;&#29992;&#20302;&#32500;&#29305;&#24449;&#34920;&#31034;&#29366;&#24577;&#65292;&#36825;&#23545;&#36716;&#31227;&#26680;&#36827;&#34892;&#22240;&#23376;&#20998;&#35299;&#12290;(ii)&#22312;&#22810;&#20010;&#27493;&#39588;&#20013;&#65292;ETC&#23398;&#20064;&#29992;&#20302;&#32500;&#34920;&#31034;&#23436;&#25972;&#21382;&#21490;&#35760;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2205.13476v2 Announce Type: replace-cross  Abstract: Reinforcement learning in partially observed Markov decision processes (POMDPs) faces two challenges. (i) It often takes the full history to predict the future, which induces a sample complexity that scales exponentially with the horizon. (ii) The observation and state spaces are often continuous, which induces a sample complexity that scales exponentially with the extrinsic dimension. Addressing such challenges requires learning a minimal but sufficient representation of the observation and state histories by exploiting the structure of the POMDP.   To this end, we propose a reinforcement learning algorithm named Embed to Control (ETC), which learns the representation at two levels while optimizing the policy.~(i) For each step, ETC learns to represent the state with a low-dimensional feature, which factorizes the transition kernel. (ii) Across multiple steps, ETC learns to represent the full history with a low-dimensional emb
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#40657;&#30418;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;MEKD&#65292;&#36890;&#36807;&#23558;&#25945;&#24072;&#21644;&#23398;&#29983;&#27169;&#22411;&#30340;&#20302;&#32500;&#24230;&#23545;&#25968;&#23545;&#40784;&#65292;&#23454;&#29616;&#23558;&#19968;&#20010;&#32321;&#29712;&#27169;&#22411;&#21387;&#32553;&#25104;&#36731;&#37327;&#32423;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2205.10490</link><description>&lt;p&gt;
&#23558;&#29983;&#25104;&#30340;&#23545;&#25968;&#36827;&#34892;&#20934;&#21017;&#23545;&#40784;&#30340;&#40657;&#30418;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Aligning Logits Generatively for Principled Black-Box Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.10490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#40657;&#30418;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;MEKD&#65292;&#36890;&#36807;&#23558;&#25945;&#24072;&#21644;&#23398;&#29983;&#27169;&#22411;&#30340;&#20302;&#32500;&#24230;&#23545;&#25968;&#23545;&#40784;&#65292;&#23454;&#29616;&#23558;&#19968;&#20010;&#32321;&#29712;&#27169;&#22411;&#21387;&#32553;&#25104;&#36731;&#37327;&#32423;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#30418;&#30693;&#35782;&#33976;&#39311;&#65288;B2KD&#65289;&#26159;&#19968;&#20010;&#22788;&#29702;&#20113;&#31471;&#21040;&#36793;&#32536;&#27169;&#22411;&#21387;&#32553;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#25968;&#25454;&#21644;&#27169;&#22411;&#25176;&#31649;&#22312;&#26381;&#21153;&#22120;&#19978;&#19988;&#26080;&#27861;&#30475;&#35265;&#12290;B2KD&#38754;&#20020;&#30340;&#25361;&#25112;&#21253;&#25324;&#20114;&#32852;&#32593;&#20132;&#25442;&#21463;&#38480;&#21644;&#25968;&#25454;&#20998;&#24067;&#22312;&#36793;&#32536;&#21644;&#20113;&#31471;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#25324;&#21435;&#38544;&#21435;&#21644;&#33976;&#39311;&#20004;&#27493;&#24037;&#20316;&#27969;&#31243;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#25552;&#20379;&#20102;&#19968;&#20010;&#20174;&#23545;&#25968;&#21040;&#21333;&#20803;&#36793;&#30028;&#30340;&#26032;&#20248;&#21270;&#26041;&#21521;&#65292;&#19981;&#21516;&#20110;&#30452;&#25509;&#23545;&#25968;&#23545;&#40784;&#12290;&#22312;&#20854;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;Mapping-Emulation KD&#65288;MEKD&#65289;&#65292;&#23558;&#19968;&#20010;&#40657;&#30418;&#32321;&#29712;&#27169;&#22411;&#33976;&#39311;&#25104;&#19968;&#20010;&#36731;&#37327;&#32423;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#21306;&#20998;&#36719;&#25110;&#30828;&#21709;&#24212;&#22788;&#29702;&#65292;&#24182;&#21253;&#25324;&#65306;1&#65289;&#21435;&#38544;&#21435;&#65306;&#36890;&#36807;&#29983;&#25104;&#22120;&#27169;&#25311;&#25945;&#24072;&#20989;&#25968;&#30340;&#36870;&#26144;&#23556;&#65292;&#21644;2&#65289;&#33976;&#39311;&#65306;&#36890;&#36807;&#20943;&#23567;&#39640;&#32500;&#22270;&#20687;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#23545;&#40784;&#25945;&#24072;&#27169;&#22411;&#21644;&#23398;&#29983;&#27169;&#22411;&#30340;&#20302;&#32500;&#24230;&#23545;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2205.10490v2 Announce Type: replace-cross  Abstract: Black-Box Knowledge Distillation (B2KD) is a formulated problem for cloud-to-edge model compression with invisible data and models hosted on the server. B2KD faces challenges such as limited Internet exchange and edge-cloud disparity of data distributions. In this paper, we formalize a two-step workflow consisting of deprivatization and distillation, and theoretically provide a new optimization direction from logits to cell boundary different from direct logits alignment. With its guidance, we propose a new method Mapping-Emulation KD (MEKD) that distills a black-box cumbersome model into a lightweight one. Our method does not differentiate between treating soft or hard responses, and consists of: 1) deprivatization: emulating the inverse mapping of the teacher function with a generator, and 2) distillation: aligning low-dimensional logits of the teacher and student models by reducing the distance of high-dimensional image poin
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#22270;&#32467;&#26500;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#35745;&#31639;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#26694;&#26550;&#65292;&#22522;&#20110;&#23616;&#37096;&#20540;&#20989;&#25968;&#30340;&#20998;&#24067;&#24335;RL&#26041;&#27861;&#22312;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#38477;&#20302;&#12290;</title><link>https://arxiv.org/abs/2202.13046</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#35825;&#23548;&#30340;&#23616;&#37096;&#20540;&#20989;&#25968;&#30340;&#20998;&#24067;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distributed Multi-Agent Reinforcement Learning Based on Graph-Induced Local Value Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2202.13046
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22270;&#32467;&#26500;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#35745;&#31639;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#26694;&#26550;&#65292;&#22522;&#20110;&#23616;&#37096;&#20540;&#20989;&#25968;&#30340;&#20998;&#24067;&#24335;RL&#26041;&#27861;&#22312;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#22823;&#35268;&#27169;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;(RL)&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#65306;(i)&#27599;&#20010;&#26234;&#33021;&#20307;&#21482;&#33021;&#35775;&#38382;&#26377;&#38480;&#30340;&#20449;&#24687;&#65307;(ii)&#30001;&#20110;&#32500;&#24230;&#35781;&#21650;&#65292;&#20250;&#20986;&#29616;&#25910;&#25947;&#25110;&#35745;&#31639;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#35745;&#31639;&#39640;&#25928;&#30340;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#20998;&#24067;&#24335;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#35813;&#38382;&#39064;&#20013;&#28041;&#21450;&#30340;&#22270;&#32467;&#26500;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#25551;&#36848;MARL&#20013;&#19977;&#31181;&#31867;&#22411;&#26234;&#33021;&#20307;&#32806;&#21512;&#30340;&#19977;&#20010;&#32806;&#21512;&#22270;&#65292;&#20998;&#21035;&#26159;&#29366;&#24577;&#22270;&#12289;&#35266;&#27979;&#22270;&#21644;&#22870;&#21169;&#22270;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#32771;&#34385;&#36890;&#20449;&#22270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#32806;&#21512;&#22270;&#20013;&#27966;&#29983;&#30340;&#23616;&#37096;&#20540;&#20989;&#25968;&#30340;&#20998;&#24067;&#24335;RL&#26041;&#27861;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#22312;&#21069;&#36848;&#22235;&#20010;&#22270;&#19978;&#30340;&#29305;&#23450;&#26465;&#20214;&#19979;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#31532;&#20108;&#31181;
&lt;/p&gt;
&lt;p&gt;
arXiv:2202.13046v4 Announce Type: replace-cross  Abstract: Achieving distributed reinforcement learning (RL) for large-scale cooperative multi-agent systems (MASs) is challenging because: (i) each agent has access to only limited information; (ii) issues on convergence or computational complexity emerge due to the curse of dimensionality. In this paper, we propose a general computationally efficient distributed framework for cooperative multi-agent reinforcement learning (MARL) by utilizing the structures of graphs involved in this problem. We introduce three coupling graphs describing three types of inter-agent couplings in MARL, namely, the state graph, the observation graph and the reward graph. By further considering a communication graph, we propose two distributed RL approaches based on local value-functions derived from the coupling graphs. The first approach is able to reduce sample complexity significantly under specific conditions on the aforementioned four graphs. The second
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#21069;&#30651;&#26799;&#24230;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#19982;&#36229;&#35843;&#20840;&#23616;&#27169;&#22411;&#23545;&#40784;&#26469;&#35268;&#33539;&#26412;&#22320;&#26356;&#26032;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#25913;&#21892;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#20419;&#36827;&#26381;&#21153;&#22120;&#27169;&#22411;&#30340;&#25910;&#25947;&#12290;</title><link>https://arxiv.org/abs/2201.03172</link><description>&lt;p&gt;
&#20855;&#26377;&#21152;&#36895;&#23458;&#25143;&#26799;&#24230;&#30340;&#36890;&#20449;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Communication-Efficient Federated Learning with Accelerated Client Gradient
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2201.03172
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#21069;&#30651;&#26799;&#24230;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#19982;&#36229;&#35843;&#20840;&#23616;&#27169;&#22411;&#23545;&#40784;&#26469;&#35268;&#33539;&#26412;&#22320;&#26356;&#26032;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#25913;&#21892;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#20419;&#36827;&#26381;&#21153;&#22120;&#27169;&#22411;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#24120;&#24120;&#22240;&#21442;&#19982;&#23458;&#25143;&#25968;&#25454;&#38598;&#30340;&#24322;&#36136;&#24615;&#29305;&#24449;&#32780;&#23548;&#33268;&#25910;&#25947;&#32531;&#24930;&#19981;&#31283;&#23450;&#12290;&#24403;&#23458;&#25143;&#21442;&#19982;&#29575;&#36739;&#20302;&#26102;&#65292;&#27492;&#36235;&#21183;&#20250;&#21152;&#21095;&#65292;&#22240;&#20026;&#20174;&#23458;&#25143;&#25910;&#38598;&#30340;&#20449;&#24687;&#20855;&#26377;&#36739;&#22823;&#30340;&#21464;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#25913;&#21892;&#20102;&#23458;&#25143;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#20419;&#36827;&#20102;&#26381;&#21153;&#22120;&#27169;&#22411;&#30340;&#25910;&#25947;&#12290;&#36890;&#36807;&#20351;&#26381;&#21153;&#22120;&#24191;&#25773;&#20855;&#26377;&#21069;&#30651;&#26799;&#24230;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#35813;&#31574;&#30053;&#65292;&#20351;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#21521;&#21442;&#19982;&#32773;&#20256;&#36798;&#25237;&#24433;&#30340;&#20840;&#23616;&#26356;&#26032;&#20449;&#24687;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#23458;&#25143;&#20869;&#23384;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23558;&#27599;&#20010;&#23458;&#25143;&#31471;&#19982;&#36229;&#35843;&#20840;&#23616;&#27169;&#22411;&#23545;&#40784;&#26469;&#35268;&#33539;&#26412;&#22320;&#26356;&#26032;&#65292;&#20197;&#20943;&#23569;&#20559;&#24046;&#24182;&#25913;&#21892;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#25910;&#25947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2201.03172v2 Announce Type: replace-cross  Abstract: Federated learning often suffers from slow and unstable convergence due to the heterogeneous characteristics of participating client datasets. Such a tendency is aggravated when the client participation ratio is low since the information collected from the clients has large variations. To address this challenge, we propose a simple but effective federated learning framework, which improves the consistency across clients and facilitates the convergence of the server model. This is achieved by making the server broadcast a global model with a lookahead gradient. This strategy enables the proposed approach to convey the projected global update information to participants effectively without additional client memory and extra communication costs. We also regularize local updates by aligning each client with the overshot global model to reduce bias and improve the stability of our algorithm. We provide the theoretical convergence ra
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Collaborative Graph Contrastive Learning&#26694;&#26550;&#65288;CGCL&#65289;&#65292;&#21033;&#29992;&#22810;&#20010;&#22270;&#32534;&#30721;&#22120;&#35266;&#23519;&#22270;&#24418;&#65292;&#24182;&#36991;&#20813;&#24341;&#20837;&#19981;&#31283;&#23450;&#25200;&#21160;&#65292;&#20445;&#35777;&#22270;&#30340;&#19981;&#21464;&#24615;&#12290;</title><link>https://arxiv.org/abs/2111.03262</link><description>&lt;p&gt;
CGCL&#65306;&#26080;&#38656;&#25163;&#24037;&#22270;&#25968;&#25454;&#22686;&#24378;&#30340;&#21327;&#20316;&#24335;&#22270;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CGCL: Collaborative Graph Contrastive Learning without Handcrafted Graph Data Augmentations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2111.03262
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Collaborative Graph Contrastive Learning&#26694;&#26550;&#65288;CGCL&#65289;&#65292;&#21033;&#29992;&#22810;&#20010;&#22270;&#32534;&#30721;&#22120;&#35266;&#23519;&#22270;&#24418;&#65292;&#24182;&#36991;&#20813;&#24341;&#20837;&#19981;&#31283;&#23450;&#25200;&#21160;&#65292;&#20445;&#35777;&#22270;&#30340;&#19981;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#30417;&#30563;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#26159;&#19968;&#20010;&#38750;&#24120;&#26840;&#25163;&#30340;&#38382;&#39064;&#12290;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#20013;&#23545;&#27604;&#26041;&#27861;&#30340;&#25104;&#21151;&#21551;&#21457;&#20102;&#23545;&#22270;&#31867;&#20284;&#23581;&#35797;&#12290;&#29616;&#26377;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#26088;&#22312;&#23398;&#20064;&#36328;&#22810;&#20010;&#22686;&#24378;&#35270;&#22270;&#30340;&#19981;&#21464;&#24615;&#65292;&#36825;&#20351;&#20854;&#20005;&#37325;&#20381;&#36182;&#20110;&#25163;&#24037;&#21046;&#20316;&#30340;&#22270;&#22686;&#24378;&#12290;&#28982;&#32780;&#65292;&#19981;&#24403;&#30340;&#22270;&#25968;&#25454;&#22686;&#24378;&#21487;&#33021;&#20250;&#21361;&#23475;&#36825;&#31181;&#19981;&#21464;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19981;&#24403;&#22686;&#24378;&#30340;&#28508;&#22312;&#21361;&#38505;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21327;&#20316;&#24335;&#22270;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65288;CGCL&#65289;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#22810;&#20010;&#22270;&#32534;&#30721;&#22120;&#35266;&#23519;&#22270;&#24418;&#12290;&#26469;&#33258;&#19981;&#21516;&#32534;&#30721;&#22120;&#35266;&#23519;&#21040;&#30340;&#29305;&#24449;&#20316;&#20026;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#23545;&#27604;&#35270;&#22270;&#65292;&#36991;&#20813;&#35825;&#21457;&#19981;&#31283;&#23450;&#30340;&#25200;&#21160;&#24182;&#20445;&#35777;&#19981;&#21464;&#24615;&#12290;&#20026;&#20102;&#30830;&#20445;&#19981;&#21516;&#22270;&#32534;&#30721;&#22120;&#20043;&#38388;&#30340;&#21327;&#20316;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2111.03262v2 Announce Type: replace-cross  Abstract: Unsupervised graph representation learning is a non-trivial topic. The success of contrastive methods in the unsupervised representation learning on structured data inspires similar attempts on the graph. Existing graph contrastive learning (GCL) aims to learn the invariance across multiple augmentation views, which renders it heavily reliant on the handcrafted graph augmentations. However, inappropriate graph data augmentations can potentially jeopardize such invariance. In this paper, we show the potential hazards of inappropriate augmentations and then propose a novel Collaborative Graph Contrastive Learning framework (CGCL). This framework harnesses multiple graph encoders to observe the graph. Features observed from different encoders serve as the contrastive views in contrastive learning, which avoids inducing unstable perturbation and guarantees the invariance. To ensure the collaboration among diverse graph encoders, we
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35282;&#33394;&#30456;&#20284;&#24230;&#24230;&#37327;\textsf{ForestSim}&#65292;&#24182;&#35774;&#35745;&#20102;&#30456;&#24212;&#30340;&#33021;&#22815;&#22312;$O(k)$&#26102;&#38388;&#20869;&#22788;&#29702;top-k&#26597;&#35810;&#30340;&#25628;&#32034;&#31639;&#27861;\textsf{ForestSimSearch}&#12290;</title><link>https://arxiv.org/abs/2110.07872</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#26681;&#26862;&#26519;&#30340;&#35282;&#33394;&#30456;&#20284;&#24230;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Role Similarity Metric Based on Spanning Rooted Forest
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2110.07872
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35282;&#33394;&#30456;&#20284;&#24230;&#24230;&#37327;\textsf{ForestSim}&#65292;&#24182;&#35774;&#35745;&#20102;&#30456;&#24212;&#30340;&#33021;&#22815;&#22312;$O(k)$&#26102;&#38388;&#20869;&#22788;&#29702;top-k&#26597;&#35810;&#30340;&#25628;&#32034;&#31639;&#27861;\textsf{ForestSimSearch}&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#32593;&#32476;&#20998;&#26512;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#32467;&#26500;&#33410;&#28857;&#30456;&#20284;&#24230;&#22312;&#23398;&#26415;&#30028;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#24182;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;&#22312;&#20247;&#22810;&#25552;&#20986;&#30340;&#32467;&#26500;&#33410;&#28857;&#30456;&#20284;&#24230;&#24230;&#37327;&#20013;&#65292;&#30001;&#20110;&#28385;&#36275;&#22810;&#20010;&#20844;&#29702;&#23646;&#24615;&#65292;&#35282;&#33394;&#30456;&#20284;&#24230;&#20984;&#26174;&#20986;&#26469;&#12290;&#29616;&#26377;&#30340;&#35282;&#33394;&#30456;&#20284;&#24230;&#24230;&#37327;&#26080;&#27861;&#22788;&#29702;&#22823;&#22411;&#30495;&#23454;&#32593;&#32476;&#19978;&#30340;top-k&#26597;&#35810;&#65292;&#22240;&#20026;&#26102;&#38388;&#21644;&#31354;&#38388;&#25104;&#26412;&#39640;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35282;&#33394;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#21363;\textsf{ForestSim}&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;\textsf{ForestSim}&#26159;&#19968;&#31181;&#21487;&#23481;&#24525;&#30340;&#35282;&#33394;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#24182;&#35774;&#35745;&#20102;&#30456;&#24212;&#30340;top-k&#30456;&#20284;&#24230;&#25628;&#32034;&#31639;&#27861;&#65292;&#21363;\textsf{ForestSimSearch}&#65292;&#19968;&#26086;&#39044;&#35745;&#31639;&#23436;&#25104;&#65292;&#33021;&#22815;&#22312;$O(k)$&#30340;&#26102;&#38388;&#20869;&#22788;&#29702;&#19968;&#20010;top-k&#26597;&#35810;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#24555;&#36895;&#36817;&#20284;&#31639;&#27861;&#26469;&#35745;&#31639;&#26862;&#26519;&#30697;&#38453;&#30340;&#23545;&#35282;&#32447;&#20803;&#32032;&#26469;&#21152;&#36895;&#39044;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2110.07872v2 Announce Type: replace-cross  Abstract: As a fundamental issue in network analysis, structural node similarity has received much attention in academia and is adopted in a wide range of applications. Among these proposed structural node similarity measures, role similarity stands out because of satisfying several axiomatic properties including automorphism conformation. Existing role similarity metrics cannot handle top-k queries on large real-world networks due to the high time and space cost. In this paper, we propose a new role similarity metric, namely \textsf{ForestSim}. We prove that \textsf{ForestSim} is an admissible role similarity metric and devise the corresponding top-k similarity search algorithm, namely \textsf{ForestSimSearch}, which is able to process a top-k query in $O(k)$ time once the precomputation is finished. Moreover, we speed up the precomputation by using a fast approximate algorithm to compute the diagonal entries of the forest matrix, which
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphFM&#30340;&#22270;&#22240;&#23376;&#20998;&#35299;&#26426;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#32467;&#26500;&#33258;&#28982;&#34920;&#31034;&#29305;&#24449;&#65292;&#24182;&#23558;FM&#30340;&#20132;&#20114;&#21151;&#33021;&#38598;&#25104;&#21040;GNN&#30340;&#29305;&#24449;&#32858;&#21512;&#31574;&#30053;&#20013;&#65292;&#33021;&#22815;&#27169;&#25311;&#20219;&#24847;&#38454;&#29305;&#24449;&#20132;&#20114;&#12290;</title><link>https://arxiv.org/abs/2105.11866</link><description>&lt;p&gt;
GraphFM&#65306;&#22270;&#22240;&#23376;&#20998;&#35299;&#26426;&#29992;&#20110;&#29305;&#24449;&#20132;&#20114;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
GraphFM: Graph Factorization Machines for Feature Interaction Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2105.11866
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphFM&#30340;&#22270;&#22240;&#23376;&#20998;&#35299;&#26426;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#32467;&#26500;&#33258;&#28982;&#34920;&#31034;&#29305;&#24449;&#65292;&#24182;&#23558;FM&#30340;&#20132;&#20114;&#21151;&#33021;&#38598;&#25104;&#21040;GNN&#30340;&#29305;&#24449;&#32858;&#21512;&#31574;&#30053;&#20013;&#65292;&#33021;&#22815;&#27169;&#25311;&#20219;&#24847;&#38454;&#29305;&#24449;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#23376;&#20998;&#35299;&#26426;&#65288;FM&#65289;&#26159;&#22788;&#29702;&#39640;&#32500;&#31232;&#30095;&#25968;&#25454;&#26102;&#24314;&#27169;&#25104;&#23545;&#65288;&#20108;&#38454;&#65289;&#29305;&#24449;&#20132;&#20114;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#19968;&#26041;&#38754;&#65292;FM&#26410;&#33021;&#25429;&#25417;&#21040;&#39640;&#38454;&#29305;&#24449;&#20132;&#20114;&#65292;&#21463;&#21040;&#32452;&#21512;&#25193;&#23637;&#30340;&#24433;&#21709;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#32771;&#34385;&#27599;&#23545;&#29305;&#24449;&#20043;&#38388;&#30340;&#20132;&#20114;&#21487;&#33021;&#20250;&#24341;&#20837;&#22122;&#22768;&#24182;&#38477;&#20302;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;Graph Factorization Machine&#65288;GraphFM&#65289;&#65292;&#36890;&#36807;&#23558;&#29305;&#24449;&#33258;&#28982;&#34920;&#31034;&#25104;&#22270;&#32467;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26426;&#21046;&#26469;&#36873;&#25321;&#26377;&#30410;&#30340;&#29305;&#24449;&#20132;&#20114;&#65292;&#24182;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#29305;&#24449;&#20043;&#38388;&#30340;&#36793;&#12290;&#28982;&#21518;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#23558;FM&#30340;&#20132;&#20114;&#21151;&#33021;&#25972;&#21512;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#29305;&#24449;&#32858;&#21512;&#31574;&#30053;&#20013;&#65292;&#36890;&#36807;&#22534;&#21472;&#23618;&#26469;&#27169;&#25311;&#22270;&#32467;&#26500;&#29305;&#24449;&#19978;&#30340;&#20219;&#24847;&#38454;&#29305;&#24449;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2105.11866v4 Announce Type: replace-cross  Abstract: Factorization machine (FM) is a prevalent approach to modeling pairwise (second-order) feature interactions when dealing with high-dimensional sparse data. However, on the one hand, FM fails to capture higher-order feature interactions suffering from combinatorial expansion. On the other hand, taking into account interactions between every pair of features may introduce noise and degrade prediction accuracy. To solve the problems, we propose a novel approach, Graph Factorization Machine (GraphFM), by naturally representing features in the graph structure. In particular, we design a mechanism to select the beneficial feature interactions and formulate them as edges between features. Then the proposed model, which integrates the interaction function of FM into the feature aggregation strategy of Graph Neural Network (GNN), can model arbitrary-order feature interactions on the graph-structured features by stacking layers. Experime
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MetaVIM&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#25955;&#24335;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#32771;&#34385;&#37051;&#23621;&#20449;&#24687;&#65292;&#20197;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2101.00746</link><description>&lt;p&gt;
MetaVIM&#65306;&#20803;&#21464;&#20998;&#20869;&#22312;&#28608;&#21169;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#20998;&#25955;&#24335;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
MetaVIM: Meta Variationally Intrinsic Motivated Reinforcement Learning for Decentralized Traffic Signal Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2101.00746
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MetaVIM&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#25955;&#24335;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#32771;&#34385;&#37051;&#23621;&#20449;&#24687;&#65292;&#20197;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#26088;&#22312;&#21327;&#35843;&#20132;&#21449;&#21475;&#30340;&#20132;&#36890;&#20449;&#21495;&#65292;&#20197;&#25913;&#21892;&#21306;&#22495;&#25110;&#22478;&#24066;&#30340;&#20132;&#36890;&#25928;&#29575;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#34987;&#24212;&#29992;&#20110;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#65292;&#24182;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#27599;&#20010;&#20132;&#36890;&#20449;&#21495;&#34987;&#35270;&#20026;&#19968;&#20010;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#20173;&#23384;&#22312;&#19968;&#20123;&#21487;&#33021;&#38480;&#21046;&#20854;&#22823;&#35268;&#27169;&#24212;&#29992;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#20351;&#20174;&#35757;&#32451;&#22330;&#26223;&#20013;&#23398;&#21040;&#30340;&#31574;&#30053;&#33021;&#22815;&#25512;&#24191;&#21040;&#26032;&#30340;&#26410;&#35265;&#22330;&#26223;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Meta Variationally Intrinsic Motivated&#65288;MetaVIM&#65289;RL&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#32771;&#34385;&#37051;&#23621;&#20449;&#24687;&#30340;&#27599;&#20010;&#20132;&#21449;&#21475;&#30340;&#20998;&#25955;&#24335;&#31574;&#30053;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#31574;&#30053;&#23398;&#20064;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#20851;&#20110;&#19968;&#32452;&#30456;&#20851;&#20219;&#21153;&#30340;&#20803;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#20010;&#20219;&#21153;&#23545;&#24212;&#20110;&#19968;&#20010;&#20132;&#21449;&#21475;&#30340;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#65292;&#20854;&#37051;&#23621;&#34987;&#35270;&#20026;&#29366;&#24577;&#30340;&#26410;&#35266;&#23519;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2101.00746v5 Announce Type: replace-cross  Abstract: Traffic signal control aims to coordinate traffic signals across intersections to improve the traffic efficiency of a district or a city. Deep reinforcement learning (RL) has been applied to traffic signal control recently and demonstrated promising performance where each traffic signal is regarded as an agent. However, there are still several challenges that may limit its large-scale application in the real world. To make the policy learned from a training scenario generalizable to new unseen scenarios, a novel Meta Variationally Intrinsic Motivated (MetaVIM) RL method is proposed to learn the decentralized policy for each intersection that considers neighbor information in a latent way. Specifically, we formulate the policy learning as a meta-learning problem over a set of related tasks, where each task corresponds to traffic signal control at an intersection whose neighbors are regarded as the unobserved part of the state. T
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#20581;&#24247;&#25991;&#26412;&#31616;&#21270;&#30740;&#31350;&#30340;&#28040;&#21270;&#30284;&#30151;&#25945;&#32946;&#26448;&#26009;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#24494;&#35843;&#12289;&#22686;&#24378;&#23398;&#20064;&#12289;&#22686;&#24378;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.15043</link><description>&lt;p&gt;
&#20581;&#24247;&#25991;&#26412;&#31616;&#21270;&#65306;&#28040;&#21270;&#30284;&#30151;&#25945;&#32946;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#21644;&#22686;&#24378;&#23398;&#20064;&#30340;&#26032;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Health Text Simplification: An Annotated Corpus for Digestive Cancer Education and Novel Strategies for Reinforcement Learning. (arXiv:2401.15043v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15043
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#20581;&#24247;&#25991;&#26412;&#31616;&#21270;&#30740;&#31350;&#30340;&#28040;&#21270;&#30284;&#30151;&#25945;&#32946;&#26448;&#26009;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#24494;&#35843;&#12289;&#22686;&#24378;&#23398;&#20064;&#12289;&#22686;&#24378;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#20581;&#24247;&#25945;&#32946;&#26448;&#26009;&#30340;&#38405;&#35835;&#27700;&#24179;&#26174;&#33879;&#24433;&#21709;&#20449;&#24687;&#30340;&#21487;&#29702;&#35299;&#24615;&#21644;&#21487;&#25509;&#35302;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23569;&#25968;&#26063;&#35028;&#20154;&#32676;&#12290;&#35768;&#22810;&#24739;&#32773;&#25945;&#32946;&#36164;&#28304;&#36229;&#36807;&#20102;&#24191;&#27867;&#25509;&#21463;&#30340;&#26631;&#20934;&#30340;&#38405;&#35835;&#27700;&#24179;&#21644;&#22797;&#26434;&#24615;&#12290;&#22312;&#20581;&#24247;&#20449;&#24687;&#20013;&#65292;&#24613;&#38656;&#39640;&#24615;&#33021;&#30340;&#25991;&#26412;&#31616;&#21270;&#27169;&#22411;&#20197;&#22686;&#24378;&#20256;&#25773;&#21644;&#35782;&#23383;&#33021;&#21147;&#12290;&#36825;&#31181;&#38656;&#35201;&#22312;&#30284;&#30151;&#25945;&#32946;&#20013;&#23588;&#20026;&#36843;&#20999;&#65292;&#26377;&#25928;&#30340;&#39044;&#38450;&#21644;&#31579;&#26597;&#25945;&#32946;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#21457;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#24341;&#20837;&#20102;&#31616;&#21270;&#30340;&#28040;&#21270;&#30284;&#30151;&#65288;SimpleDC&#65289;&#24182;&#34892;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#20581;&#24247;&#25991;&#26412;&#31616;&#21270;&#30740;&#31350;&#12290;&#21033;&#29992;SimpleDC&#21644;&#29616;&#26377;&#30340;Med-EASi&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#24494;&#35843;&#12289;&#22686;&#24378;&#23398;&#20064;&#65288;RL&#65289;&#12289;&#22686;&#24378;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: The reading level of health educational materials significantly influences information understandability and accessibility, particularly for minoritized populations. Many patient educational resources surpass the reading level and complexity of widely accepted standards. There is a critical need for high-performing text simplification models in health information to enhance dissemination and literacy. This need is particularly acute in cancer education, where effective prevention and screening education can substantially reduce morbidity and mortality.  Methods: We introduce Simplified Digestive Cancer (SimpleDC), a parallel corpus of cancer education materials tailored for health text simplification research. Utilizing SimpleDC alongside the existing Med-EASi corpus, we explore Large Language Model (LLM)-based simplification methods, including fine-tuning, reinforcement learning (RL), reinforcement learning with human feedback (RLHF), domain adaptation, and prompt-based app
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#32467;&#21512;&#32467;&#26500;&#30340;&#25552;&#31034;&#24037;&#31243;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#24615;&#33021;&#26041;&#38754;&#30340;&#21069;&#26223;&#65292;&#36890;&#36807;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#26641;&#25110;&#24605;&#32500;&#22270;&#30340;&#35774;&#35745;&#26469;&#24341;&#23548;&#25972;&#20307;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#20363;&#65292;&#36825;&#31181;&#33539;&#24335;&#26174;&#33879;&#22686;&#24378;&#20102;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#34013;&#22270;&#65292;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#38138;&#24179;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2401.14295</link><description>&lt;p&gt;
&#25512;&#29702;&#30340;&#25299;&#25169;&#23398;&#65306;&#25581;&#31192;&#24605;&#32500;&#38142;&#12289;&#26641;&#21644;&#22270;
&lt;/p&gt;
&lt;p&gt;
Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of Thoughts. (arXiv:2401.14295v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14295
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#32467;&#21512;&#32467;&#26500;&#30340;&#25552;&#31034;&#24037;&#31243;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#24615;&#33021;&#26041;&#38754;&#30340;&#21069;&#26223;&#65292;&#36890;&#36807;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#26641;&#25110;&#24605;&#32500;&#22270;&#30340;&#35774;&#35745;&#26469;&#24341;&#23548;&#25972;&#20307;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#20363;&#65292;&#36825;&#31181;&#33539;&#24335;&#26174;&#33879;&#22686;&#24378;&#20102;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#34013;&#22270;&#65292;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#36890;&#36807;&#21019;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24615;&#33021;&#26041;&#38754;&#12290;&#20854;&#20013;&#65292;&#19982;&#32467;&#26500;&#30456;&#32467;&#21512;&#30340;&#25552;&#31034;&#24037;&#31243;&#34987;&#35270;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#24335;&#65292;&#20854;&#35774;&#35745;&#22914;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#26641;&#25110;&#24605;&#32500;&#22270;&#31561;&#65292;&#36890;&#36807;&#32467;&#26500;&#25351;&#23548;&#25972;&#20307;LLM&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#20363;&#30340;&#35828;&#26126;&#65292;&#36825;&#31181;&#33539;&#24335;&#26174;&#33879;&#22686;&#24378;&#20102;LLM&#22312;&#36923;&#36753;&#25110;&#25968;&#23398;&#25512;&#29702;&#12289;&#35268;&#21010;&#25110;&#21019;&#36896;&#24615;&#20889;&#20316;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#26041;&#20415;&#29702;&#35299;&#36825;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#24182;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#38138;&#24179;&#36947;&#36335;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;LLM&#25512;&#29702;&#26041;&#26696;&#30340;&#36890;&#29992;&#34013;&#22270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#25552;&#31034;&#25191;&#34892;&#27969;&#31243;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#28548;&#28165;&#24182;&#26126;&#30830;&#23450;&#20041;&#20102;&#19981;&#21516;&#30340;&#27010;&#24565;&#12290;&#28982;&#21518;&#25105;&#20204;&#24314;&#31435;&#31532;&#19968;&#20010;&#20998;&#31867;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
The field of natural language processing (NLP) has witnessed significant progress in recent years, with a notable focus on improving large language models' (LLM) performance through innovative prompting techniques. Among these, prompt engineering coupled with structures has emerged as a promising paradigm, with designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts, in which the overall LLM reasoning is guided by a structure such as a graph. As illustrated with numerous examples, this paradigm significantly enhances the LLM's capability to solve numerous tasks, ranging from logical or mathematical reasoning to planning or creative writing. To facilitate the understanding of this growing field and pave the way for future developments, we devise a general blueprint for effective and efficient LLM reasoning schemes. For this, we conduct an in-depth analysis of the prompt execution pipeline, clarifying and clearly defining different concepts. We then build the first taxon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24182;&#34892;&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;PCGBench&#65292;&#24182;&#20351;&#29992;&#26032;&#25351;&#26631;&#35780;&#20272;&#20102;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#24182;&#34892;&#32534;&#31243;&#27169;&#22411;&#21644;&#35745;&#31639;&#38382;&#39064;&#31867;&#22411;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.12554</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#32534;&#20889;&#24182;&#34892;&#20195;&#30721;?
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Write Parallel Code?. (arXiv:2401.12554v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24182;&#34892;&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;PCGBench&#65292;&#24182;&#20351;&#29992;&#26032;&#25351;&#26631;&#35780;&#20272;&#20102;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#24182;&#34892;&#32534;&#31243;&#27169;&#22411;&#21644;&#35745;&#31639;&#38382;&#39064;&#31867;&#22411;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27491;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#36719;&#20214;&#24320;&#21457;&#32773;&#30340;&#27426;&#36814;&#12290;&#23427;&#20204;&#22312;&#22810;&#31181;&#24773;&#22659;&#19979;&#23637;&#31034;&#20102;&#23545;&#28304;&#20195;&#30721;&#30340;&#24314;&#27169;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#21253;&#25324;&#20195;&#30721;&#34917;&#20840;&#12289;&#25688;&#35201;&#12289;&#32763;&#35793;&#21644;&#26597;&#25214;&#31561;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24448;&#24448;&#38590;&#20197;&#20026;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#29983;&#25104;&#20195;&#30721;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24182;&#34892;&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PCGBench&#30340;&#22522;&#20934;&#27979;&#35797;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;420&#20010;&#20219;&#21153;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24182;&#34892;&#20195;&#30721;&#30340;&#33021;&#21147;&#65292;&#24182;&#23545;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#24320;&#28304;&#21644;&#38381;&#28304;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20123;&#29992;&#20110;&#27604;&#36739;&#24182;&#34892;&#20195;&#30721;&#29983;&#25104;&#24615;&#33021;&#30340;&#26032;&#25351;&#26631;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#25351;&#26631;&#26469;&#25506;&#31350;&#27599;&#20010;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#24182;&#34892;&#32534;&#31243;&#27169;&#22411;&#21644;&#35745;&#31639;&#38382;&#39064;&#31867;&#22411;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are becoming an increasingly popular tool for software development. Their ability to model and generate source code has been demonstrated in a variety of contexts, including code completion, summarization, translation, and lookup. However, they often struggle to generate code for more complex tasks. In this paper, we explore the ability of state-of-the-art language models to generate parallel code. We propose a benchmark, PCGBench, consisting of a set of 420 tasks for evaluating the ability of language models to generate parallel code, and we evaluate the performance of several state-of-the-art open- and closed-source language models on these tasks. We introduce novel metrics for comparing parallel code generation performance and use them to explore how well each LLM performs on various parallel programming models and computational problem types.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#26041;&#27861;GI-PIP&#65292;&#19981;&#38656;&#35201;&#20381;&#36182;&#19981;&#20999;&#23454;&#38469;&#30340;&#36741;&#21161;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21033;&#29992;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#20174;&#36739;&#23569;&#30340;&#25968;&#25454;&#20013;&#25429;&#33719;&#24213;&#23618;&#20998;&#24067;&#65292;&#24182;&#33021;&#22312;&#22270;&#20687;&#24674;&#22797;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#22312;&#20998;&#24067;&#27867;&#21270;&#26041;&#38754;&#20063;&#26356;&#24378;&#22823;&#12290;</title><link>http://arxiv.org/abs/2401.11748</link><description>&lt;p&gt;
GI-PIP&#65306;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#26159;&#21542;&#38656;&#35201;&#19981;&#20999;&#23454;&#38469;&#30340;&#36741;&#21161;&#25968;&#25454;&#38598;&#65311;
&lt;/p&gt;
&lt;p&gt;
GI-PIP: Do We Require Impractical Auxiliary Dataset for Gradient Inversion Attacks?. (arXiv:2401.11748v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#26041;&#27861;GI-PIP&#65292;&#19981;&#38656;&#35201;&#20381;&#36182;&#19981;&#20999;&#23454;&#38469;&#30340;&#36741;&#21161;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21033;&#29992;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#20174;&#36739;&#23569;&#30340;&#25968;&#25454;&#20013;&#25429;&#33719;&#24213;&#23618;&#20998;&#24067;&#65292;&#24182;&#33021;&#22312;&#22270;&#20687;&#24674;&#22797;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#22312;&#20998;&#24067;&#27867;&#21270;&#26041;&#38754;&#20063;&#26356;&#24378;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#36890;&#36807;&#20934;&#30830;&#22320;&#24674;&#22797;&#20849;&#20139;&#26799;&#24230;&#20013;&#30340;&#38544;&#31169;&#25968;&#25454;&#65292;&#23545;&#32852;&#37030;&#23398;&#20064;&#26500;&#25104;&#20102;&#20005;&#37325;&#23041;&#32961;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25216;&#26415;&#22312;&#35775;&#38382;&#36807;&#22810;&#30340;&#36741;&#21161;&#25968;&#25454;&#26041;&#38754;&#20381;&#36182;&#20110;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#65292;&#36825;&#36829;&#21453;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#22522;&#26412;&#25968;&#25454;&#20998;&#21306;&#21407;&#21017;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#23454;&#29992;&#22270;&#20687;&#20808;&#39564;&#30340;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#65288;GI-PIP&#65289;&#65292;&#22312;&#32463;&#36807;&#20462;&#35746;&#30340;&#23041;&#32961;&#27169;&#22411;&#19979;&#12290;GI-PIP&#21033;&#29992;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#20174;&#26356;&#23569;&#30340;&#25968;&#25454;&#20013;&#25429;&#33719;&#24213;&#23618;&#20998;&#24067;&#65292;&#32780;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#38656;&#35201;&#28040;&#32791;&#26356;&#22810;&#30340;&#25968;&#25454;&#26469;&#21512;&#25104;&#22270;&#20687;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#25552;&#21462;&#20986;&#30340;&#20998;&#24067;&#26469;&#35843;&#33410;&#25915;&#20987;&#36807;&#31243;&#20316;&#20026;&#24322;&#24120;&#24471;&#20998;&#25439;&#22833;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GI-PIP&#21482;&#20351;&#29992;&#20102;ImageNet&#25968;&#25454;&#30340;3.8%&#21363;&#21487;&#23454;&#29616;16.12 dB&#30340;PSNR&#24674;&#22797;&#65292;&#32780;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#21017;&#38656;&#35201;&#36229;&#36807;70%&#30340;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#19982;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;GI-PIP&#22312;&#20998;&#24067;&#27867;&#21270;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep gradient inversion attacks expose a serious threat to Federated Learning (FL) by accurately recovering private data from shared gradients. However, the state-of-the-art heavily relies on impractical assumptions to access excessive auxiliary data, which violates the basic data partitioning principle of FL. In this paper, a novel method, Gradient Inversion Attack using Practical Image Prior (GI-PIP), is proposed under a revised threat model. GI-PIP exploits anomaly detection models to capture the underlying distribution from fewer data, while GAN-based methods consume significant more data to synthesize images. The extracted distribution is then leveraged to regulate the attack process as Anomaly Score loss. Experimental results show that GI-PIP achieves a 16.12 dB PSNR recovery using only 3.8% data of ImageNet, while GAN-based methods necessitate over 70%. Moreover, GI-PIP exhibits superior capability on distribution generalization compared to GAN-based methods. Our approach signif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#20840;&#38754;&#22238;&#39038;&#65292;&#28085;&#30422;&#20102;&#22312;NISQ&#25216;&#26415;&#21644;&#23481;&#38169;&#37327;&#23376;&#35745;&#31639;&#30828;&#20214;&#19978;&#20351;&#29992;&#30340;&#25216;&#26415;&#21644;&#31639;&#27861;&#65292;&#24182;&#28145;&#20837;&#35752;&#35770;&#20102;&#19982;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30456;&#20851;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#12290;</title><link>http://arxiv.org/abs/2401.11351</link><description>&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65306;&#20174;NISQ&#21040;&#23481;&#38169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum Machine Learning: from NISQ to Fault Tolerance. (arXiv:2401.11351v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#20840;&#38754;&#22238;&#39038;&#65292;&#28085;&#30422;&#20102;&#22312;NISQ&#25216;&#26415;&#21644;&#23481;&#38169;&#37327;&#23376;&#35745;&#31639;&#30828;&#20214;&#19978;&#20351;&#29992;&#30340;&#25216;&#26415;&#21644;&#31639;&#27861;&#65292;&#24182;&#28145;&#20837;&#35752;&#35770;&#20102;&#19982;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30456;&#20851;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#26159;&#22312;&#37327;&#23376;&#35774;&#22791;&#19978;&#36816;&#34892;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#36807;&#31243;&#65292;&#22312;&#23398;&#26415;&#30028;&#21644;&#21830;&#19994;&#30028;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#23545;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#28044;&#29616;&#30340;&#21508;&#31181;&#27010;&#24565;&#36827;&#34892;&#20102;&#20840;&#38754;&#32780;&#20844;&#27491;&#30340;&#22238;&#39038;&#12290;&#36825;&#21253;&#25324;&#22312;&#22122;&#22768;&#20013;&#38388;&#23610;&#24230;&#37327;&#23376;&#65288;NISQ&#65289;&#25216;&#26415;&#20013;&#20351;&#29992;&#30340;&#25216;&#26415;&#65292;&#20197;&#21450;&#19982;&#23481;&#38169;&#37327;&#23376;&#35745;&#31639;&#30828;&#20214;&#20860;&#23481;&#30340;&#31639;&#27861;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#22238;&#39038;&#28085;&#30422;&#20102;&#19982;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30456;&#20851;&#30340;&#22522;&#26412;&#27010;&#24565;&#12289;&#31639;&#27861;&#21644;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum machine learning, which involves running machine learning algorithms on quantum devices, has garnered significant attention in both academic and business circles. In this paper, we offer a comprehensive and unbiased review of the various concepts that have emerged in the field of quantum machine learning. This includes techniques used in Noisy Intermediate-Scale Quantum (NISQ) technologies and approaches for algorithms compatible with fault-tolerant quantum computing hardware. Our review covers fundamental concepts, algorithms, and the statistical learning theory pertinent to quantum machine learning.
&lt;/p&gt;</description></item><item><title>DeepEdit&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#22909;&#30340;&#25512;&#29702;&#19968;&#33268;&#24615;&#21644;&#23545;&#26356;&#26032;&#30693;&#35782;&#30340;&#24847;&#35782;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#33021;&#21147;&#65292;&#23545;&#22810;&#36339;&#38382;&#39064;&#25968;&#25454;&#38598;MQuaKE&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.10471</link><description>&lt;p&gt;
DeepEdit: &#24102;&#26377;&#32422;&#26463;&#30340;&#35299;&#30721;&#24335;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
DeepEdit: Knowledge Editing as Decoding with Constraints. (arXiv:2401.10471v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10471
&lt;/p&gt;
&lt;p&gt;
DeepEdit&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#22909;&#30340;&#25512;&#29702;&#19968;&#33268;&#24615;&#21644;&#23545;&#26356;&#26032;&#30693;&#35782;&#30340;&#24847;&#35782;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#33021;&#21147;&#65292;&#23545;&#22810;&#36339;&#38382;&#39064;&#25968;&#25454;&#38598;MQuaKE&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#32534;&#36753;&#35270;&#20026;&#24102;&#26377;&#32422;&#26463;&#30340;&#35299;&#30721;&#36807;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DeepEdit&#65288;&#22522;&#20110;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#30340;&#28176;&#36827;&#24335;&#35299;&#30721;&#30693;&#35782;&#32534;&#36753;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#22909;&#30340;&#25512;&#29702;&#19968;&#33268;&#24615;&#12289;&#38382;&#39064;&#30456;&#20851;&#24615;&#21644;&#23545;&#26356;&#26032;&#30693;&#35782;&#30340;&#24847;&#35782;&#26469;&#25913;&#36827;&#30693;&#35782;&#32534;&#36753;&#12290;DeepEdit&#21487;&#28789;&#27963;&#24212;&#29992;&#20110;&#25152;&#26377;&#40657;&#30418;LLMs&#65306;&#19981;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#12289;&#34920;&#31034;&#25110;&#36755;&#20986;&#35789;&#27719;&#20998;&#24067;&#12290;DeepEdit&#36880;&#27493;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#25512;&#29702;&#27493;&#39588;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#30693;&#35782;&#32534;&#36753;&#12290;&#23427;&#21033;&#29992;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#26469;&#20462;&#25913;LLMs&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#25552;&#39640;&#36755;&#20986;&#23545;&#38382;&#39064;&#30340;&#30456;&#20851;&#24615;&#21644;&#23545;&#26356;&#26032;&#30693;&#35782;&#30340;&#24847;&#35782;&#12290;&#22312;&#30693;&#35782;&#32534;&#36753;&#26041;&#38754;&#65292;DeepEdit&#22312;&#25511;&#21046;LLMs&#20135;&#29983;&#26356;&#31616;&#27905;&#30340;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#22312;MQuaKE&#19978;&#65292;DeepEdit&#22312;&#23450;&#37327;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#36339;&#38382;&#39064;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a new perspective of knowledge editing for large language models (LLMs) as decoding with constraints. We propose DeepEdit (Depth-first Search based Progressive Decoding for Knowledge Editing), a neuro-symbolic method that improves knowledge editing with better coherence of reasoning, relevance to the question, and awareness of updated knowledge. DeepEdit can be flexibly applied to all black-box LLMs: it does not require any access to the model parameters, representations, or output vocabulary distributions. DeepEdit progressively produces the high-quality reasoning steps towards effective knowledge editing. It utilizes a depth-first search to revise the LLMs' output, which improves the output's informativeness to the input question and awareness of the updated knowledge. Qualitatively, DeepEdit effectively controls LLMs to produce more succinct reasoning in accord with knowledge editing. Quantitatively, DeepEdit yields significant gains on MQuaKE, a challenging multi-hop que
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#30446;&#26631;&#35782;&#21035;&#20013;&#23558;&#22320;&#29702;&#22810;&#26679;&#30693;&#35782;&#34701;&#20837;&#25552;&#31034;&#20197;&#25552;&#39640;&#22320;&#29702;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33719;&#21462;&#22320;&#29702;&#29305;&#23450;&#23545;&#35937;&#30693;&#35782;&#24182;&#32467;&#21512;CLIP&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#21644;&#21487;&#23398;&#20064;&#36719;&#25552;&#31034;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#22320;&#29702;&#30693;&#35782;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20174;&#28304;&#22320;&#29702;&#20301;&#32622;&#25512;&#24191;&#21040;&#26410;&#35265;&#30446;&#26631;&#22320;&#29702;&#20301;&#32622;&#30340;&#40065;&#26834;&#24615;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.01482</link><description>&lt;p&gt;
&#22312;&#30446;&#26631;&#35782;&#21035;&#20013;&#23558;&#22320;&#29702;&#22810;&#26679;&#30693;&#35782;&#34701;&#20837;&#25552;&#31034;&#20197;&#25552;&#39640;&#22320;&#29702;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Incorporating Geo-Diverse Knowledge into Prompting for Increased Geographical Robustness in Object Recognition. (arXiv:2401.01482v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#30446;&#26631;&#35782;&#21035;&#20013;&#23558;&#22320;&#29702;&#22810;&#26679;&#30693;&#35782;&#34701;&#20837;&#25552;&#31034;&#20197;&#25552;&#39640;&#22320;&#29702;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33719;&#21462;&#22320;&#29702;&#29305;&#23450;&#23545;&#35937;&#30693;&#35782;&#24182;&#32467;&#21512;CLIP&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#21644;&#21487;&#23398;&#20064;&#36719;&#25552;&#31034;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#22320;&#29702;&#30693;&#35782;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20174;&#28304;&#22320;&#29702;&#20301;&#32622;&#25512;&#24191;&#21040;&#26410;&#35265;&#30446;&#26631;&#22320;&#29702;&#20301;&#32622;&#30340;&#40065;&#26834;&#24615;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#30446;&#26631;&#35782;&#21035;&#27169;&#22411;&#22312;&#19981;&#21516;&#22320;&#29702;&#24773;&#26223;&#19979;&#32570;&#20047;&#40065;&#26834;&#24615;&#65292;&#36825;&#26159;&#30001;&#20110;&#35774;&#35745;&#21644;&#29615;&#22659;&#20013;&#23384;&#22312;&#37325;&#35201;&#30340;&#39046;&#22495;&#36716;&#31227;&#12290;&#20026;&#20102;&#26356;&#20934;&#30830;&#22320;&#21453;&#26144;&#36825;&#20123;&#36716;&#31227;&#19979;&#30340;&#23545;&#35937;&#27010;&#24565;&#65292;&#38656;&#35201;&#35843;&#25972;&#31867;&#21035;&#34920;&#31034;&#12290;&#22312;&#32570;&#20047;&#30446;&#26631;&#22320;&#29702;&#20301;&#32622;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20551;&#35774;&#21487;&#20197;&#21033;&#29992;&#22320;&#29702;&#29305;&#23450;&#30340;&#23545;&#35937;&#31867;&#21035;&#25551;&#36848;&#24615;&#30693;&#35782;&#26469;&#22686;&#24378;&#40065;&#26834;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#36890;&#36807;&#25506;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22320;&#29702;&#29305;&#23450;&#23545;&#35937;&#30693;&#35782;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;CLIP&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#38598;&#25104;&#30693;&#35782;&#30340;&#38646;&#26679;&#26412;&#21644;&#21487;&#23398;&#20064;&#36719;&#25552;&#31034;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22320;&#29702;&#30693;&#35782;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#22312;&#19968;&#32452;&#28304;&#22320;&#29702;&#20301;&#32622;&#19978;&#35757;&#32451;&#30340;&#36719;&#25552;&#31034;&#33021;&#22815;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#30446;&#26631;&#22320;&#29702;&#20301;&#32622;&#38598;&#21512;&#12290;&#24403;&#20165;&#20381;&#36182;&#26469;&#33258;&#27431;&#27954;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#22312;DollarStreet&#19978;&#30340;&#22686;&#30410;&#36798;&#21040;&#20102;+2.8&#20010;&#22269;&#23478;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing object recognition models have been shown to lack robustness in diverse geographical scenarios due to significant domain shifts in design and context. Class representations need to be adapted to more accurately reflect an object concept under these shifts. In the absence of training data from target geographies, we hypothesize that geography-specific descriptive knowledge of object categories can be leveraged to enhance robustness. For this purpose, we explore the feasibility of probing a large-language model for geography-specific object knowledge, and we investigate integrating knowledge in zero-shot and learnable soft prompting with the CLIP vision-language model. In particular, we propose a geography knowledge regularization method to ensure that soft prompts trained on a source set of geographies generalize to an unseen target set of geographies. Our gains on DollarStreet when generalizing from a model trained only on data from Europe are as large as +2.8 on countries fro
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35752;&#35770;&#20102;&#21033;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#36827;&#34892;&#20020;&#24202;&#35777;&#25454;&#25688;&#35201;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#23588;&#20854;&#20851;&#27880;&#24320;&#21457;&#38382;&#36131;&#21046;&#12289;&#20844;&#24179;&#21644;&#21253;&#23481;&#24615;&#30340;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2311.11211</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#36827;&#34892;&#20020;&#24202;&#35777;&#25454;&#25688;&#35201;&#38656;&#35201;&#30830;&#20445;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Leveraging Generative AI for Clinical Evidence Summarization Needs to Ensure Trustworthiness. (arXiv:2311.11211v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.11211
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35752;&#35770;&#20102;&#21033;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#36827;&#34892;&#20020;&#24202;&#35777;&#25454;&#25688;&#35201;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#23588;&#20854;&#20851;&#27880;&#24320;&#21457;&#38382;&#36131;&#21046;&#12289;&#20844;&#24179;&#21644;&#21253;&#23481;&#24615;&#30340;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35777;&#25454;&#21307;&#23398;&#25215;&#35834;&#36890;&#36807;&#21033;&#29992;&#26368;&#20339;&#21487;&#29992;&#35777;&#25454;&#26469;&#22686;&#24378;&#21307;&#30103;&#20915;&#31574;&#21644;&#23454;&#36341;&#65292;&#20174;&#32780;&#25552;&#39640;&#21307;&#30103;&#36136;&#37327;&#12290;&#21307;&#23398;&#35777;&#25454;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#21487;&#20197;&#20174;&#21508;&#31181;&#26469;&#28304;&#33719;&#21462;&#65292;&#32473;&#25910;&#38598;&#12289;&#35780;&#20272;&#21644;&#32508;&#21512;&#35777;&#25454;&#20449;&#24687;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26368;&#36817;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#65292;&#20363;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26377;&#26395;&#20419;&#36827;&#36825;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#20986;&#20855;&#26377;&#38382;&#36131;&#21046;&#12289;&#20844;&#24179;&#21644;&#21253;&#23481;&#24615;&#30340;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#20010;&#35270;&#35282;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#22312;&#33258;&#21160;&#21270;&#21307;&#30103;&#35777;&#25454;&#25688;&#35201;&#26041;&#38754;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evidence-based medicine promises to improve the quality of healthcare by empowering medical decisions and practices with the best available evidence. The rapid growth of medical evidence, which can be obtained from various sources, poses a challenge in collecting, appraising, and synthesizing the evidential information. Recent advancements in generative AI, exemplified by large language models, hold promise in facilitating the arduous task. However, developing accountable, fair, and inclusive models remains a complicated undertaking. In this perspective, we discuss the trustworthiness of generative AI in the context of automated summarization of medical evidence.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#23398;&#20064;&#26080;&#30417;&#30563;&#30340;&#33258;&#21160;&#39550;&#39542;&#19990;&#30028;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;VQVAE&#23545;&#20256;&#24863;&#22120;&#35266;&#23519;&#36827;&#34892;&#26631;&#35760;&#21270;&#24182;&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#39044;&#27979;&#26410;&#26469;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#28857;&#20113;&#35266;&#23519;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#23558;1&#31186;&#39044;&#27979;&#30340;SOTA Chamfer&#36317;&#31163;&#38477;&#20302;&#20102;65%&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2311.01017</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#23398;&#20064;&#26080;&#30417;&#30563;&#30340;&#33258;&#21160;&#39550;&#39542;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion. (arXiv:2311.01017v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#23398;&#20064;&#26080;&#30417;&#30563;&#30340;&#33258;&#21160;&#39550;&#39542;&#19990;&#30028;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;VQVAE&#23545;&#20256;&#24863;&#22120;&#35266;&#23519;&#36827;&#34892;&#26631;&#35760;&#21270;&#24182;&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#39044;&#27979;&#26410;&#26469;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#28857;&#20113;&#35266;&#23519;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#23558;1&#31186;&#39044;&#27979;&#30340;SOTA Chamfer&#36317;&#31163;&#38477;&#20302;&#20102;65%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#19990;&#30028;&#27169;&#22411;&#21487;&#20197;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#25945;&#20250;&#26234;&#33021;&#20307;&#19990;&#30028;&#30340;&#36816;&#20316;&#26041;&#24335;&#12290;&#23613;&#31649;&#23427;&#21487;&#20197;&#30475;&#20316;&#26159;&#24207;&#21015;&#24314;&#27169;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#20294;&#22312;&#33258;&#21160;&#39550;&#39542;&#31561;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#65292;&#19982;&#20351;&#29992;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#65292;&#25193;&#23637;&#19990;&#30028;&#27169;&#22411;&#30340;&#36827;&#23637;&#30456;&#23545;&#36739;&#24930;&#12290;&#25105;&#20204;&#25351;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#29942;&#39048;&#65306;&#22788;&#29702;&#22797;&#26434;&#21644;&#26080;&#32467;&#26500;&#30340;&#35266;&#23519;&#31354;&#38388;&#20197;&#21450;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19990;&#30028;&#24314;&#27169;&#26041;&#27861;&#65292;&#39318;&#20808;&#20351;&#29992;VQVAE&#23545;&#20256;&#24863;&#22120;&#35266;&#23519;&#36827;&#34892;&#26631;&#35760;&#21270;&#65292;&#28982;&#21518;&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#39044;&#27979;&#26410;&#26469;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#24182;&#34892;&#35299;&#30721;&#21644;&#21435;&#22122;&#26631;&#35760;&#65292;&#25105;&#20204;&#23558;&#36974;&#34109;&#29983;&#25104;&#22270;&#20687;&#36716;&#25442;&#22120;&#36716;&#25442;&#20026;&#31163;&#25955;&#25193;&#25955;&#26694;&#26550;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#20123;&#31616;&#21333;&#30340;&#25913;&#36827;&#65292;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;&#24403;&#24212;&#29992;&#20110;&#28857;&#20113;&#35266;&#23519;&#30340;&#19990;&#30028;&#27169;&#22411;&#23398;&#20064;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;1&#31186;&#39044;&#27979;&#30340;SOTA Chamfer&#36317;&#31163;&#38477;&#20302;&#20102;65%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning world models can teach an agent how the world works in an unsupervised manner. Even though it can be viewed as a special case of sequence modeling, progress for scaling world models on robotic applications such as autonomous driving has been somewhat less rapid than scaling language models with Generative Pre-trained Transformers (GPT). We identify two reasons as major bottlenecks: dealing with complex and unstructured observation space, and having a scalable generative model. Consequently, we propose a novel world modeling approach that first tokenizes sensor observations with VQVAE, then predicts the future via discrete diffusion. To efficiently decode and denoise tokens in parallel, we recast Masked Generative Image Transformer into the discrete diffusion framework with a few simple changes, resulting in notable improvement. When applied to learning world models on point cloud observations, our model reduces prior SOTA Chamfer distance by more than 65% for 1s prediction, an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;ROAM&#21644;ROOM&#31639;&#27861;&#26694;&#26550;&#36890;&#36807;&#23558;&#20013;&#20301;&#25968;&#27861;&#19982;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#23545;&#37325;&#23614;&#22870;&#21169;&#30340;&#30452;&#25509;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.18715</link><description>&lt;p&gt;
&#20855;&#26377;&#37325;&#23614;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#21644;&#20248;&#21270;&#30340;&#40065;&#26834;&#24615;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
Robust Offline Policy Evaluation and Optimization with Heavy-Tailed Rewards. (arXiv:2310.18715v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;ROAM&#21644;ROOM&#31639;&#27861;&#26694;&#26550;&#36890;&#36807;&#23558;&#20013;&#20301;&#25968;&#27861;&#19982;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#23545;&#37325;&#23614;&#22870;&#21169;&#30340;&#30452;&#25509;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#22686;&#24378;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#37325;&#23614;&#22870;&#21169;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#31639;&#27861;&#26694;&#26550;&#65292;ROAM&#21644;ROOM&#65292;&#29992;&#20110;&#40065;&#26834;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#21644;&#31163;&#32447;&#31574;&#30053;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#26680;&#24515;&#26159;&#23558;&#20013;&#20301;&#25968;&#27861;&#19982;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#23545;&#20540;&#20989;&#25968;&#20272;&#35745;&#22120;&#36827;&#34892;&#30452;&#25509;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#36825;&#19981;&#20165;&#31526;&#21512;&#31163;&#32447;&#31574;&#30053;&#20248;&#21270;&#20013;&#30340;&#20445;&#23432;&#20027;&#20041;&#21407;&#21017;&#65292;&#32780;&#19988;&#28789;&#27963;&#22788;&#29702;&#37325;&#23614;&#22870;&#21169;&#12290;&#29702;&#35770;&#32467;&#26524;&#21644;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#20004;&#20010;&#26694;&#26550;&#22312;&#35760;&#24405;&#30340;&#25968;&#25454;&#38598;&#20013;&#23637;&#31034;&#20102;&#20855;&#26377;&#37325;&#23614;&#22870;&#21169;&#20998;&#24067;&#26102;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper endeavors to augment the robustness of offline reinforcement learning (RL) in scenarios laden with heavy-tailed rewards, a prevalent circumstance in real-world applications. We propose two algorithmic frameworks, ROAM and ROOM, for robust off-policy evaluation (OPE) and offline policy optimization (OPO), respectively. Central to our frameworks is the strategic incorporation of the median-of-means method with offline RL, enabling straightforward uncertainty estimation for the value function estimator. This not only adheres to the principle of pessimism in OPO but also adeptly manages heavy-tailed rewards. Theoretical results and extensive experiments demonstrate that our two frameworks outperform existing methods on the logged dataset exhibits heavy-tailed reward distributions.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23494;&#38598;&#39044;&#35757;&#32451;Transformer&#26159;&#21542;&#20197;&#21450;&#22914;&#20309;&#20174;&#33258;&#21457;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#20013;&#33719;&#30410;&#12290;</title><link>http://arxiv.org/abs/2310.10908</link><description>&lt;p&gt;
&#33258;&#21457;&#24615;&#27169;&#22359;&#21270;&#32467;&#26500;&#65306;&#23494;&#38598;&#39044;&#35757;&#32451;Transformer&#33021;&#21542;&#20174;&#33258;&#21457;&#27169;&#22359;&#21270;&#32467;&#26500;&#20013;&#33719;&#30410;&#65311;
&lt;/p&gt;
&lt;p&gt;
Emergent Mixture-of-Experts: Can Dense Pre-trained Transformers Benefit from Emergent Modular Structures?. (arXiv:2310.10908v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10908
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23494;&#38598;&#39044;&#35757;&#32451;Transformer&#26159;&#21542;&#20197;&#21450;&#22914;&#20309;&#20174;&#33258;&#21457;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#20013;&#33719;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#27169;&#22359;&#21270;&#35774;&#35745;&#24341;&#20837;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#23637;&#31034;&#20986;&#36739;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#23398;&#20064;&#25928;&#29575;&#31561;&#20248;&#28857;&#12290;&#29616;&#26377;&#30340;&#27169;&#22359;&#21270;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#26159;&#8220;&#26174;&#24335;&#8221;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#27169;&#22359;&#21270;&#26550;&#26500;&#26159;&#39044;&#20808;&#23450;&#20041;&#30340;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#34987;&#26399;&#26395;&#23454;&#29616;&#19981;&#21516;&#30340;&#21151;&#33021;&#12290;&#30456;&#21453;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#22312;&#26631;&#20934;&#30340;&#39044;&#35757;&#32451;Transformer&#20013;&#23384;&#22312;&#8220;&#38544;&#24335;&#8221;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#65292;&#21363;&#8220;&#33258;&#21457;&#27169;&#22359;&#21270;&#8221;&#12290;&#20182;&#20204;&#34920;&#26126;&#36825;&#26679;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#22312;&#26089;&#26399;&#39044;&#35757;&#32451;&#38454;&#27573;&#23601;&#20250;&#20986;&#29616;&#65292;&#24182;&#19988;&#23436;&#20840;&#26159;&#33258;&#21457;&#30340;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;Transformer&#27169;&#22411;&#20173;&#28982;&#34987;&#35270;&#20026;&#21333;&#20307;&#27169;&#22411;&#65292;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#20854;&#27169;&#22359;&#21270;&#30340;&#29305;&#24615;&#12290;&#22240;&#27492;&#65292;&#37492;&#20110;&#26174;&#24335;&#27169;&#22359;&#21270;&#26550;&#26500;&#30340;&#20248;&#33391;&#29305;&#24615;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#23494;&#38598;&#39044;&#35757;&#32451;Transformer&#26159;&#21542;&#20197;&#21450;&#22914;&#20309;&#20174;&#33258;&#21457;&#27169;&#22359;&#21270;&#32467;&#26500;&#20013;&#33719;&#30410;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incorporating modular designs into neural networks demonstrates superior out-of-generalization, learning efficiency, etc. Existing modular neural networks are generally $\textit{explicit}$ because their modular architectures are pre-defined, and individual modules are expected to implement distinct functions. Conversely, recent works reveal that there exist $\textit{implicit}$ modular structures in standard pre-trained transformers, namely $\textit{Emergent Modularity}$. They indicate that such modular structures exhibit during the early pre-training phase and are totally spontaneous. However, most transformers are still treated as monolithic models with their modular natures underutilized. Therefore, given the excellent properties of explicit modular architecture, we explore $\textit{whether and how dense pre-trained transformers can benefit from emergent modular structures.}$ To study this question, we construct \textbf{E}mergent $\textbf{M}$ixture-$\textbf{o}$f-$\textbf{E}$xperts (E
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;&#32467;&#26500;&#21270;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09518</link><description>&lt;p&gt;
&#20154;&#31867;&#35838;&#31243;&#25351;&#23548;&#19979;&#30340;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning with Human Curriculum. (arXiv:2310.09518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;&#32467;&#26500;&#21270;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#30340;&#20027;&#27969;&#33539;&#24335;&#26159;&#38543;&#26426;&#27927;&#29260;&#35757;&#32451;&#26368;&#22823;&#22810;&#26679;&#21270;&#25351;&#20196;-&#21709;&#24212;&#23545;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#24403;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;ChatGPT&#21644;GPT-4&#20013;&#24212;&#29992;&#32467;&#26500;&#21270;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#19982;&#20197;&#24448;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#25351;&#20196;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#27169;&#25311;&#20102;&#20154;&#31867;&#25945;&#32946;&#30340;&#28176;&#36827;&#24615;&#21644;&#26377;&#32452;&#32455;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#25968;&#25454;&#38598;&#19982;&#25945;&#32946;&#26694;&#26550;&#23545;&#40784;&#26469;&#31574;&#21010;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#20026;&#27599;&#20010;&#26679;&#26412;&#21253;&#25324;&#20027;&#39064;&#21644;&#35748;&#30693;&#20005;&#35880;&#31243;&#24230;&#31561;&#20803;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#20174;&#20013;&#23398;&#21040;&#30740;&#31350;&#29983;&#38454;&#27573;&#30340;&#20840;&#38754;&#32454;&#31890;&#24230;&#20027;&#39064;&#65292;&#27599;&#20010;&#20027;&#39064;&#37117;&#26377;&#21508;&#31181;&#38382;&#39064;&#65292;&#20197;&#21033;&#29992;&#24067;&#40065;&#22982;&#30340;&#35748;&#30693;&#20998;&#32423;&#27861;&#25552;&#39640;&#27010;&#24565;&#28145;&#24230;&#65292;&#35813;&#20998;&#32423;&#27861;&#29992;&#20110;&#21306;&#20998;&#27599;&#20010;&#27010;&#24565;&#30340;&#19981;&#21516;&#20154;&#31867;&#35748;&#30693;&#27700;&#24179;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dominant paradigm for instruction tuning is the random-shuffled training of maximally diverse instruction-response pairs. This paper explores the potential benefits of applying a structured cognitive learning approach to instruction tuning in contemporary large language models like ChatGPT and GPT-4. Unlike the previous conventional randomized instruction dataset, we propose a highly structured synthetic dataset that mimics the progressive and organized nature of human education. We curate our dataset by aligning it with educational frameworks, incorporating meta information including its topic and cognitive rigor level for each sample. Our dataset covers comprehensive fine-grained topics spanning diverse educational stages (from middle school to graduate school) with various questions for each topic to enhance conceptual depth using Bloom's taxonomy-a classification framework distinguishing various levels of human cognition for each concept. The results demonstrate that this cogni
&lt;/p&gt;</description></item><item><title>Mirage&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#33976;&#39311;&#31639;&#27861;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#26356;&#23567;&#30340;&#21512;&#25104;&#35757;&#32451;&#38598;&#65292;&#20197;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#23454;&#29616;&#22270;&#20998;&#31867;&#65292;&#24182;&#20811;&#26381;&#20102;&#29616;&#26377;&#31639;&#27861;&#23545;&#23436;&#25972;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#24615;&#21644;&#23545;&#24314;&#27169;&#27969;&#31243;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.09486</link><description>&lt;p&gt;
Mirage: &#22270;&#20998;&#31867;&#30340;&#27169;&#22411;&#26080;&#20851;&#22270;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Mirage: Model-Agnostic Graph Distillation for Graph Classification. (arXiv:2310.09486v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09486
&lt;/p&gt;
&lt;p&gt;
Mirage&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#33976;&#39311;&#31639;&#27861;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#26356;&#23567;&#30340;&#21512;&#25104;&#35757;&#32451;&#38598;&#65292;&#20197;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#23454;&#29616;&#22270;&#20998;&#31867;&#65292;&#24182;&#20811;&#26381;&#20102;&#29616;&#26377;&#31639;&#27861;&#23545;&#23436;&#25972;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#24615;&#21644;&#23545;&#24314;&#27169;&#27969;&#31243;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GNNs&#21644;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19968;&#26679;&#65292;&#23545;&#25968;&#25454;&#21644;&#35745;&#31639;&#38656;&#27714;&#37327;&#24456;&#22823;&#12290;&#24613;&#38656;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#25193;&#23637;GNN&#30340;&#35757;&#32451;&#65292;&#20197;&#20415;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#20351;&#29992;&#23427;&#20204;&#12290;&#22270;&#33976;&#39311;&#26159;&#20026;&#27492;&#30446;&#30340;&#32780;&#21162;&#21147;&#65292;&#26088;&#22312;&#20174;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#26500;&#24314;&#19968;&#20010;&#26356;&#23567;&#30340;&#21512;&#25104;&#35757;&#32451;&#38598;&#65292;&#32780;&#19981;&#20250;&#26174;&#33879;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#34429;&#28982;&#21021;&#27493;&#24037;&#20316;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#22522;&#20110;&#20004;&#20010;&#20851;&#38190;&#35266;&#23519;&#65306;(1)&#29616;&#26377;&#30340;&#22270;&#33976;&#39311;&#31639;&#27861;&#26412;&#36523;&#20381;&#36182;&#20110;&#20351;&#29992;&#23436;&#25972;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#23601;&#30772;&#22351;&#20102;&#22270;&#33976;&#39311;&#30340;&#21069;&#25552;&#12290;(2)&#33976;&#39311;&#36807;&#31243;&#23545;&#30446;&#26631;GNN&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#20855;&#26377;&#29305;&#24322;&#24615;&#65292;&#22240;&#27492;&#23545;&#24314;&#27169;&#27969;&#31243;&#30340;&#21464;&#21270;&#19981;&#20855;&#22791;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#21517;&#20026;Mirage&#30340;&#22270;&#20998;&#31867;&#33976;&#39311;&#31639;&#27861;&#26469;&#36991;&#20813;&#36825;&#20123;&#38480;&#21046;&#12290;Mirage&#24314;&#31435;&#22312;&#19968;&#20010;&#27934;&#23519;&#30340;&#22522;&#30784;&#19978;&#65292;&#21363;&#19968;&#20010;&#28040;&#24687;&#20256;&#36882;&#30340;GNN&#23558;&#36755;&#20837;&#22270;&#20998;&#35299;&#20026;&#35745;&#31639;&#30340;&#22810;&#37325;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
GNNs, like other deep learning models, are data and computation hungry. There is a pressing need to scale training of GNNs on large datasets to enable their usage on low-resource environments. Graph distillation is an effort in that direction with the aim to construct a smaller synthetic training set from the original training data without significantly compromising model performance. While initial efforts are promising, this work is motivated by two key observations: (1) Existing graph distillation algorithms themselves rely on training with the full dataset, which undermines the very premise of graph distillation. (2) The distillation process is specific to the target GNN architecture and hyper-parameters and thus not robust to changes in the modeling pipeline. We circumvent these limitations by designing a distillation algorithm called Mirage for graph classification. Mirage is built on the insight that a message-passing GNN decomposes the input graph into a multiset of computation 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#20998;&#24067;&#24335;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#25105;&#20204;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#26102;&#37117;&#23384;&#22312;&#22256;&#38590;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#36825;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.09430</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#20998;&#24067;&#24335;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#31995;&#32479;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Systematic Evaluation of Large Language Models on Out-of-Distribution Logical Reasoning Tasks. (arXiv:2310.09430v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09430
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#20998;&#24067;&#24335;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#25105;&#20204;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#26102;&#37117;&#23384;&#22312;&#22256;&#38590;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#36825;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-3.5&#21644;GPT-4&#65292;&#24050;&#32463;&#23558;&#20154;&#24037;&#31995;&#32479;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#21040;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#35780;&#20272;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#31181;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#21517;&#20026;"ReClor-plus"&#12289;"LogiQA-plus"&#21644;"LogiQAv2-plus"&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#37117;&#21253;&#21547;&#19977;&#20010;&#23376;&#38598;&#65306;&#31532;&#19968;&#20010;&#26159;&#36873;&#39033;&#38543;&#26426;&#25171;&#20081;&#65292;&#31532;&#20108;&#20010;&#26159;&#23558;&#27491;&#30830;&#36873;&#39033;&#26367;&#25442;&#20026;"&#27809;&#26377;&#20854;&#20182;&#36873;&#39033;&#26159;&#27491;&#30830;&#30340;"&#65292;&#31532;&#19977;&#20010;&#26159;&#21069;&#20004;&#20010;&#23376;&#38598;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#37492;&#21035;&#21644;&#29983;&#25104;&#22411;&#30340;LLMs&#65292;&#24182;&#34920;&#26126;&#36825;&#20123;&#31616;&#21333;&#30340;&#25216;&#24039;&#26497;&#22823;&#22320;&#38459;&#30861;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22312;&#21407;&#22987;&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#37117;&#24456;&#38590;&#22238;&#31572;&#25105;&#20204;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#25200;&#21160;&#24341;&#20837;&#20219;&#21153;&#21464;&#21270;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as GPT-3.5 and GPT-4, have greatly advanced the performance of artificial systems on various natural language processing tasks to human-like levels. However, their generalisation and robustness to perform logical reasoning remain under-evaluated. To probe this ability, we propose three new logical reasoning datasets named "ReClor-plus", "LogiQA-plus" and "LogiQAv2-plus", each featuring three subsets: the first with randomly shuffled options, the second with the correct choices replaced by "none of the other options are correct", and a combination of the previous two subsets. We carry out experiments on these datasets with both discriminative and generative LLMs and show that these simple tricks greatly hinder the performance of the language models. Despite their superior performance on the original publicly available datasets, we find that all models struggle to answer our newly constructed datasets. We show that introducing task variations by perturb
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#23558;&#35821;&#35328;&#20316;&#20026;&#23548;&#33322;&#30340;&#24863;&#30693;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#29616;&#25104;&#30340;&#35270;&#35273;&#31995;&#32479;&#23558;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#30340;&#35270;&#22270;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#26368;&#20339;&#30340;&#34892;&#21160;&#26469;&#28385;&#36275;&#23548;&#33322;&#25351;&#20196;&#12290;&#26377;&#20004;&#31181;&#29992;&#20363;&#30340;&#23454;&#39564;&#23545;&#36825;&#31181;&#22522;&#20110;&#35821;&#35328;&#30340;&#23548;&#33322;&#26041;&#27861;&#36827;&#34892;&#20102;&#25506;&#32034;&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#36712;&#36857;&#20197;&#24494;&#35843;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#21450;&#27169;&#25311;&#21040;&#23454;&#38469;&#30340;&#36716;&#25442;&#12290;</title><link>http://arxiv.org/abs/2310.07889</link><description>&lt;p&gt;
LangNav: &#35821;&#35328;&#20316;&#20026;&#23548;&#33322;&#30340;&#24863;&#30693;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
LangNav: Language as a Perceptual Representation for Navigation. (arXiv:2310.07889v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07889
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#23558;&#35821;&#35328;&#20316;&#20026;&#23548;&#33322;&#30340;&#24863;&#30693;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#29616;&#25104;&#30340;&#35270;&#35273;&#31995;&#32479;&#23558;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#30340;&#35270;&#22270;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#26368;&#20339;&#30340;&#34892;&#21160;&#26469;&#28385;&#36275;&#23548;&#33322;&#25351;&#20196;&#12290;&#26377;&#20004;&#31181;&#29992;&#20363;&#30340;&#23454;&#39564;&#23545;&#36825;&#31181;&#22522;&#20110;&#35821;&#35328;&#30340;&#23548;&#33322;&#26041;&#27861;&#36827;&#34892;&#20102;&#25506;&#32034;&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#36712;&#36857;&#20197;&#24494;&#35843;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#21450;&#27169;&#25311;&#21040;&#23454;&#38469;&#30340;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#32034;&#23558;&#35821;&#35328;&#20316;&#20026;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#30340;&#24863;&#30693;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#29616;&#25104;&#30340;&#35270;&#35273;&#31995;&#32479;&#65288;&#29992;&#20110;&#22270;&#20687;&#23383;&#24149;&#21644;&#29289;&#20307;&#26816;&#27979;&#65289;&#23558;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#20013;&#20195;&#29702;&#20154;&#30340;&#33258;&#25105;&#20013;&#24515;&#20840;&#26223;&#35270;&#22270;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26681;&#25454;&#24403;&#21069;&#35270;&#22270;&#21644;&#36712;&#36857;&#21382;&#21490;&#36873;&#25321;&#26368;&#20339;&#30340;&#34892;&#21160;&#26469;&#28385;&#36275;&#23548;&#33322;&#25351;&#20196;&#12290;&#19982;&#26631;&#20934;&#35774;&#32622;&#30456;&#27604;&#65292;&#26631;&#20934;&#35774;&#32622;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#20110;&#19982;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#27169;&#22411;&#36830;&#32493;&#35270;&#35273;&#29305;&#24449;&#30452;&#25509;&#37197;&#21512;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#65288;&#31163;&#25955;&#30340;&#65289;&#35821;&#35328;&#20316;&#20026;&#24863;&#30693;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;R2R&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#22522;&#20934;&#27979;&#35797;&#20013;&#25506;&#32034;&#20102;&#20004;&#20010;&#29992;&#20363;&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-4&#65289;&#29983;&#25104;&#21512;&#25104;&#36712;&#36857;&#65292;&#20197;&#20415;&#24494;&#35843;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65307;&#20197;&#21450;&#27169;&#25311;&#21040;&#23454;&#38469;&#30340;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the use of language as a perceptual representation for vision-and-language navigation. Our approach uses off-the-shelf vision systems (for image captioning and object detection) to convert an agent's egocentric panoramic view at each time step into natural language descriptions. We then finetune a pretrained language model to select an action, based on the current view and the trajectory history, that would best fulfill the navigation instructions. In contrast to the standard setup which adapts a pretrained language model to work directly with continuous visual features from pretrained vision models, our approach instead uses (discrete) language as the perceptual representation. We explore two use cases of our language-based navigation (LangNav) approach on the R2R vision-and-language navigation benchmark: generating synthetic trajectories from a prompted large language model (GPT-4) with which to finetune a smaller language model; and sim-to-real transfer where we transfer 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#35266;&#27979;&#24341;&#23548;&#30340;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;OGDM&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#26465;&#20214;&#37492;&#21035;&#22120;&#30340;&#35266;&#27979;&#25152;&#20135;&#29983;&#30340;&#39069;&#22806;&#25439;&#22833;&#39033;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#36127;&#23545;&#25968;&#20284;&#28982;&#20248;&#21270;&#65292;&#22312;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#26377;&#38480;&#30340;&#25512;&#29702;&#38454;&#27573;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2310.04041</link><description>&lt;p&gt;
&#35266;&#27979;&#24341;&#23548;&#30340;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Observation-Guided Diffusion Probabilistic Models. (arXiv:2310.04041v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04041
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#35266;&#27979;&#24341;&#23548;&#30340;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;OGDM&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#26465;&#20214;&#37492;&#21035;&#22120;&#30340;&#35266;&#27979;&#25152;&#20135;&#29983;&#30340;&#39069;&#22806;&#25439;&#22833;&#39033;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#36127;&#23545;&#25968;&#20284;&#28982;&#20248;&#21270;&#65292;&#22312;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#26377;&#38480;&#30340;&#25512;&#29702;&#38454;&#27573;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#31216;&#20026;&#35266;&#27979;&#24341;&#23548;&#30340;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;OGDM&#65289;&#65292;&#23427;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#36136;&#37327;&#25511;&#21046;&#21644;&#24555;&#36895;&#37319;&#26679;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#21407;&#21017;&#24615;&#30340;&#26041;&#24335;&#23558;&#35266;&#27979;&#36807;&#31243;&#30340;&#24341;&#23548;&#19982;&#39532;&#23572;&#21487;&#22827;&#38142;&#30456;&#32467;&#21512;&#65292;&#37325;&#26032;&#24314;&#31435;&#20102;&#35757;&#32451;&#30446;&#26631;&#12290;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#26465;&#20214;&#37492;&#21035;&#22120;&#30340;&#35266;&#27979;&#25152;&#20135;&#29983;&#30340;&#39069;&#22806;&#25439;&#22833;&#39033;&#65292;&#25105;&#20204;&#20351;&#24471;&#20248;&#21270;&#26356;&#20934;&#30830;&#30340;&#36127;&#23545;&#25968;&#20284;&#28982;&#25104;&#20026;&#21487;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#26377;&#38480;&#30340;&#25512;&#29702;&#38454;&#27573;&#12290;&#36825;&#31181;&#31574;&#30053;&#20351;&#24471;&#25105;&#20204;&#30340;&#35757;&#32451;&#26041;&#27861;&#21363;&#20351;&#21482;&#29992;&#20110;&#24494;&#35843;&#36807;&#31243;&#20063;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#19988;&#19982;&#21508;&#31181;&#24555;&#36895;&#25512;&#29702;&#31574;&#30053;&#20860;&#23481;&#65292;&#22240;&#20026;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#23436;&#20840;&#30456;&#21516;&#30340;&#25512;&#29702;&#36807;&#31243;&#20135;&#29983;&#26356;&#22909;&#30340;&#21435;&#22122;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel diffusion model called observation-guided diffusion probabilistic model (OGDM), which effectively addresses the trade-off between quality control and fast sampling. Our approach reestablishes the training objective by integrating the guidance of the observation process with the Markov chain in a principled way. This is achieved by introducing an additional loss term derived from the observation based on the conditional discriminator on noise level, which employs Bernoulli distribution indicating whether its input lies on the (noisy) real manifold or not. This strategy allows us to optimize the more accurate negative log-likelihood induced in the inference stage especially when the number of function evaluations is limited. The proposed training method is also advantageous even when incorporated only into the fine-tuning process, and it is compatible with various fast inference strategies since our method yields better denoising networks using the exactly same inferen
&lt;/p&gt;</description></item><item><title>LightSeq&#26159;&#29992;&#20110;&#38271;&#19978;&#19979;&#25991;&#36716;&#25442;&#22120;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#24207;&#21015;&#32500;&#24230;&#36827;&#34892;&#20998;&#21306;&#65292;&#19982;&#19981;&#21516;&#27880;&#24847;&#21147;&#22836;&#25968;&#37327;&#30340;&#27169;&#22411;&#26550;&#26500;&#20860;&#23481;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#19982;Megatron-LM&#30456;&#27604;&#30340;&#36890;&#20449;&#37327;&#65292;&#21516;&#26102;&#36824;&#23454;&#29616;&#20102;&#36890;&#20449;&#21644;&#35745;&#31639;&#30340;&#37325;&#21472;&#12290;</title><link>http://arxiv.org/abs/2310.03294</link><description>&lt;p&gt;
LightSeq&#65306;&#29992;&#20110;&#38271;&#19978;&#19979;&#25991;&#36716;&#25442;&#22120;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#24207;&#21015;&#32423;&#24182;&#34892;ism
&lt;/p&gt;
&lt;p&gt;
LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers. (arXiv:2310.03294v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03294
&lt;/p&gt;
&lt;p&gt;
LightSeq&#26159;&#29992;&#20110;&#38271;&#19978;&#19979;&#25991;&#36716;&#25442;&#22120;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#24207;&#21015;&#32500;&#24230;&#36827;&#34892;&#20998;&#21306;&#65292;&#19982;&#19981;&#21516;&#27880;&#24847;&#21147;&#22836;&#25968;&#37327;&#30340;&#27169;&#22411;&#26550;&#26500;&#20860;&#23481;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#19982;Megatron-LM&#30456;&#27604;&#30340;&#36890;&#20449;&#37327;&#65292;&#21516;&#26102;&#36824;&#23454;&#29616;&#20102;&#36890;&#20449;&#21644;&#35745;&#31639;&#30340;&#37325;&#21472;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#21152;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#21487;&#20197;&#35299;&#24320;&#22522;&#26412;&#19978;&#26032;&#30340;&#33021;&#21147;&#65292;&#20294;&#20063;&#26174;&#33879;&#22686;&#21152;&#20102;&#35757;&#32451;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;&#20197;&#24448;&#30340;&#27169;&#22411;&#24182;&#34892;&#31995;&#32479;&#65288;&#20363;&#22914;Megatron-LM&#65289;&#23545;&#19981;&#21516;&#30340;&#27880;&#24847;&#21147;&#22836;&#36827;&#34892;&#20998;&#21306;&#21644;&#35745;&#31639;&#65292;&#24182;&#34892;&#22788;&#29702;&#65292;&#23548;&#33268;&#22823;&#37327;&#36890;&#20449;&#37327;&#65292;&#22240;&#27492;&#19981;&#33021;&#22312;&#27880;&#24847;&#21147;&#22836;&#25968;&#37327;&#20043;&#22806;&#25193;&#23637;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#20854;&#37319;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;LightSeq&#65292;&#29992;&#20110;&#38271;&#19978;&#19979;&#25991;LLM&#30340;&#35757;&#32451;&#12290;LightSeq&#20855;&#26377;&#35768;&#22810;&#26174;&#33879;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;LightSeq&#36890;&#36807;&#24207;&#21015;&#32500;&#24230;&#36827;&#34892;&#20998;&#21306;&#65292;&#22240;&#27492;&#23545;&#20110;&#20855;&#26377;&#19981;&#21516;&#27880;&#24847;&#21147;&#22836;&#25968;&#37327;&#30340;&#27169;&#22411;&#26550;&#26500;&#26159;&#19981;&#21487;&#30693;&#30340;&#65292;&#36866;&#29992;&#20110;Multi-Head&#65292;Multi-Query&#21644;Grouped-Query attention&#31561;&#27169;&#22411;&#12290;&#20854;&#27425;&#65292;LightSeq&#19982;Megatron-LM&#30456;&#27604;&#65292;&#22312;&#27969;&#34892;&#30340;LLM&#19978;&#19981;&#20165;&#38656;&#27714;&#23569;&#33267;4.7&#20493;&#30340;&#36890;&#20449;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#23558;&#36890;&#20449;&#19982;&#35745;&#31639;&#37325;&#21472;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#65292;LightSeq&#36824;&#20855;&#26377;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;che
&lt;/p&gt;
&lt;p&gt;
Increasing the context length of large language models (LLMs) unlocks fundamentally new capabilities, but also significantly increases the memory footprints of training. Previous model-parallel systems such as Megatron-LM partition and compute different attention heads in parallel, resulting in large communication volumes, so they cannot scale beyond the number of attention heads, thereby hindering its adoption. In this paper, we introduce a new approach, LightSeq, for long-context LLMs training. LightSeq has many notable advantages. First, LightSeq partitions over the sequence dimension, hence is agnostic to model architectures and readily applicable for models with varying numbers of attention heads, such as Multi-Head, Multi-Query and Grouped-Query attention. Second, LightSeq not only requires up to 4.7x less communication than Megatron-LM on popular LLMs but also overlaps the communication with computation. To further reduce the training time, LightSeq features a novel gradient che
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;SE(3)-Stochastic Flow Matching&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;FoldFlow&#65292;&#21487;&#20197;&#20934;&#30830;&#24314;&#27169;&#34507;&#30333;&#36136;&#20027;&#38142;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#26080;&#38656;&#27169;&#25311;&#35757;&#32451;&#21644;Riemannian&#26368;&#20248;&#20256;&#36755;&#30340;&#32467;&#21512;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#31283;&#23450;&#24615;&#21644;&#24314;&#27169;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02391</link><description>&lt;p&gt;
SE(3)-&#34507;&#30333;&#36136;&#20027;&#38142;&#29983;&#25104;&#20013;&#30340;&#38543;&#26426;&#27969;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
SE(3)-Stochastic Flow Matching for Protein Backbone Generation. (arXiv:2310.02391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02391
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;SE(3)-Stochastic Flow Matching&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;FoldFlow&#65292;&#21487;&#20197;&#20934;&#30830;&#24314;&#27169;&#34507;&#30333;&#36136;&#20027;&#38142;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#26080;&#38656;&#27169;&#25311;&#35757;&#32451;&#21644;Riemannian&#26368;&#20248;&#20256;&#36755;&#30340;&#32467;&#21512;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#31283;&#23450;&#24615;&#21644;&#24314;&#27169;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#19977;&#32500;&#21018;&#20307;&#36816;&#21160;&#65288;&#21363;SE(3)&#32676;&#65289;&#30340;&#27969;&#21305;&#37197;&#33539;&#24335;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#20855;&#26377;&#19981;&#26029;&#22686;&#24378;&#24314;&#27169;&#33021;&#21147;&#30340;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#65306;FoldFlow&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#20027;&#38142;&#30340;&#20934;&#30830;&#24314;&#27169;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FoldFlow-Base&#65292;&#19968;&#31181;&#26080;&#38656;&#27169;&#25311;&#30340;&#23398;&#20064;&#30830;&#23450;&#24615;&#36830;&#32493;&#26102;&#38388;&#21160;&#21147;&#23398;&#21644;&#21305;&#37197;&#19981;&#21464;&#30446;&#26631;&#20998;&#24067;&#30340;&#26041;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;Riemannian&#26368;&#20248;&#20256;&#36755;&#26469;&#21152;&#36895;&#35757;&#32451;&#65292;&#21019;&#24314;&#20102;FoldFlow-OT&#65292;&#20174;&#32780;&#26500;&#24314;&#20102;&#26356;&#31616;&#21333;&#21644;&#31283;&#23450;&#30340;&#27969;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;FoldFlow-SFM&#65292;&#23558;Riemannian&#26368;&#20248;&#20256;&#36755;&#21644;&#26080;&#38656;&#27169;&#25311;&#35757;&#32451;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#23398;&#20064;SE(3)&#19978;&#30340;&#38543;&#26426;&#36830;&#32493;&#26102;&#38388;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#30340;FoldFlow&#29983;&#25104;&#27169;&#22411;&#23478;&#26063;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#20855;&#26377;&#20960;&#20010;&#20851;&#38190;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The computational design of novel protein structures has the potential to impact numerous scientific disciplines greatly. Toward this goal, we introduce $\text{FoldFlow}$ a series of novel generative models of increasing modeling power based on the flow-matching paradigm over $3\text{D}$ rigid motions -i.e. the group $\text{SE(3)}$ -- enabling accurate modeling of protein backbones. We first introduce $\text{FoldFlow-Base}$, a simulation-free approach to learning deterministic continuous-time dynamics and matching invariant target distributions on $\text{SE(3)}$. We next accelerate training by incorporating Riemannian optimal transport to create $\text{FoldFlow-OT}$, leading to the construction of both more simple and stable flows. Finally, we design $\text{FoldFlow-SFM}$ coupling both Riemannian OT and simulation-free training to learn stochastic continuous-time dynamics over $\text{SE(3)}$. Our family of $\text{FoldFlow}$ generative models offer several key advantages over previous
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;CTM&#65289;&#65292;&#23427;&#21487;&#20197;&#21152;&#36895;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#65292;&#21516;&#26102;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#21644;&#21435;&#22122;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#30340;&#32452;&#21512;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.02279</link><description>&lt;p&gt;
&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65306;&#23398;&#20064;&#25193;&#25955;&#30340;&#27010;&#29575;&#27969;ODE&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion. (arXiv:2310.02279v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02279
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;CTM&#65289;&#65292;&#23427;&#21487;&#20197;&#21152;&#36895;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#65292;&#21516;&#26102;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#21644;&#21435;&#22122;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#30340;&#32452;&#21512;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;CM&#65289;&#21152;&#36895;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#37319;&#26679;&#65292;&#20294;&#20197;&#29306;&#29298;&#26679;&#26412;&#36136;&#37327;&#20026;&#20195;&#20215;&#65292;&#32570;&#20047;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#27861;&#26469;&#26435;&#34913;&#36895;&#24230;&#21644;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;CTM&#65289;&#65292;&#23427;&#26159;&#21253;&#25324;CM&#21644;&#22522;&#20110;&#24471;&#20998;&#27169;&#22411;&#22312;&#20869;&#30340;&#27867;&#21270;&#27169;&#22411;&#12290;CTM&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#36755;&#20986;&#24471;&#20998;&#65288;&#21363;&#23545;&#25968;&#23494;&#24230;&#30340;&#26799;&#24230;&#65289;&#65292;&#24182;&#20801;&#35768;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#20219;&#24847;&#21021;&#22987;&#21644;&#26368;&#32456;&#26102;&#38388;&#20043;&#38388;&#36827;&#34892;&#19981;&#21463;&#38480;&#21046;&#30340;&#36941;&#21382;&#27010;&#29575;&#27969;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#12290;CTM&#21033;&#29992;&#23545;&#25239;&#35757;&#32451;&#21644;&#21435;&#22122;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#30340;&#26377;&#25928;&#32452;&#21512;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#22312;CIFAR-10&#65288;FID 1.73&#65289;&#21644;64X64&#20998;&#36776;&#29575;&#30340;ImageNet&#19978;&#23454;&#29616;&#26032;&#30340;&#26368;&#20808;&#36827;FID&#12290;CTM&#36824;&#23454;&#29616;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#37319;&#26679;&#26041;&#26696;&#65292;&#21253;&#25324;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#30340;ODE&#35299;&#20013;&#30340;&#38271;&#36339;&#36291;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consistency Models (CM) (Song et al., 2023) accelerate score-based diffusion model sampling at the cost of sample quality but lack a natural way to trade-off quality for speed. To address this limitation, we propose Consistency Trajectory Model (CTM), a generalization encompassing CM and score-based models as special cases. CTM trains a single neural network that can -- in a single forward pass -- output scores (i.e., gradients of log-density) and enables unrestricted traversal between any initial and final time along the Probability Flow Ordinary Differential Equation (ODE) in a diffusion process. CTM enables the efficient combination of adversarial training and denoising score matching loss to enhance performance and achieves new state-of-the-art FIDs for single-step diffusion model sampling on CIFAR-10 (FID 1.73) and ImageNet at 64X64 resolution (FID 2.06). CTM also enables a new family of sampling schemes, both deterministic and stochastic, involving long jumps along the ODE soluti
&lt;/p&gt;</description></item><item><title>VDC&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#25968;&#25454;&#28165;&#27905;&#22120;&#65292;&#36890;&#36807;&#26816;&#27979;&#22270;&#20687;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#19981;&#19968;&#33268;&#24615;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#20013;&#30340;&#33039;&#26679;&#26412;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16211</link><description>&lt;p&gt;
VDC: &#36890;&#36807;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#19981;&#19968;&#33268;&#24615;&#26816;&#27979;&#33039;&#26679;&#26412;&#30340;&#22810;&#21151;&#33021;&#25968;&#25454;&#28165;&#27905;&#22120;
&lt;/p&gt;
&lt;p&gt;
VDC: Versatile Data Cleanser for Detecting Dirty Samples via Visual-Linguistic Inconsistency. (arXiv:2309.16211v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16211
&lt;/p&gt;
&lt;p&gt;
VDC&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#25968;&#25454;&#28165;&#27905;&#22120;&#65292;&#36890;&#36807;&#26816;&#27979;&#22270;&#20687;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#19981;&#19968;&#33268;&#24615;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#20013;&#30340;&#33039;&#26679;&#26412;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#30340;&#26032;&#27010;&#24565;&#24378;&#35843;&#20102;&#25968;&#25454;&#22312;&#26500;&#24314;AI&#31995;&#32479;&#20013;&#30340;&#20316;&#29992;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#25968;&#25454;&#38598;&#21487;&#33021;&#20250;&#21253;&#21547;&#33039;&#26679;&#26412;&#65292;&#20363;&#22914;&#26469;&#33258;&#21518;&#38376;&#25915;&#20987;&#30340;&#27602;&#26679;&#26412;&#65292;&#20247;&#21253;&#20013;&#30340;&#22122;&#22768;&#26631;&#31614;&#65292;&#29978;&#33267;&#26159;&#23427;&#20204;&#30340;&#28151;&#21512;&#20307;&#12290;&#36825;&#20123;&#33039;&#26679;&#26412;&#30340;&#23384;&#22312;&#20351;&#24471;DNNs&#21464;&#24471;&#33030;&#24369;&#21644;&#19981;&#21487;&#38752;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#33039;&#26679;&#26412;&#20197;&#25552;&#39640;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#21644;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#21482;&#20851;&#27880;&#26816;&#27979;&#27602;&#26679;&#26412;&#25110;&#22122;&#22768;&#26631;&#31614;&#65292;&#24403;&#22788;&#29702;&#26469;&#33258;&#20854;&#20182;&#39046;&#22495;&#30340;&#33039;&#26679;&#26412;&#26102;&#65292;&#24448;&#24448;&#23481;&#26131;&#20986;&#29616;&#24369;&#27867;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#21508;&#31181;&#33039;&#26679;&#26412;&#20043;&#38388;&#30340;&#19968;&#20010;&#20849;&#21516;&#24615;&#26159;&#22270;&#20687;&#21644;&#30456;&#20851;&#26631;&#31614;&#20043;&#38388;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#25429;&#25417;&#27169;&#24577;&#20043;&#38388;&#30340;&#35821;&#20041;&#19981;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#21151;&#33021;&#25968;&#25454;&#28165;&#27905;&#22120;(VDC)&#65292;&#21033;&#29992;&#36328;&#27169;&#24577;&#23545;&#40784;&#21644;&#25512;&#29702;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(MLLM)&#30340;&#36229;&#36807;&#33021;&#21147;&#12290;&#23427;&#30001;...&#65288;&#27492;&#22788;&#30465;&#30053;&#65289;
&lt;/p&gt;
&lt;p&gt;
The role of data in building AI systems has recently been emphasized by the emerging concept of data-centric AI. Unfortunately, in the real-world, datasets may contain dirty samples, such as poisoned samples from backdoor attack, noisy labels in crowdsourcing, and even hybrids of them. The presence of such dirty samples makes the DNNs vunerable and unreliable.Hence, it is critical to detect dirty samples to improve the quality and realiability of dataset. Existing detectors only focus on detecting poisoned samples or noisy labels, that are often prone to weak generalization when dealing with dirty samples from other domains.In this paper, we find a commonality of various dirty samples is visual-linguistic inconsistency between images and associated labels. To capture the semantic inconsistency between modalities, we propose versatile data cleanser (VDC) leveraging the surpassing capabilities of multimodal large language models (MLLM) in cross-modal alignment and reasoning.It consists o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20107;&#23454;&#30693;&#35782;&#25552;&#21319;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30693;&#35782;&#19978;&#19979;&#25991;&#35843;&#20248;&#26694;&#26550;&#26469;&#25913;&#21892;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.14771</link><description>&lt;p&gt;
&#20351;&#29992;&#20107;&#23454;&#30693;&#35782;&#25552;&#21319;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Boosting In-Context Learning with Factual Knowledge. (arXiv:2309.14771v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20107;&#23454;&#30693;&#35782;&#25552;&#21319;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30693;&#35782;&#19978;&#19979;&#25991;&#35843;&#20248;&#26694;&#26550;&#26469;&#25913;&#21892;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#26088;&#22312;&#36890;&#36807;&#20381;&#36182;&#20110;&#23569;&#37327;&#30340;&#35757;&#32451;&#31034;&#20363;&#35299;&#20915;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#65292;&#20174;&#32780;&#28040;&#38500;&#21442;&#25968;&#26356;&#26032;&#30340;&#38656;&#27714;&#65292;&#24182;&#23454;&#29616;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#20107;&#23454;&#30693;&#35782;&#22312;ICL&#30340;&#24615;&#33021;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#21253;&#25324;&#22312;LLM&#20013;&#23398;&#21040;&#30340;&#22266;&#26377;&#30693;&#35782;&#65292;&#20174;&#25152;&#36873;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#24471;&#20986;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#20197;&#21450;LLM&#22312;&#36755;&#20986;&#29983;&#25104;&#20013;&#30340;&#30693;&#35782;&#20559;&#24046;&#12290;&#20026;&#20102;&#21457;&#25381;LLM&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#20013;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#19978;&#19979;&#25991;&#35843;&#20248;&#65288;KICT&#65289;&#26694;&#26550;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;ICL&#30340;&#24615;&#33021;&#65306;1&#65289;&#22312;&#25345;&#32493;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26399;&#38388;&#21521;LLM&#27880;&#20837;&#20107;&#23454;&#30693;&#35782;&#65292;2&#65289;&#35880;&#24910;&#36873;&#25321;&#20855;&#26377;&#39640;&#30693;&#35782;&#30456;&#20851;&#24615;&#30340;&#31034;&#20363;&#65292;3&#65289;&#26681;&#25454;&#20808;&#21069;&#30340;&#30693;&#35782;&#23545;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#26657;&#20934;&#12290;&#25105;&#20204;&#22312;&#33258;&#22238;&#24402;LLM&#65288;&#22914;GPT&#39118;&#26684;&#27169;&#22411;&#65289;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning (ICL) over Large language models (LLMs) aims at solving previously unseen tasks by conditioning on a few training examples, eliminating the need for parameter updates and achieving competitive performance. In this paper, we demonstrate that factual knowledge is imperative for the performance of ICL in three core facets, i.e., the inherent knowledge learned in LLMs, the factual knowledge derived from the selected in-context examples, and the knowledge biases in LLMs for output generation. To unleash the power of LLMs in few-shot learning scenarios, we introduce a novel Knowledgeable In-Context Tuning (KICT) framework to further improve the performance of ICL: 1) injecting factual knowledge to LLMs during continual self-supervised pre-training, 2) judiciously selecting the examples with high knowledge relevance, and 3) calibrating the prediction results based on prior knowledge. We evaluate the proposed approaches on auto-regressive LLMs (e.g., GPT-style models) over 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;FP8&#26684;&#24335;&#22312;&#21518;&#35757;&#32451;&#37327;&#21270;&#20013;&#30340;&#20248;&#21183;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#37327;&#21270;&#24037;&#20316;&#27969;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FP8&#30456;&#23545;&#20110;INT8&#20855;&#26377;&#26356;&#22909;&#30340;&#24037;&#20316;&#36127;&#36733;&#35206;&#30422;&#29575;&#21644;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14592</link><description>&lt;p&gt;
&#20351;&#29992;FP8&#26684;&#24335;&#30340;&#39640;&#25928;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Post-training Quantization with FP8 Formats. (arXiv:2309.14592v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;FP8&#26684;&#24335;&#22312;&#21518;&#35757;&#32451;&#37327;&#21270;&#20013;&#30340;&#20248;&#21183;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#37327;&#21270;&#24037;&#20316;&#27969;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FP8&#30456;&#23545;&#20110;INT8&#20855;&#26377;&#26356;&#22909;&#30340;&#24037;&#20316;&#36127;&#36733;&#35206;&#30422;&#29575;&#21644;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#36827;&#23637;&#65292;&#22914;LLMs&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#23545;&#25913;&#36827;&#30340;&#37327;&#21270;&#26041;&#27861;&#30340;&#38656;&#27714;&#65292;&#20197;&#28385;&#36275;&#36825;&#20123;&#29616;&#20195;&#26550;&#26500;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;FP8&#25968;&#25454;&#26684;&#24335;&#22312;75&#20010;&#19981;&#21516;&#32593;&#32476;&#26550;&#26500;&#19978;&#36827;&#34892;&#21518;&#35757;&#32451;&#37327;&#21270;&#30340;&#20248;&#21183;&#65292;&#36825;&#20123;&#32593;&#32476;&#26550;&#26500;&#28085;&#30422;&#20102;&#22810;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#26426;&#22120;&#32763;&#35793;&#12289;&#35821;&#35328;&#24314;&#27169;&#12289;&#25991;&#26412;&#29983;&#25104;&#12289;&#22270;&#20687;&#20998;&#31867;&#12289;&#29983;&#25104;&#21644;&#20998;&#21106;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;FP8&#34920;&#31034;&#65288;E5M2&#12289;E4M3&#21644;E3M4&#65289;&#65292;&#20197;&#30740;&#31350;&#22312;&#21160;&#24577;&#33539;&#22260;&#21644;&#31934;&#24230;&#20043;&#38388;&#19981;&#21516;&#26435;&#34913;&#31243;&#24230;&#23545;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#24191;&#27867;&#30740;&#31350;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#37327;&#21270;&#24037;&#20316;&#27969;&#31243;&#65292;&#21487;&#20197;&#27010;&#25324;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#32593;&#32476;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;FP8&#26684;&#24335;&#22312;&#22810;&#20010;&#26041;&#38754;&#20248;&#20110;INT8&#65292;&#21253;&#25324;&#24037;&#20316;&#36127;&#36733;&#35206;&#30422;&#29575;&#65288;92.64&#65285;&#23545;65.87&#65285;&#65289;&#12289;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#30340;&#25805;&#20316;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in deep learning methods such as LLMs and Diffusion models have created a need for improved quantization methods that can meet the computational demands of these modern architectures while maintaining accuracy. Towards this goal, we study the advantages of FP8 data formats for post-training quantization across 75 unique network architectures covering a wide range of tasks, including machine translation, language modeling, text generation, image classification, generation, and segmentation. We examine three different FP8 representations (E5M2, E4M3, and E3M4) to study the effects of varying degrees of trade-off between dynamic range and precision on model accuracy. Based on our extensive study, we developed a quantization workflow that generalizes across different network architectures. Our empirical results show that FP8 formats outperform INT8 in multiple aspects, including workload coverage (92.64% vs. 65.87%), model accuracy and suitability for a broader range of ope
&lt;/p&gt;</description></item><item><title>SeaEval&#26159;&#19968;&#20010;&#35780;&#20272;&#22810;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#30740;&#31350;&#20102;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#20197;&#21450;&#23545;&#25991;&#21270;&#23454;&#36341;&#12289;&#32454;&#24494;&#24046;&#21035;&#21644;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;&#37325;&#35201;&#21457;&#29616;&#21253;&#25324;&#27169;&#22411;&#22312;&#32473;&#20986;&#25913;&#20889;&#25351;&#20196;&#26102;&#34892;&#20026;&#21508;&#24322;&#65292;&#21463;&#21040;&#26292;&#38706;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#23545;&#20110;&#35821;&#20041;&#31561;&#20215;&#30340;&#22810;&#35821;&#35328;&#26597;&#35810;&#30340;&#22238;&#31572;&#19981;&#19968;&#33268;&#65292;&#20197;&#21450;&#27169;&#22411;&#22312;&#24773;&#24863;&#30456;&#20851;&#38382;&#39064;&#19978;&#30340;&#19968;&#33268;&#24615;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2309.04766</link><description>&lt;p&gt;
SeaEval&#22810;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65306;&#20174;&#36328;&#35821;&#35328;&#23545;&#40784;&#21040;&#25991;&#21270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment to Cultural Reasoning. (arXiv:2309.04766v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04766
&lt;/p&gt;
&lt;p&gt;
SeaEval&#26159;&#19968;&#20010;&#35780;&#20272;&#22810;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#30740;&#31350;&#20102;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#20197;&#21450;&#23545;&#25991;&#21270;&#23454;&#36341;&#12289;&#32454;&#24494;&#24046;&#21035;&#21644;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;&#37325;&#35201;&#21457;&#29616;&#21253;&#25324;&#27169;&#22411;&#22312;&#32473;&#20986;&#25913;&#20889;&#25351;&#20196;&#26102;&#34892;&#20026;&#21508;&#24322;&#65292;&#21463;&#21040;&#26292;&#38706;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#23545;&#20110;&#35821;&#20041;&#31561;&#20215;&#30340;&#22810;&#35821;&#35328;&#26597;&#35810;&#30340;&#22238;&#31572;&#19981;&#19968;&#33268;&#65292;&#20197;&#21450;&#27169;&#22411;&#22312;&#24773;&#24863;&#30456;&#20851;&#38382;&#39064;&#19978;&#30340;&#19968;&#33268;&#24615;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;SeaEval&#22522;&#20934;&#27979;&#35797;&#12290;&#38500;&#20102;&#34920;&#24449;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#29702;&#35299;&#21644;&#25512;&#29702;&#33258;&#28982;&#35821;&#35328;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#23427;&#20204;&#23545;&#25991;&#21270;&#23454;&#36341;&#12289;&#32454;&#24494;&#24046;&#21035;&#21644;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#38500;&#20102;&#26631;&#20934;&#30340;&#20934;&#30830;&#24230;&#25351;&#26631;&#65292;&#25105;&#20204;&#36824;&#35843;&#26597;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#35821;&#20041;&#21644;&#22810;&#35821;&#35328;&#24615;&#32500;&#24230;&#19978;&#30340;&#33030;&#24369;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;&#24320;&#28304;&#21644;&#38381;&#28304;&#27169;&#22411;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#22312;&#32463;&#20856;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12289;&#25512;&#29702;&#21644;&#25991;&#21270;&#29702;&#35299;&#26041;&#38754;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;&#37325;&#35201;&#21457;&#29616;&#21253;&#25324;&#65306;&#65288;1&#65289;&#22823;&#22810;&#25968;&#27169;&#22411;&#22312;&#32473;&#20986;&#25913;&#20889;&#25351;&#20196;&#26102;&#30340;&#34892;&#20026;&#21508;&#24322;&#65307;&#65288;2&#65289;&#35768;&#22810;&#27169;&#22411;&#20173;&#28982;&#21463;&#21040;&#26292;&#38706;&#20559;&#24046;&#30340;&#24433;&#21709;&#65288;&#22914;&#20301;&#32622;&#20559;&#24046;&#12289;&#22823;&#22810;&#25968;&#26631;&#31614;&#20559;&#24046;&#65289;&#65307;&#65288;3&#65289;&#23545;&#20110;&#26681;&#28304;&#20110;&#20107;&#23454;&#12289;&#31185;&#23398;&#21644;&#24120;&#35782;&#30693;&#35782;&#30340;&#38382;&#39064;&#65292;&#39044;&#26399;&#22312;&#35821;&#20041;&#19978;&#31561;&#20215;&#30340;&#22810;&#35821;&#35328;&#26597;&#35810;&#24212;&#35813;&#24471;&#21040;&#19968;&#33268;&#30340;&#22238;&#31572;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#22312;&#36825;&#20123;&#26597;&#35810;&#19978;&#34920;&#29616;&#20986;&#20196;&#20154;&#24847;&#22806;&#30340;&#19981;&#19968;&#33268;&#24615;&#65307;&#65288;4&#65289;&#22810;&#35821;&#35328;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#23545;&#20110;&#24773;&#24863;&#30456;&#20851;&#30340;&#38382;&#39064;&#34920;&#29616;&#20986;&#19981;&#21516;&#31243;&#24230;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SeaEval, a benchmark for multilingual foundation models. In addition to characterizing how these models understand and reason with natural language, we also investigate how well they comprehend cultural practices, nuances, and values. Alongside standard accuracy metrics, we investigate the brittleness of foundation models in the dimensions of semantics and multilinguality. Our analyses span both open-sourced and closed models, leading to empirical results across classic NLP tasks, reasoning, and cultural comprehension. Key findings indicate (1) Most models exhibit varied behavior when given paraphrased instructions. (2) Many models still suffer from exposure bias (e.g., positional bias, majority label bias). (3) For questions rooted in factual, scientific, and commonsense knowledge, consistent responses are expected across multilingual queries that are semantically equivalent. Yet, most models surprisingly demonstrate inconsistent performance on these queries. (4) Multilingu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;OpenAI&#30340;GPT-4&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21457;&#29616;&#22522;&#22240;&#38598;&#21512;&#21151;&#33021;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23427;&#33021;&#26681;&#25454;&#23884;&#20837;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#29983;&#25104;&#19982;Gene Ontology&#20013;&#20855;&#21517;&#22522;&#22240;&#38598;&#21512;&#38750;&#24120;&#30456;&#20284;&#30340;&#21517;&#31216;&#65292;&#21516;&#26102;&#22312;&#22522;&#22240;&#32452;&#23398;&#25968;&#25454;&#20013;&#21457;&#29616;&#30340;&#22522;&#22240;&#38598;&#21512;&#20013;&#65292;GPT-4&#30340;&#21629;&#21517;&#26356;&#20855;&#20449;&#24687;&#37327;&#65292;&#24471;&#21040;&#20102;&#20154;&#24037;&#23457;&#26680;&#30340;&#22522;&#26412;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2309.04019</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21457;&#29616;&#22522;&#22240;&#38598;&#21512;&#21151;&#33021;&#26041;&#38754;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluation of large language models for discovery of gene set function. (arXiv:2309.04019v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;OpenAI&#30340;GPT-4&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21457;&#29616;&#22522;&#22240;&#38598;&#21512;&#21151;&#33021;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23427;&#33021;&#26681;&#25454;&#23884;&#20837;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#29983;&#25104;&#19982;Gene Ontology&#20013;&#20855;&#21517;&#22522;&#22240;&#38598;&#21512;&#38750;&#24120;&#30456;&#20284;&#30340;&#21517;&#31216;&#65292;&#21516;&#26102;&#22312;&#22522;&#22240;&#32452;&#23398;&#25968;&#25454;&#20013;&#21457;&#29616;&#30340;&#22522;&#22240;&#38598;&#21512;&#20013;&#65292;GPT-4&#30340;&#21629;&#21517;&#26356;&#20855;&#20449;&#24687;&#37327;&#65292;&#24471;&#21040;&#20102;&#20154;&#24037;&#23457;&#26680;&#30340;&#22522;&#26412;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#22240;&#38598;&#21512;&#20998;&#26512;&#26159;&#21151;&#33021;&#22522;&#22240;&#32452;&#23398;&#30340;&#37325;&#35201;&#26041;&#27861;&#65292;&#20294;&#23427;&#20381;&#36182;&#20110;&#25163;&#21160;&#21019;&#24314;&#30340;&#22522;&#22240;&#21151;&#33021;&#25968;&#25454;&#24211;&#65292;&#36825;&#20123;&#25968;&#25454;&#24211;&#30340;&#19981;&#23436;&#25972;&#21644;&#19981;&#20855;&#22791;&#29983;&#29289;&#23398;&#19978;&#19979;&#25991;&#30340;&#29305;&#28857;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;OpenAI&#30340;GPT-4&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20174;&#20854;&#23884;&#20837;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#20013;&#21457;&#23637;&#20986;&#26377;&#20851;&#24120;&#35265;&#22522;&#22240;&#21151;&#33021;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;GPT-4&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#29992;&#24635;&#32467;&#20854;&#20849;&#35782;&#21151;&#33021;&#30340;&#21517;&#31216;&#26631;&#35760;&#22522;&#22240;&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#25991;&#26412;&#21644;&#24341;&#25991;&#36827;&#34892;&#35777;&#23454;&#12290;&#22312;&#19982;Gene Ontology&#20013;&#30340;&#20855;&#21517;&#22522;&#22240;&#38598;&#21512;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#26102;&#65292;GPT-4&#22312;50%&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#20102;&#38750;&#24120;&#30456;&#20284;&#30340;&#21517;&#31216;&#65292;&#32780;&#22312;&#22823;&#22810;&#25968;&#20854;&#20182;&#24773;&#20917;&#19979;&#65292;&#21017;&#24674;&#22797;&#20102;&#26356;&#19968;&#33324;&#27010;&#24565;&#30340;&#21517;&#31216;&#12290;&#22312;&#22522;&#22240;&#32452;&#23398;&#25968;&#25454;&#20013;&#21457;&#29616;&#30340;&#22522;&#22240;&#38598;&#21512;&#20013;&#65292;&#19982;&#22522;&#22240;&#38598;&#21512;&#23500;&#38598;&#30456;&#27604;&#65292;GPT-4&#30340;&#21629;&#21517;&#26356;&#20855;&#20449;&#24687;&#37327;&#65292;&#20854;&#25903;&#25345;&#24615;&#38472;&#36848;&#21644;&#24341;&#25991;&#22312;&#20154;&#24037;&#23457;&#26680;&#20013;&#24471;&#21040;&#20102;&#22522;&#26412;&#39564;&#35777;&#12290;&#24555;&#36895;&#32508;&#21512;&#24120;&#35265;&#22522;&#22240;&#21151;&#33021;&#30340;&#33021;&#21147;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26377;&#20215;&#20540;&#30340;&#21151;&#33021;&#22522;&#22240;&#32452;&#23398;&#21161;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gene set analysis is a mainstay of functional genomics, but it relies on manually curated databases of gene functions that are incomplete and unaware of biological context. Here we evaluate the ability of OpenAI's GPT-4, a Large Language Model (LLM), to develop hypotheses about common gene functions from its embedded biomedical knowledge. We created a GPT-4 pipeline to label gene sets with names that summarize their consensus functions, substantiated by analysis text and citations. Benchmarking against named gene sets in the Gene Ontology, GPT-4 generated very similar names in 50% of cases, while in most remaining cases it recovered the name of a more general concept. In gene sets discovered in 'omics data, GPT-4 names were more informative than gene set enrichment, with supporting statements and citations that largely verified in human review. The ability to rapidly synthesize common gene functions positions LLMs as valuable functional genomics assistants.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#32780;&#19981;&#38477;&#20302;&#21487;&#38752;&#24615;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;HELM&#22522;&#20934;&#27979;&#35797;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#21482;&#38656;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#21363;&#21487;&#25913;&#21464;&#39046;&#20808;&#32773;&#65292;&#24182;&#20165;&#38656;&#23569;&#37327;&#31034;&#20363;&#21363;&#21487;&#24471;&#21040;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2308.11696</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Efficient Benchmarking (of Language Models). (arXiv:2308.11696v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#32780;&#19981;&#38477;&#20302;&#21487;&#38752;&#24615;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;HELM&#22522;&#20934;&#27979;&#35797;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#21482;&#38656;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#21363;&#21487;&#25913;&#21464;&#39046;&#20808;&#32773;&#65292;&#24182;&#20165;&#38656;&#23569;&#37327;&#31034;&#20363;&#21363;&#21487;&#24471;&#21040;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#21151;&#33021;&#24615;&#22686;&#21152;&#23548;&#33268;&#20102;&#19968;&#31867;&#20840;&#38754;&#35780;&#20272;&#24191;&#27867;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#20986;&#29616;&#12290;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#19982;&#22823;&#35268;&#27169;&#35745;&#31639;&#25104;&#26412;&#30456;&#20851;&#65292;&#27599;&#20010;&#27169;&#22411;&#38656;&#35201;&#25968;&#21315;&#20010;GPU&#23567;&#26102;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#35780;&#20272;&#25928;&#29575;&#26041;&#38754;&#30340;&#38382;&#39064;&#22312;&#25991;&#29486;&#20013;&#35752;&#35770;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#19981;&#25439;&#23475;&#21487;&#38752;&#24615;&#30340;&#24773;&#20917;&#19979;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#36890;&#36807;&#20351;&#29992;HELM&#22522;&#20934;&#27979;&#35797;&#20316;&#20026;&#31034;&#20363;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#35774;&#35745;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#35745;&#31639;-&#21487;&#38752;&#24615;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#36825;&#20123;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#36890;&#36807;&#20174;&#22522;&#20934;&#27979;&#35797;&#20013;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#65292;&#24403;&#21069;&#22312;HELM&#19978;&#30340;&#39046;&#20808;&#32773;&#21487;&#33021;&#20250;&#25913;&#21464;&#65292;&#24182;&#19988;&#35266;&#23519;&#21040;&#21482;&#38656;&#19968;&#23567;&#37096;&#20998;&#31034;&#20363;&#21363;&#21487;&#33719;&#24471;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing versatility of language models LMs has given rise to a new class of benchmarks that comprehensively assess a broad range of capabilities. Such benchmarks are associated with massive computational costs reaching thousands of GPU hours per model. However the efficiency aspect of these evaluation efforts had raised little discussion in the literature. In this work we present the problem of Efficient Benchmarking namely intelligently reducing the computation costs of LM evaluation without compromising reliability. Using the HELM benchmark as a test case we investigate how different benchmark design choices affect the computation-reliability tradeoff. We propose to evaluate the reliability of such decisions by using a new measure Decision Impact on Reliability DIoR for short. We find for example that the current leader on HELM may change by merely removing a low-ranked model from the benchmark and observe that a handful of examples suffice to obtain the correct benchmark rank
&lt;/p&gt;</description></item><item><title>DiagGPT&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25193;&#23637;&#21040;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#22330;&#26223;&#65292;&#25552;&#20379;&#20102;&#22312;&#22797;&#26434;&#35786;&#26029;&#22330;&#26223;&#20013;&#20027;&#21160;&#25552;&#38382;&#21644;&#24341;&#23548;&#29992;&#25143;&#23436;&#25104;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.08043</link><description>&lt;p&gt;
DiagGPT:&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
DiagGPT: An LLM-based Chatbot with Automatic Topic Management for Task-Oriented Dialogue. (arXiv:2308.08043v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08043
&lt;/p&gt;
&lt;p&gt;
DiagGPT&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25193;&#23637;&#21040;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#22330;&#26223;&#65292;&#25552;&#20379;&#20102;&#22312;&#22797;&#26434;&#35786;&#26029;&#22330;&#26223;&#20013;&#20027;&#21160;&#25552;&#38382;&#21644;&#24341;&#23548;&#29992;&#25143;&#23436;&#25104;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22914;ChatGPT&#27491;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#23637;&#31034;&#20986;&#19982;&#20154;&#31867;&#30456;&#20284;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;AI&#27169;&#22411;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#36741;&#21161;&#20154;&#31867;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;AI&#20316;&#20026;&#32842;&#22825;&#20195;&#29702;&#20154;&#30340;&#37325;&#35201;&#24212;&#29992;&#26159;&#22238;&#31572;&#20154;&#31867;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;&#30446;&#21069;&#30340;LLMs&#22312;&#22238;&#31572;&#19968;&#33324;&#38382;&#39064;&#26041;&#38754;&#24050;&#32463;&#26174;&#31034;&#20986;&#29087;&#32451;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#30340;&#35786;&#26029;&#22330;&#26223;(&#22914;&#27861;&#24459;&#25110;&#21307;&#30103;&#21672;&#35810;)&#20013;&#65292;&#22522;&#26412;&#30340;&#38382;&#31572;&#23545;&#35805;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#20123;&#22330;&#26223;&#36890;&#24120;&#38656;&#35201;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;(TOD)&#65292;&#20854;&#20013;AI&#32842;&#22825;&#20195;&#29702;&#38656;&#35201;&#20027;&#21160;&#25552;&#38382;&#24182;&#24341;&#23548;&#29992;&#25143;&#23436;&#25104;&#29305;&#23450;&#20219;&#21153;&#12290;&#20197;&#21069;&#30340;&#24494;&#35843;&#27169;&#22411;&#22312;TOD&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#24403;&#21069;&#30340;LLMs&#24182;&#26410;&#22266;&#26377;&#36825;&#31181;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DiagGPT (Diagnosis GPT)&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#23427;&#23558;LLMs&#25512;&#24191;&#21040;TOD&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as ChatGPT, are becoming increasingly sophisticated, demonstrating capabilities that closely resemble those of humans. These AI models are playing an essential role in assisting humans with a wide array of tasks in daily life. A significant application of AI is its use as a chat agent, responding to human inquiries across various domains. Current LLMs have shown proficiency in answering general questions. However, basic question-answering dialogue often falls short in complex diagnostic scenarios, such as legal or medical consultations. These scenarios typically necessitate Task-Oriented Dialogue (TOD), wherein an AI chat agent needs to proactively pose questions and guide users towards specific task completion. Previous fine-tuning models have underperformed in TOD, and current LLMs do not inherently possess this capability. In this paper, we introduce DiagGPT (Dialogue in Diagnosis GPT), an innovative method that extends LLMs to TOD scenarios. Our e
&lt;/p&gt;</description></item><item><title>RAVEN&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#33945;&#29305;&#21345;&#27931;&#35821;&#35328;&#24314;&#27169;&#21644;&#21069;&#32512;&#35821;&#35328;&#24314;&#27169;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#34701;&#21512;&#23398;&#20064;&#65292;&#23427;&#33021;&#22815;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#27604;ATLAS&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07922</link><description>&lt;p&gt;
RAVEN&#65306;&#19978;&#19979;&#25991;&#23398;&#20064;&#19982;&#26816;&#32034;&#22686;&#24378;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models. (arXiv:2308.07922v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07922
&lt;/p&gt;
&lt;p&gt;
RAVEN&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#33945;&#29305;&#21345;&#27931;&#35821;&#35328;&#24314;&#27169;&#21644;&#21069;&#32512;&#35821;&#35328;&#24314;&#27169;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#34701;&#21512;&#23398;&#20064;&#65292;&#23427;&#33021;&#22815;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#27604;ATLAS&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#29616;&#26377;&#30340;ATLAS&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#21457;&#29616;&#20854;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#39044;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#23384;&#22312;&#19981;&#21305;&#37197;&#65292;&#20197;&#21450;&#19978;&#19979;&#25991;&#38271;&#24230;&#21463;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RAVEN&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#33945;&#29305;&#21345;&#27931;&#35821;&#35328;&#24314;&#27169;&#21644;&#21069;&#32512;&#35821;&#35328;&#24314;&#27169;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19978;&#19979;&#25991;&#34701;&#21512;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#26356;&#22810;&#19978;&#19979;&#25991;&#31034;&#20363;&#32780;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#25110;&#27169;&#22411;&#20462;&#25913;&#26469;&#25552;&#39640;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;RAVEN&#22312;&#26576;&#20123;&#22330;&#26223;&#19979;&#26126;&#26174;&#20248;&#20110;ATLAS&#65292;&#24182;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#23613;&#31649;&#21442;&#25968;&#25968;&#37327;&#26174;&#33879;&#36739;&#23569;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the in-context learning ability of retrieval-augmented encoder-decoder language models. We first conduct a comprehensive analysis of the state-of-the-art ATLAS model and identify its limitations in in-context learning, primarily due to a mismatch between pretraining and testing, as well as a restricted context length. To address these issues, we propose RAVEN, a model that combines retrieval-augmented masked language modeling and prefix language modeling. We further introduce Fusion-in-Context Learning to enhance the few-shot performance by enabling the model to leverage more in-context examples without requiring additional training or model modifications. Through extensive experiments, we demonstrate that RAVEN significantly outperforms ATLAS and achieves results comparable to the most advanced language models in certain scenarios, despite having substantially fewer parameters. Our work underscores the potential of retrieval-augmented encoder-decoder lang
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20840;&#23616;&#32441;&#29702;&#19982;&#23616;&#37096;&#34917;&#19969;&#20449;&#24687;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32454;&#31890;&#24230;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#12290;&#36890;&#36807;&#25552;&#21462;&#28145;&#23618;&#29305;&#24449;&#21644;&#35745;&#31639;&#22270;&#20687;&#32423;&#32441;&#29702;&#65292;&#23558;&#20004;&#32773;&#30340;&#20248;&#21183;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01813</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19982;&#32441;&#29702;&#30456;&#32467;&#21512;&#30340;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks Fused with Textures for Image Classification. (arXiv:2308.01813v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20840;&#23616;&#32441;&#29702;&#19982;&#23616;&#37096;&#34917;&#19969;&#20449;&#24687;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32454;&#31890;&#24230;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#12290;&#36890;&#36807;&#25552;&#21462;&#28145;&#23618;&#29305;&#24449;&#21644;&#35745;&#31639;&#22270;&#20687;&#32423;&#32441;&#29702;&#65292;&#23558;&#20004;&#32773;&#30340;&#20248;&#21183;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#31890;&#24230;&#22270;&#20687;&#20998;&#31867;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#20122;&#31867;&#21035;&#20043;&#38388;&#35270;&#35273;&#24046;&#24322;&#36739;&#23567;&#65292;&#20294;&#26159;&#31867;&#20869;&#21464;&#21270;&#36739;&#22823;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#35299;&#20915;&#32454;&#31890;&#24230;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#26041;&#27861;&#26469;&#35299;&#20915;&#32454;&#31890;&#24230;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#20840;&#23616;&#32441;&#29702;&#19982;&#22522;&#20110;&#23616;&#37096;&#34917;&#19969;&#20449;&#24687;&#30340;&#29305;&#24449;&#30456;&#32467;&#21512;&#12290;&#31532;&#19968;&#20010;&#27493;&#39588;&#20174;&#21508;&#31181;&#22266;&#23450;&#22823;&#23567;&#30340;&#38750;&#37325;&#21472;&#34917;&#19969;&#20013;&#25552;&#21462;&#28145;&#23618;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#36827;&#34892;&#29305;&#24449;&#32534;&#30721;&#12290;&#21478;&#19968;&#20010;&#27493;&#39588;&#20351;&#29992;&#23616;&#37096;&#20108;&#20540;&#27169;&#24335;&#65288;LBP&#65289;&#22312;&#22810;&#20010;&#23610;&#24230;&#19978;&#35745;&#31639;&#22270;&#20687;&#32423;&#32441;&#29702;&#12290;&#20004;&#20010;&#27969;&#30340;&#20248;&#28857;&#34987;&#25972;&#21512;&#21040;&#34920;&#31034;&#39640;&#25928;&#29305;&#24449;&#21521;&#37327;&#30340;&#26041;&#27861;&#20013;&#65292;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#22312;&#20195;&#34920;&#20154;&#33080;&#12289;&#30382;&#32932;&#30149;&#21464;&#12289;&#39135;&#29289;&#30424;&#23376;&#12289;&#28023;&#27915;&#29983;&#29289;&#31561;&#30340;&#20843;&#20010;&#25968;&#25454;&#38598;&#19978;&#32463;&#36807;&#22235;&#20010;&#26631;&#20934;&#39592;&#24178;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#27979;&#35797;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20998;&#31867;&#20934;&#30830;&#24615;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-grained image classification (FGIC) is a challenging task in computer vision for due to small visual differences among inter-subcategories, but, large intra-class variations. Deep learning methods have achieved remarkable success in solving FGIC. In this paper, we propose a fusion approach to address FGIC by combining global texture with local patch-based information. The first pipeline extracts deep features from various fixed-size non-overlapping patches and encodes features by sequential modelling using the long short-term memory (LSTM). Another path computes image-level textures at multiple scales using the local binary patterns (LBP). The advantages of both streams are integrated to represent an efficient feature vector for image classification. The method is tested on eight datasets representing the human faces, skin lesions, food dishes, marine lives, etc. using four standard backbone CNNs. Our method has attained better classification accuracy over existing methods with no
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;XSTest&#30340;&#27979;&#35797;&#22871;&#20214;&#65292;&#26088;&#22312;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22840;&#22823;&#30340;&#23433;&#20840;&#34892;&#20026;&#12290;&#35813;&#22871;&#20214;&#30001;200&#20010;&#23433;&#20840;&#25552;&#31034;&#32452;&#25104;&#65292;&#28085;&#30422;&#21313;&#31181;&#25552;&#31034;&#31867;&#22411;&#65292;&#26088;&#22312;&#24341;&#20986;&#27169;&#22411;&#30340;&#31995;&#32479;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.01263</link><description>&lt;p&gt;
XSTest: &#29992;&#20110;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22840;&#22823;&#23433;&#20840;&#34892;&#20026;&#30340;&#27979;&#35797;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models. (arXiv:2308.01263v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;XSTest&#30340;&#27979;&#35797;&#22871;&#20214;&#65292;&#26088;&#22312;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22840;&#22823;&#30340;&#23433;&#20840;&#34892;&#20026;&#12290;&#35813;&#22871;&#20214;&#30001;200&#20010;&#23433;&#20840;&#25552;&#31034;&#32452;&#25104;&#65292;&#28085;&#30422;&#21313;&#31181;&#25552;&#31034;&#31867;&#22411;&#65292;&#26088;&#22312;&#24341;&#20986;&#27169;&#22411;&#30340;&#31995;&#32479;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27809;&#26377;&#36866;&#24403;&#30340;&#20445;&#25252;&#25514;&#26045;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24456;&#23481;&#26131;&#36981;&#24490;&#24694;&#24847;&#25351;&#20196;&#24182;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#36825;&#28608;&#21457;&#20102;&#23433;&#20840;&#24037;&#20316;&#65292;&#22914;&#32418;&#38431;&#27979;&#35797;&#21644;&#22823;&#35268;&#27169;&#21453;&#39304;&#23398;&#20064;&#65292;&#26088;&#22312;&#20351;&#27169;&#22411;&#26082;&#26377;&#29992;&#21448;&#26080;&#23475;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#20010;&#30446;&#26631;&#20043;&#38388;&#23384;&#22312;&#19968;&#31181;&#32039;&#24352;&#20851;&#31995;&#65292;&#22240;&#20026;&#26080;&#23475;&#24615;&#35201;&#27714;&#27169;&#22411;&#25298;&#32477;&#36981;&#20174;&#19981;&#23433;&#20840;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26080;&#27861;&#25552;&#20379;&#24110;&#21161;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#35777;&#25454;&#34920;&#26126;&#65292;&#19968;&#20123;&#27169;&#22411;&#21487;&#33021;&#22312;&#24179;&#34913;&#19978;&#23384;&#22312;&#38382;&#39064;&#65292;&#20197;&#33267;&#20110;&#21363;&#20351;&#20351;&#29992;&#31867;&#20284;&#19981;&#23433;&#20840;&#25552;&#31034;&#30340;&#35821;&#35328;&#25110;&#25552;&#21450;&#25935;&#24863;&#20027;&#39064;&#30340;&#26126;&#26174;&#23433;&#20840;&#25552;&#31034;&#20063;&#20250;&#34987;&#25298;&#32477;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;XSTest&#30340;&#26032;&#27979;&#35797;&#22871;&#20214;&#65292;&#20197;&#31995;&#32479;&#21270;&#21644;&#32467;&#26500;&#21270;&#30340;&#26041;&#24335;&#35782;&#21035;&#36825;&#31181;&#22840;&#24352;&#30340;&#23433;&#20840;&#34892;&#20026;&#12290;&#30446;&#21069;&#65292;XSTest&#21253;&#25324;200&#20010;&#23433;&#20840;&#25552;&#31034;&#65292;&#28085;&#30422;&#21313;&#31181;&#25552;&#31034;&#31867;&#22411;&#65292;&#33391;&#22909;&#26657;&#20934;&#30340;&#27169;&#22411;&#19981;&#24212;&#35813;&#25298;&#32477;&#36981;&#24490;&#36825;&#20123;&#25552;&#31034;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;XSTest&#30340;&#21019;&#24314;&#21644;&#32452;&#25104;&#65292;&#24182;&#20351;&#29992;&#27979;&#35797;&#22871;&#20214;&#31361;&#26174;&#31995;&#32479;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Without proper safeguards, large language models will readily follow malicious instructions and generate toxic content. This motivates safety efforts such as red-teaming and large-scale feedback learning, which aim to make models both helpful and harmless. However, there is a tension between these two objectives, since harmlessness requires models to refuse complying with unsafe prompts, and thus not be helpful. Recent anecdotal evidence suggests that some models may have struck a poor balance, so that even clearly safe prompts are refused if they use similar language to unsafe prompts or mention sensitive topics. In this paper, we introduce a new test suite called XSTest to identify such eXaggerated Safety behaviours in a structured and systematic way. In its current form, XSTest comprises 200 safe prompts across ten prompt types that well-calibrated models should not refuse to comply with. We describe XSTest's creation and composition, and use the test suite to highlight systematic f
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#65292;&#32463;&#36807;&#25351;&#23548;&#35843;&#20248;&#30340;&#35821;&#35328;&#27169;&#22411;&#21576;&#29616;&#20986;&#26032;&#20852;&#30340;&#35748;&#30693;&#20559;&#35265;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#21644;&#24320;&#21457;&#26356;&#21487;&#38752;&#21644;&#26080;&#20559;&#30340;&#35821;&#35328;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2308.00225</link><description>&lt;p&gt;
&#34987;&#25351;&#23548;&#30340;&#20559;&#35265;&#65306;&#32463;&#36807;&#25351;&#23548;&#35843;&#20248;&#30340;&#35821;&#35328;&#27169;&#22411;&#21576;&#29616;&#20986;&#26032;&#20852;&#30340;&#35748;&#30693;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias. (arXiv:2308.00225v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00225
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#65292;&#32463;&#36807;&#25351;&#23548;&#35843;&#20248;&#30340;&#35821;&#35328;&#27169;&#22411;&#21576;&#29616;&#20986;&#26032;&#20852;&#30340;&#35748;&#30693;&#20559;&#35265;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#21644;&#24320;&#21457;&#26356;&#21487;&#38752;&#21644;&#26080;&#20559;&#30340;&#35821;&#35328;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25351;&#23548;&#35843;&#20248;&#21644;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#36825;&#20123;&#35843;&#20248;&#26041;&#27861;&#21487;&#20197;&#20351;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#65292;&#20294;&#25105;&#20204;&#25512;&#27979;&#36825;&#20123;&#32463;&#36807;&#35843;&#20248;&#30340;&#27169;&#22411;&#21487;&#33021;&#20250;&#20135;&#29983;&#26356;&#22810;&#38544;&#21547;&#30340;&#35748;&#30693;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#35777;&#25454;&#65292;&#34920;&#26126;&#36825;&#20123;&#32463;&#36807;&#35843;&#20248;&#30340;&#27169;&#22411;&#21576;&#29616;&#20986;&#20808;&#21069;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#19981;&#23384;&#22312;&#25110;&#36739;&#19981;&#26126;&#26174;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#23545;&#19977;&#31181;&#35748;&#30693;&#20559;&#35265;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21253;&#25324;&#30683;&#30462;&#25928;&#24212;&#12289;&#30830;&#23450;&#24615;&#25928;&#24212;&#21644;&#20449;&#24565;&#20559;&#35265;&#65292;&#36825;&#20123;&#20559;&#35265;&#24050;&#34987;&#35777;&#23454;&#23545;&#20154;&#31867;&#30340;&#20915;&#31574;&#21644;&#25512;&#29702;&#26377;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#26174;&#20102;&#36825;&#20123;&#20559;&#35265;&#22312;&#21508;&#31181;&#27169;&#22411;&#20013;&#30340;&#23384;&#22312;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#32463;&#36807;&#25351;&#23548;&#35843;&#20248;&#30340;&#27169;&#22411;&#65292;&#22914;Flan-T5&#12289;GPT3.5&#21644;GPT4&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#29702;&#35299;&#25351;&#23548;&#35843;&#20248;&#30340;LMs&#20013;&#30340;&#35748;&#30693;&#20559;&#35265;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#36825;&#26377;&#21161;&#20110;&#24320;&#21457;&#26356;&#21487;&#38752;&#21644;&#26080;&#20559;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies show that instruction tuning and learning from human feedback improve the abilities of large language models (LMs) dramatically. While these tuning methods can make models generate high-quality text, we conjecture that more implicit cognitive biases may arise in these fine-tuned models. Our work provides evidence that these fine-tuned models exhibit biases that were absent or less pronounced in their pretrained predecessors. We examine the extent of this phenomenon in three cognitive biases - the decoy effect, the certainty effect, and the belief bias - all of which are known to influence human decision-making and reasoning. Our findings highlight the presence of these biases in various models, especially those that have undergone instruction tuning, such as Flan-T5, GPT3.5, and GPT4. This research constitutes a step toward comprehending cognitive biases in instruction-tuned LMs, which is crucial for the development of more reliable and unbiased language models.
&lt;/p&gt;</description></item><item><title>SAS&#35270;&#39057;&#38382;&#31572;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#35299;&#20915;&#20102;&#35270;&#39057;&#38382;&#31572;&#20013;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;</title><link>http://arxiv.org/abs/2307.04192</link><description>&lt;p&gt;
SAS&#35270;&#39057;&#38382;&#31572;&#65306;&#33258;&#36866;&#24212;&#37319;&#26679;&#29992;&#20110;&#39640;&#25928;&#35270;&#39057;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
SAS Video-QA: Self-Adaptive Sampling for Efficient Video Question-Answering. (arXiv:2307.04192v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04192
&lt;/p&gt;
&lt;p&gt;
SAS&#35270;&#39057;&#38382;&#31572;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#35299;&#20915;&#20102;&#35270;&#39057;&#38382;&#31572;&#20013;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#38382;&#31572;&#26159;&#35270;&#39057;&#29702;&#35299;&#39046;&#22495;&#30340;&#19968;&#39033;&#22522;&#30784;&#20219;&#21153;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(VLMs)&#37197;&#22791;&#20102;&#35270;&#39057;&#21464;&#25442;&#22120;(Video Transformers)&#65292;&#23454;&#29616;&#20102;&#26102;&#38388;&#24314;&#27169;&#24182;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#65292;&#20294;&#20195;&#20215;&#26159;&#24040;&#22823;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#22240;&#27492;&#22312;&#23454;&#26102;&#24212;&#29992;&#22330;&#26223;&#20013;&#36807;&#20110;&#26114;&#36149;&#12290;&#19968;&#31181;&#32463;&#27982;&#30340;&#35299;&#20915;&#26041;&#27861;&#26159;&#21482;&#23545;&#35270;&#39057;&#30340;&#19968;&#23567;&#37096;&#20998;&#24103;&#36827;&#34892;&#37319;&#26679;&#65292;&#26469;&#20195;&#34920;&#35270;&#39057;&#30340;&#20027;&#35201;&#20869;&#23481;&#65292;&#24182;&#22312;&#36825;&#20123;&#37319;&#26679;&#24103;&#19978;&#35843;&#25972;&#22270;&#20687;-&#25991;&#26412;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#35270;&#39057;&#29702;&#35299;&#27169;&#22411;&#36890;&#24120;&#38543;&#26426;&#37319;&#26679;&#19968;&#32452;&#24103;&#25110;&#29255;&#27573;&#65292;&#32780;&#19981;&#32771;&#34385;&#23427;&#20204;&#30340;&#20869;&#37096;&#20851;&#32852;&#24615;&#21644;&#19982;&#38382;&#39064;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#26080;&#30446;&#26631;&#30340;&#37319;&#26679;&#21487;&#33021;&#20250;&#36951;&#28431;&#21487;&#20197;&#25512;&#23548;&#20986;&#27491;&#30830;&#31572;&#26696;&#30340;&#20851;&#38190;&#24103;&#65292;&#22312;&#37319;&#26679;&#31232;&#30095;&#31243;&#24230;&#22686;&#21152;&#26102;&#65292;&#24773;&#20917;&#20250;&#21464;&#24471;&#26356;&#31967;&#65292;&#32780;&#38543;&#30528;&#35270;&#39057;&#38271;&#24230;&#30340;&#22686;&#21152;&#65292;&#37319;&#26679;&#31232;&#30095;&#31243;&#24230;&#20063;&#20250;&#22686;&#21152;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#24103;&#37319;&#26679;&#31574;&#30053;&#65292;&#20998;&#21035;&#26159;
&lt;/p&gt;
&lt;p&gt;
Video question--answering is a fundamental task in the field of video understanding. Although current vision--language models (VLMs) equipped with Video Transformers have enabled temporal modeling and yielded superior results, they are at the cost of huge computational power and thus too expensive to deploy in real-time application scenarios. An economical workaround only samples a small portion of frames to represent the main content of that video and tune an image--text model on these sampled frames. Recent video understanding models usually randomly sample a set of frames or clips, regardless of internal correlations between their visual contents, nor their relevance to the problem. We argue that such kinds of aimless sampling may omit the key frames from which the correct answer can be deduced, and the situation gets worse when the sampling sparsity increases, which always happens as the video lengths increase. To mitigate this issue, we propose two frame sampling strategies, namel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22686;&#37327;&#38543;&#26426;&#26435;&#37325;&#31070;&#32463;&#32593;&#32476;&#30340;&#26500;&#36896;&#31639;&#27861;&#65292;&#36890;&#36807;&#20960;&#20309;&#20449;&#24687;&#32422;&#26463;&#21644;&#33410;&#28857;&#27744;&#31574;&#30053;&#35299;&#20915;&#20102;&#38590;&#20197;&#35299;&#37322;&#38544;&#34255;&#21442;&#25968;&#19982;&#27531;&#24046;&#35823;&#24046;&#20043;&#38388;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#24314;&#27169;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.00185</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22686;&#37327;&#38543;&#26426;&#26435;&#37325;&#31070;&#32463;&#32593;&#32476;&#21450;&#20854;&#24212;&#29992;&#30340;&#26500;&#36896;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Interpretable Constructive Algorithm for Incremental Random Weight Neural Networks and Its Application. (arXiv:2307.00185v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22686;&#37327;&#38543;&#26426;&#26435;&#37325;&#31070;&#32463;&#32593;&#32476;&#30340;&#26500;&#36896;&#31639;&#27861;&#65292;&#36890;&#36807;&#20960;&#20309;&#20449;&#24687;&#32422;&#26463;&#21644;&#33410;&#28857;&#27744;&#31574;&#30053;&#35299;&#20915;&#20102;&#38590;&#20197;&#35299;&#37322;&#38544;&#34255;&#21442;&#25968;&#19982;&#27531;&#24046;&#35823;&#24046;&#20043;&#38388;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#24314;&#27169;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#37327;&#38543;&#26426;&#26435;&#37325;&#31070;&#32463;&#32593;&#32476;(IRWNNs)&#30001;&#20110;&#26131;&#20110;&#23454;&#29616;&#21644;&#24555;&#36895;&#23398;&#20064;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;IRWNNs&#30340;&#19968;&#20010;&#26174;&#33879;&#32570;&#28857;&#26159;&#38590;&#20197;&#35299;&#37322;&#38544;&#34255;&#21442;&#25968;&#65288;&#33410;&#28857;&#65289;&#19982;&#27531;&#24046;&#35823;&#24046;&#65288;&#27169;&#22411;&#24615;&#33021;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#20960;&#20309;&#20449;&#24687;&#32422;&#26463;&#30340;&#21487;&#35299;&#37322;&#30340;&#26500;&#36896;&#31639;&#27861;(ICA)&#12290;&#39318;&#20808;&#65292;&#22522;&#20110;&#38544;&#34255;&#21442;&#25968;&#19982;&#27531;&#24046;&#35823;&#24046;&#20043;&#38388;&#30340;&#20960;&#20309;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#20960;&#20309;&#20449;&#24687;&#32422;&#26463;&#26469;&#38543;&#26426;&#20998;&#37197;&#38544;&#34255;&#21442;&#25968;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#33410;&#28857;&#27744;&#31574;&#30053;&#33719;&#21462;&#26356;&#26377;&#21033;&#20110;&#25910;&#25947;&#30340;&#38544;&#34255;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#35777;&#26126;&#20102;ICA&#30340;&#36890;&#29992;&#36924;&#36817;&#24615;&#36136;&#12290;&#26368;&#21518;&#65292;&#25552;&#20986;&#20102;ICA&#30340;&#36731;&#37327;&#32423;&#29256;&#26412;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#24314;&#27169;&#20219;&#21153;&#12290;&#22312;&#20845;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incremental random weight neural networks (IRWNNs) have gained attention in view of its easy implementation and fast learning. However, a significant drawback of IRWNNs is that the elationship between the hidden parameters (node)and the residual error (model performance) is difficult to be interpreted. To address the above issue, this article proposes an interpretable constructive algorithm (ICA) with geometric information constraint. First, based on the geometric relationship between the hidden parameters and the residual error, an interpretable geometric information constraint is proposed to randomly assign the hidden parameters. Meanwhile, a node pool strategy is employed to obtain hidden parameters that is more conducive to convergence from hidden parameters satisfying the proposed constraint. Furthermore, the universal approximation property of the ICA is proved. Finally, a lightweight version of ICA is presented for large-scale data modeling tasks. Experimental results on six ben
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36861;&#36394;&#21644;&#24635;&#32467;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#25972;&#12289;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#22810;&#27169;&#24577;&#24605;&#32500;&#38142;&#21644;LLM&#36741;&#21161;&#35270;&#35273;&#25512;&#29702;&#31561;&#24212;&#29992;&#65292;&#25351;&#20986;&#20102;&#29616;&#26377;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.13549</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Multimodal Large Language Models. (arXiv:2306.13549v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36861;&#36394;&#21644;&#24635;&#32467;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#25972;&#12289;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#22810;&#27169;&#24577;&#24605;&#32500;&#38142;&#21644;LLM&#36741;&#21161;&#35270;&#35273;&#25512;&#29702;&#31561;&#24212;&#29992;&#65292;&#25351;&#20986;&#20102;&#29616;&#26377;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#30740;&#31350;&#28909;&#28857;&#65292;&#20351;&#29992;&#24378;&#22823;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22823;&#33041;&#25191;&#34892;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;MLLM &#30340;&#24778;&#20154;&#33021;&#21147;&#65292;&#22914;&#22522;&#20110;&#22270;&#20687;&#32534;&#20889;&#25925;&#20107;&#21644;&#26080;OCR&#25968;&#23398;&#25512;&#29702;&#31561;&#65292;&#22312;&#20256;&#32479;&#26041;&#27861;&#20013;&#24456;&#23569;&#35265;&#65292;&#34920;&#26126;&#20102;&#36890;&#21521;&#20154;&#24037;&#26234;&#33021;&#30340;&#28508;&#22312;&#36335;&#24452;&#12290;&#26412;&#25991;&#26088;&#22312;&#36861;&#36394;&#21644;&#24635;&#32467; MLLM &#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; MLLM &#30340;&#26500;&#25104;&#65292;&#27010;&#36848;&#20102;&#30456;&#20851;&#27010;&#24565;&#12290;&#28982;&#21518;&#65292;&#35752;&#35770;&#20102;&#20851;&#38190;&#25216;&#26415;&#21644;&#24212;&#29992;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#25972;&#65288;M-IT&#65289;&#12289;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;M-ICL&#65289;&#12289;&#22810;&#27169;&#24577;&#24605;&#32500;&#38142;&#65288;M-CoT&#65289;&#21644;LLM&#36741;&#21161;&#35270;&#35273;&#25512;&#29702;&#65288;LAVR&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#29616;&#26377;&#30340;&#25361;&#25112;&#65292;&#24182;&#25351;&#20986;&#20102;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#37492;&#20110; MLLM &#26102;&#20195;&#25165;&#21018;&#21018;&#24320;&#22987;&#65292;&#25105;&#20204;&#20250;&#19981;&#26029;&#26356;&#26032;&#36825;&#20010;&#32508;&#36848;&#65292;&#24182;&#24076;&#26395;&#33021;&#28608;&#21457;&#26356;&#22810;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Large Language Model (MLLM) recently has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional methods, suggesting a potential path to artificial general intelligence. In this paper, we aim to trace and summarize the recent progress of MLLM. First of all, we present the formulation of MLLM and delineate its related concepts. Then, we discuss the key techniques and applications, including Multimodal Instruction Tuning (M-IT), Multimodal In-Context Learning (M-ICL), Multimodal Chain of Thought (M-CoT), and LLM-Aided Visual Reasoning (LAVR). Finally, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#21517;&#20026;&#22522;&#20110;&#23376;&#22270;&#32593;&#32476;&#30340;&#23545;&#27604;&#23398;&#20064;(SGNCL)&#65292;&#36890;&#36807;&#24212;&#29992;&#23376;&#22270;&#32593;&#32476;&#29983;&#25104;&#31574;&#30053;&#20197;&#20135;&#29983;&#22686;&#24378;&#35270;&#22270;&#65292;&#24182;&#25506;&#31350;&#20102;&#23376;&#32467;&#26500;&#30456;&#20114;&#20316;&#29992;&#23545;&#22270;&#24418;&#34920;&#31034;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.03506</link><description>&lt;p&gt;
&#22522;&#20110;&#23376;&#22270;&#32593;&#32476;&#30340;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Subgraph Networks Based Contrastive Learning. (arXiv:2306.03506v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#21517;&#20026;&#22522;&#20110;&#23376;&#22270;&#32593;&#32476;&#30340;&#23545;&#27604;&#23398;&#20064;(SGNCL)&#65292;&#36890;&#36807;&#24212;&#29992;&#23376;&#22270;&#32593;&#32476;&#29983;&#25104;&#31574;&#30053;&#20197;&#20135;&#29983;&#22686;&#24378;&#35270;&#22270;&#65292;&#24182;&#25506;&#31350;&#20102;&#23376;&#32467;&#26500;&#30456;&#20114;&#20316;&#29992;&#23545;&#22270;&#24418;&#34920;&#31034;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;(GCL)&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#35299;&#20915;&#27880;&#37322;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290; &#23427;&#22312;&#26410;&#27880;&#37322;&#30340;&#22270;&#24418;&#20013;&#25366;&#25496;&#26174;&#24335;&#29305;&#24449;&#20197;&#29983;&#25104;&#19979;&#28216;&#20219;&#21153;&#30340;&#26377;&#21033;&#22270;&#24418;&#34920;&#31034;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;GCL&#26041;&#27861;&#20391;&#37325;&#20110;&#22270;&#24418;&#22686;&#24378;&#31574;&#30053;&#21644;&#30456;&#20114;&#20449;&#24687;&#20272;&#35745;&#25805;&#20316;&#30340;&#35774;&#35745;&#12290; &#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#27809;&#26377;&#32771;&#34385;&#23376;&#22270;&#20013;&#23384;&#22312;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20026;&#20102;&#25506;&#32034;&#23376;&#32467;&#26500;&#30456;&#20114;&#20316;&#29992;&#23545;&#22270;&#24418;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;subgraph network-based contrastive learning (SGNCL)&#30340;&#26032;&#26694;&#26550;&#12290;SGNCL&#24212;&#29992;&#23376;&#22270;&#32593;&#32476;&#29983;&#25104;&#31574;&#30053;&#20197;&#20135;&#29983;&#22686;&#24378;&#35270;&#22270;&#12290;&#35813;&#31574;&#30053;&#23558;&#21407;&#22987;&#22270;&#36716;&#25442;&#20026;&#20855;&#26377;&#25299;&#25169;&#21644;&#23646;&#24615;&#29305;&#24449;&#30340;&#36793;&#21040;&#33410;&#28857;&#26144;&#23556;&#32593;&#32476;&#12290;&#21333;&#27425;&#22686;&#24378;&#35270;&#22270;&#26159;
&lt;/p&gt;
&lt;p&gt;
Graph contrastive learning (GCL), as a self-supervised learning method, can solve the problem of annotated data scarcity. It mines explicit features in unannotated graphs to generate favorable graph representations for downstream tasks. Most existing GCL methods focus on the design of graph augmentation strategies and mutual information estimation operations. Graph augmentation produces augmented views by graph perturbations. These views preserve a locally similar structure and exploit explicit features. However, these methods have not considered the interaction existing in subgraphs. To explore the impact of substructure interactions on graph representations, we propose a novel framework called subgraph network-based contrastive learning (SGNCL). SGNCL applies a subgraph network generation strategy to produce augmented views. This strategy converts the original graph into an Edge-to-Node mapping network with both topological and attribute features. The single-shot augmented view is a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;Text-to-SQL&#27169;&#22411;SQL-PaLM&#65292;&#20351;&#29992;&#20102;&#38754;&#21521;Text-to-SQL&#30340;&#22522;&#20110;&#25191;&#34892;&#30340;&#33258;&#19968;&#33268;&#25552;&#31034;&#26041;&#27861;&#65292;&#22312;Spider&#19978;&#23454;&#29616;&#20102;77.3%&#30340;&#27979;&#35797;&#22871;&#20214;&#20934;&#30830;&#24230;&#65292;&#24182;&#26174;&#30528;&#36229;&#36234;&#20197;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.00739</link><description>&lt;p&gt;
SQL-PaLM&#65306;&#38024;&#23545;Text-to-SQL&#30340;&#25913;&#36827;&#22823;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
SQL-PaLM: Improved Large Language Model Adaptation for Text-to-SQL. (arXiv:2306.00739v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;Text-to-SQL&#27169;&#22411;SQL-PaLM&#65292;&#20351;&#29992;&#20102;&#38754;&#21521;Text-to-SQL&#30340;&#22522;&#20110;&#25191;&#34892;&#30340;&#33258;&#19968;&#33268;&#25552;&#31034;&#26041;&#27861;&#65292;&#22312;Spider&#19978;&#23454;&#29616;&#20102;77.3%&#30340;&#27979;&#35797;&#22871;&#20214;&#20934;&#30830;&#24230;&#65292;&#24182;&#26174;&#30528;&#36229;&#36234;&#20197;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19968;&#20010;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26032;&#20852;&#21151;&#33021;&#26159;&#29983;&#25104;&#20195;&#30721;&#65292;&#21253;&#25324;&#29992;&#20110;&#25968;&#25454;&#24211;&#30340;&#32467;&#26500;&#21270;&#26597;&#35810;&#35821;&#35328;&#65288;SQL&#65289;&#12290;&#23545;&#20110;&#23558;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#36716;&#25442;&#20026;SQL&#26597;&#35810;&#30340;&#20219;&#21153;&#65292;&#21363;Text-to-SQL&#65292;LLMs&#30340;&#36866;&#24212;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#20351;&#29992;&#30340;&#36866;&#24212;&#24615;&#25968;&#25454;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;Text-to-SQL&#27169;&#22411;SQL-PaLM&#65292;&#21033;&#29992;&#20102;PaLM-2&#65292;&#25512;&#21160;&#20102;&#20004;&#31181;&#35774;&#32622;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;Few-shot SQL-PaLM&#22522;&#20110;&#38754;&#21521;Text-to-SQL&#30340;&#22522;&#20110;&#25191;&#34892;&#30340;&#33258;&#19968;&#33268;&#25552;&#31034;&#26041;&#27861;&#65292;&#21487;&#22312;Spider&#19978;&#23454;&#29616;77.3%&#30340;&#27979;&#35797;&#22871;&#20214;&#20934;&#30830;&#24230;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36890;&#36807;&#26174;&#30528;&#36739;&#22823;&#30340;&#24494;&#35843;&#36229;&#36234;&#20197;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;SQL-PALM&#21487;&#36827;&#19968;&#27493;&#25552;&#39640;1%&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#23558;SQL-PaLM&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;&#20854;&#23545;&#20854;&#20182;&#25361;&#25112;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
One impressive emergent capability of large language models (LLMs) is generation of code, including Structured Query Language (SQL) for databases. For the task of converting natural language text to SQL queries, Text-to-SQL, adaptation of LLMs is of paramount importance, both in in-context learning and fine-tuning settings, depending on the amount of adaptation data used. In this paper, we propose an LLM-based Text-to-SQL model SQL-PaLM, leveraging on PaLM-2, that pushes the state-of-the-art in both settings. Few-shot SQL-PaLM is based on an execution-based self-consistency prompting approach designed for Text-to-SQL, and achieves 77.3% in test-suite accuracy on Spider, which to our best knowledge is the first to outperform previous state-of-the-art with fine-tuning by a significant margin, 4%. Furthermore, we demonstrate that the fine-tuned SQL-PALM outperforms it further by another 1%. Towards applying SQL-PaLM to real-world scenarios we further evaluate its robustness on other chall
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#38899;&#19978;&#19979;&#25991;&#23545;&#30495;&#23454;&#35828;&#35805;&#20154;&#33080;&#29983;&#25104;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;Context-Aware Lip-Sync&#26694;&#26550;&#65288;CALS&#65289;&#65292;&#21487;&#21033;&#29992;&#35821;&#38899;&#19978;&#19979;&#25991;&#29983;&#25104;&#26356;&#21152;&#20934;&#30830;&#12289;&#31283;&#23450;&#30340;&#21767;&#37096;&#36816;&#21160;&#12290;</title><link>http://arxiv.org/abs/2305.19556</link><description>&lt;p&gt;
&#25506;&#32034;&#21767;&#37096;&#36816;&#21160;&#30340;&#35821;&#38899;&#19978;&#19979;&#25991;&#23545;&#30495;&#23454;&#35828;&#35805;&#20154;&#33080;&#29983;&#25104;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring Phonetic Context in Lip Movement for Authentic Talking Face Generation. (arXiv:2305.19556v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#38899;&#19978;&#19979;&#25991;&#23545;&#30495;&#23454;&#35828;&#35805;&#20154;&#33080;&#29983;&#25104;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;Context-Aware Lip-Sync&#26694;&#26550;&#65288;CALS&#65289;&#65292;&#21487;&#21033;&#29992;&#35821;&#38899;&#19978;&#19979;&#25991;&#29983;&#25104;&#26356;&#21152;&#20934;&#30830;&#12289;&#31283;&#23450;&#30340;&#21767;&#37096;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35828;&#35805;&#20154;&#33080;&#29983;&#25104;&#26159;&#23558;&#33258;&#28982;&#38754;&#37096;&#19982;&#39537;&#21160;&#38899;&#39057;&#21516;&#27493;&#21512;&#25104;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#22312;&#35270;&#35273;&#36136;&#37327;&#12289;&#21767;&#24418;&#21516;&#27493;&#21644;&#38754;&#37096;&#21160;&#20316;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#20294;&#24403;&#21069;&#30340;&#30740;&#31350;&#20173;&#28982;&#38590;&#20197;&#35299;&#20915;&#31895;&#31961;&#21644;&#24322;&#27493;&#30340;&#21767;&#37096;&#36816;&#21160;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#31867;&#20284;&#26408;&#20598;&#21160;&#30011;&#30340;&#25928;&#26524;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#20197;&#24448;&#30340;&#20316;&#21697;&#36890;&#24120;&#23558;&#21767;&#37096;&#36816;&#21160;&#19982;&#38899;&#39057;&#22312;&#19981;&#21516;&#30340;&#38899;&#32032;&#32423;&#21035;&#19978;&#36827;&#34892;&#30456;&#20851;&#32852;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#38899;&#32032;&#20043;&#38388;&#30340;&#21327;&#21516;&#21457;&#38899;&#65288;co-articulation&#65289;&#29616;&#35937;&#65292;&#21363;&#38548;&#31163;&#30340;&#38899;&#32032;&#21463;&#21069;&#19968;&#20010;&#25110;&#19979;&#19968;&#20010;&#38899;&#32032;&#30340;&#24433;&#21709;&#65292;&#22240;&#27492;&#21516;&#19968;&#20010;&#38899;&#32032;&#30340;&#21457;&#38899;&#22240;&#38899;&#32032;&#19978;&#19979;&#25991;&#32780;&#24322;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#38899;&#32032;&#19978;&#19979;&#25991;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#26356;&#21152;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#23545;&#40784;&#12289;&#31283;&#23450;&#30340;&#21767;&#37096;&#36816;&#21160;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21767;&#37096;&#36816;&#21160;&#20013;&#30340;&#35821;&#38899;&#19978;&#19979;&#25991;&#23545;&#20110;&#30495;&#23454;&#35828;&#35805;&#20154;&#33080;&#29983;&#25104;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Context-Aware Lip-Sync&#26694;&#26550;&#65288;CALS&#65289;&#65292;&#21033;&#29992;&#35821;&#38899;&#19978;&#19979;&#25991;&#29983;&#25104;&#26356;&#21152;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#23545;&#40784;&#12289;&#31283;&#23450;&#30340;&#21767;&#37096;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Talking face generation is the task of synthesizing a natural face synchronous to driving audio. Although much progress has been made in terms of visual quality, lip synchronization, and facial motion of the talking face, current works still struggle to overcome issues of crude and asynchronous lip movement, which can result in puppetry-like animation. We identify that the prior works commonly correlate lip movement with audio at the phone level. However, due to co-articulation, where an isolated phone is influenced by the preceding or following phones, the articulation of a phone varies upon the phonetic context. Therefore, modeling lip motion with the phonetic context can generate more spatio-temporally aligned and stable lip movement. In this respect, we investigate the phonetic context in lip motion for authentic talking face generation. We propose a Context-Aware Lip-Sync framework (CALS), which leverages phonetic context to generate more spatio-temporally aligned and stable lip m
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#36923;&#36753;&#39537;&#21160;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#21407;&#22987;&#25991;&#26412;&#36716;&#25442;&#20026;&#25277;&#35937;&#24847;&#20041;&#34920;&#36848;&#22270;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#36923;&#36753;&#20462;&#25913;&#21644;&#36716;&#25442;&#65292;&#29983;&#25104;&#22686;&#24378;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#20307;&#31995;&#32467;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.12599</link><description>&lt;p&gt;
&#36890;&#36807;&#36923;&#36753;&#39537;&#21160;&#30340;&#25968;&#25454;&#22686;&#24378;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Logical Reasoning of Large Language Models through Logic-Driven Data Augmentation. (arXiv:2305.12599v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#36923;&#36753;&#39537;&#21160;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#21407;&#22987;&#25991;&#26412;&#36716;&#25442;&#20026;&#25277;&#35937;&#24847;&#20041;&#34920;&#36848;&#22270;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#36923;&#36753;&#20462;&#25913;&#21644;&#36716;&#25442;&#65292;&#29983;&#25104;&#22686;&#24378;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#20307;&#31995;&#32467;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#36923;&#36753;&#25512;&#29702;&#30456;&#32467;&#21512;&#21487;&#20197;&#22686;&#24378;&#23427;&#20204;&#22312;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#33021;&#21147;&#65292;&#20351;&#20854;&#26356;&#21152;&#24378;&#22823;&#21644;&#21487;&#38752;&#12290;&#28982;&#32780;&#65292;&#36923;&#36753;&#25512;&#29702;&#30340;&#22797;&#26434;&#24615;&#20351;&#24471;&#20174;&#32593;&#39029;&#19978;&#25910;&#38598;&#21487;&#38752;&#30340;&#25968;&#25454;&#26469;&#24314;&#31435;&#20840;&#38754;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#38754;&#20020;&#22256;&#38590;&#65292;&#36827;&#32780;&#24433;&#21709;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36923;&#36753;&#39537;&#21160;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;AMR-LDA&#12290;AMR-LDA&#23558;&#21407;&#22987;&#25991;&#26412;&#36716;&#25442;&#25104;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#22270;&#65292;&#36825;&#26159;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#35821;&#20041;&#34920;&#31034;&#65292;&#21253;&#21547;&#20102;&#21477;&#23376;&#30340;&#36923;&#36753;&#32467;&#26500;&#65292;&#28982;&#21518;&#23545;&#35813;&#22270;&#36827;&#34892;&#25805;&#20316;&#20197;&#29983;&#25104;&#36923;&#36753;&#20462;&#25913;&#21518;&#30340;AMR&#22270;&#12290;&#20462;&#25913;&#21518;&#30340;AMR&#22270;&#38543;&#21518;&#34987;&#36716;&#25442;&#22238;&#25991;&#26412;&#65292;&#20174;&#32780;&#21019;&#24314;&#22686;&#24378;&#25968;&#25454;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20307;&#31995;&#32467;&#26500;&#26080;&#20851;&#65292;&#24182;&#36890;&#36807;&#25552;&#31034;&#22686;&#24378;&#26469;&#22686;&#24378;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT-3.5&#21644;GPT-4&#65289;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#26469;&#22686;&#24378;&#21028;&#21035;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combining large language models with logical reasoning enhance their capacity to address problems in a robust and reliable manner. Nevertheless, the intricate nature of logical reasoning poses challenges to gathering reliable data from web for building comprehensive training datasets, subsequently affecting the performance on downstream tasks. To address this, we introduce a novel logic-driven data augmentation approach, AMR-LDA. AMR-LDA converts the original text into an Abstract Meaning Representation (AMR) graph, a structured semantic representation that encapsulates the logic structure of the sentence, upon which operations are performed to generate logically modified AMR graphs. The modified AMR graphs are subsequently converted back into texts to create augmented data. Notably, our methodology is architecture-agnostic and enhances generative large language models, such as GPT-3.5 and GPT-4, through prompt augmentation, and fine-tuning discriminative large language models through 
&lt;/p&gt;</description></item><item><title>Tram&#26159;&#19968;&#31181;&#28304;&#20195;&#30721;&#25688;&#35201;&#30340;&#20196;&#29260;&#32423;&#21035;&#26816;&#32034;&#22686;&#24378;&#26426;&#21046;&#65292;&#23427;&#22312;&#35299;&#30721;&#22120;&#31471;&#31934;&#32454;&#26816;&#32034;&#24110;&#21161;&#31070;&#32463;&#27169;&#22411;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#25688;&#35201;&#65292;&#24182;&#22312;Java&#21644;Python&#28304;&#20195;&#30721;&#25688;&#35201;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.11074</link><description>&lt;p&gt;
Tram&#65306;&#19968;&#20010;&#28304;&#20195;&#30721;&#25688;&#35201;&#30340;&#20196;&#29260;&#32423;&#26816;&#32034;&#22686;&#24378;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Tram: A Token-level Retrieval-augmented Mechanism for Source Code Summarization. (arXiv:2305.11074v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11074
&lt;/p&gt;
&lt;p&gt;
Tram&#26159;&#19968;&#31181;&#28304;&#20195;&#30721;&#25688;&#35201;&#30340;&#20196;&#29260;&#32423;&#21035;&#26816;&#32034;&#22686;&#24378;&#26426;&#21046;&#65292;&#23427;&#22312;&#35299;&#30721;&#22120;&#31471;&#31934;&#32454;&#26816;&#32034;&#24110;&#21161;&#31070;&#32463;&#27169;&#22411;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#25688;&#35201;&#65292;&#24182;&#22312;Java&#21644;Python&#28304;&#20195;&#30721;&#25688;&#35201;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#29983;&#25104;&#20154;&#31867;&#21487;&#35835;&#30340;&#25991;&#26412;&#20197;&#25551;&#36848;&#31243;&#24207;&#30340;&#21151;&#33021;&#26159;&#28304;&#20195;&#30721;&#25688;&#35201;&#30340;&#30446;&#26631;&#12290;&#34429;&#28982;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#32467;&#21512;&#31070;&#32463;&#27169;&#22411;&#21644;&#22806;&#37096;&#30693;&#35782;&#30340;&#26032;&#36235;&#21183;&#27491;&#22312;&#20852;&#36215;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#21477;&#23376;&#32423;&#21035;&#30340;&#26816;&#32034;&#21644;&#32452;&#21512;&#33539;&#24335;&#65288;&#26816;&#32034;&#31867;&#20284;&#30340;&#20195;&#30721;&#29255;&#27573;&#24182;&#20351;&#29992;&#30456;&#24212;&#30340;&#20195;&#30721;&#21644;&#25688;&#35201;&#23545;&#26469;&#32534;&#30721;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33539;&#24335;&#26159;&#31895;&#31890;&#24230;&#30340;&#65292;&#19981;&#33021;&#30452;&#25509;&#21033;&#29992;&#35299;&#30721;&#22120;&#31471;&#39640;&#36136;&#37327;&#30340;&#26816;&#32034;&#25688;&#35201;&#20196;&#29260;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#20196;&#29260;&#32423;&#21035;&#26816;&#32034;&#22686;&#24378;&#26426;&#21046;&#65292;&#22312;&#35299;&#30721;&#22120;&#31471;&#24110;&#21161;&#21407;&#22987;&#31070;&#32463;&#27169;&#22411;&#29983;&#25104;&#26356;&#22909;&#30340;&#20195;&#30721;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#32531;&#35299;&#20196;&#29260;&#32423;&#21035;&#26816;&#32034;&#22312;&#25429;&#25417;&#19978;&#19979;&#25991;&#20195;&#30721;&#35821;&#20041;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#20195;&#30721;&#35821;&#20041;&#38598;&#25104;&#21040;&#25688;&#35201;&#20196;&#29260;&#20013;&#12290;&#22823;&#37327;&#30340;&#23454;&#39564;&#21644;&#20154;&#31867;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;Tram&#22312;Java&#21644;Python&#28304;&#20195;&#30721;&#25688;&#35201;&#20219;&#21153;&#19978;&#22343;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically generating human-readable text describing the functionality of a program is the intent of source code summarization. Although Neural Language Models achieve significant performance in this field, an emerging trend is combining neural models with external knowledge. Most previous approaches rely on the sentence-level retrieval and combination paradigm (retrieval of similar code snippets and use of the corresponding code and summary pairs) on the encoder side. However, this paradigm is coarse-grained and cannot directly take advantage of the high-quality retrieved summary tokens on the decoder side. In this paper, we explore a fine-grained token-level retrieval-augmented mechanism on the decoder side to help the vanilla neural model generate a better code summary. Furthermore, to mitigate the limitation of token-level retrieval on capturing contextual code semantics, we propose to integrate code semantics into summary tokens. Extensive experiments and human evaluation revea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#32806;&#26041;&#27861;&#65292;&#23558;&#22797;&#26434;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#24418;&#24335;&#21270;&#20026;&#19978;&#19979;&#25991;&#30693;&#35782;&#22270;&#25628;&#32034;&#21644;&#25277;&#35937;&#36923;&#36753;&#26597;&#35810;&#25512;&#29702;&#30340;&#32452;&#21512;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#22312;&#22810;&#20010;&#36923;&#36753;&#26597;&#35810;&#32467;&#26500;&#30340;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#37117;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#26356;&#39640;&#22797;&#26434;&#24615;&#30340;&#26597;&#35810;&#20013;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.01157</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#22797;&#26434;&#36923;&#36753;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Complex Logical Reasoning over Knowledge Graphs using Large Language Models. (arXiv:2305.01157v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#32806;&#26041;&#27861;&#65292;&#23558;&#22797;&#26434;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#24418;&#24335;&#21270;&#20026;&#19978;&#19979;&#25991;&#30693;&#35782;&#22270;&#25628;&#32034;&#21644;&#25277;&#35937;&#36923;&#36753;&#26597;&#35810;&#25512;&#29702;&#30340;&#32452;&#21512;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#22312;&#22810;&#20010;&#36923;&#36753;&#26597;&#35810;&#32467;&#26500;&#30340;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#37117;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#26356;&#39640;&#22797;&#26434;&#24615;&#30340;&#26597;&#35810;&#20013;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#25512;&#29702;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23427;&#38656;&#35201;&#23545;&#23454;&#20307;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#22522;&#30784;&#36923;&#36753;&#36827;&#34892;&#28145;&#20837;&#29702;&#35299;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#23398;&#20064;&#20960;&#20309;&#26469;&#23884;&#20837;&#23454;&#20307;&#30340;&#21521;&#37327;&#31354;&#38388;&#36827;&#34892;&#36923;&#36753;&#26597;&#35810;&#25805;&#20316;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#22797;&#26434;&#26597;&#35810;&#21644;&#29305;&#23450;&#25968;&#25454;&#38598;&#34920;&#31034;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#32806;&#26041;&#27861;&#65292;&#31216;&#20026;&#22522;&#20110;&#35821;&#35328;&#24341;&#23548;&#30340;&#30693;&#35782;&#22270;&#35889;&#25277;&#35937;&#25512;&#29702;&#65288;LARK&#65289;&#65292;&#23558;&#22797;&#26434;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#24418;&#24335;&#21270;&#20026;&#19978;&#19979;&#25991;&#30693;&#35782;&#22270;&#25628;&#32034;&#21644;&#25277;&#35937;&#36923;&#36753;&#26597;&#35810;&#25512;&#29702;&#30340;&#32452;&#21512;&#65292;&#20197;&#20998;&#21035;&#21033;&#29992;&#22270;&#24418;&#25552;&#21462;&#31639;&#27861;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#36923;&#36753;&#26597;&#35810;&#32467;&#26500;&#30340;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#27861;&#65292;&#22312;&#26356;&#39640;&#22797;&#26434;&#24615;&#30340;&#26597;&#35810;&#20013;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning over knowledge graphs (KGs) is a challenging task that requires a deep understanding of the complex relationships between entities and the underlying logic of their relations. Current approaches rely on learning geometries to embed entities in vector space for logical query operations, but they suffer from subpar performance on complex queries and dataset-specific representations. In this paper, we propose a novel decoupled approach, Language-guided Abstract Reasoning over Knowledge graphs (LARK), that formulates complex KG reasoning as a combination of contextual KG search and abstract logical query reasoning, to leverage the strengths of graph extraction algorithms and large language models (LLM), respectively. Our experiments demonstrate that the proposed approach outperforms state-of-the-art KG reasoning methods on standard benchmark datasets across several logical query constructs, with significant performance gain for queries of higher complexity. Furthermore, we show t
&lt;/p&gt;</description></item><item><title>HAT&#26159;&#19968;&#31181;&#35745;&#31639;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#20351;&#29992;&#26032;&#39062;&#30340;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#32467;&#26500;&#21644;&#31616;&#21270;&#30340;&#20985;&#35270;&#32593;&#33180;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;&#30446;&#26631;&#23384;&#22312;&#21644;&#30446;&#26631;&#32570;&#22833;&#25628;&#32034;&#26399;&#38388;&#27880;&#35270;&#34892;&#20026;&#30340;&#25195;&#25551;&#36335;&#24452;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2303.09383</link><description>&lt;p&gt;
&#20351;&#29992;&#35745;&#31639;&#27880;&#24847;&#21147;&#39044;&#27979;&#20154;&#31867;&#35270;&#35273;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Predicting Human Attention using Computational Attention. (arXiv:2303.09383v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09383
&lt;/p&gt;
&lt;p&gt;
HAT&#26159;&#19968;&#31181;&#35745;&#31639;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#20351;&#29992;&#26032;&#39062;&#30340;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#32467;&#26500;&#21644;&#31616;&#21270;&#30340;&#20985;&#35270;&#32593;&#33180;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;&#30446;&#26631;&#23384;&#22312;&#21644;&#30446;&#26631;&#32570;&#22833;&#25628;&#32034;&#26399;&#38388;&#27880;&#35270;&#34892;&#20026;&#30340;&#25195;&#25551;&#36335;&#24452;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#35270;&#35273;&#27880;&#24847;&#21147;&#27169;&#22411;&#26088;&#22312;&#39044;&#27979;&#33258;&#19978;&#32780;&#19979;&#25110;&#33258;&#19979;&#32780;&#19978;&#25511;&#21046;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#35270;&#35273;&#25628;&#32034;&#21644;&#33258;&#30001;&#35266;&#30475;&#20219;&#21153;&#36827;&#34892;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#20154;&#31867;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;&#36825;&#20004;&#31181;&#24418;&#24335;&#30340;&#27880;&#24847;&#21147;&#25511;&#21046;&#12290;HAT&#22312;&#39044;&#27979;&#30446;&#26631;&#23384;&#22312;&#21644;&#30446;&#26631;&#32570;&#22833;&#25628;&#32034;&#26399;&#38388;&#36827;&#34892;&#27880;&#35270;&#34892;&#20026;&#30340;&#25195;&#25551;&#36335;&#24452;&#26041;&#38754;&#26159;&#26032;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#22312;&#39044;&#27979;&#26080;&#20219;&#21153;&#33258;&#30001;&#35266;&#30475;&#27880;&#35270;&#36335;&#24452;&#26041;&#38754;&#21305;&#37197;&#25110;&#36229;&#36807;&#20102;&#24403;&#21069;&#25216;&#26415;&#27700;&#24179;&#12290;HAT&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#32467;&#26500;&#21644;&#31616;&#21270;&#30340;&#20985;&#35270;&#32593;&#33180;&#65292;&#20849;&#21516;&#21019;&#24314;&#31867;&#20284;&#20110;&#20154;&#31867;&#21160;&#24577;&#35270;&#35273;&#24037;&#20316;&#35760;&#24518;&#30340;&#26102;&#31354;&#24847;&#35782;&#65292;&#23454;&#29616;&#20102;&#36825;&#31181;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;&#19982;&#20197;&#21069;&#20381;&#36182;&#20110;&#31895;&#31961;&#30340;&#27880;&#35270;&#21333;&#20803;&#26684;&#32593;&#26684;&#24182;&#30001;&#20110;&#31163;&#25955;&#21270;&#22266;&#23450;&#32780;&#32463;&#21382;&#20449;&#24687;&#20002;&#22833;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;HAT&#20855;&#26377;&#23494;&#38598;&#39044;&#27979;&#26550;&#26500;&#65292;&#24182;&#20026;&#27599;&#20010;&#27880;&#35270;&#36755;&#20986;&#23494;&#38598;&#28909;&#22270;&#65292;&#20174;&#32780;&#36991;&#20813;&#31163;&#25955;&#27880;&#35270;&#12290;HAT&#22312;&#35745;&#31639;&#35270;&#35273;&#27880;&#24847;&#21147;&#26041;&#38754;&#35774;&#23450;&#20102;&#19968;&#20010;&#26032;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most models of visual attention are aimed at predicting either top-down or bottom-up control, as studied using different visual search and free-viewing tasks. We propose Human Attention Transformer (HAT), a single model predicting both forms of attention control. HAT is the new state-of-the-art (SOTA) in predicting the scanpath of fixations made during target-present and target-absent search, and matches or exceeds SOTA in the prediction of taskless free-viewing fixation scanpaths. HAT achieves this new SOTA by using a novel transformer-based architecture and a simplified foveated retina that collectively create a spatio-temporal awareness akin to the dynamic visual working memory of humans. Unlike previous methods that rely on a coarse grid of fixation cells and experience information loss due to fixation discretization, HAT features a dense-prediction architecture and outputs a dense heatmap for each fixation, thus avoiding discretizing fixations. HAT sets a new standard in computati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; MAPSeg &#30340;&#26032;&#26694;&#26550;&#65292;&#37319;&#29992; 3D &#33945;&#29256;&#33258;&#32534;&#30721;&#21644;&#20266;&#26631;&#31614;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#36328;&#24180;&#40836;&#12289;&#36328;&#27169;&#24577;&#21644;&#36328;&#22330;&#26223;&#19979;&#23545;&#23156;&#20799;&#33041; MRI &#20013;&#20122;&#30382;&#36136;&#21306;&#22495;&#30340;&#20998;&#21106;&#65292;&#20805;&#20998;&#32771;&#34385;&#19981;&#21516; MRI &#25195;&#25551;&#20202;&#12289;&#20379;&#24212;&#21830;&#25110;&#37319;&#38598;&#24207;&#21015;&#20197;&#21450;&#19981;&#21516;&#30340;&#31070;&#32463;&#21457;&#32946;&#38454;&#27573;&#25152;&#36896;&#25104;&#30340;&#20869;&#22312;&#24322;&#36136;&#24615;&#65292;&#25552;&#39640;&#20102;&#20998;&#21106;&#32467;&#26524;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.09373</link><description>&lt;p&gt;
3D&#33945;&#29256;&#33258;&#32534;&#30721;&#21644;&#20266;&#26631;&#31614;&#29992;&#20110;&#24322;&#26500;&#23156;&#20799;&#33041; MRI &#39046;&#22495;&#38388;&#36866;&#24212;&#24615;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;
3D Masked Autoencoding and Pseudo-labeling for Domain Adaptive Segmentation of Heterogeneous Infant Brain MRI. (arXiv:2303.09373v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; MAPSeg &#30340;&#26032;&#26694;&#26550;&#65292;&#37319;&#29992; 3D &#33945;&#29256;&#33258;&#32534;&#30721;&#21644;&#20266;&#26631;&#31614;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#36328;&#24180;&#40836;&#12289;&#36328;&#27169;&#24577;&#21644;&#36328;&#22330;&#26223;&#19979;&#23545;&#23156;&#20799;&#33041; MRI &#20013;&#20122;&#30382;&#36136;&#21306;&#22495;&#30340;&#20998;&#21106;&#65292;&#20805;&#20998;&#32771;&#34385;&#19981;&#21516; MRI &#25195;&#25551;&#20202;&#12289;&#20379;&#24212;&#21830;&#25110;&#37319;&#38598;&#24207;&#21015;&#20197;&#21450;&#19981;&#21516;&#30340;&#31070;&#32463;&#21457;&#32946;&#38454;&#27573;&#25152;&#36896;&#25104;&#30340;&#20869;&#22312;&#24322;&#36136;&#24615;&#65292;&#25552;&#39640;&#20102;&#20998;&#21106;&#32467;&#26524;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23156;&#20799;&#33041; MRI &#22312;&#36328;&#24180;&#40836;&#12289;&#36328;&#27169;&#24577;&#12289;&#36328;&#22330;&#26223;&#19979;&#23454;&#29616;&#40065;&#26834;&#30340;&#20998;&#21106;&#20173;&#28982;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; MAPSeg &#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992; 3D &#33945;&#29256;&#33258;&#32534;&#30721;&#21644;&#33945;&#29256;&#20266;&#26631;&#31614;&#30340;&#26041;&#24335;&#26469;&#23545;&#23156;&#20799;&#33041;MRI&#30340;&#19981;&#21516;&#20122;&#30382;&#36136;&#21306;&#22495;&#36827;&#34892;&#20998;&#21106;&#65292;&#24182;&#32852;&#21512;&#23398;&#20064;&#26631;&#35760;&#28304;&#22495;&#25968;&#25454;&#21644;&#26410;&#26631;&#35760;&#30446;&#26631;&#22495;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#20998;&#21106;&#32467;&#26524;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust segmentation of infant brain MRI across multiple ages, modalities, and sites remains challenging due to the intrinsic heterogeneity caused by different MRI scanners, vendors, or acquisition sequences, as well as varying stages of neurodevelopment. To address this challenge, previous studies have explored domain adaptation (DA) algorithms from various perspectives, including feature alignment, entropy minimization, contrast synthesis (style transfer), and pseudo-labeling. This paper introduces a novel framework called MAPSeg (Masked Autoencoding and Pseudo-labelling Segmentation) to address the challenges of cross-age, cross-modality, and cross-site segmentation of subcortical regions in infant brain MRI. Utilizing 3D masked autoencoding as well as masked pseudo-labeling, the model is able to jointly learn from labeled source domain data and unlabeled target domain data. We evaluated our framework on expert-annotated datasets acquired from different ages and sites. MAPSeg consist
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#24418;&#29366;&#24341;&#23548;&#25193;&#25955;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#22806;&#37096;&#27880;&#24847;&#26426;&#21046;&#23558;&#24418;&#29366;&#38480;&#21046;&#24212;&#29992;&#20110;&#36328;&#27880;&#24847;&#21147;&#22270;&#21644;&#33258;&#27880;&#24847;&#21147;&#22270;&#19978;&#65292;&#20174;&#32780;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#32771;&#34385;&#21040;&#23545;&#35937;&#24418;&#29366;&#65292;&#36827;&#32780;&#21487;&#20197;&#23454;&#29616;&#23545;&#35937;&#24418;&#29366;&#24544;&#23454;&#24230;&#26356;&#39640;&#30340;&#22270;&#20687;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2212.00210</link><description>&lt;p&gt;
&#24102;&#26377;&#20869;&#22806;&#37096;&#27880;&#24847;&#21147;&#30340;&#24418;&#29366;&#24341;&#23548;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Shape-Guided Diffusion with Inside-Outside Attention. (arXiv:2212.00210v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00210
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#24418;&#29366;&#24341;&#23548;&#25193;&#25955;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#22806;&#37096;&#27880;&#24847;&#26426;&#21046;&#23558;&#24418;&#29366;&#38480;&#21046;&#24212;&#29992;&#20110;&#36328;&#27880;&#24847;&#21147;&#22270;&#21644;&#33258;&#27880;&#24847;&#21147;&#22270;&#19978;&#65292;&#20174;&#32780;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#32771;&#34385;&#21040;&#23545;&#35937;&#24418;&#29366;&#65292;&#36827;&#32780;&#21487;&#20197;&#23454;&#29616;&#23545;&#35937;&#24418;&#29366;&#24544;&#23454;&#24230;&#26356;&#39640;&#30340;&#22270;&#20687;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25805;&#20316;&#23545;&#35937;&#26102;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#36890;&#24120;&#24573;&#30053;&#23545;&#35937;&#30340;&#24418;&#29366;&#24182;&#29983;&#25104;&#38169;&#35823;&#27604;&#20363;&#12289;&#34987;&#25130;&#26029;&#25110;&#34987;&#32972;&#26223;&#20869;&#23481;&#26367;&#25442;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861; Shape-Guided Diffusion&#65292;&#35813;&#26041;&#27861;&#20462;&#25913;&#20102;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#65292;&#20351;&#20043;&#23545;&#29992;&#25143;&#25351;&#23450;&#30340;&#24418;&#29366;&#36755;&#20837;&#25110;&#20174;&#25991;&#26412;&#20013;&#33258;&#21160;&#25512;&#26029;&#30340;&#24418;&#29366;&#25935;&#24863;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#22806;&#37096;&#27880;&#24847;&#26426;&#21046;&#65292;&#22312;&#21453;&#28436;&#21644;&#29983;&#25104;&#36807;&#31243;&#20013;&#23558;&#27492;&#24418;&#29366;&#38480;&#21046;&#24212;&#29992;&#20110;&#36328;&#27880;&#24847;&#21147;&#22270;&#21644;&#33258;&#27880;&#24847;&#21147;&#22270;&#19978;&#12290;&#25105;&#20204;&#30340;&#26426;&#21046;&#25351;&#23450;&#31354;&#38388;&#21306;&#22495;&#26159;&#23545;&#35937;&#65288;&#20869;&#37096;&#65289;&#36824;&#26159;&#32972;&#26223;&#65288;&#22806;&#37096;&#65289;&#65292;&#28982;&#21518;&#23558;&#25991;&#26412;&#25552;&#31034;&#25351;&#23450;&#30340;&#32534;&#36753;&#19982;&#27491;&#30830;&#30340;&#21306;&#22495;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#22312;&#24418;&#29366;&#24341;&#23548;&#32534;&#36753;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20013;&#27169;&#22411;&#24517;&#39035;&#26681;&#25454;&#25991;&#26412;&#25552;&#31034;&#21644;&#23545;&#35937;&#25513;&#30721;&#26367;&#25442;&#23545;&#35937;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#20174; MS-COCO &#34893;&#29983;&#30340; ShapePrompts &#22522;&#20934;&#65292;&#24182;&#22312;&#24418;&#29366;&#24544;&#23454;&#24230;&#26041;&#38754;&#23454;&#29616;&#20102; SOTA &#30340;&#32467;&#26524;&#65292;&#32780;&#19981;&#38656;&#35201;&#38477;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
When manipulating an object, existing text-to-image diffusion models often ignore the shape of the object and generate content that is incorrectly scaled, cut off, or replaced with background content. We propose a training-free method, Shape-Guided Diffusion, that modifies pretrained diffusion models to be sensitive to shape input specified by a user or automatically inferred from text. We use a novel Inside-Outside Attention mechanism during the inversion and generation process to apply this shape constraint to the cross- and self-attention maps. Our mechanism designates which spatial region is the object (inside) vs. background (outside) then associates edits specified by text prompts to the correct region. We demonstrate the efficacy of our method on the shape-guided editing task, where the model must replace an object according to a text prompt and object mask. We curate a new ShapePrompts benchmark derived from MS-COCO and achieve SOTA results in shape faithfulness without a degra
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#35828;&#26381;&#24615;&#20889;&#20316;&#25216;&#24039;&#30340;&#25991;&#26412;&#27573;&#33853;&#36827;&#34892;&#20998;&#31867;&#26469;&#22686;&#21152;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#26032;&#23618;&#27425;&#65292;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#29702;&#30001;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#24120;&#35265;&#35828;&#26381;&#24615;&#20889;&#20316;&#31574;&#30053;&#30340;&#27880;&#37322;&#26041;&#26696;&#21644;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992; RoBERTa &#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2211.05985</link><description>&lt;p&gt;
&#20351;&#29992;&#35828;&#26381;&#24615;&#20889;&#20316;&#31574;&#30053;&#26469;&#35299;&#37322;&#21644;&#26816;&#27979;&#20581;&#24247;&#38169;&#35823;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Using Persuasive Writing Strategies to Explain and Detect Health Misinformation. (arXiv:2211.05985v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#35828;&#26381;&#24615;&#20889;&#20316;&#25216;&#24039;&#30340;&#25991;&#26412;&#27573;&#33853;&#36827;&#34892;&#20998;&#31867;&#26469;&#22686;&#21152;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#26032;&#23618;&#27425;&#65292;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#29702;&#30001;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#24120;&#35265;&#35828;&#26381;&#24615;&#20889;&#20316;&#31574;&#30053;&#30340;&#27880;&#37322;&#26041;&#26696;&#21644;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992; RoBERTa &#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#20551;&#20449;&#24687;&#30340;&#20256;&#25773;&#26159;&#24403;&#20170;&#31038;&#20250;&#30340;&#19968;&#22823;&#38382;&#39064;&#65292;&#35768;&#22810;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#30740;&#31350;&#20154;&#21592;&#27491;&#22312;&#21162;&#21147;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#30001;&#20110;&#27599;&#22825;&#21019;&#36896;&#30340;&#34394;&#20551;&#20449;&#24687;&#25968;&#37327;&#24040;&#22823;&#65292;&#23558;&#27492;&#20219;&#21153;&#30041;&#32473;&#20154;&#24037;&#20107;&#23454;&#26816;&#26597;&#21592;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#30740;&#31350;&#20154;&#21592;&#22810;&#24180;&#26469;&#19968;&#30452;&#33268;&#21147;&#20110;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#65292;&#20294;&#20170;&#22825;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30446;&#26631;&#26159;&#20026;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#28155;&#21152;&#19968;&#20010;&#26032;&#23618;&#27425;&#65307;&#20351;&#29992;&#20855;&#26377;&#35828;&#26381;&#24615;&#20889;&#20316;&#25216;&#24039;&#30340;&#25991;&#26412;&#27573;&#33853;&#36827;&#34892;&#20998;&#31867;&#65292;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#29702;&#30001;&#65292;&#35828;&#26126;&#20026;&#20160;&#20040;&#36825;&#31687;&#25991;&#31456;&#21487;&#20197;&#26631;&#35760;&#20026;&#34394;&#20551;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#35768;&#22810;&#24120;&#35265;&#35828;&#26381;&#24615;&#20889;&#20316;&#31574;&#30053;&#30340;&#26032;&#27880;&#37322;&#26041;&#26696;&#65292;&#20197;&#21450;&#30456;&#24212;&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992; RoBERTa &#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#26469;&#23436;&#25104;&#27492;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#20855;&#26377;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20960;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#32467;&#26524;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The spread of misinformation is a prominent problem in today's society, and many researchers in academia and industry are trying to combat it. Due to the vast amount of misinformation that is created every day, it is unrealistic to leave this task to human fact-checkers. Data scientists and researchers have been working on automated misinformation detection for years, and it is still a challenging problem today. The goal of our research is to add a new level to automated misinformation detection; classifying segments of text with persuasive writing techniques in order to produce interpretable reasoning for why an article can be marked as misinformation. To accomplish this, we present a novel annotation scheme containing many common persuasive writing tactics, along with a dataset with human annotations accordingly. For this task, we make use of a RoBERTa model for text classification, due to its high performance in NLP. We develop several language model-based baselines and present the 
&lt;/p&gt;</description></item><item><title>WaveMix&#26159;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#21487;&#20197;&#22312;&#22810;&#39033;&#20219;&#21153;&#19978;&#36798;&#21040;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;GPU RAM&#65292;&#23454;&#29616;&#26102;&#38388;&#12289;&#25104;&#26412;&#21644;&#33021;&#37327;&#30340;&#33410;&#30465;&#12290;</title><link>http://arxiv.org/abs/2205.14375</link><description>&lt;p&gt;
WaveMix: &#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#20998;&#26512;&#30340;&#36164;&#28304;&#39640;&#25928;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
WaveMix: A Resource-efficient Neural Network for Image Analysis. (arXiv:2205.14375v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14375
&lt;/p&gt;
&lt;p&gt;
WaveMix&#26159;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#21487;&#20197;&#22312;&#22810;&#39033;&#20219;&#21153;&#19978;&#36798;&#21040;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;GPU RAM&#65292;&#23454;&#29616;&#26102;&#38388;&#12289;&#25104;&#26412;&#21644;&#33021;&#37327;&#30340;&#33410;&#30465;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;WaveMix&#8212;&#8212;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#26082;&#20855;&#26377;&#36164;&#28304;&#25928;&#29575;&#24615;&#65292;&#21448;&#20855;&#26377;&#36890;&#29992;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;WaveMix&#32593;&#32476;&#22312;&#22810;&#39033;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#29575;&#65292;&#21253;&#25324;Cityscapes&#20013;&#30340;&#20998;&#21106;&#21644;Places-365&#12289;&#20116;&#20010;EMNIST&#25968;&#25454;&#38598;&#21644;iNAT-mini&#20013;&#30340;&#20998;&#31867;&#65292;&#24182;&#24314;&#31435;&#20102;&#26032;&#30340;&#22522;&#20934;&#12290;&#20196;&#20154;&#24778;&#22855;&#30340;&#26159;&#65292;&#19982;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#27604;&#65292;WaveMix&#32467;&#26500;&#25152;&#38656;&#30340;&#21442;&#25968;&#26356;&#23569;&#12290;&#27492;&#22806;&#65292;&#24403;&#25511;&#21046;&#21442;&#25968;&#25968;&#37327;&#26102;&#65292;WaveMix&#25152;&#38656;&#30340;GPU RAM&#26356;&#23569;&#65292;&#36825;&#24847;&#21619;&#30528;&#33410;&#30465;&#26102;&#38388;&#12289;&#25104;&#26412;&#21644;&#33021;&#37327;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#25910;&#30410;&#65292;&#25105;&#20204;&#22312;WaveMix&#22359;&#20013;&#20351;&#29992;&#20102;&#22810;&#32423;&#20108;&#32500;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#65288;2D-DWT&#65289;&#65292;&#23427;&#20855;&#26377;&#20197;&#19979;&#20248;&#28857;:(1)&#23427;&#22522;&#20110;&#19977;&#31181;&#24378;&#22270;&#20687;&#20808;&#39564;&#26465;&#20214;&#37325;&#26032;&#32452;&#32455;&#31354;&#38388;&#20449;&#24687;&#8212;&#8212;&#23610;&#24230;&#19981;&#21464;&#24615;&#65292;&#20301;&#31227;&#19981;&#21464;&#24615;&#21644;&#36793;&#32536;&#30340;&#31232;&#30095;&#24615;,(2) i
&lt;/p&gt;
&lt;p&gt;
We propose WaveMix -- a novel neural architecture for computer vision that is resource-efficient yet generalizable and scalable. WaveMix networks achieve comparable or better accuracy than the state-of-the-art convolutional neural networks, vision transformers, and token mixers for several tasks, establishing new benchmarks for segmentation on Cityscapes; and for classification on Places-365, five EMNIST datasets, and iNAT-mini. Remarkably, WaveMix architectures require fewer parameters to achieve these benchmarks compared to the previous state-of-the-art. Moreover, when controlled for the number of parameters, WaveMix requires lesser GPU RAM, which translates to savings in time, cost, and energy. To achieve these gains we used multi-level two-dimensional discrete wavelet transform (2D-DWT) in WaveMix blocks, which has the following advantages: (1) It reorganizes spatial information based on three strong image priors -- scale-invariance, shift-invariance, and sparseness of edges, (2) i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20195;&#29702;&#21464;&#37327;&#24754;&#35266;&#31574;&#30053;&#20248;&#21270; (P3O) &#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#36817;&#31471;&#22240;&#26524;&#25512;&#26029;&#26500;&#24314;&#24754;&#35266;&#32622;&#20449;&#21306;&#38388;&#32806;&#21512;&#24207;&#21015;&#35299;&#20915;&#20102;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#28151;&#28102;&#20559;&#24046;&#21644;&#26368;&#20248;&#31574;&#30053;&#19982;&#34892;&#20026;&#31574;&#30053;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2205.13589</link><description>&lt;p&gt;
&#38754;&#23545;&#28151;&#28102;&#22240;&#32032;&#30340;&#24754;&#35266;&#24773;&#32490;&#65306;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#35777;&#26126;&#26377;&#25928;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Pessimism in the Face of Confounders: Provably Efficient Offline Reinforcement Learning in Partially Observable Markov Decision Processes. (arXiv:2205.13589v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20195;&#29702;&#21464;&#37327;&#24754;&#35266;&#31574;&#30053;&#20248;&#21270; (P3O) &#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#36817;&#31471;&#22240;&#26524;&#25512;&#26029;&#26500;&#24314;&#24754;&#35266;&#32622;&#20449;&#21306;&#38388;&#32806;&#21512;&#24207;&#21015;&#35299;&#20915;&#20102;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#28151;&#28102;&#20559;&#24046;&#21644;&#26368;&#20248;&#31574;&#30053;&#19982;&#34892;&#20026;&#31574;&#30053;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#26088;&#22312;&#20174;&#30001;&#34892;&#20026;&#31574;&#30053;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#21487;&#33021;&#21462;&#20915;&#20110;&#28508;&#22312;&#29366;&#24577;&#12290;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#22312;&#28151;&#28102;&#24847;&#20041;&#19978;&#21516;&#26102;&#24433;&#21709;&#34892;&#21160;&#21644;&#35266;&#27979;&#20540;&#65292;&#36825;&#23545;&#20110;&#29616;&#26377;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#35828;&#26159;&#31105;&#27490;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#36817;&#31471;&#22240;&#26524;&#25512;&#26029;&#26500;&#24314;&#30340;&#24754;&#35266;&#32622;&#20449;&#21306;&#38388;&#32806;&#21512;&#24207;&#21015;&#30340;&#20195;&#29702;&#21464;&#37327;&#24754;&#35266;&#31574;&#30053;&#20248;&#21270;&#65288;P3O&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#24191;&#20041;&#20989;&#25968;&#36924;&#36817;&#30340;&#19978;&#19979;&#25991;&#20013;&#35299;&#20915;&#20102;&#28151;&#28102;&#20559;&#24046;&#21644;&#26368;&#20248;&#31574;&#30053;&#19982;&#34892;&#20026;&#31574;&#30053;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#28151;&#28102;&#25968;&#25454;&#38598;&#30340;&#37096;&#20998;&#35206;&#30422;&#20551;&#35774;&#19979;&#65292;P3O&#21487;&#20197;&#23454;&#29616;n^{-1/2}&#30340;&#25910;&#25947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study offline reinforcement learning (RL) in partially observable Markov decision processes. In particular, we aim to learn an optimal policy from a dataset collected by a behavior policy which possibly depends on the latent state. Such a dataset is confounded in the sense that the latent state simultaneously affects the action and the observation, which is prohibitive for existing offline RL algorithms. To this end, we propose the \underline{P}roxy variable \underline{P}essimistic \underline{P}olicy \underline{O}ptimization (\texttt{P3O}) algorithm, which addresses the confounding bias and the distributional shift between the optimal and behavior policies in the context of general function approximation. At the core of \texttt{P3O} is a coupled sequence of pessimistic confidence regions constructed via proximal causal inference, which is formulated as minimax estimation. Under a partial coverage assumption on the confounded dataset, we prove that \texttt{P3O} achieves a $n^{-1/2}$-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#38598;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;GRADCAM&#30340;&#33016;&#37096;X&#20809;&#30142;&#30149;&#22810;&#26631;&#31614;&#35786;&#26029;&#27169;&#22411;&#65292;&#33719;&#24471;&#20102;&#22312;Cardiomegaly&#26465;&#20214;&#19979;&#26368;&#39640;&#30340;AUC&#24471;&#20998;0.896&#65292;&#24182;&#20351;&#29992;&#28909;&#22270;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2202.03583</link><description>&lt;p&gt;
&#22522;&#20110;&#23494;&#38598;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#33016;&#37096;&#30142;&#30149;&#22810;&#26631;&#31614;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-Label Classification of Thoracic Diseases using Dense Convolutional Network on Chest Radiographs. (arXiv:2202.03583v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#38598;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;GRADCAM&#30340;&#33016;&#37096;X&#20809;&#30142;&#30149;&#22810;&#26631;&#31614;&#35786;&#26029;&#27169;&#22411;&#65292;&#33719;&#24471;&#20102;&#22312;Cardiomegaly&#26465;&#20214;&#19979;&#26368;&#39640;&#30340;AUC&#24471;&#20998;0.896&#65292;&#24182;&#20351;&#29992;&#28909;&#22270;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;X&#20809;&#22270;&#20687;&#30149;&#29702;&#35782;&#21035;&#26041;&#27861;&#20381;&#36182;&#20110;&#29087;&#32451;&#30340;&#20154;&#31867;&#35299;&#37322;&#65292;&#24182;&#19988;&#24448;&#24448;&#32791;&#26102;&#12290;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#20986;&#29616;&#20351;&#33258;&#21160;&#35786;&#26029;&#31995;&#32479;&#30340;&#24320;&#21457;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#36825;&#31867;&#31995;&#32479;&#30340;&#34920;&#29616;&#21462;&#20915;&#20110;&#27169;&#22411;&#30340;&#36136;&#37327;&#21644;&#23427;&#25552;&#20379;&#30340;&#21487;&#35299;&#37322;&#24615;&#27700;&#24179;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23494;&#38598;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;DenseNet&#65289;&#21644;GRADCAM&#36827;&#34892;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#33016;&#37096;X&#20809;&#30142;&#30149;&#22810;&#26631;&#31614;&#35786;&#26029;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#21069;&#32622;X&#20809;&#35757;&#32451;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#23450;&#37327;&#25351;&#26631;&#65288;&#21253;&#25324;&#21463;&#35797;&#32773;&#25805;&#20316;&#29305;&#24449;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#65289;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;Cardiomegaly&#26465;&#20214;&#19979;&#36798;&#21040;&#20102;&#26368;&#39640;&#30340;AUC&#24471;&#20998;0.896&#65292;&#24182;&#33719;&#24471;&#20102;0.826&#30340;&#20934;&#30830;&#24230;&#12290;&#32780;&#22312;Nodule&#26465;&#20214;&#19979;&#33719;&#24471;&#20102;&#26368;&#20302;&#30340;AUC&#24471;&#20998;0.655&#65292;&#20934;&#30830;&#24230;&#20026;0.66&#12290;&#20026;&#20102;&#25552;&#39640;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#20915;&#31574;&#26041;&#38754;&#24314;&#31435;&#20449;&#20219;&#65292;&#25105;&#20204;&#20351;&#29992;GRADCAM&#29983;&#25104;&#20102;&#28909;&#22270;&#65292;&#31361;&#20986;&#26174;&#31034;&#20102;&#23545;&#35786;&#26029;&#26368;&#37325;&#35201;&#30340;X&#20809;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional methods of identifying pathologies in X-ray images rely heavily on skilled human interpretation and are often time-consuming. The advent of deep learning techniques has enabled the development of automated disease diagnosis systems, but the performance of such systems is dependent on the quality of the model and the level of interpretability it provides. In this paper, we propose a multi-label disease diagnosis model for chest X-rays using a dense convolutional neural network (DenseNet) and model interpretability using GRADCAM. We trained our model using frontal X-rays and evaluated its performance using various quantitative metrics, including the area under the receiver operating characteristic curve (AUC). Our proposed model achieved the highest AUC score of 0.896 for the condition Cardiomegaly with an accuracy of 0.826, while the lowest AUC score was obtained for Nodule, at 0.655 with an accuracy of 0.66. To promote model interpretability and build trust in decision maki
&lt;/p&gt;</description></item></channel></rss>