<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>DualCross&#26159;&#19968;&#20010;&#36328;&#27169;&#24577;&#21644;&#36328;&#39046;&#22495;&#30340;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#26088;&#22312;&#20351;&#21333;&#30446;BEV&#24863;&#30693;&#26356;&#21152;&#40065;&#26834;&#65292;&#24182;&#23454;&#29616;&#20102;&#36328;&#39046;&#22495;&#36328;&#20256;&#24863;&#22120;&#24863;&#30693;&#21644;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2305.03724</link><description>&lt;p&gt;
DualCross: &#21333;&#30446;BEV&#24863;&#30693;&#30340;&#36328;&#27169;&#24577;&#19982;&#36328;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DualCross: Cross-Modality Cross-Domain Adaptation for Monocular BEV Perception. (arXiv:2305.03724v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03724
&lt;/p&gt;
&lt;p&gt;
DualCross&#26159;&#19968;&#20010;&#36328;&#27169;&#24577;&#21644;&#36328;&#39046;&#22495;&#30340;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#26088;&#22312;&#20351;&#21333;&#30446;BEV&#24863;&#30693;&#26356;&#21152;&#40065;&#26834;&#65292;&#24182;&#23454;&#29616;&#20102;&#36328;&#39046;&#22495;&#36328;&#20256;&#24863;&#22120;&#24863;&#30693;&#21644;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#65292;&#32553;&#23567;&#35757;&#32451;&#21644;&#37096;&#32626;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#24182;&#32467;&#21512;&#22810;&#31181;&#20256;&#24863;&#22120;&#27169;&#24577;&#26159;&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#21482;&#20851;&#27880;&#19978;&#36848;&#20004;&#20010;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#65292;&#24573;&#35270;&#20102;&#23454;&#38469;&#22330;&#26223;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#39046;&#22495;&#21644;&#27169;&#24577;&#30340;&#21516;&#26102;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DualCross&#65292;&#19968;&#20010;&#36328;&#27169;&#24577;&#21644;&#36328;&#39046;&#22495;&#30340;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#20197;&#20419;&#36827;&#23398;&#20064;&#26356;&#20026;&#31283;&#20581;&#30340;&#21333;&#30446;&#40479;&#30640;&#24863;&#30693;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#22312;&#35757;&#32451;&#38454;&#27573;&#20174;&#19968;&#20010;&#39046;&#22495;&#30340;LiDAR&#20256;&#24863;&#22120;&#20013;&#36716;&#31227;&#28857;&#20113;&#30693;&#35782;&#65292;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#20165;&#25668;&#20687;&#22836;&#30340;&#27979;&#35797;&#22330;&#26223;&#20013;&#12290;&#36825;&#39033;&#24037;&#20316;&#26159;&#31532;&#19968;&#27425;&#23545;&#21333;&#30446;3D&#20219;&#21153;&#30340;&#36328;&#39046;&#22495;&#36328;&#20256;&#24863;&#22120;&#24863;&#30693;&#21644;&#36866;&#24212;&#36827;&#34892;&#20102;&#24320;&#25918;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#35206;&#30422;&#20102;&#24191;&#27867;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Closing the domain gap between training and deployment and incorporating multiple sensor modalities are two challenging yet critical topics for self-driving. Existing work only focuses on single one of the above topics, overlooking the simultaneous domain and modality shift which pervasively exists in real-world scenarios. A model trained with multi-sensor data collected in Europe may need to run in Asia with a subset of input sensors available. In this work, we propose DualCross, a cross-modality cross-domain adaptation framework to facilitate the learning of a more robust monocular bird's-eye-view (BEV) perception model, which transfers the point cloud knowledge from a LiDAR sensor in one domain during the training phase to the camera-only testing scenario in a different domain. This work results in the first open analysis of cross-domain cross-sensor perception and adaptation for monocular 3D tasks in the wild. We benchmark our approach on large-scale datasets under a wide range of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#23398;&#32773;&#22914;&#20309;&#22312;&#25776;&#20889;&#35770;&#25991;&#26102;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#65292;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#26368;&#20339;&#23454;&#36341;&#26631;&#20934;&#65292;&#21516;&#26102;&#25506;&#31350;&#20102;&#20351;&#29992;AI&#26159;&#21542;&#36829;&#21453;&#29256;&#26435;&#25110;&#23646;&#20110;&#20844;&#24179;&#20351;&#29992;&#30340;&#23433;&#20840;&#28207;&#65292;&#20026;&#23398;&#26415;&#20889;&#20316;&#19982;&#20154;&#24037;&#26234;&#33021;&#20849;&#23384;&#25552;&#20379;&#20102;&#27861;&#24459;&#19982;&#23398;&#26415;&#22522;&#30784;&#30340;&#25506;&#35752;&#12290;</title><link>http://arxiv.org/abs/2305.03722</link><description>&lt;p&gt;
ChatGPT&#21450;&#20854;&#23398;&#26415;&#24212;&#29992;&#65306;&#22312;&#20351;&#29992;AI&#25776;&#20889;&#35770;&#25991;&#26102;&#30340;&#26368;&#20339;&#23454;&#36341;&#19982;&#27861;&#24459;&#38519;&#38449;
&lt;/p&gt;
&lt;p&gt;
ChatGPT and Works Scholarly: Best Practices and Legal Pitfalls in Writing with AI. (arXiv:2305.03722v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#23398;&#32773;&#22914;&#20309;&#22312;&#25776;&#20889;&#35770;&#25991;&#26102;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#65292;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#26368;&#20339;&#23454;&#36341;&#26631;&#20934;&#65292;&#21516;&#26102;&#25506;&#31350;&#20102;&#20351;&#29992;AI&#26159;&#21542;&#36829;&#21453;&#29256;&#26435;&#25110;&#23646;&#20110;&#20844;&#24179;&#20351;&#29992;&#30340;&#23433;&#20840;&#28207;&#65292;&#20026;&#23398;&#26415;&#20889;&#20316;&#19982;&#20154;&#24037;&#26234;&#33021;&#20849;&#23384;&#25552;&#20379;&#20102;&#27861;&#24459;&#19982;&#23398;&#26415;&#22522;&#30784;&#30340;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#21457;&#20102;&#26159;&#21542;&#22312;&#21508;&#31181;&#19987;&#19994;&#32972;&#26223;&#19979;&#20351;&#29992;AI&#26159;&#21542;&#21512;&#36866;&#21644;&#21512;&#27861;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20174;&#23398;&#32773;&#22914;&#20309;&#19982;AI&#20849;&#21516;&#25776;&#20889;&#35770;&#25991;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#38416;&#36848;&#20102;&#22914;&#20309;&#35780;&#20272;&#27492;&#31867;AI&#20889;&#20316;&#26159;&#21542;&#36829;&#21453;&#29256;&#26435;&#25110;&#23646;&#20110;&#20844;&#24179;&#20351;&#29992;&#30340;&#23433;&#20840;&#28207;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26368;&#20339;&#23454;&#36341;&#26631;&#20934;&#65292;&#20197;&#20445;&#35777;&#35770;&#25991;&#30340;&#25220;&#34989;&#12289;&#29256;&#26435;&#21644;&#20844;&#24179;&#20351;&#29992;&#30340;&#26631;&#20934;&#12290;&#38543;&#30528;AI&#22312;&#26410;&#26469;&#20960;&#24180;&#20869;&#21487;&#33021;&#21464;&#24471;&#26356;&#21152;&#24378;&#22823;&#65292;&#23558;&#20854;&#25972;&#21512;&#21040;&#23398;&#26415;&#20889;&#20316;&#27963;&#21160;&#20013;&#26159;&#24688;&#24403;&#30340;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#24314;&#31435;&#33391;&#22909;&#30340;&#27861;&#24459;&#21644;&#23398;&#26415;&#22522;&#30784;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in artificial intelligence (AI) have raised questions about whether the use of AI is appropriate and legal in various professional contexts. Here, we present a perspective on how scholars may approach writing in conjunction with AI, and offer approaches to evaluating whether or not such AI-writing violates copyright or falls within the safe harbor of fair use. We present a set of best practices for standard of care with regard to plagiarism, copyright, and fair use. As AI is likely to grow more capable in the coming years, it is appropriate to begin integrating AI into scholarly writing activities. We offer a framework for establishing sound legal and scholarly foundations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#29256;&#26435;&#27861;&#20013;&#20844;&#27491;&#20351;&#29992;&#29256;&#26435;&#20316;&#21697;&#36827;&#34892;&#20154;&#24037;&#26234;&#33021;&#35757;&#32451;&#30340;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#35768;&#21487;&#21644;&#34917;&#20607;&#31561;&#26041;&#27861;&#65292;&#24378;&#35843;&#24179;&#34913;AI&#24320;&#21457;&#32773;&#12289;&#29256;&#26435;&#25152;&#26377;&#32773;&#21644;&#25972;&#20010;&#31038;&#20250;&#30340;&#21033;&#30410;&#21644;&#26435;&#21033;&#38656;&#35201;&#19968;&#31181;&#32454;&#33268;&#32780;&#28789;&#27963;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.03720</link><description>&lt;p&gt;
&#35757;&#32451;&#25165;&#26159;&#19968;&#20999;&#65306;&#20154;&#24037;&#26234;&#33021;&#12289;&#29256;&#26435;&#21644;&#20844;&#24179;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Training Is Everything: Artificial Intelligence, Copyright, and Fair Training. (arXiv:2305.03720v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#29256;&#26435;&#27861;&#20013;&#20844;&#27491;&#20351;&#29992;&#29256;&#26435;&#20316;&#21697;&#36827;&#34892;&#20154;&#24037;&#26234;&#33021;&#35757;&#32451;&#30340;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#35768;&#21487;&#21644;&#34917;&#20607;&#31561;&#26041;&#27861;&#65292;&#24378;&#35843;&#24179;&#34913;AI&#24320;&#21457;&#32773;&#12289;&#29256;&#26435;&#25152;&#26377;&#32773;&#21644;&#25972;&#20010;&#31038;&#20250;&#30340;&#21033;&#30410;&#21644;&#26435;&#21033;&#38656;&#35201;&#19968;&#31181;&#32454;&#33268;&#32780;&#28789;&#27963;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23398;&#20064;&#34892;&#20026;&#65292;&#24403;&#21069;&#36825;&#19968;&#20195;&#38761;&#21629;&#24615;&#30340;&#20154;&#24037;&#26234;&#33021;&#24517;&#39035;&#22312;&#28023;&#37327;&#30340;&#20986;&#29256;&#22270;&#20687;&#12289;&#25991;&#23383;&#21644;&#22768;&#38899;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;&#24456;&#22810;&#23646;&#20110;&#29256;&#26435;&#27861;&#30340;&#26680;&#24515;&#20869;&#23481;&#12290;&#19968;&#20123;&#20154;&#35748;&#20026;&#65292;&#23558;&#29256;&#26435;&#20316;&#21697;&#29992;&#20316;AI&#30340;&#35757;&#32451;&#38598;&#21482;&#26159;&#19968;&#31181;&#36807;&#28193;&#24615;&#30340;&#12289;&#38750;&#28040;&#32791;&#24615;&#30340;&#20351;&#29992;&#65292;&#19981;&#20250;&#23545;&#25152;&#26377;&#32773;&#30340;&#20869;&#23481;&#25110;&#20445;&#25252;&#20854;&#29256;&#26435;&#30340;&#33879;&#20316;&#26435;&#20135;&#29983;&#23454;&#36136;&#24615;&#30340;&#24178;&#25200;&#12290;&#20351;&#29992;&#36825;&#31181;&#20869;&#23481;&#26469;&#35757;&#32451;&#20182;&#20204;&#30340;AI&#24341;&#25806;&#30340;&#20844;&#21496;&#36890;&#24120;&#35748;&#20026;&#65292;&#36825;&#31181;&#20351;&#29992;&#24212;&#35813;&#34987;&#35270;&#20026;&#32654;&#22269;&#27861;&#24459;&#19979;&#30340;&#8220;&#20844;&#24179;&#20351;&#29992;&#8221;&#65288;&#22312;&#20854;&#20182;&#22269;&#23478;&#26377;&#26102;&#34987;&#31216;&#20026;&#8220;&#20844;&#24179;&#20351;&#29992;&#8221;&#65289;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#35768;&#22810;&#29256;&#26435;&#25152;&#26377;&#32773;&#21450;&#20854;&#25903;&#25345;&#32773;&#35748;&#20026;&#65292;&#23558;&#29256;&#26435;&#20316;&#21697;&#32435;&#20837;AI&#30340;&#35757;&#32451;&#38598;&#20013;&#26500;&#25104;&#20102;&#23545;&#25152;&#26377;&#32773;&#30693;&#35782;&#20135;&#26435;&#30340;&#30423;&#29992;&#65292;&#22240;&#27492;&#26174;&#28982;&#19981;&#26159;&#27861;&#24459;&#19979;&#30340;&#20844;&#24179;&#20351;&#29992;&#12290;&#36825;&#22330;&#36777;&#35770;&#23545;&#20110;AI&#21450;&#20854;&#24212;&#29992;&#30340;&#26410;&#26469;&#36712;&#36857;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#29256;&#26435;&#27861;&#20013;&#20844;&#24179;&#20351;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#29256;&#26435;&#20316;&#21697;&#35757;&#32451;AI&#24341;&#25806;&#30340;&#35770;&#28857;&#65292;&#25506;&#35752;&#20102;&#21508;&#31181;&#26041;&#27861;&#30340;&#27861;&#24459;&#21644;&#25919;&#31574;&#24433;&#21709;&#65292;&#21253;&#25324;&#35768;&#21487;&#21644;&#34917;&#20607;&#65292;&#20197;&#21450;&#23545;&#25991;&#21270;&#21644;&#21019;&#24847;&#34920;&#36798;&#30340;&#28508;&#22312;&#21361;&#23475;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#20027;&#24352;&#65292;&#24179;&#34913;AI&#24320;&#21457;&#32773;&#12289;&#29256;&#26435;&#25152;&#26377;&#32773;&#21644;&#25972;&#20010;&#31038;&#20250;&#30340;&#21033;&#30410;&#21644;&#26435;&#21033;&#38656;&#35201;&#19968;&#31181;&#32454;&#33268;&#32780;&#28789;&#27963;&#30340;&#26041;&#27861;&#65292;&#35748;&#35782;&#21040;&#35757;&#32451;&#22312;&#24320;&#21457;&#26377;&#25928;&#30340;AI&#31995;&#32479;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To learn how to behave, the current revolutionary generation of AIs must be trained on vast quantities of published images, written works, and sounds, many of which fall within the core subject matter of copyright law. To some, the use of copyrighted works as training sets for AI is merely a transitory and non-consumptive use that does not materially interfere with owners' content or copyrights protecting it. Companies that use such content to train their AI engine often believe such usage should be considered "fair use" under United States law (sometimes known as "fair dealing" in other countries). By contrast, many copyright owners, as well as their supporters, consider the incorporation of copyrighted works into training sets for AI to constitute misappropriation of owners' intellectual property, and, thus, decidedly not fair use under the law. This debate is vital to the future trajectory of AI and its applications.  In this article, we analyze the arguments in favor of, and agains
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;AI&#30340;&#27835;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27835;&#29702;&#27169;&#24335;&#65292;&#21363;&#30001;&#20154;&#31867;&#21644;AI&#21512;&#20316;&#65292;&#21508;&#33258;&#21457;&#25381;&#20854;&#21508;&#33258;&#30340;&#20248;&#21183;&#65292;&#20197;&#23454;&#29616;&#23545;&#35813;&#25216;&#26415;&#30340;&#26377;&#25928;&#27835;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.03719</link><description>&lt;p&gt;
AI&#30340;&#27835;&#29702;&#65292;&#30001;AI&#27835;&#29702;&#65292;&#20026;AI&#27835;&#29702;
&lt;/p&gt;
&lt;p&gt;
Governance of the AI, by the AI, and for the AI. (arXiv:2305.03719v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;AI&#30340;&#27835;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27835;&#29702;&#27169;&#24335;&#65292;&#21363;&#30001;&#20154;&#31867;&#21644;AI&#21512;&#20316;&#65292;&#21508;&#33258;&#21457;&#25381;&#20854;&#21508;&#33258;&#30340;&#20248;&#21183;&#65292;&#20197;&#23454;&#29616;&#23545;&#35813;&#25216;&#26415;&#30340;&#26377;&#25928;&#27835;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21322;&#20010;&#19990;&#32426;&#20013;&#65292;&#26377;&#20960;&#27425;&#26366;&#23459;&#24067;&#20154;&#31867;&#21382;&#21490;&#23558;&#34987;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24443;&#24213;&#25913;&#21464;&#30340;&#34394;&#20551;&#40654;&#26126;&#12290;&#20316;&#32773;&#20204;&#35748;&#20026;&#65292;AI&#26102;&#20195;&#32456;&#20110;&#21040;&#26469;&#20102;&#12290;&#24378;&#22823;&#30340;&#22270;&#20687;&#29983;&#25104;&#22120;&#65292;&#22914;DALL-E2&#21644;Midjourney&#65292;&#31361;&#28982;&#38388;&#20351;&#24471;&#20219;&#20309;&#26377;&#26435;&#38480;&#30340;&#20154;&#20204;&#37117;&#21487;&#20197;&#36731;&#26494;&#22320;&#21019;&#20316;&#20986;&#23500;&#26377;&#22797;&#26434;&#24615;&#30340;&#33402;&#26415;&#21697;&#12290;&#31867;&#20284;&#22320;&#65292;&#25991;&#26412;&#29983;&#25104;&#22120;&#65292;&#22914;GPT3.5(&#21253;&#25324;ChatGPT)&#21644;BLOOM&#65292;&#21487;&#20197;&#35753;&#29992;&#25143;&#25776;&#20889;&#26377;&#20851;&#35768;&#22810;&#24863;&#20852;&#36259;&#30340;&#20027;&#39064;&#30340;&#35814;&#32454;&#20889;&#20316;&#25551;&#36848;&#12290;&#29616;&#22312;&#65292;&#21363;&#20351;&#26159;&#27809;&#26377;&#32534;&#20889;&#36719;&#20214;&#26041;&#38754;&#24191;&#27867;&#19987;&#19994;&#30693;&#35782;&#30340;&#20154;&#20063;&#21487;&#20197;&#20351;&#29992;AI&#26469;&#29983;&#25104;&#21487;&#20197;&#23454;&#29616;&#26080;&#25968;&#24212;&#29992;&#31243;&#24207;&#30340;&#20195;&#30721;&#12290;&#34429;&#28982;AI&#23558;&#32487;&#32493;&#21457;&#23637;&#21644;&#25913;&#36827;&#65292;&#21487;&#33021;&#26159;&#20197;&#26497;&#24555;&#30340;&#36895;&#24230;&#65292;&#20294;&#24403;&#21069;&#30340;AI&#29366;&#24577;&#24050;&#32463;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#31038;&#20250;&#37096;&#38376;&#24102;&#26469;&#20102;&#28145;&#21051;&#30340;&#21464;&#38761;&#12290;&#27599;&#19968;&#31181;&#26032;&#25216;&#26415;&#37117;&#25361;&#25112;&#30528;&#20154;&#31867;&#26126;&#26234;&#22320;&#36827;&#34892;&#27835;&#29702;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#27835;&#29702;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#21487;&#33021;&#30340;&#21644;&#24517;&#35201;&#30340;&#12290;&#22312;AI&#30340;&#24773;&#20917;&#19979;&#65292;&#27835;&#29702;&#30001;&#20110;&#36879;&#26126;&#24230;&#12289;&#38382;&#36131;&#21046;&#21644;&#20844;&#24179;&#24615;&#31561;&#38382;&#39064;&#32780;&#21464;&#24471;&#22797;&#26434;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;AI&#27835;&#29702;&#65292;&#30001;AI&#27835;&#29702;&#21644;&#20026;AI&#27835;&#29702;&#30340;&#24819;&#27861;&#65292;&#35748;&#20026;AI&#26412;&#36523;&#21487;&#20197;&#22312;&#23454;&#29616;&#26377;&#25928;&#27835;&#29702;&#25216;&#26415;&#26041;&#38754;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#27835;&#29702;&#27169;&#24335;&#65292;&#28041;&#21450;&#20154;&#31867;&#21644;AI&#20043;&#38388;&#30340;&#21512;&#20316;&#65292;&#21508;&#33258;&#21457;&#25381;&#20854;&#21508;&#33258;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past half century, there have been several false dawns during which the "arrival" of world-changing artificial intelligence (AI) has been heralded. Tempting fate, the authors believe the age of AI has, indeed, finally arrived. Powerful image generators, such as DALL-E2 and Midjourney have suddenly allowed anyone with access the ability easily to create rich and complex art. In a similar vein, text generators, such as GPT3.5 (including ChatGPT) and BLOOM, allow users to compose detailed written descriptions of many topics of interest. And, it is even possible now for a person without extensive expertise in writing software to use AI to generate code capable of myriad applications. While AI will continue to evolve and improve, probably at a rapid rate, the current state of AI is already ushering in profound changes to many different sectors of society. Every new technology challenges the ability of humanity to govern it wisely. However, governance is usually viewed as both possi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20559;&#35265;&#26080;&#20851;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#22312;&#30446;&#26631;&#31867;&#21035;&#19978;&#37327;&#21270;&#8220;&#20559;&#35265;&#23545;&#40784;/&#19981;&#23545;&#40784;&#8221;&#65292;&#20197;&#36991;&#20813;&#36890;&#36807;&#32593;&#32476;&#20256;&#25773;&#20559;&#35265;-&#30446;&#26631;&#23545;&#40784;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.03691</link><description>&lt;p&gt;
&#20174;&#27888;&#26862;&#22810;&#36793;&#24418;&#20013;&#25366;&#25496;&#20559;&#35265;-&#30446;&#26631;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Mining bias-target Alignment from Voronoi Cells. (arXiv:2305.03691v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20559;&#35265;&#26080;&#20851;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#22312;&#30446;&#26631;&#31867;&#21035;&#19978;&#37327;&#21270;&#8220;&#20559;&#35265;&#23545;&#40784;/&#19981;&#23545;&#40784;&#8221;&#65292;&#20197;&#36991;&#20813;&#36890;&#36807;&#32593;&#32476;&#20256;&#25773;&#20559;&#35265;-&#30446;&#26631;&#23545;&#40784;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20173;&#28982;&#23481;&#26131;&#20986;&#29616;&#20559;&#24046;:&#36825;&#24341;&#36215;&#20102;&#20182;&#20204;&#20844;&#27491;&#24615;&#30340;&#25285;&#24551;&#24182;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20559;&#35265;&#26080;&#20851;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;&#19982;&#20256;&#32479;&#30340;&#21435;&#20559;&#35265;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#20381;&#38752;&#19968;&#20010;&#25351;&#26631;&#26469;&#37327;&#21270;&#30446;&#26631;&#31867;&#21035;&#19978;&#30340;&#8220;&#20559;&#35265;&#23545;&#40784;/&#19981;&#23545;&#40784;&#8221;&#65292;&#24182;&#21033;&#29992;&#27492;&#20449;&#24687;&#26469;&#36991;&#20813;&#36890;&#36807;&#32593;&#32476;&#20256;&#25773;&#20559;&#35265;-&#30446;&#26631;&#23545;&#40784;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#24120;&#29992;&#30340;&#21435;&#20559;&#35265;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26377;&#30417;&#30563;&#21644;&#20559;&#35265;&#29305;&#23450;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#23384;&#22312;&#22810;&#20010;&#26679;&#26412;&#20013;&#30340;&#20559;&#35265;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19981;&#23384;&#22312;&#20559;&#35265;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#26377;&#30417;&#30563;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant research efforts, deep neural networks are still vulnerable to biases: this raises concerns about their fairness and limits their generalization. In this paper, we propose a bias-agnostic approach to mitigate the impact of bias in deep neural networks. Unlike traditional debiasing approaches, we rely on a metric to quantify ``bias alignment/misalignment'' on target classes, and use this information to discourage the propagation of bias-target alignment information through the network. We conduct experiments on several commonly used datasets for debiasing and compare our method to supervised and bias-specific approaches. Our results indicate that the proposed method achieves comparable performance to state-of-the-art supervised approaches, although it is bias-agnostic, even in presence of multiple biases in the same sample.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20837;&#38498;&#35760;&#24405;&#36827;&#34892;&#24739;&#32773;&#24182;&#21457;&#30151;&#39118;&#38505;&#35780;&#20272;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;Longformer&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#24739;&#32773;&#30340;&#39118;&#38505;&#35780;&#20998;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#27431;&#27954;&#21307;&#38498;&#30340;&#24739;&#32773;&#25968;&#25454;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#21644;&#20854;&#20182;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.03661</link><description>&lt;p&gt;
&#20174;&#20837;&#38498;&#35760;&#24405;&#39044;&#27979;COVID-19&#21644;&#32954;&#28814;&#24182;&#21457;&#30151;
&lt;/p&gt;
&lt;p&gt;
Predicting COVID-19 and pneumonia complications from admission texts. (arXiv:2305.03661v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20837;&#38498;&#35760;&#24405;&#36827;&#34892;&#24739;&#32773;&#24182;&#21457;&#30151;&#39118;&#38505;&#35780;&#20272;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;Longformer&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#24739;&#32773;&#30340;&#39118;&#38505;&#35780;&#20998;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#27431;&#27954;&#21307;&#38498;&#30340;&#24739;&#32773;&#25968;&#25454;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#21644;&#20854;&#20182;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20837;&#38498;&#25253;&#21578;&#30340;&#39118;&#38505;&#35780;&#20272;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#22240;&#32954;&#28814;&#25110;COVID-19&#32780;&#20303;&#38498;&#30340;&#24739;&#32773;&#30340;&#24182;&#21457;&#30151;&#39118;&#38505;&#12290;&#25105;&#20204;&#23558;Longformer&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#20837;&#38498;&#35760;&#24405;&#21644;&#20854;&#20182;&#21487;&#29992;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#35745;&#31639;&#20986;&#24739;&#32773;&#30340;&#39118;&#38505;&#35780;&#20998;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#27431;&#27954;&#21307;&#38498;&#30340;&#24739;&#32773;&#25968;&#25454;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;Transformer&#22522;&#32447;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#26426;&#26500;&#21644;&#35786;&#26029;&#20013;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#30340;&#20854;&#20182;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present a novel approach to risk assessment for patients hospitalized with pneumonia or COVID-19 based on their admission reports. We applied a Longformer neural network to admission reports and other textual data available shortly after admission to compute risk scores for the patients. We used patient data of multiple European hospitals to demonstrate that our approach outperforms the Transformer baselines. Our experiments show that the proposed model generalises across institutions and diagnoses. Also, our method has several other advantages described in the paper.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#23884;&#20837;&#26469;&#26816;&#32034;&#30456;&#24212;&#30340;&#20505;&#36873;&#25918;&#23556;&#23398;&#25991;&#26412;&#65292;&#24182;&#20351;&#29992;&#36890;&#29992;&#39046;&#22495;&#29983;&#25104;&#27169;&#22411;&#26469;&#29983;&#25104;&#25253;&#21578;&#65292;&#21487;&#25233;&#21046;&#34394;&#26500;&#30340;&#29983;&#25104;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#20020;&#24202;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.03660</link><description>&lt;p&gt;
&#21033;&#29992;OpenAI GPT&#27169;&#22411;&#30340;&#26816;&#32034;&#22686;&#24378;&#30340;&#33016;&#37096;X&#23556;&#32447;&#25253;&#21578;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Retrieval Augmented Chest X-Ray Report Generation using OpenAI GPT models. (arXiv:2305.03660v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03660
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#23884;&#20837;&#26469;&#26816;&#32034;&#30456;&#24212;&#30340;&#20505;&#36873;&#25918;&#23556;&#23398;&#25991;&#26412;&#65292;&#24182;&#20351;&#29992;&#36890;&#29992;&#39046;&#22495;&#29983;&#25104;&#27169;&#22411;&#26469;&#29983;&#25104;&#25253;&#21578;&#65292;&#21487;&#25233;&#21046;&#34394;&#26500;&#30340;&#29983;&#25104;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#20020;&#24202;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Retrieval Augmented Generation (RAG) &#30340;&#26041;&#27861;&#26469;&#33258;&#21160;&#29983;&#25104;&#25918;&#23556;&#23398;&#25253;&#21578;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#23884;&#20837;&#26469;&#26816;&#32034;&#30456;&#24212;&#30340;&#20505;&#36873;&#25918;&#23556;&#23398;&#25991;&#26412;&#65292;&#24182;&#20351;&#29992;&#20687;OpenAI text-davinci-003&#12289;gpt-3.5-turbo&#21644;gpt-4&#36825;&#26679;&#30340;&#36890;&#29992;&#39046;&#22495;&#29983;&#25104;&#27169;&#22411;&#26469;&#29983;&#25104;&#25253;&#21578;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25233;&#21046;&#34394;&#26500;&#30340;&#29983;&#25104;&#24182;&#25552;&#20379;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#65292;&#20197;&#25105;&#20204;&#25152;&#38656;&#30340;&#26684;&#24335;&#29983;&#25104;&#25253;&#21578;&#20869;&#23481;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20020;&#24202;&#25351;&#26631;&#65292;BERTScore&#20026;0.2865&#65288;&#916;+25.88%&#65289;&#65292;Semb Score&#20026;0.4026&#65288;&#916;+6.31%&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24191;&#27867;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#20020;&#24202;&#35774;&#32622;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#22686;&#24378;&#33258;&#21160;&#29983;&#25104;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#36807;&#31243;&#65292;&#21516;&#26102;&#20855;&#22791;&#36866;&#21512;&#35813;&#35774;&#32622;&#30340;&#30456;&#20851;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Retrieval Augmented Generation (RAG) as an approach for automated radiology report writing that leverages multimodally aligned embeddings from a contrastively pretrained vision language model for retrieval of relevant candidate radiology text for an input radiology image and a general domain generative model like OpenAI text-davinci-003, gpt-3.5-turbo and gpt-4 for report generation using the relevant radiology text retrieved. This approach keeps hallucinated generations under check and provides capabilities to generate report content in the format we desire leveraging the instruction following capabilities of these generative models. Our approach achieves better clinical metrics with a BERTScore of 0.2865 ({\Delta}+ 25.88%) and Semb score of 0.4026 ({\Delta}+ 6.31%). Our approach can be broadly relevant for different clinical settings as it allows to augment the automated radiology report generation process with content relevant for that setting while also having the abilit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21487;&#25193;&#23637;&#26368;&#32456;&#20248;&#21270;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#33021;&#21147;&#20013;&#30340;LaCAM&#31639;&#27861;&#65292;&#25913;&#36827;&#20102;&#20854;&#20219;&#20309;&#26102;&#26399;&#29256;&#26412;LaCAM*&#21644;&#21518;&#32487;&#29983;&#25104;&#65292;&#25104;&#21151;&#20943;&#23569;&#35268;&#21010;&#24037;&#20316;&#37327;&#24182;&#21462;&#24471;&#20102;&#30456;&#24212;&#23454;&#39564;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03632</link><description>&lt;p&gt;
&#25552;&#39640;LaCAM&#31639;&#27861;&#30340;&#21487;&#25193;&#23637;&#26368;&#32456;&#20248;&#21270;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving LaCAM for Scalable Eventually Optimal Multi-Agent Pathfinding. (arXiv:2305.03632v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21487;&#25193;&#23637;&#26368;&#32456;&#20248;&#21270;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#33021;&#21147;&#20013;&#30340;LaCAM&#31639;&#27861;&#65292;&#25913;&#36827;&#20102;&#20854;&#20219;&#20309;&#26102;&#26399;&#29256;&#26412;LaCAM*&#21644;&#21518;&#32487;&#29983;&#25104;&#65292;&#25104;&#21151;&#20943;&#23569;&#35268;&#21010;&#24037;&#20316;&#37327;&#24182;&#21462;&#24471;&#20102;&#30456;&#24212;&#23454;&#39564;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#26368;&#36817;&#24320;&#21457;&#30340;LaCAM&#31639;&#27861;&#65292;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#65288;MAPF&#65289;&#12290;LaCAM&#26159;&#19968;&#31181;&#27425;&#20248;&#30340;&#22522;&#20110;&#25628;&#32034;&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#25042;&#24816;&#30340;&#21518;&#32487;&#29983;&#25104;&#26469;&#22823;&#24133;&#20943;&#23569;&#35268;&#21010;&#24037;&#20316;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#22686;&#24378;&#21151;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20854;&#20219;&#20309;&#26102;&#26399;&#29256;&#26412;&#65292;&#31216;&#20026;LaCAM*&#65292;&#20854;&#26368;&#32456;&#25910;&#25947;&#20110;&#26368;&#20339;&#35299;&#65292;&#21069;&#25552;&#26159;&#35299;&#20915;&#26041;&#26696;&#25104;&#26412;&#26159;&#32047;&#31215;&#30340;&#36716;&#25442;&#25104;&#26412;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#21518;&#32487;&#29983;&#25104;&#65292;&#20197;&#24555;&#36895;&#33719;&#24471;&#21021;&#22987;&#35299;&#12290;&#35814;&#23613;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#25928;&#29992;&#12290;&#20363;&#22914;&#65292;LaCAM*&#22312;&#26631;&#20934;&#26700;&#38754;PC&#19978;&#65292;&#22312;&#30830;&#20445;&#26368;&#32456;&#25910;&#25947;&#21040;&#26368;&#20339;&#35299;&#30340;&#24773;&#20917;&#19979;&#65292;&#27425;&#20248;&#22320;&#35299;&#20915;&#20102;&#20174;MAPF&#22522;&#20934;&#26816;&#32034;&#30340;99&#65285;&#30340;&#23454;&#20363;&#65292;&#20854;&#20013;&#20195;&#29702;&#25968;&#37327;&#26368;&#39640;&#36798;1000&#20010;&#65292;&#29992;&#26102;&#19981;&#36229;&#36807;&#21313;&#31186;;&#24320;&#21457;&#20102;&#26032;&#30340;MAPF&#31639;&#27861;&#35270;&#37326;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study extends the recently-developed LaCAM algorithm for multi-agent pathfinding (MAPF). LaCAM is a sub-optimal search-based algorithm that uses lazy successor generation to dramatically reduce the planning effort. We present two enhancements. First, we propose its anytime version, called LaCAM*, which eventually converges to optima, provided that solution costs are accumulated transition costs. Second, we improve the successor generation to quickly obtain initial solutions. Exhaustive experiments demonstrate their utility. For instance, LaCAM* sub-optimally solved 99% of the instances retrieved from the MAPF benchmark, where the number of agents varied up to a thousand, within ten seconds on a standard desktop PC, while ensuring eventual convergence to optima; developing a new horizon of MAPF algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#31350;&#20102;&#36890;&#36807;&#25968;&#25454;&#25972;&#21512;&#26469;&#25552;&#39640;&#22270;&#20687;&#23383;&#24149;&#36136;&#37327;&#30340;&#26041;&#26696;&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#36164;&#28304;&#35757;&#32451;&#20102;&#27604;&#22522;&#20934;&#27169;&#22411;&#26356;&#22909;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.03610</link><description>&lt;p&gt;
&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#22270;&#20687;&#23383;&#24149;&#25968;&#25454;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Data Curation for Image Captioning with Text-to-Image Generative Models. (arXiv:2305.03610v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#31350;&#20102;&#36890;&#36807;&#25968;&#25454;&#25972;&#21512;&#26469;&#25552;&#39640;&#22270;&#20687;&#23383;&#24149;&#36136;&#37327;&#30340;&#26041;&#26696;&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#36164;&#28304;&#35757;&#32451;&#20102;&#27604;&#22522;&#20934;&#27169;&#22411;&#26356;&#22909;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22270;&#20687;&#23383;&#24149;&#25216;&#26415;&#30340;&#21457;&#23637;&#20027;&#35201;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#26085;&#30410;&#20381;&#36182;&#20110;&#35745;&#31639;&#36164;&#28304;&#21644;&#36234;&#26469;&#36234;&#22823;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#36890;&#36807;&#25968;&#25454;&#25972;&#21512;&#30340;&#20004;&#31181;&#26041;&#27861;&#25506;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#25552;&#39640;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#26412;&#36136;&#37327;&#26469;&#25913;&#21892;&#24615;&#33021;&#65306;&#19968;&#31181;&#26041;&#27861;&#20551;&#23450;&#30001;&#20110;&#22270;&#20687;&#21644;&#23383;&#24149;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65292;&#26576;&#20123;&#31034;&#20363;&#24212;&#35813;&#36991;&#20813;&#20351;&#29992;&#65292;&#21478;&#19968;&#31181;&#26041;&#27861;&#21017;&#20551;&#23450;&#19981;&#21305;&#37197;&#21487;&#20197;&#36890;&#36807;&#26367;&#25442;&#22270;&#20687;&#26469;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in image captioning are mainly driven by large-scale vision-language pretraining, relying heavily on computational resources and increasingly large multimodal datasets. Instead of scaling up pretraining data, we ask whether it is possible to improve performance by improving the quality of the samples in existing datasets. We pursue this question through two approaches to data curation: one that assumes that some examples should be avoided due to mismatches between the image and caption, and one that assumes that the mismatch can be addressed by replacing the image, for which we use the state-of-the-art Stable Diffusion model. These approaches are evaluated using the BLIP model on MS COCO and Flickr30K in both finetuning and few-shot learning settings. Our simple yet effective approaches consistently outperform baselines, indicating that better image captioning models can be trained by curating existing resources. Finally, we conduct a human study to understand the error
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#20154;&#31867;&#27880;&#24847;&#21147;&#30693;&#35782;&#23884;&#20837;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#20013;&#30340;&#25928;&#26524;&#12290;&#24320;&#21457;&#20102;&#22522;&#20110;&#26799;&#24230;&#21644;&#20154;&#31867;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#20004;&#31181;&#35299;&#37322;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#23545;&#20110;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#65292;&#20154;&#31867;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#26041;&#27861;&#22312;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#21644;&#21512;&#29702;&#24615;&#26041;&#38754;&#20248;&#20110;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.03601</link><description>&lt;p&gt;
&#20154;&#31867;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#23545;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Human Attention-Guided Explainable Artificial Intelligence for Computer Vision Models. (arXiv:2305.03601v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#20154;&#31867;&#27880;&#24847;&#21147;&#30693;&#35782;&#23884;&#20837;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#20013;&#30340;&#25928;&#26524;&#12290;&#24320;&#21457;&#20102;&#22522;&#20110;&#26799;&#24230;&#21644;&#20154;&#31867;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#20004;&#31181;&#35299;&#37322;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#23545;&#20110;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#65292;&#20154;&#31867;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#26041;&#27861;&#22312;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#21644;&#21512;&#29702;&#24615;&#26041;&#38754;&#20248;&#20110;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#20154;&#31867;&#27880;&#24847;&#21147;&#30693;&#35782;&#23884;&#20837;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#20013;&#26159;&#21542;&#21487;&#20197;&#22686;&#24378;&#20854;&#21512;&#29702;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#38024;&#23545;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#24320;&#21457;&#20102;&#26032;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;XAI&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#24403;&#21069;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#26041;&#27861;&#29983;&#25104;&#38754;&#21521;&#23545;&#35937;&#30340;&#35299;&#37322;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#34429;&#28982;&#36825;&#20123;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#21487;&#20197;&#24456;&#22909;&#22320;&#35299;&#37322;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#65292;&#20294;&#26159;&#24403;&#29992;&#20110;&#35299;&#37322;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#26102;&#65292;&#25152;&#24471;&#21040;&#30340;&#26174;&#33879;&#24615;&#22270;&#36890;&#24120;&#27604;&#25191;&#34892;&#21516;&#19968;&#20219;&#21153;&#30340;&#20154;&#31867;&#27880;&#24847;&#21147;&#22270;&#30340;&#20934;&#30830;&#24615;&#20302;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20154;&#31867;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;XAI&#65288;HAG-XAI&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#35757;&#32451;&#30340;&#28608;&#27963;&#20989;&#25968;&#21644;&#24179;&#28369;&#26680;&#26469;&#26368;&#22823;&#21270;XAI&#26174;&#33879;&#24615;&#22270;&#19982;&#20154;&#31867;&#27880;&#24847;&#21147;&#22270;&#30340;&#30456;&#20284;&#24615;&#65292;&#20174;&#20154;&#31867;&#27880;&#24847;&#21147;&#23398;&#20064;&#22914;&#20309;&#26368;&#22909;&#22320;&#32467;&#21512;&#27169;&#22411;&#30340;&#35299;&#37322;&#20449;&#24687;&#20197;&#22686;&#24378;&#35299;&#37322;&#30340;&#21512;&#29702;&#24615;&#12290;&#23545;&#20110;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#65292;HAG-XAI&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#34920;&#29616;&#30456;&#20284;&#65292;&#20294;&#26159;&#23545;&#20110;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#65292;HAG-XAI&#29983;&#25104;&#30340;&#35299;&#37322;&#22312;&#20934;&#30830;&#24615;&#21644;&#21512;&#29702;&#24615;&#26041;&#38754;&#37117;&#26174;&#33879;&#20248;&#20110;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examined whether embedding human attention knowledge into saliency-based explainable AI (XAI) methods for computer vision models could enhance their plausibility and faithfulness. We first developed new gradient-based XAI methods for object detection models to generate object-specific explanations by extending the current methods for image classification models. Interestingly, while these gradient-based methods worked well for explaining image classification models, when being used for explaining object detection models, the resulting saliency maps generally had lower faithfulness than human attention maps when performing the same task. We then developed Human Attention-Guided XAI (HAG-XAI) to learn from human attention how to best combine explanatory information from the models to enhance explanation plausibility by using trainable activation functions and smoothing kernels to maximize XAI saliency map's similarity to human attention maps. While for image classification models, HAG
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#26816;&#32034;&#25903;&#25345;&#20107;&#23454;&#26469;&#30830;&#23450;&#33258;&#28982;&#35821;&#35328;&#38472;&#36848;&#21644;CTR&#20043;&#38388;&#30340;&#25512;&#29702;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.03598</link><description>&lt;p&gt;
NLI4CT&#65306;&#38754;&#21521;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#30340;&#22810;&#35777;&#25454;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports. (arXiv:2305.03598v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#26816;&#32034;&#25903;&#25345;&#20107;&#23454;&#26469;&#30830;&#23450;&#33258;&#28982;&#35821;&#35328;&#38472;&#36848;&#21644;CTR&#20043;&#38388;&#30340;&#25512;&#29702;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#35299;&#37322;&#21644;&#26816;&#32034;&#29992;&#20110;&#25903;&#25345;&#20020;&#24202;&#20915;&#31574;&#30340;&#21307;&#23398;&#35777;&#25454;&#65311;&#22810;&#24180;&#26469;&#65292;&#31215;&#32047;&#19979;&#26469;&#30340;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#21253;&#21547;&#20102;&#21457;&#23637;&#20010;&#24615;&#21270;&#21307;&#23398;&#25152;&#24517;&#38656;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#25214;&#21040;&#26368;&#20339;&#30340;&#23454;&#39564;&#27835;&#30103;&#35777;&#25454;&#65292;&#25163;&#21160;&#26816;&#26597;&#36229;&#36807;400,000&#20010;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#26159;&#23454;&#38469;&#19978;&#19981;&#21487;&#34892;&#30340;&#12290;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20801;&#35768;&#21487;&#25193;&#23637;&#35745;&#31639;&#25991;&#26412;&#34164;&#21547;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;NLI&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#35821;&#26009;&#24211;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#20043;&#21069;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#26080;&#27861;&#25429;&#25417;CTR&#25512;&#29702;&#30340;&#20840;&#37096;&#22797;&#26434;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36164;&#28304;&#65292;&#20197;&#25512;&#36827;&#20851;&#20110;CTR&#25512;&#29702;&#30340;NLI&#30740;&#31350;&#12290;&#35813;&#36164;&#28304;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#20219;&#21153;&#12290;&#39318;&#20808;&#65292;&#30830;&#23450;&#33258;&#28982;&#35821;&#35328;&#38472;&#36848;&#21644;CTR&#20043;&#38388;&#30340;&#25512;&#29702;&#20851;&#31995;&#12290;&#20854;&#27425;&#65292;&#26816;&#32034;&#25903;&#25345;&#20107;&#23454;&#20197;&#35777;&#26126;&#39044;&#27979;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;NLI4CT&#65292;&#19968;&#20010;&#22522;&#20110;CTR&#30340;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we interpret and retrieve medical evidence to support clinical decisions? Clinical trial reports (CTR) amassed over the years contain indispensable information for the development of personalized medicine. However, it is practically infeasible to manually inspect over 400,000+ clinical trial reports in order to find the best evidence for experimental treatments. Natural Language Inference (NLI) offers a potential solution to this problem, by allowing the scalable computation of textual entailment. However, existing NLI models perform poorly on biomedical corpora, and previously published datasets fail to capture the full complexity of inference over CTRs. In this work, we present a novel resource to advance research on NLI for reasoning on CTRs. The resource includes two main tasks. Firstly, to determine the inference relation between a natural language statement, and a CTR. Secondly, to retrieve supporting facts to justify the predicted relation. We provide NLI4CT, a corpus of
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#29983;&#35789;&#25193;&#23637;&#8221;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#20010;&#24615;&#21270;&#30340;&#8220;&#29983;&#35789;&#36866;&#37197;&#22120;&#8221;&#26469;&#23398;&#20064;&#20010;&#24615;&#21270;&#35789;&#27719;&#65292;&#25552;&#39640;&#20102;&#29983;&#35789;&#35206;&#30422;&#29575;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.03584</link><description>&lt;p&gt;
&#29616;&#22312;&#23427;&#21548;&#36215;&#26469;&#20687;&#20320;&#20102;&#65306;&#22312;&#35774;&#22791;&#19978;&#23398;&#20064;&#20010;&#24615;&#21270;&#35789;&#27719;
&lt;/p&gt;
&lt;p&gt;
Now It Sounds Like You: Learning Personalized Vocabulary On Device. (arXiv:2305.03584v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03584
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#29983;&#35789;&#25193;&#23637;&#8221;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#20010;&#24615;&#21270;&#30340;&#8220;&#29983;&#35789;&#36866;&#37197;&#22120;&#8221;&#26469;&#23398;&#20064;&#20010;&#24615;&#21270;&#35789;&#27719;&#65292;&#25552;&#39640;&#20102;&#29983;&#35789;&#35206;&#30422;&#29575;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#32852;&#37030;&#23398;&#20064;&#22312;&#36827;&#34892;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#26041;&#38754;&#24050;&#32463;&#26174;&#31034;&#20986;&#26174;&#33879;&#36827;&#23637;&#12290;&#36825;&#39033;&#24037;&#20316;&#20391;&#37325;&#20110;&#24212;&#29992;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#35774;&#22791;&#31471;&#35821;&#35328;&#24314;&#27169;&#12290;&#30001;&#20110;&#20869;&#23384;&#21644;&#24310;&#36831;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#25903;&#25345;&#23376;&#21333;&#35789;&#26631;&#35760;&#25110;&#27874;&#26463;&#25628;&#32034;&#35299;&#30721;&#30340;&#22797;&#26434;&#24615;&#65292;&#22240;&#27492;&#20915;&#23450;&#37096;&#32626;&#23553;&#38381;&#35789;&#27719;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23553;&#38381;&#35789;&#27719;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#29305;&#23450;&#29992;&#25143;&#30340;&#29983;&#35789;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#29983;&#35789;&#25193;&#23637;&#8221;&#30340;&#26032;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#25552;&#39640;&#20102;&#29983;&#35789;&#35206;&#30422;&#29575;&#65292;&#22686;&#21152;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#20102;&#23545;&#20869;&#23384;&#21644;&#24310;&#36831;&#30340;&#24433;&#21709;&#12290;&#36825;&#31181;&#26041;&#27861;&#24341;&#20837;&#20102;&#20010;&#24615;&#21270;&#30340;&#8220;&#29983;&#35789;&#36866;&#37197;&#22120;&#8221;&#65292;&#26377;&#25928;&#22320;&#20174;&#20013;&#22830;&#27169;&#22411;&#20256;&#36755;&#30693;&#35782;&#65292;&#24182;&#20026;&#20010;&#24615;&#21270;&#35789;&#27719;&#23398;&#20064;&#21333;&#35789;&#23884;&#20837;&#12290;&#22312;&#19968;&#32452;&#24120;&#35265;&#30340;&#32852;&#37030;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#29983;&#35789;&#25193;&#23637;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#26631;&#20934;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Federated Learning (FL) has shown significant advancements in its ability to perform various natural language processing (NLP) tasks. This work focuses on applying personalized FL for on-device language modeling. Due to limitations of memory and latency, these models cannot support the complexity of sub-word tokenization or beam search decoding, resulting in the decision to deploy a closed-vocabulary language model. However, closed-vocabulary models are unable to handle out-of-vocabulary (OOV) words belonging to specific users. To address this issue, We propose a novel technique called "OOV expansion" that improves OOV coverage and increases model accuracy while minimizing the impact on memory and latency. This method introduces a personalized "OOV adapter" that effectively transfers knowledge from a central model and learns word embedding for personalized vocabulary. OOV expansion significantly outperforms standard FL personalization methods on a set of common FL benc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#26426;&#22120;&#32763;&#35793;&#30340;&#22312;&#35821;&#22659;&#23398;&#20064;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#22312;&#35821;&#22659;&#23398;&#20064;&#24212;&#35813;&#26159;&#20445;&#25345;&#32467;&#26524;&#19982;&#20854;&#35821;&#22659;&#20855;&#26377;&#36830;&#36143;&#24615;&#30340;&#29983;&#25104;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#20855;&#26377;&#38271;&#26399;&#36830;&#36143;&#24615;&#30340;&#26679;&#20363;&#29992;&#20110;&#32763;&#35793;&#21487;&#20197;&#25552;&#39640;&#32763;&#35793;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.03573</link><description>&lt;p&gt;
&#22312;&#35821;&#22659;&#23398;&#20064;&#20013;&#20445;&#25345;&#36830;&#36143;&#24615;&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21363;&#24109;&#26426;&#22120;&#32763;&#35793;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
In-context Learning as Maintaining Coherency: A Study of On-the-fly Machine Translation Using Large Language Models. (arXiv:2305.03573v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#26426;&#22120;&#32763;&#35793;&#30340;&#22312;&#35821;&#22659;&#23398;&#20064;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#22312;&#35821;&#22659;&#23398;&#20064;&#24212;&#35813;&#26159;&#20445;&#25345;&#32467;&#26524;&#19982;&#20854;&#35821;&#22659;&#20855;&#26377;&#36830;&#36143;&#24615;&#30340;&#29983;&#25104;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#20855;&#26377;&#38271;&#26399;&#36830;&#36143;&#24615;&#30340;&#26679;&#20363;&#29992;&#20110;&#32763;&#35793;&#21487;&#20197;&#25552;&#39640;&#32763;&#35793;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#25226;&#22312;&#35821;&#22659;&#23398;&#20064;&#29616;&#35937;&#30475;&#20570;&#26159;&#8220;&#20174;&#20363;&#23376;&#20013;&#23398;&#20064;&#8221;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#32763;&#35793;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#23558;&#22312;&#35821;&#22659;&#23398;&#20064;&#30475;&#20316;&#26159;&#29983;&#25104;&#32467;&#26524;&#20219;&#21153;&#65292;&#35201;&#27714;&#32467;&#26524;&#19982;&#20854;&#35821;&#22659;&#20855;&#26377;&#36830;&#36143;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;4&#20010;&#39046;&#22495;&#30340;&#38543;&#26426;&#26679;&#20363;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#26174;&#31034;&#39046;&#22495;&#20869;&#26679;&#20363;&#26102;&#65292;&#32763;&#35793;&#24615;&#33021;&#24471;&#21040;&#20102;&#25552;&#21319;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#31227;&#21160;&#31383;&#21475;&#30340;&#39046;&#22495;&#20869;&#35774;&#32622;&#20013;&#30340;&#36830;&#36143;&#24615;&#65292;&#24182;&#23558;&#20854;&#19982;&#25991;&#29486;&#20013;&#20808;&#21069;&#30830;&#23450;&#30340;&#20854;&#20182;&#22240;&#32032;&#22914;&#38271;&#24230;&#12289;&#34920;&#38754;&#30456;&#20284;&#24615;&#21644;&#21477;&#23376;&#23884;&#20837;&#30456;&#20284;&#24615;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;3&#20010;&#27169;&#22411;&#65288;GPTNeo2.7B&#12289;Bloom3B&#12289;XGLM2.9B&#65289;&#21644;3&#20010;&#32763;&#35793;&#26041;&#21521;&#65288;en&#8594;{pt&#65292;de&#65292;fr}&#65289;&#20013;&#65292;&#26679;&#20363;&#19982;&#27979;&#35797;&#21477;&#23376;&#30340;&#38271;&#26399;&#36830;&#36143;&#24615;&#26159;&#27969;&#21521;&#32763;&#35793;&#24615;&#33021;&#30340;&#33391;&#22909;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
The phenomena of in-context learning has typically been thought of as "learning from examples". In this work which focuses on Machine Translation, we present a perspective of in-context learning as the desired generation task maintaining coherency with its context, i.e., the prompt examples. We first investigate randomly sampled prompts across 4 domains, and find that translation performance improves when shown in-domain prompts. Next, we investigate coherency for the in-domain setting, which uses prompt examples from a moving window. We study this with respect to other factors that have previously been identified in the literature such as length, surface similarity and sentence embedding similarity. Our results across 3 models (GPTNeo2.7B, Bloom3B, XGLM2.9B), and three translation directions (\texttt{en}$\rightarrow$\{\texttt{pt, de, fr}\}) suggest that the long-term coherency of the prompts and the test sentence is a good indicator of downstream translation performance. In doing so, 
&lt;/p&gt;</description></item><item><title>LeHoPP&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#36827;&#34892;&#36755;&#20837;&#20687;&#32032;&#20462;&#21098;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#22522;&#20110;&#22270;&#20687;&#30340;&#28210;&#26579;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#21512;&#25104;&#36136;&#37327;&#21644;&#20687;&#32032;&#29575;&#20043;&#38388;&#36798;&#21040;&#33391;&#22909;&#24179;&#34913;&#65292;&#25552;&#39640;&#22270;&#20687;&#21512;&#25104;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.03572</link><description>&lt;p&gt;
&#23398;&#20064;&#22914;&#20309;&#20026;&#22810;&#35270;&#35282;&#31070;&#32463;&#22270;&#20687;&#21512;&#25104;&#20462;&#21098;&#20687;&#32032;
&lt;/p&gt;
&lt;p&gt;
Learn how to Prune Pixels for Multi-view Neural Image-based Synthesis. (arXiv:2305.03572v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03572
&lt;/p&gt;
&lt;p&gt;
LeHoPP&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#36827;&#34892;&#36755;&#20837;&#20687;&#32032;&#20462;&#21098;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#22522;&#20110;&#22270;&#20687;&#30340;&#28210;&#26579;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#21512;&#25104;&#36136;&#37327;&#21644;&#20687;&#32032;&#29575;&#20043;&#38388;&#36798;&#21040;&#33391;&#22909;&#24179;&#34913;&#65292;&#25552;&#39640;&#22270;&#20687;&#21512;&#25104;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#20687;&#30340;&#28210;&#26579;&#25216;&#26415;&#26159;&#29992;&#25143;&#27785;&#28024;&#24335;&#20307;&#39564;&#30340;&#26680;&#24515;&#65292;&#23427;&#20204;&#22312;&#32473;&#23450;&#22810;&#20010;&#36755;&#20837;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#26032;&#35270;&#22270;&#12290;&#30001;&#20110;&#23427;&#20204;&#22312;&#23458;&#35266;&#21644;&#20027;&#35266;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#22240;&#27492;&#30740;&#31350;&#31038;&#21306;&#33268;&#21147;&#20110;&#23427;&#20204;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#22788;&#20110;&#24102;&#23485;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#26102;&#25152;&#38656;&#25968;&#25454;&#30340;&#22823;&#37327;&#20351;&#24471;&#22312;&#23454;&#26102;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LeHoPP&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#26597;&#27599;&#20010;&#36755;&#20837;&#20687;&#32032;&#23545;&#20110;&#28210;&#26579;&#30340;&#35270;&#22270;&#30340;&#37325;&#35201;&#24615;&#24182;&#36991;&#20813;&#20351;&#29992;&#19981;&#30456;&#20851;&#30340;&#20687;&#32032;&#30340;&#26041;&#27861;&#36827;&#34892;&#36755;&#20837;&#20687;&#32032;&#20462;&#21098;&#12290;&#21363;&#20351;&#19981;&#37325;&#26032;&#35757;&#32451;&#22522;&#20110;&#22270;&#20687;&#30340;&#28210;&#26579;&#32593;&#32476;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#31034;&#20986;&#21512;&#25104;&#36136;&#37327;&#21644;&#20687;&#32032;&#29575;&#20043;&#38388;&#30340;&#33391;&#22909;&#24179;&#34913;&#12290;&#22312;&#27979;&#35797;&#31070;&#32463;&#28210;&#26579;&#26694;&#26550;&#26102;&#65292;&#19982;&#20854;&#20182;&#20462;&#21098;&#22522;&#32447;&#30456;&#27604;&#65292;LeHoPP&#24179;&#22343;&#22686;&#30410;&#22312;0.9 dB&#21040;3.6 dB&#20043;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-based rendering techniques stand at the core of an immersive experience for the user, as they generate novel views given a set of multiple input images. Since they have shown good performance in terms of objective and subjective quality, the research community devotes great effort to their improvement. However, the large volume of data necessary to render at the receiver's side hinders applications in limited bandwidth environments or prevents their employment in real-time applications. We present LeHoPP, a method for input pixel pruning, where we examine the importance of each input pixel concerning the rendered view, and we avoid the use of irrelevant pixels. Even without retraining the image-based rendering network, our approach shows a good trade-off between synthesis quality and pixel rate. When tested in the general neural rendering framework, compared to other pruning baselines, LeHoPP gains between $0.9$ dB and $3.6$ dB on average.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#23376;&#31354;&#38388;&#23398;&#20064;&#30340;&#40657;&#21283;&#23376;&#25552;&#31034;&#35843;&#20248;&#65288;BSL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#22312;&#30456;&#20284;&#20219;&#21153;&#30340;&#38598;&#21512;&#20013;&#35782;&#21035;&#23376;&#31354;&#38388;&#65292;&#20197;&#25913;&#21892;&#40657;&#21283;&#23376;&#25552;&#31034;&#35843;&#20248;&#30340;&#36890;&#29992;&#24615;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#21644;LLMs&#19978;&#37117;&#33021;&#22815;&#25345;&#32493;&#36798;&#21040;&#31454;&#20105;&#24615;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.03518</link><description>&lt;p&gt;
&#22522;&#20110;&#23376;&#31354;&#38388;&#23398;&#20064;&#30340;&#40657;&#21283;&#23376;&#25552;&#31034;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Black-box Prompt Tuning with Subspace Learning. (arXiv:2305.03518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#23376;&#31354;&#38388;&#23398;&#20064;&#30340;&#40657;&#21283;&#23376;&#25552;&#31034;&#35843;&#20248;&#65288;BSL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#22312;&#30456;&#20284;&#20219;&#21153;&#30340;&#38598;&#21512;&#20013;&#35782;&#21035;&#23376;&#31354;&#38388;&#65292;&#20197;&#25913;&#21892;&#40657;&#21283;&#23376;&#25552;&#31034;&#35843;&#20248;&#30340;&#36890;&#29992;&#24615;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#21644;LLMs&#19978;&#37117;&#33021;&#22815;&#25345;&#32493;&#36798;&#21040;&#31454;&#20105;&#24615;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#21283;&#23376;&#25552;&#31034;&#35843;&#20248;&#20351;&#29992;&#26080;&#23548;&#25968;&#20248;&#21270;&#31639;&#27861;&#22312;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#23398;&#20064;&#25552;&#31034;&#65292;&#32780;&#19981;&#26159;&#36890;&#36807;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#32593;&#32476;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#40657;&#21283;&#23376;&#25552;&#31034;&#35843;&#20248;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;LLMs&#19978;&#32570;&#20047;&#36890;&#29992;&#24615;&#65292;&#36825;&#19982;&#19981;&#24688;&#24403;&#30340;&#23376;&#31354;&#38388;&#36873;&#25321;&#26377;&#20851;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#23376;&#31354;&#38388;&#23398;&#20064;&#30340;&#40657;&#21283;&#23376;&#25552;&#31034;&#35843;&#20248;&#65288;BSL&#65289;&#26469;&#25913;&#21892;&#40657;&#21283;&#23376;&#25552;&#31034;&#35843;&#20248;&#30340;&#36890;&#29992;&#24615;&#12290;&#22522;&#20110;&#30456;&#20284;&#20219;&#21153;&#30340;&#38598;&#21512;&#19978;&#36827;&#34892;&#20803;&#23398;&#20064;&#20197;&#30830;&#23450;&#23376;&#31354;&#38388;&#21487;&#20197;&#35782;&#21035;&#31867;&#20284;&#20219;&#21153;&#30340;&#26368;&#20248;&#25552;&#31034;&#65292;&#24182;&#19988;&#21487;&#20197;&#20445;&#35777;&#22312;&#30456;&#20284;&#20219;&#21153;&#19978;&#36827;&#34892;&#23376;&#31354;&#38388;&#20248;&#21270;&#25214;&#21040;&#19968;&#20010;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#25552;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;BSL&#26694;&#26550;&#26080;&#35770;&#22312;&#19979;&#28216;&#20219;&#21153;&#21644;LLMs&#19978;&#37117;&#33021;&#22815;&#25345;&#32493;&#36798;&#21040;&#31454;&#20105;&#24615;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Black-box prompt tuning uses derivative-free optimization algorithms to learn prompts in low-dimensional subspaces instead of back-propagating through the network of Large Language Models (LLMs). Recent studies have found that black-box prompt tuning lacks versatility across tasks and LLMs, which we believe is related to the inappropriate choice of subspaces. In this paper, we propose Black-box prompt tuning with Subspace Learning (BSL) to improve the versatility of black-box prompt tuning. Based on the assumption that nearly optimal prompts for similar tasks exist in a common subspace, we propose identifying such subspaces by meta-learning on a set of similar source tasks. Therefore, for a target task that shares similarities with source tasks, we guarantee that optimizing in the subspace can find a prompt that performs well on the target task. Experiments confirm that our BSL framework consistently achieves competitive performance regardless of downstream tasks and LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#34701;&#21512;&#21644;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#35757;&#32451;&#24182;&#19988;&#36866;&#24212;&#20110;&#26032;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.03517</link><description>&lt;p&gt;
&#20174;&#25991;&#26412;&#20013;&#23454;&#29616;&#23569;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#35270;&#35273;&#34701;&#21512;&#20107;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Few-shot Domain-Adaptive Visually-fused Event Detection from Text. (arXiv:2305.03517v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#34701;&#21512;&#21644;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#35757;&#32451;&#24182;&#19988;&#36866;&#24212;&#20110;&#26032;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23558;&#36741;&#21161;&#27169;&#24577;&#22914;&#22270;&#20687;&#25972;&#21512;&#21040;&#20107;&#20214;&#26816;&#27979;&#27169;&#22411;&#20013;&#24050;&#32463;&#24341;&#36215;&#20154;&#20204;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#24773;&#22659;&#30340;&#22797;&#26434;&#24615;&#20419;&#20351;&#30740;&#31350;&#20154;&#21592;&#21033;&#29992;&#30456;&#20851;&#30340;&#35270;&#35273;&#19978;&#19979;&#25991;&#26469;&#25552;&#39640;&#20107;&#20214;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36825;&#20010;&#39046;&#22495;&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#31232;&#32570;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#38656;&#35201;&#35768;&#22810;&#26631;&#35760;&#22909;&#30340;&#25991;&#26412;-&#22270;&#20687;&#23545;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#22312;&#25512;&#26029;&#26102;&#26080;&#27861;&#33719;&#24471;&#35270;&#35273;&#19978;&#19979;&#25991;&#30340;&#38480;&#21046;&#20063;&#20250;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#20351;&#20854;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#38590;&#20197;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#35270;&#35273;&#34701;&#21512;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#24456;&#23569;&#30340;&#26631;&#35760;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#35757;&#32451;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#35270;&#35273;&#24819;&#35937;&#22120;&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#27809;&#26377;&#35270;&#35273;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#20174;&#25991;&#26412;&#20013;&#21512;&#25104;&#22270;&#20687;&#65292;&#24182;&#19988;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#23450;&#21046;&#21040;&#29305;&#23450;&#30340;&#39046;&#22495;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28040;&#38500;&#20102;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#24182;&#19988;&#21482;&#38656;&#23569;&#37327;&#30340;&#26631;&#35760;&#22270;&#20687;-&#25991;&#26412;&#23545;&#23601;&#21487;&#20197;&#36866;&#24212;&#20110;&#26032;&#39046;&#22495;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#26174;&#31034;&#20854;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incorporating auxiliary modalities such as images into event detection models has attracted increasing interest over the last few years. The complexity of natural language in describing situations has motivated researchers to leverage the related visual context to improve event detection performance. However, current approaches in this area suffer from data scarcity, where a large amount of labelled text-image pairs are required for model training. Furthermore, limited access to the visual context at inference time negatively impacts the performance of such models, which makes them practically ineffective in real-world scenarios. In this paper, we present a novel domain-adaptive visually-fused event detection approach that can be trained on a few labelled image-text paired data points. Specifically, we introduce a visual imaginator method that synthesises images from text in the absence of visual context. Moreover, the imaginator can be customised to a specific domain. In doing so, our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20915;&#31574;&#26641;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#32852;&#21512;&#20248;&#21270;&#25152;&#26377;&#26641;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#36138;&#24515;&#31639;&#27861;&#36896;&#25104;&#27425;&#20248;&#35299;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#20108;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#22312;&#22810;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03515</link><description>&lt;p&gt;
&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20915;&#31574;&#26641;
&lt;/p&gt;
&lt;p&gt;
Learning Decision Trees with Gradient Descent. (arXiv:2305.03515v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20915;&#31574;&#26641;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#32852;&#21512;&#20248;&#21270;&#25152;&#26377;&#26641;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#36138;&#24515;&#31639;&#27861;&#36896;&#25104;&#27425;&#20248;&#35299;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#20108;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#22312;&#22810;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#26159;&#29992;&#20110;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#24120;&#35265;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#39640;&#24230;&#30340;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20915;&#31574;&#26641;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#26159;&#38750;&#20984;&#21644;&#38750;&#21487;&#24494;&#30340;&#12290;&#22240;&#27492;&#65292;&#36890;&#24120;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#19968;&#31181;&#36138;&#23146;&#29983;&#38271;&#31639;&#27861;&#26469;&#23398;&#20064;&#20915;&#31574;&#26641;&#65292;&#22312;&#27599;&#20010;&#20869;&#37096;&#33410;&#28857;&#19978;&#23616;&#37096;&#26368;&#23567;&#21270;&#19981;&#32431;&#24230;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#36138;&#24515;&#36807;&#31243;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#30340;&#20915;&#31574;&#26641;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#38590;&#20197;&#22788;&#29702;&#30340;&#36724;&#23545;&#40784;&#20915;&#31574;&#26641;&#30340;&#26032;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#21644;&#30452;&#36890;&#31639;&#23376;&#22312;&#23494;&#38598;&#30340;&#20915;&#31574;&#26641;&#34920;&#31034;&#19978;&#32852;&#21512;&#20248;&#21270;&#25152;&#26377;&#26641;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20108;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#31867;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision Trees (DTs) are commonly used for many machine learning tasks due to their high degree of interpretability. However, learning a DT from data is a difficult optimization problem, as it is non-convex and non-differentiable. Therefore, common approaches learn DTs using a greedy growth algorithm that minimizes the impurity locally at each internal node. Unfortunately, this greedy procedure can lead to suboptimal trees. In this paper, we present a novel approach for learning hard, axis-aligned DTs with gradient descent. The proposed method uses backpropagation with a straight-through operator on a dense DT representation to jointly optimize all tree parameters. Our approach outperforms existing methods on binary classification benchmarks and achieves competitive results for multi-class tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#32763;&#35793;&#23545;&#40784;&#30340;&#26041;&#24335;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#12289;&#36328;&#35821;&#35328;&#30340;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#35813;&#26694;&#26550;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#22810;&#35821;&#35328;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.03510</link><description>&lt;p&gt;
&#22522;&#20110;&#32763;&#35793;&#23545;&#40784;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36328;&#35821;&#35328;&#36801;&#31227;&#30340;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Cross-lingual Transfer of Vision and Language Models via Translation-based Alignment. (arXiv:2305.03510v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#32763;&#35793;&#23545;&#40784;&#30340;&#26041;&#24335;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#12289;&#36328;&#35821;&#35328;&#30340;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#35813;&#26694;&#26550;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#22810;&#35821;&#35328;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#22312;&#36830;&#25509;&#22270;&#20687;&#21644;&#33521;&#35821;&#25991;&#26412;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#23613;&#31649;&#26368;&#36817;&#35797;&#22270;&#25193;&#23637;CLIP&#20197;&#25903;&#25345;&#20854;&#20182;&#35821;&#35328;&#65292;&#20294;&#30001;&#20110;&#36164;&#28304;&#19981;&#24179;&#34913;&#65292;&#35266;&#23519;&#21040;&#20102;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#26041;&#27861;&#20250;&#28040;&#32791;&#22823;&#37327;&#36164;&#28304;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#39640;&#25928;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#22522;&#20110;&#32763;&#35793;&#30340;&#23545;&#40784;&#26041;&#27861;&#26469;&#20943;&#36731;&#22810;&#35821;&#35328;&#24046;&#24322;&#65292;&#24182;&#25506;&#32034;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#26469;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#12290;&#22312;XTD&#21644;Multi30K&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#28085;&#30422;&#20102;&#38646;-shot&#12289;few-shot&#21644;&#20840;&#25968;&#25454;&#38598;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;11&#31181;&#35821;&#35328;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#26694;&#26550;&#26174;&#33879;&#20943;&#23569;&#20102;&#35821;&#35328;&#20043;&#38388;&#30340;&#22810;&#35821;&#35328;&#24046;&#24322;&#65292;&#24182;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained vision and language models such as CLIP have witnessed remarkable success in connecting images and texts with a primary focus on English texts. Despite recent efforts to extend CLIP to support other languages, disparities in performance among different languages have been observed due to uneven resource availability. Additionally, current cross-lingual transfer methods of those pre-trained models would consume excessive resources for a large number of languages. Therefore, we propose a new parameter-efficient cross-lingual transfer learning framework that utilizes a translation-based alignment method to mitigate multilingual disparities and explores parameter-efficient fine-tuning methods for parameter-efficient cross-lingual transfer. Extensive experiments on XTD and Multi30K datasets, covering 11 languages under zero-shot, few-shot, and full-dataset learning scenarios, show that our framework significantly reduces the multilingual disparities among languages and improves 
&lt;/p&gt;</description></item><item><title>Diffusion Explainer&#26159;&#31532;&#19968;&#20010;&#21487;&#20132;&#20114;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#35299;&#37322;&#31283;&#23450;&#25193;&#25955;&#22914;&#20309;&#23558;&#25991;&#26412;&#25552;&#31034;&#36716;&#21270;&#20026;&#22270;&#20687;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#21160;&#30011;&#21644;&#20132;&#20114;&#20803;&#32032;&#27969;&#30021;&#22320;&#22312;&#22810;&#20010;&#25277;&#35937;&#32423;&#21035;&#20043;&#38388;&#36807;&#28193;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#25552;&#31034;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.03509</link><description>&lt;p&gt;
Diffusion Explainer&#65306;&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#31283;&#23450;&#25193;&#25955;&#30340;&#21487;&#35270;&#21270;&#35299;&#37322;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion. (arXiv:2305.03509v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03509
&lt;/p&gt;
&lt;p&gt;
Diffusion Explainer&#26159;&#31532;&#19968;&#20010;&#21487;&#20132;&#20114;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#35299;&#37322;&#31283;&#23450;&#25193;&#25955;&#22914;&#20309;&#23558;&#25991;&#26412;&#25552;&#31034;&#36716;&#21270;&#20026;&#22270;&#20687;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#21160;&#30011;&#21644;&#20132;&#20114;&#20803;&#32032;&#27969;&#30021;&#22320;&#22312;&#22810;&#20010;&#25277;&#35937;&#32423;&#21035;&#20043;&#38388;&#36807;&#28193;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#25552;&#31034;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#21019;&#36896;&#36924;&#30495;&#30340;&#22270;&#20687;&#32780;&#33719;&#24471;&#20102;&#20840;&#29699;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22797;&#26434;&#30340;&#20869;&#37096;&#32467;&#26500;&#21644;&#25805;&#20316;&#24448;&#24448;&#20351;&#24471;&#38750;&#19987;&#19994;&#20154;&#21592;&#38590;&#20197;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; Diffusion Explainer&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#35299;&#37322;&#31283;&#23450;&#25193;&#25955;&#22914;&#20309;&#23558;&#25991;&#26412;&#25552;&#31034;&#36716;&#21270;&#20026;&#22270;&#20687;&#12290;Diffusion Explainer&#32039;&#23494;&#22320;&#23558;&#31283;&#23450;&#25193;&#25955;&#30340;&#22797;&#26434;&#32452;&#20214;&#30340;&#35270;&#35273;&#27010;&#36848;&#19982;&#20854;&#28508;&#22312;&#25805;&#20316;&#30340;&#35814;&#32454;&#35828;&#26126;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#21160;&#30011;&#21644;&#20132;&#20114;&#20803;&#32032;&#20351;&#29992;&#25143;&#21487;&#20197;&#27969;&#30021;&#22320;&#22312;&#22810;&#20010;&#25277;&#35937;&#32423;&#21035;&#20043;&#38388;&#36807;&#28193;&#12290;&#36890;&#36807;&#27604;&#36739;&#30001;&#20004;&#20010;&#30456;&#20851;&#25991;&#26412;&#25552;&#31034;&#24341;&#23548;&#30340;&#22270;&#20687;&#34920;&#31034;&#30340;&#28436;&#21464;&#26469;&#25351;&#23548;&#31934;&#32454;&#26102;&#38388;&#27493;&#38271;&#65292;&#29992;&#25143;&#21487;&#20197;&#21457;&#29616;&#25552;&#31034;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#24433;&#21709;&#12290;Diffusion Explainer&#22312;&#29992;&#25143;&#30340;Web&#27983;&#35272;&#22120;&#20013;&#26412;&#22320;&#36816;&#34892;&#65292;&#26080;&#38656;&#23433;&#35013;&#25110;&#19987;&#38376;&#30340;&#30828;&#20214;&#65292;&#25193;&#22823;&#20102;&#20844;&#20247;&#23545;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#25945;&#32946;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based generative models' impressive ability to create convincing images has captured global attention. However, their complex internal structures and operations often make them difficult for non-experts to understand. We present Diffusion Explainer, the first interactive visualization tool that explains how Stable Diffusion transforms text prompts into images. Diffusion Explainer tightly integrates a visual overview of Stable Diffusion's complex components with detailed explanations of their underlying operations, enabling users to fluidly transition between multiple levels of abstraction through animations and interactive elements. By comparing the evolutions of image representations guided by two related text prompts over refinement timesteps, users can discover the impact of prompts on image generation. Diffusion Explainer runs locally in users' web browsers without the need for installation or specialized hardware, broadening the public's education access to modern AI tec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21152;&#23494;&#25991;&#26412;&#25968;&#25454;&#19978;&#35757;&#32451;NLP&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#38750;&#21152;&#23494;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.03497</link><description>&lt;p&gt;
&#22312;&#21152;&#23494;&#25991;&#26412;&#19978;&#35757;&#32451;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20197;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Training Natural Language Processing Models on Encrypted Text for Enhanced Privacy. (arXiv:2305.03497v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21152;&#23494;&#25991;&#26412;&#25968;&#25454;&#19978;&#35757;&#32451;NLP&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#38750;&#21152;&#23494;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20113;&#26381;&#21153;&#22312;&#35757;&#32451;&#21644;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#25968;&#25454;&#38544;&#31169;&#24050;&#25104;&#20026;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#28857;&#12290;&#36825;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#22788;&#29702;&#28041;&#21450;&#20010;&#20154;&#36890;&#20449;&#21644;&#26426;&#23494;&#25991;&#20214;&#31561;&#25935;&#24863;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21152;&#23494;&#25991;&#26412;&#25968;&#25454;&#19978;&#35757;&#32451;NLP&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#38750;&#21152;&#23494;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#26550;&#26500;&#65292;&#21363;Doc2Vec + XGBoost&#21644;Doc2Vec + LSTM&#65292;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;20 Newsgroups&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21152;&#23494;&#21644;&#38750;&#21152;&#23494;&#27169;&#22411;&#22343;&#21487;&#23454;&#29616;&#30456;&#20284;&#30340;&#24615;&#33021;&#65292;&#35828;&#26126;&#25105;&#20204;&#30340;&#21152;&#23494;&#26041;&#27861;&#22312;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#26377;&#25928;&#22320;&#32500;&#25252;&#20102;&#25968;&#25454;&#38544;&#31169;&#12290;&#20026;&#20102;&#22797;&#21046;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#22312;&#20197;&#19979;&#22320;&#22336;&#25552;&#20379;&#20102;Colab&#31508;&#35760;&#26412;&#65306;https://t.ly/lR-TP
&lt;/p&gt;
&lt;p&gt;
With the increasing use of cloud-based services for training and deploying machine learning models, data privacy has become a major concern. This is particularly important for natural language processing (NLP) models, which often process sensitive information such as personal communications and confidential documents. In this study, we propose a method for training NLP models on encrypted text data to mitigate data privacy concerns while maintaining similar performance to models trained on non-encrypted data. We demonstrate our method using two different architectures, namely Doc2Vec+XGBoost and Doc2Vec+LSTM, and evaluate the models on the 20 Newsgroups dataset. Our results indicate that both encrypted and non-encrypted models achieve comparable performance, suggesting that our encryption method is effective in preserving data privacy without sacrificing model accuracy. In order to replicate our experiments, we have provided a Colab notebook at the following address: https://t.ly/lR-TP
&lt;/p&gt;</description></item><item><title>&#22312;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#21644; beam search &#30340;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#25913;&#36827;&#25552;&#31034;&#65292;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.03495</link><description>&lt;p&gt;
&#22522;&#20110;&#8220;&#26799;&#24230;&#19979;&#38477;&#8221;&#19982; beam search &#30340;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Automatic Prompt Optimization with "Gradient Descent" and Beam Search. (arXiv:2305.03495v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03495
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#21644; beam search &#30340;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#25913;&#36827;&#25552;&#31034;&#65292;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#36890;&#29992;&#26234;&#33021;&#26041;&#38754;&#23637;&#29616;&#20102;&#20986;&#33394;&#24615;&#33021;&#65292;&#20294;&#20854;&#33021;&#21147;&#20173;&#39640;&#24230;&#20381;&#36182;&#20110;&#25163;&#20889;&#30340;&#25552;&#31034;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35797;&#38169;&#23581;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#38750;&#21442;&#25968;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#8212;&#8212;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#65288;APO&#65289;&#65292;&#20854;&#28789;&#24863;&#26469;&#33258;&#20110;&#20351;&#29992;&#25968;&#20540;&#26799;&#24230;&#19979;&#38477;&#33258;&#21160;&#25913;&#36827;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown impressive performance as general purpose agents, but their abilities remain highly dependent on prompts which are hand written with onerous trial-and-error effort. We propose a simple and nonparametric solution to this problem, Automatic Prompt Optimization (APO), which is inspired by numerical gradient descent to automatically improve prompts, assuming access to training data and an LLM API. The algorithm uses minibatches of data to form natural language ``gradients'' that criticize the current prompt. The gradients are then ``propagated'' into the prompt by editing the prompt in the opposite semantic direction of the gradient. These gradient descent steps are guided by a beam search and bandit selection procedure which significantly improves algorithmic efficiency. Preliminary results across three benchmark NLP tasks and the novel problem of LLM jailbreak detection suggest that Automatic Prompt Optimization can outperform prior prompt editing 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#29983;&#25104;&#24335;&#38544;&#20889;&#25193;&#25955;&#8221;&#65288;GSD&#65289;&#30340;&#26032;&#22411;&#29983;&#25104;&#24335;&#38544;&#20889;&#26041;&#26696;&#65292;&#21033;&#29992;&#19968;&#31181;&#21453;&#28436;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;100&#65285;&#22320;&#24674;&#22797;&#38544;&#34255;&#30340;&#31192;&#23494;&#25968;&#25454;&#65292;&#24182;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#38544;&#20889;&#22270;&#20687;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2305.03472</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#38544;&#20889;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Generative Steganography Diffusion. (arXiv:2305.03472v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03472
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#29983;&#25104;&#24335;&#38544;&#20889;&#25193;&#25955;&#8221;&#65288;GSD&#65289;&#30340;&#26032;&#22411;&#29983;&#25104;&#24335;&#38544;&#20889;&#26041;&#26696;&#65292;&#21033;&#29992;&#19968;&#31181;&#21453;&#28436;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;100&#65285;&#22320;&#24674;&#22797;&#38544;&#34255;&#30340;&#31192;&#23494;&#25968;&#25454;&#65292;&#24182;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#38544;&#20889;&#22270;&#20687;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#38544;&#20889;&#26415;&#65288;GS&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#25216;&#26415;&#65292;&#23427;&#30452;&#25509;&#20174;&#31192;&#23494;&#25968;&#25454;&#20013;&#29983;&#25104;&#38544;&#34255;&#20449;&#24687;&#22270;&#20687;&#12290;&#26368;&#36817;&#24050;&#32463;&#24320;&#21457;&#20102;&#22522;&#20110;GAN&#25110;Flow&#30340;&#21508;&#31181;GS&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;GAN&#30340;GS&#26041;&#27861;&#30001;&#20110;&#32570;&#20047;&#32593;&#32476;&#21487;&#36870;&#24615;&#65292;&#26080;&#27861;&#23436;&#20840;&#24674;&#22797;&#38544;&#34255;&#30340;&#31192;&#23494;&#25968;&#25454;&#65292;&#32780;&#22522;&#20110;Flow&#30340;&#26041;&#27861;&#30001;&#20110;&#27599;&#20010;&#27169;&#22359;&#20013;&#20005;&#26684;&#30340;&#21487;&#36870;&#38480;&#21046;&#32780;&#20135;&#29983;&#36136;&#37327;&#36739;&#24046;&#30340;&#22270;&#20687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GS&#26041;&#26696;&#65292;&#31216;&#20026;&#8220;&#29983;&#25104;&#24335;&#38544;&#20889;&#25193;&#25955;&#8221;&#65288;GSD&#65289;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#21453;&#28436;&#25193;&#25955;&#27169;&#22411;&#8220;StegoDiffusion&#8221;&#26469;&#23454;&#29616;&#12290;&#23427;&#19981;&#20165;&#29983;&#25104;&#36924;&#30495;&#30340;&#38544;&#20889;&#22270;&#20687;&#65292;&#32780;&#19988;&#20801;&#35768;100&#65285;&#22320;&#24674;&#22797;&#38544;&#34255;&#30340;&#31192;&#23494;&#25968;&#25454;&#12290;&#25152;&#25552;&#20986;&#30340;StegoDiffusion&#27169;&#22411;&#21033;&#29992;&#38750;&#39532;&#23572;&#21487;&#22827;&#38142;&#21644;&#24555;&#36895;&#37319;&#26679;&#25216;&#26415;&#23454;&#29616;&#39640;&#25928;&#30340;&#38544;&#20889;&#22270;&#20687;&#29983;&#25104;&#12290;&#36890;&#36807;&#26681;&#25454;StegoDiffusion&#20013;&#30340;&#29983;&#25104;&#36807;&#31243;&#30340;&#36716;&#31227;&#27010;&#29575;&#26500;&#36896;&#19968;&#20010;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#65292;&#31192;&#23494;&#25968;&#25454;&#21644;&#38544;&#20889;&#20449;&#24687;&#21487;&#20197;&#34987;&#26377;&#25928;&#21644;&#21487;&#36870;&#22320;&#23884;&#20837;&#21040;&#38544;&#20889;&#22270;&#20687;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;GSD&#26041;&#26696;&#22312;&#38544;&#20889;&#22270;&#20687;&#36136;&#37327;&#21644;&#31192;&#23494;&#25968;&#25454;&#24674;&#22797;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative steganography (GS) is an emerging technique that generates stego images directly from secret data. Various GS methods based on GANs or Flow have been developed recently. However, existing GAN-based GS methods cannot completely recover the hidden secret data due to the lack of network invertibility, while Flow-based methods produce poor image quality due to the stringent reversibility restriction in each module. To address this issue, we propose a novel GS scheme called "Generative Steganography Diffusion" (GSD) by devising an invertible diffusion model named "StegoDiffusion". It not only generates realistic stego images but also allows for 100\% recovery of the hidden secret data. The proposed StegoDiffusion model leverages a non-Markov chain with a fast sampling technique to achieve efficient stego image generation. By constructing an ordinary differential equation (ODE) based on the transition probability of the generation process in StegoDiffusion, secret data and stego i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#35270;&#35282;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#30340;&#28151;&#21512;&#22411;&#25968;&#20540;&#25512;&#29702;&#38382;&#31572;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#34920;&#26684;&#35270;&#35282;&#12289;&#20851;&#31995;&#35270;&#35282;&#21644;&#25968;&#20540;&#35270;&#35282;&#25429;&#25417;&#28151;&#21512;&#25968;&#25454;&#20043;&#38388;&#30340;&#31890;&#24230;&#20851;&#31995;&#21644;&#31354;&#38388;&#32467;&#26500;&#20449;&#24687;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#25968;&#20540;&#25512;&#29702;&#20219;&#21153;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.03458</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#35299;&#20915;&#28151;&#21512;&#22411;&#25968;&#20540;&#25512;&#29702;&#38382;&#31572;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Multi-View Graph Representation Learning for Answering Hybrid Numerical Reasoning Question. (arXiv:2305.03458v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#35270;&#35282;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#30340;&#28151;&#21512;&#22411;&#25968;&#20540;&#25512;&#29702;&#38382;&#31572;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#34920;&#26684;&#35270;&#35282;&#12289;&#20851;&#31995;&#35270;&#35282;&#21644;&#25968;&#20540;&#35270;&#35282;&#25429;&#25417;&#28151;&#21512;&#25968;&#25454;&#20043;&#38388;&#30340;&#31890;&#24230;&#20851;&#31995;&#21644;&#31354;&#38388;&#32467;&#26500;&#20449;&#24687;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#25968;&#20540;&#25512;&#29702;&#20219;&#21153;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#22411;&#38382;&#31572; (HybridQA) &#25968;&#25454;&#21547;&#26377;&#25991;&#26412;&#19982;&#34920;&#26684;&#25968;&#25454;&#65292;&#38656;&#35201;&#27169;&#22411;&#36873;&#25321;&#21512;&#36866;&#35777;&#25454;&#36827;&#34892;&#25968;&#20540;&#25512;&#29702;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#30340;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#34920;&#36798;&#26641;&#30340;&#35299;&#30721;&#22120;&#26469;&#35299;&#20915;&#25968;&#20540;&#25512;&#29702;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#32534;&#30721;&#22120;&#26356;&#20542;&#21521;&#20110;&#20351;&#29992;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;(MRC)&#26041;&#27861;,&#23427;&#20197;&#34920;&#26684;&#24207;&#21015;&#21644;&#25991;&#26412;&#25340;&#25509;&#20316;&#20026;&#36755;&#20837;&#65292;&#30772;&#22351;&#20102;&#34920;&#26684;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#31890;&#24230;&#20851;&#31995;&#20197;&#21450;&#34920;&#26684;&#26412;&#36523;&#30340;&#31354;&#38388;&#32467;&#26500;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#35270;&#35282;&#22270;&#24418; (MVG) &#32534;&#30721;&#22120;&#26469;&#32771;&#34385;&#31890;&#24230;&#20043;&#38388;&#20851;&#31995;&#24182;&#20174;&#22810;&#20010;&#35270;&#35282;&#25429;&#25417;&#20851;&#31995;&#12290;&#36890;&#36807;&#20351;&#29992;MVGE&#20316;&#20026;&#27169;&#22359;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#38754;&#21521;&#20445;&#30041;&#28151;&#21512;&#25968;&#25454;&#21407;&#22987;&#29305;&#24449;&#30340;&#34920;&#26684;&#35270;&#22270;&#12289;&#20851;&#31995;&#35270;&#22270;&#21644;&#25968;&#20540;&#35270;&#22270;&#12290;&#25105;&#20204;&#22312;&#20844;&#24320;&#30340;&#34920;&#26684;&#25991;&#26412;&#28151;&#21512;QA&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#26174;&#31034;&#25105;&#20204;&#25552;&#20986;&#30340;MVGE&#22312;&#25968;&#20540;&#25512;&#29702;&#23376;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#20934;&#30830;&#29575;&#30456;&#23545;&#25552;&#39640;&#20102;27.0%&#12290;
&lt;/p&gt;
&lt;p&gt;
Hybrid question answering (HybridQA) over the financial report contains both textual and tabular data, and requires the model to select the appropriate evidence for the numerical reasoning task. Existing methods based on encoder-decoder framework employ a expression tree-based decoder to solve numerical reasoning problems. However, encoders rely more on Machine Reading Comprehension (MRC) methods, which take table serialization and text splicing as input, damaging the granularity relationship between table and text as well as the spatial structure information of table itself. In order to solve these problems, the paper proposes a Multi-View Graph (MVG) Encoder to take the relations among the granularity into account and capture the relations from multiple view. By utilizing MVGE as a module, we constuct Tabular View, Relation View and Numerical View which aim to retain the original characteristics of the hybrid data. We validate our model on the publicly available table-text hybrid QA 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32508;&#36848;&#20102;&#26426;&#22120;&#26234;&#33021;&#25216;&#26415;&#22312;&#23556;&#30005;&#22270;&#20687;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#23556;&#30005;&#26143;&#31995;&#30340;&#24418;&#24577;&#20998;&#31867;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.03435</link><description>&lt;p&gt;
&#23556;&#30005;&#22270;&#20687;&#31435;&#26041;&#20307;&#20998;&#31867;&#30340;&#26032;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Advances on the classification of radio image cubes. (arXiv:2305.03435v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32508;&#36848;&#20102;&#26426;&#22120;&#26234;&#33021;&#25216;&#26415;&#22312;&#23556;&#30005;&#22270;&#20687;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#23556;&#30005;&#26143;&#31995;&#30340;&#24418;&#24577;&#20998;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#23556;&#30005;&#26395;&#36828;&#38236;&#27599;&#22825;&#23558;&#20135;&#29983;&#25968;&#37327;&#32423;&#20026;&#33406;&#23383;&#33410;&#30340;&#25968;&#25454;&#65292;&#22914;Square Kilometre Array&#65288;SKA&#65289;&#31995;&#32479;&#12290;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26159;&#26410;&#30693;&#21644;&#32597;&#35265;&#22825;&#20307;&#29289;&#29702;&#29616;&#35937;&#30340;&#26469;&#28304;&#65292;&#36825;&#20123;&#29616;&#35937;&#20250;&#23548;&#33268;&#21457;&#29616;&#12290;&#28982;&#32780;&#65292;&#20165;&#20381;&#38752;&#20154;&#24037;&#36741;&#21161;&#21644;&#20256;&#32479;&#32479;&#35745;&#25216;&#26415;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#38656;&#35201;&#21033;&#29992;&#24378;&#22823;&#30340;&#26426;&#22120;&#26234;&#33021;&#12290;&#26368;&#36817;&#65292;&#26377;&#36234;&#26469;&#36234;&#22810;&#30340;&#31185;&#23398;&#25991;&#29486;&#30528;&#30524;&#20110;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22788;&#29702;&#23556;&#30005;&#22825;&#25991;&#23398;&#20013;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#28304;&#25552;&#21462;&#12289;&#24418;&#24577;&#20998;&#31867;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;&#26412;&#30740;&#31350;&#31616;&#35201;&#27010;&#36848;&#20102;&#26426;&#22120;&#26234;&#33021;&#25216;&#26415;&#22312;&#23556;&#30005;&#22270;&#20687;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#26159;&#23556;&#30005;&#26143;&#31995;&#30340;&#24418;&#24577;&#20998;&#31867;&#12290;&#23427;&#26088;&#22312;&#22522;&#20110;&#25968;&#25454;&#22797;&#26434;&#24615;&#12289;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#23556;&#30005;&#22825;&#25991;&#23398;&#26041;&#27861;&#30340;&#26032;&#39062;&#24615;&#65292;&#23545;&#30456;&#20851;&#35770;&#25991;&#36827;&#34892;&#35814;&#32454;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern radio telescopes will daily generate data sets on the scale of exabytes for systems like the Square Kilometre Array (SKA). Massive data sets are a source of unknown and rare astrophysical phenomena that lead to discoveries. Nonetheless, this is only plausible with the exploitation of intensive machine intelligence to complement human-aided and traditional statistical techniques. Recently, there has been a surge in scientific publications focusing on the use of artificial intelligence in radio astronomy, addressing challenges such as source extraction, morphological classification, and anomaly detection. This study presents a succinct, but comprehensive review of the application of machine intelligence techniques on radio images with emphasis on the morphological classification of radio galaxies. It aims to present a detailed synthesis of the relevant papers summarizing the literature based on data complexity, data pre-processing, and methodological novelty in radio astronomy. Th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#25945;&#24072;&#19982;&#23398;&#29983;&#30340;&#20114;&#21160;&#65292;&#25552;&#39640;&#25945;&#23398;&#36136;&#37327;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#25945;&#32946;&#25968;&#25454;&#38598;&#65292;&#22788;&#29702;&#38271;&#26102;&#38388;&#30340;&#23545;&#35805;&#20197;&#21450;&#21387;&#32553;&#20449;&#24687;&#20197;&#26356;&#22909;&#22320;&#23436;&#25104;&#26356;&#22810;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.03433</link><description>&lt;p&gt;
&#25506;&#32034;&#24212;&#29992;&#24378;&#22823;&#30340;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#35838;&#22530;&#25945;&#23398;&#20013;&#65306;&#26426;&#36935;&#12289;&#25361;&#25112;&#21644;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
Towards Applying Powerful Large AI Models in Classroom Teaching: Opportunities, Challenges and Prospects. (arXiv:2305.03433v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#25945;&#24072;&#19982;&#23398;&#29983;&#30340;&#20114;&#21160;&#65292;&#25552;&#39640;&#25945;&#23398;&#36136;&#37327;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#25945;&#32946;&#25968;&#25454;&#38598;&#65292;&#22788;&#29702;&#38271;&#26102;&#38388;&#30340;&#23545;&#35805;&#20197;&#21450;&#21387;&#32553;&#20449;&#24687;&#20197;&#26356;&#22909;&#22320;&#23436;&#25104;&#26356;&#22810;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#20114;&#21160;&#22330;&#26223;&#65292;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22686;&#24378;&#35838;&#22530;&#25945;&#23398;&#65292;&#22914;&#23545;&#35805;&#33258;&#21160;&#23436;&#25104;&#12289;&#30693;&#35782;&#21644;&#39118;&#26684;&#36716;&#31227;&#20197;&#21450;&#35780;&#20272;AI&#29983;&#25104;&#20869;&#23481;&#12290;&#36890;&#36807;&#21033;&#29992;&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#65292;&#26412;&#25991;&#25506;&#32034;&#20102;AI&#22686;&#24378;&#21644;&#20016;&#23500;&#24072;&#29983;&#23545;&#35805;&#12289;&#25552;&#39640;&#25945;&#23398;&#36136;&#37327;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#25945;&#24072;&#19982;&#23398;&#29983;&#20043;&#38388;&#20135;&#29983;&#21019;&#26032;&#21644;&#26377;&#24847;&#20041;&#30340;&#23545;&#35805;&#65292;&#21046;&#23450;&#35780;&#20272;&#26631;&#20934;&#65292;&#24182;&#25552;&#39640;AI&#25945;&#32946;&#35745;&#21010;&#30340;&#25928;&#21147;&#12290;&#22312;&#31532;&#19977;&#33410;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#21033;&#29992;&#29616;&#26377;LLM&#26377;&#25928;&#23436;&#25104;&#25945;&#32946;&#20219;&#21153;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#25945;&#32946;&#25968;&#25454;&#38598;&#65292;&#22788;&#29702;&#38271;&#26102;&#38388;&#30340;&#23545;&#35805;&#20197;&#21450;&#21387;&#32553;&#20449;&#24687;&#20197;&#26356;&#22909;&#22320;&#23436;&#25104;&#26356;&#22810;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#22312;&#31532;&#22235;&#33410;&#20013;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#20851;&#38190;&#20219;&#21153;&#65292;&#21253;&#25324;&#25945;&#24072;-&#23398;&#29983;&#23545;&#35805;&#33258;&#21160;&#23436;&#25104;&#12289;&#31034;&#33539;&#31572;&#26696;&#21644;AI&#25903;&#25345;&#30340;&#20889;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
This perspective paper proposes a series of interactive scenarios that utilize Artificial Intelligence (AI) to enhance classroom teaching, such as dialogue auto-completion, knowledge and style transfer, and assessment of AI-generated content. By leveraging recent developments in Large Language Models (LLMs), we explore the potential of AI to augment and enrich teacher-student dialogues and improve the quality of teaching. Our goal is to produce innovative and meaningful conversations between teachers and students, create standards for evaluation, and improve the efficacy of AI-for-Education initiatives. In Section 3, we discuss the challenges of utilizing existing LLMs to effectively complete the educated tasks and present a unified framework for addressing diverse education dataset, processing lengthy conversations, and condensing information to better accomplish more downstream tasks. In Section 4, we summarize the pivoting tasks including Teacher-Student Dialogue Auto-Completion, Ex
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#23376;&#31354;&#38388;&#32858;&#31867;&#26041;&#27861;&#65288;AGCSC&#65289;&#65292;&#23558;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#21644;&#31995;&#25968;&#30697;&#38453;&#32422;&#26463;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#26356;&#30495;&#23454;&#30340;&#23376;&#31354;&#38388;&#32467;&#26500;&#25581;&#31034;&#65292;&#24182;&#22312;&#22823;&#37327;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.03414</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#23376;&#31354;&#38388;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Adaptive Graph Convolutional Subspace Clustering. (arXiv:2305.03414v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#23376;&#31354;&#38388;&#32858;&#31867;&#26041;&#27861;&#65288;AGCSC&#65289;&#65292;&#23558;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#21644;&#31995;&#25968;&#30697;&#38453;&#32422;&#26463;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#26356;&#30495;&#23454;&#30340;&#23376;&#31354;&#38388;&#32467;&#26500;&#25581;&#31034;&#65292;&#24182;&#22312;&#22823;&#37327;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#35889;&#22411;&#23376;&#31354;&#38388;&#32858;&#31867;&#31639;&#27861;&#22312;&#35768;&#22810;&#23376;&#31354;&#38388;&#32858;&#31867;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#29616;&#26377;&#30340;&#20809;&#35889;&#22411;&#23376;&#31354;&#38388;&#32858;&#31867;&#31639;&#27861;&#35201;&#20040;&#19987;&#27880;&#20110;&#35774;&#35745;&#37325;&#26500;&#31995;&#25968;&#30697;&#38453;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#35201;&#20040;&#20351;&#29992;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#25214;&#21040;&#21407;&#22987;&#25968;&#25454;&#26679;&#26412;&#30340;&#28508;&#22312;&#29305;&#24449;&#12290;&#26412;&#25991;&#21463;&#21040;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#21551;&#21457;&#65292;&#20351;&#29992;&#22270;&#21367;&#31215;&#25216;&#26415;&#21516;&#26102;&#24320;&#21457;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#21644;&#31995;&#25968;&#30697;&#38453;&#32422;&#26463;&#12290;&#25105;&#20204;&#22312;&#25552;&#20986;&#30340;&#31639;&#27861;&#20013;&#36845;&#20195;&#21644;&#33258;&#36866;&#24212;&#22320;&#26356;&#26032;&#22270;&#21367;&#31215;&#31639;&#23376;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#31216;&#20026;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#23376;&#31354;&#38388;&#32858;&#31867;&#65288;AGCSC&#65289;&#12290;&#25105;&#20204;&#22768;&#31216;&#65292;&#36890;&#36807;&#20351;&#29992;AGCSC&#65292;&#21407;&#22987;&#25968;&#25454;&#26679;&#26412;&#30340;&#32858;&#21512;&#29305;&#24449;&#34920;&#31034;&#36866;&#21512;&#20110;&#23376;&#31354;&#38388;&#32858;&#31867;&#65292;&#31995;&#25968;&#30697;&#38453;&#21487;&#20197;&#26356;&#30495;&#23454;&#22320;&#25581;&#31034;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;&#23376;&#31354;&#38388;&#32467;&#26500;&#12290;&#26368;&#21518;&#65292;&#36827;&#34892;&#20102;&#22823;&#37327;&#23376;&#31354;&#38388;&#32858;&#31867;&#23454;&#39564;&#65292;&#20197;&#39564;&#35777;AGCSC&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectral-type subspace clustering algorithms have shown excellent performance in many subspace clustering applications. The existing spectral-type subspace clustering algorithms either focus on designing constraints for the reconstruction coefficient matrix or feature extraction methods for finding latent features of original data samples. In this paper, inspired by graph convolutional networks, we use the graph convolution technique to develop a feature extraction method and a coefficient matrix constraint simultaneously. And the graph-convolutional operator is updated iteratively and adaptively in our proposed algorithm. Hence, we call the proposed method adaptive graph convolutional subspace clustering (AGCSC). We claim that by using AGCSC, the aggregated feature representation of original data samples is suitable for subspace clustering, and the coefficient matrix could reveal the subspace structure of the original data set more faithfully. Finally, plenty of subspace clustering ex
&lt;/p&gt;</description></item><item><title>&#38543;&#30528;&#33258;&#20027;&#31995;&#32479;&#22312;&#31038;&#20250;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#35780;&#20272;&#20854;&#21487;&#20449;&#24230;&#26159;&#24517;&#35201;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#29992;&#20316;&#33258;&#20027;&#31995;&#32479;&#21487;&#20449;&#24230;&#35780;&#20272;&#26694;&#26550;&#30340;&#36807;&#31243;&#27010;&#36848;&#12290;</title><link>http://arxiv.org/abs/2305.03411</link><description>&lt;p&gt;
&#33258;&#20027;&#31995;&#32479;&#30340;&#21487;&#20449;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Assessing Trustworthiness of Autonomous Systems. (arXiv:2305.03411v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03411
&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#20027;&#31995;&#32479;&#22312;&#31038;&#20250;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#35780;&#20272;&#20854;&#21487;&#20449;&#24230;&#26159;&#24517;&#35201;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#29992;&#20316;&#33258;&#20027;&#31995;&#32479;&#21487;&#20449;&#24230;&#35780;&#20272;&#26694;&#26550;&#30340;&#36807;&#31243;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#20027;&#31995;&#32479;&#22312;&#31038;&#20250;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#23545;&#20110;&#25105;&#20204;&#30340;&#23433;&#20840;&#21644;&#19982;&#23427;&#20204;&#30340;&#20132;&#20114;&#36234;&#26469;&#36234;&#39057;&#32321;&#65292;&#20351;&#23427;&#20204;&#20449;&#24471;&#36807;&#26159;&#24517;&#35201;&#30340;&#12290;&#35780;&#20272;&#33258;&#20027;&#31995;&#32479;&#30340;&#21487;&#20449;&#24230;&#23545;&#20110;&#39564;&#35777;&#21644;&#24320;&#21457;&#31038;&#21306;&#26469;&#35828;&#26159;&#19968;&#39033;&#24517;&#35201;&#30340;&#25361;&#25112;&#12290;&#36825;&#23558;&#38656;&#35201;&#36866;&#24403;&#30340;&#26631;&#20934;&#21644;&#36866;&#24403;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#21487;&#20197;&#22312;&#24403;&#21069;&#21644;&#26410;&#26469;&#30340;&#24191;&#27867;&#24212;&#29992;&#33539;&#22260;&#20869;&#23458;&#35266;&#22320;&#21644;&#27604;&#36739;&#22320;&#21028;&#26029;AS&#30340;&#21487;&#20449;&#24230;&#12290;&#22312;&#25991;&#31456;&#20013;&#65292;&#32771;&#34385;&#21040;&#25991;&#29486;&#20013;&#26500;&#25104;&#36825;&#20010;&#35789;&#30340;&#30456;&#20851;&#29305;&#36136;&#65292;&#23545;&#33258;&#20027;&#31995;&#32479;&#65288;AS&#65289;&#20013;&#30340;&#8220;&#21487;&#20449;&#24230;&#8221;&#19968;&#35789;&#36827;&#34892;&#20102;&#23457;&#26597;&#12290;&#22238;&#39038;&#20102;&#25903;&#25345;&#33258;&#20027;&#31995;&#32479;&#20445;&#35777;&#30340;&#26631;&#20934;&#21644;&#26694;&#26550;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;&#25105;&#20204;&#21015;&#20986;&#20102;&#31038;&#21306;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#29992;&#20316;&#33258;&#20027;&#31995;&#32479;&#21487;&#20449;&#24230;&#35780;&#20272;&#26694;&#26550;&#30340;&#36807;&#31243;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Autonomous Systems (AS) become more ubiquitous in society, more responsible for our safety and our interaction with them more frequent, it is essential that they are trustworthy. Assessing the trustworthiness of AS is a mandatory challenge for the verification and development community. This will require appropriate standards and suitable metrics that may serve to objectively and comparatively judge trustworthiness of AS across the broad range of current and future applications. The meta-expression `trustworthiness' is examined in the context of AS capturing the relevant qualities that comprise this term in the literature. Recent developments in standards and frameworks that support assurance of autonomous systems are reviewed. A list of key challenges are identified for the community and we present an outline of a process that can be used as a trustworthiness assessment framework for AS.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CAAFE&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#25968;&#25454;&#38598;&#25551;&#36848;&#29983;&#25104;&#26356;&#22810;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#33021;&#22815;&#25552;&#39640;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;ROC AUC&#34920;&#29616;&#25552;&#39640;&#33267;0.822&#12290;</title><link>http://arxiv.org/abs/2305.03403</link><description>&lt;p&gt;
GPT&#29992;&#20110;&#21322;&#33258;&#21160;&#21270;&#25968;&#25454;&#31185;&#23398;&#65306;&#24341;&#20837;CAAFE&#23454;&#29616;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
GPT for Semi-Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering. (arXiv:2305.03403v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03403
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CAAFE&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#25968;&#25454;&#38598;&#25551;&#36848;&#29983;&#25104;&#26356;&#22810;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#33021;&#22815;&#25552;&#39640;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;ROC AUC&#34920;&#29616;&#25552;&#39640;&#33267;0.822&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#32435;&#20837;&#36825;&#20123;&#31995;&#32479;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24378;&#22823;&#21151;&#33021;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#65292;&#21517;&#20026;&#19978;&#19979;&#25991;&#24863;&#30693;&#33258;&#21160;&#29305;&#24449;&#24037;&#31243;&#65288;CAAFE&#65289;&#65292;&#23427;&#21033;&#29992;LLM&#26681;&#25454;&#25968;&#25454;&#38598;&#30340;&#25551;&#36848;&#29983;&#25104;&#26356;&#22810;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#20135;&#29983;&#29992;&#20110;&#21019;&#24314;&#26032;&#29305;&#24449;&#30340;Python&#20195;&#30721;&#65292;&#24182;&#25552;&#20379;&#29983;&#25104;&#29305;&#24449;&#30340;&#25928;&#29992;&#35828;&#26126;&#12290;&#23613;&#31649;&#26041;&#27861;&#35770;&#19978;&#24456;&#31616;&#21333;&#65292;&#20294;CAAFE&#25552;&#39640;&#20102;14&#20010;&#25968;&#25454;&#38598;&#20013;11&#20010;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#19982;2&#20010;&#25968;&#25454;&#38598;&#24182;&#21015;&#65292;&#21482;&#26377;1&#20010;&#25968;&#25454;&#38598;&#24615;&#33021;&#19979;&#38477;&#65292;&#20174;&#32780;&#20351;&#25152;&#26377;&#25968;&#25454;&#38598;&#30340;&#24179;&#22343;ROC AUC&#34920;&#29616;&#20174;0.798&#25552;&#21319;&#33267;0.822&#12290;&#23545;&#20110;&#25152;&#35780;&#20272;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#19968;&#25913;&#36827;&#19982;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#65288;AUC 0.782&#65289;&#20195;&#26367;&#36923;&#36753;&#22238;&#24402;&#65288;AUC 0.754&#65289;&#25152;&#33719;&#24471;&#30340;&#24179;&#22343;&#25913;&#36827;&#30456;&#20284;&#12290;&#27492;&#22806;&#65292;
&lt;/p&gt;
&lt;p&gt;
As the field of automated machine learning (AutoML) advances, it becomes increasingly important to include domain knowledge within these systems. We present an approach for doing so by harnessing the power of large language models (LLMs). Specifically, we introduce Context-Aware Automated Feature Engineering (CAAFE), a feature engineering method for tabular datasets that utilizes an LLM to generate additional semantically meaningful features for tabular datasets based on their descriptions. The method produces both Python code for creating new features and explanations for the utility of the generated features.  Despite being methodologically simple, CAAFE enhances performance on 11 out of 14 datasets, ties on 2 and looses on 1 - boosting mean ROC AUC performance from 0.798 to 0.822 across all datasets. On the evaluated datasets, this improvement is similar to the average improvement achieved by using a random forest (AUC 0.782) instead of logistic regression (AUC 0.754).  Furthermore,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22270;&#20013;&#24515;&#24615;&#30340;&#28388;&#27874;&#22120;&#21098;&#26525;&#21387;&#32553;&#26694;&#26550;&#65292;&#29992;&#20110;&#38899;&#39057;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#21487;&#20943;&#23569;95&#65285;&#30340;&#35745;&#31639;&#21644;97&#65285;&#30340;&#23384;&#20648;&#12290;</title><link>http://arxiv.org/abs/2305.03391</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#20013;&#24515;&#24615;&#28388;&#27874;&#21098;&#26525;&#30340;&#38899;&#39057;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Compressing audio CNNs with graph centrality based filter pruning. (arXiv:2305.03391v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22270;&#20013;&#24515;&#24615;&#30340;&#28388;&#27874;&#22120;&#21098;&#26525;&#21387;&#32553;&#26694;&#26550;&#65292;&#29992;&#20110;&#38899;&#39057;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#21487;&#20943;&#23569;95&#65285;&#30340;&#35745;&#31639;&#21644;97&#65285;&#30340;&#23384;&#20648;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#35299;&#20915;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#65292;&#22914;&#38899;&#39057;&#20998;&#31867;&#20013;&#65292;&#24050;&#32463;&#25104;&#20026;&#24120;&#35265;&#30340;&#39640;&#24615;&#33021;&#35299;&#20915;&#26041;&#26696;&#12290;&#20294;&#26159;&#65292;CNN&#20855;&#26377;&#35768;&#22810;&#21442;&#25968;&#21644;&#28388;&#27874;&#22120;&#65292;&#20854;&#20013;&#19968;&#20123;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#27604;&#20854;&#20182;&#26356;&#22823;&#12290;&#36825;&#24847;&#21619;&#30528;&#32593;&#32476;&#21487;&#33021;&#21253;&#21547;&#35768;&#22810;&#19981;&#24517;&#35201;&#30340;&#28388;&#27874;&#22120;&#65292;&#22686;&#21152;&#20102;CNN&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#65292;&#21516;&#26102;&#25552;&#20379;&#26377;&#38480;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;&#20026;&#20102;&#20351;CNN&#26356;&#39640;&#25928;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21098;&#26525;&#26694;&#26550;&#65292;&#28040;&#38500;&#20855;&#26377;&#26368;&#39640;&#8220;&#20849;&#36890;&#24615;&#8221;&#30340;&#28388;&#27874;&#22120;&#12290;&#25105;&#20204;&#20351;&#29992;&#22270;&#35770;&#30340;&#8220;&#20013;&#24515;&#24615;&#8221;&#27010;&#24565;&#26469;&#34913;&#37327;&#36825;&#31181;&#20849;&#36890;&#24615;&#12290;&#25105;&#20204;&#20551;&#35774;&#20855;&#26377;&#39640;&#20013;&#24515;&#24615;&#30340;&#28388;&#27874;&#22120;&#24212;&#35813;&#34987;&#28040;&#38500;&#65292;&#22240;&#20026;&#23427;&#20195;&#34920;&#20102;&#20849;&#36890;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20854;&#20182;&#28388;&#27874;&#22120;&#26367;&#25442;&#32780;&#19981;&#22826;&#24433;&#21709;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#25552;&#20986;&#30340;&#26694;&#26550;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#24212;&#29992;&#20110;&#22768;&#23398;&#22330;&#26223;&#20998;&#31867;&#21644;&#38899;&#39057;&#26631;&#35760;&#12290;&#22312;DCASE 2021&#20219;&#21153;1A&#22522;&#32447;&#32593;&#32476;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20943;&#23569;&#20102;95&#65285;&#30340;&#35745;&#31639;&#21644;97&#65285;&#30340;&#23384;&#20648;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks (CNNs) are commonplace in high-performing solutions to many real-world problems, such as audio classification. CNNs have many parameters and filters, with some having a larger impact on the performance than others. This means that networks may contain many unnecessary filters, increasing a CNN's computation and memory requirements while providing limited performance benefits. To make CNNs more efficient, we propose a pruning framework that eliminates filters with the highest "commonality". We measure this commonality using the graph-theoretic concept of "centrality". We hypothesise that a filter with a high centrality should be eliminated as it represents commonality and can be replaced by other filters without affecting the performance of a network much. An experimental evaluation of the proposed framework is performed on acoustic scene classification and audio tagging. On the DCASE 2021 Task 1A baseline network, our proposed method reduces computations p
&lt;/p&gt;</description></item><item><title>AI&#38656;&#35201;&#22823;&#37327;&#30340;&#8220;&#26080;&#24418;&#21171;&#21160;&#8221;&#65292;&#22899;&#24615;&#20027;&#20041;&#20132;&#21449;XAI&#21644;&#21046;&#22270;&#26159;&#35299;&#37322;&#38544;&#24418;&#21171;&#21160;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.03376</link><description>&lt;p&gt;
&#35299;&#35835;"&#24189;&#28789;"&#65306;&#22899;&#24615;&#20027;&#20041;&#20132;&#21449;XAI&#21644;&#21046;&#22270;&#20316;&#20026;&#35299;&#37322;&#38544;&#24418;&#21171;&#21160;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Explaining the ghosts: Feminist intersectional XAI and cartography as methods to account for invisible labour. (arXiv:2305.03376v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03376
&lt;/p&gt;
&lt;p&gt;
AI&#38656;&#35201;&#22823;&#37327;&#30340;&#8220;&#26080;&#24418;&#21171;&#21160;&#8221;&#65292;&#22899;&#24615;&#20027;&#20041;&#20132;&#21449;XAI&#21644;&#21046;&#22270;&#26159;&#35299;&#37322;&#38544;&#24418;&#21171;&#21160;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#20154;&#24037;&#26234;&#33021;&#36890;&#36807;&#33258;&#21160;&#21270;&#38656;&#35201;&#22823;&#37327;&#30340;&#24149;&#21518;&#20154;&#21147;&#21171;&#21160;&#65292;&#36825;&#31181;&#21171;&#21160;&#24448;&#24448;&#34987;&#38544;&#30610;&#21644;&#20302;&#20272;&#12290;&#30001;&#20110;&#21253;&#25324;&#26631;&#35760;&#21644;&#32500;&#25252;&#24037;&#20316;&#22312;&#20869;&#30340;&#38544;&#24418;&#21171;&#21160;&#26159;&#24403;&#20195;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#19968;&#20010;&#37325;&#35201;&#37096;&#20998;&#65292;&#22240;&#27492;&#35753;&#29992;&#25143;&#24847;&#35782;&#21040;&#36825;&#31181;&#21171;&#21160;&#30340;&#20316;&#29992;&#20381;&#28982;&#24456;&#37325;&#35201;&#12290;&#25105;&#20204;&#24314;&#35758;&#21487;&#20197;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;AI&#65288;XAI&#65289;&#35774;&#35745;&#65292;&#29305;&#21035;&#26159;&#22899;&#24615;&#20027;&#20041;&#20132;&#21449;XAI&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21046;&#22270;&#30340;&#26041;&#27861;&#65292;&#23427;&#26469;&#33258;&#22899;&#24615;&#20027;&#20041;&#20132;&#21449;&#30740;&#31350;&#65292;&#20197;&#32472;&#21046;AI&#30340;&#31995;&#32479;&#24615;&#35270;&#35282;&#65292;&#24182;&#21253;&#25324;&#19982;&#38544;&#24418;&#21171;&#21160;&#30456;&#20851;&#30340;AI&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contemporary automation through AI entails a substantial amount of behind-the-scenes human labour, which is often both invisibilised and underpaid. Since invisible labour, including labelling and maintenance work, is an integral part of contemporary AI systems, it remains important to sensitise users to its role. We suggest that this could be done through explainable AI (XAI) design, particularly feminist intersectional XAI. We propose the method of cartography, which stems from feminist intersectional research, to draw out a systemic perspective of AI and include dimensions of AI that pertain to invisible labour.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21628;&#21505;&#22312;&#35745;&#31639;&#21644;&#27010;&#24565;&#21270;&#26041;&#27861;&#20013;&#37319;&#29992;&#25209;&#21028;&#24615;&#26041;&#27861;&#65292;&#20851;&#27880;&#20132;&#21449;&#30340;&#22899;&#24615;&#20027;&#20041;&#35270;&#35282;&#65292;&#24182;&#25506;&#35752;&#22312;HCXAI&#30740;&#31350;&#21644;&#35774;&#35745;&#20013;&#23454;&#29616;&#22899;&#24615;&#20027;&#20041;&#20132;&#21449;&#35270;&#35282;&#25152;&#38656;&#30340;&#21709;&#24212;&#33021;&#21147;&#21644;&#20851;&#27880;&#36793;&#32536;&#21270;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2305.03375</link><description>&lt;p&gt;
&#36208;&#21521;&#22899;&#24615;&#20027;&#20041;&#20132;&#21449;XAI&#65306;&#20174;&#21487;&#35299;&#37322;&#24615;&#21040;&#21709;&#24212;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Towards Feminist Intersectional XAI: From Explainability to Response-Ability. (arXiv:2305.03375v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21628;&#21505;&#22312;&#35745;&#31639;&#21644;&#27010;&#24565;&#21270;&#26041;&#27861;&#20013;&#37319;&#29992;&#25209;&#21028;&#24615;&#26041;&#27861;&#65292;&#20851;&#27880;&#20132;&#21449;&#30340;&#22899;&#24615;&#20027;&#20041;&#35270;&#35282;&#65292;&#24182;&#25506;&#35752;&#22312;HCXAI&#30740;&#31350;&#21644;&#35774;&#35745;&#20013;&#23454;&#29616;&#22899;&#24615;&#20027;&#20041;&#20132;&#21449;&#35270;&#35282;&#25152;&#38656;&#30340;&#21709;&#24212;&#33021;&#21147;&#21644;&#20851;&#27880;&#36793;&#32536;&#21270;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21628;&#24212;&#23545;&#35745;&#31639;&#21644;&#27010;&#24565;&#21270;&#30340;&#20020;&#24202;&#26041;&#27861;&#30340;&#21628;&#21505;&#65292;&#20851;&#27880;&#20132;&#21449;&#30340;&#22899;&#24615;&#20027;&#20041;&#65292;&#21435;&#27542;&#27665;&#21270;&#30340;&#20154;&#26426;&#20132;&#20114;&#21644;&#20154;&#24037;&#26234;&#33021;&#35774;&#35745;&#26041;&#38754;&#65292;&#24182;&#35810;&#38382;&#22899;&#24615;&#20027;&#20041;&#20132;&#21449;&#35270;&#35282;&#22312;HCXAI&#30740;&#31350;&#21644;&#35774;&#35745;&#26041;&#38754;&#30340;&#21487;&#33021;&#24615;&#12290;&#21246;&#30011;&#21021;&#27493;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#23545;&#21487;&#35299;&#37322;AI&#35774;&#35745;&#30340;&#24433;&#21709;&#65292;&#24314;&#35758;&#20174;&#22899;&#24615;&#20027;&#20041;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#21487;&#35299;&#37322;&#24615;&#24212;&#21253;&#25324;&#22521;&#20859;&#21709;&#24212;&#33021;&#21147;&#8212;&#8212;&#25209;&#21028;&#35780;&#20272;&#21644;&#24212;&#23545;AI&#31995;&#32479;&#30340;&#33021;&#21147;&#8212;&#8212;&#24182;&#23558;&#36793;&#32536;&#21270;&#30340;&#35270;&#35282;&#32622;&#20110;&#20013;&#24515;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper follows calls for critical approaches to computing and conceptualisations of intersectional, feminist, decolonial HCI and AI design and asks what a feminist intersectional perspective in HCXAI research and design might look like. Sketching out initial research directions and implications for explainable AI design, it suggests that explainability from a feminist perspective would include the fostering of response-ability - the capacity to critically evaluate and respond to AI systems - and would centre marginalised perspectives.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#19968;&#31687;&#20851;&#20110;&#31163;&#32447;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26368;&#26032;&#36827;&#23637;&#30340;&#32508;&#36848;&#24615;&#25991;&#31456;&#65292;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#27010;&#24565;&#12289;&#26041;&#27861;&#21644;&#25361;&#25112;&#12290;&#25991;&#29486;&#32508;&#36848;&#20171;&#32461;&#20102;&#24403;&#21069;&#31163;&#32447;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#20851;&#38190;&#25991;&#29486;&#65292;&#24182;&#37325;&#28857;&#35752;&#35770;&#20102;&#22914;&#20309;&#35299;&#20915;&#20998;&#24067;&#21464;&#21270;&#38382;&#39064;&#12290;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#20063;&#34987;&#35752;&#35770;&#21644;&#24635;&#32467;&#12290;</title><link>http://arxiv.org/abs/2305.03360</link><description>&lt;p&gt;
&#31163;&#32447;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Offline Model-Based Reinforcement Learning. (arXiv:2305.03360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#19968;&#31687;&#20851;&#20110;&#31163;&#32447;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26368;&#26032;&#36827;&#23637;&#30340;&#32508;&#36848;&#24615;&#25991;&#31456;&#65292;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#27010;&#24565;&#12289;&#26041;&#27861;&#21644;&#25361;&#25112;&#12290;&#25991;&#29486;&#32508;&#36848;&#20171;&#32461;&#20102;&#24403;&#21069;&#31163;&#32447;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#20851;&#38190;&#25991;&#29486;&#65292;&#24182;&#37325;&#28857;&#35752;&#35770;&#20102;&#22914;&#20309;&#35299;&#20915;&#20998;&#24067;&#21464;&#21270;&#38382;&#39064;&#12290;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#20063;&#34987;&#35752;&#35770;&#21644;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#30001;&#20110;&#27169;&#22411;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#20013;&#22823;&#35268;&#27169;&#21382;&#21490;&#25968;&#25454;&#38598;&#30340;&#20248;&#21183;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#38750;&#24120;&#39640;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#23545;&#30446;&#21069;&#31163;&#32447;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#36827;&#34892;&#20102;&#25991;&#29486;&#32508;&#36848;&#12290;&#32508;&#36848;&#31616;&#35201;&#20171;&#32461;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#27010;&#24565;&#21644;&#26368;&#26032;&#21457;&#23637;&#65292;&#24182;&#25506;&#35752;&#20102;&#20004;&#20010;&#39046;&#22495;&#30340;&#20132;&#21449;&#28857;&#12290;&#25509;&#30528;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#31163;&#32447;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#20851;&#38190;&#25991;&#29486;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#23427;&#20204;&#22312;&#35299;&#20915;&#20998;&#24067;&#21464;&#21270;&#38382;&#39064;&#26041;&#38754;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#24403;&#21069;&#25152;&#26377;&#31163;&#32447;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#38754;&#20020;&#30340;&#20027;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#36824;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based approaches are becoming increasingly popular in the field of offline reinforcement learning, with high potential in real-world applications due to the model's capability of thoroughly utilizing the large historical datasets available with supervised learning techniques. This paper presents a literature review of recent work in offline model-based reinforcement learning, a field that utilizes model-based approaches in offline reinforcement learning. The survey provides a brief overview of the concepts and recent developments in both offline reinforcement learning and model-based reinforcement learning, and discuss the intersection of the two fields. We then presents key relevant papers in the field of offline model-based reinforcement learning and discuss their methods, particularly their approaches in solving the issue of distributional shift, the main problem faced by all current offline model-based reinforcement learning methods. We further discuss key challenges faced by
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35299;&#26512;-&#25191;&#34892;-&#20248;&#21270;&#65288;Parse-Execute-Refine&#65289;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#21521;&#30693;&#35782;&#24211;&#38382;&#39064;&#24212;&#31572;&#27169;&#22411;&#28436;&#31034;&#25191;&#34892;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#65292;&#21487;&#20197;&#25552;&#39640;&#22797;&#26434;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.03356</link><description>&lt;p&gt;
&#20174;&#35299;&#26512;-&#25191;&#34892;&#21040;&#35299;&#26512;-&#25191;&#34892;-&#20248;&#21270;&#65306;&#25552;&#39640;&#22797;&#26434;&#38382;&#39064;&#31572;&#26696;&#22522;&#20110;&#30693;&#35782;&#24211;&#30340;&#35821;&#20041;&#35299;&#26512;&#22120;
&lt;/p&gt;
&lt;p&gt;
From Parse-Execute to Parse-Execute-Refine: Improving Semantic Parser for Complex Question Answering over Knowledge Base. (arXiv:2305.03356v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35299;&#26512;-&#25191;&#34892;-&#20248;&#21270;&#65288;Parse-Execute-Refine&#65289;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#21521;&#30693;&#35782;&#24211;&#38382;&#39064;&#24212;&#31572;&#27169;&#22411;&#28436;&#31034;&#25191;&#34892;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#65292;&#21487;&#20197;&#25552;&#39640;&#22797;&#26434;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25226;&#38382;&#39064;&#35299;&#26512;&#25104;&#21487;&#25191;&#34892;&#30340;&#36923;&#36753;&#24418;&#24335;&#23545;&#20110;&#30693;&#35782;&#24211;&#38382;&#39064;&#24212;&#31572;&#26377;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22797;&#26434;&#30340;&#30693;&#35782;&#24211;&#38382;&#39064;&#24212;&#31572;&#26159;&#19968;&#39033;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#36827;&#34892;&#22797;&#26434;&#30340;&#22810;&#27493;&#25512;&#29702;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KoPL&#30340;&#26032;&#22411;&#35821;&#20041;&#35299;&#26512;&#22120;&#65292;&#26088;&#22312;&#26174;&#24335;&#22320;&#27169;&#25311;&#25512;&#29702;&#36807;&#31243;&#65292;&#22312;&#22797;&#26434;&#30693;&#35782;&#24211;&#38382;&#39064;&#24212;&#31572;&#39046;&#22495;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#19968;&#31181;&#31616;&#21333;&#30340;&#35299;&#26512;-&#25191;&#34892;-&#20248;&#21270;&#33539;&#24335;&#26469;&#24320;&#21457;&#35821;&#20041;&#35299;&#26512;&#22120;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#21521;&#30693;&#35782;&#24211;&#38382;&#39064;&#24212;&#31572;&#27169;&#22411;&#28436;&#31034;&#25191;&#34892;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#26469;&#23436;&#21892;&#21644;&#25913;&#36827;KoPL&#35299;&#26512;&#22120;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#26679;&#31616;&#21333;&#30340;&#31574;&#30053;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22797;&#26434;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#35299;&#26512;&#38454;&#27573;&#65292;&#25191;&#34892;&#38454;&#27573;&#21644;&#20248;&#21270;&#38454;&#27573;&#65292;&#20197;&#22686;&#24378;&#22797;&#26434;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#35299;&#26512;&#22120;&#20351;&#29992;KoPL&#29983;&#25104;&#36879;&#26126;&#30340;&#36923;&#36753;&#24418;&#24335;&#12290;&#28982;&#21518;&#65292;&#25191;&#34892;&#38454;&#27573;&#23545;&#40784;&#21644;&#25191;&#34892;&#36825;&#20123;&#36923;&#36753;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parsing questions into executable logical forms has showed impressive results for knowledge-base question answering (KBQA). However, complex KBQA is a more challenging task that requires to perform complex multi-step reasoning. Recently, a new semantic parser called KoPL has been proposed to explicitly model the reasoning processes, which achieved the state-of-the-art on complex KBQA. In this paper, we further explore how to unlock the reasoning ability of semantic parsers by a simple proposed parse-execute-refine paradigm. We refine and improve the KoPL parser by demonstrating the executed intermediate reasoning steps to the KBQA model. We show that such simple strategy can significantly improve the ability of complex reasoning. Specifically, we propose three components: a parsing stage, an execution stage and a refinement stage, to enhance the ability of complex reasoning. The parser uses the KoPL to generate the transparent logical forms. Then, the execution stage aligns and execute
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#23384;&#22312;&#38544;&#31169;&#39118;&#38505;&#24182;&#21487;&#33021;&#25918;&#22823;&#27169;&#22411;&#30340;&#19981;&#20844;&#24179;&#24615;&#65292;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.03355</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#21387;&#32553;&#32508;&#21512;&#30740;&#31350;&#65306;&#24615;&#33021;&#12289;&#38544;&#31169;&#12289;&#40065;&#26834;&#24615;&#20197;&#21450;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study on Dataset Distillation: Performance, Privacy, Robustness and Fairness. (arXiv:2305.03355v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#23384;&#22312;&#38544;&#31169;&#39118;&#38505;&#24182;&#21487;&#33021;&#25918;&#22823;&#27169;&#22411;&#30340;&#19981;&#20844;&#24179;&#24615;&#65292;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#21387;&#32553;&#26088;&#22312;&#23558;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;&#20016;&#23500;&#29305;&#24449;&#32534;&#30721;&#25104;&#23567;&#22411;&#25968;&#25454;&#38598;&#65292;&#26159;&#19968;&#31181;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#30456;&#20851;&#30740;&#31350;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#21387;&#32553;&#22270;&#20687;&#30340;&#20449;&#24687;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#20174;&#23433;&#20840;&#24615;&#35282;&#24230;&#20840;&#38754;&#20998;&#26512;&#36825;&#19968;&#25216;&#26415;&#30340;&#24037;&#20316;&#65292;&#24182;&#19988;&#23545;&#28508;&#22312;&#39118;&#38505;&#32570;&#20047;&#31995;&#32479;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#12290;&#25105;&#20204;&#25104;&#21151;&#20351;&#29992;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#26469;&#26174;&#31034;&#20173;&#28982;&#23384;&#22312;&#38544;&#31169;&#39118;&#38505;&#12290;&#26412;&#25991;&#36824;&#34920;&#26126;&#65292;&#25968;&#25454;&#38598;&#21387;&#32553;&#22312;&#27169;&#22411;&#40065;&#26834;&#24615;&#26041;&#38754;&#21487;&#33021;&#20250;&#20135;&#29983;&#19981;&#21516;&#31243;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#36827;&#34892;&#39044;&#27979;&#26102;&#25918;&#22823;&#31867;&#21035;&#38388;&#30340;&#27169;&#22411;&#19981;&#20844;&#24179;&#24615;&#12290;&#26412;&#30740;&#31350;&#20026;&#25968;&#25454;&#38598;&#21387;&#32553;&#35780;&#20272;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of dataset distillation is to encode the rich features of an original dataset into a tiny dataset. It is a promising approach to accelerate neural network training and related studies. Different approaches have been proposed to improve the informativeness and generalization performance of distilled images. However, no work has comprehensively analyzed this technique from a security perspective and there is a lack of systematic understanding of potential risks. In this work, we conduct extensive experiments to evaluate current state-of-the-art dataset distillation methods. We successfully use membership inference attacks to show that privacy risks still remain. Our work also demonstrates that dataset distillation can cause varying degrees of impact on model robustness and amplify model unfairness across classes when making predictions. This work offers a large-scale benchmarking framework for dataset distillation evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#21160;&#24577;&#35748;&#30693;&#36923;&#36753;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#25506;&#35752;&#20102;&#29702;&#35299;&#24515;&#26234;&#29702;&#35770;&#30340;&#26041;&#27861;&#12290;&#34429;&#28982;GPT-4&#34920;&#29616;&#20986;&#25913;&#36827;&#30340;&#33021;&#21147;&#65292;&#20294;&#38656;&#35201;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.03353</link><description>&lt;p&gt;
MindGames&#65306;&#21033;&#29992;&#21160;&#24577;&#35748;&#30693;&#27169;&#24577;&#36923;&#36753;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#38024;&#23545;&#24515;&#26234;&#29702;&#35770;&#36827;&#34892;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic. (arXiv:2305.03353v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#21160;&#24577;&#35748;&#30693;&#36923;&#36753;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#25506;&#35752;&#20102;&#29702;&#35299;&#24515;&#26234;&#29702;&#35770;&#30340;&#26041;&#27861;&#12290;&#34429;&#28982;GPT-4&#34920;&#29616;&#20986;&#25913;&#36827;&#30340;&#33021;&#21147;&#65292;&#20294;&#38656;&#35201;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#26234;&#29702;&#35770;(ToM)&#26159;&#26234;&#33021;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#20934;&#30830;&#24230;&#37327;&#23427;&#20173;&#28982;&#26159;&#19968;&#20010;&#20105;&#35758;&#35805;&#39064;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#23581;&#35797;&#23558;&#20154;&#31867;ToM&#35780;&#20272;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#65292;&#20351;&#29992;&#20154;&#31867;&#21019;&#24314;&#30340;&#26631;&#20934;&#21270;&#27979;&#35797;&#25110;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#26495;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#31616;&#21333;&#30340;&#25512;&#29702;&#19978;&#65292;&#24182;&#38656;&#35201;&#36827;&#19968;&#27493;&#39564;&#35777;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20855;&#26377;&#19982;ToM&#37325;&#21472;&#30340;&#21160;&#24577;&#35748;&#30693;&#36923;&#36753;&#26469;&#29983;&#25104;&#26356;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#26032;&#30340;&#35821;&#35328;&#25216;&#24039;&#26469;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#29305;&#23450;&#30340;&#35821;&#35328;&#27169;&#22411;&#32553;&#25918;&#65288;&#20174;70M&#21040;6B&#21644;350M&#21040;174B&#65289;&#24182;&#19981;&#19968;&#33268;&#22320;&#20135;&#29983;&#27604;&#38543;&#26426;&#32467;&#26524;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#34429;&#28982;GPT-4&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#35748;&#30693;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#20173;&#26377;&#25552;&#21319;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312;&#20197;&#19979;&#38142;&#25509;&#20844;&#24320;&#33719;&#21462;&#65306;https://github.com/antoinelrnld/modlog https://huggingface.co/datasets/sileo
&lt;/p&gt;
&lt;p&gt;
Theory of Mind (ToM) is a critical component of intelligence, yet accurately measuring it continues to be a subject of debate. Prior research has attempted to apply human ToM assessments to natural language processing models using either human-created standardized tests or rule-based templates. However, these methods primarily focus on simplistic reasoning and require further validation. In this study, we utilize dynamic epistemic logic, which has established overlaps with ToM, to generate more intricate problems. We also introduce novel verbalization techniques to express these problems using natural language. Our findings indicate that certain language model scaling (from 70M to 6B and 350M to 174B) does not consistently yield results better than random chance. While GPT-4 demonstrates improved epistemic reasoning capabilities, there is still room for enhancement. Our code and datasets are publicly available https://github.com/antoinelrnld/modlog https://huggingface.co/datasets/sileo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#27169;&#22411;&#20026;&#22522;&#30784;&#65292;&#20998;&#26512;&#29983;&#24577;&#28436;&#21270;&#21160;&#21147;&#23398;&#20013;&#29983;&#24577;&#21644;&#20010;&#20307;&#22522;&#22240;&#22411;/&#34920;&#22411;&#31867;&#22411;&#22797;&#26434;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.03340</link><description>&lt;p&gt;
&#23450;&#21521;&#28436;&#21270;&#21644;&#29983;&#24577;&#36827;&#21270;&#21160;&#21147;&#23398;&#30340;&#29983;&#29289;&#29289;&#29702;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Biophysical Cybernetics of Directed Evolution and Eco-evolutionary Dynamics. (arXiv:2305.03340v1 [q-bio.PE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#27169;&#22411;&#20026;&#22522;&#30784;&#65292;&#20998;&#26512;&#29983;&#24577;&#28436;&#21270;&#21160;&#21147;&#23398;&#20013;&#29983;&#24577;&#21644;&#20010;&#20307;&#22522;&#22240;&#22411;/&#34920;&#22411;&#31867;&#22411;&#22797;&#26434;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#21160;&#21147;&#23398;&#20013;&#30340;&#35768;&#22810;&#37325;&#35201;&#38382;&#39064;&#21487;&#20197;&#22312;&#21338;&#24328;&#29702;&#35770;&#32972;&#26223;&#19979;&#20197;&#38543;&#26426;&#36712;&#36857;&#20998;&#26512;&#30340;&#26041;&#24335;&#26377;&#24847;&#20041;&#22320;&#26144;&#23556;&#21040;&#20998;&#26512;&#20013;&#12290;&#36890;&#24120;&#30340;&#26041;&#27861;&#26159;&#20998;&#26512;&#23569;&#37327;&#19981;&#21516;&#30340;&#32676;&#20307;&#21644;/&#25110;&#20551;&#35774;&#21160;&#21147;&#23398;&#21457;&#29983;&#22312;&#20154;&#21475;&#35268;&#27169;&#22823;&#21040;&#36275;&#20197;&#20351;&#30830;&#23450;&#24615;&#36712;&#36857;&#25104;&#20026;&#29616;&#23454;&#30340;&#21306;&#22495;&#12290;&#34987;&#31216;&#20026;&#8220;&#29983;&#24577;&#28436;&#21270;&#21160;&#21147;&#23398;&#8221;&#30340;&#29983;&#24577;&#22240;&#32032;&#30340;&#28155;&#21152;&#36827;&#19968;&#27493;&#22797;&#26434;&#21270;&#20102;&#21160;&#21147;&#23398;&#65292;&#24182;&#23548;&#33268;&#35768;&#22810;&#38382;&#39064;&#38590;&#20197;&#22788;&#29702;&#25110;&#24403;&#21069;&#30340;&#29702;&#35770;&#26041;&#27861;&#38590;&#20197;&#23454;&#38469;&#24212;&#29992;&#12290;&#20294;&#26159;&#65292;&#19968;&#31181;&#31867;&#20284;&#20294;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#30340;&#26041;&#27861;&#26159;&#23558;&#37325;&#28857;&#25918;&#22312;&#27169;&#22411;&#26412;&#36523;&#30340;&#19981;&#30830;&#23450;&#24615;&#19978;&#65292;&#20381;&#25454;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#21644;&#30456;&#37051;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#30340;&#35821;&#35328;&#65292;&#32780;&#34987;&#31216;&#20026;&#8220;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#8221;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#23545;&#20598;&#24615;&#65292;&#23558;&#21516;&#26102;&#32771;&#34385;&#29983;&#24577;&#21644;&#20010;&#20307;&#22522;&#22240;&#22411;/&#34920;&#22411;&#31867;&#22411;&#30340;&#22797;&#26434;&#24615;&#26144;&#23556;&#21040;&#19968;&#20010;&#26032;&#30340;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#27169;&#22411;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many major questions in the theory of evolutionary dynamics can in a meaningful sense be mapped to analyses of stochastic trajectories in game theoretic contexts. Often the approach is to analyze small numbers of distinct populations and/or to assume dynamics occur within a regime of population sizes large enough that deterministic trajectories are an excellent approximation of reality. The addition of ecological factors, termed "eco-evolutionary dynamics", further complicates the dynamics and results in many problems which are intractable or impractically messy for current theoretical methods. However, an analogous but underexplored approach is to analyze these systems with an eye primarily towards uncertainty in the models themselves. In the language of researchers in Reinforcement Learning and adjacent fields, a Partially Observable Markov Process. Here we introduce a duality which maps the complexity of accounting for both ecology and individual genotypic/phenotypic types onto a pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;QCRI&#22312;SemEval-2023&#20219;&#21153;3&#20013;&#20351;&#29992;&#22810;&#35821;&#35328;&#27169;&#22411;&#25104;&#21151;&#25490;&#21517;&#21069;&#19977;&#30340;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#21435;&#35299;&#20915;&#20998;&#26512;&#21644;&#39564;&#35777;&#22312;&#32447;&#26032;&#38395;&#20256;&#25773;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.03336</link><description>&lt;p&gt;
QCRI&#22312;SemEval-2023&#20219;&#21153;3 &#20013;&#30340;&#34920;&#29616;&#65306;&#20351;&#29992;&#22810;&#35821;&#35328;&#27169;&#22411;&#26816;&#27979;&#26032;&#38395;&#31867;&#22411;&#12289;&#26694;&#26550;&#21644;&#35828;&#26381;&#25216;&#24039;
&lt;/p&gt;
&lt;p&gt;
QCRI at SemEval-2023 Task 3: News Genre, Framing and Persuasion Techniques Detection using Multilingual Models. (arXiv:2305.03336v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;QCRI&#22312;SemEval-2023&#20219;&#21153;3&#20013;&#20351;&#29992;&#22810;&#35821;&#35328;&#27169;&#22411;&#25104;&#21151;&#25490;&#21517;&#21069;&#19977;&#30340;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#21435;&#35299;&#20915;&#20998;&#26512;&#21644;&#39564;&#35777;&#22312;&#32447;&#26032;&#38395;&#20256;&#25773;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#27969;&#21644;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#34394;&#20551;&#20449;&#24687;&#20256;&#25773;&#19968;&#30452;&#20197;&#26469;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#35823;&#23548;&#29992;&#25143;&#12290;&#30001;&#35760;&#32773;&#21644;&#20107;&#23454;&#26680;&#26597;&#21592;&#36827;&#34892;&#25163;&#21160;&#26816;&#27979;&#21644;&#39564;&#35777;&#30340;&#21162;&#21147;&#24050;&#32463;&#26080;&#27861;&#24212;&#23545;&#34394;&#20551;&#20449;&#24687;&#30340;&#24555;&#36895;&#25193;&#25955;&#21644;&#22823;&#35268;&#27169;&#20256;&#25773;&#12290;&#36825;&#28608;&#21169;&#20102;&#30740;&#31350;&#21644;&#24037;&#19994;&#30028;&#30340;&#21162;&#21147;&#65292;&#20197;&#24320;&#21457;&#29992;&#20110;&#20998;&#26512;&#21644;&#39564;&#35777;&#22312;&#32447;&#26032;&#38395;&#20256;&#25773;&#30340;&#31995;&#32479;&#12290; SemEval-2023&#20219;&#21153;3 &#23581;&#35797;&#35299;&#20915;&#22312;&#36825;&#20010;&#24635;&#20307;&#38382;&#39064;&#19979;&#30340;&#20960;&#20010;&#23376;&#20219;&#21153;&#65292;&#38024;&#23545;&#26032;&#38395;&#25991;&#31456;&#20013;&#20351;&#29992;&#30340;&#20889;&#20316;&#25216;&#24039;&#20197;&#24433;&#21709;&#35835;&#32773;&#30340;&#35266;&#28857;&#12290;&#35813;&#20219;&#21153;&#20351;&#29992;&#20845;&#31181;&#35821;&#35328;&#26469;&#35299;&#20915;&#19977;&#20010;&#23376;&#20219;&#21153;&#65292;&#27492;&#22806;&#36824;&#26377;&#19977;&#31181;&#8220;&#20986;&#20154;&#24847;&#26009;&#8221;&#30340;&#27979;&#35797;&#35821;&#35328;&#65292;&#20849;&#35745;27&#31181;&#19981;&#21516;&#30340;&#27979;&#35797;&#29615;&#22659;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#21442;&#19982;&#35813;&#20219;&#21153;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#22242;&#38431;&#26159;6&#20010;&#25104;&#21151;&#25552;&#20132;&#25152;&#26377;&#29615;&#22659;&#36816;&#34892;&#30340;&#22242;&#38431;&#20043;&#19968;&#12290;&#23448;&#26041;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;27&#31181;&#27979;&#35797;&#29615;&#22659;&#20013;&#26377;10&#20010;&#25490;&#21517;&#22312;&#21069;&#19977;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
Misinformation spreading in mainstream and social media has been misleading users in different ways. Manual detection and verification efforts by journalists and fact-checkers can no longer cope with the great scale and quick spread of misleading information. This motivated research and industry efforts to develop systems for analyzing and verifying news spreading online. The SemEval-2023 Task 3 is an attempt to address several subtasks under this overarching problem, targeting writing techniques used in news articles to affect readers' opinions. The task addressed three subtasks with six languages, in addition to three ``surprise'' test languages, resulting in 27 different test setups. This paper describes our participating system to this task. Our team is one of the 6 teams that successfully submitted runs for all setups. The official results show that our system is ranked among the top 3 systems for 10 out of the 27 setups.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20449;&#20219;&#26694;&#26550;&#65292;&#24110;&#21161;&#38750;&#19987;&#23478;&#23454;&#29616;&#22312;&#20154;&#24037;&#26234;&#33021;&#35774;&#35745;&#20013;&#20805;&#20998;&#21033;&#29992;&#29992;&#25143;&#20449;&#20219;&#30340;&#28508;&#21147;&#65307;&#30456;&#20851;&#30740;&#31350;&#21457;&#29616;&#20102;&#26576;&#20123;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#35805;&#35821;&#20013;&#30340;&#29992;&#25143;&#20449;&#20219;&#35823;&#35299;&#65292;&#24182;&#36827;&#34892;&#20102;&#26377;&#25928;&#24615;&#35780;&#20272;&#65292;&#35813;&#30740;&#31350;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#20026;&#25269;&#21046;&#35774;&#35745;&#20197;&#25216;&#26415;&#20026;&#20013;&#24515;&#30340;&#26131;&#21463;&#25915;&#20987;&#30340;&#20132;&#20114;&#26041;&#24335;&#30340;&#36235;&#21183;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2305.03306</link><description>&lt;p&gt;
&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20449;&#20219;&#26694;&#26550;&#65306;&#19968;&#20010;&#20154;&#26426;&#20132;&#20114;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Human-centered trust framework: An HCI perspective. (arXiv:2305.03306v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20449;&#20219;&#26694;&#26550;&#65292;&#24110;&#21161;&#38750;&#19987;&#23478;&#23454;&#29616;&#22312;&#20154;&#24037;&#26234;&#33021;&#35774;&#35745;&#20013;&#20805;&#20998;&#21033;&#29992;&#29992;&#25143;&#20449;&#20219;&#30340;&#28508;&#21147;&#65307;&#30456;&#20851;&#30740;&#31350;&#21457;&#29616;&#20102;&#26576;&#20123;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#35805;&#35821;&#20013;&#30340;&#29992;&#25143;&#20449;&#20219;&#35823;&#35299;&#65292;&#24182;&#36827;&#34892;&#20102;&#26377;&#25928;&#24615;&#35780;&#20272;&#65292;&#35813;&#30740;&#31350;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#20026;&#25269;&#21046;&#35774;&#35745;&#20197;&#25216;&#26415;&#20026;&#20013;&#24515;&#30340;&#26131;&#21463;&#25915;&#20987;&#30340;&#20132;&#20114;&#26041;&#24335;&#30340;&#36235;&#21183;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30340;&#20986;&#21457;&#28857;&#22522;&#20110;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#29992;&#25143;&#20449;&#20219;&#35805;&#39064;&#30340;&#35752;&#35770;&#65292;&#26088;&#22312;&#25552;&#20986;&#26032;&#39062;&#30340;&#20197;&#20449;&#20219;&#20026;&#36741;&#21161;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#27861;&#65292;&#20174;&#32780;&#20419;&#36827;&#24403;&#21069;&#25216;&#26415;&#30340;&#37319;&#29992;&#21644;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65288;HCTFrame&#65289;&#65292;&#20197;&#25351;&#23548;&#38750;&#19987;&#23478;&#22312;&#20154;&#24037;&#26234;&#33021;&#35774;&#35745;&#20013;&#20805;&#20998;&#21033;&#29992;&#29992;&#25143;&#20449;&#20219;&#30340;&#28508;&#21147;&#12290;&#36890;&#36807;&#23545;&#19977;&#20010;&#25991;&#29486;&#32508;&#36848;&#30340;&#21457;&#29616;&#36827;&#34892;&#25968;&#25454;&#19977;&#35282;&#21270;&#65292;&#21487;&#28040;&#38500;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#35805;&#35821;&#20013;&#20851;&#20110;&#29992;&#25143;&#20449;&#20219;&#30340;&#19968;&#20123;&#35823;&#35299;&#65292;&#24182;&#36827;&#34892;&#19977;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#35780;&#20272;&#24515;&#29702;&#27979;&#37327;&#37327;&#34920;&#22312;&#26144;&#23556;&#28508;&#22312;&#29992;&#25143;&#20449;&#20219;&#30772;&#22351;&#21644;&#25285;&#24551;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#25991;&#20027;&#35201;&#36129;&#29486;&#20110;&#25269;&#21046;&#35774;&#35745;&#20197;&#25216;&#26415;&#20026;&#20013;&#24515;&#30340;&#26131;&#21463;&#25915;&#20987;&#30340;&#20132;&#20114;&#26041;&#24335;&#30340;&#36235;&#21183;&#65292;&#36825;&#21487;&#33021;&#26368;&#32456;&#23548;&#33268;&#26356;&#22810;&#23454;&#38469;&#21644;&#24863;&#30693;&#19978;&#30340;&#20449;&#20219;&#30772;&#35010;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#29992;&#20110;&#25351;&#23548;&#31995;&#32479;&#35774;&#35745;&#20154;&#21592;&#22914;&#20309;&#26144;&#23556;&#21644;&#23450;&#20041;&#29992;&#25143;&#20449;&#20219;&#20197;&#21450;&#31038;&#20250;&#20262;&#29702;&#21644;&#32452;&#32455;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rationale of this work is based on the current user trust discourse of Artificial Intelligence (AI). We aim to produce novel HCI approaches that use trust as a facilitator for the uptake (or appropriation) of current technologies. We propose a framework (HCTFrame) to guide non-experts to unlock the full potential of user trust in AI design. Results derived from a data triangulation of findings from three literature reviews demystify some misconceptions of user trust in computer science and AI discourse, and three case studies are conducted to assess the effectiveness of a psychometric scale in mapping potential users' trust breakdowns and concerns. This work primarily contributes to the fight against the tendency to design technical-centered vulnerable interactions, which can eventually lead to additional real and perceived breaches of trust. The proposed framework can be used to guide system designers on how to map and define user trust and the socioethical and organisational need
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21477;&#23376;&#20316;&#20026;&#22359;&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#23558;&#22359;&#30340;&#36328;&#24230;&#35782;&#21035;&#20026;&#20803;&#32452;&#20851;&#31995;&#21644;&#21442;&#25968;&#65292;&#37319;&#29992;Chunk-OIE&#36827;&#34892;&#20803;&#32452;&#25552;&#21462;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03299</link><description>&lt;p&gt;
&#22522;&#20110;&#22359;&#30340;&#24320;&#25918;&#24335;&#20449;&#24687;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Open Information Extraction via Chunks. (arXiv:2305.03299v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21477;&#23376;&#20316;&#20026;&#22359;&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#23558;&#22359;&#30340;&#36328;&#24230;&#35782;&#21035;&#20026;&#20803;&#32452;&#20851;&#31995;&#21644;&#21442;&#25968;&#65292;&#37319;&#29992;Chunk-OIE&#36827;&#34892;&#20803;&#32452;&#25552;&#21462;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#24335;&#20449;&#24687;&#25277;&#21462;(OIE)&#33268;&#21147;&#20110;&#20174;&#24320;&#25918;&#39046;&#22495;&#30340;&#21477;&#23376;&#20013;&#25552;&#21462;&#20851;&#31995;&#20803;&#32452;&#12290;&#29616;&#26377;&#30340;OIE&#31995;&#32479;&#23558;&#21477;&#23376;&#25286;&#25104;&#26631;&#35760;&#21518;&#35782;&#21035;&#26631;&#35760;&#30340;&#36328;&#24230;&#20316;&#20026;&#20803;&#32452;&#20851;&#31995;&#21644;&#21442;&#25968;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#21477;&#23376;&#20316;&#20026;&#22359;&#24207;&#21015;(SaC)&#65292;&#24182;&#23558;&#22359;&#30340;&#36328;&#24230;&#35782;&#21035;&#20026;&#20803;&#32452;&#20851;&#31995;&#21644;&#21442;&#25968;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;SaC&#23545;&#20110;OIE&#20855;&#26377;&#27604;&#26631;&#35760;&#24207;&#21015;&#26356;&#22909;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#29305;&#24615;&#65292;&#24182;&#38024;&#23545;&#22235;&#31181;&#22359;&#30340;&#36873;&#25321;&#65288;&#21363;CoNLL&#22359;&#12289;&#31616;&#21333;&#30701;&#35821;&#12289;NP&#22359;&#21644;SpanOIE&#30340;&#36328;&#24230;&#65289;&#38024;&#23545;gold OIE&#20803;&#32452;&#36827;&#34892;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#30340;&#21477;&#23376;&#20998;&#22359;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;SaC&#20043;&#19978;&#36827;&#34892;&#30340;&#20803;&#32452;&#25552;&#21462;&#30340;Chunk-OIE&#12290;Chunk-OIE&#22312;&#22810;&#20010;OIE&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#65292;&#26174;&#31034;&#20102;SaC&#23545;OIE&#20219;&#21153;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open Information Extraction (OIE) aims to extract relational tuples from open-domain sentences. Existing OIE systems split a sentence into tokens and recognize token spans as tuple relations and arguments. We instead propose Sentence as Chunk sequence (SaC) and recognize chunk spans as tuple relations and arguments. We argue that SaC has better quantitative and qualitative properties for OIE than sentence as token sequence, and evaluate four choices of chunks (i.e., CoNLL chunks, simple phrases, NP chunks, and spans from SpanOIE) against gold OIE tuples. Accordingly, we propose a simple BERT-based model for sentence chunking, and propose Chunk-OIE for tuple extraction on top of SaC. Chunk-OIE achieves state-of-the-art results on multiple OIE datasets, showing that SaC benefits OIE task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;BadSAM&#65292;&#36825;&#26159;&#23545;&#22270;&#20687;&#20998;&#21106;&#22522;&#30784;&#27169;&#22411;&#30340;&#39318;&#27425;&#21518;&#38376;&#25915;&#20987;&#65292;&#35813;&#25915;&#20987;&#23558;&#23548;&#33268;&#23450;&#21046;&#21270;SAM&#27169;&#22411;&#38754;&#20020;&#26356;&#21152;&#20005;&#23803;&#30340;&#23433;&#20840;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.03289</link><description>&lt;p&gt;
BadSAM&#65306;&#36890;&#36807;&#21518;&#38376;&#25915;&#20987;&#25506;&#32034;SAM&#30340;&#23433;&#20840;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
BadSAM: Exploring Security Vulnerabilities of SAM via Backdoor Attacks. (arXiv:2305.03289v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BadSAM&#65292;&#36825;&#26159;&#23545;&#22270;&#20687;&#20998;&#21106;&#22522;&#30784;&#27169;&#22411;&#30340;&#39318;&#27425;&#21518;&#38376;&#25915;&#20987;&#65292;&#35813;&#25915;&#20987;&#23558;&#23548;&#33268;&#23450;&#21046;&#21270;SAM&#27169;&#22411;&#38754;&#20020;&#26356;&#21152;&#20005;&#23803;&#30340;&#23433;&#20840;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24378;&#22823;&#24615;&#33021;&#65292;&#20998;&#21106;&#20219;&#20309;&#27169;&#22411;&#65288;SAM&#65289;&#20197;&#22270;&#20687;&#20998;&#21106;&#22522;&#30784;&#27169;&#22411;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24050;&#32463;&#21457;&#29616;&#24403;&#38754;&#23545;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#19979;&#28216;&#20219;&#21153;&#26102;&#65292;SAM&#24182;&#19981;&#24635;&#26159;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#23548;&#33268;&#19979;&#28216;&#29992;&#25143;&#35201;&#27714;&#23450;&#21046;&#30340;SAM&#27169;&#22411;&#65292;&#20197;&#36866;&#24212;&#36825;&#20123;&#19979;&#28216;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;BadSAM&#65292;&#36825;&#26159;&#23545;&#22270;&#20687;&#20998;&#21106;&#22522;&#30784;&#27169;&#22411;&#30340;&#39318;&#27425;&#21518;&#38376;&#25915;&#20987;&#12290;&#25105;&#20204;&#22312;CAMO&#25968;&#25454;&#38598;&#19978;&#30340;&#21021;&#27493;&#23454;&#39564;&#23637;&#31034;&#20102;BadSAM&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the Segment Anything Model (SAM) has gained significant attention as an image segmentation foundation model due to its strong performance on various downstream tasks. However, it has been found that SAM does not always perform satisfactorily when faced with challenging downstream tasks. This has led downstream users to demand a customized SAM model that can be adapted to these downstream tasks. In this paper, we present BadSAM, the first backdoor attack on the image segmentation foundation model. Our preliminary experiments on the CAMO dataset demonstrate the effectiveness of BadSAM.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102; Mix Prompt Tuning&#65288;MPT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25163;&#21160;&#25552;&#31034;&#27169;&#26495;&#19982;&#33258;&#21160;&#23398;&#20064;&#30340;&#36830;&#32493;&#25552;&#31034;&#27169;&#26495;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#22810;&#31890;&#24230;&#23398;&#26415;&#21151;&#33021;&#35782;&#21035;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#20943;&#36731;&#23545;&#27880;&#37322;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2305.03287</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#20010;&#25552;&#31034;&#30693;&#35782;&#30340;&#20302;&#36164;&#28304;&#22810;&#31890;&#24230;&#23398;&#26415;&#21151;&#33021;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Low-Resource Multi-Granularity Academic Function Recognition Based on Multiple Prompt Knowledge. (arXiv:2305.03287v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03287
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102; Mix Prompt Tuning&#65288;MPT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25163;&#21160;&#25552;&#31034;&#27169;&#26495;&#19982;&#33258;&#21160;&#23398;&#20064;&#30340;&#36830;&#32493;&#25552;&#31034;&#27169;&#26495;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#22810;&#31890;&#24230;&#23398;&#26415;&#21151;&#33021;&#35782;&#21035;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#20943;&#36731;&#23545;&#27880;&#37322;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;Fine-tuning &#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#22914; SciBERT&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#27880;&#37322;&#25968;&#25454;&#25165;&#33021;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#33719;&#21462;&#31185;&#23398; NLP &#20219;&#21153;&#30340; fine-tune &#25968;&#25454;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#26114;&#36149;&#24615;&#12290;&#21463;&#25552;&#31034;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102; Mix Prompt Tuning&#65288;MPT&#65289;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#26088;&#22312;&#20943;&#36731;&#23545;&#27880;&#37322;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#24182;&#20351;&#29992;&#24456;&#23569;&#25968;&#37327;&#30340;&#26631;&#35760;&#31034;&#20363;&#25552;&#39640;&#22810;&#31890;&#24230;&#23398;&#26415;&#21151;&#33021;&#35782;&#21035;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#23558;&#25163;&#21160;&#25552;&#31034;&#27169;&#26495;&#19982;&#33258;&#21160;&#23398;&#20064;&#30340;&#36830;&#32493;&#25552;&#31034;&#27169;&#26495;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#22810;&#26041;&#38754;&#30340;&#34920;&#31034;&#65292;&#20197;&#24110;&#21161;&#32473;&#23450;&#30340;&#23398;&#26415;&#21151;&#33021;&#35782;&#21035;&#20219;&#21153;&#20805;&#20998;&#21033;&#29992; PLMs &#20013;&#30340;&#30693;&#35782;&#12290;&#22522;&#20110;&#36825;&#20123;&#25552;&#31034;&#27169;&#26495;&#21644; fine-tuned PLM&#65292;&#22823;&#37327;&#30340;&#20266;&#26631;&#31614;&#34987;&#20998;&#37197;&#32473;&#26410;&#26631;&#35760;&#30340;&#23454;&#20363;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning pre-trained language models (PLMs), e.g., SciBERT, generally requires large numbers of annotated data to achieve state-of-the-art performance on a range of NLP tasks in the scientific domain. However, obtaining the fine-tune data for scientific NLP task is still challenging and expensive. Inspired by recent advancement in prompt learning, in this paper, we propose the Mix Prompt Tuning (MPT), which is a semi-supervised method to alleviate the dependence on annotated data and improve the performance of multi-granularity academic function recognition tasks with a small number of labeled examples. Specifically, the proposed method provides multi-perspective representations by combining manual prompt templates with automatically learned continuous prompt templates to help the given academic function recognition task take full advantage of knowledge in PLMs. Based on these prompt templates and the fine-tuned PLM, a large number of pseudo labels are assigned to the unlabeled exam
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20010;&#21028;&#21035;&#22120;&#22312;&#31867; GAN &#35774;&#32622;&#20013;&#30452;&#25509;&#23398;&#20064;&#26469;&#33258;&#22810;&#20010;&#21442;&#32771;&#21160;&#20316;&#30340;&#29305;&#23450;&#36523;&#20307;&#37096;&#20301;&#30340;&#35299;&#32806;&#21160;&#20316;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#22797;&#21512;&#21644;&#20219;&#21153;&#39537;&#21160;&#30340;&#21160;&#20316;&#25511;&#21046;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#22810;&#20010;&#20219;&#21153;&#30340;&#22870;&#21169;&#21644;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#30340;&#22810;&#30446;&#26631;&#25511;&#21046;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2305.03286</link><description>&lt;p&gt;
&#20351;&#29992;&#20219;&#21153;&#25511;&#21046;&#30340;&#22797;&#21512;&#21160;&#20316;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Composite Motion Learning with Task Control. (arXiv:2305.03286v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03286
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20010;&#21028;&#21035;&#22120;&#22312;&#31867; GAN &#35774;&#32622;&#20013;&#30452;&#25509;&#23398;&#20064;&#26469;&#33258;&#22810;&#20010;&#21442;&#32771;&#21160;&#20316;&#30340;&#29305;&#23450;&#36523;&#20307;&#37096;&#20301;&#30340;&#35299;&#32806;&#21160;&#20316;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#22797;&#21512;&#21644;&#20219;&#21153;&#39537;&#21160;&#30340;&#21160;&#20316;&#25511;&#21046;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#22810;&#20010;&#20219;&#21153;&#30340;&#22870;&#21169;&#21644;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#30340;&#22810;&#30446;&#26631;&#25511;&#21046;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#29289;&#29702;&#27169;&#25311;&#35282;&#33394;&#30340;&#22797;&#21512;&#21644;&#20219;&#21153;&#39537;&#21160;&#30340;&#21160;&#20316;&#25511;&#21046;&#12290;&#19982;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#27169;&#20223;&#20840;&#36523;&#21160;&#20316;&#30340;&#29616;&#26377;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#31867; GAN &#35774;&#32622;&#20013;&#21033;&#29992;&#22810;&#20010;&#21028;&#21035;&#22120;&#30452;&#25509;&#21516;&#26102;&#23398;&#20064;&#26469;&#33258;&#22810;&#20010;&#21442;&#32771;&#21160;&#20316;&#30340;&#29305;&#23450;&#36523;&#20307;&#37096;&#20301;&#30340;&#35299;&#32806;&#21160;&#20316;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#19981;&#38656;&#35201;&#25163;&#21160;&#21046;&#20316;&#29992;&#20110;&#23398;&#20064;&#30340;&#22797;&#21512;&#21442;&#32771;&#21160;&#20316;&#12290;&#30456;&#21453;&#65292;&#25511;&#21046;&#31574;&#30053;&#33258;&#34892;&#25506;&#32034;&#22914;&#20309;&#33258;&#21160;&#22320;&#32452;&#21512;&#22797;&#21512;&#21160;&#20316;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#32771;&#34385;&#20102;&#22810;&#20010;&#20219;&#21153;&#29305;&#23450;&#22870;&#21169;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#30340;&#22810;&#30446;&#26631;&#25511;&#21046;&#31574;&#30053;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#30446;&#26631;&#23398;&#20064;&#26694;&#26550;&#65292;&#33258;&#36866;&#24212;&#24179;&#34913;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#21644;&#22810;&#20010;&#30446;&#26631;&#23450;&#21521;&#25511;&#21046;&#30446;&#26631;&#30340;&#19981;&#21516;&#36816;&#21160;&#30340;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#22797;&#21512;&#21160;&#20316;&#36890;&#24120;&#26159;&#26356;&#31616;&#21333;&#34892;&#20026;&#30340;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a deep learning method for composite and task-driven motion control for physically simulated characters. In contrast to existing data-driven approaches using reinforcement learning that imitate full-body motions, we learn decoupled motions for specific body parts from multiple reference motions simultaneously and directly by leveraging the use of multiple discriminators in a GAN-like setup. In this process, there is no need of any manual work to produce composite reference motions for learning. Instead, the control policy explores by itself how the composite motions can be combined automatically. We further account for multiple task-specific rewards and train a single, multi-objective control policy. To this end, we propose a novel framework for multi-objective learning that adaptively balances the learning of disparate motions from multiple sources and multiple goal-directed control objectives. In addition, as composite motions are typically augmentations of simpler behavio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViT&#65289;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#19981;&#21516;&#26550;&#26500;&#65292;&#35752;&#35770;&#20102;ViT&#22312;&#20998;&#22359;&#21010;&#20998;&#26041;&#26696;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22238;&#39038;&#21644;&#27604;&#36739;&#20102;&#19981;&#21516;ViT&#26550;&#26500;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;ViT&#26550;&#26500;&#30340;&#23835;&#36215;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#26367;&#20195;&#20256;&#32479;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#36235;&#21183;&#19981;&#26029;&#22686;&#24378;&#12290;</title><link>http://arxiv.org/abs/2305.03273</link><description>&lt;p&gt;
&#20351;&#29992;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#35821;&#20041;&#20998;&#21106;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Semantic Segmentation using Vision Transformers: A survey. (arXiv:2305.03273v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViT&#65289;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#19981;&#21516;&#26550;&#26500;&#65292;&#35752;&#35770;&#20102;ViT&#22312;&#20998;&#22359;&#21010;&#20998;&#26041;&#26696;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22238;&#39038;&#21644;&#27604;&#36739;&#20102;&#19981;&#21516;ViT&#26550;&#26500;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;ViT&#26550;&#26500;&#30340;&#23835;&#36215;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#26367;&#20195;&#20256;&#32479;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#36235;&#21183;&#19981;&#26029;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#20998;&#21106;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#22303;&#22320;&#35206;&#30422;&#20998;&#26512;&#12289;&#33258;&#21160;&#39550;&#39542;&#21644;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#12290;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViT&#65289;&#20026;&#35821;&#20041;&#20998;&#21106;&#25552;&#20379;&#20102;&#26550;&#26500;&#27169;&#22411;&#12290;&#23613;&#31649;ViT&#22312;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#20854;&#20998;&#22359;&#21010;&#20998;&#26041;&#26696;&#65292;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;&#20687;&#22270;&#20687;&#20998;&#21106;&#21644;&#30446;&#26631;&#26816;&#27979;&#36825;&#26679;&#30340;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#65292;&#22240;&#27492;ViT&#19981;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#20027;&#24178;&#26550;&#26500;&#12290;&#26412;&#35843;&#26597;&#35752;&#35770;&#20102;&#19968;&#20123;&#21487;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;&#30340;&#19981;&#21516;ViT&#26550;&#26500;&#20197;&#21450;&#23427;&#20204;&#22914;&#20309;&#24212;&#23545;&#19978;&#36848;&#25361;&#25112;&#12290;ViT&#30340;&#23835;&#36215;&#20197;&#21450;&#20854;&#39640;&#25104;&#21151;&#29575;&#30340;&#34920;&#29616;&#65292;&#20419;&#20351;&#31038;&#21306;&#36880;&#28176;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#20195;&#20256;&#32479;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#22238;&#39038;&#21644;&#27604;&#36739;&#35774;&#35745;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;&#30340;ViT&#26550;&#26500;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic segmentation has a broad range of applications in a variety of domains including land coverage analysis, autonomous driving, and medical image analysis. Convolutional neural networks (CNN) and Vision Transformers (ViTs) provide the architecture models for semantic segmentation. Even though ViTs have proven success in image classification, they cannot be directly applied to dense prediction tasks such as image segmentation and object detection since ViT is not a general purpose backbone due to its patch partitioning scheme. In this survey, we discuss some of the different ViT architectures that can be used for semantic segmentation and how their evolution managed the above-stated challenge. The rise of ViT and its performance with a high success rate motivated the community to slowly replace the traditional convolutional neural networks in various computer vision tasks. This survey aims to review and compare the performances of ViT architectures designed for semantic segmentati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#38480;&#21046;&#35748;&#30693;&#36127;&#33655;&#30340;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#26032;&#31639;&#27861;&#21644;&#29702;&#35770;&#25104;&#26524;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#24605;&#24819;&#22914;&#20309;&#24212;&#29992;&#20110;&#30740;&#31350;&#35748;&#30693;&#21644;&#34892;&#20026;&#31185;&#23398;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.03263</link><description>&lt;p&gt;
&#38480;&#21046;&#35748;&#30693;&#36127;&#33655;&#30340;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bayesian Reinforcement Learning with Limited Cognitive Load. (arXiv:2305.03263v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#38480;&#21046;&#35748;&#30693;&#36127;&#33655;&#30340;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#26032;&#31639;&#27861;&#21644;&#29702;&#35770;&#25104;&#26524;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#24605;&#24819;&#22914;&#20309;&#24212;&#29992;&#20110;&#30740;&#31350;&#35748;&#30693;&#21644;&#34892;&#20026;&#31185;&#23398;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25152;&#26377;&#30340;&#29983;&#29289;&#21644;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#37117;&#24517;&#39035;&#22312;&#20449;&#24687;&#22788;&#29702;&#33021;&#21147;&#19978;&#26377;&#38480;&#21046;&#19979;&#36827;&#34892;&#23398;&#20064;&#21644;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#19968;&#33324;&#30340;&#33258;&#36866;&#24212;&#34892;&#20026;&#29702;&#35770;&#24212;&#35813;&#33021;&#22815;&#35828;&#26126;&#20195;&#29702;&#30340;&#23398;&#20064;&#21382;&#21490;&#12289;&#20915;&#31574;&#21644;&#33021;&#21147;&#38480;&#21046;&#20043;&#38388;&#30340;&#22797;&#26434;&#20114;&#21160;&#12290;&#26368;&#36817;&#65292;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#30340;&#30740;&#31350;&#24320;&#22987;&#36890;&#36807;&#23558;&#24378;&#21270;&#23398;&#20064;&#12289;&#36125;&#21494;&#26031;&#20915;&#31574;&#21644;&#36895;&#29575;-&#22833;&#30495;&#29702;&#35770;&#30340;&#24605;&#24819;&#34701;&#21512;&#65292;&#28548;&#28165;&#22609;&#36896;&#36825;&#20123;&#21160;&#24577;&#30340;&#21407;&#21017;&#12290;&#36825;&#19968;&#39046;&#22495;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#31181;&#20250;&#32771;&#34385;&#22788;&#29702;&#38480;&#21046;&#23545;&#23398;&#20064;&#21644;&#21160;&#20316;&#36873;&#25321;&#30340;&#24433;&#21709;&#30340;&#32479;&#19968;&#35268;&#33539;&#26694;&#26550;&#65292;&#21363;&#23481;&#37327;&#26377;&#38480;&#30340;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31687;&#26131;&#20110;&#29702;&#35299;&#30340;&#32508;&#36848;&#65292;&#20171;&#32461;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#26368;&#26032;&#31639;&#27861;&#21644;&#29702;&#35770;&#25104;&#26524;&#65292;&#29305;&#21035;&#20851;&#27880;&#36825;&#20123;&#24605;&#24819;&#22914;&#20309;&#24212;&#29992;&#20110;&#30740;&#31350;&#35748;&#30693;&#21644;&#34892;&#20026;&#31185;&#23398;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
All biological and artificial agents must learn and make decisions given limits on their ability to process information. As such, a general theory of adaptive behavior should be able to account for the complex interactions between an agent's learning history, decisions, and capacity constraints. Recent work in computer science has begun to clarify the principles that shape these dynamics by bridging ideas from reinforcement learning, Bayesian decision-making, and rate-distortion theory. This body of work provides an account of capacity-limited Bayesian reinforcement learning, a unifying normative framework for modeling the effect of processing constraints on learning and action selection. Here, we provide an accessible review of recent algorithms and theoretical results in this setting, paying special attention to how these ideas can be applied to studying questions in the cognitive and behavioral sciences.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#26426;&#22120;&#20154;&#36741;&#21161;&#31359;&#34915;&#36807;&#31243;&#20013;&#30340;&#26381;&#35013;&#25235;&#21462;&#21644;&#23637;&#24320;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;RGB-D&#35821;&#20041;&#20998;&#21106;&#30340;&#21452;&#21521;&#20998;&#24418;&#20132;&#21449;&#34701;&#21512;&#32593;&#32476;&#65292;&#21487;&#20197;&#35782;&#21035;&#21487;&#25235;&#21462;&#21306;&#22495;&#25552;&#20379;&#26356;&#22810;&#25235;&#21462;&#21487;&#33021;&#24615;&#65292;&#24182;&#19988;&#20351;&#29992;RGB&#21644;&#28145;&#24230;&#25968;&#25454;&#34701;&#21512;&#26469;&#32771;&#34385;&#22270;&#20687;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.03259</link><description>&lt;p&gt;
&#22522;&#20110;RGB-D&#35821;&#20041;&#20998;&#21106;&#30340;&#26381;&#35013;&#25235;&#21462;&#21644;&#23637;&#24320;
&lt;/p&gt;
&lt;p&gt;
Clothes Grasping and Unfolding Based on RGB-D Semantic Segmentation. (arXiv:2305.03259v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#26426;&#22120;&#20154;&#36741;&#21161;&#31359;&#34915;&#36807;&#31243;&#20013;&#30340;&#26381;&#35013;&#25235;&#21462;&#21644;&#23637;&#24320;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;RGB-D&#35821;&#20041;&#20998;&#21106;&#30340;&#21452;&#21521;&#20998;&#24418;&#20132;&#21449;&#34701;&#21512;&#32593;&#32476;&#65292;&#21487;&#20197;&#35782;&#21035;&#21487;&#25235;&#21462;&#21306;&#22495;&#25552;&#20379;&#26356;&#22810;&#25235;&#21462;&#21487;&#33021;&#24615;&#65292;&#24182;&#19988;&#20351;&#29992;RGB&#21644;&#28145;&#24230;&#25968;&#25454;&#34701;&#21512;&#26469;&#32771;&#34385;&#22270;&#20687;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26381;&#35013;&#30340;&#25235;&#21462;&#21644;&#23637;&#24320;&#26159;&#26426;&#22120;&#20154;&#36741;&#21161;&#31359;&#34915;&#30340;&#26680;&#24515;&#27493;&#39588;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#20316;&#21697;&#21033;&#29992;&#26381;&#35013;&#30340;&#28145;&#24230;&#22270;&#20687;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20197;&#35782;&#21035;&#21512;&#36866;&#30340;&#25235;&#21462;&#28857;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#29289;&#29702;&#24341;&#25806;&#26469;&#21512;&#25104;&#28145;&#24230;&#22270;&#20687;&#20197;&#20943;&#23569;&#30495;&#23454;&#26631;&#35760;&#25968;&#25454;&#30340;&#25910;&#38598;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#21512;&#25104;&#19982;&#30495;&#23454;&#22270;&#20687;&#20043;&#38388;&#30340;&#33258;&#28982;&#39046;&#22495;&#24046;&#36317;&#24120;&#24120;&#23548;&#33268;&#36825;&#20123;&#26041;&#27861;&#22312;&#23454;&#38469;&#25968;&#25454;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#22312;&#25235;&#21462;&#28857;&#34987;&#26381;&#35013;&#29289;&#21697;&#26412;&#36523;&#36974;&#25377;&#30340;&#22330;&#26223;&#20013;&#38590;&#20197;&#22788;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#21521;&#20998;&#24418;&#20132;&#21449;&#34701;&#21512;&#32593;&#32476;&#65288;BiFCNet&#65289;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#65292;&#23454;&#29616;&#23545;&#21487;&#25235;&#21462;&#21306;&#22495;&#30340;&#35782;&#21035;&#20197;&#25552;&#20379;&#26356;&#22810;&#30340;&#25235;&#21462;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#19981;&#20165;&#20351;&#29992;&#28145;&#24230;&#22270;&#20687;&#65292;&#36824;&#21033;&#29992;&#20855;&#26377;&#20016;&#23500;&#33394;&#24425;&#29305;&#24449;&#30340;RGB&#22270;&#20687;&#20316;&#20026;&#32593;&#32476;&#36755;&#20837;&#65292;&#20854;&#20013;&#20998;&#24418;&#20132;&#21449;&#34701;&#21512;&#65288;FCF&#65289;&#27169;&#22359;&#36890;&#36807;&#23558;RGB&#21644;&#28145;&#24230;&#25968;&#25454;&#34701;&#21512;&#26469;&#32771;&#34385;&#22270;&#20687;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clothes grasping and unfolding is a core step in robotic-assisted dressing. Most existing works leverage depth images of clothes to train a deep learning-based model to recognize suitable grasping points. These methods often utilize physics engines to synthesize depth images to reduce the cost of real labeled data collection. However, the natural domain gap between synthetic and real images often leads to poor performance of these methods on real data. Furthermore, these approaches often struggle in scenarios where grasping points are occluded by the clothing item itself. To address the above challenges, we propose a novel Bi-directional Fractal Cross Fusion Network (BiFCNet) for semantic segmentation, enabling recognition of graspable regions in order to provide more possibilities for grasping. Instead of using depth images only, we also utilize RGB images with rich color features as input to our network in which the Fractal Cross Fusion (FCF) module fuses RGB and depth data by consid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;OOD&#24847;&#22270;&#26816;&#27979;&#26694;&#26550;&#65288;Caro&#65289;&#65292;&#29992;&#20110;&#27169;&#25311;OOD&#24847;&#22270;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#22810;&#36718;&#23545;&#35805;&#19978;&#19979;&#25991;&#65292;&#24182;&#22312;&#25552;&#21462;&#31283;&#20581;&#30340;&#34920;&#31034;&#26102;&#21024;&#38500;&#19982;&#24847;&#22270;&#26816;&#27979;&#26080;&#20851;&#30340;&#22810;&#20313;&#20449;&#24687;&#12290;Caro&#22312;&#22810;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36234;&#20102;&#20808;&#21069;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.03237</link><description>&lt;p&gt;
&#32771;&#34385;&#22810;&#36718;&#23545;&#35805;&#19978;&#19979;&#25991;&#30340;&#39046;&#22495;&#22806;&#24847;&#22270;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Out-of-Domain Intent Detection Considering Multi-turn Dialogue Contexts. (arXiv:2305.03237v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;OOD&#24847;&#22270;&#26816;&#27979;&#26694;&#26550;&#65288;Caro&#65289;&#65292;&#29992;&#20110;&#27169;&#25311;OOD&#24847;&#22270;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#22810;&#36718;&#23545;&#35805;&#19978;&#19979;&#25991;&#65292;&#24182;&#22312;&#25552;&#21462;&#31283;&#20581;&#30340;&#34920;&#31034;&#26102;&#21024;&#38500;&#19982;&#24847;&#22270;&#26816;&#27979;&#26080;&#20851;&#30340;&#22810;&#20313;&#20449;&#24687;&#12290;Caro&#22312;&#22810;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36234;&#20102;&#20808;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#22806;&#65288;OOD&#65289;&#24847;&#22270;&#26816;&#27979;&#23545;&#20110;&#23454;&#29992;&#30340;&#23545;&#35805;&#31995;&#32479;&#38750;&#24120;&#37325;&#35201;&#65292;&#36890;&#24120;&#38656;&#35201;&#32771;&#34385;&#22810;&#36718;&#23545;&#35805;&#19978;&#19979;&#25991;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;OOD&#24847;&#22270;&#26816;&#27979;&#26041;&#27861;&#20165;&#38480;&#20110;&#21333;&#36718;&#23545;&#35805;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;OOD&#24847;&#22270;&#26816;&#27979;&#65288;Caro&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;OOD&#24847;&#22270;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#22810;&#36718;&#19978;&#19979;&#25991;&#36827;&#34892;&#24314;&#27169;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#36981;&#24490;&#20449;&#24687;&#29942;&#39048;&#21407;&#21017;&#20174;&#22810;&#36718;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#31283;&#20581;&#30340;&#34920;&#31034;&#12290;&#27599;&#20010;&#36755;&#20837;&#26679;&#26412;&#26500;&#24314;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#35270;&#35282;&#65292;&#20351;&#29992;&#22810;&#35270;&#22270;&#20449;&#24687;&#29942;&#39048;&#25439;&#22833;&#21024;&#38500;&#19982;&#24847;&#22270;&#26816;&#27979;&#26080;&#20851;&#30340;&#22810;&#20313;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#22312;Caro&#20013;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#24341;&#20837;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#35757;&#32451;&#36807;&#31243;&#26469;&#20174;&#36825;&#20123;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#25366;&#25496;OOD&#26679;&#26412;&#65292;&#24182;&#20351;&#29992;&#33258;&#20030;&#26041;&#27861;&#29992;&#36825;&#20123;OOD&#26679;&#26412;&#26469;&#35757;&#32451;&#29983;&#25104;&#30340;&#27169;&#22411;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Caro&#22312;OOD&#24847;&#22270;&#26816;&#27979;&#20219;&#21153;&#30340;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24314;&#31435;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36234;&#20102;&#20165;&#32771;&#34385;&#21333;&#36718;&#19978;&#19979;&#25991;&#30340;&#20808;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-Domain (OOD) intent detection is vital for practical dialogue systems, and it usually requires considering multi-turn dialogue contexts. However, most previous OOD intent detection approaches are limited to single dialogue turns. In this paper, we introduce a context-aware OOD intent detection (Caro) framework to model multi-turn contexts in OOD intent detection tasks. Specifically, we follow the information bottleneck principle to extract robust representations from multi-turn dialogue contexts. Two different views are constructed for each input sample and the superfluous information not related to intent detection is removed using a multi-view information bottleneck loss. Moreover, we also explore utilizing unlabeled data in Caro. A two-stage training process is introduced to mine OOD samples from these unlabeled data, and these OOD samples are used to train the resulting model with a bootstrapping approach. Comprehensive experiments demonstrate that Caro establishes state-of-
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#39318;&#27425;&#32508;&#36848;&#20102;&#26368;&#26032;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;&#26681;&#25454;&#31639;&#27861;&#20351;&#29992;&#30340;&#25968;&#25454;&#65292;&#23558;&#26041;&#27861;&#20998;&#25104;&#19977;&#31867;&#65292;&#24182;&#20171;&#32461;&#20102;&#30456;&#20851;&#25968;&#25454;&#38598;&#12289;&#24212;&#29992;&#21644;&#24230;&#37327;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.03236</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#22522;&#20110;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Out-of-Distribution Detection in NLP. (arXiv:2305.03236v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03236
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#39318;&#27425;&#32508;&#36848;&#20102;&#26368;&#26032;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;&#26681;&#25454;&#31639;&#27861;&#20351;&#29992;&#30340;&#25968;&#25454;&#65292;&#23558;&#26041;&#27861;&#20998;&#25104;&#19977;&#31867;&#65292;&#24182;&#20171;&#32461;&#20102;&#30456;&#20851;&#25968;&#25454;&#38598;&#12289;&#24212;&#29992;&#21644;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#22522;&#20110;&#22806;&#37096;&#20998;&#24067;&#65288;OOD&#65289;&#30340;&#26816;&#27979;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#21487;&#38752;&#21644;&#23433;&#20840;&#30340;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#36807;&#21435;&#20960;&#24180;&#21462;&#24471;&#20102;&#26497;&#22823;&#36827;&#23637;&#12290;&#26412;&#25991;&#37325;&#28857;&#20851;&#27880;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65292;&#24182;&#39318;&#27425;&#32508;&#36848;&#20102;OOD&#26816;&#27979;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#32473;&#20986;OOD&#26816;&#27979;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#35752;&#35770;&#20102;&#20960;&#20010;&#30456;&#20851;&#39046;&#22495;&#12290;&#28982;&#21518;&#65292;&#26681;&#25454;&#31639;&#27861;&#20351;&#29992;&#30340;&#25968;&#25454;&#65292;&#23558;&#26368;&#36817;&#30340;&#31639;&#27861;&#20998;&#25104;&#19977;&#31867;&#65306;&#65288;1&#65289;&#21487;&#29992;OOD&#25968;&#25454;&#65292;&#65288;2&#65289;OOD&#25968;&#25454;&#19981;&#21487;&#29992;+&#20869;&#37096;&#20998;&#24067;&#65288;ID&#65289;&#26631;&#31614;&#21487;&#29992;&#65292;&#65288;3&#65289;OOD&#25968;&#25454;&#19981;&#21487;&#29992;+ID&#26631;&#31614;&#19981;&#21487;&#29992;&#12290;&#31532;&#19977;&#65292;&#20171;&#32461;&#25968;&#25454;&#38598;&#12289;&#24212;&#29992;&#21644;&#24230;&#37327;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#24635;&#32467;&#29616;&#26377;&#24037;&#20316;&#24182;&#25552;&#20986;&#28508;&#22312;&#30340;&#26410;&#26469;&#30740;&#31350;&#35838;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection is essential for the reliable and safe deployment of machine learning systems in the real world. Great progress has been made over the past years. This paper presents the first review of recent advances in OOD detection with a particular focus on natural language processing approaches. First, we provide a formal definition of OOD detection and discuss several related fields. We then categorize recent algorithms into three classes according to the data they used: (1) OOD data available, (2) OOD data unavailable + in-distribution (ID) label available, and (3) OOD data unavailable + ID label unavailable. Third, we introduce datasets, applications, and metrics. Finally, we summarize existing work and present potential future research topics.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;3D&#27169;&#25311;&#36229;&#20998;&#36776;&#29575;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#25928;&#12289;&#36924;&#30495;&#22320;&#22686;&#24378;&#20302;&#25104;&#26412;&#12289;&#23454;&#26102;&#29289;&#29702;&#27169;&#25311;&#20135;&#29983;&#30340;&#38754;&#37096;&#34920;&#29616;&#65292;&#20351;&#20854;&#25509;&#36817;&#20110;&#20855;&#26377;&#26356;&#39640;&#20998;&#36776;&#29575;&#21644;&#20934;&#30830;&#29289;&#29702;&#24314;&#27169;&#30340;&#21442;&#32771;&#36136;&#37327;&#31163;&#32447;&#27169;&#25311;&#22120;&#12290;</title><link>http://arxiv.org/abs/2305.03216</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;3D&#27169;&#25311;&#36229;&#20998;&#36776;&#29575;&#23454;&#29616;&#36817;&#23454;&#26102;&#38754;&#37096;&#21160;&#30011;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Near-realtime Facial Animation by Deep 3D Simulation Super-Resolution. (arXiv:2305.03216v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03216
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;3D&#27169;&#25311;&#36229;&#20998;&#36776;&#29575;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#25928;&#12289;&#36924;&#30495;&#22320;&#22686;&#24378;&#20302;&#25104;&#26412;&#12289;&#23454;&#26102;&#29289;&#29702;&#27169;&#25311;&#20135;&#29983;&#30340;&#38754;&#37096;&#34920;&#29616;&#65292;&#20351;&#20854;&#25509;&#36817;&#20110;&#20855;&#26377;&#26356;&#39640;&#20998;&#36776;&#29575;&#21644;&#20934;&#30830;&#29289;&#29702;&#24314;&#27169;&#30340;&#21442;&#32771;&#36136;&#37327;&#31163;&#32447;&#27169;&#25311;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#25311;&#36229;&#20998;&#36776;&#29575;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#25928;&#12289;&#36924;&#30495;&#22320;&#22686;&#24378;&#20302;&#25104;&#26412;&#12289;&#23454;&#26102;&#29289;&#29702;&#27169;&#25311;&#20135;&#29983;&#30340;&#38754;&#37096;&#34920;&#29616;&#65292;&#20351;&#20854;&#25509;&#36817;&#20110;&#20855;&#26377;&#26356;&#39640;&#20998;&#36776;&#29575;&#65288;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#39640;&#36798;26&#20493;&#30340;&#20803;&#32032;&#25968;&#65289;&#21644;&#20934;&#30830;&#29289;&#29702;&#24314;&#27169;&#30340;&#21442;&#32771;&#36136;&#37327;&#31163;&#32447;&#27169;&#25311;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28304;&#20110;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#26500;&#24314;&#19968;&#32452;&#37197;&#23545;&#24103;&#24207;&#21015;&#30340;&#33021;&#21147;&#65292;&#36825;&#20123;&#24207;&#21015;&#20998;&#21035;&#26469;&#33258;&#20110;&#20302;&#20998;&#36776;&#29575;&#21644;&#39640;&#20998;&#36776;&#29575;&#27169;&#25311;&#22120;&#65292;&#24182;&#19988;&#22312;&#35821;&#20041;&#19978;&#30456;&#20114;&#23545;&#24212;&#12290;&#25105;&#20204;&#20197;&#38754;&#37096;&#21160;&#30011;&#20026;&#20363;&#65292;&#21019;&#36896;&#36825;&#31181;&#35821;&#20041;&#19968;&#33268;&#24615;&#30340;&#26041;&#24335;&#23601;&#26159;&#22312;&#20004;&#20010;&#27169;&#25311;&#22120;&#20013;&#35843;&#25972;&#21516;&#26679;&#30340;&#32908;&#32905;&#28608;&#27963;&#25511;&#21046;&#21644;&#39592;&#26550;&#23039;&#21183;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#36229;&#20998;&#36776;&#29575;&#26694;&#26550;&#20174;&#36825;&#20010;&#35757;&#32451;&#38598;&#20013;&#27867;&#21270;&#21040;&#30475;&#19981;&#35265;&#30340;&#34920;&#24773;&#65292;&#24182;&#19988;&#34917;&#20607;&#20004;&#20010;&#27169;&#25311;&#20043;&#38388;&#30340;&#24314;&#27169;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a neural network-based simulation super-resolution framework that can efficiently and realistically enhance a facial performance produced by a low-cost, realtime physics-based simulation to a level of detail that closely approximates that of a reference-quality off-line simulator with much higher resolution (26x element count in our examples) and accurate physical modeling. Our approach is rooted in our ability to construct - via simulation - a training set of paired frames, from the low- and high-resolution simulators respectively, that are in semantic correspondence with each other. We use face animation as an exemplar of such a simulation domain, where creating this semantic congruence is achieved by simply dialing in the same muscle actuation controls and skeletal pose in the two simulators. Our proposed neural network super-resolution framework generalizes from this training set to unseen expressions, compensates for modeling discrepancies between the two simulations du
&lt;/p&gt;</description></item><item><title>LLM2Loss&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20026;&#40657;&#30418;&#27169;&#22411;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#35786;&#26029;&#65292;&#36890;&#36807;&#25552;&#21462;&#25968;&#25454;&#28857;&#30340;&#35821;&#20041;&#29305;&#24449;&#65292;&#25581;&#31034;&#23548;&#33268;&#27169;&#22411;&#22833;&#36133;&#21644;&#20559;&#24046;&#30340;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2305.03212</link><description>&lt;p&gt;
LLM2Loss: &#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
LLM2Loss: Leveraging Language Models for Explainable Model Diagnostics. (arXiv:2305.03212v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03212
&lt;/p&gt;
&lt;p&gt;
LLM2Loss&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20026;&#40657;&#30418;&#27169;&#22411;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#35786;&#26029;&#65292;&#36890;&#36807;&#25552;&#21462;&#25968;&#25454;&#28857;&#30340;&#35821;&#20041;&#29305;&#24449;&#65292;&#25581;&#31034;&#23548;&#33268;&#27169;&#22411;&#22833;&#36133;&#21644;&#20559;&#24046;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#37327;&#25968;&#25454;&#30340;&#35757;&#32451;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#25277;&#35937;&#31354;&#38388;&#20013;&#23545;&#30456;&#24403;&#22797;&#26434;&#30340;&#25991;&#26412;&#36755;&#20837;&#36827;&#34892;&#24314;&#27169;&#65292;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#25104;&#21151;&#21644;&#27010;&#25324;&#33021;&#21147;&#65292;&#20174;&#32780;&#25104;&#20026;&#20102;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#36825;&#31181;&#33021;&#21147;&#21487;&#20197;&#36890;&#36807;&#36328;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;CLIP&#65292;&#26469;&#25193;&#23637;&#21040;&#20854;&#20182;&#27169;&#24577;&#65292;&#22914;&#35270;&#35273;&#22495;&#65292;&#20174;&#32780;&#21487;&#20197;&#20174;&#35270;&#35273;&#36755;&#20837;&#20013;&#25552;&#21462;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#27169;&#22411;&#22833;&#36133;&#21644;&#20559;&#24046;&#27169;&#24335;&#30340;&#35821;&#20041;&#35265;&#35299;&#12290;&#23545;&#20110;&#32473;&#23450;&#30340;&#40657;&#30418;&#27169;&#22411;&#65292;&#20854;&#35757;&#32451;&#25968;&#25454;&#21644;&#20219;&#21153;&#23450;&#20041;&#65292;&#25105;&#20204;&#39318;&#20808;&#35745;&#31639;&#20854;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#20219;&#21153;&#30456;&#20851;&#25439;&#22833;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#21462;&#27599;&#20010;&#35757;&#32451;&#25968;&#25454;&#28857;&#30340;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#65288;&#20363;&#22914;&#26469;&#33258;&#20854;&#35270;&#35273;&#32534;&#30721;&#22120;&#30340;CLIP&#23884;&#20837;&#65289;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#35786;&#26029;&#27169;&#22411;&#65292;&#23558;&#36825;&#20010;&#25968;&#25454;&#28857;&#30340;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#26144;&#23556;&#21040;&#20854;&#20219;&#21153;&#25439;&#22833;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#35782;&#21035;&#23545;&#27169;&#22411;&#38590;&#20197;&#23398;&#20064;&#30340;&#25968;&#25454;&#28857;&#65292;&#32780;&#19988;&#21487;&#20197;&#35782;&#21035;&#23548;&#33268;&#27169;&#22411;&#22833;&#36133;&#21644;&#20559;&#24046;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trained on a vast amount of data, Large Language models (LLMs) have achieved unprecedented success and generalization in modeling fairly complex textual inputs in the abstract space, making them powerful tools for zero-shot learning. Such capability is extended to other modalities such as the visual domain using cross-modal foundation models such as CLIP, and as a result, semantically meaningful representation are extractable from visual inputs.  In this work, we leverage this capability and propose an approach that can provide semantic insights into a model's patterns of failures and biases. Given a black box model, its training data, and task definition, we first calculate its task-related loss for each data point. We then extract a semantically meaningful representation for each training data point (such as CLIP embeddings from its visual encoder) and train a lightweight diagnosis model which maps this semantically meaningful representation of a data point to its task loss. We show 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21360;&#24230;&#35821;&#31995;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#35789;&#27719;&#20849;&#20139;&#38382;&#39064;&#65292;&#25506;&#32034;&#20102;&#25968;&#25454;&#37319;&#26679;&#21644;&#35789;&#27719;&#37327;&#20043;&#38388;&#30340;&#32763;&#35793;&#24615;&#33021;&#26435;&#34913;&#65292;&#20197;&#21450;&#23383;&#27597;&#36716;&#20889;&#26159;&#21542;&#26377;&#21161;&#20110;&#20419;&#36827;&#36328;&#33050;&#26412;&#27010;&#25324;&#65292;&#21457;&#29616;&#23383;&#27597;&#36716;&#20889;&#23545;&#36328;&#33050;&#26412;&#32763;&#35793;&#25928;&#26524;&#27809;&#26377;&#26126;&#26174;&#25913;&#36827;&#65292;&#23545;&#20110;&#36739;&#20302;&#36164;&#28304;&#30340;&#35821;&#35328;&#65292;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#35757;&#32451;&#22312;&#21407;&#22987;&#33050;&#26412;&#19978;&#20284;&#20046;&#24050;&#32463;&#33021;&#22815;&#25269;&#25239;&#33050;&#26412;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.03207</link><description>&lt;p&gt;
&#30740;&#31350;&#21360;&#24230;&#35821;&#31995;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#35789;&#27719;&#20849;&#20139;
&lt;/p&gt;
&lt;p&gt;
Investigating Lexical Sharing in Multilingual Machine Translation for Indian Languages. (arXiv:2305.03207v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21360;&#24230;&#35821;&#31995;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#35789;&#27719;&#20849;&#20139;&#38382;&#39064;&#65292;&#25506;&#32034;&#20102;&#25968;&#25454;&#37319;&#26679;&#21644;&#35789;&#27719;&#37327;&#20043;&#38388;&#30340;&#32763;&#35793;&#24615;&#33021;&#26435;&#34913;&#65292;&#20197;&#21450;&#23383;&#27597;&#36716;&#20889;&#26159;&#21542;&#26377;&#21161;&#20110;&#20419;&#36827;&#36328;&#33050;&#26412;&#27010;&#25324;&#65292;&#21457;&#29616;&#23383;&#27597;&#36716;&#20889;&#23545;&#36328;&#33050;&#26412;&#32763;&#35793;&#25928;&#26524;&#27809;&#26377;&#26126;&#26174;&#25913;&#36827;&#65292;&#23545;&#20110;&#36739;&#20302;&#36164;&#28304;&#30340;&#35821;&#35328;&#65292;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#35757;&#32451;&#22312;&#21407;&#22987;&#33050;&#26412;&#19978;&#20284;&#20046;&#24050;&#32463;&#33021;&#22815;&#25269;&#25239;&#33050;&#26412;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#33021;&#21147;&#12290;&#20026;&#20102;&#25913;&#36827;&#36825;&#20123;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#33021;&#21147;&#65292;&#19968;&#20123;&#31574;&#30053;&#21253;&#25324;&#25913;&#36827;&#23383;&#31526;&#32454;&#20998;&#32780;&#19981;&#26159;&#23376;&#35789;&#21644;&#23545;&#23383;&#27597;&#36716;&#20889;&#30340;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#21360;&#22320;&#35821;&#12289;&#21476;&#21513;&#25289;&#29305;&#35821;&#12289;&#23612;&#27850;&#23572;&#35821;&#21040;&#33521;&#35821;&#30340;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#35789;&#27719;&#20849;&#20139;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#25968;&#25454;&#37319;&#26679;&#21644;&#35789;&#27719;&#37327;&#20043;&#38388;&#23384;&#22312;&#30340;&#32763;&#35793;&#24615;&#33021;&#26435;&#34913;&#65292;&#24182;&#25506;&#35752;&#20102;&#23383;&#27597;&#36716;&#20889;&#26159;&#21542;&#26377;&#21161;&#20110;&#20419;&#36827;&#36328;&#33050;&#26412;&#27010;&#25324;&#12290;&#25105;&#20204;&#36824;&#39564;&#35777;&#20102;&#19981;&#21516;&#35774;&#32622;&#22914;&#20309;&#25512;&#24191;&#21040;&#30475;&#19981;&#35265;&#30340;&#35821;&#35328;&#65288;&#39532;&#25289;&#22320;&#35821;&#21644;&#23391;&#21152;&#25289;&#35821;&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#23383;&#27597;&#36716;&#20889;&#27809;&#26377;&#26126;&#26174;&#30340;&#25913;&#36827;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#21407;&#22987;&#33050;&#26412;&#19978;&#35757;&#32451;&#26102;&#65292;&#21363;&#20351;&#23545;&#20110;&#30456;&#23545;&#20302;&#36164;&#28304;&#30340;&#35821;&#35328;&#65292;&#20284;&#20046;&#24050;&#32463;&#33021;&#22815;&#25269;&#25239;&#33050;&#26412;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual language models have shown impressive cross-lingual transfer ability across a diverse set of languages and tasks. To improve the cross-lingual ability of these models, some strategies include transliteration and finer-grained segmentation into characters as opposed to subwords. In this work, we investigate lexical sharing in multilingual machine translation (MT) from Hindi, Gujarati, Nepali into English. We explore the trade-offs that exist in translation performance between data sampling and vocabulary size, and we explore whether transliteration is useful in encouraging cross-script generalisation. We also verify how the different settings generalise to unseen languages (Marathi and Bengali). We find that transliteration does not give pronounced improvements and our analysis suggests that our multilingual MT models trained on original scripts seem to already be robust to cross-script differences even for relatively low-resource languages
&lt;/p&gt;</description></item><item><title>GPT-4&#26159;OpenAI&#24320;&#21457;&#30340;&#31532;&#22235;&#20195;GPT&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#36229;&#36807;1&#19975;&#20159;&#30340;&#27169;&#22411;&#35268;&#27169;&#12289;&#26356;&#22909;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#21644;&#25913;&#36827;&#30340;&#35821;&#22659;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#23427;&#26377;&#26395;&#24212;&#29992;&#20110;&#32842;&#22825;&#26426;&#22120;&#20154;&#12289;&#20010;&#20154;&#21161;&#29702;&#12289;&#35821;&#35328;&#32763;&#35793;&#12289;&#25991;&#26412;&#25688;&#35201;&#21644;&#38382;&#31572;&#31995;&#32479;&#65292;&#20294;&#20063;&#23384;&#22312;&#35745;&#31639;&#35201;&#27714;&#12289;&#25968;&#25454;&#35201;&#27714;&#21644;&#36947;&#24503;&#38382;&#39064;&#31561;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.03195</link><description>&lt;p&gt;
Gpt-4&#65306;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#36827;&#23637;&#21644;&#26426;&#36935;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Gpt-4: A Review on Advancements and Opportunities in Natural Language Processing. (arXiv:2305.03195v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03195
&lt;/p&gt;
&lt;p&gt;
GPT-4&#26159;OpenAI&#24320;&#21457;&#30340;&#31532;&#22235;&#20195;GPT&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#36229;&#36807;1&#19975;&#20159;&#30340;&#27169;&#22411;&#35268;&#27169;&#12289;&#26356;&#22909;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#21644;&#25913;&#36827;&#30340;&#35821;&#22659;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#23427;&#26377;&#26395;&#24212;&#29992;&#20110;&#32842;&#22825;&#26426;&#22120;&#20154;&#12289;&#20010;&#20154;&#21161;&#29702;&#12289;&#35821;&#35328;&#32763;&#35793;&#12289;&#25991;&#26412;&#25688;&#35201;&#21644;&#38382;&#31572;&#31995;&#32479;&#65292;&#20294;&#20063;&#23384;&#22312;&#35745;&#31639;&#35201;&#27714;&#12289;&#25968;&#25454;&#35201;&#27714;&#21644;&#36947;&#24503;&#38382;&#39064;&#31561;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#8221;&#65288;GPT&#65289;&#31995;&#21015;&#30340;&#31532;&#22235;&#20195;&#35821;&#35328;&#27169;&#22411;GPT-4&#30001;OpenAI&#24320;&#21457;&#65292;&#26377;&#26395;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#23454;&#29616;&#26174;&#33879;&#36827;&#23637;&#12290;&#26412;&#30740;&#31350;&#35770;&#25991;&#35752;&#35770;&#20102;GPT-4&#30340;&#29305;&#28857;&#12289;&#28508;&#22312;&#24212;&#29992;&#20197;&#21450;&#21487;&#33021;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#23558;GPT-4&#19982;&#20854;&#21069;&#36523;GPT-3&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#30456;&#27604;GPT-3&#65292;GPT-4&#20855;&#26377;&#26356;&#22823;&#30340;&#27169;&#22411;&#35268;&#27169;&#65288;&#36229;&#36807;1&#19975;&#20159;&#65289;&#12289;&#26356;&#22909;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#12289;&#25913;&#36827;&#30340;&#35821;&#22659;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;GPT-4&#30340;&#19968;&#20123;&#28508;&#22312;&#24212;&#29992;&#21253;&#25324;&#32842;&#22825;&#26426;&#22120;&#20154;&#12289;&#20010;&#20154;&#21161;&#29702;&#12289;&#35821;&#35328;&#32763;&#35793;&#12289;&#25991;&#26412;&#25688;&#35201;&#21644;&#38382;&#31572;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;GPT-4&#20063;&#23384;&#22312;&#35832;&#22810;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#22914;&#35745;&#31639;&#35201;&#27714;&#12289;&#25968;&#25454;&#35201;&#27714;&#21644;&#36947;&#24503;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Pre-trained Transformer 4 (GPT-4) is the fourth-generation language model in the GPT series, developed by OpenAI, which promises significant advancements in the field of natural language processing (NLP). In this research article, we have discussed the features of GPT-4, its potential applications, and the challenges that it might face. We have also compared GPT-4 with its predecessor, GPT-3. GPT-4 has a larger model size (more than one trillion), better multilingual capabilities, improved contextual understanding, and reasoning capabilities than GPT-3. Some of the potential applications of GPT-4 include chatbots, personal assistants, language translation, text summarization, and question-answering. However, GPT-4 poses several challenges and limitations such as computational requirements, data requirements, and ethical concerns.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#30340;&#31232;&#30095;&#24352;&#37327;&#22312;3D&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#23567;&#22411;&#27169;&#22411;&#65292;&#36890;&#36807;&#37319;&#29992;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#20943;&#23567;&#27169;&#22411;&#30340;&#22823;&#23567;&#24182;&#33719;&#24471;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03188</link><description>&lt;p&gt;
Smaller3d&#65306;&#20351;&#29992;Minkowski Engine&#21644;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#36827;&#34892;3D&#35821;&#20041;&#20998;&#21106;&#30340;&#23567;&#22411;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Smaller3d: Smaller Models for 3D Semantic Segmentation Using Minkowski Engine and Knowledge Distillation Methods. (arXiv:2305.03188v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#30340;&#31232;&#30095;&#24352;&#37327;&#22312;3D&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#23567;&#22411;&#27169;&#22411;&#65292;&#36890;&#36807;&#37319;&#29992;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#20943;&#23567;&#27169;&#22411;&#30340;&#22823;&#23567;&#24182;&#33719;&#24471;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;3D&#39046;&#22495;&#65292;&#26377;&#35768;&#22810;&#20248;&#21270;&#25216;&#26415;&#65292;&#21253;&#25324;&#22522;&#20110;&#28857;&#20113;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32593;&#26684;&#12289;&#32441;&#29702;&#21644;&#20307;&#32032;&#26469;&#20248;&#21270;3D&#23384;&#20648;&#21644;&#35745;&#31639;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20026;&#20102;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#32780;&#37319;&#29992;&#30340;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#31232;&#30095;&#24352;&#37327;&#22312;3D&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#20998;&#26512;&#21644;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21253;&#25324;&#26631;&#20934;&#26041;&#27861;&#21644;&#21508;&#31181;&#25439;&#22833;&#30340;&#32452;&#21512;&#65292;&#20197;&#27169;&#25311;&#19981;&#21516;&#31232;&#30095;&#21367;&#31215;NN&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26159;&#22312;&#26631;&#20934;&#30340;ScanNet V2&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#32467;&#26524;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20339;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are various optimization techniques in the realm of 3D, including point cloud-based approaches that use mesh, texture, and voxels which optimize how you store, and how do calculate in 3D. These techniques employ methods such as feed-forward networks, 3D convolutions, graph neural networks, transformers, and sparse tensors. However, the field of 3D is one of the most computationally expensive fields, and these methods have yet to achieve their full potential due to their large capacity, complexity, and computation limits. This paper proposes the application of knowledge distillation techniques, especially for sparse tensors in 3D deep learning, to reduce model sizes while maintaining performance. We analyze and purpose different loss functions, including standard methods and combinations of various losses, to simulate the performance of state-of-the-art models of different Sparse Convolutional NNs. Our experiments are done on the standard ScanNet V2 dataset, and we achieved around
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;POET&#30340;&#22522;&#20110;Python&#30340;&#26694;&#26550;&#65292;&#33021;&#33258;&#23398;&#20064;PROFINET&#32593;&#32476;&#30340;&#22522;&#30784;&#35774;&#26045;&#20449;&#24687;&#65292;&#21253;&#25324;PLCs&#12289;&#29616;&#22330;&#35774;&#22791;&#21644;&#20154;&#26426;&#30028;&#38754;&#65292;&#21033;&#29992;&#30693;&#35782;&#22270;&#26469;&#20248;&#21270;&#24322;&#24120;&#26816;&#27979;&#21644;&#20998;&#31867;&#32467;&#26524;&#65292;&#33021;&#26377;&#25928;&#25552;&#39640;&#24037;&#19994;&#32593;&#32476;&#23433;&#20840;&#12290;</title><link>http://arxiv.org/abs/2305.03175</link><description>&lt;p&gt;
POET: PROFINET&#24037;&#19994;&#25805;&#20316;&#34892;&#20026;&#30340;&#33258;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
POET: A Self-learning Framework for PROFINET Industrial Operations Behaviour. (arXiv:2305.03175v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03175
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;POET&#30340;&#22522;&#20110;Python&#30340;&#26694;&#26550;&#65292;&#33021;&#33258;&#23398;&#20064;PROFINET&#32593;&#32476;&#30340;&#22522;&#30784;&#35774;&#26045;&#20449;&#24687;&#65292;&#21253;&#25324;PLCs&#12289;&#29616;&#22330;&#35774;&#22791;&#21644;&#20154;&#26426;&#30028;&#38754;&#65292;&#21033;&#29992;&#30693;&#35782;&#22270;&#26469;&#20248;&#21270;&#24322;&#24120;&#26816;&#27979;&#21644;&#20998;&#31867;&#32467;&#26524;&#65292;&#33021;&#26377;&#25928;&#25552;&#39640;&#24037;&#19994;&#32593;&#32476;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;2010&#24180;&#20197;&#26469;&#65292;&#22810;&#27425;&#23545;&#24037;&#19994;&#22522;&#30784;&#35774;&#26045;&#30340;&#32593;&#32476;&#25915;&#20987;&#20107;&#20214;&#65288;&#20363;&#22914;Stuxnet&#21644;CrashOverride&#65289;&#19968;&#20877;&#26292;&#38706;&#20102;&#24037;&#19994;&#25511;&#21046;&#31995;&#32479;&#65288;ICS&#65289;&#23545;&#32593;&#32476;&#23041;&#32961;&#30340;&#33030;&#24369;&#24615;&#12290;&#24037;&#19994;&#31995;&#32479;&#30340;&#25237;&#20837;&#20351;&#29992;&#26102;&#38388;&#38271;&#65292;&#38271;&#36798;&#20960;&#21313;&#24180;&#65292;&#24448;&#24448;&#23548;&#33268;&#19982;&#24037;&#19994;&#32593;&#32476;&#23433;&#20840;&#26426;&#21046;&#30340;&#25216;&#26415;&#36827;&#23637;&#19981;&#31526;&#12290;&#32593;&#32476;&#22522;&#30784;&#35774;&#26045;&#20449;&#24687;&#30340;&#19981;&#21487;&#29992;&#24615;&#20351;&#24471;&#35774;&#35745;&#23433;&#20840;&#31574;&#30053;&#25110;&#37197;&#32622;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;NIDS&#65289;&#31561;&#32593;&#32476;&#23433;&#20840;&#23545;&#31574;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#19968;&#20010;&#23454;&#35777;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#20174;&#30417;&#35270;&#30340;&#32593;&#32476;&#27969;&#37327;&#20013;&#33258;&#23398;&#20064;&#24037;&#19994;&#31995;&#32479;&#30340;&#32593;&#32476;&#22522;&#30784;&#35774;&#26045;&#20449;&#24687;&#65292;&#20197;&#20351;&#32593;&#32476;&#23545;&#19979;&#28216;&#20998;&#26512;&#20219;&#21153;&#65288;&#20363;&#22914;&#24322;&#24120;&#26816;&#27979;&#65289;&#36879;&#26126;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Python&#30340;PROFINET&#36890;&#20449;&#33539;&#24335;&#24863;&#30693;&#26694;&#26550;&#65292;&#21517;&#20026;&#8220;PROFINET&#25805;&#20316;&#26522;&#20030;&#21644;&#36319;&#36394;&#8221;&#65288;POET&#65289;&#65292;&#29992;&#20110;&#26522;&#20030;&#21487;&#32534;&#31243;&#36923;&#36753;&#25511;&#21046;&#22120;&#65288;PLC&#65289;&#25191;&#34892;&#30340;&#19981;&#21516;&#24037;&#19994;&#25805;&#20316;&#65292;&#24182;&#26500;&#24314;&#24037;&#19994;&#36890;&#20449;&#30340;&#30693;&#35782;&#22270;&#12290;POET&#34987;&#35774;&#35745;&#20026;&#20174;&#32593;&#32476;&#27969;&#37327;&#20013;&#33258;&#23398;&#20064;&#32593;&#32476;&#22522;&#30784;&#35774;&#26045;&#20449;&#24687;&#65292;&#21253;&#25324;PLCs&#12289;&#29616;&#22330;&#35774;&#22791;&#21644;&#20154;&#26426;&#30028;&#38754;&#65292;&#24182;&#20351;&#29992;&#30693;&#35782;&#22270;&#26469;&#20248;&#21270;&#24322;&#24120;&#26816;&#27979;&#21644;&#20998;&#31867;&#32467;&#26524;&#12290;&#22312;PROFINET&#27979;&#35797;&#24179;&#21488;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35780;&#20272;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;POET&#26694;&#26550;&#22312;&#24037;&#19994;&#32593;&#32476;&#23433;&#20840;&#19978;&#30340;&#39640;&#25928;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since 2010, multiple cyber incidents on industrial infrastructure, such as Stuxnet and CrashOverride, have exposed the vulnerability of Industrial Control Systems (ICS) to cyber threats. The industrial systems are commissioned for longer duration amounting to decades, often resulting in non-compliance to technological advancements in industrial cybersecurity mechanisms. The unavailability of network infrastructure information makes designing the security policies or configuring the cybersecurity countermeasures such as Network Intrusion Detection Systems (NIDS) challenging. An empirical solution is to self-learn the network infrastructure information of an industrial system from its monitored network traffic to make the network transparent for downstream analyses tasks such as anomaly detection. In this work, a Python-based industrial communication paradigm-aware framework, named PROFINET Operations Enumeration and Tracking (POET), that enumerates different industrial operations execut
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#24773;&#24863;&#20998;&#26512;&#24182;&#26816;&#27979;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#34255;&#23618;&#29305;&#24449;&#22270;&#36896;&#25104;&#30340;&#24433;&#21709;&#36880;&#28176;&#26174;&#29616;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#23616;&#38480;&#65292;&#19988;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#35782;&#21035;&#26368;&#26032;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2305.03173</link><description>&lt;p&gt;
&#22522;&#20110;&#24773;&#24863;&#20998;&#26512;&#30340;&#26032;&#22411;&#23545;&#25239;&#24615;&#22270;&#20687;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
New Adversarial Image Detection Based on Sentiment Analysis. (arXiv:2305.03173v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#24773;&#24863;&#20998;&#26512;&#24182;&#26816;&#27979;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#34255;&#23618;&#29305;&#24449;&#22270;&#36896;&#25104;&#30340;&#24433;&#21709;&#36880;&#28176;&#26174;&#29616;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#23616;&#38480;&#65292;&#19988;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#35782;&#21035;&#26368;&#26032;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#26131;&#21463;&#21040;&#23545;&#25239;&#26679;&#26412;&#30340;&#25915;&#20987;&#65292;&#32780;&#23545;&#25239;&#25915;&#20987;&#27169;&#22411;&#65288;&#20363;&#22914;DeepFool&#65289;&#27491;&#22312;&#23835;&#36215;&#24182;&#36229;&#36234;&#23545;&#25239;&#24615;&#26679;&#26412;&#26816;&#27979;&#25216;&#26415;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#26816;&#27979;&#22120;&#65292;&#21487;&#22312;&#35782;&#21035;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#38754;&#36229;&#36234;&#29616;&#26377;&#25216;&#26415;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#24773;&#24863;&#20998;&#26512;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#36825;&#26159;&#36890;&#36807;&#26816;&#27979;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#34255;&#23618;&#29305;&#24449;&#22270;&#36896;&#25104;&#30340;&#24433;&#21709;&#36880;&#28176;&#26174;&#29616;&#26469;&#23454;&#29616;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#19968;&#20010;&#20855;&#26377;&#26368;&#23569;&#21487;&#23398;&#20064;&#21442;&#25968;&#30340;&#27169;&#22359;&#21270;&#23884;&#20837;&#23618;&#65292;&#23558;&#38544;&#34255;&#23618;&#29305;&#24449;&#26144;&#23556;&#21040;&#35789;&#21521;&#37327;&#20013;&#65292;&#32452;&#25104;&#21487;&#20197;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#30340;&#21477;&#23376;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26032;&#22411;&#26816;&#27979;&#22120;&#22312;&#26816;&#27979;&#38024;&#23545;CIFAR-10&#12289;SVHN&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#30340;ResNet&#21644;Inception&#20013;&#24615;&#32593;&#32476;&#21457;&#36215;&#30340;&#26368;&#26032;&#25915;&#20987;&#26041;&#38754;&#22343;&#33021;&#19968;&#33268;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) are vulnerable to adversarial examples, while adversarial attack models, e.g., DeepFool, are on the rise and outrunning adversarial example detection techniques. This paper presents a new adversarial example detector that outperforms state-of-the-art detectors in identifying the latest adversarial attacks on image datasets. Specifically, we propose to use sentiment analysis for adversarial example detection, qualified by the progressively manifesting impact of an adversarial perturbation on the hidden-layer feature maps of a DNN under attack. Accordingly, we design a modularized embedding layer with the minimum learnable parameters to embed the hidden-layer feature maps into word vectors and assemble sentences ready for sentiment analysis. Extensive experiments demonstrate that the new detector consistently surpasses the state-of-the-art detection algorithms in detecting the latest attacks launched against ResNet and Inception neutral networks on the CIFAR-1
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; Graph VAE &#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;ISPE&#65292;&#33021;&#22815;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#20445;&#25345;&#35821;&#20041;&#30340;&#23884;&#20837;&#36923;&#36753;&#20844;&#24335;&#65292;&#24182;&#19988;&#35813;&#26041;&#27861;&#26159;&#21487;&#36870;&#30340;&#12290;&#22312;&#35821;&#21477;&#34917;&#20840;&#21644;&#25512;&#29702;&#31561;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.03143</link><description>&lt;p&gt;
&#26500;&#24314;&#21487;&#36870;&#30340;&#35821;&#20041;&#20445;&#25345;&#30340;&#36923;&#36753;&#20844;&#24335;&#23884;&#20837;&#65306;&#19968;&#31181;&#22522;&#20110; Graph VAE &#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Invertible Semantic-Preserving Embeddings of Logical Formulae. (arXiv:2305.03143v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; Graph VAE &#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;ISPE&#65292;&#33021;&#22815;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#20445;&#25345;&#35821;&#20041;&#30340;&#23884;&#20837;&#36923;&#36753;&#20844;&#24335;&#65292;&#24182;&#19988;&#35813;&#26041;&#27861;&#26159;&#21487;&#36870;&#30340;&#12290;&#22312;&#35821;&#21477;&#34917;&#20840;&#21644;&#25512;&#29702;&#31561;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36923;&#36753;&#26159;&#33258;&#21160;&#25512;&#29702;&#30340;&#20027;&#35201;&#24418;&#24335;&#35821;&#35328;&#65292;&#21516;&#26102;&#20063;&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#35821;&#35328;&#12290;&#23398;&#20064;&#21644;&#20248;&#21270;&#36923;&#36753;&#35268;&#21017;&#19968;&#30452;&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26159;&#22522;&#20110;&#36830;&#32493;&#31354;&#38388;&#30340;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#65292;&#32780;&#23398;&#20064;&#36923;&#36753;&#21017;&#22788;&#20110;&#31163;&#25955;&#30340;&#35821;&#27861;&#31354;&#38388;&#20013;&#12290;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#38656;&#35201;&#25552;&#20986;&#19968;&#31181;&#33021;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#20445;&#25345;&#35821;&#20041;&#30340;&#23884;&#20837;&#26041;&#27861;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#34429;&#28982;&#33021;&#20445;&#25345;&#35821;&#20041;&#65292;&#20294;&#26159;&#19981;&#21487;&#36870;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#21363;&#21487;&#36870;&#35821;&#20041;&#20445;&#25345;&#23884;&#20837;&#65288;ISPE&#65289;&#65292;&#21033;&#29992;&#22522;&#20110; Graph VAE &#30340;&#28145;&#24230;&#32467;&#26500;&#23454;&#29616;&#20102;&#23884;&#20837;&#12290;&#35813;&#26041;&#27861;&#22312;&#35821;&#21477;&#34917;&#20840;&#21644;&#25512;&#29702;&#31561;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Logic is the main formal language to perform automated reasoning, and it is further a human-interpretable language, at least for small formulae. Learning and optimising logic requirements and rules has always been an important problem in Artificial Intelligence. State of the art Machine Learning (ML) approaches are mostly based on gradient descent optimisation in continuous spaces, while learning logic is framed in the discrete syntactic space of formulae. Using continuous optimisation to learn logic properties is a challenging problem, requiring to embed formulae in a continuous space in a meaningful way, i.e. preserving the semantics. Current methods are able to construct effective semantic-preserving embeddings via kernel methods (for linear temporal logic), but the map they define is not invertible. In this work we address this problem, learning how to invert such an embedding leveraging deep architectures based on the Graph Variational Autoencoder framework. We propose a novel mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20851;&#27880;ChatGPT&#38754;&#20020;&#30340;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;SPADE&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#32531;&#35299;&#21644;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.03123</link><description>&lt;p&gt;
ChatGPT &#38656;&#35201;&#36827;&#34892;SPADE&#65288;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#65289;&#35780;&#20272;&#65306;&#19968;&#39033;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review. (arXiv:2305.03123v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20851;&#27880;ChatGPT&#38754;&#20020;&#30340;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;SPADE&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#32531;&#35299;&#21644;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#21478;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#30001;&#20110;&#20854;&#24615;&#33021;&#21644;&#26377;&#25928;&#30340;&#23545;&#35805;&#33021;&#21147;&#65292;&#22312;&#30740;&#31350;&#21644;&#24037;&#19994;&#30028;&#20013;&#24471;&#21040;&#20102;&#24040;&#22823;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#21457;&#34920;&#65292;&#20197;&#23637;&#31034;ChatGPT&#21644;&#20854;&#20182;LLMs&#30340;&#26377;&#25928;&#24615;&#12289;&#25928;&#29575;&#12289;&#38598;&#25104;&#21644;&#24773;&#24863;&#12290;&#30456;&#21453;&#65292;&#26412;&#30740;&#31350;&#20851;&#27880;&#30340;&#26159;&#22823;&#22810;&#25968;&#34987;&#24573;&#35270;&#30340;&#37325;&#35201;&#26041;&#38754;&#65292;&#21363;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#65292;&#24182;&#24314;&#35758;&#19981;&#20165;&#20165;&#26159;ChatGPT&#65292;&#32780;&#26159;&#22312;&#23545;&#35805;&#26426;&#22120;&#20154;&#31867;&#21035;&#20013;&#30340;&#27599;&#19968;&#20010;&#21518;&#32493;&#20837;&#21475;&#37117;&#24212;&#35813;&#36827;&#34892;SPADE&#35780;&#20272;&#12290;&#26412;&#25991;&#35814;&#32454;&#35752;&#35770;&#20102;&#20851;&#20110;ChatGPT&#30340;&#38382;&#39064;&#21644;&#20851;&#27880;&#28857;&#19982;&#19978;&#36848;&#29305;&#24449;&#19968;&#33268;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20123;&#21021;&#27493;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#21487;&#35270;&#21270;&#20197;&#21450;&#20551;&#35774;&#30340;&#20107;&#23454;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#36824;&#20026;&#27599;&#20010;&#38382;&#39064;&#25552;&#20986;&#20102;&#32531;&#35299;&#21644;&#24314;&#35758;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20123;&#26410;&#26469;&#26041;&#21521;&#21644;&#24320;&#25918;&#38382;&#39064;&#30340;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is another large language model (LLM) inline but due to its performance and ability to converse effectively, it has gained a huge popularity amongst research as well as industrial community. Recently, many studies have been published to show the effectiveness, efficiency, integration, and sentiments of chatGPT and other LLMs. In contrast, this study focuses on the important aspects that are mostly overlooked, i.e. sustainability, privacy, digital divide, and ethics and suggests that not only chatGPT but every subsequent entry in the category of conversational bots should undergo Sustainability, PrivAcy, Digital divide, and Ethics (SPADE) evaluation. This paper discusses in detail about the issues and concerns raised over chatGPT in line with aforementioned characteristics. We support our hypothesis by some preliminary data collection and visualizations along with hypothesized facts. We also suggest mitigations and recommendations for each of the concerns. Furthermore, we also s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20154;&#31867;&#36741;&#21161;&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#65292;&#31216;&#20026;HAISTA-NET&#65292;&#22686;&#24378;&#20102;&#29616;&#26377;&#30340;&#23454;&#20363;&#20998;&#21106;&#32593;&#32476;&#65292;&#24341;&#20837;&#20102;&#20154;&#31867;&#25351;&#23450;&#30340;&#37096;&#20998;&#36793;&#30028;&#22320;&#22270;&#65292;&#20197;&#29983;&#25104;&#26356;&#31934;&#30830;&#30340;&#20998;&#21106;&#25513;&#27169;&#12290;</title><link>http://arxiv.org/abs/2305.03105</link><description>&lt;p&gt;
HAISTA-NET: &#36890;&#36807;&#27880;&#24847;&#21147;&#36827;&#34892;&#20154;&#31867;&#36741;&#21161;&#30340;&#23454;&#20363;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
HAISTA-NET: Human Assisted Instance Segmentation Through Attention. (arXiv:2305.03105v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03105
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20154;&#31867;&#36741;&#21161;&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#65292;&#31216;&#20026;HAISTA-NET&#65292;&#22686;&#24378;&#20102;&#29616;&#26377;&#30340;&#23454;&#20363;&#20998;&#21106;&#32593;&#32476;&#65292;&#24341;&#20837;&#20102;&#20154;&#31867;&#25351;&#23450;&#30340;&#37096;&#20998;&#36793;&#30028;&#22320;&#22270;&#65292;&#20197;&#29983;&#25104;&#26356;&#31934;&#30830;&#30340;&#20998;&#21106;&#25513;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20363;&#20998;&#21106;&#26159;&#22270;&#20687;&#26816;&#27979;&#30340;&#19968;&#31181;&#24418;&#24335;&#65292;&#22312;&#29289;&#20307;&#32454;&#21270;&#12289;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#21644;&#22270;&#20687;/&#35270;&#39057;&#32534;&#36753;&#31561;&#26041;&#38754;&#26377;&#24191;&#27867;&#24212;&#29992;&#65292;&#36825;&#20123;&#24212;&#29992;&#37117;&#38656;&#35201;&#39640;&#24230;&#31934;&#30830;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#21363;&#20415;&#26159;&#26368;&#20808;&#36827;&#30340;&#23436;&#20840;&#33258;&#21160;&#21270;&#23454;&#20363;&#20998;&#21106;&#31639;&#27861;&#65292;&#20854;&#31934;&#24230;&#24120;&#24120;&#26080;&#27861;&#36798;&#21040;&#12290;&#23545;&#20110;&#23567;&#32780;&#22797;&#26434;&#30340;&#23545;&#35937;&#26469;&#35828;&#65292;&#24615;&#33021;&#24046;&#36317;&#23588;&#20026;&#26126;&#26174;&#12290;&#36890;&#24120;&#65292;&#20174;&#19994;&#32773;&#21482;&#33021;&#37319;&#29992;&#23436;&#20840;&#25163;&#21160;&#30340;&#27880;&#37322;&#26041;&#27861;&#65292;&#36825;&#21487;&#33021;&#26159;&#19968;&#20010;&#32321;&#29712;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#39044;&#27979;&#65292;&#24182;&#20026;&#39640;&#26354;&#29575;&#12289;&#22797;&#26434;&#21644;&#23567;&#35268;&#27169;&#23545;&#35937;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#20998;&#21106;&#25513;&#27169;&#12290;&#25105;&#20204;&#30340;&#20154;&#31867;&#36741;&#21161;&#20998;&#21106;&#27169;&#22411;HAISTA-NET&#65292;&#25193;&#20805;&#20102;&#29616;&#26377;&#30340;Strong Mask R-CNN&#32593;&#32476;&#65292;&#20197;&#21253;&#25324;&#20154;&#31867;&#25351;&#23450;&#30340;&#37096;&#20998;&#36793;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#25163;&#32472;&#37096;&#20998;&#29289;&#20307;&#36793;&#30028;&#30340;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;&#20154;&#31867;&#27880;&#24847;&#21147;&#22320;&#22270;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23427;&#32771;&#34385;&#21040;&#20102;&#37096;&#20998;&#27880;&#24847;&#21147;&#22320;&#22270;&#21644;&#21407;&#22987;&#25513;&#27169;&#25552;&#35758;&#65292;&#36825;&#20351;&#32593;&#32476;&#33021;&#22815;&#20851;&#27880;&#20154;&#20204;&#35748;&#20026;&#26368;&#37325;&#35201;&#30340;&#21306;&#22495;&#12290;&#22312;PASCAL VOC&#21644;COCO&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#24378;&#22522;&#32447;&#65292;&#24182;&#22312;&#23567;&#32780;&#22797;&#26434;&#23545;&#35937;&#23454;&#20363;&#20998;&#21106;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instance segmentation is a form of image detection which has a range of applications, such as object refinement, medical image analysis, and image/video editing, all of which demand a high degree of accuracy. However, this precision is often beyond the reach of what even state-of-the-art, fully automated instance segmentation algorithms can deliver. The performance gap becomes particularly prohibitive for small and complex objects. Practitioners typically resort to fully manual annotation, which can be a laborious process. In order to overcome this problem, we propose a novel approach to enable more precise predictions and generate higher-quality segmentation masks for high-curvature, complex and small-scale objects. Our human-assisted segmentation model, HAISTA-NET, augments the existing Strong Mask R-CNN network to incorporate human-specified partial boundaries. We also present a dataset of hand-drawn partial object boundaries, which we refer to as human attention maps. In addition, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#22810;&#20803;&#22270;&#20687;&#23436;&#25104;&#26041;&#27861;&#36827;&#34892;&#20083;&#33146;&#25195;&#25551;&#24322;&#24120;&#23450;&#20301;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25506;&#32034;&#23436;&#25104;&#30340;&#22810;&#20803;&#24615;&#26469;&#25552;&#39640;&#35780;&#20272;&#26631;&#20934;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.03098</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#22810;&#20803;&#22270;&#20687;&#23436;&#25104;&#36827;&#34892;&#39640;&#20998;&#36776;&#29575;&#20083;&#33146;&#25195;&#25551;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Unsupervised anomaly localization in high-resolution breast scans using deep pluralistic image completion. (arXiv:2305.03098v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#22810;&#20803;&#22270;&#20687;&#23436;&#25104;&#26041;&#27861;&#36827;&#34892;&#20083;&#33146;&#25195;&#25551;&#24322;&#24120;&#23450;&#20301;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25506;&#32034;&#23436;&#25104;&#30340;&#22810;&#20803;&#24615;&#26469;&#25552;&#39640;&#35780;&#20272;&#26631;&#20934;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#20083;&#33146;&#25668;&#24433;&#20013;&#30340;&#33258;&#21160;&#32959;&#30244;&#26816;&#27979;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#32959;&#30244;&#24456;&#23569;&#20986;&#29616;&#65292;&#20083;&#25151;&#32452;&#32455;&#21464;&#24322;&#21644;&#39640;&#20998;&#36776;&#29575;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#37492;&#20110;&#24322;&#24120;&#22270;&#20687;&#30340;&#31232;&#32570;&#24615;&#21644;&#27491;&#24120;&#22270;&#20687;&#30340;&#20016;&#23500;&#24615;&#65292;&#24322;&#24120;&#26816;&#27979;/&#23450;&#20301;&#26041;&#27861;&#21487;&#33021;&#38750;&#24120;&#36866;&#21512;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#23398;&#20064;&#20013;&#22823;&#37096;&#20998;&#24322;&#24120;&#23450;&#20301;&#30740;&#31350;&#38598;&#20013;&#22312;&#38750;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#22312;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36866;&#24212;&#24615;&#19981;&#36275;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#35299;&#20915;&#22270;&#20687;&#23436;&#25104;&#35270;&#35282;&#19979;&#30340;&#20219;&#21153;&#24471;&#21040;&#32531;&#35299;&#65292;&#20854;&#20013;&#24322;&#24120;&#30340;&#23384;&#22312;&#21487;&#20197;&#36890;&#36807;&#21407;&#22987;&#22806;&#35266;&#19982;&#20854;&#29615;&#22659;&#26465;&#20214;&#19979;&#33258;&#21160;&#23436;&#25104;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#25351;&#31034;&#12290;&#28982;&#32780;&#65292;&#22312;DBT&#25968;&#25454;&#38598;&#20013;&#65292;&#24448;&#24448;&#26377;&#24456;&#22810;&#30456;&#21516;&#29615;&#22659;&#26465;&#20214;&#19979;&#30340;&#26377;&#25928;&#30340;&#27491;&#24120;&#23436;&#25104;&#65292;&#20351;&#36825;&#20010;&#35780;&#20272;&#26631;&#20934;&#19981;&#22826;&#31934;&#30830;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#32771;&#34385;&#36890;&#36807;&#25506;&#32034;&#23436;&#25104;&#30340;&#22810;&#20803;&#24615;&#26469;&#36827;&#34892;&#22270;&#20687;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated tumor detection in Digital Breast Tomosynthesis (DBT) is a difficult task due to natural tumor rarity, breast tissue variability, and high resolution. Given the scarcity of abnormal images and the abundance of normal images for this problem, an anomaly detection/localization approach could be well-suited. However, most anomaly localization research in machine learning focuses on non-medical datasets, and we find that these methods fall short when adapted to medical imaging datasets. The problem is alleviated when we solve the task from the image completion perspective, in which the presence of anomalies can be indicated by a discrepancy between the original appearance and its auto-completion conditioned on the surroundings. However, there are often many valid normal completions given the same surroundings, especially in the DBT dataset, making this evaluation criterion less precise. To address such an issue, we consider pluralistic image completion by exploring the distributi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;FEDORA&#30340;&#32852;&#37030;&#38598;&#25104;&#25351;&#23548;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#25552;&#28860;&#23458;&#25143;&#32676;&#20307;&#30340;&#38598;&#20307;&#26234;&#24935;&#65292;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#21253;&#25324;&#22312;&#21512;&#24182;&#30340;&#25968;&#25454;&#27719;&#24635;&#20013;&#36827;&#34892;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#21508;&#31181;&#22797;&#26434;&#30340;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.03097</link><description>&lt;p&gt;
&#32852;&#37030;&#38598;&#25104;&#25351;&#23548;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Federated Ensemble-Directed Offline Reinforcement Learning. (arXiv:2305.03097v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;FEDORA&#30340;&#32852;&#37030;&#38598;&#25104;&#25351;&#23548;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#25552;&#28860;&#23458;&#25143;&#32676;&#20307;&#30340;&#38598;&#20307;&#26234;&#24935;&#65292;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#21253;&#25324;&#22312;&#21512;&#24182;&#30340;&#25968;&#25454;&#27719;&#24635;&#20013;&#36827;&#34892;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#21508;&#31181;&#22797;&#26434;&#30340;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#32852;&#37030;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#36825;&#19968;&#22330;&#26223;&#19979;&#65292;&#20998;&#24067;&#24335;&#30340;&#23398;&#20064;&#20195;&#29702;&#24517;&#39035;&#20165;&#20351;&#29992;&#30001;&#19981;&#21516;&#30340;&#26410;&#30693;&#30340;&#34892;&#20026;&#31574;&#30053;&#29983;&#25104;&#30340;&#23567;&#22411;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#21327;&#20316;&#23398;&#20064;&#20986;&#39640;&#36136;&#37327;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;&#31528;&#25305;&#22320;&#23558;&#26631;&#20934;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#19982;&#26631;&#20934;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#32452;&#21512;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#21487;&#33021;&#20250;&#23548;&#33268;&#34920;&#29616;&#19981;&#20339;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#22240;&#27492;&#35774;&#35745;&#20102;Federated Ensemble-Directed Offline Reinforcement Learning Algorithm (FEDORA)&#65292;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#25552;&#28860;&#23458;&#25143;&#32676;&#20307;&#30340;&#38598;&#20307;&#26234;&#24935;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;FEDORA&#20195;&#30721;&#24211;&#65292;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#19978;&#30340;&#20998;&#24067;&#24335;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;FEDORA&#22312;&#21508;&#31181;&#22797;&#26434;&#30340;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#22343;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#21253;&#25324;&#22312;&#21512;&#24182;&#30340;&#25968;&#25454;&#27719;&#24635;&#20013;&#36827;&#34892;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;FEDORA&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of federated offline reinforcement learning (RL), a scenario under which distributed learning agents must collaboratively learn a high-quality control policy only using small pre-collected datasets generated according to different unknown behavior policies. Naively combining a standard offline RL approach with a standard federated learning approach to solve this problem can lead to poorly performing policies. In response, we develop the Federated Ensemble-Directed Offline Reinforcement Learning Algorithm (FEDORA), which distills the collective wisdom of the clients using an ensemble learning approach. We develop the FEDORA codebase to utilize distributed compute resources on a federated learning platform. We show that FEDORA significantly outperforms other approaches, including offline RL over the combined data pool, in various complex continuous control environments and real world datasets. Finally, we demonstrate the performance of FEDORA in the real-world on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20849;&#21516;&#24314;&#27169;&#8220;&#35201;&#38382;&#20160;&#20040;&#8221;&#21644;&#8220;&#22914;&#20309;&#38382;&#8221;&#30340;&#38382;&#39064;&#65292;&#22312;&#19981;&#26126;&#30830;&#31572;&#26696;&#30340;&#35774;&#32622;&#20013;&#38750;&#24120;&#23454;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.03088</link><description>&lt;p&gt;
&#38754;&#21521;&#26080;&#31572;&#26696;&#30340;&#20250;&#35805;&#38382;&#31572;&#29983;&#25104;&#30340;&#8220;&#35201;&#38382;&#20160;&#20040;&#8221;&#21644;&#8220;&#22914;&#20309;&#38382;&#8221;&#30340;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Modeling What-to-ask and How-to-ask for Answer-unaware Conversational Question Generation. (arXiv:2305.03088v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20849;&#21516;&#24314;&#27169;&#8220;&#35201;&#38382;&#20160;&#20040;&#8221;&#21644;&#8220;&#22914;&#20309;&#38382;&#8221;&#30340;&#38382;&#39064;&#65292;&#22312;&#19981;&#26126;&#30830;&#31572;&#26696;&#30340;&#35774;&#32622;&#20013;&#38750;&#24120;&#23454;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#38382;&#31572;&#29983;&#25104;&#65288;CQG&#65289;&#26159;&#26426;&#22120;&#36890;&#36807;&#23545;&#35805;&#24110;&#21161;&#20154;&#31867;&#28385;&#36275;&#20449;&#24687;&#38656;&#27714;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#36825;&#20010;&#20219;&#21153;&#19968;&#33324;&#20998;&#20026;&#20004;&#31181;&#19981;&#21516;&#30340;&#35774;&#32622;&#65306;&#26126;&#30830;&#31572;&#26696;&#21644;&#19981;&#26126;&#30830;&#31572;&#26696;&#12290;&#34429;&#28982;&#21069;&#32773;&#36890;&#36807;&#26292;&#38706;&#39044;&#26399;&#31572;&#26696;&#26469;&#24110;&#21161;&#27169;&#22411;&#65292;&#20294;&#21518;&#32773;&#26356;&#21152;&#29616;&#23454;&#24182;&#26368;&#36817;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#19981;&#26126;&#30830;&#31572;&#26696;&#30340;&#35774;&#32622;&#20013;&#65292;&#8220;&#35201;&#38382;&#20160;&#20040;&#8221;&#21644;&#8220;&#22914;&#20309;&#38382;&#8221;&#26159;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#31532;&#19968;&#20010;&#25361;&#25112;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#23558;&#19978;&#19979;&#25991;&#20013;&#30340;&#36830;&#32493;&#21477;&#23376;&#20316;&#20026;&#29702;&#30001;&#36873;&#25321;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20351;&#29992;&#36825;&#31181;&#26420;&#32032;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#29983;&#25104;&#30340;&#23545;&#35805;&#21487;&#33021;&#19981;&#22815;&#33258;&#28982;&#65292;&#22240;&#20026;&#22312;&#29616;&#23454;&#20013;&#65292;&#23545;&#35805;&#32773;&#32463;&#24120;&#35848;&#35770;&#30456;&#20851;&#30340;&#20869;&#23481;&#65292;&#36825;&#20123;&#20869;&#23481;&#19981;&#19968;&#23450;&#26159;&#36830;&#32493;&#30340;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#38544;&#21547;&#20915;&#23450;&#35201;&#29983;&#25104;&#30340;&#38382;&#39064;&#31867;&#22411;&#65288;&#24067;&#23572;/&#36328;&#24230;&#65289;&#65292;&#26174;&#24335;&#24314;&#27169;&#38382;&#39064;&#31867;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#31572;&#26696;&#65288;&#25552;&#31034;&#27169;&#22411;&#29983;&#25104;&#26576;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#65289;&#22312;&#26080;&#31572;&#26696;&#35774;&#32622;&#20013;&#19981;&#21487;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#31471;&#23545;&#31471;&#30340;&#26041;&#24335;&#20849;&#21516;&#27169;&#25311;&#8220;&#35201;&#38382;&#20160;&#20040;&#8221;&#21644;&#8220;&#22914;&#20309;&#38382;&#8221;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#30001;&#36873;&#25321;&#26041;&#27861;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#21644;&#20505;&#36873;&#21477;&#23376;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#26356;&#33258;&#28982;&#30340;&#23545;&#35805;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#38382;&#39064;&#31867;&#22411;&#39044;&#27979;&#22120;&#26469;&#26126;&#30830;&#27169;&#22411;&#20013;&#30340;&#30446;&#26631;&#38382;&#39064;&#31867;&#22411;&#12290;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational Question Generation (CQG) is a critical task for machines to assist humans in fulfilling their information needs through conversations. The task is generally cast into two different settings: answer-aware and answer-unaware. While the former facilitates the models by exposing the expected answer, the latter is more realistic and receiving growing attentions recently. What-to-ask and how-to-ask are the two main challenges in the answer-unaware setting. To address the first challenge, existing methods mainly select sequential sentences in context as the rationales. We argue that the conversation generated using such naive heuristics may not be natural enough as in reality, the interlocutors often talk about the relevant contents that are not necessarily sequential in context. Additionally, previous methods decide the type of question to be generated (boolean/span-based) implicitly. Modeling the question type explicitly is crucial as the answer, which hints the models to ge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#29992;&#20110;&#24748;&#33218;&#26753;&#25439;&#20260;&#26816;&#27979;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#23558;&#21367;&#31215;&#32593;&#32476;&#30340;&#22788;&#29702;&#33021;&#21147;&#19982;&#36923;&#36753;&#26597;&#35810;&#20132;&#20114;&#25511;&#21046;&#30456;&#32467;&#21512;&#65292;&#19981;&#20165;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#25439;&#20260;&#65292;&#32780;&#19988;&#36824;&#33021;&#22815;&#25552;&#20379;&#35299;&#37322;&#21644;&#23450;&#20301;&#65292;&#20351;&#20854;&#22312;&#25805;&#20316;&#26465;&#20214;&#19979;&#26356;&#21487;&#38752;&#21644;&#21487;&#20449;&#12290;</title><link>http://arxiv.org/abs/2305.03063</link><description>&lt;p&gt;
&#24748;&#33218;&#26753;&#25439;&#20260;&#26816;&#27979;&#30340;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Neuro-symbolic model for cantilever beams damage detection. (arXiv:2305.03063v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#29992;&#20110;&#24748;&#33218;&#26753;&#25439;&#20260;&#26816;&#27979;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#23558;&#21367;&#31215;&#32593;&#32476;&#30340;&#22788;&#29702;&#33021;&#21147;&#19982;&#36923;&#36753;&#26597;&#35810;&#20132;&#20114;&#25511;&#21046;&#30456;&#32467;&#21512;&#65292;&#19981;&#20165;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#25439;&#20260;&#65292;&#32780;&#19988;&#36824;&#33021;&#22815;&#25552;&#20379;&#35299;&#37322;&#21644;&#23450;&#20301;&#65292;&#20351;&#20854;&#22312;&#25805;&#20316;&#26465;&#20214;&#19979;&#26356;&#21487;&#38752;&#21644;&#21487;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#25439;&#20260;&#26816;&#27979;&#26041;&#27861;&#36805;&#36895;&#20174;&#20808;&#36827;&#30340;&#20449;&#21495;&#22788;&#29702;&#26041;&#27861;&#36716;&#21464;&#20026;&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#20934;&#30830;&#22320;&#12289;&#38750;&#20405;&#20837;&#24615;&#22320;&#20272;&#35745;&#26753;&#32467;&#26500;&#29366;&#24577;&#12290;&#20294;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36798;&#21040;&#24005;&#23792;&#34920;&#29616;&#65292;&#20154;&#20204;&#20063;&#35266;&#23519;&#21040;&#20102;&#23427;&#20204;&#36866;&#29992;&#24615;&#30340;&#38480;&#21046;&#21644;&#26131;&#21463;&#25915;&#20987;&#30340;&#24369;&#28857;&#12290;&#20854;&#20013;&#26368;&#37325;&#35201;&#30340;&#19968;&#20010;&#21407;&#22240;&#26159;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#30340;&#32570;&#22833;&#65292;&#30001;&#20110;&#30693;&#35782;&#32534;&#30721;&#22312;&#24352;&#37327;&#20540;&#20013;&#32780;&#27809;&#26377;&#21253;&#21547;&#36923;&#36753;&#32422;&#26463;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#65292;&#22522;&#20110;&#26032;&#39062;&#30340;&#35748;&#30693;&#26550;&#26500;&#65292;&#23558;&#21367;&#31215;&#32593;&#32476;&#30340;&#22788;&#29702;&#33021;&#21147;&#19982;&#30452;&#25509;&#23558;&#23454;&#38469;&#36923;&#36753;&#21253;&#21547;&#21040;&#27169;&#22411;&#20013;&#30340;&#26597;&#35810;&#20132;&#20114;&#25511;&#21046;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#24748;&#33218;&#26753;&#25439;&#20260;&#26816;&#27979;&#12290;&#35813;&#28151;&#21512;&#21028;&#21035;&#27169;&#22411;&#19981;&#20165;&#33021;&#22815;&#31934;&#30830;&#22320;&#26816;&#27979;&#25439;&#20260;&#65292;&#32780;&#19988;&#33021;&#22815;&#25552;&#20379;&#25439;&#20260;&#30340;&#35299;&#37322;&#21644;&#23450;&#20301;&#65292;&#20351;&#20854;&#22312;&#25805;&#20316;&#26465;&#20214;&#19979;&#26356;&#21487;&#38752;&#21644;&#21487;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last decade, damage detection approaches swiftly changed from advanced signal processing methods to machine learning and especially deep learning models, to accurately and non-intrusively estimate the state of the beam structures. But as the deep learning models reached their peak performances, also their limitations in applicability and vulnerabilities were observed. One of the most important reason for the lack of trustworthiness in operational conditions is the absence of intrinsic explainability of the deep learning system, due to the encoding of the knowledge in tensor values and without the inclusion of logical constraints. In this paper, we propose a neuro-symbolic model for the detection of damages in cantilever beams based on a novel cognitive architecture in which we join the processing power of convolutional networks with the interactive control offered by queries realized through the inclusion of real logic directly into the model. The hybrid discriminative model is 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22914;&#20309;&#22312;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#19979;&#65292;&#21033;&#29992;&#26799;&#24230;&#20449;&#24687;&#35782;&#21035;&#35757;&#32451;&#20013;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#25968;&#25454;&#36873;&#25321;&#21644;&#20272;&#20215;&#12290;</title><link>http://arxiv.org/abs/2305.02942</link><description>&lt;p&gt;
&#21033;&#29992;&#26799;&#24230;&#34913;&#37327;&#25968;&#25454;&#36873;&#25321;&#21644;&#20272;&#20215;&#22312;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Leveraging gradient-derived metrics for data selection and valuation in differentially private training. (arXiv:2305.02942v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02942
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22914;&#20309;&#22312;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#19979;&#65292;&#21033;&#29992;&#26799;&#24230;&#20449;&#24687;&#35782;&#21035;&#35757;&#32451;&#20013;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#25968;&#25454;&#36873;&#25321;&#21644;&#20272;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#30417;&#31649;&#25285;&#24551;&#21644;&#21442;&#19982;&#24230;&#30340;&#19981;&#36275;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21327;&#20316;&#35757;&#32451;&#33719;&#21462;&#39640;&#36136;&#37327;&#25968;&#25454;&#21487;&#33021;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#65288;PET&#65289;&#26159;&#35299;&#20915;&#30417;&#31649;&#38382;&#39064;&#30340;&#19968;&#31181;&#24120;&#29992;&#26041;&#27861;&#65292;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#35757;&#32451;&#26159;&#20854;&#20013;&#26368;&#24120;&#29992;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#26799;&#24230;&#20449;&#24687;&#26469;&#35782;&#21035;&#38544;&#31169;&#35757;&#32451;&#20013;&#24863;&#20852;&#36259;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#26368;&#20005;&#26684;&#30340;&#38544;&#31169;&#35774;&#32622;&#20013;&#65292;&#23384;&#22312;&#30528;&#33021;&#22815;&#20026;&#23458;&#25143;&#25552;&#20379;&#26377;&#21407;&#21017;&#30340;&#25968;&#25454;&#36873;&#25321;&#24037;&#20855;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Obtaining high-quality data for collaborative training of machine learning models can be a challenging task due to A) the regulatory concerns and B) lack of incentive to participate. The first issue can be addressed through the use of privacy enhancing technologies (PET), one of the most frequently used one being differentially private (DP) training. The second challenge can be addressed by identifying which data points can be beneficial for model training and rewarding data owners for sharing this data. However, DP in deep learning typically adversely affects atypical (often informative) data samples, making it difficult to assess the usefulness of individual contributions. In this work we investigate how to leverage gradient information to identify training samples of interest in private training settings. We show that there exist techniques which are able to provide the clients with the tools for principled data selection even in strictest privacy settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#23618;&#21487;&#25193;&#23637;&#22270;Transformer (HSGT)&#29992;&#20110;&#35299;&#20915;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#35268;&#27169;&#38382;&#39064;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#25429;&#33719;&#19981;&#36275;&#38382;&#39064;&#65292;&#36890;&#36807;&#26500;&#24314;&#22810;&#23610;&#24230;&#22270;&#20998;&#23618;&#32467;&#26500;&#65292;HSGT&#23454;&#29616;&#20102;&#23545;&#22823;&#22411;&#22270;&#30340;&#24555;&#36895;&#21644;&#20869;&#23384;&#39640;&#25928;&#22788;&#29702;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.02866</link><description>&lt;p&gt;
&#20998;&#23618;Transformer&#29992;&#20110;&#21487;&#25193;&#23637;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Transformer for Scalable Graph Learning. (arXiv:2305.02866v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#23618;&#21487;&#25193;&#23637;&#22270;Transformer (HSGT)&#29992;&#20110;&#35299;&#20915;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#35268;&#27169;&#38382;&#39064;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#25429;&#33719;&#19981;&#36275;&#38382;&#39064;&#65292;&#36890;&#36807;&#26500;&#24314;&#22810;&#23610;&#24230;&#22270;&#20998;&#23618;&#32467;&#26500;&#65292;HSGT&#23454;&#29616;&#20102;&#23545;&#22823;&#22411;&#22270;&#30340;&#24555;&#36895;&#21644;&#20869;&#23384;&#39640;&#25928;&#22788;&#29702;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;Transformer&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#24182;&#22312;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24403;&#21069;&#23454;&#29616;&#30340;&#22270;Transformer&#20027;&#35201;&#38598;&#20013;&#22312;&#23398;&#20064;&#23567;&#35268;&#27169;&#22270;&#30340;&#34920;&#31034;&#19978;&#65292;&#20840;&#23616;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#23545;&#20110;&#24212;&#29992;&#20110;&#36739;&#22823;&#35268;&#27169;&#22270;&#30340;&#20840;&#25209;&#37327;&#35757;&#32451;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#24517;&#35201;&#30340;&#39640;&#23618;&#27425;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#23548;&#33268;&#24615;&#33021;&#20005;&#37325;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#23618;&#21487;&#25193;&#23637;&#22270;Transformer (HSGT)&#20316;&#20026;&#36825;&#20123;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;HSGT&#25104;&#21151;&#22320;&#23558;Transformer&#26550;&#26500;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#22270;&#19978;&#30340;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#21033;&#29992;&#36890;&#36807;&#31895;&#21270;&#25216;&#26415;&#26500;&#24314;&#30340;&#22270;&#20998;&#23618;&#32467;&#26500;&#65292;HSGT&#26377;&#25928;&#22320;&#26356;&#26032;&#21644;&#23384;&#20648;&#22810;&#23610;&#24230;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#22823;&#22411;&#22270;&#30340;&#24555;&#36895;&#21644;&#20869;&#23384;&#39640;&#25928;&#22788;&#29702;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;HSGT&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Transformer is gaining increasing attention in the field of machine learning and has demonstrated state-of-the-art performance on benchmarks for graph representation learning. However, as current implementations of Graph Transformer primarily focus on learning representations of small-scale graphs, the quadratic complexity of the global self-attention mechanism presents a challenge for full-batch training when applied to larger graphs. Additionally, conventional sampling-based methods fail to capture necessary high-level contextual information, resulting in a significant loss of performance. In this paper, we introduce the Hierarchical Scalable Graph Transformer (HSGT) as a solution to these challenges. HSGT successfully scales the Transformer architecture to node representation learning tasks on large-scale graphs, while maintaining high performance. By utilizing graph hierarchies constructed through coarsening techniques, HSGT efficiently updates and stores multi-scale informat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ansible Wisdom&#30340;&#33258;&#28982;&#35821;&#35328;&#36716;Ansible-YAML&#20195;&#30721;&#30340;&#24037;&#20855;&#65292;&#21487;&#33258;&#21160;&#21270;&#29983;&#25104;Ansible&#33050;&#26412;&#65292;&#25552;&#39640;IT&#33258;&#21160;&#21270;&#29983;&#20135;&#21147;&#65292;&#24182;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#36798;&#21040;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.02783</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#25216;&#26415;&#20219;&#21153;&#20013;&#33258;&#21160;&#29983;&#25104;YAML&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;
Automated Code generation for Information Technology Tasks in YAML through Large Language Models. (arXiv:2305.02783v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02783
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ansible Wisdom&#30340;&#33258;&#28982;&#35821;&#35328;&#36716;Ansible-YAML&#20195;&#30721;&#30340;&#24037;&#20855;&#65292;&#21487;&#33258;&#21160;&#21270;&#29983;&#25104;Ansible&#33050;&#26412;&#65292;&#25552;&#39640;IT&#33258;&#21160;&#21270;&#29983;&#20135;&#21147;&#65292;&#24182;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#36798;&#21040;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#19981;&#26029;&#25552;&#21319;&#65292;&#22312;&#36890;&#29992;&#32534;&#31243;&#35821;&#35328;&#26041;&#38754;&#30340;&#21463;&#30410;&#26368;&#22823;&#65292;&#32780;&#38024;&#23545;IT&#33258;&#21160;&#21270;&#31561;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#30340;&#30740;&#31350;&#36739;&#23569;&#12290;&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;Ansible-YAML&#30340;&#29983;&#25104;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ansible Wisdom&#30340;&#33258;&#28982;&#35821;&#35328;&#36716;Ansible-YAML&#20195;&#30721;&#30340;&#24037;&#20855;&#65292;&#26088;&#22312;&#25552;&#39640;IT&#33258;&#21160;&#21270;&#29983;&#20135;&#21147;&#12290;&#30740;&#31350;&#37319;&#29992;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;&#21253;&#21547;Ansible-YAML&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#25193;&#23637;&#35757;&#32451;&#12290;&#21516;&#26102;&#65292;&#36824;&#24320;&#21457;&#20102;&#20004;&#20010;&#29992;&#20110;&#25429;&#25417;&#27492;&#39046;&#22495;&#29305;&#24449;&#30340;YAML&#21644;Ansible&#24615;&#33021;&#25351;&#26631;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;Ansible Wisdom&#21487;&#20197;&#31934;&#30830;&#22320;&#20174;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#20013;&#29983;&#25104;Ansible&#33050;&#26412;&#65292;&#24182;&#19988;&#20854;&#24615;&#33021;&#21487;&#19982;&#29616;&#26377;&#25216;&#26415;&#30340;&#29366;&#24577;&#30456;&#23218;&#32654;&#25110;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent improvement in code generation capabilities due to the use of large language models has mainly benefited general purpose programming languages. Domain specific languages, such as the ones used for IT Automation, have received far less attention, despite involving many active developers and being an essential component of modern cloud platforms. This work focuses on the generation of Ansible-YAML, a widely used markup language for IT Automation. We present Ansible Wisdom, a natural-language to Ansible-YAML code generation tool, aimed at improving IT automation productivity. Ansible Wisdom is a transformer-based model, extended by training with a new dataset containing Ansible-YAML. We also develop two novel performance metrics for YAML and Ansible to capture the specific characteristics of this domain. Results show that Ansible Wisdom can accurately generate Ansible script from natural language prompts with performance comparable or better than existing state of the art code 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25552;&#39640;&#25506;&#27979;&#34892;&#26143;&#20449;&#21495;&#20998;&#31867;&#22120;&#24615;&#33021;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#22810;&#37325;&#24615;&#22686;&#24378;&#20998;&#31867;&#22120;&#65292;&#22522;&#20110;&#29616;&#26377;&#30340;&#20998;&#31867;&#22120;&#24182;&#20351;&#29992;&#22810;&#37325;&#24615;&#20449;&#24687;&#26469;&#39564;&#35777;69&#20010;&#26032;&#30340;&#31995;&#22806;&#34892;&#26143;&#12290;</title><link>http://arxiv.org/abs/2305.02470</link><description>&lt;p&gt;
&#25506;&#27979;&#34892;&#26143;&#20449;&#21495;&#30340;&#22810;&#37325;&#24615;&#22686;&#24378;&#20998;&#31867;&#22120;&#65306;&#20351;&#29992;ExoMiner&#30340;&#22810;&#37325;&#24615;&#22686;&#24378;&#39564;&#35777;69&#20010;&#26032;&#34892;&#26143;
&lt;/p&gt;
&lt;p&gt;
Multiplicity Boost Of Transit Signal Classifiers: Validation of 69 New Exoplanets Using The Multiplicity Boost of ExoMiner. (arXiv:2305.02470v1 [astro-ph.EP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02470
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25552;&#39640;&#25506;&#27979;&#34892;&#26143;&#20449;&#21495;&#20998;&#31867;&#22120;&#24615;&#33021;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#22810;&#37325;&#24615;&#22686;&#24378;&#20998;&#31867;&#22120;&#65292;&#22522;&#20110;&#29616;&#26377;&#30340;&#20998;&#31867;&#22120;&#24182;&#20351;&#29992;&#22810;&#37325;&#24615;&#20449;&#24687;&#26469;&#39564;&#35777;69&#20010;&#26032;&#30340;&#31995;&#22806;&#34892;&#26143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#24050;&#30693;&#30340;&#31995;&#22806;&#34892;&#26143;&#26159;&#36890;&#36807;&#39564;&#35777;&#25216;&#26415;&#32780;&#19981;&#26159;&#36890;&#36807;&#34917;&#20805;&#35266;&#27979;&#36827;&#34892;&#30830;&#35748;&#30340;&#12290;&#36825;&#20123;&#25216;&#26415;&#29983;&#25104;&#30340;&#20998;&#25968;&#36890;&#24120;&#20195;&#34920;&#20102;&#26377;&#20851;&#20449;&#21495;&#30340;&#26576;&#20123;&#20449;&#24687;&#65288;&#29992;x&#34920;&#31034;&#65289;&#32473;&#20986;&#30340;&#25506;&#27979;&#34892;&#26143;&#20449;&#21495;&#30340;&#27010;&#29575;&#65288;y&#65288;x&#65289;=&#34892;&#26143;&#65289;&#12290;&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#21363;&#22312;&#32473;&#23450;&#30340;&#25506;&#27979;&#34892;&#26143;&#20449;&#21495;&#30830;&#35748;&#22120;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#22810;&#37325;&#24615;&#20449;&#24687;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;&#27492;&#26694;&#26550;&#24212;&#29992;&#20110;&#20960;&#20010;&#29616;&#26377;&#30340;&#20998;&#31867;&#22120;&#65292;&#21253;&#25324;vespa&#65288;Morton&#31561;&#20154;2016&#65289;&#12289;Robovetter&#65288;Coughlin&#31561;&#20154;2017&#65289;&#12289;AstroNet&#65288;Shallue&#21644;Vanderburg 2018&#65289;&#12289;ExoNet&#65288;Ansdel&#31561;&#20154;2018&#65289;&#12289;GPC&#21644;RFC&#65288;Armstrong&#31561;&#20154;2020&#65289;&#20197;&#21450;ExoMiner&#65288;Valizadegan&#31561;&#20154;2022&#65289;&#65292;&#20197;&#25903;&#25345;&#25105;&#20204;&#30340;&#20998;&#31867;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing exoplanets are discovered using validation techniques rather than being confirmed by complementary observations. These techniques generate a score that is typically the probability of the transit signal being an exoplanet (y(x)=exoplanet) given some information related to that signal (represented by x). Except for the validation technique in Rowe et al. (2014) that uses multiplicity information to generate these probability scores, the existing validation techniques ignore the multiplicity boost information. In this work, we introduce a framework with the following premise: given an existing transit signal vetter (classifier), improve its performance using multiplicity information. We apply this framework to several existing classifiers, which include vespa (Morton et al. 2016), Robovetter (Coughlin et al. 2017), AstroNet (Shallue &amp; Vanderburg 2018), ExoNet (Ansdel et al. 2018), GPC and RFC (Armstrong et al. 2020), and ExoMiner (Valizadegan et al. 2022), to support our cl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26102;&#38388;&#25139;&#25968;&#25454;&#30340;LTL&#27491;&#29255;&#27573;&#21046;&#23450;&#30340;&#26597;&#35810;&#21453;&#21521;&#24037;&#31243;&#38382;&#39064;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#23558;&#32473;&#23450;&#31572;&#26696;&#19982;&#38750;&#31572;&#26696;&#20998;&#24320;&#30340;&#26597;&#35810;&#35821;&#35328;&#65292;&#24182;&#32771;&#34385;&#20102;LTL&#26412;&#20307;&#20171;&#23548;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2305.01248</link><description>&lt;p&gt;
LTL&#26412;&#20307;&#20013;&#30340;&#26102;&#38388;&#26597;&#35810;&#21453;&#21521;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Reverse Engineering of Temporal Queries Mediated by LTL Ontologies. (arXiv:2305.01248v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26102;&#38388;&#25139;&#25968;&#25454;&#30340;LTL&#27491;&#29255;&#27573;&#21046;&#23450;&#30340;&#26597;&#35810;&#21453;&#21521;&#24037;&#31243;&#38382;&#39064;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#23558;&#32473;&#23450;&#31572;&#26696;&#19982;&#38750;&#31572;&#26696;&#20998;&#24320;&#30340;&#26597;&#35810;&#35821;&#35328;&#65292;&#24182;&#32771;&#34385;&#20102;LTL&#26412;&#20307;&#20171;&#23548;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#24211;&#26597;&#35810;&#30340;&#21453;&#21521;&#24037;&#31243;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#20174;&#32473;&#23450;&#30340;&#31572;&#26696;&#21644;&#38750;&#31572;&#26696;&#38598;&#21512;&#26500;&#24314;&#19968;&#20010;&#26597;&#35810;; &#28982;&#21518;&#21487;&#20197;&#23558;&#20854;&#29992;&#20110;&#36827;&#19968;&#27493;&#25506;&#32034;&#25968;&#25454;&#25110;&#20316;&#20026;&#31572;&#26696;&#21644;&#38750;&#31572;&#26696;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#26102;&#38388;&#25139;&#25968;&#25454;&#30340;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;LTL&#30340;&#27491;&#29255;&#27573;&#21046;&#23450;&#30340;&#26597;&#35810;&#30340;&#36825;&#20010;&#26597;&#35810;-by-example&#38382;&#39064;&#65292;&#37325;&#28857;&#26159;&#35774;&#35745;&#21512;&#36866;&#30340;&#26597;&#35810;&#35821;&#35328;&#21644;&#20915;&#23450;&#26159;&#21542;&#23384;&#22312;&#19968;&#20010;&#26597;&#35810;&#22312;&#32473;&#23450;&#30340;&#35821;&#35328;&#20013;&#21487;&#20197;&#23558;&#32473;&#23450;&#30340;&#31572;&#26696;&#19982;&#38750;&#31572;&#26696;&#20998;&#24320;&#30340;&#32452;&#21512;&#21644;&#25968;&#25454;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#21516;&#26102;&#32771;&#34385;&#20102;&#26222;&#36890;&#30340;LTL&#26597;&#35810;&#21644;&#37027;&#20123;&#36890;&#36807;LTL&#26412;&#20307;&#20171;&#23548;&#30340;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reverse engineering of database queries, we aim to construct a query from a given set of answers and non-answers; it can then be used to explore the data further or as an explanation of the answers and non-answers. We investigate this query-by-example problem for queries formulated in positive fragments of linear temporal logic LTL over timestamped data, focusing on the design of suitable query languages and the combined and data complexity of deciding whether there exists a query in the given language that separates the given answers from non-answers. We consider both plain LTL queries and those mediated by LTL-ontologies.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LSTM&#30340;ACC&#31995;&#32479;&#65292;&#21487;&#20197;&#23398;&#20064;&#36807;&#21435;&#30340;&#39550;&#39542;&#32463;&#39564;&#65292;&#36866;&#24212;&#21644;&#39044;&#27979;&#26032;&#24773;&#20917;&#65292;&#24182;&#19988;&#22312;&#27169;&#25311;&#39550;&#39542;&#29615;&#22659;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.01095</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;LSTM&#30340;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#22120;&#65292;&#22312;&#36710;&#36947;&#21464;&#25442;&#26102;&#21487;&#20197;&#39044;&#27979;&#20808;&#21069;&#36710;&#36742;&#30340;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
LSTM-based Preceding Vehicle Behaviour Prediction during Aggressive Lane Change for ACC Application. (arXiv:2305.01095v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01095
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LSTM&#30340;ACC&#31995;&#32479;&#65292;&#21487;&#20197;&#23398;&#20064;&#36807;&#21435;&#30340;&#39550;&#39542;&#32463;&#39564;&#65292;&#36866;&#24212;&#21644;&#39044;&#27979;&#26032;&#24773;&#20917;&#65292;&#24182;&#19988;&#22312;&#27169;&#25311;&#39550;&#39542;&#29615;&#22659;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#65288;ACC&#65289;&#31995;&#32479;&#30340;&#21457;&#23637;&#26088;&#22312;&#36890;&#36807;&#33258;&#21160;&#35843;&#33410;&#36710;&#36895;&#26469;&#30830;&#20445;&#19982;&#21069;&#36710;&#23433;&#20840;&#36317;&#31163;&#65292;&#20174;&#32780;&#22686;&#24378;&#36710;&#36742;&#30340;&#23433;&#20840;&#24615;&#21644;&#33298;&#36866;&#24615;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;ACC&#31995;&#32479;&#26080;&#27861;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#39550;&#39542;&#26465;&#20214;&#21644;&#39550;&#39542;&#32773;&#30340;&#34892;&#20026;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LSTM&#30340;ACC&#31995;&#32479;&#65292;&#21487;&#20197;&#20174;&#20197;&#24448;&#30340;&#39550;&#39542;&#32463;&#39564;&#20013;&#23398;&#20064;&#65292;&#24182;&#23454;&#26102;&#22320;&#36866;&#24212;&#21644;&#39044;&#27979;&#26032;&#30340;&#24773;&#20917;&#12290;&#35813;&#27169;&#22411;&#26159;&#22522;&#20110;&#23454;&#38469;&#30340;&#39640;&#36895;&#20844;&#36335;&#39640;D&#25968;&#25454;&#38598;&#26500;&#24314;&#30340;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#21033;&#29992;&#35013;&#22791;&#26377;&#25668;&#20687;&#22836;&#30340;&#26080;&#20154;&#26426;&#22312;&#24503;&#22269;&#39640;&#36895;&#20844;&#36335;&#19978;&#33719;&#21462;&#30340;&#12290;&#25105;&#20204;&#22312;&#20391;&#38754;&#36710;&#36947;&#21069;&#36710;&#21098;&#20999;&#24182;&#24378;&#21046;&#30446;&#26631;&#39550;&#39542;&#21592;&#20943;&#36895;&#30340;&#28608;&#36827;&#36710;&#36947;&#21464;&#21270;&#26102;&#35780;&#20272;&#20102;ACC&#31995;&#32479;&#12290;&#20026;&#27492;&#65292;&#23558;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#22312;&#27169;&#25311;&#39550;&#39542;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#19982;&#39304;&#36865;&#21069;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#27169;&#22411;&#21644;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of Adaptive Cruise Control (ACC) systems aims to enhance the safety and comfort of vehicles by automatically regulating the speed of the vehicle to ensure a safe gap from the preceding vehicle. However, conventional ACC systems are unable to adapt themselves to changing driving conditions and drivers' behavior. To address this limitation, we propose a Long Short-Term Memory (LSTM) based ACC system that can learn from past driving experiences and adapt and predict new situations in real time. The model is constructed based on the real-world highD dataset, acquired from German highways with the assistance of camera-equipped drones. We evaluated the ACC system under aggressive lane changes when the side lane preceding vehicle cut off, forcing the targeted driver to reduce speed. To this end, the proposed system was assessed on a simulated driving environment and compared with a feedforward Artificial Neural Network (ANN) model and Model Predictive Control (MPC) model. The 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;Pascal VOC&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35821;&#20041;&#20998;&#21106;&#30340;&#30740;&#31350;&#65292;&#20351;&#29992;&#20102;FCN&#20316;&#20026;&#22522;&#20934;&#24182;&#25913;&#36827;&#20102;&#20854;&#38382;&#39064;&#65292;&#26368;&#32456;&#21457;&#29616;&#20351;&#29992;ResNet&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#30340;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2304.13216</link><description>&lt;p&gt;
&#21033;&#29992;CNN&#36827;&#34892;Pascal VOC&#30340;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Exploiting CNNs for Semantic Segmentation with Pascal VOC. (arXiv:2304.13216v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;Pascal VOC&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35821;&#20041;&#20998;&#21106;&#30340;&#30740;&#31350;&#65292;&#20351;&#29992;&#20102;FCN&#20316;&#20026;&#22522;&#20934;&#24182;&#25913;&#36827;&#20102;&#20854;&#38382;&#39064;&#65292;&#26368;&#32456;&#21457;&#29616;&#20351;&#29992;ResNet&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#30340;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;Pascal VOC&#25968;&#25454;&#38598;&#19978;&#23545;&#35821;&#20041;&#20998;&#21106;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#23436;&#20840;&#21367;&#31215;&#32593;&#32476;&#65288;FCN&#65289;&#20316;&#20026;&#22522;&#20934;&#65292;&#24182;&#38024;&#23545;&#20854;&#38382;&#39064;&#36827;&#34892;&#25913;&#36827;&#65292;&#21253;&#25324;&#20313;&#24358;&#36864;&#28779;&#23398;&#20064;&#29575;&#35843;&#24230;&#22120;&#12289;&#25968;&#25454;&#22686;&#24378;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#26435;&#37325;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26550;&#26500;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;ResNet&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#30340;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a comprehensive study on semantic segmentation with the Pascal VOC dataset. Here, we have to label each pixel with a class which in turn segments the entire image based on the objects/entities present. To tackle this, we firstly use a Fully Convolution Network (FCN) baseline which gave 71.31% pixel accuracy and 0.0527 mean IoU. We analyze its performance and working and subsequently address the issues in the baseline with three improvements: a) cosine annealing learning rate scheduler(pixel accuracy: 72.86%, IoU: 0.0529), b) data augmentation(pixel accuracy: 69.88%, IoU: 0.0585) c) class imbalance weights(pixel accuracy: 68.98%, IoU: 0.0596). Apart from these changes in training pipeline, we also explore three different architectures: a) Our proposed model -- Advanced FCN (pixel accuracy: 67.20%, IoU: 0.0602) b) Transfer Learning with ResNet (Best performance) (pixel accuracy: 71.33%, IoU: 0.0926 ) c) U-Net(pixel accuracy: 72.15%, IoU: 0.0649). We observe that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#22312;&#25104;&#23545;&#22270;&#20687;&#36716;&#25442;&#25216;&#26415;&#19978;&#30340;&#36827;&#23637;&#30340;&#65292;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;LArTPC&#25506;&#27979;&#22120;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#20043;&#38388;&#30340;&#31995;&#32479;&#24046;&#24322;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.12858</link><description>&lt;p&gt;
&#38754;&#21521;&#31185;&#23398;&#30340;&#26080;&#30417;&#30563;&#22495;&#36716;&#31227;&#65306;&#25506;&#32034;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#20855;&#26377;&#19981;&#21516;&#21709;&#24212;&#27169;&#22411;&#30340;LArTPC&#25506;&#27979;&#22120;&#27169;&#25311;&#20043;&#38388;&#30340;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Domain Transfer for Science: Exploring Deep Learning Methods for Translation between LArTPC Detector Simulations with Differing Response Models. (arXiv:2304.12858v1 [hep-ex])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#22312;&#25104;&#23545;&#22270;&#20687;&#36716;&#25442;&#25216;&#26415;&#19978;&#30340;&#36827;&#23637;&#30340;&#65292;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;LArTPC&#25506;&#27979;&#22120;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#20043;&#38388;&#30340;&#31995;&#32479;&#24046;&#24322;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#31185;&#23398;&#20013;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#23547;&#27714;&#31616;&#21270;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#21644;&#21457;&#29616;&#30340;&#36335;&#24452;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;DL&#27169;&#22411;&#32463;&#24120;&#22312;&#27169;&#25311;&#32467;&#26524;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#30495;&#23454;&#23454;&#39564;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#20043;&#38388;&#30340;&#20219;&#20309;&#31995;&#32479;&#24046;&#24322;&#21487;&#33021;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#36825;&#31181;&#25928;&#24212;&#31216;&#20026;&#8220;&#39046;&#22495;&#28418;&#31227;&#8221;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#31995;&#32479;&#24046;&#24322;&#30340;&#29609;&#20855;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#26041;&#27861;&#26469;&#20943;&#23569;&#20004;&#20010;&#31995;&#32479;&#19981;&#21516;&#30340;&#26679;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#26368;&#36817;&#22312;&#25104;&#23545;&#22270;&#20687;&#36716;&#25442;&#25216;&#26415;&#19978;&#30340;&#36827;&#23637;&#65292;&#24182;&#22312;&#20004;&#32452;&#27169;&#25311;&#28082;&#27689;&#26102;&#38388;&#25237;&#24433;&#23460;&#65288;LArTPC&#65289;&#25506;&#27979;&#22120;&#20107;&#20214;&#26679;&#26412;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#36825;&#20123;&#26679;&#26412;&#34987;&#21019;&#24314;&#20197;&#25511;&#21046;&#22320;&#28436;&#31034;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#20043;&#38388;&#30340;&#24120;&#35265;&#31995;&#32479;&#24046;&#24322;&#12290;LArTPC&#25506;&#27979;&#22120;&#20195;&#34920;&#20102;&#19979;&#19968;&#20195;&#31890;&#23376;&#25506;&#27979;&#22120;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) techniques have broad applications in science, especially in seeking to streamline the pathway to potential solutions and discoveries. Frequently, however, DL models are trained on the results of simulation yet applied to real experimental data. As such, any systematic differences between the simulated and real data may degrade the model's performance -- an effect known as "domain shift." This work studies a toy model of the systematic differences between simulated and real data. It presents a fully unsupervised, task-agnostic method to reduce differences between two systematically different samples. The method is based on the recent advances in unpaired image-to-image translation techniques and is validated on two sets of samples of simulated Liquid Argon Time Projection Chamber (LArTPC) detector events, created to illustrate common systematic differences between the simulated and real data in a controlled way. LArTPC-based detectors represent the next-generation pa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;LLM+P&#26694;&#26550;&#65292;&#23558;&#32463;&#20856;&#35268;&#21010;&#22120;&#30340;&#20248;&#28857;&#34701;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#36890;&#36807;&#23558;&#35821;&#35328;&#25551;&#36848;&#36716;&#25442;&#20026;&#29992;&#35268;&#21010;&#39046;&#22495;&#23450;&#20041;&#35821;&#35328;&#65288;PDDL&#65289;&#32534;&#20889;&#30340;&#25991;&#20214;&#26469;&#35299;&#20915;&#35745;&#21010;&#38382;&#39064;&#65292;&#26088;&#22312;&#32508;&#21512;&#20004;&#32773;&#30340;&#20248;&#21183;&#65292;&#36171;&#33021;LLMs&#26368;&#20248;&#35268;&#21010;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.11477</link><description>&lt;p&gt;
LLM+P&#65306;&#36171;&#33021;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26368;&#20248;&#35268;&#21010;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
LLM+P: Empowering Large Language Models with Optimal Planning Proficiency. (arXiv:2304.11477v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;LLM+P&#26694;&#26550;&#65292;&#23558;&#32463;&#20856;&#35268;&#21010;&#22120;&#30340;&#20248;&#28857;&#34701;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#36890;&#36807;&#23558;&#35821;&#35328;&#25551;&#36848;&#36716;&#25442;&#20026;&#29992;&#35268;&#21010;&#39046;&#22495;&#23450;&#20041;&#35821;&#35328;&#65288;PDDL&#65289;&#32534;&#20889;&#30340;&#25991;&#20214;&#26469;&#35299;&#20915;&#35745;&#21010;&#38382;&#39064;&#65292;&#26088;&#22312;&#32508;&#21512;&#20004;&#32773;&#30340;&#20248;&#21183;&#65292;&#36171;&#33021;LLMs&#26368;&#20248;&#35268;&#21010;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#65306;&#26368;&#20808;&#36827;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#21487;&#20197;&#25552;&#20379;&#23545;&#35768;&#22810;&#26085;&#24120;&#29983;&#27963;&#20013;&#24120;&#35265;&#38382;&#39064;&#30340;&#21512;&#29702;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;LLM&#19981;&#33021;&#21487;&#38752;&#22320;&#35299;&#20915;&#38271;&#26102;&#31243;&#35268;&#21010;&#38382;&#39064;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#32463;&#20856;&#35268;&#21010;&#22120;&#19968;&#26086;&#20197;&#26684;&#24335;&#21270;&#30340;&#26041;&#24335;&#25552;&#20379;&#38382;&#39064;&#65292;&#21017;&#21487;&#20197;&#20351;&#29992;&#39640;&#25928;&#30340;&#25628;&#32034;&#31639;&#27861;&#24555;&#36895;&#35782;&#21035;&#27491;&#30830;&#25110;&#29978;&#33267;&#26368;&#20248;&#30340;&#35745;&#21010;&#12290;&#20026;&#20102;&#32508;&#21512;&#20004;&#32773;&#30340;&#20248;&#21183;&#65292;&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;LLM+P&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;&#32463;&#20856;&#35268;&#21010;&#22120;&#30340;&#20248;&#28857;&#34701;&#20837;LLM&#30340;&#26694;&#26550;&#12290;LLM+P&#25509;&#25910;&#19968;&#20010;&#35745;&#21010;&#38382;&#39064;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#28982;&#21518;&#20197;&#33258;&#28982;&#35821;&#35328;&#36820;&#22238;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#27491;&#30830;&#65288;&#25110;&#26368;&#20248;&#65289;&#35745;&#21010;&#12290;LLM+P&#39318;&#20808;&#23558;&#35821;&#35328;&#25551;&#36848;&#36716;&#25442;&#20026;&#29992;&#35268;&#21010;&#39046;&#22495;&#23450;&#20041;&#35821;&#35328;&#65288;PDDL&#65289;&#32534;&#20889;&#30340;&#25991;&#20214;&#65292;&#28982;&#21518;&#21033;&#29992;&#32463;&#20856;&#35268;&#21010;&#22120;&#24555;&#36895;&#25214;&#21040;&#35299;&#20915;&#26041;&#26696;&#65292;&#26368;&#21518;&#23558;&#25214;&#21040;&#30340;&#35299;&#20915;&#26041;&#26696;&#32763;&#35793;&#22238;&#33258;&#28982;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable zero-shot generalization abilities: state-of-the-art chatbots can provide plausible answers to many common questions that arise in daily life. However, so far, LLMs cannot reliably solve long-horizon planning problems. By contrast, classical planners, once a problem is given in a formatted way, can use efficient search algorithms to quickly identify correct, or even optimal, plans. In an effort to get the best of both worlds, this paper introduces LLM+P, the first framework that incorporates the strengths of classical planners into LLMs. LLM+P takes in a natural language description of a planning problem, then returns a correct (or optimal) plan for solving that problem in natural language. LLM+P does so by first converting the language description into a file written in the planning domain definition language (PDDL), then leveraging classical planners to quickly find a solution, and then translating the found solution back into
&lt;/p&gt;</description></item><item><title>&#21069;&#20154;&#30340;&#20154;&#33080;&#38450;&#20266;&#65288;FAS&#65289;&#25216;&#26415;&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#20173;&#28982;&#23384;&#22312;&#38480;&#21046;&#65292;&#22240;&#20026;&#24403;&#21069;&#20844;&#24320;&#30340;FAS&#25968;&#25454;&#38598;&#25968;&#37327;&#21644;&#22810;&#26679;&#24615;&#19981;&#36275;&#65292;&#24341;&#36215;&#36807;&#25311;&#21512;&#21644;&#22330;&#26223;&#35823;&#21028;&#12290;&#36890;&#36807;&#24341;&#20837;&#37326;&#22806;&#20154;&#33080;&#38450;&#20266;&#65288;WFAS&#65289;&#25968;&#25454;&#38598;&#65292;&#35813;&#30740;&#31350;&#25552;&#39640;&#20102;&#25968;&#25454;&#35268;&#27169;&#21644;&#22810;&#26679;&#21270;&#31243;&#24230;&#65292;&#20419;&#36827;&#20102;FAS&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2304.05753</link><description>&lt;p&gt;
&#37326;&#22806;&#20154;&#33080;&#38450;&#20266;&#25361;&#25112;&#36187;2023&#65306;&#22522;&#20934;&#21644;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Wild Face Anti-Spoofing Challenge 2023: Benchmark and Results. (arXiv:2304.05753v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05753
&lt;/p&gt;
&lt;p&gt;
&#21069;&#20154;&#30340;&#20154;&#33080;&#38450;&#20266;&#65288;FAS&#65289;&#25216;&#26415;&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#20173;&#28982;&#23384;&#22312;&#38480;&#21046;&#65292;&#22240;&#20026;&#24403;&#21069;&#20844;&#24320;&#30340;FAS&#25968;&#25454;&#38598;&#25968;&#37327;&#21644;&#22810;&#26679;&#24615;&#19981;&#36275;&#65292;&#24341;&#36215;&#36807;&#25311;&#21512;&#21644;&#22330;&#26223;&#35823;&#21028;&#12290;&#36890;&#36807;&#24341;&#20837;&#37326;&#22806;&#20154;&#33080;&#38450;&#20266;&#65288;WFAS&#65289;&#25968;&#25454;&#38598;&#65292;&#35813;&#30740;&#31350;&#25552;&#39640;&#20102;&#25968;&#25454;&#35268;&#27169;&#21644;&#22810;&#26679;&#21270;&#31243;&#24230;&#65292;&#20419;&#36827;&#20102;FAS&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#38450;&#20266;&#65288;FAS&#65289;&#26159;&#20445;&#25252;&#33258;&#21160;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#23436;&#25972;&#24615;&#30340;&#37325;&#35201;&#26426;&#21046;&#12290;&#23613;&#31649;&#26377;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#23558;&#29616;&#26377;&#26041;&#27861;&#25512;&#24191;&#21040;&#23454;&#38469;&#24212;&#29992;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36825;&#31181;&#38480;&#21046;&#21487;&#20197;&#24402;&#22240;&#20110;&#20844;&#24320;&#21487;&#29992;&#30340;FAS&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#21644;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#36825;&#32463;&#24120;&#23548;&#33268;&#35757;&#32451;&#26399;&#38388;&#36807;&#25311;&#21512;&#25110;&#27979;&#35797;&#26399;&#38388;&#39281;&#21644;&#12290;&#23601;&#25968;&#37327;&#32780;&#35328;&#65292;&#27450;&#35784;&#20027;&#20307;&#30340;&#25968;&#37327;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20915;&#23450;&#22240;&#32032;&#12290;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#20165;&#21253;&#25324;&#23569;&#20110;2,000&#20010;&#21463;&#35797;&#32773;&#12290;&#23601;&#22810;&#26679;&#24615;&#32780;&#35328;&#65292;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#30001;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#20351;&#29992;&#37325;&#22797;&#26426;&#26800;&#21270;&#36807;&#31243;&#25910;&#38598;&#30340;&#27450;&#35784;&#26679;&#26412;&#32452;&#25104;&#12290;&#36825;&#31181;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#23548;&#33268;&#21516;&#36136;&#21270;&#26679;&#26412;&#21644;&#22330;&#26223;&#22810;&#26679;&#24615;&#30340;&#21294;&#20047;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#37326;&#22806;&#20154;&#33080;&#38450;&#20266;&#65288;WFAS&#65289;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;FAS&#25968;&#25454;&#38598;&#65292;&#21487;&#22312;&#19981;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#25910;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face anti-spoofing (FAS) is an essential mechanism for safeguarding the integrity of automated face recognition systems. Despite substantial advancements, the generalization of existing approaches to real-world applications remains challenging. This limitation can be attributed to the scarcity and lack of diversity in publicly available FAS datasets, which often leads to overfitting during training or saturation during testing. In terms of quantity, the number of spoof subjects is a critical determinant. Most datasets comprise fewer than 2,000 subjects. With regard to diversity, the majority of datasets consist of spoof samples collected in controlled environments using repetitive, mechanical processes. This data collection methodology results in homogenized samples and a dearth of scenario diversity. To address these shortcomings, we introduce the Wild Face Anti-Spoofing (WFAS) dataset, a large-scale, diverse FAS dataset collected in unconstrained settings. Our dataset encompasses 853
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22810;&#20010;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;ChatGPT&#21644;GPT-4&#22312;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#26500;&#36896;&#20102;&#19968;&#20010;&#36923;&#36753;&#25512;&#29702;&#30340;&#20998;&#24067;&#20043;&#22806;&#30340;&#25968;&#25454;&#38598;&#26469;&#30740;&#31350;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22312;&#22823;&#22810;&#25968;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#36828;&#20248;&#20110;RoBERTa&#24494;&#35843;&#26041;&#27861;&#65292;&#32780;GPT-4&#30340;&#34920;&#29616;&#21017;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2304.03439</link><description>&lt;p&gt;
&#35780;&#20272;ChatGPT&#21644;GPT-4&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4. (arXiv:2304.03439v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22810;&#20010;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;ChatGPT&#21644;GPT-4&#22312;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#26500;&#36896;&#20102;&#19968;&#20010;&#36923;&#36753;&#25512;&#29702;&#30340;&#20998;&#24067;&#20043;&#22806;&#30340;&#25968;&#25454;&#38598;&#26469;&#30740;&#31350;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22312;&#22823;&#22810;&#25968;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#36828;&#20248;&#20110;RoBERTa&#24494;&#35843;&#26041;&#27861;&#65292;&#32780;GPT-4&#30340;&#34920;&#29616;&#21017;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#12290;&#38543;&#30528;&#20808;&#36827;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;4&#65288;GPT-4&#65289;&#30340;&#21457;&#24067;&#65292;&#25105;&#20204;&#28212;&#26395;&#20102;&#35299;GPT-4&#22312;&#21508;&#31181;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#22810;&#20010;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;LogiQA&#21644;ReClor&#31561;&#24120;&#29992;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#21450;&#20687;AR-LSAT&#36825;&#26679;&#30340;&#26032;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23545;&#38656;&#35201;&#36923;&#36753;&#25512;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;&#36827;&#34892;&#20102;&#22810;&#39033;&#36873;&#25321;&#38405;&#35835;&#29702;&#35299;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#27979;&#35797;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#26500;&#36896;&#20102;&#19968;&#20010;&#36923;&#36753;&#25512;&#29702;&#30340;&#20998;&#24067;&#20043;&#22806;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#30740;&#31350;ChatGPT&#21644;GPT-4&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;ChatGPT&#21644;GPT-4&#20043;&#38388;&#30340;&#24615;&#33021;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;ChatGPT&#30340;&#34920;&#29616;&#36828;&#36828;&#20248;&#20110;RoBERTa&#24494;&#35843;&#26041;&#27861;&#12290;GPT-4&#22312;&#25105;&#20204;&#30340;&#25163;&#21160;&#27979;&#35797;&#20013;&#34920;&#29616;&#26356;&#39640;&#12290;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;ChatGPT&#21644;GPT-4&#30340;&#34920;&#29616;&#30456;&#23545;&#36739;&#20026;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Harnessing logical reasoning ability is a comprehensive natural language understanding endeavor. With the release of Generative Pretrained Transformer 4 (GPT-4), highlighted as "advanced" at reasoning tasks, we are eager to learn the GPT-4 performance on various logical reasoning tasks. This report analyses multiple logical reasoning datasets, with popular benchmarks like LogiQA and ReClor, and newly-released datasets like AR-LSAT. We test the multi-choice reading comprehension and natural language inference tasks with benchmarks requiring logical reasoning. We further construct a logical reasoning out-of-distribution dataset to investigate the robustness of ChatGPT and GPT-4. We also make a performance comparison between ChatGPT and GPT-4. Experiment results show that ChatGPT performs significantly better than the RoBERTa fine-tuning method on most logical reasoning benchmarks. GPT-4 shows even higher performance on our manual tests. Among benchmarks, ChatGPT and GPT-4 do relatively w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#20813;&#30123;&#27835;&#30103;&#39044;&#21518;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#30740;&#31350;&#20102;&#23567;&#26679;&#26412;&#23398;&#20064;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#23545;&#27604;&#20102;&#22522;&#32447;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#20934;&#30830;&#24230;&#26041;&#38754;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#31361;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#20020;&#24202;&#30740;&#31350;&#20013;&#25913;&#21892;&#26089;&#26399;&#26816;&#27979;&#21644;&#24178;&#39044;&#19981;&#21516;&#30142;&#30149;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.12692</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#21518;&#39044;&#27979;&#20013;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Language Models are Few-shot Learners for Prognostic Prediction. (arXiv:2302.12692v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#20813;&#30123;&#27835;&#30103;&#39044;&#21518;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#30740;&#31350;&#20102;&#23567;&#26679;&#26412;&#23398;&#20064;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#23545;&#27604;&#20102;&#22522;&#32447;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#20934;&#30830;&#24230;&#26041;&#38754;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#31361;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#20020;&#24202;&#30740;&#31350;&#20013;&#25913;&#21892;&#26089;&#26399;&#26816;&#27979;&#21644;&#24178;&#39044;&#19981;&#21516;&#30142;&#30149;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#39044;&#27979;&#26159;&#21307;&#30103;&#20445;&#20581;&#34892;&#19994;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#24314;&#31435;&#22312;&#29702;&#35770;&#26694;&#26550;Transformers&#20043;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#21151;&#24182;&#26410;&#24310;&#20280;&#21040;&#36825;&#20010;&#39046;&#22495;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#30495;&#23454;&#19990;&#30028;&#20013;&#24739;&#32773;&#30340;&#20020;&#24202;&#25968;&#25454;&#21644;&#20998;&#23376;&#29305;&#24449;&#25506;&#32034;&#20102;Transformers&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#20813;&#30123;&#27835;&#30103;&#39044;&#21518;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;Transformers&#30456;&#23545;&#20110;&#24120;&#35268;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#20020;&#24202;&#39044;&#27979;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#21457;&#29616;&#20102;&#22312;&#39044;&#27979;&#32597;&#35265;&#30142;&#30149;&#39046;&#22495;&#19979;&#23567;&#26679;&#26412;&#23398;&#20064;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#35813;&#30740;&#31350;&#23545;&#27604;&#20102;&#22522;&#32447;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#31181;&#30284;&#30151;&#31867;&#22411;&#30340;&#39044;&#21518;&#39044;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#23567;&#26679;&#26412;&#24773;&#20917;&#19979;&#19981;&#21516;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#22312;&#20934;&#30830;&#24230;&#26041;&#38754;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#31361;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#20020;&#24202;&#30740;&#31350;&#20013;&#25913;&#21892;&#26089;&#26399;&#26816;&#27979;&#21644;&#24178;&#39044;&#19981;&#21516;&#30142;&#30149;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical prediction is an essential task in the healthcare industry. However, the recent success of transformers, on which large language models are built, has not been extended to this domain. In this research, we explore the use of transformers and language models in prognostic prediction for immunotherapy using real-world patients' clinical data and molecular profiles. This paper investigates the potential of transformers to improve clinical prediction compared to conventional machine learning approaches and addresses the challenge of few-shot learning in predicting rare disease areas. The study benchmarks the efficacy of baselines and language models on prognostic prediction across multiple cancer types and investigates the impact of different pretrained language models under few-shot regimes. The results demonstrate significant improvements in accuracy and highlight the potential of NLP in clinical research to improve early detection and intervention for different diseases.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#21521;&#37327;&#65306;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#22312;&#21487;&#33021;&#26816;&#32034;&#21040;&#30340;&#25968;&#25454;&#20013;&#31574;&#30053;&#24615;&#22320;&#27880;&#20837;&#25552;&#31034;&#26469;&#36828;&#31243;&#21033;&#29992; LLM &#38598;&#25104;&#24212;&#29992;&#31243;&#24207;&#12290;&#35813;&#25915;&#20987;&#23545;&#25968;&#25454;&#30423;&#31363;&#12289;&#34837;&#34411;&#12289;&#20449;&#24687;&#29983;&#24577;&#31995;&#32479;&#27745;&#26579;&#31561;&#36896;&#25104;&#23041;&#32961;&#12290;</title><link>http://arxiv.org/abs/2302.12173</link><description>&lt;p&gt;
&#19981;&#26159;&#20320;&#25152;&#31614;&#32626;&#30340;&#65306;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#22952;&#23475;&#29616;&#23454;&#19990;&#30028; LLM &#19968;&#20307;&#21270;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection. (arXiv:2302.12173v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#21521;&#37327;&#65306;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#22312;&#21487;&#33021;&#26816;&#32034;&#21040;&#30340;&#25968;&#25454;&#20013;&#31574;&#30053;&#24615;&#22320;&#27880;&#20837;&#25552;&#31034;&#26469;&#36828;&#31243;&#21033;&#29992; LLM &#38598;&#25104;&#24212;&#29992;&#31243;&#24207;&#12290;&#35813;&#25915;&#20987;&#23545;&#25968;&#25454;&#30423;&#31363;&#12289;&#34837;&#34411;&#12289;&#20449;&#24687;&#29983;&#24577;&#31995;&#32479;&#27745;&#26579;&#31561;&#36896;&#25104;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#26085;&#30410;&#34987;&#25972;&#21512;&#21040;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#20013;&#12290;&#26368;&#36817;&#30340; LLM &#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#28789;&#27963;&#35843;&#33410;&#21151;&#33021;&#12290;&#36825;&#20351;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#38024;&#23545;&#24615;&#30340;&#23545;&#25239;&#24615;&#25552;&#31034;&#25915;&#20987;&#65292;&#20363;&#22914; Prompt Injection (PI) &#25915;&#20987;&#20351;&#25915;&#20987;&#32773;&#33021;&#22815;&#35206;&#30422;&#21407;&#22987;&#25351;&#20196;&#21644;&#20351;&#29992;&#30340;&#25511;&#20214;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#20154;&#20204;&#35748;&#20026;&#29992;&#25143;&#30452;&#25509;&#25552;&#31034; LLM&#12290;&#20294;&#26159;&#65292;&#22914;&#26524;&#19981;&#26159;&#29992;&#25143;&#25552;&#31034;&#21602;&#65311;&#25105;&#20204;&#35748;&#20026; LLM &#19968;&#20307;&#21270;&#24212;&#29992;&#31243;&#24207;&#27169;&#31946;&#20102;&#25968;&#25454;&#21644;&#25351;&#20196;&#20043;&#38388;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#20351;&#29992;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#30340;&#26032;&#25915;&#20987;&#21521;&#37327;&#65292;&#20351;&#25915;&#20987;&#32773;&#33021;&#22815;&#36890;&#36807;&#22312;&#21487;&#33021;&#26816;&#32034;&#21040;&#30340;&#25968;&#25454;&#20013;&#31574;&#30053;&#24615;&#22320;&#27880;&#20837;&#25552;&#31034;&#26469;&#36828;&#31243;&#65288;&#27809;&#26377;&#30452;&#25509;&#25509;&#21475;&#65289;&#21033;&#29992; LLM &#38598;&#25104;&#24212;&#29992;&#31243;&#24207;&#12290;&#25105;&#20204;&#20174;&#35745;&#31639;&#26426;&#23433;&#20840;&#30340;&#35282;&#24230;&#25512;&#23548;&#20986;&#19968;&#20010;&#20840;&#38754;&#30340;&#20998;&#31867;&#27861;&#65292;&#20197;&#31995;&#32479;&#22320;&#35843;&#26597;&#24433;&#21709;&#21644;&#28431;&#27934;&#65292;&#21253;&#25324;&#25968;&#25454;&#30423;&#31363;&#12289;&#34837;&#34411;&#12289;&#20449;&#24687;&#29983;&#24577;&#31995;&#32479;&#27745;&#26579;&#20197;&#21450;&#20854;&#20182;&#21019;&#26032;&#24615;&#30340;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly being integrated into various applications. The functionalities of recent LLMs can be flexibly modulated via natural language prompts. This renders them susceptible to targeted adversarial prompting, e.g., Prompt Injection (PI) attacks enable attackers to override original instructions and employed controls. So far, it was assumed that the user is directly prompting the LLM. But, what if it is not the user prompting? We argue that LLM-Integrated Applications blur the line between data and instructions. We reveal new attack vectors, using Indirect Prompt Injection, that enable adversaries to remotely (without a direct interface) exploit LLM-integrated applications by strategically injecting prompts into data likely to be retrieved. We derive a comprehensive taxonomy from a computer security perspective to systematically investigate impacts and vulnerabilities, including data theft, worming, information ecosystem contamination, and other nove
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#30899;&#36275;&#36857;&#36234;&#26469;&#36234;&#22823;&#65292;&#22240;&#27492;&#20986;&#29616;&#20102;&#32511;&#33394;AI&#36825;&#20010;&#39046;&#22495;&#26469;&#35299;&#20915;AI&#30340;&#29615;&#22659;&#21487;&#25345;&#32493;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#23545;98&#31687;Green AI&#25991;&#29486;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#25581;&#31034;&#20102;&#35813;&#39046;&#22495;&#22312;2020&#24180;&#20197;&#21518;&#24471;&#21040;&#20102;&#26174;&#33879;&#22686;&#38271;&#12290;&#20854;&#20013;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#38598;&#20013;&#22312;&#30417;&#27979;AI&#27169;&#22411;&#30340;&#30899;&#36275;&#36857;&#12289;&#35843;&#25972;&#36229;&#21442;&#25968;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#25345;&#32493;&#24615;&#65292;&#25110;&#23545;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2301.11047</link><description>&lt;p&gt;
&#12298;&#32511;&#33394;AI&#30340;&#31995;&#32479;&#24615;&#30740;&#31350;&#12299;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Systematic Review of Green AI. (arXiv:2301.11047v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11047
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#30899;&#36275;&#36857;&#36234;&#26469;&#36234;&#22823;&#65292;&#22240;&#27492;&#20986;&#29616;&#20102;&#32511;&#33394;AI&#36825;&#20010;&#39046;&#22495;&#26469;&#35299;&#20915;AI&#30340;&#29615;&#22659;&#21487;&#25345;&#32493;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#23545;98&#31687;Green AI&#25991;&#29486;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#25581;&#31034;&#20102;&#35813;&#39046;&#22495;&#22312;2020&#24180;&#20197;&#21518;&#24471;&#21040;&#20102;&#26174;&#33879;&#22686;&#38271;&#12290;&#20854;&#20013;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#38598;&#20013;&#22312;&#30417;&#27979;AI&#27169;&#22411;&#30340;&#30899;&#36275;&#36857;&#12289;&#35843;&#25972;&#36229;&#21442;&#25968;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#25345;&#32493;&#24615;&#65292;&#25110;&#23545;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#37319;&#29992;&#65292;AI&#30340;&#30899;&#36275;&#36857;&#24050;&#32463;&#19981;&#21487;&#24573;&#35270;&#12290;&#22240;&#27492;&#65292;AI&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#34987;&#25958;&#20419;&#23545;&#20182;&#20204;&#35774;&#35745;&#21644;&#20351;&#29992;&#30340;AI&#27169;&#22411;&#30340;&#30899;&#25490;&#25918;&#36127;&#36131;&#12290;&#36825;&#23548;&#33268;&#36817;&#24180;&#26469;&#20986;&#29616;&#20102;&#30740;&#31350;&#32511;&#33394;AI&#29615;&#22659;&#21487;&#25345;&#32493;&#24615;&#30340;&#30740;&#31350;&#65292;&#36825;&#20010;&#39046;&#22495;&#34987;&#31216;&#20026;&#32511;&#33394;AI&#12290;&#23613;&#31649;&#23545;&#35813;&#20027;&#39064;&#30340;&#20852;&#36259;&#36805;&#36895;&#22686;&#38271;&#65292;&#20294;&#33267;&#20170;&#20173;&#32570;&#20047;&#32508;&#21512;&#24615;&#30340;&#32511;&#33394;AI&#30740;&#31350;&#27010;&#36848;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;,&#26412;&#25991;&#23545;Green AI&#25991;&#29486;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#32508;&#36848;&#12290;&#20174;&#20998;&#26512;98&#31687;&#21407;&#22987;&#30740;&#31350;&#20013;&#65292;&#20986;&#29616;&#20102;&#19981;&#21516;&#30340;&#27169;&#24335;&#12290;&#36825;&#20010;&#20027;&#39064;&#20174;2020&#24180;&#24320;&#22987;&#32463;&#21382;&#20102;&#26174;&#33879;&#30340;&#22686;&#38271;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#32771;&#34385;&#30417;&#27979;AI&#27169;&#22411;&#30340;&#30899;&#36275;&#36857;&#12289;&#35843;&#25972;&#36229;&#21442;&#25968;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#25345;&#32493;&#24615;&#25110;&#23545;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#28151;&#21512;&#20102;&#20301;&#32622;&#35770;&#25991;&#12289;&#35266;&#23519;&#30740;&#31350;&#21644;&#35299;&#20915;&#26041;&#26696;&#35770;&#25991;&#12290;&#22823;&#22810;&#25968;&#35770;&#25991;&#38598;&#20013;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
With the ever-growing adoption of AI-based systems, the carbon footprint of AI is no longer negligible. AI researchers and practitioners are therefore urged to hold themselves accountable for the carbon emissions of the AI models they design and use. This led in recent years to the appearance of researches tackling AI environmental sustainability, a field referred to as Green AI. Despite the rapid growth of interest in the topic, a comprehensive overview of Green AI research is to date still missing. To address this gap, in this paper, we present a systematic review of the Green AI literature. From the analysis of 98 primary studies, different patterns emerge. The topic experienced a considerable growth from 2020 onward. Most studies consider monitoring AI model footprint, tuning hyperparameters to improve model sustainability, or benchmarking models. A mix of position papers, observational studies, and solution papers are present. Most papers focus on the training phase, are algorithm
&lt;/p&gt;</description></item><item><title>xTrimoABFold&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#25239;&#20307;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#25239;&#20307;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;&#65292;&#26080;&#38656;&#22810;&#24207;&#21015;&#27604;&#23545;&#65292;&#26377;&#26395;&#20419;&#36827;&#39640;&#36890;&#37327;&#33647;&#29289;&#35774;&#35745;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2212.00735</link><description>&lt;p&gt;
xTrimoABFold&#65306;&#26080;&#22810;&#24207;&#21015;&#27604;&#23545;&#30340;&#26032;&#22411;&#25239;&#20307;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
xTrimoABFold: De novo Antibody Structure Prediction without MSA. (arXiv:2212.00735v2 [q-bio.QM] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00735
&lt;/p&gt;
&lt;p&gt;
xTrimoABFold&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#25239;&#20307;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#25239;&#20307;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;&#65292;&#26080;&#38656;&#22810;&#24207;&#21015;&#27604;&#23545;&#65292;&#26377;&#26395;&#20419;&#36827;&#39640;&#36890;&#37327;&#33647;&#29289;&#35774;&#35745;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25239;&#20307;&#24037;&#31243;&#39046;&#22495;&#65292;&#35774;&#35745;&#19968;&#20010;&#26032;&#22411;&#25239;&#20307;&#20197;&#27491;&#30830;&#22320;&#32467;&#21512;&#29305;&#23450;&#25239;&#21407;&#30340;&#34920;&#20301;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#20102;&#35299;&#25239;&#20307;&#32467;&#26500;&#21644;&#20854;&#34920;&#20301;&#21487;&#20197;&#20419;&#36827;&#23545;&#20854;&#21151;&#33021;&#30340;&#26426;&#21046;&#29702;&#35299;&#12290;&#22240;&#27492;&#65292;&#20174;&#20854;&#24207;&#21015;&#39044;&#27979;&#25239;&#20307;&#32467;&#26500;&#19968;&#30452;&#26159;&#19968;&#39033;&#39640;&#24230;&#26377;&#20215;&#20540;&#30340;&#20219;&#21153;&#65292;&#32780;AlphaFold2&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#34507;&#30333;&#36136;&#24207;&#21015;&#39044;&#27979;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23545;&#20110;&#25239;&#20307;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#25239;&#20307;&#30340;&#20114;&#34917;&#20915;&#23450;&#21306;&#65288;CDRs&#65289;&#65292;&#20854;&#39044;&#27979;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#26377;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of antibody engineering, an essential task is to design a novel antibody whose paratopes bind to a specific antigen with correct epitopes. Understanding antibody structure and its paratope can facilitate a mechanistic understanding of its function. Therefore, antibody structure prediction from its sequence alone has always been a highly valuable problem for de novo antibody design. AlphaFold2, a breakthrough in the field of structural biology, provides a solution to predict protein structure based on protein sequences and computationally expensive coevolutionary multiple sequence alignments (MSAs). However, the computational efficiency and undesirable prediction accuracy of antibodies, especially on the complementarity-determining regions (CDRs) of antibodies limit their applications in the industrially high-throughput drug design. To learn an informative representation of antibodies, we employed a deep antibody language model (ALM) on curated sequences from the observed a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23884;&#20837;&#26041;&#27861;&#30340;SNAFUE&#25216;&#26415;&#65292;&#29992;&#20110;&#23547;&#25214;&#22797;&#21046;/&#31896;&#36148;&#25915;&#20987;&#65292;&#24182;&#20351;&#29992;&#35813;&#25216;&#26415;&#23545;ImageNet&#20998;&#31867;&#22120;&#36827;&#34892;&#20102;&#27979;&#35797;&#24182;&#21457;&#29616;&#35768;&#22810;&#26131;&#20110;&#25551;&#36848;&#30340;&#28431;&#27934;&#12290;</title><link>http://arxiv.org/abs/2211.10024</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#22797;&#21046;/&#31896;&#36148;&#25915;&#20987;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Diagnostics for Deep Neural Networks with Automated Copy/Paste Attacks. (arXiv:2211.10024v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23884;&#20837;&#26041;&#27861;&#30340;SNAFUE&#25216;&#26415;&#65292;&#29992;&#20110;&#23547;&#25214;&#22797;&#21046;/&#31896;&#36148;&#25915;&#20987;&#65292;&#24182;&#20351;&#29992;&#35813;&#25216;&#26415;&#23545;ImageNet&#20998;&#31867;&#22120;&#36827;&#34892;&#20102;&#27979;&#35797;&#24182;&#21457;&#29616;&#35768;&#22810;&#26131;&#20110;&#25551;&#36848;&#30340;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#24110;&#21161;&#20154;&#31867;&#30417;&#30563;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#12290;&#23545;&#25239;&#26679;&#26412;&#21487;&#20197;&#36890;&#36807;&#25581;&#31034;DNN&#30340;&#24369;&#28857;&#26469;&#24110;&#21161;&#65292;&#20294;&#24456;&#38590;&#35299;&#37322;&#25110;&#20174;&#20013;&#24471;&#20986;&#21487;&#23454;&#26045;&#30340;&#32467;&#35770;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#20351;&#29992;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#21253;&#25324;&#22797;&#21046;/&#31896;&#36148;&#25915;&#20987;&#65292;&#20854;&#20013;&#19968;&#24352;&#33258;&#28982;&#22270;&#20687;&#31896;&#36148;&#21040;&#21478;&#19968;&#24352;&#22270;&#20687;&#20250;&#23548;&#33268;&#24847;&#22806;&#30340;&#20998;&#31867;&#38169;&#35823;&#12290;&#26412;&#25991;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#20004;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20351;&#29992;&#23884;&#20837;&#30340;SNAFUE&#65288;Search for Natural Adversarial Features Using Embeddings&#65289;&#23547;&#25214;&#22797;&#21046;/&#31896;&#36148;&#25915;&#20987;&#30340;&#20840;&#33258;&#21160;&#26041;&#27861;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;SNAFUE&#26469;&#27979;&#35797;ImageNet&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#37325;&#29616;&#20102;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;&#22797;&#21046;/&#31896;&#36148;&#25915;&#20987;&#65292;&#24182;&#21457;&#29616;&#20102;&#25968;&#30334;&#20010;&#20854;&#20182;&#26131;&#20110;&#25551;&#36848;&#30340;&#28431;&#27934;&#65292;&#20840;&#36807;&#31243;&#26080;&#38656;&#20154;&#20026;&#24178;&#39044;&#12290;&#20195;&#30721;&#24050;&#22312; https://github.com/thestephencasper/snafue &#19978;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers the problem of helping humans exercise scalable oversight over deep neural networks (DNNs). Adversarial examples can be useful by helping to reveal weaknesses in DNNs, but they can be difficult to interpret or draw actionable conclusions from. Some previous works have proposed using human-interpretable adversarial attacks including copy/paste attacks in which one natural image pasted into another causes an unexpected misclassification. We build on these with two contributions. First, we introduce Search for Natural Adversarial Features Using Embeddings (SNAFUE) which offers a fully automated method for finding copy/paste attacks. Second, we use SNAFUE to red team an ImageNet classifier. We reproduce copy/paste attacks from previous works and find hundreds of other easily-describable vulnerabilities, all without a human in the loop. Code is available at https://github.com/thestephencasper/snafue
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20449;&#20219;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#32676;&#20307;&#26234;&#33021;&#30340;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2211.08407</link><description>&lt;p&gt;
&#22522;&#20110;&#20449;&#20219;&#24863;&#30693;&#30340;&#32676;&#20307;&#26234;&#33021;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Trust-Awareness to Secure Swarm Intelligence from Data Injection Attack. (arXiv:2211.08407v3 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20449;&#20219;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#32676;&#20307;&#26234;&#33021;&#30340;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32676;&#20307;&#26234;&#33021;&#65288;SI&#65289;&#20511;&#21161;&#26032;&#20852;&#30340;&#24037;&#19994;&#26234;&#33021;&#20307;&#65288;IA&#65289;&#25216;&#26415;&#24471;&#20197;&#23454;&#29616;&#65292;&#34987;&#39044;&#35745;&#22312;&#30001;&#31532;&#20845;&#20195;&#31227;&#21160;&#36890;&#20449;&#21644;&#25968;&#23383;&#23402;&#29983;&#65288;DT&#65289;&#26500;&#25104;&#30340;&#26410;&#26469;&#24037;&#19994;&#29289;&#32852;&#32593;&#65288;IIoT&#65289;&#20013;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;SI &#23545;&#20110;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#21487;&#33021;&#20250;&#20351;&#20854;&#26080;&#27861;&#23454;&#38469;&#37096;&#32626;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20449;&#20219;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545; SI &#30340;&#36825;&#19968;&#23433;&#20840;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Enabled by the emerging industrial agent (IA) technology, swarm intelligence (SI) is envisaged to play an important role in future industrial Internet of Things (IIoT) that is shaped by Sixth Generation (6G) mobile communications and digital twin (DT). However, its fragility against data injection attack may halt it from practical deployment. In this paper we propose an efficient trust approach to address this security concern for SI.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#25511;&#21046;1&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#34880;&#31958;&#27700;&#24179;&#20013;&#30340;&#24212;&#29992;&#65292;&#37319;&#29992;BCQ&#12289;CQL&#21644;TD3-BC&#31639;&#27861;&#26377;&#25928;&#22320;&#31649;&#29702;&#34880;&#31958;&#27700;&#24179;&#65292;&#20811;&#26381;&#20102;&#22312;&#32447;&#23398;&#20064;&#36807;&#31243;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#19981;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.03376</link><description>&lt;p&gt;
&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#29992;&#20110;&#26356;&#23433;&#20840;&#22320;&#25511;&#21046;1&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#30340;&#34880;&#31958;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
Offline Reinforcement Learning for Safer Blood Glucose Control in People with Type 1 Diabetes. (arXiv:2204.03376v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.03376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#25511;&#21046;1&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#34880;&#31958;&#27700;&#24179;&#20013;&#30340;&#24212;&#29992;&#65292;&#37319;&#29992;BCQ&#12289;CQL&#21644;TD3-BC&#31639;&#27861;&#26377;&#25928;&#22320;&#31649;&#29702;&#34880;&#31958;&#27700;&#24179;&#65292;&#20811;&#26381;&#20102;&#22312;&#32447;&#23398;&#20064;&#36807;&#31243;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#19981;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#28151;&#21512;&#38381;&#29615;&#31995;&#32479;&#30340;&#24191;&#27867;&#24212;&#29992;&#23558;&#26159;1&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#25252;&#29702;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#37324;&#31243;&#30865;&#12290;&#36825;&#20123;&#35774;&#22791;&#36890;&#24120;&#21033;&#29992;&#31616;&#21333;&#30340;&#25511;&#21046;&#31639;&#27861;&#36873;&#25321;&#26368;&#20339;&#33008;&#23707;&#32032;&#21058;&#37327;&#26469;&#32500;&#25345;&#34880;&#31958;&#27700;&#24179;&#22312;&#20581;&#24247;&#33539;&#22260;&#20869;&#12290;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#24050;&#34987;&#29992;&#20316;&#36827;&#19968;&#27493;&#22686;&#24378;&#36825;&#20123;&#35774;&#22791;&#30340;&#34880;&#31958;&#25511;&#21046;&#26041;&#27861;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#19982;&#20256;&#32479;&#25511;&#21046;&#31639;&#27861;&#30456;&#27604;&#38477;&#20302;&#20102;&#24739;&#32773;&#39118;&#38505;&#65292;&#24182;&#25913;&#21892;&#20102;&#22312;&#30446;&#26631;&#33539;&#22260;&#20869;&#30340;&#26102;&#38388;&#65292;&#20294;&#24448;&#24448;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#19981;&#31283;&#23450;&#65292;&#23548;&#33268;&#36873;&#25321;&#19981;&#23433;&#20840;&#30340;&#34892;&#20026;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#24320;&#21457;&#26377;&#25928;&#30340;&#21058;&#37327;&#31574;&#30053;&#65292;&#26080;&#38656;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36827;&#34892;&#28508;&#22312;&#21361;&#38505;&#30340;&#24739;&#32773;&#20132;&#20114;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;BCQ&#12289;CQL&#21644;TD3-BC&#22312;&#31649;&#29702;FDA&#25209;&#20934;&#30340;30&#21517;&#34394;&#25311;&#30149;&#20154;&#30340;&#34880;&#31958;&#27700;&#24179;&#20013;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread adoption of effective hybrid closed loop systems would represent an important milestone of care for people living with type 1 diabetes (T1D). These devices typically utilise simple control algorithms to select the optimal insulin dose for maintaining blood glucose levels within a healthy range. Online reinforcement learning (RL) has been utilised as a method for further enhancing glucose control in these devices. Previous approaches have been shown to reduce patient risk and improve time spent in the target range when compared to classical control algorithms, but are prone to instability in the learning process, often resulting in the selection of unsafe actions. This work presents an evaluation of offline RL for developing effective dosing policies without the need for potentially dangerous patient interaction during training. This paper examines the utility of BCQ, CQL and TD3-BC in managing the blood glucose of the 30 virtual patients available within the FDA-approved
&lt;/p&gt;</description></item><item><title>&#20174;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#20013;&#25552;&#21462;&#21306;&#20998;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#21387;&#32553;&#34920;&#31034;&#31354;&#38388;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;Q-Score&#33258;&#30417;&#30563;&#34920;&#31034;&#36136;&#37327;&#20998;&#25968;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#39044;&#27979;&#32447;&#24615;&#35780;&#20272;&#26399;&#38388;&#30340;&#38169;&#35823;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2203.01881</link><description>&lt;p&gt;
&#20351;&#29992;&#21306;&#20998;&#29305;&#24449;&#24230;&#37327;&#19979;&#28216;&#20998;&#31867;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Measuring Self-Supervised Representation Quality for Downstream Classification using Discriminative Features. (arXiv:2203.01881v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.01881
&lt;/p&gt;
&lt;p&gt;
&#20174;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#20013;&#25552;&#21462;&#21306;&#20998;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#21387;&#32553;&#34920;&#31034;&#31354;&#38388;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;Q-Score&#33258;&#30417;&#30563;&#34920;&#31034;&#36136;&#37327;&#20998;&#25968;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#39044;&#27979;&#32447;&#24615;&#35780;&#20272;&#26399;&#38388;&#30340;&#38169;&#35823;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#23427;&#20204;&#30340;&#22833;&#36133;&#27169;&#24335;&#21644;&#23398;&#20064;&#34920;&#31034;&#30340;&#35299;&#37322;&#65292;&#23384;&#22312;&#30528;&#26377;&#38480;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102; SimCLR&#12289;SwaV&#12289;MoCo&#12289;BYOL&#12289;DINO&#12289;SimSiam&#12289;VICReg &#21644; Barlow Twins &#31561;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#12290;&#22312;&#19981;&#20351;&#29992;&#31867;&#26631;&#31614;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#23545;&#24212;&#20110;&#22270;&#20687;&#20013;&#29420;&#29305;&#29289;&#29702;&#23646;&#24615;&#30340;&#21306;&#20998;&#29305;&#24449;&#65292;&#36825;&#20123;&#21306;&#20998;&#29305;&#24449;&#20027;&#35201;&#23384;&#22312;&#20110;&#27491;&#30830;&#20998;&#31867;&#30340;&#34920;&#31034;&#20013;&#12290;&#20351;&#29992;&#36825;&#20123;&#29305;&#24449;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#34920;&#31034;&#31354;&#38388;&#21387;&#32553;&#22810;&#36798; 40%&#65292;&#32780;&#19981;&#20250;&#26174;&#33879;&#24433;&#21709;&#32447;&#24615;&#20998;&#31867;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#30417;&#30563;&#34920;&#31034;&#36136;&#37327;&#20998;&#25968;&#65288;&#25110; Q-Score&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#12289;&#26080;&#30417;&#30563;&#30340;&#20998;&#25968;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#39044;&#27979;&#19968;&#20010;&#32473;&#23450;&#26679;&#26412;&#22312;&#32447;&#24615;&#35780;&#20272;&#26399;&#38388;&#26159;&#21542;&#21487;&#33021;&#34987;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#22312; ImageNet-100 &#21644; ImageNet-1K &#19978;&#23454;&#29616;&#20102; AUPRC &#20998;&#21035;&#20026; 91.45 &#21644; 78.78&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning has shown impressive results in downstream classification tasks. However, there is limited work in understanding their failure modes and interpreting their learned representations. In this paper, we study the representation space of state-of-the-art self-supervised models including SimCLR, SwaV, MoCo, BYOL, DINO, SimSiam, VICReg and Barlow Twins. Without the use of class label information, we discover discriminative features that correspond to unique physical attributes in images, present mostly in correctly-classified representations. Using these features, we can compress the representation space by up to $40\%$ without significantly affecting linear classification performance. We then propose Self-Supervised Representation Quality Score (or Q-Score), a model-agnostic, unsupervised score that can reliably predict if a given sample is likely to be mis-classified during linear evaluation, achieving AUPRC of 91.45 on ImageNet-100 and 78.78 on ImageNet-1K. Q-Score
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23601;3D&#30446;&#26631;&#26816;&#27979;&#39046;&#22495;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#21644;&#31995;&#32479;&#21270;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#26694;&#26550;&#29992;&#20110;&#27604;&#36739;3D&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#24555;&#36895;&#20102;&#35299;&#35813;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2201.09354</link><description>&lt;p&gt;
3D Object Detection Models and Methods &#30340;&#35843;&#26597;&#21644;&#31995;&#32479;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Survey and Systematization of 3D Object Detection Models and Methods. (arXiv:2201.09354v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.09354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23601;3D&#30446;&#26631;&#26816;&#27979;&#39046;&#22495;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#21644;&#31995;&#32479;&#21270;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#26694;&#26550;&#29992;&#20110;&#27604;&#36739;3D&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#24555;&#36895;&#20102;&#35299;&#35813;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#24378;&#28872;&#38656;&#27714;&#21644;3D&#20256;&#24863;&#22120;&#24191;&#27867;&#26222;&#21450;&#65292;&#19981;&#26029;&#25512;&#21160;&#30528;3D&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#30340;&#25552;&#20986;&#12290;&#26412;&#25991;&#20840;&#38754;&#35843;&#30740;&#20102;2012&#24180;&#33267;2021&#24180;&#38388;3D&#30446;&#26631;&#26816;&#27979;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#21253;&#25324;&#36755;&#20837;&#25968;&#25454;&#12289;&#25968;&#25454;&#34920;&#31034;&#21644;&#29305;&#24449;&#25552;&#21462;&#20197;&#21450;&#23454;&#38469;&#26816;&#27979;&#27169;&#22359;&#30340;&#23436;&#25972;&#27969;&#31243;&#12290;&#20171;&#32461;&#20102;&#22522;&#26412;&#27010;&#24565;&#65292;&#20851;&#27880;&#20102;&#36807;&#21435;&#21313;&#24180;&#37324;&#28044;&#29616;&#30340;&#21508;&#31181;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#21270;&#30340;&#26041;&#27861;&#65292;&#20026;&#27604;&#36739;&#36825;&#20123;&#26041;&#27861;&#25552;&#20379;&#20102;&#23454;&#29992;&#26694;&#26550;&#65292;&#20197;&#25351;&#23548;&#26410;&#26469;&#30340;&#24320;&#21457;&#12289;&#35780;&#20272;&#21644;&#24212;&#29992;&#27963;&#21160;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#35843;&#26597;&#21644;&#31995;&#32479;&#21270;&#30740;&#31350;&#21487;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#24555;&#36895;&#20102;&#35299;&#35813;&#39046;&#22495;&#65292;&#23558;3D&#30446;&#26631;&#26816;&#27979;&#30340;&#35299;&#20915;&#26041;&#26696;&#20998;&#35299;&#20026;&#26356;&#26131;&#22788;&#29702;&#30340;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Strong demand for autonomous vehicles and the wide availability of 3D sensors are continuously fueling the proposal of novel methods for 3D object detection. In this paper, we provide a comprehensive survey of recent developments from 2012-2021 in 3D object detection covering the full pipeline from input data, over data representation and feature extraction to the actual detection modules. We introduce fundamental concepts, focus on a broad range of different approaches that have emerged over the past decade, and propose a systematization that provides a practical framework for comparing these approaches with the goal of guiding future development, evaluation and application activities. Specifically, our survey and systematization of 3D object detection models and methods can help researchers and practitioners to get a quick overview of the field by decomposing 3DOD solutions into more manageable pieces.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23569;&#26679;&#26412;&#30340;&#36923;&#36753;&#30693;&#35782;&#26465;&#20214;&#19979;&#25991;&#26412;&#29983;&#25104;&#30340;&#32479;&#19968;&#26694;&#26550;LOGEN&#65292;&#36890;&#36807;&#33258;&#35757;&#32451;&#21644;&#22522;&#20110;&#20869;&#23481;&#21644;&#32467;&#26500;&#19968;&#33268;&#24615;&#25277;&#26679;&#20266;&#36923;&#36753;&#24418;&#24335;&#65292;&#23454;&#29616;&#20102;&#22312;&#23569;&#37327;&#26679;&#26412;&#19979;&#30340;&#25991;&#26412;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2112.01404</link><description>&lt;p&gt;
LOGEN&#65306;&#22522;&#20110;&#36923;&#36753;&#30693;&#35782;&#26465;&#20214;&#30340;&#33258;&#35757;&#32451;&#25991;&#26412;&#29983;&#25104;&#22312;&#23569;&#26679;&#26412;&#19979;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
LOGEN: Few-shot Logical Knowledge-Conditioned Text Generation with Self-training. (arXiv:2112.01404v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.01404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23569;&#26679;&#26412;&#30340;&#36923;&#36753;&#30693;&#35782;&#26465;&#20214;&#19979;&#25991;&#26412;&#29983;&#25104;&#30340;&#32479;&#19968;&#26694;&#26550;LOGEN&#65292;&#36890;&#36807;&#33258;&#35757;&#32451;&#21644;&#22522;&#20110;&#20869;&#23481;&#21644;&#32467;&#26500;&#19968;&#33268;&#24615;&#25277;&#26679;&#20266;&#36923;&#36753;&#24418;&#24335;&#65292;&#23454;&#29616;&#20102;&#22312;&#23569;&#37327;&#26679;&#26412;&#19979;&#30340;&#25991;&#26412;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20027;&#35201;&#38598;&#20013;&#22312;&#34920;&#38754;&#23618;&#38754;&#25551;&#36848;&#65292;&#20854;&#23384;&#22312;&#25511;&#21046;&#20869;&#23481;&#36873;&#25321;&#22256;&#38590;&#21644;&#20302;&#20445;&#30495;&#24230;&#30340;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21033;&#29992;&#36923;&#36753;&#24418;&#24335;&#26469;&#20419;&#36827;&#36923;&#36753;&#30693;&#35782;&#26465;&#20214;&#19979;&#30340;&#25991;&#26412;&#29983;&#25104;&#12290;&#34429;&#28982;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#26159;&#23427;&#20204;&#23545;&#25968;&#25454;&#30340;&#38656;&#27714;&#37327;&#36739;&#22823;&#65292;&#36825;&#20351;&#24471;&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23569;&#26679;&#26412;&#30340;&#36923;&#36753;&#30693;&#35782;&#26465;&#20214;&#19979;&#25991;&#26412;&#29983;&#25104;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#20351;&#29992;&#23569;&#37327;&#31181;&#23376;&#36923;&#36753;&#24418;&#24335;&#65288;&#22914;20/100&#31181;&#23376;&#65289; &#65292;&#24182;&#21033;&#29992;&#33258;&#35757;&#32451;&#21644;&#22522;&#20110;&#20869;&#23481;&#21644;&#32467;&#26500;&#19968;&#33268;&#24615;&#25277;&#26679;&#20266;&#36923;&#36753;&#24418;&#24335;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#27604;&#22522;&#20934;&#26041;&#27861;&#33719;&#24471;&#26356;&#22909;&#30340;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language generation from structured data mainly focuses on surface-level descriptions, suffering from uncontrollable content selection and low fidelity. Previous works leverage logical forms to facilitate logical knowledge-conditioned text generation. Though achieving remarkable progress, they are data-hungry, which makes the adoption for real-world applications challenging with limited data. To this end, this paper proposes a unified framework for logical knowledge-conditioned text generation in the few-shot setting. With only a few seeds logical forms (e.g., 20/100 shot), our approach leverages self-training and samples pseudo logical forms based on content and structure consistency. Experimental results demonstrate that our approach can obtain better few-shot performance than baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PredProp&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#33258;&#36866;&#24212;&#21152;&#26435;&#21442;&#25968;&#26356;&#26032;&#26469;&#20248;&#21270;&#39044;&#27979;&#32534;&#30721;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#21644;&#29366;&#24577;&#12290;&#36890;&#36807;&#20351;&#29992;&#20256;&#25773;&#35823;&#24046;&#21327;&#26041;&#24046;&#23454;&#29616;&#36817;&#20284;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#65292;&#20351;&#24471;PredProp&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#27604;Adam&#34920;&#29616;&#26356;&#20248;&#12290;&#27492;&#22806;&#65292;PredProp&#21487;&#20197;&#36890;&#36807;&#35823;&#24046;&#31934;&#24230;&#25552;&#39640;&#26435;&#37325;&#21442;&#25968;&#30340;&#20248;&#21270;&#25928;&#26524;&#65292;&#24182;&#19988;&#22312;&#23618;&#27425;&#32467;&#26500;&#20013;&#21487;&#20197;&#20998;&#35299;&#31934;&#24230;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2111.08792</link><description>&lt;p&gt;
PredProp: &#24102;&#31934;&#24230;&#21152;&#26435;&#39044;&#27979;&#32534;&#30721;&#30340;&#21452;&#21521;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PredProp: Bidirectional Stochastic Optimization with Precision Weighted Predictive Coding. (arXiv:2111.08792v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.08792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PredProp&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#33258;&#36866;&#24212;&#21152;&#26435;&#21442;&#25968;&#26356;&#26032;&#26469;&#20248;&#21270;&#39044;&#27979;&#32534;&#30721;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#21644;&#29366;&#24577;&#12290;&#36890;&#36807;&#20351;&#29992;&#20256;&#25773;&#35823;&#24046;&#21327;&#26041;&#24046;&#23454;&#29616;&#36817;&#20284;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#65292;&#20351;&#24471;PredProp&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#27604;Adam&#34920;&#29616;&#26356;&#20248;&#12290;&#27492;&#22806;&#65292;PredProp&#21487;&#20197;&#36890;&#36807;&#35823;&#24046;&#31934;&#24230;&#25552;&#39640;&#26435;&#37325;&#21442;&#25968;&#30340;&#20248;&#21270;&#25928;&#26524;&#65292;&#24182;&#19988;&#22312;&#23618;&#27425;&#32467;&#26500;&#20013;&#21487;&#20197;&#20998;&#35299;&#31934;&#24230;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PredProp&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#20256;&#25773;&#30340;&#35823;&#24046;&#21644;&#31070;&#32463;&#27963;&#21160;&#30340;&#31934;&#24230;&#65292;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#33258;&#36866;&#24212;&#21152;&#26435;&#21442;&#25968;&#26356;&#26032;&#26469;&#20248;&#21270;&#39044;&#27979;&#32534;&#30721;&#32593;&#32476;(PCN)&#20013;&#30340;&#26435;&#37325;&#21644;&#29366;&#24577;&#12290;&#30001;&#20110;&#20256;&#25773;&#35823;&#24046;&#21327;&#26041;&#24046;&#19982;Fisher&#20449;&#24687;&#30697;&#38453;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;PredProp&#23454;&#29616;&#20102;&#36817;&#20284;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#12290;&#25105;&#20204;&#22312;&#23494;&#38598;&#35299;&#30721;&#22120;&#32593;&#32476;&#21644;&#31616;&#21333;&#22270;&#20687;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#23637;&#31034;&#20102;PredProp&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#27979;&#35797;&#37197;&#32622;&#20013;&#65292;PredProp&#34920;&#29616;&#20248;&#20110;Adam&#65292;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#20248;&#21270;&#22120;&#12290;&#27492;&#22806;&#65292;&#21487;&#29992;&#20110;&#26435;&#37325;&#21442;&#25968;&#30340;&#20248;&#21270;&#26041;&#27861;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21463;&#30410;&#20110;&#20351;&#29992;PredProp&#30340;&#35823;&#24046;&#31934;&#24230;&#12290;&#30001;&#20110;&#23618;&#27425;&#39044;&#27979;&#32534;&#30721;&#23618;&#26159;&#36890;&#36807;&#23616;&#37096;&#35823;&#24046;&#21333;&#29420;&#20248;&#21270;&#30340;&#65292;&#25152;&#20197;&#25152;&#38656;&#31934;&#24230;&#22312;&#23618;&#27425;&#32467;&#26500;&#19978;&#20998;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present PredProp, a method for optimization of weights and states in predictive coding networks (PCNs) based on the precision of propagated errors and neural activity. PredProp jointly addresses inference and learning via stochastic gradient descent and adaptively weights parameter updates by approximate curvature. Due to the relation between propagated error covariance and the Fisher information matrix, PredProp implements approximate Natural Gradient Descent. We demonstrate PredProp's effectiveness in the context of dense decoder networks and simple image benchmark datasets. We found that PredProp performs favorably over Adam, a widely used adaptive learning rate optimizer in the tested configurations. Furthermore, available optimization methods for weight parameters benefit from using PredProp's error precision during inference. Since hierarchical predictive coding layers are optimised individually using local errors, the required precisions factorize over hierarchical layers. Ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#39564;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#25913;&#36827;&#36125;&#21494;&#26031;&#20998;&#23618;&#28151;&#21512;&#32858;&#31867;&#27169;&#22411;&#65292;&#22312;&#27599;&#20010;&#23618;&#32423;&#23545;&#33410;&#28857;&#23454;&#26045;&#26368;&#22823;&#38388;&#38548;&#32422;&#26463;&#20197;&#22686;&#24378;&#38598;&#32676;&#30340;&#20998;&#31163;&#12290;</title><link>http://arxiv.org/abs/2105.06903</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#20998;&#23618;&#28151;&#21512;&#32858;&#31867;&#20013;&#30340;&#21518;&#39564;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Posterior Regularization on Bayesian Hierarchical Mixture Clustering. (arXiv:2105.06903v7 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.06903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#39564;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#25913;&#36827;&#36125;&#21494;&#26031;&#20998;&#23618;&#28151;&#21512;&#32858;&#31867;&#27169;&#22411;&#65292;&#22312;&#27599;&#20010;&#23618;&#32423;&#23545;&#33410;&#28857;&#23454;&#26045;&#26368;&#22823;&#38388;&#38548;&#32422;&#26463;&#20197;&#22686;&#24378;&#38598;&#32676;&#30340;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20998;&#23618;&#28151;&#21512;&#32858;&#31867;&#36890;&#36807;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#29992;&#23618;&#32423;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#28151;&#21512;&#27169;&#22411;(HDPMM)&#26367;&#25442;&#20256;&#32479;&#30340;&#39640;&#26031;-&#39640;&#26031;&#26680;&#26469;&#23454;&#29616;&#20174;&#29238;&#33410;&#28857;&#21040;&#23376;&#33410;&#28857;&#30340;&#25193;&#25955;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#20256;&#32479;&#30340;&#36125;&#21494;&#26031;&#20998;&#23618;&#32858;&#31867;&#12290;&#28982;&#32780;&#65292;BHMC&#21487;&#33021;&#20250;&#20135;&#29983;&#20855;&#26377;&#39640;&#33410;&#28857;&#26041;&#24046;&#30340;&#26641;&#65292;&#34920;&#26126;&#22312;&#36739;&#39640;&#23618;&#32423;&#20043;&#38388;&#30340;&#33410;&#28857;&#20043;&#38388;&#23384;&#22312;&#36739;&#24369;&#30340;&#20998;&#31163;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21518;&#39564;&#27491;&#21017;&#21270;(Posterior Regularization)&#65292;&#23427;&#23545;&#27599;&#20010;&#23618;&#32423;&#30340;&#33410;&#28857;&#23454;&#26045;&#26368;&#22823;&#38388;&#38548;&#32422;&#26463;&#20197;&#22686;&#24378;&#38598;&#32676;&#30340;&#20998;&#31163;&#12290;&#25105;&#20204;&#38416;&#36848;&#20102;&#22914;&#20309;&#23558;PR&#24212;&#29992;&#20110;BHMC&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#22312;&#25913;&#36827;BHMC&#27169;&#22411;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian hierarchical mixture clustering (BHMC) improves traditionalBayesian hierarchical clustering by replacing conventional Gaussian-to-Gaussian kernels with a Hierarchical Dirichlet Process Mixture Model(HDPMM) for parent-to-child diffusion in the generative process. However,BHMC may produce trees with high nodal variance, indicating weak separation between nodes at higher levels. To address this issue, we employ Posterior Regularization, which imposes max-margin constraints on nodes at every level to enhance cluster separation. We illustrate how to apply PR toBHMC and demonstrate its effectiveness in improving the BHMC model.
&lt;/p&gt;</description></item></channel></rss>