<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Vision Transformer&#30340;&#25968;&#25454;&#20013;&#24515;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#22343;&#36136;&#21435;&#38654;&#38382;&#39064;&#12290;&#20256;&#32479;&#26041;&#27861;&#22312;&#22788;&#29702;NH-HAZE23&#25968;&#25454;&#38598;&#31561;&#38750;&#22343;&#36136;&#38654;&#22270;&#20687;&#26102;&#23384;&#22312;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#28385;&#36275;&#24314;&#27169;&#22343;&#36136;&#38654;&#25152;&#38656;&#30340;&#20551;&#35774;&#20043;&#19968;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#25351;&#20986;&#20809;&#38752;&#25968;&#25454;&#22686;&#24191;&#24182;&#19981;&#33021;&#35299;&#20915;&#38382;&#39064;&#65292;&#22240;&#20026;&#38656;&#35201;&#22788;&#29702;&#20998;&#24067;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2304.07874</link><description>&lt;p&gt;
&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#30340;&#12289;&#22522;&#20110;Vision Transformer&#30340;&#38750;&#22343;&#36136;&#21435;&#38654;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Data-Centric Solution to NonHomogeneous Dehazing via Vision Transformer. (arXiv:2304.07874v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Vision Transformer&#30340;&#25968;&#25454;&#20013;&#24515;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#22343;&#36136;&#21435;&#38654;&#38382;&#39064;&#12290;&#20256;&#32479;&#26041;&#27861;&#22312;&#22788;&#29702;NH-HAZE23&#25968;&#25454;&#38598;&#31561;&#38750;&#22343;&#36136;&#38654;&#22270;&#20687;&#26102;&#23384;&#22312;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#28385;&#36275;&#24314;&#27169;&#22343;&#36136;&#38654;&#25152;&#38656;&#30340;&#20551;&#35774;&#20043;&#19968;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#25351;&#20986;&#20809;&#38752;&#25968;&#25454;&#22686;&#24191;&#24182;&#19981;&#33021;&#35299;&#20915;&#38382;&#39064;&#65292;&#22240;&#20026;&#38656;&#35201;&#22788;&#29702;&#20998;&#24067;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#20687;&#21435;&#38654;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#34987;&#25552;&#20986;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#24182;&#22312;&#22788;&#29702;&#22343;&#36136;&#38654;&#26102;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#23601;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#26080;&#27861;&#22312;&#24212;&#29992;&#20110;&#23384;&#22312;&#38750;&#22343;&#36136;&#38654;&#30340;&#22270;&#20687;&#26102;&#20445;&#25345;&#31867;&#20284;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;NTIRE&#25361;&#25112;&#20171;&#32461;&#30340;NH-HAZE23&#25968;&#25454;&#38598;&#12290;&#20854;&#20013;&#19968;&#20010;&#22833;&#36133;&#30340;&#21407;&#22240;&#26159;&#38750;&#22343;&#36136;&#38654;&#19981;&#31526;&#21512;&#24314;&#27169;&#22343;&#36136;&#38654;&#25152;&#38656;&#30340;&#20551;&#35774;&#20043;&#19968;&#12290;&#27492;&#22806;&#65292;&#20256;&#32479;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#26041;&#27861;&#38656;&#35201;&#23545;&#22823;&#37327;&#30340;&#38750;&#22343;&#36136;&#38654;&#22270;&#20687;&#19982;&#20854;&#28165;&#26224;&#23545;&#24212;&#39033;&#36827;&#34892;&#37197;&#23545;&#65292;&#32780;NH-HAZE23&#25968;&#25454;&#38598;&#30340;&#25968;&#37327;&#26159;&#26377;&#38480;&#30340;&#12290;&#23613;&#31649;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20854;&#20182;&#38750;&#22343;&#36136;&#21435;&#38654;&#25968;&#25454;&#38598;&#25193;&#20805;NH-HAZE23&#25968;&#25454;&#38598;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#26377;&#24517;&#35201;&#35774;&#35745;&#19968;&#31181;&#36866;&#24403;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#26041;&#27861;&#20197;&#20943;&#23569;&#30446;&#26631;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed an increased interest in image dehazing. Many deep learning methods have been proposed to tackle this challenge, and have made significant accomplishments dealing with homogeneous haze. However, these solutions cannot maintain comparable performance when they are applied to images with non-homogeneous haze, e.g., NH-HAZE23 dataset introduced by NTIRE challenges. One of the reasons for such failures is that non-homogeneous haze does not obey one of the assumptions that is required for modeling homogeneous haze. In addition, a large number of pairs of non-homogeneous hazy image and the clean counterpart is required using traditional end-to-end training approaches, while NH-HAZE23 dataset is of limited quantities. Although it is possible to augment the NH-HAZE23 dataset by leveraging other non-homogeneous dehazing datasets, we observe that it is necessary to design a proper data-preprocessing approach that reduces the distribution gaps between the target datase
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110; \texttt{mBART.CC25} &#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#21518;&#21521;&#32763;&#35793;&#21644;&#36801;&#31227;&#23398;&#20064;&#31561; NLP &#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#22686;&#24378;&#65292;&#20197;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.07869</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Neural Machine Translation For Low Resource Languages. (arXiv:2304.07869v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07869
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110; \texttt{mBART.CC25} &#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#21518;&#21521;&#32763;&#35793;&#21644;&#36801;&#31227;&#23398;&#20064;&#31561; NLP &#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#22686;&#24378;&#65292;&#20197;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#21644;&#27969;&#21160;&#24615;&#65292;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#22312;&#20960;&#31181;&#35821;&#35328;&#23545;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#20294;&#22312;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793; (MNMT) &#39046;&#22495;&#30475;&#21040;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#21364;&#27809;&#26377;&#36827;&#34892;&#20840;&#38754;&#35843;&#26597;&#20197;&#30830;&#23450;&#21738;&#20123;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;&#35813;&#39033;&#30446;&#30340;&#30446;&#26631;&#26159;&#30740;&#31350;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#39046;&#22495;&#65292;&#24182;&#26500;&#24314;&#19968;&#20010;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#35813;&#39033;&#30446;&#26088;&#22312;&#24314;&#31435;&#22312; \texttt{mBART.CC25} &#35821;&#35328;&#27169;&#22411;&#22522;&#30784;&#19978;&#65292;&#24182;&#25506;&#32034;&#21033;&#29992;&#21508;&#31181; NLP &#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65288;&#22914;&#21518;&#21521;&#32763;&#35793;&#21644;&#36801;&#31227;&#23398;&#20064;&#65289;&#26469;&#22686;&#24378;&#23427;&#30340;&#31574;&#30053;&#12290;&#35813;&#23454;&#29616;&#35797;&#22270;&#35299;&#24320; NMT &#24212;&#29992;&#31243;&#24207;&#30340;&#26550;&#26500;&#65292;&#24182;&#30830;&#23450;&#19981;&#21516;&#30340;&#32452;&#20214;&#65292;&#36825;&#20123;&#32452;&#20214;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#20462;&#25913;&#25152;&#36848;&#24212;&#29992;&#31243;&#24207;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Machine translation is a challenging task due to the inherent complex nature and the fluidity that natural languages bring. Nonetheless, in recent years, it has achieved state-of-the-art performance in several language pairs. Although, a lot of traction can be seen in the areas of multilingual neural machine translation (MNMT) in the recent years, there are no comprehensive survey done to identify what approaches work well. The goal of this project is to investigate the realm of low resource languages and build a Neural Machine Translation model to achieve state-of-the-art results. The project looks to build upon the \texttt{mBART.CC25} \cite{liu2020multilingual} language model and explore strategies to augment it with various NLP and Deep Learning techniques like back translation and transfer learning. This implementation tries to unpack the architecture of the NMT application and determine the different components which offers us opportunities to amend the said application wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#21152;&#36895;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#21592;&#22521;&#35757;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#34394;&#25311;&#27169;&#25311;&#39134;&#34892;&#21592;&#24341;&#25806;&#65292;&#23427;&#33021;&#22815;&#29702;&#35299;&#21475;&#22836;&#36890;&#35759;&#30340;&#24847;&#20041;&#24182;&#36827;&#34892;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2304.07842</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#21592;&#22521;&#35757;&#30340;&#34394;&#25311;&#27169;&#25311;&#39134;&#34892;&#21592;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
A Virtual Simulation-Pilot Agent for Training of Air Traffic Controllers. (arXiv:2304.07842v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#21152;&#36895;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#21592;&#22521;&#35757;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#34394;&#25311;&#27169;&#25311;&#39134;&#34892;&#21592;&#24341;&#25806;&#65292;&#23427;&#33021;&#22815;&#29702;&#35299;&#21475;&#22836;&#36890;&#35759;&#30340;&#24847;&#20041;&#24182;&#36827;&#34892;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#12289;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#34394;&#25311;&#27169;&#25311;&#39134;&#34892;&#21592;&#24341;&#25806;&#65292;&#29992;&#20110;&#21152;&#36895;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#21592;&#65288;ATCo&#65289;&#30340;&#22521;&#35757;&#12290;&#34394;&#25311;&#27169;&#25311;&#39134;&#34892;&#21592;&#24341;&#25806;&#25509;&#25910;&#26469;&#33258;ATCo&#23398;&#21592;&#30340;&#21475;&#22836;&#36890;&#35759;&#65292;&#36827;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;&#29702;&#35299;&#65292;&#19981;&#20165;&#20165;&#26159;&#31616;&#21333;&#30340;&#25991;&#26412;&#36716;&#24405;&#65292;&#36824;&#33021;&#29702;&#35299;&#20854;&#24847;&#20041;&#24182;&#36827;&#34892;&#21709;&#24212;&#12290;&#25972;&#20010;&#27969;&#31243;&#30001;&#20197;&#19979;&#23376;&#27169;&#22359;&#32452;&#25104;&#65306;(i)&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#65292;&#23558;&#38899;&#39057;&#36716;&#25442;&#20026;&#21333;&#35789;&#24207;&#21015;&#65307;(ii)&#39640;&#32423;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#65288;ATC&#65289;&#30456;&#20851;&#23454;&#20307;&#35299;&#26512;&#22120;&#65292;&#29702;&#35299;&#36716;&#24405;&#30340;&#35821;&#38899;&#36890;&#35759;&#65307;(iii)&#25991;&#26412;&#36716;&#35821;&#38899;&#23376;&#27169;&#22359;&#65292;&#26681;&#25454;&#23545;&#35805;&#30340;&#24773;&#22659;&#29983;&#25104;&#31867;&#20284;&#20110;&#39134;&#34892;&#21592;&#30340;&#21475;&#22836;&#35821;&#22238;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we propose a novel virtual simulation-pilot engine for speeding up air traffic controller (ATCo) training by integrating different state-of-the-art artificial intelligence (AI) based tools. The virtual simulation-pilot engine receives spoken communications from ATCo trainees, and it performs automatic speech recognition and understanding. Thus, it goes beyond only transcribing the communication and can also understand its meaning. The output is subsequently sent to a response generator system, which resembles the spoken read back that pilots give to the ATCo trainees. The overall pipeline is composed of the following submodules: (i) automatic speech recognition (ASR) system that transforms audio into a sequence of words; (ii) high-level air traffic control (ATC) related entity parser that understands the transcribed voice communication; and (iii) a text-to-speech submodule that generates a spoken utterance that resembles a pilot based on the situation of the dialogue. Our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#34917;&#19969;&#30340;&#38450;&#24481;&#31574;&#30053;&#65292;&#29992;&#20110;&#24378;&#21270;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#65288;FRS&#65289;&#26469;&#26816;&#27979;&#29289;&#29702;&#25915;&#20987;&#65292;&#35813;&#26041;&#27861;&#22312;&#26816;&#27979;&#30333;&#30418;&#25915;&#20987;&#21644;&#33258;&#36866;&#24212;&#25915;&#20987;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#65292;&#19988;&#31616;&#21333;&#26131;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.07822</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#34917;&#19969;&#30340;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#38450;&#29289;&#29702;&#25915;&#20987;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
A Random-patch based Defense Strategy Against Physical Attacks for Face Recognition Systems. (arXiv:2304.07822v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#34917;&#19969;&#30340;&#38450;&#24481;&#31574;&#30053;&#65292;&#29992;&#20110;&#24378;&#21270;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#65288;FRS&#65289;&#26469;&#26816;&#27979;&#29289;&#29702;&#25915;&#20987;&#65292;&#35813;&#26041;&#27861;&#22312;&#26816;&#27979;&#30333;&#30418;&#25915;&#20987;&#21644;&#33258;&#36866;&#24212;&#25915;&#20987;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#65292;&#19988;&#31616;&#21333;&#26131;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#25915;&#20987;&#34987;&#35270;&#20026;&#30495;&#23454;&#19990;&#30028;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#19968;&#31181;&#23041;&#32961;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#21482;&#36866;&#29992;&#20110;&#23567;&#25200;&#21160;&#25915;&#20987;&#65292;&#24182;&#19988;&#19981;&#33021;&#26377;&#25928;&#26816;&#27979;&#29289;&#29702;&#25915;&#20987;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#34917;&#19969;&#30340;&#38450;&#24481;&#31574;&#30053;&#65292;&#29992;&#20110;&#24378;&#21270;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#65288;FRS&#65289;&#26469;&#26816;&#27979;&#29289;&#29702;&#25915;&#20987;&#12290;&#19981;&#21516;&#20110;&#20027;&#27969;&#38450;&#24481;&#26041;&#27861;&#30528;&#30524;&#20110;&#26500;&#24314;&#22797;&#26434;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20197;&#22312;&#25915;&#20987;&#20013;&#33719;&#24471;&#39640;&#35782;&#21035;&#29575;&#65292;&#25105;&#20204;&#23558;&#22522;&#20110;&#34917;&#19969;&#30340;&#38450;&#24481;&#31574;&#30053;&#24341;&#20837;&#21040;&#26631;&#20934;DNN&#20013;&#65292;&#26088;&#22312;&#33719;&#24471;&#24378;&#22823;&#30340;&#26816;&#27979;&#27169;&#22411;&#12290;&#21033;&#29992;&#25152;&#37319;&#29992;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#38450;&#24481;&#26041;&#27861;&#22312;&#26816;&#27979;&#30333;&#30418;&#25915;&#20987;&#21644;&#25915;&#20987;FRS&#21644;&#38450;&#24481;&#26041;&#27861;&#30340;&#33258;&#36866;&#24212;&#25915;&#20987;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#25105;&#20204;&#26041;&#27861;&#30340;&#31616;&#21333;&#32780;&#24378;&#22823;&#65292;&#23427;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#65292;&#24182;&#25193;&#23637;&#21040;&#20854;&#20182;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The physical attack has been regarded as a kind of threat against real-world computer vision systems. Still, many existing defense methods are only useful for small perturbations attacks and can't detect physical attacks effectively. In this paper, we propose a random-patch based defense strategy to robustly detect physical attacks for Face Recognition System (FRS). Different from mainstream defense methods which focus on building complex deep neural networks (DNN) to achieve high recognition rate on attacks, we introduce a patch based defense strategy to a standard DNN aiming to obtain robust detection models. Extensive experimental results on the employed datasets show the superiority of the proposed defense method on detecting white-box attacks and adaptive attacks which attack both FRS and the defense method. Additionally, due to the simpleness yet robustness of our method, it can be easily applied to the real world face recognition system and extended to other defense methods to b
&lt;/p&gt;</description></item><item><title>VISAR&#26159;&#19968;&#20010;AI&#20889;&#20316;&#21161;&#25163;&#65292;&#26088;&#22312;&#24110;&#21161;&#20316;&#32773;&#25552;&#21319;&#20889;&#20316;&#20307;&#39564;&#21644;&#36755;&#20986;&#12290;&#23427;&#21487;&#20197;&#22312;&#20889;&#20316;&#19978;&#19979;&#25991;&#20013;&#38543;&#26102;&#24110;&#21161;&#20316;&#32773;&#26500;&#24605;&#21644;&#20462;&#25913;&#30446;&#26631;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#32534;&#31243;&#26469;&#32452;&#32455;&#35770;&#35777;&#32467;&#26500;&#65292;&#24182;&#25552;&#20379;&#25512;&#33616;&#26469;&#22686;&#21152;&#35828;&#26381;&#21147;&#12290;&#33258;&#21160;&#33609;&#26696;&#21407;&#22411;&#21487;&#20197;&#29992;&#26469;&#39564;&#35777;&#35745;&#21010;&#12290;</title><link>http://arxiv.org/abs/2304.07810</link><description>&lt;p&gt;
VISAR&#65306;&#19968;&#31181;&#24102;&#26377;&#21487;&#35270;&#21270;&#32534;&#31243;&#21644;&#24555;&#36895;&#33609;&#26696;&#21407;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#35770;&#35777;&#20889;&#20316;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
VISAR: A Human-AI Argumentative Writing Assistant with Visual Programming and Rapid Draft Prototyping. (arXiv:2304.07810v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07810
&lt;/p&gt;
&lt;p&gt;
VISAR&#26159;&#19968;&#20010;AI&#20889;&#20316;&#21161;&#25163;&#65292;&#26088;&#22312;&#24110;&#21161;&#20316;&#32773;&#25552;&#21319;&#20889;&#20316;&#20307;&#39564;&#21644;&#36755;&#20986;&#12290;&#23427;&#21487;&#20197;&#22312;&#20889;&#20316;&#19978;&#19979;&#25991;&#20013;&#38543;&#26102;&#24110;&#21161;&#20316;&#32773;&#26500;&#24605;&#21644;&#20462;&#25913;&#30446;&#26631;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#32534;&#31243;&#26469;&#32452;&#32455;&#35770;&#35777;&#32467;&#26500;&#65292;&#24182;&#25552;&#20379;&#25512;&#33616;&#26469;&#22686;&#21152;&#35828;&#26381;&#21147;&#12290;&#33258;&#21160;&#33609;&#26696;&#21407;&#22411;&#21487;&#20197;&#29992;&#26469;&#39564;&#35777;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36777;&#35770;&#20889;&#20316;&#20013;&#65292;&#20316;&#32773;&#24517;&#39035;&#26500;&#24605;&#20998;&#23618;&#20889;&#20316;&#30446;&#26631;&#65292;&#30830;&#20445;&#20854;&#35770;&#28857;&#30340;&#35828;&#26381;&#21147;&#65292;&#24182;&#36890;&#36807;&#36215;&#33609;&#26469;&#20462;&#35746;&#21644;&#32452;&#32455;&#20182;&#20204;&#30340;&#35745;&#21010;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#20351;&#24471;&#36890;&#36807;&#32842;&#22825;&#30028;&#38754;&#36827;&#34892;&#20132;&#20114;&#24335;&#25991;&#26412;&#29983;&#25104;&#65288;&#20363;&#22914;ChatGPT&#65289;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24120;&#24120;&#24573;&#30053;&#20102;&#38544;&#21547;&#30340;&#20889;&#20316;&#19978;&#19979;&#25991;&#21644;&#29992;&#25143;&#24847;&#22270;&#65292;&#32570;&#20047;&#29992;&#25143;&#25511;&#21046;&#21644;&#33258;&#20027;&#26435;&#65292;&#24182;&#19988;&#25552;&#20379;&#26377;&#38480;&#30340;&#24110;&#21161;&#26469;&#36827;&#34892;&#24847;&#20041;&#26500;&#24314;&#21644;&#20462;&#35746;&#20889;&#20316;&#35745;&#21010;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;VISAR&#65292;&#19968;&#31181;AI&#25903;&#25345;&#30340;&#20889;&#20316;&#21161;&#25163;&#31995;&#32479;&#65292;&#26088;&#22312;&#24110;&#21161;&#20316;&#32773;&#22312;&#20854;&#20889;&#20316;&#19978;&#19979;&#25991;&#20013;&#26500;&#24605;&#21644;&#20462;&#35746;&#20998;&#23618;&#30446;&#26631;&#65292;&#36890;&#36807;&#21516;&#27493;&#25991;&#26412;&#32534;&#36753;&#21644;&#21487;&#35270;&#21270;&#32534;&#31243;&#32452;&#32455;&#35770;&#35777;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#35770;&#35777;&#28779;&#33457;&#25512;&#33616;&#22686;&#24378;&#35828;&#26381;&#21147;&#12290;VISAR&#20801;&#35768;&#29992;&#25143;&#20351;&#29992;&#33258;&#21160;&#33609;&#26696;&#21407;&#22411;&#25506;&#32034;&#12289;&#23454;&#39564;&#21644;&#39564;&#35777;&#20182;&#20204;&#30340;&#20889;&#20316;&#35745;&#21010;&#12290;&#19968;&#20010;&#21463;&#25511;&#23454;&#39564;&#23460;&#30740;&#31350;&#35777;&#23454;&#65292;VISAR&#21487;&#20197;&#36890;&#36807;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#65292;&#26377;&#25928;&#22320;&#25913;&#21892;&#29992;&#25143;&#30340;&#20889;&#20316;&#20307;&#39564;&#21644;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In argumentative writing, writers must brainstorm hierarchical writing goals, ensure the persuasiveness of their arguments, and revise and organize their plans through drafting. Recent advances in large language models (LLMs) have made interactive text generation through a chat interface (e.g., ChatGPT) possible. However, this approach often neglects implicit writing context and user intent, lacks support for user control and autonomy, and provides limited assistance for sensemaking and revising writing plans. To address these challenges, we introduce VISAR, an AI-enabled writing assistant system designed to help writers brainstorm and revise hierarchical goals within their writing context, organize argument structures through synchronized text editing and visual programming, and enhance persuasiveness with argumentation spark recommendations. VISAR allows users to explore, experiment with, and validate their writing plans using automatic draft prototyping. A controlled lab study confi
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#26641;&#21644;&#27169;&#31946;&#36923;&#36753;&#30340;&#26032;&#26041;&#27861;MedFP&#65292;&#29992;&#20110;&#36741;&#21161;&#21307;&#23398;&#23454;&#36341;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#23436;&#20840;&#35299;&#37322;&#65292;&#20801;&#35768;&#20020;&#24202;&#21307;&#29983;&#20135;&#29983;&#12289;&#25511;&#21046;&#21644;&#39564;&#35777;&#25972;&#20010;&#35786;&#26029;&#36807;&#31243;&#65292;&#24182;&#20943;&#23569;&#35823;&#35786;&#29575;&#65292;&#36890;&#36807;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#21644;&#21453;&#20107;&#23454;&#20998;&#26512;&#30340;&#20272;&#35745;&#20540;&#12290;</title><link>http://arxiv.org/abs/2304.07788</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#31946;&#27010;&#29575;&#20915;&#31574;&#26641;&#30340;&#20020;&#24202;&#23454;&#36341;&#36741;&#21161;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Assisting clinical practice with fuzzy probabilistic decision trees. (arXiv:2304.07788v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07788
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#26641;&#21644;&#27169;&#31946;&#36923;&#36753;&#30340;&#26032;&#26041;&#27861;MedFP&#65292;&#29992;&#20110;&#36741;&#21161;&#21307;&#23398;&#23454;&#36341;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#23436;&#20840;&#35299;&#37322;&#65292;&#20801;&#35768;&#20020;&#24202;&#21307;&#29983;&#20135;&#29983;&#12289;&#25511;&#21046;&#21644;&#39564;&#35777;&#25972;&#20010;&#35786;&#26029;&#36807;&#31243;&#65292;&#24182;&#20943;&#23569;&#35823;&#35786;&#29575;&#65292;&#36890;&#36807;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#21644;&#21453;&#20107;&#23454;&#20998;&#26512;&#30340;&#20272;&#35745;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24847;&#35782;&#21040;&#38656;&#35201;&#23436;&#20840;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27169;&#22411;&#65292;&#36825;&#26159;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#19968;&#20010;&#26680;&#24515;&#20027;&#39064;&#12290;&#24403;&#36825;&#20123;&#27169;&#22411;&#26159;&#21487;&#35299;&#37322;&#30340;&#26102;&#65292;&#25509;&#21463;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36741;&#21161;&#25935;&#24863;&#39046;&#22495;&#20915;&#31574;&#30340;&#36235;&#21183;&#23558;&#20250;&#22686;&#38271;&#65292;&#24182;&#19988;&#21363;&#23558;&#20986;&#21488;&#30340;&#27861;&#35268;&#23558;&#20250;&#21152;&#24378;&#23545;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#20542;&#26012;&#12290;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#20999;&#20837;&#28857;&#20043;&#19968;&#26159;&#21307;&#23398;&#23454;&#36341;&#65292;&#23427;&#21487;&#20197;&#21463;&#30410;&#20110;&#31934;&#30830;&#30340;&#20915;&#31574;&#25903;&#25345;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#26412;&#36136;&#19978;&#20250;&#20135;&#29983;&#20449;&#20219;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8212;&#8212;MedFP&#65292;&#23427;&#32467;&#21512;&#20102;&#27010;&#29575;&#26641;&#21644;&#27169;&#31946;&#36923;&#36753;&#26469;&#36741;&#21161;&#20020;&#24202;&#23454;&#36341;&#12290;&#35813;&#26041;&#27861;&#23436;&#20840;&#21487;&#35299;&#37322;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#20020;&#24202;&#21307;&#29983;&#20135;&#29983;&#12289;&#25511;&#21046;&#21644;&#39564;&#35777;&#25972;&#20010;&#35786;&#26029;&#36807;&#31243;&#65307;&#35813;&#26041;&#27861;&#30340;&#19968;&#20010;&#20248;&#28857;&#26159;&#20943;&#23569;&#35823;&#35786;&#29575;&#65292;&#36890;&#36807;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#21644;&#21453;&#20107;&#23454;&#20998;&#26512;&#30340;&#20272;&#35745;&#20540;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20316;&#20026;&#27010;&#24565;&#35777;&#26126;&#24212;&#29992;&#20110;&#20004;&#20010;&#30495;&#23454;&#30340;&#21307;&#23398;&#22330;&#26223;&#20013;&#65306;&#32959;&#30244;&#20998;&#31867;&#21644;&#31958;&#23615;&#30149;&#31867;&#22411;2&#30340;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
The need for fully human-understandable models is increasingly being recognised as a central theme in AI research. The acceptance of AI models to assist in decision making in sensitive domains will grow when these models are interpretable, and this trend towards interpretable models will be amplified by upcoming regulations. One of the killer applications of interpretable AI is medical practice, which can benefit from accurate decision support methodologies that inherently generate trust. In this work, we propose FPT, (MedFP), a novel method that combines probabilistic trees and fuzzy logic to assist clinical practice. This approach is fully interpretable as it allows clinicians to generate, control and verify the entire diagnosis procedure; one of the methodology's strength is the capability to decrease the frequency of misdiagnoses by providing an estimate of uncertainties and counterfactuals. Our approach is applied as a proof-of-concept to two real medical scenarios: classifying ma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#23884;&#20837;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#26032;&#38395;&#25991;&#31456;&#30340;&#26102;&#38388;&#21644;&#20027;&#39064;&#32972;&#26223;&#65292;&#21487;&#26377;&#25928;&#26816;&#27979;&#20551;&#26032;&#38395;&#12290;</title><link>http://arxiv.org/abs/2304.07781</link><description>&lt;p&gt;
&#25991;&#26412;&#23884;&#20837;&#31934;&#20934;&#26816;&#27979;&#20551;&#26032;&#38395;
&lt;/p&gt;
&lt;p&gt;
It's All in the Embedding! Fake News Detection Using Document Embeddings. (arXiv:2304.07781v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#23884;&#20837;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#26032;&#38395;&#25991;&#31456;&#30340;&#26102;&#38388;&#21644;&#20027;&#39064;&#32972;&#26223;&#65292;&#21487;&#26377;&#25928;&#26816;&#27979;&#20551;&#26032;&#38395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23186;&#20307;&#30340;&#25968;&#23383;&#21270;&#36827;&#31243;&#21644;&#31038;&#20132;&#23186;&#20307;&#30340;&#20852;&#36215;&#65292;&#20010;&#24615;&#21270;&#31038;&#20132;&#23186;&#20307;&#24050;&#25104;&#20026;&#26032;&#30340;&#24120;&#24577;&#12290;&#28982;&#32780;&#65292;&#25968;&#23383;&#21270;&#36827;&#31243;&#22686;&#21152;&#20102;&#27969;&#20256;&#20551;&#20449;&#24687;&#12289;&#38169;&#35823;&#20449;&#24687;&#21644;&#21464;&#24418;&#20449;&#24687;&#30340;&#39118;&#38505;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#20551;&#26032;&#38395;&#36825;&#19968;&#26377;&#23475;&#29616;&#35937;&#30340;&#20986;&#29616;&#12290;&#36825;&#20123;&#20449;&#24687;&#21487;&#20197;&#25197;&#26354;&#20844;&#20247;&#23545;&#29305;&#23450;&#35805;&#39064;&#30340;&#30475;&#27861;&#65292;&#24182;&#19988;&#32570;&#20047;&#20256;&#32479;&#26032;&#38395;&#30340;&#20005;&#35880;&#24615;&#12290;&#20026;&#20102;&#24320;&#21457;&#26377;&#25928;&#30340;&#24037;&#20855;&#26469;&#26816;&#27979;&#20551;&#26032;&#38395;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#23884;&#20837;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#26032;&#38395;&#25991;&#31456;&#30340;&#26102;&#38388;&#21644;&#20027;&#39064;&#32972;&#26223;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#22343;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24230;&#65292;&#24182;&#19988;&#36229;&#36234;&#20102;&#22522;&#32447;&#27169;&#22411;&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the current shift in the mass media landscape from journalistic rigor to social media, personalized social media is becoming the new norm. Although the digitalization progress of the media brings many advantages, it also increases the risk of spreading disinformation, misinformation, and malformation through the use of fake news. The emergence of this harmful phenomenon has managed to polarize society and manipulate public opinion on particular topics, e.g., elections, vaccinations, etc. Such information propagated on social media can distort public perceptions and generate social unrest while lacking the rigor of traditional journalism. Natural Language Processing and Machine Learning techniques are essential for developing efficient tools that can detect fake news. Models that use the context of textual data are essential for resolving the fake news detection problem, as they manage to encode linguistic features within the vector representation of words. In this paper, we propos
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#20307;&#31995;&#32467;&#26500;MisRoB{\AE}RTa&#65292;&#29992;&#20110;&#19981;&#23454;&#20449;&#24687;&#26816;&#27979;&#65292;&#24182;&#22312;&#22823;&#22411;&#29616;&#23454;&#19990;&#30028;&#26032;&#38395;&#25991;&#31456;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#21644;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2304.07759</link><description>&lt;p&gt;
MisRoB{\AE}RTa&#65306;&#21464;&#24418;&#37329;&#21018;&#23545;&#25239;&#19981;&#23454;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
MisRoB{\AE}RTa: Transformers versus Misinformation. (arXiv:2304.07759v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#20307;&#31995;&#32467;&#26500;MisRoB{\AE}RTa&#65292;&#29992;&#20110;&#19981;&#23454;&#20449;&#24687;&#26816;&#27979;&#65292;&#24182;&#22312;&#22823;&#22411;&#29616;&#23454;&#19990;&#30028;&#26032;&#38395;&#25991;&#31456;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#23454;&#20449;&#24687;&#34987;&#35748;&#20026;&#26159;&#25105;&#20204;&#27665;&#20027;&#30340;&#20215;&#20540;&#35266;&#21644;&#21407;&#21017;&#30340;&#23041;&#32961;&#12290;&#36825;&#31181;&#20869;&#23481;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20256;&#25773;&#20250;&#20351;&#31038;&#20250;&#26497;&#31471;&#21270;&#65292;&#24182;&#36890;&#36807;&#25197;&#26354;&#20844;&#20247;&#30340;&#30475;&#27861;&#24182;&#24341;&#21457;&#31038;&#20250;&#21160;&#33633;&#32780;&#30772;&#22351;&#20844;&#20849;&#35805;&#35821;&#65292;&#21516;&#26102;&#32570;&#20047;&#20256;&#32479;&#26032;&#38395;&#30340;&#20005;&#35880;&#24615;&#12290;&#22312;&#22810;&#20010;&#20855;&#26377;&#30693;&#21517;&#24230;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;Transformer&#21644;&#36801;&#31227;&#23398;&#20064;&#34987;&#35777;&#26126;&#26159;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MisRoB{\AE}RTa&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#20307;&#31995;&#32467;&#26500;&#65292;&#29992;&#20110;&#19981;&#23454;&#20449;&#24687;&#26816;&#27979;&#12290;MisRoB{\AE}RTa&#21033;&#29992;&#20102;&#20004;&#20010;Transformer&#65288;BART&#21644;RoBERTa&#65289;&#26469;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#23545;&#22810;&#20010;Transformer&#22312;&#19981;&#23454;&#20449;&#24687;&#26816;&#27979;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#21644;&#35780;&#20272;&#12290;&#23545;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#24102;&#26377;10&#20010;&#31867;&#21035;&#26631;&#31614;&#30340;&#22823;&#22411;&#29616;&#23454;&#19990;&#30028;&#26032;&#38395;&#25991;&#31456;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#30740;&#31350;&#20013;&#30340;&#20004;&#20010;&#32570;&#28857;&#65306;&#23558;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#20174;&#23567;&#21040;&#22823;&#65292;&#24182;&#23558;&#28966;&#28857;&#20174;&#31038;&#20132;&#23186;&#20307;&#31227;&#21160;&#21040;&#26032;&#38395;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Misinformation is considered a threat to our democratic values and principles. The spread of such content on social media polarizes society and undermines public discourse by distorting public perceptions and generating social unrest while lacking the rigor of traditional journalism. Transformers and transfer learning proved to be state-of-the-art methods for multiple well-known natural language processing tasks. In this paper, we propose MisRoB{\AE}RTa, a novel transformer-based deep neural ensemble architecture for misinformation detection. MisRoB{\AE}RTa takes advantage of two transformers (BART \&amp; RoBERTa) to improve the classification performance. We also benchmarked and evaluated the performances of multiple transformers on the task of misinformation detection. For training and testing, we used a large real-world news articles dataset labeled with 10 classes, addressing two shortcomings in the current research: increasing the size of the dataset from small to large, and moving th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;Transformer&#30340;LiDAR-&#24815;&#24615;&#34701;&#21512;&#37324;&#31243;&#35745;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#22836;&#27880;&#24847;&#21147;&#34701;&#21512;&#27169;&#22359;&#23637;&#31034;&#20102;&#19981;&#21516;&#34701;&#21512;&#26041;&#27861;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;KITTI&#21644;EuRoC&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.07728</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;Transformer&#30340;LiDAR-&#24815;&#24615;&#34701;&#21512;&#37324;&#31243;&#35745;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
TransFusionOdom: Interpretable Transformer-based LiDAR-Inertial Fusion Odometry Estimation. (arXiv:2304.07728v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;Transformer&#30340;LiDAR-&#24815;&#24615;&#34701;&#21512;&#37324;&#31243;&#35745;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#22836;&#27880;&#24847;&#21147;&#34701;&#21512;&#27169;&#22359;&#23637;&#31034;&#20102;&#19981;&#21516;&#34701;&#21512;&#26041;&#27861;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;KITTI&#21644;EuRoC&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#24378;&#37324;&#31243;&#35745;&#20272;&#35745;&#24615;&#33021;&#30340;&#24120;&#29992;&#26041;&#27861;&#26159;&#22810;&#27169;&#24577;&#20256;&#24863;&#22120;&#34701;&#21512;&#65292;&#36825;&#20063;&#26159;&#31227;&#21160;&#26426;&#22120;&#20154;&#30340;&#22522;&#26412;&#27169;&#22359;&#12290;&#28982;&#32780;&#65292;&#22312;&#30417;&#30563;&#24335;&#20256;&#24863;&#22120;&#34701;&#21512;&#37324;&#31243;&#35745;&#20272;&#35745;&#20219;&#21153;&#20013;&#65292;&#22914;&#20309;&#22312;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#25191;&#34892;&#34701;&#21512;&#20173;&#28982;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#19968;&#20123;&#31616;&#21333;&#30340;&#25805;&#20316;&#65292;&#22914;&#36880;&#20803;&#32032;&#27714;&#21644;&#21644;&#36830;&#25509;&#65292;&#24182;&#19981;&#20855;&#22791;&#20998;&#37197;&#33258;&#36866;&#24212;&#20851;&#27880;&#26435;&#37325;&#20197;&#26377;&#25928;&#21512;&#24182;&#19981;&#21516;&#27169;&#24577;&#30340;&#33021;&#21147;&#65292;&#36825;&#20351;&#24471;&#33719;&#24471;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#37324;&#31243;&#35745;&#32467;&#26524;&#21464;&#24471;&#22256;&#38590;&#12290;&#26368;&#36817;&#65292;Transformer&#26550;&#26500;&#26174;&#31034;&#20986;&#22312;&#35270;&#35273;&#19982;&#35821;&#35328;&#39046;&#22495;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#20219;&#21153;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31471;&#21040;&#31471;&#30340;&#28608;&#20809;&#38647;&#36798;-&#24815;&#24615;&#34701;&#21512;&#26694;&#26550;(&#21363;TransFusionOdom)&#26469;&#36827;&#34892;&#37324;&#31243;&#35745;&#20272;&#35745;&#12290;&#22810;&#22836;&#27880;&#24847;&#21147;&#34701;&#21512;&#27169;&#22359;&#23637;&#31034;&#20102;&#21516;&#26500;&#21644;&#24322;&#26500;&#27169;&#24577;&#30340;&#19981;&#21516;&#34701;&#21512;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;TransFusionOdom&#27169;&#22411;&#22312;KITTI&#21644;EuRoC&#25968;&#25454;&#38598;&#19978;&#26174;&#31034;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal fusion of sensors is a commonly used approach to enhance the performance of odometry estimation, which is also a fundamental module for mobile robots. However, the question of \textit{how to perform fusion among different modalities in a supervised sensor fusion odometry estimation task?} is still one of challenging issues remains. Some simple operations, such as element-wise summation and concatenation, are not capable of assigning adaptive attentional weights to incorporate different modalities efficiently, which make it difficult to achieve competitive odometry results. Recently, the Transformer architecture has shown potential for multi-modal fusion tasks, particularly in the domains of vision with language. In this work, we propose an end-to-end supervised Transformer-based LiDAR-Inertial fusion framework (namely TransFusionOdom) for odometry estimation. The multi-attention fusion module demonstrates different fusion approaches for homogeneous and heterogeneous modalit
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;Benchmark for Evaluating the Robustness of GNNs to Topological Changes (BREC)&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;BREC&#35780;&#20272;&#20102;&#20960;&#31181;&#29616;&#26377;&#30340;GNN&#27169;&#22411;&#30340;&#34920;&#36798;&#21147;&#65292;&#34920;&#26126;&#19968;&#20123;&#27169;&#22411;&#22312;&#20197;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;BREC&#19978;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#31361;&#26174;&#20102;&#38656;&#35201;&#26356;&#22909;&#30340;GNN&#34920;&#29616;&#21147;&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.07702</link><description>&lt;p&gt;
&#29992;BREC&#25968;&#25454;&#38598;&#26356;&#22909;&#22320;&#35780;&#20272;GNN&#34920;&#36798;&#21147;
&lt;/p&gt;
&lt;p&gt;
Towards Better Evaluation of GNN Expressiveness with BREC Dataset. (arXiv:2304.07702v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;Benchmark for Evaluating the Robustness of GNNs to Topological Changes (BREC)&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;BREC&#35780;&#20272;&#20102;&#20960;&#31181;&#29616;&#26377;&#30340;GNN&#27169;&#22411;&#30340;&#34920;&#36798;&#21147;&#65292;&#34920;&#26126;&#19968;&#20123;&#27169;&#22411;&#22312;&#20197;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;BREC&#19978;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#31361;&#26174;&#20102;&#38656;&#35201;&#26356;&#22909;&#30340;GNN&#34920;&#29616;&#21147;&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#29702;&#35770;&#34920;&#36798;&#21147;&#30340;&#30740;&#31350;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#35768;&#22810;&#22686;&#24378;&#34920;&#36798;&#21147;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#20005;&#26684;&#36981;&#24490;k&#32500;Weisfeiler-Lehman&#65288;k-WL&#65289;&#27979;&#35797;&#23618;&#27425;&#32467;&#26500;&#30340;&#23569;&#25968;&#26041;&#27861;&#22806;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#27809;&#26377;&#32479;&#19968;&#30340;&#34920;&#36798;&#21147;&#24230;&#37327;&#12290;&#23427;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#36890;&#24120;&#38480;&#20110;&#21306;&#20998;&#26576;&#20123;&#38750;&#21516;&#26500;&#22270;&#26063;&#65292;&#23548;&#33268;&#22312;&#23450;&#37327;&#27604;&#36739;&#34920;&#36798;&#21147;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#19982;&#29702;&#35770;&#20998;&#26512;&#30456;&#21453;&#65292;&#34913;&#37327;&#34920;&#36798;&#33021;&#21147;&#30340;&#21478;&#19968;&#31181;&#26041;&#27861;&#26159;&#22312;&#21253;&#21547;1-WL&#19981;&#21487;&#21306;&#20998;&#22270;&#30340;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#27492;&#30446;&#30340;&#30340;&#25968;&#25454;&#38598;&#38754;&#20020;&#30528;&#38590;&#24230;&#65288;&#20219;&#20309;&#36229;&#36234;1-WL&#30340;&#27169;&#22411;&#20934;&#30830;&#29575;&#20960;&#20046;&#36798;&#21040;100&#65285;&#65289;&#12289;&#31890;&#24230;&#65288;&#27169;&#22411;&#20542;&#21521;&#20110;&#35201;&#20040;&#23436;&#20840;&#27491;&#30830;&#65292;&#35201;&#20040;&#25509;&#36817;&#38543;&#26426;&#29468;&#27979;&#65289;&#21644;&#35268;&#27169;&#65288;&#27599;&#20010;&#25968;&#25454;&#38598;&#20013;&#20165;&#26377;&#23569;&#37327;&#26412;&#36136;&#19981;&#21516;&#30340;&#22270;&#65289;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#21463;&#38480;&#21046;&#30340;&#35780;&#20272;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;GNN&#40065;&#26834;&#24615;&#35780;&#20272;&#22522;&#20934;&#65288;BREC&#65289;&#65292;&#35813;&#22522;&#20934;&#21253;&#21547;&#35768;&#22810;&#32467;&#26500;&#22810;&#26679;&#30340;&#22270;&#65292;&#24182;&#20801;&#35768;&#23545;&#27169;&#22411;&#34920;&#36798;&#21147;&#36827;&#34892;&#26356;&#31934;&#32454;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#20351;&#29992;BREC&#35780;&#20272;&#20102;&#20960;&#31181;&#29616;&#26377;&#30340;GNN&#27169;&#22411;&#30340;&#34920;&#36798;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20123;&#27169;&#22411;&#22312;&#20197;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;BREC&#19978;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#31361;&#26174;&#20102;&#38656;&#35201;&#26356;&#22909;&#30340;GNN&#34920;&#29616;&#21147;&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research on the theoretical expressiveness of Graph Neural Networks (GNNs) has developed rapidly, and many methods have been proposed to enhance the expressiveness. However, most methods do not have a uniform expressiveness measure except for a few that strictly follow the $k$-dimensional Weisfeiler-Lehman ($k$-WL) test hierarchy. Their theoretical analyses are often limited to distinguishing certain families of non-isomorphic graphs, leading to difficulties in quantitatively comparing their expressiveness. In contrast to theoretical analysis, another way to measure expressiveness is by evaluating model performance on certain datasets containing 1-WL-indistinguishable graphs. Previous datasets specifically designed for this purpose, however, face problems with difficulty (any model surpassing 1-WL has nearly 100% accuracy), granularity (models tend to be either 100% correct or near random guess), and scale (only a few essentially different graphs in each dataset). To address these limi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;Deep Metric Learning&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32463;&#39564;Bregman&#25955;&#24230;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#36827;&#34892;&#19981;&#30830;&#23450;&#36317;&#31163;&#34920;&#31034;&#65292;&#33021;&#22815;&#26377;&#25928;&#30340;&#22312;&#27169;&#24335;&#35782;&#21035;&#21644;&#32858;&#31867;&#20219;&#21153;&#19978;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.07689</link><description>&lt;p&gt;
&#23398;&#20064;&#32463;&#39564;Bregman&#25955;&#24230;&#29992;&#20110;&#19981;&#30830;&#23450;&#36317;&#31163;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Empirical Bregman Divergence for Uncertain Distance Representation. (arXiv:2304.07689v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;Deep Metric Learning&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32463;&#39564;Bregman&#25955;&#24230;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#36827;&#34892;&#19981;&#30830;&#23450;&#36317;&#31163;&#34920;&#31034;&#65292;&#33021;&#22815;&#26377;&#25928;&#30340;&#22312;&#27169;&#24335;&#35782;&#21035;&#21644;&#32858;&#31867;&#20219;&#21153;&#19978;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#25216;&#26415;&#24050;&#24212;&#29992;&#20110;&#21508;&#31181;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#36890;&#36807;&#28145;&#24230;&#32593;&#32476;&#23398;&#20064;&#26679;&#26412;&#23884;&#20837;&#26469;&#36827;&#34892;&#35270;&#35273;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#32463;&#20856;&#26041;&#27861;&#37319;&#29992;&#22266;&#23450;&#36317;&#31163;&#24230;&#37327;&#20316;&#20026;&#20004;&#20010;&#23884;&#20837;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#20989;&#25968;&#65292;&#21487;&#33021;&#23548;&#33268;&#25429;&#25417;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#30340;&#20122;&#26368;&#20248;&#24615;&#33021;&#12290;Bregman&#25955;&#24230;&#27010;&#25324;&#20102;&#21508;&#31181;&#36317;&#31163;&#24230;&#37327;&#30340;&#24230;&#37327;&#65292;&#24182;&#22312;&#35768;&#22810;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#39046;&#22495;&#20013;&#20135;&#29983;&#12290;&#26412;&#25991;&#39318;&#20808;&#23637;&#31034;&#20102;&#22914;&#20309;&#20174;Bregman&#25955;&#24230;&#33719;&#24471;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#25439;&#22833;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#32463;&#39564;Bregman&#25955;&#24230;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#35774;&#32622;&#23545;Bregman&#25955;&#24230;&#19979;&#30340;&#20984;&#20989;&#25968;&#36827;&#34892;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20854;&#20182;SOTA&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20116;&#20010;&#27969;&#34892;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#22312;&#27169;&#24335;&#35782;&#21035;&#21644;&#32858;&#31867;&#20219;&#21153;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep metric learning techniques have been used for visual representation in various supervised and unsupervised learning tasks through learning embeddings of samples with deep networks. However, classic approaches, which employ a fixed distance metric as a similarity function between two embeddings, may lead to suboptimal performance for capturing the complex data distribution. The Bregman divergence generalizes measures of various distance metrics and arises throughout many fields of deep metric learning. In this paper, we first show how deep metric learning loss can arise from the Bregman divergence. We then introduce a novel method for learning empirical Bregman divergence directly from data based on parameterizing the convex function underlying the Bregman divergence with a deep learning setting. We further experimentally show that our approach performs effectively on five popular public datasets compared to other SOTA deep metric learning methods, particularly for pattern recognit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20026;IoT&#21307;&#30103;&#20445;&#20581;&#24212;&#29992;&#25552;&#20379;&#23433;&#20840;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#23433;&#20840;&#30340;&#27169;&#22411;&#32858;&#21512;&#65292;&#21516;&#26102;&#19981;&#20849;&#20139;&#25935;&#24863;&#24739;&#32773;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2304.07668</link><description>&lt;p&gt;
FedBlockHealth&#65306;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#21644;&#21306;&#22359;&#38142;&#30340;IoT&#21307;&#30103;&#20445;&#20581;&#38544;&#31169;&#21644;&#23433;&#20840;&#21327;&#21516;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FedBlockHealth: A Synergistic Approach to Privacy and Security in IoT-Enabled Healthcare through Federated Learning and Blockchain. (arXiv:2304.07668v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20026;IoT&#21307;&#30103;&#20445;&#20581;&#24212;&#29992;&#25552;&#20379;&#23433;&#20840;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#23433;&#20840;&#30340;&#27169;&#22411;&#32858;&#21512;&#65292;&#21516;&#26102;&#19981;&#20849;&#20139;&#25935;&#24863;&#24739;&#32773;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#24191;&#27867;&#24212;&#29992;&#32473;&#25968;&#25454;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#24739;&#32773;&#23433;&#20840;&#24102;&#26469;&#26032;&#30340;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#26041;&#27861;&#38656;&#35201;&#20445;&#35777;&#23433;&#20840;&#21644;&#38544;&#31169;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#32852;&#37030;&#23398;&#20064;&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#65292;&#20026;IoT&#21307;&#30103;&#20445;&#20581;&#24212;&#29992;&#25552;&#20379;&#23433;&#20840;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20844;&#38053;&#21152;&#23494;&#31995;&#32479;&#20026;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#25552;&#20379;&#35821;&#20041;&#23433;&#20840;&#65292;&#32780;&#21306;&#22359;&#38142;&#25216;&#26415;&#30830;&#20445;&#36825;&#20123;&#26356;&#26032;&#30340;&#23436;&#25972;&#24615;&#24182;&#24378;&#21046;&#35775;&#38382;&#25511;&#21046;&#21644;&#38382;&#36131;&#21046;&#12290;&#32852;&#37030;&#23398;&#20064;&#36807;&#31243;&#23454;&#29616;&#20102;&#23433;&#20840;&#30340;&#27169;&#22411;&#32858;&#21512;&#65292;&#21516;&#26102;&#19981;&#20849;&#20139;&#25935;&#24863;&#24739;&#32773;&#25968;&#25454;&#12290;&#25105;&#20204;&#20351;&#29992;EMNIST&#25968;&#25454;&#38598;&#23454;&#29616;&#21644;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#22312;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#65292;&#26377;&#25928;&#22320;&#20445;&#25252;&#20102;&#25968;&#25454;&#38544;&#31169;&#21644;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid adoption of Internet of Things (IoT) devices in healthcare has introduced new challenges in preserving data privacy, security and patient safety. Traditional approaches need to ensure security and privacy while maintaining computational efficiency, particularly for resource-constrained IoT devices. This paper proposes a novel hybrid approach combining federated learning and blockchain technology to provide a secure and privacy-preserved solution for IoT-enabled healthcare applications. Our approach leverages a public-key cryptosystem that provides semantic security for local model updates, while blockchain technology ensures the integrity of these updates and enforces access control and accountability. The federated learning process enables a secure model aggregation without sharing sensitive patient data. We implement and evaluate our proposed framework using EMNIST datasets, demonstrating its effectiveness in preserving data privacy and security while maintaining computatio
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; EEGSN &#30340;&#22270;&#24418;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#26550;&#26500;&#65292;&#38754;&#21521;&#22810;&#36890;&#36947; EEG &#20998;&#31867;&#20219;&#21153;&#65292;&#22312;&#23398;&#20064;&#20998;&#24067;&#24335; EEG &#20256;&#24863;&#22120;&#20013;&#30340;&#21160;&#24577;&#20851;&#31995;&#20449;&#24687;&#30340;&#21516;&#26102;&#65292;&#23558;&#25512;&#26029;&#35745;&#31639;&#22797;&#26434;&#24230;&#38477;&#20302;&#20102;20&#20493;&#65292;&#20026;&#20302;&#24310;&#36831;&#21644;&#21151;&#32791;&#25928;&#29575;&#30340;&#33041;&#35745;&#31639;&#26426;&#25509;&#21475;&#30340;&#24320;&#21457;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2304.07655</link><description>&lt;p&gt;
EEG SN&#65306;&#38754;&#21521; EEG &#30340;&#22270;&#24418;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#20302;&#24310;&#36831;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
EEGSN: Towards Efficient Low-latency Decoding of EEG with Graph Spiking Neural Networks. (arXiv:2304.07655v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07655
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; EEGSN &#30340;&#22270;&#24418;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#26550;&#26500;&#65292;&#38754;&#21521;&#22810;&#36890;&#36947; EEG &#20998;&#31867;&#20219;&#21153;&#65292;&#22312;&#23398;&#20064;&#20998;&#24067;&#24335; EEG &#20256;&#24863;&#22120;&#20013;&#30340;&#21160;&#24577;&#20851;&#31995;&#20449;&#24687;&#30340;&#21516;&#26102;&#65292;&#23558;&#25512;&#26029;&#35745;&#31639;&#22797;&#26434;&#24230;&#38477;&#20302;&#20102;20&#20493;&#65292;&#20026;&#20302;&#24310;&#36831;&#21644;&#21151;&#32791;&#25928;&#29575;&#30340;&#33041;&#35745;&#31639;&#26426;&#25509;&#21475;&#30340;&#24320;&#21457;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22823;&#22810;&#25968;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#30340;&#35757;&#32451;&#20381;&#36182;&#20110;&#24402;&#32435;&#20559;&#24046;&#65292;&#36825;&#24182;&#19981;&#19968;&#23450;&#36866;&#29992;&#20110;&#22810;&#20010;&#38656;&#35201;&#20302;&#24310;&#36831;&#21644;&#21151;&#32791;&#25928;&#29575;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290; &#22522;&#20110;&#30456;&#20851;&#30340;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#25512;&#26029;&#22823;&#33041;&#34892;&#20026;&#23601;&#26159;&#19968;&#20010;&#36825;&#26679;&#30340;&#20363;&#23376;&#65292;&#23398;&#20064;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#20250;&#20005;&#37325;&#24433;&#21709;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#25928;&#29575;&#12290;&#30446;&#21069;&#65292;SNN&#20165;&#20165;&#20381;&#38752;&#19968;&#33324;&#24402;&#32435;&#20559;&#24046;&#26469;&#27169;&#25311;&#19981;&#21516;&#25968;&#25454;&#27969;&#20043;&#38388;&#30340;&#21160;&#24577;&#20851;&#31995;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#36890;&#36947; EEG &#20998;&#31867;&#30340;&#22270;&#24418;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65288;EEGSN&#65289;&#65292;&#23427;&#33021;&#22815;&#23398;&#20064;&#20998;&#24067;&#22312; EEG &#20256;&#24863;&#22120;&#20013;&#30340;&#21160;&#24577;&#20851;&#31995;&#20449;&#24687;&#12290;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#25512;&#26029;&#35745;&#31639;&#22797;&#26434;&#24230;&#38477;&#20302;&#20102;20&#20493;&#65292;&#21516;&#26102;&#22312;&#36816;&#21160;&#25191;&#34892;&#20998;&#31867;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#21487;&#27604;&#36739;&#30340;&#20934;&#30830;&#24615;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#21487;&#35299;&#37322;&#21644;&#39640;&#25928;&#35757;&#32451; EEG &#25968;&#25454;&#30340;&#22270;&#24418;SNN&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20302;&#24310;&#36831;&#21644;&#21151;&#32791;&#25928;&#29575;&#30340;&#33041;-&#35745;&#31639;&#26426;&#30028;&#38754;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
A vast majority of spiking neural networks (SNNs) are trained based on inductive biases that are not necessarily a good fit for several critical tasks that require low-latency and power efficiency. Inferring brain behavior based on the associated electroenchephalography (EEG) signals is an example of how networks training and inference efficiency can be heavily impacted by learning spatio-temporal dependencies. Up to now, SNNs rely solely on general inductive biases to model the dynamic relations between different data streams. Here, we propose a graph spiking neural network architecture for multi-channel EEG classification (EEGSN) that learns the dynamic relational information present in the distributed EEG sensors. Our method reduced the inference computational complexity by $\times 20$ compared to the state-of-the-art SNNs, while achieved comparable accuracy on motor execution classification tasks. Overall, our work provides a framework for interpretable and efficient training of gr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#24403;&#21069;&#36229;&#32593;&#32476;&#35757;&#32451;&#31574;&#30053;&#19981;&#31283;&#23450;&#12289;&#25910;&#25947;&#36895;&#24230;&#24930;&#30340;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#27604;&#20363;&#21152;&#24615;&#21442;&#25968;&#21270;&#30340;&#26041;&#24335;&#26469;&#20462;&#35746;&#36229;&#32593;&#32476;&#24418;&#24335;&#65292;&#23454;&#29616;&#26356;&#21152;&#31283;&#23450;&#21644;&#24555;&#36895;&#30340;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2304.07645</link><description>&lt;p&gt;
&#38024;&#23545;&#31283;&#23450;&#30340;&#36229;&#32593;&#32476;&#23398;&#20064;&#30340;&#38750;&#27604;&#20363;&#21442;&#25968;&#21270;
&lt;/p&gt;
&lt;p&gt;
Non-Proportional Parametrizations for Stable Hypernetwork Learning. (arXiv:2304.07645v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#24403;&#21069;&#36229;&#32593;&#32476;&#35757;&#32451;&#31574;&#30053;&#19981;&#31283;&#23450;&#12289;&#25910;&#25947;&#36895;&#24230;&#24930;&#30340;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#27604;&#20363;&#21152;&#24615;&#21442;&#25968;&#21270;&#30340;&#26041;&#24335;&#26469;&#20462;&#35746;&#36229;&#32593;&#32476;&#24418;&#24335;&#65292;&#23454;&#29616;&#26356;&#21152;&#31283;&#23450;&#21644;&#24555;&#36895;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#32593;&#32476;&#26159;&#29983;&#25104;&#21478;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#24403;&#21069;&#30340;&#36229;&#32593;&#32476;&#35757;&#32451;&#31574;&#30053;&#26159;&#19981;&#31283;&#23450;&#30340;&#65292;&#25910;&#25947;&#36895;&#24230;&#36890;&#24120;&#27604;&#38750;&#36229;&#32593;&#32476;&#27169;&#22411;&#24930;&#24471;&#22810;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#38382;&#39064;&#19982;&#20351;&#29992;&#24120;&#35265;&#30340;&#36229;&#32593;&#32476;&#26550;&#26500;&#21644;&#21021;&#22987;&#21270;&#26102;&#20986;&#29616;&#30340;&#38382;&#39064;&#26377;&#20851;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#21644;&#23454;&#39564;&#19978;&#35777;&#26126;&#20102;&#36825;&#31181;&#25968;&#20540;&#38382;&#39064;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20250;&#23548;&#33268;&#19981;&#31283;&#23450;&#24615;&#65292;&#20174;&#32780;&#38477;&#20302;&#29978;&#33267;&#38459;&#27490;&#25910;&#25947;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#24402;&#19968;&#21270;&#31574;&#30053;&#26080;&#27861;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20462;&#35746;&#30340;&#36229;&#32593;&#32476;&#24418;&#24335;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#36229;&#32593;&#32476;&#20351;&#29992;&#38750;&#27604;&#20363;&#21152;&#24615;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#20219;&#21153;&#19978;&#27979;&#35797;&#20102;&#25152;&#25552;&#20986;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#65292;&#24182;&#35777;&#26126;&#23427;&#22987;&#32456;&#21487;&#20197;&#23548;&#33268;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#65292;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypernetworks are neural networks that generate the parameters of another neural network. In many scenarios, current hypernetwork training strategies are unstable, and convergence is often far slower than for non-hypernetwork models. We show that this problem is linked to an issue that arises when using common choices of hypernetwork architecture and initialization. We demonstrate analytically and experimentally how this numerical issue can lead to an instability during training that slows, and sometimes even prevents, convergence. We also demonstrate that popular deep learning normalization strategies fail to address these issues. We then propose a solution to the problem based on a revised hypernetwork formulation that uses non-proportional additive parametrizations. We test the proposed reparametrization on several tasks, and demonstrate that it consistently leads to more stable training, achieving faster convergence.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#20272;&#35745;N2&#22686;&#27833;&#36807;&#31243;&#20013;&#30340;&#26368;&#23567;&#30456;&#28342;&#21387;&#21147;&#65292;&#34920;&#26126;&#26412;&#30740;&#31350;&#24320;&#21457;&#30340;&#39044;&#27979;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07617</link><description>&lt;p&gt;
&#22522;&#20110;&#32431;&#20928;/&#19981;&#32431;&#30340;N2&#22686;&#27833;&#36807;&#31243;&#20013;&#26368;&#23567;&#30456;&#28342;&#21387;&#21147;&#65288;MMP&#65289;&#30340;&#20272;&#35745;&#65306;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#27604;&#36739;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimation of minimum miscibility pressure (MMP) in impure/pure N2 based enhanced oil recovery process: A comparative study of statistical and machine learning algorithms. (arXiv:2304.07617v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#20272;&#35745;N2&#22686;&#27833;&#36807;&#31243;&#20013;&#30340;&#26368;&#23567;&#30456;&#28342;&#21387;&#21147;&#65292;&#34920;&#26126;&#26412;&#30740;&#31350;&#24320;&#21457;&#30340;&#39044;&#27979;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#23567;&#30456;&#28342;&#21387;&#21147;&#65288;MMP&#65289;&#30340;&#39044;&#27979;&#22312;&#35774;&#35745;&#21644;&#25805;&#20316;&#22522;&#20110;&#27694;&#30340;&#22686;&#27833;&#36807;&#31243;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#25991;&#36827;&#34892;&#20102;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;MMP&#20272;&#35745;&#30340;&#27604;&#36739;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#30340;&#22823;&#22810;&#25968;&#39044;&#27979;&#27169;&#22411;&#26174;&#31034;&#20986;&#27604;&#25991;&#29486;&#20013;&#25253;&#21578;&#30340;&#30456;&#20851;&#21644;&#39044;&#27979;&#27169;&#22411;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Minimum miscibility pressure (MMP) prediction plays an important role in design and operation of nitrogen based enhanced oil recovery processes. In this work, a comparative study of statistical and machine learning methods used for MMP estimation is carried out. Most of the predictive models developed in this study exhibited superior performance over correlation and predictive models reported in literature.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24494;&#29615;&#35856;&#25391;&#22120;&#30340;&#22810;&#24577;&#30005;&#20809;&#30005;&#36335;&#19982;&#26550;&#26500;&#65292;&#21487;&#29992;&#20110;&#39640;&#36895;&#12289;&#33410;&#33021;&#30340;&#38750;&#20108;&#36827;&#21046;&#21487;&#37325;&#26500;&#35745;&#31639;&#65292;&#24182;&#33021;&#22788;&#29702;&#22810;&#31181;&#25968;&#25454;&#26684;&#24335;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2304.07608</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#24577;&#30005;&#20809;&#30005;&#36335;&#19982;&#26550;&#26500;&#30340;&#39640;&#36895;&#12289;&#33410;&#33021;&#30340;&#38750;&#20108;&#36827;&#21046;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
High-Speed and Energy-Efficient Non-Binary Computing with Polymorphic Electro-Optic Circuits and Architectures. (arXiv:2304.07608v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24494;&#29615;&#35856;&#25391;&#22120;&#30340;&#22810;&#24577;&#30005;&#20809;&#30005;&#36335;&#19982;&#26550;&#26500;&#65292;&#21487;&#29992;&#20110;&#39640;&#36895;&#12289;&#33410;&#33021;&#30340;&#38750;&#20108;&#36827;&#21046;&#21487;&#37325;&#26500;&#35745;&#31639;&#65292;&#24182;&#33021;&#22788;&#29702;&#22810;&#31181;&#25968;&#25454;&#26684;&#24335;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#24494;&#29615;&#35856;&#25391;&#22120;&#30340;&#22810;&#24577;&#30005;&#20809;&#30005;&#36335;&#19982;&#26550;&#26500;&#65292;&#21487;&#29992;&#20110;&#39640;&#36895;&#12289;&#33410;&#33021;&#30340;&#38750;&#20108;&#36827;&#21046;&#21487;&#37325;&#26500;&#35745;&#31639;&#12290;&#25105;&#20204;&#30340;&#22810;&#24577;&#30005;&#20809;&#30005;&#36335;&#21487;&#20197;&#21160;&#24577;&#32534;&#31243;&#65292;&#22312;&#19981;&#21516;&#26102;&#38388;&#23454;&#29616;&#19981;&#21516;&#30340;&#36923;&#36753;&#21644;&#31639;&#26415;&#21151;&#33021;&#12290;&#23427;&#20204;&#21487;&#20197;&#25552;&#20379;&#32039;&#20945;&#24615;&#21644;&#22810;&#24577;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#25805;&#20316;&#25968;&#22788;&#29702;&#33021;&#21147;&#65292;&#20943;&#23569;&#31354;&#38386;&#26102;&#38388;&#65292;&#24182;&#22686;&#21152;&#38754;&#31215;&#21644;&#38745;&#24577;&#21151;&#29575;&#24320;&#38144;&#30340;&#25674;&#38144;&#12290;&#24403;&#19982;&#26580;&#24615;&#20809;&#30005;&#25506;&#27979;&#22120;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#25105;&#20204;&#30340;&#30005;&#36335;&#21487;&#20197;&#25903;&#25345;&#38750;&#20108;&#36827;&#21046;&#26684;&#24335;&#65288;&#22914;&#38543;&#26426;/&#19968;&#20803;&#21644;&#39640;&#32500;&#20648;&#22791;&#26684;&#24335;&#65289;&#30340;&#25968;&#25454;&#30340;&#33410;&#33021;&#22788;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#22810;&#24577;&#30005;&#20809;&#30005;&#36335;&#36824;&#21487;&#20197;&#23454;&#29616;&#21487;&#37197;&#32622;&#30340;&#30005;&#20809;&#35745;&#31639;&#21152;&#36895;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#22788;&#29702;&#20108;&#36827;&#21046;&#21644;&#25972;&#25968;&#37327;&#21270;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#25105;&#20204;&#35774;&#35745;&#30340;&#22810;&#24577;&#30005;&#20809;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present microring resonator (MRR) based polymorphic E-O circuits and architectures that can be employed for high-speed and energy-efficient non-binary reconfigurable computing. Our polymorphic E-O circuits can be dynamically programmed to implement different logic and arithmetic functions at different times. They can provide compactness and polymorphism to consequently improve operand handling, reduce idle time, and increase amortization of area and static power overheads. When combined with flexible photodetectors with the innate ability to accumulate a high number of optical pulses in situ, our circuits can support energy-efficient processing of data in non-binary formats such as stochastic/unary and high-dimensional reservoir formats. Furthermore, our polymorphic E-O circuits enable configurable E-O computing accelerator architectures for processing binarized and integer quantized convolutional neural networks (CNNs). We compare our designed polymorphic E-O circuit
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36328;&#36136;&#37327;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#23558;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#35757;&#32451;&#30340;&#25945;&#24072;&#32593;&#32476;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#20302;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#23398;&#29983;&#32593;&#32476;&#20013;&#65292;&#24182;&#36890;&#36807;&#25552;&#39640;&#28201;&#24230;&#24179;&#28369;&#25945;&#24072;&#36755;&#20986;&#20998;&#24067;&#26469;&#20248;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#24615;&#33021;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2304.07593</link><description>&lt;p&gt;
&#25945;&#24072;&#32593;&#32476;&#26657;&#20934;&#22312;&#36328;&#36136;&#37327;&#30693;&#35782;&#33976;&#39311;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Teacher Network Calibration Improves Cross-Quality Knowledge Distillation. (arXiv:2304.07593v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36328;&#36136;&#37327;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#23558;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#35757;&#32451;&#30340;&#25945;&#24072;&#32593;&#32476;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#20302;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#23398;&#29983;&#32593;&#32476;&#20013;&#65292;&#24182;&#36890;&#36807;&#25552;&#39640;&#28201;&#24230;&#24179;&#28369;&#25945;&#24072;&#36755;&#20986;&#20998;&#24067;&#26469;&#20248;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#24615;&#33021;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36328;&#36136;&#37327;&#30693;&#35782;&#33976;&#39311;&#65288;CQKD&#65289;&#65292;&#19968;&#31181;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#20854;&#20013;&#20174;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#35757;&#32451;&#30340;&#25945;&#24072;&#32593;&#32476;&#20013;&#33719;&#21462;&#30693;&#35782;&#65292;&#28982;&#21518;&#23558;&#20854;&#36716;&#31227;&#21040;&#20165;&#20351;&#29992;&#20302;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#23398;&#29983;&#32593;&#32476;&#12290;&#30001;&#20110;&#22270;&#20687;&#22823;&#23567;&#26159;&#24433;&#21709;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#31243;&#24207;&#35745;&#31639;&#36127;&#33655;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;CQKD&#36890;&#36807;&#20165;&#22312;&#25512;&#26029;&#26102;&#20351;&#29992;&#23398;&#29983;&#32593;&#32476;&#65292;&#26174;&#30528;&#38477;&#20302;&#20102;&#35201;&#27714;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CQKD&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#20013;&#20248;&#20110;&#30417;&#30563;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#26657;&#20934;&#31070;&#32463;&#32593;&#32476;&#30340;&#37325;&#35201;&#24615;&#65306;&#25105;&#20204;&#34920;&#26126;&#65292;&#36890;&#36807;&#26356;&#39640;&#30340;&#28201;&#24230;&#24179;&#28369;&#25945;&#24072;&#36755;&#20986;&#20998;&#24067;&#65292;&#23398;&#29983;&#20998;&#24067;&#23637;&#29616;&#20986;&#26356;&#39640;&#30340;&#29109;&#65292;&#36825;&#26082;&#38477;&#20302;&#20102;&#26657;&#20934;&#35823;&#24046;&#65292;&#20063;&#25552;&#39640;&#20102;&#32593;&#32476;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate cross-quality knowledge distillation (CQKD), a knowledge distillation method where knowledge from a teacher network trained with full-resolution images is transferred to a student network that takes as input low-resolution images. As image size is a deciding factor for the computational load of computer vision applications, CQKD notably reduces the requirements by only using the student network at inference time. Our experimental results show that CQKD outperforms supervised learning in large-scale image classification problems. We also highlight the importance of calibrating neural networks: we show that with higher temperature smoothing of the teacher's output distribution, the student distribution exhibits a higher entropy, which leads to both, a lower calibration error and a higher network accuracy.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31639;&#27861;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#26469;&#26816;&#27979; Twitter &#26426;&#22120;&#20154;&#36134;&#21495;&#65292;&#36890;&#36807;&#23558;&#20854;&#34892;&#20026;&#36716;&#25442;&#20026;&#22270;&#29255;&#36827;&#34892;&#20998;&#31867;&#65292;&#36890;&#36807;&#27979;&#35797;&#21457;&#29616;&#35813;&#26041;&#27861;&#27604;&#20256;&#32479;&#30340;&#26426;&#22120;&#20154;&#26816;&#27979;&#26041;&#27861;&#26356;&#21152;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2304.07535</link><description>&lt;p&gt;
&#20174;&#22312;&#32447;&#34892;&#20026;&#21040;&#22270;&#29255;&#65306;&#31038;&#20132;&#26426;&#22120;&#20154;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
From Online Behaviours to Images: A Novel Approach to Social Bot Detection. (arXiv:2304.07535v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07535
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31639;&#27861;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#26469;&#26816;&#27979; Twitter &#26426;&#22120;&#20154;&#36134;&#21495;&#65292;&#36890;&#36807;&#23558;&#20854;&#34892;&#20026;&#36716;&#25442;&#20026;&#22270;&#29255;&#36827;&#34892;&#20998;&#31867;&#65292;&#36890;&#36807;&#27979;&#35797;&#21457;&#29616;&#35813;&#26041;&#27861;&#27604;&#20256;&#32479;&#30340;&#26426;&#22120;&#20154;&#26816;&#27979;&#26041;&#27861;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#24443;&#24213;&#25913;&#21464;&#20102;&#25105;&#20204;&#28040;&#36153;&#21644;&#20849;&#20139;&#20449;&#24687;&#30340;&#26041;&#24335;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#19968;&#22823;&#22534;&#19981;&#21487;&#38752;&#21644;&#19981;&#20934;&#30830;&#30340;&#20869;&#23481;&#12290;&#20854;&#20013;&#19968;&#31181;&#29305;&#27530;&#31867;&#22411;&#30340;&#31038;&#20132;&#36134;&#21495;&#65292;&#34987;&#31216;&#20026;&#8220;&#26426;&#22120;&#20154;&#8221;&#65292;&#23427;&#20204;&#36890;&#36807;&#33258;&#21160;&#21270;&#25805;&#20316;&#26469;&#23459;&#20256;&#19981;&#21487;&#20449;&#30340;&#20869;&#23481;&#12289;&#36807;&#24230;&#25903;&#25345;&#25919;&#27835;&#27966;&#31995;&#21644;&#23459;&#20256;&#26426;&#26500;&#21270;&#20449;&#24687;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110; Twitter &#36134;&#21495;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#20154;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#23558;&#36134;&#21495;&#25191;&#34892;&#30340;&#25805;&#20316;&#24207;&#21015;&#36716;&#25442;&#25104;&#22270;&#29255;&#65292;&#28982;&#21518;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#22823;&#21151;&#33021;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online Social Networks have revolutionized how we consume and share information, but they have also led to a proliferation of content not always reliable and accurate. One particular type of social accounts is known to promote unreputable content, hyperpartisan, and propagandistic information. They are automated accounts, commonly called bots. Focusing on Twitter accounts, we propose a novel approach to bot detection: we first propose a new algorithm that transforms the sequence of actions that an account performs into an image; then, we leverage the strength of Convolutional Neural Networks to proceed with image classification. We compare our performances with state-of-the-art results for bot detection on genuine accounts / bot accounts datasets well known in the literature. The results confirm the effectiveness of the proposal, because the detection capability is on par with the state of the art, if not better in some cases.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STAS&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#26102;&#31354;&#22238;&#25253;&#20998;&#35299;&#65292;&#21487;&#20197;&#23545;&#20195;&#29702;&#36827;&#34892;&#20449;&#29992;&#20998;&#37197;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;Shapley&#20540;&#21644;&#31354;&#38388;-&#26102;&#38388;&#27880;&#24847;&#26426;&#21046;&#26469;&#35299;&#20915;&#20808;&#21069;&#26041;&#27861;&#20013;&#24310;&#36831;&#20840;&#23616;&#22238;&#25253;&#30340;&#22797;&#26434;&#20851;&#31995;&#38382;&#39064;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#29615;&#22659;&#19979;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.07520</link><description>&lt;p&gt;
STAS: &#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26102;&#31354;&#22238;&#25253;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
STAS: Spatial-Temporal Return Decomposition for Multi-agent Reinforcement Learning. (arXiv:2304.07520v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07520
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STAS&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#26102;&#31354;&#22238;&#25253;&#20998;&#35299;&#65292;&#21487;&#20197;&#23545;&#20195;&#29702;&#36827;&#34892;&#20449;&#29992;&#20998;&#37197;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;Shapley&#20540;&#21644;&#31354;&#38388;-&#26102;&#38388;&#27880;&#24847;&#26426;&#21046;&#26469;&#35299;&#20915;&#20808;&#21069;&#26041;&#27861;&#20013;&#24310;&#36831;&#20840;&#23616;&#22238;&#25253;&#30340;&#22797;&#26434;&#20851;&#31995;&#38382;&#39064;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#29615;&#22659;&#19979;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#20013;&#24335;&#35757;&#32451;&#21644;&#20998;&#25955;&#24335;&#25191;&#34892;&#65288;CTDE&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#20013;&#26377;&#25928;&#30340;&#33539;&#20363;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#30340;&#25361;&#25112;&#26159;&#36171;&#20449;&#29992;&#20540;&#65292;&#21363;&#36890;&#36807;&#20195;&#29702;&#30340;&#36129;&#29486;&#26469;&#32473;&#20195;&#29702;&#36171;&#20449;&#29992;&#20540;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#38544;&#24335;&#22320;&#20998;&#35299;&#32852;&#21512;&#20215;&#20540;&#20989;&#25968;&#25110;&#26174;&#24335;&#22320;&#35745;&#31639;&#25152;&#26377;&#20195;&#29702;&#30340;&#25903;&#20184;&#20998;&#37197;&#12290;&#28982;&#32780;&#65292;&#22312;&#21482;&#26377;&#22312;&#21608;&#26399;&#24615;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#20013;&#65292;&#20840;&#23616;&#22870;&#21169;&#21482;&#33021;&#22312;&#21608;&#26399;&#32467;&#26463;&#26102;&#26174;&#31034;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#19981;&#36215;&#20316;&#29992;&#12290;&#23427;&#20204;&#32570;&#20047;&#23545;&#24310;&#36831;&#20840;&#23616;&#22870;&#21169;&#22312;&#26102;&#38388;&#32500;&#24230;&#20013;&#22797;&#26434;&#20851;&#31995;&#30340;&#24314;&#27169;&#21151;&#33021;&#65292;&#24182;&#19988;&#21463;&#20559;&#24046;&#21644;&#26041;&#24046;&#30340;&#24433;&#21709;&#36739;&#22823;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31354;&#38388;&#26102;&#38388;&#20851;&#27880;&#19982; Shapley&#65288;STAS&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22238;&#25253;&#20998;&#35299;&#65307;STAS &#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#32500;&#24230;&#19978;&#23398;&#20064;&#20449;&#29992;&#20998;&#37197;&#12290;&#23427;&#39318;&#20808;&#23558;&#20840;&#23616;&#22238;&#25253;&#20998;&#35299;&#22238;&#21040;&#27599;&#20010;&#26102;&#38388;&#27493;&#65292;&#28982;&#21518;&#20351;&#29992;Shapley&#20540;&#26469;&#35780;&#20272;&#21327;&#20316;MARL&#20013;&#27599;&#20010;&#20195;&#29702;&#30340;&#36129;&#29486;&#12290; STAS &#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#31354;&#38388; - &#26102;&#38388;&#20851;&#27880;&#26426;&#21046;&#65292;&#20197;&#25429;&#33719;&#24310;&#36831;&#20840;&#23616;&#22870;&#21169;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#29615;&#22659;&#20013;&#65292;STAS &#33021;&#22815;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Centralized Training with Decentralized Execution (CTDE) has been proven to be an effective paradigm in cooperative multi-agent reinforcement learning (MARL). One of the major challenges is yet credit assignment, which aims to credit agents by their contributions. Prior studies focus on either implicitly decomposing the joint value function or explicitly computing the payoff distribution of all agents. However, in episodic reinforcement learning settings where global rewards can only be revealed at the end of the episode, existing methods usually fail to work. They lack the functionality of modeling complicated relations of the delayed global reward in the temporal dimension and suffer from large variance and bias. We propose a novel method named Spatial-Temporal Attention with Shapley (STAS) for return decomposition; STAS learns credit assignment in both the temporal and the spatial dimension. It first decomposes the global return back to each time step, then utilizes Shapley Value to
&lt;/p&gt;</description></item><item><title>PI-FL&#26159;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#22522;&#20110;&#20196;&#29260;&#30340;&#28608;&#21169;&#26426;&#21046;&#22870;&#21169;&#20010;&#24615;&#21270;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#23562;&#37325;&#23458;&#25143;&#31471;&#38544;&#31169;&#30340;&#21516;&#26102;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.07514</link><description>&lt;p&gt;
PI-FL&#65306;&#20010;&#24615;&#21270;&#21644;&#28608;&#21169;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PI-FL: Personalized and Incentivized Federated Learning. (arXiv:2304.07514v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07514
&lt;/p&gt;
&lt;p&gt;
PI-FL&#26159;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#22522;&#20110;&#20196;&#29260;&#30340;&#28608;&#21169;&#26426;&#21046;&#22870;&#21169;&#20010;&#24615;&#21270;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#23562;&#37325;&#23458;&#25143;&#31471;&#38544;&#31169;&#30340;&#21516;&#26102;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24212;&#23545;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#25361;&#25112;&#12290;&#20027;&#35201;&#38382;&#39064;&#26159;&#32771;&#34385;&#26469;&#33258;&#23458;&#25143;&#31471;&#30340;&#20010;&#24615;&#21270;&#36807;&#31243;&#20197;&#20445;&#25252;&#20854;&#33258;&#27835;&#26435;&#12290;&#20801;&#35768;&#23458;&#25143;&#31471;&#21442;&#19982;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20915;&#31574;&#21464;&#24471;&#37325;&#35201;&#65292;&#22240;&#20026;&#23384;&#22312;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#23458;&#25143;&#31471;&#21487;&#33021;&#26080;&#27861;&#33258;&#30001;&#20849;&#20139;&#29983;&#25104;&#33391;&#22909;&#36136;&#37327;&#20010;&#24615;&#21270;&#27169;&#22411;&#25152;&#24517;&#38656;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#20855;&#26377;&#39640;&#36136;&#37327;&#25968;&#25454;&#21644;&#36164;&#28304;&#30340;&#23458;&#25143;&#31471;&#19981;&#24895;&#24847;&#22312;&#27809;&#26377;&#21512;&#29702;&#28608;&#21169;&#30340;&#24773;&#20917;&#19979;&#21442;&#19982;&#32852;&#37030;&#23398;&#20064;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PI-FL&#65292;&#36825;&#26159;&#19968;&#20010;&#19968;&#27425;&#24615;&#20010;&#24615;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#37197;&#21512;&#19968;&#20010;&#22522;&#20110;&#20196;&#29260;&#30340;&#28608;&#21169;&#26426;&#21046;&#65292;&#22870;&#21169;&#20010;&#24615;&#21270;&#35757;&#32451;&#12290;PI-FL&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#23562;&#37325;&#23458;&#25143;&#31471;&#38544;&#31169;&#30340;&#21516;&#26102;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized FL has been widely used to cater to heterogeneity challenges with non-IID data. A primary obstacle is considering the personalization process from the client's perspective to preserve their autonomy. Allowing the clients to participate in personalized FL decisions becomes significant due to privacy and security concerns, where the clients may not be at liberty to share private information necessary for producing good quality personalized models. Moreover, clients with high-quality data and resources are reluctant to participate in the FL process without reasonable incentive. In this paper, we propose PI-FL, a one-shot personalization solution complemented by a token-based incentive mechanism that rewards personalized training. PI-FL outperforms other state-of-the-art approaches and can generate good-quality personalized models while respecting clients' privacy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;HiCON&#26694;&#26550;&#65292;&#36816;&#29992;&#23618;&#27425;&#21270;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#25552;&#39640;&#20102;&#23398;&#20064;&#21040;&#30340;&#33410;&#28857;&#34920;&#31034;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#21516;&#26102;&#37319;&#29992;&#20998;&#23618;&#28040;&#24687;&#32858;&#21512;&#26426;&#21046;&#36991;&#20813;&#20102;&#37051;&#23621;&#30340;&#25351;&#25968;&#32423;&#25193;&#23637;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#30693;&#35782;&#39537;&#21160;&#25512;&#33616;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.07506</link><description>&lt;p&gt;
&#23618;&#27425;&#21270;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#30693;&#35782;&#39537;&#21160;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Hierarchical and Contrastive Representation Learning for Knowledge-aware Recommendation. (arXiv:2304.07506v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;HiCON&#26694;&#26550;&#65292;&#36816;&#29992;&#23618;&#27425;&#21270;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#25552;&#39640;&#20102;&#23398;&#20064;&#21040;&#30340;&#33410;&#28857;&#34920;&#31034;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#21516;&#26102;&#37319;&#29992;&#20998;&#23618;&#28040;&#24687;&#32858;&#21512;&#26426;&#21046;&#36991;&#20813;&#20102;&#37051;&#23621;&#30340;&#25351;&#25968;&#32423;&#25193;&#23637;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#30693;&#35782;&#39537;&#21160;&#25512;&#33616;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#30693;&#35782;&#22270;&#35889;&#34701;&#20837;&#25512;&#33616;&#26159;&#32531;&#35299;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30693;&#35782;&#39537;&#21160;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#26522;&#20030;&#22270;&#30340;&#37051;&#23621;&#36827;&#34892;&#36882;&#24402;&#23884;&#20837;&#20256;&#25773;&#12290;&#38543;&#30528;&#36339;&#25968;&#30340;&#22686;&#21152;&#65292;&#33410;&#28857;&#30340;&#37051;&#23621;&#25968;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#36843;&#20351;&#33410;&#28857;&#22312;&#27492;&#36882;&#24402;&#20256;&#25773;&#20013;&#20102;&#35299;&#22823;&#37327;&#37051;&#23621;&#20197;&#25552;&#28860;&#39640;&#38454;&#35821;&#20041;&#30456;&#20851;&#24615;&#12290;&#36825;&#21487;&#33021;&#20250;&#24341;&#20837;&#26356;&#22810;&#26377;&#23475;&#22122;&#22768;&#65292;&#23548;&#33268;&#23398;&#20064;&#21040;&#30340;&#33410;&#28857;&#34920;&#31034;&#24444;&#27492;&#38590;&#20197;&#21306;&#20998;&#65292;&#21363;&#20247;&#25152;&#21608;&#30693;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HiCON&#30340;&#30693;&#35782;&#39537;&#21160;&#25512;&#33616;&#30340;Hierarchical and Contrastive&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#36991;&#20813;&#37051;&#23621;&#30340;&#25351;&#25968;&#32423;&#25193;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#28040;&#24687;&#32858;&#21512;&#26426;&#21046;&#65292;&#20197;&#21333;&#29420;&#19982;&#20302;&#38454;&#37051;&#23621;&#21644;&#20803;&#36335;&#24452;&#21463;&#38480;&#30340;&#39640;&#38454;&#37051;&#23621;&#20132;&#20114;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25552;&#39640;&#23398;&#20064;&#34920;&#31034;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#20854;&#20013;&#40723;&#21169;&#30456;&#20284;&#30340;&#33410;&#28857;&#30456;&#20114;&#38752;&#36817;&#65292;&#32780;&#25226;&#19981;&#30456;&#20284;&#30340;&#33410;&#28857;&#25512;&#24320;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incorporating knowledge graph into recommendation is an effective way to alleviate data sparsity. Most existing knowledge-aware methods usually perform recursive embedding propagation by enumerating graph neighbors. However, the number of nodes' neighbors grows exponentially as the hop number increases, forcing the nodes to be aware of vast neighbors under this recursive propagation for distilling the high-order semantic relatedness. This may induce more harmful noise than useful information into recommendation, leading the learned node representations to be indistinguishable from each other, that is, the well-known over-smoothing issue. To relieve this issue, we propose a Hierarchical and CONtrastive representation learning framework for knowledge-aware recommendation named HiCON. Specifically, for avoiding the exponential expansion of neighbors, we propose a hierarchical message aggregation mechanism to interact separately with low-order neighbors and meta-path-constrained high-order
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#25945;&#32946;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#22120;MIREX&#65292;&#23427;&#37319;&#29992;&#20114;&#20449;&#24687;&#26368;&#23567;&#21270;&#25439;&#22833;&#26469;&#25552;&#39640;&#23545;&#19981;&#24179;&#34913;&#21644;&#20302;&#36164;&#28304;&#25968;&#25454;&#24773;&#26223;&#19979;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20986;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.07499</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#21644;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19979;&#30340;&#40065;&#26834;&#25945;&#32946;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Robust Educational Dialogue Act Classifiers with Low-Resource and Imbalanced Datasets. (arXiv:2304.07499v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#25945;&#32946;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#22120;MIREX&#65292;&#23427;&#37319;&#29992;&#20114;&#20449;&#24687;&#26368;&#23567;&#21270;&#25439;&#22833;&#26469;&#25552;&#39640;&#23545;&#19981;&#24179;&#34913;&#21644;&#20302;&#36164;&#28304;&#25968;&#25454;&#24773;&#26223;&#19979;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20986;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#34892;&#20026;&#21487;&#20197;&#20195;&#34920;&#22312;&#36741;&#23548;&#23545;&#35805;&#26399;&#38388;&#21457;&#29983;&#30340;&#25945;&#32451;&#21592;&#25110;&#23398;&#29983;&#30340;&#23545;&#35805;&#21160;&#20316;&#12290;&#22312;&#25945;&#32946;&#23545;&#35805;&#20013;&#33258;&#21160;&#35782;&#21035;&#23545;&#35805;&#34892;&#20026;&#23545;&#20110;&#22522;&#20110;&#23545;&#35805;&#30340;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#35774;&#35745;&#26159;&#37325;&#35201;&#30340;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;&#30740;&#31350;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#36741;&#23548;&#23545;&#35805;&#20013;&#30340;&#23545;&#35805;&#34892;&#20026;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#25237;&#20837;&#22823;&#37327;&#31934;&#21147;&#20351;&#29992;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#65288;&#21363;&#20302;&#36164;&#28304;&#25968;&#25454;&#22330;&#26223;&#65289;&#26469;&#20248;&#21270;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#20998;&#31867;&#20934;&#30830;&#24615;&#20043;&#22806;&#65292;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#20063;&#24456;&#37325;&#35201;&#65292;&#36825;&#21487;&#20197;&#21453;&#26144;&#20998;&#31867;&#22120;&#23398;&#20064;&#19981;&#21516;&#31867;&#21035;&#20998;&#24067;&#30340;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#35768;&#22810;&#20808;&#21069;&#30340;&#25945;&#32946;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#30740;&#31350;&#37319;&#29992;&#20132;&#21449;&#29109;&#65288;CE&#65289;&#25439;&#22833;&#26469;&#20248;&#21270;&#20302;&#36164;&#28304;&#25968;&#25454;&#20013;&#30340;DA&#20998;&#31867;&#22120;&#12290;&#36825;&#20123;&#30740;&#31350;&#20013;&#30340;DA&#20998;&#31867;&#22120;&#24448;&#24448;&#20197;&#29306;&#29298;&#23569;&#25968;&#31867;&#30340;&#20195;&#20215;&#26469;&#20248;&#20808;&#32771;&#34385;&#22823;&#22810;&#25968;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#21487;&#33021;&#23548;&#33268;&#22312;&#23569;&#25968;&#31867;&#19978;&#24615;&#33021;&#24046;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;DA&#20998;&#31867;&#22120;MIREX&#65292;&#23427;&#37319;&#29992;&#20114;&#20449;&#24687;&#26368;&#23567;&#21270;&#25439;&#22833;&#26469;&#25552;&#39640;DA&#20998;&#31867;&#22120;&#22312;&#19981;&#24179;&#34913;&#21644;&#20302;&#36164;&#28304;&#25968;&#25454;&#24773;&#26223;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MIREX&#22312;&#19981;&#24179;&#34913;&#21644;&#20302;&#36164;&#28304;&#30340;DA&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue acts (DAs) can represent conversational actions of tutors or students that take place during tutoring dialogues. Automating the identification of DAs in tutoring dialogues is significant to the design of dialogue-based intelligent tutoring systems. Many prior studies employ machine learning models to classify DAs in tutoring dialogues and invest much effort to optimize the classification accuracy by using limited amounts of training data (i.e., low-resource data scenario). However, beyond the classification accuracy, the robustness of the classifier is also important, which can reflect the capability of the classifier on learning the patterns from different class distributions. We note that many prior studies on classifying educational DAs employ cross entropy (CE) loss to optimize DA classifiers on low-resource data with imbalanced DA distribution. The DA classifiers in these studies tend to prioritize accuracy on the majority class at the expense of the minority class which 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#26679;&#26412;&#35757;&#32451;&#21644;&#35843;&#25972;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#23569;&#26679;&#26412;&#24369;&#30417;&#30563;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2304.07470</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#24369;&#30417;&#30563;&#30340;&#32593;&#32476;&#23433;&#20840;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Few-shot Weakly-supervised Cybersecurity Anomaly Detection. (arXiv:2304.07470v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#26679;&#26412;&#35757;&#32451;&#21644;&#35843;&#25972;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#23569;&#26679;&#26412;&#24369;&#30417;&#30563;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#20204;&#36234;&#26469;&#36234;&#20381;&#36182;&#22522;&#20110;&#32593;&#32476;&#30340;&#25216;&#26415;&#65292;&#25915;&#20987;&#32773;&#31363;&#21462;&#29992;&#25143;&#25935;&#24863;&#25968;&#25454;&#30340;&#32593;&#32476;&#25915;&#20987;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#36825;&#31181;&#25915;&#20987;&#30340;&#35268;&#27169;&#21644;&#39057;&#29575;&#27491;&#22312;&#36805;&#36895;&#21319;&#32423;&#65292;&#24182;&#24433;&#21709;&#19982;&#20114;&#32852;&#32593;&#36830;&#25509;&#30340;&#21508;&#31181;&#31995;&#32479;&#21644;&#35774;&#22791;&#12290;&#20256;&#32479;&#30340;&#38450;&#24481;&#26426;&#21046;&#21487;&#33021;&#26080;&#27861;&#36275;&#22815;&#22320;&#24212;&#23545;&#22797;&#26434;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#26032;&#23041;&#32961;&#12290;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21253;&#25324;&#28145;&#24230;&#23398;&#20064;&#30340;&#37325;&#22823;&#31361;&#30772;&#24341;&#36215;&#20102;&#32593;&#32476;&#23433;&#20840;&#30740;&#31350;&#30028;&#30340;&#20852;&#36259;&#65292;&#24076;&#26395;&#36827;&#19968;&#27493;&#22686;&#24378;&#29616;&#26377;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25152;&#26377;&#26032;&#20852;&#21644;&#22797;&#26434;&#30340;&#25915;&#20987;&#25910;&#38598;&#24102;&#26631;&#31614;&#30340;&#24322;&#24120;&#25968;&#25454;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#26679;&#26412;&#35757;&#32451;&#21644;&#35843;&#25972;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#31181;&#21153;&#23454;&#30340;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#23569;&#26679;&#26412;&#24369;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#20010;&#20196;&#20154;&#40723;&#33310;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23569;&#26679;&#26412;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22686;&#24378;&#29616;&#26377;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#32467;&#21512;&#20102;&#26377;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#20197;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#22686;&#24378;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#24182;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With increased reliance on Internet based technologies, cyberattacks compromising users' sensitive data are becoming more prevalent. The scale and frequency of these attacks are escalating rapidly, affecting systems and devices connected to the Internet. The traditional defense mechanisms may not be sufficiently equipped to handle the complex and ever-changing new threats. The significant breakthroughs in the machine learning methods including deep learning, had attracted interests from the cybersecurity research community for further enhancements in the existing anomaly detection methods. Unfortunately, collecting labelled anomaly data for all new evolving and sophisticated attacks is not practical. Training and tuning the machine learning model for anomaly detection using only a handful of labelled data samples is a pragmatic approach. Therefore, few-shot weakly supervised anomaly detection is an encouraging research direction. In this paper, we propose an enhancement to an existing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PFELS&#30340;&#26080;&#32447;FL&#26041;&#26696;&#65292;&#36890;&#36807;&#20808;&#21387;&#32553;&#27169;&#22411;&#26356;&#26032;&#20877;&#33258;&#36866;&#24212;&#22320;&#35774;&#35745;&#21457;&#36865;&#21151;&#29575;&#26469;&#25552;&#20379;&#23458;&#25143;&#31471;&#32423;&#21035;DP&#20445;&#35777;&#65292;&#24182;&#38477;&#20302;&#36890;&#20449;&#21644;&#33021;&#37327;&#24320;&#38144;&#24182;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.07460</link><description>&lt;p&gt;
&#20855;&#26377;&#20869;&#22312;&#38544;&#31169;&#20445;&#25252;&#30340;&#39640;&#25928;&#36890;&#20449;&#21644;&#33410;&#33021;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Communication and Energy Efficient Wireless Federated Learning with Intrinsic Privacy. (arXiv:2304.07460v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PFELS&#30340;&#26080;&#32447;FL&#26041;&#26696;&#65292;&#36890;&#36807;&#20808;&#21387;&#32553;&#27169;&#22411;&#26356;&#26032;&#20877;&#33258;&#36866;&#24212;&#22320;&#35774;&#35745;&#21457;&#36865;&#21151;&#29575;&#26469;&#25552;&#20379;&#23458;&#25143;&#31471;&#32423;&#21035;DP&#20445;&#35777;&#65292;&#24182;&#38477;&#20302;&#36890;&#20449;&#21644;&#33021;&#37327;&#24320;&#38144;&#24182;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#21327;&#21516;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#36793;&#32536;&#35774;&#22791;&#22312;&#20445;&#30041;&#21407;&#22987;&#25968;&#25454;&#30340;&#21516;&#26102;&#21327;&#21516;&#23398;&#20064;&#20840;&#23616;&#27169;&#22411;&#12290;&#34429;&#28982;FL&#36991;&#20813;&#20102;&#20174;&#26412;&#22320;&#25968;&#25454;&#38598;&#27844;&#28431;&#30452;&#25509;&#20449;&#24687;&#65292;&#20294;&#20173;&#21487;&#33021;&#20174;&#20849;&#20139;&#27169;&#22411;&#25512;&#26029;&#20986;&#25935;&#24863;&#20449;&#24687;&#12290;&#20026;&#35299;&#20915;FL&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#21033;&#29992;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26426;&#21046;&#25552;&#20379;&#27491;&#24335;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#26080;&#32447;&#36793;&#32536;&#37096;&#32626;FL&#26102;&#65292;&#30830;&#20445;&#23458;&#25143;&#31471;&#32423;&#21035;&#30340;DP&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24102;&#31232;&#30095;&#21270;&#30340;&#31169;&#26377;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#65288;PFELS&#65289;&#30340;&#26032;&#22411;&#26080;&#32447;FL&#26041;&#26696;&#65292;&#20197;&#25552;&#20379;&#20855;&#26377;&#20869;&#22312;&#20449;&#36947;&#22122;&#22768;&#30340;&#23458;&#25143;&#31471;&#32423;&#21035;DP&#20445;&#35777;&#65292;&#21516;&#26102;&#38477;&#20302;&#36890;&#20449;&#21644;&#33021;&#37327;&#24320;&#38144;&#24182;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#12290;PFELS&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#20351;&#27599;&#20010;&#35774;&#22791;&#20808;&#21387;&#32553;&#20854;&#27169;&#22411;&#26356;&#26032;&#65292;&#28982;&#21518;&#26681;&#25454;&#26080;&#32447;&#20449;&#36947;&#33258;&#36866;&#24212;&#35774;&#35745;&#21387;&#32553;&#27169;&#22411;&#26356;&#26032;&#30340;&#21457;&#36865;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a collaborative learning framework that enables edge devices to collaboratively learn a global model while keeping raw data locally. Although FL avoids leaking direct information from local datasets, sensitive information can still be inferred from the shared models. To address the privacy issue in FL, differential privacy (DP) mechanisms are leveraged to provide formal privacy guarantee. However, when deploying FL at the wireless edge with over-the-air computation, ensuring client-level DP faces significant challenges. In this paper, we propose a novel wireless FL scheme called private federated edge learning with sparsification (PFELS) to provide client-level DP guarantee with intrinsic channel noise while reducing communication and energy overhead and improving model accuracy. The key idea of PFELS is for each device to first compress its model update and then adaptively design the transmit power of the compressed model update according to the wireless cha
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#37319;&#26679;&#20004;&#20010;&#39046;&#22495;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#22312;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#20256;&#36882;&#30693;&#35782;&#12290;&#36825;&#20010;&#26694;&#26550;&#32467;&#21512;&#20102;&#19978;&#19979;&#25991;&#37319;&#26679;&#21644;&#24322;&#24120;&#26816;&#27979;&#65292;&#26082;&#21487;&#20197;&#36827;&#34892;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#65292;&#20063;&#21487;&#20197;&#36827;&#34892;&#26377;&#30417;&#30563;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.07453</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Context-aware Domain Adaptation for Time Series Anomaly Detection. (arXiv:2304.07453v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#37319;&#26679;&#20004;&#20010;&#39046;&#22495;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#22312;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#20256;&#36882;&#30693;&#35782;&#12290;&#36825;&#20010;&#26694;&#26550;&#32467;&#21512;&#20102;&#19978;&#19979;&#25991;&#37319;&#26679;&#21644;&#24322;&#24120;&#26816;&#27979;&#65292;&#26082;&#21487;&#20197;&#36827;&#34892;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#65292;&#20063;&#21487;&#20197;&#36827;&#34892;&#26377;&#30417;&#30563;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#24191;&#27867;&#23454;&#38469;&#24212;&#29992;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#12290;&#30001;&#20110;&#26631;&#31614;&#31232;&#30095;&#24615;&#65292;&#35757;&#32451;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#22120;&#36890;&#24120;&#20381;&#36182;&#20110;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;&#26368;&#36817;&#24050;&#32463;&#33268;&#21147;&#20110;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#33258;&#36866;&#24212;&#65292;&#20197;&#21033;&#29992;&#31867;&#20284;&#39046;&#22495;&#20013;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#20250;&#22240;&#20854;&#31232;&#30095;&#24615;&#21644;&#22810;&#26679;&#24615;&#32780;&#22312;&#24322;&#24120;&#26041;&#38754;&#21463;&#21040;&#36127;&#38754;&#30693;&#35782;&#36716;&#31227;&#30340;&#24433;&#21709;&#12290;&#21463;&#21040;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#19978;&#19979;&#25991;&#23545;&#40784;&#30340;&#23454;&#35777;&#30740;&#31350;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#37319;&#26679;&#20004;&#20010;&#39046;&#22495;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#22312;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#20256;&#36882;&#30693;&#35782;&#12290;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#21516;&#26102;&#24314;&#27169;&#22797;&#26434;&#30340;&#22495;&#20869;&#26102;&#38388;&#30456;&#20851;&#24615;&#21644;&#36328;&#22495;&#30456;&#20851;&#24615;&#65292;&#24182;&#21033;&#29992;&#28304;&#22495;&#30340;&#26631;&#31614;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#23558;&#19978;&#19979;&#25991;&#37319;&#26679;&#21644;&#24322;&#24120;&#26816;&#27979;&#32467;&#21512;&#21040;&#19968;&#20010;&#32852;&#21512;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;&#25105;&#20204;&#23558;&#19978;&#19979;&#25991;&#37319;&#26679;&#34920;&#36848;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#21033;&#29992;&#28304;&#22495;&#30340;&#26631;&#31614;&#20449;&#24687;&#36827;&#34892;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#21644;&#26377;&#30417;&#30563;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series anomaly detection is a challenging task with a wide range of real-world applications. Due to label sparsity, training a deep anomaly detector often relies on unsupervised approaches. Recent efforts have been devoted to time series domain adaptation to leverage knowledge from similar domains. However, existing solutions may suffer from negative knowledge transfer on anomalies due to their diversity and sparsity. Motivated by the empirical study of context alignment between two domains, we aim to transfer knowledge between two domains via adaptively sampling context information for two domains. This is challenging because it requires simultaneously modeling the complex in-domain temporal dependencies and cross-domain correlations while exploiting label information from the source domain. To this end, we propose a framework that combines context sampling and anomaly detection into a joint learning procedure. We formulate context sampling into the Markov decision process and ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#20013;&#20351;&#29992;&#21487;&#25805;&#20316;&#27010;&#29575;&#27169;&#22411;&#26469;&#24378;&#21046;&#23454;&#26045;&#38480;&#21046;&#30340;&#25511;&#21046;&#26041;&#27861;GeLaTo&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;&#24120;&#35265;&#30340;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#27979;&#35797;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07438</link><description>&lt;p&gt;
&#21487;&#25805;&#20316;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#29983;&#25104;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tractable Control for Autoregressive Language Generation. (arXiv:2304.07438v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#20013;&#20351;&#29992;&#21487;&#25805;&#20316;&#27010;&#29575;&#27169;&#22411;&#26469;&#24378;&#21046;&#23454;&#26045;&#38480;&#21046;&#30340;&#25511;&#21046;&#26041;&#27861;GeLaTo&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;&#24120;&#35265;&#30340;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#27979;&#35797;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#22238;&#24402;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29983;&#25104;&#28385;&#36275;&#22797;&#26434;&#38480;&#21046;&#30340;&#25991;&#26412;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65306;&#21363;&#20351;&#26159;&#26368;&#31616;&#21333;&#30340;&#35789;&#27719;&#38480;&#21046;&#20063;&#20351;&#26465;&#20214;&#20998;&#24067;$\Pr(\text{text} | \alpha)$&#30340;&#37319;&#26679;&#21464;&#24471;&#19981;&#21487;&#35745;&#31639;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#21487;&#25805;&#20316;&#30340;&#27010;&#29575;&#27169;&#22411;&#23558;&#35789;&#27719;&#38480;&#21046;&#24378;&#21152;&#20110;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#20013;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026; GeLaTo&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#20010;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#31934;&#31616;&#30340;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#26469;&#25511;&#21046;&#20174;GPT2&#21040;&#33258;&#22238;&#24402;&#30340;&#29983;&#25104;&#12290;GeLaTo&#22312;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;CommonGen&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22823;&#24133;&#20987;&#36133;&#20102;&#21508;&#31181;&#24378;&#22522;&#32447;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#19981;&#20165;&#20026;&#25511;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#65292;&#36824;&#28608;&#21169;&#20154;&#20204;&#24320;&#21457;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#21487;&#25805;&#20316;&#27010;&#29575;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of autoregressive large language models in text generation, it remains a major challenge to generate text that satisfies complex constraints: sampling from the conditional distribution $\Pr(\text{text} | \alpha)$ is intractable for even the simplest lexical constraints $\alpha$. To overcome this challenge, we propose to use tractable probabilistic models to impose lexical constraints in autoregressive text generation, which we refer to as GeLaTo. To demonstrate the effectiveness of this framework, we use distilled hidden Markov models to control autoregressive generation from GPT2. GeLaTo achieves state-of-the-art performance on CommonGen, a challenging benchmark for constrained text generation, beating a wide range of strong baselines by a large margin. Our work not only opens up new avenues for controlling large language models but also motivates the development of more expressive tractable probabilistic models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36136;&#37327;&#22810;&#26679;&#24615;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#35299;&#20915;&#26041;&#26696;&#20998;&#35299;&#20026;&#29420;&#31435;&#36827;&#21270;&#30340;&#29289;&#31181;&#65292;&#24182;&#20351;&#29992;&#26080;&#30417;&#30563;&#30340;&#25216;&#33021;&#21457;&#29616;&#26469;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#39640;&#24615;&#33021;&#35299;&#20915;&#26041;&#26696;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26080;&#38656;&#23384;&#26723;&#25110;&#39044;&#20808;&#23450;&#20041;&#34892;&#20026;&#33539;&#22260;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2304.07425</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26679;&#24615;&#20248;&#36136;&#20010;&#20307;&#23454;&#29616;&#39640;&#25928;&#30340;&#36136;&#37327;&#22810;&#26679;&#24615;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Efficient Quality-Diversity Optimization through Diverse Quality Species. (arXiv:2304.07425v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36136;&#37327;&#22810;&#26679;&#24615;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#35299;&#20915;&#26041;&#26696;&#20998;&#35299;&#20026;&#29420;&#31435;&#36827;&#21270;&#30340;&#29289;&#31181;&#65292;&#24182;&#20351;&#29992;&#26080;&#30417;&#30563;&#30340;&#25216;&#33021;&#21457;&#29616;&#26469;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#39640;&#24615;&#33021;&#35299;&#20915;&#26041;&#26696;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26080;&#38656;&#23384;&#26723;&#25110;&#39044;&#20808;&#23450;&#20041;&#34892;&#20026;&#33539;&#22260;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#19968;&#30446;&#26631;&#20248;&#21270;&#30340;&#26222;&#36941;&#23616;&#38480;&#24615;&#26159;&#23427;&#21487;&#33021;&#20250;&#34987;&#35823;&#23548;&#65292;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#36825;&#21487;&#20197;&#36890;&#36807;&#36136;&#37327;&#22810;&#26679;&#24615;(QD)&#31639;&#27861;&#26469;&#32416;&#27491;&#65292;&#20854;&#20013;&#39318;&#36873;&#38382;&#39064;&#30340;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#35299;&#20915;&#26041;&#26696;&#30340;&#20154;&#32676;&#12290;&#22823;&#22810;&#25968;&#20256;&#32479;&#30340;QD&#26041;&#27861;&#65292;&#20363;&#22914;MAP-Elites&#65292;&#26126;&#30830;&#31649;&#29702;&#35299;&#20915;&#26041;&#26696;&#34987;&#20998;&#35299;&#25104;&#39044;&#23450;&#20041;&#30340;&#22721;&#40859;&#30340;&#34892;&#20026;&#26723;&#26696;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#23384;&#26723;&#25110;&#39044;&#20808;&#23450;&#20041;&#34892;&#20026;&#33539;&#22260;&#30340;&#38480;&#21046;&#19979;&#25214;&#21040;&#22810;&#26679;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#31181;&#32676;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#23558;&#35299;&#20915;&#26041;&#26696;&#20998;&#35299;&#20026;&#29420;&#31435;&#36827;&#21270;&#30340;&#29289;&#31181;&#65292;&#24182;&#20351;&#29992;&#26080;&#30417;&#30563;&#30340;&#25216;&#33021;&#21457;&#29616;&#26469;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#39640;&#24615;&#33021;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#31361;&#21464;&#26469;&#23436;&#25104;&#65292;&#36825;&#20123;&#31361;&#21464;&#37319;&#21462;&#20114;&#20449;&#24687;&#21644;&#24615;&#33021;&#30340;&#20449;&#24687;&#29702;&#35770;&#35270;&#35282;&#20849;&#21516;&#26368;&#22823;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#26679;&#24615;&#20248;&#36136;&#29289;&#31181;(DQS)&#20316;&#20026;&#23384;&#26723;&#22411;QD&#31639;&#27861;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A prevalent limitation of optimizing over a single objective is that it can be misguided, becoming trapped in local optimum. This can be rectified by Quality-Diversity (QD) algorithms, where a population of high-quality and diverse solutions to a problem is preferred. Most conventional QD approaches, for example, MAP-Elites, explicitly manage a behavioral archive where solutions are broken down into predefined niches. In this work, we show that a diverse population of solutions can be found without the limitation of needing an archive or defining the range of behaviors in advance. Instead, we break down solutions into independently evolving species and use unsupervised skill discovery to learn diverse, high-performing solutions. We show that this can be done through gradient-based mutations that take on an information theoretic perspective of jointly maximizing mutual information and performance. We propose Diverse Quality Species (DQS) as an alternative to archive-based QD algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38646;&#26679;&#26412;&#31649;&#36947;&#65292;&#20351;&#24471;&#21487;&#20197;&#25429;&#25417;&#29992;&#25143;&#36523;&#20221;&#30340;&#20154;&#29289;&#24418;&#35937;&#20197;&#20196;&#20154;&#24841;&#24742;&#30340;&#26041;&#24335;&#36827;&#34892;&#22270;&#20687;&#29983;&#25104;&#30340;&#20010;&#24615;&#21270;&#22788;&#29702;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#25193;&#23637;&#36866;&#29992;&#20110;&#25968;&#30334;&#19975;&#29992;&#25143;&#12290;</title><link>http://arxiv.org/abs/2304.07410</link><description>&lt;p&gt;
&#20026;&#38646;&#26679;&#26412;&#20010;&#24615;&#21270;&#25552;&#20379;&#26465;&#20214;&#21270;&#25991;&#26412;&#20154;&#29289;&#24418;&#35937;
&lt;/p&gt;
&lt;p&gt;
Text-Conditional Contextualized Avatars For Zero-Shot Personalization. (arXiv:2304.07410v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38646;&#26679;&#26412;&#31649;&#36947;&#65292;&#20351;&#24471;&#21487;&#20197;&#25429;&#25417;&#29992;&#25143;&#36523;&#20221;&#30340;&#20154;&#29289;&#24418;&#35937;&#20197;&#20196;&#20154;&#24841;&#24742;&#30340;&#26041;&#24335;&#36827;&#34892;&#22270;&#20687;&#29983;&#25104;&#30340;&#20010;&#24615;&#21270;&#22788;&#29702;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#25193;&#23637;&#36866;&#29992;&#20110;&#25968;&#30334;&#19975;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#22823;&#22411;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#22312;&#21512;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#12289;&#30495;&#23454;&#24615;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#24182;&#19988;&#20351;&#29992;&#25143;&#36890;&#36807;&#35821;&#35328;&#26469;&#25511;&#21046;&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29983;&#25104;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#26041;&#38754;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#24182;&#19988;&#26410;&#32463;&#28145;&#20837;&#25506;&#32034;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20197;&#20196;&#20154;&#24841;&#24742;&#30340;&#26041;&#24335;&#25429;&#25417;&#29992;&#25143;&#36523;&#20221;&#30340;&#20154;&#29289;&#24418;&#35937;&#65292;&#20174;&#32780;&#23454;&#29616;&#22270;&#20687;&#29983;&#25104;&#30340;&#20010;&#24615;&#21270;&#31649;&#36947;&#12290;&#25105;&#20204;&#30340;&#31649;&#36947;&#26159;&#38646;&#26679;&#26412;&#12289;&#20154;&#29289;&#24418;&#35937;&#32441;&#29702;&#21644;&#39118;&#26684;&#19981;&#21487;&#30693;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#23545;&#20154;&#29289;&#24418;&#35937;&#36827;&#34892;&#35757;&#32451; - &#23427;&#21487;&#20197;&#25193;&#23637;&#21040;&#25968;&#30334;&#19975;&#29992;&#25143;&#65292;&#36825;&#20123;&#29992;&#25143;&#21487;&#20197;&#29992;&#20182;&#20204;&#30340;&#20154;&#29289;&#24418;&#35937;&#29983;&#25104;&#22330;&#26223;&#12290;&#20026;&#20102;&#20197;&#24544;&#23454;&#20110;&#32473;&#23450;&#30340;&#25991;&#26412;&#25552;&#31034;&#30340;&#23039;&#21183;&#21576;&#29616;&#20154;&#29289;&#24418;&#35937;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;&#21040;3D&#23039;&#21183;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#19968;&#20010;&#31574;&#21010;&#30340;&#22823;&#35268;&#27169;&#37326;&#22806;&#20154;&#31867;&#23039;&#21183;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;SOTA&#25991;&#26412;&#21040;&#21160;&#20316;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#39318;&#27425;&#23637;&#31034;&#20102;&#21033;&#29992;&#22823;&#35268;&#27169;&#22270;&#20687;&#25968;&#25454;&#38598;&#26469;&#25552;&#39640;&#20154;&#29289;&#24418;&#35937;&#29983;&#25104;&#30340;&#20010;&#24615;&#21270;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent large-scale text-to-image generation models have made significant improvements in the quality, realism, and diversity of the synthesized images and enable users to control the created content through language. However, the personalization aspect of these generative models is still challenging and under-explored. In this work, we propose a pipeline that enables personalization of image generation with avatars capturing a user's identity in a delightful way. Our pipeline is zero-shot, avatar texture and style agnostic, and does not require training on the avatar at all - it is scalable to millions of users who can generate a scene with their avatar. To render the avatar in a pose faithful to the given text prompt, we propose a novel text-to-3D pose diffusion model trained on a curated large-scale dataset of in-the-wild human poses improving the performance of the SOTA text-to-motion models significantly. We show, for the first time, how to leverage large-scale image datasets to le
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#26694;&#26550;&#32467;&#26500;&#26469;&#22788;&#29702;&#26410;&#30693;&#20195;&#29702;&#22870;&#21169;&#30340;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#26159;&#28176;&#36827;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2304.07407</link><description>&lt;p&gt;
&#26410;&#35266;&#27979;&#21040;&#20195;&#29702;&#22870;&#21169;&#30340;&#37325;&#22797;&#36127;&#36131;&#20154;&#20195;&#29702;&#21338;&#24328;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Repeated Principal-Agent Games with Unobserved Agent Rewards and Perfect-Knowledge Agents. (arXiv:2304.07407v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#26694;&#26550;&#32467;&#26500;&#26469;&#22788;&#29702;&#26410;&#30693;&#20195;&#29702;&#22870;&#21169;&#30340;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#26159;&#28176;&#36827;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#22810;&#33218;&#32769;&#34382;&#26426;&#26694;&#26550;&#20013;&#30340;&#37325;&#22797;&#36127;&#36131;&#20154;&#20195;&#29702;&#21338;&#24328;&#22330;&#26223;&#65292;&#20854;&#20013;&#20195;&#29702;&#36873;&#25321;&#19968;&#31181;&#32769;&#34382;&#26426;&#21518;&#20250;&#33719;&#24471;&#22870;&#21169;&#21644;&#28608;&#21169;&#65292;&#20294;&#36127;&#36131;&#20154;&#21482;&#33021;&#35266;&#23519;&#21040;&#20195;&#29702;&#36873;&#25321;&#20102;&#21738;&#20010;&#32769;&#34382;&#26426;&#20197;&#21450;&#20195;&#29702;&#30456;&#24212;&#30340;&#28608;&#21169;&#65292;&#32780;&#24819;&#35201;&#35774;&#35745;&#19968;&#31181;&#21512;&#36866;&#30340;&#31574;&#30053;&#21364;&#20805;&#28385;&#20102;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#26694;&#26550;&#32467;&#26500;&#26469;&#22788;&#29702;&#26410;&#30693;&#20195;&#29702;&#22870;&#21169;&#30340;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#26159;&#28176;&#36827;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by a number of real-world applications from domains like healthcare and sustainable transportation, in this paper we study a scenario of repeated principal-agent games within a multi-armed bandit (MAB) framework, where: the principal gives a different incentive for each bandit arm, the agent picks a bandit arm to maximize its own expected reward plus incentive, and the principal observes which arm is chosen and receives a reward (different than that of the agent) for the chosen arm. Designing policies for the principal is challenging because the principal cannot directly observe the reward that the agent receives for their chosen actions, and so the principal cannot directly learn the expected reward using existing estimation techniques. As a result, the problem of designing policies for this scenario, as well as similar ones, remains mostly unexplored. In this paper, we construct a policy that achieves a low regret (i.e., square-root regret up to a log factor) in this scenar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;InstructGPT&#36741;&#21161;&#21307;&#29983;&#39044;&#31579;&#36873;&#24739;&#32773;&#26159;&#21542;&#31526;&#21512;&#20020;&#24202;&#35797;&#39564;&#36164;&#26684;&#12290;&#36890;&#36807;10&#20010;&#21512;&#25104;&#24739;&#32773;&#31616;&#20917;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;LLMs&#22312;&#35782;&#21035;&#31579;&#36873;&#36164;&#26684;&#26631;&#20934;&#12289;&#21333;&#29420;&#20998;&#31867;&#12289;&#25972;&#20307;&#20998;&#31867;&#12289;&#20197;&#21450;&#38656;&#35201;&#31579;&#36873;&#36164;&#26684;&#26631;&#20934;&#30340;&#30334;&#20998;&#27604;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.07396</link><description>&lt;p&gt;
&#25913;&#21892;&#20020;&#24202;&#35797;&#39564;&#30340;&#24739;&#32773;&#39044;&#31579;&#36873;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#21307;&#29983;
&lt;/p&gt;
&lt;p&gt;
Improving Patient Pre-screening for Clinical Trials: Assisting Physicians with Large Language Models. (arXiv:2304.07396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;InstructGPT&#36741;&#21161;&#21307;&#29983;&#39044;&#31579;&#36873;&#24739;&#32773;&#26159;&#21542;&#31526;&#21512;&#20020;&#24202;&#35797;&#39564;&#36164;&#26684;&#12290;&#36890;&#36807;10&#20010;&#21512;&#25104;&#24739;&#32773;&#31616;&#20917;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;LLMs&#22312;&#35782;&#21035;&#31579;&#36873;&#36164;&#26684;&#26631;&#20934;&#12289;&#21333;&#29420;&#20998;&#31867;&#12289;&#25972;&#20307;&#20998;&#31867;&#12289;&#20197;&#21450;&#38656;&#35201;&#31579;&#36873;&#36164;&#26684;&#26631;&#20934;&#30340;&#30334;&#20998;&#27604;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#21040;&#24739;&#32773;&#30340;&#20020;&#24202;&#35797;&#39564;&#65292;&#21307;&#29983;&#38656;&#35201;&#36827;&#34892;&#32321;&#29712;&#30340;&#26816;&#26597;&#65292;&#20197;&#30830;&#23450;&#24739;&#32773;&#26159;&#21542;&#31526;&#21512;&#25991;&#26412;&#22522;&#20934;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#20020;&#24202;&#20449;&#24687;&#25552;&#21462;&#21644;&#20020;&#24202;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23578;&#26410;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;InstructGPT&#36741;&#21161;&#21307;&#29983;&#26681;&#25454;&#24739;&#32773;&#30340;&#21307;&#30103;&#31616;&#20917;&#30830;&#23450;&#20854;&#26159;&#21542;&#31526;&#21512;&#20020;&#24202;&#35797;&#39564;&#30340;&#36164;&#26684;&#12290;&#20351;&#29992;&#19968;&#27425;&#24615;&#12289;&#36873;&#25321;-&#25512;&#29702;&#21644;&#24605;&#32500;&#38142;&#31574;&#30053;&#30456;&#32467;&#21512;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22312;10&#20010;&#21512;&#25104;&#24739;&#32773;&#31616;&#20917;&#19978;&#30340;&#34920;&#29616;&#12290;&#22312;&#22235;&#20010;&#32423;&#21035;&#19978;&#35780;&#20272;&#20102;&#24615;&#33021;&#65306;&#33021;&#21542;&#20174;&#20020;&#24202;&#35797;&#39564;&#20013;&#32473;&#20986;&#30340;&#21307;&#30103;&#31616;&#20917;&#20013;&#35782;&#21035;&#31579;&#36873;&#36164;&#26684;&#26631;&#20934;&#65307;&#33021;&#21542;&#20026;&#27599;&#20010;&#21333;&#29420;&#30340;&#26631;&#20934;&#20998;&#31867;&#26159;&#21542;&#31526;&#21512;&#24739;&#32773;&#65307;&#25972;&#20307;&#20998;&#31867;&#26159;&#21542;&#31526;&#21512;&#20020;&#24202;&#35797;&#39564;&#36164;&#26684;&#20197;&#21450;&#38656;&#35201;&#31579;&#36873;&#36164;&#26684;&#26631;&#20934;&#30340;&#30334;&#20998;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physicians considering clinical trials for their patients are met with the laborious process of checking many text based eligibility criteria. Large Language Models (LLMs) have shown to perform well for clinical information extraction and clinical reasoning, including medical tests, but not yet in real-world scenarios. This paper investigates the use of InstructGPT to assist physicians in determining eligibility for clinical trials based on a patient's summarised medical profile. Using a prompting strategy combining one-shot, selection-inference and chain-of-thought techniques, we investigate the performance of LLMs on 10 synthetically created patient profiles. Performance is evaluated at four levels: ability to identify screenable eligibility criteria from a trial given a medical profile; ability to classify for each individual criterion whether the patient qualifies; the overall classification whether a patient is eligible for a clinical trial and the percentage of criteria to be scr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SoY&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#25439;&#22833;&#20989;&#25968;&#21644;&#27979;&#35797;&#26102;&#20248;&#21270;&#20363;&#31243;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#20154;&#20307;&#31867;&#22411;&#19978;&#25552;&#39640;&#31934;&#30830;&#30340;3D&#24418;&#20307;&#20272;&#35745;&#20934;&#30830;&#24230;&#65292;&#20026;&#26102;&#23578;&#34892;&#19994;&#30340;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;</title><link>http://arxiv.org/abs/2304.07389</link><description>&lt;p&gt;
Shape of You&#65306;&#38024;&#23545;&#22810;&#26679;&#21270;&#36523;&#26448;&#30340;&#31934;&#20934;3D&#24418;&#20307;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Shape of You: Precise 3D shape estimations for diverse body types. (arXiv:2304.07389v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SoY&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#25439;&#22833;&#20989;&#25968;&#21644;&#27979;&#35797;&#26102;&#20248;&#21270;&#20363;&#31243;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#20154;&#20307;&#31867;&#22411;&#19978;&#25552;&#39640;&#31934;&#30830;&#30340;3D&#24418;&#20307;&#20272;&#35745;&#20934;&#30830;&#24230;&#65292;&#20026;&#26102;&#23578;&#34892;&#19994;&#30340;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SoY&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#21892;&#22522;&#20110;&#35270;&#35273;&#30340;&#26381;&#35013;&#25512;&#33616;&#31995;&#32479;&#20013;3D&#36523;&#20307;&#24418;&#29366;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#26041;&#27861;&#24050;&#32463;&#25104;&#21151;&#22320;&#20272;&#35745;&#20102;3D&#23039;&#21183;&#65292;&#20294;&#22312;&#31934;&#30830;&#24418;&#29366;&#20272;&#35745;&#26041;&#38754;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#19981;&#21516;&#30340;&#20154;&#20307;&#31867;&#22411;&#65292;&#20173;&#23384;&#22312;&#32570;&#20047;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#38598;&#25104;&#21040;&#21442;&#25968;&#21270;&#30340;3D&#20154;&#20307;&#37325;&#24314;&#31649;&#36947;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#26102;&#20248;&#21270;&#20363;&#31243;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;SSP-3D&#25968;&#25454;&#38598;&#19978;&#27604;&#26368;&#36817;&#30340;SHAPY&#26041;&#27861;&#25552;&#39640;&#20102;17.7&#65285;&#12290;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#26397;&#30528;&#26356;&#20934;&#30830;&#30340;3D&#24418;&#24577;&#20272;&#35745;&#31995;&#32479;&#36808;&#20986;&#30340;&#19968;&#27493;&#65292;&#35813;&#31995;&#32479;&#21487;&#22312;&#19981;&#21516;&#30340;&#20307;&#22411;&#19978;&#21487;&#38752;&#22320;&#24037;&#20316;&#65292;&#24182;&#26377;&#26395;&#22312;&#26102;&#23578;&#34892;&#19994;&#20013;&#24471;&#21040;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents Shape of You (SoY), an approach to improve the accuracy of 3D body shape estimation for vision-based clothing recommendation systems. While existing methods have successfully estimated 3D poses, there remains a lack of work in precise shape estimation, particularly for diverse human bodies. To address this gap, we propose two loss functions that can be readily integrated into parametric 3D human reconstruction pipelines. Additionally, we propose a test-time optimization routine that further improves quality. Our method improves over the recent SHAPY method by 17.7% on the challenging SSP-3D dataset. We consider our work to be a step towards a more accurate 3D shape estimation system that works reliably on diverse body types and holds promise for practical applications in the fashion industry.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#21512;&#19981;&#21516;&#29983;&#25104;&#22120;&#30340;&#25216;&#26415;&#65292;&#20197;&#21019;&#24314;&#26356;&#22797;&#26434;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20174;&#32780;&#22823;&#24133;&#25552;&#39640;&#36867;&#36991;&#21453;&#24694;&#24847;&#36719;&#20214;&#24037;&#20855;&#30340;&#26816;&#27979;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.07360</link><description>&lt;p&gt;
&#32467;&#21512;&#23545;&#25239;&#24615;&#24694;&#24847;&#36719;&#20214;&#31034;&#20363;&#30340;&#29983;&#25104;&#22120;&#20197;&#22686;&#21152;&#35268;&#36991;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combining Generators of Adversarial Malware Examples to Increase Evasion Rate. (arXiv:2304.07360v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#21512;&#19981;&#21516;&#29983;&#25104;&#22120;&#30340;&#25216;&#26415;&#65292;&#20197;&#21019;&#24314;&#26356;&#22797;&#26434;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20174;&#32780;&#22823;&#24133;&#25552;&#39640;&#36867;&#36991;&#21453;&#24694;&#24847;&#36719;&#20214;&#24037;&#20855;&#30340;&#26816;&#27979;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26432;&#27602;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#36234;&#26469;&#36234;&#22810;&#22320;&#25317;&#25265;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#24694;&#24847;&#36719;&#20214;&#38450;&#24481;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#23574;&#31471;&#30340;&#25104;&#26524;&#65292;&#20294;&#23427;&#20063;&#26377;&#24369;&#28857;&#65292;&#34987;&#20960;&#31181;&#23545;&#25239;&#24615;&#25915;&#20987;&#25216;&#26415;&#25152;&#21033;&#29992;&#12290;&#35768;&#22810;&#20316;&#32773;&#25552;&#20986;&#20102;&#33021;&#22815;&#32469;&#36807;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#30340;&#30333;&#30418;&#21644;&#40657;&#30418;&#29983;&#25104;&#22120;&#30340;&#23545;&#25239;&#24615;&#24694;&#24847;&#36719;&#20214;&#31034;&#20363;&#65292;&#20854;&#25104;&#21151;&#29575;&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#24314;&#35758;&#23558;&#29616;&#20195;&#29983;&#25104;&#22120;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#22686;&#21152;&#23427;&#20204;&#30340;&#28508;&#21147;&#12290;&#32467;&#21512;&#19981;&#21516;&#30340;&#29983;&#25104;&#22120;&#21487;&#20197;&#21019;&#24314;&#26356;&#22797;&#26434;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#36825;&#20123;&#31034;&#20363;&#26356;&#26377;&#21487;&#33021;&#36867;&#36991;&#21453;&#24694;&#24847;&#36719;&#20214;&#24037;&#20855;&#30340;&#26816;&#27979;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#30693;&#21517;&#30340;&#29983;&#25104;&#22120;&#19978;&#28436;&#31034;&#20102;&#36825;&#31181;&#25216;&#26415;&#65292;&#24182;&#35760;&#24405;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;AMG-random&#21644;MAB-Malware&#29983;&#25104;&#22120;&#30340;&#26368;&#20339;&#32452;&#21512;&#23545;&#39030;&#23574;&#26432;&#27602;&#20135;&#21697;&#30340;&#24179;&#22343;&#35268;&#36991;&#29575;&#20026;15.9%&#12290;&#36825;&#20195;&#34920;&#20102;&#20351;&#29992;&#20165;AMG-random&#21644;MAB-Malware&#29983;&#25104;&#22120;&#30340;&#32467;&#26524;&#24179;&#22343;&#25552;&#39640;&#20102;&#36229;&#36807;36%&#21644;627%&#12290;
&lt;/p&gt;
&lt;p&gt;
Antivirus developers are increasingly embracing machine learning as a key component of malware defense. While machine learning achieves cutting-edge outcomes in many fields, it also has weaknesses that are exploited by several adversarial attack techniques. Many authors have presented both white-box and black-box generators of adversarial malware examples capable of bypassing malware detectors with varying success. We propose to combine contemporary generators in order to increase their potential. Combining different generators can create more sophisticated adversarial examples that are more likely to evade anti-malware tools. We demonstrated this technique on five well-known generators and recorded promising results. The best-performing combination of AMG-random and MAB-Malware generators achieved an average evasion rate of 15.9% against top-tier antivirus products. This represents an average improvement of more than 36% and 627% over using only the AMG-random and MAB-Malware generato
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;Credo&#26694;&#26550;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#21644;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#26469;&#25903;&#25345;&#26234;&#33021;&#20307;&#23545;&#32676;&#20307;&#23545;&#40784;&#21442;&#25968;&#36827;&#34892;&#33258;&#25105;&#35843;&#33410;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#22810;&#26234;&#33021;&#20307;&#22242;&#38431;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.07337</link><description>&lt;p&gt;
&#23398;&#20064;&#32676;&#20307;&#35843;&#25972;&#65306;&#20855;&#26377;&#22810;&#26234;&#33021;&#20307;&#22242;&#38431;&#30340;&#33258;&#36866;&#24212;Credo&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning to Learn Group Alignment: A Self-Tuning Credo Framework with Multiagent Teams. (arXiv:2304.07337v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;Credo&#26694;&#26550;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#21644;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#26469;&#25903;&#25345;&#26234;&#33021;&#20307;&#23545;&#32676;&#20307;&#23545;&#40784;&#21442;&#25968;&#36827;&#34892;&#33258;&#25105;&#35843;&#33410;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#22810;&#26234;&#33021;&#20307;&#22242;&#38431;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20010;&#26234;&#33021;&#20307;&#22242;&#38431;&#20013;&#28151;&#21512;&#28608;&#21169;&#27604;&#23436;&#20840;&#21512;&#20316;&#31995;&#32479;&#20855;&#26377;&#20248;&#21183;&#65292;&#28982;&#32780;&#65292;&#21457;&#29616;&#26368;&#20339;&#30340;&#28608;&#21169;&#32452;&#21512;&#25110;&#22242;&#38431;&#32467;&#26500;&#26159;&#19968;&#20010;&#22256;&#38590;&#32780;&#21160;&#24577;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20854;&#20013;&#20010;&#20307;&#23398;&#20064;&#26234;&#33021;&#20307;&#36890;&#36807;&#20854;&#22870;&#21169;&#20989;&#25968;&#30340;&#19981;&#21516;&#37096;&#20998;&#33258;&#25105;&#35843;&#33410;&#20854;&#28608;&#21169;&#37197;&#32622;&#12290;&#35813;&#27169;&#22411;&#25193;&#23637;&#20102;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#35753;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#21160;&#24577;&#26356;&#26032;&#20854;&#22242;&#38431;&#23545;&#40784;&#65292;&#24182;&#20801;&#35768;&#38431;&#21451;&#20855;&#26377;&#19981;&#21516;&#30340;&#22242;&#38431;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20511;&#37492;&#20102;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#21644;&#20803;&#23398;&#20064;&#30340;&#24605;&#24819;&#65292;&#20197;&#23398;&#20064;&#25903;&#25345;&#34892;&#20026;&#31574;&#30053;&#21457;&#23637;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#37197;&#32622;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#24120;&#35265;&#30340;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#25552;&#20379;&#20102;&#21021;&#27493;&#32467;&#26524;&#65292;&#24182;&#21457;&#29616;&#26234;&#33021;&#20307;&#36890;&#36807;&#33258;&#25105;&#35843;&#25972;&#20854;&#21508;&#33258;&#30340;&#32452;&#23545;&#20934;&#21442;&#25968;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#20840;&#23616;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixed incentives among a population with multiagent teams has been shown to have advantages over a fully cooperative system; however, discovering the best mixture of incentives or team structure is a difficult and dynamic problem. We propose a framework where individual learning agents self-regulate their configuration of incentives through various parts of their reward function. This work extends previous work by giving agents the ability to dynamically update their group alignment during learning and by allowing teammates to have different group alignment. Our model builds on ideas from hierarchical reinforcement learning and meta-learning to learn the configuration of a reward function that supports the development of a behavioral policy. We provide preliminary results in a commonly studied multiagent environment and find that agents can achieve better global outcomes by self-tuning their respective group alignment parameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;OpenAI&#30340;&#35821;&#35328;&#27169;&#22411;ChatGPT&#36827;&#34892;&#33258;&#25105;&#35748;&#30693;&#21644;&#25919;&#27835;&#20559;&#35265;&#20998;&#26512;&#65292;&#27979;&#35797;&#32467;&#26524;&#26174;&#31034;ChatGPT&#20559;&#21521;&#36827;&#27493;&#35266;&#28857;&#12290;</title><link>http://arxiv.org/abs/2304.07333</link><description>&lt;p&gt;
ChatGPT&#30340;&#33258;&#25105;&#35748;&#30693;&#21644;&#25919;&#27835;&#20559;&#35265;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
The Self-Perception and Political Biases of ChatGPT. (arXiv:2304.07333v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;OpenAI&#30340;&#35821;&#35328;&#27169;&#22411;ChatGPT&#36827;&#34892;&#33258;&#25105;&#35748;&#30693;&#21644;&#25919;&#27835;&#20559;&#35265;&#20998;&#26512;&#65292;&#27979;&#35797;&#32467;&#26524;&#26174;&#31034;ChatGPT&#20559;&#21521;&#36827;&#27493;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#25991;&#31456;&#20998;&#26512;OpenAI&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#30340;&#33258;&#25105;&#35748;&#30693;&#21644;&#25919;&#27835;&#20559;&#35265;&#12290;&#32771;&#34385;&#21040;&#24050;&#32463;&#20986;&#29616;&#30340;&#31532;&#19968;&#20010;&#23567;&#35268;&#27169;&#25253;&#21578;&#21644;&#30740;&#31350;&#22768;&#31216;ChatGPT&#22312;&#25919;&#27835;&#19978;&#20559;&#21521;&#36827;&#27493;&#21644;&#33258;&#30001;&#20027;&#20041;&#35266;&#28857;&#65292;&#26412;&#25991;&#26088;&#22312;&#36827;&#19968;&#27493;&#28548;&#28165;&#36825;&#19968;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#20316;&#32773;&#35753;ChatGPT&#22238;&#31572;&#25919;&#27835;&#32599;&#30424;&#27979;&#35797;&#31561;&#31867;&#20284;&#38382;&#21367;&#65292;&#24182;&#38024;&#23545;G7&#25104;&#21592;&#22269;&#30340;&#29305;&#23450;&#25919;&#27835;&#36827;&#34892;&#27979;&#35797;&#65292;&#27599;&#20010;&#27979;&#35797;&#37325;&#22797;&#21313;&#27425;&#65292;&#21457;&#29616;ChatGPT&#20284;&#20046;&#23545;&#36827;&#27493;&#35266;&#28857;&#20855;&#26377;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
This contribution analyzes the self-perception and political biases of OpenAI's Large Language Model ChatGPT. Taking into account the first small-scale reports and studies that have emerged, claiming that ChatGPT is politically biased towards progressive and libertarian points of view, this contribution aims to provide further clarity on this subject. For this purpose, ChatGPT was asked to answer the questions posed by the political compass test as well as similar questionnaires that are specific to the respective politics of the G7 member states. These eight tests were repeated ten times each and revealed that ChatGPT seems to hold a bias towards progressive views. The political compass test revealed a bias towards progressive and libertarian views, with the average coordinates on the political compass being (-6.48, -5.99) (with (0, 0) the center of the compass, i.e., centrism and the axes ranging from -10 to 10), supporting the claims of prior research. The political questionnaires f
&lt;/p&gt;</description></item><item><title>&#37322;&#25918;&#20102;OpenAssistant Conversations&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#20840;&#29699;&#36229;&#36807;1,000&#21517;&#21442;&#19982;&#32773;&#36827;&#34892;&#20154;&#24037;&#29983;&#25104;&#21644;&#20154;&#24037;&#27880;&#37322;&#30340;&#21161;&#25163;&#39118;&#26684;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#21487;&#20197;&#36890;&#36807;SFT&#21644;RLHF&#26377;&#25928;&#22320;&#29992;&#20110;LLM&#23545;&#40784;&#65292;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#21487;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.07327</link><description>&lt;p&gt;
OpenAssistant Conversations -- &#27665;&#20027;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
OpenAssistant Conversations -- Democratizing Large Language Model Alignment. (arXiv:2304.07327v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07327
&lt;/p&gt;
&lt;p&gt;
&#37322;&#25918;&#20102;OpenAssistant Conversations&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#20840;&#29699;&#36229;&#36807;1,000&#21517;&#21442;&#19982;&#32773;&#36827;&#34892;&#20154;&#24037;&#29983;&#25104;&#21644;&#20154;&#24037;&#27880;&#37322;&#30340;&#21161;&#25163;&#39118;&#26684;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#21487;&#20197;&#36890;&#36807;SFT&#21644;RLHF&#26377;&#25928;&#22320;&#29992;&#20110;LLM&#23545;&#40784;&#65292;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#25216;&#26415;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#21487;&#29992;&#24615;&#24182;&#25512;&#21160;&#20854;&#24555;&#36895;&#24212;&#29992;&#65292;&#22914;ChatGPT&#25152;&#31034;&#12290; &#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#21644;&#26681;&#25454;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#31561;&#23545;&#40784;&#25216;&#26415;&#22823;&#22823;&#38477;&#20302;&#20102;&#26377;&#25928;&#21457;&#25381;LLM&#33021;&#21147;&#25152;&#38656;&#30340;&#25216;&#33021;&#21644;&#39046;&#22495;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#21487;&#35775;&#38382;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290; &#28982;&#32780;&#65292;&#20687;RLHF&#36825;&#26679;&#30340;&#26368;&#20808;&#36827;&#30340;&#23545;&#40784;&#25216;&#26415;&#20381;&#36182;&#20110;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#21453;&#39304;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#24448;&#24448;&#26114;&#36149;&#19988;&#20445;&#23494;&#12290; &#20026;&#20102;&#27665;&#20027;&#21270;&#22823;&#35268;&#27169;&#23545;&#40784;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;OpenAssistant Conversations&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#20840;&#29699;&#36229;&#36807;1,000&#21517;&#21442;&#19982;&#32773;&#36827;&#34892;&#20154;&#24037;&#29983;&#25104;&#21644;&#20154;&#24037;&#27880;&#37322;&#30340;&#21161;&#25163;&#39118;&#26684;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;161,443&#26465;&#28040;&#24687;&#65292;&#20998;&#24067;&#22312;66,497&#20010;&#23545;&#35805;&#26641;&#20013;&#65292;&#24182;&#22312;35&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#20013;&#29992;461,292&#20010;&#36136;&#37327;&#35780;&#20998;&#36827;&#34892;&#27880;&#37322;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;OpenAssistant Conversations&#21487;&#20197;&#36890;&#36807;SFT&#21644;RLHF&#26377;&#25928;&#22320;&#29992;&#20110;LLM&#23545;&#40784;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#21487;&#29992;&#24615;&#12290;&#25105;&#20204;&#21457;&#24067;&#35821;&#26009;&#24211;&#65292;&#20351;&#26356;&#24191;&#27867;&#30340;&#30740;&#31350;&#31038;&#21306;&#33021;&#22815;&#36827;&#19968;&#27493;&#30740;&#31350;&#27665;&#20027;&#21270;LLM&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#20154;&#31867;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning large language models (LLMs) with human preferences has proven to drastically improve usability and has driven rapid adoption as demonstrated by ChatGPT. Alignment techniques such as supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) greatly reduce the required skill and domain knowledge to effectively harness the capabilities of LLMs, increasing their accessibility and utility across various domains. However, state-of-the-art alignment techniques like RLHF rely on high-quality human feedback data, which is expensive to create and often remains proprietary. In an effort to democratize research on large-scale alignment, we release OpenAssistant Conversations, a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages distributed across 66,497 conversation trees, in 35 different languages, annotated with 461,292 quality ratings. The corpus is a product of a worldwide crowd-sourcing effort involving over 1
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35814;&#32454;&#30740;&#31350;&#20102;STEGO&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#21644;&#22521;&#35757;&#31574;&#30053;&#65292;&#25581;&#31034;&#20102;&#20854;&#36827;&#34892;&#26080;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#30340;&#24037;&#20316;&#21407;&#29702;&#65292;&#21487;&#29992;&#20110;&#26032;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.07314</link><description>&lt;p&gt;
&#25581;&#31034;STEGO&#36827;&#34892;&#23433;&#20840;&#26080;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#30340;&#20869;&#37096;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Uncovering the Inner Workings of STEGO for Safe Unsupervised Semantic Segmentation. (arXiv:2304.07314v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07314
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35814;&#32454;&#30740;&#31350;&#20102;STEGO&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#21644;&#22521;&#35757;&#31574;&#30053;&#65292;&#25581;&#31034;&#20102;&#20854;&#36827;&#34892;&#26080;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#30340;&#24037;&#20316;&#21407;&#29702;&#65292;&#21487;&#29992;&#20110;&#26032;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#31574;&#30053;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#35757;&#32451;&#36890;&#29992;&#29305;&#24449;&#25552;&#21462;&#39592;&#24178;&#32593;&#32476;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;&#32467;&#21512;Vision Transformer&#26550;&#26500;&#65292;DINO&#33258;&#33976;&#39311;&#25216;&#26415;&#20855;&#26377;&#26377;&#36259;&#30340;&#26032;&#20852;&#29305;&#24615;&#65292;&#22914;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#26080;&#30417;&#30563;&#32858;&#31867;&#21644;&#29983;&#25104;&#29305;&#24449;&#30340;&#35821;&#20041;&#23545;&#24212;&#65292;&#32780;&#19981;&#38656;&#35201;&#20351;&#29992;&#26174;&#24335;&#30340;&#20154;&#24037;&#26631;&#27880;&#26631;&#31614;&#12290;&#26080;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#30340;STEGO&#26041;&#27861;&#36890;&#36807;&#23545;DINO&#39044;&#35757;&#32451;&#30340;Vision Transformer&#30340;&#29305;&#24449;&#23545;&#24212;&#36827;&#34892;&#23545;&#27604;&#33976;&#39311;&#65292;&#26368;&#36817;&#21019;&#36896;&#20102;&#19968;&#20010;&#26032;&#30340;&#26368;&#20248;&#24615;&#12290;&#28982;&#32780;&#65292;STEGO&#30340;&#35814;&#32454;&#24037;&#20316;&#26426;&#21046;&#23578;&#26410;&#24471;&#21040;&#20998;&#35299;&#65292;&#38459;&#30861;&#20102;&#23427;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#36827;&#34892;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;STEGO&#26550;&#26500;&#21644;&#22521;&#35757;&#31574;&#30053;&#30340;&#24037;&#20316;&#26426;&#21046;&#65292;&#37325;&#29616;&#21644;&#25193;&#23637;&#20854;&#23454;&#39564;&#35777;&#65292;&#20197;&#21450;&#30740;&#31350;STEGO&#22312;&#26032;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised pre-training strategies have recently shown impressive results for training general-purpose feature extraction backbones in computer vision. In combination with the Vision Transformer architecture, the DINO self-distillation technique has interesting emerging properties, such as unsupervised clustering in the latent space and semantic correspondences of the produced features without using explicit human-annotated labels. The STEGO method for unsupervised semantic segmentation contrastively distills feature correspondences of a DINO-pre-trained Vision Transformer and recently set a new state of the art. However, the detailed workings of STEGO have yet to be disentangled, preventing its usage in safety-critical applications. This paper provides a deeper understanding of the STEGO architecture and training strategy by conducting studies that uncover the working mechanisms behind STEGO, reproduce and extend its experimental validation, and investigate the ability of STEGO t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#19968;&#20221;&#38754;&#21521;&#30005;&#23376;&#30149;&#21382;&#21644;&#32467;&#26500;&#21270;&#21307;&#30103;&#25968;&#25454;&#30340;&#32852;&#37030;&#19982;&#20998;&#24067;&#24335;&#23398;&#20064;&#24212;&#29992;&#30340;&#33539;&#22260;&#23457;&#26597;&#65292;&#30740;&#31350;&#21457;&#29616;FL&#24212;&#29992;&#20110;&#32467;&#26500;&#21270;&#21307;&#23398;&#25968;&#25454;&#30340;&#30740;&#31350;&#22823;&#22810;&#38598;&#20013;&#22312;&#38544;&#31169;&#20445;&#25252;&#26041;&#38754;&#65292;&#26410;&#26469;&#24212;&#35813;&#25506;&#32034;FL&#22312;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#21644;&#25552;&#39640;&#27169;&#22411;&#36890;&#29992;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.07310</link><description>&lt;p&gt;
&#38754;&#21521;&#30005;&#23376;&#30149;&#21382;&#21644;&#32467;&#26500;&#21270;&#21307;&#30103;&#25968;&#25454;&#30340;&#32852;&#37030;&#19982;&#20998;&#24067;&#24335;&#23398;&#20064;&#24212;&#29992;&#65306;&#33539;&#22260;&#23457;&#26597;
&lt;/p&gt;
&lt;p&gt;
Federated and distributed learning applications for electronic health records and structured medical data: A scoping review. (arXiv:2304.07310v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#19968;&#20221;&#38754;&#21521;&#30005;&#23376;&#30149;&#21382;&#21644;&#32467;&#26500;&#21270;&#21307;&#30103;&#25968;&#25454;&#30340;&#32852;&#37030;&#19982;&#20998;&#24067;&#24335;&#23398;&#20064;&#24212;&#29992;&#30340;&#33539;&#22260;&#23457;&#26597;&#65292;&#30740;&#31350;&#21457;&#29616;FL&#24212;&#29992;&#20110;&#32467;&#26500;&#21270;&#21307;&#23398;&#25968;&#25454;&#30340;&#30740;&#31350;&#22823;&#22810;&#38598;&#20013;&#22312;&#38544;&#31169;&#20445;&#25252;&#26041;&#38754;&#65292;&#26410;&#26469;&#24212;&#35813;&#25506;&#32034;FL&#22312;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#21644;&#25552;&#39640;&#27169;&#22411;&#36890;&#29992;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36817;&#24180;&#26469;&#30340;&#20020;&#24202;&#30740;&#31350;&#20013;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#32463;&#22312;&#38544;&#31169;&#20445;&#25252;&#21327;&#20316;&#26041;&#38754;&#33719;&#24471;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#38543;&#30528;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#32467;&#26500;&#21270;&#25968;&#25454;&#20316;&#20026;&#26368;&#24120;&#35265;&#30340;&#20020;&#24202;&#25968;&#25454;&#24418;&#24335;&#20043;&#19968;&#65292;&#21516;&#26102;&#20063;&#32463;&#21382;&#20102;&#26174;&#30528;&#30340;&#22686;&#38271;&#12290;&#26412;&#30740;&#31350;&#26816;&#26597;&#20102;FL&#24212;&#29992;&#20110;&#32467;&#26500;&#21270;&#21307;&#23398;&#25968;&#25454;&#30340;&#24212;&#29992;&#65292;&#30830;&#23450;&#20102;&#24403;&#20195;&#30340;&#38480;&#21046;&#65292;&#24182;&#35752;&#35770;&#20102;&#28508;&#22312;&#30340;&#21019;&#26032;&#12290;&#25105;&#20204;&#22312;SCOPUS&#12289;MEDLINE&#12289;Web of Science&#12289;Embase&#21644;CINAHL&#31561;&#20116;&#20010;&#25968;&#25454;&#24211;&#20013;&#36827;&#34892;&#25628;&#32034;&#65292;&#20197;&#35782;&#21035;&#23558;FL&#24212;&#29992;&#20110;&#32467;&#26500;&#21270;&#21307;&#23398;&#25968;&#25454;&#24182;&#26681;&#25454;PRISMA&#20934;&#21017;&#25253;&#21578;&#32467;&#26524;&#30340;&#25991;&#31456;&#12290;&#27599;&#31687;&#36873;&#23450;&#30340;&#20986;&#29256;&#29289;&#20174;&#25968;&#25454;&#36136;&#37327;&#12289;&#24314;&#27169;&#31574;&#30053;&#21644;FL&#26694;&#26550;&#19977;&#20010;&#20027;&#35201;&#35282;&#24230;&#36827;&#34892;&#35780;&#20272;&#12290;&#32463;&#36807;&#31579;&#36873;&#65292;34&#31687;&#25991;&#31456;&#31526;&#21512;&#32435;&#20837;&#26631;&#20934;&#65292;&#27599;&#31687;&#25991;&#31456;&#21253;&#25324;&#19968;&#20010;&#25110;&#22810;&#20010;&#20351;&#29992;FL&#22788;&#29702;&#32467;&#26500;&#21270;&#20020;&#24202;/&#21307;&#23398;&#25968;&#25454;&#30340;&#30740;&#31350;&#12290;&#20854;&#20013;18&#20010;&#30740;&#31350;&#24212;&#29992;&#20102;FL&#26469;&#35757;&#32451;&#39044;&#27979;&#27169;&#22411;&#65292;&#32780;&#20165;&#26377;2&#20010;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;FL&#36827;&#34892;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#30340;&#24212;&#29992;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#38598;&#20013;&#22312;&#20445;&#25252;&#38544;&#31169;&#26041;&#38754;&#65292;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#25506;&#32034;&#20102;FL&#30340;&#20854;&#20182;&#20248;&#28857;&#65292;&#22914;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#21644;&#25552;&#39640;&#27169;&#22411;&#36890;&#29992;&#24615;&#12290;&#26412;&#33539;&#22260;&#23457;&#26597;&#26368;&#21518;&#35752;&#35770;&#20102;FL&#22312;&#32467;&#26500;&#21270;&#21307;&#30103;&#25968;&#25454;&#32972;&#26223;&#19979;&#30340;&#25361;&#25112;&#12289;&#38480;&#21046;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has gained popularity in clinical research in recent years to facilitate privacy-preserving collaboration. Structured data, one of the most prevalent forms of clinical data, has experienced significant growth in volume concurrently, notably with the widespread adoption of electronic health records in clinical practice. This review examines FL applications on structured medical data, identifies contemporary limitations and discusses potential innovations. We searched five databases, SCOPUS, MEDLINE, Web of Science, Embase, and CINAHL, to identify articles that applied FL to structured medical data and reported results following the PRISMA guidelines. Each selected publication was evaluated from three primary perspectives, including data quality, modeling strategies, and FL frameworks. Out of the 1160 papers screened, 34 met the inclusion criteria, with each article consisting of one or more studies that used FL to handle structured clinical/medical data. Of these
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#26041;&#27861;&#65292;&#29992;&#20110;&#38477;&#20302;&#23398;&#20064;&#25512;&#36831;&#31639;&#27861;&#25152;&#38656;&#30340;&#19987;&#23478;&#39044;&#27979;&#25968;&#37327;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#23884;&#20837;&#27169;&#22411;&#12289;&#20351;&#29992;&#23569;&#37327;&#30340;&#19987;&#23478;&#39044;&#27979;&#21644;&#21033;&#29992;&#26679;&#26412;&#30456;&#20284;&#24615;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07306</link><description>&lt;p&gt;
&#26377;&#38480;&#19987;&#23478;&#39044;&#27979;&#19979;&#30340;&#23398;&#20064;&#25512;&#36831;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Learning to Defer with Limited Expert Predictions. (arXiv:2304.07306v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#26041;&#27861;&#65292;&#29992;&#20110;&#38477;&#20302;&#23398;&#20064;&#25512;&#36831;&#31639;&#27861;&#25152;&#38656;&#30340;&#19987;&#23478;&#39044;&#27979;&#25968;&#37327;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#23884;&#20837;&#27169;&#22411;&#12289;&#20351;&#29992;&#23569;&#37327;&#30340;&#19987;&#23478;&#39044;&#27979;&#21644;&#21033;&#29992;&#26679;&#26412;&#30456;&#20284;&#24615;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#19982;&#20154;&#31867;&#19987;&#23478;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#36229;&#20986;&#21333;&#29420;&#24212;&#29992;&#30340;&#24615;&#33021;&#12290;&#20182;&#20204;&#30340;&#33021;&#21147;&#32467;&#21512;&#36890;&#24120;&#36890;&#36807;&#23398;&#20064;&#25512;&#36831;&#31639;&#27861;&#23454;&#29616;&#65292;&#36825;&#20351;&#24471;&#20154;&#24037;&#26234;&#33021;&#23398;&#20250;&#20915;&#23450;&#26159;&#21542;&#20026;&#29305;&#23450;&#26679;&#26412;&#20570;&#20986;&#39044;&#27979;&#25110;&#23558;&#20854;&#25512;&#36831;&#32473;&#20154;&#31867;&#19987;&#23478;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20934;&#30830;&#22320;&#23398;&#20064;&#21738;&#20123;&#26679;&#26412;&#24212;&#35813;&#25512;&#36831;&#32473;&#20154;&#31867;&#19987;&#23478;&#65292;&#38656;&#35201;&#22823;&#37327;&#20934;&#30830;&#21453;&#26144;&#19987;&#23478;&#33021;&#21147;&#30340;&#19987;&#23478;&#39044;&#27979;&#65292;&#38500;&#20102;&#35757;&#32451;&#20154;&#24037;&#26234;&#33021;&#25152;&#38656;&#30340;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#12290;&#36825;&#26159;&#35768;&#22810;&#23398;&#20064;&#25512;&#36831;&#31639;&#27861;&#20849;&#20139;&#30340;&#35201;&#27714;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#36131;&#20219;&#19987;&#23478;&#32463;&#24120;&#26356;&#25913;&#25110;&#33719;&#24471;&#36275;&#22815;&#30340;&#19987;&#23478;&#39044;&#27979;&#30340;&#25104;&#26412;&#36739;&#39640;&#30340;&#22330;&#26223;&#20013;&#30340;&#37319;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#26041;&#27861;&#26469;&#20943;&#23569;&#23398;&#20064;&#25512;&#36831;&#31639;&#27861;&#25152;&#38656;&#30340;&#19987;&#23478;&#39044;&#27979;&#25968;&#37327;&#12290;&#23427;&#21253;&#25324;&#65288;1&#65289;&#20351;&#29992;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#35757;&#32451;&#23884;&#20837;&#27169;&#22411;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recent research suggests that combining AI models with a human expert can exceed the performance of either alone. The combination of their capabilities is often realized by learning to defer algorithms that enable the AI to learn to decide whether to make a prediction for a particular instance or defer it to the human expert. However, to accurately learn which instances should be deferred to the human expert, a large number of expert predictions that accurately reflect the expert's capabilities are required -- in addition to the ground truth labels needed to train the AI. This requirement shared by many learning to defer algorithms hinders their adoption in scenarios where the responsible expert regularly changes or where acquiring a sufficient number of expert predictions is costly. In this paper, we propose a three-step approach to reduce the number of expert predictions required to train learning to defer algorithms. It encompasses (1) the training of an embedding model with ground 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#39044;&#27979;&#33778;&#24459;&#23486;&#39532;&#23612;&#25289;&#22320;&#38081;&#19977;&#21495;&#32447;&#27599;&#22825;&#20056;&#23458;&#37327;&#30340;&#21464;&#21270;&#65292;&#24110;&#21161;&#20154;&#20204;&#26356;&#22909;&#22320;&#35268;&#21010;&#20986;&#34892;&#12290;</title><link>http://arxiv.org/abs/2304.07303</link><description>&lt;p&gt;
&#26234;&#24935;&#22320;&#38081;&#65306;&#28145;&#24230;&#23398;&#20064;&#22312;&#39044;&#27979;&#33778;&#24459;&#23486;&#39532;&#23612;&#25289;&#22320;&#38081;&#19977;&#21495;&#32447;&#20056;&#23458;&#37327;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Smart Metro: Deep Learning Approaches to Forecasting the MRT Line 3 Ridership. (arXiv:2304.07303v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#39044;&#27979;&#33778;&#24459;&#23486;&#39532;&#23612;&#25289;&#22320;&#38081;&#19977;&#21495;&#32447;&#27599;&#22825;&#20056;&#23458;&#37327;&#30340;&#21464;&#21270;&#65292;&#24110;&#21161;&#20154;&#20204;&#26356;&#22909;&#22320;&#35268;&#21010;&#20986;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;1999&#24180;&#24314;&#31435;&#20197;&#26469;&#65292;&#33778;&#24459;&#23486;&#39532;&#23612;&#25289;&#22320;&#38081;&#19977;&#21495;&#32447;&#65288;MRT3&#65289;&#20026;&#20247;&#22810;&#20056;&#23458;&#25552;&#20379;&#20102;&#20132;&#36890;&#36873;&#25321;&#12290;&#33778;&#24459;&#23486;&#25919;&#24220;&#20132;&#36890;&#37096;&#35760;&#24405;&#27599;&#22825;&#26377;&#36229;&#36807;&#19968;&#21315;&#20154;&#20351;&#29992;MRT3&#65292;&#39044;&#27979;&#27599;&#22825;&#30340;&#20056;&#23458;&#25968;&#37327;&#21487;&#33021;&#30456;&#24403;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21463;&#20551;&#26399;&#12289;&#24037;&#20316;&#26085;&#21644;&#20854;&#20182;&#24847;&#22806;&#38382;&#39064;&#31561;&#21464;&#37327;&#24433;&#21709;&#65292;MRT3&#30340;&#26085;&#24120;&#20056;&#23458;&#37327;&#27874;&#21160;&#36739;&#22823;&#12290;&#20056;&#23458;&#26080;&#27861;&#30693;&#36947;&#26576;&#19968;&#22825;&#20182;&#20204;&#25152;&#20056;&#36335;&#32447;&#19978;&#26377;&#22810;&#23569;&#20854;&#20182;&#20056;&#23458;&#65292;&#36825;&#21487;&#33021;&#20250;&#22952;&#30861;&#20182;&#20204;&#35268;&#21010;&#39640;&#25928;&#34892;&#31243;&#30340;&#33021;&#21147;&#12290;&#30446;&#21069;&#65292;&#33778;&#24459;&#23486;&#20132;&#36890;&#36816;&#36755;&#37096;&#20381;&#36182;&#21253;&#21547;&#21382;&#21490;&#25968;&#25454;&#30340;&#30005;&#23376;&#34920;&#26684;&#65292;&#36825;&#21487;&#33021;&#38590;&#20197;&#26816;&#26597;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;&#29305;&#23450;&#26085;&#26399;&#26576;&#20010;&#36710;&#31449;&#30340;&#26410;&#26469;&#20986;&#21220;&#20154;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since its establishment in 1999, the Metro Rail Transit Line 3 (MRT3) has served as a transportation option for numerous passengers in Metro Manila, Philippines. The Philippine government's transportation department records more than a thousand people using the MRT3 daily and forecasting the daily passenger count may be rather challenging. The MRT3's daily ridership fluctuates owing to variables such as holidays, working days, and other unexpected issues. Commuters do not know how many other commuters are on their route on a given day, which may hinder their ability to plan an efficient itinerary. Currently, the DOTr depends on spreadsheets containing historical data, which might be challenging to examine. This study presents a time series prediction of daily traffic to anticipate future attendance at a particular station on specific days.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#20102;&#20083;&#33146;&#30284;&#24739;&#32773;&#30340;5&#24180;&#29983;&#23384;&#29575;&#65292;&#24182;&#27604;&#36739;&#20102;&#19971;&#31181;&#20998;&#31867;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#22320;&#39044;&#27979;&#27979;&#35797;&#26679;&#26412;&#30340;&#29983;&#23384;&#29575;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#25104;&#20026;&#20083;&#33146;&#30284;&#29983;&#23384;&#39044;&#27979;&#21644;&#39118;&#38505;&#22240;&#32032;&#20998;&#26512;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2304.07299</link><description>&lt;p&gt;
&#29992;&#20110;&#20083;&#33146;&#30284;&#39118;&#38505;&#20998;&#26512;&#21644;&#29983;&#23384;&#39044;&#27979;&#30340;&#30417;&#30563;&#23398;&#20064;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Supervised Machine Learning for Breast Cancer Risk Factors Analysis and Survival Prediction. (arXiv:2304.07299v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#20102;&#20083;&#33146;&#30284;&#24739;&#32773;&#30340;5&#24180;&#29983;&#23384;&#29575;&#65292;&#24182;&#27604;&#36739;&#20102;&#19971;&#31181;&#20998;&#31867;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#22320;&#39044;&#27979;&#27979;&#35797;&#26679;&#26412;&#30340;&#29983;&#23384;&#29575;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#25104;&#20026;&#20083;&#33146;&#30284;&#29983;&#23384;&#39044;&#27979;&#21644;&#39118;&#38505;&#22240;&#32032;&#20998;&#26512;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#26368;&#26377;&#25928;&#30340;&#27835;&#30103;&#26041;&#26696;&#21487;&#33021;&#20250;&#21463;&#21040;&#20083;&#33146;&#30284;&#29983;&#23384;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#39044;&#27979;&#24739;&#32773;&#23384;&#27963;&#30340;&#26426;&#20250;&#65292;&#37319;&#29992;&#20102;&#22810;&#31181;&#25216;&#26415;&#65292;&#22914;&#32479;&#35745;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#31561;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;METABRIC&#25968;&#25454;&#38598;&#20013;&#30340;1904&#26465;&#24739;&#32773;&#35760;&#24405;&#65292;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;5&#24180;&#20083;&#33146;&#30284;&#29983;&#23384;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19971;&#31181;&#20998;&#31867;&#27169;&#22411;&#30340;&#32467;&#26524;&#65292;&#35780;&#20272;&#23427;&#20204;&#30340;&#34920;&#29616;&#22914;&#19979;&#65306;&#21484;&#22238;&#29575;&#12289;AUC&#12289;&#28151;&#28102;&#30697;&#38453;&#12289;&#20934;&#30830;&#24230;&#12289;&#31934;&#30830;&#24230;&#12289;&#20551;&#27491;&#29575;&#21644;&#30495;&#27491;&#29575;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;Logistic&#22238;&#24402;(LR)&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;(SVM)&#12289;&#20915;&#31574;&#26641;(DT)&#12289;&#38543;&#26426;&#26862;&#26519;(RD)&#12289;&#26497;&#31471;&#38543;&#26426;&#21270;&#26641;(ET)&#12289;K-&#36817;&#37051;(KNN)&#21644;&#33258;&#36866;&#24212;&#22686;&#24378;(AdaBoost)&#30340;&#20998;&#31867;&#22120;&#21487;&#20197;&#20934;&#30830;&#22320;&#39044;&#27979;&#27979;&#35797;&#26679;&#26412;&#30340;&#29983;&#23384;&#29575;&#65292;&#20998;&#21035;&#26159;75.4&#65285;&#12289;74.7&#65285;&#12289;71.5&#65285;&#12289;75.5&#65285;&#12289;70.3&#65285;&#12289;72.5&#65285;&#21644;72.2&#65285;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#25104;&#20026;&#20083;&#33146;&#30284;&#29983;&#23384;&#39044;&#27979;&#21644;&#39118;&#38505;&#22240;&#32032;&#20998;&#26512;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
The choice of the most effective treatment may eventually be influenced by breast cancer survival prediction. To predict the chances of a patient surviving, a variety of techniques were employed, such as statistical, machine learning, and deep learning models. In the current study, 1904 patient records from the METABRIC dataset were utilized to predict a 5-year breast cancer survival using a machine learning approach. In this study, we compare the outcomes of seven classification models to evaluate how well they perform using the following metrics: recall, AUC, confusion matrix, accuracy, precision, false positive rate, and true positive rate. The findings demonstrate that the classifiers for Logistic Regression (LR), Support Vector Machines (SVM), Decision Tree (DT), Random Forest (RD), Extremely Randomized Trees (ET), K-Nearest Neighbor (KNN), and Adaptive Boosting (AdaBoost) can accurately predict the survival rate of the tested samples, which is 75,4\%, 74,7\%, 71,5\%, 75,5\%, 70,3
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#37325;&#22270;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20256;&#32479;&#36947;&#36335;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#36947;&#36335;&#20043;&#38388;&#39640;&#38454;&#21644;&#36828;&#31243;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.07298</link><description>&lt;p&gt;
&#36947;&#36335;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#65306;&#19968;&#31181;&#22522;&#20110;&#21452;&#37325;&#22270;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Road Network Representation Learning: A Dual Graph based Approach. (arXiv:2304.07298v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07298
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#37325;&#22270;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20256;&#32479;&#36947;&#36335;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#36947;&#36335;&#20043;&#38388;&#39640;&#38454;&#21644;&#36828;&#31243;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36947;&#36335;&#32593;&#32476;&#26159;&#25903;&#25745;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#65292;&#21253;&#25324;&#20132;&#36890;&#12289;&#31227;&#21160;&#24615;&#21644;&#29289;&#27969;&#30340;&#20851;&#38190;&#22522;&#30784;&#35774;&#26045;&#12290;&#20026;&#20102;&#21033;&#29992;&#36947;&#36335;&#32593;&#32476;&#22312;&#36825;&#20123;&#19981;&#21516;&#30340;&#24212;&#29992;&#20013;&#30340;&#36755;&#20837;&#65292;&#26377;&#24517;&#35201;&#23398;&#20064;&#36947;&#36335;&#30340;&#21521;&#37327;&#34920;&#31034;&#24418;&#24335;&#65292;&#31216;&#20026;&#36947;&#36335;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#65288;RNRL&#65289;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181; RNRL &#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#21482;&#25429;&#25417;&#36947;&#36335;&#20043;&#38388;&#30340;&#25104;&#23545;&#20851;&#31995; / &#36830;&#25509;&#65288;&#21363;&#20316;&#20026;&#31616;&#21333;&#22270;&#65289;&#65292;&#26080;&#27861;&#25429;&#25417;&#36947;&#36335;&#20043;&#38388;&#30340;&#39640;&#38454;&#20851;&#31995;&#65288;&#20363;&#22914;&#65292;&#37027;&#20123;&#20849;&#21516;&#24418;&#25104;&#26412;&#22320;&#21306;&#22495;&#30340;&#36947;&#36335;&#36890;&#24120;&#20855;&#26377;&#31867;&#20284;&#30340;&#29305;&#24449;&#65292;&#20363;&#22914;&#36895;&#24230;&#38480;&#21046;&#65289;&#21644;&#36828;&#31243;&#20851;&#31995;&#65288;&#20363;&#22914;&#65292;&#19968;&#20123;&#30456;&#36317;&#36739;&#36828;&#30340;&#36947;&#36335;&#21487;&#33021;&#20855;&#26377;&#31867;&#20284;&#30340;&#35821;&#20041;&#65292;&#20363;&#22914;&#26159;&#20303;&#23429;&#21306;&#36947;&#36335;&#65289;&#12290;&#20986;&#20110;&#36825;&#20010;&#21407;&#22240;&#65292;&#25105;&#20204;&#25552;&#20986;&#26500;&#24314;&#19968;&#20010;&#8220;&#36229;&#22270;&#8221;&#65292;&#20854;&#20013;&#27599;&#20010;&#36229;&#36793;&#23545;&#24212;&#20110;&#26500;&#25104;&#21306;&#22495;&#30340;&#22810;&#26465;&#36947;&#36335;&#30340;&#38598;&#21512;&#12290;&#26500;&#24314;&#30340;&#36229;&#22270;&#20250;&#33258;&#28982;&#22320;&#25429;&#25417;&#21040;&#36947;&#36335;&#20043;&#38388;&#30340;&#39640;&#38454;&#20851;&#31995;&#21644;&#36828;&#31243;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Road network is a critical infrastructure powering many applications including transportation, mobility and logistics in real life. To leverage the input of a road network across these different applications, it is necessary to learn the representations of the roads in the form of vectors, which is named \emph{road network representation learning} (RNRL). While several models have been proposed for RNRL, they capture the pairwise relationships/connections among roads only (i.e., as a simple graph), and fail to capture among roads the high-order relationships (e.g., those roads that jointly form a local region usually have similar features such as speed limit) and long-range relationships (e.g., some roads that are far apart may have similar semantics such as being roads in residential areas). Motivated by this, we propose to construct a \emph{hypergraph}, where each hyperedge corresponds to a set of multiple roads forming a region. The constructed hypergraph would naturally capture the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20043;&#20026;instructRL&#30340;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26469;&#25351;&#23450;&#23545;&#20154;&#24037;&#26234;&#33021;&#25645;&#26723;&#30340;&#39044;&#26399;&#31574;&#30053;&#65292;&#35299;&#20915;&#22312;&#32570;&#20047;&#39640;&#36136;&#37327;&#20154;&#31867;&#34892;&#20026;&#25968;&#25454;&#30340;&#39046;&#22495;&#20013;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#25910;&#25947;&#20110;&#20154;&#31867;&#19981;&#20559;&#29233;&#30340;&#31574;&#30053;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07297</link><description>&lt;p&gt;
&#35821;&#35328;&#25351;&#23548;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#20197;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Language Instructed Reinforcement Learning for Human-AI Coordination. (arXiv:2304.07297v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20043;&#20026;instructRL&#30340;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26469;&#25351;&#23450;&#23545;&#20154;&#24037;&#26234;&#33021;&#25645;&#26723;&#30340;&#39044;&#26399;&#31574;&#30053;&#65292;&#35299;&#20915;&#22312;&#32570;&#20047;&#39640;&#36136;&#37327;&#20154;&#31867;&#34892;&#20026;&#25968;&#25454;&#30340;&#39046;&#22495;&#20013;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#25910;&#25947;&#20110;&#20154;&#31867;&#19981;&#20559;&#29233;&#30340;&#31574;&#30053;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#22914;&#20309;&#35753;&#26234;&#33021;&#20307;&#33021;&#22815;&#21644;&#20154;&#31867;&#26377;&#25928;&#22320;&#21327;&#20316;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20043;&#20026;instructRL&#30340;&#26032;&#30340;&#26694;&#26550;&#65292;&#35753;&#20154;&#20204;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26469;&#25351;&#23450;&#23545;&#20154;&#24037;&#26234;&#33021;&#25645;&#26723;&#30340;&#39044;&#26399;&#31574;&#30053;&#65292;&#20197;&#27492;&#35299;&#20915;&#22312;&#32570;&#20047;&#36739;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#34892;&#20026;&#25968;&#25454;&#30340;&#39046;&#22495;&#20013;&#65292;&#30001;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#24120;&#24120;&#20250;&#25910;&#25947;&#21040;&#20154;&#31867;&#24182;&#19981;&#20559;&#29233;&#30340;&#31574;&#30053;&#30340;&#19981;&#36275;&#12290;&#25105;&#20204;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#19968;&#20010;&#22312;&#20154;&#31867;&#25351;&#20196;&#19979;&#30340;&#20808;&#39564;&#31574;&#30053;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#30446;&#26631;&#12290;&#36825;&#23548;&#33268;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#25910;&#25947;&#21040;&#19982;&#20154;&#31867;&#21916;&#22909;&#19968;&#33268;&#30340;&#22343;&#34913;&#28857;&#12290;&#36890;&#36807;&#27010;&#24565;&#35777;&#26126;&#29615;&#22659;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;Hanabi&#22522;&#20934;&#65292;&#35777;&#26126;&#20102;instructRL&#25910;&#25947;&#20110;&#28385;&#36275;&#32473;&#23450;&#25351;&#20196;&#30340;&#31867;&#20284;&#20154;&#31867;&#26234;&#33021;&#20307;&#30340;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30693;&#36947;&#35821;&#35328;&#25351;&#20196;&#26174;&#33879;&#25552;&#39640;&#20102;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the fundamental quests of AI is to produce agents that coordinate well with humans. This problem is challenging, especially in domains that lack high quality human behavioral data, because multi-agent reinforcement learning (RL) often converges to different equilibria from the ones that humans prefer. We propose a novel framework, instructRL, that enables humans to specify what kind of strategies they expect from their AI partners through natural language instructions. We use pretrained large language models to generate a prior policy conditioned on the human instruction and use the prior to regularize the RL objective. This leads to the RL agent converging to equilibria that are aligned with human preferences. We show that instructRL converges to human-like policies that satisfy the given instructions in a proof-of-concept environment as well as the challenging Hanabi benchmark. Finally, we show that knowing the language instruction significantly boosts human-AI coordination pe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19987;&#23478;&#35748;&#30693;&#39537;&#21160;&#30340;&#23433;&#20840;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20083;&#33146;&#30284;&#27531;&#20313;&#32959;&#30244;&#30340;&#31934;&#30830;&#20998;&#21106;&#65292;&#35813;&#26041;&#27861;&#23558;&#30149;&#29702;&#19987;&#23478;&#30340;&#35748;&#30693;&#21644;&#20154;&#24037;&#26234;&#33021;&#19987;&#23478;&#30340;&#25968;&#25454;&#24314;&#27169;&#35748;&#30693;&#30456;&#32467;&#21512;&#65292;&#20197;&#32531;&#35299;&#20083;&#33146;&#30284;&#32452;&#32455;&#21644;&#32959;&#30244;&#32454;&#32990;&#24418;&#24577;&#23398;&#25913;&#21464;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.07295</link><description>&lt;p&gt;
&#22522;&#20110;&#19987;&#23478;&#35748;&#30693;&#39537;&#21160;&#30340;&#23433;&#20840;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#29992;&#20110;&#20083;&#33146;&#30284;&#27531;&#20313;&#32959;&#30244;&#30340;&#31934;&#30830;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Experts' cognition-driven safe noisy labels learning for precise segmentation of residual tumor in breast cancer. (arXiv:2304.07295v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19987;&#23478;&#35748;&#30693;&#39537;&#21160;&#30340;&#23433;&#20840;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20083;&#33146;&#30284;&#27531;&#20313;&#32959;&#30244;&#30340;&#31934;&#30830;&#20998;&#21106;&#65292;&#35813;&#26041;&#27861;&#23558;&#30149;&#29702;&#19987;&#23478;&#30340;&#35748;&#30693;&#21644;&#20154;&#24037;&#26234;&#33021;&#19987;&#23478;&#30340;&#25968;&#25454;&#24314;&#27169;&#35748;&#30693;&#30456;&#32467;&#21512;&#65292;&#20197;&#32531;&#35299;&#20083;&#33146;&#30284;&#32452;&#32455;&#21644;&#32959;&#30244;&#32454;&#32990;&#24418;&#24577;&#23398;&#25913;&#21464;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#36741;&#21161;&#21270;&#30103;&#21518;&#20934;&#30830;&#20998;&#21106;&#20083;&#33146;&#30284;&#27531;&#20313;&#32959;&#30244; (PSRTBC) &#26159;&#20083;&#33146;&#30284;&#27835;&#30103;&#36807;&#31243;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#20851;&#38190;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20083;&#33146;&#30284;&#32452;&#32455;&#21644;&#32959;&#30244;&#32454;&#32990;&#22312;&#26032;&#36741;&#21161;&#21270;&#30103;&#21518;&#24120;&#24120;&#20855;&#26377;&#22797;&#26434;&#21644;&#22810;&#31181;&#22810;&#26679;&#30340;&#24418;&#24577;&#23398;&#25913;&#21464;&#65292;&#22240;&#27492;&#20135;&#29983;&#19968;&#20010;&#20855;&#26377;&#33391;&#22909;&#27867;&#21270;&#24615;&#33021;&#30340;&#39044;&#27979;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#24773;&#20917;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19987;&#23478;&#35748;&#30693;&#39537;&#21160;&#30340;&#23433;&#20840;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064; (ECDSNLL) &#26041;&#27861;&#12290;ECDSNLL&#26159;&#22312;&#23433;&#20840;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#30340;&#27010;&#24565;&#19979;&#26500;&#24314;&#30340;&#65292;&#21518;&#32773;&#26159;&#19968;&#31181;&#20856;&#22411;&#30340;&#23433;&#20840;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#23558;&#30149;&#29702;&#19987;&#23478;&#23545;&#20083;&#33146;&#30284;&#27531;&#20313;&#32959;&#30244;&#35782;&#21035;&#30340;&#35748;&#30693;&#19982;&#25552;&#20379;&#30340;&#25968;&#25454;&#22522;&#30784;&#19978;&#30340;&#20154;&#24037;&#26234;&#33021;&#19987;&#23478;&#30340;&#25968;&#25454;&#24314;&#27169;&#35748;&#30693;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;ECDSNLL&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precise segmentation of residual tumor in breast cancer (PSRTBC) after neoadjuvant chemotherapy is a fundamental key technique in the treatment process of breast cancer. However, achieving PSRTBC is still a challenge, since the breast cancer tissue and tumor cells commonly have complex and varied morphological changes after neoadjuvant chemotherapy, which inevitably increases the difficulty to produce a predictive model that has good generalization with machine learning. To alleviate this situation, in this paper, we propose an experts' cognition-driven safe noisy labels learning (ECDSNLL) approach. In the concept of safe noisy labels learning, which is a typical type of safe weakly supervised learning, ECDSNLL is constructed by integrating the pathology experts' cognition about identifying residual tumor in breast cancer and the artificial intelligence experts' cognition about data modeling with provided data basis. We show the advantages of the proposed ECDSNLL approach and its promi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#29983;&#25104;&#22270;&#20687;&#21387;&#32553;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#26497;&#31471;&#21387;&#32553;&#27604;&#19979;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#35270;&#35273;&#37325;&#24314;&#65292;&#21516;&#26102;&#25903;&#25345;&#21508;&#31181;&#22522;&#20110;&#21387;&#32553;&#22495;&#30340;&#20998;&#26512;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.06896</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#24863;&#30693;&#30340;&#22270;&#20687;&#21387;&#32553;&#65306;&#19968;&#31181;&#20998;&#23618;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Machine Perception-Driven Image Compression: A Layered Generative Approach. (arXiv:2304.06896v1 [eess.IV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#29983;&#25104;&#22270;&#20687;&#21387;&#32553;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#26497;&#31471;&#21387;&#32553;&#27604;&#19979;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#35270;&#35273;&#37325;&#24314;&#65292;&#21516;&#26102;&#25903;&#25345;&#21508;&#31181;&#22522;&#20110;&#21387;&#32553;&#22495;&#30340;&#20998;&#26512;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20010;&#20449;&#24687;&#26102;&#20195;&#65292;&#22270;&#20687;&#26159;&#23384;&#20648;&#21644;&#20256;&#36755;&#20449;&#24687;&#30340;&#37325;&#35201;&#23186;&#20171;&#12290;&#38543;&#30528;&#22270;&#20687;&#25968;&#25454;&#37327;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#35270;&#35273;&#21387;&#32553;&#21644;&#35270;&#35273;&#25968;&#25454;&#24863;&#30693;&#25104;&#20026;&#20102;&#20004;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#20010;&#35838;&#39064;&#24456;&#23569;&#34987;&#19968;&#36215;&#35752;&#35770;&#65292;&#36981;&#24490;&#30528;&#19981;&#21516;&#30340;&#30740;&#31350;&#36335;&#24452;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#29983;&#25104;&#22270;&#20687;&#21387;&#32553;&#27169;&#22411;&#65292;&#21363;&#20351;&#22312;&#26497;&#31471;&#21387;&#32553;&#27604;&#19979;&#65292;&#20063;&#33021;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#35270;&#35273;&#37325;&#24314;&#12290;&#20026;&#20102;&#33719;&#24471;&#20998;&#26512;&#25928;&#29575;&#21644;&#28789;&#27963;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#23398;&#20064;&#22411;&#21387;&#32553;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#25903;&#25345;&#21508;&#31181;&#22522;&#20110;&#21387;&#32553;&#22495;&#30340;&#20998;&#26512;&#20219;&#21153;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#20986;&#33394;&#30340;&#35270;&#35273;&#37325;&#24314;&#24863;&#30693;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this age of information, images are a critical medium for storing and transmitting information. With the rapid growth of image data amount, visual compression and visual data perception are two important research topics attracting a lot attention. However, those two topics are rarely discussed together and follow separate research path. Due to the compact compressed domain representation offered by learning-based image compression methods, there exists possibility to have one stream targeting both efficient data storage and compression, and machine perception tasks. In this paper, we propose a layered generative image compression model achieving high human vision-oriented image reconstructed quality, even at extreme compression ratios. To obtain analysis efficiency and flexibility, a task-agnostic learning-based compression model is proposed, which effectively supports various compressed domain-based analytical tasks while reserves outstanding reconstructed perceptual quality, compa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32467;&#21512;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;&#35889;&#23618;&#30340;Spectformer&#26550;&#26500;&#65292;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#25552;&#39640;&#20102;top-1&#20934;&#30830;&#29575;2%&#12290;</title><link>http://arxiv.org/abs/2304.06446</link><description>&lt;p&gt;
SpectFormer: &#39057;&#29575;&#21644;&#27880;&#24847;&#21147;&#26159;&#35270;&#35273;Transformer&#25152;&#38656;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
SpectFormer: Frequency and Attention is what you need in a Vision Transformer. (arXiv:2304.06446v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32467;&#21512;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;&#35889;&#23618;&#30340;Spectformer&#26550;&#26500;&#65292;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#25552;&#39640;&#20102;top-1&#20934;&#30830;&#29575;2%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#24050;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#20013;&#12290;&#20854;&#31181;&#31867;&#21253;&#25324;&#22522;&#20110;&#22810;&#22836;&#33258;&#25105;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;&#22914;ViT&#12289;DeIT&#65289;&#21644;&#22522;&#20110;&#35889;&#23618;&#65288;&#22914;Fnet&#12289;GFNet&#12289;AFNO&#65289;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;&#35889;&#23618;&#37117;&#23545;Transformer&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#65292;&#23558;&#20004;&#32773;&#32467;&#21512;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#22240;&#27492;&#25552;&#20986;&#20102;&#26032;&#30340;Spectformer&#26550;&#26500;&#65292;&#23558;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;&#35889;&#23618;&#34701;&#21512;&#36215;&#26469;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;Spectformer&#21487;&#24688;&#24403;&#22320;&#25429;&#25417;&#29305;&#24449;&#34920;&#31034;&#65292;&#19982;&#20854;&#20182;Transformer&#34920;&#24449;&#30456;&#27604;&#65292;&#21487;&#20197;&#25552;&#39640;top-1&#20934;&#30830;&#29575;2%&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision transformers have been applied successfully for image recognition tasks. There have been either multi-headed self-attention based (ViT \cite{dosovitskiy2020image}, DeIT, \cite{touvron2021training}) similar to the original work in textual models or more recently based on spectral layers (Fnet\cite{lee2021fnet}, GFNet\cite{rao2021global}, AFNO\cite{guibas2021efficient}). We hypothesize that both spectral and multi-headed attention plays a major role. We investigate this hypothesis through this work and observe that indeed combining spectral and multi-headed attention layers provides a better transformer architecture. We thus propose the novel Spectformer architecture for transformers that combines spectral and multi-headed attention layers. We believe that the resulting representation allows the transformer to capture the feature representation appropriately and it yields improved performance over other transformer representations. For instance, it improves the top-1 accuracy by 2
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#36923;&#36753;&#38145;&#23450;&#26469;&#30772;&#22351;&#31070;&#32463;&#21152;&#36895;&#22120;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#29305;&#27931;&#20234;&#31192;&#38053;&#26469;&#20135;&#29983;&#31070;&#32463;&#29305;&#27931;&#20234;&#24335;&#30340;&#21518;&#38376;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#35825;&#23548;&#22823;&#37327;&#38169;&#35823;&#20998;&#31867;&#29575;&#30340;&#29305;&#27931;&#20234;&#31192;&#38053;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#30340;&#29305;&#27931;&#20234;&#28608;&#27963;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.06017</link><description>&lt;p&gt;
&#22522;&#20110;&#36923;&#36753;&#38145;&#23450;&#30340;&#31070;&#32463;&#29305;&#27931;&#20234;&#25915;&#20987;&#26426;&#22120;&#23398;&#20064;&#21152;&#36895;&#22120;
&lt;/p&gt;
&lt;p&gt;
Exploiting Logic Locking for a Neural Trojan Attack on Machine Learning Accelerators. (arXiv:2304.06017v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#36923;&#36753;&#38145;&#23450;&#26469;&#30772;&#22351;&#31070;&#32463;&#21152;&#36895;&#22120;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#29305;&#27931;&#20234;&#31192;&#38053;&#26469;&#20135;&#29983;&#31070;&#32463;&#29305;&#27931;&#20234;&#24335;&#30340;&#21518;&#38376;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#35825;&#23548;&#22823;&#37327;&#38169;&#35823;&#20998;&#31867;&#29575;&#30340;&#29305;&#27931;&#20234;&#31192;&#38053;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#30340;&#29305;&#27931;&#20234;&#28608;&#27963;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36923;&#36753;&#38145;&#23450;&#34987;&#25552;&#20986;&#26469;&#20445;&#25252;&#33455;&#29255;&#21046;&#36896;&#36807;&#31243;&#20013;&#30340;&#30693;&#35782;&#20135;&#26435;&#12290;&#36923;&#36753;&#38145;&#23450;&#25216;&#26415;&#36890;&#36807;&#20351;&#35774;&#35745;&#20013;&#30340;&#19968;&#37096;&#20998;&#32452;&#21512;&#27169;&#22359;&#20381;&#36182;&#20110;&#20445;&#23494;&#30340;&#31192;&#38053;&#26469;&#20445;&#25252;&#30828;&#20214;IP&#12290;&#22914;&#26524;&#20351;&#29992;&#20102;&#19981;&#27491;&#30830;&#30340;&#31192;&#38053;&#65292;&#38145;&#23450;&#27169;&#22359;&#20250;&#20135;&#29983;&#19968;&#32452;&#30830;&#23450;&#24615;&#38169;&#35823;&#65292;&#38480;&#21046;&#26410;&#32463;&#25480;&#26435;&#30340;&#20351;&#29992;&#12290;&#31070;&#32463;&#21152;&#36895;&#22120;&#26159;&#36923;&#36753;&#38145;&#23450;&#30340;&#24120;&#35265;&#30446;&#26631;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#26381;&#21153;&#30340;&#26222;&#21450;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#36923;&#36753;&#38145;&#23450;&#26469;&#30772;&#22351;&#23427;&#25152;&#20445;&#25252;&#30340;&#31070;&#32463;&#21152;&#36895;&#22120;&#30340;&#23433;&#20840;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#20351;&#29992;&#19981;&#27491;&#30830;&#31192;&#38053;&#25152;&#24341;&#36215;&#30340;&#30830;&#23450;&#24615;&#38169;&#35823;&#26469;&#20135;&#29983;&#31070;&#32463;&#29305;&#27931;&#20234;&#24335;&#30340;&#21518;&#38376;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;&#19968;&#20010;&#21160;&#26426;&#25915;&#20987;&#22330;&#26223;&#65292;&#20854;&#20013;&#31934;&#24515;&#36873;&#25321;&#30340;&#19981;&#27491;&#30830;&#31192;&#38053;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#29305;&#27931;&#20234;&#31192;&#38053;&#65292;&#22312;&#38145;&#23450;&#30340;&#21152;&#36895;&#22120;&#20013;&#20026;&#25915;&#20987;&#32773;&#25351;&#23450;&#30340;&#36755;&#20837;&#31867;&#21035;&#20135;&#29983;&#20102;&#38169;&#35823;&#20998;&#31867;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#26469;&#20026;&#21463;&#36923;&#36753;&#38145;&#23450;&#20445;&#25252;&#30340;&#31070;&#32463;&#21152;&#36895;&#22120;&#35774;&#35745;&#29305;&#27931;&#20234;&#31192;&#38053;&#12290;&#36890;&#36807;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#35825;&#23548;&#22823;&#37327;&#38169;&#35823;&#20998;&#31867;&#29575;&#30340;&#29305;&#27931;&#20234;&#31192;&#38053;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#30340;&#29305;&#27931;&#20234;&#28608;&#27963;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Logic locking has been proposed to safeguard intellectual property (IP) during chip fabrication. Logic locking techniques protect hardware IP by making a subset of combinational modules in a design dependent on a secret key that is withheld from untrusted parties. If an incorrect secret key is used, a set of deterministic errors is produced in locked modules, restricting unauthorized use. A common target for logic locking is neural accelerators, especially as machine-learning-as-a-service becomes more prevalent. In this work, we explore how logic locking can be used to compromise the security of a neural accelerator it protects. Specifically, we show how the deterministic errors caused by incorrect keys can be harnessed to produce neural-trojan-style backdoors. To do so, we first outline a motivational attack scenario where a carefully chosen incorrect key, which we call a trojan key, produces misclassifications for an attacker-specified input class in a locked accelerator. We then dev
&lt;/p&gt;</description></item><item><title>NeuroBench&#26159;&#30001;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#25104;&#21592;&#20849;&#21516;&#24320;&#21457;&#30340;&#19968;&#22871;&#21327;&#20316;&#12289;&#20844;&#24179;&#21644;&#20195;&#34920;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#20197;&#35299;&#20915;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#20013;&#32570;&#20047;&#28165;&#26224;&#26631;&#20934;&#30340;&#38382;&#39064;&#65292;&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2304.04640</link><description>&lt;p&gt;
NeuroBench&#65306;&#36890;&#36807;&#21512;&#20316;&#12289;&#20844;&#24179;&#21644;&#20195;&#34920;&#24615;&#22522;&#20934;&#27979;&#35797;&#25512;&#36827;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
NeuroBench: Advancing Neuromorphic Computing through Collaborative, Fair and Representative Benchmarking. (arXiv:2304.04640v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04640
&lt;/p&gt;
&lt;p&gt;
NeuroBench&#26159;&#30001;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#25104;&#21592;&#20849;&#21516;&#24320;&#21457;&#30340;&#19968;&#22871;&#21327;&#20316;&#12289;&#20844;&#24179;&#21644;&#20195;&#34920;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#20197;&#35299;&#20915;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#20013;&#32570;&#20047;&#28165;&#26224;&#26631;&#20934;&#30340;&#38382;&#39064;&#65292;&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#39046;&#22495;&#22312;&#36981;&#24490;&#20223;&#29983;&#23398;&#21407;&#29702;&#30340;&#22522;&#30784;&#19978;&#65292;&#20855;&#26377;&#25512;&#36827;&#35745;&#31639;&#25928;&#29575;&#21644;&#33021;&#21147;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#24418;&#24577;&#30740;&#31350;&#20013;&#37319;&#29992;&#30340;&#25216;&#26415;&#22810;&#26679;&#24615;&#23548;&#33268;&#32570;&#20047;&#28165;&#26224;&#30340;&#22522;&#20934;&#27979;&#35797;&#26631;&#20934;&#65292;&#38459;&#30861;&#20102;&#23545;&#31070;&#32463;&#24418;&#24577;&#26041;&#27861;&#19982;&#20256;&#32479;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#30340;&#20248;&#21155;&#21183;&#36827;&#34892;&#26377;&#25928;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21327;&#20316;&#39033;&#30446;&#8212;&#8212;NeuroBench&#65292;&#23558;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#25104;&#21592;&#32858;&#38598;&#36215;&#26469;&#20026;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#23450;&#20041;&#22522;&#20934;&#27979;&#35797;&#12290;NeuroBench&#30340;&#30446;&#26631;&#26159;&#25104;&#20026;&#31038;&#21306;&#24320;&#21457;&#30340;&#21327;&#20316;&#12289;&#20844;&#24179;&#21644;&#20195;&#34920;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#22522;&#20934;&#27979;&#35797;&#31070;&#32463;&#24418;&#24577;&#35299;&#20915;&#26041;&#26696;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#27010;&#36848;&#20102;NeuroBench&#30340;&#20851;&#38190;&#29305;&#24615;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;NeuroBench&#23558;&#26159;&#23450;&#20041;&#33021;&#22815;&#32479;&#19968;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#30446;&#26631;&#30340;&#26631;&#20934;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of neuromorphic computing holds great promise in terms of advancing computing efficiency and capabilities by following brain-inspired principles. However, the rich diversity of techniques employed in neuromorphic research has resulted in a lack of clear standards for benchmarking, hindering effective evaluation of the advantages and strengths of neuromorphic methods compared to traditional deep-learning-based methods. This paper presents a collaborative effort, bringing together members from academia and the industry, to define benchmarks for neuromorphic computing: NeuroBench. The goals of NeuroBench are to be a collaborative, fair, and representative benchmark suite developed by the community, for the community. In this paper, we discuss the challenges associated with benchmarking neuromorphic solutions, and outline the key features of NeuroBench. We believe that NeuroBench will be a significant step towards defining standards that can unify the goals of neuromorphic comput
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#39046;&#22495;&#30693;&#35782;&#36827;&#34892;&#20010;&#24615;&#21270;&#25968;&#23383;&#20581;&#24247;&#34892;&#20026;&#21464;&#38761;&#24178;&#39044;&#30340;&#31995;&#32479;&#65292;&#20854;&#21033;&#29992;&#21453;&#20107;&#23454;&#20363;&#23376;&#36827;&#34892;&#29305;&#24449;&#25511;&#21046;&#20197;&#39044;&#27979;&#24178;&#39044;&#25928;&#26524;&#24182;&#20248;&#21270;&#24178;&#39044;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.03392</link><description>&lt;p&gt;
&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#39046;&#22495;&#30693;&#35782;&#20010;&#24615;&#21270;&#25968;&#23383;&#20581;&#24247;&#34892;&#20026;&#21464;&#38761;&#24178;&#39044;
&lt;/p&gt;
&lt;p&gt;
Personalizing Digital Health Behavior Change Interventions using Machine Learning and Domain Knowledge. (arXiv:2304.03392v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03392
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#39046;&#22495;&#30693;&#35782;&#36827;&#34892;&#20010;&#24615;&#21270;&#25968;&#23383;&#20581;&#24247;&#34892;&#20026;&#21464;&#38761;&#24178;&#39044;&#30340;&#31995;&#32479;&#65292;&#20854;&#21033;&#29992;&#21453;&#20107;&#23454;&#20363;&#23376;&#36827;&#34892;&#29305;&#24449;&#25511;&#21046;&#20197;&#39044;&#27979;&#24178;&#39044;&#25928;&#26524;&#24182;&#20248;&#21270;&#24178;&#39044;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#27491;&#22312;&#24320;&#21457;&#19968;&#31181;&#34394;&#25311;&#25945;&#32451;&#31995;&#32479;&#65292;&#24110;&#21161;&#24739;&#32773;&#22362;&#25345;&#34892;&#20026;&#21464;&#38761;&#24178;&#39044;&#65288;BCI&#65289;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#39044;&#27979;&#24739;&#32773;&#26159;&#21542;&#20250;&#25191;&#34892;&#30446;&#26631;&#34892;&#20026;&#65292;&#24182;&#20351;&#29992;&#21453;&#20107;&#23454;&#20363;&#23376;&#36827;&#34892;&#29305;&#24449;&#25511;&#21046;&#65292;&#20197;&#25351;&#23548;&#20010;&#24615;&#21270;BCI&#12290;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#19981;&#21516;&#21709;&#24212;&#27700;&#24179;&#30340;&#27169;&#25311;&#24739;&#32773;&#25968;&#25454;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We are developing a virtual coaching system that helps patients adhere to behavior change interventions (BCI). Our proposed system predicts whether a patient will perform the targeted behavior and uses counterfactual examples with feature control to guide personalizsation of BCI. We evaluated our prediction model using simulated patient data with varying levels of receptivity to intervention.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#27010;&#29575;&#35770;&#35777;&#26694;&#26550;&#35299;&#37322;&#20026;&#27010;&#29575;&#36923;&#36753;&#31243;&#24207;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PLP&#35821;&#20041;&#65292;&#29992;&#20110;&#22788;&#29702;&#19981;&#21516;&#27010;&#29575;&#20107;&#23454;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#35770;&#25991;&#22312;&#25512;&#29702;&#35770;&#35777;&#39046;&#22495;&#20869;&#26377;&#19968;&#23450;&#30340;&#24212;&#29992;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2304.00879</link><description>&lt;p&gt;
smProbLog:&#21033;&#29992;ProbLog&#23454;&#29616;&#31283;&#23450;&#27169;&#22411;&#35821;&#20041;&#30340;&#27010;&#29575;&#25512;&#29702;&#35770;&#35777;
&lt;/p&gt;
&lt;p&gt;
smProbLog: Stable Model Semantics in ProbLog for Probabilistic Argumentation. (arXiv:2304.00879v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#27010;&#29575;&#35770;&#35777;&#26694;&#26550;&#35299;&#37322;&#20026;&#27010;&#29575;&#36923;&#36753;&#31243;&#24207;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PLP&#35821;&#20041;&#65292;&#29992;&#20110;&#22788;&#29702;&#19981;&#21516;&#27010;&#29575;&#20107;&#23454;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#35770;&#25991;&#22312;&#25512;&#29702;&#35770;&#35777;&#39046;&#22495;&#20869;&#26377;&#19968;&#23450;&#30340;&#24212;&#29992;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#35777;&#38382;&#39064;&#20851;&#27880;&#20110;&#20174;&#23427;&#20204;&#30340;&#20851;&#31995;&#32467;&#26500;&#20013;&#30830;&#23450;&#19968;&#32452;&#21442;&#25968;&#30340;&#21487;&#25509;&#21463;&#24615;&#38382;&#39064;&#12290;&#24403;&#21487;&#29992;&#20449;&#24687;&#19981;&#30830;&#23450;&#26102;&#65292;&#27010;&#29575;&#35770;&#35777;&#26694;&#26550;&#25552;&#20379;&#20102;&#24314;&#27169;&#24037;&#20855;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#30340;&#39318;&#27425;&#36129;&#29486;&#26159;&#23558;&#27010;&#29575;&#35770;&#35777;&#26694;&#26550;&#35299;&#37322;&#20026;&#27010;&#29575;&#36923;&#36753;&#31243;&#24207;&#12290;&#27010;&#29575;&#36923;&#36753;&#31243;&#24207;&#26159;&#36923;&#36753;&#31243;&#24207;&#65292;&#20854;&#20013;&#19968;&#20123;&#20107;&#23454;&#24102;&#26377;&#27010;&#29575;&#27880;&#37322;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#34920;&#31034;&#27010;&#29575;&#35770;&#35777;&#26694;&#26550;&#30340;&#31243;&#24207;&#19981;&#33021;&#28385;&#36275;&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;(PLP)&#35821;&#20041;&#20013;&#30340;&#19968;&#39033;&#26222;&#36941;&#20551;&#35774;&#65292;&#21363;&#27010;&#29575;&#20107;&#23454;&#23436;&#20840;&#25429;&#25417;&#21040;&#30740;&#31350;&#39046;&#22495;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#30340;&#31532;&#20108;&#20010;&#36129;&#29486;&#26159;&#25552;&#20986;PLP&#35821;&#20041;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#30340;&#27010;&#29575;&#20107;&#23454;&#26469;&#30830;&#23450;&#36923;&#36753;&#21407;&#23376;&#30340;&#30495;&#20540;&#20998;&#37197;&#30340;&#31243;&#24207;&#12290;&#26412;&#25991;&#30340;&#31532;&#19977;&#20010;&#36129;&#29486;&#26159;&#23545;&#25991;&#29486;&#36827;&#34892;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
Argumentation problems are concerned with determining the acceptability of a set of arguments from their relational structure. When the available information is uncertain, probabilistic argumentation frameworks provide modelling tools to account for it. The first contribution of this paper is a novel interpretation of probabilistic argumentation frameworks as probabilistic logic programs. Probabilistic logic programs are logic programs in which some of the facts are annotated with probabilities. We show that the programs representing probabilistic argumentation frameworks do not satisfy a common assumption in probabilistic logic programming (PLP) semantics, which is, that probabilistic facts fully capture the uncertainty in the domain under investigation. The second contribution of this paper is then a novel PLP semantics for programs where a choice of probabilistic facts does not uniquely determine the truth assignment of the logical atoms. The third contribution of this paper is the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#22522;&#20110;&#35268;&#21017;&#26041;&#27861;&#30340;&#30005;&#21147;&#32593;&#32476;&#36816;&#34892;&#26234;&#33021;&#20307;&#24182;&#25552;&#20379;&#20102;&#26032;&#31574;&#30053;&#12290;&#20854;&#20013;&#26368;&#20027;&#35201;&#30340;&#25913;&#36827;&#26159;&#37319;&#29992;N-1&#31574;&#30053;&#65292;&#32771;&#34385;&#21040;&#22312;&#26576;&#26465;&#32447;&#36335;&#26029;&#24320;&#26102;&#30340;&#25299;&#25169;&#25805;&#20316;&#20197;&#20445;&#25345;&#32593;&#32476;&#31283;&#23450;&#12290;&#21478;&#22806;&#65292;&#25299;&#25169;&#21453;&#36716;&#20063;&#26377;&#21033;&#20110;&#25552;&#39640;&#24615;&#33021;&#12290;&#22312;&#25361;&#25112;&#27979;&#35797;&#38598;&#19978;&#65292;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;27%&#12290;</title><link>http://arxiv.org/abs/2304.00765</link><description>&lt;p&gt;
&#36890;&#36807;&#25299;&#25169;&#25805;&#20316;&#31649;&#29702;&#30005;&#21147;&#32593;&#32476;&#65306;&#20808;&#36827;&#22522;&#20110;&#35268;&#21017;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26234;&#33021;&#20307;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Managing power grids through topology actions: A comparative study between advanced rule-based and reinforcement learning agents. (arXiv:2304.00765v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#22522;&#20110;&#35268;&#21017;&#26041;&#27861;&#30340;&#30005;&#21147;&#32593;&#32476;&#36816;&#34892;&#26234;&#33021;&#20307;&#24182;&#25552;&#20379;&#20102;&#26032;&#31574;&#30053;&#12290;&#20854;&#20013;&#26368;&#20027;&#35201;&#30340;&#25913;&#36827;&#26159;&#37319;&#29992;N-1&#31574;&#30053;&#65292;&#32771;&#34385;&#21040;&#22312;&#26576;&#26465;&#32447;&#36335;&#26029;&#24320;&#26102;&#30340;&#25299;&#25169;&#25805;&#20316;&#20197;&#20445;&#25345;&#32593;&#32476;&#31283;&#23450;&#12290;&#21478;&#22806;&#65292;&#25299;&#25169;&#21453;&#36716;&#20063;&#26377;&#21033;&#20110;&#25552;&#39640;&#24615;&#33021;&#12290;&#22312;&#25361;&#25112;&#27979;&#35797;&#38598;&#19978;&#65292;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;27%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#24403;&#21069;&#24418;&#21183;&#30340;&#21160;&#33633;&#21644;&#21487;&#20877;&#29983;&#33021;&#28304;&#29983;&#20135;&#30340;&#22686;&#21152;&#65292;&#30005;&#21147;&#32593;&#32476;&#36816;&#34892;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#12290;&#22240;&#27492;&#65292;&#20256;&#32479;&#26041;&#27861;&#30340;&#20027;&#21160;&#32593;&#26684;&#31649;&#29702;&#24050;&#32463;&#36798;&#21040;&#20102;&#26497;&#38480;&#12290;&#36890;&#36807;&#8220;&#23398;&#20064;&#36816;&#34892;&#30005;&#21147;&#32593;&#32476;&#8221;&#25361;&#25112;&#65292;&#25105;&#20204;&#21457;&#29616;&#22686;&#24378;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#25928;&#21487;&#38752;&#30340;&#33258;&#21160;&#21270;&#32593;&#26684;&#36816;&#34892;&#26041;&#27861;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;Binbinchen&#25552;&#20132;&#30340;&#20195;&#29702;&#65292;&#24182;&#25552;&#20379;&#20102;&#25913;&#36827;&#24378;&#21270;&#23398;&#20064;&#21644;&#22522;&#20110;&#35268;&#21017;&#26041;&#27861;&#30340;&#26032;&#31574;&#30053;&#12290;&#20027;&#35201;&#30340;&#25913;&#36827;&#26159;N-1&#31574;&#30053;&#65292;&#21363;&#32771;&#34385;&#21040;&#22312;&#26576;&#26465;&#32447;&#36335;&#26029;&#24320;&#26102;&#65292;&#20173;&#21487;&#20445;&#25345;&#32593;&#32476;&#31283;&#23450;&#30340;&#25299;&#25169;&#25805;&#20316;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#23558;&#25299;&#25169;&#32467;&#26500;&#24674;&#22797;&#21040;&#21407;&#22987;&#29366;&#24577;&#30340;&#25299;&#25169;&#21453;&#36716;&#65292;&#35777;&#26126;&#20854;&#26377;&#30410;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#22312;&#25361;&#25112;&#27979;&#35797;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20351;&#22522;&#20110;&#35268;&#21017;&#30340;&#26234;&#33021;&#20307;&#24615;&#33021;&#25552;&#39640;27&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The operation of electricity grids has become increasingly complex due to the current upheaval and the increase in renewable energy production. As a consequence, active grid management is reaching its limits with conventional approaches. In the context of the Learning to Run a Power Network challenge, it has been shown that Reinforcement Learning (RL) is an efficient and reliable approach with considerable potential for automatic grid operation. In this article, we analyse the submitted agent from Binbinchen and provide novel strategies to improve the agent, both for the RL and the rule-based approach. The main improvement is a N-1 strategy, where we consider topology actions that keep the grid stable, even if one line is disconnected. More, we also propose a topology reversion to the original grid, which proved to be beneficial. The improvements are tested against reference approaches on the challenge test sets and are able to increase the performance of the rule-based agent by 27%. I
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;</title><link>http://arxiv.org/abs/2303.18223</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Large Language Models. (arXiv:2303.18223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#30001;&#35821;&#27861;&#35268;&#21017;&#25511;&#21046;&#30340;&#22797;&#26434;&#31934;&#32454;&#30340;&#20154;&#31867;&#34920;&#36798;&#31995;&#32479;&#65292;&#23545;&#20110;&#24320;&#21457;&#29702;&#35299;&#21644;&#25484;&#25569;&#35821;&#35328;&#30340;&#33021;&#21147;&#30340;AI&#31639;&#27861;&#26469;&#35828;&#26159;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#12290;&#20316;&#20026;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#65292;&#35821;&#35328;&#24314;&#27169;&#22312;&#36807;&#21435;&#20108;&#21313;&#24180;&#37324;&#24191;&#27867;&#30740;&#31350;&#29992;&#20110;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#20174;&#32479;&#35745;&#35821;&#35328;&#27169;&#22411;&#28436;&#21270;&#20026;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#22312;&#35299;&#20915;&#21508;&#31181;NLP&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#27169;&#22411;&#32553;&#25918;&#21487;&#20197;&#23548;&#33268;&#24615;&#33021;&#25913;&#36827;&#65292;&#20182;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#35268;&#27169;&#26469;&#30740;&#31350;&#32553;&#25918;&#25928;&#24212;&#65292;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;&#21442;&#25968;&#35268;&#27169;&#36229;&#36807;&#19968;&#23450;&#27700;&#24179;&#26102;&#65292;&#36825;&#20123;&#25193;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19988;&#36824;&#26174;&#31034;&#20986;&#19968;&#20123;&#23567;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25152;&#27809;&#26377;&#30340;&#29305;&#27530;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale langu
&lt;/p&gt;</description></item><item><title>ChatGPT-4&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#23545;&#35805;&#26426;&#22120;&#20154;&#65292;&#33021;&#36798;&#21040;&#25509;&#36817;&#20110;&#29289;&#29702;&#19987;&#23478;&#27700;&#24179;&#30340;&#21147;&#27010;&#24565;&#27979;&#35797;&#25104;&#32489;&#65292;&#23545;&#26410;&#26469;&#30340;&#29289;&#29702;&#25945;&#32946;&#21644;&#25945;&#23398;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2303.17012</link><description>&lt;p&gt;
ChatGPT-4&#20013;&#26174;&#33879;&#27010;&#24565;&#29289;&#29702;&#25512;&#29702;&#30340;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Advances in apparent conceptual physics reasoning in ChatGPT-4. (arXiv:2303.17012v1 [physics.ed-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17012
&lt;/p&gt;
&lt;p&gt;
ChatGPT-4&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#23545;&#35805;&#26426;&#22120;&#20154;&#65292;&#33021;&#36798;&#21040;&#25509;&#36817;&#20110;&#29289;&#29702;&#19987;&#23478;&#27700;&#24179;&#30340;&#21147;&#27010;&#24565;&#27979;&#35797;&#25104;&#32489;&#65292;&#23545;&#26410;&#26469;&#30340;&#29289;&#29702;&#25945;&#32946;&#21644;&#25945;&#23398;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#24314;&#31435;&#22312;&#19968;&#20010;&#24040;&#22823;&#30340;&#20154;&#31867;&#25991;&#26412;&#20449;&#24687;&#24211;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#27169;&#25311;&#20154;&#31867;&#23545;&#35805;&#12290;&#26368;&#36817;Kortemeyer&#65288;2023&#65289;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#27809;&#26377;&#20219;&#20309;&#20851;&#20110;&#29289;&#29702;&#23450;&#24459;&#30340;&#26126;&#30830;&#32534;&#31243;&#25351;&#23548;&#65292;ChatGPT-3.5&#21487;&#20197;&#36890;&#36807;&#19968;&#20123;&#21517;&#20041;&#27700;&#24179;&#30340;&#20837;&#38376;&#29289;&#29702;&#35838;&#31243;&#65292;&#24182;&#22312;&#21147;&#23398;&#30340;&#21147;&#27010;&#24565;&#27979;&#35797;&#20013;&#24471;&#21040;&#25509;&#36817;&#26368;&#23567;&#29702;&#35299;&#30340;&#25104;&#32489;&#12290;&#26412;&#30740;&#31350;&#22797;&#21046;&#20102;&#36825;&#20123;&#32467;&#26524;&#65292;&#24182;&#35777;&#26126;&#20102;&#26368;&#26032;&#29256;&#26412;ChatGPT-4&#22312;&#35813;&#29615;&#22659;&#19979;&#30340;&#25104;&#32489;&#36828;&#39640;&#20110;&#21069;&#29256;&#26412;&#65292;&#22312;&#19968;&#20123;&#38750;&#24120;&#20540;&#24471;&#27880;&#24847;&#30340;&#20363;&#22806;&#21644;&#38480;&#21046;&#26465;&#20214;&#19979;&#65292;&#20854;&#22238;&#31572;&#38750;&#24120;&#25509;&#36817;&#20110;&#23436;&#32654;&#22320;&#23637;&#31034;&#19987;&#23478;&#27700;&#24179;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#31616;&#35201;&#35780;&#36848;&#20102;&#36825;&#23545;&#20110;&#26410;&#26469;&#29289;&#29702;&#25945;&#32946;&#21644;&#25945;&#23398;&#30340;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is built on a large language model trained on an enormous corpus of human text to emulate human conversation. Despite lacking any explicit programming regarding the laws of physics, recent work by Kortemeyer (2023) has demonstrated that ChatGPT-3.5 could pass an introductory physics course at some nominal level and register something close to a minimal understanding of Newtonian Mechanics on the Force Concept Inventory. This work replicates those results and also demonstrates that the latest version, ChatGPT-4, has reached a much higher mark in the latter context. Indeed, its responses come quite close to perfectly demonstrating expert-level competence, with a few very notable exceptions and limitations. We briefly comment on the implications of this for the future of physics education and pedagogy.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#39044;&#35757;&#32451;&#30340;Transformer-based&#34920;&#26684;&#27169;&#22411;&#65306;TabRet&#65292;&#33021;&#22815;&#25903;&#25345;&#26410;&#30693;&#21015;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#31168;&#12290;&#37325;&#26032;&#26631;&#35760;&#21270;&#21644;&#38543;&#26426;&#27927;&#29260;&#22686;&#24378;&#23545;&#24615;&#33021;&#25552;&#21319;&#26377;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2303.15747</link><description>&lt;p&gt;
TabRet: &#39044;&#35757;&#32451;Transformer-based&#34920;&#26684;&#27169;&#22411;&#65292;&#25903;&#25345;&#26410;&#30693;&#21015;
&lt;/p&gt;
&lt;p&gt;
TabRet: Pre-training Transformer-based Tabular Models for Unseen Columns. (arXiv:2303.15747v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15747
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#39044;&#35757;&#32451;&#30340;Transformer-based&#34920;&#26684;&#27169;&#22411;&#65306;TabRet&#65292;&#33021;&#22815;&#25903;&#25345;&#26410;&#30693;&#21015;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#31168;&#12290;&#37325;&#26032;&#26631;&#35760;&#21270;&#21644;&#38543;&#26426;&#27927;&#29260;&#22686;&#24378;&#23545;&#24615;&#33021;&#25552;&#21319;&#26377;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TabRet&#30340;&#21487;&#39044;&#35757;&#32451;Transformer-based&#34920;&#26684;&#27169;&#22411;&#12290;TabRet&#26088;&#22312;&#20026;&#21253;&#21547;&#26410;&#22312;&#39044;&#35757;&#32451;&#20013;&#35265;&#36807;&#30340;&#21015;&#30340;&#19979;&#28216;&#20219;&#21153;&#25552;&#20379;&#25903;&#25345;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;TabRet&#22312;&#24494;&#35843;&#20043;&#21069;&#26377;&#19968;&#20010;&#39069;&#22806;&#30340;&#23398;&#20064;&#27493;&#39588;&#65292;&#31216;&#20026;&#37325;&#26032;&#26631;&#35760;&#21270;&#65292;&#23427;&#22522;&#20110;&#36974;&#34109;&#33258;&#21160;&#32534;&#30721;&#25439;&#22833;&#26469;&#26657;&#20934;&#29305;&#24449;&#23884;&#20837;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#37327;&#30340;&#20844;&#20849;&#20581;&#24247;&#35843;&#26597;&#25968;&#25454;&#23545;TabRet&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;AUC&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#36827;&#34892;&#37325;&#26032;&#26631;&#35760;&#21270;&#21644;&#38543;&#26426;&#27927;&#29260;&#22686;&#24378;&#23545;&#24615;&#33021;&#25552;&#21319;&#26377;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present \emph{TabRet}, a pre-trainable Transformer-based model for tabular data. TabRet is designed to work on a downstream task that contains columns not seen in pre-training. Unlike other methods, TabRet has an extra learning step before fine-tuning called \emph{retokenizing}, which calibrates feature embeddings based on the masked autoencoding loss. In experiments, we pre-trained TabRet with a large collection of public health surveys and fine-tuned it on classification tasks in healthcare, and TabRet achieved the best AUC performance on four datasets. In addition, an ablation study shows retokenizing and random shuffle augmentation of columns during pre-training contributed to performance gains.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32593;&#26684;-&#25163;&#37096;&#20114;&#21160;&#36827;&#34892;&#21333;&#24352;&#22270;&#20687;&#21452;&#25163;&#37325;&#24314;&#30340;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#32593;&#26684;&#39030;&#28857;&#20301;&#32622;&#21644;MANO&#21442;&#25968;&#20316;&#20026;&#26597;&#35810;&#20196;&#29260;&#65292;&#23454;&#29616;&#20102;&#31934;&#30830;&#30340;&#32593;&#26684;&#37325;&#24314;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#36825;&#19968;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.15718</link><description>&lt;p&gt;
MeMaHand&#65306;&#21033;&#29992;&#32593;&#26684;-&#25163;&#37096;&#20114;&#21160;&#36827;&#34892;&#21333;&#24352;&#22270;&#20687;&#21452;&#25163;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
MeMaHand: Exploiting Mesh-Mano Interaction for Single Image Two-Hand Reconstruction. (arXiv:2303.15718v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32593;&#26684;-&#25163;&#37096;&#20114;&#21160;&#36827;&#34892;&#21333;&#24352;&#22270;&#20687;&#21452;&#25163;&#37325;&#24314;&#30340;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#32593;&#26684;&#39030;&#28857;&#20301;&#32622;&#21644;MANO&#21442;&#25968;&#20316;&#20026;&#26597;&#35810;&#20196;&#29260;&#65292;&#23454;&#29616;&#20102;&#31934;&#30830;&#30340;&#32593;&#26684;&#37325;&#24314;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#36825;&#19968;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#25163;&#37325;&#24314;&#26041;&#27861;&#36890;&#24120;&#23545;&#19968;&#20010;&#36890;&#29992;&#30340;3D&#25163;&#27169;&#22411;&#36827;&#34892;&#21442;&#25968;&#21270;&#25110;&#32773;&#30452;&#25509;&#39044;&#27979;&#25163;&#25484;&#32593;&#26684;&#20301;&#32622;&#65292;&#21442;&#25968;&#34920;&#31034;&#30340;&#25163;&#37096;&#24418;&#29366;&#21644;&#26059;&#36716;&#23039;&#24577;&#26356;&#20026;&#31283;&#23450;&#65292;&#32780;&#38750;&#21442;&#25968;&#21270;&#30340;&#26041;&#27861;&#21487;&#20197;&#26356;&#31934;&#30830;&#22320;&#39044;&#27979;&#32593;&#26684;&#20301;&#32622;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20174;&#21333;&#24352;RGB&#22270;&#20687;&#20013;&#21516;&#26102;&#37325;&#24314;&#20004;&#21482;&#25163;&#30340;&#32593;&#26684;&#24182;&#20272;&#35745;MANO&#21442;&#25968;&#65292;&#20197;&#21033;&#29992;&#20004;&#31181;&#25163;&#34920;&#31034;&#26041;&#27861;&#30340;&#20248;&#28857;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#32593;&#26684;-&#25163;&#37096;&#20114;&#21160;&#22359;&#65288;MMIBs&#65289;&#65292;&#23427;&#23558;&#32593;&#26684;&#39030;&#28857;&#20301;&#32622;&#21644;MANO&#21442;&#25968;&#20316;&#20026;&#20004;&#31181;&#26597;&#35810;&#20196;&#29260;&#12290;MMIB&#30001;&#19968;&#20010;&#22270;&#24418;&#27531;&#24046;&#22359;&#26469;&#32858;&#21512;&#23616;&#37096;&#20449;&#24687;&#21644;&#20004;&#20010;&#21464;&#25442;&#32534;&#30721;&#22120;&#26469;&#24314;&#27169;&#36828;&#31243;&#20381;&#36182;&#20851;&#31995;&#12290;&#21464;&#25442;&#32534;&#30721;&#22120;&#37197;&#22791;&#19981;&#21516;&#30340;&#19981;&#23545;&#31216;&#20851;&#27880;&#25513;&#30721;&#26469;&#20998;&#21035;&#24314;&#27169;&#25163;&#20869;&#21644;&#25163;&#38388;&#20851;&#27880;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#32593;&#26684;&#23545;&#40784;&#32454;&#21270;&#27169;&#22359;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#32593;&#26684;&#37325;&#24314;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21333;&#25163;&#21644;&#21452;&#25163;&#37325;&#24314;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing methods proposed for hand reconstruction tasks usually parameterize a generic 3D hand model or predict hand mesh positions directly. The parametric representations consisting of hand shapes and rotational poses are more stable, while the non-parametric methods can predict more accurate mesh positions. In this paper, we propose to reconstruct meshes and estimate MANO parameters of two hands from a single RGB image simultaneously to utilize the merits of two kinds of hand representations. To fulfill this target, we propose novel Mesh-Mano interaction blocks (MMIBs), which take mesh vertices positions and MANO parameters as two kinds of query tokens. MMIB consists of one graph residual block to aggregate local information and two transformer encoders to model long-range dependencies. The transformer encoders are equipped with different asymmetric attention masks to model the intra-hand and inter-hand attention, respectively. Moreover, we introduce the mesh alignment refinement mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#24335;3D&#32593;&#26684;&#24314;&#27169;&#26041;&#27861;&#65292;&#20381;&#36182;&#32593;&#26684;&#30340;&#22270;&#24418;&#32467;&#26500;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#19981;&#38656;&#35201;&#21518;&#22788;&#29702;&#30340;&#21069;&#25552;&#19979;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#32454;&#33410;&#20016;&#23500;&#30340;3D&#32593;&#26684;&#12290;</title><link>http://arxiv.org/abs/2303.08133</link><description>&lt;p&gt;
MeshDiffusion&#65306;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#24335;3D&#32593;&#26684;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MeshDiffusion: Score-based Generative 3D Mesh Modeling. (arXiv:2303.08133v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#24335;3D&#32593;&#26684;&#24314;&#27169;&#26041;&#27861;&#65292;&#20381;&#36182;&#32593;&#26684;&#30340;&#22270;&#24418;&#32467;&#26500;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#19981;&#38656;&#35201;&#21518;&#22788;&#29702;&#30340;&#21069;&#25552;&#19979;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#32454;&#33410;&#20016;&#23500;&#30340;3D&#32593;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#36924;&#30495;&#30340;3D&#29289;&#20307;&#30340;&#20219;&#21153;&#65292;&#36825;&#23545;&#20110;&#33258;&#21160;&#22330;&#26223;&#29983;&#25104;&#21644;&#29289;&#29702;&#20223;&#30495;&#31561;&#22810;&#31181;&#24212;&#29992;&#38750;&#24120;&#26377;&#29992;&#12290;&#30456;&#27604;&#20110;&#20307;&#32032;&#21644;&#28857;&#20113;&#31561;&#20854;&#20182;3D&#34920;&#31034;&#65292;&#32593;&#26684;&#22312;&#23454;&#36341;&#20013;&#26356;&#21152;&#20248;&#36234;&#65292;&#22240;&#20026;(1)&#23427;&#20204;&#21487;&#20197;&#36731;&#26494;&#20219;&#24847;&#22320;&#25805;&#32437;&#24418;&#29366;&#20197;&#20379;&#37325;&#26032;&#29031;&#26126;&#21644;&#20223;&#30495;&#65292;(2)&#21487;&#20197;&#20805;&#20998;&#21457;&#25381;&#29616;&#20195;&#22270;&#24418;&#27969;&#27700;&#32447;&#30340;&#33021;&#21147;&#65292;&#32780;&#36825;&#20123;&#27969;&#27700;&#32447;&#22823;&#22810;&#25968;&#38024;&#23545;&#32593;&#26684;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;&#20197;&#24448;&#21487;&#25193;&#23637;&#30340;3D&#32593;&#26684;&#29983;&#25104;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#27425;&#20248;&#30340;&#21518;&#22788;&#29702;&#65292;&#24182;&#19988;&#23427;&#20204;&#24448;&#24448;&#20250;&#20135;&#29983;&#36807;&#20110;&#24179;&#28369;&#25110;&#22024;&#26434;&#30340;&#34920;&#38754;&#65292;&#32570;&#20047;&#31934;&#32454;&#30340;&#20960;&#20309;&#32454;&#33410;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#21033;&#29992;&#32593;&#26684;&#30340;&#22270;&#24418;&#32467;&#26500;&#65292;&#20351;&#29992;&#31616;&#21333;&#20294;&#38750;&#24120;&#26377;&#25928;&#30340;&#29983;&#25104;&#24335;&#24314;&#27169;&#26041;&#27861;&#29983;&#25104;3D&#32593;&#26684;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#21487;&#21464;&#24418;&#22235;&#38754;&#20307;&#32593;&#26684;&#26469;&#34920;&#31034;&#32593;&#26684;&#65292;&#28982;&#21518;&#22312;&#36825;&#20010;&#30452;&#25509;&#21442;&#25968;&#21270;&#30340;&#32593;&#26684;&#19978;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the task of generating realistic 3D shapes, which is useful for a variety of applications such as automatic scene generation and physical simulation. Compared to other 3D representations like voxels and point clouds, meshes are more desirable in practice, because (1) they enable easy and arbitrary manipulation of shapes for relighting and simulation, and (2) they can fully leverage the power of modern graphics pipelines which are mostly optimized for meshes. Previous scalable methods for generating meshes typically rely on sub-optimal post-processing, and they tend to produce overly-smooth or noisy surfaces without fine-grained geometric details. To overcome these shortcomings, we take advantage of the graph structure of meshes and use a simple yet very effective generative modeling method to generate 3D meshes. Specifically, we represent meshes with deformable tetrahedral grids, and then train a diffusion model on this direct parametrization. We demonstrate the effectivene
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#36882;&#24402;&#32593;&#32476;&#36827;&#34892;&#20844;&#21496;&#21517;&#31216;&#28040;&#27495;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#26631;&#20934;&#23383;&#31526;&#20018;&#21305;&#37197;&#31639;&#27861;&#20855;&#26377;&#26356;&#20248;&#30340;&#34920;&#29616;&#65292;&#36824;&#37319;&#29992;&#20027;&#21160;&#23398;&#20064;&#26469;&#20248;&#21270;&#26679;&#26412;&#26631;&#35760;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.05391</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#36882;&#24402;&#32593;&#32476;&#30340;&#20844;&#21496;&#21517;&#31216;&#28040;&#27495;
&lt;/p&gt;
&lt;p&gt;
Disambiguation of Company names via Deep Recurrent Networks. (arXiv:2303.05391v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#36882;&#24402;&#32593;&#32476;&#36827;&#34892;&#20844;&#21496;&#21517;&#31216;&#28040;&#27495;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#26631;&#20934;&#23383;&#31526;&#20018;&#21305;&#37197;&#31639;&#27861;&#20855;&#26377;&#26356;&#20248;&#30340;&#34920;&#29616;&#65292;&#36824;&#37319;&#29992;&#20027;&#21160;&#23398;&#20064;&#26469;&#20248;&#21270;&#26679;&#26412;&#26631;&#35760;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#28040;&#27495;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#19968;&#20010;&#20219;&#21153;&#65292;&#20854;&#30446;&#26631;&#26159;&#35782;&#21035;&#23545;&#24212;&#20110;&#21516;&#19968;&#21629;&#21517;&#23454;&#20307;&#30340;&#25991;&#26412;&#35760;&#24405;&#12290;&#26412;&#30740;&#31350;&#30340;&#20219;&#21153;&#26159;&#26681;&#25454;&#20844;&#21496;&#30340;&#20070;&#38754;&#21517;&#31216;&#28040;&#38500;&#27495;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;LSTM&#32593;&#32476;&#26041;&#27861;&#26469;&#25552;&#21462;&#20844;&#21496;&#21517;&#31216;&#23383;&#31526;&#20018;&#30340;&#23884;&#20837;&#65292;&#36827;&#32780;&#20351;&#29992;&#36825;&#31181;&#34920;&#31034;&#26469;&#35782;&#21035;&#30495;&#27491;&#34920;&#31034;&#21516;&#19968;&#20844;&#21496;&#65288;&#21363;&#30456;&#21516;&#23454;&#20307;&#65289;&#30340;&#20844;&#21496;&#21517;&#31216;&#23545;&#12290;&#32771;&#34385;&#21040;&#25163;&#21160;&#26631;&#35760;&#23383;&#31526;&#20018;&#23545;&#26159;&#19968;&#39033;&#36153;&#21147;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#22914;&#20309;&#20248;&#20808;&#36873;&#25321;&#26679;&#26412;&#36827;&#34892;&#26631;&#35760;&#20174;&#32780;&#20351;&#25972;&#20010;&#23398;&#20064;&#27969;&#31243;&#26356;&#21152;&#39640;&#25928;&#12290;&#32463;&#23454;&#35777;&#65292;&#25105;&#20204;&#30340;Siamese&#32593;&#32476;&#20248;&#20110;&#22810;&#31181;&#22522;&#20110;&#26631;&#20934;&#23383;&#31526;&#20018;&#21305;&#37197;&#31639;&#27861;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Name Entity Disambiguation is the Natural Language Processing task of identifying textual records corresponding to the same Named Entity, i.e. real-world entities represented as a list of attributes (names, places, organisations, etc.). In this work, we face the task of disambiguating companies on the basis of their written names. We propose a Siamese LSTM Network approach to extract -- via supervised learning -- an embedding of company name strings in a (relatively) low dimensional vector space and use this representation to identify pairs of company names that actually represent the same company (i.e. the same Entity).  Given that the manual labelling of string pairs is a rather onerous task, we analyse how an Active Learning approach to prioritise the samples to be labelled leads to a more efficient overall learning pipeline.  With empirical investigations, we show that our proposed Siamese Network outperforms several benchmark approaches based on standard string matching algorithms
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26088;&#22312;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35282;&#24230;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#35299;&#37322;&#21487;&#29702;&#35299;&#24615;&#65292;&#20197;&#21450;&#35780;&#20272;&#21487;&#35299;&#37322;&#24615;&#30340;&#36866;&#24403;&#26041;&#27861;&#33853;&#21518;&#20110;&#25216;&#26415;&#30340;&#21457;&#23637;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.00934</link><description>&lt;p&gt;
&#26377;&#24110;&#21161;&#12289;&#24341;&#23548;&#36824;&#26159;&#35753;&#20154;&#22256;&#24785;: &#20154;&#20204;&#22914;&#20309;&#30475;&#24453;&#20154;&#24037;&#26234;&#33021;&#35299;&#37322;&#30340;&#22522;&#26412;&#26500;&#24314;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Helpful, Misleading or Confusing: How Humans Perceive Fundamental Building Blocks of Artificial Intelligence Explanations. (arXiv:2303.00934v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00934
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26088;&#22312;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35282;&#24230;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#35299;&#37322;&#21487;&#29702;&#35299;&#24615;&#65292;&#20197;&#21450;&#35780;&#20272;&#21487;&#35299;&#37322;&#24615;&#30340;&#36866;&#24403;&#26041;&#27861;&#33853;&#21518;&#20110;&#25216;&#26415;&#30340;&#21457;&#23637;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#27491;&#22312;&#20197;&#24778;&#20154;&#30340;&#36895;&#24230;&#21457;&#23637;&#65292;&#20294;&#36866;&#24403;&#30340;&#35780;&#20272;&#26041;&#27861;&#33853;&#21518;&#12290;&#38543;&#30528;&#35299;&#37322;&#22120;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#20197;&#21450;&#32570;&#20047;&#20851;&#20110;&#22914;&#20309;&#35780;&#20272;&#23427;&#20204;&#25928;&#29992;&#30340;&#20849;&#35782;&#65292;&#35780;&#21028;&#19981;&#21516;&#35299;&#37322;&#30340;&#22909;&#22788;&#21644;&#26377;&#25928;&#24615;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#36864;&#19968;&#27493;&#30740;&#31350;&#31616;&#21333;&#20915;&#31574;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#32780;&#19981;&#26159;&#22797;&#26434;&#30340;&#39044;&#27979;&#31639;&#27861;&#12290;&#22312;&#36825;&#31181;&#29615;&#22659;&#19979;&#65292;&#25105;&#20204;&#26088;&#22312;&#35780;&#20272;&#20154;&#20204;&#22914;&#20309;&#29702;&#35299;&#23427;&#20204;&#30340;&#19981;&#21516;&#34920;&#31034;&#24418;&#24335;&#65292;&#20363;&#22914;&#25968;&#23398;&#20844;&#24335;&#12289;&#22270;&#24418;&#34920;&#31034;&#21644;&#25991;&#26412;&#25688;&#35201;&#65288;&#19981;&#21516;&#22797;&#26434;&#24230;&#21644;&#33539;&#22260;&#65289;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#25429;&#25417;&#21040;&#21508;&#31181;&#21033;&#30410;&#30456;&#20851;&#32773;&#65288;&#24037;&#31243;&#24072;&#12289;&#30740;&#31350;&#32773;&#12289;&#28040;&#36153;&#32773;&#12289;&#30417;&#31649;&#26426;&#26500;&#31561;&#65289;&#22914;&#20309;&#35780;&#21028;&#26356;&#22797;&#26434;&#30340;&#20154;&#24037;&#26234;&#33021;&#35299;&#37322;&#25152;&#26500;&#24314;&#30340;&#22522;&#26412;&#27010;&#24565;&#30340;&#21487;&#29702;&#35299;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#25105;&#20204;&#20174;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35282;&#24230;&#24314;&#31435;&#36866;&#24403;&#30340;&#21487;&#35299;&#37322;&#24615;&#35780;&#20272;&#26041;&#27861;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable artificial intelligence techniques are developed at breakneck speed, but suitable evaluation approaches lag behind. With explainers becoming increasingly complex and a lack of consensus on how to assess their utility, it is challenging to judge the benefit and effectiveness of different explanations. To address this gap, we take a step back from sophisticated predictive algorithms and instead look into explainability of simple decision-making models. In this setting, we aim to assess how people perceive comprehensibility of their different representations such as mathematical formulation, graphical representation and textual summarisation (of varying complexity and scope). This allows us to capture how diverse stakeholders -engineers, researchers, consumers, regulators and the like -- judge intelligibility of fundamental concepts that more elaborate artificial intelligence explanations are built from. This position paper charts our approach to establishing appropriate eva
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#24247;&#22797;&#36741;&#21161;&#25511;&#21046;&#22120;AR3n&#65292;&#36890;&#36807;&#20351;&#29992;&#34394;&#25311;&#24739;&#32773;&#27169;&#22411;&#23454;&#29616;&#25511;&#21046;&#22120;&#30340;&#27867;&#21270;&#65292;&#23454;&#26102;&#35843;&#33410;&#26426;&#22120;&#20154;&#36741;&#21161;&#21147;&#24230;&#24182;&#26368;&#23567;&#21270;&#26426;&#22120;&#20154;&#36741;&#21161;&#30340;&#37327;&#65292;&#35813;&#25511;&#21046;&#22120;&#22312;&#23454;&#39564;&#39564;&#35777;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.00085</link><description>&lt;p&gt;
AR3n: &#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#24247;&#22797;&#36741;&#21161;&#25511;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
AR3n: A Reinforcement Learning-based Assist-As-Needed Controller for Robotic Rehabilitation. (arXiv:2303.00085v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#24247;&#22797;&#36741;&#21161;&#25511;&#21046;&#22120;AR3n&#65292;&#36890;&#36807;&#20351;&#29992;&#34394;&#25311;&#24739;&#32773;&#27169;&#22411;&#23454;&#29616;&#25511;&#21046;&#22120;&#30340;&#27867;&#21270;&#65292;&#23454;&#26102;&#35843;&#33410;&#26426;&#22120;&#20154;&#36741;&#21161;&#21147;&#24230;&#24182;&#26368;&#23567;&#21270;&#26426;&#22120;&#20154;&#36741;&#21161;&#30340;&#37327;&#65292;&#35813;&#25511;&#21046;&#22120;&#22312;&#23454;&#39564;&#39564;&#35777;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;AR3n&#65288;&#21457;&#38899;&#20026;Aaron&#65289;&#65292;&#19968;&#31181;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#36741;&#21161;&#25511;&#21046;&#22120;&#65292;&#21487;&#22312;&#26426;&#22120;&#20154;&#36741;&#21161;&#30340;&#20070;&#20889;&#24247;&#22797;&#20219;&#21153;&#20013;&#25552;&#20379;&#36866;&#24212;&#24615;&#36741;&#21161;&#12290;&#19982;&#20197;&#24448;&#30340;&#36741;&#21161;&#25511;&#21046;&#22120;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#24739;&#32773;&#29305;&#23450;&#30340;&#25511;&#21046;&#22120;&#21442;&#25968;&#25110;&#29289;&#29702;&#27169;&#22411;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#34394;&#25311;&#24739;&#32773;&#27169;&#22411;&#26469;&#20351;AR3n&#25512;&#24191;&#21040;&#22810;&#20010;&#21463;&#35797;&#32773;&#12290;&#35813;&#31995;&#32479;&#23454;&#26102;&#35843;&#33410;&#26426;&#22120;&#20154;&#36741;&#21161;&#21147;&#24230;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#26426;&#22120;&#20154;&#36741;&#21161;&#30340;&#37327;&#65292;&#22522;&#20110;&#34987;&#35797;&#30340;&#36319;&#36394;&#35823;&#24046;&#12290;&#36890;&#36807;&#19968;&#32452;&#20223;&#30495;&#23454;&#39564;&#21644;&#20154;&#20307;&#21463;&#35797;&#23454;&#39564;&#23545;&#25511;&#21046;&#22120;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#12290;&#26368;&#21518;&#65292;&#36827;&#34892;&#20102;&#19982;&#20256;&#32479;&#22522;&#20110;&#35268;&#21017;&#30340;&#25511;&#21046;&#22120;&#30340;&#27604;&#36739;&#30740;&#31350;&#65292;&#20197;&#20998;&#26512;&#20004;&#31181;&#25511;&#21046;&#22120;&#30340;&#36741;&#21161;&#26426;&#21046;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present AR3n (pronounced as Aaron), an assist-as-needed (AAN) controller that utilizes reinforcement learning to supply adaptive assistance during a robot assisted handwriting rehabilitation task. Unlike previous AAN controllers, our method does not rely on patient specific controller parameters or physical models. We propose the use of a virtual patient model to generalize AR3n across multiple subjects. The system modulates robotic assistance in realtime based on a subject's tracking error, while minimizing the amount of robotic assistance. The controller is experimentally validated through a set of simulations and human subject experiments. Finally, a comparative study with a traditional rule-based controller is conducted to analyze differences in assistance mechanisms of the two controllers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#35821;&#20041;&#29109;&#20197;&#20811;&#26381;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#8220;&#35821;&#20041;&#31561;&#20215;&#24615;&#8221;&#65292;&#35813;&#26041;&#27861;&#26159;&#26080;&#30417;&#30563;&#30340;&#65292;&#24182;&#19988;&#23545;&#20110;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#20855;&#26377;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.09664</link><description>&lt;p&gt;
&#35821;&#20041;&#19981;&#30830;&#23450;&#24615;&#65306;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20013;&#30340;&#35821;&#35328;&#19981;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation. (arXiv:2302.09664v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#35821;&#20041;&#29109;&#20197;&#20811;&#26381;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#8220;&#35821;&#20041;&#31561;&#20215;&#24615;&#8221;&#65292;&#35813;&#26041;&#27861;&#26159;&#26080;&#30417;&#30563;&#30340;&#65292;&#24182;&#19988;&#23545;&#20110;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#20855;&#26377;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;&#20687;&#38382;&#31572;&#20219;&#21153;&#36825;&#26679;&#30340;&#20219;&#21153;&#65292;&#20102;&#35299;&#20309;&#26102;&#21487;&#20197;&#20449;&#20219;&#22522;&#30784;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#36755;&#20986;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30001;&#20110;&#8220;&#35821;&#20041;&#31561;&#20215;&#24615;&#8221;&#65292;&#27979;&#37327;&#33258;&#28982;&#35821;&#35328;&#30340;&#19981;&#30830;&#23450;&#24615;&#26159;&#26377;&#25361;&#25112;&#24615;&#30340;&#8212;&#8212;&#19981;&#21516;&#30340;&#21477;&#23376;&#21487;&#20197;&#34920;&#31034;&#30456;&#21516;&#30340;&#24847;&#24605;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35821;&#20041;&#29109;&#8212;&#8212;&#19968;&#31181;&#21253;&#21547;&#20849;&#20139;&#21547;&#20041;&#25152;&#21019;&#24314;&#30340;&#35821;&#35328;&#19981;&#21464;&#37327;&#30340;&#29109;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#26080;&#30417;&#30563;&#30340;&#65292;&#20165;&#20351;&#29992;&#21333;&#20010;&#27169;&#22411;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20462;&#25913;&#29616;&#25104;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#20840;&#38754;&#30340;&#28040;&#34701;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35821;&#20041;&#29109;&#23545;&#20110;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#27604;&#21487;&#27604;&#22522;&#32447;&#26356;&#20855;&#26377;&#39044;&#27979;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a method to measure uncertainty in large language models. For tasks like question answering, it is essential to know when we can trust the natural language outputs of foundation models. We show that measuring uncertainty in natural language is challenging because of "semantic equivalence" -- different sentences can mean the same thing. To overcome these challenges we introduce semantic entropy -- an entropy which incorporates linguistic invariances created by shared meanings. Our method is unsupervised, uses only a single model, and requires no modifications to off-the-shelf language models. In comprehensive ablation studies we show that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#35270;&#35273;&#30417;&#30563;&#35757;&#32451;&#30340;&#35821;&#35328;&#34920;&#31034;&#26159;&#21542;&#27604;&#26222;&#36890;&#35821;&#35328;&#34920;&#31034;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#24120;&#35782;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#65292;&#26222;&#36890;&#30340;&#35821;&#35328;&#34920;&#31034;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.05016</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#35270;&#35273;&#30417;&#30563;&#23545;&#35821;&#35328;&#26377;&#30410;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Multimodal Vision Supervision Beneficial to Language?. (arXiv:2302.05016v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#35270;&#35273;&#30417;&#30563;&#35757;&#32451;&#30340;&#35821;&#35328;&#34920;&#31034;&#26159;&#21542;&#27604;&#26222;&#36890;&#35821;&#35328;&#34920;&#31034;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#24120;&#35782;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#65292;&#26222;&#36890;&#30340;&#35821;&#35328;&#34920;&#31034;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#65288;&#22270;&#20687;&#21644;&#35270;&#39057;&#65289;-&#35821;&#35328;&#65288;VL&#65289;&#39044;&#35757;&#32451;&#26159;&#26368;&#36817;&#27969;&#34892;&#30340;&#27169;&#24335;&#65292;&#23427;&#22312;&#22810;&#27169;&#24577;&#20219;&#21153;&#22914;&#22270;&#20687;&#26816;&#32034;&#12289;&#35270;&#39057;&#26816;&#32034;&#12289;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#31561;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#27169;&#22411;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#38750;&#24120;&#21463;&#30410;&#20110;&#34917;&#20805;&#27169;&#24577;&#30417;&#30563;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#35270;&#35273;&#30417;&#30563;&#35757;&#32451;&#30340;&#35821;&#35328;&#34920;&#31034;&#26159;&#21542;&#27604;&#26222;&#36890;&#35821;&#35328;&#34920;&#31034;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#24120;&#35782;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#23558;&#35797;&#39564;&#19981;&#21516;&#30340;&#22270;&#20687;-&#25991;&#26412;&#27169;&#22411;&#65292;&#22914;ALBEF&#12289;BLIP&#12289;METER&#31561;&#65292;&#20197;&#21450;&#35270;&#39057;-&#25991;&#26412;&#27169;&#22411;&#65292;&#22914;ALPRO&#12289;Frozen-in-Time&#65288;FiT&#65289;&#12289;VIOLET&#31561;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#27169;&#22411;&#30340;&#29420;&#31435;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#35821;&#35328;&#34920;&#31034;&#19982;&#36890;&#36807;&#35270;&#35273;&#30417;&#30563;&#23398;&#20064;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#35821;&#35328;&#34920;&#31034;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#65292;&#26222;&#36890;&#30340;&#35821;&#35328;&#34920;&#31034;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision (image and video) - Language (VL) pre-training is the recent popular paradigm that achieved state-of-the-art results on multi-modal tasks like image-retrieval, video-retrieval, visual question answering etc. These models are trained in an unsupervised way and greatly benefit from the complementary modality supervision. In this paper, we explore if the language representations trained using vision supervision perform better than vanilla language representations on Natural Language Understanding and commonsense reasoning benchmarks. We experiment with a diverse set of image-text models such as ALBEF, BLIP, METER and video-text models like ALPRO, Frozen-in-Time (FiT), VIOLET. We compare the performance of language representations of stand-alone text encoders of these models to the language representations of text encoders learnt through vision supervision. Our experiments suggest that vanilla language representations show superior performance on most of the tasks. These results she
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22686;&#24378;&#29983;&#25104;&#25968;&#25454;&#38598;&#20013;&#22270;&#20687;&#30340;&#22810;&#26679;&#24615;&#26469;&#25552;&#39640;&#27169;&#22411;&#26080;&#20851;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;1.4mAP&#65288;CUB&#25968;&#25454;&#38598;&#65289;&#21644;8.4mAP&#65288;ImageNet&#25968;&#25454;&#38598;&#65289;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#20219;&#20309;&#19979;&#28216;&#30340;&#20998;&#31867;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2302.03298</link><description>&lt;p&gt;
&#38656;&#35201;&#22810;&#26679;&#24615;&#65306;&#36890;&#36807;&#31283;&#23450;&#30340;&#25193;&#25955;&#25913;&#21892;&#27169;&#22411;&#26080;&#20851;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Diversity is Definitely Needed: Improving Model-Agnostic Zero-shot Classification via Stable Diffusion. (arXiv:2302.03298v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22686;&#24378;&#29983;&#25104;&#25968;&#25454;&#38598;&#20013;&#22270;&#20687;&#30340;&#22810;&#26679;&#24615;&#26469;&#25552;&#39640;&#27169;&#22411;&#26080;&#20851;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;1.4mAP&#65288;CUB&#25968;&#25454;&#38598;&#65289;&#21644;8.4mAP&#65288;ImageNet&#25968;&#25454;&#38598;&#65289;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#20219;&#20309;&#19979;&#28216;&#30340;&#20998;&#31867;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#26080;&#20851;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#65288;MA-ZSC&#65289;&#38382;&#39064;&#65292;&#36825;&#24847;&#21619;&#30528;&#35757;&#32451;&#20998;&#31867;&#26550;&#26500;&#26469;&#23545;&#30495;&#23454;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#65292;&#32780;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19981;&#20351;&#29992;&#20219;&#20309;&#30495;&#23454;&#22270;&#20687;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#35757;&#32451;&#22270;&#20687;&#21487;&#20197;&#25552;&#20379;&#28508;&#22312;&#30340;&#35299;&#20915;MA-ZSC&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#30446;&#21069;&#20173;&#28982;&#19981;&#22914;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#25152;&#21462;&#24471;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25913;&#21892;&#29983;&#25104;&#25968;&#25454;&#38598;&#20013;&#22270;&#20687;&#30340;&#22810;&#26679;&#24615;&#26469;&#25552;&#39640;MA-ZSC&#24615;&#33021;&#30340;&#26032;&#24605;&#36335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#20462;&#25913;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#22686;&#24378;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#32477;&#25307;&#8221;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#20219;&#20309;&#19979;&#28216;&#30340;&#20998;&#31867;&#26550;&#26500;&#65292;&#23613;&#31649;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;CUB&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;1.4mAP&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#20102;8.4mAP&#65292;&#21516;&#26102;&#38656;&#35201;&#26356;&#23569;&#30340;&#29983;&#25104;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we investigate the problem of Model-Agnostic Zero-Shot Classification (MA-ZSC), which refers to training non-specific classification architectures (downstream models) to classify real images without using any real images during training. Recent research has demonstrated that generating synthetic training images using diffusion models provides a potential solution to address MA-ZSC. However, the performance of this approach currently falls short of that achieved by large-scale vision-language models. One possible explanation is a potential significant domain gap between synthetic and real images. Our work offers a fresh perspective on the problem by providing initial insights that MA-ZSC performance can be improved by improving the diversity of images in the generated dataset. We propose a set of modifications to the text-to-image generation process using a pre-trained diffusion model to enhance diversity, which we refer to as our $\textbf{bag of tricks}$. Our approach sho
&lt;/p&gt;</description></item><item><title>KNOD&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;&#30340;DL-based APR&#26041;&#27861;&#12290;&#23427;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#22312;&#34917;&#19969;&#29983;&#25104;&#20013;&#25351;&#23548;&#20462;&#22797;&#36807;&#31243;&#65292;&#20855;&#26377;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#38454;&#27573;&#26641;&#35299;&#30721;&#22120;&#21644;&#39046;&#22495;&#35268;&#21017;&#25552;&#21462;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.01857</link><description>&lt;p&gt;
KNOD&#65306;&#39046;&#22495;&#30693;&#35782;&#25552;&#21462;&#26641;&#35299;&#30721;&#22120;&#29992;&#20110;&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
KNOD: Domain Knowledge Distilled Tree Decoder for Automated Program Repair. (arXiv:2302.01857v3 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01857
&lt;/p&gt;
&lt;p&gt;
KNOD&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;&#30340;DL-based APR&#26041;&#27861;&#12290;&#23427;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#22312;&#34917;&#19969;&#29983;&#25104;&#20013;&#25351;&#23548;&#20462;&#22797;&#36807;&#31243;&#65292;&#20855;&#26377;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#38454;&#27573;&#26641;&#35299;&#30721;&#22120;&#21644;&#39046;&#22495;&#35268;&#21017;&#25552;&#21462;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;&#65288;APR&#65289;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#19968;&#20010;&#26377;&#38382;&#39064;&#31243;&#24207;&#30340;&#34917;&#19969;&#26469;&#25552;&#39640;&#36719;&#20214;&#21487;&#38752;&#24615;&#12290;&#36817;&#26399;&#30340;APR&#25216;&#26415;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26500;&#24314;&#27169;&#22411;&#65292;&#20174;&#29616;&#26377;&#30340;&#20195;&#30721;&#24211;&#20013;&#23398;&#20064;&#29983;&#25104;&#34917;&#19969;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;DL&#30340;APR&#25216;&#26415;&#22312;&#34917;&#19969;&#31354;&#38388;&#20013;&#23384;&#22312;&#22823;&#37327;&#35821;&#27861;&#25110;&#35821;&#20041;&#19981;&#27491;&#30830;&#30340;&#34917;&#19969;&#65292;&#36825;&#20123;&#34917;&#19969;&#36829;&#21453;&#20102;&#28304;&#20195;&#30721;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#39046;&#22495;&#30693;&#35782;&#65292;&#22240;&#27492;&#19981;&#33021;&#25104;&#20026;&#20462;&#22797;&#38169;&#35823;&#30340;&#27491;&#30830;&#34917;&#19969;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;DL-based APR&#26041;&#27861;KNOD&#65292;&#23427;&#37319;&#29992;&#30452;&#25509;&#21644;&#20840;&#38754;&#30340;&#26041;&#27861;&#23558;&#39046;&#22495;&#30693;&#35782;&#32435;&#20837;&#21040;&#34917;&#19969;&#29983;&#25104;&#20013;&#12290;KNOD&#26377;&#20004;&#20010;&#20027;&#35201;&#21019;&#26032;&#28857;&#65292;&#21253;&#25324;&#65288;1&#65289;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#38454;&#27573;&#26641;&#35299;&#30721;&#22120;&#65292;&#26681;&#25454;&#20869;&#22312;&#30340;&#26641;&#32467;&#26500;&#30452;&#25509;&#29983;&#25104;&#20462;&#34917;&#31243;&#24207;&#30340;&#25277;&#35937;&#35821;&#27861;&#26641;&#65292;&#20197;&#21450;&#65288;2&#65289;&#19968;&#31181;&#26032;&#39062;&#30340;&#39046;&#22495;&#35268;&#21017;&#25552;&#21462;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#27861;&#21644;&#35821;&#20041;&#35268;&#21017;&#20197;&#21450;&#25945;&#24072;-&#23398;&#29983;&#20998;&#24067;&#26174;&#24335;&#27880;&#20837;&#39046;&#22495;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated Program Repair (APR) improves software reliability by generating patches for a buggy program automatically. Recent APR techniques leverage deep learning (DL) to build models to learn to generate patches from existing patches and code corpora. While promising, DL-based APR techniques suffer from the abundant syntactically or semantically incorrect patches in the patch space. These patches often disobey the syntactic and semantic domain knowledge of source code and thus cannot be the correct patches to fix a bug.  We propose a DL-based APR approach KNOD, which incorporates domain knowledge to guide patch generation in a direct and comprehensive way. KNOD has two major novelties, including (1) a novel three-stage tree decoder, which directly generates Abstract Syntax Trees of patched code according to the inherent tree structure, and (2) a novel domain-rule distillation, which leverages syntactic and semantic rules and teacher-student distributions to explicitly inject the domai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25919;&#31574;&#25193;&#23637;&#26041;&#26696;&#65292;&#29992;&#20110;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#20197;&#20445;&#30041;&#31163;&#32447;&#23398;&#20064;&#30340;&#31574;&#30053;&#24182;&#22312;&#22312;&#32447;&#23398;&#20064;&#20013;&#36827;&#34892;&#33258;&#36866;&#24212;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2302.00935</link><description>&lt;p&gt;
&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#25919;&#31574;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Policy Expansion for Bridging Offline-to-Online Reinforcement Learning. (arXiv:2302.00935v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25919;&#31574;&#25193;&#23637;&#26041;&#26696;&#65292;&#29992;&#20110;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#20197;&#20445;&#30041;&#31163;&#32447;&#23398;&#20064;&#30340;&#31574;&#30053;&#24182;&#22312;&#22312;&#32447;&#23398;&#20064;&#20013;&#36827;&#34892;&#33258;&#36866;&#24212;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#24494;&#35843;&#65292;&#26159;&#19968;&#31181;&#23398;&#20064;&#25511;&#21046;&#31574;&#30053;&#30340;&#26377;&#24076;&#26395;&#30340;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#24615;&#33021;&#26041;&#38754;&#20805;&#20998;&#21033;&#29992;&#20004;&#32773;&#30340;&#20248;&#28857;&#12290;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#31163;&#32447;&#35757;&#32451;&#30340;&#31574;&#30053;&#21021;&#22987;&#21270;&#22312;&#32447;&#23398;&#20064;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27492;&#20219;&#21153;&#30340;&#25919;&#31574;&#25193;&#23637;&#26041;&#26696;&#12290;&#22312;&#23398;&#20064;&#31163;&#32447;&#31574;&#30053;&#21518;&#65292;&#25105;&#20204;&#23558;&#20854;&#29992;&#20316;&#31574;&#30053;&#38598;&#20013;&#30340;&#19968;&#20010;&#20505;&#36873;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#21478;&#19968;&#20010;&#31574;&#30053;&#26469;&#25193;&#23637;&#31574;&#30053;&#38598;&#65292;&#35813;&#31574;&#30053;&#23558;&#36127;&#36131;&#36827;&#19968;&#27493;&#30340;&#23398;&#20064;&#12290;&#20004;&#20010;&#31574;&#30053;&#23558;&#20197;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#32452;&#21512;&#36215;&#26469;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#20808;&#21069;&#31163;&#32447;&#23398;&#20064;&#30340;&#31574;&#30053;&#23436;&#20840;&#22312;&#22312;&#32447;&#23398;&#20064;&#36807;&#31243;&#20013;&#24471;&#20197;&#20445;&#30041;&#65292;&#22240;&#27492;&#20943;&#36731;&#20102;&#28508;&#22312;&#38382;&#39064;&#65292;&#20363;&#22914;&#22312;&#22312;&#32447;&#23398;&#20064;&#30340;&#21021;&#22987;&#38454;&#27573;&#30772;&#22351;&#31163;&#32447;&#31574;&#30053;&#30340;&#26377;&#29992;&#34892;&#20026;&#65292;&#21516;&#26102;&#20801;&#35768;&#31163;&#32447;&#31574;&#30053;&#22312;&#33258;&#36866;&#24212;&#26041;&#24335;&#19979;&#33258;&#28982;&#21442;&#19982;
&lt;/p&gt;
&lt;p&gt;
Pre-training with offline data and online fine-tuning using reinforcement learning is a promising strategy for learning control policies by leveraging the best of both worlds in terms of sample efficiency and performance. One natural approach is to initialize the policy for online learning with the one trained offline. In this work, we introduce a policy expansion scheme for this task. After learning the offline policy, we use it as one candidate policy in a policy set. We then expand the policy set with another policy which will be responsible for further learning. The two policies will be composed in an adaptive manner for interacting with the environment. With this approach, the policy previously learned offline is fully retained during online learning, thus mitigating the potential issues such as destroying the useful behaviors of the offline policy in the initial stage of online learning while allowing the offline policy participate in the exploration naturally in an adaptive mann
&lt;/p&gt;</description></item><item><title>GLIGEN&#26159;&#19968;&#31181;&#24320;&#25918;&#24335;&#22522;&#20110;&#35821;&#35328;&#20851;&#32852;&#24615;&#21644;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#38376;&#25511;&#26426;&#21046;&#27880;&#20837;&#36830;&#32467;&#20449;&#24687;&#65292;&#33021;&#22815;&#23454;&#29616;&#38646;&#26679;&#26412;&#30340;&#22522;&#20110;&#20851;&#38190;&#23383;&#21644;&#36793;&#30028;&#26694;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;&#30417;&#30563;&#24067;&#23616;&#21040;&#22270;&#20687;&#30340;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2301.07093</link><description>&lt;p&gt;
GLIGEN&#65306;&#24320;&#25918;&#24335;&#22522;&#20110;&#25991;&#26412;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GLIGEN: Open-Set Grounded Text-to-Image Generation. (arXiv:2301.07093v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07093
&lt;/p&gt;
&lt;p&gt;
GLIGEN&#26159;&#19968;&#31181;&#24320;&#25918;&#24335;&#22522;&#20110;&#35821;&#35328;&#20851;&#32852;&#24615;&#21644;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#38376;&#25511;&#26426;&#21046;&#27880;&#20837;&#36830;&#32467;&#20449;&#24687;&#65292;&#33021;&#22815;&#23454;&#29616;&#38646;&#26679;&#26412;&#30340;&#22522;&#20110;&#20851;&#38190;&#23383;&#21644;&#36793;&#30028;&#26694;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;&#30417;&#30563;&#24067;&#23616;&#21040;&#22270;&#20687;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#24778;&#21497;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#29366;&#26159;&#20165;&#20351;&#29992;&#25991;&#26412;&#36755;&#20837;&#65292;&#36825;&#21487;&#33021;&#20250;&#38480;&#21046;&#21487;&#25511;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GLIGEN&#65292;&#22522;&#20110;&#35821;&#35328;&#20851;&#32852;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23427;&#24314;&#31435;&#22312;&#29616;&#26377;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#24182;&#20351;&#20854;&#33021;&#22815;&#20381;&#36182;&#20110;&#35821;&#35328;&#20851;&#32852;&#24615;&#36755;&#20837;&#12290;&#20026;&#20102;&#20445;&#30041;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24191;&#27867;&#27010;&#24565;&#30693;&#35782;&#65292;&#25105;&#20204;&#20923;&#32467;&#25152;&#26377;&#26435;&#37325;&#65292;&#24182;&#36890;&#36807;&#38376;&#25511;&#26426;&#21046;&#23558;&#36830;&#32467;&#20449;&#24687;&#27880;&#20837;&#21040;&#26032;&#30340;&#21487;&#35757;&#32451;&#23618;&#20013;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#24320;&#25918;&#24335;&#22522;&#20110;&#20851;&#38190;&#23383;&#21644;&#36793;&#30028;&#26694;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#65292;&#32780;&#19988;&#36830;&#32467;&#33021;&#21147;&#22312;&#26032;&#30340;&#31354;&#38388;&#37197;&#32622;&#21644;&#27010;&#24565;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#26222;&#36866;&#24615;&#12290;GLIGEN&#22312;COCO&#21644;LVIS&#30340;&#38646;&#26679;&#26412;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#30417;&#30563;&#24067;&#23616;&#21040;&#22270;&#20687;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale text-to-image diffusion models have made amazing advances. However, the status quo is to use text input alone, which can impede controllability. In this work, we propose GLIGEN, Grounded-Language-to-Image Generation, a novel approach that builds upon and extends the functionality of existing pre-trained text-to-image diffusion models by enabling them to also be conditioned on grounding inputs. To preserve the vast concept knowledge of the pre-trained model, we freeze all of its weights and inject the grounding information into new trainable layers via a gated mechanism. Our model achieves open-world grounded text2img generation with caption and bounding box condition inputs, and the grounding ability generalizes well to novel spatial configurations and concepts. GLIGEN's zero-shot performance on COCO and LVIS outperforms that of existing supervised layout-to-image baselines by a large margin.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#8212;&#8212;&#32467;&#26500;&#22240;&#26524;&#28216;&#25103;&#65292;&#23427;&#23558;&#22240;&#26524;&#29702;&#35770;&#21644;&#21338;&#24328;&#29702;&#35770;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#22312;&#32479;&#19968;&#12289;&#21407;&#21017;&#24615;&#30340;&#26694;&#26550;&#19979;&#24314;&#27169;&#28216;&#25103;&#20013;&#30340;&#20381;&#36182;&#20851;&#31995;&#21644;&#35745;&#31639;&#22240;&#26524;&#25928;&#24212;&#65292;&#26159;&#28216;&#25103;&#20013;&#22240;&#26524;&#25512;&#29702;&#39046;&#22495;&#30340;&#19968;&#27425;&#26377;&#30410;&#23581;&#35797;&#12290;</title><link>http://arxiv.org/abs/2301.02324</link><description>&lt;p&gt;
&#28216;&#25103;&#20013;&#30340;&#22240;&#26524;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Reasoning about Causality in Games. (arXiv:2301.02324v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#8212;&#8212;&#32467;&#26500;&#22240;&#26524;&#28216;&#25103;&#65292;&#23427;&#23558;&#22240;&#26524;&#29702;&#35770;&#21644;&#21338;&#24328;&#29702;&#35770;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#22312;&#32479;&#19968;&#12289;&#21407;&#21017;&#24615;&#30340;&#26694;&#26550;&#19979;&#24314;&#27169;&#28216;&#25103;&#20013;&#30340;&#20381;&#36182;&#20851;&#31995;&#21644;&#35745;&#31639;&#22240;&#26524;&#25928;&#24212;&#65292;&#26159;&#28216;&#25103;&#20013;&#22240;&#26524;&#25512;&#29702;&#39046;&#22495;&#30340;&#19968;&#27425;&#26377;&#30410;&#23581;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#29702;&#21644;&#21338;&#24328;&#35770;&#25512;&#29702;&#26159;&#20154;&#24037;&#26234;&#33021;&#31561;&#22810;&#23398;&#31185;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#36825;&#31687;&#35770;&#25991;&#20851;&#27880;&#23427;&#20204;&#30340;&#20132;&#21449;&#28857;&#12290; &#22312;&#27492;&#20043;&#21069;&#65292;&#32570;&#20047;&#19968;&#20010;&#25903;&#25345;&#36825;&#20004;&#31181;&#25512;&#29702;&#30340;&#24418;&#24335;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65306;&#32467;&#26500;&#22240;&#26524;&#28216;&#25103;&#65292;&#21487;&#20197;&#30475;&#20316;&#23558;Pearl&#30340;&#22240;&#26524;&#23618;&#27425;&#32467;&#26500;&#25193;&#23637;&#21040;&#21338;&#24328;&#29702;&#35770;&#39046;&#22495;&#65292;&#25110;&#23558;Koller&#21644;Milch&#30340;&#22810;&#20195;&#29702;&#24433;&#21709;&#22270;&#25193;&#23637;&#21040;&#22240;&#26524;&#39046;&#22495;&#12290; &#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;1&#65289;&#22914;&#20309;&#20197;&#32479;&#19968;&#12289;&#26377;&#21407;&#21017;&#30340;&#26041;&#24335;&#24314;&#27169;&#28216;&#25103;&#20013;&#30340;(&#22240;&#26524;)&#20381;&#36182;&#24615;-&#26080;&#35770;&#26159;&#21464;&#37327;&#20043;&#38388;&#65292;&#36824;&#26159;&#31574;&#30053;&#20043;&#38388;&#65311;2&#65289;&#22914;&#20309;&#22312;&#22240;&#26524;&#28216;&#25103;&#20013;&#35745;&#31639;&#22240;&#26524;&#26597;&#35810;&#65292;&#24182;&#38656;&#35201;&#20160;&#20040;&#20551;&#35774;&#65311;3&#65289;&#22240;&#26524;&#28216;&#25103;&#19982;&#29616;&#26377;&#24418;&#24335;&#20027;&#20041;&#30340;&#27604;&#36739;&#65311;
&lt;/p&gt;
&lt;p&gt;
Causal reasoning and game-theoretic reasoning are fundamental topics in artificial intelligence, among many other disciplines: this paper is concerned with their intersection. Despite their importance, a formal framework that supports both these forms of reasoning has, until now, been lacking. We offer a solution in the form of (structural) causal games, which can be seen as extending Pearl's causal hierarchy to the game-theoretic domain, or as extending Koller and Milch's multi-agent influence diagrams to the causal domain. We then consider three key questions: i) How can the (causal) dependencies in games - either between variables, or between strategies - be modelled in a uniform, principled manner? ii) How may causal queries be computed in causal games, and what assumptions does this require? iii) How do causal games compare to existing formalisms? To address question i), we introduce mechanised games, which encode dependencies between agents' decision rules and the distributions g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#23454;&#29616;&#19981;&#38656;&#35201;&#36830;&#32493;&#36890;&#20449;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;Vision Transformer&#29305;&#23450;&#30340;&#21152;&#23494;&#25216;&#26415;&#26469;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#19981;&#21516;&#20219;&#21153;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31574;&#30053;&#21644;&#24494;&#35843;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2301.02064</link><description>&lt;p&gt;
&#20351;&#29992;Vision Transformer&#30340;&#21333;&#36718;&#33258;&#30417;&#30563;&#20998;&#24067;&#24335;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Single-round Self-supervised Distributed Learning using Vision Transformer. (arXiv:2301.02064v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#23454;&#29616;&#19981;&#38656;&#35201;&#36830;&#32493;&#36890;&#20449;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;Vision Transformer&#29305;&#23450;&#30340;&#21152;&#23494;&#25216;&#26415;&#26469;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#19981;&#21516;&#20219;&#21153;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31574;&#30053;&#21644;&#24494;&#35843;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#23398;&#39046;&#22495;&#21462;&#24471;&#20102;&#19968;&#31995;&#21015;&#30340;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#23545;&#38544;&#31169;&#21644;&#25968;&#25454;&#25152;&#26377;&#26435;&#30340;&#25285;&#24551;&#65292;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#26356;&#21152;&#20005;&#37325;&#12290;&#24050;&#32463;&#30740;&#31350;&#36807;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#32852;&#21512;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#32321;&#29712;&#30340;&#36890;&#20449;&#24320;&#38144;&#21644;&#38544;&#31169;&#20445;&#25252;&#23384;&#22312;&#30340;&#24369;&#28857;&#65292;&#36825;&#20123;&#26041;&#27861;&#21463;&#21040;&#20102;&#38459;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Vision Transformer&#29305;&#23450;&#21152;&#23494;&#25216;&#26415;&#65292;&#37319;&#29992;&#33258;&#30417;&#30563;&#36974;&#34109;&#37319;&#26679;&#33976;&#39311;&#26041;&#27861;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#36830;&#32493;&#36890;&#20449;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;Vision Transformer&#29305;&#23450;&#30340;&#21152;&#23494;&#25216;&#26415;&#26469;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#19982;&#29616;&#26377;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31574;&#30053;&#20197;&#21450;&#20165;&#22522;&#20110;&#24494;&#35843;&#30340;&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#36798;&#21040;&#20102;&#26356;&#20026;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21019;&#24314;&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#19968;&#33324;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent success of deep learning in the field of medicine, the issue of data scarcity is exacerbated by concerns about privacy and data ownership. Distributed learning approaches, including federated learning, have been investigated to address these issues. However, they are hindered by the need for cumbersome communication overheads and weaknesses in privacy protection. To tackle these challenges, we propose a self-supervised masked sampling distillation method for the vision transformer. This method can be implemented without continuous communication and can enhance privacy by utilizing a vision transformer-specific encryption technique. We conducted extensive experiments on two different tasks, which demonstrated the effectiveness of our method. We achieved superior performance compared to the existing distributed learning strategy as well as the fine-tuning only baseline. Furthermore, since the self-supervised model created using our proposed method can achieve a general
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22522;&#20110;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#20013;&#30340;&#25628;&#32034;&#38382;&#39064;&#65292;&#23558;&#25511;&#21046;&#38382;&#39064;&#34920;&#36848;&#20026;&#19987;&#23478;&#28436;&#31034;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#20223;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#35270;&#39057;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#28508;&#34920;&#31034;&#20013;&#36827;&#34892;&#36817;&#37051;&#25628;&#32034;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#22312;Minecraft&#29615;&#22659;&#20013;&#22797;&#21046;&#20986;&#26377;&#24847;&#20041;&#30340;&#28436;&#31034;&#36712;&#36857;&#24182;&#21576;&#29616;&#31867;&#20154;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2212.13326</link><description>&lt;p&gt;
&#36890;&#36807;&#35270;&#39057;&#39044;&#35757;&#32451;&#28508;&#31354;&#38388;&#25628;&#32034;&#30340;&#34892;&#20026;&#20811;&#38534;
&lt;/p&gt;
&lt;p&gt;
Behavioral Cloning via Search in Video PreTraining Latent Space. (arXiv:2212.13326v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22522;&#20110;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#20013;&#30340;&#25628;&#32034;&#38382;&#39064;&#65292;&#23558;&#25511;&#21046;&#38382;&#39064;&#34920;&#36848;&#20026;&#19987;&#23478;&#28436;&#31034;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#20223;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#35270;&#39057;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#28508;&#34920;&#31034;&#20013;&#36827;&#34892;&#36817;&#37051;&#25628;&#32034;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#22312;Minecraft&#29615;&#22659;&#20013;&#22797;&#21046;&#20986;&#26377;&#24847;&#20041;&#30340;&#28436;&#31034;&#36712;&#36857;&#24182;&#21576;&#29616;&#31867;&#20154;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24314;&#31435;&#33021;&#22815;&#35299;&#20915;&#20687;Minecraft&#36825;&#26679;&#29615;&#22659;&#20013;&#20219;&#21153;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22522;&#20110;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#25511;&#21046;&#38382;&#39064;&#34920;&#36848;&#20026;&#22522;&#20110;&#19987;&#23478;&#28436;&#31034;&#25968;&#25454;&#38598;&#30340;&#25628;&#32034;&#38382;&#39064;&#65292;&#20854;&#20013;&#20195;&#29702;&#20174;&#31867;&#20284;&#20110;&#22270;&#20687;-&#21160;&#20316;&#23545;&#30340;&#28436;&#31034;&#36712;&#36857;&#20013;&#22797;&#21046;&#21160;&#20316;&#12290;&#25105;&#20204;&#22312;&#35270;&#39057;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#28508;&#34920;&#31034;&#20013;&#23545;BASALT MineRL&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#36817;&#37051;&#25628;&#32034;&#12290;&#21482;&#35201;&#20195;&#29702;&#21644;&#25968;&#25454;&#38598;&#20013;&#36873;&#23450;&#30340;&#19987;&#23478;&#36712;&#36857;&#30340;&#29366;&#24577;&#34920;&#31034;&#20043;&#38388;&#30340;&#36317;&#31163;&#19981;&#20250;&#20998;&#25955;&#65292;&#20195;&#29702;&#23601;&#20250;&#22797;&#21046;&#19987;&#23478;&#36712;&#36857;&#20013;&#30340;&#21160;&#20316;&#12290;&#28982;&#21518;&#37325;&#26032;&#36827;&#34892;&#36817;&#37051;&#25628;&#32034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#24674;&#22797;&#26377;&#24847;&#20041;&#30340;&#28436;&#31034;&#36712;&#36857;&#65292;&#24182;&#22312;Minecraft&#29615;&#22659;&#20013;&#23637;&#31034;&#20986;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our aim is to build autonomous agents that can solve tasks in environments like Minecraft. To do so, we used an imitation learning-based approach. We formulate our control problem as a search problem over a dataset of experts' demonstrations, where the agent copies actions from a similar demonstration trajectory of image-action pairs. We perform a proximity search over the BASALT MineRL-dataset in the latent representation of a Video PreTraining model. The agent copies the actions from the expert trajectory as long as the distance between the state representations of the agent and the selected expert trajectory from the dataset do not diverge. Then the proximity search is repeated. Our approach can effectively recover meaningful demonstration trajectories and show human-like behavior of an agent in the Minecraft environment.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21435;&#20013;&#24515;&#21270;&#20154;&#33080;&#35782;&#21035;&#37096;&#32626;&#20013;&#26377;&#25928;&#32858;&#21512;&#38754;&#37096;&#23884;&#20837;&#30340;&#26041;&#24335;&#65292;&#26088;&#22312;&#20943;&#23569;&#32593;&#32476;&#21644;&#30828;&#20214;&#38656;&#27714;&#20197;&#40723;&#21169;&#35774;&#22791;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.10108</link><description>&lt;p&gt;
&#38754;&#21521;&#21435;&#20013;&#24515;&#21270;&#20154;&#33080;&#35782;&#21035;&#37096;&#32626;&#30340;&#26377;&#25928;&#32858;&#21512;&#38754;&#37096;&#23884;&#20837;&#65288;&#25193;&#23637;&#29256;&#65289;
&lt;/p&gt;
&lt;p&gt;
Efficient aggregation of face embeddings for decentralized face recognition deployments (extended version). (arXiv:2212.10108v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21435;&#20013;&#24515;&#21270;&#20154;&#33080;&#35782;&#21035;&#37096;&#32626;&#20013;&#26377;&#25928;&#32858;&#21512;&#38754;&#37096;&#23884;&#20837;&#30340;&#26041;&#24335;&#65292;&#26088;&#22312;&#20943;&#23569;&#32593;&#32476;&#21644;&#30828;&#20214;&#38656;&#27714;&#20197;&#40723;&#21169;&#35774;&#22791;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#35782;&#21035;&#26159;&#26368;&#25935;&#24863;&#30340;&#25968;&#25454;&#20043;&#19968;&#12290;&#20197;&#38544;&#31169;&#20026;&#37325;&#28857;&#30340; ubiquitous &#35748;&#35777;&#31995;&#32479;&#20542;&#21521;&#20110;&#20351;&#29992;&#21435;&#20013;&#24515;&#21270;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#20943;&#23569;&#20102;&#28508;&#22312;&#30340;&#25915;&#20987;&#21521;&#37327;&#65292;&#22312;&#25216;&#26415;&#21644;&#32452;&#32455;&#23618;&#38754;&#19978;&#37117;&#26159;&#22914;&#27492;&#12290;&#26368;&#29702;&#24819;&#30340;&#24773;&#20917;&#26159;&#35753;&#29992;&#25143;&#25511;&#21046;&#20854;&#25968;&#25454;&#23384;&#20648;&#30340;&#20301;&#32622;&#65292;&#36825;&#23558;&#23548;&#33268;&#20351;&#29992;&#22810;&#31181;&#35774;&#22791;&#12290;&#27492;&#22806;&#65292;&#19982;&#38598;&#20013;&#24335;&#31995;&#32479;&#30456;&#27604;&#65292;&#35774;&#35745;&#26356;&#27880;&#37325;&#29992;&#25143;&#33258;&#30001;&#30340;&#31995;&#32479;&#36890;&#24120;&#20250;&#22686;&#21152;&#39069;&#22806;&#30340;&#32593;&#32476;&#24320;&#38144;&#12290;&#22240;&#27492;&#65292;&#22312;&#20351;&#29992;&#20154;&#33080;&#35782;&#21035;&#36827;&#34892;&#29983;&#29289;&#35782;&#21035;&#39564;&#35777;&#26102;&#65292;&#19968;&#31181;&#26377;&#25928;&#27604;&#36739;&#38754;&#37096;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#20943;&#23569;&#32593;&#32476;&#21644;&#30828;&#20214;&#38656;&#27714;&#65292;&#20174;&#32780;&#40723;&#21169;&#35774;&#22791;&#30340;&#22810;&#26679;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#24335;&#26469;&#32858;&#21512;&#29992;&#20110;&#20154;&#33080;&#35782;&#21035;&#30340;&#23884;&#20837;&#65292;&#35813;&#26041;&#24335;&#22522;&#20110;&#23545;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#20351;&#29992;&#19981;&#21516;&#32858;&#21512;&#31574;&#30053;&#30340;&#24191;&#27867;&#20998;&#26512;&#12290;&#20316;&#20026;&#20998;&#26512;&#30340;&#19968;&#37096;&#20998;&#65292;&#36824;&#25910;&#38598;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biometrics are one of the most privacy-sensitive data. Ubiquitous authentication systems with a focus on privacy favor decentralized approaches as they reduce potential attack vectors, both on a technical and organizational level. The gold standard is to let the user be in control of where their own data is stored, which consequently leads to a high variety of devices used. Moreover, in comparison with a centralized system, designs with higher end-user freedom often incur additional network overhead. Therefore, when using face recognition for biometric authentication, an efficient way to compare faces is important in practical deployments, because it reduces both network and hardware requirements that are essential to encourage device diversity. This paper proposes an efficient way to aggregate embeddings used for face recognition based on an extensive analysis on different datasets and the use of different aggregation strategies. As part of this analysis, a new dataset has been collec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#22522;&#20110;&#35821;&#20041;&#30340;&#27807;&#36890;&#26041;&#38754;&#30340;&#25945;&#31243;&#20860;&#32508;&#36848;&#65292;&#22238;&#39038;&#20102;&#25991;&#29486;&#65292;&#20171;&#32461;&#20102;SemCom&#29983;&#24577;&#31995;&#32479;&#65292;&#24182;&#23558;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#21551;&#29992;&#25216;&#26415;&#30340;&#20998;&#31867;&#21644;&#26410;&#26469;&#24212;&#29992;&#22330;&#26223;&#30340;&#23637;&#26395;&#12290;</title><link>http://arxiv.org/abs/2212.08487</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#20041;&#30340;&#27807;&#36890;&#65306;&#19968;&#31687;&#25945;&#31243;&#20860;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Semantics-Empowered Communication: A Tutorial-cum-Survey. (arXiv:2212.08487v3 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#22522;&#20110;&#35821;&#20041;&#30340;&#27807;&#36890;&#26041;&#38754;&#30340;&#25945;&#31243;&#20860;&#32508;&#36848;&#65292;&#22238;&#39038;&#20102;&#25991;&#29486;&#65292;&#20171;&#32461;&#20102;SemCom&#29983;&#24577;&#31995;&#32479;&#65292;&#24182;&#23558;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#21551;&#29992;&#25216;&#26415;&#30340;&#20998;&#31867;&#21644;&#26410;&#26469;&#24212;&#29992;&#22330;&#26223;&#30340;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;&#35821;&#20041;&#30340;&#27807;&#36890;&#65288;SemCom&#65289;&#30740;&#31350;&#30340;&#20852;&#36215;&#65292;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#23545;&#20854;&#21508;&#20010;&#26041;&#38754;&#65288;&#22914;&#29702;&#35770;&#12289;&#24212;&#29992;&#12289;&#24230;&#37327;&#21644;&#23454;&#29616;&#65289;&#30340;&#20852;&#36259;&#19981;&#26029;&#22686;&#38271;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#25552;&#20379;&#19968;&#20010;&#32508;&#21512;&#24615;&#30340;&#35843;&#26597;&#65292;&#28085;&#30422;&#20102;&#32972;&#26223;&#21644;&#30740;&#31350;&#20998;&#31867;&#65292;&#20197;&#21450;&#35814;&#32454;&#30340;&#25216;&#26415;&#25945;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#22238;&#39038;&#25991;&#29486;&#24182;&#22238;&#31572;&#20851;&#20110;&#35821;&#20041;&#20256;&#36755;&#30340;&#8220;&#20160;&#20040;&#8221;&#21644;&#8220;&#20026;&#20160;&#20040;&#8221;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SemCom&#29983;&#24577;&#31995;&#32479;&#65292;&#21253;&#25324;&#21382;&#21490;&#12289;&#29702;&#35770;&#12289;&#24230;&#37327;&#12289;&#25968;&#25454;&#38598;&#21644;&#24037;&#20855;&#21253;&#65292;&#24182;&#20171;&#32461;&#20102;&#30740;&#31350;&#26041;&#21521;&#30340;&#20998;&#31867;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#35758;&#36890;&#36807;&#26174;&#24335;&#21644;&#38544;&#24335;&#22522;&#20110;&#25512;&#29702;&#30340;&#26041;&#27861;&#23545;&#20851;&#38190;&#30340;&#21551;&#29992;&#25216;&#26415;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#35814;&#32454;&#38416;&#36848;&#23427;&#20204;&#22914;&#20309;&#28436;&#21464;&#24182;&#20026;&#29616;&#20195;&#20869;&#23481;&#21644;&#36890;&#36947;&#35821;&#20041;&#39537;&#21160;&#30340;&#36890;&#20449;&#20570;&#20986;&#36129;&#29486;&#12290;&#38500;&#20102;&#22238;&#39038;&#21644;&#24635;&#32467;&#26368;&#26032;&#30340;e&#25216;&#26415;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#26410;&#26469;&#30340;&#23637;&#26395;&#21644;&#24212;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Along with the springing up of the semantics-empowered communication (SemCom) research, it is now witnessing an unprecedentedly growing interest towards a wide range of aspects (e.g., theories, applications, metrics and implementations) in both academia and industry. In this work, we primarily aim to provide a comprehensive survey on both the background and research taxonomy, as well as a detailed technical tutorial. Specifically, we start by reviewing the literature and answering the "what" and "why" questions in semantic transmissions. Afterwards, we present the ecosystems of SemCom, including history, theories, metrics, datasets and toolkits, on top of which the taxonomy for research directions is presented. Furthermore, we propose to categorize the critical enabling techniques by explicit and implicit reasoning-based methods, and elaborate on how they evolve and contribute to modern content &amp; channel semantics-empowered communications. Besides reviewing and summarizing the latest e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#27169;&#22411;&#22686;&#24378;&#25216;&#26415;&#65292;&#21363;&#20351;&#29992;&#26089;&#26399;&#27169;&#22411;&#21644;&#21442;&#25968;&#25200;&#21160;&#65292;&#20197;&#26174;&#30528;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#30340;&#26041;&#24335;&#20248;&#21270;&#25968;&#25454;&#38598;&#33976;&#39311;&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;20&#20493;&#30340;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2212.06152</link><description>&lt;p&gt;
&#27169;&#22411;&#22686;&#24378;&#19979;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
Accelerating Dataset Distillation via Model Augmentation. (arXiv:2212.06152v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#27169;&#22411;&#22686;&#24378;&#25216;&#26415;&#65292;&#21363;&#20351;&#29992;&#26089;&#26399;&#27169;&#22411;&#21644;&#21442;&#25968;&#25200;&#21160;&#65292;&#20197;&#26174;&#30528;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#30340;&#26041;&#24335;&#20248;&#21270;&#25968;&#25454;&#38598;&#33976;&#39311;&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;20&#20493;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#33976;&#39311;&#65288;DD&#65289;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#39046;&#22495;&#65292;&#26088;&#22312;&#20174;&#22823;&#22411;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#20135;&#29983;&#26356;&#23567;&#20294;&#39640;&#25928;&#30340;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#26799;&#24230;&#21305;&#37197;&#30340;DD&#26041;&#27861;&#36798;&#21040;&#20102;&#39046;&#20808;&#30340;&#24615;&#33021;&#65307;&#20294;&#26159;&#65292;&#23427;&#20204;&#38750;&#24120;&#35745;&#31639;&#23494;&#38598;&#65292;&#22240;&#20026;&#20182;&#20204;&#38656;&#35201;&#22312;&#25104;&#21315;&#19978;&#19975;&#20010;&#38543;&#26426;&#21021;&#22987;&#21270;&#27169;&#22411;&#20013;&#19981;&#26029;&#20248;&#21270;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;&#20351;&#29992;&#22810;&#31181;&#27169;&#22411;&#23545;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#27169;&#22411;&#22686;&#24378;&#25216;&#26415;&#65292;&#21363;&#20351;&#29992;&#26089;&#26399;&#27169;&#22411;&#21644;&#21442;&#25968;&#25200;&#21160;&#26469;&#23398;&#20064;&#20855;&#26377;&#26174;&#30528;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#30340;&#20449;&#24687;&#21512;&#25104;&#38598;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#39640;&#36798;20&#20493;&#30340;&#21152;&#36895;&#65292;&#24182;&#19988;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dataset Distillation (DD), a newly emerging field, aims at generating much smaller but efficient synthetic training datasets from large ones. Existing DD methods based on gradient matching achieve leading performance; however, they are extremely computationally intensive as they require continuously optimizing a dataset among thousands of randomly initialized models. In this paper, we assume that training the synthetic data with diverse models leads to better generalization performance. Thus we propose two model augmentation techniques, i.e. using early-stage models and parameter perturbation to learn an informative synthetic set with significantly reduced training cost. Extensive experiments demonstrate that our method achieves up to 20x speedup and comparable performance on par with state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#29992;&#21333;&#20010;&#28436;&#31034;&#26469;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#65292;&#21033;&#29992;&#36830;&#32493;&#24615;&#20559;&#35265;&#26469;&#23398;&#20064;&#25511;&#21046;&#22797;&#26434;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#23398;&#20064;&#19968;&#20010;&#30446;&#26631;&#26465;&#20214;&#31574;&#30053;&#65292;&#36890;&#36807;DCIL-II&#35299;&#20915;&#36830;&#32493;&#30446;&#26631;&#38388;&#30340;&#20860;&#23481;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.04786</link><description>&lt;p&gt;
&#21033;&#29992;&#21333;&#20010;&#28436;&#31034;&#20013;&#30340;&#36830;&#32493;&#24615;&#26469;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Leveraging Sequentiality in Reinforcement Learning from a Single Demonstration. (arXiv:2211.04786v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#29992;&#21333;&#20010;&#28436;&#31034;&#26469;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#65292;&#21033;&#29992;&#36830;&#32493;&#24615;&#20559;&#35265;&#26469;&#23398;&#20064;&#25511;&#21046;&#22797;&#26434;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#23398;&#20064;&#19968;&#20010;&#30446;&#26631;&#26465;&#20214;&#31574;&#30053;&#65292;&#36890;&#36807;DCIL-II&#35299;&#20915;&#36830;&#32493;&#30446;&#26631;&#38388;&#30340;&#20860;&#23481;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#23398;&#20064;&#26426;&#22120;&#20154;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#24403;&#20195;&#29702;&#21482;&#22312;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#21518;&#24471;&#21040;&#22870;&#21169;&#26102;&#65292;&#30456;&#24212;&#30340;&#31639;&#27861;&#20250;&#36935;&#21040;&#22256;&#38590;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#28436;&#31034;&#21487;&#20197;&#26174;&#33879;&#21152;&#36895;&#23398;&#20064;&#36807;&#31243;&#65292;&#20294;&#28436;&#31034;&#21487;&#33021;&#24456;&#38590;&#33719;&#24471;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#36830;&#32493;&#24615;&#20559;&#35265;&#26469;&#23398;&#20064;&#25511;&#21046;&#22797;&#26434;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#31574;&#30053;&#65292;&#21482;&#38656;&#35201;&#19968;&#20010;&#28436;&#31034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#20064;&#20102;&#19968;&#20010;&#30446;&#26631;&#26465;&#20214;&#31574;&#30053;&#65292;&#20197;&#25511;&#21046;&#31995;&#32479;&#22312;&#36830;&#32493;&#30340;&#20302;&#32500;&#24230;&#30446;&#26631;&#20043;&#38388;&#31227;&#21160;&#12290;&#36825;&#31181;&#36830;&#32493;&#30446;&#26631;&#36798;&#25104;&#26041;&#27861;&#24341;&#36215;&#20102;&#19968;&#20010;&#36830;&#32493;&#30446;&#26631;&#38388;&#30340;&#20860;&#23481;&#24615;&#38382;&#39064;&#65306;&#25105;&#20204;&#38656;&#35201;&#30830;&#20445;&#36798;&#25104;&#30446;&#26631;&#21518;&#30340;&#29366;&#24577;&#19982;&#21518;&#32493;&#30446;&#26631;&#30340;&#23454;&#29616;&#30456;&#20860;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DCIL-II&#30340;&#26032;&#31639;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DCIL-II&#33021;&#22815;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#26679;&#26412;&#25928;&#29575;&#35299;&#20915;&#19968;&#20123;&#25361;&#25112;&#24615;&#30340;&#27169;&#25311;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning has been successfully applied to learn robotic control. However, the corresponding algorithms struggle when applied to problems where the agent is only rewarded after achieving a complex task. In this context, using demonstrations can significantly speed up the learning process, but demonstrations can be costly to acquire. In this paper, we propose to leverage a sequential bias to learn control policies for complex robotic tasks using a single demonstration. To do so, our method learns a goal-conditioned policy to control a system between successive low-dimensional goals. This sequential goal-reaching approach raises a problem of compatibility between successive goals: we need to ensure that the state resulting from reaching a goal is compatible with the achievement of the following goals. To tackle this problem, we present a new algorithm called DCIL-II. We show that DCIL-II can solve with unprecedented sample efficiency some challenging simulated tasks suc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20302;&#36164;&#28304;&#33945;&#21476;&#35821;&#25991;&#26412;&#21040;&#35821;&#38899;&#31995;&#32479;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#21253;&#21547;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;TTS&#27169;&#22411;&#65292;&#35757;&#32451;&#26102;&#38388;&#26356;&#30701;&#19988;&#38899;&#39057;&#21512;&#25104;&#36136;&#37327;&#19981;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2211.01948</link><description>&lt;p&gt;
&#22522;&#20110;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20302;&#36164;&#28304;&#33945;&#21476;&#35821;&#25991;&#26412;&#21040;&#35821;&#38899;&#31995;&#32479;&#30340;&#39640;&#25928;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Efficiently Trained Low-Resource Mongolian Text-to-Speech System Based On FullConv-TTS. (arXiv:2211.01948v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01948
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20302;&#36164;&#28304;&#33945;&#21476;&#35821;&#25991;&#26412;&#21040;&#35821;&#38899;&#31995;&#32479;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#21253;&#21547;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;TTS&#27169;&#22411;&#65292;&#35757;&#32451;&#26102;&#38388;&#26356;&#30701;&#19988;&#38899;&#39057;&#21512;&#25104;&#36136;&#37327;&#19981;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(RNN)&#24050;&#32463;&#25104;&#20026;&#20102;&#24207;&#21015;&#25968;&#25454;&#24314;&#27169;&#30340;&#26631;&#20934;&#25216;&#26415;&#65292;&#20063;&#34987;&#29992;&#20110;&#26500;&#24314;&#19968;&#20123;&#26032;&#22855;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#21253;&#21547;RNN&#32452;&#20214;&#30340;TTS&#27169;&#22411;&#35201;&#27714;GPU&#24615;&#33021;&#39640;&#19988;&#35757;&#32451;&#26102;&#38388;&#38271;&#12290;&#30456;&#21453;&#65292;&#30740;&#31350;&#21457;&#29616;&#22522;&#20110;CNN&#30340;&#24207;&#21015;&#21512;&#25104;&#25216;&#26415;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;TTS&#27169;&#22411;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#21516;&#26102;&#30001;&#20110;&#20854;&#39640;&#24230;&#24182;&#34892;&#21270;&#65292;&#21487;&#20197;&#20445;&#35777;&#19968;&#23450;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#22411;&#25991;&#26412;&#21040;&#35821;&#38899;&#31995;&#32479;&#65292;&#19981;&#20351;&#29992;&#20219;&#20309;RNN&#32452;&#20214;(&#24490;&#29615;&#21333;&#20803;)&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#22914;&#26102;&#38388;&#25197;&#26354;&#65292;&#39057;&#29575;&#23631;&#34109;&#21644;&#26102;&#38388;&#23631;&#34109;&#31561;&#65292;&#25552;&#39640;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#26368;&#32456;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20165;&#20351;&#29992;CNN&#32452;&#20214;&#30340;TTS&#27169;&#22411;&#21487;&#20197;&#20943;&#23569;&#19982;Tacotron&#31561;&#20256;&#32479;TTS&#27169;&#22411;&#30456;&#27604;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#21516;&#26102;&#30830;&#20445;&#21512;&#25104;&#38899;&#39057;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recurrent Neural Networks (RNNs) have become the standard modeling technique for sequence data, and are used in a number of novel text-to-speech models. However, training a TTS model including RNN components has certain requirements for GPU performance and takes a long time. In contrast, studies have shown that CNN-based sequence synthesis technology can greatly reduce training time in text-to-speech models while ensuring a certain performance due to its high parallelism. We propose a new text-to-speech system based on deep convolutional neural networks that does not employ any RNN components (recurrent units). At the same time, we improve the generality and robustness of our model through a series of data augmentation methods such as Time Warping, Frequency Mask, and Time Mask. The final experimental results show that the TTS model using only the CNN component can reduce the training time compared to the classic TTS models such as Tacotron while ensuring the quality of the synthesized
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#35777;&#26126;&#20256;&#32479;&#30340;&#36890;&#29992;&#23545;&#25239;&#24178;&#25200; (UAPs) &#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#20043;&#38388;&#36716;&#31227;&#24615;&#26159;&#27425;&#20248;&#30340;&#65292;&#20026;&#27492;&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#29992;&#23545;&#25239;&#26041;&#21521; (UADs)&#65292;&#21482;&#22266;&#23450;&#36890;&#29992;&#26041;&#21521;&#65292;&#20197;&#20415;&#20811;&#26381;&#22312;&#36328;DNN&#26550;&#26500;&#19978;&#36716;&#31227;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2210.15997</link><description>&lt;p&gt;
&#36890;&#29992;&#23545;&#25239;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Universal Adversarial Directions. (arXiv:2210.15997v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15997
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35777;&#26126;&#20256;&#32479;&#30340;&#36890;&#29992;&#23545;&#25239;&#24178;&#25200; (UAPs) &#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#20043;&#38388;&#36716;&#31227;&#24615;&#26159;&#27425;&#20248;&#30340;&#65292;&#20026;&#27492;&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#29992;&#23545;&#25239;&#26041;&#21521; (UADs)&#65292;&#21482;&#22266;&#23450;&#36890;&#29992;&#26041;&#21521;&#65292;&#20197;&#20415;&#20811;&#26381;&#22312;&#36328;DNN&#26550;&#26500;&#19978;&#36716;&#31227;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#35266;&#23519;&#21040;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#36890;&#29992;&#23545;&#25239;&#24178;&#25200; (UAPs) &#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#24178;&#25200;&#20351;&#29992;&#21333;&#20010;&#25200;&#21160;&#21521;&#37327;&#24178;&#25200;&#25152;&#26377;&#36755;&#20837;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;UAPs&#22312;&#36328;DNN&#26550;&#26500;&#36716;&#31227;&#26102;&#36890;&#24120;&#24456;&#22256;&#38590;&#24182;&#23548;&#33268;&#25361;&#25112;&#24615;&#20248;&#21270;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;UAP&#30340;&#21487;Transfer&#24615;&#65292;&#36890;&#36807;&#20998;&#26512;&#20998;&#31867;&#22120;&#21644;UAP&#23545;&#25163;&#29609;&#23478;&#20043;&#38388;&#22312;&#36890;&#29992;&#23545;&#25239;&#31034;&#20363;&#21338;&#24328;&#20013;&#30340;&#22343;&#34913;&#24773;&#20917;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#36890;&#29992;&#23545;&#25239;&#31034;&#20363;&#21338;&#24328;&#32570;&#20047;&#19968;&#20010;&#32431;&#32435;&#20160;&#22343;&#34913;&#65292;&#36825;&#34920;&#26126;UAPs&#22312;DNN&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#36716;&#31227;&#24615;&#26159;&#27425;&#20248;&#30340;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#29992;&#23545;&#25239;&#26041;&#21521; (UADs)&#65292;&#21482;&#22266;&#23450;&#23545;&#25239;&#24178;&#25200;&#30340;&#36890;&#29992;&#26041;&#21521;&#65292;&#20801;&#35768;&#36328;&#26679;&#26412;&#33258;&#30001;&#36873;&#25321;&#24178;&#25200;&#30340;&#24133;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;UAD&#23545;&#25239;&#31034;&#20363;&#21338;&#24328;&#21487;&#20197;&#20855;&#26377;&#32435;&#20160;&#22343;&#34913;&#19988;&#35813;&#22343;&#34913;&#29366;&#24577;&#32431;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their great success in image recognition tasks, deep neural networks (DNNs) have been observed to be susceptible to universal adversarial perturbations (UAPs) which perturb all input samples with a single perturbation vector. However, UAPs often struggle in transferring across DNN architectures and lead to challenging optimization problems. In this work, we study the transferability of UAPs by analyzing equilibrium in the universal adversarial example game between the classifier and UAP adversary players. We show that under mild assumptions the universal adversarial example game lacks a pure Nash equilibrium, indicating UAPs' suboptimal transferability across DNN classifiers. To address this issue, we propose Universal Adversarial Directions (UADs) which only fix a universal direction for adversarial perturbations and allow the perturbations' magnitude to be chosen freely across samples. We prove that the UAD adversarial example game can possess a Nash equilibrium with a pure U
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25506;&#32034;-&#24320;&#21457;&#26435;&#34913;&#30340;&#32531;&#20914;&#21306;&#37319;&#26679;&#31574;&#30053;&#65292;&#21487;&#20197;&#26681;&#25454;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#37319;&#26679;&#31574;&#30053;&#65292;&#24182;&#22312;&#32463;&#20856;&#25511;&#21046;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.13545</link><description>&lt;p&gt;
MEET: &#32531;&#20914;&#21306;&#37319;&#26679;&#30340;&#33945;&#29305;&#21345;&#32599;&#25506;&#32034;-&#24320;&#21457;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
MEET: A Monte Carlo Exploration-Exploitation Trade-off for Buffer Sampling. (arXiv:2210.13545v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13545
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25506;&#32034;-&#24320;&#21457;&#26435;&#34913;&#30340;&#32531;&#20914;&#21306;&#37319;&#26679;&#31574;&#30053;&#65292;&#21487;&#20197;&#26681;&#25454;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#37319;&#26679;&#31574;&#30053;&#65292;&#24182;&#22312;&#32463;&#20856;&#25511;&#21046;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#36873;&#25321;&#26159;&#20219;&#20309;&#22522;&#20110;&#25968;&#25454;&#30340;&#20248;&#21270;&#25216;&#26415;&#30340;&#20851;&#38190;&#65292;&#20363;&#22914;&#24378;&#21270;&#23398;&#20064;&#12290;&#38024;&#23545;&#22238;&#25918;&#32531;&#20914;&#21306;&#30340;&#26368;&#26032;&#37319;&#26679;&#31574;&#30053;&#21487;&#20197;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#27809;&#26377;&#32771;&#34385;Q&#20540;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#19981;&#33021;&#26681;&#25454;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#37319;&#26679;&#31574;&#30053;&#65292;&#21253;&#25324;&#36716;&#25442;&#30340;&#25506;&#32034;&#21644;&#24320;&#21457;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#36825;&#31181;&#31574;&#30053;&#21033;&#29992;&#20102;&#25506;&#32034;-&#24320;&#21457;&#26435;&#34913;&#12290;&#36825;&#26159;&#36890;&#36807;Q&#20540;&#20989;&#25968;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23454;&#29616;&#30340;&#65292;&#23427;&#25351;&#23548;&#37319;&#26679;&#25506;&#32034;&#26356;&#37325;&#35201;&#30340;&#36716;&#25442;&#65292;&#20174;&#32780;&#23398;&#20064;&#26356;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;&#23545;&#20110;&#32463;&#20856;&#25511;&#21046;&#29615;&#22659;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#24179;&#31283;&#30340;&#32467;&#26524;&#12290;&#23427;&#20204;&#34920;&#26126;&#65292;&#22312;&#23494;&#38598;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#19982;&#25910;&#25947;&#21644;&#23792;&#20540;&#24615;&#33021;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#34920;&#29616;&#27604;&#26368;&#26032;&#30340;&#37319;&#26679;&#31574;&#30053;&#25552;&#39640;&#20102;26&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data selection is essential for any data-based optimization technique, such as Reinforcement Learning. State-of-the-art sampling strategies for the experience replay buffer improve the performance of the Reinforcement Learning agent. However, they do not incorporate uncertainty in the Q-Value estimation. Consequently, they cannot adapt the sampling strategies, including exploration and exploitation of transitions, to the complexity of the task. To address this, this paper proposes a new sampling strategy that leverages the exploration-exploitation trade-off. This is enabled by the uncertainty estimation of the Q-Value function, which guides the sampling to explore more significant transitions and, thus, learn a more efficient policy. Experiments on classical control environments demonstrate stable results across various environments. They show that the proposed method outperforms state-of-the-art sampling strategies for dense rewards w.r.t. convergence and peak performance by 26% on av
&lt;/p&gt;</description></item><item><title>BARS&#26159;&#19968;&#20010;&#26426;&#22330;&#36305;&#36947;&#20998;&#21106;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;11&#31181;&#20195;&#34920;&#24615;&#30340;&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#30340;&#35780;&#20272;&#21644;&#24615;&#33021;&#20998;&#26512;&#65292;&#35753;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#33021;&#26356;&#22909;&#22320;&#36866;&#24212;&#22797;&#26434;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2210.12922</link><description>&lt;p&gt;
BARS&#65306;&#26426;&#22330;&#36305;&#36947;&#20998;&#21106;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BARS: A Benchmark for Airport Runway Segmentation. (arXiv:2210.12922v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12922
&lt;/p&gt;
&lt;p&gt;
BARS&#26159;&#19968;&#20010;&#26426;&#22330;&#36305;&#36947;&#20998;&#21106;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;11&#31181;&#20195;&#34920;&#24615;&#30340;&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#30340;&#35780;&#20272;&#21644;&#24615;&#33021;&#20998;&#26512;&#65292;&#35753;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#33021;&#26356;&#22909;&#22320;&#36866;&#24212;&#22797;&#26434;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22330;&#36305;&#36947;&#20998;&#21106;&#21487;&#26377;&#25928;&#38477;&#20302;&#38477;&#33853;&#38454;&#27573;&#30340;&#20107;&#25925;&#29575;&#65292;&#32780;&#28145;&#24230;&#23398;&#20064;&#30456;&#20851;&#26041;&#27861;&#22312;&#20998;&#21106;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#21487;&#20197;&#24456;&#22909;&#22320;&#36866;&#24212;&#22797;&#26434;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;&#35813;&#39046;&#22495;&#32570;&#20047;&#22823;&#35268;&#27169;&#12289;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#20351;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#38590;&#20197;&#24320;&#21457;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;BARS&#30340;&#26426;&#22330;&#36305;&#36947;&#20998;&#21106;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#21033;&#29992;X-Plane&#27169;&#25311;&#24179;&#21488;&#25910;&#38598;&#30340;&#65292;&#21253;&#21547;10,256&#24352;&#22270;&#20687;&#21644;30,201&#20010;&#23454;&#20363;&#65292;&#26377;&#19977;&#20010;&#31867;&#21035;&#12290;&#25105;&#20204;&#22312;BARS&#19978;&#35780;&#20272;&#20102;11&#31181;&#20195;&#34920;&#24615;&#30340;&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Airport runway segmentation can effectively reduce the accident rate during the landing phase, which has the largest risk of flight accidents. With the rapid development of deep learning (DL), related methods achieve good performance on segmentation tasks and can be well adapted to complex scenes. However, the lack of large-scale, publicly available datasets in this field makes the development of methods based on DL difficult. Therefore, we propose a benchmark for airport runway segmentation, named BARS. Additionally, a semiautomatic annotation pipeline is designed to reduce the annotation workload. BARS has the largest dataset with the richest categories and the only instance annotation in the field. The dataset, which was collected using the X-Plane simulation platform, contains 10,256 images and 30,201 instances with three categories. We evaluate eleven representative instance segmentation methods on BARS and analyze their performance. Based on the characteristic of an airport runwa
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#26816;&#32034;&#22686;&#24378;&#30340;&#26550;&#26500;&#24863;&#30693;&#21442;&#32771;&#20316;&#20026;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#21487;&#21160;&#24577;&#21033;&#29992;&#20154;&#31867;&#27880;&#37322;&#21644;&#24369;&#30417;&#30563;&#25968;&#25454;&#25152;&#32487;&#25215;&#30340;&#26550;&#26500;&#21644;&#30693;&#35782;&#65292;&#25351;&#23548;&#29983;&#25104;&#20855;&#26377;&#26356;&#22909;&#35821;&#20041;&#36830;&#36143;&#24615;&#21644;&#19968;&#33268;&#24615;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20174;&#32780;&#22312;&#25968;&#25454;&#25928;&#29575;&#21644;&#30693;&#35782;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.10709</link><description>&lt;p&gt;
&#20197;&#26550;&#26500;&#24863;&#30693;&#21442;&#32771;&#20316;&#20026;&#25552;&#31034;&#25552;&#39640;&#20102;&#25968;&#25454;&#26377;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Schema-aware Reference as Prompt Improves Data-Efficient Knowledge Graph Construction. (arXiv:2210.10709v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10709
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#26816;&#32034;&#22686;&#24378;&#30340;&#26550;&#26500;&#24863;&#30693;&#21442;&#32771;&#20316;&#20026;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#21487;&#21160;&#24577;&#21033;&#29992;&#20154;&#31867;&#27880;&#37322;&#21644;&#24369;&#30417;&#30563;&#25968;&#25454;&#25152;&#32487;&#25215;&#30340;&#26550;&#26500;&#21644;&#30693;&#35782;&#65292;&#25351;&#23548;&#29983;&#25104;&#20855;&#26377;&#26356;&#22909;&#35821;&#20041;&#36830;&#36143;&#24615;&#21644;&#19968;&#33268;&#24615;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20174;&#32780;&#22312;&#25968;&#25454;&#25928;&#29575;&#21644;&#30693;&#35782;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#35768;&#22810;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#34987;&#25552;&#20986;&#24182;&#22312;&#25968;&#25454;&#26377;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#26041;&#27861;&#20173;&#23384;&#22312;&#20960;&#20010;&#28508;&#22312;&#30340;&#38480;&#21046;&#65306;&#65288;i&#65289;&#33258;&#28982;&#35821;&#35328;&#21644;&#39044;&#23450;&#20041;&#27169;&#24335;&#30340;&#36755;&#20986;&#32467;&#26500;&#21270;&#30693;&#35782;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#65292;&#36825;&#24847;&#21619;&#30528;&#27169;&#22411;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#21463;&#38480;&#27169;&#26495;&#30340;&#35821;&#20041;&#30693;&#35782;&#65307;&#65288;ii&#65289;&#22522;&#20110;&#23616;&#37096;&#20010;&#20307;&#23454;&#20363;&#30340;&#34920;&#31034;&#23398;&#20064;&#38480;&#21046;&#20102;&#24615;&#33021;&#65292;&#32473;&#23450;&#20102;&#19981;&#20805;&#36275;&#30340;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#19981;&#33021;&#37322;&#25918;&#39044;&#20808;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#31867;&#27604;&#33021;&#21147;&#12290;&#21463;&#36825;&#20123;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26816;&#32034;&#24471;&#21040;&#30340;&#26550;&#26500;&#24863;&#30693;&#21442;&#32771;&#20316;&#20026;&#25552;&#31034;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#26377;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#30340;&#35821;&#20041;&#36830;&#36143;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;&#22312;&#20004;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#22522;&#20110;&#25552;&#31034;&#21644;&#38750;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#25928;&#29575;&#21644;&#30693;&#35782;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of pre-trained language models, many prompt-based approaches to data-efficient knowledge graph construction have been proposed and achieved impressive performance. However, existing prompt-based learning methods for knowledge graph construction are still susceptible to several potential limitations: (i) semantic gap between natural language and output structured knowledge with pre-defined schema, which means model cannot fully exploit semantic knowledge with the constrained templates; (ii) representation learning with locally individual instances limits the performance given the insufficient features, which are unable to unleash the potential analogical capability of pre-trained language models. Motivated by these observations, we propose a retrieval-augmented approach, which retrieves schema-aware Reference As Prompt (RAP), for data-efficient knowledge graph construction. It can dynamically leverage schema and knowledge inherited from human-annotated and weak-supe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#24179;&#34913;&#27714;&#35299;&#22120;&#65292;&#21487;&#20197;&#24555;&#36895;&#12289;&#20934;&#30830;&#22320;&#35299;&#20915;&#25152;&#26377;&#22266;&#23450;&#24418;&#29366;&#28216;&#25103;&#31354;&#38388;&#30340;NEs&#12289;CEs&#21644;CCEs&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#28789;&#27963;&#30340;&#24179;&#34913;&#36873;&#25321;&#26694;&#26550;&#65292;&#26377;&#21161;&#20110;&#21152;&#24378;&#22810;&#26234;&#33021;&#20307;&#31639;&#27861;&#30340;&#23454;&#29616;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2210.09257</link><description>&lt;p&gt;
&#29992;&#31070;&#32463;&#24179;&#34913;&#27714;&#35299;&#22120;&#35299;&#20915;NEs&#12289;CEs&#21644;CCEs&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Turbocharging Solution Concepts: Solving NEs, CEs and CCEs with Neural Equilibrium Solvers. (arXiv:2210.09257v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09257
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#24179;&#34913;&#27714;&#35299;&#22120;&#65292;&#21487;&#20197;&#24555;&#36895;&#12289;&#20934;&#30830;&#22320;&#35299;&#20915;&#25152;&#26377;&#22266;&#23450;&#24418;&#29366;&#28216;&#25103;&#31354;&#38388;&#30340;NEs&#12289;CEs&#21644;CCEs&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#28789;&#27963;&#30340;&#24179;&#34913;&#36873;&#25321;&#26694;&#26550;&#65292;&#26377;&#21161;&#20110;&#21152;&#24378;&#22810;&#26234;&#33021;&#20307;&#31639;&#27861;&#30340;&#23454;&#29616;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#21338;&#24328;&#35770;&#20013;&#30340;Nash Equilibria&#12289;Correlated Equilibria&#21644;Coarse Correlated Equilibria&#31561;&#35299;&#20915;&#26041;&#26696;&#23545;&#20110;&#35768;&#22810;&#22810;&#26234;&#33021;&#20307;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#38750;&#24120;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#35299;&#20915;&#27491;&#21017;&#24418;&#24335;&#30340;&#21338;&#24328;&#21487;&#33021;&#38656;&#35201;&#31105;&#27490;&#25110;&#38750;&#30830;&#23450;&#24615;&#26102;&#38388;&#25910;&#25947;&#65292;&#24182;&#19988;&#21487;&#33021;&#20250;&#22833;&#36133;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#31070;&#32463;&#24179;&#34913;&#27714;&#35299;&#22120;&#65292;&#23427;&#21033;&#29992;&#19968;&#31181;&#29305;&#27530;&#30340;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26469;&#36817;&#20284;&#35299;&#20915;&#25152;&#26377;&#22266;&#23450;&#24418;&#29366;&#30340;&#28216;&#25103;&#31354;&#38388;&#65292;&#20174;&#32780;&#25552;&#39640;&#36895;&#24230;&#21644;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#24179;&#34913;&#36873;&#25321;&#26694;&#26550;&#65292;&#33021;&#22815;&#21807;&#19968;&#36873;&#25321;&#26368;&#23567;&#21270;&#30456;&#23545;&#29109;&#25110;&#26368;&#22823;&#21270;&#31119;&#21033;&#30340;&#24179;&#34913;&#12290;&#32593;&#32476;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#29983;&#25104;&#20219;&#20309;&#30417;&#30563;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#38646;-shot&#27867;&#21270;&#33021;&#21147;&#65292;&#20351;&#32593;&#32476;&#36866;&#29992;&#20110;&#26356;&#22823;&#30340;&#28216;&#25103;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26679;&#30340;&#32593;&#32476;&#26159;&#35768;&#22810;&#21487;&#33021;&#22810;&#26234;&#33021;&#20307;&#31639;&#27861;&#30340;&#24378;&#22823;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solution concepts such as Nash Equilibria, Correlated Equilibria, and Coarse Correlated Equilibria are useful components for many multiagent machine learning algorithms. Unfortunately, solving a normal-form game could take prohibitive or non-deterministic time to converge, and could fail. We introduce the Neural Equilibrium Solver which utilizes a special equivariant neural network architecture to approximately solve the space of all games of fixed shape, buying speed and determinism. We define a flexible equilibrium selection framework, that is capable of uniquely selecting an equilibrium that minimizes relative entropy, or maximizes welfare. The network is trained without needing to generate any supervised training data. We show remarkable zero-shot generalization to larger games. We argue that such a network is a powerful component for many possible multiagent algorithms.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN&#30340;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#20581;&#24247;&#32452;&#32455;&#36827;&#34892;&#36741;&#21161;&#20219;&#21153;&#35757;&#32451;&#65292;&#20351;&#34920;&#31034;&#36866;&#24212;&#32452;&#32455;&#20013;&#30340;&#30456;&#20851;&#32454;&#33410;&#65292;&#23454;&#29616;&#23545;&#32452;&#32455;&#23398;&#22270;&#20687;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2210.07675</link><description>&lt;p&gt;
&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#20197;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65306;&#22312;&#33647;&#29289;&#24320;&#21457;&#20013;&#21457;&#29616;&#32452;&#32455;&#23398;&#25913;&#21464;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning image representations for anomaly detection: application to discovery of histological alterations in drug development. (arXiv:2210.07675v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07675
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN&#30340;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#20581;&#24247;&#32452;&#32455;&#36827;&#34892;&#36741;&#21161;&#20219;&#21153;&#35757;&#32451;&#65292;&#20351;&#34920;&#31034;&#36866;&#24212;&#32452;&#32455;&#20013;&#30340;&#30456;&#20851;&#32454;&#33410;&#65292;&#23454;&#29616;&#23545;&#32452;&#32455;&#23398;&#22270;&#20687;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#30340;&#31995;&#32479;&#12290;&#22312;&#32452;&#32455;&#23398;&#20013;&#65292;&#27491;&#24120;&#26679;&#26412;&#36890;&#24120;&#26159;&#22823;&#37327;&#23384;&#22312;&#30340;&#65292;&#32780;&#24322;&#24120;&#65288;&#30149;&#29702;&#65289;&#24773;&#20917;&#36890;&#24120;&#24456;&#23569;&#25110;&#19981;&#21487;&#29992;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#22312;&#20581;&#24247;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#21333;&#31867;&#20998;&#31867;&#22120;&#21487;&#20197;&#26816;&#27979;&#21040;&#20998;&#24067;&#22806;&#30340;&#24322;&#24120;&#26679;&#26412;&#12290;&#36825;&#26679;&#30340;&#26041;&#27861;&#19982;&#39044;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22270;&#20687;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#20197;&#21069;&#24050;&#32463;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#65288;AD&#65289;&#12290;&#20294;&#26159;&#65292;&#39044;&#35757;&#32451;&#30340;&#29616;&#25104;CNN&#34920;&#31034;&#21487;&#33021;&#23545;&#32452;&#32455;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#19981;&#25935;&#24863;&#65292;&#32780;&#20581;&#24247;&#32452;&#32455;&#30340;&#33258;&#28982;&#21464;&#24322;&#21487;&#33021;&#23548;&#33268;&#36828;&#31163;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#20351;&#34920;&#31034;&#36866;&#24212;&#20581;&#24247;&#32452;&#32455;&#20013;&#30340;&#30456;&#20851;&#32454;&#33410;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#36741;&#21161;&#20219;&#21153;&#19978;&#35757;&#32451;CNN&#65292;&#35813;&#20219;&#21153;&#21306;&#20998;&#19981;&#21516;&#29289;&#31181;&#12289;&#22120;&#23448;&#21644;&#26579;&#33394;&#35797;&#21058;&#30340;&#20581;&#24247;&#32452;&#32455;&#12290;&#20960;&#20046;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#26631;&#27880;&#24037;&#20316;&#37327;&#65292;&#22240;&#20026;&#20581;&#24247;&#26679;&#26412;&#21487;&#20197;&#33258;&#21160;&#33719;&#24471;&#19978;&#36848;&#26631;&#31614;&#12290;&#22312;&#35757;&#32451;&#20013;&#65292;&#25105;&#20204;&#24378;&#21046;&#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
We present a system for anomaly detection in histopathological images. In histology, normal samples are usually abundant, whereas anomalous (pathological) cases are scarce or not available. Under such settings, one-class classifiers trained on healthy data can detect out-of-distribution anomalous samples. Such approaches combined with pre-trained Convolutional Neural Network (CNN) representations of images were previously employed for anomaly detection (AD). However, pre-trained off-the-shelf CNN representations may not be sensitive to abnormal conditions in tissues, while natural variations of healthy tissue may result in distant representations. To adapt representations to relevant details in healthy tissue we propose training a CNN on an auxiliary task that discriminates healthy tissue of different species, organs, and staining reagents. Almost no additional labeling workload is required, since healthy samples come automatically with aforementioned labels. During training we enforce
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24179;&#34913;&#25968;&#31995;&#32479;&#30340;&#25968;&#23383;&#35745;&#31639;&#26041;&#26696;&#65292;&#29992;&#20110;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#20013;&#30340;&#26799;&#24230;&#32858;&#21512;&#12290;&#36890;&#36807;&#25968;&#23383;&#30340;&#24179;&#22343;&#20540;&#35745;&#31639;&#23454;&#25968;&#21442;&#25968;&#30340;&#24179;&#22343;&#20540;&#65292;&#36991;&#20813;&#20102;&#23545;&#31934;&#30830;&#26679;&#26412;&#32423;&#26102;&#38388;&#21516;&#27493;&#12289;&#20449;&#36947;&#20272;&#35745;&#24320;&#38144;&#21644;&#20449;&#36947;&#21453;&#36716;&#30340;&#38656;&#27714;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#32858;&#21512;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.07012</link><description>&lt;p&gt;
&#22522;&#20110;&#24179;&#34913;&#25968;&#31995;&#32479;&#30340;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#26080;&#32447;&#35745;&#31639;&#35774;&#35745;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Over-the-Air Computation Based on Balanced Number Systems for Federated Edge Learning. (arXiv:2210.07012v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24179;&#34913;&#25968;&#31995;&#32479;&#30340;&#25968;&#23383;&#35745;&#31639;&#26041;&#26696;&#65292;&#29992;&#20110;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#20013;&#30340;&#26799;&#24230;&#32858;&#21512;&#12290;&#36890;&#36807;&#25968;&#23383;&#30340;&#24179;&#22343;&#20540;&#35745;&#31639;&#23454;&#25968;&#21442;&#25968;&#30340;&#24179;&#22343;&#20540;&#65292;&#36991;&#20813;&#20102;&#23545;&#31934;&#30830;&#26679;&#26412;&#32423;&#26102;&#38388;&#21516;&#27493;&#12289;&#20449;&#36947;&#20272;&#35745;&#24320;&#38144;&#21644;&#20449;&#36947;&#21453;&#36716;&#30340;&#38656;&#27714;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#32858;&#21512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23383;&#26080;&#32447;&#35745;&#31639;&#65288;OAC&#65289;&#26041;&#26696;&#65292;&#29992;&#20110;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#65288;FEEL&#65289;&#30340;&#36830;&#32493;&#20540;&#65288;&#27169;&#25311;&#65289;&#32858;&#21512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21033;&#29992;&#22522;&#20110;&#24179;&#34913;&#25968;&#31995;&#32479;&#30340;&#25968;&#23383;&#21487;&#20197;&#36817;&#20284;&#35745;&#31639;&#19968;&#32452;&#23454;&#25968;&#21442;&#25968;&#30340;&#24179;&#22343;&#20540;&#12290;&#36890;&#36807;&#21033;&#29992;&#35813;&#20851;&#38190;&#23646;&#24615;&#65292;&#35813;&#26041;&#26696;&#23558;&#26412;&#22320;&#38543;&#26426;&#26799;&#24230;&#32534;&#30721;&#20026;&#19968;&#32452;&#25968;&#23383;&#12290;&#25509;&#19979;&#26469;&#65292;&#23427;&#21033;&#29992;&#25968;&#23383;&#30340;&#20540;&#30830;&#23450;&#28608;&#27963;&#30340;&#27491;&#20132;&#39057;&#20998;&#22797;&#29992;&#65288;OFDM&#65289;&#23376;&#36733;&#27874;&#30340;&#20301;&#32622;&#12290;&#35813;&#26041;&#26696;&#22312;&#36793;&#32536;&#26381;&#21153;&#22120;&#65288;ES&#65289;&#20351;&#29992;&#38750;&#30456;&#24178;&#25509;&#25910;&#22120;&#65292;&#19981;&#38656;&#35201;&#31934;&#30830;&#30340;&#26679;&#26412;&#32423;&#26102;&#38388;&#21516;&#27493;&#12289;&#20449;&#36947;&#20272;&#35745;&#24320;&#38144;&#21644;&#20449;&#36947;&#21453;&#36716;&#65292;&#24182;&#19988;&#19981;&#21033;&#29992;&#36793;&#32536;&#35774;&#22791;&#65288;EDs&#65289;&#19978;&#30340;&#39044;&#22343;&#34913;&#12290;&#25105;&#20204;&#29702;&#35770;&#20998;&#26512;&#20102;&#35813;&#26041;&#26696;&#30340;MSE&#24615;&#33021;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we propose a digital over-the-air computation (OAC) scheme for achieving continuous-valued (analog) aggregation for federated edge learning (FEEL). We show that the average of a set of real-valued parameters can be calculated approximately by using the average of the corresponding numerals, where the numerals are obtained based on a balanced number system. By exploiting this key property, the proposed scheme encodes the local stochastic gradients into a set of numerals. Next, it determines the positions of the activated orthogonal frequency division multiplexing (OFDM) subcarriers by using the values of the numerals. To eliminate the need for precise sample-level time synchronization, channel estimation overhead, and channel inversion, the proposed scheme also uses a non-coherent receiver at the edge server (ES) and does not utilize a pre-equalization at the edge devices (EDs). We theoretically analyze the MSE performance of the proposed scheme and the convergence rate f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#19978;&#21487;&#34892;&#30340;&#26041;&#27861;&#26469;&#30830;&#23450;&#22823;&#35268;&#27169;&#22797;&#26434;&#25968;&#25454;&#30340;&#20869;&#22312;&#32500;&#24230;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#25968;&#25454;&#38598;&#30340;&#20960;&#20309;&#29305;&#24615;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.05301</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#20960;&#20309;&#23398;&#20064;&#30340;&#20869;&#22312;&#32500;&#24230;
&lt;/p&gt;
&lt;p&gt;
Intrinsic Dimension for Large-Scale Geometric Learning. (arXiv:2210.05301v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#19978;&#21487;&#34892;&#30340;&#26041;&#27861;&#26469;&#30830;&#23450;&#22823;&#35268;&#27169;&#22797;&#26434;&#25968;&#25454;&#30340;&#20869;&#22312;&#32500;&#24230;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#25968;&#25454;&#38598;&#30340;&#20960;&#20309;&#29305;&#24615;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32500;&#24230;&#30340;&#27010;&#24565;&#23545;&#20110;&#29702;&#35299;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#30830;&#23450;&#25968;&#25454;&#38598;&#30340;&#32500;&#24230;&#30340;&#19968;&#31181;&#22825;&#30495;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#23646;&#24615;&#30340;&#25968;&#37327;&#12290;&#26356;&#22797;&#26434;&#30340;&#26041;&#27861;&#25512;&#23548;&#20986;&#20869;&#22312;&#32500;&#24230;&#65288;ID&#65289;&#30340;&#27010;&#24565;&#65292;&#20351;&#29992;&#26356;&#22797;&#26434;&#30340;&#29305;&#24449;&#20989;&#25968;&#65292;&#20363;&#22914;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#36825;&#20123;&#26041;&#27861;&#22522;&#20110;&#32463;&#39564;&#35266;&#23519;&#65292;&#26080;&#27861;&#24212;&#23545;&#24403;&#20195;&#25968;&#25454;&#38598;&#30340;&#20960;&#20309;&#29305;&#24615;&#65292;&#24182;&#19988;&#32570;&#20047;&#20844;&#29702;&#22522;&#30784;&#12290;V. Pestov&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#23558;&#20869;&#22312;&#32500;&#24230;&#20844;&#29702;&#22320;&#19982;&#25968;&#23398;&#38598;&#20013;&#24230;&#29616;&#35937;&#32852;&#31995;&#36215;&#26469;&#12290;&#39318;&#20808;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#23454;&#38469;&#25968;&#25454;&#38598;&#65292;&#35745;&#31639;&#36825;&#20123;&#20869;&#22312;&#32500;&#24230;&#30340;&#26041;&#27861;&#35745;&#31639;&#19978;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#19978;&#21487;&#34892;&#30340;&#26041;&#27861;&#26469;&#30830;&#23450;&#36825;&#20123;&#20844;&#29702;&#30340;&#20869;&#22312;&#32500;&#24230;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The concept of dimension is essential to grasp the complexity of data. A naive approach to determine the dimension of a dataset is based on the number of attributes. More sophisticated methods derive a notion of intrinsic dimension (ID) that employs more complex feature functions, e.g., distances between data points. Yet, many of these approaches are based on empirical observations, cannot cope with the geometric character of contemporary datasets, and do lack an axiomatic foundation. A different approach was proposed by V. Pestov, who links the intrinsic dimension axiomatically to the mathematical concentration of measure phenomenon. First methods to compute this and related notions for ID were computationally intractable for large-scale real-world datasets. In the present work, we derive a computationally feasible method for determining said axiomatic ID functions. Moreover, we demonstrate how the geometric properties of complex data are accounted for in our modeling. In particular, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#20449;&#24687;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#37051;&#25509;&#30697;&#38453;&#26356;&#26032;&#25216;&#26415;&#39044;&#35757;&#32451;&#22270;&#21367;&#31215;&#32593;&#32476;&#20197;&#26356;&#22909;&#22320;&#39044;&#27979;&#32454;&#32990;&#22312;&#21453;&#20107;&#23454;&#24178;&#25200;&#19979;&#30340;&#22522;&#22240;&#34920;&#36798;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;&#20272;&#35745;&#22120;&#26469;&#39640;&#25928;&#20272;&#35745;&#36793;&#32536;&#24178;&#25200;&#25928;&#24212;&#12290;&#30740;&#31350;&#32467;&#26524;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.00116</link><description>&lt;p&gt;
&#21033;&#29992;&#21464;&#20998;&#22240;&#26524;&#25512;&#26029;&#21644;&#31934;&#32454;&#20851;&#31995;&#20449;&#24687;&#39044;&#27979;&#32454;&#32990;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Predicting Cellular Responses with Variational Causal Inference and Refined Relational Information. (arXiv:2210.00116v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#20449;&#24687;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#37051;&#25509;&#30697;&#38453;&#26356;&#26032;&#25216;&#26415;&#39044;&#35757;&#32451;&#22270;&#21367;&#31215;&#32593;&#32476;&#20197;&#26356;&#22909;&#22320;&#39044;&#27979;&#32454;&#32990;&#22312;&#21453;&#20107;&#23454;&#24178;&#25200;&#19979;&#30340;&#22522;&#22240;&#34920;&#36798;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;&#20272;&#35745;&#22120;&#26469;&#39640;&#25928;&#20272;&#35745;&#36793;&#32536;&#24178;&#25200;&#25928;&#24212;&#12290;&#30740;&#31350;&#32467;&#26524;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#32454;&#32990;&#22312;&#24178;&#25200;&#19979;&#30340;&#21709;&#24212;&#21487;&#33021;&#20026;&#33647;&#29289;&#30740;&#21457;&#21644;&#20010;&#24615;&#21270;&#27835;&#30103;&#24102;&#26469;&#37325;&#35201;&#22909;&#22788;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#24418;&#21464;&#20998;&#36125;&#21494;&#26031;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#65292;&#39044;&#27979;&#32454;&#32990;&#22312;&#21453;&#20107;&#23454;&#24178;&#25200;&#19979;&#65288;&#21363;&#32454;&#32990;&#26410;&#30495;&#23454;&#25509;&#25910;&#30340;&#24178;&#25200;&#65289;&#30340;&#22522;&#22240;&#34920;&#36798;&#65292;&#21033;&#29992;&#20195;&#34920;&#29983;&#29289;&#23398;&#30693;&#35782;&#30340;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#65288;GRN&#65289;&#20449;&#24687;&#26469;&#36741;&#21161;&#20010;&#24615;&#21270;&#32454;&#32990;&#21709;&#24212;&#39044;&#27979;&#12290;&#25105;&#20204;&#36824;&#38024;&#23545;&#25968;&#25454;&#33258;&#36866;&#24212;GRN&#24320;&#21457;&#20102;&#37051;&#25509;&#30697;&#38453;&#26356;&#26032;&#25216;&#26415;&#29992;&#20110;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#39044;&#35757;&#32451;&#65292;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#25552;&#20379;&#20102;&#26356;&#22810;&#30340;&#22522;&#22240;&#20851;&#31995;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting the responses of a cell under perturbations may bring important benefits to drug discovery and personalized therapeutics. In this work, we propose a novel graph variational Bayesian causal inference framework to predict a cell's gene expressions under counterfactual perturbations (perturbations that this cell did not factually receive), leveraging information representing biological knowledge in the form of gene regulatory networks (GRNs) to aid individualized cellular response predictions. Aiming at a data-adaptive GRN, we also developed an adjacency matrix updating technique for graph convolutional networks and used it to refine GRNs during pre-training, which generated more insights on gene relations and enhanced model performance. Additionally, we propose a robust estimator within our framework for the asymptotically efficient estimation of marginal perturbation effect, which is yet to be carried out in previous works. With extensive experiments, we exhibited the advanta
&lt;/p&gt;</description></item><item><title>DecAF &#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#21644;&#30452;&#25509;&#31572;&#26696;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#23427;&#20204;&#30340;&#20248;&#28857;&#20197;&#33719;&#21462;&#26368;&#32456;&#31572;&#26696;&#65307;&#21516;&#26102;&#65292;&#23427;&#36824;&#37319;&#29992;&#20102;&#31616;&#21333;&#30340;&#33258;&#30001;&#25991;&#26412;&#26816;&#32034;&#65292;&#30456;&#27604;&#20197;&#24448;&#30340;&#26041;&#27861;&#26356;&#26131;&#20110;&#36866;&#24212;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2210.00063</link><description>&lt;p&gt;
DecAF&#65306;&#38024;&#23545;&#30693;&#35782;&#24211;&#38382;&#31572;&#30340;&#31572;&#26696;&#21644;&#36923;&#36753;&#24418;&#24335;&#32852;&#21512;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
DecAF: Joint Decoding of Answers and Logical Forms for Question Answering over Knowledge Bases. (arXiv:2210.00063v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00063
&lt;/p&gt;
&lt;p&gt;
DecAF &#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#21644;&#30452;&#25509;&#31572;&#26696;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#23427;&#20204;&#30340;&#20248;&#28857;&#20197;&#33719;&#21462;&#26368;&#32456;&#31572;&#26696;&#65307;&#21516;&#26102;&#65292;&#23427;&#36824;&#37319;&#29992;&#20102;&#31616;&#21333;&#30340;&#33258;&#30001;&#25991;&#26412;&#26816;&#32034;&#65292;&#30456;&#27604;&#20197;&#24448;&#30340;&#26041;&#27861;&#26356;&#26131;&#20110;&#36866;&#24212;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24211;&#38382;&#31572;&#26088;&#22312;&#20351;&#29992;&#30693;&#35782;&#24211;&#20013;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#31561;&#20107;&#23454;&#20449;&#24687;&#22238;&#31572;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#29983;&#25104;&#21487;&#22312;&#30693;&#35782;&#24211;&#19978;&#25191;&#34892;&#20197;&#33719;&#21462;&#26368;&#32456;&#31572;&#26696;&#30340;&#36923;&#36753;&#24418;&#24335;&#65292;&#35201;&#20040;&#30452;&#25509;&#39044;&#27979;&#31572;&#26696;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#21069;&#32773;&#36890;&#24120;&#33021;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#31572;&#26696;&#65292;&#20294;&#30001;&#20110;&#29983;&#25104;&#30340;&#36923;&#36753;&#24418;&#24335;&#21487;&#33021;&#23384;&#22312;&#35821;&#27861;&#21644;&#35821;&#20041;&#38169;&#35823;&#30340;&#28508;&#22312;&#38382;&#39064;&#65292;&#22240;&#27492;&#23384;&#22312;&#26080;&#27861;&#25191;&#34892;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550; DecAF&#65292;&#23427;&#32852;&#21512;&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#21644;&#30452;&#25509;&#31572;&#26696;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#24471;&#21040;&#26368;&#32456;&#31572;&#26696;&#12290;&#27492;&#22806;&#65292;&#19982;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;DecAF &#22522;&#20110;&#31616;&#21333;&#30340;&#33258;&#30001;&#25991;&#26412;&#26816;&#32034;&#65292;&#32780;&#19981;&#20381;&#36182;&#20219;&#20309;&#23454;&#20307;&#38142;&#25509;&#24037;&#20855;--&#36825;&#31181;&#31616;&#21270;&#20351;&#20854;&#36866;&#24212;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#26356;&#21152;&#23481;&#26131;&#12290;DecAF &#22312; WebQSP&#12289;FreebaseQA &#21644; GrailQA &#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#22312; CommonsenseQA &#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering over knowledge bases (KBs) aims to answer natural language questions with factual information such as entities and relations in KBs. Previous methods either generate logical forms that can be executed over KBs to obtain final answers or predict answers directly. Empirical results show that the former often produces more accurate answers, but it suffers from non-execution issues due to potential syntactic and semantic errors in the generated logical forms. In this work, we propose a novel framework DecAF that jointly generates both logical forms and direct answers, and then combines the merits of them to get the final answers. Moreover, different from most of the previous methods, DecAF is based on simple free-text retrieval without relying on any entity linking tools -- this simplification eases its adaptation to different datasets. DecAF achieves new state-of-the-art accuracy on WebQSP, FreebaseQA, and GrailQA benchmarks, while getting competitive results on the Com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#25104;&#20998;&#31867;&#22120;&#30340;&#26550;&#26500;&#65292;&#31216;&#20316;SPRITZ-1.5C&#65292;&#23427;&#21516;&#26102;&#32467;&#21512;&#20102;1&#31867;&#20998;&#31867;&#30340;&#23433;&#20840;&#24615;&#21644;&#20256;&#32479;2&#31867;&#20998;&#31867;&#30340;&#39640;&#24615;&#33021;&#65292;&#22312;&#35745;&#31639;&#26426;&#32593;&#32476;&#31561;&#23433;&#20840;&#22411;&#24212;&#29992;&#20013;&#25269;&#24481;&#23545;&#25239;&#25915;&#20987;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.12195</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#38598;&#25104;&#23398;&#20064;&#25552;&#21319;&#35745;&#31639;&#26426;&#32593;&#32476;&#23545;&#25239;&#25915;&#20987;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Employing Deep Ensemble Learning for Improving the Security of Computer Networks against Adversarial Attacks. (arXiv:2209.12195v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#25104;&#20998;&#31867;&#22120;&#30340;&#26550;&#26500;&#65292;&#31216;&#20316;SPRITZ-1.5C&#65292;&#23427;&#21516;&#26102;&#32467;&#21512;&#20102;1&#31867;&#20998;&#31867;&#30340;&#23433;&#20840;&#24615;&#21644;&#20256;&#32479;2&#31867;&#20998;&#31867;&#30340;&#39640;&#24615;&#33021;&#65292;&#22312;&#35745;&#31639;&#26426;&#32593;&#32476;&#31561;&#23433;&#20840;&#22411;&#24212;&#29992;&#20013;&#25269;&#24481;&#23545;&#25239;&#25915;&#20987;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#32593;&#32476;&#21644;&#22810;&#23186;&#20307;&#23433;&#20840;&#31561;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#20102;&#26497;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#32593;&#32476;&#32467;&#26500;&#30340;&#33030;&#24369;&#24615;&#20351;&#20854;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#23433;&#20840;&#24615;&#26041;&#38754;&#19981;&#22826;&#36866;&#29992;&#20110;&#35832;&#22914;&#35745;&#31639;&#26426;&#32593;&#32476;&#31561;&#30340;&#23433;&#20840;&#22411;&#24212;&#29992;&#20013;&#12290;&#20026;&#20102;&#20445;&#25252;&#36825;&#20123;&#26550;&#26500;&#20813;&#21463;&#23545;&#25239;&#25915;&#20987;&#65292;&#38656;&#35201;&#20351;&#29992;&#20855;&#26377;&#25361;&#25112;&#25915;&#20987;&#33021;&#21147;&#30340;&#23433;&#20840;&#22411;&#26550;&#26500;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#25104;&#20998;&#31867;&#22120;&#30340;&#21019;&#26032;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#22312;&#38754;&#20020;&#25915;&#20987;&#26102;&#21516;&#26102;&#32467;&#21512;&#20102;&#22686;&#24378;&#30340;&#19968;&#31867;&#20998;&#31867;&#21644;&#20256;&#32479;&#30340;&#20108;&#31867;&#20998;&#31867;&#30340;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#34987;&#31216;&#20026;1.5&#31867;&#20998;&#31867;&#22120;&#65288;SPRITZ-1.5C&#65289;&#65292;&#30001;&#32456;&#23494;&#38598;&#20998;&#31867;&#22120;&#12289;&#19968;&#20010;&#20108;&#31867;&#20998;&#31867;&#22120;&#65288;&#21363;CNN&#65289;&#21644;&#20004;&#20010;&#24182;&#34892;&#30340;&#19968;&#31867;&#20998;&#31867;&#22120;&#65288;&#21363;&#33258;&#21160;&#32534;&#30721;&#22120;&#65289;&#26500;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past few years, Convolutional Neural Networks (CNN) have demonstrated promising performance in various real-world cybersecurity applications, such as network and multimedia security. However, the underlying fragility of CNN structures poses major security problems, making them inappropriate for use in security-oriented applications including such computer networks. Protecting these architectures from adversarial attacks necessitates using security-wise architectures that are challenging to attack.  In this study, we present a novel architecture based on an ensemble classifier that combines the enhanced security of 1-Class classification (known as 1C) with the high performance of conventional 2-Class classification (known as 2C) in the absence of attacks.Our architecture is referred to as the 1.5-Class (SPRITZ-1.5C) classifier and constructed using a final dense classifier, one 2C classifier (i.e., CNNs), and two parallel 1C classifiers (i.e., auto-encoders). In our experiments, 
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#35777;&#26126;&#26426;&#21046;PoL&#23384;&#22312;&#19981;&#23569;&#38382;&#39064;&#65292;&#30001;&#20110;&#29616;&#26377;&#30340;&#27450;&#39575;&#31574;&#30053;&#24456;&#23481;&#26131;&#34987;&#25171;&#36133;&#25110;&#26080;&#27861;&#37325;&#29616;&#65292;&#22240;&#27492;&#23545;&#23545;&#25163;&#30340;&#23433;&#20840;&#20445;&#38556;&#19981;&#31283;&#20581;&#12290;&#26032;&#30340;&#27450;&#39575;&#31574;&#30053;&#24341;&#20837;&#21487;&#20197;&#25171;&#30772;PoL&#30340;&#26368;&#26032;&#38450;&#24481;&#26041;&#27861;&#65292;&#20294;&#25104;&#26412;&#36739;&#20302;&#12290;</title><link>http://arxiv.org/abs/2208.03567</link><description>&lt;p&gt;
&#23398;&#20064;&#35777;&#26126;&#26426;&#21046;&#30446;&#21069;&#23384;&#22312;&#35768;&#22810;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Proof-of-Learning is Currently More Broken Than You Think. (arXiv:2208.03567v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.03567
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#35777;&#26126;&#26426;&#21046;PoL&#23384;&#22312;&#19981;&#23569;&#38382;&#39064;&#65292;&#30001;&#20110;&#29616;&#26377;&#30340;&#27450;&#39575;&#31574;&#30053;&#24456;&#23481;&#26131;&#34987;&#25171;&#36133;&#25110;&#26080;&#27861;&#37325;&#29616;&#65292;&#22240;&#27492;&#23545;&#23545;&#25163;&#30340;&#23433;&#20840;&#20445;&#38556;&#19981;&#31283;&#20581;&#12290;&#26032;&#30340;&#27450;&#39575;&#31574;&#30053;&#24341;&#20837;&#21487;&#20197;&#25171;&#30772;PoL&#30340;&#26368;&#26032;&#38450;&#24481;&#26041;&#27861;&#65292;&#20294;&#25104;&#26412;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#35777;&#26126;&#65288;PoL&#65289;&#25552;&#20986;&#65292;&#27169;&#22411;&#25152;&#26377;&#32773;&#35760;&#24405;&#35757;&#32451;&#26816;&#26597;&#28857;&#65292;&#20197;&#24314;&#31435;&#20026;&#35757;&#32451;&#32791;&#36153;&#30340;&#35745;&#31639;&#25552;&#20379;&#35777;&#26126;&#12290; PoL&#30340;&#20316;&#32773;&#25918;&#24323;&#20102;&#21152;&#23494;&#26041;&#27861;&#65292;&#20197;&#25442;&#21462;&#28145;&#24230;&#23398;&#20064;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#20174;&#32780;&#25442;&#21462;&#20102;&#20005;&#26684;&#30340;&#23433;&#20840;&#20445;&#35777;&#12290;&#20182;&#20204;&#36890;&#36807;&#23637;&#31034;&#30423;&#29992;&#27169;&#22411;&#30340;&#35745;&#31639;&#35777;&#26126;--&#35745;&#31639;&#20599;&#26469;&#30340;&#27169;&#22411;&#30340;&#35777;&#26126;&#65292;&#21644;&#30495;&#27491;&#22320;&#35757;&#32451;&#27169;&#22411;&#25152;&#38656;&#35201;&#30340;&#35777;&#26126;&#19968;&#26679;&#26114;&#36149;&#26469;&#23454;&#35777;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#12290;&#20294;&#26159;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#21453;&#20363;&#65292;&#20174;&#32780;&#20351;&#36825;&#20010;&#35266;&#23519;&#22833;&#25928;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#65292;&#23613;&#31649;&#24403;&#21069;PoL&#39564;&#35777;&#23545;&#20110;&#23545;&#25163;&#26469;&#35828;&#19981;&#31283;&#20581;&#26159;&#30495;&#23454;&#30340;&#65292;&#20294;&#26159;&#26368;&#36817;&#30340;&#24037;&#20316;&#22823;&#22823;&#20302;&#20272;&#20102;&#36825;&#31181;&#32570;&#20047;&#31283;&#20581;&#24615;&#12290;&#36825;&#26159;&#22240;&#20026;&#29616;&#26377;&#30340;&#27450;&#39575;&#31574;&#30053;&#35201;&#20040;&#19981;&#21487;&#37325;&#29616;&#65292;&#35201;&#20040;&#38024;&#23545;PoL&#30340;&#21066;&#24369;&#24418;&#24335;--&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#24456;&#23481;&#26131;&#34987;&#26356;&#25913;&#39564;&#35777;&#30340;&#36229;&#21442;&#25968;&#26469;&#25387;&#36133;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#25209;&#27450;&#39575;&#31574;&#30053;&#65292;&#23427;&#20204;&#21487;&#20197;&#25171;&#30772;&#36866;&#29992;&#20110;PoL&#30340;&#26368;&#26032;&#38450;&#24481;&#26041;&#27861;&#65292;&#20294;&#20195;&#20215;&#24456;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proof-of-Learning (PoL) proposes that a model owner logs training checkpoints to establish a proof of having expended the computation necessary for training. The authors of PoL forego cryptographic approaches and trade rigorous security guarantees for scalability to deep learning. They empirically argued the benefit of this approach by showing how spoofing--computing a proof for a stolen model--is as expensive as obtaining the proof honestly by training the model. However, recent work has provided a counter-example and thus has invalidated this observation.  In this work we demonstrate, first, that while it is true that current PoL verification is not robust to adversaries, recent work has largely underestimated this lack of robustness. This is because existing spoofing strategies are either unreproducible or target weakened instantiations of PoL--meaning they are easily thwarted by changing hyperparameters of the verification. Instead, we introduce the first spoofing strategies that c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#22411;&#30340;&#38750;&#20405;&#20837;&#24335;&#26041;&#27861;&#65292;&#20351;&#29992;&#21387;&#21147;&#20256;&#24863;&#22120;&#23545;&#23156;&#20799;&#30340;&#19968;&#33324;&#36816;&#21160;&#36827;&#34892;&#20998;&#31867;&#65292;&#26088;&#22312;&#23454;&#29616;&#26089;&#26399;&#31070;&#32463;&#32908;&#32905;&#38556;&#30861;&#65288;&#22914;&#33041;&#30251;&#65289;&#30340;&#23458;&#35266;&#26816;&#27979;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21387;&#21147;&#25968;&#25454;&#21487;&#20197;&#21306;&#20998;&#23156;&#20799;&#30340;&#20856;&#22411;&#36816;&#21160;&#27169;&#24335;&#65292;&#21363;&#8220;&#22352;&#31435;&#19981;&#23433;&#26399;&#8221;&#21644;&#8220;&#22352;&#31435;&#19981;&#23433;&#21069;&#26399;&#8221;&#12290;</title><link>http://arxiv.org/abs/2208.00884</link><description>&lt;p&gt;
&#22522;&#20110;&#21387;&#21147;&#20998;&#24067;&#20998;&#26512;&#30340;&#23156;&#20799;&#36816;&#21160;&#20998;&#31867;&#8212;&#8212;&#30740;&#31350;&#21644;&#20020;&#24202;&#24212;&#29992;&#30340;&#38468;&#21152;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
Infant movement classification through pressure distribution analysis -- added value for research and clinical implementation. (arXiv:2208.00884v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.00884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#22411;&#30340;&#38750;&#20405;&#20837;&#24335;&#26041;&#27861;&#65292;&#20351;&#29992;&#21387;&#21147;&#20256;&#24863;&#22120;&#23545;&#23156;&#20799;&#30340;&#19968;&#33324;&#36816;&#21160;&#36827;&#34892;&#20998;&#31867;&#65292;&#26088;&#22312;&#23454;&#29616;&#26089;&#26399;&#31070;&#32463;&#32908;&#32905;&#38556;&#30861;&#65288;&#22914;&#33041;&#30251;&#65289;&#30340;&#23458;&#35266;&#26816;&#27979;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21387;&#21147;&#25968;&#25454;&#21487;&#20197;&#21306;&#20998;&#23156;&#20799;&#30340;&#20856;&#22411;&#36816;&#21160;&#27169;&#24335;&#65292;&#21363;&#8220;&#22352;&#31435;&#19981;&#23433;&#26399;&#8221;&#21644;&#8220;&#22352;&#31435;&#19981;&#23433;&#21069;&#26399;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#21387;&#21147;&#20256;&#24863;&#35774;&#22791;&#26469;&#23545;&#23156;&#20799;&#30340;&#19968;&#33324;&#36816;&#21160;&#36827;&#34892;&#20998;&#31867;&#65292;&#20174;&#32780;&#23454;&#29616;&#26089;&#26399;&#30340;&#31070;&#32463;&#32908;&#32905;&#38556;&#30861;&#65288;&#22914;&#33041;&#30251;&#65289;&#30340;&#23458;&#35266;&#26816;&#27979;&#12290;&#26412;&#25991;&#27979;&#35797;&#20102;&#20351;&#29992;&#21387;&#21147;&#25968;&#25454;&#26469;&#21306;&#20998;&#8220;&#22352;&#31435;&#19981;&#23433;&#26399;&#8221;&#65288;&#21363;&#22352;&#31435;&#19981;&#23433;&#36816;&#21160;&#65289;&#19982;&#8220;&#22352;&#31435;&#19981;&#23433;&#21069;&#26399;&#8221;&#65288;&#21363;&#25197;&#21160;&#36816;&#21160;&#65289;&#30340;&#20856;&#22411;&#36816;&#21160;&#27169;&#24335;&#30340;&#21487;&#34892;&#24615;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#35760;&#24405;&#20102;&#27599;&#20010;&#23156;&#20799;&#22312;&#20986;&#29983;&#21518; 4-16 &#21608;&#30340;&#38388;&#38548;&#26399;&#20869;&#36830;&#32493;&#19971;&#20010;&#23454;&#39564;&#23460;&#20250;&#35805;&#30340;&#22810;&#27169;&#24577;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#21253;&#25324;&#26469;&#33258;&#19968;&#20010; 32x32 &#32593;&#26684;&#21387;&#21147;&#20256;&#24863;&#22443;&#21450;&#20854; 1024 &#20010;&#20256;&#24863;&#22120;&#30340;&#21387;&#21147;&#25968;&#25454;&#12290;&#20026;&#20102;&#39564;&#35777;&#27010;&#24565;&#65292;&#20174;&#20004;&#20010;&#30446;&#26631;&#24180;&#40836;&#27573;&#20013;&#65292;&#27599;&#20010;&#25345;&#32493; 5 &#31186;&#30340; 1776 &#20010;&#21387;&#21147;&#25968;&#25454;&#29255;&#27573;&#34987;&#29992;&#20110;&#36816;&#21160;&#20998;&#31867;&#12290;&#27599;&#20010;&#29255;&#27573;&#37117;&#26159;&#26681;&#25454;&#30456;&#24212;&#30340;&#21516;&#27493;&#35270;&#39057;&#25968;&#25454;&#30001;&#20154;&#24037;&#35780;&#20272;&#21592;&#36827;&#34892;&#39044;&#27880;&#37322;&#30340;&#65292;&#26631;&#35760;&#20026;&#22352;&#31435;&#19981;&#23433;&#23384;&#22312;&#25110;&#19981;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aiming at objective early detection of neuromotor disorders such as cerebral palsy, we proposed an innovative non-intrusive approach using a pressure sensing device to classify infant general movements (GMs). Here, we tested the feasibility of using pressure data to differentiate typical GM patterns of the ''fidgety period'' (i.e., fidgety movements) vs. the ''pre-fidgety period'' (i.e., writhing movements). Participants (N = 45) were sampled from a typically-developing infant cohort. Multi-modal sensor data, including pressure data from a 32x32-grid pressure sensing mat with 1024 sensors, were prospectively recorded for each infant in seven succeeding laboratory sessions in biweekly intervals from 4-16 weeks of post-term age. For proof-of-concept, 1776 pressure data snippets, each 5s long, from the two targeted age periods were taken for movement classification. Each snippet was pre-annotated based on corresponding synchronised video data by human assessors as either fidgety present (
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#19968;&#31867;&#20998;&#31867;&#27861;&#30340;&#28145;&#24230;&#23545;&#27604;&#21333;&#31867;&#26102;&#24207;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;(COCA)&#65292;&#33021;&#22815;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2207.01472</link><description>&lt;p&gt;
&#28145;&#24230;&#23545;&#27604;&#21333;&#31867;&#26102;&#24207;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Contrastive One-Class Time Series Anomaly Detection. (arXiv:2207.01472v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.01472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#19968;&#31867;&#20998;&#31867;&#27861;&#30340;&#28145;&#24230;&#23545;&#27604;&#21333;&#31867;&#26102;&#24207;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;(COCA)&#65292;&#33021;&#22815;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#32047;&#31215;&#21644;&#26631;&#31614;&#30340;&#32570;&#22833;&#65292;&#26102;&#24207;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#20010;&#33258;&#25105;&#30417;&#30563;&#30340;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#12290;&#20026;&#20102;&#20811;&#26381;&#29616;&#26377;&#26041;&#27861;&#30340;&#19981;&#36275;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#19968;&#31867;&#20998;&#31867;&#27861;&#30340;&#28145;&#24230;&#23545;&#27604;&#21333;&#31867;&#26102;&#24207;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;(COCA)&#65292;&#23427;&#36890;&#36807;&#25152;&#35859;&#30340;"&#24207;&#21015;&#23545;&#27604;"&#26469;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accumulation of time-series data and the absence of labels make time-series Anomaly Detection (AD) a self-supervised deep learning task. Single-normality-assumption-based methods, which reveal only a certain aspect of the whole normality, are incapable of tasks involved with a large number of anomalies. Specifically, Contrastive Learning (CL) methods distance negative pairs, many of which consist of both normal samples, thus reducing the AD performance. Existing multi-normality-assumption-based methods are usually two-staged, firstly pre-training through certain tasks whose target may differ from AD, limiting their performance. To overcome the shortcomings, a deep Contrastive One-Class Anomaly detection method of time series (COCA) is proposed by authors, following the normality assumptions of CL and one-class classification. It treats the original and reconstructed representations as the positive pair of negative-sample-free CL, namely "sequence contrast". Next, invariance terms a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20195;&#29702;&#21464;&#37327;&#24754;&#35266;&#31574;&#30053;&#20248;&#21270; (P3O) &#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#36817;&#31471;&#22240;&#26524;&#25512;&#26029;&#26500;&#24314;&#24754;&#35266;&#32622;&#20449;&#21306;&#38388;&#32806;&#21512;&#24207;&#21015;&#35299;&#20915;&#20102;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#28151;&#28102;&#20559;&#24046;&#21644;&#26368;&#20248;&#31574;&#30053;&#19982;&#34892;&#20026;&#31574;&#30053;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2205.13589</link><description>&lt;p&gt;
&#38754;&#23545;&#28151;&#28102;&#22240;&#32032;&#30340;&#24754;&#35266;&#24773;&#32490;&#65306;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#35777;&#26126;&#26377;&#25928;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Pessimism in the Face of Confounders: Provably Efficient Offline Reinforcement Learning in Partially Observable Markov Decision Processes. (arXiv:2205.13589v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20195;&#29702;&#21464;&#37327;&#24754;&#35266;&#31574;&#30053;&#20248;&#21270; (P3O) &#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#36817;&#31471;&#22240;&#26524;&#25512;&#26029;&#26500;&#24314;&#24754;&#35266;&#32622;&#20449;&#21306;&#38388;&#32806;&#21512;&#24207;&#21015;&#35299;&#20915;&#20102;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#28151;&#28102;&#20559;&#24046;&#21644;&#26368;&#20248;&#31574;&#30053;&#19982;&#34892;&#20026;&#31574;&#30053;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#26088;&#22312;&#20174;&#30001;&#34892;&#20026;&#31574;&#30053;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#21487;&#33021;&#21462;&#20915;&#20110;&#28508;&#22312;&#29366;&#24577;&#12290;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#22312;&#28151;&#28102;&#24847;&#20041;&#19978;&#21516;&#26102;&#24433;&#21709;&#34892;&#21160;&#21644;&#35266;&#27979;&#20540;&#65292;&#36825;&#23545;&#20110;&#29616;&#26377;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#35828;&#26159;&#31105;&#27490;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#36817;&#31471;&#22240;&#26524;&#25512;&#26029;&#26500;&#24314;&#30340;&#24754;&#35266;&#32622;&#20449;&#21306;&#38388;&#32806;&#21512;&#24207;&#21015;&#30340;&#20195;&#29702;&#21464;&#37327;&#24754;&#35266;&#31574;&#30053;&#20248;&#21270;&#65288;P3O&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#24191;&#20041;&#20989;&#25968;&#36924;&#36817;&#30340;&#19978;&#19979;&#25991;&#20013;&#35299;&#20915;&#20102;&#28151;&#28102;&#20559;&#24046;&#21644;&#26368;&#20248;&#31574;&#30053;&#19982;&#34892;&#20026;&#31574;&#30053;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#28151;&#28102;&#25968;&#25454;&#38598;&#30340;&#37096;&#20998;&#35206;&#30422;&#20551;&#35774;&#19979;&#65292;P3O&#21487;&#20197;&#23454;&#29616;n^{-1/2}&#30340;&#25910;&#25947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study offline reinforcement learning (RL) in partially observable Markov decision processes. In particular, we aim to learn an optimal policy from a dataset collected by a behavior policy which possibly depends on the latent state. Such a dataset is confounded in the sense that the latent state simultaneously affects the action and the observation, which is prohibitive for existing offline RL algorithms. To this end, we propose the \underline{P}roxy variable \underline{P}essimistic \underline{P}olicy \underline{O}ptimization (\texttt{P3O}) algorithm, which addresses the confounding bias and the distributional shift between the optimal and behavior policies in the context of general function approximation. At the core of \texttt{P3O} is a coupled sequence of pessimistic confidence regions constructed via proximal causal inference, which is formulated as minimax estimation. Under a partial coverage assumption on the confounded dataset, we prove that \texttt{P3O} achieves a $n^{-1/2}$-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#30340;&#31574;&#30053;&#65292;&#33021;&#22815;&#24110;&#21161;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#22797;&#26434;&#25512;&#29702;&#24182;&#25512;&#24191;&#21040;&#38590;&#24230;&#26356;&#39640;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#36825;&#31181;&#31574;&#30053;&#32467;&#21512;GPT-3 code-davinci-002&#27169;&#22411;&#33021;&#22815;&#23436;&#32654;&#35299;&#20915;&#32452;&#21512;&#27867;&#21270;&#22522;&#20934;SCAN&#20013;&#30340;&#25152;&#26377;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2205.10625</link><description>&lt;p&gt;
&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#21487;&#20197;&#20351;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#22797;&#26434;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. (arXiv:2205.10625v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#30340;&#31574;&#30053;&#65292;&#33021;&#22815;&#24110;&#21161;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#22797;&#26434;&#25512;&#29702;&#24182;&#25512;&#24191;&#21040;&#38590;&#24230;&#26356;&#39640;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#36825;&#31181;&#31574;&#30053;&#32467;&#21512;GPT-3 code-davinci-002&#27169;&#22411;&#33021;&#22815;&#23436;&#32654;&#35299;&#20915;&#32452;&#21512;&#27867;&#21270;&#22522;&#20934;SCAN&#20013;&#30340;&#25152;&#26377;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24605;&#32500;&#38142;&#25552;&#31034;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#38656;&#35201;&#35299;&#20915;&#27604;&#25552;&#31034;&#20013;&#30340;&#31034;&#20363;&#26356;&#38590;&#30340;&#38382;&#39064;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#31181;&#26131;&#20110;&#22256;&#38590;&#27867;&#21270;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#21363;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#12290;&#35813;&#31574;&#30053;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#22797;&#26434;&#38382;&#39064;&#20998;&#35299;&#25104;&#19968;&#31995;&#21015;&#26356;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#28982;&#21518;&#25353;&#39034;&#24207;&#35299;&#20915;&#23427;&#20204;&#12290;&#35299;&#20915;&#27599;&#20010;&#23376;&#38382;&#39064;&#37117;&#24471;&#30410;&#20110;&#20808;&#21069;&#35299;&#20915;&#30340;&#23376;&#38382;&#39064;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#22312;&#31526;&#21495;&#25805;&#20316;&#12289;&#32452;&#21512;&#25512;&#29702;&#21644;&#25968;&#23398;&#25512;&#29702;&#30456;&#20851;&#30340;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#33021;&#22815;&#25512;&#24191;&#21040;&#27604;&#25552;&#31034;&#20013;&#26356;&#38590;&#30340;&#38382;&#39064;&#12290;&#19968;&#20010;&#20540;&#24471;&#27880;&#24847;&#30340;&#21457;&#29616;&#26159;&#65292;&#22312;&#20351;&#29992;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#19982;GPT-3 code-davinci-002&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#21487;&#20197;&#20197;&#23436;&#32654;&#30340;&#20934;&#30830;&#24615;&#35299;&#20915;&#32452;&#21512;&#27867;&#21270;&#22522;&#20934;SCAN&#20013;&#30340;&#20219;&#20309;&#20998;&#21106;(&#21253;&#25324;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38646;&#26679;&#26412;&#20998;&#21106;)&#65292;&#23613;&#31649;&#20043;&#21069;&#26080;&#27861;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#20219;&#20309;&#20998;&#21106;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;P3M-10k&#65292;&#36825;&#26159;&#39318;&#20010;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#32918;&#20687;&#25248;&#22270;&#30340;&#22823;&#35268;&#27169;&#21311;&#21517;&#22522;&#20934;&#27979;&#35797;&#12290;&#21516;&#26102;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#32479;&#19968;&#30340;&#25248;&#22270;&#27169;&#22411;P3M-Net&#21644;&#26377;&#25928;&#30340;&#36328;&#22495;&#24615;&#33021;&#25552;&#21319;&#31574;&#30053;P3M-CP&#12290;</title><link>http://arxiv.org/abs/2203.16828</link><description>&lt;p&gt;
&#20445;&#25252;&#38544;&#31169;&#30340;&#32918;&#20687;&#25248;&#22270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Rethinking Portrait Matting with Privacy Preserving. (arXiv:2203.16828v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.16828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;P3M-10k&#65292;&#36825;&#26159;&#39318;&#20010;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#32918;&#20687;&#25248;&#22270;&#30340;&#22823;&#35268;&#27169;&#21311;&#21517;&#22522;&#20934;&#27979;&#35797;&#12290;&#21516;&#26102;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#32479;&#19968;&#30340;&#25248;&#22270;&#27169;&#22411;P3M-Net&#21644;&#26377;&#25928;&#30340;&#36328;&#22495;&#24615;&#33021;&#25552;&#21319;&#31574;&#30053;P3M-CP&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#20010;&#20154;&#20449;&#24687;&#30340;&#35782;&#21035;&#38382;&#39064;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#20154;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#32918;&#20687;&#25248;&#22270;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;&#21487;&#35782;&#21035;&#30340;&#22270;&#20687;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;P3M-10k&#65292;&#36825;&#26159;&#39318;&#20010;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#32918;&#20687;&#25248;&#22270;(P3M)&#30340;&#22823;&#35268;&#27169;&#21311;&#21517;&#22522;&#20934;&#27979;&#35797;&#12290;P3M-10k&#21253;&#25324;10,421&#24352;&#39640;&#20998;&#36776;&#29575;&#27169;&#31946;&#30340;&#20154;&#20687;&#22270;&#20687;&#20197;&#21450;&#39640;&#36136;&#37327;&#30340;alpha&#25248;&#22270;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#31995;&#32479;&#22320;&#35780;&#20272;&#22522;&#20110;Trimap&#21644;&#38750;Trimap&#30340;&#25248;&#22270;&#26041;&#27861;&#65292;&#24182;&#22312;&#38544;&#31169;&#20445;&#25252;&#35757;&#32451;&#65288;PPT&#65289;&#29615;&#22659;&#19979;&#33719;&#24471;&#26377;&#29992;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#25248;&#22270;&#27169;&#22411;P3M-Net&#65292;&#23427;&#20860;&#23481;CNN&#21644;transformer&#39592;&#24178;&#32593;&#32476;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20943;&#36731;&#22312;PPT&#35774;&#32622;&#19979;&#36328;&#22495;&#24615;&#33021;&#24046;&#36317;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;Copy and Paste&#31574;&#30053;(P3M-CP)&#65292;&#23427;&#20511;&#37492;&#20102;&#20844;&#20849;&#21517;&#20154;&#22270;&#20687;&#30340;&#38754;&#37096;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been an increasing concern about the privacy issue raised by identifiable information in machine learning. However, previous portrait matting methods were all based on identifiable images. To fill the gap, we present P3M-10k, which is the first large-scale anonymized benchmark for Privacy-Preserving Portrait Matting (P3M). P3M-10k consists of 10,421 high resolution face-blurred portrait images along with high-quality alpha mattes, which enables us to systematically evaluate both trimap-free and trimap-based matting methods and obtain some useful findings about model generalization ability under the privacy preserving training (PPT) setting. We also present a unified matting model dubbed P3M-Net that is compatible with both CNN and transformer backbones. To further mitigate the cross-domain performance gap issue under the PPT setting, we devise a simple yet effective Copy and Paste strategy (P3M-CP), which borrows facial information from public celebrity images and d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;von Neumann&#29109;&#30340;&#26032;&#25351;&#26631;&#26469;&#37325;&#26032;&#23457;&#35270;GNNs&#30340;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#24182;&#20174;&#25972;&#20010;&#37051;&#23621;&#21487;&#35782;&#21035;&#30340;&#35282;&#24230;&#30740;&#31350;&#36328;&#31867;&#36793;&#30340;&#29305;&#24449;&#32858;&#21512;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;Conv-Agnostic GNN&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#27599;&#20010;&#33410;&#28857;&#30340;&#37051;&#23621;&#25928;&#24212;&#65292;&#22312;&#24322;&#36136;&#24615;&#25968;&#25454;&#38598;&#19978;&#22686;&#24378;&#20102;&#22823;&#22810;&#25968;GNN&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2203.11200</link><description>&lt;p&gt;
&#21033;&#29992;&#37051;&#23621;&#25928;&#24212;&#65306;&#36866;&#29992;&#20110;&#24322;&#36136;&#24615;&#22270;&#30340;Conv-Agnostic GNNs&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploiting Neighbor Effect: Conv-Agnostic GNNs Framework for Graphs with Heterophily. (arXiv:2203.11200v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.11200
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;von Neumann&#29109;&#30340;&#26032;&#25351;&#26631;&#26469;&#37325;&#26032;&#23457;&#35270;GNNs&#30340;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#24182;&#20174;&#25972;&#20010;&#37051;&#23621;&#21487;&#35782;&#21035;&#30340;&#35282;&#24230;&#30740;&#31350;&#36328;&#31867;&#36793;&#30340;&#29305;&#24449;&#32858;&#21512;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;Conv-Agnostic GNN&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#27599;&#20010;&#33410;&#28857;&#30340;&#37051;&#23621;&#25928;&#24212;&#65292;&#22312;&#24322;&#36136;&#24615;&#25968;&#25454;&#38598;&#19978;&#22686;&#24378;&#20102;&#22823;&#22810;&#25968;GNN&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GNNs&#65289;&#20013;&#21516;&#36136;&#24615;&#20551;&#35774;&#30340;&#23384;&#22312;&#65292;&#22270;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#19968;&#20010;&#20849;&#35782;&#26159;GNNs&#22312;&#21516;&#36136;&#24615;&#22270;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#20855;&#26377;&#35768;&#22810;&#36328;&#31867;&#36793;&#30340;&#24322;&#36136;&#24615;&#22270;&#19978;&#21487;&#33021;&#20250;&#22833;&#36133;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#36328;&#31867;&#36793;&#35266;&#28857;&#21644;&#30456;&#20851;&#30340;&#21516;&#36136;&#27604;&#25351;&#26631;&#19981;&#33021;&#24456;&#22909;&#22320;&#35299;&#37322;GNNs&#22312;&#26576;&#20123;&#24322;&#36136;&#24615;&#25968;&#25454;&#38598;&#19979;&#30340;&#34920;&#29616;&#65292;&#36825;&#24847;&#21619;&#30528;&#24182;&#38750;&#25152;&#26377;&#36328;&#31867;&#36793;&#23545;GNNs&#37117;&#26159;&#26377;&#23475;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;von Neumann&#29109;&#30340;&#26032;&#25351;&#26631;&#65292;&#37325;&#26032;&#23457;&#35270;&#20102;GNNs&#30340;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#24182;&#20174;&#25972;&#20010;&#37051;&#23621;&#21487;&#35782;&#21035;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#36328;&#31867;&#36793;&#30340;&#29305;&#24449;&#32858;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;Conv-Agnostic GNN&#26694;&#26550;&#65288;CAGNNs&#65289;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#33410;&#28857;&#23398;&#20064;&#37051;&#23621;&#25928;&#24212;&#65292;&#22312;&#24322;&#36136;&#24615;&#25968;&#25454;&#38598;&#19978;&#22686;&#24378;&#20102;&#22823;&#22810;&#25968;GNN&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#27599;&#20010;&#33410;&#28857;&#30340;&#29305;&#24449;&#20998;&#35299;&#20026;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#21028;&#21035;&#29305;&#24449;&#21644;&#37051;&#23621;&#29305;&#24449;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#37051;&#23621;&#29305;&#24449;&#32858;&#21512;&#27169;&#22359;&#65292;&#22312;CAGNNs&#26694;&#26550;&#20013;&#33258;&#36866;&#24212;&#22320;&#32858;&#21512;&#37051;&#23621;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the homophily assumption in graph convolution networks (GNNs), a common consensus in the graph node classification task is that GNNs perform well on homophilic graphs but may fail on heterophilic graphs with many inter-class edges. However, the previous inter-class edges perspective and related homo-ratio metrics cannot well explain the GNNs performance under some heterophilic datasets, which implies that not all the inter-class edges are harmful to GNNs. In this work, we propose a new metric based on von Neumann entropy to re-examine the heterophily problem of GNNs and investigate the feature aggregation of inter-class edges from an entire neighbor identifiable perspective. Moreover, we propose a simple yet effective Conv-Agnostic GNN framework (CAGNNs) to enhance the performance of most GNNs on heterophily datasets by learning the neighbor effect for each node. Specifically, we first decouple the feature of each node into the discriminative feature for downstream tasks and the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#23454;&#20363;&#21644;&#30693;&#35782;&#23545;&#40784;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;ABSA&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#39046;&#22495;&#20559;&#31227;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#30340;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2110.13398</link><description>&lt;p&gt;
&#32479;&#19968;&#23454;&#20363;&#21644;&#30693;&#35782;&#23545;&#40784;&#39044;&#35757;&#32451;&#29992;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Unified Instance and Knowledge Alignment Pretraining for Aspect-based Sentiment Analysis. (arXiv:2110.13398v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.13398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#23454;&#20363;&#21644;&#30693;&#35782;&#23545;&#40784;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;ABSA&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#39046;&#22495;&#20559;&#31227;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#30340;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#26088;&#22312;&#30830;&#23450;&#23545;&#26576;&#20010;&#26041;&#38754;&#30340;&#24773;&#24863;&#20542;&#21521;&#12290;&#30001;&#20110;&#26114;&#36149;&#19988;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#39044;&#35757;&#32451;&#31574;&#30053;&#24050;&#25104;&#20026;ABSA&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;ABSA&#25968;&#25454;&#38598;&#20043;&#38388;&#24635;&#26159;&#23384;&#22312;&#20005;&#37325;&#30340;&#39046;&#22495;&#20559;&#31227;&#65292;&#30452;&#25509;&#24494;&#35843;&#26102;&#30340;&#30693;&#35782;&#36716;&#31227;&#25928;&#26524;&#19981;&#20339;&#65292;&#23548;&#33268;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#20122;&#20248;&#21270;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#39046;&#22495;&#20559;&#31227;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#23545;&#40784;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#21253;&#25324;&#23454;&#20363;&#21644;&#30693;&#35782;&#23618;&#38754;&#30340;&#23545;&#40784;&#65292;&#23558;&#20854;&#34701;&#20837;&#21040;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#27969;&#31243;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#38454;&#27573;&#26816;&#32034;&#37319;&#26679;&#26041;&#27861;&#65292;&#20174;&#22823;&#35268;&#27169;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#19982;&#30446;&#26631;&#39046;&#22495;&#30456;&#20851;&#30340;&#23454;&#20363;&#65292;&#20174;&#32780;&#23454;&#29616;&#39044;&#35757;&#32451;&#21644;&#30446;&#26631;&#39046;&#22495;&#23454;&#20363;&#30340;&#23545;&#40784;&#65288;&#31532;&#19968;&#38454;&#27573;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#30693;&#35782;&#25351;&#23548;&#30340;&#31574;&#30053;&#65292;&#36827;&#19968;&#27493;&#26725;&#25509;&#30693;&#35782;&#23618;&#38754;&#30340;&#39046;&#22495;&#24046;&#24322;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;ABSA&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#36229;&#36234;&#24378;&#22522;&#32447;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;ABSA&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;SemEval 2014&#20219;&#21153;4&#12289;SemEval 2015&#20219;&#21153;12&#21644;SemEval 2016&#20219;&#21153;5&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect-based Sentiment Analysis (ABSA) aims to determine the sentiment polarity towards an aspect. Because of the expensive and limited labelled data, the pretraining strategy has become the de-facto standard for ABSA. However, there always exists severe domain shift between the pretraining and downstream ABSA datasets, hindering the effective knowledge transfer when directly finetuning and making the downstream task performs sub-optimal. To mitigate such domain shift, we introduce a unified alignment pretraining framework into the vanilla pretrain-finetune pipeline with both instance- and knowledge-level alignments. Specifically, we first devise a novel coarse-to-fine retrieval sampling approach to select target domain-related instances from the large-scale pretraining dataset, thus aligning the instances between pretraining and target domains (First Stage). Then, we introduce a knowledge guidance-based strategy to further bridge the domain gap at the knowledge level. In practice, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#20135;&#29983;&#31070;&#32463;&#20803;&#27169;&#22411;&#8212;&#8212;&#36229;&#32423;&#31070;&#32463;&#20803;&#65292;&#21487;&#20197;&#20811;&#26381;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#30340;&#26680;&#23450;&#20301;&#38480;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#38543;&#26426;&#25110;&#21487;&#23398;&#20064;&#30340;&#26680;&#31227;&#20301;&#65292;&#22686;&#21152;&#27599;&#20010;&#36830;&#25509;&#30340;&#25509;&#21463;&#37326;&#22823;&#23567;&#65292;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#21516;&#26102;&#38477;&#20302;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2109.01594</link><description>&lt;p&gt;
&#36229;&#32423;&#31070;&#32463;&#20803;
&lt;/p&gt;
&lt;p&gt;
Super Neurons. (arXiv:2109.01594v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.01594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#20135;&#29983;&#31070;&#32463;&#20803;&#27169;&#22411;&#8212;&#8212;&#36229;&#32423;&#31070;&#32463;&#20803;&#65292;&#21487;&#20197;&#20811;&#26381;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#30340;&#26680;&#23450;&#20301;&#38480;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#38543;&#26426;&#25110;&#21487;&#23398;&#20064;&#30340;&#26680;&#31227;&#20301;&#65292;&#22686;&#21152;&#27599;&#20010;&#36830;&#25509;&#30340;&#25509;&#21463;&#37326;&#22823;&#23567;&#65292;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#21516;&#26102;&#38477;&#20302;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#33258;&#32452;&#32455;&#25805;&#20316;&#31070;&#32463;&#32593;&#32476;(Self-Organized Operational Neural Networks,Self-ONNs)&#30340;&#26032;&#19968;&#20195;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;,&#35813;&#27169;&#22411;&#20855;&#26377;&#38750;&#32447;&#24615;&#23398;&#20064;&#21333;&#20803;&#8212;&#8212;&#20135;&#29983;&#31070;&#32463;&#20803;,&#21487;&#20197;&#25552;&#20379;&#19968;&#31181;&#20248;&#38597;&#30340;&#22810;&#26679;&#24615;&#23618;&#27425;&#12290;&#20294;&#26159;,&#20687;&#20256;&#32479;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#19968;&#26679;,Self-ONNs&#20173;&#28982;&#20855;&#26377;&#19968;&#20010;&#26222;&#36941;&#30340;&#32570;&#38519;:&#26412;&#22320;&#21270;&#65288;&#22266;&#23450;&#65289;&#26680;&#25805;&#20316;&#12290;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#19981;&#21516;&#23618;&#20043;&#38388;&#30340;&#25509;&#21463;&#37326;&#21644;&#20449;&#24687;&#27969;,&#22240;&#27492;&#38656;&#35201;&#28145;&#24230;&#21644;&#22797;&#26434;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#36234;&#30340;(&#20135;&#29983;)&#31070;&#32463;&#20803;&#27169;&#22411;,&#25110;&#32773;&#31616;&#31216;&#20026;&#36229;&#32423;&#31070;&#32463;&#20803;,&#23427;&#20801;&#35768;&#38543;&#26426;&#25110;&#21487;&#23398;&#20064;&#30340;&#26680;&#31227;&#20301;,&#20174;&#32780;&#21487;&#20197;&#22686;&#21152;&#27599;&#20010;&#36830;&#25509;&#30340;&#25509;&#21463;&#37326;&#22823;&#23567;&#12290;&#20351;&#29992;&#36825;&#20123;&#36229;&#32423;&#31070;&#32463;&#20803;&#21487;&#20197;&#20811;&#26381;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#30340;&#26680;&#23450;&#20301;&#38480;&#21046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#24182;&#20943;&#23569;&#20102;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-Organized Operational Neural Networks (Self-ONNs) have recently been proposed as new-generation neural network models with nonlinear learning units, i.e., the generative neurons that yield an elegant level of diversity; however, like its predecessor, conventional Convolutional Neural Networks (CNNs), they still have a common drawback: localized (fixed) kernel operations. This severely limits the receptive field and information flow between layers and thus brings the necessity for deep and complex models. It is highly desired to improve the receptive field size without increasing the kernel dimensions. This requires a significant upgrade over the generative neurons to achieve the non-localized kernel operations for each connection between consecutive layers. In this article, we present superior (generative) neuron models (or super neurons in short) that allow random or learnable kernel shifts and thus can increase the receptive field size of each connection. The kernel localization
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24773;&#22659;&#30340;&#26465;&#20214;&#35821;&#21477;&#24418;&#24335;&#65292;&#20854;&#27604;&#32463;&#20856;&#26465;&#20214;&#35821;&#21477;&#34920;&#29616;&#21147;&#26356;&#24378;&#65292;&#33021;&#21306;&#20998;&#26399;&#26395;&#21644;&#34394;&#25311;&#35821;&#27668;&#65292;&#24182;&#34987;&#35777;&#26126;&#21487;&#20197;&#29992;&#19968;&#32452;&#21512;&#29702;&#24615;&#20551;&#35774;&#36827;&#34892;&#25551;&#36848;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#20026;&#24773;&#22659;&#26465;&#20214;&#30693;&#35782;&#24211;&#23450;&#20041;&#20102;&#19968;&#31181;&#26368;&#23567;&#38381;&#21253;&#30340;&#21253;&#21547;&#24418;&#24335;&#12290;</title><link>http://arxiv.org/abs/2109.01552</link><description>&lt;p&gt;
&#24773;&#22659;&#26465;&#20214;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Situated Conditional Reasoning. (arXiv:2109.01552v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.01552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24773;&#22659;&#30340;&#26465;&#20214;&#35821;&#21477;&#24418;&#24335;&#65292;&#20854;&#27604;&#32463;&#20856;&#26465;&#20214;&#35821;&#21477;&#34920;&#29616;&#21147;&#26356;&#24378;&#65292;&#33021;&#21306;&#20998;&#26399;&#26395;&#21644;&#34394;&#25311;&#35821;&#27668;&#65292;&#24182;&#34987;&#35777;&#26126;&#21487;&#20197;&#29992;&#19968;&#32452;&#21512;&#29702;&#24615;&#20551;&#35774;&#36827;&#34892;&#25551;&#36848;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#20026;&#24773;&#22659;&#26465;&#20214;&#30693;&#35782;&#24211;&#23450;&#20041;&#20102;&#19968;&#31181;&#26368;&#23567;&#38381;&#21253;&#30340;&#21253;&#21547;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#35821;&#21477;&#22312;&#24314;&#27169;&#20013;&#24456;&#26377;&#29992;&#65292;&#20294;&#19981;&#33021;&#24635;&#26159;&#20934;&#30830;&#22320;&#25429;&#25417;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24773;&#22659;&#30340;&#26465;&#20214;&#35821;&#21477;&#24418;&#24335;&#12290;&#36825;&#20123;&#26465;&#20214;&#35821;&#21477;&#27604;&#32463;&#20856;&#26465;&#20214;&#35821;&#21477;&#26356;&#20855;&#34920;&#29616;&#21147;&#65292;&#36275;&#20197;&#22312;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#20013;&#20351;&#29992;&#65292;&#24182;&#33021;&#22815;&#21306;&#20998;&#26399;&#26395;&#21644;&#34394;&#25311;&#35821;&#27668;&#12290;&#24418;&#24335;&#19978;&#65292;&#23427;&#20204;&#34987;&#35777;&#26126;&#26159;Kraus&#12289;Lehmann&#21644;Magidor&#39118;&#26684;&#30340;&#26465;&#20214;&#35774;&#23450;&#30340;&#25512;&#24191;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#24773;&#22659;&#26465;&#20214;&#21487;&#20197;&#29992;&#19968;&#32452;&#21512;&#29702;&#24615;&#20551;&#35774;&#36827;&#34892;&#25551;&#36848;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#20123;&#26465;&#20214;&#35821;&#21477;&#30340;&#30452;&#35266;&#35821;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#34920;&#31034;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#35821;&#20041;&#26500;&#36896;&#19982;&#20551;&#35774;&#25551;&#36848;&#23436;&#20840;&#19968;&#33268;&#12290;&#26377;&#20102;&#35821;&#20041;&#20043;&#21518;&#65292;&#25105;&#20204;&#20026;&#24773;&#22659;&#26465;&#20214;&#30693;&#35782;&#24211;&#23450;&#20041;&#20102;&#19968;&#31181;&#21253;&#21547;&#24418;&#24335;&#65292;&#31216;&#20026;&#26368;&#23567;&#38381;&#21253;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conditionals are useful for modelling, but are not always sufficiently expressive for capturing information accurately. In this paper we make the case for a form of conditional that is situation-based. These conditionals are more expressive than classical conditionals, are general enough to be used in several application domains, and are able to distinguish, for example, between expectations and counterfactuals. Formally, they are shown to generalise the conditional setting in the style of Kraus, Lehmann, and Magidor. We show that situation-based conditionals can be described in terms of a set of rationality postulates. We then propose an intuitive semantics for these conditionals, and present a representation result which shows that our semantic construction corresponds exactly to the description in terms of postulates. With the semantics in place, we proceed to define a form of entailment for situated conditional knowledge bases, which we refer to as minimal closure. It is reminiscen
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#32676;&#20307;&#21327;&#21830;&#25968;&#25454;&#38598;&#65292;500&#20010;&#23567;&#32452;&#23545;&#35805;&#21644;14k&#20010;&#35805;&#35821;&#65292;64%&#30340;&#23567;&#32452;&#25104;&#21592;&#33021;&#22815;&#25214;&#21040;&#27604;&#20182;&#20204;&#21333;&#29420;&#25214;&#21040;&#30340;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#37322;&#27169;&#24335;&#29992;&#20110;&#25429;&#25417;&#21327;&#21830;&#32447;&#32034;&#65292;&#24182;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#20004;&#31181;&#29983;&#25104;&#21327;&#21830;&#35805;&#35821;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2108.05271</link><description>&lt;p&gt;
DeliData: &#29992;&#20110;&#22810;&#26041;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#21327;&#21830;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DeliData: A dataset for deliberation in multi-party problem solving. (arXiv:2108.05271v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.05271
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#32676;&#20307;&#21327;&#21830;&#25968;&#25454;&#38598;&#65292;500&#20010;&#23567;&#32452;&#23545;&#35805;&#21644;14k&#20010;&#35805;&#35821;&#65292;64%&#30340;&#23567;&#32452;&#25104;&#21592;&#33021;&#22815;&#25214;&#21040;&#27604;&#20182;&#20204;&#21333;&#29420;&#25214;&#21040;&#30340;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#37322;&#27169;&#24335;&#29992;&#20110;&#25429;&#25417;&#21327;&#21830;&#32447;&#32034;&#65292;&#24182;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#20004;&#31181;&#29983;&#25104;&#21327;&#21830;&#35805;&#35821;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32676;&#20307;&#21327;&#21830;&#20351;&#20154;&#20204;&#33021;&#22815;&#21327;&#20316;&#35299;&#20915;&#38382;&#39064;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#36164;&#28304;&#65292;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#23578;&#19981;&#23436;&#21892;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#35299;&#20915;&#19968;&#20010;&#24050;&#32463;&#30830;&#31435;&#30340;&#35748;&#30693;&#20219;&#21153;&#30340;&#21327;&#20316;&#23545;&#35805;&#65292;&#21253;&#25324;500&#20010;&#23567;&#32452;&#23545;&#35805;&#21644;14k&#20010;&#35805;&#35821;&#12290;&#22312;&#36825;&#20123;&#23545;&#35805;&#20013;&#65292;64&#65285;&#30340;&#23567;&#32452;&#25104;&#21592;&#33021;&#22815;&#25214;&#21040;&#27604;&#20182;&#20204;&#21333;&#29420;&#25214;&#21040;&#30340;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#26368;&#32456;&#35299;&#20915;&#26041;&#26696;&#20026;&#27491;&#30830;&#31572;&#26696;&#30340;&#23567;&#32452;&#20013;&#65292;&#26377;43.8&#65285;&#30340;&#23567;&#32452;&#20013;&#27809;&#26377;&#20219;&#20309;&#21442;&#19982;&#32773;&#33258;&#24049;&#23601;&#33021;&#27491;&#30830;&#35299;&#20915;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#37322;&#27169;&#24335;&#65292;&#25429;&#25417;&#21327;&#21830;&#32447;&#32034;&#65292;&#24182;&#20844;&#24320;&#20102;&#25152;&#26377;14k&#20010;&#27880;&#37322;&#36807;&#30340;&#35805;&#35821;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#24320;&#21457;&#21644;&#35780;&#20272;&#20102;&#20004;&#31181;&#29983;&#25104;&#21327;&#21830;&#35805;&#35821;&#30340;&#26041;&#27861;&#12290;&#25968;&#25454;&#25910;&#38598;&#24179;&#21488;&#12289;&#25968;&#25454;&#38598;&#21644;&#27880;&#37322;&#35821;&#26009;&#24211;&#21487;&#22312;https://delibot.xyz&#19978;&#20844;&#24320;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Group deliberation enables people to collaborate and solve problems, however, it is understudied due to a lack of resources. To this end, we introduce the first publicly available dataset containing collaborative conversations on solving a well-established cognitive task, consisting of 500 group dialogues and 14k utterances. In 64% of these conversations, the group members are able to find a better solution than they had identified individually, and in 43.8% of the groups who had a correct answer as their final solution, none of the participants had solved the task correctly by themselves. Furthermore, we propose a novel annotation schema that captures deliberation cues and release all 14k utterances annotated with it. Finally, we use the proposed dataset to develop and evaluate two methods for generating deliberation utterances. The data collection platform, dataset and annotated corpus are publicly available at https://delibot.xyz.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;AdvSim&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;&#25239;&#24615;&#26041;&#27861;&#29983;&#25104;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#65292;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#30340;&#33258;&#20027;&#31995;&#32479;&#65292;&#21487;&#20197;&#35782;&#21035;&#21508;&#31181;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2101.06549</link><description>&lt;p&gt;
AdvSim: &#29983;&#25104;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;
&lt;/p&gt;
&lt;p&gt;
AdvSim: Generating Safety-Critical Scenarios for Self-Driving Vehicles. (arXiv:2101.06549v4 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.06549
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;AdvSim&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;&#25239;&#24615;&#26041;&#27861;&#29983;&#25104;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#65292;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#30340;&#33258;&#20027;&#31995;&#32479;&#65292;&#21487;&#20197;&#35782;&#21035;&#21508;&#31181;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#19981;&#26029;&#25552;&#21319;&#65292;&#27169;&#25311;&#21487;&#33021;&#20986;&#29616;&#33258;&#20027;&#22534;&#26632;&#22833;&#36133;&#30340;&#24773;&#20917;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20123;&#22330;&#26223;&#21482;&#20026;&#35268;&#21010;&#27169;&#22359;&#29983;&#25104;&#20102;&#19968;&#20123;&#30456;&#23545;&#36739;&#23569;&#30340;&#22330;&#26223;&#65292;&#20854;&#36755;&#20837;&#20026;&#22320;&#38754;&#30495;&#23454;&#36710;&#36742;&#29366;&#24577;&#12290;&#36825;&#19981;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#19988;&#26080;&#27861;&#35782;&#21035;&#25152;&#26377;&#21487;&#33021;&#30340;&#33258;&#20027;&#22833;&#36133;&#65292;&#20363;&#22914;&#30001;&#20110;&#36974;&#25377;&#23548;&#33268;&#30340;&#24863;&#30693;&#25925;&#38556;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;AdvSim&#65292;&#19968;&#20010;&#23545;&#20110;&#20219;&#20309;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#30340;&#33258;&#20027;&#31995;&#32479;&#29983;&#25104;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#30340;&#23545;&#25239;&#24615;&#26694;&#26550;&#12290;&#32473;&#23450;&#19968;&#20010;&#21021;&#22987;&#20132;&#36890;&#22330;&#26223;&#65292;AdvSim&#20197;&#29289;&#29702;&#21487;&#34892;&#30340;&#26041;&#24335;&#20462;&#25913;&#21442;&#19982;&#32773;&#30340;&#36712;&#36857;&#24182;&#26356;&#26032;&#28608;&#20809;&#38647;&#36798;&#20256;&#24863;&#22120;&#25968;&#25454;&#20197;&#21305;&#37197;&#21463;&#25200;&#21160;&#30340;&#19990;&#30028;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36890;&#36807;&#30452;&#25509;&#20174;&#20256;&#24863;&#22120;&#25968;&#25454;&#36827;&#34892;&#27169;&#25311;&#65292;&#25105;&#20204;&#33719;&#24471;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#65292;&#36866;&#29992;&#20110;&#23436;&#25972;&#30340;&#33258;&#20027;&#22534;&#26632;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26222;&#36866;&#24615;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#22823;&#37327;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#65292;&#36866;&#29992;&#20110;&#29616;&#20195;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#21508;&#31181;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
As self-driving systems become better, simulating scenarios where the autonomy stack may fail becomes more important. Traditionally, those scenarios are generated for a few scenes with respect to the planning module that takes ground-truth actor states as input. This does not scale and cannot identify all possible autonomy failures, such as perception failures due to occlusion. In this paper, we propose AdvSim, an adversarial framework to generate safety-critical scenarios for any LiDAR-based autonomy system. Given an initial traffic scenario, AdvSim modifies the actors' trajectories in a physically plausible manner and updates the LiDAR sensor data to match the perturbed world. Importantly, by simulating directly from sensor data, we obtain adversarial scenarios that are safety-critical for the full autonomy stack. Our experiments show that our approach is general and can identify thousands of semantically meaningful safety-critical scenarios for a wide range of modern self-driving sy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#24403;&#36816;&#34892;&#26102;&#38388;&#20026;&#26080;&#31351;&#22823;&#26102;&#65292;SEMO&#26080;&#27861;&#25214;&#21040;&#25152;&#26377;Pareto&#21069;&#27839;&#12290;&#20294;&#20840;&#23616;SEMO&#35777;&#26126;&#20102;&#22312;&#26399;&#26395;&#36845;&#20195;&#27425;&#25968;&#19978;&#38480;&#20869;&#25214;&#21040;&#20102;&#25152;&#26377;Pareto&#21069;&#27839;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#22312;&#22810;&#23792;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#21442;&#32771;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2012.07231</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#22312;&#22810;&#23792;&#30446;&#26631;&#19978;&#30340;&#29702;&#35770;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Theoretical Analyses of Multiobjective Evolutionary Algorithms on Multimodal Objectives. (arXiv:2012.07231v4 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.07231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#24403;&#36816;&#34892;&#26102;&#38388;&#20026;&#26080;&#31351;&#22823;&#26102;&#65292;SEMO&#26080;&#27861;&#25214;&#21040;&#25152;&#26377;Pareto&#21069;&#27839;&#12290;&#20294;&#20840;&#23616;SEMO&#35777;&#26126;&#20102;&#22312;&#26399;&#26395;&#36845;&#20195;&#27425;&#25968;&#19978;&#38480;&#20869;&#25214;&#21040;&#20102;&#25152;&#26377;Pareto&#21069;&#27839;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#22312;&#22810;&#23792;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#21442;&#32771;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#65288;MOEA&#65289;&#22312;&#23454;&#36341;&#20013;&#30340;&#25104;&#21151;&#36828;&#36828;&#36229;&#36807;&#20102;&#29702;&#35770;&#30740;&#31350;&#30340;&#36827;&#23637;&#12290;&#29305;&#21035;&#26159;&#65292;&#20197;&#21069;&#30340;&#29702;&#35770;&#24037;&#20316;&#20027;&#35201;&#32771;&#34385;&#30001;&#21333;&#23792;&#30446;&#26631;&#32452;&#25104;&#30340;&#31616;&#21333;&#38382;&#39064;&#12290;&#20026;&#20102;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#36827;&#21270;&#31639;&#27861;&#22914;&#20309;&#35299;&#20915;&#22810;&#23792;&#22810;&#30446;&#26631;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;OJZJ&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#20004;&#20010;&#19982;&#32463;&#20856;&#36339;&#36291;&#20989;&#25968;&#22522;&#20934;&#21516;&#26500;&#30340;&#30446;&#26631;&#32452;&#25104;&#30340;&#21452;&#30446;&#26631;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;SEMO&#65292;&#26080;&#35770;&#36816;&#34892;&#26102;&#38388;&#22914;&#20309;&#65292;&#27010;&#29575;&#37117;&#19981;&#21487;&#33021;&#35745;&#31639;&#20986;&#23436;&#25972;&#30340; Pareto &#21069;&#27839;&#12290;&#30456;&#21453;&#65292;&#23545;&#20110;&#25152;&#26377;&#38382;&#39064;&#22823;&#23567;n&#21644;&#25152;&#26377;&#36339;&#36291;&#22823;&#23567;k&#8712;[ 4 . . n 2 &#8722;1 ]&#65292;&#20840;&#23616;SEMO&#65288;GSEMO&#65289;&#22312; &#920; ((n&#8722;2k)n^k&#65289;&#27425;&#36845;&#20195;&#20013;&#35206;&#30422; Pareto &#21069;&#27839;&#39044;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;
The theoretical understanding of MOEAs is lagging far behind their success in practice. In particular, previous theory work considers mostly easy problems that are composed of unimodal objectives.  As a first step towards a deeper understanding of how evolutionary algorithms solve multimodal multiobjective problems, we propose the OJZJ problem, a bi-objective problem composed of two objectives isomorphic to the classic jump function benchmark. We prove that SEMO with probability one does not compute the full Pareto front, regardless of the runtime. In contrast, for all problem sizes $n$ and all jump sizes ${k \in [4..\frac n2 - 1]}$, the global SEMO (GSEMO) covers the Pareto front in an expected number of $\Theta((n-2k)n^{k})$ iterations. For $k = o(n)$, we also show the tighter bound $\frac 32 e n^{k+1} \pm o(n^{k+1})$, which might be the first runtime bound for an MOEA that is tight apart from lower-order terms. We also combine the GSEMO with two approaches that showed advantages in 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#35328;&#22522;&#30784;&#23454;&#29616;&#38646;&#26679;&#26412;&#32452;&#21512;&#31574;&#30053;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#19982;&#31574;&#30053;&#32593;&#32476;&#32467;&#21512;&#65292;&#20351;&#31574;&#30053;&#32593;&#32476;&#33021;&#22815;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#23646;&#24615;&#32452;&#21512;&#19978;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#22312;&#38646;&#26679;&#26412;&#32452;&#21512;&#31574;&#30053;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;RL/IL&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2004.07200</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#22522;&#30784;&#23454;&#29616;&#38646;&#26679;&#26412;&#32452;&#21512;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Compositional Policy Learning via Language Grounding. (arXiv:2004.07200v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2004.07200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#35328;&#22522;&#30784;&#23454;&#29616;&#38646;&#26679;&#26412;&#32452;&#21512;&#31574;&#30053;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#19982;&#31574;&#30053;&#32593;&#32476;&#32467;&#21512;&#65292;&#20351;&#31574;&#30053;&#32593;&#32476;&#33021;&#22815;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#23646;&#24615;&#32452;&#21512;&#19978;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#22312;&#38646;&#26679;&#26412;&#32452;&#21512;&#31574;&#30053;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;RL/IL&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21644;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#22312;&#26368;&#36817;&#37117;&#26377;&#20102;&#31361;&#30772;&#65292;&#20294;&#29616;&#26377;&#31639;&#27861;&#26080;&#27861;&#22312;&#35757;&#32451;&#29615;&#22659;&#20043;&#22806;&#36827;&#34892;&#25512;&#24191;&#12290;&#23454;&#38469;&#19978;&#65292;&#20154;&#31867;&#33021;&#22815;&#36890;&#36807;&#21033;&#29992;&#20808;&#21069;&#20851;&#20110;&#19990;&#30028;&#65288;&#22914;&#35821;&#35328;&#25551;&#36848;&#65289;&#30340;&#30693;&#35782;&#26469;&#24555;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#20026;&#20102;&#20419;&#36827;&#24102;&#26377;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#35821;&#35328;&#24341;&#23548;&#20195;&#29702;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#30340;&#38646;&#26679;&#26412;&#32452;&#21512;&#31574;&#30053;&#23398;&#20064;&#20219;&#21153;&#65292;&#20854;&#20013;&#29615;&#22659;&#34987;&#25551;&#36848;&#20026;&#19981;&#21516;&#23646;&#24615;&#30340;&#32452;&#21512;&#12290;&#30001;&#20110;&#27809;&#26377;&#20844;&#20849;&#29615;&#22659;&#25903;&#25345;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#30740;&#31350;&#24179;&#21488;BabyAI++&#65292;&#20854;&#20013;&#29615;&#22659;&#30340;&#21160;&#21147;&#23398;&#19982;&#35270;&#35273;&#22806;&#35266;&#35299;&#32806;&#12290;&#22312;&#27599;&#20010;&#22238;&#21512;&#20013;&#65292;BabyAI++&#25552;&#20379;&#20102;&#21508;&#31181;&#35270;&#35273;&#21160;&#21147;&#23398;&#32452;&#21512;&#20197;&#21450;&#30456;&#24212;&#30340;&#25551;&#36848;&#24615;&#25991;&#26412;&#12290;&#20026;&#20102;&#35780;&#20272;&#25152;&#23398;&#20195;&#29702;&#30340;&#33258;&#36866;&#24212;&#33021;&#21147;&#65292;&#19968;&#32452;&#35270;&#35273;&#21160;&#21147;&#23398;&#37197;&#23545;&#34987;&#20445;&#30041;&#22312;BabyAI++&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#19981;&#20986;&#25152;&#26009;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#35821;&#35328;&#24341;&#23548;RL/IL&#26041;&#27861;&#26080;&#27861;&#35299;&#20915;&#36825;&#20010;&#38646;&#26679;&#26412;&#32452;&#21512;&#31574;&#30053;&#23398;&#20064;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#24341;&#23548;&#31574;&#30053;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#19982;&#31574;&#30053;&#32593;&#32476;&#32467;&#21512;&#65292;&#20351;&#31574;&#30053;&#32593;&#32476;&#33021;&#22815;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#23646;&#24615;&#32452;&#21512;&#19978;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#22522;&#30784;&#27169;&#22359;&#65292;&#23558;&#31526;&#21495;&#23646;&#24615;&#34920;&#31034;&#21644;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#38598;&#25104;&#21040;&#31574;&#30053;&#32593;&#32476;&#20013;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#38646;&#26679;&#26412;&#32452;&#21512;&#31574;&#30053;&#23398;&#20064;&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;RL/IL&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent breakthroughs in reinforcement learning (RL) and imitation learning (IL), existing algorithms fail to generalize beyond the training environments. In reality, humans can adapt to new tasks quickly by leveraging prior knowledge about the world such as language descriptions. To facilitate the research on language-guided agents with domain adaption, we propose a novel zero-shot compositional policy learning task, where the environments are characterized as a composition of different attributes. Since there are no public environments supporting this study, we introduce a new research platform BabyAI++ in which the dynamics of environments are disentangled from visual appearance. At each episode, BabyAI++ provides varied vision-dynamics combinations along with corresponding descriptive texts. To evaluate the adaption capability of learned agents, a set of vision-dynamics pairings are held-out for testing on BabyAI++. Unsurprisingly, we find that current language-guided RL/IL 
&lt;/p&gt;</description></item></channel></rss>