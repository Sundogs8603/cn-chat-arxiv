<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35745;&#31639;&#36741;&#21161;&#35777;&#26126;&#21644;&#33258;&#21160;&#21270;&#26041;&#27861;&#22312;&#25968;&#23398;&#21644;STEAM&#25945;&#32946;&#20013;&#30340;&#24212;&#29992;&#65292;&#24378;&#35843;&#20102;&#25506;&#32034;-&#21457;&#29616;-&#29468;&#24819;-&#35777;&#26126;&#26041;&#26696;&#19982;21&#19990;&#32426;&#25945;&#32946;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#26426;&#22120;&#19982;&#20154;&#20043;&#38388;&#30340;&#20132;&#27969;&#19982;&#25209;&#21028;&#24615;&#24605;&#32500;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.10166</link><description>&lt;p&gt;
&#35745;&#31639;&#36741;&#21161;&#35777;&#26126;&#19982;&#25968;&#23398;&#25945;&#32946;&#20013;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Computer Assisted Proofs and Automated Methods in Mathematics Education. (arXiv:2303.10166v1 [math.HO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35745;&#31639;&#36741;&#21161;&#35777;&#26126;&#21644;&#33258;&#21160;&#21270;&#26041;&#27861;&#22312;&#25968;&#23398;&#21644;STEAM&#25945;&#32946;&#20013;&#30340;&#24212;&#29992;&#65292;&#24378;&#35843;&#20102;&#25506;&#32034;-&#21457;&#29616;-&#29468;&#24819;-&#35777;&#26126;&#26041;&#26696;&#19982;21&#19990;&#32426;&#25945;&#32946;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#26426;&#22120;&#19982;&#20154;&#20043;&#38388;&#30340;&#20132;&#27969;&#19982;&#25209;&#21028;&#24615;&#24605;&#32500;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#35770;&#25991;&#26159; ThEdu'22 &#30740;&#35752;&#20250;&#30340;&#29305;&#36992;&#20027;&#39064;&#28436;&#35762;&#30340;&#25193;&#23637;&#29256;&#26412;&#65292;&#20110; 2022 &#24180; 8 &#26376;&#22312;&#20197;&#33394;&#21015;&#28023;&#27861;&#20030;&#34892;&#12290;&#22312;&#31616;&#35201;&#20171;&#32461;&#35745;&#31639;&#26426;&#20195;&#25968;&#31995;&#32479;&#12289;&#21160;&#24577;&#20960;&#20309;&#36719;&#20214;&#21644;&#20854;&#20182;&#26377;&#29992;&#25216;&#26415;&#30340;&#21457;&#23637;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20854;&#22312;&#25968;&#23398;&#25945;&#32946;&#21644;&#26356;&#24191;&#27867;&#30340; STEAM &#25945;&#32946;&#26694;&#26550;&#20013;&#30340;&#24433;&#21709;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23558;&#25968;&#23398;&#25945;&#32946;&#36716;&#21270;&#20026;&#25506;&#32034;-&#21457;&#29616;-&#29468;&#24819;-&#35777;&#26126;&#30340;&#26041;&#26696;&#65292;&#36991;&#20813;&#20351;&#29992;&#25104;&#20026;&#19968;&#20010;&#40657;&#30418;&#23376;&#12290;&#36825;&#31181;&#26041;&#26696;&#24456;&#22909;&#22320;&#31526;&#21512; 21 &#19990;&#32426;&#25945;&#32946;&#25152;&#31216;&#30340; 4C&#65292;&#21363;&#21019;&#26032;&#12289;&#21327;&#20316;&#12289;&#25209;&#21028;&#24615;&#24605;&#32500;&#21644;&#27807;&#36890;&#12290;&#24378;&#35843;&#20102;&#20154;&#19982;&#20154;&#12289;&#26426;&#22120;&#19982;&#26426;&#22120;&#12289;&#20154;&#19982;&#26426;&#22120;&#20043;&#38388;&#30340;&#20132;&#27969;&#19982;&#21327;&#20316;&#12290;&#23545;&#20110;&#36755;&#20986;&#30340;&#29305;&#23450;&#29305;&#24449;&#21152;&#24378;&#20102;&#25209;&#21028;&#24615;&#24605;&#32500;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#21033;&#29992;&#33258;&#21160;&#21270;&#21629;&#20196;&#36827;&#34892;&#25506;&#32034;&#21644;&#21457;&#29616;&#30340;&#26041;&#24335;&#65292;&#24182;&#25552;&#21450;&#20854;&#23384;&#22312;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#29992;&#21442;&#25968;&#31215;&#20998;&#65288;&#25551;&#36848;&#25968;&#23398;&#27010;&#24565;&#30340;&#8220;&#35748;&#30693;&#39046;&#22495;&#8221;&#65289;&#12289;&#24179;&#38754;&#20960;&#20309;&#65288;&#24341;&#20837;&#33258;&#21160;&#21270;&#35777;&#26126;&#30340;&#8220;&#29468;&#24819;&#19981;&#21464;&#37327;&#8221;&#27010;&#24565;&#65289;&#21644;&#20195;&#25968;&#25805;&#20316;&#30340;&#20363;&#23376;&#35828;&#26126;&#20102;&#36825;&#20010;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey paper is an expanded version of an invited keynote at the ThEdu'22 workshop, August 2022, in Haifa (Israel). After a short introduction on the developments of CAS, DGS and other useful technologies, we show implications in Mathematics Education, and in the broader frame of STEAM Education. In particular, we discuss the transformation of Mathematics Education into exploration-discovery-conjecture-proof scheme, avoiding usage as a black box . This scheme fits well into the so-called 4 C's of 21st Century Education. Communication and Collaboration are emphasized not only between humans, but also between machines, and between man and machine. Specific characteristics of the outputs enhance the need of Critical Thinking. The usage of automated commands for exploration and discovery is discussed, with mention of limitations where they exist. We illustrate the topic with examples from parametric integrals (describing a "cognitive neighborhood" of a mathematical notion), plane geom
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#30340;&#24517;&#35201;&#24615;&#24182;&#20174;&#19977;&#20010;&#19968;&#33324;&#24615;&#25968;&#25454;&#20013;&#24515;&#30446;&#26631;&#21644;&#20195;&#34920;&#24615;&#26041;&#27861;&#30340;&#20840;&#38754;&#35270;&#35282;&#36827;&#34892;&#20102;&#20171;&#32461;&#12290;&#35813;&#32508;&#36848;&#20174;&#33258;&#21160;&#21270;&#21644;&#21327;&#20316;&#30340;&#35282;&#24230;&#32452;&#32455;&#20102;&#29616;&#26377;&#25991;&#29486;&#24182;&#35752;&#35770;&#20102;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.10158</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#32508;&#36848;&#65306;&#19968;&#20221;&#35843;&#26597;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-centric Artificial Intelligence: A Survey. (arXiv:2303.10158v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#30340;&#24517;&#35201;&#24615;&#24182;&#20174;&#19977;&#20010;&#19968;&#33324;&#24615;&#25968;&#25454;&#20013;&#24515;&#30446;&#26631;&#21644;&#20195;&#34920;&#24615;&#26041;&#27861;&#30340;&#20840;&#38754;&#35270;&#35282;&#36827;&#34892;&#20102;&#20171;&#32461;&#12290;&#35813;&#32508;&#36848;&#20174;&#33258;&#21160;&#21270;&#21644;&#21327;&#20316;&#30340;&#35282;&#24230;&#32452;&#32455;&#20102;&#29616;&#26377;&#25991;&#29486;&#24182;&#35752;&#35770;&#20102;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27491;&#22312;&#20960;&#20046;&#25152;&#26377;&#39046;&#22495;&#20135;&#29983;&#28145;&#36828;&#30340;&#24433;&#21709;&#65292;&#20854;&#25104;&#21151;&#30340;&#20851;&#38190;&#20043;&#19968;&#26159;&#21487;&#29992;&#20110;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20016;&#23500;&#39640;&#36136;&#37327;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#25968;&#25454;&#22312;AI&#20013;&#30340;&#20316;&#29992;&#24471;&#21040;&#20102;&#26174;&#33879;&#25918;&#22823;&#65292;&#24341;&#21457;&#20102;&#25968;&#25454;&#20013;&#24515;AI&#36825;&#19968;&#26032;&#20852;&#27010;&#24565;&#30340;&#20986;&#29616;&#12290;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#30340;&#27880;&#24847;&#21147;&#36880;&#28176;&#20174;&#25512;&#36827;&#27169;&#22411;&#35774;&#35745;&#36716;&#21521;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#21644;&#25968;&#37327;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25968;&#25454;&#20013;&#24515;AI&#30340;&#24517;&#35201;&#24615;&#65292;&#38543;&#21518;&#20174;&#35757;&#32451;&#25968;&#25454;&#24320;&#21457;&#12289;&#25512;&#29702;&#25968;&#25454;&#24320;&#21457;&#21644;&#25968;&#25454;&#32500;&#25252;&#19977;&#20010;&#19968;&#33324;&#24615;&#25968;&#25454;&#20013;&#24515;&#30446;&#26631;&#20197;&#21450;&#20195;&#34920;&#24615;&#26041;&#27861;&#30340;&#20840;&#38754;&#35270;&#35282;&#36827;&#34892;&#20102;&#20171;&#32461;&#12290;&#25105;&#20204;&#36824;&#20174;&#33258;&#21160;&#21270;&#21644;&#21327;&#20316;&#30340;&#35282;&#24230;&#32452;&#32455;&#20102;&#29616;&#26377;&#25991;&#29486;&#65292;&#35752;&#35770;&#20102;&#25361;&#25112;&#65292;&#24182;&#21015;&#20986;&#20102;&#21508;&#31181;&#20219;&#21153;&#30340;&#27979;&#35797;&#22522;&#20934;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#26159;&#31532;&#19968;&#20221;&#25552;&#20379;&#36328;&#36234;&#21508;&#20010;&#38454;&#27573;&#19968;&#31995;&#21015;&#20219;&#21153;&#30340;&#20840;&#29699;&#35270;&#35282;&#30340;&#32508;&#21512;&#24615;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) is making a profound impact in almost every domain. A vital enabler of its great success is the availability of abundant and high-quality data for building machine learning models. Recently, the role of data in AI has been significantly magnified, giving rise to the emerging concept of data-centric AI. The attention of researchers and practitioners has gradually shifted from advancing model design to enhancing the quality and quantity of the data. In this survey, we discuss the necessity of data-centric AI, followed by a holistic view of three general data-centric goals (training data development, inference data development, and data maintenance) and the representative methods. We also organize the existing literature from automation and collaboration perspectives, discuss the challenges, and tabulate the benchmarks for various tasks. We believe this is the first comprehensive survey that provides a global view of a spectrum of tasks across various stages o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#24212;&#29992;&#28151;&#21512;&#32463;&#20856;-&#37327;&#23376;&#35745;&#31639;&#27169;&#22411;&#36827;&#34892;&#20083;&#33146;&#30284;&#20998;&#26399;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#20256;&#32479;&#32463;&#20856;&#27169;&#22411;&#65292;&#20855;&#26377;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.10142</link><description>&lt;p&gt;
&#28151;&#21512;&#32463;&#20856;-&#37327;&#23376;&#35745;&#31639;&#22312;&#20083;&#33146;&#28024;&#28070;&#24615;&#23548;&#31649;&#30284;&#20998;&#26399;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Hybrid Classic-Quantum Computing for Staging of Invasive Ductal Carcinoma of Breast. (arXiv:2303.10142v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10142
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#24212;&#29992;&#28151;&#21512;&#32463;&#20856;-&#37327;&#23376;&#35745;&#31639;&#27169;&#22411;&#36827;&#34892;&#20083;&#33146;&#30284;&#20998;&#26399;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#20256;&#32479;&#32463;&#20856;&#27169;&#22411;&#65292;&#20855;&#26377;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#20855;&#26377;&#26497;&#22823;&#30340;&#29616;&#23454;&#24847;&#20041;&#65292;&#24182;&#19988;&#36825;&#38376;&#23398;&#31185;&#20026;&#35768;&#22810;&#39046;&#22495;&#24102;&#26469;&#20102;&#38750;&#20961;&#30340;&#21019;&#26032;&#65292;&#20854;&#20013;&#24403;&#28982;&#21253;&#25324;&#21307;&#23398;&#65292;&#20294;&#26159;&#22312;&#21307;&#23398;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#19987;&#23478;&#27491;&#22312;&#23547;&#27714;&#35299;&#20915;&#29616;&#26377;&#20154;&#24037;&#26234;&#33021;&#26041;&#26696;&#26080;&#27861;&#25552;&#20379;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#30340;&#38382;&#39064;&#30340;&#26032;&#36884;&#24452;&#12290;&#22240;&#27492;&#65292;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#36873;&#25321;&#21487;&#33021;&#26159;&#20351;&#29992;&#37327;&#23376;&#21147;&#23398;&#30340;&#27010;&#24565;&#21644;&#24605;&#24819;&#26500;&#24314;&#22522;&#20110;&#37327;&#23376;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#26412;&#25991;&#20174;&#28151;&#21512;&#32463;&#20856;-&#37327;&#23376;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#35770;&#36848;&#20102;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#22312;&#20083;&#33146;&#28024;&#28070;&#24615;&#23548;&#31649;&#30284;&#20998;&#26399;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the great current relevance of Artificial Intelligence, and the extraordinary innovations that this discipline has brought to many fields -among which, without a doubt, medicine is found-, experts in medical applications of Artificial Intelligence are looking for new alternatives to solve problems for which current Artificial Intelligence programs do not provide with optimal solutions. For this, one promising option could be the use of the concepts and ideas of Quantum Mechanics, for the construction of quantum-based Artificial Intelligence systems. From a hybrid classical-quantum perspective, this article deals with the application of quantum computing techniques for the staging of Invasive Ductal Carcinoma of the breast. It includes: (1) a general explanation of a classical, and well-established, approach for medical reasoning, (2) a description of the clinical problem, (3) a conceptual model for staging invasive ductal carcinoma, (4) some basic notions about Quantum Rule-Bas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Distill n' Explain (DnX)&#26041;&#27861;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#23398;&#20064;&#31616;&#21333;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;&#31616;&#21333;&#30340;&#20984;&#35268;&#21010;&#25552;&#21462;&#33410;&#28857;&#25110;&#36793;&#32423;&#21035;&#30340;&#35299;&#37322;&#65292;&#20174;&#32780;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;DnX&#21644;FastDnX&#36890;&#24120;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;GNN&#35299;&#37322;&#22120;&#65292;&#24182;&#19988;&#36816;&#34892;&#36895;&#24230;&#24555;&#24471;&#22810;&#12290;</title><link>http://arxiv.org/abs/2303.10139</link><description>&lt;p&gt;
Distill n' Explain&#65306;&#20351;&#29992;&#31616;&#21333;&#26367;&#20195;&#27169;&#22411;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Distill n' Explain: explaining graph neural networks using simple surrogates. (arXiv:2303.10139v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Distill n' Explain (DnX)&#26041;&#27861;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#23398;&#20064;&#31616;&#21333;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;&#31616;&#21333;&#30340;&#20984;&#35268;&#21010;&#25552;&#21462;&#33410;&#28857;&#25110;&#36793;&#32423;&#21035;&#30340;&#35299;&#37322;&#65292;&#20174;&#32780;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;DnX&#21644;FastDnX&#36890;&#24120;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;GNN&#35299;&#37322;&#22120;&#65292;&#24182;&#19988;&#36816;&#34892;&#36895;&#24230;&#24555;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#33410;&#28857;&#39044;&#27979;&#30340;&#26041;&#27861;&#36890;&#24120;&#26159;&#25214;&#21040;&#20445;&#25345;&#39044;&#27979;&#30340;&#22270;&#23376;&#32467;&#26500;&#12290;&#36825;&#36890;&#24120;&#24847;&#21619;&#30528;&#21453;&#21521;&#20256;&#25773;&#30001;&#20110;GNN&#30340;&#22797;&#26434;&#24615;&#65288;&#20363;&#22914;&#65292;&#23618;&#25968;&#65289;&#32780;&#23548;&#33268;&#35299;&#37322;&#30340;&#25104;&#26412;&#19978;&#21319;&#12290;&#22240;&#27492;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;Distill n' Explain (DnX)&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;DnX&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#26469;&#23398;&#20064;&#26367;&#20195;&#30340;GNN&#12290;&#28982;&#21518;&#65292;DnX&#36890;&#36807;&#35299;&#20915;&#31616;&#21333;&#30340;&#20984;&#35268;&#21010;&#26469;&#25552;&#21462;&#33410;&#28857;&#25110;&#36793;&#32423;&#21035;&#30340;&#35299;&#37322;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;FastDnX&#65292;&#36825;&#26159;DnX&#30340;&#26356;&#24555;&#29256;&#26412;&#65292;&#23427;&#21033;&#29992;&#20102;&#25105;&#20204;&#26367;&#20195;&#27169;&#22411;&#30340;&#32447;&#24615;&#20998;&#35299;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;DnX&#21644;FastDnX&#36890;&#24120;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;GNN&#35299;&#37322;&#22120;&#65292;&#24182;&#19988;&#36816;&#34892;&#36895;&#24230;&#24555;&#24471;&#22810;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#29702;&#35770;&#32467;&#26524;&#25903;&#25345;&#20102;&#25105;&#20204;&#30340;&#23454;&#39564;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explaining node predictions in graph neural networks (GNNs) often boils down to finding graph substructures that preserve predictions. Finding these structures usually implies back-propagating through the GNN, bonding the complexity (e.g., number of layers) of the GNN to the cost of explaining it. This naturally begs the question: Can we break this bond by explaining a simpler surrogate GNN? To answer the question, we propose Distill n' Explain (DnX). First, DnX learns a surrogate GNN via knowledge distillation. Then, DnX extracts node or edge-level explanations by solving a simple convex program. We also propose FastDnX, a faster version of DnX that leverages the linear decomposition of our surrogate model. Experiments show that DnX and FastDnX often outperform state-of-the-art GNN explainers while being orders of magnitude faster. Additionally, we support our empirical findings with theoretical results linking the quality of the surrogate model (i.e., distillation error) to the faith
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ToolWriter&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#29983;&#25104;&#26597;&#35810;&#29305;&#23450;&#30340;&#31243;&#24207;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#36716;&#25442;&#34920;&#26684;&#65292;&#20197;&#25552;&#39640;&#34920;&#26684;&#38382;&#31572;&#65288;TQA&#65289;&#30340;&#24615;&#33021;&#12290;&#36825;&#20010;&#24037;&#20855;&#36890;&#36807;&#29983;&#25104;&#34892;&#36807;&#28388;&#24037;&#20855;&#65292;&#25913;&#36827;&#20102;WikiTableQuestions&#21644;WikiSQL&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;&#38543;&#30528;&#34920;&#26684;&#23610;&#23544;&#30340;&#22686;&#21152;&#65292;&#20256;&#32479;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#34920;&#26684;&#30340;&#36807;&#31243;&#20013;&#23384;&#22312;&#20449;&#24687;&#20002;&#22833;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#31243;&#24207;&#21270;&#24037;&#20855;&#19982;&#31070;&#32463;&#32452;&#20214;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#24191;&#27867;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.10138</link><description>&lt;p&gt;
&#23545;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#38382;&#39064;&#29305;&#23450;&#24037;&#20855;&#21512;&#25104;&#65306;&#29983;&#25104;&#12289;&#36716;&#25442;&#12289;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Generate, Transform, Answer: Question Specific Tool Synthesis for Tabular Data. (arXiv:2303.10138v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ToolWriter&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#29983;&#25104;&#26597;&#35810;&#29305;&#23450;&#30340;&#31243;&#24207;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#36716;&#25442;&#34920;&#26684;&#65292;&#20197;&#25552;&#39640;&#34920;&#26684;&#38382;&#31572;&#65288;TQA&#65289;&#30340;&#24615;&#33021;&#12290;&#36825;&#20010;&#24037;&#20855;&#36890;&#36807;&#29983;&#25104;&#34892;&#36807;&#28388;&#24037;&#20855;&#65292;&#25913;&#36827;&#20102;WikiTableQuestions&#21644;WikiSQL&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;&#38543;&#30528;&#34920;&#26684;&#23610;&#23544;&#30340;&#22686;&#21152;&#65292;&#20256;&#32479;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#34920;&#26684;&#30340;&#36807;&#31243;&#20013;&#23384;&#22312;&#20449;&#24687;&#20002;&#22833;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#31243;&#24207;&#21270;&#24037;&#20855;&#19982;&#31070;&#32463;&#32452;&#20214;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#24191;&#27867;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#38382;&#31572;&#65288;TQA&#65289;&#23545;&#20110;&#31070;&#32463;&#31995;&#32479;&#26469;&#35828;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35774;&#32622;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#33258;&#28982;&#35821;&#35328;&#19982;&#22823;&#37327;&#21322;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#32852;&#21512;&#25512;&#29702;&#12290;&#19982;&#20154;&#31867;&#20351;&#29992;&#31243;&#24207;&#21270;&#24037;&#20855;&#65288;&#22914;&#36807;&#28388;&#22120;&#65289;&#22312;&#22788;&#29702;&#20043;&#21069;&#36716;&#25442;&#25968;&#25454;&#19981;&#21516;&#65292;TQA&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#22788;&#29702;&#34920;&#26684;&#65292;&#38543;&#30528;&#34920;&#26684;&#23610;&#23544;&#30340;&#22686;&#21152;&#65292;&#20250;&#23548;&#33268;&#20449;&#24687;&#20002;&#22833;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ToolWriter&#65292;&#29992;&#20110;&#29983;&#25104;&#26597;&#35810;&#29305;&#23450;&#30340;&#31243;&#24207;&#65292;&#24182;&#22312;&#38656;&#35201;&#26102;&#26816;&#27979;&#20309;&#26102;&#23558;&#20854;&#24212;&#29992;&#20110;&#36716;&#25442;&#34920;&#26684;&#65292;&#24182;&#23558;&#20854;&#19982;TQA&#27169;&#22411;&#30340;&#33021;&#21147;&#23545;&#40784;&#12290;&#23558;ToolWriter&#19987;&#27880;&#20110;&#29983;&#25104;&#34892;&#36807;&#28388;&#24037;&#20855;&#65292;&#25913;&#36827;&#20102;WikiTableQuestions&#21644;WikiSQL&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#26368;&#22823;&#30340;&#24615;&#33021;&#25552;&#21319;&#20986;&#29616;&#22312;&#38271;&#34920;&#26684;&#19978;&#12290;&#36890;&#36807;&#35843;&#26597;&#31354;&#38388;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#31361;&#20986;&#20102;&#23558;&#31243;&#24207;&#21270;&#24037;&#20855;&#19982;&#31070;&#32463;&#32452;&#20214;&#30456;&#32467;&#21512;&#20197;&#25805;&#20316;&#22823;&#37327;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#24191;&#27867;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tabular question answering (TQA) presents a challenging setting for neural systems by requiring joint reasoning of natural language with large amounts of semi-structured data. Unlike humans who use programmatic tools like filters to transform data before processing, language models in TQA process tables directly, resulting in information loss as table size increases. In this paper we propose ToolWriter to generate query specific programs and detect when to apply them to transform tables and align them with the TQA model's capabilities. Focusing ToolWriter to generate row-filtering tools improves the state-of-the-art for WikiTableQuestions and WikiSQL with the most performance gained on long tables. By investigating headroom, our work highlights the broader potential for programmatic tools combined with neural components to manipulate large amounts of structured data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#35843;&#26597;&#20102;56&#39033;&#19982;&#36719;&#20214;&#24320;&#21457;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#21457;&#29616;&#24615;&#21035;&#20195;&#35789;&#19982;&#19981;&#21516;&#20219;&#21153;&#30340;&#30456;&#20851;&#24615;&#26126;&#26174;&#19981;&#21516;&#12290;&#20854;&#20013;&#65292;&#21482;&#26377;6%&#30340;&#38656;&#27714;&#25910;&#38598;&#20219;&#21153;&#19982;&#20195;&#35789;&#8220;&#20182;&#8221;&#30456;&#20851;&#32852;&#65292;&#32780;&#27979;&#35797;&#20219;&#21153;&#21017;&#22312;100%&#30340;&#24773;&#20917;&#19979;&#19982;&#8220;&#20182;&#8221;&#30456;&#20851;&#32852;&#12290;&#27492;&#22806;&#65292;&#24110;&#21161;&#20182;&#20154;&#30340;&#20219;&#21153;&#26377;91%&#30340;&#30456;&#20851;&#24615;&#19982;&#8220;&#20182;&#8221;&#30456;&#20851;&#32852;&#12290;</title><link>http://arxiv.org/abs/2303.10131</link><description>&lt;p&gt;
&#22905;&#25910;&#38598;&#38656;&#27714;&#65292;&#20182;&#36827;&#34892;&#27979;&#35797;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36719;&#20214;&#24037;&#31243;&#24615;&#21035;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
She Elicits Requirements and He Tests: Software Engineering Gender Bias in Large Language Models. (arXiv:2303.10131v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#35843;&#26597;&#20102;56&#39033;&#19982;&#36719;&#20214;&#24320;&#21457;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#21457;&#29616;&#24615;&#21035;&#20195;&#35789;&#19982;&#19981;&#21516;&#20219;&#21153;&#30340;&#30456;&#20851;&#24615;&#26126;&#26174;&#19981;&#21516;&#12290;&#20854;&#20013;&#65292;&#21482;&#26377;6%&#30340;&#38656;&#27714;&#25910;&#38598;&#20219;&#21153;&#19982;&#20195;&#35789;&#8220;&#20182;&#8221;&#30456;&#20851;&#32852;&#65292;&#32780;&#27979;&#35797;&#20219;&#21153;&#21017;&#22312;100%&#30340;&#24773;&#20917;&#19979;&#19982;&#8220;&#20182;&#8221;&#30456;&#20851;&#32852;&#12290;&#27492;&#22806;&#65292;&#24110;&#21161;&#20182;&#20154;&#30340;&#20219;&#21153;&#26377;91%&#30340;&#30456;&#20851;&#24615;&#19982;&#8220;&#20182;&#8221;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#24320;&#21457;&#20013;&#30340;&#38544;&#24615;&#24615;&#21035;&#20559;&#35265;&#26159;&#19968;&#20010;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#38382;&#39064;&#65292;&#27604;&#22914;&#23558;&#25216;&#26415;&#35282;&#33394;&#19982;&#30007;&#24615;&#32852;&#31995;&#22312;&#19968;&#36215;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#20559;&#35265;&#65292;&#26356;&#35814;&#32454;&#22320;&#20102;&#35299;&#23427;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#35843;&#26597;&#19982;&#36719;&#20214;&#24320;&#21457;&#30456;&#20851;&#30340;56&#39033;&#20219;&#21153;&#65288;&#22914;&#20998;&#37197;GitHub&#38382;&#39064;&#21644;&#27979;&#35797;&#65289;&#65292;&#20197;&#20102;&#35299;&#23884;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#24615;&#24615;&#21035;&#20559;&#35265;&#25152;&#20135;&#29983;&#30340;&#24433;&#21709;&#31243;&#24230;&#12290;&#25105;&#20204;&#23558;&#27599;&#20010;&#20219;&#21153;&#20174;&#33521;&#35821;&#31995;&#32479;&#22320;&#32763;&#35793;&#25104;&#26080;&#24615;&#21035;&#35821;&#35328;&#65292;&#28982;&#21518;&#20877;&#32763;&#35793;&#22238;&#33521;&#35821;&#65292;&#24182;&#35843;&#26597;&#19982;&#27599;&#20010;&#20219;&#21153;&#30456;&#20851;&#30340;&#20195;&#35789;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#25490;&#21015;&#20013;&#21453;&#22797;&#32763;&#35793;&#27599;&#20010;&#20219;&#21153;100&#27425;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19981;&#21516;&#20219;&#21153;&#19982;&#24615;&#21035;&#20195;&#35789;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21482;&#26377;6%&#30340;&#38656;&#27714;&#25910;&#38598;&#20219;&#21153;&#19982;&#20195;&#35789;&#8220;&#20182;&#8221;&#30456;&#20851;&#32852;&#65292;&#32780;&#27979;&#35797;&#20219;&#21153;&#21017;&#22312;100%&#30340;&#24773;&#20917;&#19979;&#19982;&#8220;&#20182;&#8221;&#30456;&#20851;&#32852;&#12290;&#27492;&#22806;&#65292;&#28041;&#21450;&#24110;&#21161;&#20182;&#20154;&#30340;&#20219;&#21153;&#26377;91%&#30340;&#30456;&#20851;&#24615;&#19982;&#8220;&#20182;&#8221;&#30456;&#20851;&#32852;&#65292;&#32780;&#25191;&#34892;&#21516;&#26679;&#20219;&#21153;&#30340;&#22899;&#24615;&#21017;&#24456;&#23481;&#26131;&#34987;&#24573;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit gender bias in software development is a well-documented issue, such as the association of technical roles with men. To address this bias, it is important to understand it in more detail. This study uses data mining techniques to investigate the extent to which 56 tasks related to software development, such as assigning GitHub issues and testing, are affected by implicit gender bias embedded in large language models. We systematically translated each task from English into a genderless language and back, and investigated the pronouns associated with each task. Based on translating each task 100 times in different permutations, we identify a significant disparity in the gendered pronoun associations with different tasks. Specifically, requirements elicitation was associated with the pronoun "he" in only 6% of cases, while testing was associated with "he" in 100% of cases. Additionally, tasks related to helping others had a 91% association with "he" while the same association fo
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;GPT&#65288;&#22823;&#35821;&#35328;&#27169;&#22411;&#65289;&#21644;&#30456;&#20851;&#25216;&#26415;&#23545;&#32654;&#22269;&#21171;&#21160;&#21147;&#24066;&#22330;&#30340;&#28508;&#22312;&#24433;&#21709;&#65292;&#21457;&#29616;&#22823;&#32422;80%&#30340;&#32654;&#22269;&#21171;&#21160;&#21147;&#21487;&#33021;&#20250;&#21463;&#21040;10%&#30340;&#24037;&#20316;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#28085;&#30422;&#20102;&#25152;&#26377;&#24037;&#36164;&#27700;&#24179;&#21644;&#21508;&#34892;&#21508;&#19994;&#65292;&#39044;&#31034;&#30528;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#26174;&#33879;&#30340;&#32463;&#27982;&#12289;&#31038;&#20250;&#21644;&#25919;&#31574;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.10130</link><description>&lt;p&gt;
GPT&#26159;GPT&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#21171;&#21160;&#21147;&#24066;&#22330;&#24433;&#21709;&#30340;&#26089;&#26399;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models. (arXiv:2303.10130v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10130
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;GPT&#65288;&#22823;&#35821;&#35328;&#27169;&#22411;&#65289;&#21644;&#30456;&#20851;&#25216;&#26415;&#23545;&#32654;&#22269;&#21171;&#21160;&#21147;&#24066;&#22330;&#30340;&#28508;&#22312;&#24433;&#21709;&#65292;&#21457;&#29616;&#22823;&#32422;80%&#30340;&#32654;&#22269;&#21171;&#21160;&#21147;&#21487;&#33021;&#20250;&#21463;&#21040;10%&#30340;&#24037;&#20316;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#28085;&#30422;&#20102;&#25152;&#26377;&#24037;&#36164;&#27700;&#24179;&#21644;&#21508;&#34892;&#21508;&#19994;&#65292;&#39044;&#31034;&#30528;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#26174;&#33879;&#30340;&#32463;&#27982;&#12289;&#31038;&#20250;&#21644;&#25919;&#31574;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#21644;&#30456;&#20851;&#25216;&#26415;&#23545;&#32654;&#22269;&#21171;&#21160;&#21147;&#24066;&#22330;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;&#20351;&#29992;&#26032;&#30340;&#26631;&#20934;&#65292;&#25105;&#20204;&#35780;&#20272;&#32844;&#19994;&#19982;GPT&#33021;&#21147;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#32467;&#21512;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#21644;GPT-4&#30340;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#32422;80%&#30340;&#32654;&#22269;&#21171;&#21160;&#21147;&#21487;&#33021;&#20250;&#33267;&#23569;&#26377;10%&#30340;&#24037;&#20316;&#20219;&#21153;&#21463;&#21040;GPT&#24341;&#20837;&#30340;&#24433;&#21709;&#65292;&#32780;&#32422;19%&#30340;&#24037;&#20154;&#21487;&#33021;&#20250;&#30475;&#21040;&#33267;&#23569;50%&#30340;&#20219;&#21153;&#21463;&#21040;&#24433;&#21709;&#12290;&#24433;&#21709;&#33539;&#22260;&#28085;&#30422;&#20102;&#25152;&#26377;&#24037;&#36164;&#27700;&#24179;&#65292;&#39640;&#25910;&#20837;&#24037;&#20316;&#21487;&#33021;&#38754;&#20020;&#26356;&#22823;&#30340;&#39118;&#38505;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#24433;&#21709;&#24182;&#19981;&#23616;&#38480;&#20110;&#26368;&#36817;&#29983;&#20135;&#29575;&#22686;&#38271;&#36739;&#39640;&#30340;&#34892;&#19994;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#20855;&#26377;&#36890;&#29992;&#25216;&#26415;&#65288;GPT&#65289;&#30340;&#29305;&#24615;&#65292;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#26174;&#33879;&#30340;&#32463;&#27982;&#12289;&#31038;&#20250;&#21644;&#25919;&#31574;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the potential implications of Generative Pre-trained Transformer (GPT) models and related technologies on the U.S. labor market. Using a new rubric, we assess occupations based on their correspondence with GPT capabilities, incorporating both human expertise and classifications from GPT-4. Our findings indicate that approximately 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of GPTs, while around 19% of workers may see at least 50% of their tasks impacted. The influence spans all wage levels, with higher-income jobs potentially facing greater exposure. Notably, the impact is not limited to industries with higher recent productivity growth. We conclude that Generative Pre-trained Transformers exhibit characteristics of general-purpose technologies (GPTs), suggesting that as these models could have notable economic, social, and policy implications.
&lt;/p&gt;</description></item><item><title>Clingraph&#26159;&#19968;&#27454;ASP&#21487;&#35270;&#21270;&#31995;&#32479;&#65292;&#20351;&#29992;ASP&#20107;&#23454;&#30340;&#22270;&#24418;&#35268;&#33539;&#20256;&#36882;&#32473;&#22270;&#24418;&#31995;&#32479;&#65292;&#25552;&#20379;&#20102;&#26497;&#22909;&#30340;&#25509;&#21475;&#65292;&#19988;&#20855;&#26377;Python API&#65292;&#21487;&#36830;&#25509;&#21644;&#30417;&#35270;&#27714;&#35299;&#36807;&#31243;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2303.10118</link><description>&lt;p&gt;
Clingraph&#65306;ASP&#21487;&#35270;&#21270;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
$\textit{Clingraph}$: A System for ASP-based Visualization. (arXiv:2303.10118v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10118
&lt;/p&gt;
&lt;p&gt;
Clingraph&#26159;&#19968;&#27454;ASP&#21487;&#35270;&#21270;&#31995;&#32479;&#65292;&#20351;&#29992;ASP&#20107;&#23454;&#30340;&#22270;&#24418;&#35268;&#33539;&#20256;&#36882;&#32473;&#22270;&#24418;&#31995;&#32479;&#65292;&#25552;&#20379;&#20102;&#26497;&#22909;&#30340;&#25509;&#21475;&#65292;&#19988;&#20855;&#26377;Python API&#65292;&#21487;&#36830;&#25509;&#21644;&#30417;&#35270;&#27714;&#35299;&#36807;&#31243;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;ASP&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;clingraph&#65292;&#23427;&#26088;&#22312;&#36890;&#36807;ASP&#26412;&#36523;&#26469;&#21487;&#35270;&#21270;ASP&#30340;&#21508;&#31181;&#27010;&#24565;&#12290;&#36825;&#20010;&#24819;&#27861;&#21487;&#20197;&#36861;&#28335;&#21040;aspviz&#24037;&#20855;&#65292;&#32780;clingraph&#21017;&#22312;&#29616;&#20195;ASP&#31995;&#32479;&#30340;&#32972;&#26223;&#19979;&#37325;&#26032;&#24320;&#21457;&#21644;&#25193;&#23637;&#23427;&#12290;&#26356;&#31934;&#30830;&#22320;&#35828;&#65292;clingraph&#36890;&#36807;ASP&#20107;&#23454;&#30340;&#22270;&#24418;&#35268;&#33539;&#23558;&#23427;&#20204;&#20256;&#36882;&#32473;&#22270;&#24418;&#21487;&#35270;&#21270;&#31995;&#32479;graphviz&#12290;ASP&#30340;&#20351;&#29992;&#25552;&#20379;&#20102;&#36923;&#36753;&#31243;&#24207;&#21644;/&#25110;&#31572;&#26696;&#38598;&#21450;&#20854;&#21487;&#35270;&#21270;&#20043;&#38388;&#30340;&#26497;&#22909;&#25509;&#21475;&#12290;&#21478;&#22806;&#65292;clingraph&#25552;&#20379;&#20102;&#19968;&#20010;Python API&#65292;&#25193;&#23637;&#20102;&#36825;&#31181;&#26131;&#20110;&#25509;&#21475;&#21270;&#21040;clingo&#30340;API&#65292;&#20174;&#32780;&#36830;&#25509;&#21644;&#30417;&#35270;&#27714;&#35299;&#36807;&#31243;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the ASP-based visualization tool $\textit{clingraph}$ which aims at visualizing various concepts of ASP by means of ASP itself. This idea traces back to the $\textit{aspviz}$ tool and $\textit{clingraph}$ redevelops and extends it in the context of modern ASP systems. More precisely, $\textit{clingraph}$ takes graph specifications in terms of ASP facts and hands them over to the graph visualization system $\textit{graphviz}$. The use of ASP provides a great interface between logic programs and/or answer sets and their visualization. Also, $\textit{clingraph}$ offers a $\textit{python}$ API that extends this ease of interfacing to $\textit{clingo}$'s API, and in turn to connect and monitor various aspects of the solving process.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#20010;&#22810;&#20803;&#20851;&#31995;&#26694;&#26550;&#26469;&#25351;&#23548;&#26426;&#26500;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#21644;&#37319;&#32435;&#65292;&#35299;&#20915;&#31038;&#20250;&#25216;&#26415;&#35805;&#35821;&#20013;&#30340;&#20851;&#31995;&#38382;&#39064;&#65292;&#21253;&#25324;&#35821;&#20041;&#27169;&#31946;&#12289;&#27010;&#24565;&#20043;&#38388;&#32570;&#20047;&#26126;&#30830;&#30340;&#20851;&#31995;&#21644;&#19981;&#21516;&#30340;&#26631;&#20934;&#26415;&#35821;&#65292;&#24110;&#21161;&#35780;&#20272;&#26426;&#26500;AI&#31995;&#32479;&#65292;&#36991;&#20813;&#27010;&#24565;&#23396;&#31435;&#12290;</title><link>http://arxiv.org/abs/2303.10106</link><description>&lt;p&gt;
&#19968;&#20010;&#22810;&#20803;&#20851;&#31995;&#26694;&#26550;&#25351;&#23548;&#26426;&#26500;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#21644;&#37319;&#32435;
&lt;/p&gt;
&lt;p&gt;
A multidomain relational framework to guide institutional AI research and adoption. (arXiv:2303.10106v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10106
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#20010;&#22810;&#20803;&#20851;&#31995;&#26694;&#26550;&#26469;&#25351;&#23548;&#26426;&#26500;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#21644;&#37319;&#32435;&#65292;&#35299;&#20915;&#31038;&#20250;&#25216;&#26415;&#35805;&#35821;&#20013;&#30340;&#20851;&#31995;&#38382;&#39064;&#65292;&#21253;&#25324;&#35821;&#20041;&#27169;&#31946;&#12289;&#27010;&#24565;&#20043;&#38388;&#32570;&#20047;&#26126;&#30830;&#30340;&#20851;&#31995;&#21644;&#19981;&#21516;&#30340;&#26631;&#20934;&#26415;&#35821;&#65292;&#24110;&#21161;&#35780;&#20272;&#26426;&#26500;AI&#31995;&#32479;&#65292;&#36991;&#20813;&#27010;&#24565;&#23396;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#25351;&#23548;&#26426;&#26500;&#21644;&#20844;&#20849;&#31649;&#29702;&#20013;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#37319;&#32435;&#30340;&#26032;&#25351;&#26631;&#12289;&#25216;&#26415;&#26631;&#20934;&#21644;&#31649;&#29702;&#26426;&#21046;&#30340;&#21628;&#21505;&#29616;&#24050;&#21496;&#31354;&#35265;&#24815;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26088;&#22312;&#20102;&#35299;&#37319;&#32435;AI&#30340;&#24433;&#21709;&#30340;&#30740;&#31350;&#21644;&#25919;&#31574;&#21162;&#21147;&#24448;&#24448;&#21482;&#20248;&#20808;&#32771;&#34385;&#23569;&#25968;&#24819;&#27861;&#65292;&#32780;&#26410;&#23436;&#20840;&#32771;&#34385;&#25152;&#26377;&#28508;&#22312;&#30456;&#20851;&#30340;&#19981;&#21516;&#35270;&#35282;&#21644;&#20027;&#39064;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#25991;&#20214;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#36951;&#28431;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#28304;&#20110;&#25105;&#20204;&#25152;&#31216;&#30340;&#31038;&#20250;&#25216;&#26415;&#35805;&#35821;&#20013;&#30340;&#20851;&#31995;&#38382;&#39064;:&#22522;&#26412;&#30340;&#26412;&#20307;&#35770;&#38382;&#39064;&#23578;&#26410;&#35299;&#20915;&#65292;&#21253;&#25324;&#35821;&#20041;&#27169;&#31946;&#12289;&#27010;&#24565;&#20043;&#38388;&#32570;&#20047;&#26126;&#30830;&#30340;&#20851;&#31995;&#21644;&#19981;&#21516;&#30340;&#26631;&#20934;&#26415;&#35821;&#12290;&#36825;&#23548;&#33268;&#22312;&#35780;&#20272;&#26426;&#26500;AI&#31995;&#32479;&#30340;&#19981;&#21516;&#25512;&#29702;&#27169;&#24335;&#20197;&#21450;&#30740;&#31350;&#23427;&#20204;&#30340;&#39046;&#22495;&#65292;&#21253;&#25324;&#26426;&#22120;&#23398;&#20064;&#12289;&#20154;&#31867;&#22240;&#32032;&#12289;&#31038;&#20250;&#31185;&#23398;&#21644;&#25919;&#31574;&#26041;&#38754;&#23384;&#22312;&#27010;&#24565;&#23396;&#31435;&#12290;&#22312;&#21457;&#23637;&#20102;&#36825;&#19968;&#25209;&#21028;&#20043;&#21518;&#65292;
&lt;/p&gt;
&lt;p&gt;
Calls for new metrics, technical standards and governance mechanisms to guide the adoption of Artificial Intelligence (AI) in institutions and public administration are now commonplace. Yet, most research and policy efforts aimed at understanding the implications of adopting AI tend to prioritize only a handful of ideas; they do not fully account for all the different perspectives and topics that are potentially relevant. In this position paper, we contend that this omission stems, in part, from what we call the relational problem in socio-technical discourse: fundamental ontological issues have not yet been settled-including semantic ambiguity, a lack of clear relations between concepts and differing standard terminologies. This contributes to the persistence of disparate modes of reasoning to assess institutional AI systems, and the prevalence of conceptual isolation in the fields that study them including ML, human factors, social science and policy. After developing this critique, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#19978;&#19979;&#25991;&#22312;&#30446;&#26631;&#26816;&#27979;&#21306;&#22495;-&#35789;&#23545;&#40784;&#20013;&#20316;&#29992;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#25552;&#39640;&#20102;&#23646;&#24615;&#30340;&#20316;&#29992;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#30446;&#26631;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.10093</link><description>&lt;p&gt;
&#25552;&#39640;&#19978;&#19979;&#25991;&#22312;&#30446;&#26631;&#26816;&#27979;&#21306;&#22495;-&#35789;&#23545;&#40784;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Enhancing the Role of Context in Region-Word Alignment for Object Detection. (arXiv:2303.10093v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#19978;&#19979;&#25991;&#22312;&#30446;&#26631;&#26816;&#27979;&#21306;&#22495;-&#35789;&#23545;&#40784;&#20013;&#20316;&#29992;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#25552;&#39640;&#20102;&#23646;&#24615;&#30340;&#20316;&#29992;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#30446;&#26631;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#23398;&#20064;&#22270;&#20687;-&#26631;&#27880;&#37197;&#23545;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#21306;&#22495;-&#35789;&#23545;&#40784;&#65292;&#25512;&#21160;&#20102;&#24320;&#25918;&#35789;&#27719;&#30446;&#26631;&#26816;&#27979;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#21306;&#22495;-&#35789;&#23545;&#40784;&#26041;&#27861;&#36890;&#24120;&#20165;&#38024;&#23545;&#30446;&#26631;&#21517;&#35789;&#22312;&#26816;&#27979;&#20013;&#20351;&#29992;&#65292;&#20854;&#20182;&#19978;&#19979;&#25991;&#65292;&#20363;&#22914;&#23646;&#24615;&#65292;&#23545;&#26816;&#27979;&#30340;&#24433;&#21709;&#19981;&#26126;&#30830;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#35821;&#35328;&#19978;&#19979;&#25991;&#22914;&#20309;&#24433;&#21709;&#19979;&#28216;&#30446;&#26631;&#26816;&#27979;&#65292;&#24182;&#25552;&#35758;&#22686;&#24378;&#19978;&#19979;&#25991;&#30340;&#20316;&#29992;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#31574;&#30053;&#24615;&#22320;&#23558;&#25509;&#22320;&#39044;&#35757;&#32451;&#30446;&#26631;&#24773;&#22659;&#21270;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#23646;&#24615;&#20316;&#20026;&#29305;&#21035;&#26377;&#29992;&#30340;&#30446;&#26631;&#19978;&#19979;&#25991;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#24418;&#23481;&#35789;&#21644;&#21517;&#35789;&#30340;&#36127;&#37319;&#26679;&#31574;&#30053;&#65292;&#20197;&#22686;&#21152;&#23545;&#23427;&#20204;&#30340;&#23545;&#27604;&#23398;&#20064;&#30340;&#20851;&#27880;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#19982;&#21306;&#22495;-&#35789;&#39044;&#35757;&#32451;&#30340;&#26368;&#26032;&#25216;&#26415;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#21319;&#20102;&#30446;&#26631;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#25991;&#26412;-&#21306;&#22495;&#21487;&#35270;&#21270;&#26174;&#31034;&#23646;&#24615;&#25935;&#24863;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language pretraining to learn a fine-grained, region-word alignment between image-caption pairs has propelled progress in open-vocabulary object detection. We observe that region-word alignment methods are typically used in detection with respect to only object nouns, and the impact of other rich context in captions, such as attributes, is unclear. In this study, we explore how language context affects downstream object detection and propose to enhance the role of context. In particular, we show how to strategically contextualize the grounding pretraining objective for improved alignment. We further hone in on attributes as especially useful object context and propose a novel adjective and noun-based negative sampling strategy for increasing their focus in contrastive learning. Overall, our methods enhance object detection when compared to the state-of-the-art in region-word pretraining. We also highlight the fine-grained utility of an attribute-sensitive model through text-regi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;-&#36229;&#24179;&#38754;&#26680;&#20989;&#25968;&#26063;&#65292;&#33021;&#22815;&#22312;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#31215;&#26497;&#23398;&#20064;&#20013;&#20351;&#29992;&#65292;&#20197;&#24314;&#27169;&#38750;&#24179;&#31283;&#24615;&#21644;&#38750;&#32447;&#24615;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.10022</link><description>&lt;p&gt;
&#23618;&#27425;-&#36229;&#24179;&#38754;&#26680;&#22312;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#31215;&#26497;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hierarchical-Hyperplane Kernels for Actively Learning Gaussian Process Models of Nonstationary Systems. (arXiv:2303.10022v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;-&#36229;&#24179;&#38754;&#26680;&#20989;&#25968;&#26063;&#65292;&#33021;&#22815;&#22312;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#31215;&#26497;&#23398;&#20064;&#20013;&#20351;&#29992;&#65292;&#20197;&#24314;&#27169;&#38750;&#24179;&#31283;&#24615;&#21644;&#38750;&#32447;&#24615;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22797;&#26434;&#35745;&#31639;&#26426;&#27169;&#25311;&#21644;&#29289;&#29702;&#26426;&#22120;&#30340;&#31934;&#30830;&#20195;&#29702;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#38271;&#26102;&#38388;&#25110;&#26114;&#36149;&#30340;&#23454;&#39564;&#12290;&#27492;&#22806;&#65292;&#24314;&#27169;&#30340;&#29289;&#29702;&#20381;&#36182;&#20851;&#31995;&#34920;&#29616;&#20986;&#38750;&#32447;&#24615;&#21644;&#38750;&#24179;&#31283;&#24615;&#12290;&#22240;&#27492;&#65292;&#29992;&#20110;&#20135;&#29983;&#20195;&#29702;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24212;&#36890;&#36807;&#25552;&#20379;&#20445;&#25345;&#26597;&#35810;&#25968;&#37327;&#23569;&#30340;&#26041;&#26696;&#65288;&#20363;&#22914;&#20351;&#29992;&#31215;&#26497;&#23398;&#20064;&#65289;&#65292;&#24182;&#33021;&#22815;&#25429;&#33719;&#31995;&#32479;&#30340;&#38750;&#32447;&#24615;&#21644;&#38750;&#24179;&#31283;&#29305;&#24615;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#19968;&#31181;&#24314;&#27169;&#38750;&#24179;&#31283;&#24615;&#30340;&#26041;&#27861;&#26159;&#24341;&#20837;&#36755;&#20837;&#20998;&#21306;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#39640;&#26031;&#36807;&#31243;&#30340;&#31215;&#26497;&#23398;&#20064;&#20013;&#34987;&#35777;&#26126;&#26159;&#26377;&#20248;&#21183;&#30340;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#20551;&#23450;&#24050;&#30693;&#20998;&#21306;&#65292;&#38656;&#35201;&#24341;&#20837;&#22797;&#26434;&#30340;&#25277;&#26679;&#26041;&#26696;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#38750;&#24120;&#31616;&#21333;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#24378;&#22823;&#30340;&#26680;&#20989;&#25968;&#26063;&#65292;&#23427;&#21253;&#25324;&#19968;&#20010;&#21487;&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#36827;&#34892;&#23398;&#20064;&#30340;&#20998;&#21306;&#65292;&#24182;&#20351;&#29992;&#26356;&#28789;&#27963;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning precise surrogate models of complex computer simulations and physical machines often require long-lasting or expensive experiments. Furthermore, the modeled physical dependencies exhibit nonlinear and nonstationary behavior. Machine learning methods that are used to produce the surrogate model should therefore address these problems by providing a scheme to keep the number of queries small, e.g. by using active learning and be able to capture the nonlinear and nonstationary properties of the system. One way of modeling the nonstationarity is to induce input-partitioning, a principle that has proven to be advantageous in active learning for Gaussian processes. However, these methods either assume a known partitioning, need to introduce complex sampling schemes or rely on very simple geometries. In this work, we present a simple, yet powerful kernel family that incorporates a partitioning that: i) is learnable via gradient-based methods, ii) uses a geometry that is more flexible
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#20449;&#25509;&#21475;gRPC&#65292;&#20197;&#25913;&#21892;DareFightingICE&#20013;AI&#25968;&#25454;&#20256;&#36755;&#30340;&#25928;&#29575;&#65292;&#20943;&#23569;&#28216;&#25103;&#26381;&#21153;&#22120;&#20449;&#24687;&#25509;&#25910;&#25152;&#33457;&#36153;&#30340;&#26102;&#38388;&#12290;&#22312;&#23454;&#29616;&#25511;&#21046;&#38750;&#29609;&#23478;&#35282;&#33394;&#30340;AI&#30340;&#24212;&#29992;&#20013;&#65292;&#35813;&#26041;&#27861;&#21313;&#20998;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2303.10001</link><description>&lt;p&gt;
&#20351;&#29992;gRPC&#25552;&#39640;DareFightingICE&#20013;AI&#30340;&#25968;&#25454;&#20256;&#36755;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Improving Data Transfer Efficiency for AIs in the DareFightingICE using gRPC. (arXiv:2303.10001v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#20449;&#25509;&#21475;gRPC&#65292;&#20197;&#25913;&#21892;DareFightingICE&#20013;AI&#25968;&#25454;&#20256;&#36755;&#30340;&#25928;&#29575;&#65292;&#20943;&#23569;&#28216;&#25103;&#26381;&#21153;&#22120;&#20449;&#24687;&#25509;&#25910;&#25152;&#33457;&#36153;&#30340;&#26102;&#38388;&#12290;&#22312;&#23454;&#29616;&#25511;&#21046;&#38750;&#29609;&#23478;&#35282;&#33394;&#30340;AI&#30340;&#24212;&#29992;&#20013;&#65292;&#35813;&#26041;&#27861;&#21313;&#20998;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DareFightingICE&#24179;&#21488;&#30340;&#19968;&#31181;&#26032;&#30340;&#36890;&#20449;&#25509;&#21475;&#65292;&#35813;&#24179;&#21488;&#26159;&#19968;&#20010;&#22522;&#20110;Java&#30340;&#26684;&#26007;&#28216;&#25103;&#65292;&#20854;&#37325;&#28857;&#26159;&#23454;&#29616;&#25511;&#21046;&#38750;&#29609;&#23478;&#35282;&#33394;&#30340;AI&#12290;&#35813;&#25509;&#21475;&#20351;&#29992;&#19968;&#20010;&#24320;&#28304;&#30340;&#36828;&#31243;&#36807;&#31243;&#35843;&#29992;&#31995;&#32479;&#65292;&#21363;gRPC&#65292;&#20197;&#25552;&#39640;&#28216;&#25103;&#21644;AI&#20043;&#38388;&#30340;&#25968;&#25454;&#20256;&#36755;&#25928;&#29575;&#65292;&#20943;&#23569;&#20174;&#28216;&#25103;&#26381;&#21153;&#22120;&#25509;&#25910;&#20449;&#24687;&#25152;&#33457;&#36153;&#30340;&#26102;&#38388;&#12290;&#36825;&#26159;&#24456;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;&#22312;&#26684;&#26007;&#28216;&#25103;&#20013;&#23454;&#29616;AI&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#38656;&#35201;AI&#22312;&#30701;&#26102;&#38388;&#20869;&#36873;&#25321;&#35201;&#25191;&#34892;&#30340;&#21160;&#20316;&#12290;DareFightingICE&#24179;&#21488;&#24050;&#32463;&#38598;&#25104;&#20102;Py4J&#65292;&#20801;&#35768;&#24320;&#21457;&#20154;&#21592;&#20351;&#29992;Python&#21019;&#24314;AI&#12290;&#28982;&#32780;&#65292;Py4J&#22788;&#29702;&#22823;&#37327;&#25968;&#25454;&#30340;&#25928;&#29575;&#36739;&#20302;&#65292;&#23548;&#33268;&#24310;&#36831;&#36807;&#39640;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;gRPC&#24456;&#36866;&#21512;&#20256;&#36755;&#22823;&#37327;&#25968;&#25454;&#12290;&#20026;&#20102;&#35780;&#20272;&#26032;&#30340;&#36890;&#20449;&#25509;&#21475;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;gRPC&#21644;Py4J&#30340;&#24310;&#36831;&#65292;&#20351;&#29992;&#30340;&#26159;&#19968;&#20010;&#22522;&#20110;&#35268;&#21017;&#30340;AI&#65292;&#35813;AI&#21457;&#36865;&#20102;&#19968;&#20010;ki
&lt;/p&gt;
&lt;p&gt;
This paper presents a new communication interface for the DareFightingICE platform, a Java-based fighting game focused on implementing AI for controlling a non-player character. The interface uses an open-source remote procedure call, gRPC to improve the efficiency of data transfer between the game and the AI, reducing the time spent on receiving information from the game server. This is important because the main challenge of implementing AI in a fighting game is the need for the AI to select an action to perform within a short response time. The DareFightingICE platform has been integrated with Py4J, allowing developers to create AIs using Python. However, Py4J is less efficient at handling large amounts of data, resulting in excessive latency. In contrast, gRPC is well-suited for transmitting large amounts of data. To evaluate the effectiveness of the new communication interface, we conducted an experiment comparing the latency of gRPC and Py4J, using a rule-based AI that sends a ki
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20351;&#29992;&#22810;&#39033;&#24335;&#20272;&#31639;&#22120;&#23454;&#29616;&#20102;&#38543;&#26426;&#36830;&#32493;&#36138;&#24515;&#31639;&#27861;&#30340; $(1-1/e) \approx 63\%$ &#30340;&#36817;&#20284;&#27604;&#25928;&#26524;&#65292;&#21516;&#26102;&#26174;&#33879;&#38477;&#20302;&#20102;&#25191;&#34892;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2303.09960</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#39033;&#24335;&#20272;&#31639;&#22120;&#30340;&#38543;&#26426;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Stochastic Submodular Maximization via Polynomial Estimators. (arXiv:2303.09960v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20351;&#29992;&#22810;&#39033;&#24335;&#20272;&#31639;&#22120;&#23454;&#29616;&#20102;&#38543;&#26426;&#36830;&#32493;&#36138;&#24515;&#31639;&#27861;&#30340; $(1-1/e) \approx 63\%$ &#30340;&#36817;&#20284;&#27604;&#25928;&#26524;&#65292;&#21516;&#26102;&#26174;&#33879;&#38477;&#20302;&#20102;&#25191;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22312;&#32447;&#23398;&#20064;&#12289;&#22242;&#38431;&#32452;&#24314;&#12289;&#35774;&#26045;&#36873;&#22336;&#12289;&#24433;&#21709;&#26368;&#22823;&#21270;&#12289;&#20027;&#21160;&#23398;&#20064;&#21644;&#24863;&#30693;&#30446;&#26631;&#20989;&#25968;&#31561;&#39046;&#22495;&#20013;&#33258;&#28982;&#32780;&#28982;&#20986;&#29616;&#30340;&#24102;&#26377;&#19968;&#33324;&#25311;&#38453;&#32422;&#26463;&#30340;&#38543;&#26426;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#38024;&#23545;&#19968;&#31867;&#30001;&#26410;&#30693;&#20998;&#24067;&#19979;&#30340;&#23376;&#27169;&#20989;&#25968;&#26399;&#26395;&#23450;&#20041;&#30340;&#38543;&#26426;&#26368;&#22823;&#21270;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#36825;&#31181;&#21333;&#35843;&#20989;&#25968;&#65292;&#20351;&#29992;&#22810;&#39033;&#24335;&#26799;&#24230;&#20272;&#31639;&#22120;&#30340;&#38543;&#26426;&#36830;&#32493;&#36138;&#24515;&#31639;&#27861;&#21487;&#20197;&#36798;&#21040; $(1-1/e) \approx 63\%$ &#30340;&#36817;&#20284;&#27604;&#65288;&#26399;&#26395;&#20540;&#65289;&#65292;&#21516;&#26102;&#25105;&#20204;&#20063;&#35777;&#26126;&#20102;&#20351;&#29992;&#22810;&#39033;&#24335;&#20272;&#31639;&#22120;&#20195;&#26367;&#20808;&#21069;&#37319;&#29992;&#37319;&#26679;&#30340;&#31639;&#27861;&#33021;&#22815;&#28040;&#38500;&#19968;&#20123;&#38543;&#26426;&#22240;&#32032;&#24182;&#26174;&#33879;&#38477;&#20302;&#25191;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study stochastic submodular maximization problems with general matroid constraints, that naturally arise in online learning, team formation, facility location, influence maximization, active learning and sensing objective functions. In other words, we focus on maximizing submodular functions that are defined as expectations over a class of submodular functions with an unknown distribution. We show that for monotone functions of this form, the stochastic continuous greedy algorithm attains an approximation ratio (in expectation) arbitrarily close to $(1-1/e) \approx 63\%$ using a polynomial estimation of the gradient. We argue that using this polynomial estimator instead of the prior art that uses sampling eliminates a source of randomness and experimentally reduces execution time.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GNNFormer&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#32467;&#21512;&#30340;&#22522;&#20110;&#22270;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#32454;&#32990;&#30149;&#29702;&#23398;&#25253;&#21578;&#29983;&#25104;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26126;&#30830;&#23545;&#30149;&#29702;&#22270;&#20687;&#20013;&#30340;&#32454;&#32990;&#20043;&#38388;&#30340;&#32467;&#26500;&#20449;&#24687;&#36827;&#34892;&#24314;&#27169;&#30340;&#25253;&#21578;&#29983;&#25104;&#26041;&#27861;&#65292;&#24182;&#22312;&#35821;&#35328;&#29983;&#25104;&#25351;&#26631;&#21644;&#35786;&#26029;&#20934;&#30830;&#24230;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2303.09956</link><description>&lt;p&gt;
GNNFormer&#65306;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#32454;&#32990;&#30149;&#29702;&#23398;&#25253;&#21578;&#29983;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
GNNFormer: A Graph-based Framework for Cytopathology Report Generation. (arXiv:2303.09956v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09956
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GNNFormer&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#32467;&#21512;&#30340;&#22522;&#20110;&#22270;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#32454;&#32990;&#30149;&#29702;&#23398;&#25253;&#21578;&#29983;&#25104;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26126;&#30830;&#23545;&#30149;&#29702;&#22270;&#20687;&#20013;&#30340;&#32454;&#32990;&#20043;&#38388;&#30340;&#32467;&#26500;&#20449;&#24687;&#36827;&#34892;&#24314;&#27169;&#30340;&#25253;&#21578;&#29983;&#25104;&#26041;&#27861;&#65292;&#24182;&#22312;&#35821;&#35328;&#29983;&#25104;&#25351;&#26631;&#21644;&#35786;&#26029;&#20934;&#30830;&#24230;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#32990;&#30149;&#29702;&#23398;&#25253;&#21578;&#30340;&#33258;&#21160;&#29983;&#25104;&#26159;&#30149;&#29702;&#22270;&#20687;&#26631;&#20934;&#21270;&#26816;&#39564;&#30340;&#24517;&#35201;&#27493;&#39588;&#65292;&#20294;&#26159;&#25163;&#21160;&#25776;&#20889;&#35814;&#32454;&#25253;&#21578;&#20250;&#32473;&#30149;&#29702;&#23398;&#23478;&#24102;&#26469;&#27785;&#37325;&#30340;&#24037;&#20316;&#36127;&#25285;&#12290;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#65292;&#19968;&#20123;&#29616;&#26377;&#30340;&#24037;&#20316;&#30740;&#31350;&#20102;&#21033;&#29992;&#26368;&#21021;&#38024;&#23545;&#33258;&#28982;&#22270;&#20687;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#24212;&#29992;&#20110;&#32454;&#32990;&#30149;&#29702;&#22270;&#20687;&#30340;&#22270;&#20687;&#25551;&#36848;&#29983;&#25104;&#26694;&#26550;&#65292;&#33258;&#21160;&#29983;&#25104;&#32454;&#32990;&#30149;&#29702;&#23398;&#25253;&#21578;&#12290;&#36825;&#20123;&#24037;&#20316;&#30340;&#19968;&#20010;&#20849;&#21516;&#32570;&#28857;&#26159;&#23427;&#20204;&#27809;&#26377;&#26126;&#30830;&#22320;&#23545;&#32454;&#32990;&#20043;&#38388;&#30340;&#32467;&#26500;&#20449;&#24687;&#36827;&#34892;&#24314;&#27169;&#65292;&#32780;&#36825;&#26159;&#30149;&#29702;&#22270;&#20687;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#24182;&#20026;&#35786;&#26029;&#25552;&#20379;&#20102;&#37325;&#35201;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#30340;&#26694;&#26550;GNNFormer&#65292;&#26080;&#32541;&#22320;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;Transformer&#38598;&#25104;&#21040;&#21516;&#19968;&#26694;&#26550;&#20013;&#65292;&#29992;&#20110;&#32454;&#32990;&#30149;&#29702;&#23398;&#25253;&#21578;&#29983;&#25104;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;GNNFormer&#26159;&#31532;&#19968;&#20010;&#26126;&#30830;&#23545;&#30149;&#29702;&#22270;&#20687;&#20013;&#30340;&#32454;&#32990;&#20043;&#38388;&#30340;&#32467;&#26500;&#20449;&#24687;&#36827;&#34892;&#24314;&#27169;&#30340;&#25253;&#21578;&#29983;&#25104;&#26041;&#27861;&#12290;&#23427;&#36824;&#33021;&#22815;&#21516;&#26102;&#36731;&#26494;&#22788;&#29702;&#32454;&#32990;&#32423;&#20998;&#31867;&#21644;&#25253;&#21578;&#32423;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;GNNFormer&#22312;&#35821;&#35328;&#29983;&#25104;&#25351;&#26631;&#21644;&#35786;&#26029;&#20934;&#30830;&#24230;&#26041;&#38754;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cytopathology report generation is a necessary step for the standardized examination of pathology images. However, manually writing detailed reports brings heavy workloads for pathologists. To improve efficiency, some existing works have studied automatic generation of cytopathology reports, mainly by applying image caption generation frameworks with visual encoders originally proposed for natural images. A common weakness of these works is that they do not explicitly model the structural information among cells, which is a key feature of pathology images and provides significant information for making diagnoses. In this paper, we propose a novel graph-based framework called GNNFormer, which seamlessly integrates graph neural network (GNN) and Transformer into the same framework, for cytopathology report generation. To the best of our knowledge, GNNFormer is the first report generation method that explicitly models the structural information among cells in pathology images. It also eff
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;mCPT&#27169;&#22411;&#29992;&#20110;&#22810;&#35821;&#35328;&#30340;&#12289;&#22810;&#26631;&#31614;&#30340;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#30340;&#26694;&#26550;&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#22312;&#35199;&#29677;&#29273;&#35821;&#21644;&#20854;&#20182;8&#31181;&#35821;&#35328;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25104;&#32489;&#12290;&#35813;&#26041;&#26696;&#37319;&#29992;&#20102;&#22522;&#20110;&#22810;&#35821;&#35328;&#21464;&#21387;&#22120;&#30340;&#39044;&#35757;&#32451;&#31243;&#24207;&#65292;&#20351;&#29992;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.09901</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;3&#19978;&#30340;mCPT&#65306;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#26694;&#26550;&#26816;&#27979;&#30340;&#22810;&#35821;&#35328;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
mCPT at SemEval-2023 Task 3: Multilingual Label-Aware Contrastive Pre-Training of Transformers for Few- and Zero-shot Framing Detection. (arXiv:2303.09901v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;mCPT&#27169;&#22411;&#29992;&#20110;&#22810;&#35821;&#35328;&#30340;&#12289;&#22810;&#26631;&#31614;&#30340;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#30340;&#26694;&#26550;&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#22312;&#35199;&#29677;&#29273;&#35821;&#21644;&#20854;&#20182;8&#31181;&#35821;&#35328;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25104;&#32489;&#12290;&#35813;&#26041;&#26696;&#37319;&#29992;&#20102;&#22522;&#20110;&#22810;&#35821;&#35328;&#21464;&#21387;&#22120;&#30340;&#39044;&#35757;&#32451;&#31243;&#24207;&#65292;&#20351;&#29992;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38646;&#26679;&#26412;&#30340;&#35199;&#29677;&#29273;&#35821;&#26694;&#26550;&#26816;&#27979;&#20219;&#21153;&#30340;&#33719;&#32988;&#31995;&#32479;&#65292;&#24182;&#22312;&#21478;&#22806;&#20843;&#31181;&#35821;&#35328;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25104;&#32489;&#12290;&#26694;&#26550;&#26816;&#27979;&#20219;&#21153;&#30340;&#25361;&#25112;&#22312;&#20110;&#22312;&#21482;&#26377;&#23569;&#37327;&#25110;&#38646;&#20010;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#35782;&#21035;&#19968;&#32452;14&#20010;&#26694;&#26550;&#65292;&#21363;&#22810;&#35821;&#35328;&#22810;&#26631;&#31614;&#30340;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#35774;&#32622;&#12290;&#25105;&#20204;&#24320;&#21457;&#30340;&#35299;&#20915;&#26041;&#26696;&#37319;&#29992;&#20102;&#22522;&#20110;&#22810;&#35821;&#35328;&#21464;&#21387;&#22120;&#30340;&#39044;&#35757;&#32451;&#31243;&#24207;&#65292;&#20351;&#29992;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#12290;&#38500;&#20102;&#25551;&#36848;&#31995;&#32479;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23884;&#20837;&#31354;&#38388;&#20998;&#26512;&#21644;&#28040;&#34701;&#30740;&#31350;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#31243;&#24207;&#22914;&#20309;&#25903;&#25345;&#26694;&#26550;&#26816;&#27979;&#20197;&#25512;&#36827;&#35745;&#31639;&#26694;&#26550;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the winning system for the zero-shot Spanish framing detection task, which also achieves competitive places in eight additional languages. The challenge of the framing detection task lies in identifying a set of 14 frames when only a few or zero samples are available, i.e., a multilingual multi-label few- or zero-shot setting. Our developed solution employs a pre-training procedure based on multilingual Transformers using a label-aware contrastive loss function. In addition to describing the system, we perform an embedding space analysis and ablation study to demonstrate how our pre-training procedure supports framing detection to advance computational framing analysis.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36716;&#23548;&#24335;&#38646;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#29983;&#25104;&#26410;&#30693;&#25968;&#25454;&#30340;&#35821;&#20041;&#23646;&#24615;&#65292;&#24182;&#23558;&#23427;&#20204;&#23548;&#20837;&#21040;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#23646;&#24615;&#19968;&#33268;&#30340;&#26410;&#30693;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.09849</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#20041;&#23646;&#24615;&#30340;&#36716;&#23548;&#24335;&#38646;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Exploiting Semantic Attributes for Transductive Zero-Shot Learning. (arXiv:2303.09849v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09849
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36716;&#23548;&#24335;&#38646;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#29983;&#25104;&#26410;&#30693;&#25968;&#25454;&#30340;&#35821;&#20041;&#23646;&#24615;&#65292;&#24182;&#23558;&#23427;&#20204;&#23548;&#20837;&#21040;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#23646;&#24615;&#19968;&#33268;&#30340;&#26410;&#30693;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#27867;&#21270;&#20174;&#24050;&#30693;&#31867;&#21035;&#23398;&#20064;&#21040;&#30340;&#35270;&#35273;&#29305;&#24449;&#21644;&#35821;&#20041;&#23646;&#24615;&#30340;&#20851;&#31995;&#26469;&#35782;&#21035;&#26410;&#30693;&#31867;&#21035;&#12290;&#26368;&#36817;&#30340;&#19968;&#31181;&#31216;&#20026;&#36716;&#23548;&#24335;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#33539;&#24335;&#65292;&#26356;&#36827;&#19968;&#27493;&#21033;&#29992;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19981;&#26631;&#35760;&#30340;&#26410;&#30693;&#25968;&#25454;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24635;&#26159;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20174;&#23646;&#24615;&#20013;&#21512;&#25104;&#26410;&#30693;&#29305;&#24449;&#65292;&#20197;&#20943;&#23569;&#23545;&#24050;&#30693;&#31867;&#21035;&#30340;&#20559;&#24046;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#24573;&#35270;&#20102;&#26410;&#26631;&#35760;&#30340;&#26410;&#30693;&#25968;&#25454;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#22240;&#27492;&#26080;&#27861;&#29983;&#25104;&#39640;&#20445;&#30495;&#30340;&#23646;&#24615;&#19968;&#33268;&#30340;&#26410;&#30693;&#29305;&#24449;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36716;&#23548;&#24335;&#38646;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#29983;&#25104;&#26410;&#30693;&#25968;&#25454;&#30340;&#35821;&#20041;&#23646;&#24615;&#65292;&#24182;&#23558;&#23427;&#20204;&#23548;&#20837;&#21040;&#29983;&#25104;&#36807;&#31243;&#20013;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#35757;&#32451;&#19968;&#20010;&#35821;&#20041;&#23646;&#24615;&#35299;&#30721;&#22120;&#65292;&#35813;&#35299;&#30721;&#22120;&#23398;&#20064;&#20174;&#35270;&#35273;&#29305;&#24449;&#21040;&#35821;&#20041;&#23646;&#24615;&#30340;&#26144;&#23556;&#12290;&#28982;&#21518;&#65292;&#20174;&#23646;&#24615;&#35299;&#30721;&#22120;&#20013;&#65292;&#25105;&#20204;&#33719;&#24471;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#30340;&#20266;&#23646;&#24615;&#65292;&#24182;&#23558;&#23427;&#20204;&#38598;&#25104;&#21040;&#29983;&#25104;&#30340;&#36807;&#31243;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot learning (ZSL) aims to recognize unseen classes by generalizing the relation between visual features and semantic attributes learned from the seen classes. A recent paradigm called transductive zero-shot learning further leverages unlabeled unseen data during training and has obtained impressive results. These methods always synthesize unseen features from attributes through a generative adversarial network to mitigate the bias towards seen classes. However, they neglect the semantic information in the unlabeled unseen data and thus fail to generate high-fidelity attribute-consistent unseen features. To address this issue, we present a novel transductive ZSL method that produces semantic attributes of the unseen data and imposes them on the generative process. In particular, we first train an attribute decoder that learns the mapping from visual features to semantic attributes. Then, from the attribute decoder, we obtain pseudo-attributes of unlabeled data and integrate them 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; DUDES &#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#21644;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#65292;&#20934;&#30830;&#22320;&#20272;&#31639;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21516;&#26102;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DUDES &#33021;&#22815;&#20934;&#30830;&#22320;&#25429;&#33719;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#19981;&#29306;&#29298;&#20998;&#21106;&#20219;&#21153;&#24615;&#33021;&#30340;&#21069;&#25552;&#19979;&#65292;&#20855;&#26377;&#26816;&#27979;&#38169;&#35823;&#20998;&#31867;&#23454;&#20363;&#21644;&#26816;&#27979;&#20998;&#24067;&#20043;&#22806;&#26679;&#26412;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.09843</link><description>&lt;p&gt;
DUDES: &#20351;&#29992;&#38598;&#25104;&#23398;&#20064;&#30340;&#28145;&#24230;&#19981;&#30830;&#23450;&#24615;&#33976;&#39311;&#22312;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
DUDES: Deep Uncertainty Distillation using Ensembles for Semantic Segmentation. (arXiv:2303.09843v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; DUDES &#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#21644;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#65292;&#20934;&#30830;&#22320;&#20272;&#31639;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21516;&#26102;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DUDES &#33021;&#22815;&#20934;&#30830;&#22320;&#25429;&#33719;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#19981;&#29306;&#29298;&#20998;&#21106;&#20219;&#21153;&#24615;&#33021;&#30340;&#21069;&#25552;&#19979;&#65292;&#20855;&#26377;&#26816;&#27979;&#38169;&#35823;&#20998;&#31867;&#23454;&#20363;&#21644;&#26816;&#27979;&#20998;&#24067;&#20043;&#22806;&#26679;&#26412;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#19988;&#24448;&#24448;&#36807;&#20110;&#33258;&#20449;&#65292;&#36825;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#65288;&#22914;&#33258;&#21160;&#39550;&#39542;&#12289;&#21307;&#30103;&#25104;&#20687;&#25110;&#23545;&#21487;&#38752;&#24615;&#35201;&#27714;&#36739;&#39640;&#30340;&#26426;&#22120;&#35270;&#35273;&#20219;&#21153;&#65289;&#20013;&#20250;&#24102;&#26469;&#20005;&#37325;&#38382;&#39064;&#12290;&#37327;&#21270;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#26159;&#24320;&#21457;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#27492;&#31867;&#24212;&#29992;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#20294;&#24403;&#21069;&#21487;&#29992;&#26041;&#27861;&#35745;&#31639;&#36127;&#25285;&#36739;&#22823;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026; Deep Uncertainty Distillation using Ensembles &#65288;DUDES&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#21487;&#38752;&#22320;&#20272;&#31639;&#19981;&#30830;&#23450;&#24615;&#12290;DUDES &#20351;&#29992;&#28145;&#24230;&#38598;&#25104;&#30340;&#30693;&#35782;&#33976;&#39311;&#65292;&#22312;&#20445;&#25345;&#31616;&#21333;&#26131;&#29992;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#21333;&#20010;&#21521;&#21069;&#20256;&#36882;&#20934;&#30830;&#22320;&#36817;&#20284;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DUDES &#22312;&#19981;&#29306;&#29298;&#20998;&#21106;&#20219;&#21153;&#30340;&#24615;&#33021;&#30340;&#21069;&#25552;&#19979;&#20934;&#30830;&#22320;&#25429;&#33719;&#20102;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#34920;&#29616;&#20986;&#35782;&#21035;&#38169;&#35823;&#20998;&#31867;&#23454;&#20363;&#21644;&#26816;&#27979;&#21040;&#20998;&#24067;&#20043;&#22806;&#26679;&#26412;&#30340;&#20986;&#33394;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks lack interpretability and tend to be overconfident, which poses a serious problem in safety-critical applications like autonomous driving, medical imaging, or machine vision tasks with high demands on reliability. Quantifying the predictive uncertainty is a promising endeavour to open up the use of deep neural networks for such applications. Unfortunately, current available methods are computationally expensive. In this work, we present a novel approach for efficient and reliable uncertainty estimation which we call Deep Uncertainty Distillation using Ensembles for Segmentation (DUDES). DUDES applies student-teacher distillation with a Deep Ensemble to accurately approximate predictive uncertainties with a single forward pass while maintaining simplicity and adaptability. Experimentally, DUDES accurately captures predictive uncertainties without sacrificing performance on the segmentation task and indicates impressive capabilities of identifying wrongly classified 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DORIC&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#39046;&#22495;&#23545;&#35805;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#21462;&#21160;&#35789;-&#23486;&#35821;&#23545;&#20197;&#36798;&#21040;&#28040;&#38500;&#19981;&#24517;&#35201;&#20449;&#24687;&#30340;&#30446;&#30340;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#39046;&#22495;&#25968;&#25454;&#38598;&#19978;&#30340;&#39640;&#31934;&#24230;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2303.09827</link><description>&lt;p&gt;
DORIC: &#36890;&#36807;&#20381;&#36182;&#35299;&#26512;&#36827;&#34892;&#39046;&#22495;&#40065;&#26834;&#24494;&#35843;&#30340;&#24320;&#25918;&#24847;&#22270;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
DORIC : Domain Robust Fine-Tuning for Open Intent Clustering through Dependency Parsing. (arXiv:2303.09827v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09827
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DORIC&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#39046;&#22495;&#23545;&#35805;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#21462;&#21160;&#35789;-&#23486;&#35821;&#23545;&#20197;&#36798;&#21040;&#28040;&#38500;&#19981;&#24517;&#35201;&#20449;&#24687;&#30340;&#30446;&#30340;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#39046;&#22495;&#25968;&#25454;&#38598;&#19978;&#30340;&#39640;&#31934;&#24230;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;Dialog System Technology Challenges 11&#65288;DSTC11&#65289;&#30340;&#31532;2&#36712;&#36947;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#24037;&#20316;&#12290;DSTC11-Track2&#26088;&#22312;&#20026;0-shot&#65292;&#36328;&#39046;&#22495;&#30340;&#24847;&#22270;&#38598;&#24402;&#32435;&#25552;&#20379;&#22522;&#20934;&#12290;&#22312;&#27809;&#26377;&#39046;&#22495;&#20869;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#38656;&#35201;&#24378;&#22823;&#30340;&#35805;&#35821;&#34920;&#31034;&#65292;&#21487;&#29992;&#20110;&#36328;&#39046;&#22495;&#24402;&#32435;&#29992;&#25143;&#24847;&#22270;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#21033;&#29992;&#22810;&#39046;&#22495;&#23545;&#35805;&#25968;&#25454;&#38598;&#26469;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#25552;&#21462;&#21160;&#35789;-&#23486;&#35821;&#23545;&#20197;&#28040;&#38500;&#19981;&#24517;&#35201;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20026;&#32858;&#31867;&#32467;&#26524;&#30340;&#21487;&#35299;&#37322;&#24615;&#29983;&#25104;&#27599;&#20010;&#32676;&#38598;&#30340;&#21517;&#31216;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#39046;&#22495;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20986;&#20102;&#20248;&#31168;&#30340;&#20934;&#30830;&#24615;&#21644;&#26631;&#20934;&#21270;&#20114;&#20449;&#24687;&#65288;NMI&#65289;&#24471;&#20998;&#65292;&#30456;&#36739;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#25105;&#20204;&#22312;&#31934;&#24230;&#24471;&#20998;&#19978;&#33719;&#24471;&#20102;&#31532;&#19977;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present our work on Track 2 in the Dialog System Technology Challenges 11 (DSTC11). DSTC11-Track2 aims to provide a benchmark for zero-shot, cross-domain, intent-set induction. In the absence of in-domain training dataset, robust utterance representation that can be used across domains is necessary to induce users' intentions. To achieve this, we leveraged a multi-domain dialogue dataset to fine-tune the language model and proposed extracting Verb-Object pairs to remove the artifacts of unnecessary information. Furthermore, we devised the method that generates each cluster's name for the explainability of clustered results. Our approach achieved 3rd place in the precision score and showed superior accuracy and normalized mutual information (NMI) score than the baseline model on various domain datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#29616;&#26377;&#30340;&#33258;&#21160;&#39550;&#39542;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#21253;&#25324;&#31649;&#36947;&#35268;&#21010;&#21644;&#31471;&#21040;&#31471;&#35268;&#21010;&#26041;&#27861;&#12290;&#22312;&#25361;&#25112;&#21644;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#25552;&#20379;&#20102;&#35752;&#35770;&#65292;&#26377;&#21161;&#20110;&#20026;&#26234;&#33021;&#27773;&#36710;&#30340;&#21457;&#23637;&#25552;&#20379;&#26356;&#22909;&#30340;&#35268;&#21010;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.09824</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36335;&#24452;&#35268;&#21010;&#65306;&#29616;&#29366;&#19982;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Path Planning for Autonomous Driving: The State of the Art and Perspectives. (arXiv:2303.09824v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#29616;&#26377;&#30340;&#33258;&#21160;&#39550;&#39542;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#21253;&#25324;&#31649;&#36947;&#35268;&#21010;&#21644;&#31471;&#21040;&#31471;&#35268;&#21010;&#26041;&#27861;&#12290;&#22312;&#25361;&#25112;&#21644;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#25552;&#20379;&#20102;&#35752;&#35770;&#65292;&#26377;&#21161;&#20110;&#20026;&#26234;&#33021;&#27773;&#36710;&#30340;&#21457;&#23637;&#25552;&#20379;&#26356;&#22909;&#30340;&#35268;&#21010;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#27773;&#36710;&#30001;&#20110;&#25552;&#39640;&#30340;&#20415;&#21033;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#28508;&#22312;&#30340;&#21830;&#19994;&#20215;&#20540;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#20294;&#30001;&#20110;&#21508;&#31181;&#38382;&#39064;&#65292;&#22914;&#23433;&#20840;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#35268;&#21010;&#26041;&#27861;&#30340;&#27867;&#21270;&#31561;&#38480;&#21046;&#65292;&#23427;&#20204;&#30340;&#37096;&#32626;&#20173;&#23616;&#38480;&#20110;&#23567;&#35268;&#27169;&#39564;&#35777;&#38454;&#27573;&#12290;&#26412;&#25991;&#26088;&#22312;&#32508;&#36848;&#26368;&#20808;&#36827;&#30340;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#21253;&#25324;&#31649;&#36947;&#35268;&#21010;&#21644;&#31471;&#21040;&#31471;&#35268;&#21010;&#26041;&#27861;&#12290;&#38024;&#23545;&#31649;&#36947;&#26041;&#27861;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#36873;&#21462;&#31639;&#27861;&#30340;&#27010;&#36848;&#65292;&#24182;&#35752;&#35770;&#20102;&#25193;&#23637;&#21644;&#20248;&#21270;&#26426;&#21046;&#65307;&#38024;&#23545;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#26412;&#25991;&#24378;&#35843;&#22521;&#35757;&#21644;&#39564;&#35777;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#25361;&#25112;&#21644;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#26377;&#21161;&#20110;&#20026;&#26234;&#33021;&#27773;&#36710;&#30340;&#21457;&#23637;&#25552;&#20379;&#26356;&#22909;&#30340;&#35268;&#21010;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent vehicles (IVs) have attracted wide attention thanks to the augmented convenience, safety advantages, and potential commercial value. Although a few of autonomous driving unicorns assert that IVs will be commercially deployable by 2025, their deployment is still restricted to small-scale validation due to various issues, among which safety, reliability, and generalization of planning methods are prominent concerns. Precise computation of control commands or trajectories by planning methods remains a prerequisite for IVs, owing to perceptual imperfections under complex environments, which pose an obstacle to the successful commercialization of IVs. This paper aims to review state-of-the-art planning methods, including pipeline planning and end-to-end planning methods. In terms of pipeline methods, a survey of selecting algorithms is provided along with a discussion of the expansion and optimization mechanisms, whereas in end-to-end methods, the training approaches and verific
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Transformer&#21644;Ensemble&#26041;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#38463;&#35821;&#24694;&#24847;&#35328;&#35770;&#30340;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#22810;&#25968;&#34920;&#20915;&#30340;&#38598;&#25104;&#26041;&#27861;&#20855;&#26377;&#26368;&#20339;&#25928;&#26524;&#65292;&#20854;&#22312;&#27979;&#35797;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#20026;0.86&#65292;F1&#20998;&#25968;&#20026;0.60&#12290;</title><link>http://arxiv.org/abs/2303.09823</link><description>&lt;p&gt;
Transformers&#21644;Ensemble&#26041;&#27861;&#65306;&#38463;&#35821;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#30340;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Transformers and Ensemble methods: A solution for Hate Speech Detection in Arabic languages. (arXiv:2303.09823v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Transformer&#21644;Ensemble&#26041;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#38463;&#35821;&#24694;&#24847;&#35328;&#35770;&#30340;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#22810;&#25968;&#34920;&#20915;&#30340;&#38598;&#25104;&#26041;&#27861;&#20855;&#26377;&#26368;&#20339;&#25928;&#26524;&#65292;&#20854;&#22312;&#27979;&#35797;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#20026;0.86&#65292;F1&#20998;&#25968;&#20026;0.60&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#21442;&#21152;CERIST NLP&#25361;&#25112;&#36187;2022&#20013;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#20849;&#20139;&#20219;&#21153;&#30340;&#23454;&#39564;&#36807;&#31243;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;6&#20010;Transformer&#27169;&#22411;&#21450;&#20854;&#32452;&#21512;&#30340;&#24615;&#33021;&#65292;&#24182;&#20351;&#29992;&#20102;2&#31181;&#38598;&#25104;&#26041;&#27861;&#12290;&#22312;&#20116;&#25240;&#20132;&#21449;&#39564;&#35777;&#30340;&#35757;&#32451;&#38598;&#19978;&#65292;&#22522;&#20110;&#22810;&#25968;&#34920;&#20915;&#30340;&#38598;&#25104;&#26041;&#27861;&#33719;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;&#22312;&#27979;&#35797;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#20026;F1&#20998;&#25968;&#20026;0.60&#65292;&#20934;&#30830;&#24615;&#20026;0.86&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes our participation in the shared task of hate speech detection, which is one of the subtasks of the CERIST NLP Challenge 2022. Our experiments evaluate the performance of six transformer models and their combination using 2 ensemble approaches. The best results on the training set, in a five-fold cross validation scenario, were obtained by using the ensemble approach based on the majority vote. The evaluation of this approach on the test set resulted in an F1-score of 0.60 and an Accuracy of 0.86.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;DiffusionSeg&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#35299;&#20915;&#20102;&#26080;&#30417;&#30563;&#30446;&#26631;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#21028;&#21035;&#24615;&#38382;&#39064;&#65292;&#21253;&#21547;&#20004;&#20010;&#38454;&#27573;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#21512;&#25104;&#22823;&#37327;&#22270;&#20687;&#21644;Pixelwise&#22238;&#24402;&#26469;&#35299;&#20915;&#26631;&#35760;&#25968;&#25454;&#19981;&#36275;&#21644;&#29983;&#25104;&#12289;&#21028;&#21035;&#27169;&#22411;&#32467;&#26500;&#19981;&#21516;&#30340;&#38382;&#39064;&#12290;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.09813</link><description>&lt;p&gt;
DiffusionSeg&#65306;&#23558;&#25193;&#25955;&#25216;&#26415;&#24212;&#29992;&#20110;&#26080;&#30417;&#30563;&#30446;&#26631;&#21457;&#29616;&#20013;&#30340;&#33258;&#36866;&#24212;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
DiffusionSeg: Adapting Diffusion Towards Unsupervised Object Discovery. (arXiv:2303.09813v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;DiffusionSeg&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#35299;&#20915;&#20102;&#26080;&#30417;&#30563;&#30446;&#26631;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#21028;&#21035;&#24615;&#38382;&#39064;&#65292;&#21253;&#21547;&#20004;&#20010;&#38454;&#27573;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#21512;&#25104;&#22823;&#37327;&#22270;&#20687;&#21644;Pixelwise&#22238;&#24402;&#26469;&#35299;&#20915;&#26631;&#35760;&#25968;&#25454;&#19981;&#36275;&#21644;&#29983;&#25104;&#12289;&#21028;&#21035;&#27169;&#22411;&#32467;&#26500;&#19981;&#21516;&#30340;&#38382;&#39064;&#12290;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#27169;&#22411;&#20174;&#22823;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#29616;&#22312;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#12290;&#20316;&#20026;&#27969;&#34892;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#25193;&#25955;&#27169;&#22411;&#25429;&#25417;&#20102;&#20302;&#32423;&#35270;&#35273;&#30693;&#35782;&#21644;&#39640;&#32423;&#35821;&#20041;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36825;&#31181;&#30693;&#35782;&#24211;&#25193;&#25955;&#27169;&#22411;&#26469;&#36827;&#34892;&#20027;&#27969;&#21028;&#21035;&#24615;&#20219;&#21153;&#8212;&#8212;&#26080;&#30417;&#30563;&#30446;&#26631;&#21457;&#29616;&#65306;&#26174;&#30528;&#24615;&#20998;&#21106;&#21644;&#30446;&#26631;&#23450;&#20301;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29983;&#25104;&#21644;&#21028;&#21035;&#24335;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#19968;&#31181;&#32467;&#26500;&#19978;&#30340;&#24046;&#24322;&#65292;&#36825;&#38480;&#21046;&#20102;&#30452;&#25509;&#20351;&#29992;&#30340;&#21487;&#33021;&#24615;&#12290;&#27492;&#22806;&#65292;&#26174;&#24335;&#26631;&#35760;&#25968;&#25454;&#30340;&#32570;&#20047;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#26174;&#33879;&#38480;&#21046;&#20102;&#24615;&#33021;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DiffusionSeg&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#20004;&#20010;&#38454;&#27573;&#31574;&#30053;&#30340;&#26032;&#22411;&#21512;&#25104;-&#21033;&#29992;&#26694;&#26550;&#12290;&#20026;&#20102;&#20943;&#36731;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#31532;&#19968;&#20010;&#21512;&#25104;&#38454;&#27573;&#21512;&#25104;&#22823;&#37327;&#30340;&#22270;&#20687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;AttentionCut&#26469;&#33719;&#24471;&#25513;&#33180;&#12290;&#22312;&#31532;&#20108;&#20010;&#21033;&#29992;&#38454;&#27573;&#20013;&#65292;&#20026;&#20102;&#24357;&#34917;&#29983;&#25104;&#21644;&#21028;&#21035;&#24335;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Pixelwise&#22238;&#24402;&#65292;&#36890;&#36807;&#29289;&#20307;&#20301;&#32622;&#32422;&#26463;&#29983;&#25104;&#25193;&#25955;&#36807;&#31243;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#26080;&#30417;&#30563;&#30446;&#26631;&#21457;&#29616;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;&#21028;&#21035;&#24615;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from a large corpus of data, pre-trained models have achieved impressive progress nowadays. As popular generative pre-training, diffusion models capture both low-level visual knowledge and high-level semantic relations. In this paper, we propose to exploit such knowledgeable diffusion models for mainstream discriminative tasks, i.e., unsupervised object discovery: saliency segmentation and object localization. However, the challenges exist as there is one structural difference between generative and discriminative models, which limits the direct use. Besides, the lack of explicitly labeled data significantly limits performance in unsupervised settings. To tackle these issues, we introduce DiffusionSeg, one novel synthesis-exploitation framework containing two-stage strategies. To alleviate data insufficiency, we synthesize abundant images, and propose a novel training-free AttentionCut to obtain masks in the first synthesis stage. In the second exploitation stage, to bridge th
&lt;/p&gt;</description></item><item><title>TKN&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#23454;&#26102;&#35270;&#39057;&#39044;&#27979;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21463;&#38480;&#20449;&#24687;&#25552;&#21462;&#21644;&#24182;&#34892;&#39044;&#27979;&#26041;&#26696;&#26469;&#25552;&#21319;&#39044;&#27979;&#36807;&#31243;&#30340;&#36895;&#24230;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2303.09807</link><description>&lt;p&gt;
TKN&#65306;&#22522;&#20110;Transformer&#30340;&#23454;&#26102;&#35270;&#39057;&#20851;&#38190;&#28857;&#39044;&#27979;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
TKN: Transformer-based Keypoint Prediction Network For Real-time Video Prediction. (arXiv:2303.09807v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09807
&lt;/p&gt;
&lt;p&gt;
TKN&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#23454;&#26102;&#35270;&#39057;&#39044;&#27979;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21463;&#38480;&#20449;&#24687;&#25552;&#21462;&#21644;&#24182;&#34892;&#39044;&#27979;&#26041;&#26696;&#26469;&#25552;&#21319;&#39044;&#27979;&#36807;&#31243;&#30340;&#36895;&#24230;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#39044;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#24191;&#27867;&#29992;&#36884;&#30340;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26041;&#27861;&#36807;&#20110;&#24378;&#35843;&#20934;&#30830;&#24615;&#65292;&#24573;&#35270;&#20102;&#30001;&#20110;&#36807;&#20110;&#22797;&#26434;&#30340;&#27169;&#22411;&#32467;&#26500;&#32780;&#23548;&#33268;&#30340;&#36739;&#24930;&#30340;&#39044;&#27979;&#36895;&#24230;&#20197;&#21450;&#36807;&#22810;&#30340;&#20887;&#20313;&#20449;&#24687;&#23398;&#20064;&#21644;GPU&#20869;&#23384;&#28040;&#32791;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TKN&#65292;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#20851;&#38190;&#28857;&#39044;&#27979;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#21463;&#38480;&#20449;&#24687;&#25552;&#21462;&#21644;&#24182;&#34892;&#39044;&#27979;&#26041;&#26696;&#26469;&#25552;&#21319;&#39044;&#27979;&#36807;&#31243;&#30340;&#36895;&#24230;&#12290;TKN&#26159;&#25105;&#20204;&#30446;&#21069;&#25152;&#30693;&#30340;&#31532;&#19968;&#20010;&#23454;&#26102;&#35270;&#39057;&#39044;&#27979;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#24182;&#20445;&#25345;&#20854;&#20182;&#24615;&#33021;&#12290;&#22312;KTH&#21644;Human Action 3D&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;TKN&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video prediction is a complex time-series forecasting task with great potential in many use cases. However, conventional methods overemphasize accuracy while ignoring the slow prediction speed caused by complicated model structures that learn too much redundant information with excessive GPU memory consumption. Furthermore, conventional methods mostly predict frames sequentially (frame-by-frame) and thus are hard to accelerate. Consequently, valuable use cases such as real-time danger prediction and warning cannot achieve fast enough inference speed to be applicable in reality. Therefore, we propose a transformer-based keypoint prediction neural network (TKN), an unsupervised learning method that boost the prediction process via constrained information extraction and parallel prediction scheme. TKN is the first real-time video prediction solution to our best knowledge, while significantly reducing computation costs and maintaining other performance. Extensive experiments on KTH and Hum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;POI-MetaBlock&#30340;&#26032;&#27169;&#22359;&#65292;&#32467;&#21512;&#21306;&#22495;&#30340;&#21151;&#33021;&#24615;&#21644;&#20132;&#36890;&#29305;&#24449;&#65292;&#37319;&#29992;&#33258;&#27880;&#24847;&#21147;&#26550;&#26500;&#36827;&#34892;&#20132;&#36890;&#27969;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2303.09789</link><description>&lt;p&gt;
&#22522;&#20110;&#22478;&#24066;&#21306;&#22495;&#21151;&#33021;&#24341;&#23548;&#30340;&#20132;&#36890;&#27969;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Urban Regional Function Guided Traffic Flow Prediction. (arXiv:2303.09789v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;POI-MetaBlock&#30340;&#26032;&#27169;&#22359;&#65292;&#32467;&#21512;&#21306;&#22495;&#30340;&#21151;&#33021;&#24615;&#21644;&#20132;&#36890;&#29305;&#24449;&#65292;&#37319;&#29992;&#33258;&#27880;&#24847;&#21147;&#26550;&#26500;&#36827;&#34892;&#20132;&#36890;&#27969;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#27969;&#39044;&#27979;&#26159;&#26102;&#31354;&#20998;&#26512;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#36817;&#24180;&#26469;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#38500;&#20102;&#26102;&#31354;&#30456;&#20851;&#24615;&#65292;&#22478;&#24066;&#21306;&#22495;&#30340;&#21151;&#33021;&#24615;&#20063;&#22312;&#20132;&#36890;&#27969;&#39044;&#27979;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#21306;&#22495;&#21151;&#33021;&#23646;&#24615;&#30340;&#25506;&#32034;&#20027;&#35201;&#38598;&#20013;&#22312;&#28155;&#21152;&#39069;&#22806;&#30340;&#25299;&#25169;&#32467;&#26500;&#19978;&#65292;&#24573;&#30053;&#20102;&#21151;&#33021;&#23646;&#24615;&#23545;&#21306;&#22495;&#20132;&#36890;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;&#19982;&#29616;&#26377;&#20316;&#21697;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;POI-MetaBlock&#30340;&#26032;&#27169;&#22359;&#65292;&#21033;&#29992;&#27599;&#20010;&#21306;&#22495;&#30340;&#21151;&#33021;&#24615;&#65288;&#30001;&#20852;&#36259;&#28857;&#20998;&#24067;&#34920;&#31034;&#65289;&#20316;&#20026;&#20803;&#25968;&#25454;&#26469;&#36827;&#19968;&#27493;&#25366;&#25496;&#19981;&#21516;&#21151;&#33021;&#21306;&#22495;&#30340;&#19981;&#21516;&#20132;&#36890;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;POI-MetaBlock&#37319;&#29992;&#33258;&#27880;&#24847;&#21147;&#26550;&#26500;&#65292;&#32467;&#21512;POI&#21644;&#26102;&#38388;&#20449;&#24687;&#29983;&#25104;&#27599;&#20010;&#21306;&#22495;&#30340;&#21160;&#24577;&#27880;&#24847;&#21442;&#25968;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#21508;&#31181;&#21306;&#22495;&#20132;&#36890;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prediction of traffic flow is a challenging yet crucial problem in spatial-temporal analysis, which has recently gained increasing interest. In addition to spatial-temporal correlations, the functionality of urban areas also plays a crucial role in traffic flow prediction. However, the exploration of regional functional attributes mainly focuses on adding additional topological structures, ignoring the influence of functional attributes on regional traffic patterns. Different from the existing works, we propose a novel module named POI-MetaBlock, which utilizes the functionality of each region (represented by Point of Interest distribution) as metadata to further mine different traffic characteristics in areas with different functions. Specifically, the proposed POI-MetaBlock employs a self-attention architecture and incorporates POI and time information to generate dynamic attention parameters for each region, which enables the model to fit different traffic patterns of various ar
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#19988;&#26377;&#25928;&#30340;&#22270;&#32467;&#26500;&#23398;&#20064;&#26694;&#26550;SE-GSL&#65292;&#36890;&#36807;&#21033;&#29992;&#32467;&#26500;&#29109;&#21644;&#32534;&#30721;&#26641;&#20013;&#30340;&#23618;&#27425;&#32467;&#26500;&#26469;&#26368;&#22823;&#21270;&#23884;&#20837;&#20449;&#24687;&#20869;&#23481;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#22768;&#24930;&#26500;&#24314;&#26368;&#20248;&#32534;&#30721;&#26641;&#30340;&#26041;&#26696;&#12290;&#35813;&#26694;&#26550;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#26679;&#26412;&#30340;&#26426;&#21046;&#65292;&#36890;&#36807;&#33410;&#28857;&#32467;&#26500;&#29109;&#20998;&#24067;&#26469;&#24674;&#22797;&#22270;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2303.09778</link><description>&lt;p&gt;
SE-GSL&#65306;&#19968;&#31181;&#36890;&#36807;&#32467;&#26500;&#29109;&#20248;&#21270;&#23454;&#29616;&#36890;&#29992;&#26377;&#25928;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
SE-GSL: A General and Effective Graph Structure Learning Framework through Structural Entropy Optimization. (arXiv:2303.09778v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#19988;&#26377;&#25928;&#30340;&#22270;&#32467;&#26500;&#23398;&#20064;&#26694;&#26550;SE-GSL&#65292;&#36890;&#36807;&#21033;&#29992;&#32467;&#26500;&#29109;&#21644;&#32534;&#30721;&#26641;&#20013;&#30340;&#23618;&#27425;&#32467;&#26500;&#26469;&#26368;&#22823;&#21270;&#23884;&#20837;&#20449;&#24687;&#20869;&#23481;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#22768;&#24930;&#26500;&#24314;&#26368;&#20248;&#32534;&#30721;&#26641;&#30340;&#26041;&#26696;&#12290;&#35813;&#26694;&#26550;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#26679;&#26412;&#30340;&#26426;&#21046;&#65292;&#36890;&#36807;&#33410;&#28857;&#32467;&#26500;&#29109;&#20998;&#24067;&#26469;&#24674;&#22797;&#22270;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#23398;&#20064;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#12290; &#28982;&#32780;&#65292;&#23427;&#23481;&#26131;&#21463;&#21040;&#20302;&#36136;&#37327;&#21644;&#19981;&#21487;&#38752;&#32467;&#26500;&#30340;&#24433;&#21709;&#65292;&#36825;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#20013;&#26159;&#24120;&#24577;&#32780;&#19981;&#26159;&#20363;&#22806;&#12290;&#29616;&#26377;&#30340;&#22270;&#32467;&#26500;&#23398;&#20064;&#26694;&#26550;&#20173;&#28982;&#32570;&#20047;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#32467;&#26500;&#29109;&#21644;&#32534;&#30721;&#26641;&#20013;&#25277;&#35937;&#30340;&#22270;&#23618;&#27425;&#32467;&#26500;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;GSL&#26694;&#26550;SE-GSL&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#32500;&#32467;&#26500;&#29109;&#26469;&#26368;&#22823;&#21270;&#23884;&#20837;&#20449;&#24687;&#20869;&#23481;&#65292;&#24403;&#36741;&#21161;&#37051;&#22495;&#23646;&#24615;&#34987;&#34701;&#21512;&#20197;&#22686;&#24378;&#21407;&#22987;&#22270;&#26102;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;&#26368;&#20248;&#32534;&#30721;&#26641;&#30340;&#26032;&#26041;&#26696;&#65292;&#20197;&#22312;&#20998;&#23618;&#25277;&#35937;&#20013;&#26368;&#23567;&#21270;&#22270;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#22122;&#38899;&#65292;&#21516;&#26102;&#30830;&#20445;&#36866;&#24403;&#30340;&#31038;&#21306;&#21010;&#20998;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#26679;&#26412;&#30340;&#26426;&#21046;&#65292;&#36890;&#36807;&#33410;&#28857;&#32467;&#26500;&#29109;&#20998;&#24067;&#26469;&#24674;&#22797;&#22270;&#32467;&#26500;&#12290;&#23427;&#22686;&#21152;&#20102;&#26356;&#22823;&#19981;&#30830;&#23450;&#24615;&#30340;&#33410;&#28857;&#20043;&#38388;&#30340;&#36830;&#36890;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are de facto solutions to structural data learning. However, it is susceptible to low-quality and unreliable structure, which has been a norm rather than an exception in real-world graphs. Existing graph structure learning (GSL) frameworks still lack robustness and interpretability. This paper proposes a general GSL framework, SE-GSL, through structural entropy and the graph hierarchy abstracted in the encoding tree. Particularly, we exploit the one-dimensional structural entropy to maximize embedded information content when auxiliary neighbourhood attributes are fused to enhance the original graph. A new scheme of constructing optimal encoding trees is proposed to minimize the uncertainty and noises in the graph whilst assuring proper community partition in hierarchical abstraction. We present a novel sample-based mechanism for restoring the graph structure via node structural entropy distribution. It increases the connectivity among nodes with larger unce
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#21463;&#24433;&#21709;&#30340;&#21033;&#30410;&#30456;&#20851;&#26041;&#23545;&#20110;&#20248;&#20808;&#32771;&#34385;&#26377;&#38480;&#20303;&#25151;&#36164;&#28304;&#30340; AI &#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#30340;&#30475;&#27861;&#65292;&#21457;&#29616;&#30456;&#20851;&#21033;&#30410;&#30456;&#20851;&#26041;&#21487;&#25552;&#20379;&#20855;&#20307;&#21644;&#20851;&#38190;&#24615;&#30340;&#21453;&#39304;&#65292;&#21363;&#20351;&#20182;&#20204;&#27809;&#26377; AI &#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2303.09743</link><description>&lt;p&gt;
&#29702;&#35299;&#21069;&#32447;&#24037;&#20316;&#32773;&#21644;&#26080;&#23478;&#21487;&#24402;&#20154;&#21592;&#23545;&#26080;&#23478;&#32773;&#26381;&#21153;&#20013;&#20351;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
Understanding Frontline Workers' and Unhoused Individuals' Perspectives on AI Used in Homeless Services. (arXiv:2303.09743v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09743
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#21463;&#24433;&#21709;&#30340;&#21033;&#30410;&#30456;&#20851;&#26041;&#23545;&#20110;&#20248;&#20808;&#32771;&#34385;&#26377;&#38480;&#20303;&#25151;&#36164;&#28304;&#30340; AI &#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#30340;&#30475;&#27861;&#65292;&#21457;&#29616;&#30456;&#20851;&#21033;&#30410;&#30456;&#20851;&#26041;&#21487;&#25552;&#20379;&#20855;&#20307;&#21644;&#20851;&#38190;&#24615;&#30340;&#21453;&#39304;&#65292;&#21363;&#20351;&#20182;&#20204;&#27809;&#26377; AI &#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65288;ADS&#65289;&#22312;&#26080;&#23478;&#21487;&#24402;&#32773;&#26381;&#21153;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#25105;&#20204;&#23545;&#20110;&#28041;&#21450;&#35813;&#25216;&#26415;&#30340;&#21033;&#30410;&#30456;&#20851;&#26041;&#30340;&#24895;&#26395;&#21644;&#20851;&#27880;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#20102;&#35299;&#21463;&#24433;&#21709;&#21033;&#30410;&#30456;&#20851;&#26041;&#23545;&#19968;&#20010;&#20248;&#20808;&#32771;&#34385;&#26377;&#38480;&#20303;&#25151;&#36164;&#28304;&#30340; ADS &#30340;&#30475;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102; AI &#29983;&#21629;&#21608;&#26399;&#28459;&#30011;&#26495;&#35774;&#35745;&#26041;&#27861;&#30340;&#25913;&#36827;&#29256;&#26412;&#36827;&#34892;&#21453;&#39304;&#25628;&#38598;&#21644;&#35774;&#35745;&#24605;&#36335;&#25910;&#38598;&#36328; AI &#31995;&#32479;&#35774;&#35745;&#30340;&#21508;&#20010;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#20174;&#27599;&#26085;&#25805;&#20316; ADS &#30340;&#21439;&#24037;&#20316;&#20154;&#21592;&#65292;&#30452;&#25509;&#21463;&#21040; ADS &#24433;&#21709;&#30340;&#26381;&#21153;&#25552;&#20379;&#32773;&#20197;&#21450;&#35813;&#22320;&#21306;&#30340;&#26080;&#23478;&#21487;&#24402;&#20010;&#20307;&#20013;&#33719;&#21462;&#21453;&#39304;&#12290;&#25105;&#20204;&#30340;&#21442;&#19982;&#32773;&#20998;&#20139;&#20102;&#23545; AI &#31995;&#32479;&#30340;&#25972;&#20307;&#30446;&#26631;&#12289;&#29305;&#23450;&#27169;&#22411;&#35774;&#35745;&#36873;&#25321;&#12289;&#25968;&#25454;&#38598;&#36873;&#25321;&#20197;&#21450;&#22312;&#37096;&#32626;&#20013;&#20351;&#29992;&#30340;&#20851;&#27880;&#21644;&#35774;&#35745;&#24314;&#35758;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#21033;&#30410;&#30456;&#20851;&#26041;&#21363;&#20351;&#27809;&#26377; AI &#30693;&#35782;&#65292;&#20063;&#33021;&#25552;&#20379;&#23545;&#20110; AI &#31995;&#32479;&#35774;&#35745;&#21644;&#37096;&#32626;&#30340;&#20855;&#20307;&#21644;&#20851;&#38190;&#24615;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen growing adoption of AI-based decision-support systems (ADS) in homeless services, yet we know little about stakeholder desires and concerns surrounding their use. In this work, we aim to understand impacted stakeholders' perspectives on a deployed ADS that prioritizes scarce housing resources. We employed AI lifecycle comicboarding, an adapted version of the comicboarding method, to elicit stakeholder feedback and design ideas across various components of an AI system's design. We elicited feedback from county workers who operate the ADS daily, service providers whose work is directly impacted by the ADS, and unhoused individuals in the region. Our participants shared concerns and design suggestions around the AI system's overall objective, specific model design choices, dataset selection, and use in deployment. Our findings demonstrate that stakeholders, even without AI knowledge, can provide specific and critical feedback on an AI system's design and deployment
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21160;&#24577;&#32467;&#26500;&#21098;&#26525;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#20248;&#21270;&#27599;&#23618;&#20869;&#36890;&#36947;&#21098;&#26525;&#30340;&#26368;&#20248;&#31890;&#24230;&#65292;&#20174;&#32780;&#26356;&#21152;&#32039;&#20945;&#21644;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2303.09736</link><description>&lt;p&gt;
&#21387;&#32553;CNN&#30340;&#21160;&#24577;&#32467;&#26500;&#21098;&#26525;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dynamic Structure Pruning for Compressing CNNs. (arXiv:2303.09736v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21160;&#24577;&#32467;&#26500;&#21098;&#26525;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#20248;&#21270;&#27599;&#23618;&#20869;&#36890;&#36947;&#21098;&#26525;&#30340;&#26368;&#20248;&#31890;&#24230;&#65292;&#20174;&#32780;&#26356;&#21152;&#32039;&#20945;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21098;&#26525;&#26159;&#19968;&#31181;&#21387;&#32553;&#21644;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#23613;&#31649;&#22312;&#23454;&#38469;&#21152;&#36895;&#21644;&#30828;&#20214;&#20860;&#23481;&#24615;&#26041;&#38754;&#28388;&#27874;&#21644;&#36890;&#36947;&#21098;&#26525;&#26159;&#20248;&#20110;&#20854;&#20182;&#32467;&#26500;&#21098;&#26525;&#26041;&#27861;&#30340;&#65292;&#20294;&#32454;&#31890;&#24230;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#22914;&#20869;&#36890;&#36947;&#21098;&#26525;&#65292;&#26377;&#26395;&#20135;&#29983;&#26356;&#32039;&#20945;&#21644;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#30340;&#32593;&#32476;&#12290;&#29616;&#26377;&#30340;&#20869;&#36890;&#36947;&#21098;&#26525;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#38745;&#24577;&#21644;&#25163;&#21160;&#21046;&#23450;&#30340;&#21098;&#26525;&#31890;&#24230;&#65292;&#36825;&#30041;&#32473;&#20102;&#20248;&#21270;&#21098;&#26525;&#24615;&#33021;&#30340;&#31354;&#38388;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#21098;&#26525;&#26041;&#27861;&#65292;&#31216;&#20026;&#21160;&#24577;&#32467;&#26500;&#21098;&#26525;&#65292;&#26469;&#30830;&#23450;&#20869;&#36890;&#36947;&#21098;&#26525;&#30340;&#26368;&#20248;&#21098;&#26525;&#31890;&#24230;&#12290;&#19982;&#29616;&#26377;&#30340;&#20869;&#36890;&#36947;&#21098;&#26525;&#26041;&#27861;&#19981;&#21516;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26102;&#33258;&#21160;&#20248;&#21270;&#27599;&#23618;&#30340;&#21160;&#24577;&#21098;&#26525;&#31890;&#24230;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#21306;&#20998;&#30340;&#32452;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Structure pruning is an effective method to compress and accelerate neural networks. While filter and channel pruning are preferable to other structure pruning methods in terms of realistic acceleration and hardware compatibility, pruning methods with a finer granularity, such as intra-channel pruning, are expected to be capable of yielding more compact and computationally efficient networks. Typical intra-channel pruning methods utilize a static and hand-crafted pruning granularity due to a large search space, which leaves room for improvement in their pruning performance. In this work, we introduce a novel structure pruning method, termed as dynamic structure pruning, to identify optimal pruning granularities for intra-channel pruning. In contrast to existing intra-channel pruning methods, the proposed method automatically optimizes dynamic pruning granularities in each layer while training deep neural networks. To achieve this, we propose a differentiable group learning method desig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38450;&#24481;&#27169;&#22359;&#65292;&#29992;&#20110;&#20445;&#25252;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#22522;&#20110;LiDAR&#30340;&#29289;&#20307;&#26816;&#27979;&#22120;&#20813;&#36973;&#20986;&#29616;&#25915;&#20987;&#12290;&#35813;&#27169;&#22359;&#26681;&#25454;&#23616;&#37096;&#37096;&#20998;&#30340;&#29289;&#20307;&#24615;&#20302;&#19979;&#26469;&#28040;&#38500;&#20266;&#36896;&#30340;&#38556;&#30861;&#29289;&#65292;&#24182;&#36890;&#36807;&#21464;&#20307;&#36830;&#20307;&#32593;&#32476;&#36827;&#34892;&#35782;&#21035;&#12290;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#24179;&#22343;&#28857;&#23545;&#28857;F1&#20998;&#25968;&#20026;96.2&#65285;&#12290;</title><link>http://arxiv.org/abs/2303.09731</link><description>&lt;p&gt;
&#39537;&#37034;&#8220;Wraith&#8221;&#65306;&#20445;&#25252;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#22522;&#20110;LiDAR&#30340;&#29289;&#20307;&#26816;&#27979;&#22120;&#20813;&#36973;&#20986;&#29616;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Exorcising ''Wraith'': Protecting LiDAR-based Object Detector in Automated Driving System from Appearing Attacks. (arXiv:2303.09731v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38450;&#24481;&#27169;&#22359;&#65292;&#29992;&#20110;&#20445;&#25252;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#22522;&#20110;LiDAR&#30340;&#29289;&#20307;&#26816;&#27979;&#22120;&#20813;&#36973;&#20986;&#29616;&#25915;&#20987;&#12290;&#35813;&#27169;&#22359;&#26681;&#25454;&#23616;&#37096;&#37096;&#20998;&#30340;&#29289;&#20307;&#24615;&#20302;&#19979;&#26469;&#28040;&#38500;&#20266;&#36896;&#30340;&#38556;&#30861;&#29289;&#65292;&#24182;&#36890;&#36807;&#21464;&#20307;&#36830;&#20307;&#32593;&#32476;&#36827;&#34892;&#35782;&#21035;&#12290;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#24179;&#22343;&#28857;&#23545;&#28857;F1&#20998;&#25968;&#20026;96.2&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20381;&#36182;&#20110;3D&#29289;&#20307;&#26816;&#27979;&#22120;&#20174;LiDAR&#28857;&#20113;&#20013;&#35782;&#21035;&#21487;&#33021;&#30340;&#38556;&#30861;&#29289;&#65292;&#20294;&#26159;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#20351;&#29992;&#23569;&#37327;&#30340;&#20266;&#36896;&#28857;&#65288;&#21363;&#20986;&#29616;&#25915;&#20987;&#65289;&#22312;&#39044;&#27979;&#32467;&#26524;&#20013;&#20266;&#36896;&#19981;&#23384;&#22312;&#30340;&#36710;&#36742;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#25514;&#26045;&#36890;&#36807;&#21435;&#38500;&#32479;&#35745;&#19978;&#30340;&#31163;&#32676;&#20540;&#26469;&#38024;&#23545;&#29305;&#23450;&#25915;&#20987;&#25110;&#36890;&#36807;&#39044;&#23450;&#20041;&#30340;&#21551;&#21457;&#24335;&#35268;&#21017;&#36827;&#34892;&#38450;&#24481;&#12290;&#20026;&#20102;&#26356;&#20840;&#38754;&#22320;&#20943;&#36731;&#25915;&#20987;&#65292;&#25105;&#20204;&#39318;&#20808;&#31995;&#32479;&#22320;&#26816;&#26597;&#20102;&#26368;&#36817;&#20986;&#29616;&#30340;&#25915;&#20987;&#26426;&#21046;&#65306;&#20182;&#20204;&#30340;&#20849;&#21516;&#24369;&#28857;&#22312;&#20110;&#20266;&#36896;&#30340;&#38556;&#30861;&#29289;(i)&#19982;&#30495;&#23454;&#38556;&#30861;&#29289;&#30456;&#27604;&#20855;&#26377;&#26126;&#26174;&#30340;&#23616;&#37096;&#24046;&#24322;&#65292;(ii)&#36829;&#21453;&#20102;&#28145;&#24230;&#21644;&#28857;&#23494;&#24230;&#20043;&#38388;&#30340;&#29289;&#29702;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21363;&#25554;&#21363;&#29992;&#30340;&#38450;&#24481;&#27169;&#22359;&#65292;&#23427;&#21487;&#20197;&#22312;&#21463;&#36807;&#35757;&#32451;&#30340;&#22522;&#20110;LiDAR&#30340;&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#19968;&#20391;&#24037;&#20316;&#65292;&#20197;&#28040;&#38500;&#20266;&#36896;&#30340;&#38556;&#30861;&#29289;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#23616;&#37096;&#37096;&#20998;&#30340;&#29289;&#20307;&#24615;&#36739;&#20302;&#65292;&#21363;&#23427;&#23646;&#20110;&#23454;&#38469;&#29289;&#20307;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#35774;&#35745;&#30340;&#26680;&#24515;&#26159;&#21464;&#20307;&#36830;&#20307;&#32593;&#32476;&#65288;VSN&#65289;&#65292;&#23427;&#23558;&#26816;&#27979;&#22120;&#30340;&#21407;&#22987;&#21644;&#23631;&#34109;&#36755;&#20837;&#37117;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#20026;&#35782;&#21035;&#20551;&#28857;&#29983;&#25104;&#25104;&#23545;&#30456;&#20284;&#24615;&#22270;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#20808;&#21069;&#25915;&#20987;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#38450;&#24481;&#20986;&#29616;&#25915;&#20987;&#65292;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#28857;&#23545;&#28857;F1&#20998;&#25968;&#20026;96.2&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated driving systems rely on 3D object detectors to recognize possible obstacles from LiDAR point clouds. However, recent works show the adversary can forge non-existent cars in the prediction results with a few fake points (i.e., appearing attack). By removing statistical outliers, existing defenses are however designed for specific attacks or biased by predefined heuristic rules. Towards more comprehensive mitigation, we first systematically inspect the mechanism of recent appearing attacks: Their common weaknesses are observed in crafting fake obstacles which (i) have obvious differences in the local parts compared with real obstacles and (ii) violate the physical relation between depth and point density. In this paper, we propose a novel plug-and-play defensive module which works by side of a trained LiDAR-based object detector to eliminate forged obstacles where a major proportion of local parts have low objectness, i.e., to what degree it belongs to a real object. At the cor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.09716</link><description>&lt;p&gt;
&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#25919;&#31574;&#36845;&#20195;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A New Policy Iteration Algorithm For Reinforcement Learning in Zero-Sum Markov Games. (arXiv:2303.09716v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#34987;&#35270;&#20026;&#20855;&#26377;&#20004;&#20010;&#38454;&#27573;: &#23398;&#20064;&#38454;&#27573;&#21644;&#35268;&#21010;&#38454;&#27573;&#12290;&#22312;&#26631;&#20934;MDPs&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#20351;&#29992;&#20215;&#20540;&#36845;&#20195;&#25110;&#31574;&#30053;&#36845;&#20195;&#26469;&#35299;&#20915;&#23398;&#20064;&#38382;&#39064;&#12290;&#20294;&#22312;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#24773;&#20917;&#19979;&#65292;&#27809;&#26377;&#26377;&#25928;&#30340;&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#65292;&#20197;&#21069;&#30340;&#23581;&#35797;&#37117;&#26377;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31574;&#30053;&#36845;&#20195;&#21464;&#20307;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many model-based reinforcement learning (RL) algorithms can be viewed as having two phases that are iteratively implemented: a learning phase where the model is approximately learned and a planning phase where the learned model is used to derive a policy. In the case of standard MDPs, the learning problem can be solved using either value iteration or policy iteration. However, in the case of zero-sum Markov games, there is no efficient policy iteration algorithm; e.g., it has been shown in Hansen et al. (2013) that one has to solve Omega(1/(1-alpha)) MDPs, where alpha is the discount factor, to implement the only known convergent version of policy iteration. Another algorithm for Markov zero-sum games, called naive policy iteration, is easy to implement but is only provably convergent under very restrictive assumptions. Prior attempts to fix naive policy iteration algorithm have several limitations. Here, we show that a simple variant of naive policy iteration for games converges, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#38142;&#25509;&#25512;&#33616;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#23545;&#31038;&#20132;&#32593;&#32476;&#28436;&#21270;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#25512;&#33616;&#38142;&#25509;&#26377;&#20196;&#20154;&#24778;&#35766;&#30340;&#24310;&#36831;&#21644;&#38388;&#25509;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.09700</link><description>&lt;p&gt;
&#25512;&#33616;&#38142;&#25509;&#30340;&#24310;&#26102;&#21644;&#38388;&#25509;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Delayed and Indirect Impacts of Link Recommendations. (arXiv:2303.09700v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#38142;&#25509;&#25512;&#33616;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#23545;&#31038;&#20132;&#32593;&#32476;&#28436;&#21270;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#25512;&#33616;&#38142;&#25509;&#26377;&#20196;&#20154;&#24778;&#35766;&#30340;&#24310;&#36831;&#21644;&#38388;&#25509;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#38142;&#25509;&#23545;&#31038;&#20132;&#32593;&#32476;&#30340;&#24433;&#21709;&#24456;&#38590;&#35780;&#20272;&#65292;&#36804;&#20170;&#20026;&#27490;&#30740;&#31350;&#21463;&#38480;&#20110;&#26377;&#38480;&#30340;&#29615;&#22659;&#12290;&#35266;&#23519;&#24615;&#30740;&#31350;&#21463;&#38480;&#20110;&#23427;&#20204;&#25152;&#33021;&#22238;&#31572;&#30340;&#22240;&#26524;&#38382;&#39064;&#30340;&#31181;&#31867;&#65292;&#22825;&#30495;&#30340; A/B &#27979;&#35797;&#24120;&#24120;&#20250;&#30001;&#20110;&#26410;&#32771;&#34385;&#21040;&#30340;&#32593;&#32476;&#24178;&#25200;&#32780;&#23548;&#33268;&#20559;&#35265;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#27169;&#25311;&#29615;&#22659;&#20013;&#30340;&#35780;&#20272;&#32463;&#24120;&#23616;&#38480;&#20110;&#38745;&#24577;&#32593;&#32476;&#27169;&#22411;&#65292;&#19981;&#32771;&#34385;&#38142;&#25509;&#25512;&#33616;&#21644;&#26377;&#26426;&#32593;&#32476;&#28436;&#21270;&#20043;&#38388;&#30340;&#28508;&#22312;&#21453;&#39304;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30740;&#31350;&#20102;&#25512;&#33616;&#23545;&#31038;&#20132;&#32593;&#32476;&#30340;&#24433;&#21709;&#12290;&#37319;&#29992;&#27169;&#25311;&#26041;&#27861;&#65292;&#32771;&#34385;&#19968;&#20010;&#26174;&#24335;&#30340;&#21160;&#24577;&#24418;&#25104;&#27169;&#22411;&#8212;&#8212;&#33879;&#21517;&#30340;Jackson-Rogers&#27169;&#22411;&#30340;&#25193;&#23637;&#8212;&#8212;&#24182;&#30740;&#31350;&#38142;&#25509;&#25512;&#33616;&#22914;&#20309;&#38543;&#26102;&#38388;&#24433;&#21709;&#32593;&#32476;&#28436;&#21270;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#38142;&#25509;&#25512;&#33616;&#23545;&#32593;&#32476;&#32467;&#26500;&#23646;&#24615;&#26377;&#20196;&#20154;&#24778;&#35766;&#30340;&#24310;&#36831;&#21644;&#38388;&#25509;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The impacts of link recommendations on social networks are challenging to evaluate, and so far they have been studied in limited settings. Observational studies are restricted in the kinds of causal questions they can answer and naive A/B tests often lead to biased evaluations due to unaccounted network interference. Furthermore, evaluations in simulation settings are often limited to static network models that do not take into account the potential feedback loops between link recommendation and organic network evolution. To this end, we study the impacts of recommendations on social networks in dynamic settings. Adopting a simulation-based approach, we consider an explicit dynamic formation model -- an extension of the celebrated Jackson-Rogers model -- and investigate how link recommendations affect network evolution over time. Empirically, we find that link recommendations have surprising delayed and indirect effects on the structural properties of networks. Specifically, we find th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26694;&#26550; DiGeo&#65292;&#23398;&#20064;&#20960;&#20309;&#24863;&#30693;&#29305;&#24449;&#26469;&#23454;&#29616;&#36328;&#31867;&#21035;&#38388;&#30340;&#20998;&#31163;&#21644;&#31867;&#20869;&#32039;&#23494;&#24615;&#12290;&#36890;&#36807;&#24341;&#23548;&#29305;&#24449;&#32858;&#31867;&#30340;&#20998;&#31163;&#65292;&#37319;&#29992;&#33258;&#36866;&#24212;&#30340;&#31867;&#29305;&#23450;&#38388;&#36317;&#65292;&#26412;&#25991;&#26041;&#27861;&#26377;&#25928;&#25552;&#21319;&#20102;&#24191;&#20041;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.09674</link><description>&lt;p&gt;
DiGeo: &#21028;&#21035;&#20960;&#20309;&#24863;&#30693;&#23398;&#20064;&#29992;&#20110;&#24191;&#20041;&#23569;&#26679;&#26412;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
DiGeo: Discriminative Geometry-Aware Learning for Generalized Few-Shot Object Detection. (arXiv:2303.09674v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26694;&#26550; DiGeo&#65292;&#23398;&#20064;&#20960;&#20309;&#24863;&#30693;&#29305;&#24449;&#26469;&#23454;&#29616;&#36328;&#31867;&#21035;&#38388;&#30340;&#20998;&#31163;&#21644;&#31867;&#20869;&#32039;&#23494;&#24615;&#12290;&#36890;&#36807;&#24341;&#23548;&#29305;&#24449;&#32858;&#31867;&#30340;&#20998;&#31163;&#65292;&#37319;&#29992;&#33258;&#36866;&#24212;&#30340;&#31867;&#29305;&#23450;&#38388;&#36317;&#65292;&#26412;&#25991;&#26041;&#27861;&#26377;&#25928;&#25552;&#21319;&#20102;&#24191;&#20041;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#20041;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#26088;&#22312;&#23454;&#29616;&#22522;&#30784;&#31867;&#21035;&#21644;&#26032;&#39062;&#31867;&#21035;&#30340;&#31934;&#20934;&#26816;&#27979;&#12290;&#29616;&#26377;&#26041;&#27861;&#22312;&#22686;&#24378;&#23569;&#26679;&#26412;&#27867;&#21270;&#30340;&#21516;&#26102;&#29306;&#29298;&#20102;&#22522;&#30784;&#31867;&#21035;&#30340;&#24615;&#33021;&#65292;&#25110;&#32773;&#22312;&#22522;&#30784;&#31867;&#21035;&#26816;&#27979;&#31934;&#24230;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#26032;&#39062;&#31867;&#21035;&#30340;&#36866;&#24212;&#24615;&#26041;&#38754;&#26377;&#38480;&#30340;&#25552;&#39640;&#12290;&#26412;&#25991;&#25351;&#20986;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#21407;&#22240;&#22312;&#20110;&#25152;&#26377;&#31867;&#21035;&#30340;&#19981;&#36275;&#30340;&#21028;&#21035;&#29305;&#24449;&#23398;&#20064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26694;&#26550; DiGeo&#65292;&#23398;&#20064;&#20960;&#20309;&#24863;&#30693;&#29305;&#24449;&#26469;&#23454;&#29616;&#36328;&#31867;&#21035;&#38388;&#30340;&#20998;&#31163;&#21644;&#31867;&#20869;&#32039;&#23494;&#24615;&#12290;&#20026;&#20102;&#25351;&#23548;&#29305;&#24449;&#32858;&#31867;&#30340;&#20998;&#31163;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#31163;&#32447;&#30340;&#31616;&#21333;&#20809;&#35282;&#32039;&#26694;&#26550;&#65288;ETF&#65289;&#20998;&#31867;&#22120;&#65292;&#20854;&#26435;&#37325;&#20316;&#20026;&#31867;&#20013;&#24515;&#24182;&#26368;&#22823;&#12289;&#22343;&#31561;&#30340;&#20998;&#24320;&#12290;&#20026;&#20102;&#32039;&#23494;&#27599;&#20010;&#31867;&#21035;&#30340;&#32858;&#31867;&#65292;&#25105;&#20204;&#23558;&#33258;&#36866;&#24212;&#30340;&#31867;&#29305;&#23450;&#38388;&#36317;&#28155;&#21152;&#21040;&#20998;&#31867;&#25439;&#22833;&#20013;&#65292;&#24182;&#40723;&#21169;&#25509;&#36817;&#31867;&#20013;&#24515;&#30340;&#29305;&#24449;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#22312;&#20844;&#20849;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalized few-shot object detection aims to achieve precise detection on both base classes with abundant annotations and novel classes with limited training data. Existing approaches enhance few-shot generalization with the sacrifice of base-class performance, or maintain high precision in base-class detection with limited improvement in novel-class adaptation. In this paper, we point out the reason is insufficient Discriminative feature learning for all of the classes. As such, we propose a new training framework, DiGeo, to learn Geometry-aware features of inter-class separation and intra-class compactness. To guide the separation of feature clusters, we derive an offline simplex equiangular tight frame (ETF) classifier whose weights serve as class centers and are maximally and equally separated. To tighten the cluster for each class, we include adaptive class-specific margins into the classification loss and encourage the features close to the class centers. Experimental studies on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#32452;&#36127;&#36131;&#20219;&#30340;&#32676;&#20307;&#20998;&#26512;&#35774;&#35745;&#25351;&#21335;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;TribalGram&#24037;&#20855;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#21487;&#35270;&#21270;&#26469;&#23545;&#32676;&#20307;&#20998;&#26512;&#36827;&#34892;&#25512;&#26029;&#35780;&#20272;&#12289;&#27169;&#22411;&#35299;&#37322;&#12289;&#25968;&#25454;&#21327;&#20316;&#21644;&#29702;&#35299;&#65292;&#20197;&#22686;&#24378;&#23545;&#20998;&#26512;&#32676;&#20307;&#30340;&#28145;&#21051;&#29702;&#35299;&#65292;&#24182;&#38450;&#27490;&#22240;&#21051;&#26495;&#21360;&#35937;&#21644;&#36807;&#24230;&#27010;&#25324;&#23548;&#33268;&#30340;&#28508;&#22312;&#21361;&#23475;&#12290;</title><link>http://arxiv.org/abs/2303.09664</link><description>&lt;p&gt;
&#29992;TribalGram&#23545;&#32676;&#20307;&#38388;&#24046;&#24322;&#36827;&#34892;&#20851;&#38190;&#26816;&#26597;&#65306;&#37096;&#33853;&#20998;&#26512;&#30340;&#25209;&#21028;&#24615;&#35270;&#23519;
&lt;/p&gt;
&lt;p&gt;
Tribe or Not? Critical Inspection of Group Differences Using TribalGram. (arXiv:2303.09664v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#32452;&#36127;&#36131;&#20219;&#30340;&#32676;&#20307;&#20998;&#26512;&#35774;&#35745;&#25351;&#21335;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;TribalGram&#24037;&#20855;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#21487;&#35270;&#21270;&#26469;&#23545;&#32676;&#20307;&#20998;&#26512;&#36827;&#34892;&#25512;&#26029;&#35780;&#20272;&#12289;&#27169;&#22411;&#35299;&#37322;&#12289;&#25968;&#25454;&#21327;&#20316;&#21644;&#29702;&#35299;&#65292;&#20197;&#22686;&#24378;&#23545;&#20998;&#26512;&#32676;&#20307;&#30340;&#28145;&#21051;&#29702;&#35299;&#65292;&#24182;&#38450;&#27490;&#22240;&#21051;&#26495;&#21360;&#35937;&#21644;&#36807;&#24230;&#27010;&#25324;&#23548;&#33268;&#30340;&#28508;&#22312;&#21361;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#21644;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#30340;&#20852;&#36215;&#65292;&#32676;&#20307;&#20998;&#26512;&#21644;&#32676;&#20307;&#23618;&#38754;&#30340;&#20998;&#26512;&#22312;&#21253;&#25324;&#25919;&#31574;&#21046;&#23450;&#21644;&#30452;&#25509;&#33829;&#38144;&#22312;&#20869;&#30340;&#35768;&#22810;&#39046;&#22495;&#20013;&#24471;&#21040;&#20102;&#36234;&#26469;&#36234;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#65292;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#32479;&#35745;&#20449;&#24687;&#21487;&#33021;&#20026;&#19968;&#20010;&#32676;&#20307;&#30340;&#20849;&#21516;&#29305;&#24449;&#25552;&#20379;&#35265;&#35299;&#65307;&#32780;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#65292;&#32676;&#20307;&#23618;&#38754;&#30340;&#20998;&#26512;&#21487;&#33021;&#20250;&#23548;&#33268;&#21051;&#26495;&#21360;&#35937;&#21644;&#31995;&#32479;&#24615;&#21387;&#36843;&#12290;&#20998;&#26512;&#24037;&#20855;&#22914;&#20309;&#20419;&#36827;&#26356;&#21152;&#26377;&#24847;&#35782;&#30340;&#32676;&#20307;&#20998;&#26512;&#36807;&#31243;&#65311;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#32452;&#36127;&#36131;&#20219;&#30340;&#32676;&#20307;&#20998;&#26512;&#35774;&#35745;&#25351;&#21335;&#65292;&#20197;&#38416;&#26126;&#32676;&#20307;&#24046;&#24322;&#30340;&#38656;&#35201;&#21644;&#38450;&#27490;&#23545;&#32676;&#20307;&#30340;&#36807;&#24230;&#27010;&#25324;&#12290;&#36981;&#24490;&#36825;&#20123;&#35774;&#35745;&#25351;&#21335;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;TribalGram&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#21487;&#35270;&#21270;&#26469;&#25552;&#20379;&#25512;&#26029;&#35780;&#20272;&#12289;&#27169;&#22411;&#35299;&#37322;&#12289;&#25968;&#25454;&#21327;&#20316;&#21644;&#29702;&#35299;&#30340;&#35270;&#35273;&#20998;&#26512;&#24037;&#20855;&#12290;&#36890;&#36807;&#19982;&#39046;&#22495;&#19987;&#23478;&#30340;&#35775;&#35848;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35774;&#35745;&#21644;&#24037;&#20855;&#22914;&#20309;&#24102;&#26469;&#23545;&#20998;&#26512;&#32676;&#20307;&#30340;&#26356;&#28145;&#20837;&#29702;&#35299;&#65292;&#24182;&#38450;&#27490;&#30001;&#20110;&#21051;&#26495;&#21360;&#35937;&#21644;&#36807;&#24230;&#27010;&#25324;&#32780;&#24102;&#26469;&#30340;&#28508;&#22312;&#21361;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rise of AI and data mining techniques, group profiling and group-level analysis have been increasingly used in many domains including policy making and direct marketing. In some cases, the statistics extracted from data may provide insights to a group's shared characteristics; in others, the group-level analysis can lead to problems including stereotyping and systematic oppression. How can analytic tools facilitate a more conscientious process in group analysis? In this work, we identify a set of accountable group analytics design guidelines to explicate the needs for group differentiation and preventing overgeneralization of a group. Following the design guidelines, we develop TribalGram, a visual analytic suite that leverages interpretable machine learning algorithms and visualization to offer inference assessment, model explanation, data corroboration, and sense-making. Through the interviews with domain experts, we showcase how our design and tools can bring a richer under
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ESCAPE&#65292;&#19968;&#20010;&#20132;&#20114;&#24335;&#35270;&#35273;&#20998;&#26512;&#31995;&#32479;&#65292;&#21033;&#29992;&#20154;&#26426;&#20132;&#20114;&#24110;&#21161;&#29992;&#25143;&#21457;&#29616;&#24182;&#20462;&#27491;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#31995;&#32479;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2303.09657</link><description>&lt;p&gt;
ESCAPE&#65306;&#36890;&#36807;&#20132;&#20114;&#24335;&#35270;&#35273;&#20998;&#26512;&#28040;&#38500;&#26426;&#22120;&#30340;&#30450;&#21306;&#31995;&#32479;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
ESCAPE: Countering Systematic Errors from Machine's Blind Spots via Interactive Visual Analysis. (arXiv:2303.09657v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ESCAPE&#65292;&#19968;&#20010;&#20132;&#20114;&#24335;&#35270;&#35273;&#20998;&#26512;&#31995;&#32479;&#65292;&#21033;&#29992;&#20154;&#26426;&#20132;&#20114;&#24110;&#21161;&#29992;&#25143;&#21457;&#29616;&#24182;&#20462;&#27491;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#31995;&#32479;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#27169;&#22411;&#23398;&#20064;&#25512;&#24191;&#25968;&#25454;&#21644;&#30446;&#26631;&#31867;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#20154;&#21592;&#36234;&#26469;&#36234;&#22810;&#22320;&#35266;&#23519;&#21040;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#24456;&#23481;&#26131;&#22312;AI&#24212;&#29992;&#31243;&#24207;&#20013;&#23548;&#33268;&#31995;&#32479;&#38169;&#35823;&#65292;&#36825;&#31181;&#29616;&#35937;&#31216;&#20026;AI&#30450;&#21306;&#12290;&#24403;&#27169;&#22411;&#20351;&#29992;&#35757;&#32451;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#65288;&#20363;&#22914;&#65292;&#29483;/&#29399;&#20998;&#31867;&#65289;&#65292;&#20854;&#20013;&#37325;&#35201;&#27169;&#24335;&#65288;&#20363;&#22914;&#65292;&#40657;&#29483;&#65289;&#32570;&#22833;&#25110;&#21608;&#36793;/&#19981;&#33391;&#27169;&#24335;&#65288;&#20363;&#22914;&#65292;&#33609;&#22320;&#32972;&#26223;&#30340;&#29399;&#65289;&#20250;&#35823;&#23548;&#21040;&#26576;&#20010;&#31867;&#26102;&#65292;&#20250;&#20986;&#29616;&#36825;&#20123;&#30450;&#21306;&#12290;&#29978;&#33267;&#26356;&#22797;&#26434;&#30340;&#25216;&#26415;&#20063;&#19981;&#33021;&#20445;&#35777;&#25429;&#33719;&#12289;&#29702;&#35299;&#21644;&#38450;&#27490;&#20266;&#20851;&#32852;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ESCAPE&#30340;&#35270;&#35273;&#20998;&#26512;&#31995;&#32479;&#65292;&#23427;&#20419;&#36827;&#20102;&#28040;&#38500;&#31995;&#32479;&#35823;&#24046;&#30340;&#20154;&#26426;&#20132;&#20114;&#24037;&#20316;&#27969;&#31243;&#12290;&#36890;&#36807;&#20801;&#35768;&#20154;&#31867;&#29992;&#25143;&#36731;&#26494;&#26816;&#26597;&#34394;&#20551;&#20851;&#32852;&#65292;&#35813;&#31995;&#32479;&#26377;&#21161;&#20110;&#29992;&#25143;&#33258;&#21457;&#22320;&#35782;&#21035;&#19982;&#38169;&#35823;&#20998;&#31867;&#30456;&#20851;&#30340;&#27010;&#24565;&#24182;&#35780;&#20272;&#32531;&#35299;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classification models learn to generalize the associations between data samples and their target classes. However, researchers have increasingly observed that machine learning practice easily leads to systematic errors in AI applications, a phenomenon referred to as AI blindspots. Such blindspots arise when a model is trained with training samples (e.g., cat/dog classification) where important patterns (e.g., black cats) are missing or periphery/undesirable patterns (e.g., dogs with grass background) are misleading towards a certain class. Even more sophisticated techniques cannot guarantee to capture, reason about, and prevent the spurious associations. In this work, we propose ESCAPE, a visual analytic system that promotes a human-in-the-loop workflow for countering systematic errors. By allowing human users to easily inspect spurious associations, the system facilitates users to spontaneously recognize concepts associated misclassifications and evaluate mitigation strategies that ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#35299;&#20915;&#21608;&#26399;&#24615;MDP&#20013;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#22235;&#31181;&#31639;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;PUCRL2&#65292;PUCRLB&#65292;U-PUCRL2&#21644;U-PUCRLB&#12290;PUCRLB&#34920;&#29616;&#26356;&#22909;&#65292;&#19988;&#20854;&#36951;&#25022;&#38543;&#21608;&#26399;$N$&#30340;&#21464;&#21270;&#20026;$O(\sqrt{N})$&#12290;</title><link>http://arxiv.org/abs/2303.09629</link><description>&lt;p&gt;
&#21608;&#26399;&#24615;MDP&#20013;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Reinforcement Learning in Periodic MDP. (arXiv:2303.09629v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#35299;&#20915;&#21608;&#26399;&#24615;MDP&#20013;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#22235;&#31181;&#31639;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;PUCRL2&#65292;PUCRLB&#65292;U-PUCRL2&#21644;U-PUCRLB&#12290;PUCRLB&#34920;&#29616;&#26356;&#22909;&#65292;&#19988;&#20854;&#36951;&#25022;&#38543;&#21608;&#26399;$N$&#30340;&#21464;&#21270;&#20026;$O(\sqrt{N})$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21608;&#26399;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#31181;&#29305;&#27530;&#30340;&#38750;&#24179;&#31283;MDP&#65292;&#20854;&#20013;&#29366;&#24577;&#36716;&#31227;&#27010;&#29575;&#21644;&#22870;&#21169;&#20989;&#25968;&#37117;&#20250;&#21608;&#26399;&#24615;&#21464;&#21270;&#65292;&#22312;&#24179;&#22343;&#22870;&#21169;&#26368;&#22823;&#21270;&#30340;&#35774;&#32622;&#19979;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#29366;&#24577;&#31354;&#38388;&#20013;&#28155;&#21152;&#21608;&#26399;&#32034;&#24341;&#26469;&#23558;&#38382;&#39064;&#24402;&#32467;&#20026;&#38745;&#24577;MDP&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21608;&#26399;&#24615;&#19978;&#32622;&#20449;&#21306;&#38388;&#24378;&#21270;&#23398;&#20064;-2&#65288;PUCRL2&#65289;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;PUCRL2&#30340;&#36951;&#25022;&#20540;&#38543;&#21608;&#26399;$N$&#32447;&#24615;&#21464;&#21270;&#65292;&#24182;&#19988;&#38543;&#30528;&#26102;&#38480;&#38271;&#24230;$T$&#21576;$\mathcal{O}(\sqrt{Tlog T})$&#30340;&#21464;&#21270;&#12290;&#21033;&#29992;&#22686;&#24191;MDP&#30340;&#36716;&#31227;&#30697;&#38453;&#30340;&#31232;&#30095;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#21478;&#19968;&#20010;&#31639;&#27861;PUCRLB&#65292;&#23427;&#22312;&#36951;&#25022;&#65288;&#21608;&#26399;&#30340;$O(\sqrt{N})$&#20381;&#36182;&#20851;&#31995;&#65289;&#21644;&#32463;&#39564;&#34920;&#29616;&#26041;&#38754;&#37117;&#27604;PUCRL2&#26356;&#20986;&#33394;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20004;&#20010;&#31639;&#27861;U-PUCRL2&#21644;U-PUCRLB&#65292;&#29992;&#20110;&#29615;&#22659;&#20013;&#25193;&#23637;&#19981;&#30830;&#23450;&#24615;&#65292;&#20854;&#20013;&#21608;&#26399;&#26410;&#30693;&#65292;&#20294;&#24050;&#30693;&#19968;&#32452;&#20505;&#36873;&#21608;&#26399;&#12290;&#25968;&#20540;&#32467;&#26524;&#35777;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
We study learning in periodic Markov Decision Process (MDP), a special type of non-stationary MDP where both the state transition probabilities and reward functions vary periodically, under the average reward maximization setting. We formulate the problem as a stationary MDP by augmenting the state space with the period index, and propose a periodic upper confidence bound reinforcement learning-2 (PUCRL2) algorithm. We show that the regret of PUCRL2 varies linearly with the period $N$ and as $\mathcal{O}(\sqrt{Tlog T})$ with the horizon length $T$. Utilizing the information about the sparsity of transition matrix of augmented MDP, we propose another algorithm PUCRLB which enhances upon PUCRL2, both in terms of regret ($O(\sqrt{N})$ dependency on period) and empirical performance. Finally, we propose two other algorithms U-PUCRL2 and U-PUCRLB for extended uncertainty in the environment in which the period is unknown but a set of candidate periods are known. Numerical results demonstrate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#25351;&#23548;&#24615;&#35270;&#35273;&#32534;&#36753;&#12290;&#36890;&#36807;&#25910;&#38598;&#34987;&#32534;&#36753;&#22270;&#20687;&#30340;&#20154;&#31867;&#21453;&#39304;&#65292;&#24182;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#25429;&#25417;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#21487;&#20197;&#32531;&#35299;&#25968;&#25454;&#38480;&#21046;&#25152;&#24102;&#26469;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.09618</link><description>&lt;p&gt;
HIVE&#65306;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#25351;&#23548;&#24615;&#35270;&#35273;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
HIVE: Harnessing Human Feedback for Instructional Visual Editing. (arXiv:2303.09618v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#25351;&#23548;&#24615;&#35270;&#35273;&#32534;&#36753;&#12290;&#36890;&#36807;&#25910;&#38598;&#34987;&#32534;&#36753;&#22270;&#20687;&#30340;&#20154;&#31867;&#21453;&#39304;&#65292;&#24182;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#25429;&#25417;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#21487;&#20197;&#32531;&#35299;&#25968;&#25454;&#38480;&#21046;&#25152;&#24102;&#26469;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;&#20154;&#31867;&#21453;&#39304;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#23545;&#40784;&#21040;&#20154;&#31867;&#20559;&#22909;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20551;&#35774;&#65292;&#26368;&#20808;&#36827;&#30340;&#25351;&#23548;&#24615;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#65292;&#20854;&#36755;&#20986;&#22522;&#20110;&#36755;&#20837;&#22270;&#20687;&#21644;&#32534;&#36753;&#25351;&#20196;&#65292;&#21516;&#26679;&#21487;&#20197;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#21463;&#30410;&#65292;&#22240;&#20026;&#20854;&#36755;&#20986;&#21487;&#33021;&#19981;&#31526;&#21512;&#29992;&#25143;&#30340;&#27491;&#30830;&#25351;&#20196;&#21644;&#20559;&#22909;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#25351;&#23548;&#24615;&#35270;&#35273;&#32534;&#36753;&#65288;HIVE&#65289;&#30340;&#26032;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#32534;&#36753;&#30340;&#22270;&#20687;&#19978;&#25910;&#38598;&#20154;&#31867;&#21453;&#39304;&#24182;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#20197;&#25429;&#25417;&#22522;&#30784;&#29992;&#25143;&#20559;&#22909;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#21487;&#25193;&#23637;&#30340;&#25193;&#25955;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#65292;&#21487;&#26681;&#25454;&#20272;&#35745;&#30340;&#22870;&#21169;&#20540;&#34701;&#20837;&#20154;&#31867;&#20559;&#22909;&#12290;&#27492;&#22806;&#65292;&#20026;&#20943;&#36731;&#25968;&#25454;&#38480;&#21046;&#24102;&#26469;&#30340;&#20559;&#24046;&#65292;&#25105;&#20204;&#36129;&#29486;&#20102;1M&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;3.6K&#22870;&#21169;&#25968;&#25454;&#38598;&#20197;&#29992;&#20110;&#22870;&#21169;&#23398;&#20064;&#65292;&#20197;&#21450;1K&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#20197;&#25552;&#39640;&#25351;&#23548;&#24615;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incorporating human feedback has been shown to be crucial to align text generated by large language models to human preferences. We hypothesize that state-of-the-art instructional image editing models, where outputs are generated based on an input image and an editing instruction, could similarly benefit from human feedback, as their outputs may not adhere to the correct instructions and preferences of users. In this paper, we present a novel framework to harness human feedback for instructional visual editing (HIVE). Specifically, we collect human feedback on the edited images and learn a reward function to capture the underlying user preferences. We then introduce scalable diffusion model fine-tuning methods that can incorporate human preferences based on the estimated reward. Besides, to mitigate the bias brought by the limitation of data, we contribute a new 1M training dataset, a 3.6K reward dataset for rewards learning, and a 1K evaluation dataset to boost the performance of inst
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;BERT&#26550;&#26500;&#25913;&#36827;SATD&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#22312;&#36328;&#39033;&#30446;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#20110;&#25152;&#26377;&#20808;&#21069;&#26041;&#27861;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.09617</link><description>&lt;p&gt;
&#35770;&#26816;&#27979;&#33258;&#25105;&#25215;&#35748;&#25216;&#26415;&#20538;&#21153;F$_1$-&#24471;&#20998;&#30340;&#25913;&#36827;&#26041;&#27861;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Measuring Improvement of F$_1$-Scores in Detection of Self-Admitted Technical Debt. (arXiv:2303.09617v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;BERT&#26550;&#26500;&#25913;&#36827;SATD&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#22312;&#36328;&#39033;&#30446;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#20110;&#25152;&#26377;&#20808;&#21069;&#26041;&#27861;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24555;&#36895;&#32780;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#65292;&#30740;&#31350;&#20154;&#21592;&#21033;&#29992;&#36719;&#20214;&#24037;&#31243;&#20013;&#30340;&#29256;&#26412;&#24211;&#27880;&#37322;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20934;&#30830;&#26816;&#27979;20&#20010;&#24320;&#28304;Java&#39033;&#30446;&#20195;&#30721;&#20013;&#33258;&#25105;&#25215;&#35748;&#25216;&#26415;&#20538;&#21153;&#65288;SATD&#65289;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36716;&#25442;&#22120;&#20013;&#30340;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#65288;BERT&#65289;&#26550;&#26500;&#25913;&#36827;SATD&#26816;&#27979;&#12290;&#20026;&#20102;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#37325;&#26032;&#35780;&#20272;&#20102;&#20197;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#20998;&#23618;&#30340;10&#25240;&#20132;&#21449;&#39564;&#35777;&#26469;&#25253;&#21578;&#21487;&#38752;&#30340;F$_{1}$-&#24471;&#20998;&#12290;&#25105;&#20204;&#22312;&#36328;&#39033;&#30446;&#21644;&#20869;&#37096;&#39033;&#30446;&#19978;&#26816;&#39564;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#38024;&#23545;&#27599;&#20010;&#19978;&#19979;&#25991;&#65292;&#25105;&#20204;&#20351;&#29992;&#37325;&#26032;&#25277;&#26679;&#21644;&#22797;&#21046;&#20316;&#20026;&#22686;&#24191;&#31574;&#30053;&#65292;&#20197;&#32771;&#34385;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;&#35757;&#32451;BERT&#27169;&#22411;&#22312;19&#20010;&#39033;&#30446;&#30340;&#36328;&#39033;&#30446;&#24773;&#20917;&#19979;&#20248;&#20110;&#25152;&#26377;&#20808;&#21069;&#26041;&#27861;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence and Machine Learning have witnessed rapid, significant improvements in Natural Language Processing (NLP) tasks. Utilizing Deep Learning, researchers have taken advantage of repository comments in Software Engineering to produce accurate methods for detecting Self-Admitted Technical Debt (SATD) from 20 open-source Java projects' code. In this work, we improve SATD detection with a novel approach that leverages the Bidirectional Encoder Representations from Transformers (BERT) architecture. For comparison, we re-evaluated previous deep learning methods and applied stratified 10-fold cross-validation to report reliable F$_1$-scores. We examine our model in both cross-project and intra-project contexts. For each context, we use re-sampling and duplication as augmentation strategies to account for data imbalance. We find that our trained BERT model improves over the best performance of all previous methods in 19 of the 20 projects in cross-project scenarios. However,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#24515;&#29702;&#27835;&#30103;&#20027;&#39064;&#25512;&#33616;&#30340;AI&#20276;&#20387;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#25429;&#33719;&#30495;&#23454;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#36712;&#36857;&#21487;&#35270;&#21270;&#25552;&#20379;&#23545;&#19981;&#21516;&#22870;&#21169;&#20449;&#21495;&#21644;&#19981;&#21516;&#20020;&#24202;&#35786;&#26029;&#19979;&#35757;&#32451;&#30340;&#31574;&#30053;&#30340;&#29420;&#29305;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2303.09601</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#24515;&#29702;&#27835;&#30103;AI&#20276;&#20387;&#19982;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Psychotherapy AI Companion with Reinforcement Learning Recommendations and Interpretable Policy Dynamics. (arXiv:2303.09601v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#24515;&#29702;&#27835;&#30103;&#20027;&#39064;&#25512;&#33616;&#30340;AI&#20276;&#20387;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#25429;&#33719;&#30495;&#23454;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#36712;&#36857;&#21487;&#35270;&#21270;&#25552;&#20379;&#23545;&#19981;&#21516;&#22870;&#21169;&#20449;&#21495;&#21644;&#19981;&#21516;&#20020;&#24202;&#35786;&#26029;&#19979;&#35757;&#32451;&#30340;&#31574;&#30053;&#30340;&#29420;&#29305;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Deep Reinforcement Learning&#65288;DRL&#65289;&#29983;&#25104;&#24515;&#29702;&#27835;&#30103;&#20027;&#39064;&#25512;&#33616;&#30340;&#24378;&#21270;&#23398;&#20064;&#24515;&#29702;&#27835;&#30103;AI&#20276;&#20387;&#12290;&#35813;&#31995;&#32479;&#38024;&#23545;&#22235;&#31181;&#19981;&#21516;&#30340;&#31934;&#31070;&#30142;&#30149;&#65288;&#28966;&#34385;&#30151;&#65292;&#25233;&#37057;&#30151;&#65292;&#31934;&#31070;&#20998;&#35010;&#30151;&#21644;&#33258;&#26432;&#30149;&#20363;&#65289;&#20351;&#29992;&#22810;&#30446;&#26631;&#31574;&#30053;&#29983;&#25104;&#22120;&#36827;&#34892;&#29983;&#25104;&#65292;&#21516;&#26102;&#36890;&#36807;&#19977;&#20010;&#19981;&#21516;&#30340;&#24037;&#20316;&#32852;&#30431;&#35780;&#20998;&#26631;&#20934;&#65288;&#20219;&#21153;&#65292;&#20851;&#31995;&#21644;&#30446;&#26631;&#65289;&#26469;&#26816;&#39564;&#25512;&#33616;&#20027;&#39064;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31995;&#32479;&#33021;&#22815;&#30456;&#23545;&#36739;&#22909;&#22320;&#25429;&#33719;&#30495;&#23454;&#25968;&#25454;&#65288;&#27835;&#30103;&#24072;&#35752;&#35770;&#30340;&#21382;&#21490;&#20027;&#39064;&#65289;&#65292;&#26368;&#20339;&#27169;&#22411;&#30340;&#34920;&#29616;&#22240;&#30142;&#30149;&#21644;&#35780;&#32423;&#26631;&#20934;&#32780;&#24322;&#12290;&#20026;&#20102;&#33719;&#24471;&#21487;&#35299;&#37322;&#30340;&#27934;&#35265;&#65292;&#25105;&#20204;&#22312;2D&#20027;&#25104;&#20998;&#20998;&#26512;&#31354;&#38388;&#21644;&#36716;&#31227;&#30697;&#38453;&#20013;&#21487;&#35270;&#21270;&#31574;&#30053;&#36712;&#36857;&#12290;&#36825;&#20123;&#21487;&#35270;&#21270;&#21576;&#29616;&#20102;&#22312;&#19981;&#21516;&#22870;&#21169;&#20449;&#21495;&#21644;&#19981;&#21516;&#20020;&#24202;&#35786;&#26029;&#19979;&#35757;&#32451;&#30340;&#31574;&#30053;&#20043;&#38388;&#30340;&#29420;&#29305;&#27169;&#24335;&#12290;&#26412;&#31995;&#32479;&#22312;&#29983;&#25104;&#22810;&#30446;&#26631;&#31574;&#30053;&#24515;&#29702;&#27835;&#30103;&#20027;&#39064;&#26041;&#38754;&#30340;&#25104;&#21151;&#34920;&#29616;&#20026;&#24320;&#21457;&#33021;&#22815;&#24110;&#21161;&#27835;&#30103;&#24072;&#25552;&#20379;&#20010;&#24615;&#21270;&#27835;&#30103;&#30340;AI&#20276;&#20387;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a Reinforcement Learning Psychotherapy AI Companion that generates topic recommendations for therapists based on patient responses. The system uses Deep Reinforcement Learning (DRL) to generate multi-objective policies for four different psychiatric conditions: anxiety, depression, schizophrenia, and suicidal cases. We present our experimental results on the accuracy of recommended topics using three different scales of working alliance ratings: task, bond, and goal. We show that the system is able to capture the real data (historical topics discussed by the therapists) relatively well, and that the best performing models vary by disorder and rating scale. To gain interpretable insights into the learned policies, we visualize policy trajectories in a 2D principal component analysis space and transition matrices. These visualizations reveal distinct patterns in the policies trained with different reward signals and trained on different clinical diagnoses. Our system's succe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#27531;&#24046;&#29289;&#29702;&#23398;&#20064;&#26041;&#27861;&#21644;&#31995;&#32479;&#35782;&#21035;&#65292;&#22312;&#36731;&#37327;&#21270;&#12289;&#26580;&#36719;&#26426;&#22120;&#20154;BALLU&#19978;&#23454;&#29616;&#20102;&#20174;&#27169;&#25311;&#21040;&#30495;&#23454;&#19990;&#30028;&#30340;&#31283;&#20581;&#25511;&#21046;&#36716;&#31227;&#12290;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#22806;&#37096;&#21147;&#31574;&#30053;&#20197;&#21305;&#37197;&#30495;&#23454;&#19990;&#30028;&#36712;&#36857;&#65292;&#24314;&#27169;&#27531;&#24046;&#29289;&#29702;&#23398;&#65292;&#25552;&#39640;&#20102;&#20223;&#30495;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.09597</link><description>&lt;p&gt;
&#22522;&#20110;&#27531;&#24046;&#29289;&#29702;&#23398;&#20064;&#21644;&#31995;&#32479;&#35782;&#21035;&#30340;&#28014;&#21147;&#36741;&#21161;&#33151;&#24335;&#26426;&#22120;&#20154;&#30340;&#27169;&#25311;&#21040;&#30495;&#23454;&#19990;&#30028;&#36716;&#31227;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Residual Physics Learning and System Identification for Sim-to-real Transfer of Policies on Buoyancy Assisted Legged Robots. (arXiv:2303.09597v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#27531;&#24046;&#29289;&#29702;&#23398;&#20064;&#26041;&#27861;&#21644;&#31995;&#32479;&#35782;&#21035;&#65292;&#22312;&#36731;&#37327;&#21270;&#12289;&#26580;&#36719;&#26426;&#22120;&#20154;BALLU&#19978;&#23454;&#29616;&#20102;&#20174;&#27169;&#25311;&#21040;&#30495;&#23454;&#19990;&#30028;&#30340;&#31283;&#20581;&#25511;&#21046;&#36716;&#31227;&#12290;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#22806;&#37096;&#21147;&#31574;&#30053;&#20197;&#21305;&#37197;&#30495;&#23454;&#19990;&#30028;&#36712;&#36857;&#65292;&#24314;&#27169;&#27531;&#24046;&#29289;&#29702;&#23398;&#65292;&#25552;&#39640;&#20102;&#20223;&#30495;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28014;&#21147;&#36741;&#21161;&#36731;&#37327;&#21270;&#33151;&#37096;&#26426;&#22120;&#20154;&#20855;&#26377;&#36731;&#20415;&#21644;&#26580;&#36719;&#30340;&#29305;&#24615;&#65292;&#19982;&#35768;&#22810;&#27785;&#37325;&#21644;&#21018;&#24615;&#26426;&#22120;&#20154;&#30456;&#27604;&#65292;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#25552;&#20379;&#20869;&#22312;&#23433;&#20840;&#24615;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#29420;&#29305;&#32780;&#25935;&#24863;&#30340;&#21160;&#24577;&#29305;&#24615;&#23545;&#20110;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#33719;&#24471;&#31283;&#20581;&#30340;&#25511;&#21046;&#31574;&#30053;&#32780;&#35328;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#31995;&#32479;&#35782;&#21035;&#21644;&#25105;&#20204;&#30340;&#26032;&#39062;&#30340;&#27531;&#24046;&#29289;&#29702;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#29615;&#22659;&#27169;&#22411;&#65288;EnvMimic&#65289;&#65292;&#23637;&#31034;&#20102;&#22312;BALLU&#26426;&#22120;&#20154;&#19978;&#31283;&#20581;&#30340;&#27169;&#25311;&#21040;&#30495;&#23454;&#19990;&#30028;&#25511;&#21046;&#31574;&#30053;&#36716;&#31227;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#25910;&#38598;&#30828;&#20214;&#25968;&#25454;&#24182;&#20248;&#21270;&#27169;&#25311;&#21442;&#25968;&#26469;&#24314;&#27169;&#25191;&#34892;&#22120;&#30340;&#38750;&#32447;&#24615;&#21160;&#24577;&#12290;&#25105;&#20204;&#19981;&#20381;&#36182;&#26631;&#20934;&#30340;&#30417;&#30563;&#24335;&#23398;&#20064;&#20844;&#24335;&#65292;&#32780;&#26159;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#35757;&#32451;&#19968;&#20010;&#22806;&#37096;&#21147;&#31574;&#30053;&#20197;&#21305;&#37197;&#30495;&#23454;&#19990;&#30028;&#36712;&#36857;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#24314;&#27169;&#27531;&#24046;&#29289;&#29702;&#23398;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#20223;&#30495;&#36712;&#36857;&#26469;&#20998;&#26512;&#25913;&#36827;&#30340;&#20223;&#30495;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The light and soft characteristics of Buoyancy Assisted Lightweight Legged Unit (BALLU) robots have a great potential to provide intrinsically safe interactions in environments involving humans, unlike many heavy and rigid robots. However, their unique and sensitive dynamics impose challenges to obtaining robust control policies in the real world. In this work, we demonstrate robust sim-to-real transfer of control policies on the BALLU robots via system identification and our novel residual physics learning method, Environment Mimic (EnvMimic). First, we model the nonlinear dynamics of the actuators by collecting hardware data and optimizing the simulation parameters. Rather than relying on standard supervised learning formulations, we utilize deep reinforcement learning to train an external force policy to match real-world trajectories, which enables us to model residual physics with greater fidelity. We analyze the improved simulation fidelity by comparing the simulation trajectories
&lt;/p&gt;</description></item><item><title>&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#33021;&#21147;&#19981;&#26029;&#25552;&#21319;&#65292;&#25511;&#21046;&#26576;&#20123;&#33021;&#21147;&#23558;&#26377;&#21161;&#20110;&#38450;&#27490;&#20854;&#28389;&#29992;&#65292;&#36825;&#20123;&#38480;&#21046;&#21487;&#33021;&#21253;&#25324;&#25511;&#21046;&#35775;&#38382;&#12289;&#20351;&#29992;&#30446;&#30340;&#12289;&#36755;&#20986;&#19982;&#28335;&#28304;&#20197;&#21450;&#24320;&#21457;&#36164;&#28304;&#65292;&#38750;AI&#33021;&#21147;&#38480;&#21046;&#20063;&#26159;&#24517;&#35201;&#30340;&#12290;&#23613;&#31649;&#21487;&#33021;&#20250;&#38477;&#20302;&#20351;&#29992;&#29575;&#32780;&#22686;&#21152;&#28389;&#29992;&#39118;&#38505;&#65292;&#20294;&#36825;&#20123;&#38480;&#21046;&#26159;&#24403;&#20854;&#20182;&#24178;&#39044;&#34892;&#19981;&#36890;&#12289;&#28508;&#22312;&#21361;&#23475;&#24615;&#39640;&#12289;&#26377;&#26377;&#38024;&#23545;&#24615;&#26041;&#24335;&#24178;&#39044;&#26102;&#25152;&#24517;&#35201;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.09377</link><description>&lt;p&gt;
&#20445;&#25252;&#31038;&#20250;&#20813;&#21463;AI&#28389;&#29992;&#65306;&#20309;&#26102;&#38480;&#21046;AI&#33021;&#21147;&#26159;&#24517;&#35201;&#30340;&#65311;
&lt;/p&gt;
&lt;p&gt;
Protecting Society from AI Misuse: When are Restrictions on Capabilities Warranted?. (arXiv:2303.09377v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09377
&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#33021;&#21147;&#19981;&#26029;&#25552;&#21319;&#65292;&#25511;&#21046;&#26576;&#20123;&#33021;&#21147;&#23558;&#26377;&#21161;&#20110;&#38450;&#27490;&#20854;&#28389;&#29992;&#65292;&#36825;&#20123;&#38480;&#21046;&#21487;&#33021;&#21253;&#25324;&#25511;&#21046;&#35775;&#38382;&#12289;&#20351;&#29992;&#30446;&#30340;&#12289;&#36755;&#20986;&#19982;&#28335;&#28304;&#20197;&#21450;&#24320;&#21457;&#36164;&#28304;&#65292;&#38750;AI&#33021;&#21147;&#38480;&#21046;&#20063;&#26159;&#24517;&#35201;&#30340;&#12290;&#23613;&#31649;&#21487;&#33021;&#20250;&#38477;&#20302;&#20351;&#29992;&#29575;&#32780;&#22686;&#21152;&#28389;&#29992;&#39118;&#38505;&#65292;&#20294;&#36825;&#20123;&#38480;&#21046;&#26159;&#24403;&#20854;&#20182;&#24178;&#39044;&#34892;&#19981;&#36890;&#12289;&#28508;&#22312;&#21361;&#23475;&#24615;&#39640;&#12289;&#26377;&#26377;&#38024;&#23545;&#24615;&#26041;&#24335;&#24178;&#39044;&#26102;&#25152;&#24517;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#19981;&#26029;&#25552;&#39640;&#33021;&#21147;&#65292;&#20854;&#34987;&#29992;&#20110;&#36896;&#25104;&#20260;&#23475;&#30340;&#24773;&#20917;&#23558;&#20250;&#36234;&#26469;&#36234;&#22810;&#12290;&#20107;&#23454;&#19978;&#65292;AI&#31995;&#32479;&#24050;&#32463;&#24320;&#22987;&#29992;&#20110;&#33258;&#21160;&#21270;&#30340;&#27450;&#35784;&#27963;&#21160;&#12289;&#20405;&#29359;&#20154;&#26435;&#12289;&#21019;&#24314;&#26377;&#23475;&#30340;&#34394;&#20551;&#22270;&#20687;&#20197;&#21450;&#35782;&#21035;&#21361;&#38505;&#27602;&#32032;&#12290;&#20026;&#20102;&#38450;&#27490;AI&#30340;&#26576;&#20123;&#28389;&#29992;&#65292;&#25105;&#20204;&#35748;&#20026;&#26377;&#24517;&#35201;&#23545;&#26576;&#20123;&#33021;&#21147;&#36827;&#34892;&#26377;&#38024;&#23545;&#24615;&#30340;&#24178;&#39044;&#12290;&#36825;&#20123;&#38480;&#21046;&#21487;&#33021;&#21253;&#25324;&#25511;&#21046;&#35841;&#33021;&#35775;&#38382;&#26576;&#20123;&#31867;&#22411;&#30340;AI&#27169;&#22411;&#12289;&#23427;&#20204;&#21487;&#20197;&#29992;&#20110;&#20160;&#20040;&#12289;&#26159;&#21542;&#36807;&#28388;&#36755;&#20986;&#25110;&#32773;&#21487;&#20197;&#36861;&#28335;&#21040;&#20351;&#29992;&#32773;&#20197;&#21450;&#24320;&#21457;&#23427;&#20204;&#25152;&#38656;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#36824;&#35748;&#20026;&#65292;&#19968;&#20123;&#23545;&#28389;&#29992;&#25152;&#38656;&#30340;&#38750;AI&#33021;&#21147;&#38480;&#21046;&#20063;&#26159;&#24517;&#35201;&#30340;&#12290;&#34429;&#28982;&#33021;&#21147;&#38480;&#21046;&#21487;&#33021;&#20250;&#38477;&#20302;&#20351;&#29992;&#29575;&#32780;&#19981;&#26159;&#28389;&#29992;&#29575;&#65288;&#23384;&#22312;&#19981;&#21033;&#30340;&#28389;&#29992;-&#20351;&#29992;&#26435;&#34913;&#65289;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#24403;&#20854;&#20182;&#24178;&#39044;&#34892;&#19981;&#36890;&#12289;&#28508;&#22312;&#28389;&#29992;&#30340;&#21361;&#23475;&#24615;&#24456;&#39640;&#65292;&#24182;&#19988;&#26377;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#24335;&#26469;&#24178;&#39044;&#33021;&#21147;&#26102;&#65292;&#24178;&#39044;&#33021;&#21147;&#26159;&#24517;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) systems will increasingly be used to cause harm as they grow more capable. In fact, AI systems are already starting to be used to automate fraudulent activities, violate human rights, create harmful fake images, and identify dangerous toxins. To prevent some misuses of AI, we argue that targeted interventions on certain capabilities will be warranted. These restrictions may include controlling who can access certain types of AI models, what they can be used for, whether outputs are filtered or can be traced back to their user, and the resources needed to develop them. We also contend that some restrictions on non-AI capabilities needed to cause harm will be required. Though capability restrictions risk reducing use more than misuse (facing an unfavorable Misuse-Use Tradeoff), we argue that interventions on capabilities are warranted when other interventions are insufficient, the potential harm from misuse is high, and there are targeted ways to intervene on
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#22522;&#20110; CRF &#21644;&#28145;&#24230;&#23398;&#20064;&#65288;&#22914; BanglaBERT&#65289;&#30340;&#40065;&#26834;&#23391;&#21152;&#25289;&#22797;&#26434;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102; CNER &#20219;&#21153;&#65292;&#22635;&#34917;&#20102;&#23391;&#21152;&#25289;&#35821;&#22797;&#26434;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#39046;&#22495;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2303.09306</link><description>&lt;p&gt;
&#26500;&#24314;&#40065;&#26834;&#30340;&#23391;&#21152;&#25289;&#22797;&#26434;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Bangla Complex Named Entity Recognition. (arXiv:2303.09306v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#22522;&#20110; CRF &#21644;&#28145;&#24230;&#23398;&#20064;&#65288;&#22914; BanglaBERT&#65289;&#30340;&#40065;&#26834;&#23391;&#21152;&#25289;&#22797;&#26434;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102; CNER &#20219;&#21153;&#65292;&#22635;&#34917;&#20102;&#23391;&#21152;&#25289;&#35821;&#22797;&#26434;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#39046;&#22495;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035; (NER) &#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#22522;&#30784;&#20219;&#21153;&#65292;&#21253;&#25324;&#22312;&#25991;&#26412;&#20013;&#35782;&#21035;&#21644;&#20998;&#31867;&#21629;&#21517;&#23454;&#20307;&#12290;&#23613;&#31649;&#23391;&#21152;&#25289;&#35821;&#26159;&#20840;&#29699;&#31532;&#19971;&#22823;&#20351;&#29992;&#35821;&#35328;&#65292;&#20294;&#38024;&#23545;&#23391;&#21152;&#25289;&#35821;&#22797;&#26434;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#24037;&#20316;&#36824;&#24456;&#23569;&#12290;CNER &#26159;&#19968;&#39033;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#35782;&#21035;&#21644;&#20998;&#31867;&#22797;&#26434;&#21644;&#22797;&#21512;&#23454;&#20307;&#65292;&#32780;&#36825;&#22312;&#23391;&#21152;&#25289;&#35821;&#20013;&#19981;&#24120;&#35265;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#20915; BanglaCoNER &#25968;&#25454;&#38598;&#19978;&#30340; CNER &#20219;&#21153;&#30340;&#33719;&#32988;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#21363;&#26465;&#20214;&#38543;&#26426;&#22330; (CRF) &#21644;&#22522;&#20110; finetuning transformer &#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#22914; BanglaBERT&#65289;&#12290;&#25968;&#25454;&#38598;&#21253;&#25324; 15300 &#20010;&#29992;&#20110;&#35757;&#32451;&#30340;&#21477;&#23376;&#21644; 800 &#20010;&#29992;&#20110;&#39564;&#35777;&#30340;&#21477;&#23376;&#65292;&#26684;&#24335;&#20026; .conll&#12290;&#23545;&#25968;&#25454;&#38598;&#30340;&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512; (EDA) &#25581;&#31034;&#20986;&#25968;&#25454;&#38598;&#26377; 7 &#31181;&#19981;&#21516;&#30340; NER &#26631;&#31614;&#65292;&#20854;&#20013;&#26377;&#33521;&#35821;&#21333;&#35789;&#30340;&#26126;&#26174;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named Entity Recognition (NER) is a fundamental task in natural language processing that involves identifying and classifying named entities in text. But much work hasn't been done for complex named entity recognition in Bangla, despite being the seventh most spoken language globally. CNER is a more challenging task than traditional NER as it involves identifying and classifying complex and compound entities, which are not common in Bangla language. In this paper, we present the winning solution of Bangla Complex Named Entity Recognition Challenge - addressing the CNER task on BanglaCoNER dataset using two different approaches, namely Conditional Random Fields (CRF) and finetuning transformer based Deep Learning models such as BanglaBERT.  The dataset consisted of 15300 sentences for training and 800 sentences for validation, in the .conll format. Exploratory Data Analysis (EDA) on the dataset revealed that the dataset had 7 different NER tags, with notable presence of English words, s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;t-SPN&#31639;&#27861;&#21644;&#28388;&#27874;&#25216;&#26415;&#30340;&#32454;&#32990;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#36793;&#32536;&#21644;L2&#27491;&#21017;&#21270;&#65292;&#35813;&#26041;&#27861;&#22312;HEp-2&#21644;Feulgen&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.09065</link><description>&lt;p&gt;
&#22522;&#20110;t-SPN&#21644;&#28388;&#27874;&#30340;&#32454;&#32990;&#20998;&#31867;&#30340;&#26368;&#22823;&#38388;&#38548;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Maximum Margin Learning of t-SPNs for Cell Classification with Filtering. (arXiv:2303.09065v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;t-SPN&#31639;&#27861;&#21644;&#28388;&#27874;&#25216;&#26415;&#30340;&#32454;&#32990;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#36793;&#32536;&#21644;L2&#27491;&#21017;&#21270;&#65292;&#35813;&#26041;&#27861;&#22312;HEp-2&#21644;Feulgen&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#27010;&#29575;&#20307;&#31995;&#32467;&#26500;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#26641;&#24418;&#27714;&#21644;&#20135;&#21697;&#32593;&#32476;(t-SPN)&#65292;&#29992;&#20110;&#32454;&#32990;&#20998;&#31867;&#12290;&#26500;&#24314;t-SPN&#30340;&#30446;&#30340;&#26159;&#34920;&#31034;&#26410;&#24402;&#19968;&#21270;&#27010;&#29575;&#20316;&#20026;&#26368;&#30456;&#20284;&#30340;&#32454;&#32990;&#31867;&#21035;&#30340;&#26465;&#20214;&#27010;&#29575;&#12290;&#36890;&#36807;&#26368;&#22823;&#21270;&#36793;&#32536;&#26469;&#23398;&#20064;&#26500;&#24314;&#30340;t-SPN&#20307;&#31995;&#32467;&#26500;&#65292;&#35813;&#36793;&#32536;&#26159;&#30495;&#23454;&#26631;&#31614;&#21644;&#26368;&#26377;&#31454;&#20105;&#21147;&#30340;&#38169;&#35823;&#26631;&#31614;&#20043;&#38388;&#30340;&#26465;&#20214;&#27010;&#29575;&#24046;&#12290;&#20026;&#20102;&#22686;&#24378;&#20307;&#31995;&#32467;&#26500;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;L2&#27491;&#21017;&#21270;&#65288;REG&#65289;&#21644;&#26368;&#22823;&#38388;&#38548;&#65288;MM&#65289;&#26631;&#20934;&#12290;&#20026;&#20102;&#31361;&#20986;&#32454;&#32990;&#29305;&#24449;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#20004;&#31181;&#36890;&#29992;&#30340;&#39640;&#36890;&#28388;&#27874;&#22120;&#30340;&#26377;&#25928;&#24615;&#65306;&#29702;&#24819;&#39640;&#36890;&#28388;&#27874;&#21644;&#25289;&#26222;&#25289;&#26031;&#28388;&#27874;(Log)&#12290;&#22312;HEp-2&#21644;Feulgen&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#22522;&#20110;&#26368;&#22823;&#38388;&#38548;&#20934;&#21017;&#19982;&#27491;&#21017;&#21270;&#23398;&#20064;&#30340;t-SPN&#20307;&#31995;&#32467;&#26500;&#20135;&#29983;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
An algorithm based on a deep probabilistic architecture referred to as a tree-structured sum-product network (t-SPN) is considered for cell classification. The t-SPN is constructed such that the unnormalized probability is represented as conditional probabilities of a subset of most similar cell classes. The constructed t-SPN architecture is learned by maximizing the margin, which is the difference in the conditional probability between the true and the most competitive false label. To enhance the generalization ability of the architecture, L2-regularization (REG) is considered along with the maximum margin (MM) criterion in the learning process. To highlight cell features, this paper investigates the effectiveness of two generic high-pass filters: ideal high-pass filtering and the Laplacian of Gaussian (LOG) filtering. On both HEp-2 and Feulgen benchmark datasets, the t-SPN architecture learned based on the max-margin criterion with regularization produced the highest accuracy rate co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#35782;&#30693;&#35782;&#36741;&#21161;&#30340;&#32454;&#31890;&#24230;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#36890;&#35782;&#30693;&#35782;&#25512;&#29702;&#27169;&#22359;&#22788;&#29702;&#30001;&#22522;&#20934;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#22120;&#32473;&#20986;&#30340;&#31895;&#31890;&#24230;&#26631;&#31614;&#65292;&#20174;&#32780;&#25552;&#39640;&#30446;&#26631;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#35813;&#26041;&#27861;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#37327;&#21644;&#26631;&#27880;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2303.09026</link><description>&lt;p&gt;
&#36890;&#35782;&#30693;&#35782;&#36741;&#21161;&#30340;&#36164;&#28304;&#21463;&#38480;&#21644;&#32454;&#31890;&#24230;&#30446;&#26631;&#26816;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Commonsense Knowledge Assisted Deep Learning for Resource-constrained and Fine-grained Object Detection. (arXiv:2303.09026v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#35782;&#30693;&#35782;&#36741;&#21161;&#30340;&#32454;&#31890;&#24230;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#36890;&#35782;&#30693;&#35782;&#25512;&#29702;&#27169;&#22359;&#22788;&#29702;&#30001;&#22522;&#20934;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#22120;&#32473;&#20986;&#30340;&#31895;&#31890;&#24230;&#26631;&#31614;&#65292;&#20174;&#32780;&#25552;&#39640;&#30446;&#26631;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#35813;&#26041;&#27861;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#37327;&#21644;&#26631;&#27880;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#36793;&#32536;&#35745;&#31639;&#31561;&#36164;&#28304;&#21463;&#38480;&#22330;&#26223;&#19979;&#30340;&#32454;&#31890;&#24230;&#22270;&#20687;&#30446;&#26631;&#26816;&#27979;&#38382;&#39064;&#12290;&#38024;&#23545;&#20351;&#29992;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#30446;&#26631;&#26816;&#27979;&#22120;&#26102;&#38656;&#35201;&#20351;&#29992;&#22823;&#22411;&#27169;&#22411;&#21644;&#22823;&#37327;&#25968;&#25454;&#26631;&#27880;&#30340;&#31934;&#20934;&#32454;&#31890;&#24230;&#26816;&#27979;&#38656;&#27714;&#65292;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#36890;&#35782;&#30693;&#35782;&#36741;&#21161;&#31895;&#31890;&#24230;&#30446;&#26631;&#26816;&#27979;&#22120;&#33719;&#21462;&#31934;&#20934;&#30340;&#32454;&#31890;&#24230;&#26816;&#27979;&#32467;&#26524;&#12290;&#24341;&#20837;&#36890;&#35782;&#30693;&#35782;&#25512;&#29702;&#27169;&#22359;(CKIM)&#22788;&#29702;&#30001;&#22522;&#20934;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#22120;&#32473;&#20986;&#30340;&#31895;&#31890;&#24230;&#26631;&#31614;&#65292;&#20174;&#32780;&#29983;&#25104;&#32454;&#31890;&#24230;&#26631;&#31614;&#12290;&#35770;&#25991;&#20013;&#32771;&#34385;&#20102;&#27169;&#31946;&#35268;&#21017;&#21644;&#28165;&#26224;&#35268;&#21017;&#30340;&#25512;&#29702;&#65292;&#21069;&#32773;&#29992;&#20110;&#22788;&#29702;&#30446;&#26631;&#35821;&#20041;&#26631;&#31614;&#30340;&#27169;&#31946;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#30446;&#26631;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#37327;&#21644;&#26631;&#27880;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider fine-grained image object detection in resource-constrained cases such as edge computing. Deep learning (DL), namely learning with deep neural networks (DNNs), has become the dominating approach to object detection. To achieve accurate fine-grained detection, one needs to employ a large enough DNN model and a vast amount of data annotations, which brings a challenge for using modern DL object detectors in resource-constrained cases. To this end, we propose an approach, which leverages commonsense knowledge to assist a coarse-grained object detector to get accurate fine-grained detection results. Specifically, we introduce a commonsense knowledge inference module (CKIM) to process coarse-grained lables given by a benchmark DL detector to produce fine-grained lables. We consider both crisp-rule and fuzzy-rule based inference in our CKIM; the latter is used to handle ambiguity in the target semantic labels. We implement our method based on several modern DL dete
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#38190;&#28857;&#30340;RGB-D&#36755;&#20837;&#30340;&#20845;&#33258;&#30001;&#24230;&#25235;&#21462;&#23039;&#24577;&#21512;&#25104;&#26041;&#27861;&#65292;&#26082;&#21487;&#20197;&#20174;&#20851;&#38190;&#28857;&#26816;&#27979;&#20013;&#39044;&#27979;&#25235;&#21462;&#23039;&#24577;&#65292;&#20063;&#21487;&#20197;&#39044;&#27979;&#30456;&#23545;&#20110;&#30456;&#26426;&#30340;&#23610;&#24230;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.05617</link><description>&lt;p&gt;
KGNv2: &#22522;&#20110;&#20851;&#38190;&#28857;&#30340;RGB-D&#36755;&#20837;&#20845;&#33258;&#30001;&#24230;&#25235;&#21462;&#21512;&#25104;&#20013;&#30340;&#23610;&#24230;&#21644;&#23039;&#24577;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
KGNv2: Separating Scale and Pose Prediction for Keypoint-based 6-DoF Grasp Synthesis on RGB-D input. (arXiv:2303.05617v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#38190;&#28857;&#30340;RGB-D&#36755;&#20837;&#30340;&#20845;&#33258;&#30001;&#24230;&#25235;&#21462;&#23039;&#24577;&#21512;&#25104;&#26041;&#27861;&#65292;&#26082;&#21487;&#20197;&#20174;&#20851;&#38190;&#28857;&#26816;&#27979;&#20013;&#39044;&#27979;&#25235;&#21462;&#23039;&#24577;&#65292;&#20063;&#21487;&#20197;&#39044;&#27979;&#30456;&#23545;&#20110;&#30456;&#26426;&#30340;&#23610;&#24230;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;6&#33258;&#30001;&#24230;&#25235;&#21462;&#23039;&#24577;&#21512;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#20851;&#38190;&#28857;&#20174;2D/2.5D&#36755;&#20837;&#20013;&#36827;&#34892;&#12290;&#22312;&#21069;&#26399;&#30740;&#31350;&#20013;&#65292;&#22522;&#20110;&#20851;&#38190;&#28857;&#30340;&#25235;&#21462;&#26816;&#27979;&#22120;&#24050;&#32463;&#35777;&#26126;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#20854;&#20013;&#24425;&#33394;&#22270;&#20687;&#25552;&#20379;&#30340;&#39069;&#22806;&#35270;&#35273;&#20449;&#24687;&#24357;&#34917;&#20102;&#22024;&#26434;&#30340;&#28145;&#24230;&#24863;&#30693;&#12290;&#28982;&#32780;&#65292;&#23427;&#20005;&#37325;&#20381;&#36182;&#20110;&#20934;&#30830;&#39044;&#27979;&#22270;&#20687;&#31354;&#38388;&#20013;&#30340;&#20851;&#38190;&#28857;&#20301;&#32622;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#25235;&#21462;&#29983;&#25104;&#32593;&#32476;&#65292;&#26082;&#21487;&#20197;&#20174;&#20851;&#38190;&#28857;&#26816;&#27979;&#20013;&#39044;&#27979;&#25235;&#21462;&#23039;&#24577;&#65292;&#20063;&#21487;&#20197;&#39044;&#27979;&#30456;&#23545;&#20110;&#30456;&#26426;&#30340;&#23610;&#24230;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#37325;&#26032;&#35774;&#35745;&#20102;&#20851;&#38190;&#28857;&#36755;&#20986;&#31354;&#38388;&#65292;&#20197;&#20943;&#36731;&#20851;&#38190;&#28857;&#39044;&#27979;&#22122;&#22768;&#23545;&#36879;&#35270;n&#28857;(PnP)&#31639;&#27861;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#27604;&#22522;&#32447;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20248;&#36234;&#24615;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#23613;&#31649;&#26159;&#22312;&#31616;&#21333;&#30340;&#21512;&#25104;&#23545;&#35937;&#19978;&#35757;&#32451;&#30340;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#21487;&#20197;&#29992;&#20110;&#30495;&#23454;&#29289;&#20307;&#19978;&#30340;&#25235;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new 6-DoF grasp pose synthesis approach from 2D/2.5D input based on keypoints. Keypoint-based grasp detector from image input has demonstrated promising results in the previous study, where the additional visual information provided by color images compensates for the noisy depth perception. However, it relies heavily on accurately predicting the location of keypoints in the image space. In this paper, we devise a new grasp generation network that reduces the dependency on precise keypoint estimation. Given an RGB-D input, our network estimates both the grasp pose from keypoint detection as well as scale towards the camera. We further re-design the keypoint output space in order to mitigate the negative impact of keypoint prediction noise to Perspective-n-Point (PnP) algorithm. Experiments show that the proposed method outperforms the baseline by a large margin, validating the efficacy of our approach. Finally, despite trained on simple synthetic objects, our method demons
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25439;&#22833;&#37325;&#26032;&#21152;&#26435;&#31574;&#30053;&#30340;&#26080;&#28304;&#33258;&#36866;&#24212;&#22495;&#33258;&#36866;&#24212;&#65288;SF-UDA&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#36866;&#24212;&#30446;&#26631;&#22495;&#65292;&#20854;&#20851;&#38190;&#26159;&#36890;&#36807;&#20272;&#35745;&#20266;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#25351;&#23548;&#20854;&#36827;&#19968;&#27493;&#31934;&#21270;&#65292;&#24182;&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#26694;&#26550;&#20316;&#20026;&#30446;&#26631;&#31354;&#38388;&#30340;&#27491;&#21017;&#21270;&#22120;&#20197;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.03770</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#25351;&#23548;&#20266;&#26631;&#31614;&#30340;&#26080;&#28304;&#33258;&#36866;&#24212;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Guiding Pseudo-labels with Uncertainty Estimation for Source-free Unsupervised Domain Adaptation. (arXiv:2303.03770v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25439;&#22833;&#37325;&#26032;&#21152;&#26435;&#31574;&#30053;&#30340;&#26080;&#28304;&#33258;&#36866;&#24212;&#22495;&#33258;&#36866;&#24212;&#65288;SF-UDA&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#36866;&#24212;&#30446;&#26631;&#22495;&#65292;&#20854;&#20851;&#38190;&#26159;&#36890;&#36807;&#20272;&#35745;&#20266;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#25351;&#23548;&#20854;&#36827;&#19968;&#27493;&#31934;&#21270;&#65292;&#24182;&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#26694;&#26550;&#20316;&#20026;&#30446;&#26631;&#31354;&#38388;&#30340;&#27491;&#21017;&#21270;&#22120;&#20197;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#20551;&#23450;&#22312;&#36866;&#24212;&#36807;&#31243;&#20013;&#21516;&#26102;&#21487;&#29992;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26080;&#28304;&#33258;&#36866;&#24212;&#22495;&#33258;&#36866;&#24212;&#65288;SF-UDA&#65289;&#26041;&#27861;&#65292;&#23427;&#26159;UDA&#30340;&#19968;&#20010;&#29305;&#27530;&#24773;&#20917;&#65292;&#22312;&#35813;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#22312;&#27809;&#26377;&#35775;&#38382;&#28304;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#30446;&#26631;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;SF-UDA&#35774;&#32622;&#65292;&#22522;&#20110;&#25439;&#22833;&#37325;&#26032;&#21152;&#26435;&#31574;&#30053;&#65292;&#20197;&#22686;&#24378;&#23545;&#20266;&#26631;&#31614;&#30340;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;&#35813;&#20998;&#31867;&#25439;&#22833;&#22522;&#20110;&#20272;&#35745;&#20854;&#19981;&#30830;&#23450;&#24615;&#26469;&#37325;&#26032;&#21152;&#26435;&#65292;&#20197;&#25351;&#23548;&#20266;&#26631;&#31614;&#30340;&#36827;&#19968;&#27493;&#31934;&#21270;&#65292;&#24182;&#36890;&#36807;&#32858;&#38598;&#30456;&#37051;&#26679;&#26412;&#30340;&#30693;&#35782;&#26469;&#36880;&#27493;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#30417;&#30563;&#23545;&#27604;&#26694;&#26550;&#26469;&#20316;&#20026;&#30446;&#26631;&#31354;&#38388;&#30340;&#27491;&#21017;&#21270;&#22120;&#65292;&#20197;&#22686;&#24378;&#30693;&#35782;&#30340;&#32858;&#21512;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36127;&#26679;&#26412;&#23545;&#25490;&#38500;&#31574;&#30053;&#65292;&#20197;&#35782;&#21035;&#21644;&#25490;&#38500;&#30001;&#20849;&#20139;&#30456;&#21516;&#29305;&#24449;&#30340;&#26679;&#26412;&#26500;&#25104;&#30340;&#36127;&#26679;&#26412;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard Unsupervised Domain Adaptation (UDA) methods assume the availability of both source and target data during the adaptation. In this work, we investigate Source-free Unsupervised Domain Adaptation (SF-UDA), a specific case of UDA where a model is adapted to a target domain without access to source data. We propose a novel approach for the SF-UDA setting based on a loss reweighting strategy that brings robustness against the noise that inevitably affects the pseudo-labels. The classification loss is reweighted based on the reliability of the pseudo-labels that is measured by estimating their uncertainty. Guided by such reweighting strategy, the pseudo-labels are progressively refined by aggregating knowledge from neighbouring samples. Furthermore, a self-supervised contrastive framework is leveraged as a target space regulariser to enhance such knowledge aggregation. A novel negative pairs exclusion strategy is proposed to identify and exclude negative pairs made of samples shari
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26435;&#37325;&#20462;&#21098;&#21644;&#22855;&#24322;&#20540;&#20998;&#35299;&#26469;&#20248;&#21270;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#27492;&#26041;&#27861;&#21487;&#20197;&#22312;&#24179;&#34913;&#20934;&#30830;&#24615;&#12289;&#36895;&#24230;&#21644;&#27169;&#22411;&#22823;&#23567;&#36807;&#31243;&#20013;&#26377;&#25928;&#22320;&#20248;&#21270;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.02735</link><description>&lt;p&gt;
&#21033;&#29992;&#26435;&#37325;&#20462;&#21098;&#21644;&#22855;&#24322;&#20540;&#20998;&#35299;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#36827;&#34892;&#21487;&#25193;&#23637;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Scalable Object Detection on Embedded Devices Using Weight Pruning and Singular Value Decomposition. (arXiv:2303.02735v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26435;&#37325;&#20462;&#21098;&#21644;&#22855;&#24322;&#20540;&#20998;&#35299;&#26469;&#20248;&#21270;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#27492;&#26041;&#27861;&#21487;&#20197;&#22312;&#24179;&#34913;&#20934;&#30830;&#24615;&#12289;&#36895;&#24230;&#21644;&#27169;&#22411;&#22823;&#23567;&#36807;&#31243;&#20013;&#26377;&#25928;&#22320;&#20248;&#21270;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#21033;&#29992;&#26435;&#37325;&#20462;&#21098;&#21644;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#36827;&#34892;&#20248;&#21270;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#22312;https://universe.roboflow.com/roboflow-100/street-work&#33719;&#21462;&#30340;&#34903;&#36947;&#24037;&#20316;&#22270;&#20687;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#19978;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;611&#20010;&#35757;&#32451;&#22270;&#20687;&#65292;175&#20010;&#39564;&#35777;&#22270;&#20687;&#21644;87&#20010;&#20855;&#26377;7&#20010;&#20998;&#31867;&#30340;&#27979;&#35797;&#22270;&#20687;&#12290;&#25105;&#20204;&#36890;&#36807;&#24103;&#29575;&#12289;&#24179;&#22343;&#31934;&#24230;&#65288;mAP@50&#65289;&#21644;&#26435;&#37325;&#22823;&#23567;&#31561;&#26041;&#38754;&#27604;&#36739;&#20102;&#20248;&#21270;&#21518;&#30340;&#27169;&#22411;&#19982;&#21407;&#22987;&#26410;&#20248;&#21270;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26435;&#37325;&#20462;&#21098;+SVD&#27169;&#22411;&#20855;&#26377;0.724 mAP@50&#65292;&#24103;&#29575;&#20026;1.48 FPS&#65292;&#26435;&#37325;&#22823;&#23567;&#20026;12.1 MB&#65292;&#20248;&#20110;&#21407;&#22987;&#27169;&#22411;&#65288;0.717 mAP@50&#65292;1.50 FPS&#21644;12.3 MB&#65289;&#12290;&#25152;&#26377;&#27169;&#22411;&#30340;&#31934;&#24230;-&#21484;&#22238;&#26354;&#32447;&#20063;&#32472;&#21046;&#20986;&#26469;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#24179;&#34913;&#20934;&#30830;&#24615;&#12289;&#36895;&#24230;&#21644;&#27169;&#22411;&#22823;&#23567;&#30340;&#21516;&#26102;&#26377;&#25928;&#22320;&#20248;&#21270;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a method for optimizing object detection models by combining weight pruning and singular value decomposition (SVD). The proposed method was evaluated on a custom dataset of street work images obtained from https://universe.roboflow.com/roboflow-100/street-work. The dataset consists of 611 training images, 175 validation images, and 87 test images with 7 classes. We compared the performance of the optimized models with the original unoptimized model in terms of frame rate, mean average precision (mAP@50), and weight size. The results show that the weight pruning + SVD model achieved a 0.724 mAP@50 with a frame rate of 1.48 FPS and a weight size of 12.1 MB, outperforming the original model (0.717 mAP@50, 1.50 FPS, and 12.3 MB). Precision-recall curves were also plotted for all models. Our work demonstrates that the proposed method can effectively optimize object detection models while balancing accuracy, speed, and model size.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#28151;&#21512;&#31995;&#32479;&#20013;&#30340;&#24863;&#30693;&#22833;&#36133;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#22996;&#27966;&#31649;&#29702;&#20195;&#29702;&#26469;&#36873;&#25321;&#20154;&#31867;&#25110;&#33258;&#20027;&#31995;&#32479;&#25511;&#21046;&#65292;&#20197;&#34917;&#20607;&#21487;&#33021;&#21457;&#29983;&#30340;&#24863;&#30693;&#32570;&#22833;&#65292;&#20445;&#38556;&#31995;&#32479;&#23433;&#20840;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.01300</link><description>&lt;p&gt;
&#36890;&#36807;&#22996;&#27966;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#28151;&#21512;&#31995;&#32479;&#20013;&#24357;&#34917;&#24863;&#30693;&#22833;&#36133;
&lt;/p&gt;
&lt;p&gt;
Compensating for Sensing Failures via Delegation in Human-AI Hybrid Systems. (arXiv:2303.01300v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#28151;&#21512;&#31995;&#32479;&#20013;&#30340;&#24863;&#30693;&#22833;&#36133;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#22996;&#27966;&#31649;&#29702;&#20195;&#29702;&#26469;&#36873;&#25321;&#20154;&#31867;&#25110;&#33258;&#20027;&#31995;&#32479;&#25511;&#21046;&#65292;&#20197;&#34917;&#20607;&#21487;&#33021;&#21457;&#29983;&#30340;&#24863;&#30693;&#32570;&#22833;&#65292;&#20445;&#38556;&#31995;&#32479;&#23433;&#20840;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26234;&#33021;&#31995;&#32479;&#22312;&#33258;&#20027;&#34892;&#20026;&#25110;&#22686;&#24378;&#20154;&#31867;&#27963;&#21160;&#26041;&#38754;&#30340;&#26222;&#21450;&#65292;&#32771;&#34385;&#20154;&#31867;&#12289;&#33258;&#20027;&#31995;&#32479;&#25110;&#20004;&#32773;&#37117;&#20250;&#22240;&#22810;&#31181;&#22240;&#32032;&#20043;&#19968;&#65288;&#20363;&#22914;&#24863;&#30693;&#65289;&#32780;&#20986;&#29616;&#22833;&#36133;&#30340;&#24773;&#20917;&#38750;&#24120;&#37325;&#35201;&#12290;&#26080;&#35770;&#26159;&#20154;&#31867;&#36824;&#26159;&#33258;&#20027;&#20195;&#29702;&#30340;&#22833;&#36133;&#37117;&#21487;&#33021;&#23548;&#33268;&#38477;&#20302;&#24615;&#33021;&#27700;&#24179;&#65292;&#29978;&#33267;&#20005;&#37325;&#21040;&#23548;&#33268;&#20260;&#23475;&#25110;&#27515;&#20129;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#28151;&#21512;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#30340;&#24773;&#20917;&#65292;&#20854;&#20013;&#31649;&#29702;&#20195;&#29702;&#36127;&#36131;&#35782;&#21035;&#20309;&#26102;&#25191;&#34892;&#22996;&#27966;&#20219;&#21153;&#20197;&#21450;&#20154;&#31867;&#25110;&#33258;&#20027;&#31995;&#32479;&#26159;&#21542;&#24212;&#35813;&#25484;&#25511;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#31649;&#29702;&#32773;&#23558;&#26681;&#25454;&#20182;&#20204;&#30340;&#24863;&#30693;&#33021;&#21147;&#21644;&#21487;&#33021;&#30340;&#32570;&#38519;&#65292;&#20272;&#35745;&#26368;&#20339;&#34892;&#21160;&#65292;&#20197;&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#20195;&#29702;&#30340;&#22833;&#36133;&#20960;&#29575;&#12290;&#25105;&#20204;&#27169;&#25311;&#29615;&#22659;&#32972;&#26223;&#22914;&#20309;&#23548;&#33268;&#25110;&#21152;&#21095;&#24863;&#30693;&#33021;&#21147;&#30340;&#32570;&#38519;&#65292;&#20197;&#25552;&#20379;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given an increasing prevalence of intelligent systems capable of autonomous actions or augmenting human activities, it is important to consider scenarios in which the human, autonomous system, or both can exhibit failures as a result of one of several contributing factors (e.g. perception). Failures for either humans or autonomous agents can lead to simply a reduced performance level, or a failure can lead to something as severe as injury or death. For our topic, we consider the hybrid human-AI teaming case where a managing agent is tasked with identifying when to perform a delegation assignment and whether the human or autonomous system should gain control. In this context, the manager will estimate its best action based on the likelihood of either (human, autonomous) agent failure as a result of their sensing capabilities and possible deficiencies. We model how the environmental context can contribute to, or exacerbate, the sensing deficiencies. These contexts provide cases where the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36328;&#27169;&#24577;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#31934;&#30830;&#22320;&#20272;&#35745;4D&#38647;&#36798;&#22330;&#26223;&#27969;&#65292;&#24182;&#22312;&#36816;&#21160;&#20998;&#21106;&#21644;&#33258;&#25105;&#36816;&#21160;&#20272;&#35745;&#31561;&#23376;&#20219;&#21153;&#20013;&#26174;&#31034;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.00462</link><description>&lt;p&gt;
&#38544;&#34255;&#30340;&#23453;&#30707;&#65306;&#20351;&#29992;&#36328;&#27169;&#24577;&#30417;&#30563;&#30340;4D&#38647;&#36798;&#22330;&#26223;&#27969;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hidden Gems: 4D Radar Scene Flow Learning Using Cross-Modal Supervision. (arXiv:2303.00462v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36328;&#27169;&#24577;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#31934;&#30830;&#22320;&#20272;&#35745;4D&#38647;&#36798;&#22330;&#26223;&#27969;&#65292;&#24182;&#22312;&#36816;&#21160;&#20998;&#21106;&#21644;&#33258;&#25105;&#36816;&#21160;&#20272;&#35745;&#31561;&#23376;&#20219;&#21153;&#20013;&#26174;&#31034;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#36328;&#27169;&#24577;&#23398;&#20064;&#36827;&#34892;4D&#38647;&#36798;&#22522;&#30784;&#22330;&#26223;&#27969;&#37327;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21463;&#21040;&#29616;&#20195;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#21516;&#19968;&#20301;&#32622;&#30340;&#20256;&#24863;&#22120;&#20887;&#20313;&#30340;&#21551;&#21457;&#12290;&#36825;&#31181;&#20887;&#20313;&#38544;&#21547;&#22320;&#20026;&#38647;&#36798;&#22330;&#26223;&#27969;&#20272;&#35745;&#25552;&#20379;&#20102;&#21508;&#31181;&#24418;&#24335;&#30340;&#30417;&#30563;&#32447;&#32034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#38024;&#23545;&#24050;&#30830;&#23450;&#30340;&#36328;&#27169;&#24577;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#20351;&#29992;&#22810;&#20010;&#36328;&#27169;&#24577;&#32422;&#26463;&#26426;&#20250;&#26377;&#25928;&#22320;&#36827;&#34892;&#22330;&#26223;&#27969;&#37327;&#20272;&#35745;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#26174;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#20102;&#36328;&#27169;&#24577;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#25512;&#26029;&#26356;&#20934;&#30830;&#30340;4D&#38647;&#36798;&#22330;&#26223;&#27969;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23427;&#23545;&#20004;&#20010;&#23376;&#20219;&#21153;-&#36816;&#21160;&#20998;&#21106;&#21644;&#33258;&#25105;&#36816;&#21160;&#20272;&#35745;&#30340;&#26377;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;&#23558;&#22312;https://github.com/Toytiny/CMFlow&#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes a novel approach to 4D radar-based scene flow estimation via cross-modal learning. Our approach is motivated by the co-located sensing redundancy in modern autonomous vehicles. Such redundancy implicitly provides various forms of supervision cues to the radar scene flow estimation. Specifically, we introduce a multi-task model architecture for the identified cross-modal learning problem and propose loss functions to opportunistically engage scene flow estimation using multiple cross-modal constraints for effective model training. Extensive experiments show the state-of-the-art performance of our method and demonstrate the effectiveness of cross-modal supervised learning to infer more accurate 4D radar scene flow. We also show its usefulness to two subtasks - motion segmentation and ego-motion estimation. Our source code will be available on https://github.com/Toytiny/CMFlow.
&lt;/p&gt;</description></item><item><title>EAGLE&#24341;&#20837;&#20102;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#26032;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#26032;&#30340;&#32593;&#26684;&#21464;&#25442;&#22120;&#65292;&#33021;&#22815;&#39044;&#27979;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27969;&#20307;&#21160;&#21147;&#23398;&#25968;&#25454;&#38598;&#20013;&#30340;&#21387;&#21147;&#21644;&#36895;&#24230;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2302.10803</link><description>&lt;p&gt;
Eagle: &#22522;&#20110;&#32593;&#26684;&#21464;&#25442;&#22120;&#30340;&#28237;&#27969;&#27969;&#20307;&#21160;&#21147;&#23398;&#22823;&#35268;&#27169;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Eagle: Large-Scale Learning of Turbulent Fluid Dynamics with Mesh Transformers. (arXiv:2302.10803v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10803
&lt;/p&gt;
&lt;p&gt;
EAGLE&#24341;&#20837;&#20102;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#26032;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#26032;&#30340;&#32593;&#26684;&#21464;&#25442;&#22120;&#65292;&#33021;&#22815;&#39044;&#27979;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27969;&#20307;&#21160;&#21147;&#23398;&#25968;&#25454;&#38598;&#20013;&#30340;&#21387;&#21147;&#21644;&#36895;&#24230;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#36890;&#36807;&#27169;&#25311;&#21644;&#35745;&#31639;&#25968;&#20540;&#27169;&#22411;&#35299;&#20915;Navier-Stokes&#26041;&#31243;&#26469;&#20272;&#35745;&#27969;&#20307;&#21160;&#21147;&#23398;&#65292;&#21363;&#20351;&#22312;&#39640;&#31471;&#30828;&#20214;&#19978;&#20063;&#35201;&#32791;&#36153;&#22823;&#37327;&#30340;&#35745;&#31639;&#26102;&#38388;&#21644;&#36164;&#28304;&#65292;&#36825;&#26159;&#19968;&#20010;&#26497;&#20854;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#21464;&#31181;&#30340;&#26041;&#27861;&#24050;&#32463;&#24320;&#22987;&#23581;&#35797;&#35299;&#20915;&#27492;&#31867;&#38382;&#39064;&#65292;&#20294;&#26159;&#36825;&#20123;&#26041;&#27861;&#37117;&#21482;&#38024;&#23545;&#20960;&#20309;&#24418;&#29366;&#22266;&#23450;&#30340;&#38745;&#24577;&#22330;&#26223;&#20013;&#30340;&#38745;&#24577;&#23545;&#35937;&#36827;&#34892;&#20102;&#25968;&#25454;&#38598;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#35797;&#22270;&#36229;&#36234;&#29616;&#26377;&#24037;&#20316;&#30340;&#22797;&#26434;&#24230;&#65292;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#12289;&#26041;&#27861;&#21644;&#22522;&#20934;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;EAGLE&#65292;&#19968;&#20010;&#21253;&#21547;1.1&#30334;&#19975;&#20010;&#20108;&#32500;&#32593;&#26684;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#32593;&#26684;&#26159;&#30001;&#19968;&#20010;&#31227;&#21160;&#27969;&#20307;&#28304;&#24341;&#36215;&#30340;&#19981;&#31283;&#23450;&#27969;&#20307;&#21160;&#21147;&#23398;&#27169;&#25311;&#29983;&#25104;&#30340;&#65292;&#20854;&#30456;&#20114;&#20316;&#29992;&#23548;&#33268;&#20102;&#38750;&#32447;&#24615;&#22330;&#26223;&#32467;&#26500;&#65292;&#20854;&#20013;&#21253;&#25324;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;600&#20010;&#19981;&#21516;&#22330;&#26223;&#12290;&#20026;&#20102;&#23545;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;EAGLE&#25968;&#25454;&#38598;&#36827;&#34892;&#26410;&#26469;&#30340;&#21387;&#21147;&#21644;&#36895;&#24230;&#39044;&#27979;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#32593;&#26684;&#21464;&#25442;&#22120;&#65292;&#23427;&#21033;&#29992;&#20102;&#33410;&#28857;&#32858;&#31867;&#12289;&#22270;&#27744;&#21270;&#21644;&#20840;&#23616;&#27744;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating fluid dynamics is classically done through the simulation and integration of numerical models solving the Navier-Stokes equations, which is computationally complex and time-consuming even on high-end hardware. This is a notoriously hard problem to solve, which has recently been addressed with machine learning, in particular graph neural networks (GNN) and variants trained and evaluated on datasets of static objects in static scenes with fixed geometry. We attempt to go beyond existing work in complexity and introduce a new model, method and benchmark. We propose EAGLE, a large-scale dataset of 1.1 million 2D meshes resulting from simulations of unsteady fluid dynamics caused by a moving flow source interacting with nonlinear scene structure, comprised of 600 different scenes of three different types. To perform future forecasting of pressure and velocity on the challenging EAGLE dataset, we introduce a new mesh transformer. It leverages node clustering, graph pooling and glo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#21160;&#21147;&#31995;&#32479;&#30340;&#35270;&#35282;&#65292;&#26469;&#35299;&#37322;&#24847;&#35782;&#30340;&#20016;&#23500;&#24615;&#21644;&#38590;&#20197;&#35328;&#35828;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#24847;&#35782;&#20307;&#39564;&#30340;&#20016;&#23500;&#24615;&#23545;&#24212;&#20110;&#24847;&#35782;&#29366;&#24577;&#20013;&#30340;&#20449;&#24687;&#37327;&#65292;&#32780;&#38590;&#20197;&#35328;&#35828;&#24615;&#21017;&#23545;&#24212;&#20110;&#19981;&#21516;&#22788;&#29702;&#38454;&#27573;&#20002;&#22833;&#30340;&#20449;&#24687;&#37327;&#12290;</title><link>http://arxiv.org/abs/2302.06403</link><description>&lt;p&gt;
&#29616;&#35937;&#24847;&#35782;&#29366;&#24577;&#30340;&#20016;&#23500;&#24615;&#21644;&#38590;&#20197;&#35328;&#35828;&#24615;&#30340;&#26469;&#28304;
&lt;/p&gt;
&lt;p&gt;
Sources of Richness and Ineffability for Phenomenally Conscious States. (arXiv:2302.06403v3 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#21160;&#21147;&#31995;&#32479;&#30340;&#35270;&#35282;&#65292;&#26469;&#35299;&#37322;&#24847;&#35782;&#30340;&#20016;&#23500;&#24615;&#21644;&#38590;&#20197;&#35328;&#35828;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#24847;&#35782;&#20307;&#39564;&#30340;&#20016;&#23500;&#24615;&#23545;&#24212;&#20110;&#24847;&#35782;&#29366;&#24577;&#20013;&#30340;&#20449;&#24687;&#37327;&#65292;&#32780;&#38590;&#20197;&#35328;&#35828;&#24615;&#21017;&#23545;&#24212;&#20110;&#19981;&#21516;&#22788;&#29702;&#38454;&#27573;&#20002;&#22833;&#30340;&#20449;&#24687;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides an information theoretic dynamical systems perspective on the richness and ineffability of consciousness. In their framework, the richness of conscious experience corresponds to the amount of information in a conscious state and ineffability corresponds to the amount of information lost at different stages of processing.
&lt;/p&gt;
&lt;p&gt;
&#24847;&#35782;&#29366;&#24577;&#65288;&#21363;&#26377;&#26576;&#31181;&#24863;&#21463;&#30340;&#29366;&#24577;&#65289;&#20284;&#20046;&#26082;&#20016;&#23500;&#21448;&#20805;&#28385;&#32454;&#33410;&#65292;&#21448;&#38590;&#20197;&#23436;&#20840;&#25551;&#36848;&#25110;&#22238;&#24518;&#12290;&#29305;&#21035;&#26159;&#38590;&#20197;&#35328;&#35828;&#24615;&#30340;&#38382;&#39064;&#26159;&#21746;&#23398;&#19978;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#37096;&#20998;&#28608;&#21457;&#20102;&#35299;&#37322;&#40511;&#27807;&#30340;&#20449;&#24565;&#65306;&#24847;&#35782;&#19981;&#33021;&#24402;&#32467;&#20026;&#22522;&#30784;&#29289;&#29702;&#36807;&#31243;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#21160;&#21147;&#31995;&#32479;&#30340;&#35270;&#35282;&#65292;&#26469;&#35299;&#37322;&#24847;&#35782;&#30340;&#20016;&#23500;&#24615;&#21644;&#38590;&#20197;&#35328;&#35828;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#24847;&#35782;&#20307;&#39564;&#30340;&#20016;&#23500;&#24615;&#23545;&#24212;&#20110;&#24847;&#35782;&#29366;&#24577;&#20013;&#30340;&#20449;&#24687;&#37327;&#65292;&#32780;&#38590;&#20197;&#35328;&#35828;&#24615;&#21017;&#23545;&#24212;&#20110;&#19981;&#21516;&#22788;&#29702;&#38454;&#27573;&#20002;&#22833;&#30340;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#24037;&#20316;&#35760;&#24518;&#20013;&#30340;&#21560;&#24341;&#23376;&#21160;&#21147;&#23398;&#22914;&#20309;&#23548;&#33268;&#25105;&#20204;&#21407;&#22987;&#20307;&#39564;&#30340;&#36139;&#20047;&#22238;&#24518;&#65292;&#35821;&#35328;&#30340;&#31163;&#25955;&#31526;&#21495;&#24615;&#36136;&#19981;&#36275;&#20197;&#25551;&#36848;&#20307;&#39564;&#30340;&#20016;&#23500;&#21644;&#39640;&#32500;&#32467;&#26500;&#65292;&#20197;&#21450;&#35748;&#30693;&#21151;&#33021;&#30456;&#20284;&#24615;&#22914;&#20309;&#24433;&#21709;&#20307;&#39564;&#30340;&#20849;&#20139;&#21644;&#20132;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conscious states (states that there is something it is like to be in) seem both rich or full of detail, and ineffable or hard to fully describe or recall. The problem of ineffability, in particular, is a longstanding issue in philosophy that partly motivates the explanatory gap: the belief that consciousness cannot be reduced to underlying physical processes. Here, we provide an information theoretic dynamical systems perspective on the richness and ineffability of consciousness. In our framework, the richness of conscious experience corresponds to the amount of information in a conscious state and ineffability corresponds to the amount of information lost at different stages of processing. We describe how attractor dynamics in working memory would induce impoverished recollections of our original experiences, how the discrete symbolic nature of language is insufficient for describing the rich and high-dimensional structure of experiences, and how similarity in the cognitive function o
&lt;/p&gt;</description></item><item><title>LAVA&#26159;&#19968;&#20010;&#22522;&#20110;&#39063;&#31890;&#21270;&#31070;&#32463;&#20803;&#32423;&#21035;&#21487;&#35299;&#37322;&#24615;&#30340; AI &#26694;&#26550;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#38745;&#33033;&#33639;&#20809;&#25104;&#20687;&#20013;&#35780;&#20272;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#36827;&#31243;&#65292;&#39564;&#35777;&#35270;&#32593;&#33180;&#34880;&#31649;&#31995;&#32479;&#20026; AD &#35780;&#20272;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#21644;&#35786;&#26029;&#25163;&#27573;&#12290;</title><link>http://arxiv.org/abs/2302.03008</link><description>&lt;p&gt;
LAVA&#65306;&#22522;&#20110;&#39063;&#31890;&#21270;&#31070;&#32463;&#20803;&#32423;&#21035;&#21487;&#35299;&#37322;&#24615;&#30340;&#38745;&#33033;&#33639;&#20809;&#25104;&#20687;&#35786;&#26029;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340; AI
&lt;/p&gt;
&lt;p&gt;
LAVA: Granular Neuron-Level Explainable AI for Alzheimer's Disease Assessment from Fundus Images. (arXiv:2302.03008v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03008
&lt;/p&gt;
&lt;p&gt;
LAVA&#26159;&#19968;&#20010;&#22522;&#20110;&#39063;&#31890;&#21270;&#31070;&#32463;&#20803;&#32423;&#21035;&#21487;&#35299;&#37322;&#24615;&#30340; AI &#26694;&#26550;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#38745;&#33033;&#33639;&#20809;&#25104;&#20687;&#20013;&#35780;&#20272;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#36827;&#31243;&#65292;&#39564;&#35777;&#35270;&#32593;&#33180;&#34880;&#31649;&#31995;&#32479;&#20026; AD &#35780;&#20272;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#21644;&#35786;&#26029;&#25163;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#26159;&#28176;&#36827;&#24615;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#65292;&#26159;&#30196;&#21574;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#26089;&#26399;&#35786;&#26029;&#23545;&#24739;&#32773;&#20174;&#28508;&#22312;&#30340;&#24178;&#39044;&#21644;&#27835;&#30103;&#20013;&#33719;&#30410;&#33267;&#20851;&#37325;&#35201;&#12290;&#35270;&#32593;&#33180;&#22240;&#20854;&#19982;&#22823;&#33041;&#30340;&#35299;&#21078;&#32852;&#31995;&#65292;&#34987;&#20551;&#23450;&#20026; AD &#26816;&#27979;&#30340;&#35786;&#26029;&#37096;&#20301;&#12290;&#28982;&#32780;&#30456;&#20851;&#30340; AI &#27169;&#22411;&#23578;&#26410;&#25552;&#20379;&#20851;&#20110;&#20915;&#31574;&#30340;&#21512;&#29702;&#35299;&#37322;&#65292;&#20063;&#26080;&#27861;&#25512;&#26029;&#30142;&#30149;&#36827;&#23637;&#30340;&#38454;&#27573;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#21487;&#35299;&#37322;&#24615; AI &#26694;&#26550;&#65292;&#31216;&#20026; Granular Neuron-level Explainer&#65288;LAVA&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#35299;&#37322;&#21407;&#22411;&#65292;&#21487;&#20197;&#25506;&#27979;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#27169;&#22411;&#30340;&#20013;&#38388;&#23618;&#20197;&#30452;&#25509;&#20174;&#35270;&#32593;&#33180;&#25104;&#20687;&#20013;&#35780;&#20272; AD &#36827;&#31243;&#12290;&#26412;&#26041;&#27861;&#24212;&#29992;&#20110;&#39564;&#35777;&#35270;&#32593;&#33180;&#34880;&#31649;&#31995;&#32479;&#20026; AD &#35780;&#20272;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#21644;&#35786;&#26029;&#25163;&#27573;&#12290;&#36890;&#36807; UK Biobank &#35748;&#30693;&#27979;&#35797;&#21644;&#34880;&#31649;&#37492;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alzheimer's Disease (AD) is a progressive neurodegenerative disease and the leading cause of dementia. Early diagnosis is critical for patients to benefit from potential intervention and treatment. The retina has been hypothesized as a diagnostic site for AD detection owing to its anatomical connection with the brain. Developed AI models for this purpose have yet to provide a rational explanation about the decision and neither infer the stage of disease's progression. Along this direction, we propose a novel model-agnostic explainable-AI framework, called Granular Neuron-level Explainer (LAVA), an interpretation prototype that probes into intermediate layers of the Convolutional Neural Network (CNN) models to assess the AD continuum directly from the retinal imaging without longitudinal or clinical evaluation. This method is applied to validate the retinal vasculature as a biomarker and diagnostic modality for Alzheimer's Disease (AD) evaluation. UK Biobank cognitive tests and vascular
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32593;&#32476;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;NetPeace&#65292;&#33021;&#22815;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#20013;&#33719;&#24471;&#26174;&#33879;&#25552;&#21319;&#65292;&#23588;&#20854;&#26159;&#22312;&#31264;&#23494;&#30340;&#30693;&#35782;&#22270;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2302.02614</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Pre-training Framework for Knowledge Graph Completion. (arXiv:2302.02614v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32593;&#32476;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;NetPeace&#65292;&#33021;&#22815;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#20013;&#33719;&#24471;&#26174;&#33879;&#25552;&#21319;&#65292;&#23588;&#20854;&#26159;&#22312;&#31264;&#23494;&#30340;&#30693;&#35782;&#22270;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;(KGC)&#26159;&#30830;&#23450;&#30693;&#35782;&#22270;&#35889;&#20013;&#26032;&#20107;&#23454;&#30340;&#26377;&#25928;&#26041;&#27861;&#20043;&#19968;&#12290;&#38500;&#20102;&#19968;&#20123;&#22522;&#20110;&#22270;&#32593;&#32476;&#30340;&#26041;&#27861;&#22806;&#65292;&#22823;&#37096;&#20998;KGC&#26041;&#27861;&#20542;&#21521;&#20110;&#22522;&#20110;&#29420;&#31435;&#30340;&#19977;&#20803;&#32452;&#36827;&#34892;&#35757;&#32451;&#65292;&#24456;&#38590;&#32771;&#34385;&#21040;&#30693;&#35782;&#32593;&#32476;&#20013;&#21253;&#21547;&#30340;&#20840;&#23616;&#32593;&#32476;&#36830;&#25509;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#26377;&#25928;&#30340;&#22522;&#20110;&#32593;&#32476;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;NetPeace&#65292;&#35813;&#26694;&#26550;&#32771;&#34385;&#20102;&#30693;&#35782;&#22270;&#35889;&#20013;&#20840;&#23616;&#32593;&#32476;&#36830;&#25509;&#20449;&#24687;&#21644;&#26412;&#22320;&#19977;&#20803;&#32452;&#20851;&#31995;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;NetPeace&#26694;&#26550;&#20013;&#30340;&#22810;&#20010;KGC&#27169;&#22411;&#23545;&#22522;&#20934;&#27979;&#35797;&#20013;&#65288;&#20363;&#22914;&#65292;&#22312;FB15k-237&#25968;&#25454;&#38598;&#19978;&#65292;TuckER&#30340;Hits@1&#24471;&#20998;&#22686;&#21152;&#20102;36.45%&#65292;MRR&#24471;&#20998;&#22686;&#21152;&#20102;27.40%&#65289;&#65292;&#29305;&#21035;&#26159;&#31264;&#23494;&#30693;&#35782;&#22270;&#30340;&#25552;&#21319;&#25345;&#32493;&#19988;&#26174;&#33879;&#12290;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20302;&#36164;&#28304;&#20219;&#21153;&#19978;&#65292;&#21463;&#30410;&#20110;KG&#30340;&#20840;&#23616;&#29305;&#24449;&#65292;NetPeace&#33719;&#24471;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#65288;104.03%&#30340;MRR&#22686;&#30410;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph completion (KGC) is one of the effective methods to identify new facts in knowledge graph. Except for a few methods based on graph network, most of KGC methods trend to be trained based on independent triples, while are difficult to take a full account of the information of global network connection contained in knowledge network. To address these issues, in this study, we propose a simple and effective Network-based Pre-training framework for knowledge graph completion (termed NetPeace), which takes into account the information of global network connection and local triple relationships in knowledge graph. Experiments show that in NetPeace framework, multiple KGC models yields consistent and significant improvements on benchmarks (e.g., 36.45% Hits@1 and 27.40% MRR improvements for TuckER on FB15k-237), especially dense knowledge graph. On the challenging low-resource task, NetPeace that benefits from the global features of KG achieves higher performance (104.03% MRR a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36138;&#23146;&#37325;&#25490;&#26435;&#37325;&#30697;&#38453;&#30340;&#31639;&#27861;AEIUOrder&#65292;&#33021;&#22815;&#26368;&#22823;&#21270;&#24635;&#30340;&#32463;&#36807;&#20805;&#20998;&#35757;&#32451;&#30340;&#23618;&#25152;&#36129;&#29486;&#30340;"well-trainedness"&#25351;&#26631;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#24182;&#22312;&#21508;&#31181;&#32763;&#35793;&#20219;&#21153;&#19978;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.02123</link><description>&lt;p&gt;
&#22522;&#20110;Transformers&#30340;&#36138;&#23146;&#25490;&#24207;&#20248;&#21270;&#32763;&#35793;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Greedy Ordering of Layer Weight Matrices in Transformers Improves Translation. (arXiv:2302.02123v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36138;&#23146;&#37325;&#25490;&#26435;&#37325;&#30697;&#38453;&#30340;&#31639;&#27861;AEIUOrder&#65292;&#33021;&#22815;&#26368;&#22823;&#21270;&#24635;&#30340;&#32463;&#36807;&#20805;&#20998;&#35757;&#32451;&#30340;&#23618;&#25152;&#36129;&#29486;&#30340;"well-trainedness"&#25351;&#26631;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#24182;&#22312;&#21508;&#31181;&#32763;&#35793;&#20219;&#21153;&#19978;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#24037;&#20316;&#35797;&#22270;&#22312;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#23376;&#22270;&#23618;&#30340;&#23618;&#27425;&#19978;&#29702;&#35299;&#22522;&#20110;Transformer&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#30340;&#20869;&#37096;&#32467;&#26500;&#21644;&#21151;&#33021;&#12290;&#20294;&#26159;&#65292;&#22914;&#26524;&#19981;&#26816;&#26597;&#20302;&#23618;&#27425;&#30340;&#32467;&#26500;&#65292;&#23601;&#19981;&#33021;&#28145;&#20837;&#29702;&#35299;&#23376;&#23618;&#37325;&#25490;&#32972;&#21518;&#30340;&#21160;&#26426;&#12290;&#26412;&#25991;&#36890;&#36807;AEIUOrder&#31639;&#27861;&#65292;&#36890;&#36807;&#34913;&#37327;&#32463;&#36807;&#20805;&#20998;&#35757;&#32451;&#30340;&#23618;&#30340;Heavy-Tailed Self-Regularization&#65288;HT-SR&#65289;&#25351;&#26631;&#65292;&#36138;&#23146;&#22320;&#23545;&#32534;&#30721;&#22120;&#20013;&#30340;&#23618;&#37325;&#37327;&#30697;&#38453;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#65292;&#28982;&#21518;&#30456;&#24212;&#22320;&#25490;&#24207;&#35299;&#30721;&#22120;&#30697;&#38453;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#24635;&#30340;&#32463;&#36807;&#20805;&#20998;&#35757;&#32451;&#30340;&#23618;&#25152;&#36129;&#29486;&#30340;"well-trainedness"&#25351;&#26631;&#36827;&#34892;&#37325;&#25490;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#34920;&#31034;&#65292;&#24182;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#32763;&#35793;&#36755;&#20986;&#65292;&#22312;&#21508;&#31181;&#32763;&#35793;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior work has attempted to understand the internal structures and functionalities of Transformer-based encoder-decoder architectures on the level of multi-head attention and feed-forward sublayers. Interpretations have focused on the encoder and decoder, along with the combinatorial possibilities of the self-attention, cross-attention, and feed-forward sublayers. However, without examining the low-level structures, one gains limited understanding of the motivation behind sublayer reordering. Could we dive into the sublayer abstraction and permute layer weight matrices to improve the quality of translation? We propose AEIUOrder to greedily reorder layer weight matrices in the encoder by their well-trainedness, as measured by Heavy-Tailed Self-Regularization (HT-SR) metrics, and order the decoder matrices correspondingly. Our results suggest that greedily reordering layer weight matrices to maximize Total well-trainedness facilitates the model to learn representations and generate trans
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#23398;&#20064;&#33258;&#21160;&#35780;&#20998;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#37197;&#21512;&#21305;&#37197;&#26631;&#26412;&#20316;&#20026;&#19979;&#19968;&#21477;&#39044;&#27979;&#25216;&#26415;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#31185;&#23398;&#25945;&#32946;&#39046;&#22495;&#30340;&#35770;&#35777;&#20219;&#21153;&#65292;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#35757;&#32451;&#25104;&#26412;&#21644;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2301.08771</link><description>&lt;p&gt;
&#21305;&#37197;&#26631;&#26412;&#20316;&#20026;&#19979;&#19968;&#21477;&#39044;&#27979;&#65306;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31185;&#23398;&#25945;&#32946;&#20013;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#33258;&#21160;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Matching Exemplar as Next Sentence Prediction (MeNSP): Zero-shot Prompt Learning for Automatic Scoring in Science Education. (arXiv:2301.08771v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#23398;&#20064;&#33258;&#21160;&#35780;&#20998;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#37197;&#21512;&#21305;&#37197;&#26631;&#26412;&#20316;&#20026;&#19979;&#19968;&#21477;&#39044;&#27979;&#25216;&#26415;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#31185;&#23398;&#25945;&#32946;&#39046;&#22495;&#30340;&#35770;&#35777;&#20219;&#21153;&#65292;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#35757;&#32451;&#25104;&#26412;&#21644;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#33021;&#22815;&#33258;&#21160;&#35780;&#20998;&#31185;&#23398;&#38382;&#39064;&#30340;&#23398;&#29983;&#20070;&#38754;&#31572;&#26696;&#30340;&#27169;&#22411;&#23545;&#20110;&#31185;&#23398;&#25945;&#32946;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#21644;&#26631;&#35760;&#36275;&#22815;&#30340;&#23398;&#29983;&#31572;&#26696;&#20197;&#35757;&#32451;&#27169;&#22411;&#26159;&#32791;&#26102;&#21644;&#36153;&#29992;&#39640;&#26114;&#30340;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;prompt&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#31185;&#23398;&#25945;&#32946;&#20013;&#36824;&#27809;&#26377;&#20351;&#29992;&#36807;&#36825;&#31181;&#25552;&#31034;&#26041;&#27861;&#30340;&#30740;&#31350;&#12290;&#30001;&#20110;&#23398;&#29983;&#30340;&#31572;&#26696;&#26159;&#29992;&#33258;&#28982;&#35821;&#35328;&#21576;&#29616;&#30340;&#65292;&#22240;&#27492;&#20351;&#29992;&#25552;&#31034;&#23558;&#35780;&#20998;&#36807;&#31243;&#23545;&#40784;&#20026;&#19979;&#19968;&#21477;&#39044;&#27979;&#20219;&#21153;&#21487;&#20197;&#36339;&#36807;&#26114;&#36149;&#30340;&#35843;&#25972;&#38454;&#27573;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21305;&#37197;&#26631;&#26412;&#20316;&#20026;&#19979;&#19968;&#21477;&#39044;&#27979;&#65288;MeNSP&#65289;&#24320;&#21457;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#33258;&#21160;&#35780;&#20998;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#35757;&#32451;&#26679;&#26412;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#35780;&#20998;&#19977;&#20010;&#31185;&#23398;&#35770;&#35777;&#20219;&#21153;&#20013;&#24212;&#29992;MeNSP&#65292;&#24182;&#21457;&#29616;&#26426;&#22120;-&#20154;&#35780;&#20998;&#30340;&#19968;&#33268;&#24615;&#65292;Cohen&#30340;Kappa&#31995;&#25968;&#22312;0.30&#21040;0.57&#20043;&#38388;&#65292;F1&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
Developing models to automatically score students' written responses to science problems is critical for science education. However, collecting and labeling sufficient student responses for training models is time and cost-consuming. Recent studies suggest that pre-trained language models (PLMs) can be adapted to downstream tasks without fine-tuning with prompts. However, no research has employed such a prompt approach in science education. As student responses are presented with natural language, aligning the scoring procedure as the next sentence prediction task using prompts can skip the costly fine-tuning stage. In this study, we developed a zero-shot approach to automatically score student responses via Matching Exemplars as Next Sentence Prediction (MeNSP). This approach employs no training samples. We first apply MeNSP in scoring three assessment tasks of scientific argumentation and found machine-human scoring agreements, Cohen's Kappa ranges from 0.30 to 0.57, and F1 score ran
&lt;/p&gt;</description></item><item><title>SIRL&#26159;&#22522;&#20110;&#20154;&#25552;&#20379;&#30456;&#20284;&#24230;&#21028;&#26029;&#30340;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#24110;&#21161;&#26426;&#22120;&#20154;&#35782;&#21035;&#21644;&#38548;&#31163;&#22240;&#26524;&#29305;&#24449;&#24182;&#29983;&#25104;&#36866;&#24403;&#34892;&#20026;&#65292;&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2301.00810</link><description>&lt;p&gt;
SIRL: &#22522;&#20110;&#30456;&#20284;&#24230;&#30340;&#38544;&#24335;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SIRL: Similarity-based Implicit Representation Learning. (arXiv:2301.00810v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00810
&lt;/p&gt;
&lt;p&gt;
SIRL&#26159;&#22522;&#20110;&#20154;&#25552;&#20379;&#30456;&#20284;&#24230;&#21028;&#26029;&#30340;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#24110;&#21161;&#26426;&#22120;&#20154;&#35782;&#21035;&#21644;&#38548;&#31163;&#22240;&#26524;&#29305;&#24449;&#24182;&#29983;&#25104;&#36866;&#24403;&#34892;&#20026;&#65292;&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26426;&#22120;&#20154;&#20351;&#29992;&#39640;&#23481;&#37327;&#27169;&#22411;&#20197;&#21407;&#22987;&#29366;&#24577;&#20316;&#20026;&#36755;&#20837;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#26102;&#65292;&#20182;&#20204;&#38656;&#35201;&#21516;&#26102;&#23398;&#20064;&#20219;&#21153;&#30340;&#8220;&#29305;&#24449;&#8221;&#34920;&#31034;&#21450;&#22914;&#20309;&#23558;&#36825;&#20123;&#29305;&#24449;&#32452;&#21512;&#25104;&#19968;&#20010;&#30446;&#26631;&#12290;&#22914;&#26524;&#20182;&#20204;&#23581;&#35797;&#20174;&#29992;&#20110;&#25945;&#25480;&#23436;&#25972;&#22870;&#21169;&#20989;&#25968;&#30340;&#36755;&#20837;&#20013;&#21516;&#26102;&#23398;&#20064;&#20004;&#32773;&#65292;&#24456;&#23481;&#26131;&#20135;&#29983;&#21253;&#21547;&#25968;&#25454;&#20013;&#20551;&#30456;&#20851;&#24615;&#30340;&#34920;&#31034;&#65292;&#23548;&#33268;&#19981;&#33021;&#25512;&#24191;&#21040;&#26032;&#30340;&#29615;&#22659;&#20013;&#12290;&#25105;&#20204;&#30340;&#26368;&#32456;&#30446;&#26631;&#26159;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#35782;&#21035;&#21644;&#38548;&#31163;&#20154;&#20204;&#23454;&#38469;&#20851;&#24515;&#21644;&#20351;&#29992;&#30340;&#22240;&#26524;&#29305;&#24449;&#65292;&#24403;&#34920;&#31034;&#29366;&#24577;&#21644;&#34892;&#20026;&#26102;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#26159;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#35810;&#38382;&#29992;&#25143;&#35748;&#20026;&#30456;&#20284;&#30340;&#34892;&#20026;&#26469;&#35843;&#25972;&#36825;&#31181;&#34920;&#31034;&#65306;&#22914;&#26524;&#20851;&#38190;&#29305;&#24449;&#30456;&#20284;&#65292;&#36825;&#20123;&#34892;&#20026;&#23558;&#30456;&#20284;&#65292;&#21363;&#20351;&#20302;&#23618;&#34892;&#20026;&#26377;&#25152;&#19981;&#21516;&#65307;&#30456;&#21453;&#65292;&#22914;&#26524;&#21363;&#20351;&#26377;&#19968;&#20010;&#20851;&#38190;&#29305;&#24449;&#19981;&#21516;&#65292;&#37027;&#20040;&#36825;&#20123;&#34892;&#20026;&#23601;&#20250;&#26377;&#25152;&#19981;&#21516;&#12290;&#36825;&#27491;&#26159;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#23398;&#20064;&#20219;&#21153;&#30446;&#26631;&#24182;&#29983;&#25104;&#36866;&#24403;&#34892;&#20026;&#30340;&#20851;&#38190;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#30456;&#20284;&#24230;&#30340;&#38544;&#24335;&#34920;&#31034;&#23398;&#20064;&#65288;SIRL&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#30001;&#20154;&#25552;&#20379;&#30340;&#30456;&#20284;&#24615;&#21028;&#26029;&#26469;&#23398;&#20064;&#38544;&#24335;&#20219;&#21153;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#20219;&#21153;&#19978;&#35780;&#20272;SIRL&#65292;&#24182;&#23637;&#31034;&#23427;&#22312;&#26368;&#32456;&#20219;&#21153;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#32988;&#36807;&#20854;&#20182;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
When robots learn reward functions using high capacity models that take raw state directly as input, they need to both learn a representation for what matters in the task -- the task ``features" -- as well as how to combine these features into a single objective. If they try to do both at once from input designed to teach the full reward function, it is easy to end up with a representation that contains spurious correlations in the data, which fails to generalize to new settings. Instead, our ultimate goal is to enable robots to identify and isolate the causal features that people actually care about and use when they represent states and behavior. Our idea is that we can tune into this representation by asking users what behaviors they consider similar: behaviors will be similar if the features that matter are similar, even if low-level behavior is different; conversely, behaviors will be different if even one of the features that matter differs. This, in turn, is what enables the rob
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#35299;&#37322;&#36873;&#25321;&#27169;&#22411;&#65292;&#20351;&#29992;&#20856;&#22411;&#30340;&#27010;&#24565;&#30340;&#32447;&#24615;&#32452;&#21512;&#26469;&#35299;&#37322;&#20854;&#33258;&#36523;&#30340;&#39044;&#27979;&#65292;&#36890;&#36807;&#36873;&#25321;&#22823;&#37096;&#20998;&#28608;&#27963;&#19981;&#21516;&#27010;&#24565;&#30340;&#23376;&#24207;&#21015;&#20316;&#20026;&#20856;&#22411;&#37096;&#20998;&#26469;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#65292;&#20026;&#20102;&#26356;&#22909;&#30340;&#35299;&#37322;&#24615;&#65292;&#36824;&#35774;&#35745;&#20102;&#22810;&#20010;&#32422;&#26463;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2212.03396</link><description>&lt;p&gt;
&#23398;&#20064;&#36873;&#25321;&#20856;&#22411;&#37096;&#20998;&#20197;&#35299;&#37322;&#24207;&#21015;&#25968;&#25454;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Learning to Select Prototypical Parts for Interpretable Sequential Data Modeling. (arXiv:2212.03396v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03396
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#35299;&#37322;&#36873;&#25321;&#27169;&#22411;&#65292;&#20351;&#29992;&#20856;&#22411;&#30340;&#27010;&#24565;&#30340;&#32447;&#24615;&#32452;&#21512;&#26469;&#35299;&#37322;&#20854;&#33258;&#36523;&#30340;&#39044;&#27979;&#65292;&#36890;&#36807;&#36873;&#25321;&#22823;&#37096;&#20998;&#28608;&#27963;&#19981;&#21516;&#27010;&#24565;&#30340;&#23376;&#24207;&#21015;&#20316;&#20026;&#20856;&#22411;&#37096;&#20998;&#26469;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#65292;&#20026;&#20102;&#26356;&#22909;&#30340;&#35299;&#37322;&#24615;&#65292;&#36824;&#35774;&#35745;&#20102;&#22810;&#20010;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21407;&#22411;&#35299;&#37322;&#26041;&#27861;&#36890;&#36807;&#23558;&#26679;&#26412;&#19982;&#21442;&#32771;&#38598;&#20013;&#30340;&#20856;&#22411;&#20195;&#34920;&#36827;&#34892;&#30456;&#20284;&#24230;&#27604;&#36739;&#65292;&#25552;&#20379;&#20102;&#27169;&#22411;&#39044;&#27979;&#30340;&#30452;&#35266;&#35299;&#37322;&#12290;&#22312;&#24207;&#21015;&#25968;&#25454;&#24314;&#27169;&#39046;&#22495;&#65292;&#21407;&#22411;&#30340;&#30456;&#20284;&#24615;&#35745;&#31639;&#36890;&#24120;&#22522;&#20110;&#32534;&#30721;&#34920;&#31034;&#21521;&#37327;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#24230;&#36882;&#24402;&#30340;&#20989;&#25968;&#65292;&#21407;&#22411;&#35299;&#37322;&#19982;&#21407;&#22987;&#36755;&#20837;&#20043;&#38388;&#36890;&#24120;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#35299;&#37322;&#36873;&#25321;&#27169;&#22411;&#65288;SESM&#65289;&#65292;&#23427;&#20351;&#29992;&#20856;&#22411;&#30340;&#27010;&#24565;&#30340;&#32447;&#24615;&#32452;&#21512;&#26469;&#35299;&#37322;&#20854;&#33258;&#36523;&#30340;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#24605;&#24819;&#65292;&#36890;&#36807;&#36873;&#25321;&#22823;&#37096;&#20998;&#28608;&#27963;&#19981;&#21516;&#27010;&#24565;&#30340;&#23376;&#24207;&#21015;&#20316;&#20026;&#20856;&#22411;&#37096;&#20998;&#26469;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#65292;&#29992;&#25143;&#21487;&#20197;&#23558;&#20854;&#19982;&#36873;&#25321;&#33258;&#19981;&#21516;&#31034;&#20363;&#36755;&#20837;&#30340;&#23376;&#24207;&#21015;&#36827;&#34892;&#27604;&#36739;&#20197;&#29702;&#35299;&#27169;&#22411;&#20915;&#31574;&#12290;&#20026;&#20102;&#26356;&#22909;&#30340;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22810;&#20010;&#32422;&#26463;&#26465;&#20214;&#65292;&#21253;&#25324;&#22810;&#26679;&#24615;&#65292;&#31283;&#23450;&#24615;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prototype-based interpretability methods provide intuitive explanations of model prediction by comparing samples to a reference set of memorized exemplars or typical representatives in terms of similarity. In the field of sequential data modeling, similarity calculations of prototypes are usually based on encoded representation vectors. However, due to highly recursive functions, there is usually a non-negligible disparity between the prototype-based explanations and the original input. In this work, we propose a Self-Explaining Selective Model (SESM) that uses a linear combination of prototypical concepts to explain its own predictions. The model employs the idea of case-based reasoning by selecting sub-sequences of the input that mostly activate different concepts as prototypical parts, which users can compare to sub-sequences selected from different example inputs to understand model decisions. For better interpretability, we design multiple constraints including diversity, stabilit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#30340;&#24847;&#22270;&#35782;&#21035;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;&#32858;&#31867;&#31639;&#27861;&#21644;&#29992;&#25143;&#35805;&#35821;&#23884;&#20837;&#31354;&#38388;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;MiniLM&#19982;&#23618;&#27425;&#32858;&#31867;&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24847;&#22270;&#24402;&#32435;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.02021</link><description>&lt;p&gt;
&#19982;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#30340;&#24847;&#22270;&#35782;&#21035;&#30456;&#20851;&#30340;&#35805;&#35821;&#23884;&#20837;&#21644;&#32858;&#31867;&#26041;&#27861;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of Utterance Embeddings and Clustering Methods Related to Intent Induction for Task-Oriented Dialogue. (arXiv:2212.02021v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#30340;&#24847;&#22270;&#35782;&#21035;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;&#32858;&#31867;&#31639;&#27861;&#21644;&#29992;&#25143;&#35805;&#35821;&#23884;&#20837;&#31354;&#38388;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;MiniLM&#19982;&#23618;&#27425;&#32858;&#31867;&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24847;&#22270;&#24402;&#32435;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#20197;&#20811;&#26381;&#35774;&#35745;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#22270;&#35889;&#20013;&#30340;&#20856;&#22411;&#25361;&#25112;&#65306;&#20026;&#27599;&#20010;&#23545;&#35805;&#36716;&#25240;&#25351;&#23450;&#24847;&#22270;&#26631;&#31614;&#65288;&#24847;&#22270;&#32858;&#31867;&#65289;&#24182;&#22522;&#20110;&#24847;&#22270;&#32858;&#31867;&#26041;&#27861;&#29983;&#25104;&#19968;&#32452;&#24847;&#22270;&#65288;&#24847;&#22270;&#24402;&#32435;&#65289;&#12290;&#25105;&#20204;&#20551;&#35774;&#33258;&#21160;&#24402;&#32435;&#24847;&#22270;&#26377;&#20004;&#20010;&#26174;&#33879;&#22240;&#32032;&#65306;&#65288;1&#65289;&#24847;&#22270;&#26631;&#31614;&#30340;&#32858;&#31867;&#31639;&#27861;&#21644;&#65288;2&#65289;&#29992;&#25143;&#35805;&#35821;&#23884;&#20837;&#31354;&#38388;&#12290; &#25105;&#20204;&#26681;&#25454;DSTC11&#35780;&#20272;&#27604;&#36739;&#20102;&#29616;&#26377;&#30340;&#25104;&#21697;&#32858;&#31867;&#27169;&#22411;&#21644;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35748;&#30495;&#32771;&#34385;&#24847;&#22270;&#24402;&#32435;&#20219;&#21153;&#20013;&#35805;&#35821;&#23884;&#20837;&#21644;&#32858;&#31867;&#26041;&#27861;&#30340;&#32508;&#21512;&#36873;&#25321;&#26159;&#24517;&#35201;&#30340;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;MiniLM&#19982;&#23618;&#27425;&#32858;&#31867;&#30456;&#32467;&#21512;&#21487;&#26174;&#33879;&#25552;&#39640;&#24847;&#22270;&#24402;&#32435;&#20219;&#21153;&#20013;&#30340;NMI&#65292;ARI&#65292;F1&#65292;&#20934;&#30830;&#24615;&#21644;&#31034;&#20363;&#35206;&#30422;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/Jeiyoon/dstc11-track2&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
The focus of this work is to investigate unsupervised approaches to overcome quintessential challenges in designing task-oriented dialog schema: assigning intent labels to each dialog turn (intent clustering) and generating a set of intents based on the intent clustering methods (intent induction). We postulate there are two salient factors for automatic induction of intents: (1) clustering algorithm for intent labeling and (2) user utterance embedding space. We compare existing off-the-shelf clustering models and embeddings based on DSTC11 evaluation. Our extensive experiments demonstrate that the combined selection of utterance embedding and clustering method in the intent induction task should be carefully considered. We also present that pretrained MiniLM with Agglomerative clustering shows significant improvement in NMI, ARI, F1, accuracy and example coverage in intent induction tasks. The source codes are available at https://github.com/Jeiyoon/dstc11-track2.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;CF-&#36924;&#36817;&#31354;&#38388;&#30340;&#34920;&#31034;&#39046;&#22495;&#65292;&#20171;&#32461;&#20102;CF-&#36924;&#36817;&#31354;&#38388;&#21644;CF&#38381;&#38598;&#30340;&#27010;&#24565;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;CF-&#36924;&#36817;&#31354;&#38388;&#20013;&#30340;CF&#38381;&#38598;&#26063;&#26159;&#36830;&#32493;&#39046;&#22495;&#65292;&#20351;&#29992;&#33539;&#30068;&#35770;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;CF-&#36924;&#36817;&#31354;&#38388;&#21644;CF-&#21487;&#36924;&#36817;&#20851;&#31995;&#30340;&#33539;&#30068;&#19982;&#36830;&#32493;&#39046;&#22495;&#21644;Scott&#36830;&#32493;&#26144;&#23556;&#30340;&#33539;&#30068;&#31561;&#21516;&#12290;</title><link>http://arxiv.org/abs/2211.17099</link><description>&lt;p&gt;
&#36890;&#36807;CF-&#36924;&#36817;&#31354;&#38388;&#30340;&#34920;&#31034;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
Representations of Domains via CF-approximation Spaces. (arXiv:2211.17099v2 [math.RA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.17099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;CF-&#36924;&#36817;&#31354;&#38388;&#30340;&#34920;&#31034;&#39046;&#22495;&#65292;&#20171;&#32461;&#20102;CF-&#36924;&#36817;&#31354;&#38388;&#21644;CF&#38381;&#38598;&#30340;&#27010;&#24565;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;CF-&#36924;&#36817;&#31354;&#38388;&#20013;&#30340;CF&#38381;&#38598;&#26063;&#26159;&#36830;&#32493;&#39046;&#22495;&#65292;&#20351;&#29992;&#33539;&#30068;&#35770;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;CF-&#36924;&#36817;&#31354;&#38388;&#21644;CF-&#21487;&#36924;&#36817;&#20851;&#31995;&#30340;&#33539;&#30068;&#19982;&#36830;&#32493;&#39046;&#22495;&#21644;Scott&#36830;&#32493;&#26144;&#23556;&#30340;&#33539;&#30068;&#31561;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#30340;&#34920;&#31034;&#36890;&#24120;&#25351;&#23558;&#39046;&#22495;&#34920;&#31034;&#20026;&#19968;&#20123;&#21547;&#26377;&#26576;&#20123;&#25968;&#23398;&#32467;&#26500;&#30340;&#36866;&#24403;&#26063;&#65292;&#35813;&#26063;&#24102;&#26377;&#38598;&#21512;&#21253;&#21547;&#39034;&#24207;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;CF-&#36924;&#36817;&#31354;&#38388;&#30340;&#34920;&#31034;&#39046;&#22495;&#12290;&#20171;&#32461;&#20102;CF-&#36924;&#36817;&#31354;&#38388;&#21644;CF&#38381;&#38598;&#30340;&#27010;&#24565;&#65292;&#35777;&#26126;&#20102;&#22312;&#24102;&#26377;&#38598;&#21512;&#21253;&#21547;&#39034;&#24207;&#30340;CF-&#36924;&#36817;&#31354;&#38388;&#20013;&#65292;CF&#38381;&#38598;&#26063;&#26159;&#36830;&#32493;&#39046;&#22495;&#65292;&#32780;&#19988;&#27599;&#20010;&#36830;&#32493;&#39046;&#22495;&#37117;&#19982;&#26576;&#20010;&#24102;&#26377;&#38598;&#21512;&#21253;&#21547;&#39034;&#24207;&#30340;CF-&#36924;&#36817;&#31354;&#38388;&#30340;CF&#38381;&#38598;&#26063;&#21516;&#26500;&#12290;&#20351;&#29992;&#33539;&#30068;&#35770;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;CF-&#21487;&#36924;&#36817;&#20851;&#31995;&#30340;&#27010;&#24565;&#65292;&#36825;&#21518;&#26469;&#26377;&#21161;&#20110;&#35777;&#26126;CF-&#36924;&#36817;&#31354;&#38388;&#21644;CF-&#21487;&#36924;&#36817;&#20851;&#31995;&#30340;&#33539;&#30068;&#31561;&#21516;&#20110;&#36830;&#32493;&#39046;&#22495;&#21644;Scott&#36830;&#32493;&#26144;&#23556;&#30340;&#33539;&#30068;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representations of domains mean in a general way representing a domain as a suitable family endowed with set-inclusion order of some mathematical structures. In this paper, representations of domains via CF-approximation spaces are considered. Concepts of CF-approximation spaces and CF-closed sets are introduced. It is proved that the family of CF-closed sets in a CF-approximation space endowed with set-inclusion order is a continuous domain and that every continuous domain is isomorphic to the family of CF-closed sets of some CF-approximation space endowed with set-inclusion order. The concept of CF-approximable relations is introduced using a categorical approach, which later facilitates the proof that the category of CF-approximation spaces and CF-approximable relations is equivalent to that of continuous domains and Scott continuous maps.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;ICU&#33041;&#30005;&#30417;&#27979;&#20013;&#24120;&#35265;&#30340;6&#31181;&#33041;&#27874;&#22270;&#26696;&#30340;&#23384;&#22312;&#65292;&#24182;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#21644;&#19977;&#31181;&#35299;&#37322;&#26041;&#27861;&#65292;&#36825;&#23545;&#20110;&#24314;&#31435;AI&#30340;&#20449;&#20219;&#21644;&#20020;&#24202;&#37319;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2211.05207</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#26469;&#35782;&#21035;&#30315;&#30187;-&#38388;&#38553;-&#25439;&#20260;&#36830;&#32493;&#29366;&#24577;&#19979;&#30340;&#33041;&#30005;&#22270;&#22270;&#26696;
&lt;/p&gt;
&lt;p&gt;
An Interpretable Machine Learning System to Identify EEG Patterns on the Ictal-Interictal-Injury Continuum. (arXiv:2211.05207v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05207
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;ICU&#33041;&#30005;&#30417;&#27979;&#20013;&#24120;&#35265;&#30340;6&#31181;&#33041;&#27874;&#22270;&#26696;&#30340;&#23384;&#22312;&#65292;&#24182;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#21644;&#19977;&#31181;&#35299;&#37322;&#26041;&#27861;&#65292;&#36825;&#23545;&#20110;&#24314;&#31435;AI&#30340;&#20449;&#20219;&#21644;&#20020;&#24202;&#37319;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#21307;&#23398;&#39046;&#22495;&#65292;&#20154;&#20204;&#21628;&#21505;&#22312;&#29992;&#20110;&#20020;&#24202;&#24037;&#20316;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#22686;&#21152;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;ICU&#33041;&#30005;&#30417;&#27979;&#20013;&#24120;&#35265;&#30340;6&#31181;&#33041;&#27874;&#22270;&#26696;&#65288;&#30315;&#30187;&#12289;LPD&#12289;GPD&#12289;LRDA&#12289;GRDA&#12289;&#20854;&#20182;&#65289;&#30340;&#23384;&#22312;&#12290;&#27599;&#20010;&#39044;&#27979;&#37117;&#37197;&#26377;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#65292;&#20511;&#21161;&#20110;&#19987;&#38376;&#30340;&#29992;&#25143;&#30028;&#38754;&#25552;&#20379;&#25903;&#25345;&#12290;&#27492;&#26032;&#22411;&#27169;&#22411;&#26550;&#26500;&#23398;&#20064;&#20102;&#19968;&#32452;&#21407;&#22411;&#31034;&#20363;&#65288;&#8220;&#21407;&#22411;&#8221;&#65289;&#65292;&#24182;&#36890;&#36807;&#23558;&#26032;&#30340;EEG&#29255;&#27573;&#19982;&#36825;&#20123;&#21407;&#22411;&#36827;&#34892;&#27604;&#36739;&#26469;&#20570;&#20986;&#20915;&#31574;&#12290;&#36825;&#20123;&#21407;&#22411;&#21487;&#20197;&#26159;&#21333;&#31867;&#65288;&#20165;&#19982;&#19968;&#20010;&#31867;&#30456;&#20851;&#65289;&#25110;&#21452;&#31867;&#65288;&#19982;&#20004;&#20010;&#31867;&#30456;&#20851;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#20027;&#35201;&#30340;&#27169;&#22411;&#35299;&#37322;&#26041;&#27861;&#65306;1&#65289;&#20351;&#29992;&#20840;&#23616;&#32467;&#26500;&#20445;&#25345;&#26041;&#27861;&#65292;&#23558;1275&#32500;cEEG&#28508;&#22312;&#29305;&#24449;&#26144;&#23556;&#21040;&#20108;&#32500;&#31354;&#38388;&#20013;&#65292;&#21487;&#35270;&#21270;&#30315;&#30187;-&#38388;&#38553;-&#25439;&#20260;&#36830;&#32493;&#29366;&#24577;&#65292;&#20174;&#32780;&#28145;&#20837;&#20102;&#35299;&#20854;&#39640;&#32500;&#32467;&#26500;&#12290;2&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#35299;&#37322;&#26041;&#27861;&#65292;&#20351;&#20154;&#31867;&#19987;&#23478;&#33021;&#22815;&#26597;&#35810;&#27169;&#22411;&#39044;&#27979;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#24182;&#20197;&#33258;&#28982;&#35821;&#35328;&#25509;&#25910;&#32463;&#36807;&#19987;&#23478;&#39564;&#35777;&#30340;&#35299;&#37322;&#12290;3&#65289;&#25105;&#20204;&#21487;&#35270;&#21270;&#20102;&#23548;&#33268;&#27169;&#22411;&#20570;&#20986;&#26576;&#20010;&#20915;&#31574;&#30340;&#36755;&#20837;&#30340;&#26368;&#37325;&#35201;&#29305;&#24449;&#65292;&#20801;&#35768;&#35814;&#32454;&#26816;&#26597;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35299;&#37322;&#24615;&#27169;&#22411;&#20998;&#31867;EEG&#22270;&#26696;&#21644;&#25552;&#20379;&#19987;&#23478;&#21451;&#22909;&#30340;&#35299;&#37322;&#30340;&#23454;&#29992;&#24615;&#65292;&#36825;&#20004;&#20010;&#26041;&#38754;&#23545;&#20110;&#24314;&#31435;AI&#30340;&#20449;&#20219;&#21644;&#20020;&#24202;&#37319;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many medical subfields, there is a call for greater interpretability in the machine learning systems used for clinical work. In this paper, we design an interpretable deep learning model to predict the presence of 6 types of brainwave patterns (Seizure, LPD, GPD, LRDA, GRDA, other) commonly encountered in ICU EEG monitoring. Each prediction is accompanied by a high-quality explanation delivered with the assistance of a specialized user interface. This novel model architecture learns a set of prototypical examples (``prototypes'') and makes decisions by comparing a new EEG segment to these prototypes. These prototypes are either single-class (affiliated with only one class) or dual-class (affiliated with two classes).  We present three main ways of interpreting the model: 1) Using global-structure preserving methods, we map the 1275-dimensional cEEG latent features to a 2D space to visualize the ictal-interictal-injury continuum and gain insight into its high-dimensional structure. 2
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#29992;&#20363;&#20998;&#26512;&#25506;&#35752;&#20102;&#22522;&#20110;AI&#25216;&#26415;&#30340;&#36131;&#20219;&#21046;&#24230;&#19979;&#35777;&#26126;&#36127;&#25285;&#30340;&#25361;&#25112;&#21644;&#35268;&#21017;&#25913;&#38761;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2211.01817</link><description>&lt;p&gt;
AI&#26102;&#20195;&#30340;&#36131;&#20219;&#21046;&#24230;&#65306;&#22522;&#20110;&#29992;&#20363;&#30340;&#35777;&#26126;&#36127;&#25285;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Liability regimes in the age of AI: a use-case driven analysis of the burden of proof. (arXiv:2211.01817v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#29992;&#20363;&#20998;&#26512;&#25506;&#35752;&#20102;&#22522;&#20110;AI&#25216;&#26415;&#30340;&#36131;&#20219;&#21046;&#24230;&#19979;&#35777;&#26126;&#36127;&#25285;&#30340;&#25361;&#25112;&#21644;&#35268;&#21017;&#25913;&#38761;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39537;&#21160;&#30340;&#26032;&#20852;&#25216;&#26415;&#26377;&#21487;&#33021;&#20026;&#25105;&#20204;&#30340;&#31038;&#20250;&#24102;&#26469;&#39072;&#35206;&#24615;&#30340;&#36716;&#22411;&#65292;&#24182;&#25512;&#36827;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#30340;&#22810;&#39033;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#20854;&#20013;&#25968;&#25454;&#39537;&#21160;&#30340;&#23398;&#20064;&#26041;&#27861;&#65288;&#21363;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#65289;&#26159;&#19968;&#27425;&#30495;&#27491;&#30340;&#38761;&#21629;&#12290;&#20294;&#19982;&#27492;&#21516;&#26102;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#36825;&#20123;&#26041;&#27861;&#23398;&#22266;&#26377;&#30340;&#26576;&#20123;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#21487;&#33021;&#23545;&#23433;&#20840;&#21644;&#22522;&#26412;&#26435;&#21033;&#24102;&#26469;&#28508;&#22312;&#39118;&#38505;&#12290;&#23613;&#31649;&#22312;&#37319;&#29992;&#36807;&#31243;&#20013;&#26377;&#26426;&#21046;&#26469;&#26368;&#23567;&#21270;&#36825;&#20123;&#39118;&#38505;&#65288;&#20363;&#22914;&#23433;&#20840;&#35268;&#23450;&#65289;&#65292;&#20294;&#36825;&#24182;&#19981;&#25490;&#38500;&#25439;&#23475;&#21457;&#29983;&#30340;&#21487;&#33021;&#65292;&#22914;&#26524;&#36825;&#31181;&#24773;&#20917;&#21457;&#29983;&#65292;&#21463;&#23475;&#32773;&#24212;&#35813;&#33021;&#22815;&#23547;&#27714;&#34917;&#20607;&#12290;&#22240;&#27492;&#65292;&#36131;&#20219;&#21046;&#24230;&#23558;&#22312;&#30830;&#20445;&#20351;&#29992;&#25110;&#19982;&#36825;&#20123;&#31995;&#32479;&#20132;&#20114;&#30340;&#21463;&#23475;&#32773;&#30340;&#22522;&#26412;&#20445;&#25252;&#26041;&#38754;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20351;AI&#31995;&#32479;&#22266;&#26377;&#39118;&#38505;&#30340;&#30456;&#21516;&#29305;&#24449;&#65292;&#20363;&#22914;&#32570;&#20047;&#22240;&#26524;&#20851;&#31995;&#12289;&#19981;&#36879;&#26126;&#12289;&#19981;&#21487;&#39044;&#27979;&#25110;&#20182;&#20204;&#33258;&#25105;&#21644;&#33258;&#36866;&#24212;&#30340;&#26412;&#36136;&#65292;&#20063;&#20351;&#24471;&#20256;&#32479;&#30340;&#36131;&#20219;&#35268;&#21017;&#38590;&#20197;&#24212;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#21508;&#31181;&#29992;&#20363;&#21450;&#20854;&#28508;&#22312;&#21361;&#23475;&#30340;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#29992;&#20363;&#30340;&#36131;&#20219;&#35268;&#21017;&#35777;&#26126;&#36127;&#25285;&#20998;&#26512;&#65292;&#35782;&#21035;&#20102;&#24403;&#21069;&#36131;&#20219;&#35268;&#21017;&#22312;AI&#25216;&#26415;&#39046;&#22495;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#25506;&#35752;&#20102;&#25913;&#38761;&#36825;&#20123;&#35268;&#21017;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
New emerging technologies powered by Artificial Intelligence (AI) have the potential to disruptively transform our societies for the better. In particular, data-driven learning approaches (i.e., Machine Learning (ML)) have been a true revolution in the advancement of multiple technologies in various application domains. But at the same time there is growing concern about certain intrinsic characteristics of these methodologies that carry potential risks to both safety and fundamental rights. Although there are mechanisms in the adoption process to minimize these risks (e.g., safety regulations), these do not exclude the possibility of harm occurring, and if this happens, victims should be able to seek compensation. Liability regimes will therefore play a key role in ensuring basic protection for victims using or interacting with these systems. However, the same characteristics that make AI systems inherently risky, such as lack of causality, opacity, unpredictability or their self and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21338;&#24328;&#22343;&#34913;&#27010;&#24565;&#8212;&#8212;&#21487;&#35266;&#27979;&#23436;&#32654;&#22343;&#34913;&#65292;&#22312;&#39034;&#24207;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#20013;&#21487;&#20197;&#24110;&#21161;&#21019;&#24314;&#30495;&#27491;&#30340;&#31574;&#30053;&#20195;&#29702;&#12290;&#36825;&#31181;&#22343;&#34913;&#27010;&#24565;&#22312;&#20844;&#24320;&#35266;&#23519;&#30340;&#34892;&#21160;&#27010;&#29575;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.16506</link><description>&lt;p&gt;
&#21487;&#35266;&#27979;&#23436;&#32654;&#22343;&#34913; (Observable Perfect Equilibrium)
&lt;/p&gt;
&lt;p&gt;
Observable Perfect Equilibrium. (arXiv:2210.16506v5 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21338;&#24328;&#22343;&#34913;&#27010;&#24565;&#8212;&#8212;&#21487;&#35266;&#27979;&#23436;&#32654;&#22343;&#34913;&#65292;&#22312;&#39034;&#24207;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#20013;&#21487;&#20197;&#24110;&#21161;&#21019;&#24314;&#30495;&#27491;&#30340;&#31574;&#30053;&#20195;&#29702;&#12290;&#36825;&#31181;&#22343;&#34913;&#27010;&#24565;&#22312;&#20844;&#24320;&#35266;&#23519;&#30340;&#34892;&#21160;&#27010;&#29575;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#32435;&#20160;&#22343;&#34913;&#25104;&#20026;&#20102;&#21338;&#24328;&#35770;&#30340;&#26680;&#24515;&#35299;&#20915;&#26041;&#26696;&#27010;&#24565;&#65292;&#35768;&#22810;&#37325;&#35201;&#30340;&#21338;&#24328;&#21253;&#21547;&#22810;&#20010;&#32435;&#20160;&#22343;&#34913;&#65292;&#25105;&#20204;&#24517;&#39035;&#30830;&#23450;&#22914;&#20309;&#22312;&#20854;&#20013;&#36873;&#25321;&#65292;&#20197;&#21019;&#24314;&#30495;&#27491;&#30340;&#31574;&#30053;&#20195;&#29702;&#12290;&#20026;&#39034;&#24207;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#25552;&#20986;&#20102;&#20960;&#20010;&#32435;&#20160;&#22343;&#34913;&#32454;&#21270;&#27010;&#24565;&#65292;&#20854;&#20013;&#26368;&#31361;&#20986;&#30340;&#26159;&#39076;&#25238;&#25163;&#23436;&#32654;&#22343;&#34913;&#12289;&#25311;&#23436;&#32654;&#22343;&#34913;&#21644;&#26368;&#36817;&#25552;&#20986;&#30340;&#21333;&#20391;&#25311;&#23436;&#32654;&#22343;&#34913;&#12290;&#36825;&#20123;&#27010;&#24565;&#23545;&#26576;&#20123;&#20219;&#24847;&#23567;&#30340;&#38169;&#35823;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#20445;&#35777;&#22987;&#32456;&#23384;&#22312;&#12290;&#20294;&#25105;&#20204;&#35748;&#20026;&#65292;&#23545;&#20110;&#21457;&#23637;&#39034;&#24207;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#20013;&#24378;&#22823;&#30340;&#20195;&#29702;&#20154;&#65292;&#36825;&#20123;&#27010;&#24565;&#37117;&#19981;&#27491;&#30830;&#12290;&#25105;&#20204;&#20026;&#28216;&#25103;&#26641;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22343;&#34913;&#27010;&#24565;&#8212;&#8212;&#21487;&#35266;&#27979;&#23436;&#32654;&#22343;&#34913;&#65292;&#22312;&#20854;&#20013;&#65292;&#35299;&#20915;&#26041;&#26696;&#22312;&#20844;&#24320;&#35266;&#23519;&#30340;&#34892;&#21160;&#27010;&#29575;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#65288;&#24182;&#19981;&#19968;&#23450;&#38024;&#23545;&#25152;&#26377;&#21487;&#33021;&#19981;&#21487;&#35266;&#23519;&#30340;&#34892;&#21160;&#27010;&#29575;&#20855;&#26377;&#40065;&#26834;&#24615;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Nash equilibrium has emerged as the central game-theoretic solution concept, many important games contain several Nash equilibria and we must determine how to select between them in order to create real strategic agents. Several Nash equilibrium refinement concepts have been proposed and studied for sequential imperfect-information games, the most prominent being trembling-hand perfect equilibrium, quasi-perfect equilibrium, and recently one-sided quasi-perfect equilibrium. These concepts are robust to certain arbitrarily small mistakes, and are guaranteed to always exist; however, we argue that neither of these is the correct concept for developing strong agents in sequential games of imperfect information. We define a new equilibrium refinement concept for extensive-form games called observable perfect equilibrium in which the solution is robust over trembles in publicly-observable action probabilities (not necessarily over all action probabilities that may not be observable by
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24212;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#23558;&#34920;&#26684;&#25968;&#25454;&#24207;&#21015;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#23383;&#31526;&#20018;&#36827;&#34892;&#20998;&#31867;&#65292;&#24494;&#35843;&#21518;&#21363;&#21487;&#22312;&#38750;&#24120;&#23569;&#30340;&#26679;&#26412;&#35774;&#32622;&#19979;&#19982;&#20256;&#32479;&#22522;&#32447;&#26041;&#27861;&#31454;&#20105;&#21147;&#21313;&#36275;&#12290;</title><link>http://arxiv.org/abs/2210.10723</link><description>&lt;p&gt;
TabLLM: &#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#34920;&#26684;&#25968;&#25454;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
TabLLM: Few-shot Classification of Tabular Data with Large Language Models. (arXiv:2210.10723v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24212;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#23558;&#34920;&#26684;&#25968;&#25454;&#24207;&#21015;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#23383;&#31526;&#20018;&#36827;&#34892;&#20998;&#31867;&#65292;&#24494;&#35843;&#21518;&#21363;&#21487;&#22312;&#38750;&#24120;&#23569;&#30340;&#26679;&#26412;&#35774;&#32622;&#19979;&#19982;&#20256;&#32479;&#22522;&#32447;&#26041;&#27861;&#31454;&#20105;&#21147;&#21313;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#34920;&#26684;&#25968;&#25454;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#23558;&#34920;&#26684;&#25968;&#25454;&#24207;&#21015;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#23383;&#31526;&#20018;&#65292;&#24182;&#21152;&#19978;&#20998;&#31867;&#38382;&#39064;&#30340;&#31616;&#30701;&#25551;&#36848;&#65292;&#28982;&#21518;&#21551;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20123;&#26631;&#35760;&#26679;&#26412;&#24494;&#35843;&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#22810;&#31181;&#24207;&#21015;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#27169;&#26495;&#12289;&#34920;&#26684;&#21040;&#25991;&#26412;&#27169;&#22411;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;&#23613;&#31649;&#26041;&#27861;&#31616;&#21333;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23427;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#34920;&#26684;&#20998;&#31867;&#26041;&#27861;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#26159;&#38646;&#26679;&#26412;&#20998;&#31867;&#20063;&#33719;&#24471;&#20102;&#38750;&#24179;&#20961;&#30340;&#34920;&#29616;&#65292;&#35828;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#32534;&#30721;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;&#19982;&#35768;&#22810;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#38750;&#24120;&#23569;&#30340;&#26679;&#26412;&#35774;&#32622;&#19979;&#20063;&#19982;&#24378;&#22823;&#30340;&#20256;&#32479;&#22522;&#32447;&#26041;&#27861;&#65288;&#22914;&#26799;&#24230;&#25552;&#21319;&#26641;&#65289;&#31454;&#20105;&#21147;&#21313;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the application of large language models to zero-shot and few-shot classification of tabular data. We prompt the large language model with a serialization of the tabular data to a natural-language string, together with a short description of the classification problem. In the few-shot setting, we fine-tune the large language model using some labeled examples. We evaluate several serialization methods including templates, table-to-text models, and large language models. Despite its simplicity, we find that this technique outperforms prior deep-learning-based tabular classification methods on several benchmark datasets. In most cases, even zero-shot classification obtains non-trivial performance, illustrating the method's ability to exploit prior knowledge encoded in large language models. Unlike many deep learning methods for tabular datasets, this approach is also competitive with strong traditional baselines like gradient-boosted trees, especially in the very-few-shot setting
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;MOAN&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26497;&#24230;&#26377;&#38480;&#30340;&#26631;&#31614;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;MOAN&#30001;&#20004;&#20010;&#20114;&#34917;&#30340;&#27969;&#27700;&#32447;&#32452;&#25104;&#65306;&#21327;&#21516;&#37051;&#22495;&#25366;&#25496;&#21644;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MOAN&#22312;Dice&#35780;&#20998;&#21644;&#20854;&#20182;&#35780;&#20272;&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.13476</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;:&#26497;&#24230;&#26377;&#38480;&#26631;&#31614;&#19979;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Mine yOur owN Anatomy: Revisiting Medical Image Segmentation with Extremely Limited Labels. (arXiv:2209.13476v4 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13476
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;MOAN&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26497;&#24230;&#26377;&#38480;&#30340;&#26631;&#31614;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;MOAN&#30001;&#20004;&#20010;&#20114;&#34917;&#30340;&#27969;&#27700;&#32447;&#32452;&#25104;&#65306;&#21327;&#21516;&#37051;&#22495;&#25366;&#25496;&#21644;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MOAN&#22312;Dice&#35780;&#20998;&#21644;&#20854;&#20182;&#35780;&#20272;&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20851;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#30740;&#31350;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#32972;&#26223;&#19979;&#20165;&#20973;&#20511;&#20960;&#20010;&#26631;&#31614;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#23454;&#20307;&#21306;&#20998;&#21644;&#19981;&#21464;&#26144;&#23556;&#19978;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38754;&#20020;&#19977;&#20010;&#24120;&#35265;&#29942;&#39048;&#65306;(1)&#23614;&#37096;&#20998;&#24067;&#65306;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#36890;&#24120;&#36981;&#24490;&#38544;&#21547;&#30340;&#38271;&#23614;&#31867;&#20998;&#24067;&#12290;&#30450;&#30446;&#21033;&#29992;&#25152;&#26377;&#35757;&#32451;&#20687;&#32032;&#21487;&#33021;&#23548;&#33268;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#23548;&#33268;&#24615;&#33021;&#24694;&#21270;&#65307;(2)&#19968;&#33268;&#24615;&#65306;&#30001;&#20110;&#19981;&#21516;&#35299;&#21078;&#29305;&#24449;&#20043;&#38388;&#30340;&#31867;&#20869;&#21464;&#21270;&#65292;&#20998;&#21106;&#27169;&#22411;&#26159;&#21542;&#23398;&#20250;&#20102;&#26377;&#24847;&#20041;&#19988;&#19968;&#33268;&#30340;&#35299;&#21078;&#29305;&#24449;&#20173;&#19981;&#28165;&#26970;&#65307;&#20197;&#21450;(3)&#22810;&#26679;&#24615;&#65306;&#25972;&#20010;&#25968;&#25454;&#38598;&#20869;&#37096;&#20999;&#29255;&#30340;&#30456;&#20851;&#24615;&#21463;&#21040;&#30340;&#20851;&#27880;&#26174;&#33879;&#36739;&#23569;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#23547;&#25214;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#38598;&#26412;&#36523;&#30340;&#31574;&#30053;&#26041;&#27861;&#65292;&#20174;&#19981;&#21516;&#30340;&#35299;&#21078;&#35270;&#22270;&#20013;&#21457;&#29616;&#30456;&#20284;&#20294;&#19981;&#21516;&#30340;&#26679;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;MOAN&#65292;&#29992;&#20110;&#26497;&#24230;&#26377;&#38480;&#26631;&#31614;&#19979;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290; MOAN&#30001;&#20004;&#20010;&#20114;&#34917;&#30340;&#27969;&#27700;&#32447;&#32452;&#25104;&#65306;&#21327;&#21516;&#37051;&#22495;&#25366;&#25496;&#21644;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#12290;&#21069;&#32773;&#25552;&#21462;&#20849;&#21516;&#20381;&#36182;&#30340;&#20687;&#32032;&#21306;&#22495;&#65292;&#32780;&#21518;&#32773;&#24378;&#21046;&#32593;&#32476;&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#35299;&#21078;&#23398;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MOAN&#22312;Dice&#35780;&#20998;&#21644;&#20854;&#20182;&#35780;&#20272;&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies on contrastive learning have achieved remarkable performance solely by leveraging few labels in the context of medical image segmentation. Existing methods mainly focus on instance discrimination and invariant mapping. However, they face three common pitfalls: (1) tailness: medical image data usually follows an implicit long-tail class distribution. Blindly leveraging all pixels in training hence can lead to the data imbalance issues, and cause deteriorated performance; (2) consistency: it remains unclear whether a segmentation model has learned meaningful and yet consistent anatomical features due to the intra-class variations between different anatomical features; and (3) diversity: the intra-slice correlations within the entire dataset have received significantly less attention. This motivates us to seek a principled approach for strategically making use of the dataset itself to discover similar yet distinct samples from different anatomical views. In this paper, we i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#20851;&#31995;&#26080;&#29615;&#21270;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;&#24490;&#29615;&#20851;&#31995;&#30340;&#22240;&#26524;&#27169;&#22411;&#65292;&#20174;&#32780;&#23398;&#20064;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2208.12210</link><description>&lt;p&gt;
&#36890;&#36807;&#20851;&#31995;&#26080;&#29615;&#21270;&#23398;&#20064;&#24102;&#26377;&#24490;&#29615;&#20851;&#31995;&#30340;&#20851;&#31995;&#22240;&#26524;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Relational Causal Models with Cycles through Relational Acyclification. (arXiv:2208.12210v7 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#20851;&#31995;&#26080;&#29615;&#21270;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;&#24490;&#29615;&#20851;&#31995;&#30340;&#22240;&#26524;&#27169;&#22411;&#65292;&#20174;&#32780;&#23398;&#20064;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20114;&#30456;&#24433;&#21709;&#25110;&#20855;&#26377;&#22240;&#26524;&#25928;&#24212;&#30340;&#32852;&#36890;&#21333;&#20803;&#30340;&#29616;&#23454;&#19990;&#30028;&#29616;&#35937;&#20013;&#65292;&#24179;&#34913;&#29366;&#24577;&#36890;&#24120;&#29992;&#22270;&#27169;&#22411;&#20013;&#30340;&#24490;&#29615;&#34920;&#31034;&#12290;&#20851;&#31995;&#22240;&#26524;&#27169;&#22411;&#26159;&#19968;&#31181;&#34920;&#36798;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#19988;&#33021;&#23637;&#31034;&#24490;&#29615;&#25110;&#21453;&#39304;&#29615;&#30340;&#22270;&#27169;&#22411;&#65292;&#21487;&#20197;&#34920;&#31034;&#21644;&#25512;&#29702;&#12290;&#29616;&#26377;&#30340;&#20174;&#35266;&#27979;&#25968;&#25454;&#23398;&#20064;&#22240;&#26524;&#27169;&#22411;&#30340;&#24490;&#29615;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#20551;&#23450;&#25968;&#25454;&#23454;&#20363;&#26159;&#29420;&#31435;&#19988;&#21516;&#20998;&#24067;&#30340;&#65292;&#22240;&#27492;&#19981;&#36866;&#29992;&#20110;&#20851;&#31995;&#22240;&#26524;&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#20026;&#20851;&#31995;&#22240;&#26524;&#27169;&#22411;&#35774;&#35745;&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#20551;&#23450;&#20102;&#26080;&#29615;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#32422;&#26463;&#22522;&#30340;&#20851;&#31995;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#22312;&#20851;&#31995;&#24490;&#29615;&#22240;&#26524;&#27169;&#22411;&#20013;&#20026;&#20160;&#20040;&#26159;&#27491;&#30830;&#19988;&#23436;&#20840;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;&#25105;&#20204;&#24341;&#20837;&#20851;&#31995;&#26080;&#29615;&#21270;&#20316;&#20026;&#19968;&#20010;&#20026;&#20851;&#31995;&#27169;&#22411;&#35774;&#35745;&#30340;&#25805;&#20316;&#65292;&#33021;&#22815;&#23545;&#21487;&#30830;&#23450;&#24615;&#20570;&#20986;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world phenomena which involve mutual influence or causal effects between interconnected units, equilibrium states are typically represented with cycles in graphical models. An expressive class of graphical models, relational causal models, can represent and reason about complex dynamic systems exhibiting such cycles or feedback loops. Existing cyclic causal discovery algorithms for learning causal models from observational data assume that the data instances are independent and identically distributed which makes them unsuitable for relational causal models. At the same time, causal discovery algorithms for relational causal models assume acyclicity. In this work, we examine the necessary and sufficient conditions under which a constraint-based relational causal discovery algorithm is sound and complete for cyclic relational causal models. We introduce relational acyclification, an operation specifically designed for relational models that enables reasoning about the identifiab
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#26102;&#38388;&#27880;&#24847;&#21147;&#21333;&#20803;&#65288;TAU&#65289;&#20197;&#25552;&#39640;&#26102;&#31354;&#39044;&#27979;&#23398;&#20064;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#23558;&#26102;&#38388;&#27880;&#24847;&#21147;&#20998;&#35299;&#20026;&#24103;&#20869;&#38745;&#24577;&#27880;&#24847;&#21147;&#21644;&#24103;&#38388;&#21160;&#24577;&#27880;&#24847;&#21147;&#12290;&#21516;&#26102;&#24341;&#20837;&#24046;&#20998;&#31163;&#25955;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#32771;&#34385;&#24103;&#38388;&#21464;&#21270;&#65292;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#39044;&#27979;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2206.12126</link><description>&lt;p&gt;
&#26102;&#38388;&#27880;&#24847;&#21147;&#21333;&#20803;&#65306;&#38754;&#21521;&#39640;&#25928;&#26102;&#31354;&#39044;&#27979;&#23398;&#20064;&#30340;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Temporal Attention Unit: Towards Efficient Spatiotemporal Predictive Learning. (arXiv:2206.12126v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#26102;&#38388;&#27880;&#24847;&#21147;&#21333;&#20803;&#65288;TAU&#65289;&#20197;&#25552;&#39640;&#26102;&#31354;&#39044;&#27979;&#23398;&#20064;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#23558;&#26102;&#38388;&#27880;&#24847;&#21147;&#20998;&#35299;&#20026;&#24103;&#20869;&#38745;&#24577;&#27880;&#24847;&#21147;&#21644;&#24103;&#38388;&#21160;&#24577;&#27880;&#24847;&#21147;&#12290;&#21516;&#26102;&#24341;&#20837;&#24046;&#20998;&#31163;&#25955;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#32771;&#34385;&#24103;&#38388;&#21464;&#21270;&#65292;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#39044;&#27979;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#39044;&#27979;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#21382;&#21490;&#24103;&#26469;&#29983;&#25104;&#26410;&#26469;&#24103;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26102;&#31354;&#39044;&#27979;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#31354;&#38388;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#25429;&#25417;&#24103;&#20869;&#29305;&#24449;&#65292;&#20013;&#38388;&#30340;&#26102;&#38388;&#27169;&#22359;&#25429;&#25417;&#24103;&#38388;&#30456;&#20851;&#24615;&#12290;&#34429;&#28982;&#20027;&#27969;&#26041;&#27861;&#37319;&#29992;&#36882;&#24402;&#21333;&#20803;&#26469;&#25429;&#33719;&#38271;&#26399;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#20294;&#30001;&#20110;&#26080;&#27861;&#24182;&#34892;&#21270;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#23427;&#20204;&#30340;&#35745;&#31639;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#20102;&#24182;&#34892;&#21270;&#26102;&#38388;&#27169;&#22359;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26102;&#38388;&#27880;&#24847;&#21147;&#21333;&#20803;&#65288;TAU&#65289;&#65292;&#23558;&#26102;&#38388;&#27880;&#24847;&#21147;&#20998;&#35299;&#20026;&#24103;&#20869;&#38745;&#24577;&#27880;&#24847;&#21147;&#21644;&#24103;&#38388;&#21160;&#24577;&#27880;&#24847;&#21147;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#20391;&#37325;&#20110;&#24103;&#20869;&#35823;&#24046;&#65292;&#20294;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24046;&#20998;&#31163;&#25955;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#32771;&#34385;&#24103;&#38388;&#21464;&#21270;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#27966;&#29983;&#27169;&#22411;&#21487;&#20197;&#39640;&#25928;&#22320;&#36827;&#34892;&#39044;&#27979;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatiotemporal predictive learning aims to generate future frames by learning from historical frames. In this paper, we investigate existing methods and present a general framework of spatiotemporal predictive learning, in which the spatial encoder and decoder capture intra-frame features and the middle temporal module catches inter-frame correlations. While the mainstream methods employ recurrent units to capture long-term temporal dependencies, they suffer from low computational efficiency due to their unparallelizable architectures. To parallelize the temporal module, we propose the Temporal Attention Unit (TAU), which decomposes the temporal attention into intra-frame statical attention and inter-frame dynamical attention. Moreover, while the mean squared error loss focuses on intra-frame errors, we introduce a novel differential divergence regularization to take inter-frame variations into account. Extensive experiments demonstrate that the proposed method enables the derived mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#21306;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;GraphPart&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#24341;&#20837;&#39069;&#22806;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#30340;&#27880;&#37322;&#39044;&#31639;&#32422;&#26463;&#19979;&#24615;&#33021;&#31283;&#23450;&#12290;</title><link>http://arxiv.org/abs/2201.09391</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#21306;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Partition-Based Active Learning for Graph Neural Networks. (arXiv:2201.09391v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.09391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#21306;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;GraphPart&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#24341;&#20837;&#39069;&#22806;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#30340;&#27880;&#37322;&#39044;&#31639;&#32422;&#26463;&#19979;&#24615;&#33021;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20027;&#21160;&#23398;&#20064;&#35774;&#32622;&#19979;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36827;&#34892;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20998;&#21306;&#30340;GNN&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;GraphPart&#12290;GraphPart&#39318;&#20808;&#23558;&#22270;&#20998;&#25104;&#19981;&#30456;&#20132;&#30340;&#23376;&#22270;&#65292;&#28982;&#21518;&#22312;&#27599;&#20010;&#23376;&#22270;&#20013;&#36873;&#25321;&#20195;&#34920;&#24615;&#33410;&#28857;&#36827;&#34892;&#26597;&#35810;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#22270;&#21644;&#33410;&#28857;&#29305;&#24449;&#19979;&#30340;&#29616;&#23454;&#24179;&#28369;&#20551;&#35774;&#30340;&#20998;&#31867;&#35823;&#24046;&#30340;&#26032;&#39062;&#20998;&#26512;&#12290;&#23545;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#27880;&#37322;&#39044;&#31639;&#32422;&#26463;&#19979;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;GNN&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#19981;&#20250;&#24341;&#20837;&#39069;&#22806;&#30340;&#36229;&#21442;&#25968;&#65292;&#36825;&#23545;&#20110;&#27169;&#22411;&#35757;&#32451;&#23588;&#20026;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#27809;&#26377;&#26631;&#35760;&#39564;&#35777;&#38598;&#30340;&#20027;&#21160;&#23398;&#20064;&#35774;&#32622;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of semi-supervised learning with Graph Neural Networks (GNNs) in an active learning setup. We propose GraphPart, a novel partition-based active learning approach for GNNs. GraphPart first splits the graph into disjoint partitions and then selects representative nodes within each partition to query. The proposed method is motivated by a novel analysis of the classification error under realistic smoothness assumptions over the graph and the node features. Extensive experiments on multiple benchmark datasets demonstrate that the proposed method outperforms existing active learning methods for GNNs under a wide range of annotation budget constraints. In addition, the proposed method does not introduce additional hyperparameters, which is crucial for model training, especially in the active learning setting where a labeled validation set may not be available.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#36807;&#25311;&#21512;&#29305;&#24615;&#30340;&#21518;&#35757;&#32451;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#35782;&#21035;&#38169;&#35823;&#26631;&#35760;&#30340;&#26679;&#26412;&#65292;&#24182;&#36880;&#27493;&#21024;&#38500;&#23545;&#20915;&#31574;&#36793;&#30028;&#26377;&#36739;&#39640;&#24433;&#21709;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2106.07217</link><description>&lt;p&gt;
&#22522;&#20110;&#36807;&#25311;&#21512;&#27169;&#22411;&#29305;&#24615;&#30340;&#22122;&#22768;&#26631;&#31614;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Over-Fit: Noisy-Label Detection based on the Overfitted Model Property. (arXiv:2106.07217v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.07217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#36807;&#25311;&#21512;&#29305;&#24615;&#30340;&#21518;&#35757;&#32451;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#35782;&#21035;&#38169;&#35823;&#26631;&#35760;&#30340;&#26679;&#26412;&#65292;&#24182;&#36880;&#27493;&#21024;&#38500;&#23545;&#20915;&#31574;&#36793;&#30028;&#26377;&#36739;&#39640;&#24433;&#21709;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#39640;&#23481;&#37327;&#29305;&#24615;&#65292;&#21363;&#20351;&#26159;&#22122;&#22768;&#26631;&#31614;&#65292;&#20063;&#23481;&#26131;&#36807;&#24230;&#25311;&#21512;&#65292;&#20174;&#32780;&#38477;&#20302;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#35757;&#32451;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#21547;&#22122;&#22768;&#26631;&#31614;&#30340;&#25968;&#25454;&#20013;&#26174;&#33879;&#25552;&#39640;&#20219;&#20309;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#35757;&#32451;&#27169;&#22411;&#30340;&#36807;&#25311;&#21512;&#29305;&#24615;&#26469;&#35782;&#21035;&#38169;&#35823;&#26631;&#35760;&#30340;&#26679;&#26412;&#65292;&#36880;&#27493;&#21024;&#38500;&#23545;&#20915;&#31574;&#36793;&#30028;&#26377;&#36739;&#39640;&#24433;&#21709;&#30340;&#26679;&#26412;&#65292;&#24182;&#25913;&#21892;&#20915;&#31574;&#36793;&#30028;&#26469;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21518;&#35757;&#32451;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#20855;&#26377;&#24456;&#22909;&#30340;&#21327;&#21516;&#25928;&#26524;&#12290;&#22312;&#22810;&#31181;&#30495;&#23454;&#19990;&#30028;&#21644;&#21512;&#25104;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#21508;&#31181;&#23454;&#38469;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural network can easily overfit to even noisy labels due to its high capacity, which degrades the generalization performance of a model. To overcome this issue, we propose a new approach for learning from noisy labels (LNL) via post-training, which can significantly improve the generalization performance of any pre-trained model on noisy label data. To this end, we rather exploit the overfitting property of a trained model to identify mislabeled samples. Specifically, our post-training approach gradually removes samples with high influence on the decision boundary and refines the decision boundary to improve generalization performance. Our post-training approach creates great synergies when combined with the existing LNL methods. Experimental results on various real-world and synthetic benchmark datasets demonstrate the validity of our approach in diverse realistic scenarios.
&lt;/p&gt;</description></item></channel></rss>