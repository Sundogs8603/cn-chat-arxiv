<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#23545;Anchors&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2303.08806</link><description>&lt;p&gt;
&#29702;&#35299;&#20107;&#21518;&#35299;&#37322;&#22120;&#65306;&#20197;Anchors&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Understanding Post-hoc Explainers: The Case of Anchors. (arXiv:2303.08806v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;Anchors&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#26159;&#19968;&#39033;&#39640;&#24230;&#35201;&#27714;&#20294;&#38590;&#20197;&#23454;&#29616;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#30340;&#20010;&#20307;&#39044;&#27979;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#26412;&#22320;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20135;&#29983;&#35299;&#37322;&#30340;&#36807;&#31243;&#23545;&#20110;&#29992;&#25143;&#26469;&#35828;&#21487;&#33021;&#19982;&#35201;&#35299;&#37322;&#30340;&#39044;&#27979;&#19968;&#26679;&#31070;&#31192;&#12290;&#27492;&#22806;&#65292;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#32463;&#24120;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#19988;&#23427;&#20204;&#22312;&#31616;&#21333;&#27169;&#22411;&#19978;&#30340;&#34892;&#20026;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#12290;&#26412;&#25991;&#23545;Anchors&#65288;Ribeiro&#31561;&#20154;&#65292;2018&#65289;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65306;&#19968;&#31181;&#27969;&#34892;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#23427;&#24378;&#35843;&#19968;&#23567;&#32452;&#21333;&#35789;&#20197;&#35299;&#37322;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many scenarios, the interpretability of machine learning models is a highly required but difficult task. To explain the individual predictions of such models, local model-agnostic approaches have been proposed. However, the process generating the explanations can be, for a user, as mysterious as the prediction to be explained. Furthermore, interpretability methods frequently lack theoretical guarantees, and their behavior on simple models is frequently unknown. While it is difficult, if not impossible, to ensure that an explainer behaves as expected on a cutting-edge model, we can at least ensure that everything works on simple, already interpretable models. In this paper, we present a theoretical analysis of Anchors (Ribeiro et al., 2018): a popular rule-based interpretability method that highlights a small set of words to explain a text classifier's decision. After formalizing its algorithm and providing useful insights, we demonstrate mathematically that Anchors produces meaningf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;spaCy&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#26816;&#27979;Gmail&#20013;&#30340;&#22403;&#22334;&#37038;&#20214;&#65292;&#32467;&#26524;&#26174;&#31034;&#22810;&#23618;&#24863;&#30693;&#22120;&#31639;&#27861;&#22312;&#22403;&#22334;&#37038;&#20214;&#26816;&#27979;&#20013;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;96&#65285;&#12290;</title><link>http://arxiv.org/abs/2303.08792</link><description>&lt;p&gt;
&#20351;&#29992;spaCy&#26500;&#24314;&#26377;&#25928;&#30340;&#30005;&#23376;&#37038;&#20214;&#22403;&#22334;&#37038;&#20214;&#20998;&#31867;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Building an Effective Email Spam Classification Model with spaCy. (arXiv:2303.08792v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;spaCy&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#26816;&#27979;Gmail&#20013;&#30340;&#22403;&#22334;&#37038;&#20214;&#65292;&#32467;&#26524;&#26174;&#31034;&#22810;&#23618;&#24863;&#30693;&#22120;&#31639;&#27861;&#22312;&#22403;&#22334;&#37038;&#20214;&#26816;&#27979;&#20013;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;96&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#20154;&#20204;&#20351;&#29992;Gmail&#12289;Outlook&#12289;AOL Mail&#31561;&#30005;&#23376;&#37038;&#20214;&#26381;&#21153;&#24555;&#36895;&#30456;&#20114;&#27807;&#36890;&#65292;&#21457;&#36865;&#20449;&#24687;&#21644;&#23448;&#26041;&#20449;&#20989;&#12290;&#22403;&#22334;&#37038;&#20214;&#26159;&#36825;&#31181;&#36890;&#20449;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#36890;&#24120;&#26159;&#30001;&#20725;&#23608;&#32593;&#32476;&#21457;&#36865;&#30340;&#65292;&#26088;&#22312;&#21521;&#19981;&#21516;&#30340;&#20154;&#32676;&#24191;&#21578;&#12289;&#25439;&#23475;&#21644;&#31363;&#21462;&#20449;&#24687;&#12290;&#27599;&#22825;&#25910;&#21040;&#22823;&#37327;&#19981;&#24819;&#35201;&#30340;&#22403;&#22334;&#37038;&#20214;&#20250;&#22635;&#28385;&#25910;&#20214;&#31665;&#25991;&#20214;&#22841;&#12290;&#22240;&#27492;&#65292;&#22403;&#22334;&#37038;&#20214;&#26816;&#27979;&#26159;&#19968;&#39033;&#22522;&#26412;&#30340;&#25361;&#25112;&#65292;&#30446;&#21069;&#24050;&#32463;&#26377;&#35768;&#22810;&#24037;&#20316;&#20351;&#29992;&#32858;&#31867;&#21644;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#26469;&#26816;&#27979;&#22403;&#22334;&#37038;&#20214;&#12290;&#26412;&#25991;&#20316;&#32773;&#20351;&#29992;&#20102;spaCy&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24211;&#21644;Python&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;3&#31181;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#26420;&#32032;&#36125;&#21494;&#26031;&#65288;NB&#65289;&#12289;&#20915;&#31574;&#26641;C45&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#26469;&#26816;&#27979;&#20174;Gmail&#26381;&#21153;&#25910;&#38598;&#30340;&#22403;&#22334;&#37038;&#20214;&#12290;&#35266;&#23519;&#32467;&#26524;&#26174;&#31034;&#65292;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#31639;&#27861;&#22312;&#22403;&#22334;&#37038;&#20214;&#26816;&#27979;&#20013;&#30340;&#20934;&#30830;&#29575;&#20026;96&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Today, people use email services such as Gmail, Outlook, AOL Mail, etc. to communicate with each other as quickly as possible to send information and official letters. Spam or junk mail is a major challenge to this type of communication, usually sent by botnets with the aim of advertising, harming and stealing information in bulk to different people. Receiving unwanted spam emails on a daily basis fills up the inbox folder. Therefore, spam detection is a fundamental challenge, so far many works have been done to detect spam using clustering and text categorisation methods. In this article, the author has used the spaCy natural language processing library and 3 machine learning (ML) algorithms Naive Bayes (NB), Decision Tree C45 and Multilayer Perceptron (MLP) in the Python programming language to detect spam emails collected from the Gmail service. Observations show the accuracy rate (96%) of the Multilayer Perceptron (MLP) algorithm in spam detection.
&lt;/p&gt;</description></item><item><title>PLEX&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21033;&#29992;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#35270;&#35273;&#36816;&#21160;&#36712;&#36857;&#21644;&#22823;&#37327;&#30340;&#20219;&#21153;&#26465;&#20214;&#19979;&#30340;&#29289;&#20307;&#25805;&#20316;&#35270;&#39057;&#65292;&#22312;&#23398;&#20064;&#36890;&#29992;&#30340;&#25805;&#32437;&#20363;&#31243;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#35270;&#39057;&#28436;&#31034;&#23398;&#20064;&#22914;&#20309;&#22312;&#36825;&#20010;&#29305;&#24449;&#31354;&#38388;&#20013;&#35268;&#21010;&#21508;&#31181;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.08789</link><description>&lt;p&gt;
PLEX&#65306;&#21033;&#29992;&#21487;&#29992;&#25968;&#25454;&#36827;&#34892;&#26426;&#22120;&#20154;&#25805;&#32437;&#39044;&#35757;&#32451;&#30340;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
PLEX: Making the Most of the Available Data for Robotic Manipulation Pretraining. (arXiv:2303.08789v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08789
&lt;/p&gt;
&lt;p&gt;
PLEX&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21033;&#29992;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#35270;&#35273;&#36816;&#21160;&#36712;&#36857;&#21644;&#22823;&#37327;&#30340;&#20219;&#21153;&#26465;&#20214;&#19979;&#30340;&#29289;&#20307;&#25805;&#20316;&#35270;&#39057;&#65292;&#22312;&#23398;&#20064;&#36890;&#29992;&#30340;&#25805;&#32437;&#20363;&#31243;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#35270;&#39057;&#28436;&#31034;&#23398;&#20064;&#22914;&#20309;&#22312;&#36825;&#20010;&#29305;&#24449;&#31354;&#38388;&#20013;&#35268;&#21010;&#21508;&#31181;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20016;&#23500;&#30340;&#34920;&#24449;&#26159;&#23454;&#29616;&#26426;&#22120;&#20154;&#25805;&#32437;&#30340;&#20851;&#38190;&#65292;&#20294;&#29616;&#26377;&#30340;&#27169;&#22411;&#26550;&#26500;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#26469;&#23398;&#20064;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29702;&#24819;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#35757;&#32451;&#25968;&#25454;&#65292;&#21363;&#21508;&#31181;&#24050;&#27880;&#37322;&#20219;&#21153;&#30340;&#19987;&#23478;&#35270;&#35273;-&#21160;&#20316;&#28436;&#31034;&#65292;&#26159;&#31232;&#32570;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#26550;&#26500;PLEX&#65292;&#23427;&#26159;&#20174;&#20219;&#21153;&#19981;&#21487;&#30693;&#35270;&#35273;&#36816;&#21160;&#36712;&#36857;&#20013;&#23398;&#20064;&#30340;&#65292;&#20276;&#38543;&#30528;&#22823;&#37327;&#30340;&#20219;&#21153;&#26465;&#20214;&#19979;&#30340;&#29289;&#20307;&#25805;&#20316;&#35270;&#39057;&#8212;&#8212;&#36825;&#26159;&#19968;&#31181;&#25968;&#37327;&#21487;&#35266;&#30340;&#19982;&#26426;&#22120;&#20154;&#30456;&#20851;&#30340;&#25968;&#25454;&#12290;PLEX&#32972;&#21518;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#22312;&#35266;&#23519;&#21644;&#34892;&#21160;&#26041;&#38754;&#30340;&#36712;&#36857;&#19979;&#65292;&#26377;&#21161;&#20110;&#35825;&#23548;&#28508;&#22312;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#24182;&#35757;&#32451;&#26426;&#22120;&#20154;&#25191;&#34892;&#19982;&#20219;&#21153;&#19981;&#30456;&#20851;&#30340;&#25805;&#20316;&#20363;&#31243;&#65292;&#32780;&#22810;&#26679;&#21270;&#30340;&#20165;&#20026;&#35270;&#39057;&#28436;&#31034;&#20165;&#21487;&#20197;&#26377;&#25928;&#22320;&#25945;&#20250;&#26426;&#22120;&#20154;&#22914;&#20309;&#22312;&#36825;&#20010;&#29305;&#24449;&#31354;&#38388;&#20013;&#35268;&#21010;&#21508;&#31181;&#20219;&#21153;&#12290;&#19982;&#22823;&#22810;&#25968;&#26426;&#22120;&#20154;&#25805;&#32437;&#39044;&#22521;&#35757;&#20316;&#21697;&#19981;&#21516;&#65292;PLEX&#23398;&#20064;&#20102;&#19968;&#31181;&#21487;&#25512;&#24191;&#30340;&#24863;&#35273;&#36816;&#21160;&#22810;&#20219;&#21153;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
A rich representation is key to general robotic manipulation, but existing model architectures require a lot of data to learn it. Unfortunately, ideal robotic manipulation training data, which comes in the form of expert visuomotor demonstrations for a variety of annotated tasks, is scarce. In this work we propose PLEX, a transformer-based architecture that learns from task-agnostic visuomotor trajectories accompanied by a much larger amount of task-conditioned object manipulation videos -- a type of robotics-relevant data available in quantity. The key insight behind PLEX is that the trajectories with observations and actions help induce a latent feature space and train a robot to execute task-agnostic manipulation routines, while a diverse set of video-only demonstrations can efficiently teach the robot how to plan in this feature space for a wide variety of tasks. In contrast to most works on robotic manipulation pretraining, PLEX learns a generalizable sensorimotor multi-task polic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#20840;&#31070;&#32463;&#24418;&#24577;&#35270;&#35273;&#21040;&#25511;&#21046;&#30340;&#27969;&#31243;&#65292;&#20351;&#26080;&#20154;&#26426;&#20855;&#22791;&#20102;&#25191;&#34892;&#33258;&#20027;&#22522;&#20110;&#35270;&#35273;&#30340;&#39134;&#34892;&#25152;&#38656;&#30340;&#33021;&#21147;&#65292;&#20026;&#26426;&#22120;&#20154;&#24863;&#30693;&#21644;&#34892;&#21160;&#25552;&#20379;&#20102;&#20302;&#24310;&#36831;&#21644;&#33021;&#37327;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.08778</link><description>&lt;p&gt;
&#20840;&#31070;&#32463;&#24418;&#24577;&#35270;&#35273;&#21644;&#25511;&#21046;&#30340;&#33258;&#20027;&#39134;&#34892;&#25191;&#29031;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fully neuromorphic vision and control for autonomous drone flight. (arXiv:2303.08778v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#20840;&#31070;&#32463;&#24418;&#24577;&#35270;&#35273;&#21040;&#25511;&#21046;&#30340;&#27969;&#31243;&#65292;&#20351;&#26080;&#20154;&#26426;&#20855;&#22791;&#20102;&#25191;&#34892;&#33258;&#20027;&#22522;&#20110;&#35270;&#35273;&#30340;&#39134;&#34892;&#25152;&#38656;&#30340;&#33021;&#21147;&#65292;&#20026;&#26426;&#22120;&#20154;&#24863;&#30693;&#21644;&#34892;&#21160;&#25552;&#20379;&#20102;&#20302;&#24310;&#36831;&#21644;&#33021;&#37327;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#24863;&#30693;&#21644;&#22788;&#29702;&#26159;&#24322;&#27493;&#21644;&#31232;&#30095;&#30340;&#65292;&#23548;&#33268;&#20302;&#24310;&#36831;&#21644;&#33021;&#37327;&#26377;&#25928;&#30340;&#24863;&#30693;&#21644;&#34892;&#21160;&#12290;&#22312;&#26426;&#22120;&#20154;&#23398;&#20013;&#65292;&#20351;&#29992;&#38754;&#21521;&#20107;&#20214;&#30340;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#21644;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#25215;&#35834;&#20855;&#26377;&#31867;&#20284;&#30340;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24403;&#21069;&#23884;&#20837;&#24335;&#31070;&#32463;&#24418;&#24577;&#22788;&#29702;&#22120;&#20013;&#21463;&#38480;&#30340;&#32593;&#32476;&#35268;&#27169;&#20197;&#21450;&#35757;&#32451;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#30340;&#22256;&#38590;&#65292;&#26426;&#22120;&#20154;&#23454;&#29616;&#20165;&#38480;&#20110;&#20855;&#26377;&#20302;&#32500;&#24863;&#23448;&#36755;&#20837;&#21644;&#36816;&#21160;&#25191;&#34892;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20840;&#31070;&#32463;&#24418;&#24577;&#35270;&#35273;&#21040;&#25511;&#21046;&#30340;&#27969;&#31243;&#65292;&#20197;&#25511;&#21046;&#33258;&#30001;&#39134;&#34892;&#30340;&#26080;&#20154;&#26426;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#31070;&#32463;&#32593;&#32476;&#25509;&#21463;&#39640;&#32500;&#21407;&#22987;&#20107;&#20214;&#30456;&#26426;&#25968;&#25454;&#24182;&#36755;&#20986;&#25191;&#34892;&#33258;&#20027;&#22522;&#20110;&#35270;&#35273;&#30340;&#39134;&#34892;&#25152;&#38656;&#30340;&#20302;&#32423;&#25511;&#21046;&#21160;&#20316;&#12290;&#32593;&#32476;&#30340;&#35270;&#35273;&#37096;&#20998;&#30001;&#20116;&#23618;&#21644; 28.8k &#31070;&#32463;&#20803;&#32452;&#25104;&#65292;&#23558;&#20256;&#20837;&#30340;&#21407;&#22987;&#20107;&#20214;&#26144;&#23556;&#21040;&#33258;&#25105;&#36816;&#21160;&#20272;&#35745;&#19978;&#65292;&#24182;&#20351;&#29992;&#30495;&#23454;&#29615;&#22659;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biological sensing and processing is asynchronous and sparse, leading to low-latency and energy-efficient perception and action. In robotics, neuromorphic hardware for event-based vision and spiking neural networks promises to exhibit similar characteristics. However, robotic implementations have been limited to basic tasks with low-dimensional sensory inputs and motor actions due to the restricted network size in current embedded neuromorphic processors and the difficulties of training spiking neural networks. Here, we present the first fully neuromorphic vision-to-control pipeline for controlling a freely flying drone. Specifically, we train a spiking neural network that accepts high-dimensional raw event-based camera data and outputs low-level control actions for performing autonomous vision-based flight. The vision part of the network, consisting of five layers and 28.8k neurons, maps incoming raw events to ego-motion estimates and is trained with self-supervised learning on real e
&lt;/p&gt;</description></item><item><title>GPT-4&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#21487;&#20197;&#25509;&#25910;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#24182;&#20135;&#29983;&#25991;&#26412;&#36755;&#20986;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#19987;&#19994;&#21644;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#36890;&#36807;&#27169;&#25311;&#30340;&#24459;&#24072;&#32771;&#35797;&#12290;&#35813;&#39033;&#30446;&#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#24320;&#21457;&#22522;&#30784;&#35774;&#26045;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#22312;&#24191;&#27867;&#30340;&#35268;&#27169;&#33539;&#22260;&#20869;&#34920;&#29616;&#39044;&#27979;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.08774</link><description>&lt;p&gt;
GPT-4&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
GPT-4 Technical Report. (arXiv:2303.08774v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08774
&lt;/p&gt;
&lt;p&gt;
GPT-4&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#21487;&#20197;&#25509;&#25910;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#24182;&#20135;&#29983;&#25991;&#26412;&#36755;&#20986;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#19987;&#19994;&#21644;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#36890;&#36807;&#27169;&#25311;&#30340;&#24459;&#24072;&#32771;&#35797;&#12290;&#35813;&#39033;&#30446;&#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#24320;&#21457;&#22522;&#30784;&#35774;&#26045;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#22312;&#24191;&#27867;&#30340;&#35268;&#27169;&#33539;&#22260;&#20869;&#34920;&#29616;&#39044;&#27979;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25253;&#21578;&#20102;GPT-4&#30340;&#24320;&#21457;&#65292;&#23427;&#26159;&#19968;&#20010;&#21487;&#20197;&#25509;&#21463;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#24182;&#20135;&#29983;&#25991;&#26412;&#36755;&#20986;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#34429;&#28982;&#22312;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#20013;&#19981;&#22914;&#20154;&#31867;&#65292;&#20294;GPT-4&#22312;&#21508;&#31181;&#19987;&#19994;&#21644;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#36890;&#36807;&#27169;&#25311;&#30340;&#24459;&#24072;&#32771;&#35797;&#65292;&#25104;&#32489;&#25490;&#21517;&#22312;&#21069;10&#65285;&#24038;&#21491;&#12290;GPT-4&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#39044;&#35757;&#32451;&#29992;&#20110;&#39044;&#27979;&#25991;&#26723;&#20013;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#21518;&#35757;&#32451;&#23545;&#40784;&#36807;&#31243;&#25552;&#39640;&#20102;&#20107;&#23454;&#24615;&#21644;&#31526;&#21512;&#26399;&#26395;&#34892;&#20026;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;&#39033;&#30446;&#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#24320;&#21457;&#22522;&#30784;&#35774;&#26045;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#22312;&#24191;&#27867;&#30340;&#35268;&#27169;&#33539;&#22260;&#20869;&#34920;&#29616;&#39044;&#27979;&#24615;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;GPT-4&#30340;&#26576;&#20123;&#24615;&#33021;&#26041;&#38754;&#65292;&#32780;&#36825;&#20123;&#24615;&#33021;&#26159;&#22522;&#20110;&#20351;&#29992;&#19981;&#36229;&#36807;GPT-4&#35745;&#31639;&#33021;&#21147;&#30340;1/1,000&#30340;&#27169;&#22411;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#39640;&#25928;&#30340;&#20010;&#24615;&#21270;&#26041;&#27861;&#8212;&#8212;&#20351;&#29992;&#39640;&#24230;&#20010;&#24615;&#21270;&#25991;&#26412;&#23884;&#20837;&#26469;&#36827;&#34892;&#22270;&#20687;&#25805;&#20316;&#65292;&#21487;&#20197;&#36816;&#29992;&#20110;&#22270;&#20687;&#30340;&#32972;&#26223;&#12289;&#32441;&#29702;&#21644;&#21160;&#20316;&#30340;&#32534;&#36753;&#65292;&#19981;&#38656;&#35201;&#22810;&#20010;&#21442;&#32771;&#22270;&#20687;&#25110;&#22797;&#26434;&#35757;&#32451;&#65292;&#33021;&#23454;&#29616;&#22797;&#26434;&#35821;&#20041;&#22270;&#20687;&#32534;&#36753;&#12290;</title><link>http://arxiv.org/abs/2303.08767</link><description>&lt;p&gt;
&#22522;&#20110;&#31283;&#23450;&#25193;&#25955;&#30340;&#39640;&#24230;&#20010;&#24615;&#21270;&#25991;&#26412;&#23884;&#20837;&#22270;&#20687;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Highly Personalized Text Embedding for Image Manipulation by Stable Diffusion. (arXiv:2303.08767v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#39640;&#25928;&#30340;&#20010;&#24615;&#21270;&#26041;&#27861;&#8212;&#8212;&#20351;&#29992;&#39640;&#24230;&#20010;&#24615;&#21270;&#25991;&#26412;&#23884;&#20837;&#26469;&#36827;&#34892;&#22270;&#20687;&#25805;&#20316;&#65292;&#21487;&#20197;&#36816;&#29992;&#20110;&#22270;&#20687;&#30340;&#32972;&#26223;&#12289;&#32441;&#29702;&#21644;&#21160;&#20316;&#30340;&#32534;&#36753;&#65292;&#19981;&#38656;&#35201;&#22810;&#20010;&#21442;&#32771;&#22270;&#20687;&#25110;&#22797;&#26434;&#35757;&#32451;&#65292;&#33021;&#23454;&#29616;&#22797;&#26434;&#35821;&#20041;&#22270;&#20687;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#23637;&#29616;&#20986;&#22312;&#22270;&#20687;&#29983;&#25104;&#21644;&#25805;&#20316;&#26041;&#38754;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#20294;&#20869;&#37096;&#38543;&#26426;&#24615;&#24102;&#26469;&#30340;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#20445;&#30041;&#21644;&#25805;&#20316;&#22270;&#20687;&#20869;&#23481;&#21644;&#36523;&#20221;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#26041;&#27861;&#22914;&#8220;&#26790;&#22659;&#30456;&#26426;&#8221;&#21644;&#8220;&#25991;&#26412;&#21453;&#36716;&#8221;&#25552;&#20986;&#20102;&#20351;&#29992;&#27169;&#22411;&#25110;&#28508;&#22312;&#34920;&#31034;&#20010;&#24615;&#21270;&#26469;&#20445;&#25345;&#20869;&#23481;&#65292;&#20294;&#23427;&#20204;&#23545;&#22810;&#20010;&#21442;&#32771;&#22270;&#20687;&#21644;&#22797;&#26434;&#35757;&#32451;&#30340;&#20381;&#36182;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#21448;&#38750;&#24120;&#26377;&#25928;&#30340;&#20010;&#24615;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#39640;&#24230;&#20010;&#24615;&#21270;&#65288;HiPer&#65289;&#25991;&#26412;&#23884;&#20837;&#36890;&#36807;&#20998;&#35299;CLIP&#23884;&#20837;&#31354;&#38388;&#23454;&#29616;&#20010;&#24615;&#21270;&#21644;&#20869;&#23481;&#25805;&#20316;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#27169;&#22411;&#24494;&#35843;&#25110;&#35782;&#21035;&#31526;&#65292;&#20294;&#20173;&#21487;&#20197;&#20165;&#36890;&#36807;&#21333;&#20010;&#22270;&#20687;&#21644;&#30446;&#26631;&#25991;&#26412;&#26469;&#23454;&#29616;&#32972;&#26223;&#12289;&#32441;&#29702;&#21644;&#21160;&#20316;&#30340;&#25805;&#20316;&#12290;&#36890;&#36807;&#23545;&#22810;&#26679;&#21270;&#30340;&#30446;&#26631;&#25991;&#26412;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#33021;&#20135;&#29983;&#39640;&#24230;&#20010;&#24615;&#21270;&#21644;&#22797;&#26434;&#30340;&#35821;&#20041;&#22270;&#20687;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have shown superior performance in image generation and manipulation, but the inherent stochasticity presents challenges in preserving and manipulating image content and identity. While previous approaches like DreamBooth and Textual Inversion have proposed model or latent representation personalization to maintain the content, their reliance on multiple reference images and complex training limits their practicality. In this paper, we present a simple yet highly effective approach to personalization using highly personalized (HiPer) text embedding by decomposing the CLIP embedding space for personalization and content manipulation. Our method does not require model fine-tuning or identifiers, yet still enables manipulation of background, texture, and motion with just a single image and target text. Through experiments on diverse target texts, we demonstrate that our approach produces highly personalized and complex semantic image edits across a wide range of tasks. We
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32423;&#32852;&#25918;&#22823;&#30340;&#39640;&#20998;&#36776;&#29575;&#33322;&#31354;&#22270;&#20687;&#26816;&#27979;&#22120;&#65292;&#20351;&#29992;&#23494;&#24230;&#35009;&#21098;&#26469;&#22686;&#24378;&#23567;&#29289;&#20307;&#26816;&#27979;&#65292;&#19988;&#26131;&#20110;&#38598;&#25104;&#21040;&#20219;&#20309;&#26816;&#27979;&#22120;&#20013;&#12290;</title><link>http://arxiv.org/abs/2303.08747</link><description>&lt;p&gt;
&#22522;&#20110;&#32423;&#32852;&#25918;&#22823;&#30340;&#39640;&#20998;&#36776;&#29575;&#33322;&#31354;&#22270;&#20687;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Cascaded Zoom-in Detector for High Resolution Aerial Images. (arXiv:2303.08747v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32423;&#32852;&#25918;&#22823;&#30340;&#39640;&#20998;&#36776;&#29575;&#33322;&#31354;&#22270;&#20687;&#26816;&#27979;&#22120;&#65292;&#20351;&#29992;&#23494;&#24230;&#35009;&#21098;&#26469;&#22686;&#24378;&#23567;&#29289;&#20307;&#26816;&#27979;&#65292;&#19988;&#26131;&#20110;&#38598;&#25104;&#21040;&#20219;&#20309;&#26816;&#27979;&#22120;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33322;&#31354;&#22270;&#20687;&#20013;&#26816;&#27979;&#29289;&#20307;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#30001;&#20998;&#24067;&#19981;&#22343;&#21248;&#30340;&#25317;&#25380;&#23567;&#29289;&#20307;&#32452;&#25104;&#65292;&#20998;&#36776;&#29575;&#20063;&#24456;&#39640;&#12290;&#23494;&#24230;&#35009;&#21098;&#26159;&#25913;&#21892;&#23567;&#29289;&#20307;&#26816;&#27979;&#30340;&#24191;&#27867;&#26041;&#27861;&#65292;&#20854;&#20013;&#25552;&#21462;&#24182;&#22788;&#29702;&#39640;&#20998;&#36776;&#29575;&#19979;&#30340;&#25317;&#25380;&#23567;&#29289;&#20307;&#21306;&#22495;&#12290;&#28982;&#32780;&#65292;&#36825;&#36890;&#24120;&#36890;&#36807;&#28155;&#21152;&#20854;&#20182;&#21487;&#23398;&#20064;&#32452;&#20214;&#26469;&#23436;&#25104;&#65292;&#20174;&#32780;&#20351;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#22797;&#26434;&#21270;&#65292;&#36825;&#19981;&#31526;&#21512;&#26631;&#20934;&#26816;&#27979;&#27969;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#32423;&#32852;&#25918;&#22823;&#26816;&#27979;&#22120;&#65288;CZ&#26816;&#27979;&#22120;&#65289;&#65292;&#23427;&#37325;&#26032;&#23450;&#20301;&#26816;&#27979;&#22120;&#26412;&#36523;&#20197;&#36827;&#34892;&#23494;&#24230;&#24341;&#23548;&#24335;&#35757;&#32451;&#21644;&#25512;&#29702;&#12290;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;&#23450;&#20301;&#23494;&#24230;&#35009;&#21098;&#65292;&#26631;&#35760;&#20026;&#26032;&#31867;&#21035;&#65292;&#24182;&#29992;&#20110;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#22312;&#25512;&#29702;&#26399;&#38388;&#65292;&#23494;&#24230;&#35009;&#21098;&#19982;&#22522;&#31867;&#21035;&#23545;&#35937;&#19968;&#36215;&#39318;&#20808;&#34987;&#26816;&#27979;&#65292;&#28982;&#21518;&#36755;&#20837;&#36827;&#34892;&#31532;&#20108;&#38454;&#27573;&#30340;&#25512;&#29702;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#22320;&#38598;&#25104;&#21040;&#20219;&#20309;&#26816;&#27979;&#22120;&#20013;&#65292;&#24182;&#19988;&#19981;&#20250;&#23545;&#26631;&#20934;&#26816;&#27979;&#27969;&#31243;&#36896;&#25104;&#37325;&#22823;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting objects in aerial images is challenging because they are typically composed of crowded small objects distributed non-uniformly over high-resolution images. Density cropping is a widely used method to improve this small object detection where the crowded small object regions are extracted and processed in high resolution. However, this is typically accomplished by adding other learnable components, thus complicating the training and inference over a standard detection process. In this paper, we propose an efficient Cascaded Zoom-in (CZ) detector that re-purposes the detector itself for density-guided training and inference. During training, density crops are located, labeled as a new class, and employed to augment the training dataset. During inference, the density crops are first detected along with the base class objects, and then input for a second stage of inference. This approach is easily integrated into any detector, and creates no significant change in the standard det
&lt;/p&gt;</description></item><item><title>DACOS&#26159;&#19968;&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#21253;&#21547;&#19977;&#31181;&#19981;&#21516;&#31890;&#24230;&#30340;&#20195;&#30721;&#24322;&#21619;&#65288;&#22810;&#38754;&#25277;&#35937;&#12289;&#22797;&#26434;&#26041;&#27861;&#21644;&#38271;&#21442;&#25968;&#21015;&#34920;&#65289;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#26816;&#27979;&#20195;&#30721;&#24322;&#21619;&#30340;&#35757;&#32451;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2303.08729</link><description>&lt;p&gt;
DACOS-&#19968;&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#20195;&#30721;&#24322;&#21619;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DACOS-A Manually Annotated Dataset of Code Smells. (arXiv:2303.08729v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08729
&lt;/p&gt;
&lt;p&gt;
DACOS&#26159;&#19968;&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#21253;&#21547;&#19977;&#31181;&#19981;&#21516;&#31890;&#24230;&#30340;&#20195;&#30721;&#24322;&#21619;&#65288;&#22810;&#38754;&#25277;&#35937;&#12289;&#22797;&#26434;&#26041;&#27861;&#21644;&#38271;&#21442;&#25968;&#21015;&#34920;&#65289;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#26816;&#27979;&#20195;&#30721;&#24322;&#21619;&#30340;&#35757;&#32451;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#26816;&#27979;&#20195;&#30721;&#24322;&#21619;&#65292;&#20197;&#25269;&#28040;&#35768;&#22810;&#20195;&#30721;&#24322;&#21619;&#30340;&#20027;&#35266;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#38656;&#35201;&#19968;&#20010;&#22823;&#22411;&#12289;&#25163;&#21160;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;&#29616;&#26377;&#25991;&#29486;&#25552;&#20379;&#20102;&#19968;&#20123;&#25968;&#25454;&#38598;&#65292;&#20294;&#23427;&#20204;&#35268;&#27169;&#36739;&#23567;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#23427;&#20204;&#27809;&#26377;&#20851;&#27880;&#20027;&#35266;&#20195;&#30721;&#29255;&#27573;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DACOS&#65292;&#19968;&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;10,267&#20010;&#23545;5,192&#20010;&#20195;&#30721;&#29255;&#27573;&#30340;&#27880;&#37322;&#12290;&#35813;&#25968;&#25454;&#38598;&#38024;&#23545;&#19981;&#21516;&#31890;&#24230;&#30340;&#19977;&#31181;&#20195;&#30721;&#24322;&#21619;&#65306;&#22810;&#38754;&#25277;&#35937;&#12289;&#22797;&#26434;&#26041;&#27861;&#21644;&#38271;&#21442;&#25968;&#21015;&#34920;&#12290;&#25968;&#25454;&#38598;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#21019;&#24314;&#12290;&#31532;&#19968;&#38454;&#27573;&#36890;&#36807;&#30830;&#23450;&#29992;&#20110;&#26816;&#27979;&#27668;&#21619;&#30340;&#24230;&#37327;&#26631;&#20934;&#30340;&#38408;&#20540;&#26469;&#24110;&#21161;&#25105;&#20204;&#35782;&#21035;&#21487;&#33021;&#20855;&#26377;&#20027;&#35266;&#24615;&#30340;&#20195;&#30721;&#29255;&#27573;&#12290;&#31532;&#20108;&#38454;&#27573;&#25910;&#38598;&#21487;&#33021;&#20855;&#26377;&#20027;&#35266;&#24615;&#30340;&#20195;&#30721;&#29255;&#27573;&#30340;&#27880;&#37322;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#25193;&#23637;&#25968;&#25454;&#38598;DACOSX&#65292;&#23427;&#20351;&#29992;&#38408;&#20540;&#21253;&#21547;&#26126;&#30830;&#30340;&#33391;&#24615;&#20195;&#30721;&#29255;&#27573;&#21644;&#26126;&#30830;&#30340;&#26377;&#24322;&#21619;&#30340;&#20195;&#30721;&#29255;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers apply machine-learning techniques for code smell detection to counter the subjectivity of many code smells. Such approaches need a large, manually annotated dataset for training and benchmarking. Existing literature offers a few datasets; however, they are small in size and, more importantly, do not focus on the subjective code snippets. In this paper, we present DACOS, a manually annotated dataset containing 10,267 annotations for 5,192 code snippets. The dataset targets three kinds of code smells at different granularity: multifaceted abstraction, complex method, and long parameter list. The dataset is created in two phases. The first phase helps us identify the code snippets that are potentially subjective by determining the thresholds of metrics used to detect a smell. The second phase collects annotations for potentially subjective snippets. We also offer an extended dataset DACOSX that includes definitely benign and definitely smelly snippets by using the thresholds i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;AI&#39537;&#21160;&#30340;&#35828;&#26381;&#26410;&#26469;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21253;&#25324;&#31227;&#21160;&#35828;&#26381;&#21147;&#24179;&#34913;&#30340;&#26041;&#24335;&#12289;&#23450;&#21046;&#21270;&#30340;&#35828;&#26381;&#12289;&#34394;&#20551;&#20449;&#24687;&#30340;&#24102;&#21160;&#20197;&#21450;&#25913;&#21464;&#20154;&#31867;&#33258;&#36523;&#35328;&#35770;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#35686;&#21578;&#23384;&#22312;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#21628;&#21505;&#21152;&#24378;&#23545;&#20854;&#24320;&#21457;&#21644;&#20351;&#29992;&#30340;&#30417;&#31649;&#12290;</title><link>http://arxiv.org/abs/2303.08721</link><description>&lt;p&gt;
&#20154;&#24037;&#24433;&#21709;: AI&#39537;&#21160;&#30340;&#35828;&#26381;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Artificial Influence: An Analysis Of AI-Driven Persuasion. (arXiv:2303.08721v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;AI&#39537;&#21160;&#30340;&#35828;&#26381;&#26410;&#26469;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21253;&#25324;&#31227;&#21160;&#35828;&#26381;&#21147;&#24179;&#34913;&#30340;&#26041;&#24335;&#12289;&#23450;&#21046;&#21270;&#30340;&#35828;&#26381;&#12289;&#34394;&#20551;&#20449;&#24687;&#30340;&#24102;&#21160;&#20197;&#21450;&#25913;&#21464;&#20154;&#31867;&#33258;&#36523;&#35328;&#35770;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#35686;&#21578;&#23384;&#22312;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#21628;&#21505;&#21152;&#24378;&#23545;&#20854;&#24320;&#21457;&#21644;&#20351;&#29992;&#30340;&#30417;&#31649;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35828;&#26381;&#26159;&#20154;&#31867;&#30340;&#37325;&#35201;&#29305;&#24449;&#20043;&#19968;&#65292;&#26159;&#21830;&#19994;&#12289;&#25919;&#27835;&#31561;&#20107;&#19994;&#30340;&#26680;&#24515;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#36827;&#27493;&#24050;&#32463;&#20135;&#29983;&#20102;&#33021;&#22815;&#35828;&#26381;&#20154;&#31867;&#36141;&#20080;&#20135;&#21697;&#12289;&#35266;&#30475;&#35270;&#39057;&#12289;&#28857;&#20987;&#25628;&#32034;&#32467;&#26524;&#31561;&#30340;AI&#31995;&#32479;&#12290;&#21363;&#20351;&#27809;&#26377;&#26126;&#30830;&#35774;&#35745;&#20026;&#35828;&#26381;&#30340;&#31995;&#32479;&#65292;&#22312;&#23454;&#36341;&#20013;&#20063;&#21487;&#33021;&#20250;&#36825;&#26679;&#20570;&#12290;&#26410;&#26469;&#65292;&#36234;&#26469;&#36234;&#20855;&#26377;&#20154;&#24418;&#29305;&#24449;&#30340;AI&#31995;&#32479;&#21487;&#33021;&#20250;&#19982;&#29992;&#25143;&#24418;&#25104;&#25345;&#32493;&#30340;&#20851;&#31995;&#65292;&#25552;&#39640;&#23427;&#20204;&#30340;&#35828;&#26381;&#21147;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;AI&#31995;&#32479;&#30340;&#35828;&#26381;&#33021;&#21147;&#26410;&#26469;&#12290;&#25105;&#20204;&#32771;&#34385;&#21040;AI&#22914;&#20309;&#22312;&#31227;&#21160;&#35828;&#26381;&#21147;&#24179;&#34913;&#30340;&#22522;&#30784;&#19978;&#65292;&#23454;&#29616;&#23450;&#21046;&#21270;&#30340;&#35828;&#26381;&#65292;&#20026;&#34394;&#20551;&#20449;&#24687;&#24102;&#26469;&#21160;&#21147;&#20197;&#21450;&#25913;&#21464;&#20154;&#31867;&#22609;&#36896;&#33258;&#36523;&#35328;&#35770;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#32771;&#34385;AI&#39537;&#21160;&#30340;&#35828;&#26381;&#26041;&#24335;&#21487;&#33021;&#19982;&#20154;&#31867;&#39537;&#21160;&#30340;&#26041;&#24335;&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#35686;&#21578;&#35828;&#65292;&#26222;&#36941;&#23384;&#22312;&#39640;&#24230;&#35828;&#26381;&#21147;&#30340;AI&#31995;&#32479;&#21487;&#33021;&#23545;&#20154;&#31867;&#30340;&#33258;&#20027;&#26435;&#21644;&#31119;&#31049;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#21628;&#21505;&#21152;&#24378;&#20851;&#20110;&#20854;&#24320;&#21457;&#21644;&#20351;&#29992;&#30340;&#23545;&#35805;&#21644;&#30417;&#31649;&#12290;
&lt;/p&gt;
&lt;p&gt;
Persuasion is a key aspect of what it means to be human, and is central to business, politics, and other endeavors. Advancements in artificial intelligence (AI) have produced AI systems that are capable of persuading humans to buy products, watch videos, click on search results, and more. Even systems that are not explicitly designed to persuade may do so in practice. In the future, increasingly anthropomorphic AI systems may form ongoing relationships with users, increasing their persuasive power. This paper investigates the uncertain future of persuasive AI systems. We examine ways that AI could qualitatively alter our relationship to and views regarding persuasion by shifting the balance of persuasive power, allowing personalized persuasion to be deployed at scale, powering misinformation campaigns, and changing the way humans can shape their own discourse. We consider ways AI-driven persuasion could differ from human-driven persuasion. We warn that ubiquitous highlypersuasive AI sy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; Bi-VAEGAN &#27169;&#22411;&#65292;&#36890;&#36807;&#21452;&#21521;&#20998;&#24067;&#23545;&#40784;&#12289;L_2 &#33539;&#25968;&#29305;&#24449;&#24402;&#19968;&#21270;&#21644;&#26356;&#22797;&#26434;&#30340;&#20808;&#39564;&#20272;&#35745;&#26041;&#27861;&#65292;&#22823;&#22823;&#25913;&#21892;&#20102;&#36328;&#22495;&#38646;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.08698</link><description>&lt;p&gt;
&#38754;&#21521;&#36328;&#22495;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#21452;&#21521;&#20998;&#24067;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bi-directional Distribution Alignment for Transductive Zero-Shot Learning. (arXiv:2303.08698v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; Bi-VAEGAN &#27169;&#22411;&#65292;&#36890;&#36807;&#21452;&#21521;&#20998;&#24067;&#23545;&#40784;&#12289;L_2 &#33539;&#25968;&#29305;&#24449;&#24402;&#19968;&#21270;&#21644;&#26356;&#22797;&#26434;&#30340;&#20808;&#39564;&#20272;&#35745;&#26041;&#27861;&#65292;&#22823;&#22823;&#25913;&#21892;&#20102;&#36328;&#22495;&#38646;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#23398;&#20064;&#23384;&#22312;&#39046;&#22495;&#36716;&#31227;&#38382;&#39064;&#65292;&#21363;&#26410;&#35265;&#31867;&#21035;&#30340;&#30495;&#23454;&#21644;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#19981;&#21305;&#37197;&#12290;&#23613;&#31649;&#36716;&#23548;&#24335;&#38646;&#26679;&#26412;&#23398;&#20064;&#21487;&#20197;&#20351;&#29992;&#26410;&#35265;&#31867;&#21035;&#30340;&#26080;&#26631;&#31614;&#26679;&#26412;&#65292;&#20294;&#20998;&#24067;&#20559;&#31227;&#20173;&#28982;&#24456;&#39640;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340; TZSL &#27169;&#22411;&#65288;&#21629;&#21517;&#20026; Bi-VAEGAN&#65289;&#65292;&#36890;&#36807;&#21152;&#24378;&#35270;&#35273;&#31354;&#38388;&#21644;&#36741;&#21161;&#31354;&#38388;&#20043;&#38388;&#30340;&#20998;&#24067;&#23545;&#40784;&#65292;&#22823;&#22823;&#25913;&#21892;&#20102;&#36825;&#31181;&#20559;&#31227;&#12290;&#27169;&#22411;&#35774;&#35745;&#30340;&#20851;&#38190;&#25552;&#35758;&#21253;&#25324;&#65288;1&#65289;&#21452;&#21521;&#20998;&#24067;&#23545;&#40784;&#65292;&#65288;2&#65289;&#22522;&#20110; L_2 &#33539;&#25968;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#29305;&#24449;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#65288;3&#65289;&#26356;&#22797;&#26434;&#30340;&#26410;&#35265;&#31867;&#21035;&#20808;&#39564;&#20272;&#35745;&#26041;&#27861;&#12290;&#22312;&#20351;&#29992;&#22235;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#22522;&#20934;&#35780;&#20272;&#26102;&#65292;Bi-VAEGAN &#22312;&#26631;&#20934;&#21644;&#24191;&#20041; TZSL &#35774;&#32622;&#19979;&#22343;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;&#20195;&#30721;&#21487;&#22312; https://github.com/Zhicaiwww/Bi-VAEGAN &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is well-known that zero-shot learning (ZSL) can suffer severely from the problem of domain shift, where the true and learned data distributions for the unseen classes do not match. Although transductive ZSL (TZSL) attempts to improve this by allowing the use of unlabelled examples from the unseen classes, there is still a high level of distribution shift. We propose a novel TZSL model (named as Bi-VAEGAN), which largely improves the shift by a strengthened distribution alignment between the visual and auxiliary spaces. The key proposal of the model design includes (1) a bi-directional distribution alignment, (2) a simple but effective L_2-norm based feature normalization approach, and (3) a more sophisticated unseen class prior estimation approach. In benchmark evaluation using four datasets, Bi-VAEGAN achieves the new state of the arts under both the standard and generalized TZSL settings. Code could be found at https://github.com/Zhicaiwww/Bi-VAEGAN
&lt;/p&gt;</description></item><item><title>Mirror &#26159;&#19968;&#20010;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25903;&#25345;&#30340;&#24179;&#21488;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#30452;&#35266;&#30340;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#65292;&#21487;&#20197;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026; SQL&#65292;&#24182;&#33258;&#21160;&#29983;&#25104;&#21487;&#25191;&#34892;&#30340; SQL &#21629;&#20196;&#12290;&#29992;&#25143;&#21487;&#20197;&#39044;&#35272;&#21644;&#32534;&#36753; SQL &#20197;&#20445;&#35777;&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#65292;&#36824;&#21487;&#20197;&#33719;&#24471;&#25968;&#25454;&#25688;&#35201;&#21644;&#21487;&#35270;&#21270;&#12290;&#23427;&#36866;&#29992;&#20110;&#21508;&#31181;&#31243;&#24230;&#30340;&#25968;&#25454;&#20998;&#26512;&#20154;&#21592;&#21644;&#38750;&#25216;&#26415;&#19987;&#19994;&#20154;&#21592;&#12290;</title><link>http://arxiv.org/abs/2303.08697</link><description>&lt;p&gt;
Mirror: &#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#29992;&#20110;&#25968;&#25454;&#26597;&#35810;&#12289;&#25688;&#35201;&#21644;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Mirror: A Natural Language Interface for Data Querying, Summarization, and Visualization. (arXiv:2303.08697v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08697
&lt;/p&gt;
&lt;p&gt;
Mirror &#26159;&#19968;&#20010;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25903;&#25345;&#30340;&#24179;&#21488;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#30452;&#35266;&#30340;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#65292;&#21487;&#20197;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026; SQL&#65292;&#24182;&#33258;&#21160;&#29983;&#25104;&#21487;&#25191;&#34892;&#30340; SQL &#21629;&#20196;&#12290;&#29992;&#25143;&#21487;&#20197;&#39044;&#35272;&#21644;&#32534;&#36753; SQL &#20197;&#20445;&#35777;&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#65292;&#36824;&#21487;&#20197;&#33719;&#24471;&#25968;&#25454;&#25688;&#35201;&#21644;&#21487;&#35270;&#21270;&#12290;&#23427;&#36866;&#29992;&#20110;&#21508;&#31181;&#31243;&#24230;&#30340;&#25968;&#25454;&#20998;&#26512;&#20154;&#21592;&#21644;&#38750;&#25216;&#26415;&#19987;&#19994;&#20154;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102; Mirror&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#21160;&#30340;&#25968;&#25454;&#25506;&#32034;&#21644;&#20998;&#26512;&#24320;&#28304;&#24179;&#21488;&#12290;Mirror &#25552;&#20379;&#20102;&#19968;&#20010;&#30452;&#35266;&#30340;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#65292;&#29992;&#20110;&#26597;&#35810;&#25968;&#25454;&#24211;&#65292;&#24182;&#33258;&#21160;&#29983;&#25104;&#21487;&#25191;&#34892;&#30340; SQL &#21629;&#20196;&#26469;&#26816;&#32034;&#30456;&#20851;&#25968;&#25454;&#24182;&#29992;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#29992;&#25143;&#21487;&#20197;&#39044;&#35272;&#21644;&#25163;&#21160;&#32534;&#36753;&#29983;&#25104;&#30340; SQL &#21629;&#20196;&#65292;&#20197;&#30830;&#20445;&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#12290;Mirror &#36824;&#29983;&#25104;&#21487;&#35270;&#21270;&#22270;&#34920;&#65292;&#20197;&#20415;&#20102;&#35299;&#25968;&#25454;&#24773;&#20917;&#12290;&#35774;&#35745;&#26102;&#32771;&#34385;&#21040;&#28789;&#27963;&#24615;&#21644;&#20154;&#30340;&#36755;&#20837;&#65292;&#22240;&#27492; Mirror &#36866;&#21512;&#20110;&#32463;&#39564;&#20016;&#23500;&#30340;&#25968;&#25454;&#20998;&#26512;&#24072;&#21644;&#38750;&#25216;&#26415;&#19987;&#19994;&#20154;&#21592;&#20174;&#25968;&#25454;&#20013;&#33719;&#21462;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Mirror, an open-source platform for data exploration and analysis powered by large language models. Mirror offers an intuitive natural language interface for querying databases, and automatically generates executable SQL commands to retrieve relevant data and summarize it in natural language. In addition, users can preview and manually edit the generated SQL commands to ensure the accuracy of their queries. Mirror also generates visualizations to facilitate understanding of the data. Designed with flexibility and human input in mind, Mirror is suitable for both experienced data analysts and non-technical professionals looking to gain insights from their data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26223;&#24335;&#19968;&#38190;&#20998;&#21106;&#26041;&#27861;&#65292;&#20801;&#35768;&#20174;&#28857;&#20987;&#36755;&#20837;&#20013;&#20197;&#39640;&#25928;&#20934;&#30830;&#30340;&#26041;&#24335;&#20135;&#29983;&#20266;&#26631;&#31614;&#65292;&#29992;&#20110;&#35757;&#32451;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20102;&#26631;&#35760;&#24037;&#20316;&#21644;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2303.08689</link><description>&lt;p&gt;
&#20840;&#26223;&#24335;&#19968;&#38190;&#20998;&#21106;&#65306;&#24212;&#29992;&#20110;&#20892;&#19994;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Panoptic One-Click Segmentation: Applied to Agricultural Data. (arXiv:2303.08689v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26223;&#24335;&#19968;&#38190;&#20998;&#21106;&#26041;&#27861;&#65292;&#20801;&#35768;&#20174;&#28857;&#20987;&#36755;&#20837;&#20013;&#20197;&#39640;&#25928;&#20934;&#30830;&#30340;&#26041;&#24335;&#20135;&#29983;&#20266;&#26631;&#31614;&#65292;&#29992;&#20110;&#35757;&#32451;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20102;&#26631;&#35760;&#24037;&#20316;&#21644;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26434;&#33609;&#25511;&#21046;&#26041;&#38754;&#65292;&#31934;&#20934;&#20892;&#19994;&#21487;&#20197;&#24110;&#21161;&#22823;&#22823;&#20943;&#23569;&#38500;&#33609;&#21058;&#30340;&#20351;&#29992;&#65292;&#20174;&#32780;&#20135;&#29983;&#32463;&#27982;&#21644;&#29983;&#24577;&#25928;&#30410;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#26159;&#33021;&#22815;&#20174;&#22270;&#20687;&#25968;&#25454;&#20013;&#23450;&#20301;&#21644;&#20998;&#21106;&#25152;&#26377;&#26893;&#29289;&#12290;&#29616;&#20195;&#23454;&#20363;&#20998;&#21106;&#25216;&#26415;&#21487;&#20197;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#20294;&#26159;&#35757;&#32451;&#36825;&#26679;&#30340;&#31995;&#32479;&#38656;&#35201;&#22823;&#37327;&#25163;&#24037;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#36825;&#26159;&#26114;&#36149;&#19988;&#36153;&#21147;&#30340;&#12290;&#24369;&#30417;&#30563;&#35757;&#32451;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#26631;&#35760;&#24037;&#20316;&#21644;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20840;&#26223;&#24335;&#19968;&#38190;&#20998;&#21106;&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#20934;&#30830;&#30340;&#31163;&#32447;&#24037;&#20855;&#65292;&#21487;&#20197;&#20174;&#28857;&#20987;&#36755;&#20837;&#20013;&#20135;&#29983;&#20266;&#26631;&#31614;&#65292;&#20174;&#32780;&#20943;&#23569;&#26631;&#35760;&#24037;&#20316;&#37327;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#29420;&#31435;&#36845;&#20195;&#25152;&#26377;N&#20010;&#23545;&#35937;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32852;&#21512;&#20272;&#35745;&#20102;N&#20010;&#23545;&#35937;&#22330;&#26223;&#20013;&#27599;&#20010;&#20687;&#32032;&#30340;&#20301;&#32622;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;&#21482;&#20351;&#29992;10&#65285;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#25105;&#20204;&#30340;&#20840;&#26223;&#24335;&#19968;&#38190;&#20998;&#21106;&#26041;&#27861;&#65292;&#21363;&#21487;&#33719;&#24471;68.1&#65285;&#21644;68.8&#65285;&#30340;&#24179;&#22343;&#30446;&#26631;&#20132;&#38598;&#32852;&#21512;&#65288;IoU&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In weed control, precision agriculture can help to greatly reduce the use of herbicides, resulting in both economical and ecological benefits. A key element is the ability to locate and segment all the plants from image data. Modern instance segmentation techniques can achieve this, however, training such systems requires large amounts of hand-labelled data which is expensive and laborious to obtain. Weakly supervised training can help to greatly reduce labelling efforts and costs. We propose panoptic one-click segmentation, an efficient and accurate offline tool to produce pseudo-labels from click inputs which reduces labelling effort. Our approach jointly estimates the pixel-wise location of all N objects in the scene, compared to traditional approaches which iterate independently through all N objects; this greatly reduces training time. Using just 10% of the data to train our panoptic one-click segmentation approach yields 68.1% and 68.8% mean object intersection over union (IoU) o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20154;&#33080;&#35270;&#39057;&#30340;&#24378;&#21046;&#23545;&#40784;&#25216;&#26415;&#65292;&#34917;&#20805;&#20102;&#20351;&#29992;&#38899;&#39057;&#24378;&#21046;&#23545;&#40784;&#25216;&#26415;&#23384;&#22312;&#30340;&#32570;&#38519;&#65292;&#20294;&#30001;&#20110;&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#25216;&#26415;&#24615;&#33021;&#36739;&#20302;&#19988;&#25991;&#26412;&#21040;&#35270;&#39057;&#30340;&#32763;&#35793;&#19981;&#21487;&#38752;&#65292;&#24320;&#21457;&#21487;&#38752;&#30340;&#35270;&#35273;&#24378;&#21046;&#23545;&#40784;&#25216;&#26415;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.08670</link><description>&lt;p&gt;
&#28145;&#24230;&#35270;&#35273;&#24378;&#21046;&#23545;&#40784;: &#23398;&#20064;&#23558;&#38899;&#39057;&#25991;&#23383;&#23545;&#40784;&#21040;&#20154;&#33080;&#35270;&#39057;&#20013;
&lt;/p&gt;
&lt;p&gt;
Deep Visual Forced Alignment: Learning to Align Transcription with Talking Face Video. (arXiv:2303.08670v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20154;&#33080;&#35270;&#39057;&#30340;&#24378;&#21046;&#23545;&#40784;&#25216;&#26415;&#65292;&#34917;&#20805;&#20102;&#20351;&#29992;&#38899;&#39057;&#24378;&#21046;&#23545;&#40784;&#25216;&#26415;&#23384;&#22312;&#30340;&#32570;&#38519;&#65292;&#20294;&#30001;&#20110;&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#25216;&#26415;&#24615;&#33021;&#36739;&#20302;&#19988;&#25991;&#26412;&#21040;&#35270;&#39057;&#30340;&#32763;&#35793;&#19981;&#21487;&#38752;&#65292;&#24320;&#21457;&#21487;&#38752;&#30340;&#35270;&#35273;&#24378;&#21046;&#23545;&#40784;&#25216;&#26415;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21046;&#23545;&#40784;&#26159;&#25351;&#23558;&#32473;&#23450;&#30340;&#38899;&#39057;&#25991;&#23383;&#36716;&#24405;&#19982;&#30456;&#24212;&#30340;&#35821;&#38899;&#36827;&#34892;&#26102;&#24207;&#23545;&#40784;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#24378;&#21046;&#23545;&#40784;&#25216;&#26415;&#20351;&#29992;&#35821;&#38899;&#38899;&#39057;&#36827;&#34892;&#24320;&#21457;&#65292;&#24403;&#36755;&#20837;&#30340;&#35821;&#38899;&#38899;&#39057;&#21463;&#21040;&#22122;&#22768;&#27745;&#26579;&#25110;&#26080;&#27861;&#35775;&#38382;&#26102;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#22312;&#23545;&#40784;&#26041;&#38754;&#22833;&#36133;&#12290;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#35821;&#38899;&#21487;&#20197;&#20174;&#21478;&#19968;&#20010;&#32452;&#20214;&#20013;&#25512;&#26029;&#20986;&#65292;&#21363;&#35821;&#38899;&#35270;&#39057;(&#21363;&#65292;&#35828;&#35805;&#20154;&#30340;&#38754;&#37096;&#35270;&#39057;)&#12290;&#30001;&#20110;&#24403;&#38899;&#39057;&#20449;&#21495;&#22788;&#20110;&#19981;&#33391;&#29366;&#24577;&#26102;&#65292;&#21487;&#20197;&#20351;&#29992;&#35270;&#35273;&#20449;&#24687;&#26469;&#34917;&#20805;&#38899;&#39057;&#24378;&#21046;&#23545;&#40784;&#25216;&#26415;&#30340;&#32570;&#28857;&#65292;&#22240;&#27492;&#25105;&#20204;&#23581;&#35797;&#24320;&#21457;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#35270;&#39057;&#30340;&#24378;&#21046;&#23545;&#40784;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#19982;&#38899;&#39057;&#24378;&#21046;&#23545;&#40784;&#19981;&#21516;&#65292;&#24320;&#21457;&#21487;&#38752;&#30340;&#35270;&#35273;&#24378;&#21046;&#23545;&#40784;&#25216;&#26415;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21407;&#22240;&#22312;&#20110;: 1)&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;(VSR)&#19982;&#22522;&#20110;&#38899;&#39057;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#30456;&#27604;&#24615;&#33021;&#36739;&#20302;, 2)&#20174;&#25991;&#26412;&#21040;&#35270;&#39057;&#30340;&#32763;&#35793;&#19981;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forced alignment refers to a technology that time-aligns a given transcription with a corresponding speech. However, as the forced alignment technologies have developed using speech audio, they might fail in alignment when the input speech audio is noise-corrupted or is not accessible. We focus on that there is another component that the speech can be inferred from, the speech video (i.e., talking face video). Since the drawbacks of audio-based forced alignment can be complemented using the visual information when the audio signal is under poor condition, we try to develop a novel video-based forced alignment method. However, different from audio forced alignment, it is challenging to develop a reliable visual forced alignment technology for the following two reasons: 1) Visual Speech Recognition (VSR) has a much lower performance compared to audio-based Automatic Speech Recognition (ASR), and 2) the translation from text to video is not reliable, so the method typically used for build
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#39118;&#26684;&#36801;&#31227;&#20013;&#30340;&#38646;&#26679;&#26412;&#23545;&#27604;&#25439;&#22833;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#20855;&#26377;&#30456;&#21516;&#35821;&#20041;&#20869;&#23481;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2303.08622</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#23545;&#27604;&#25439;&#22833;&#29992;&#20110;&#25991;&#26412;&#24341;&#23548;&#25193;&#25955;&#22270;&#20687;&#39118;&#26684;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style Transfer. (arXiv:2303.08622v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#39118;&#26684;&#36801;&#31227;&#20013;&#30340;&#38646;&#26679;&#26412;&#23545;&#27604;&#25439;&#22833;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#20855;&#26377;&#30456;&#21516;&#35821;&#20041;&#20869;&#23481;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#39118;&#26684;&#36801;&#31227;&#20013;&#34920;&#29616;&#20986;&#26497;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#30001;&#20110;&#20854;&#38543;&#26426;&#24615;&#32780;&#23384;&#22312;&#39118;&#26684;&#36716;&#25442;&#21644;&#20869;&#23481;&#20445;&#25252;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#35745;&#31639;&#23494;&#38598;&#30340;&#25193;&#25955;&#27169;&#22411;&#24494;&#35843;&#25110;&#38468;&#21152;&#31070;&#32463;&#32593;&#32476;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#23545;&#27604;&#25439;&#22833;&#65292;&#23427;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#24494;&#35843;&#25110;&#36741;&#21161;&#32593;&#32476;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#29983;&#25104;&#26679;&#26412;&#21644;&#21407;&#22987;&#22270;&#20687;&#23884;&#20837;&#20043;&#38388;&#30340;&#22270;&#22359;&#23545;&#27604;&#25439;&#22833;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20197;&#38646;&#26679;&#26412;&#30340;&#26041;&#24335;&#29983;&#25104;&#20855;&#26377;&#19982;&#28304;&#22270;&#20687;&#30456;&#21516;&#35821;&#20041;&#20869;&#23481;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20445;&#30041;&#20869;&#23481;&#19988;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#30340;&#21516;&#26102;&#65292;&#22312;&#22270;&#20687;&#39118;&#26684;&#36801;&#31227;&#12289;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#21644;&#25805;&#20316;&#20013;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have shown great promise in text-guided image style transfer, but there is a trade-off between style transformation and content preservation due to their stochastic nature. Existing methods require computationally expensive fine-tuning of diffusion models or additional neural network. To address this, here we propose a zero-shot contrastive loss for diffusion models that doesn't require additional fine-tuning or auxiliary networks. By leveraging patch-wise contrastive loss between generated samples and original image embeddings in the pre-trained diffusion model, our method can generate images with the same semantic content as the source image in a zero-shot manner. Our approach outperforms existing methods while preserving content and requiring no additional training, not only for image style transfer but also for image-to-image translation and manipulation. Our experimental results validate the effectiveness of our proposed method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#26679;&#26412;&#39640;&#25928;&#31639;&#27861;&#65292;&#23558; UCB &#31639;&#27861;&#65288;Auer&#31561;&#20154;&#65292;2002&#65289;&#24212;&#29992;&#20110;&#22996;&#25176;&#20195;&#29702;&#27169;&#22411;&#30340;&#22312;&#32447;&#35774;&#32622;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#36890;&#36807;&#19982;&#31574;&#30053;&#20195;&#29702;&#22810;&#27425;&#20114;&#21160;&#26469;&#35774;&#35745;&#26368;&#20248;&#30340;&#35745;&#20998;&#35268;&#21017;&#65292;&#24182;&#23454;&#29616;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.08613</link><description>&lt;p&gt;
&#23398;&#20064;&#22870;&#21169;&#20449;&#24687;&#33719;&#21462;&#65306;&#27491;&#30830;&#35745;&#20998;&#35268;&#21017;&#36935;&#21040;&#22996;&#25176;&#20195;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning to Incentivize Information Acquisition: Proper Scoring Rules Meet Principal-Agent Model. (arXiv:2303.08613v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#26679;&#26412;&#39640;&#25928;&#31639;&#27861;&#65292;&#23558; UCB &#31639;&#27861;&#65288;Auer&#31561;&#20154;&#65292;2002&#65289;&#24212;&#29992;&#20110;&#22996;&#25176;&#20195;&#29702;&#27169;&#22411;&#30340;&#22312;&#32447;&#35774;&#32622;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#36890;&#36807;&#19982;&#31574;&#30053;&#20195;&#29702;&#22810;&#27425;&#20114;&#21160;&#26469;&#35774;&#35745;&#26368;&#20248;&#30340;&#35745;&#20998;&#35268;&#21017;&#65292;&#24182;&#23454;&#29616;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22996;&#25176;&#20195;&#29702;&#27169;&#22411;&#20013;&#30340;&#28608;&#21169;&#20449;&#24687;&#33719;&#21462;&#38382;&#39064;&#12290;&#27492;&#38382;&#39064;&#34987;&#24314;&#27169;&#20026;&#22996;&#25176;&#26041;&#21644;&#20195;&#29702;&#26041;&#20043;&#38388;&#30340; Stackelberg &#21338;&#24328;&#65292;&#20854;&#20013;&#22996;&#25176;&#20154;&#23459;&#24067;&#20102;&#19968;&#26465;&#24471;&#20998;&#35268;&#21017;&#26469;&#25351;&#23450;&#20184;&#27454;&#65292;&#28982;&#21518;&#20195;&#29702;&#26041;&#36873;&#25321;&#26368;&#22823;&#21270;&#20854;&#33258;&#36523;&#21033;&#28070;&#21644;&#25253;&#21578;&#20449;&#24687;&#30340;&#21162;&#21147;&#27700;&#24179;&#12290;&#25105;&#20204;&#20174;&#22996;&#25176;&#26041;&#30340;&#35282;&#24230;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#30340;&#22312;&#32447;&#35774;&#32622;&#65292;&#21363;&#36890;&#36807;&#19982;&#31574;&#30053;&#20195;&#29702;&#22810;&#27425;&#20132;&#20114;&#26469;&#35774;&#35745;&#26368;&#20248;&#35745;&#20998;&#35268;&#21017;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#30340;&#26679;&#26412;&#39640;&#25928;&#31639;&#27861;&#65292;&#23558; UCB &#31639;&#27861; (Auer et al., 2002) &#37327;&#36523;&#23450;&#21046;&#21040;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;&#20854;&#22312; T &#27425;&#36845;&#20195;&#21518;&#23454;&#29616;&#20102;&#27425;&#32447;&#24615; $T^{2/3}$-&#36951;&#25022;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20855;&#26377;&#23545;&#22996;&#25176;&#26041;&#26368;&#20248;&#21033;&#28070;&#36827;&#34892;&#31934;&#32454;&#20272;&#35745;&#30340;&#36807;&#31243;&#20197;&#21450;&#20445;&#23432;&#32416;&#27491;&#26041;&#26696;&#65292;&#20197;&#30830;&#20445;&#20195;&#29702;&#26041;&#30340;&#34892;&#21160;&#24471;&#21040;&#26377;&#25928;&#28608;&#21169;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#36951;&#25022;&#30028;&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#24449;&#26159;&#23427;&#26159;&#28176;&#36827;&#26368;&#23567;&#21487;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the incentivized information acquisition problem, where a principal hires an agent to gather information on her behalf. Such a problem is modeled as a Stackelberg game between the principal and the agent, where the principal announces a scoring rule that specifies the payment, and then the agent then chooses an effort level that maximizes her own profit and reports the information. We study the online setting of such a problem from the principal's perspective, i.e., designing the optimal scoring rule by repeatedly interacting with the strategic agent. We design a provably sample efficient algorithm that tailors the UCB algorithm (Auer et al., 2002) to our model, which achieves a sublinear $T^{2/3}$-regret after $T$ iterations. Our algorithm features a delicate estimation procedure for the optimal profit of the principal, and a conservative correction scheme that ensures the desired agent's actions are incentivized. Furthermore, a key feature of our regret bound is that it is i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;PG-DRR&#26694;&#26550;&#65292;&#36890;&#36807;&#21152;&#20837;&#39640;&#26031;&#36807;&#31243;&#23618;&#26469;&#25552;&#39640;&#23545;&#35805;&#21709;&#24212;&#26816;&#32034;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#24615;&#33021;&#19981;&#21464;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20302;&#30340;&#26657;&#20934;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2303.08606</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;Polya-Gamma&#22686;&#24378;&#23545;&#20110;&#23545;&#35805;&#26816;&#32034;&#27169;&#22411;&#30340;&#26657;&#20934;&#21644;&#19981;&#30830;&#23450;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Calibration and Uncertainty with P\'{o}lya-Gamma Augmentation for Dialog Retrieval Models. (arXiv:2303.08606v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;PG-DRR&#26694;&#26550;&#65292;&#36890;&#36807;&#21152;&#20837;&#39640;&#26031;&#36807;&#31243;&#23618;&#26469;&#25552;&#39640;&#23545;&#35805;&#21709;&#24212;&#26816;&#32034;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#24615;&#33021;&#19981;&#21464;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20302;&#30340;&#26657;&#20934;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#26816;&#32034;&#27169;&#22411;&#24050;&#32463;&#20805;&#20998;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#33021;&#21147;&#65292;&#20294;&#20272;&#35745;&#23427;&#20204;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22823;&#22810;&#25968;&#23545;&#35805;&#21709;&#24212;&#26816;&#32034;&#27169;&#22411;&#20026;&#27599;&#20010;&#21709;&#24212;&#36755;&#20986;&#19968;&#20010;&#30456;&#20851;&#38382;&#39064;&#30340;&#21333;&#20010;&#24471;&#20998;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22351;&#26657;&#20934;&#20250;&#23548;&#33268;&#21508;&#31181;&#19981;&#30830;&#23450;&#24615;&#21333;&#20010;&#24471;&#20998;&#65292;&#20174;&#32780;&#19981;&#21487;&#38752;&#30340;&#39044;&#27979;&#24635;&#26159;&#35823;&#23548;&#29992;&#25143;&#20915;&#31574;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26657;&#20934;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26694;&#26550;PG-DRR&#65292;&#29992;&#20110;&#23545;&#35805;&#21709;&#24212;&#26816;&#32034;&#27169;&#22411;&#65292;&#23427;&#23558;&#19968;&#20010;&#39640;&#26031;&#36807;&#31243;&#23618;&#28155;&#21152;&#21040;&#19968;&#20010;&#30830;&#23450;&#24615;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#24182;&#36890;&#36807;Polya-Gamma&#22686;&#24378;&#24674;&#22797;&#20849;&#36717;&#20197;&#20415;&#24471;&#21040;&#26131;&#22788;&#29702;&#30340;&#21518;&#39564;&#25512;&#26029;&#12290;&#26368;&#21518;&#65292;PG-DRR&#22312;&#22495;&#20869;&#25968;&#25454;&#38598;&#21644;&#20998;&#24067;&#20559;&#31227;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20302;&#30340;&#23454;&#35777;&#26657;&#20934;&#35823;&#24046;&#65288;ECE&#65289;&#65292;&#21516;&#26102;&#20445;&#25345;$R_{10}@1$&#21644;MAP&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural retrieval models have amply demonstrated their power but estimating the reliability of their predictions remains challenging. Most dialog response retrieval models output a single score for a response on how relevant it is to a given question. However, the bad calibration of deep neural network results in various uncertainty for the single score such that the unreliable predictions always misinform user decisions. To investigate these issues, we present an efficient calibration and uncertainty estimation framework PG-DRR for dialog response retrieval models which adds a Gaussian Process layer to a deterministic deep neural network and recovers conjugacy for tractable posterior inference by P\'{o}lya-Gamma augmentation. Finally, PG-DRR achieves the lowest empirical calibration error (ECE) in the in-domain datasets and the distributional shift task while keeping $R_{10}@1$ and MAP performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#26031;&#36807;&#31243;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#26694;&#26550;GPF-BERT&#29992;&#20110;&#22522;&#20110;BERT&#30340;&#23545;&#35805;&#25628;&#32034;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#31070;&#32463;&#25490;&#21517;&#22120;&#65292;&#30456;&#36739;&#20110;&#22522;&#26412;&#26657;&#20934;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#32463;&#39564;&#26657;&#20934;&#35823;&#24046;&#21644;&#26356;&#39640;&#30340;&#26816;&#32034;&#24615;&#33021;&#65292;&#22312;&#26102;&#38388;&#19978;&#20855;&#26377;8&#20493;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.08599</link><description>&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#30340;&#39640;&#25928;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#29992;&#20110;&#21487;&#38752;&#23545;&#35805;&#21709;&#24212;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Efficient Uncertainty Estimation with Gaussian Process for Reliable Dialog Response Retrieval. (arXiv:2303.08599v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#26031;&#36807;&#31243;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#26694;&#26550;GPF-BERT&#29992;&#20110;&#22522;&#20110;BERT&#30340;&#23545;&#35805;&#25628;&#32034;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#31070;&#32463;&#25490;&#21517;&#22120;&#65292;&#30456;&#36739;&#20110;&#22522;&#26412;&#26657;&#20934;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#32463;&#39564;&#26657;&#20934;&#35823;&#24046;&#21644;&#26356;&#39640;&#30340;&#26816;&#32034;&#24615;&#33021;&#65292;&#22312;&#26102;&#38388;&#19978;&#20855;&#26377;8&#20493;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#24335;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#34987;&#35777;&#26126;&#26159;&#19981;&#33391;&#26657;&#20934;&#30340;&#12290;&#34429;&#28982;&#20687;&#33945;&#29305;&#21345;&#32599;Dropout&#21644;Ensemble&#36825;&#26679;&#30340;&#22522;&#26412;&#26657;&#20934;&#26041;&#27861;&#21487;&#20197;&#24456;&#22909;&#22320;&#26657;&#20934;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#35757;&#32451;&#25110;&#25512;&#29702;&#38454;&#27573;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#26694;&#26550;GPF-BERT&#65292;&#29992;&#20110;&#22522;&#20110;BERT&#30340;&#20250;&#35805;&#25628;&#32034;&#65292;&#23427;&#37319;&#29992;&#39640;&#26031;&#36807;&#31243;&#23618;&#21644;&#28966;&#28857;&#25439;&#22833;&#22312;BERT&#26550;&#26500;&#30340;&#39030;&#37096;&#65292;&#20197;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#31070;&#32463;&#25490;&#21517;&#22120;&#12290;&#22823;&#37327;&#23454;&#39564;&#29992;&#20110;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#19982;&#22522;&#26412;&#26657;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;GPF-BERT&#22312;&#19977;&#20010;&#39046;&#22495;&#20869;&#25968;&#25454;&#38598;&#21644;&#20998;&#24067;&#20559;&#31227;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20302;&#30340;&#32463;&#39564;&#26657;&#20934;&#35823;&#24046;&#65288;ECE&#65289;&#65292;&#21516;&#26102;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20135;&#29983;&#20102;&#26368;&#39640;&#30340;$R_{10}@1$&#21644;MAP&#24615;&#33021;&#12290;&#22312;&#26102;&#38388;&#28040;&#32791;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;GPF-BERT&#20855;&#26377;8&#20493;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have achieved remarkable performance in retrieval-based dialogue systems, but they are shown to be ill calibrated. Though basic calibration methods like Monte Carlo Dropout and Ensemble can calibrate well, these methods are time-consuming in the training or inference stages. To tackle these challenges, we propose an efficient uncertainty calibration framework GPF-BERT for BERT-based conversational search, which employs a Gaussian Process layer and the focal loss on top of the BERT architecture to achieve a high-quality neural ranker. Extensive experiments are conducted to verify the effectiveness of our method. In comparison with basic calibration methods, GPF-BERT achieves the lowest empirical calibration error (ECE) in three in-domain datasets and the distributional shift tasks, while yielding the highest $R_{10}@1$ and MAP performance on most cases. In terms of time consumption, our GPF-BERT has an 8$\times$ speedup.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25935;&#24863;&#24230;&#24863;&#30693;&#30340;&#35270;&#35273;&#21442;&#25968;&#20302;&#25928;&#35843;&#25972;&#65288;SPT&#65289;&#26041;&#26696;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#23558;&#21487;&#35757;&#32451;&#21442;&#25968;&#20998;&#37197;&#21040;&#20219;&#21153;&#29305;&#23450;&#30340;&#37325;&#35201;&#20301;&#32622;&#65292;&#20197;&#25552;&#39640;&#34920;&#31034;&#33021;&#21147;&#65292;&#36866;&#24212;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#21040;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.08566</link><description>&lt;p&gt;
&#25935;&#24863;&#24230;&#24863;&#30693;&#30340;&#35270;&#35273;&#21442;&#25968;&#20302;&#25928;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Sensitivity-Aware Visual Parameter-Efficient Tuning. (arXiv:2303.08566v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25935;&#24863;&#24230;&#24863;&#30693;&#30340;&#35270;&#35273;&#21442;&#25968;&#20302;&#25928;&#35843;&#25972;&#65288;SPT&#65289;&#26041;&#26696;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#23558;&#21487;&#35757;&#32451;&#21442;&#25968;&#20998;&#37197;&#21040;&#20219;&#21153;&#29305;&#23450;&#30340;&#37325;&#35201;&#20301;&#32622;&#65292;&#20197;&#25552;&#39640;&#34920;&#31034;&#33021;&#21147;&#65292;&#36866;&#24212;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#21040;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21442;&#25968;&#20302;&#25928;&#35843;&#25972;&#65288;VPET&#65289;&#24050;&#25104;&#20026;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#21040;&#19979;&#28216;&#20219;&#21153;&#30340;&#24378;&#21170;&#26367;&#20195;&#26041;&#27861;&#12290;&#29616;&#26377;VPET&#26041;&#27861;&#26681;&#25454;&#20154;&#24037;&#21551;&#21457;&#24335;&#26041;&#27861;&#23558;&#21487;&#35757;&#32451;&#21442;&#25968;&#24341;&#20837;&#19981;&#21516;&#20219;&#21153;&#30340;&#30456;&#21516;&#20301;&#32622;&#65292;&#24573;&#30053;&#39046;&#22495;&#24046;&#24322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25935;&#24863;&#24230;&#24863;&#30693;&#30340;&#35270;&#35273;&#21442;&#25968;&#20302;&#25928;&#35843;&#25972;&#65288;SPT&#65289;&#26041;&#26696;&#65292;&#20197;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#20998;&#37197;&#21487;&#35757;&#32451;&#21442;&#25968;&#21040;&#20219;&#21153;&#29305;&#23450;&#30340;&#37325;&#35201;&#20301;&#32622;&#65292;&#32473;&#23450;&#25152;&#38656;&#30340;&#21487;&#35843;&#21442;&#25968;&#39044;&#31639;&#12290;&#26412;&#25991;&#39318;&#20808;&#20381;&#25454;&#25968;&#25454;&#30340;&#30456;&#20851;&#24615;&#24555;&#36895;&#35782;&#21035;&#29305;&#23450;&#20219;&#21153;&#25152;&#38656;&#35843;&#25972;&#30340;&#25935;&#24863;&#21442;&#25968;&#65292;&#28982;&#21518;&#25552;&#21319;&#34920;&#31034;&#33021;&#21147;&#65292;&#22686;&#22823;&#37325;&#35201;&#30340;&#26435;&#37325;&#30697;&#38453;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Parameter-Efficient Tuning (VPET) has become a powerful alternative for full fine-tuning so as to adapt pre-trained vision models to downstream tasks, which only tunes a small number of parameters while freezing the vast majority ones to ease storage burden and optimization difficulty. However, existing VPET methods introduce trainable parameters to the same positions across different tasks depending solely on human heuristics and neglect the domain gaps. To this end, we study where to introduce and how to allocate trainable parameters by proposing a novel Sensitivity-aware visual Parameter-efficient Tuning (SPT) scheme, which adaptively allocates trainable parameters to task-specific important positions given a desired tunable parameter budget. Specifically, our SPT first quickly identifies the sensitive parameters that require tuning for a given task in a data-dependent way. Next, our SPT further boosts the representational capability for the weight matrices whose number of se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#35748;&#30693;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#65292;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#35821;&#20041;&#20449;&#24687;&#26816;&#27979;&#31639;&#27861;&#21644;&#19968;&#31181;&#26377;&#25928;&#30340;&#35821;&#20041;&#32416;&#38169;&#31639;&#27861;&#65292;&#24182;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#24615;&#33021;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#20248;&#20110;&#29616;&#26377;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2303.08546</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#39537;&#21160;&#30340;&#35748;&#30693;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#65306;&#21407;&#29702;&#12289;&#23454;&#29616;&#21644;&#24615;&#33021;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Cognitive Semantic Communication Systems Driven by Knowledge Graph: Principle, Implementation, and Performance Evaluation. (arXiv:2303.08546v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#35748;&#30693;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#65292;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#35821;&#20041;&#20449;&#24687;&#26816;&#27979;&#31639;&#27861;&#21644;&#19968;&#31181;&#26377;&#25928;&#30340;&#35821;&#20041;&#32416;&#38169;&#31639;&#27861;&#65292;&#24182;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#24615;&#33021;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#20248;&#20110;&#29616;&#26377;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#36890;&#20449;&#34987;&#35270;&#20026;&#31361;&#30772;&#39321;&#20892;&#26497;&#38480;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#35821;&#20041;&#25512;&#26029;&#21644;&#35821;&#20041;&#32416;&#38169;&#30740;&#31350;&#19981;&#36275;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#30340;&#32416;&#38169;&#26041;&#27861;&#19981;&#21487;&#35299;&#37322;&#21644;&#19981;&#28789;&#27963;&#65292;&#38480;&#21046;&#20102;&#20854;&#24615;&#33021;&#12290;&#26412;&#25991;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#24320;&#21457;&#20102;&#35748;&#30693;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#65292;&#38024;&#23545;&#21333;&#29992;&#25143;&#21644;&#22810;&#29992;&#25143;&#24773;&#26223;&#25552;&#20986;&#20102;&#20004;&#31181;&#35748;&#30693;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#36890;&#29992;&#12289;&#21487;&#35299;&#37322;&#30340;&#35821;&#20041;&#20449;&#24687;&#26816;&#27979;&#31639;&#27861;&#12290;&#36827;&#19968;&#27493;&#22320;&#65292;&#20174;&#30693;&#35782;&#22270;&#35889;&#20013;&#25366;&#25496;&#25512;&#29702;&#35268;&#21017;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35821;&#20041;&#32416;&#38169;&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24674;&#22797;&#35821;&#20041;&#20449;&#24687;&#12290;&#23545;&#20110;&#22810;&#29992;&#25143;&#35748;&#30693;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#29992;&#25143;&#31038;&#20132;&#23646;&#24615;&#30340;&#20449;&#24687;&#24674;&#22797;&#31639;&#27861;&#12290;&#24615;&#33021;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#35748;&#30693;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#20248;&#20110;&#29616;&#26377;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic communication is envisioned as a promising technique to break through the Shannon limit. However, semantic inference and semantic error correction have not been well studied. Moreover, error correction methods of existing semantic communication frameworks are inexplicable and inflexible, which limits the achievable performance. In this paper, to tackle this issue, a knowledge graph is exploited to develop semantic communication systems. Two cognitive semantic communication frameworks are proposed for the single-user and multiple-user communication scenarios. Moreover, a simple, general, and interpretable semantic alignment algorithm for semantic information detection is proposed. Furthermore, an effective semantic correction algorithm is proposed by mining the inference rule from the knowledge graph. Additionally, the pre-trained model is fine-tuned to recover semantic information. For the multi-user cognitive semantic communication system, a message recovery algorithm is prop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#36125;&#21494;&#26031;&#23398;&#20064;&#26041;&#27861;&#23545;&#39564;&#35777;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#20808;&#39564;&#30693;&#35782;&#21644;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#22788;&#29702;&#65292;&#20197;&#35745;&#31639;&#20854;&#26399;&#26395;&#30340;&#20107;&#20214;&#21457;&#29983;&#29575;&#33539;&#22260;&#65292;&#36827;&#32780;&#36827;&#34892;&#24378;&#20581;&#39564;&#35777;&#12290;&#35813;&#26694;&#26550;&#25152;&#29992;&#26041;&#27861;&#21487;&#22788;&#29702;&#24120;&#35268;&#20107;&#20214;&#21644;&#32597;&#35265;&#24773;&#20917;&#65292;&#33021;&#22788;&#29702;&#30495;&#23454;&#31995;&#32479;&#20869;&#22312;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.08476</link><description>&lt;p&gt;
&#33258;&#20027;&#26426;&#22120;&#20154;&#24378;&#20581;&#39564;&#35777;&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bayesian Learning for the Robust Verification of Autonomous Robots. (arXiv:2303.08476v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#36125;&#21494;&#26031;&#23398;&#20064;&#26041;&#27861;&#23545;&#39564;&#35777;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#20808;&#39564;&#30693;&#35782;&#21644;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#22788;&#29702;&#65292;&#20197;&#35745;&#31639;&#20854;&#26399;&#26395;&#30340;&#20107;&#20214;&#21457;&#29983;&#29575;&#33539;&#22260;&#65292;&#36827;&#32780;&#36827;&#34892;&#24378;&#20581;&#39564;&#35777;&#12290;&#35813;&#26694;&#26550;&#25152;&#29992;&#26041;&#27861;&#21487;&#22788;&#29702;&#24120;&#35268;&#20107;&#20214;&#21644;&#32597;&#35265;&#24773;&#20917;&#65292;&#33021;&#22788;&#29702;&#30495;&#23454;&#31995;&#32479;&#20869;&#22312;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#36816;&#34892;&#39564;&#35777;&#33258;&#20027;&#26426;&#22120;&#20154;&#36827;&#34892;&#20851;&#38190;&#20219;&#21153;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#39564;&#35777;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#20808;&#39564;&#30693;&#35782;&#21644;&#35266;&#27979;&#25968;&#25454;&#65292;&#23398;&#20064;&#20854;&#20107;&#20214;&#21457;&#29983;&#29575;&#30340;&#39044;&#26399;&#20540;&#30340;&#33539;&#22260;&#12290;&#35813;&#26694;&#26550;&#25903;&#25345;&#22312;&#31995;&#32479;&#25805;&#20316;&#26399;&#38388;&#24120;&#35268;&#35266;&#23519;&#21040;&#30340;&#20107;&#20214;&#20197;&#21450;&#37027;&#20123;&#32597;&#35265;&#25110;&#26159;&#28798;&#38590;&#24615;&#25925;&#38556;&#31561;&#22238;&#25253;&#24040;&#22823;&#30340;&#20107;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#20107;&#20214;&#21457;&#29983;&#29575;&#33539;&#22260;&#32452;&#35013;&#21306;&#38388;&#36830;&#32493;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65292;&#24182;&#24212;&#29992;&#23450;&#37327;&#39564;&#35777;&#26041;&#27861;&#26469;&#35745;&#31639;&#20851;&#38190;&#31995;&#32479;&#23646;&#24615;&#30340;&#39044;&#26399;&#21464;&#21270;&#21306;&#38388;&#12290;&#36825;&#20123;&#21306;&#38388;&#21453;&#26144;&#20102;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#31995;&#32479;&#30340;&#20869;&#22312;&#19981;&#30830;&#23450;&#24615;&#65292;&#33021;&#22815;&#22312;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#19979;&#24378;&#20581;&#39564;&#35777;&#20854;&#23450;&#37327;&#23646;&#24615;&#12290;&#26412;&#25991;&#23558;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#27700;&#19979;&#33258;&#20027;&#26426;&#22120;&#20154;&#20219;&#21153;&#39564;&#35777;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a novel Bayesian learning framework that enables the runtime verification of autonomous robots performing critical missions in uncertain environments. Our framework exploits prior knowledge and observations of the verified robotic system to learn expected ranges of values for the occurrence rates of its events. We support both events observed regularly during system operation, and singular events such as catastrophic failures or the completion of difficult one-off tasks. Furthermore, we use the learnt event-rate ranges to assemble interval continuous-time Markov models, and we apply quantitative verification to these models to compute expected intervals of variation for key system properties. These intervals reflect the uncertainty intrinsic to many real-world systems, enabling the robust verification of their quantitative properties under parametric uncertainty. We apply the proposed framework to the case study of verification of an autonomous robotic mission for underwater
&lt;/p&gt;</description></item><item><title>&#35752;&#35770;&#27169;&#22359;&#21270;&#19982;&#31471;&#21040;&#31471;&#26550;&#26500;&#30340;&#20248;&#32570;&#28857;&#65292;&#35748;&#20026;&#27169;&#22359;&#21270;&#26550;&#26500;&#22312;&#19982;&#20154;&#31867;&#29992;&#25143;&#21327;&#20316;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#30340;&#23545;&#35805;&#26426;&#22120;&#20154;&#20013;&#26159;&#39318;&#36873;&#12290;</title><link>http://arxiv.org/abs/2303.08470</link><description>&lt;p&gt;
&#35841;&#22312;&#25484;&#25511;&#65311;&#23545;&#35805;&#26426;&#22120;&#20154;&#20915;&#31574;&#32452;&#20214;&#30340;&#35282;&#33394;&#19982;&#36131;&#20219;
&lt;/p&gt;
&lt;p&gt;
Who's in Charge? Roles and Responsibilities of Decision-Making Components in Conversational Robots. (arXiv:2303.08470v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08470
&lt;/p&gt;
&lt;p&gt;
&#35752;&#35770;&#27169;&#22359;&#21270;&#19982;&#31471;&#21040;&#31471;&#26550;&#26500;&#30340;&#20248;&#32570;&#28857;&#65292;&#35748;&#20026;&#27169;&#22359;&#21270;&#26550;&#26500;&#22312;&#19982;&#20154;&#31867;&#29992;&#25143;&#21327;&#20316;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#30340;&#23545;&#35805;&#26426;&#22120;&#20154;&#20013;&#26159;&#39318;&#36873;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#26426;&#22120;&#20154;&#30340;&#36719;&#20214;&#26550;&#26500;&#36890;&#24120;&#30001;&#22810;&#20010;&#27169;&#22359;&#32452;&#25104;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#35774;&#35745;&#29992;&#20110;&#29305;&#23450;&#30340;&#22788;&#29702;&#20219;&#21153;&#25110;&#21151;&#33021;&#12290;&#20854;&#20013;&#19968;&#20123;&#27169;&#22359;&#26159;&#20026;&#20102;&#20915;&#31574;&#26426;&#22120;&#20154;&#22312;&#24403;&#21069;&#32972;&#26223;&#19979;&#25191;&#34892;&#30340;&#19979;&#19968;&#20010;&#21160;&#20316;&#32780;&#24320;&#21457;&#30340;&#12290;&#36825;&#20123;&#21160;&#20316;&#21487;&#33021;&#28041;&#21450;&#21040;&#29289;&#29702;&#31227;&#21160;&#65292;&#22914;&#21521;&#21069;&#39537;&#21160;&#25110;&#25235;&#21462;&#29289;&#20307;&#65292;&#20294;&#20063;&#21487;&#33021;&#23545;&#24212;&#20110;&#20132;&#27969;&#34892;&#20026;&#65292;&#22914;&#21521;&#20154;&#31867;&#29992;&#25143;&#38382;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21453;&#24605;&#20154;-&#26426;&#22120;&#20154;&#20132;&#20114;&#24179;&#21488;&#20013;&#36825;&#20123;&#20915;&#31574;&#27169;&#22359;&#30340;&#32452;&#32455;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#27169;&#22359;&#21270;&#21644;&#31471;&#21040;&#31471;&#26550;&#26500;&#30340;&#30456;&#23545;&#20248;&#32570;&#28857;&#65292;&#24182;&#35748;&#20026;&#65292;&#23613;&#31649;&#31471;&#21040;&#31471;&#26041;&#27861;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#22312;&#24320;&#21457;&#26088;&#22312;&#19982;&#20154;&#31867;&#29992;&#25143;&#21327;&#20316;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#30340;&#23545;&#35805;&#26426;&#22120;&#20154;&#26102;&#65292;&#27169;&#22359;&#21270;&#26550;&#26500;&#20173;&#28982;&#26159;&#39318;&#36873;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22823;&#22810;&#25968;&#23454;&#38469;&#20154;-&#26426;&#22120;&#20154;&#20132;&#20114;&#26550;&#26500;&#24448;&#24448;&#26159;&#20197;&#26426;&#22120;&#20154;&#20026;&#20013;&#24515;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software architectures for conversational robots typically consist of multiple modules, each designed for a particular processing task or functionality. Some of these modules are developed for the purpose of making decisions about the next action that the robot ought to perform in the current context. Those actions may relate to physical movements, such as driving forward or grasping an object, but may also correspond to communicative acts, such as asking a question to the human user. In this position paper, we reflect on the organization of those decision modules in human-robot interaction platforms. We discuss the relative benefits and limitations of modular vs. end-to-end architectures, and argue that, despite the increasing popularity of end-to-end approaches, modular architectures remain preferable when developing conversational robots designed to execute complex tasks in collaboration with human users. We also show that most practical HRI architectures tend to be either robot-cen
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;Mixup&#23545;&#20110;&#29305;&#24449;&#23398;&#20064;&#30340;&#30410;&#22788;&#12290;&#28151;&#21512;&#35757;&#32451;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#28151;&#21512;&#25968;&#25454;&#20013;&#23398;&#20064;&#32597;&#35265;&#29305;&#24449;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#26631;&#20934;&#35757;&#32451;&#21487;&#33021;&#20250;&#28431;&#25481;&#36825;&#20123;&#32597;&#35265;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2303.08433</link><description>&lt;p&gt;
&#28151;&#21512;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;Mixup&#23545;&#20110;&#29305;&#24449;&#23398;&#20064;&#30340;&#30410;&#22788;
&lt;/p&gt;
&lt;p&gt;
The Benefits of Mixup for Feature Learning. (arXiv:2303.08433v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;Mixup&#23545;&#20110;&#29305;&#24449;&#23398;&#20064;&#30340;&#30410;&#22788;&#12290;&#28151;&#21512;&#35757;&#32451;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#28151;&#21512;&#25968;&#25454;&#20013;&#23398;&#20064;&#32597;&#35265;&#29305;&#24449;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#26631;&#20934;&#35757;&#32451;&#21487;&#33021;&#20250;&#28431;&#25481;&#36825;&#20123;&#32597;&#35265;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Mixup&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#32447;&#24615;&#25554;&#20540;&#38543;&#26426;&#28151;&#21512;&#20004;&#20010;&#25968;&#25454;&#28857;&#65292;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#27867;&#21270;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#20854;&#26377;&#25928;&#24615;&#30340;&#29702;&#35770;&#22522;&#30784;&#23578;&#26410;&#23436;&#20840;&#34987;&#29702;&#35299;&#12290;&#26412;&#25991;&#26088;&#22312;&#23547;&#27714;&#23545;Mixup&#30410;&#22788;&#30340;&#22522;&#26412;&#29702;&#35299;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;Mixup&#22312;&#29305;&#24449;&#21644;&#26631;&#31614;&#20351;&#29992;&#19981;&#21516;&#30340;&#32447;&#24615;&#25554;&#20540;&#21442;&#25968;&#26102;&#20173;&#21487;&#23454;&#29616;&#31867;&#20284;&#20110;&#26631;&#20934;Mixup&#30340;&#24615;&#33021;&#12290;&#36825;&#34920;&#26126;&#65292;Zhang&#31561;&#20154;&#65288;2018&#65289;&#25552;&#20986;&#30340;&#30452;&#35266;&#32447;&#24615;&#35299;&#37322;&#21487;&#33021;&#24182;&#19981;&#33021;&#23436;&#20840;&#35299;&#37322;Mixup&#30340;&#25104;&#21151;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#29305;&#24449;&#23398;&#20064;&#30340;&#35282;&#24230;&#23545;Mixup&#36827;&#34892;&#29702;&#35770;&#30740;&#31350;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#29305;&#24449;&#22122;&#22768;&#25968;&#25454;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;Mixup&#35757;&#32451;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#20854;&#19982;&#24120;&#35265;&#29305;&#24449;&#65288;&#20986;&#29616;&#22312;&#22823;&#37096;&#20998;&#25968;&#25454;&#20013;&#65289;&#28151;&#21512;&#20013;&#23398;&#20064;&#32597;&#35265;&#29305;&#24449;&#65288;&#20986;&#29616;&#22312;&#23569;&#37096;&#20998;&#25968;&#25454;&#20013;&#65289;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26631;&#20934;&#35757;&#32451;&#21487;&#33021;&#20250;&#28431;&#25481;&#36825;&#20123;&#32597;&#35265;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixup, a simple data augmentation method that randomly mixes two data points via linear interpolation, has been extensively applied in various deep learning applications to gain better generalization. However, the theoretical underpinnings of its efficacy are not yet fully understood. In this paper, we aim to seek a fundamental understanding of the benefits of Mixup. We first show that Mixup using different linear interpolation parameters for features and labels can still achieve similar performance to the standard Mixup. This indicates that the intuitive linearity explanation in Zhang et al., (2018) may not fully explain the success of Mixup. Then we perform a theoretical study of Mixup from the feature learning perspective. We consider a feature-noise data model and show that Mixup training can effectively learn the rare features (appearing in a small fraction of data) from its mixture with the common features (appearing in a large fraction of data). In contrast, standard training ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Implicit Ray-Transformers&#30340;&#22810;&#35270;&#35282;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#65292;&#20511;&#21161;&#31070;&#32463;&#22330;&#21644;Ray Transformer&#65292;&#26377;&#25928;&#22788;&#29702;&#31232;&#30095;&#26631;&#27880;&#24773;&#20917;&#19979;&#30340;&#36965;&#24863;&#22330;&#26223;&#65292;&#23454;&#29616;&#20934;&#30830;&#19988;&#35270;&#35282;&#19968;&#33268;&#30340;&#35821;&#20041;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2303.08401</link><description>&lt;p&gt;
&#22522;&#20110;Implicit Ray-Transformers&#30340;&#22810;&#35270;&#35282;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Implicit Ray-Transformers for Multi-view Remote Sensing Image Segmentation. (arXiv:2303.08401v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Implicit Ray-Transformers&#30340;&#22810;&#35270;&#35282;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#65292;&#20511;&#21161;&#31070;&#32463;&#22330;&#21644;Ray Transformer&#65292;&#26377;&#25928;&#22788;&#29702;&#31232;&#30095;&#26631;&#27880;&#24773;&#20917;&#19979;&#30340;&#36965;&#24863;&#22330;&#26223;&#65292;&#23454;&#29616;&#20934;&#30830;&#19988;&#35270;&#35282;&#19968;&#33268;&#30340;&#35821;&#20041;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22823;&#22810;&#25968;&#22522;&#20110;CNN&#30340;&#36965;&#24863;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#22823;&#37327;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20294;&#36825;&#31181;&#33539;&#24335;&#22240;&#26410;&#32771;&#34385;&#22330;&#26223;&#20013;&#30340;3D&#20449;&#24687;&#65292;&#32780;&#22312;&#20165;&#26377;&#23569;&#37327;&#26631;&#35760;&#35270;&#22270;&#30340;&#24773;&#20917;&#19979;&#38590;&#20197;&#24212;&#23545;&#36965;&#24863;&#22810;&#35270;&#35282;&#22330;&#26223;&#30340;&#20998;&#21106;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#22522;&#20110;Implicit Neural Representation&#65288;INR&#65289;&#25552;&#20986;&#20102;&#8220;Implicit Ray-Transformer&#65288;IRT&#65289;&#8221;&#65292;&#29992;&#20110;&#22788;&#29702;&#20165;&#26377;&#31232;&#30095;&#26631;&#27880;&#65288;&#22914;100&#20010;&#22270;&#20687;&#20013;&#30340;4-6&#20010;&#26631;&#31614;&#65289;&#30340;&#36965;&#24863;&#22330;&#26223;&#35821;&#20041;&#20998;&#21106;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#23558;&#22810;&#35270;&#22270;3D&#32467;&#26500;&#20808;&#39564;&#24341;&#20837;&#20219;&#21153;&#20197;&#33719;&#24471;&#20934;&#30830;&#19988;&#35270;&#35282;&#19968;&#33268;&#30340;&#35821;&#20041;&#20998;&#21106;&#30340;&#26032;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#20248;&#21270;&#31070;&#32463;&#22330;&#20197;&#32534;&#30721;&#22522;&#20110;&#22810;&#35270;&#22270;&#22270;&#20687;&#30340;&#36965;&#24863;&#22330;&#26223;&#30340;&#39068;&#33394;&#21644;3D&#32467;&#26500;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;Ray Transformer&#26469;&#21033;&#29992;&#31070;&#32463;&#22330;3D&#29305;&#24449;&#19982;2D&#32441;&#29702;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#23398;&#20064;&#26356;&#22909;&#30340;&#35821;&#20041;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
The mainstream CNN-based remote sensing (RS) image semantic segmentation approaches typically rely on massive labeled training data. Such a paradigm struggles with the problem of RS multi-view scene segmentation with limited labeled views due to the lack of considering 3D information within the scene. In this paper, we propose ''Implicit Ray-Transformer (IRT)'' based on Implicit Neural Representation (INR), for RS scene semantic segmentation with sparse labels (such as 4-6 labels per 100 images). We explore a new way of introducing multi-view 3D structure priors to the task for accurate and view-consistent semantic segmentation. The proposed method includes a two-stage learning process. In the first stage, we optimize a neural field to encode the color and 3D structure of the remote sensing scene based on multi-view images. In the second stage, we design a Ray Transformer to leverage the relations between the neural field 3D features and 2D texture features for learning better semantic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35757;&#32451;&#21442;&#25968;&#36739;&#23569;&#12289;&#20351;&#29992;&#25193;&#24352;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#21644;&#19977;&#20803;&#32452;&#25439;&#22833;&#30340;&#22270;&#20687;&#26816;&#32034;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#19981;&#22686;&#21152;&#27169;&#22411;&#22797;&#26434;&#24230;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#22270;&#20687;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.08398</link><description>&lt;p&gt;
&#19968;&#31181;&#19977;&#20803;&#32452;&#25439;&#22833;&#25193;&#24352;&#27531;&#24046;&#32593;&#32476;&#29992;&#20110;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#26816;&#32034;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Triplet-loss Dilated Residual Network for High-Resolution Representation Learning in Image Retrieval. (arXiv:2303.08398v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35757;&#32451;&#21442;&#25968;&#36739;&#23569;&#12289;&#20351;&#29992;&#25193;&#24352;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#21644;&#19977;&#20803;&#32452;&#25439;&#22833;&#30340;&#22270;&#20687;&#26816;&#32034;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#19981;&#22686;&#21152;&#27169;&#22411;&#22797;&#26434;&#24230;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#22270;&#20687;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20869;&#23481;&#30340;&#22270;&#20687;&#26816;&#32034;&#26159;&#20174;&#24191;&#27867;&#30340;&#22270;&#20687;&#24211;&#20013;&#22522;&#20110;&#35270;&#35273;&#20869;&#23481;&#65288;&#22914;&#39068;&#33394;&#65292;&#24418;&#29366;&#25110;&#31354;&#38388;&#20851;&#31995;&#21644;&#32441;&#29702;&#65289;&#26816;&#32034;&#22270;&#20687;&#23376;&#38598;&#30340;&#36807;&#31243;&#12290;&#22312;&#26576;&#20123;&#24212;&#29992;&#20013;&#65292;&#22914;&#26412;&#22320;&#21270;&#65292;&#22270;&#20687;&#26816;&#32034;&#34987;&#29992;&#20316;&#21021;&#22987;&#27493;&#39588;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26816;&#32034;&#21040;&#30340;&#21069;&#20960;&#20010;&#22270;&#20687;&#30340;&#20934;&#30830;&#24615;&#26174;&#30528;&#24433;&#21709;&#24635;&#20307;&#31995;&#32479;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22270;&#20687;&#26816;&#32034;&#31995;&#32479;&#65292;&#20854;&#21487;&#35757;&#32451;&#21442;&#25968;&#36739;&#23569;&#65292;&#21487;&#20197;&#22312;&#26816;&#32034;&#21040;&#30340;&#21069;&#20960;&#20010;&#22270;&#20687;&#20013;&#25552;&#20379;&#21487;&#25509;&#21463;&#30340;&#20934;&#30830;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#19968;&#31181;&#24102;&#26377;&#19977;&#20803;&#32452;&#25439;&#22833;&#30340;&#25193;&#24352;&#27531;&#24046;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#25193;&#22823;&#24863;&#21463;&#37326;&#20174;&#32780;&#25552;&#21462;&#26356;&#20016;&#23500;&#30340;&#20449;&#24687;&#65288;&#21363;&#39640;&#20998;&#36776;&#29575;&#34920;&#31034;&#65289;&#65292;&#20174;&#32780;&#25552;&#39640;&#22270;&#20687;&#26816;&#32034;&#20934;&#30830;&#24615;&#65292;&#32780;&#26080;&#38656;&#22686;&#21152;&#27169;&#22411;&#30340;&#28145;&#24230;&#25110;&#22797;&#26434;&#24230;&#12290;&#20026;&#22686;&#24378;&#25552;&#21462;&#34920;&#31034;&#30340;&#40065;&#26834;&#24615;&#65292;&#26412;&#30740;&#31350;&#33719;&#24471;&#20102;&#20505;&#36873;&#22270;&#20687;&#30340;&#22810;&#20010;Crop&#12290;
&lt;/p&gt;
&lt;p&gt;
Content-based image retrieval is the process of retrieving a subset of images from an extensive image gallery based on visual contents, such as color, shape or spatial relations, and texture. In some applications, such as localization, image retrieval is employed as the initial step. In such cases, the accuracy of the top-retrieved images significantly affects the overall system accuracy. The current paper introduces a simple yet efficient image retrieval system with a fewer trainable parameters, which offers acceptable accuracy in top-retrieved images. The proposed method benefits from a dilated residual convolutional neural network with triplet loss. Experimental evaluations show that this model can extract richer information (i.e., high-resolution representations) by enlarging the receptive field, thus improving image retrieval accuracy without increasing the depth or complexity of the model. To enhance the extracted representations' robustness, the current research obtains candidat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#30417;&#30563;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26426;&#26800;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#26469;&#36319;&#36394;&#32454;&#32990;&#36718;&#24275;&#19978;&#30340;&#25152;&#26377;&#28857;&#65292;&#19981;&#20165;&#22312;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#32780;&#19988;&#36824;&#39318;&#27425;&#32771;&#34385;&#20102;&#28857;&#23545;&#24212;&#65292;&#20855;&#26377;&#24212;&#29992;&#20110;&#27963;&#32454;&#32990;&#25104;&#20687;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.08364</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#26800;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#30340;&#38750;&#30417;&#30563;&#32454;&#32990;&#36718;&#24275;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Contour Tracking of Live Cells by Mechanical and Cycle Consistency Losses. (arXiv:2303.08364v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#30417;&#30563;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26426;&#26800;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#26469;&#36319;&#36394;&#32454;&#32990;&#36718;&#24275;&#19978;&#30340;&#25152;&#26377;&#28857;&#65292;&#19981;&#20165;&#22312;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#32780;&#19988;&#36824;&#39318;&#27425;&#32771;&#34385;&#20102;&#28857;&#23545;&#24212;&#65292;&#20855;&#26377;&#24212;&#29992;&#20110;&#27963;&#32454;&#32990;&#25104;&#20687;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#32454;&#32990;&#24418;&#24577;&#30340;&#21160;&#24577;&#21464;&#21270;&#23545;&#20110;&#29702;&#35299;&#27963;&#32454;&#32990;&#65288;&#21253;&#25324;&#24178;&#32454;&#32990;&#21644;&#36716;&#31227;&#24615;&#30284;&#30151;&#32454;&#32990;&#65289;&#30340;&#21508;&#31181;&#21151;&#33021;&#21644;&#29305;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#22312;&#27599;&#20010;&#27963;&#32454;&#32990;&#35270;&#39057;&#24103;&#19978;&#36319;&#36394;&#32454;&#32990;&#36718;&#24275;&#19978;&#30340;&#25152;&#26377;&#28857;&#12290;&#36718;&#24275;&#19978;&#30340;&#23616;&#37096;&#24418;&#29366;&#21644;&#32441;&#29702;&#19981;&#26126;&#26174;&#65292;&#20854;&#36816;&#21160;&#22797;&#26434;&#65292;&#24120;&#24120;&#20276;&#38543;&#23616;&#37096;&#36718;&#24275;&#29305;&#24449;&#30340;&#25193;&#24352;&#21644;&#25910;&#32553;&#12290;&#30446;&#21069;&#20809;&#27969;&#25110;&#28145;&#24230;&#28857;&#38598;&#36319;&#36394;&#30340;&#20808;&#21069;&#24037;&#20316;&#30001;&#20110;&#32454;&#32990;&#30340;&#27969;&#21160;&#24615;&#32780;&#19981;&#36866;&#29992;&#65292;&#20197;&#21069;&#30340;&#28145;&#24230;&#36718;&#24275;&#36319;&#36394;&#20063;&#27809;&#26377;&#32771;&#34385;&#28857;&#23545;&#24212;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#32454;&#32990;&#65288;&#25110;&#26356;&#19968;&#33324;&#30340;&#31896;&#24377;&#24615;&#26448;&#26009;&#65289;&#36718;&#24275;&#36319;&#36394;&#65292;&#36890;&#36807;&#34701;&#21512;&#20004;&#20010;&#36718;&#24275;&#20043;&#38388;&#30340;&#23494;&#38598;&#34920;&#31034;&#21644;&#20132;&#21449;&#20851;&#27880;&#23454;&#29616;&#28857;&#23545;&#24212;&#12290;&#30001;&#20110;&#25163;&#21160;&#26631;&#35760;&#36718;&#24275;&#19978;&#30340;&#23494;&#38598;&#36319;&#36394;&#28857;&#26159;&#19981;&#29616;&#23454;&#30340;&#65292;&#22240;&#27492;&#37319;&#29992;&#20102;&#26426;&#26800;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#30340;&#38750;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#22312;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#27963;&#32454;&#32990;&#25104;&#20687;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analyzing the dynamic changes of cellular morphology is important for understanding the various functions and characteristics of live cells, including stem cells and metastatic cancer cells. To this end, we need to track all points on the highly deformable cellular contour in every frame of live cell video. Local shapes and textures on the contour are not evident, and their motions are complex, often with expansion and contraction of local contour features. The prior arts for optical flow or deep point set tracking are unsuited due to the fluidity of cells, and previous deep contour tracking does not consider point correspondence. We propose the first deep learning-based tracking of cellular (or more generally viscoelastic materials) contours with point correspondence by fusing dense representation between two contours with cross attention. Since it is impractical to manually label dense tracking points on the contour, unsupervised learning comprised of the mechanical and cyclical cons
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#24352;&#37327;&#20998;&#20139;&#30340;&#27169;&#22411;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#32553;&#23567;&#33267;5M&#21442;&#25968;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#22312;&#20302;&#20869;&#23384;&#31070;&#32463;&#22788;&#29702;&#22120;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#22987;&#32456;&#22788;&#20110;&#36816;&#34892;&#29366;&#24577;&#30340;&#35821;&#38899;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2303.08343</link><description>&lt;p&gt;
&#22522;&#20110;&#20302;&#31209;&#24352;&#37327;&#20998;&#20139;&#30340;Tiny Ambient Speech Recognition&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Sharing Low Rank Conformer Weights for Tiny Always-On Ambient Speech Recognition Models. (arXiv:2303.08343v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#24352;&#37327;&#20998;&#20139;&#30340;&#27169;&#22411;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#32553;&#23567;&#33267;5M&#21442;&#25968;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#22312;&#20302;&#20869;&#23384;&#31070;&#32463;&#22788;&#29702;&#22120;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#22987;&#32456;&#22788;&#20110;&#36816;&#34892;&#29366;&#24577;&#30340;&#35821;&#38899;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#25345;&#32493;&#25913;&#36827;&#20026;&#20351;&#29992;&#26356;&#22823;&#30340;&#27169;&#22411;&#21644;&#26356;&#22823;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#26032;&#26426;&#20250;&#12290;&#20294;&#26159;&#65292;&#22312;&#20165;&#26377;&#20302;&#20869;&#23384;&#30340;&#26234;&#33021;&#25163;&#26426;&#12289;&#21487;&#31359;&#25140;&#35774;&#22791;&#21644;&#20854;&#20182;&#23884;&#20837;&#24335;&#29615;&#22659;&#31561;&#20302;&#21151;&#32791;&#35774;&#22791;&#19978;&#25552;&#20379;&#36825;&#20123;&#26032;&#21151;&#33021;&#30340;&#38656;&#27714;&#19982;&#26085;&#20465;&#22686;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#20943;&#23567;&#22522;&#20110;Conformer&#30340;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#27169;&#22411;&#22823;&#23567;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#20110;100M&#20010;&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#23558;&#20854;&#32553;&#23567;&#21040;&#20165;$5$M&#20010;&#21442;&#25968;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#27169;&#22411;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#20855;&#26377;&#20302;&#20869;&#23384;&#31070;&#32463;&#22788;&#29702;&#22120;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#22987;&#32456;&#22788;&#20110;&#36816;&#34892;&#29366;&#24577;&#30340;&#35821;&#38899;&#35782;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#22312;&#27169;&#22411;&#26550;&#26500;&#20013;&#30340;&#19981;&#21516;&#23618;&#27425;&#19978;&#37325;&#22797;&#20351;&#29992;&#27169;&#22411;&#26435;&#37325;: (i) &#37325;&#22797;&#25972;&#20010;Conformer&#22359;&#23618;&#65292;(ii) &#22312;&#23618;&#20043;&#38388;&#20849;&#20139;&#29305;&#23450;&#30340;Conformer&#27169;&#22359;&#65292;(iii) &#22312;Conformer&#27169;&#22359;&#20013;&#20849;&#20139;&#23376;&#37096;&#20214;&#65292;(iv) &#22312;&#20302;&#31209;&#24352;&#37327;&#20998;&#35299;&#21518;&#20849;&#20139;&#20998;&#35299;&#30340;&#23376;&#37096;&#20214;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continued improvements in machine learning techniques offer exciting new opportunities through the use of larger models and larger training datasets. However, there is a growing need to offer these new capabilities on-board low-powered devices such as smartphones, wearables and other embedded environments where only low memory is available. Towards this, we consider methods to reduce the model size of Conformer-based speech recognition models which typically require models with greater than 100M parameters down to just $5$M parameters while minimizing impact on model quality. Such a model allows us to achieve always-on ambient speech recognition on edge devices with low-memory neural processors. We propose model weight reuse at different levels within our model architecture: (i) repeating full conformer block layers, (ii) sharing specific conformer modules across layers, (iii) sharing sub-components per conformer module, and (iv) sharing decomposed sub-component weights after low-rank 
&lt;/p&gt;</description></item><item><title>FactReranker&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#36741;&#21161;&#35780;&#20272;&#22120;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#25688;&#35201;&#19982;&#25918;&#23556;&#23398;&#21457;&#29616;&#23454;&#20917;&#19968;&#33268;&#24615;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#20107;&#23454;&#24341;&#23548;&#26469;&#26377;&#25928;&#22320;&#36873;&#25321;&#26368;&#20339;&#30340;&#25688;&#35201;&#12290;</title><link>http://arxiv.org/abs/2303.08335</link><description>&lt;p&gt;
FactReranker&#65306;&#22522;&#20110;&#20107;&#23454;&#24341;&#23548;&#30340;&#36741;&#21161;&#35780;&#20272;&#22120;&#29992;&#20110;&#24544;&#23454;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
FactReranker: Fact-guided Reranker for Faithful Radiology Report Summarization. (arXiv:2303.08335v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08335
&lt;/p&gt;
&lt;p&gt;
FactReranker&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#36741;&#21161;&#35780;&#20272;&#22120;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#25688;&#35201;&#19982;&#25918;&#23556;&#23398;&#21457;&#29616;&#23454;&#20917;&#19968;&#33268;&#24615;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#20107;&#23454;&#24341;&#23548;&#26469;&#26377;&#25928;&#22320;&#36873;&#25321;&#26368;&#20339;&#30340;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#26159;&#19968;&#39033;&#33267;&#20851;&#37325;&#35201;&#30340;&#20020;&#24202;&#20219;&#21153;&#65292;&#20854;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#20445;&#25345;&#25152;&#20135;&#29983;&#30340;&#25688;&#35201;&#21644;&#22320;&#38754;&#23454;&#20917;&#25918;&#23556;&#23398;&#21457;&#29616;&#20043;&#38388;&#30340;&#23454;&#38469;&#20934;&#30830;&#24615;&#12290;&#29616;&#26377;&#30740;&#31350;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#30452;&#25509;&#20248;&#21270;&#27491;&#30830;&#35748;&#30693;&#24230;&#37327;&#25351;&#26631;&#65292;&#22914;CheXBert&#25110;RadGraph&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20351;&#29992;&#36138;&#23146;&#25628;&#32034;&#25110;&#26463;&#25628;&#32034;&#30340;&#35299;&#30721;&#26041;&#27861;&#65292;&#22312;&#36873;&#25321;&#26368;&#20339;&#20505;&#36873;&#39033;&#26102;&#27809;&#26377;&#32771;&#34385;&#20107;&#23454;&#30340;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#23454;&#38469;&#19968;&#33268;&#24615;&#30340;&#25913;&#21892;&#21463;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31532;&#20108;&#38454;&#27573;&#25688;&#35201;&#26041;&#27861;FactReranker&#65292;&#23427;&#26159;&#31532;&#19968;&#27425;&#23581;&#35797;&#22522;&#20110;&#23427;&#20204;&#20272;&#35745;&#30340;&#23454;&#38469;&#19968;&#33268;&#24615;&#24471;&#20998;&#26469;&#23398;&#20064;&#20174;&#25152;&#26377;&#20505;&#36873;&#39033;&#20013;&#36873;&#25321;&#26368;&#20339;&#25688;&#35201;&#12290;&#25105;&#20204;&#24314;&#35758;&#22522;&#20110;RadGraph&#27169;&#24335;&#25552;&#21462;&#36755;&#20837;&#21307;&#30103;&#25253;&#21578;&#12289;&#20854;&#40644;&#37329;&#25688;&#35201;&#21644;&#20505;&#36873;&#25688;&#35201;&#30340;&#21307;&#30103;&#20107;&#23454;&#65292;&#24182;&#35774;&#35745;&#22522;&#20110;&#20107;&#23454;&#24341;&#23548;&#30340;&#37325;&#26032;&#25490;&#24207;&#22120;&#65292;&#20197;&#26377;&#25928;&#22320;&#32467;&#21512;&#25552;&#21462;&#30340;&#21307;&#30103;&#20107;&#23454;&#26469;&#36873;&#25321;&#26368;&#20339;&#25688;&#35201;&#12290;&#25105;&#20204;&#20998;&#35299;&#20102;&#20107;&#23454;-
&lt;/p&gt;
&lt;p&gt;
Automatic radiology report summarization is a crucial clinical task, whose key challenge is to maintain factual accuracy between produced summaries and ground truth radiology findings. Existing research adopts reinforcement learning to directly optimize factual consistency metrics such as CheXBert or RadGraph score. However, their decoding method using greedy search or beam search considers no factual consistency when picking the optimal candidate, leading to limited factual consistency improvement. To address it, we propose a novel second-stage summarizing approach FactReranker, the first attempt that learns to choose the best summary from all candidates based on their estimated factual consistency score. We propose to extract medical facts of the input medical report, its gold summary, and candidate summaries based on the RadGraph schema and design the fact-guided reranker to efficiently incorporate the extracted medical facts for selecting the optimal summary. We decompose the fact-
&lt;/p&gt;</description></item><item><title>&#32852;&#37030;&#23398;&#20064;&#26159;&#23454;&#29616;6G&#32593;&#32476;&#20154;&#24037;&#26234;&#33021;&#26222;&#21450;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#20294;&#22312;6G&#32593;&#32476;&#20013;&#65292;&#23384;&#22312;&#19968;&#20123;&#31995;&#32479;&#21644;&#32479;&#35745;&#24322;&#26500;&#24615;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#36825;&#20123;&#24322;&#26500;&#24615;&#38382;&#39064;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.08322</link><description>&lt;p&gt;
&#24322;&#26500;6G&#32593;&#32476;&#20013;&#32852;&#37030;&#23398;&#20064;&#30340;&#20248;&#21270;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Optimization Design for Federated Learning in Heterogeneous 6G Networks. (arXiv:2303.08322v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08322
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#23454;&#29616;6G&#32593;&#32476;&#20154;&#24037;&#26234;&#33021;&#26222;&#21450;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#20294;&#22312;6G&#32593;&#32476;&#20013;&#65292;&#23384;&#22312;&#19968;&#20123;&#31995;&#32479;&#21644;&#32479;&#35745;&#24322;&#26500;&#24615;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#36825;&#20123;&#24322;&#26500;&#24615;&#38382;&#39064;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;5G&#32593;&#32476;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20159;&#19975;&#26234;&#33021;&#29289;&#32852;&#32593;&#35774;&#22791;&#21644;&#24222;&#22823;&#30340;&#25968;&#25454;&#22312;&#32593;&#32476;&#36793;&#32536;&#29983;&#25104;&#12290;&#23613;&#31649;&#20173;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#65292;&#20294;&#39044;&#35745;&#27491;&#22312;&#21457;&#23637;&#20013;&#30340;6G&#32593;&#32476;&#23558;&#37319;&#29992;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#25910;&#38598;&#12289;&#20256;&#36755;&#21644;&#23398;&#20064;&#36825;&#20123;&#23453;&#36149;&#30340;&#25968;&#25454;&#65292;&#20197;&#23454;&#29616;&#21019;&#26032;&#24212;&#29992;&#21644;&#26234;&#33021;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#23558;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#21040;&#25968;&#25454;&#20013;&#24515;&#25110;&#20113;&#20013;&#65292;&#24341;&#21457;&#20102;&#20005;&#37325;&#30340;&#29992;&#25143;&#38544;&#31169;&#38382;&#39064;&#12290;&#20316;&#20026;&#19968;&#31181;&#26032;&#20852;&#30340;&#20998;&#24067;&#24335;&#20154;&#24037;&#26234;&#33021;&#33539;&#20363;&#65292;&#20855;&#26377;&#38544;&#31169;&#20445;&#25252;&#24615;&#36136;&#30340;&#32852;&#37030;&#23398;&#20064;&#34987;&#35270;&#20026;&#23454;&#29616;6G&#32593;&#32476;&#26222;&#36941;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;6G&#32593;&#32476;&#20013;&#26377;&#25928;&#12289;&#39640;&#25928;&#22320;&#23454;&#29616;&#32852;&#37030;&#23398;&#20064;&#20173;&#23384;&#22312;&#19968;&#20123;&#31995;&#32479;&#21644;&#32479;&#35745;&#24322;&#26500;&#24615;&#25361;&#25112;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#36825;&#20123;&#24322;&#26500;&#24615;&#38382;&#39064;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid advancement of 5G networks, billions of smart Internet of Things (IoT) devices along with an enormous amount of data are generated at the network edge. While still at an early age, it is expected that the evolving 6G network will adopt advanced artificial intelligence (AI) technologies to collect, transmit, and learn this valuable data for innovative applications and intelligent services. However, traditional machine learning (ML) approaches require centralizing the training data in the data center or cloud, raising serious user-privacy concerns. Federated learning, as an emerging distributed AI paradigm with privacy-preserving nature, is anticipated to be a key enabler for achieving ubiquitous AI in 6G networks. However, there are several system and statistical heterogeneity challenges for effective and efficient FL implementation in 6G networks. In this article, we investigate the optimization approaches that can effectively address the challenging heterogeneity issues
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#25968;&#19975;&#20010;&#38646;-shot&#23454;&#39564;&#23545;&#22522;&#20110;&#21518;&#35757;&#32451;&#37327;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#21516;&#37327;&#21270;&#32452;&#20214;&#36827;&#34892;&#20102;&#32508;&#21512;&#30740;&#31350;&#65292;&#32467;&#26524;&#21457;&#29616;&#32454;&#31890;&#24230;&#37327;&#21270;&#21644;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#24456;&#37325;&#35201;&#65292;&#29992;&#31895;&#31890;&#24230;&#37327;&#21270;&#30340;&#26356;&#39640;&#20301;&#25968;&#27604;&#29992;&#38750;&#24120;&#32454;&#31890;&#24230;&#30340;&#26356;&#20302;&#20301;&#25968;&#26356;&#24378;&#22823;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#22914;&#20309;&#20026;&#19981;&#21516;&#22823;&#23567;&#30340;\llms&#21033;&#29992;&#37327;&#21270;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2303.08302</link><description>&lt;p&gt;
&#22522;&#20110;&#21518;&#35757;&#32451;&#37327;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#21512;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study on Post-Training Quantization for Large Language Models. (arXiv:2303.08302v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#25968;&#19975;&#20010;&#38646;-shot&#23454;&#39564;&#23545;&#22522;&#20110;&#21518;&#35757;&#32451;&#37327;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#21516;&#37327;&#21270;&#32452;&#20214;&#36827;&#34892;&#20102;&#32508;&#21512;&#30740;&#31350;&#65292;&#32467;&#26524;&#21457;&#29616;&#32454;&#31890;&#24230;&#37327;&#21270;&#21644;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#24456;&#37325;&#35201;&#65292;&#29992;&#31895;&#31890;&#24230;&#37327;&#21270;&#30340;&#26356;&#39640;&#20301;&#25968;&#27604;&#29992;&#38750;&#24120;&#32454;&#31890;&#24230;&#30340;&#26356;&#20302;&#20301;&#25968;&#26356;&#24378;&#22823;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#22914;&#20309;&#20026;&#19981;&#21516;&#22823;&#23567;&#30340;\llms&#21033;&#29992;&#37327;&#21270;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;&#26159;&#19968;&#31181;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#23384;&#28040;&#32791;&#21644;/&#25110;&#35745;&#31639;&#25104;&#26412;&#30340;&#26435;&#34913;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#19981;&#21516;&#37327;&#21270;&#26041;&#26696;&#12289;&#19981;&#21516;&#27169;&#22411;&#26063;&#12289;&#19981;&#21516;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#12289;&#19981;&#21516;&#37327;&#21270;&#20301;&#31934;&#24230;&#31561;&#30340;&#24433;&#21709;&#30340;&#20840;&#38754;&#30740;&#31350;&#20173;&#32570;&#22833;&#12290;&#26412;&#25991;&#36890;&#36807;&#25968;&#19975;&#20010;&#38646;-shot&#23454;&#39564;&#23545;&#36825;&#20123;&#32452;&#20214;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65306;(1)&#32454;&#31890;&#24230;&#37327;&#21270;&#21644;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;(&#32780;&#19981;&#26159;&#26420;&#32032;&#30340;&#26368;&#36817;&#33293;&#20837;&#37327;&#21270;)&#26159;&#23454;&#29616;&#33391;&#22909;&#31934;&#24230;&#30340;&#24517;&#35201;&#26465;&#20214;&#65307;(2) &#29992;&#31895;&#31890;&#24230;&#37327;&#21270;&#30340;&#26356;&#39640;&#20301;&#25968;&#65288;&#22914;5&#20301;&#65289;&#27604;&#29992;&#38750;&#24120;&#32454;&#31890;&#24230;&#30340;&#26356;&#20302;&#20301;&#25968;&#65288;&#22914;4&#20301;&#65289;&#65288;&#20854;&#26377;&#25928;&#20301;&#25968;&#19982;5&#20301;&#30456;&#20284;&#65289;&#26356;&#24378;&#22823;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#22914;&#20309;&#20026;&#19981;&#21516;&#22823;&#23567;&#30340;\llms&#21033;&#29992;&#37327;&#21270;&#30340;&#24314;&#35758;&#65292;&#24182;&#30041;&#19979;&#26410;&#26469;&#26426;&#20250;&#21644;&#31995;&#32479;&#24037;&#20316;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization (\ptq) had been recently shown as a compromising method to reduce the memory consumption and/or compute cost for large language models. However, a comprehensive study about the effect of different quantization schemes, different model families, different \ptq methods, different quantization bit precision, etc, is still missing. In this work, we provide an extensive study on those components over tens of thousands of zero-shot experiments. Our results show that (1) Fine-grained quantization and \ptq methods (instead of naive round-to-nearest quantization) are necessary to achieve good accuracy and (2) Higher bits (e.g., 5 bits) with coarse-grained quantization is more powerful than lower bits (e.g., 4 bits) with very fine-grained quantization (whose effective bits is similar to 5-bits). We also present recommendations about how to utilize quantization for \llms with different sizes, and leave suggestions of future opportunities and system work that are not res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#29305;&#24449;&#24037;&#31243;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#32593;&#32476;&#29289;&#29702;&#30005;&#21147;&#31995;&#32479;&#25968;&#25454;&#36136;&#37327;&#12289;&#35745;&#31639;&#25104;&#26412;&#21644;&#20887;&#20313;&#27979;&#37327;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25925;&#38556;&#35786;&#26029;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.08300</link><description>&lt;p&gt;
&#23398;&#20064;&#39640;&#32500;&#32593;&#32476;&#29289;&#29702;&#25968;&#25454;&#27969;&#29992;&#20110;&#26234;&#33021;&#30005;&#32593;&#30340;&#25925;&#38556;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Learning From High-Dimensional Cyber-Physical Data Streams for Diagnosing Faults in Smart Grids. (arXiv:2303.08300v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#29305;&#24449;&#24037;&#31243;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#32593;&#32476;&#29289;&#29702;&#30005;&#21147;&#31995;&#32479;&#25968;&#25454;&#36136;&#37327;&#12289;&#35745;&#31639;&#25104;&#26412;&#21644;&#20887;&#20313;&#27979;&#37327;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25925;&#38556;&#35786;&#26029;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25925;&#38556;&#35786;&#26029;&#31995;&#32479;&#30340;&#24615;&#33021;&#21463;&#32593;&#32476;&#29289;&#29702;&#30005;&#21147;&#31995;&#32479;&#25968;&#25454;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#31995;&#32479;&#20135;&#29983;&#22823;&#37327;&#25968;&#25454;&#65292;&#20351;&#31995;&#32479;&#25215;&#21463;&#36807;&#22810;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#21478;&#19968;&#20010;&#38382;&#39064;&#26159;&#35760;&#24405;&#27979;&#37327;&#20013;&#23384;&#22312;&#22122;&#22768;&#65292;&#36825;&#21487;&#20197;&#38450;&#27490;&#26500;&#24314;&#31934;&#30830;&#30340;&#20915;&#31574;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#35786;&#26029;&#27169;&#22411;&#36890;&#24120;&#37197;&#22791;&#20102;&#19968;&#32452;&#20887;&#20313;&#27979;&#37327;&#25968;&#25454;&#65292;&#21487;&#33021;&#20559;&#31163;&#27491;&#24120;&#21644;&#25925;&#38556;&#20998;&#24067;&#30340;&#23398;&#20064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#29305;&#24449;&#24037;&#31243;&#23545;&#20110;&#32531;&#35299;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#20013;&#19978;&#36848;&#25361;&#25112;&#30340;&#24433;&#21709;&#12290;&#29305;&#24449;&#36873;&#25321;&#21644;&#38477;&#32500;&#26041;&#27861;&#19982;&#20915;&#31574;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#27169;&#25311;&#20102;&#23545;118&#20010;&#24635;&#32447;&#30005;&#21147;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#39537;&#21160;&#25925;&#38556;&#35786;&#26029;&#12290;&#22240;&#27492;&#65292;&#24320;&#23637;&#20102;&#27604;&#36739;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#20004;&#20010;&#39046;&#22495;&#20013;&#30340;&#20960;&#31181;&#20808;&#36827;&#25216;&#26415;&#12290;&#21516;&#26102;&#27604;&#36739;&#20102;&#38477;&#32500;&#21644;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of fault diagnosis systems is highly affected by data quality in cyber-physical power systems. These systems generate massive amounts of data that overburden the system with excessive computational costs. Another issue is the presence of noise in recorded measurements, which prevents building a precise decision model. Furthermore, the diagnostic model is often provided with a mixture of redundant measurements that may deviate it from learning normal and fault distributions. This paper presents the effect of feature engineering on mitigating the aforementioned challenges in cyber-physical systems. Feature selection and dimensionality reduction methods are combined with decision models to simulate data-driven fault diagnosis in a 118-bus power system. A comparative study is enabled accordingly to compare several advanced techniques in both domains. Dimensionality reduction and feature selection methods are compared both jointly and separately. Finally, experiments are con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#23558;&#22238;&#25910;&#20877;&#21033;&#29992;&#26448;&#26009;&#29992;&#20110;&#25935;&#25463;&#21046;&#36896;&#65292;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#20998;&#26512;&#65292;&#20197;&#20915;&#31574;&#25903;&#25345;&#23454;&#29616;&#29615;&#22659;&#21487;&#25345;&#32493;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.08291</link><description>&lt;p&gt;
&#22312;&#22238;&#25910;&#26448;&#26009;&#21487;&#25345;&#32493;&#21270;&#30340;&#25935;&#25463;&#21046;&#36896;&#20013;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;(arXiv:2303.08291v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
Machine Learning Approaches in Agile Manufacturing with Recycled Materials for Sustainability. (arXiv:2303.08291v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#23558;&#22238;&#25910;&#20877;&#21033;&#29992;&#26448;&#26009;&#29992;&#20110;&#25935;&#25463;&#21046;&#36896;&#65292;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#20998;&#26512;&#65292;&#20197;&#20915;&#31574;&#25903;&#25345;&#23454;&#29616;&#29615;&#22659;&#21487;&#25345;&#32493;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26448;&#26009;&#31185;&#23398;&#21644;&#21046;&#36896;&#19994;&#20013;&#24320;&#21457;&#21487;&#25345;&#32493;&#30340;&#12289;&#29615;&#20445;&#30340;&#36807;&#31243;&#38750;&#24120;&#37325;&#35201;&#12290;&#20154;&#24037;&#26234;&#33021;&#22312;&#20915;&#31574;&#25903;&#25345;&#26041;&#38754;&#21487;&#20197;&#36215;&#21040;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#36825;&#19968;&#28857;&#24050;&#32463;&#22312;&#25105;&#20204;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#24471;&#20197;&#35777;&#26126;&#65292;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#24320;&#21457;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#29992;&#20110;&#35745;&#31639;&#20272;&#35745;&#21644;&#19987;&#23478;&#31995;&#32479;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20915;&#31574;&#25903;&#25345;&#23454;&#29616;&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#29615;&#22659;&#21487;&#25345;&#32493;&#24615;&#65292;&#24212;&#29992;&#22238;&#25910;&#21644;&#20877;&#29983;&#26448;&#26009;&#36827;&#34892;&#25935;&#25463;&#21046;&#36896;&#65292;&#36825;&#26159;&#19968;&#31181;&#23433;&#20840;&#12289;&#36127;&#36131;&#20219;&#30340;&#23558;&#29305;&#23450;&#24223;&#24323;&#29289;&#36716;&#21464;&#25104;&#22686;&#20540;&#20135;&#21697;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;AI&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#20998;&#26512;&#65292;&#20197;&#25351;&#23548;&#21046;&#36896;&#20013;&#30340;&#20915;&#31574;&#25903;&#25345;&#65292;&#21253;&#25324;&#21033;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#24433;&#21709;&#26448;&#26009;&#28909;&#22788;&#29702;&#21450;&#20854;&#24615;&#33021;&#30340;&#21442;&#25968;&#65292;&#20197;&#21450;&#36890;&#36807;&#20808;&#36827;&#25216;&#26415;&#65288;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65289;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#65292;&#26469;&#25506;&#32034;&#31890;&#24230;&#22823;&#23567;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is important to develop sustainable processes in materials science and manufacturing that are environmentally friendly. AI can play a significant role in decision support here as evident from our earlier research leading to tools developed using our proposed machine learning based approaches. Such tools served the purpose of computational estimation and expert systems. This research addresses environmental sustainability in materials science via decision support in agile manufacturing using recycled and reclaimed materials. It is a safe and responsible way to turn a specific waste stream to value-added products. We propose to use data-driven methods in AI by applying machine learning models for predictive analysis to guide decision support in manufacturing. This includes harnessing artificial neural networks to study parameters affecting heat treatment of materials and impacts on their properties; deep learning via advances such as convolutional neural networks to explore grain size
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#35282;&#24230;-AT&#65292;&#32467;&#21512;&#36229;&#29699;&#23884;&#20837;&#21644;&#22522;&#20110;&#35282;&#24230;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#20197;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.08289</link><description>&lt;p&gt;
&#36890;&#36807;&#36229;&#29699;&#23884;&#20837;&#21644;&#22522;&#20110;&#35282;&#24230;&#30340;&#27491;&#21017;&#21270;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Adversarial Robustness with Hypersphere Embedding and Angular-based Regularizations. (arXiv:2303.08289v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#35282;&#24230;-AT&#65292;&#32467;&#21512;&#36229;&#29699;&#23884;&#20837;&#21644;&#22522;&#20110;&#35282;&#24230;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#20197;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#25915;&#20987;&#20855;&#26377;&#19968;&#23450;&#25928;&#26524;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;AT&#30340;&#21464;&#20307;&#26469;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290; Pang&#31561;&#20154;&#26368;&#36817;&#34920;&#26126;&#65292;&#23558;&#36229;&#29699;&#23884;&#20837;&#65288;HE&#65289;&#32435;&#20837;&#29616;&#26377;&#30340;AT&#36807;&#31243;&#20013;&#21487;&#20197;&#25552;&#39640;&#23545;&#25239;&#24615;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#29616;&#26377;&#30340;AT&#36807;&#31243;&#24182;&#19981;&#26159;&#20026;HE&#26694;&#26550;&#35774;&#35745;&#30340;&#65292;&#22240;&#27492;&#26410;&#33021;&#20805;&#20998;&#23398;&#20064;HE&#26694;&#26550;&#20013;&#30340;&#35282;&#24230;&#21028;&#21035;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;HE&#19982;&#21033;&#29992;HE&#26694;&#26550;&#20013;&#30340;&#20016;&#23500;&#35282;&#24230;&#20449;&#24687;&#30340;&#27491;&#21017;&#21270;&#39033;&#30446;&#30456;&#32467;&#21512;&#65292;&#23558;&#20854;&#38598;&#25104;&#21040;AT&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#31216;&#20026;&#35282;&#24230;AT&#65292;&#23558;&#27491;&#21017;&#21270;&#39033;&#30446;&#28155;&#21152;&#21040;AT&#20013;&#65292;&#24182;&#36890;&#36807;&#35282;&#24230;&#29305;&#24449;&#26126;&#30830;&#24378;&#21046;&#26435;&#37325;&#29305;&#24449;&#32039;&#20945;&#24615;&#21644;&#31867;&#38388;&#20998;&#38548;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35282;&#24230;AT&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training (AT) methods have been found to be effective against adversarial attacks on deep neural networks. Many variants of AT have been proposed to improve its performance. Pang et al. [1] have recently shown that incorporating hypersphere embedding (HE) into the existing AT procedures enhances robustness. We observe that the existing AT procedures are not designed for the HE framework, and thus fail to adequately learn the angular discriminative information available in the HE framework. In this paper, we propose integrating HE into AT with regularization terms that exploit the rich angular information available in the HE framework. Specifically, our method, termed angular-AT, adds regularization terms to AT that explicitly enforce weight-feature compactness and inter-class separation; all expressed in terms of angular features. Experimental results show that angular-AT further improves adversarial robustness.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20511;&#21161;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25506;&#31350;&#26367;&#20195;&#29123;&#26009;&#27773;&#36710;&#30340;&#26222;&#21450;&#65292;&#21516;&#26102;&#23558;&#20854;&#19982;&#28040;&#36153;&#32773;&#30340;&#31038;&#20250;&#32463;&#27982;&#22320;&#20301;&#21644;&#31354;&#27668;&#36136;&#37327;&#25351;&#25968;&#36827;&#34892;&#20851;&#32852;&#65292;&#20174;&#32780;&#21046;&#23450;&#21512;&#36866;&#30340;&#25919;&#31574;&#12290;</title><link>http://arxiv.org/abs/2303.08286</link><description>&lt;p&gt;
&#23558;&#26367;&#20195;&#29123;&#26009;&#27773;&#36710;&#30340;&#37319;&#29992;&#19982;&#31038;&#20250;&#32463;&#27982;&#22320;&#20301;&#21644;&#31354;&#27668;&#36136;&#37327;&#25351;&#25968;&#32852;&#31995;&#36215;&#26469;
&lt;/p&gt;
&lt;p&gt;
Linking Alternative Fuel Vehicles Adoption with Socioeconomic Status and Air Quality Index. (arXiv:2303.08286v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08286
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20511;&#21161;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25506;&#31350;&#26367;&#20195;&#29123;&#26009;&#27773;&#36710;&#30340;&#26222;&#21450;&#65292;&#21516;&#26102;&#23558;&#20854;&#19982;&#28040;&#36153;&#32773;&#30340;&#31038;&#20250;&#32463;&#27982;&#22320;&#20301;&#21644;&#31354;&#27668;&#36136;&#37327;&#25351;&#25968;&#36827;&#34892;&#20851;&#32852;&#65292;&#20174;&#32780;&#21046;&#23450;&#21512;&#36866;&#30340;&#25919;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#39033;&#30740;&#31350;&#65292;&#30740;&#31350;&#26367;&#20195;&#29123;&#26009;&#27773;&#36710;&#30340;&#28508;&#22312;&#24191;&#27867;&#20351;&#29992;&#65292;&#23558;&#23427;&#20204;&#19982;&#30456;&#24212;&#28040;&#36153;&#32773;&#30340;&#31038;&#20250;&#32463;&#27982;&#22320;&#20301;&#20197;&#21450;&#23545; resulting &#31354;&#27668;&#36136;&#37327;&#25351;&#25968;&#30340;&#24433;&#21709;&#32852;&#31995;&#36215;&#26469;&#12290;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20197;&#20419;&#36827;&#36866;&#24403;&#30340;&#25919;&#31574;&#65292;&#25512;&#24191;&#26367;&#20195;&#29123;&#26009;&#27773;&#36710;&#30340;&#26222;&#21450;&#65292;&#20363;&#22914;&#30005;&#21160;&#27773;&#36710;&#65292;&#35201;&#20844;&#27491;&#23545;&#24453;&#19981;&#21516;&#30340;&#20154;&#32676;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992; Pearson &#30456;&#20851;&#31995;&#25968;&#23545;&#31038;&#20250;&#32463;&#27982;&#25968;&#25454;&#12289;&#31354;&#27668;&#36136;&#37327;&#25351;&#25968;&#21644;&#26367;&#20195;&#29123;&#26009;&#27773;&#36710;&#30340;&#25968;&#25454;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;&#22522;&#20110;&#31038;&#20250;&#32463;&#27982;&#22240;&#32032;&#65292;&#20351;&#29992;&#32447;&#24615;&#22238;&#24402;&#23545;&#31354;&#27668;&#36136;&#37327;&#25351;&#25968;&#36827;&#34892;&#39044;&#27979;&#24314;&#27169;&#65292;&#20197;&#26681;&#25454;&#26367;&#20195;&#29123;&#26009;&#27773;&#36710;&#30340;&#37319;&#29992;&#24773;&#20917;&#36827;&#34892;&#35843;&#25972;&#12290;&#36825;&#39033;&#24037;&#20316;&#35777;&#26126;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#31038;&#20250;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
This is a study on the potential widespread usage of alternative fuel vehicles, linking them with the socio-economic status of the respective consumers as well as the impact on the resulting air quality index. Research in this area aims to leverage machine learning techniques in order to promote appropriate policies for the proliferation of alternative fuel vehicles such as electric vehicles with due justice to different population groups. Pearson correlation coefficient is deployed in the modeling the relationships between socio-economic data, air quality index and data on alternative fuel vehicles. Linear regression is used to conduct predictive modeling on air quality index as per the adoption of alternative fuel vehicles, based on socio-economic factors. This work exemplifies artificial intelligence for social good.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#39118;&#38505;&#19982;&#25317;&#25380;&#29615;&#22659;&#19979;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#65292;&#36890;&#36807;&#25506;&#32034;&#20154;&#20204;&#22312;COVID-19&#30123;&#24773;&#36141;&#29289;&#22330;&#26223;&#20013;&#30340;&#36335;&#24452;&#36873;&#25321;&#20559;&#22909;&#65292;&#24182;&#35780;&#20272;&#19977;&#31181;&#27969;&#34892;&#30340;&#39118;&#38505;&#27169;&#22411;(CPT&#65292;CVaR&#21644;ER)&#65292;&#20197;&#26356;&#22909;&#22320;&#35774;&#35745;&#26426;&#22120;&#20154;&#23548;&#33322;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;(XAI)&#12290;</title><link>http://arxiv.org/abs/2303.08284</link><description>&lt;p&gt;
&#22312;&#39118;&#38505;&#19982;&#25317;&#25380;&#29615;&#22659;&#20013;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#65306;&#29702;&#35299;&#20154;&#31867;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
Robot Navigation in Risky, Crowded Environments: Understanding Human Preferences. (arXiv:2303.08284v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#39118;&#38505;&#19982;&#25317;&#25380;&#29615;&#22659;&#19979;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#65292;&#36890;&#36807;&#25506;&#32034;&#20154;&#20204;&#22312;COVID-19&#30123;&#24773;&#36141;&#29289;&#22330;&#26223;&#20013;&#30340;&#36335;&#24452;&#36873;&#25321;&#20559;&#22909;&#65292;&#24182;&#35780;&#20272;&#19977;&#31181;&#27969;&#34892;&#30340;&#39118;&#38505;&#27169;&#22411;(CPT&#65292;CVaR&#21644;ER)&#65292;&#20197;&#26356;&#22909;&#22320;&#35774;&#35745;&#26426;&#22120;&#20154;&#23548;&#33322;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;(XAI)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#38505;&#19982;&#25317;&#25380;&#29615;&#22659;(RCE)&#21253;&#21547;&#34394;&#25311;&#30340;&#39118;&#38505;&#21644;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#65292;&#32780;&#36825;&#20123;&#39118;&#38505;&#34987;&#19981;&#21516;&#30340;&#20154;&#24863;&#30693;&#20026;&#19981;&#21516;&#30340;&#34892;&#20026;&#65292;&#22240;&#27492;&#65292;&#22312;RCE&#20013;&#37096;&#32626;&#30340;&#26426;&#22120;&#20154;&#38656;&#35201;&#20855;&#22791;&#22810;&#26679;&#21270;&#30340;&#24863;&#30693;&#21644;&#35268;&#21010;&#33021;&#21147;&#65292;&#20197;&#35299;&#35835;&#20854;&#20182;&#20154;&#31867;&#30340;&#34892;&#20026;&#24182;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#20570;&#20986;&#30456;&#24212;&#30340;&#34892;&#21160;&#12290;&#20026;&#20102;&#20102;&#35299;&#36825;&#20010;&#38382;&#39064;&#22495;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#22312;RCE&#20013;&#20154;&#31867;&#30340;&#36335;&#24452;&#36873;&#25321;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#35774;&#35745;&#26426;&#22120;&#20154;&#23548;&#33322;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;(XAI)&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;COVID-19&#22823;&#27969;&#34892;&#36141;&#29289;&#22330;&#26223;&#65292;&#20854;&#20013;&#21253;&#21547;&#26102;&#38388;&#39118;&#38505;&#26435;&#34913;&#65292;&#33719;&#21462;&#20102;&#29992;&#25143;&#30340;&#36335;&#24452;&#20559;&#22909;&#12290;&#25105;&#20204;&#21457;&#29616;&#21442;&#19982;&#32773;&#23637;&#31034;&#20102;&#21508;&#31181;&#36335;&#24452;&#39318;&#36873;&#39033;&#65306;&#20174;&#20882;&#38505;&#19982;&#32039;&#24613;&#21040;&#23433;&#20840;&#19982;&#25918;&#26494;&#12290;&#20026;&#20102;&#23545;&#29992;&#25143;&#30340;&#20915;&#31574;&#36827;&#34892;&#24314;&#27169;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19977;&#31181;&#27969;&#34892;&#30340;&#39118;&#38505;&#27169;&#22411;(&#32047;&#31215;&#21069;&#26223;&#29702;&#35770;(CPT),&#26465;&#20214;&#20215;&#20540;&#39118;&#38505;(CVaR)&#21644;&#26399;&#26395;&#39118;&#38505;(ER))&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;CPT&#27604;CVaR&#21644;ER&#26356;&#20934;&#30830;&#22320;&#25429;&#25417;&#21040;&#20102;&#20154;&#20204;&#30340;&#20915;&#31574;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Risky and crowded environments (RCE) contain abstract sources of risk and uncertainty, which are perceived differently by humans, leading to a variety of behaviors. Thus, robots deployed in RCEs, need to exhibit diverse perception and planning capabilities in order to interpret other human agents' behavior and act accordingly in such environments. To understand this problem domain, we conducted a study to explore human path choices in RCEs, enabling better robotic navigational explainable AI (XAI) designs. We created a novel COVID-19 pandemic grocery shopping scenario which had time-risk tradeoffs, and acquired users' path preferences. We found that participants showcase a variety of path preferences: from risky and urgent to safe and relaxed. To model users' decision making, we evaluated three popular risk models (Cumulative Prospect Theory (CPT), Conditional Value at Risk (CVAR), and Expected Risk (ER). We found that CPT captured people's decision making more accurately than CVaR and
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#20110;&#20195;&#29702;&#26377;&#30452;&#25509;&#25511;&#21046;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#25910;&#38598;&#20449;&#24687;&#30340;&#33021;&#21147;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#34892;&#21160;&#21518;&#27979;&#37327; (ATM) &#31574;&#30053;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110; ATM &#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22810;&#20010;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.08271</link><description>&lt;p&gt;
Act-Then-Measure: &#24102;&#20027;&#21160;&#27979;&#37327;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Act-Then-Measure: Reinforcement Learning for Partially Observable Environments with Active Measuring. (arXiv:2303.08271v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#20110;&#20195;&#29702;&#26377;&#30452;&#25509;&#25511;&#21046;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#25910;&#38598;&#20449;&#24687;&#30340;&#33021;&#21147;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#34892;&#21160;&#21518;&#27979;&#37327; (ATM) &#31574;&#30053;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110; ATM &#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22810;&#20010;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243; (MDPs)&#65292;&#20854;&#20013;&#20195;&#29702;&#26377;&#30452;&#25509;&#25511;&#21046;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#25910;&#38598;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#22914; action-contingent noiselessly observable MDPs (ACNO-MPDs) &#25152;&#24418;&#24335;&#21270;&#12290;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#65292;&#21160;&#20316;&#30001;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#24433;&#21709;&#29615;&#22659;&#30340;&#25511;&#21046;&#21160;&#20316;&#21644;&#24433;&#21709;&#20195;&#29702;&#21487;&#20197;&#35266;&#23519;&#21040;&#20160;&#20040;&#30340;&#27979;&#37327;&#21160;&#20316;&#12290;&#20026;&#20102;&#35299;&#20915; ACNO-MDPs&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#34892;&#21160;&#21518;&#27979;&#37327; (ATM) &#31574;&#30053;&#65292;&#23427;&#20551;&#35774;&#22312;&#36873;&#25321;&#25511;&#21046;&#21160;&#20316;&#26102;&#21487;&#20197;&#24573;&#30053;&#26410;&#26469;&#29366;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36981;&#24490;&#27492;&#31574;&#30053;&#21487;&#33021;&#23548;&#33268;&#36739;&#30701;&#30340;&#31574;&#30053;&#35745;&#31639;&#26102;&#38388;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#21551;&#21457;&#24335;&#26041;&#27861;&#24341;&#36215;&#30340;&#24615;&#33021;&#20007;&#22833;&#30340;&#30028;&#38480;&#12290;&#20026;&#20102;&#30830;&#23450;&#26159;&#21542;&#37319;&#21462;&#27979;&#37327;&#34892;&#21160;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#27979;&#37327;&#20215;&#20540;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#22522;&#20110; ATM &#21551;&#21457;&#24335;&#26041;&#27861;&#24320;&#21457;&#20102;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#38024;&#23545;&#37096;&#20998;&#21487;&#35266;&#23519;&#22495;&#30340; Dyna-Q &#21464;&#20307;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#22810;&#20010;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study Markov decision processes (MDPs), where agents have direct control over when and how they gather information, as formalized by action-contingent noiselessly observable MDPs (ACNO-MPDs). In these models, actions consist of two components: a control action that affects the environment, and a measurement action that affects what the agent can observe. To solve ACNO-MDPs, we introduce the act-then-measure (ATM) heuristic, which assumes that we can ignore future state uncertainty when choosing control actions. We show how following this heuristic may lead to shorter policy computation times and prove a bound on the performance loss incurred by the heuristic. To decide whether or not to take a measurement action, we introduce the concept of measuring value. We develop a reinforcement learning algorithm based on the ATM heuristic, using a Dyna-Q variant adapted for partially observable domains, and showcase its superior performance compared to prior methods on a number of partially-o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#31038;&#20132;&#35268;&#21017;&#36716;&#25442;&#20026;&#19968;&#38454;&#36923;&#36753;&#65292;&#24182;&#20351;&#29992;&#31070;&#32463;&#31526;&#21495;&#23450;&#29702;&#35777;&#26126;&#22120;&#36827;&#34892;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#23637;&#29616;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#21487;&#20197;&#29992;&#26469;&#36827;&#34892;&#23545;&#31038;&#20132;&#24120;&#35782;&#38382;&#39064;&#30340;&#26126;&#30830;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2303.08264</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#24120;&#35782;&#31038;&#20250;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Neuro-symbolic Commonsense Social Reasoning. (arXiv:2303.08264v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#31038;&#20132;&#35268;&#21017;&#36716;&#25442;&#20026;&#19968;&#38454;&#36923;&#36753;&#65292;&#24182;&#20351;&#29992;&#31070;&#32463;&#31526;&#21495;&#23450;&#29702;&#35777;&#26126;&#22120;&#36827;&#34892;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#23637;&#29616;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#21487;&#20197;&#29992;&#26469;&#36827;&#34892;&#23545;&#31038;&#20132;&#24120;&#35782;&#38382;&#39064;&#30340;&#26126;&#30830;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#35268;&#33539;&#26159;&#25152;&#26377;&#20154;&#31867;&#31038;&#20132;&#20114;&#21160;&#30340;&#22522;&#30784;&#65292;&#20294;&#23558;&#20854;&#24418;&#24335;&#21270;&#24182;&#36827;&#34892;&#25512;&#29702;&#20173;&#28982;&#26159;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#20174;Social Chemistry 101&#25968;&#25454;&#38598;&#20013;&#20197;&#33258;&#28982;&#35821;&#35328;&#30340;&#24418;&#24335;&#33719;&#21462;&#31038;&#20132;&#35268;&#21017;&#65292;&#24182;&#23558;&#23427;&#20204;&#36716;&#25442;&#20026;&#19968;&#38454;&#36923;&#36753;&#65292;&#20351;&#29992;&#31070;&#32463;&#31526;&#21495;&#23450;&#29702;&#35777;&#26126;&#22120;&#36827;&#34892;&#25512;&#29702;&#12290;&#25105;&#20204;&#36890;&#36807;&#20960;&#20010;&#27493;&#39588;&#23454;&#29616;&#20102;&#36825;&#19968;&#30446;&#26631;&#12290;&#39318;&#20808;&#65292;&#23558;&#31038;&#20132;&#35268;&#21017;&#36716;&#25442;&#20026;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;(AMR)&#65292;&#21363;&#19968;&#21477;&#35805;&#20013;&#27010;&#24565;&#30340;&#22270;&#24418;&#34920;&#31034;&#65292;&#24182;&#19982;RoBERTa&#23884;&#20837;&#23545;&#40784;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#29983;&#25104;AMR&#30340;&#22791;&#29992;&#31616;&#21270;&#29256;&#26412;&#65292;&#37325;&#26032;&#32452;&#21512;&#21644;&#21512;&#24182;&#23884;&#20837;&#20197;&#22686;&#24378;&#23545;&#25991;&#26412;&#19981;&#21516;&#25514;&#36766;&#21644;&#19981;&#27491;&#30830;&#30340;AMR&#35299;&#26512;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#21518;&#23558;AMR&#36716;&#25442;&#20026;&#19968;&#38454;&#36923;&#36753;&#65292;&#24182;&#20351;&#29992;&#31070;&#32463;&#31526;&#21495;&#23450;&#29702;&#35777;&#26126;&#22120;&#36827;&#34892;&#26597;&#35810;&#12290;&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#21644;&#35780;&#20272;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#23545;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#24120;&#35782;&#35268;&#21017;&#36827;&#34892;&#26126;&#30830;&#30340;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24357;&#21512;&#20102;&#31526;&#21495;&#21644;&#31070;&#32463;&#26041;&#27861;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#24046;&#36317;&#65292;&#22312;&#19968;&#32452;&#31038;&#20132;&#24120;&#35782;&#38382;&#39064;&#19978;&#23637;&#29616;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social norms underlie all human social interactions, yet formalizing and reasoning with them remains a major challenge for AI systems. We present a novel system for taking social rules of thumb (ROTs) in natural language from the Social Chemistry 101 dataset and converting them to first-order logic where reasoning is performed using a neuro-symbolic theorem prover. We accomplish this in several steps. First, ROTs are converted into Abstract Meaning Representation (AMR), which is a graphical representation of the concepts in a sentence, and align the AMR with RoBERTa embeddings. We then generate alternate simplified versions of the AMR via a novel algorithm, recombining and merging embeddings for added robustness against different wordings of text, and incorrect AMR parses. The AMR is then converted into first-order logic, and is queried with a neuro-symbolic theorem prover. The goal of this paper is to develop and evaluate a neuro-symbolic method which performs explicit reasoning about
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;Transformer&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#24320;&#21457;&#20102;NLP&#31995;&#32479;&#65292;&#21487;&#22312;&#20020;&#24202;&#31508;&#35760;&#20013;&#25552;&#21462;&#33647;&#29289;&#21450;&#20854;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#22312;&#33647;&#29289;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.08259</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#22312;&#35821;&#22659;&#21270;&#33647;&#29289;&#20449;&#24687;&#25552;&#21462;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Contextualized Medication Information Extraction Using Transformer-based Deep Learning Architectures. (arXiv:2303.08259v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;Transformer&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#24320;&#21457;&#20102;NLP&#31995;&#32479;&#65292;&#21487;&#22312;&#20020;&#24202;&#31508;&#35760;&#20013;&#25552;&#21462;&#33647;&#29289;&#21450;&#20854;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#22312;&#33647;&#29289;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#24320;&#21457;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#31995;&#32479;&#65292;&#25552;&#21462;&#33647;&#29289;&#21450;&#26377;&#21161;&#20110;&#29702;&#35299;&#33647;&#29289;&#21464;&#21270;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#26041;&#27861;&#65306;&#24320;&#21457;&#20102;&#19977;&#20010;NLP&#31995;&#32479;&#65292;&#21253;&#25324;&#33647;&#29289;&#25552;&#21450;&#25552;&#21462;&#12289;&#20107;&#20214;&#20998;&#31867;(&#25351;&#33647;&#29289;&#21464;&#21270;&#30340;&#35752;&#35770;&#25110;&#26410;&#35752;&#35770;)&#12289;&#20197;&#21450;&#20998;&#31867;&#33647;&#29289;&#21464;&#21270;&#19978;&#19979;&#25991;&#21040;5&#20010;&#19982;&#33647;&#29289;&#21464;&#21270;&#30456;&#20851;&#30340;&#27491;&#20132;&#32500;&#24230;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;6&#20010;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#21253;&#25324;GatorTron&#65292;&#23427;&#26159;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#39044;&#35757;&#32451;&#20351;&#29992;&#36229;&#36807;90&#20159;&#20010;&#21333;&#35789;&#30340;&#25991;&#26412;(&#21253;&#25324;&#26469;&#33258;&#20315;&#32599;&#37324;&#36798;&#22823;&#23398;&#20581;&#24247;&#20013;&#24515;&#35782;&#21035;&#30340;2.9&#20159;&#22810;&#20010;&#20020;&#24202;&#31508;&#35760;&#20013;&#30340;&#36229;&#36807;80&#20159;&#20010;&#21333;&#35789;)&#12290;&#25105;&#20204;&#20351;&#29992;2022 n2c2&#30340;&#27880;&#37322;&#25968;&#25454;&#21644;&#35780;&#20272;&#33050;&#26412;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;NLP&#31995;&#32479;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#30340;GatorTron&#27169;&#22411;&#22312;&#33647;&#29289;&#25552;&#21462;&#12289;&#20107;&#20214;&#20998;&#31867;&#21644;&#19978;&#19979;&#25991;&#20998;&#31867;&#26041;&#38754;&#20998;&#21035;&#21462;&#24471;&#20102;&#26368;&#20339;&#30340; F1 &#20998;&#25968;&#65292;&#20998;&#21035;&#20026; 0.9828&#65288;&#25490;&#21517;&#31532;3&#65289;&#12289;0.9379&#65288;&#25490;&#21517;&#31532;1&#65289;&#12289;0.8375&#65288;&#25490;&#21517;&#31532;1&#65289;&#65292;&#36229;&#36807;&#20854;&#20182;5&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#26368;&#20339;&#30340;NLP&#31995;&#32479;&#22312;18&#20010;&#21442;&#36187;&#22242;&#38431;&#20013;&#25490;&#21517;&#31532;&#20108;&#65292;&#33719;&#24471;&#20102;&#24635;&#20307;F1&#24471;&#20998;0.8774 &#12290;&#32467;&#35770;&#65306;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#65292;&#22914;GatorTron&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#20020;&#24202;&#31508;&#35760;&#20013;&#25552;&#21462;&#33647;&#29289;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#22312;&#33647;&#29289;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: To develop a natural language processing (NLP) system to extract medications and contextual information that help understand drug changes. This project is part of the 2022 n2c2 challenge.  Materials and methods: We developed NLP systems for medication mention extraction, event classification (indicating medication changes discussed or not), and context classification to classify medication changes context into 5 orthogonal dimensions related to drug changes. We explored 6 state-of-the-art pretrained transformer models for the three subtasks, including GatorTron, a large language model pretrained using &gt;90 billion words of text (including &gt;80 billion words from &gt;290 million clinical notes identified at the University of Florida Health). We evaluated our NLP systems using annotated data and evaluation scripts provided by the 2022 n2c2 organizers.  Results:Our GatorTron models achieved the best F1-scores of 0.9828 for medication extraction (ranked 3rd), 0.9379 for event classif
&lt;/p&gt;</description></item><item><title>NL4Opt&#27604;&#36187;&#26088;&#22312;&#30740;&#31350;&#22914;&#20309;&#20174;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20013;&#25552;&#21462;&#20986;&#20248;&#21270;&#38382;&#39064;&#30340;&#21547;&#20041;&#21644;&#34920;&#36848;&#65292;&#24182;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#19982;&#38750;&#19987;&#19994;&#20154;&#22763;&#36827;&#34892;&#20132;&#20114;&#12290;&#31454;&#36187;&#20998;&#20026;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;(1) &#35782;&#21035;&#21644;&#26631;&#35760;&#23545;&#24212;&#20110;&#20248;&#21270;&#38382;&#39064;&#32452;&#20214;&#30340;&#35821;&#20041;&#23454;&#20307;;(2)&#20174;&#26816;&#27979;&#21040;&#30340;&#38382;&#39064;&#23454;&#20307;&#29983;&#25104;&#24847;&#20041;&#34920;&#31034;(&#21363;&#36923;&#36753;&#24418;&#24335;)&#12290;</title><link>http://arxiv.org/abs/2303.08233</link><description>&lt;p&gt;
NL4Opt &#27604;&#36187;&#65306;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26500;&#24314;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
NL4Opt Competition: Formulating Optimization Problems Based on Their Natural Language Descriptions. (arXiv:2303.08233v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08233
&lt;/p&gt;
&lt;p&gt;
NL4Opt&#27604;&#36187;&#26088;&#22312;&#30740;&#31350;&#22914;&#20309;&#20174;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20013;&#25552;&#21462;&#20986;&#20248;&#21270;&#38382;&#39064;&#30340;&#21547;&#20041;&#21644;&#34920;&#36848;&#65292;&#24182;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#19982;&#38750;&#19987;&#19994;&#20154;&#22763;&#36827;&#34892;&#20132;&#20114;&#12290;&#31454;&#36187;&#20998;&#20026;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;(1) &#35782;&#21035;&#21644;&#26631;&#35760;&#23545;&#24212;&#20110;&#20248;&#21270;&#38382;&#39064;&#32452;&#20214;&#30340;&#35821;&#20041;&#23454;&#20307;;(2)&#20174;&#26816;&#27979;&#21040;&#30340;&#38382;&#39064;&#23454;&#20307;&#29983;&#25104;&#24847;&#20041;&#34920;&#31034;(&#21363;&#36923;&#36753;&#24418;&#24335;)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#20248;&#21270;&#65288;NL4Opt&#65289;&#31454;&#36187;&#26088;&#22312;&#30740;&#31350;&#22914;&#20309;&#26681;&#25454;&#20248;&#21270;&#38382;&#39064;&#30340;&#25991;&#26412;&#25551;&#36848;&#25552;&#21462;&#20854;&#21547;&#20041;&#21644;&#34920;&#36848;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#31454;&#36187;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20013;&#20171;&#26469;&#20351;&#38750;&#19987;&#19994;&#20154;&#22763;&#33021;&#22815;&#25509;&#21475;&#20351;&#29992;&#20248;&#21270;&#27714;&#35299;&#22120;&#65292;&#20197;&#22686;&#21152;&#20854;&#21487;&#35775;&#38382;&#24615;&#21644;&#21487;&#29992;&#24615;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#25361;&#25112;&#24615;&#30446;&#26631;&#20998;&#20026;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;(1)&#35782;&#21035;&#21644;&#26631;&#35760;&#23545;&#24212;&#20110;&#20248;&#21270;&#38382;&#39064;&#32452;&#20214;&#30340;&#35821;&#20041;&#23454;&#20307;;(2)&#20174;&#26816;&#27979;&#21040;&#30340;&#38382;&#39064;&#23454;&#20307;&#29983;&#25104;&#24847;&#20041;&#34920;&#31034;(&#21363;&#36923;&#36753;&#24418;&#24335;)&#12290;&#31532;&#19968;&#20010;&#20219;&#21153;&#26088;&#22312;&#36890;&#36807;&#26816;&#27979;&#21644;&#26631;&#35760;&#20248;&#21270;&#38382;&#39064;&#30340;&#23454;&#20307;&#26469;&#20943;&#23569;&#27495;&#20041;&#12290;&#31532;&#20108;&#20010;&#20219;&#21153;&#21019;&#24314;&#20102;&#19968;&#20010;&#32447;&#24615;&#35268;&#21010;(LP)&#38382;&#39064;&#30340;&#20013;&#38388;&#34920;&#31034;&#65292;&#35813;&#20013;&#38388;&#34920;&#31034;&#34987;&#36716;&#25442;&#20026;&#21830;&#29992;&#27714;&#35299;&#22120;&#21487;&#29992;&#30340;&#26684;&#24335;&#12290;&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;LP&#21333;&#35789;&#38382;&#39064;&#25968;&#25454;&#38598;&#21644;NL4Opt&#27604;&#36187;&#30340;&#20849;&#20139;&#20219;&#21153;&#65292;&#24182;&#24635;&#32467;&#20102;&#31454;&#36187;&#26465;&#30446;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Natural Language for Optimization (NL4Opt) Competition was created to investigate methods of extracting the meaning and formulation of an optimization problem based on its text description. Specifically, the goal of the competition is to increase the accessibility and usability of optimization solvers by allowing non-experts to interface with them using natural language. We separate this challenging goal into two sub-tasks: (1) recognize and label the semantic entities that correspond to the components of the optimization problem; (2) generate a meaning representation (i.e., a logical form) of the problem from its detected problem entities. The first task aims to reduce ambiguity by detecting and tagging the entities of the optimization problems. The second task creates an intermediate representation of the linear programming (LP) problem that is converted into a format that can be used by commercial solvers. In this report, we present the LP word problem dataset and shared tasks f
&lt;/p&gt;</description></item><item><title>DeepAxe&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;DNN&#21152;&#36895;&#22120;&#30340;&#35774;&#35745;&#31354;&#38388;&#20013;&#32771;&#34385;&#36817;&#20284;&#21644;&#21487;&#38752;&#24615;&#26435;&#34913;&#30340;&#26694;&#26550;&#65292;&#36924;&#36817;&#21487;&#38752;&#24615;&#20851;&#38190;&#30340;DNN&#65292;&#24182;&#25552;&#20379;&#19968;&#32452;Pareto&#26368;&#20248;&#30340;DNN&#23454;&#29616;&#35774;&#35745;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2303.08226</link><description>&lt;p&gt;
DeepAxe: &#19968;&#31181;&#29992;&#20110;&#25506;&#32034;DNN&#21152;&#36895;&#22120;&#30340;&#36817;&#20284;&#21644;&#21487;&#38752;&#24615;&#26435;&#34913;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DeepAxe: A Framework for Exploration of Approximation and Reliability Trade-offs in DNN Accelerators. (arXiv:2303.08226v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08226
&lt;/p&gt;
&lt;p&gt;
DeepAxe&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;DNN&#21152;&#36895;&#22120;&#30340;&#35774;&#35745;&#31354;&#38388;&#20013;&#32771;&#34385;&#36817;&#20284;&#21644;&#21487;&#38752;&#24615;&#26435;&#34913;&#30340;&#26694;&#26550;&#65292;&#36924;&#36817;&#21487;&#38752;&#24615;&#20851;&#38190;&#30340;DNN&#65292;&#24182;&#25552;&#20379;&#19968;&#32452;Pareto&#26368;&#20248;&#30340;DNN&#23454;&#29616;&#35774;&#35745;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;Deep Neural Networks&#65288;DNNs&#65289;&#22312;&#24191;&#27867;&#30340;&#23433;&#20840;&#20851;&#38190;&#22411;&#24212;&#29992;&#20013;&#30340;&#20316;&#29992;&#27491;&#22312;&#25193;&#22823;&#65292;&#26032;&#20852;&#30340;DNN&#32463;&#21382;&#20102;&#35745;&#31639;&#33021;&#21147;&#26041;&#38754;&#30340;&#24040;&#22823;&#22686;&#38271;&#12290;&#36825;&#22686;&#21152;&#20102;&#25552;&#39640;DNN&#21152;&#36895;&#22120;&#21487;&#38752;&#24615;&#30340;&#24517;&#35201;&#24615;&#65292;&#21516;&#26102;&#38477;&#20302;&#30828;&#20214;&#24179;&#21488;&#19978;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#21363;&#38477;&#20302;&#33021;&#32791;&#21644;&#25191;&#34892;&#26102;&#38388;&#65292;&#25552;&#39640;DNN&#21152;&#36895;&#22120;&#30340;&#25928;&#29575;&#12290;&#22240;&#27492;&#65292;&#30828;&#20214;&#24615;&#33021;&#65288;&#21363;&#21306;&#22495;&#12289;&#21151;&#29575;&#21644;&#24310;&#36831;&#65289;&#19982;DNN&#21152;&#36895;&#22120;&#23454;&#29616;&#30340;&#21487;&#38752;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#38656;&#35201;&#24037;&#20855;&#36827;&#34892;&#20998;&#26512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;DeepAxe&#65292;&#29992;&#20110;&#22312;&#32771;&#34385;&#24212;&#29992;&#21151;&#33021;&#36817;&#20284;&#23545;&#20934;&#30830;&#24230;&#12289;&#21487;&#38752;&#24615;&#21644;&#30828;&#20214;&#24615;&#33021;&#30340;&#19977;&#26041;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#22522;&#20110;FPGA&#30340;DNN&#23454;&#29616;&#36827;&#34892;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#12290;&#35813;&#26694;&#26550;&#20351;&#24471;&#23545;&#20110;&#20851;&#38190;&#21487;&#38752;&#24615;&#30340;DNN&#36827;&#34892;&#26377;&#36873;&#25321;&#30340;&#36924;&#36817;&#65292;&#25552;&#20379;&#20102;&#19968;&#32452;Pareto&#26368;&#20248;&#30340;DNN&#23454;&#29616;&#35774;&#35745;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the role of Deep Neural Networks (DNNs) in a wide range of safety-critical applications is expanding, emerging DNNs experience massive growth in terms of computation power. It raises the necessity of improving the reliability of DNN accelerators yet reducing the computational burden on the hardware platforms, i.e. reducing the energy consumption and execution time as well as increasing the efficiency of DNN accelerators. Therefore, the trade-off between hardware performance, i.e. area, power and delay, and the reliability of the DNN accelerator implementation becomes critical and requires tools for analysis. In this paper, we propose a framework DeepAxe for design space exploration for FPGA-based implementation of DNNs by considering the trilateral impact of applying functional approximation on accuracy, reliability and hardware performance. The framework enables selective approximation of reliability-critical DNNs, providing a set of Pareto-optimal DNN implementation design spac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#21464;&#25442;&#22120;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36830;&#25509;&#33410;&#28857;&#21644;&#38750;&#36830;&#25509;&#33410;&#28857;&#30340;&#20840;&#23616;&#20851;&#31995;&#20197;&#21450;&#22522;&#20110;&#25151;&#23627;&#24067;&#23616;&#25299;&#25169;&#30340;&#26412;&#22320;&#39030;&#28857;&#20132;&#20114;&#65292;&#23454;&#29616;&#20102;&#22270;&#32422;&#26463;&#25151;&#23627;&#30340;&#31471;&#21040;&#31471;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2303.08225</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#21464;&#25442;&#22120;&#30340; GANs &#29992;&#20110;&#22270;&#32422;&#26463;&#25151;&#23627;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Graph Transformer GANs for Graph-Constrained House Generation. (arXiv:2303.08225v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#21464;&#25442;&#22120;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36830;&#25509;&#33410;&#28857;&#21644;&#38750;&#36830;&#25509;&#33410;&#28857;&#30340;&#20840;&#23616;&#20851;&#31995;&#20197;&#21450;&#22522;&#20110;&#25151;&#23627;&#24067;&#23616;&#25299;&#25169;&#30340;&#26412;&#22320;&#39030;&#28857;&#20132;&#20114;&#65292;&#23454;&#29616;&#20102;&#22270;&#32422;&#26463;&#25151;&#23627;&#30340;&#31471;&#21040;&#31471;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#21464;&#25442;&#22120;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476; (GTGAN)&#65292;&#20197;&#23398;&#20064;&#31471;&#21040;&#31471;&#30340;&#26377;&#25928;&#22270;&#33410;&#28857;&#20851;&#31995;&#65292;&#20197;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22270;&#32422;&#26463;&#25151;&#23627;&#29983;&#25104;&#20219;&#21153;&#12290;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;&#22270;&#21464;&#25442;&#22120;&#30340;&#29983;&#25104;&#22120;&#65292;&#21253;&#25324;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;&#65292;&#23427;&#32467;&#21512;&#20102; Transformer &#20013;&#30340;&#22270;&#21367;&#31215;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#23545;&#36830;&#25509;&#21644;&#38750;&#36830;&#25509;&#22270;&#33410;&#28857;&#38388;&#30340;&#26412;&#22320;&#21644;&#20840;&#23616;&#20132;&#20114;&#36827;&#34892;&#24314;&#27169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;&#36830;&#25509;&#33410;&#28857;&#27880;&#24847;&#21147; (CNA) &#21644;&#38750;&#36830;&#25509;&#33410;&#28857;&#27880;&#24847;&#21147; (NNA) &#26088;&#22312;&#25429;&#25417;&#36755;&#20837;&#22270;&#20013;&#36830;&#25509;&#33410;&#28857;&#21644;&#38750;&#36830;&#25509;&#33410;&#28857;&#20043;&#38388;&#30340;&#20840;&#23616;&#20851;&#31995;&#12290;&#25152;&#25552;&#20986;&#30340;&#22270;&#24314;&#27169;&#22359; (GMB) &#26088;&#22312;&#21033;&#29992;&#22522;&#20110;&#25151;&#23627;&#24067;&#23616;&#25299;&#25169;&#30340;&#26412;&#22320;&#39030;&#28857;&#20132;&#20114;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33410;&#28857;&#20998;&#31867;&#30340;&#26032;&#22411;&#37492;&#21035;&#22120;&#65292;&#20197;&#20445;&#30041;&#19981;&#21516;&#25151;&#23627;&#32452;&#20214;&#30340;&#39640;&#23618;&#35821;&#20041;&#21644;&#21028;&#21035;&#24615;&#33410;&#28857;&#29305;&#24449;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#30340;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#65292;&#36890;&#36807;&#24490;&#29615;&#29983;&#25104;&#21644;&#37325;&#26500;&#26469;&#22686;&#24378; GTGAN &#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel graph Transformer generative adversarial network (GTGAN) to learn effective graph node relations in an end-to-end fashion for the challenging graph-constrained house generation task. The proposed graph-Transformer-based generator includes a novel graph Transformer encoder that combines graph convolutions and self-attentions in a Transformer to model both local and global interactions across connected and non-connected graph nodes. Specifically, the proposed connected node attention (CNA) and non-connected node attention (NNA) aim to capture the global relations across connected nodes and non-connected nodes in the input graph, respectively. The proposed graph modeling block (GMB) aims to exploit local vertex interactions based on a house layout topology. Moreover, we propose a new node classification-based discriminator to preserve the high-level semantic and discriminative node features for different house components. Finally, we propose a novel graph-based cycle-co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#30340;&#22330;&#22320;&#19981;&#21487;&#30693;&#20803;&#23398;&#20064;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#65292;&#20351;&#29992;&#22312;&#22810;&#20010;&#31449;&#28857;&#30340;MRI&#25968;&#25454;&#35757;&#32451;&#20986;&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#30149;&#20154;&#21644;&#27491;&#24120;&#20154;&#30340;&#24555;&#36895;&#35782;&#21035;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2303.08224</link><description>&lt;p&gt;
&#20351;&#29992;&#22330;&#22320;&#19981;&#21487;&#30693;&#20803;&#23398;&#20064;&#21644;&#33041;&#37096;MRI&#36827;&#34892;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#30340;&#23569;&#26679;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Classification of Autism Spectrum Disorder using Site-Agnostic Meta-Learning and Brain MRI. (arXiv:2303.08224v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#30340;&#22330;&#22320;&#19981;&#21487;&#30693;&#20803;&#23398;&#20064;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#65292;&#20351;&#29992;&#22312;&#22810;&#20010;&#31449;&#28857;&#30340;MRI&#25968;&#25454;&#35757;&#32451;&#20986;&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#30149;&#20154;&#21644;&#27491;&#24120;&#20154;&#30340;&#24555;&#36895;&#35782;&#21035;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#21487;&#29992;&#24615;&#26377;&#38480;&#65292;&#35774;&#35745;&#38024;&#23545;&#32454;&#24494;&#30149;&#24773;&#65288;&#22914;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#65289;&#30340;&#25918;&#23556;&#23398;&#20998;&#31867;&#22120;&#21463;&#21040;&#38459;&#30861;&#12290;&#36801;&#31227;&#23398;&#20064;&#26159;&#35299;&#20915;&#20302;&#35757;&#32451;&#25968;&#25454;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#26412;&#25991;&#25506;&#35752;&#22312;&#20855;&#26377;&#22810;&#20010;&#31449;&#28857;&#30340;&#20808;&#21069;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#20803;&#23398;&#20064;&#30340;&#20351;&#29992;&#65292;&#21363;&#25105;&#20204;&#31216;&#20043;&#20026;&#22330;&#22320;&#19981;&#21487;&#30693;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38750;&#24120;&#20302;&#25968;&#25454;&#21046;&#24230;&#30340;&#24773;&#20917;&#12290;&#21463;&#21040;&#20803;&#23398;&#20064;&#22312;&#20248;&#21270;&#27169;&#22411;&#36328;&#22810;&#20010;&#20219;&#21153;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20351;&#20854;&#36866;&#24212;&#20110;&#19981;&#21516;&#31449;&#28857;&#20043;&#38388;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;38&#20010;&#24433;&#20687;&#31449;&#28857;&#30340;2,201&#20010;T1&#21152;&#26435;&#65288;T1-w&#65289;MRI&#25195;&#25551;&#20013;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#36825;&#20123;&#25968;&#25454;&#26159;&#20316;&#20026;&#33258;&#38381;&#30151;&#33041;&#37096;&#24433;&#20687;&#25968;&#25454;&#20132;&#25442;&#65288;ABIDE&#65289;&#30340;&#19968;&#37096;&#20998;&#25910;&#38598;&#30340;&#65288;&#24180;&#40836;&#65306;5.2-64.0&#23681;&#65289;&#12290;&#35813;&#26041;&#27861;&#34987;&#35757;&#32451;&#20026;&#26597;&#25214;&#19968;&#20010;&#33391;&#22909;&#30340;&#21021;&#22987;&#29366;&#24577;&#65292;&#20197;&#20415;&#24555;&#36895;&#36866;&#24212;&#26032;&#30340;&#26410;&#35265;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
For machine learning applications in medical imaging, the availability of training data is often limited, which hampers the design of radiological classifiers for subtle conditions such as autism spectrum disorder (ASD). Transfer learning is one method to counter this problem of low training data regimes. Here we explore the use of meta-learning for very low data regimes in the context of having prior data from multiple sites - an approach we term site-agnostic meta-learning. Inspired by the effectiveness of meta-learning for optimizing a model across multiple tasks, here we propose a framework to adapt it to learn across multiple sites. We tested our meta-learning model for classifying ASD versus typically developing controls in 2,201 T1-weighted (T1-w) MRI scans collected from 38 imaging sites as part of Autism Brain Imaging Data Exchange (ABIDE) [age: 5.2-64.0 years]. The method was trained to find a good initialization state for our model that can quickly adapt to data from new uns
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23581;&#35797;&#20351;&#29992;Vision Transformers&#23545;&#22522;&#20110;MRI&#25195;&#25551;&#30340;&#24615;&#21035;&#21644;AD&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#20854;&#20013;&#20004;&#31181;ViT&#26550;&#26500;&#21464;&#20307;&#20998;&#21035;&#23454;&#29616;&#20102;0.987&#30340;&#24615;&#21035;&#20998;&#31867;AUC&#21644;0.892&#30340;AD&#20998;&#31867;AUC&#12290;&#35813;&#26041;&#27861;&#21487;&#20026;&#22823;&#35268;&#27169;&#31070;&#32463;&#24433;&#20687;&#23398;&#35782;&#21035;&#25552;&#20379;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.08216</link><description>&lt;p&gt;
&#29992;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;&#30340;&#32467;&#26500;&#24615;MRI&#25195;&#25551;&#30340;Vision Transformers&#39640;&#25928;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Efficiently Training Vision Transformers on Structural MRI Scans for Alzheimer's Disease Detection. (arXiv:2303.08216v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23581;&#35797;&#20351;&#29992;Vision Transformers&#23545;&#22522;&#20110;MRI&#25195;&#25551;&#30340;&#24615;&#21035;&#21644;AD&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#20854;&#20013;&#20004;&#31181;ViT&#26550;&#26500;&#21464;&#20307;&#20998;&#21035;&#23454;&#29616;&#20102;0.987&#30340;&#24615;&#21035;&#20998;&#31867;AUC&#21644;0.892&#30340;AD&#20998;&#31867;AUC&#12290;&#35813;&#26041;&#27861;&#21487;&#20026;&#22823;&#35268;&#27169;&#31070;&#32463;&#24433;&#20687;&#23398;&#35782;&#21035;&#25552;&#20379;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#20154;&#32676;&#31070;&#32463;&#24433;&#20687;&#23398;&#23545;&#20110;&#35782;&#21035;&#20419;&#36827;&#25110;&#25269;&#25239;&#33041;&#30142;&#30149;&#30340;&#22240;&#32032;&#20197;&#21450;&#21327;&#21161;&#35786;&#26029;&#12289;&#20122;&#22411;&#20998;&#31867;&#21644;&#39044;&#21518;&#37117;&#20855;&#26377;&#20215;&#20540;&#12290;&#26412;&#25991;&#23581;&#35797;&#20351;&#29992;Vision Transformers(ViT)&#23545;&#22522;&#20110;&#38590;&#24230;&#35843;&#25972;&#30340;&#19968;&#31995;&#21015;&#31070;&#32463;&#24433;&#20687;&#23398;&#20219;&#21153;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21253;&#25324;&#24615;&#21035;&#21644;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;(AD)&#22522;&#20110;3D&#22823;&#33041;MRI&#30340;&#20998;&#31867;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#20004;&#31181;ViT&#26550;&#26500;&#21464;&#20307;&#20998;&#21035;&#23454;&#29616;&#20102;0.987&#30340;&#24615;&#21035;&#20998;&#31867;AUC&#21644;0.892&#30340;AD&#20998;&#31867;AUC&#12290;&#25105;&#20204;&#29420;&#31435;&#35780;&#20272;&#20102;&#27169;&#22411;&#22312;&#20004;&#20010;&#22522;&#20934;AD&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#33719;&#24471;&#20102;5%&#21644;9-10%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neuroimaging of large populations is valuable to identify factors that promote or resist brain disease, and to assist diagnosis, subtyping, and prognosis. Data-driven models such as convolutional neural networks (CNNs) have increasingly been applied to brain images to perform diagnostic and prognostic tasks by learning robust features. Vision transformers (ViT) - a new class of deep learning architectures - have emerged in recent years as an alternative to CNNs for several computer vision applications. Here we tested variants of the ViT architecture for a range of desired neuroimaging downstream tasks based on difficulty, in this case for sex and Alzheimer's disease (AD) classification based on 3D brain MRI. In our experiments, two vision transformer architecture variants achieved an AUC of 0.987 for sex and 0.892 for AD classification, respectively. We independently evaluated our models on data from two benchmark AD datasets. We achieved a performance boost of 5% and 9-10% upon fine-t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#23545;&#20110;&#25345;&#32493;&#23398;&#20064;&#20219;&#21153;&#26469;&#35828;&#65292;&#36951;&#24536;&#19981;&#26159;&#19968;&#31181;&#33391;&#22909;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#27809;&#26377;&#32771;&#34385;&#21040;&#21069;&#21521;&#36801;&#31227;&#30340;&#37327;&#24230;&#26041;&#24335;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#24230;&#26041;&#24335;&#65292;&#21457;&#29616;&#36739;&#19981;&#36951;&#24536;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.08207</link><description>&lt;p&gt;
&#36951;&#24536;&#26159;&#21542;&#26159;&#21069;&#21521;&#36801;&#31227;&#30340;&#33391;&#22909;&#24402;&#32435;&#20559;&#24046;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is forgetting less a good inductive bias for forward transfer?. (arXiv:2303.08207v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#23545;&#20110;&#25345;&#32493;&#23398;&#20064;&#20219;&#21153;&#26469;&#35828;&#65292;&#36951;&#24536;&#19981;&#26159;&#19968;&#31181;&#33391;&#22909;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#27809;&#26377;&#32771;&#34385;&#21040;&#21069;&#21521;&#36801;&#31227;&#30340;&#37327;&#24230;&#26041;&#24335;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#24230;&#26041;&#24335;&#65292;&#21457;&#29616;&#36739;&#19981;&#36951;&#24536;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#30340;&#20027;&#35201;&#21160;&#26426;&#20043;&#19968;&#26159;&#65292;&#35813;&#38382;&#39064;&#35774;&#32622;&#20801;&#35768;&#27169;&#22411;&#20174;&#36807;&#21435;&#30340;&#20219;&#21153;&#20013;&#31215;&#32047;&#30693;&#35782;&#20197;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#25152;&#20248;&#21270;&#30340;&#20851;&#38190;&#25351;&#26631;&#65292;&#21363;&#20943;&#23569;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#19981;&#19982;&#21069;&#21521;&#30693;&#35782;&#36801;&#31227;&#30456;&#20851;&#12290;&#25105;&#20204;&#35748;&#20026;&#20043;&#21069;&#30340;&#30740;&#31350;&#32467;&#35770;&#26159;&#30001;&#20110;&#20182;&#20204;&#34913;&#37327;&#21069;&#21521;&#36801;&#31227;&#30340;&#26041;&#24335;&#25152;&#33268;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#34913;&#37327;&#19968;&#20010;&#20219;&#21153;&#30340;&#21069;&#21521;&#36801;&#31227;&#19981;&#24212;&#21463;&#21040;&#20026;&#20445;&#30041;&#20808;&#21069;&#20219;&#21153;&#30693;&#35782;&#32780;&#23545;&#25345;&#32493;&#23398;&#20064;&#22120;&#26045;&#21152;&#30340;&#38480;&#21046;&#30340;&#24433;&#21709;&#12290;&#30456;&#21453;&#65292;&#21069;&#21521;&#36801;&#31227;&#24212;&#35813;&#36890;&#36807;&#25345;&#32493;&#23398;&#20064;&#20135;&#29983;&#30340;&#19968;&#32452;&#34920;&#31034;&#26469;&#35780;&#20272;&#32473;&#23450;&#19968;&#20010;&#26032;&#20219;&#21153;&#26377;&#22810;&#23481;&#26131;&#23398;&#20064;&#12290;&#22312;&#36825;&#31181;&#21069;&#21521;&#36801;&#31227;&#27010;&#24565;&#19979;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#22312;&#21508;&#31181;&#22270;&#20687;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36739;&#19981;&#36951;&#24536;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#24403;&#35757;&#32451;&#25968;&#25454;&#25968;&#37327;&#23569;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the main motivations of studying continual learning is that the problem setting allows a model to accrue knowledge from past tasks to learn new tasks more efficiently. However, recent studies suggest that the key metric that continual learning algorithms optimize, reduction in catastrophic forgetting, does not correlate well with the forward transfer of knowledge. We believe that the conclusion previous works reached is due to the way they measure forward transfer. We argue that the measure of forward transfer to a task should not be affected by the restrictions placed on the continual learner in order to preserve knowledge of previous tasks. Instead, forward transfer should be measured by how easy it is to learn a new task given a set of representations produced by continual learning on previous tasks. Under this notion of forward transfer, we evaluate different continual learning algorithms on a variety of image classification benchmarks. Our results indicate that less forgetf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#39640;&#25928;&#22320;&#25429;&#25417;&#35270;&#35273;&#33402;&#26415;&#30340;&#20803;&#32032;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#23398;&#20064;&#25216;&#26415;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#33402;&#26415;&#21697;&#25512;&#33616;&#65292;&#32467;&#26524;&#26174;&#31034;&#20004;&#32773;&#30340;&#32467;&#21512;&#21487;&#20197;&#25429;&#25417;&#26368;&#21512;&#36866;&#30340;&#38544;&#34255;&#35821;&#20041;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2303.08182</link><description>&lt;p&gt;
&#35270;&#35273;&#33402;&#26415;&#25512;&#33616;&#30340;&#35201;&#32032;&#65306;&#23398;&#20064;&#30011;&#20316;&#30340;&#28508;&#22312;&#35821;&#20041;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
The Elements of Visual Art Recommendation: Learning Latent Semantic Representations of Paintings. (arXiv:2303.08182v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#39640;&#25928;&#22320;&#25429;&#25417;&#35270;&#35273;&#33402;&#26415;&#30340;&#20803;&#32032;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#23398;&#20064;&#25216;&#26415;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#33402;&#26415;&#21697;&#25512;&#33616;&#65292;&#32467;&#26524;&#26174;&#31034;&#20004;&#32773;&#30340;&#32467;&#21512;&#21487;&#20197;&#25429;&#25417;&#26368;&#21512;&#36866;&#30340;&#38544;&#34255;&#35821;&#20041;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33402;&#26415;&#21697;&#25512;&#33616;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#29702;&#35299;&#29992;&#25143;&#22914;&#20309;&#19982;&#39640;&#24230;&#20027;&#35266;&#30340;&#20869;&#23481;&#20114;&#21160;&#65292;&#33402;&#26415;&#21697;&#20013;&#23884;&#20837;&#30340;&#27010;&#24565;&#30340;&#22797;&#26434;&#24615;&#65292;&#20197;&#21450;&#23427;&#20204;&#21487;&#33021;&#24341;&#36215;&#29992;&#25143;&#30340;&#24773;&#24863;&#21644;&#35748;&#30693;&#21453;&#24212;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#22914;&#20309;&#39640;&#25928;&#22320;&#25429;&#25417;&#35270;&#35273;&#33402;&#26415;&#30340;&#20803;&#32032;&#65288;&#21363;&#28508;&#22312;&#35821;&#20041;&#20851;&#31995;&#65289;&#65292;&#20197;&#36827;&#34892;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#22522;&#20110;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#23398;&#20064;&#25216;&#26415;&#20197;&#21450;&#23427;&#20204;&#30340;&#32452;&#21512;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;&#25105;&#20204;&#23545;&#25512;&#33616;&#36136;&#37327;&#36827;&#34892;&#20102;&#23567;&#35268;&#27169;&#21644;&#22823;&#35268;&#27169;&#30340;&#29992;&#25143;&#20013;&#24515;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25991;&#26412;&#29305;&#24449;&#27604;&#35270;&#35273;&#29305;&#24449;&#34920;&#29616;&#26356;&#22909;&#65292;&#32780;&#20004;&#32773;&#30340;&#32467;&#21512;&#21487;&#20197;&#25429;&#25417;&#33402;&#26415;&#21697;&#25512;&#33616;&#26368;&#21512;&#36866;&#30340;&#38544;&#34255;&#35821;&#20041;&#20851;&#31995;&#12290;&#26368;&#32456;&#65292;&#26412;&#25991;&#26377;&#21161;&#20110;&#29702;&#35299;&#22914;&#20309;&#25552;&#20379;&#36866;&#21512;&#29992;&#25143;&#20852;&#36259;&#21644;&#24863;&#30693;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artwork recommendation is challenging because it requires understanding how users interact with highly subjective content, the complexity of the concepts embedded within the artwork, and the emotional and cognitive reflections they may trigger in users. In this paper, we focus on efficiently capturing the elements (i.e., latent semantic relationships) of visual art for personalized recommendation. We propose and study recommender systems based on textual and visual feature learning techniques, as well as their combinations. We then perform a small-scale and a large-scale user-centric evaluation of the quality of the recommendations. Our results indicate that textual features compare favourably with visual ones, whereas a fusion of both captures the most suitable hidden semantic relationships for artwork recommendation. Ultimately, this paper contributes to our understanding of how to deliver content that suitably matches the user's interests and how they are perceived.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;medBERT.de&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#24503;&#35821;&#21307;&#23398;&#39046;&#22495;&#30340;BERT&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#30340;&#35757;&#32451;&#65292;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#21307;&#23398;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;&#35813;&#27169;&#22411;&#23545;&#38271;&#25991;&#26412;&#29305;&#21035;&#26377;&#29992;&#65292;&#32780;&#25968;&#25454;&#21435;&#37325;&#21644;&#26377;&#25928;&#30340;&#20998;&#35789;&#21017;&#21482;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#20102;&#36739;&#23567;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.08179</link><description>&lt;p&gt;
MEDBERT.de&#65306;&#19968;&#20010;&#22522;&#20110;&#24503;&#35821;&#30340;&#12289;&#38024;&#23545;&#21307;&#23398;&#39046;&#22495;&#19987;&#38376;&#35774;&#35745;&#30340;&#20840;&#38754;BERT&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MEDBERT.de: A Comprehensive German BERT Model for the Medical Domain. (arXiv:2303.08179v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;medBERT.de&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#24503;&#35821;&#21307;&#23398;&#39046;&#22495;&#30340;BERT&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#30340;&#35757;&#32451;&#65292;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#21307;&#23398;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;&#35813;&#27169;&#22411;&#23545;&#38271;&#25991;&#26412;&#29305;&#21035;&#26377;&#29992;&#65292;&#32780;&#25968;&#25454;&#21435;&#37325;&#21644;&#26377;&#25928;&#30340;&#20998;&#35789;&#21017;&#21482;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#20102;&#36739;&#23567;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;medBERT.de&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#24503;&#35821;&#21307;&#23398;&#39046;&#22495;&#19987;&#38376;&#35774;&#35745;&#30340;&#39044;&#35757;&#32451;BERT&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#24050;&#32463;&#22312;470&#19975;&#20221;&#24503;&#35821;&#21307;&#23398;&#25991;&#26723;&#30340;&#22823;&#22411;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#24182;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#21307;&#23398;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#25928;&#26524;&#65292;&#28041;&#21450;&#21508;&#31181;&#23398;&#31185;&#21644;&#21307;&#23398;&#25991;&#29486;&#31867;&#22411;&#12290;&#38500;&#20102;&#35780;&#20272;&#35813;&#27169;&#22411;&#30340;&#25972;&#20307;&#24615;&#33021;&#22806;&#65292;&#26412;&#25991;&#36824;&#23545;&#20854;&#33021;&#21147;&#36827;&#34892;&#20102;&#26356;&#28145;&#20837;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25968;&#25454;&#21435;&#37325;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#20351;&#29992;&#26356;&#26377;&#25928;&#30340;&#20998;&#35789;&#26041;&#27861;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20687;medBERT.de&#36825;&#26679;&#30340;&#39046;&#22495;&#19987;&#29992;&#27169;&#22411;&#29305;&#21035;&#36866;&#29992;&#20110;&#36739;&#38271;&#30340;&#25991;&#26412;&#65292;&#24182;&#19988;&#25968;&#25454;&#21435;&#37325;&#19981;&#19968;&#23450;&#20250;&#23548;&#33268;&#24615;&#33021;&#25913;&#21892;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#26377;&#25928;&#30340;&#20998;&#35789;&#21482;&#22312;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#21457;&#25381;&#20102;&#36739;&#23567;&#30340;&#20316;&#29992;&#65292;&#24182;&#19988;&#22823;&#22810;&#25968;&#25913;&#36827;&#28304;&#20110;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents medBERT.de, a pre-trained German BERT model specifically designed for the German medical domain. The model has been trained on a large corpus of 4.7 Million German medical documents and has been shown to achieve new state-of-the-art performance on eight different medical benchmarks covering a wide range of disciplines and medical document types. In addition to evaluating the overall performance of the model, this paper also conducts a more in-depth analysis of its capabilities. We investigate the impact of data deduplication on the model's performance, as well as the potential benefits of using more efficient tokenization methods. Our results indicate that domain-specific models such as medBERT.de are particularly useful for longer texts, and that deduplication of training data does not necessarily lead to improved performance. Furthermore, we found that efficient tokenization plays only a minor role in improving model performance, and attribute most of the improved
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#24179;&#31561;AI&#30740;&#31350;&#22278;&#26700;&#20250;&#35758;&#65288;EARR&#65289;&#65292;EARR&#20026;AI&#25216;&#26415;&#30340;&#36947;&#24503;&#21644;&#31038;&#20250;&#20260;&#23475;&#25552;&#20379;&#20851;&#38190;&#30740;&#31350;&#35270;&#35282;&#21644;&#21453;&#39304;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#21407;&#21017;&#65306;&#25193;&#22823;AI&#24320;&#21457;&#30340;&#19987;&#19994;&#30693;&#35782;&#33539;&#22260;&#65292;&#20419;&#36827;&#30693;&#35782;&#22909;&#22855;&#24515;&#21644;&#36131;&#20219;&#65292;&#20197;&#21450;&#21019;&#36896;&#30456;&#20114;&#23398;&#20064;&#30340;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2303.08177</link><description>&lt;p&gt;
&#24179;&#31561;AI&#30740;&#31350;&#22278;&#26700;&#20250;&#35758;&#65288;EARR&#65289;&#65306;&#26397;&#21521;&#31038;&#21306;&#20915;&#31574;&#30340;AI&#36127;&#36131;&#20219;&#24320;&#21457;
&lt;/p&gt;
&lt;p&gt;
The Equitable AI Research Roundtable (EARR): Towards Community-Based Decision Making in Responsible AI Development. (arXiv:2303.08177v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#24179;&#31561;AI&#30740;&#31350;&#22278;&#26700;&#20250;&#35758;&#65288;EARR&#65289;&#65292;EARR&#20026;AI&#25216;&#26415;&#30340;&#36947;&#24503;&#21644;&#31038;&#20250;&#20260;&#23475;&#25552;&#20379;&#20851;&#38190;&#30740;&#31350;&#35270;&#35282;&#21644;&#21453;&#39304;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#21407;&#21017;&#65306;&#25193;&#22823;AI&#24320;&#21457;&#30340;&#19987;&#19994;&#30693;&#35782;&#33539;&#22260;&#65292;&#20419;&#36827;&#30693;&#35782;&#22909;&#22855;&#24515;&#21644;&#36131;&#20219;&#65292;&#20197;&#21450;&#21019;&#36896;&#30456;&#20114;&#23398;&#20064;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25253;&#36947;&#20102;&#25105;&#20204;&#23545;&#24179;&#31561;AI&#30740;&#31350;&#22278;&#26700;&#20250;&#35758;&#65288;EARR&#65289;&#30340;&#21021;&#27493;&#35780;&#20272; &#8212;&#8212; &#19968;&#20010;&#30001;&#27861;&#24459;&#12289;&#25945;&#32946;&#12289;&#31038;&#21306;&#21442;&#19982;&#12289;&#31038;&#20250;&#27491;&#20041;&#21644;&#25216;&#26415;&#19987;&#23478;&#32452;&#25104;&#30340;&#32852;&#30431;&#12290;EARR&#26159;&#30001;&#22823;&#22411;&#31185;&#25216;&#20844;&#21496;&#12289;&#38750;&#33829;&#21033;&#32452;&#32455;&#12289;NGO&#30740;&#31350;&#26426;&#26500;&#21644;&#22823;&#23398;&#21512;&#20316;&#21019;&#24314;&#30340;&#65292;&#26088;&#22312;&#25552;&#20379;&#20851;&#20110;&#25216;&#26415;&#26032;&#20852;&#30340;&#36947;&#24503;&#21644;&#31038;&#20250;&#20260;&#23475;&#30340;&#20851;&#38190;&#30740;&#31350;&#35270;&#35282;&#21644;&#21453;&#39304;&#12290;&#36890;&#36807;&#21322;&#32467;&#26500;&#21270;&#30340;&#30740;&#35752;&#20250;&#21644;&#22312;&#22823;&#22411;&#31185;&#25216;&#20844;&#21496;&#20013;&#30340;&#35752;&#35770;&#65292;EARR&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#23558;&#20844;&#24179;&#21644;&#33030;&#24369;&#24615;&#19982;AI&#25216;&#26415;&#30456;&#20851;&#32852;&#30340;&#20851;&#38190;&#35266;&#28857;&#21644;&#21453;&#39304;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;EARR&#36804;&#20170;&#20026;&#27490;&#25805;&#20316;&#30340;&#19977;&#20010;&#21407;&#21017;&#65292;&#36825;&#20123;&#21407;&#21017;&#19982;FAccT&#31038;&#21306;&#30340;&#20851;&#27880;&#29305;&#21035;&#30456;&#20851;&#65306;&#22914;&#20309;&#25193;&#22823;AI&#24320;&#21457;&#20013;&#30340;&#19987;&#19994;&#30693;&#35782;&#33539;&#22260;&#65292;&#22914;&#20309;&#20419;&#36827;&#30693;&#35782;&#22909;&#22855;&#24515;&#21644;&#36131;&#20219;&#65292;&#20197;&#21450;&#22914;&#20309;&#21019;&#36896;&#30456;&#20114;&#23398;&#20064;&#30340;&#31354;&#38388;&#12290;&#26412;&#25991;&#26082;&#26159;&#20998;&#26512;&#21448;&#26159;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper reports on our initial evaluation of The Equitable AI Research Roundtable -- a coalition of experts in law, education, community engagement, social justice, and technology. EARR was created in collaboration among a large tech firm, nonprofits, NGO research institutions, and universities to provide critical research based perspectives and feedback on technology's emergent ethical and social harms. Through semi-structured workshops and discussions within the large tech firm, EARR has provided critical perspectives and feedback on how to conceptualize equity and vulnerability as they relate to AI technology. We outline three principles in practice of how EARR has operated thus far that are especially relevant to the concerns of the FAccT community: how EARR expands the scope of expertise in AI development, how it fosters opportunities for epistemic curiosity and responsibility, and that it creates a space for mutual learning. This paper serves as both an analysis and translatio
&lt;/p&gt;</description></item><item><title>WHOOPS!&#26159;&#19968;&#20010;&#26032;&#30340;&#35270;&#35273;&#24120;&#35782;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#20102;&#22270;&#20687;&#23383;&#24149;&#12289;&#36328;&#27169;&#24577;&#21305;&#37197;&#21644;&#35270;&#35273;&#38382;&#31572;&#31561;&#33509;&#24178;&#20010;&#20219;&#21153;&#65292;&#24341;&#20837;&#20102;&#35299;&#37322;&#29983;&#25104;&#20219;&#21153;&#65292;&#25361;&#25112;&#20102;AI&#27169;&#22411;&#35782;&#21035;&#21644;&#35299;&#37322;&#19981;&#21512;&#24120;&#35268;&#30340;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.07274</link><description>&lt;p&gt;
&#25171;&#30772;&#24120;&#35782;&#65306;WHOOPS&#65281;&#19968;&#20010;&#22522;&#20110;&#21512;&#25104;&#21644;&#32452;&#21512;&#22270;&#20687;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images. (arXiv:2303.07274v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07274
&lt;/p&gt;
&lt;p&gt;
WHOOPS!&#26159;&#19968;&#20010;&#26032;&#30340;&#35270;&#35273;&#24120;&#35782;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#20102;&#22270;&#20687;&#23383;&#24149;&#12289;&#36328;&#27169;&#24577;&#21305;&#37197;&#21644;&#35270;&#35273;&#38382;&#31572;&#31561;&#33509;&#24178;&#20010;&#20219;&#21153;&#65292;&#24341;&#20837;&#20102;&#35299;&#37322;&#29983;&#25104;&#20219;&#21153;&#65292;&#25361;&#25112;&#20102;AI&#27169;&#22411;&#35782;&#21035;&#21644;&#35299;&#37322;&#19981;&#21512;&#24120;&#35268;&#30340;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22855;&#24618;&#12289;&#24322;&#24120;&#21644;&#31070;&#31192;&#30340;&#22270;&#20687;&#20250;&#24341;&#36215;&#35266;&#23519;&#32773;&#30340;&#22909;&#22855;&#24515;&#65292;&#22240;&#20026;&#23427;&#20204;&#25361;&#25112;&#20102;&#24120;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;WHOOPS&#65281;&#19968;&#20010;&#26032;&#30340;&#35270;&#35273;&#24120;&#35782;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;&#35813;&#25968;&#25454;&#38598;&#30001;&#35774;&#35745;&#24072;&#20351;&#29992;Midjourney&#31561;&#20844;&#20849;&#21487;&#29992;&#22270;&#20687;&#29983;&#25104;&#24037;&#20855;&#21046;&#20316;&#65292;&#24182;&#21253;&#21547;&#33509;&#24178;&#20010;&#20219;&#21153;&#12290;&#38500;&#20102;&#22270;&#20687;&#23383;&#24149;&#12289;&#36328;&#27169;&#24577;&#21305;&#37197;&#21644;&#35270;&#35273;&#38382;&#31572;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#22256;&#38590;&#30340;&#35299;&#37322;&#29983;&#25104;&#20219;&#21153;&#65292;&#20854;&#20013;&#27169;&#22411;&#24517;&#39035;&#35782;&#21035;&#24182;&#35299;&#37322;&#32473;&#23450;&#22270;&#20687;&#30340;&#24322;&#24120;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weird, unusual, and uncanny images pique the curiosity of observers because they challenge commonsense. For example, an image released during the 2022 world cup depicts the famous soccer stars Lionel Messi and Cristiano Ronaldo playing chess, which playfully violates our expectation that their competition should occur on the football field. Humans can easily recognize and interpret these unconventional images, but can AI models do the same? We introduce WHOOPS!, a new dataset and benchmark for visual commonsense. The dataset is comprised of purposefully commonsense-defying images created by designers using publicly-available image generation tools like Midjourney. We consider several tasks posed over the dataset. In addition to image captioning, cross-modal matching, and visual question answering, we introduce a difficult explanation generation task, where models must identify and explain why a given image is unusual. Our results show that state-of-the-art models such as GPT3 and BLIP2
&lt;/p&gt;</description></item><item><title>STAIR&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#24322;&#24120;&#20540;&#27719;&#24635;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#32039;&#20945;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#35268;&#21017;&#65292;&#20197;&#27719;&#24635;&#21644;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#32467;&#26524;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#20934;&#30830;&#22320;&#24635;&#32467;&#26816;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.06261</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#24322;&#24120;&#20540;&#27719;&#24635;
&lt;/p&gt;
&lt;p&gt;
Interpretable Outlier Summarization. (arXiv:2303.06261v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06261
&lt;/p&gt;
&lt;p&gt;
STAIR&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#24322;&#24120;&#20540;&#27719;&#24635;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#32039;&#20945;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#35268;&#21017;&#65292;&#20197;&#27719;&#24635;&#21644;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#32467;&#26524;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#20934;&#30830;&#22320;&#24635;&#32467;&#26816;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
STAIR proposes an interpretable outlier summarization method by learning a compact set of human understandable rules to summarize and explain the anomaly detection results, which has strong interpretability to accurately summarize the detection results.
&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#20540;&#26816;&#27979;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20197;&#38450;&#27490;&#37329;&#34701;&#27450;&#35784;&#12289;&#38450;&#24481;&#32593;&#32476;&#20837;&#20405;&#25110;&#26816;&#27979;&#21363;&#23558;&#21457;&#29983;&#30340;&#35774;&#22791;&#25925;&#38556;&#12290;&#20026;&#20102;&#20943;&#23569;&#20154;&#21147;&#35780;&#20272;&#24322;&#24120;&#20540;&#26816;&#27979;&#32467;&#26524;&#30340;&#24037;&#20316;&#37327;&#65292;&#24182;&#26377;&#25928;&#22320;&#23558;&#24322;&#24120;&#20540;&#36716;&#21270;&#20026;&#21487;&#25805;&#20316;&#30340;&#35265;&#35299;&#65292;&#29992;&#25143;&#36890;&#24120;&#24076;&#26395;&#31995;&#32479;&#33258;&#21160;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#24322;&#24120;&#20540;&#26816;&#27979;&#32467;&#26524;&#30340;&#23376;&#32452;&#30340;&#27719;&#24635;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#27809;&#26377;&#36825;&#26679;&#30340;&#31995;&#32479;&#23384;&#22312;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;STAIR&#65292;&#23427;&#23398;&#20064;&#20102;&#19968;&#32452;&#32039;&#20945;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#35268;&#21017;&#65292;&#20197;&#27719;&#24635;&#21644;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#32467;&#26524;&#12290;STAIR&#19981;&#20351;&#29992;&#32463;&#20856;&#30340;&#20915;&#31574;&#26641;&#31639;&#27861;&#26469;&#20135;&#29983;&#36825;&#20123;&#35268;&#21017;&#65292;&#32780;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#20197;&#20135;&#29983;&#23569;&#37327;&#35268;&#21017;&#65292;&#20855;&#26377;&#26368;&#23567;&#30340;&#22797;&#26434;&#24615;&#65292;&#22240;&#27492;&#20855;&#26377;&#24378;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#20934;&#30830;&#22320;&#24635;&#32467;&#26816;&#27979;&#32467;&#26524;&#12290;STAIR&#30340;&#23398;&#20064;&#31639;&#27861;&#36890;&#36807;&#36845;&#20195;&#20998;&#21106;&#22823;&#35268;&#21017;&#26469;&#20135;&#29983;&#35268;&#21017;&#38598;&#65292;&#24182;&#22312;&#27599;&#20010;i&#20013;&#26368;&#22823;&#21270;&#36825;&#20010;&#30446;&#26631;&#65292;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Outlier detection is critical in real applications to prevent financial fraud, defend network intrusions, or detecting imminent device failures. To reduce the human effort in evaluating outlier detection results and effectively turn the outliers into actionable insights, the users often expect a system to automatically produce interpretable summarizations of subgroups of outlier detection results. Unfortunately, to date no such systems exist. To fill this gap, we propose STAIR which learns a compact set of human understandable rules to summarize and explain the anomaly detection results. Rather than use the classical decision tree algorithms to produce these rules, STAIR proposes a new optimization objective to produce a small number of rules with least complexity, hence strong interpretability, to accurately summarize the detection results. The learning algorithm of STAIR produces a rule set by iteratively splitting the large rules and is optimal in maximizing this objective in each i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#21521;&#37327;&#37327;&#21270;&#25216;&#26415;&#21644;&#21452;&#21521;&#21464;&#21387;&#22120;&#27169;&#22411;&#26469;&#29983;&#25104;&#36136;&#37327;&#26356;&#22909;&#12289;&#27169;&#22359;&#21270;&#21464;&#21270;&#26356;&#24555;&#30340;&#21512;&#25104;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2303.04743</link><description>&lt;p&gt;
&#24102;&#26377;&#21452;&#21521;&#20808;&#39564;&#27169;&#22411;&#30340;&#21521;&#37327;&#37327;&#21270;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Vector Quantized Time Series Generation with a Bidirectional Prior Model. (arXiv:2303.04743v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#21521;&#37327;&#37327;&#21270;&#25216;&#26415;&#21644;&#21452;&#21521;&#21464;&#21387;&#22120;&#27169;&#22411;&#26469;&#29983;&#25104;&#36136;&#37327;&#26356;&#22909;&#12289;&#27169;&#22359;&#21270;&#21464;&#21270;&#26356;&#24555;&#30340;&#21512;&#25104;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#19982;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21464;&#20307;&#30456;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451; GAN &#30340;&#22522;&#26412;&#38480;&#21046;&#21644;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#12290;&#27492;&#22806;&#65292;RNN&#26063;&#36890;&#24120;&#22312;&#36828;&#31243;&#26102;&#38388;&#27493;&#20043;&#38388;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#21463;&#21040;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986; TimeVQVAE&#65292;&#36825;&#26159;&#25105;&#20204;&#25152;&#30693;&#36947;&#30340;&#31532;&#19968;&#20010;&#20351;&#29992;&#21521;&#37327;&#37327;&#21270;&#65288;VQ&#65289;&#25216;&#26415;&#35299;&#20915; TSG &#38382;&#39064;&#30340;&#24037;&#20316;&#12290;&#27492;&#22806;&#65292;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#30340;&#20808;&#39564;&#20351;&#29992;&#21452;&#21521;&#21464;&#21387;&#22120;&#27169;&#22411;&#36827;&#34892;&#23398;&#20064;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#20840;&#23616;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#22312;&#26102;&#38388; - &#39057;&#29575;&#22495;&#20013;&#36827;&#34892; VQ &#24314;&#27169;&#65292;&#20998;&#20026;&#20302;&#39057;&#65288;LF&#65289;&#21644;&#39640;&#39057;&#65288;HF&#65289;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20445;&#30041;&#26102;&#38388;&#24207;&#21015;&#30340;&#37325;&#35201;&#29305;&#24449;&#65292;&#24182;&#29983;&#25104;&#36136;&#37327;&#26356;&#22909;&#12289;&#27169;&#22359;&#24615;&#21464;&#21270;&#26356;&#24555;&#30340;&#26032;&#21512;&#25104;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series generation (TSG) studies have mainly focused on the use of Generative Adversarial Networks (GANs) combined with recurrent neural network (RNN) variants. However, the fundamental limitations and challenges of training GANs still remain. In addition, the RNN-family typically has difficulties with temporal consistency between distant timesteps. Motivated by the successes in the image generation (IMG) domain, we propose TimeVQVAE, the first work, to our knowledge, that uses vector quantization (VQ) techniques to address the TSG problem. Moreover, the priors of the discrete latent spaces are learned with bidirectional transformer models that can better capture global temporal consistency. We also propose VQ modeling in a time-frequency domain, separated into low-frequency (LF) and high-frequency (HF). This allows us to retain important characteristics of the time series and, in turn, generate new synthetic signals that are of better quality, with sharper changes in modularity, t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#20027;&#27700;&#19979;&#36733;&#20307;&#30340;&#20248;&#21270;&#35774;&#35745;&#38382;&#39064;&#65292;&#36890;&#36807;&#38598;&#25104;FreeCAD&#21644;OpenFoam&#31561;&#24037;&#20855;&#36827;&#34892;&#33258;&#21160;&#21270;&#35774;&#35745;&#35780;&#20272;&#65292;&#24182;&#37319;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#35299;&#20915;&#20102;&#20248;&#21270;&#20013;&#26679;&#26412;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.14732</link><description>&lt;p&gt;
&#33258;&#20027;&#27700;&#19979;&#36710;&#20307;&#35774;&#35745;&#30340;&#32422;&#26463;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Constrained Bayesian Optimization for Automatic Underwater Vehicle Hull Design. (arXiv:2302.14732v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#20027;&#27700;&#19979;&#36733;&#20307;&#30340;&#20248;&#21270;&#35774;&#35745;&#38382;&#39064;&#65292;&#36890;&#36807;&#38598;&#25104;FreeCAD&#21644;OpenFoam&#31561;&#24037;&#20855;&#36827;&#34892;&#33258;&#21160;&#21270;&#35774;&#35745;&#35780;&#20272;&#65292;&#24182;&#37319;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#35299;&#20915;&#20102;&#20248;&#21270;&#20013;&#26679;&#26412;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#27700;&#19979;&#36710;&#20307;&#35774;&#35745;&#20248;&#21270;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#24037;&#31243;&#36807;&#31243;&#65292;&#26088;&#22312;&#28385;&#36275;&#32473;&#23450;&#35201;&#27714;&#29983;&#25104;&#20855;&#26377;&#20248;&#21270;&#29305;&#24615;&#30340;UUV&#36733;&#20307;&#12290;&#39318;&#20808;&#65292;&#23427;&#28041;&#21450;&#38598;&#25104;&#30456;&#20851;&#30340;&#22797;&#26434;&#24037;&#31243;&#20223;&#30495;&#24037;&#20855;&#12290;&#20854;&#27425;&#65292;&#23427;&#38656;&#35201;&#23558;&#26679;&#26412;&#26377;&#25928;&#30340;&#20248;&#21270;&#26694;&#26550;&#19982;&#38598;&#25104;&#24037;&#20855;&#38142;&#30456;&#32467;&#21512;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#31216;&#20026;FreeCAD&#30340;CAD&#24037;&#20855;&#19982;CFD&#24037;&#20855;openFoam&#38598;&#25104;&#65292;&#20197;&#36827;&#34892;&#33258;&#21160;&#21270;&#35774;&#35745;&#35780;&#20272;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#36827;&#34892;&#20248;&#21270;&#65292;&#36825;&#26159;&#19968;&#31181;&#20026;&#20248;&#21270;&#32791;&#26102;&#26114;&#36149;&#30340;&#24037;&#31243;&#27169;&#25311;&#32780;&#24320;&#21457;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#25216;&#26415;&#65292;&#22312;&#22810;&#31181;&#38382;&#39064;&#20013;&#24050;&#35777;&#26126;&#20855;&#26377;&#38750;&#24120;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#21253;&#25324;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#23454;&#39564;&#35774;&#35745;&#12290;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#19981;&#21487;&#34892;&#35774;&#35745;&#20316;&#20026;&#32422;&#26463;&#38598;&#25104;&#21040;&#20248;&#21270;&#36807;&#31243;&#20013;&#12290;&#36890;&#36807;&#23558;&#39046;&#22495;&#19987;&#29992;&#24037;&#20855;&#38142;&#19982;&#22522;&#20110;AI&#30340;&#20248;&#21270;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#25191;&#34892;&#20102;&#33258;&#21160;&#35774;&#35745;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic underwater vehicle hull Design optimization is a complex engineering process for generating a UUV hull with optimized properties on a given requirement. First, it involves the integration of involved computationally complex engineering simulation tools. Second, it needs integration of a sample efficient optimization framework with the integrated toolchain. To this end, we integrated the CAD tool called FreeCAD with CFD tool openFoam for automatic design evaluation. For optimization, we chose Bayesian optimization (BO), which is a well-known technique developed for optimizing time-consuming expensive engineering simulations and has proven to be very sample efficient in a variety of problems, including hyper-parameter tuning and experimental design. During the optimization process, we can handle infeasible design as constraints integrated into the optimization process. By integrating domain-specific toolchain with AI-based optimization, we executed the automatic design optimiza
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#19977;&#31181;&#22810;&#27169;&#24577;&#23545;&#35937;&#35782;&#21035;&#26041;&#27861;&#24182;&#22312;SIMMC 2.1&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#26368;&#20339;&#26041;&#27861;&#26159;&#22522;&#20110;&#22330;&#26223;&#23545;&#35805;&#30340;&#23545;&#40784;&#65292;&#30456;&#27604;&#22522;&#20934;&#27979;&#35797;&#25552;&#39640;&#20102;&#32422;20%&#30340;F1&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2302.14680</link><description>&lt;p&gt;
&#20320;&#26159;&#25351;&#21738;&#19968;&#20010;&#65311;&#22522;&#20110;&#24773;&#22659;&#23545;&#35805;&#30340;&#22810;&#27169;&#24577;&#23545;&#35937;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Which One Are You Referring To? Multimodal Object Identification in Situated Dialogue. (arXiv:2302.14680v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#19977;&#31181;&#22810;&#27169;&#24577;&#23545;&#35937;&#35782;&#21035;&#26041;&#27861;&#24182;&#22312;SIMMC 2.1&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#26368;&#20339;&#26041;&#27861;&#26159;&#22522;&#20110;&#22330;&#26223;&#23545;&#35805;&#30340;&#23545;&#40784;&#65292;&#30456;&#27604;&#22522;&#20934;&#27979;&#35797;&#25552;&#39640;&#20102;&#32422;20%&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23545;&#35805;&#31995;&#32479;&#30340;&#38656;&#27714;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#26377;&#19981;&#26029;&#19978;&#21319;&#65292;&#36825;&#24378;&#35843;&#20102;&#20174;&#23545;&#35805;&#21644;&#24773;&#22659;&#32972;&#26223;&#20013;&#29702;&#35299;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19977;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#26368;&#22823;&#30340;&#24773;&#22659;&#23545;&#35805;&#25968;&#25454;&#38598;SIMMC 2.1&#19978;&#23545;&#23427;&#20204;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#26368;&#22909;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#22330;&#26223;&#23545;&#35805;&#30340;&#23545;&#40784;&#65292;&#30456;&#27604;SIMMC 2.1&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#23427;&#25552;&#39640;&#20102;&#32422;20%&#30340;F1&#20998;&#25968;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20998;&#26512;&#21644;&#35752;&#35770;&#25105;&#20204;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#24037;&#20316;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#20844;&#24320;&#22312;https://github.com/holylovenia/multimodal-object-identification&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The demand for multimodal dialogue systems has been rising in various domains, emphasizing the importance of interpreting multimodal inputs from conversational and situational contexts. We explore three methods to tackle this problem and evaluate them on the largest situated dialogue dataset, SIMMC 2.1. Our best method, scene-dialogue alignment, improves the performance by ~20% F1-score compared to the SIMMC 2.1 baselines. We provide analysis and discussion regarding the limitation of our methods and the potential directions for future works. Our code is publicly available at https://github.com/holylovenia/multimodal-object-identification.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#20154;&#32676;&#20307;&#23545;&#36830;&#32493;&#29615;&#22659;&#30340;&#20272;&#35745;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25511;&#21046;&#31639;&#27861;&#12290;&#20854;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#21160;&#24577;&#32593;&#32476;&#25299;&#25169;&#21644;&#20915;&#31574;&#21046;&#23450;&#20043;&#38388;&#23384;&#22312;&#22240;&#26524;&#24490;&#29615;&#65292;&#24433;&#21709;&#30528;&#31934;&#24230;&#21644;&#25910;&#25947;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2302.13629</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#32676;&#20307;&#23545;&#36830;&#32493;&#29615;&#22659;&#30340;&#20272;&#35745;&#65306;&#30456;&#20851;&#32593;&#32476;&#21644;&#20915;&#31574;&#21046;&#23450;
&lt;/p&gt;
&lt;p&gt;
Estimation of continuous environments by robot swarms: Correlated networks and decision-making. (arXiv:2302.13629v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#20154;&#32676;&#20307;&#23545;&#36830;&#32493;&#29615;&#22659;&#30340;&#20272;&#35745;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25511;&#21046;&#31639;&#27861;&#12290;&#20854;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#21160;&#24577;&#32593;&#32476;&#25299;&#25169;&#21644;&#20915;&#31574;&#21046;&#23450;&#20043;&#38388;&#23384;&#22312;&#22240;&#26524;&#24490;&#29615;&#65292;&#24433;&#21709;&#30528;&#31934;&#24230;&#21644;&#25910;&#25947;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#20307;&#20915;&#31574;&#26159;&#24314;&#31435;&#22312;&#32676;&#20307;&#23618;&#38754;&#19978;&#30340;&#22823;&#35268;&#27169;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#20851;&#38190;&#33021;&#21147;&#12290;&#32676;&#20307;&#26426;&#22120;&#20154;&#38598;&#20307;&#20915;&#31574;&#30340;&#22823;&#37096;&#20998;&#25991;&#29486;&#37117;&#38598;&#20013;&#22312;&#20174;&#26377;&#38480;&#25968;&#37327;&#30340;&#36873;&#39033;&#20013;&#36827;&#34892;&#30340;&#31163;&#25955;&#20915;&#31574;&#19978;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;&#19968;&#20010;&#20998;&#25955;&#24335;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#20219;&#21153;&#20998;&#37197;&#20026;&#25506;&#32034;&#26080;&#30028;&#29615;&#22659;&#65292;&#25214;&#20986;&#21487;&#27979;&#29615;&#22659;&#29305;&#24449;&#30340;&#24179;&#22343;&#20540;&#24182;&#27719;&#24635;&#33267;&#37027;&#20010;&#20540;&#34987;&#27979;&#37327;&#30340;&#21306;&#22495;&#65288;&#20363;&#22914;&#65292;&#31561;&#39640;&#32447;&#65289;&#12290;&#35813;&#20219;&#21153;&#30340;&#29420;&#29305;&#24615;&#22312;&#20110;&#26426;&#22120;&#20154;&#30340;&#21160;&#24577;&#32593;&#32476;&#25299;&#25169;&#21644;&#23427;&#20204;&#30340;&#20915;&#31574;&#21046;&#23450;&#20043;&#38388;&#23384;&#22312;&#22240;&#26524;&#24490;&#29615;&#12290;&#20363;&#22914;&#65292;&#32593;&#32476;&#30340;&#24179;&#22343;&#33410;&#28857;&#24230;&#25968;&#24433;&#21709;&#25910;&#25947;&#26102;&#38388;&#65292;&#32780;&#24403;&#21069;&#24050;&#35748;&#21516;&#30340;&#24179;&#22343;&#20540;&#24433;&#21709;&#32676;&#20307;&#30340;&#27719;&#32858;&#20301;&#32622;&#65292;&#22240;&#27492;&#65292;&#20063;&#24433;&#21709;&#32593;&#32476;&#32467;&#26500;&#20197;&#21450;&#31934;&#24230;&#35823;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25511;&#21046;&#31639;&#27861;&#65292;&#24182;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#23454;&#38469;&#26426;&#22120;&#20154;&#32676;&#20307;&#23454;&#39564;&#26469;&#30740;&#31350;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collective decision-making is an essential capability of large-scale multi-robot systems to establish autonomy on the swarm level. A large portion of literature on collective decision-making in swarm robotics focuses on discrete decisions selecting from a limited number of options. Here we assign a decentralized robot system with the task of exploring an unbounded environment, finding consensus on the mean of a measurable environmental feature, and aggregating at areas where that value is measured (e.g., a contour line). A unique quality of this task is a causal loop between the robots' dynamic network topology and their decision-making. For example, the network's mean node degree influences time to convergence while the currently agreed-on mean value influences the swarm's aggregation location, hence, also the network structure as well as the precision error. We propose a control algorithm and study it in real-world robot swarm experiments in different environments. We show that our a
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#20171;&#32461;&#20102;&#33258;&#20027;&#23548;&#33322;&#39046;&#22495;&#20013;&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#21644;&#26041;&#27861;&#65292;&#21253;&#25324;&#38556;&#30861;&#26816;&#27979;&#12289;&#22330;&#26223;&#24863;&#30693;&#12289;&#36335;&#24452;&#35268;&#21010;&#21644;&#25511;&#21046;&#12290;&#31361;&#20986;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#24037;&#31243;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20197;&#21450;&#21019;&#26032;&#23548;&#33322;&#26041;&#27861;&#30340;&#24320;&#21457;&#21644;&#36328;&#23398;&#31185;&#30740;&#31350;&#30340;&#37325;&#35201;&#24615;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2302.11089</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#20027;&#23548;&#33322;&#20013;&#30340;&#24212;&#29992;&#19982;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;--&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Recent Advancements in Deep Learning Applications and Methods for Autonomous Navigation -- A Comprehensive Review. (arXiv:2302.11089v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#20171;&#32461;&#20102;&#33258;&#20027;&#23548;&#33322;&#39046;&#22495;&#20013;&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#21644;&#26041;&#27861;&#65292;&#21253;&#25324;&#38556;&#30861;&#26816;&#27979;&#12289;&#22330;&#26223;&#24863;&#30693;&#12289;&#36335;&#24452;&#35268;&#21010;&#21644;&#25511;&#21046;&#12290;&#31361;&#20986;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#24037;&#31243;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20197;&#21450;&#21019;&#26032;&#23548;&#33322;&#26041;&#27861;&#30340;&#24320;&#21457;&#21644;&#36328;&#23398;&#31185;&#30740;&#31350;&#30340;&#37325;&#35201;&#24615;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#20840;&#38754;&#20171;&#32461;&#20102;&#22312;&#33258;&#20027;&#23548;&#33322;&#32972;&#26223;&#19979;&#20351;&#29992;&#30340;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21253;&#25324;&#38556;&#30861;&#29289;&#26816;&#27979;&#12289;&#22330;&#26223;&#24863;&#30693;&#12289;&#36335;&#24452;&#35268;&#21010;&#21644;&#25511;&#21046;&#12290;&#35813;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#20998;&#26512;&#26368;&#36817;&#30340;&#30740;&#31350;&#25104;&#26524;&#24182;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#23454;&#26045;&#21644;&#27979;&#35797;&#26469;&#24357;&#21512;&#33258;&#20027;&#23548;&#33322;&#21644;&#28145;&#24230;&#23398;&#20064;&#20043;&#38388;&#30340;&#40511;&#27807;&#12290;&#37325;&#28857;&#24378;&#35843;&#20102;&#23548;&#33322;&#23545;&#20110;&#31227;&#21160;&#26426;&#22120;&#20154;&#12289;&#33258;&#20027;&#36710;&#36742;&#21644;&#26080;&#20154;&#26426;&#30340;&#37325;&#35201;&#24615;&#65292;&#21516;&#26102;&#20063;&#35748;&#35782;&#21040;&#30001;&#20110;&#29615;&#22659;&#22797;&#26434;&#24615;&#12289;&#19981;&#30830;&#23450;&#24615;&#12289;&#38556;&#30861;&#29289;&#12289;&#21160;&#24577;&#29615;&#22659;&#20197;&#21450;&#35268;&#21010;&#22810;&#20010;&#26234;&#33021;&#20307;&#30340;&#36335;&#24452;&#30340;&#38656;&#35201;&#32780;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#26412;&#32508;&#36848;&#31361;&#20986;&#28145;&#24230;&#23398;&#20064;&#22312;&#24037;&#31243;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#24555;&#36895;&#21457;&#23637;&#20197;&#21450;&#20854;&#21019;&#26032;&#23548;&#33322;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#35752;&#35770;&#20102;&#19982;&#26412;&#39046;&#22495;&#30456;&#20851;&#30340;&#36817;&#26399;&#36328;&#23398;&#31185;&#30740;&#31350;&#65292;&#24182;&#23545;&#28145;&#24230;&#23398;&#20064;&#30340;&#23616;&#38480;&#24615;&#12289;&#25361;&#25112;&#21644;&#28508;&#22312;&#22686;&#38271;&#39046;&#22495;&#25552;&#20986;&#20102;&#31616;&#35201;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
This review paper presents a comprehensive overview of end-to-end deep learning frameworks used in the context of autonomous navigation, including obstacle detection, scene perception, path planning, and control. The paper aims to bridge the gap between autonomous navigation and deep learning by analyzing recent research studies and evaluating the implementation and testing of deep learning methods. It emphasizes the importance of navigation for mobile robots, autonomous vehicles, and unmanned aerial vehicles, while also acknowledging the challenges due to environmental complexity, uncertainty, obstacles, dynamic environments, and the need to plan paths for multiple agents. The review highlights the rapid growth of deep learning in engineering data science and its development of innovative navigation methods. It discusses recent interdisciplinary work related to this field and provides a brief perspective on the limitations, challenges, and potential areas of growth for deep learning m
&lt;/p&gt;</description></item><item><title>DrasCLR&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20986;&#20004;&#31181;&#39046;&#22495;&#29305;&#23450;&#30340;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#26469;&#23398;&#20064;&#30142;&#30149;&#30456;&#20851;&#21644;&#35299;&#21078;&#29305;&#24322;&#24615;&#34920;&#31034;&#65292;&#29305;&#21035;&#26159;&#35299;&#20915;&#20102;&#21306;&#20998;&#30142;&#30149;&#27169;&#24335;&#21644;&#35299;&#21078;&#29305;&#24449;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2302.10390</link><description>&lt;p&gt;
DrasCLR: &#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#30142;&#30149;&#30456;&#20851;&#21644;&#35299;&#21078;&#29305;&#24322;&#24615;&#34920;&#31034;&#30340;&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
DrasCLR: A Self-supervised Framework of Learning Disease-related and Anatomy-specific Representation for 3D Medical Images. (arXiv:2302.10390v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10390
&lt;/p&gt;
&lt;p&gt;
DrasCLR&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20986;&#20004;&#31181;&#39046;&#22495;&#29305;&#23450;&#30340;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#26469;&#23398;&#20064;&#30142;&#30149;&#30456;&#20851;&#21644;&#35299;&#21078;&#29305;&#24322;&#24615;&#34920;&#31034;&#65292;&#29305;&#21035;&#26159;&#35299;&#20915;&#20102;&#21306;&#20998;&#30142;&#30149;&#27169;&#24335;&#21644;&#35299;&#21078;&#29305;&#24449;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#35268;&#27169;&#24102;&#27880;&#37322;&#30340;&#20307;&#31215;&#21307;&#23398;&#22270;&#20687;&#30340;&#33719;&#21462;&#25104;&#26412;&#39640;&#26114;&#65292;&#22240;&#27492;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20026;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#39044;&#35757;&#32451;&#21644;&#29305;&#24449;&#25552;&#21462;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#20165;&#20351;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#23454;&#20363;&#21306;&#20998;&#30340;SSL&#26041;&#27861;&#22312;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#21464;&#24471;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;SSL&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#21487;&#33021;&#20351;&#29992;&#35768;&#22810;&#22270;&#20687;&#32447;&#32034;&#26469;&#21306;&#20998;&#19968;&#20010;&#23454;&#20363;&#65292;&#32780;&#36825;&#20123;&#32447;&#32034;&#19981;&#19968;&#23450;&#19982;&#30142;&#30149;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#30149;&#29702;&#27169;&#24335;&#24120;&#24120;&#24456;&#24494;&#22937;&#21644;&#24322;&#36136;&#65292;&#38656;&#35201;&#25152;&#38656;&#26041;&#27861;&#33021;&#22815;&#34920;&#31034;&#23545;&#19981;&#21516;&#36523;&#20307;&#37096;&#20301;&#20013;&#30340;&#24322;&#24120;&#21464;&#21270;&#25935;&#24863;&#30340;&#35299;&#21078;&#29305;&#24322;&#24615;&#29305;&#24449;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DrasCLR&#30340;&#26032;&#30340;SSL&#26694;&#26550;&#65292;&#29992;&#20110;&#19977;&#32500;&#21307;&#23398;&#25104;&#20687;&#65292;&#20197;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#39046;&#22495;&#29305;&#23450;&#30340;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65306;&#19968;&#31181;&#26088;&#22312;&#25429;&#25417;&#23616;&#37096;&#35299;&#21078;&#21306;&#22495;&#20869;&#24494;&#22937;&#30142;&#30149;&#27169;&#24335;&#65292;&#21478;&#19968;&#31181;&#26088;&#22312;&#35782;&#21035;&#20855;&#26377;&#19981;&#21516;&#25361;&#25112;&#24615;&#30340;&#35299;&#21078;&#21306;&#22495;&#38388;&#30340;&#30142;&#30149;&#30456;&#20851;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale volumetric medical images with annotation are rare, costly, and time prohibitive to acquire. Self-supervised learning (SSL) offers a promising pre-training and feature extraction solution for many downstream tasks, as it only uses unlabeled data. Recently, SSL methods based on instance discrimination have gained popularity in the medical imaging domain. However, SSL pre-trained encoders may use many clues in the image to discriminate an instance that are not necessarily disease-related. Moreover, pathological patterns are often subtle and heterogeneous, requiring the ability of the desired method to represent anatomy-specific features that are sensitive to abnormal changes in different body parts. In this work, we present a novel SSL framework, named DrasCLR, for 3D medical imaging to overcome these challenges. We propose two domain-specific contrastive learning strategies: one aims to capture subtle disease patterns inside a local anatomical region, and the other aims to r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;CFD&#27169;&#25311;&#21644;&#20248;&#21270;&#31639;&#27861;&#65292;&#30740;&#31350;&#26368;&#23567;&#21270;&#38459;&#21147;&#30340;&#27700;&#19979;&#33322;&#34892;&#22120;&#35774;&#35745;&#26041;&#26696;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#23384;&#22312;&#21487;&#20197;&#36866;&#24212;&#25152;&#26377;&#36816;&#34892;&#21644;&#29615;&#22659;&#26465;&#20214;&#30340;&#36890;&#29992;&#35774;&#35745;&#65292;&#20294;&#22312;&#39640;&#36895;&#21644;&#39640;&#28237;&#27969;&#26465;&#20214;&#19979;&#25214;&#21040;&#20102;&#25509;&#36817;&#26368;&#20339;&#35774;&#35745;&#30340;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2302.09441</link><description>&lt;p&gt;
&#24212;&#29992;CFD&#23547;&#25214;&#27700;&#19979;&#33322;&#34892;&#22120;&#30340;&#26368;&#23567;&#38459;&#21147;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Search for universal minimum drag resistance underwater vehicle hull using CFD. (arXiv:2302.09441v2 [cs.CE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;CFD&#27169;&#25311;&#21644;&#20248;&#21270;&#31639;&#27861;&#65292;&#30740;&#31350;&#26368;&#23567;&#21270;&#38459;&#21147;&#30340;&#27700;&#19979;&#33322;&#34892;&#22120;&#35774;&#35745;&#26041;&#26696;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#23384;&#22312;&#21487;&#20197;&#36866;&#24212;&#25152;&#26377;&#36816;&#34892;&#21644;&#29615;&#22659;&#26465;&#20214;&#30340;&#36890;&#29992;&#35774;&#35745;&#65292;&#20294;&#22312;&#39640;&#36895;&#21644;&#39640;&#28237;&#27969;&#26465;&#20214;&#19979;&#25214;&#21040;&#20102;&#25509;&#36817;&#26368;&#20339;&#35774;&#35745;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#20027;&#27700;&#19979;&#33322;&#34892;&#22120;&#65288;AUV&#65289;&#35774;&#35745;&#20013;&#65292;&#33337;&#36523;&#38459;&#21147;&#26159;&#30830;&#23450;&#36710;&#36742;&#21151;&#29575;&#38656;&#27714;&#21644;&#33322;&#31243;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#36827;&#32780;&#24433;&#21709;&#35774;&#35745;&#20013;&#30005;&#27744;&#22823;&#23567;&#12289;&#37325;&#37327;&#21644;&#20307;&#31215;&#35201;&#27714;&#12290;&#26412;&#25991;&#21033;&#29992;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20248;&#21270;&#31639;&#27861;&#21644;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#65288;CFD&#65289;&#27169;&#25311;&#65292;&#30740;&#31350;&#26368;&#23567;&#21270;&#38459;&#21147;&#30340;&#26368;&#20339;&#33337;&#20307;&#35774;&#35745;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#30340;&#36816;&#34892;&#36895;&#24230;&#21644;&#28237;&#27969;&#24378;&#24230;&#19979;&#36816;&#34892;&#22522;&#20110;CFD&#30340;&#20248;&#21270;&#65292;&#25105;&#20204;&#24076;&#26395;&#30740;&#31350;/&#25628;&#32034;&#21487;&#22312;&#25152;&#26377;&#25805;&#20316;&#26465;&#20214;&#65288;&#25805;&#20316;&#36895;&#24230;&#65289;&#21644;&#29615;&#22659;&#26465;&#20214;&#65288;&#28237;&#27969;&#24378;&#24230;&#65289;&#19979;&#25552;&#20379;&#26368;&#23567;&#38459;&#21147;/&#25509;&#36817;&#26368;&#20339;&#35774;&#35745;&#30340;&#36890;&#29992;&#35774;&#35745;&#30340;&#21487;&#33021;&#24615;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20302;&#36895;&#21644;&#20302;&#28237;&#27969;&#26465;&#20214;&#19979;&#25214;&#21040;&#30340;&#26368;&#20339;&#35774;&#35745;&#22312;&#39640;&#36895;&#21644;&#39640;&#28237;&#27969;&#26465;&#20214;&#19979;&#34920;&#29616;&#38750;&#24120;&#24046;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#36895;&#21644;&#39640;&#28237;&#27969;&#26465;&#20214;&#19979;&#26368;&#20339;&#30340;&#35774;&#35745;&#34920;&#29616;&#25509;&#36817;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Autonomous Underwater Vehicles (AUVs) design, hull resistance is an important factor in determining the power requirements and range of vehicle and consequently affect battery size, weight, and volume requirement of the design. In this paper, we leverage on AI-based optimization algorithm along with Computational Fluid Dynamics (CFD) simulation to study the optimal hull design that minimizing the resistance. By running the CFD-based optimization at different operating velocities and turbulence intensity, we want to study/search the possibility of a universal design that will provide least resistance/near-optimal design across all operating conditions (operating velocity) and environmental conditions (turbulence intensity). Early result demonstrated that the optimal design found at low velocity and low turbulence condition performs very poor at high velocity and high turbulence conditions. However, a design that is optimal at high velocity and high turbulence conditions performs near
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SoftMatch&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#20445;&#25345;&#39640;&#25968;&#37327;&#21644;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#26469;&#20811;&#26381;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#25968;&#37327;-&#36136;&#37327;&#26435;&#34913;&#38382;&#39064;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;&#25968;&#25454;&#12290; &#22312;&#23454;&#39564;&#20013;&#65292;SoftMatch&#22312;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#24433;&#29255;&#31561;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#37117;&#26174;&#31034;&#20102;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2301.10921</link><description>&lt;p&gt;
SoftMatch&#65306;&#35299;&#20915;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#25968;&#37327;-&#36136;&#37327;&#26435;&#34913;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning. (arXiv:2301.10921v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10921
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SoftMatch&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#20445;&#25345;&#39640;&#25968;&#37327;&#21644;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#26469;&#20811;&#26381;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#25968;&#37327;-&#36136;&#37327;&#26435;&#34913;&#38382;&#39064;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;&#25968;&#25454;&#12290; &#22312;&#23454;&#39564;&#20013;&#65292;SoftMatch&#22312;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#24433;&#29255;&#31561;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#37117;&#26174;&#31034;&#20102;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#22914;&#20309;&#26377;&#25928;&#21033;&#29992;&#26377;&#38480;&#30340;&#26631;&#27880;&#25968;&#25454;&#21644;&#22823;&#37327;&#30340;&#26410;&#26631;&#27880;&#25968;&#25454;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#26412;&#25991;&#39318;&#20808;&#36890;&#36807;&#32479;&#19968;&#30340;&#26679;&#26412;&#21152;&#26435;&#20844;&#24335;&#37325;&#26032;&#23457;&#35270;&#20102;&#27969;&#34892;&#30340;&#20266;&#26631;&#31614;&#26041;&#27861;&#65292;&#24182;&#28436;&#31034;&#20102;&#20266;&#26631;&#31614;&#38408;&#20540;&#27861;&#22266;&#26377;&#30340;&#25968;&#37327;-&#36136;&#37327;&#26435;&#34913;&#38382;&#39064;&#65292;&#21487;&#33021;&#20250;&#38459;&#30861;&#23398;&#20064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SoftMatch&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#20445;&#25345;&#20266;&#26631;&#31614;&#30340;&#39640;&#25968;&#37327;&#21644;&#39640;&#36136;&#37327;&#26469;&#20811;&#26381;&#36825;&#31181;&#26435;&#34913;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#20010;&#25130;&#26029;&#30340;&#39640;&#26031;&#20989;&#25968;&#26469;&#26681;&#25454;&#26679;&#26412;&#30340;&#32622;&#20449;&#24230;&#23545;&#26679;&#26412;&#36827;&#34892;&#21152;&#26435;&#65292;&#36825;&#21487;&#20197;&#30475;&#20316;&#26159;&#32622;&#20449;&#24230;&#38408;&#20540;&#30340;&#36719;&#29256;&#26412;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#25552;&#20986;&#32479;&#19968;&#30340;&#23545;&#40784;&#26041;&#27861;&#26469;&#22686;&#24378;&#23545;&#24369;&#23398;&#20064;&#31867;&#30340;&#21033;&#29992;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;SoftMatch&#22312;&#21253;&#25324;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#24433;&#29255;&#31561;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#20102;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The critical challenge of Semi-Supervised Learning (SSL) is how to effectively leverage the limited labeled data and massive unlabeled data to improve the model's generalization performance. In this paper, we first revisit the popular pseudo-labeling methods via a unified sample weighting formulation and demonstrate the inherent quantity-quality trade-off problem of pseudo-labeling with thresholding, which may prohibit learning. To this end, we propose SoftMatch to overcome the trade-off by maintaining both high quantity and high quality of pseudo-labels during training, effectively exploiting the unlabeled data. We derive a truncated Gaussian function to weight samples based on their confidence, which can be viewed as a soft version of the confidence threshold. We further enhance the utilization of weakly-learned classes by proposing a uniform alignment approach. In experiments, SoftMatch shows substantial improvements across a wide variety of benchmarks, including image, text, and im
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25968;&#25454;&#21644;&#36923;&#36753;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#25512;&#29702;&#30340;&#29702;&#35770;&#65292;&#35299;&#20915;&#20102;&#31526;&#21495;&#30693;&#35782;&#30340;&#27010;&#29575;&#25512;&#29702;&#38382;&#39064;&#65292;&#24182;&#22312;&#23450;&#20301;&#38382;&#39064;&#20013;&#23637;&#31034;&#20986;&#26426;&#22120;&#20154;&#21487;&#20197;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#22320;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.08509</link><description>&lt;p&gt;
&#24102;&#26377;&#26102;&#38388;&#30340;&#29983;&#25104;&#36923;&#36753;&#65306;&#36229;&#36234;&#36923;&#36753;&#19968;&#33268;&#24615;&#21644;&#32479;&#35745;&#21487;&#33021;&#24615;
&lt;/p&gt;
&lt;p&gt;
Generative Logic with Time: Beyond Logical Consistency and Statistical Possibility. (arXiv:2301.08509v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25968;&#25454;&#21644;&#36923;&#36753;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#25512;&#29702;&#30340;&#29702;&#35770;&#65292;&#35299;&#20915;&#20102;&#31526;&#21495;&#30693;&#35782;&#30340;&#27010;&#29575;&#25512;&#29702;&#38382;&#39064;&#65292;&#24182;&#22312;&#23450;&#20301;&#38382;&#39064;&#20013;&#23637;&#31034;&#20986;&#26426;&#22120;&#20154;&#21487;&#20197;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#22320;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25512;&#29702;&#29702;&#35770;&#65292;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#23436;&#20840;&#36923;&#36753;&#25512;&#29702;&#31526;&#21495;&#30693;&#35782;&#12290;&#25105;&#20204;&#37319;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#26469;&#24314;&#27169;&#25968;&#25454;&#22914;&#20309;&#24341;&#36215;&#31526;&#21495;&#30693;&#35782;&#12290;&#31526;&#21495;&#30693;&#35782;&#30340;&#27010;&#29575;&#25512;&#29702;&#34987;&#24314;&#27169;&#20026;&#27491;&#21521;&#21644;&#21453;&#21521;&#36807;&#31243;&#65292;&#20998;&#21035;&#23545;&#24212;&#24418;&#24335;&#36923;&#36753;&#30340;&#35299;&#37322;&#21644;&#36870;&#35299;&#37322;&#12290;&#35813;&#29702;&#35770;&#24212;&#29992;&#20110;&#23450;&#20301;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#20855;&#26377;&#25439;&#22351;&#25110;&#22122;&#22768;&#20256;&#24863;&#22120;&#30340;&#26426;&#22120;&#20154;&#21487;&#20197;&#20197;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#26377;&#25928;&#22320;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper gives a simple theory of inference to logically reason symbolic knowledge fully from data over time. We take a Bayesian approach to model how data causes symbolic knowledge. Probabilistic reasoning with symbolic knowledge is modelled as a process of going the causality forwards and backwards. The forward and backward processes correspond to an interpretation and inverse interpretation of formal logic, respectively. The theory is applied to a localisation problem to show a robot with broken or noisy sensors can efficiently solve the problem in a fully data-driven fashion.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20219;&#21153;&#23548;&#21521;&#36890;&#20449;&#20013;&#36890;&#36807;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#23545;&#26469;&#25191;&#34892;&#20219;&#21153;&#30340;&#20449;&#24687;&#26102;&#20195;&#38382;&#39064;&#65292;&#36890;&#36807;&#20449;&#36947;&#21033;&#29992;&#29575;&#30340;&#22686;&#21152;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#38656;&#35201;&#26356;&#38271;&#30340;&#26381;&#21153;&#26102;&#38388;&#65292;&#24341;&#20837;&#20219;&#21153;&#20449;&#24687;&#30340;&#26368;&#22823;&#26102;&#20195;&#65288;PAoTI&#65289;&#26469;&#23545;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#36827;&#34892;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2301.04298</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#39537;&#21160;&#30340;&#20219;&#21153;&#23548;&#21521;&#36890;&#20449;&#20013;&#30340;&#20449;&#24687;&#26102;&#20195;
&lt;/p&gt;
&lt;p&gt;
Age of Information in Deep Learning-Driven Task-Oriented Communications. (arXiv:2301.04298v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20219;&#21153;&#23548;&#21521;&#36890;&#20449;&#20013;&#36890;&#36807;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#23545;&#26469;&#25191;&#34892;&#20219;&#21153;&#30340;&#20449;&#24687;&#26102;&#20195;&#38382;&#39064;&#65292;&#36890;&#36807;&#20449;&#36947;&#21033;&#29992;&#29575;&#30340;&#22686;&#21152;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#38656;&#35201;&#26356;&#38271;&#30340;&#26381;&#21153;&#26102;&#38388;&#65292;&#24341;&#20837;&#20219;&#21153;&#20449;&#24687;&#30340;&#26368;&#22823;&#26102;&#20195;&#65288;PAoTI&#65289;&#26469;&#23545;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#36827;&#34892;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20219;&#21153;&#23548;&#21521;&#36890;&#20449;&#20013;&#30340;&#20449;&#24687;&#26102;&#20195;&#27010;&#24565;&#65292;&#26088;&#22312;&#21033;&#29992;&#21457;&#23556;&#26426;&#22788;&#30340;&#25968;&#25454;&#22312;&#25509;&#25910;&#22120;&#22788;&#25191;&#34892;&#20219;&#21153;&#12290;&#21457;&#23556;&#26426;&#21644;&#25509;&#25910;&#26426;&#20043;&#38388;&#30340;&#25805;&#20316;&#34987;&#24314;&#27169;&#20026;&#19968;&#20010;&#32852;&#21512;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#23545;&#65292;&#24182;&#32771;&#34385;&#20102;&#20449;&#36947;&#25928;&#24212;&#12290;&#32534;&#30721;&#22120;&#23558;&#25968;&#25454;&#26679;&#26412;&#36716;&#25442;&#20026;&#23567;&#32500;&#24230;&#30340;&#29305;&#24449;&#21521;&#37327;&#65292;&#24182;&#20351;&#29992;&#23569;&#37327;&#30340;&#20449;&#36947;&#29992;&#20110;&#20256;&#36755;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#20256;&#36755;&#27425;&#25968;&#21644;&#24310;&#36831;&#12290;&#35299;&#30721;&#22120;&#19981;&#20877;&#37325;&#26500;&#36755;&#20837;&#26679;&#26412;&#65292;&#32780;&#26159;&#23545;&#25509;&#25910;&#21040;&#30340;&#20449;&#21495;&#25191;&#34892;&#20219;&#21153;&#65292;&#20363;&#22914;&#20998;&#31867;&#12290;&#36890;&#36807;&#22312;MNIST&#21644;CIFAR-10&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#19981;&#21516;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#26174;&#31034;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#24615;&#38543;&#30528;&#20449;&#36947;&#21033;&#29992;&#29575;&#30340;&#22686;&#21152;&#32780;&#25552;&#39640;&#65292;&#20294;&#38656;&#35201;&#26356;&#38271;&#30340;&#26381;&#21153;&#26102;&#38388;&#12290;&#24341;&#20837;&#20219;&#21153;&#20449;&#24687;&#30340;&#26368;&#22823;&#26102;&#20195;&#65288;PAoTI&#65289;&#26469;&#20998;&#26512;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#26435;&#34913;&#30340;&#26102;&#26426;&#65292;&#24403;&#20449;&#24687;&#24180;&#40836;&#22686;&#38271;&#26102;&#65292;&#38500;&#38750;&#25509;&#25910;&#21040;&#30340;&#20449;&#21495;&#34987;&#27491;&#30830;&#20998;&#31867;&#12290;&#36890;&#36807;&#32467;&#21512;&#36890;&#36947;&#21644;&#28304;&#30340;&#24433;&#21709;&#23545;PAoTI&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;PAoTI&#19982;&#36890;&#36947;&#30340;&#24433;&#21709;&#36235;&#21183;&#19968;&#33268;&#65292;&#20294;&#27010;&#36848;&#35823;&#24046;&#21487;&#33021;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the notion of age in task-oriented communications that aims to execute a task at a receiver utilizing the data at its transmitter. The transmitter-receiver operations are modeled as an encoder-decoder pair that is jointly trained while considering channel effects. The encoder converts data samples into feature vectors of small dimension and transmits them with a small number of channel uses thereby reducing the number of transmissions and latency. Instead of reconstructing input samples, the decoder performs a task, e.g., classification, on the received signals. Applying different deep neural networks of encoder-decoder pairs on MNIST and CIFAR-10 image datasets, the classifier accuracy is shown to increase with the number of channel uses at the expense of longer service time. The peak age of task information (PAoTI) is introduced to analyze this accuracy-latency tradeoff when the age grows unless a received signal is classified correctly. By incorporating channel an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#31454;&#25293;&#30340;&#36164;&#28304;&#20998;&#37197;&#26426;&#21046;&#65292;&#22312;&#22810;&#31199;&#25143;O-RAN&#29983;&#24577;&#31995;&#32479;&#20013;&#26368;&#23567;&#21270;&#19981;&#21516;MNO&#31199;&#25143;&#31867;&#22411;&#30340;OPEX&#65292;&#21516;&#26102;&#20445;&#35777;&#20844;&#24179;&#12290;</title><link>http://arxiv.org/abs/2301.00597</link><description>&lt;p&gt;
&#20445;&#35777;&#20844;&#24179;&#30340;&#22522;&#20110;&#31454;&#25293;&#30340;x-haul&#21644;&#20113;&#36164;&#28304;&#20998;&#37197;&#26041;&#26696;&#22312;&#22810;&#31199;&#25143;O-RAN&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Fairness Guaranteed and Auction-based x-haul and Cloud Resource Allocation in Multi-tenant O-RANs. (arXiv:2301.00597v3 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#31454;&#25293;&#30340;&#36164;&#28304;&#20998;&#37197;&#26426;&#21046;&#65292;&#22312;&#22810;&#31199;&#25143;O-RAN&#29983;&#24577;&#31995;&#32479;&#20013;&#26368;&#23567;&#21270;&#19981;&#21516;MNO&#31199;&#25143;&#31867;&#22411;&#30340;OPEX&#65292;&#21516;&#26102;&#20445;&#35777;&#20844;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#24335;&#26080;&#32447;&#25509;&#20837;&#32593;&#32476;&#65288;O-RAN&#65289;&#37319;&#29992;&#20113;&#21270;&#21644;&#32593;&#32476;&#21151;&#33021;&#34394;&#25311;&#21270;&#65292;&#36890;&#36807;&#20998;&#31163;&#30340;&#26080;&#32447;&#30005;&#21333;&#20803;&#65288;RUs&#65289;&#12289;&#20998;&#24067;&#24335;&#21333;&#20803;&#65288;DUs&#65289;&#21644;&#38598;&#20013;&#24335;&#21333;&#20803;&#65288;CUs&#65289;&#36827;&#34892;&#22522;&#24102;&#21151;&#33021;&#22788;&#29702;&#12290;&#36825;&#20351;&#24471;&#20113;RAN&#24895;&#26223;&#25104;&#20026;&#29616;&#23454;&#65292;&#22810;&#20010;&#31227;&#21160;&#32593;&#32476;&#36816;&#33829;&#21830;&#65288;MNOs&#65289;&#21487;&#20197;&#23433;&#35013;&#19987;&#26377;&#25110;&#24320;&#25918;&#24335;&#30340;RUs&#65292;&#20294;&#26159;&#36890;&#36807;&#24320;&#25918;&#30340;x-haul&#25509;&#21475;&#20174;&#20844;&#20849;&#21487;&#29992;&#30340;&#24320;&#25918;&#20113;&#20013;&#31199;&#29992;&#25353;&#38656;&#30340;DU-CU&#35745;&#31639;&#36164;&#28304;&#12290;&#26412;&#25991;&#25552;&#20986;&#24182;&#27604;&#36739;&#20102;&#26368;&#23567;&#26368;&#22823;&#20844;&#24179;&#21644;Vickrey-Clarke-Groves&#65288;VCG&#65289;&#22522;&#20110;&#31454;&#25293;&#30340;x-haul&#21644;DU-CU&#36164;&#28304;&#20998;&#37197;&#26426;&#21046;&#30340;&#24615;&#33021;&#65292;&#20197;&#21019;&#24314;&#19968;&#20010;&#36866;&#29992;&#20110;&#23567;&#12289;&#20013;&#12289;&#22823;MNOs&#21487;&#25345;&#32493;&#21457;&#23637;&#30340;&#22810;&#31199;&#25143;O-RAN&#29983;&#24577;&#31995;&#32479;&#12290;&#26368;&#23567;&#26368;&#22823;&#20844;&#24179;&#26041;&#27861;&#36890;&#36807;&#25353;&#20854;&#38656;&#27714;&#27604;&#20363;&#36827;&#34892;&#25104;&#26412;&#20998;&#25674;&#65292;&#26368;&#23567;&#21270;RUs&#30340;&#26368;&#22823;OPEX&#65292;&#32780;&#22522;&#20110;VCG&#25293;&#21334;&#30340;&#26041;&#27861;&#21017;&#21487;&#26368;&#23567;&#21270;&#21033;&#29992;&#30340;&#25152;&#26377;&#36164;&#28304;&#30340;&#24635;OPEX&#65292;&#24182;&#20174;RUs&#20013;&#25552;&#21462;&#30495;&#23454;&#38656;&#27714;&#12290;&#25105;&#20204;&#27880;&#37322;&#20102;&#35780;&#20272;&#32467;&#26524;&#30340;&#20844;&#24179;&#25351;&#25968;&#65292;&#20197;&#30830;&#20445;&#22312;&#19981;&#21516;&#30340;MNO&#31199;&#25143;&#31867;&#22411;&#20043;&#38388;&#23454;&#29616;&#20844;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
The open-radio access network (O-RAN) embraces cloudification and network function virtualization for base-band function processing by dis-aggregated radio units (RUs), distributed units (DUs), and centralized units (CUs). These enable the cloud-RAN vision in full, where multiple mobile network operators (MNOs) can install their proprietary or open RUs, but lease on-demand computational resources for DU-CU functions from commonly available open-clouds via open x-haul interfaces. In this paper, we propose and compare the performances of min-max fairness and Vickrey-Clarke-Groves (VCG) auction-based x-haul and DU-CU resource allocation mechanisms to create a multi-tenant O-RAN ecosystem that is sustainable for small, medium, and large MNOs. The min-max fair approach minimizes the maximum OPEX of RUs through cost-sharing proportional to their demands, whereas the VCG auction-based approach minimizes the total OPEX for all resources utilized while extracting truthful demands from RUs. We c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#21453;&#39304;&#30340;&#31574;&#30053;&#36866;&#24212;&#65288;PAFF&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#35753;&#31574;&#30053;&#20351;&#29992;&#38543;&#26426;&#29983;&#25104;&#30340;&#25351;&#20196;&#36827;&#34892;&#28436;&#31034;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#25552;&#20379;&#21453;&#39304;&#26469;&#37325;&#26032;&#26631;&#35760;&#28436;&#31034;&#65292;&#33258;&#21160;&#25552;&#20379;&#26032;&#30340;&#28436;&#31034;-&#25351;&#20196;&#25968;&#25454;&#23545;&#36827;&#34892;&#31574;&#30053;&#24494;&#35843;&#65292;&#20197;&#23454;&#29616;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#27867;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PAFF&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.07398</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#21453;&#39304;&#30340;&#31574;&#30053;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Policy Adaptation from Foundation Model Feedback. (arXiv:2212.07398v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#21453;&#39304;&#30340;&#31574;&#30053;&#36866;&#24212;&#65288;PAFF&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#35753;&#31574;&#30053;&#20351;&#29992;&#38543;&#26426;&#29983;&#25104;&#30340;&#25351;&#20196;&#36827;&#34892;&#28436;&#31034;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#25552;&#20379;&#21453;&#39304;&#26469;&#37325;&#26032;&#26631;&#35760;&#28436;&#31034;&#65292;&#33258;&#21160;&#25552;&#20379;&#26032;&#30340;&#28436;&#31034;-&#25351;&#20196;&#25968;&#25454;&#23545;&#36827;&#34892;&#31574;&#30053;&#24494;&#35843;&#65292;&#20197;&#23454;&#29616;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#27867;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PAFF&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#26041;&#38754;&#30340;&#36827;&#23637;&#20026;&#26500;&#24314;&#36890;&#29992;&#26426;&#22120;&#20154;&#24102;&#26469;&#20102;&#26174;&#33879;&#36827;&#27493;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#23558;&#22330;&#26223;&#21644;&#25351;&#20196;&#32534;&#30721;&#20026;&#20915;&#31574;&#36755;&#20837;&#65292;&#25351;&#20196;&#26465;&#20214;&#21270;&#31574;&#30053;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#23545;&#35937;&#21644;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#27867;&#21270;&#12290;&#23613;&#31649;&#36825;&#26159;&#20196;&#20154;&#40723;&#33310;&#30340;&#65292;&#20294;&#31574;&#30053;&#22312;&#36935;&#21040;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#25110;&#29615;&#22659;&#26102;&#20173;&#28982;&#22833;&#36133;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#21453;&#39304;&#30340;&#31574;&#30053;&#36866;&#24212;&#65288;PAFF&#65289;&#12290;&#24403;&#23558;&#35757;&#32451;&#22909;&#30340;&#31574;&#30053;&#37096;&#32626;&#21040;&#26032;&#20219;&#21153;&#25110;&#26032;&#29615;&#22659;&#26102;&#65292;&#25105;&#20204;&#39318;&#20808;&#35753;&#31574;&#30053;&#20351;&#29992;&#38543;&#26426;&#29983;&#25104;&#30340;&#25351;&#20196;&#36827;&#34892;&#28436;&#31034;&#12290;&#34429;&#28982;&#25191;&#34892;&#21487;&#33021;&#20986;&#29616;&#38169;&#35823;&#65292;&#20294;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#25552;&#20379;&#21453;&#39304;&#26469;&#37325;&#26032;&#26631;&#35760;&#28436;&#31034;&#12290;&#36825;&#33258;&#21160;&#20026;&#31574;&#30053;&#24494;&#35843;&#25552;&#20379;&#20102;&#26032;&#30340;&#28436;&#31034;-&#25351;&#20196;&#25968;&#25454;&#23545;&#12290;&#25105;&#20204;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#35774;&#32622;&#20013;&#36827;&#34892;&#20102;&#21508;&#31181;&#23454;&#39564;&#30340;&#35780;&#20272;&#65292;&#37325;&#28857;&#26159;&#22312;&#26410;&#35265;&#36807;&#30340;&#23545;&#35937;&#12289;&#20219;&#21153;&#21644;&#26410;&#35266;&#23519;&#21040;&#30340;&#29615;&#22659;&#20013;&#30340;&#27867;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PAFF&#22312;&#26368;&#32456;&#20219;&#21153;&#25104;&#21151;&#29575;&#21644;&#35757;&#32451;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress on vision-language foundation models have brought significant advancement to building general-purpose robots. By using the pre-trained models to encode the scene and instructions as inputs for decision making, the instruction-conditioned policy can generalize across different objects and tasks. While this is encouraging, the policy still fails in most cases given an unseen task or environment. In this work, we propose Policy Adaptation from Foundation model Feedback (PAFF). When deploying the trained policy to a new task or a new environment, we first let the policy play with randomly generated instructions to record the demonstrations. While the execution could be wrong, we can use the pre-trained foundation models to provide feedback to relabel the demonstrations. This automatically provides new pairs of demonstration-instruction data for policy fine-tuning. We evaluate our method on a broad range of experiments with the focus on generalization on unseen objects, unse
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HetMed&#30340;&#24322;&#26500;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#34701;&#21512;&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#20020;&#24202;&#20915;&#31574;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.15158</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#20998;&#26512;&#30340;&#24322;&#26500;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Graph Learning for Multi-modal Medical Data Analysis. (arXiv:2211.15158v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15158
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HetMed&#30340;&#24322;&#26500;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#34701;&#21512;&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#20020;&#24202;&#20915;&#31574;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30149;&#20154;&#30340;&#24120;&#35268;&#20020;&#24202;&#35775;&#38382;&#19981;&#20165;&#20250;&#20135;&#29983;&#22270;&#20687;&#25968;&#25454;&#65292;&#36824;&#20250;&#21253;&#21547;&#26377;&#20851;&#30149;&#20154;&#30340;&#20020;&#24202;&#20449;&#24687;&#31561;&#38750;&#22270;&#20687;&#25968;&#25454;&#65292;&#21363;&#21307;&#23398;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#24615;&#36136;&#12290;&#36825;&#26679;&#30340;&#24322;&#26500;&#27169;&#24577;&#25552;&#20379;&#20102;&#19981;&#21516;&#21644;&#20114;&#34917;&#30340;&#30149;&#20154;&#35270;&#35282;&#65292;&#24403;&#23427;&#20204;&#34987;&#27491;&#30830;&#22320;&#32452;&#21512;&#26102;&#65292;&#21487;&#20197;&#23548;&#33268;&#26356;&#20934;&#30830;&#30340;&#20020;&#24202;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#65292;&#22914;&#20309;&#23558;&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#26377;&#25928;&#22320;&#34701;&#21512;&#21040;&#32479;&#19968;&#26694;&#26550;&#20013;&#21364;&#21463;&#21040;&#20102;&#30456;&#23545;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HetMed&#65288;&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#20998;&#26512;&#30340;&#24322;&#26500;&#22270;&#23398;&#20064;&#65289;&#30340;&#26377;&#25928;&#22270;&#24418;&#26694;&#26550;&#65292;&#29992;&#20110;&#34701;&#21512;&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#19968;&#20010;&#21253;&#25324;&#22810;&#31181;&#30149;&#20154;&#38750;&#22270;&#20687;&#29305;&#24449;&#30340;&#22810;&#37325;&#32593;&#32476;&#65292;&#20197;&#31995;&#32479;&#21270;&#26041;&#24335;&#25429;&#33719;&#30149;&#20154;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#20174;&#32780;&#23548;&#33268;&#26356;&#20934;&#30830;&#30340;&#20020;&#24202;&#20915;&#31574;&#12290;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Routine clinical visits of a patient produce not only image data, but also non-image data containing clinical information regarding the patient, i.e., medical data is multi-modal in nature. Such heterogeneous modalities offer different and complementary perspectives on the same patient, resulting in more accurate clinical decisions when they are properly combined. However, despite its significance, how to effectively fuse the multi-modal medical data into a unified framework has received relatively little attention. In this paper, we propose an effective graph-based framework called HetMed (Heterogeneous Graph Learning for Multi-modal Medical Data Analysis) for fusing the multi-modal medical data. Specifically, we construct a multiplex network that incorporates multiple types of non-image features of patients to capture the complex relationship between patients in a systematic way, which leads to more accurate clinical decisions. Extensive experiments on various real-world datasets dem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#32806;&#21407;&#22411;&#32593;&#32476;&#65288;DPN&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#20108;&#20998;&#22270;&#21305;&#37197;&#38382;&#39064;&#20998;&#31163;&#24050;&#30693;&#21644;&#26032;&#31867;&#21035;&#26469;&#26377;&#25928;&#22320;&#23454;&#29616;&#19981;&#21516;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#24182;&#23558;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#30340;&#24050;&#30693;&#31867;&#21035;&#23545;&#40784;&#65292;&#20197;&#26174;&#24335;&#36716;&#31227;&#31867;&#21035;&#29305;&#23450;&#30340;&#30693;&#35782;&#21644;&#25429;&#33719;&#39640;&#32423;&#35821;&#20041;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2211.15115</link><description>&lt;p&gt;
&#22522;&#20110;&#35299;&#32806;&#21407;&#22411;&#32593;&#32476;&#30340;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Generalized Category Discovery with Decoupled Prototypical Network. (arXiv:2211.15115v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#32806;&#21407;&#22411;&#32593;&#32476;&#65288;DPN&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#20108;&#20998;&#22270;&#21305;&#37197;&#38382;&#39064;&#20998;&#31163;&#24050;&#30693;&#21644;&#26032;&#31867;&#21035;&#26469;&#26377;&#25928;&#22320;&#23454;&#29616;&#19981;&#21516;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#24182;&#23558;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#30340;&#24050;&#30693;&#31867;&#21035;&#23545;&#40784;&#65292;&#20197;&#26174;&#24335;&#36716;&#31227;&#31867;&#21035;&#29305;&#23450;&#30340;&#30693;&#35782;&#21644;&#25429;&#33719;&#39640;&#32423;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#65288;GCD&#65289;&#30340;&#30446;&#26631;&#26159;&#20174;&#19968;&#32452;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#35782;&#21035;&#24050;&#30693;&#21644;&#26032;&#30340;&#31867;&#21035;&#65292;&#22522;&#20110;&#21478;&#19968;&#20010;&#20165;&#24102;&#26377;&#24050;&#30693;&#31867;&#21035;&#30340;&#25968;&#25454;&#38598;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#24573;&#30053;&#24050;&#30693;&#21644;&#26032;&#31867;&#21035;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#22312;&#32806;&#21512;&#30340;&#26041;&#24335;&#19979;&#23398;&#20064;&#23427;&#20204;&#65292;&#36825;&#21487;&#33021;&#20250;&#25439;&#23475;&#27169;&#22411;&#30340;&#27867;&#21270;&#21644;&#21306;&#20998;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#32806;&#21512;&#35757;&#32451;&#26041;&#27861;&#38459;&#27490;&#36825;&#20123;&#27169;&#22411;&#26174;&#24335;&#22320;&#20174;&#26631;&#35760;&#25968;&#25454;&#21521;&#26410;&#26631;&#35760;&#25968;&#25454;&#36716;&#31227;&#31867;&#21035;&#29305;&#23450;&#30340;&#30693;&#35782;&#65292;&#36825;&#21487;&#33021;&#20250;&#20002;&#22833;&#39640;&#32423;&#35821;&#20041;&#20449;&#24687;&#24182;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#20102;&#20943;&#36731;&#19978;&#36848;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#31216;&#20026;&#35299;&#32806;&#21407;&#22411;&#32593;&#32476;&#65288;DPN&#65289;&#12290;&#36890;&#36807;&#20026;&#31867;&#21035;&#21407;&#22411;&#21046;&#23450;&#19968;&#20010;&#20108;&#20998;&#22270;&#21305;&#37197;&#38382;&#39064;&#65292;DPN&#19981;&#20165;&#21487;&#20197;&#35299;&#32806;&#24050;&#30693;&#21644;&#26032;&#30340;&#31867;&#21035;&#20197;&#26377;&#25928;&#22320;&#23454;&#29616;&#19981;&#21516;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#23558;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#30340;&#24050;&#30693;&#31867;&#21035;&#23545;&#40784;&#65292;&#20197;&#26174;&#24335;&#22320;&#36716;&#31227;&#31867;&#21035;&#29305;&#23450;&#30340;&#30693;&#35782;&#24182;&#25429;&#33719;&#39640;&#27700;&#24179;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalized Category Discovery (GCD) aims to recognize both known and novel categories from a set of unlabeled data, based on another dataset labeled with only known categories. Without considering differences between known and novel categories, current methods learn about them in a coupled manner, which can hurt model's generalization and discriminative ability. Furthermore, the coupled training approach prevents these models transferring category-specific knowledge explicitly from labeled data to unlabeled data, which can lose high-level semantic information and impair model performance. To mitigate above limitations, we present a novel model called Decoupled Prototypical Network (DPN). By formulating a bipartite matching problem for category prototypes, DPN can not only decouple known and novel categories to achieve different training targets effectively, but also align known categories in labeled and unlabeled data to transfer category-specific knowledge explicitly and capture high
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35265;&#24615;&#24341;&#23548;&#27969;&#27169;&#22359;&#30340;&#27969;&#32593;&#32476;&#27169;&#22411;VGFlow&#65292;&#21487;&#20197;&#20998;&#31163;&#20986;&#30446;&#26631;&#30340;&#21487;&#35265;&#21644;&#19981;&#21487;&#35265;&#37096;&#20998;&#20197;&#23454;&#29616;&#32441;&#29702;&#20445;&#30041;&#21644;&#39118;&#26684;&#25805;&#20316;&#65292;&#21516;&#26102;&#37319;&#29992;&#20102;&#22810;&#36335;&#24452;&#32467;&#26500;&#20197;&#22312;&#19981;&#21516;&#32423;&#21035;&#30340;&#32454;&#33410;&#19978;&#25805;&#20316;&#12290;&#22312;&#29983;&#25104;&#36924;&#30495;&#20154;&#20307;&#23039;&#21183;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.08540</link><description>&lt;p&gt;
VGFlow: &#38024;&#23545;&#20154;&#20307;&#37325;&#23450;&#20301;&#30340;&#21487;&#35265;&#24615;&#24341;&#23548;&#27969;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
VGFlow: Visibility guided Flow Network for Human Reposing. (arXiv:2211.08540v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08540
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35265;&#24615;&#24341;&#23548;&#27969;&#27169;&#22359;&#30340;&#27969;&#32593;&#32476;&#27169;&#22411;VGFlow&#65292;&#21487;&#20197;&#20998;&#31163;&#20986;&#30446;&#26631;&#30340;&#21487;&#35265;&#21644;&#19981;&#21487;&#35265;&#37096;&#20998;&#20197;&#23454;&#29616;&#32441;&#29702;&#20445;&#30041;&#21644;&#39118;&#26684;&#25805;&#20316;&#65292;&#21516;&#26102;&#37319;&#29992;&#20102;&#22810;&#36335;&#24452;&#32467;&#26500;&#20197;&#22312;&#19981;&#21516;&#32423;&#21035;&#30340;&#32454;&#33410;&#19978;&#25805;&#20316;&#12290;&#22312;&#29983;&#25104;&#36924;&#30495;&#20154;&#20307;&#23039;&#21183;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#37325;&#23450;&#20301;&#30340;&#20219;&#21153;&#28041;&#21450;&#29983;&#25104;&#19968;&#20010;&#31449;&#31435;&#22312;&#20219;&#24847;&#26500;&#24819;&#23039;&#21183;&#19979;&#30340;&#30495;&#23454;&#20154;&#29289;&#22270;&#20687;&#12290;&#29983;&#25104;&#24863;&#30693;&#20934;&#30830;&#22270;&#20687;&#23384;&#22312;&#22810;&#20010;&#22256;&#38590;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#20445;&#30041;&#32441;&#29702;&#12289;&#32500;&#25345;&#22270;&#26696;&#19968;&#33268;&#24615;&#12289;&#32771;&#34385;&#26381;&#35013;&#36793;&#30028;&#12289;&#22788;&#29702;&#36974;&#25377;&#12289;&#35843;&#25972;&#30382;&#32932;&#29983;&#25104;&#31561;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#36825;&#20123;&#22256;&#38590;&#36824;&#22240;&#21487;&#33021;&#30340;&#20154;&#20307;&#23039;&#21183;&#26041;&#21521;&#31354;&#38388;&#24040;&#22823;&#19988;&#22810;&#21464;&#12289;&#26381;&#35013;&#29289;&#21697;&#30340;&#26412;&#36136;&#39640;&#24230;&#38750;&#21018;&#24615;&#12289;&#36523;&#20307;&#24418;&#29366;&#30340;&#22810;&#26679;&#24615;&#22823;&#22823;&#19981;&#21516;&#20110;&#20154;&#21475;&#20013;&#30340;&#22810;&#26679;&#24615;&#32780;&#36827;&#19968;&#27493;&#24694;&#21270;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#22256;&#38590;&#24182;&#21512;&#25104;&#24863;&#30693;&#20934;&#30830;&#30340;&#22270;&#20687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VGFlow&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#21487;&#35265;&#24615;&#24341;&#23548;&#27969;&#27169;&#22359;&#23558;&#27969;&#20998;&#31163;&#20026;&#30446;&#26631;&#30340;&#21487;&#35265;&#21644;&#19981;&#21487;&#35265;&#37096;&#20998;&#65292;&#20197;&#23454;&#29616;&#21516;&#26102;&#32441;&#29702;&#20445;&#30041;&#21644;&#39118;&#26684;&#25805;&#20316;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#19981;&#21516;&#30340;&#36523;&#20307;&#24418;&#29366;&#21644;&#36991;&#20813;&#32593;&#32476;&#20266;&#24433;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#19968;&#20010;&#22810;&#36335;&#24452;&#32467;&#26500;&#65292;&#21253;&#25324;&#20840;&#23616;&#32467;&#26500;&#36335;&#24452;&#21644;&#22810;&#20010;&#23616;&#37096;&#32454;&#33410;&#36335;&#24452;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#32423;&#21035;&#30340;&#32454;&#33410;&#19978;&#36827;&#34892;&#25805;&#20316;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#22312;&#29983;&#25104;&#20855;&#26377;&#31934;&#32454;&#32454;&#33410;&#21644;&#20934;&#30830;&#32441;&#29702;&#30340;&#36924;&#30495;&#20154;&#20307;&#23039;&#21183;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of human reposing involves generating a realistic image of a person standing in an arbitrary conceivable pose. There are multiple difficulties in generating perceptually accurate images, and existing methods suffer from limitations in preserving texture, maintaining pattern coherence, respecting cloth boundaries, handling occlusions, manipulating skin generation, etc. These difficulties are further exacerbated by the fact that the possible space of pose orientation for humans is large and variable, the nature of clothing items is highly non-rigid, and the diversity in body shape differs largely among the population. To alleviate these difficulties and synthesize perceptually accurate images, we propose VGFlow. Our model uses a visibility-guided flow module to disentangle the flow into visible and invisible parts of the target for simultaneous texture preservation and style manipulation. Furthermore, to tackle distinct body shapes and avoid network artifacts, we also incorporat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#29983;&#29289;&#21551;&#31034;&#30340;&#26465;&#20214;&#26102;&#38388;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(BI-CTVAE)&#27169;&#22411;&#65292;&#36890;&#36807;&#25345;&#32493;&#23398;&#20064;&#29983;&#25104;(CL2Gen)&#22330;&#26223;&#65292;&#21487;&#20197;&#23545;&#19981;&#21516;&#31867;&#21035;&#30340;&#36816;&#21160;&#24207;&#21015;&#36827;&#34892;&#29983;&#25104;&#65292;&#24182;&#22312;&#19968;&#32452;&#20219;&#21153;&#19978;&#24471;&#21040;&#36739;&#39640;&#30340;&#29983;&#25104;&#20934;&#30830;&#24615;&#21644;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.05231</link><description>&lt;p&gt;
&#21463;&#29983;&#29289;&#21551;&#31034;&#30340;&#20154;&#31867;&#36816;&#21160;&#24207;&#21015;&#30340;&#25345;&#32493;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Biologically-Inspired Continual Learning of Human Motion Sequences. (arXiv:2211.05231v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#29983;&#29289;&#21551;&#31034;&#30340;&#26465;&#20214;&#26102;&#38388;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(BI-CTVAE)&#27169;&#22411;&#65292;&#36890;&#36807;&#25345;&#32493;&#23398;&#20064;&#29983;&#25104;(CL2Gen)&#22330;&#26223;&#65292;&#21487;&#20197;&#23545;&#19981;&#21516;&#31867;&#21035;&#30340;&#36816;&#21160;&#24207;&#21015;&#36827;&#34892;&#29983;&#25104;&#65292;&#24182;&#22312;&#19968;&#32452;&#20219;&#21153;&#19978;&#24471;&#21040;&#36739;&#39640;&#30340;&#29983;&#25104;&#20934;&#30830;&#24615;&#21644;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25345;&#32493;&#23398;&#20064;&#27169;&#22411;&#65292;&#19987;&#27880;&#20110;&#28041;&#21450;&#26102;&#38388;&#24207;&#21015;&#8212;&#8212;&#20855;&#20307;&#32780;&#35328;&#65292;&#26159;&#20154;&#30340;&#36816;&#21160;&#12290;&#35813;&#27169;&#22411;&#25913;&#36827;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#31867;&#20284;&#20110;&#22823;&#33041;&#30340;&#37325;&#25918;&#27169;&#22411;(BI-R)&#65292;&#23427;&#24314;&#31435;&#20102;&#19968;&#20010;&#21463;&#29983;&#29289;&#21551;&#31034;&#30340;&#26465;&#20214;&#26102;&#38388;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(BI-CTVAE)&#65292;&#20854;&#23454;&#20363;&#21270;&#20102;&#19968;&#20010;&#39640;&#26031;&#20989;&#25968;&#28151;&#21512;&#20307;&#26469;&#34920;&#31034;&#31867;&#21035;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25345;&#32493;&#23398;&#20064;&#29983;&#25104;(CL2Gen)&#22330;&#26223;&#65292;&#20854;&#20013;&#27169;&#22411;&#29983;&#25104;&#19981;&#21516;&#31867;&#21035;&#30340;&#36816;&#21160;&#24207;&#21015;&#12290;&#27169;&#22411;&#30340;&#29983;&#25104;&#20934;&#30830;&#24615;&#22312;&#19968;&#32452;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#22312;&#25353;&#39034;&#24207;&#23398;&#20064;&#25152;&#26377;&#21160;&#20316;&#31867;&#21035;&#20043;&#21518;&#65292;BI-CTVAE&#22312;&#19968;&#20010;&#20154;&#31867;&#36816;&#21160;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#32456;&#20998;&#31867;&#20934;&#30830;&#24615;&#36798;&#21040;&#20102;78&#65285;&#65292;&#27604;&#19981;&#20351;&#29992;&#37325;&#25918;&#39640;63&#65285;&#65292;&#27604;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;&#35757;&#32451;GRU&#27169;&#22411;&#20302;5.4&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes a model for continual learning on tasks involving temporal sequences, specifically, human motions. It improves on a recently proposed brain-inspired replay model (BI-R) by building a biologically-inspired conditional temporal variational autoencoder (BI-CTVAE), which instantiates a latent mixture-of-Gaussians for class representation. We investigate a novel continual-learning-to-generate (CL2Gen) scenario where the model generates motion sequences of different classes. The generative accuracy of the model is tested over a set of tasks. The final classification accuracy of BI-CTVAE on a human motion dataset after sequentially learning all action classes is 78%, which is 63% higher than using no-replay, and only 5.4% lower than a state-of-the-art offline trained GRU model.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#24577;&#36923;&#36753;&#30340;&#24418;&#24335;&#35821;&#35328;&#65292;&#29992;&#20110;&#25551;&#36848;&#21644;&#35299;&#37322;&#32479;&#35745;&#22240;&#26524;&#20851;&#31995;&#65292;&#24182;&#19988;&#33021;&#22815;&#25351;&#23450;&#21644;&#35299;&#37322;&#32479;&#35745;&#22240;&#26524;&#25512;&#26029;&#30340;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.16751</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#24577;&#36923;&#36753;&#30340;&#32479;&#35745;&#22240;&#26524;&#20851;&#31995;&#24418;&#24335;&#21270;
&lt;/p&gt;
&lt;p&gt;
Formalizing Statistical Causality via Modal Logic. (arXiv:2210.16751v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16751
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#24577;&#36923;&#36753;&#30340;&#24418;&#24335;&#35821;&#35328;&#65292;&#29992;&#20110;&#25551;&#36848;&#21644;&#35299;&#37322;&#32479;&#35745;&#22240;&#26524;&#20851;&#31995;&#65292;&#24182;&#19988;&#33021;&#22815;&#25351;&#23450;&#21644;&#35299;&#37322;&#32479;&#35745;&#22240;&#26524;&#25512;&#26029;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25551;&#36848;&#21644;&#35299;&#37322;&#32479;&#35745;&#22240;&#26524;&#20851;&#31995;&#30340;&#24418;&#24335;&#35821;&#35328;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#32479;&#35745;&#22240;&#26524;&#35821;&#35328;&#65288;StaCL&#65289;&#65292;&#29992;&#20110;&#34920;&#36798;&#38543;&#26426;&#21464;&#37327;&#19978;&#30340;&#22240;&#26524;&#25928;&#24212;&#24182;&#25351;&#23450;&#22240;&#26524;&#25512;&#26029;&#30340;&#35201;&#27714;&#12290;StaCL&#36890;&#36807;&#24178;&#39044;&#30340;&#27169;&#24577;&#36816;&#31639;&#31526;&#65292;&#22312;&#19981;&#21516;&#21487;&#33021;&#30340;&#19990;&#30028;&#30340;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#34920;&#36798;&#22240;&#26524;&#23646;&#24615;&#65292;&#22312;Kripke&#27169;&#22411;&#20013;&#35299;&#37322;&#12290;&#25105;&#20204;&#20351;&#29992;StaCL&#20844;&#24335;&#27491;&#24335;&#21270;&#27010;&#29575;&#20998;&#24067;&#12289;&#24178;&#39044;&#21644;&#22240;&#26524;&#35859;&#35789;&#30340;&#20844;&#29702;&#12290;&#36825;&#20123;&#20844;&#29702;&#36275;&#22815;&#34920;&#36798;Pearl&#30340;do-calculus&#35268;&#21017;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#31034;&#20363;&#35777;&#26126;&#20102;StaCL&#21487;&#20197;&#29992;&#20110;&#25351;&#23450;&#21644;&#35299;&#37322;&#32479;&#35745;&#22240;&#26524;&#25512;&#26029;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a formal language for describing and explaining statistical causality. Concretely, we define Statistical Causality Language (StaCL) for expressing causal effects on random variables and specifying the requirements for causal inference. StaCL incorporates modal operators for interventions to express causal properties between probability distributions in different possible worlds in a Kripke model. We formalize axioms for probability distributions, interventions, and causal predicates using StaCL formulas. These axioms are expressive enough to derive the rules of Pearl's do-calculus. Finally, we demonstrate by examples that StaCL can be used to specify and explain the correctness of statistical causal inference.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20381;&#36182;&#22686;&#24378;&#30340;&#33258;&#36866;&#24212;&#34701;&#21512;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#23427;&#23558;&#20381;&#36182;&#20449;&#24687;&#19982;&#21407;&#22987;&#35821;&#20041;&#20449;&#21495;&#33258;&#36866;&#24212;&#34701;&#21512;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#22797;&#26434;&#30340;&#35821;&#20041;&#21305;&#37197;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2210.08471</link><description>&lt;p&gt;
&#36890;&#36807;&#32467;&#26500;&#22686;&#24378;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#33258;&#36866;&#24212;&#34701;&#21512;&#25552;&#39640;&#35821;&#20041;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Improving Semantic Matching through Dependency-Enhanced Pre-trained Model with Adaptive Fusion. (arXiv:2210.08471v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20381;&#36182;&#22686;&#24378;&#30340;&#33258;&#36866;&#24212;&#34701;&#21512;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#23427;&#23558;&#20381;&#36182;&#20449;&#24687;&#19982;&#21407;&#22987;&#35821;&#20041;&#20449;&#21495;&#33258;&#36866;&#24212;&#34701;&#21512;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#22797;&#26434;&#30340;&#35821;&#20041;&#21305;&#37197;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22914;BERT&#65292;&#22312;&#35821;&#20041;&#21477;&#23376;&#21305;&#37197;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#12290;&#21516;&#26102;&#65292;&#20381;&#36182;&#24615;&#20808;&#39564;&#30693;&#35782;&#22312;&#22810;&#20010;NLP&#20219;&#21153;&#20013;&#20063;&#26174;&#31034;&#20986;&#26222;&#36941;&#30340;&#30410;&#22788;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#23558;&#20381;&#36182;&#24615;&#20808;&#39564;&#32467;&#26500;&#26377;&#25928;&#22320;&#38598;&#25104;&#21040;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#22797;&#26434;&#30340;&#35821;&#20041;&#21305;&#37197;&#20851;&#31995;&#65292;&#20173;&#26410;&#30830;&#23450;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DAFA&#30340;&#20381;&#36182;&#22686;&#24378;&#33258;&#36866;&#24212;&#34701;&#21512;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#36825;&#23558;&#20381;&#36182;&#32467;&#26500;&#26126;&#30830;&#22320;&#24341;&#20837;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#21040;&#35821;&#20041;&#20449;&#24687;&#20013;&#12290;&#20855;&#20307;&#22320;&#65292;DAFA&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#25935;&#24863;&#33539;&#24335;&#26469;&#26500;&#24314;&#19968;&#20010;&#20381;&#36182;&#30697;&#38453;&#65292;&#20197;&#26657;&#20934;&#27880;&#24847;&#21147;&#26435;&#37325;&#12290;&#23427;&#37319;&#29992;&#33258;&#36866;&#24212;&#34701;&#21512;&#27169;&#22359;&#26469;&#38598;&#25104;&#33719;&#21462;&#30340;&#20381;&#36182;&#20449;&#24687;&#21644;&#21407;&#22987;&#35821;&#20041;&#20449;&#21495;&#12290;&#27492;&#22806;&#65292;DAFA&#37325;&#26500;&#20102;&#27880;&#24847;&#21147;&#35745;&#31639;&#27969;&#31243;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based pre-trained models like BERT have achieved great progress on Semantic Sentence Matching. Meanwhile, dependency prior knowledge has also shown general benefits in multiple NLP tasks. However, how to efficiently integrate dependency prior structure into pre-trained models to better model complex semantic matching relations is still unsettled. In this paper, we propose the \textbf{D}ependency-Enhanced \textbf{A}daptive \textbf{F}usion \textbf{A}ttention (\textbf{DAFA}), which explicitly introduces dependency structure into pre-trained models and adaptively fuses it with semantic information. Specifically, \textbf{\emph{(i)}} DAFA first proposes a structure-sensitive paradigm to construct a dependency matrix for calibrating attention weights. It adopts an adaptive fusion module to integrate the obtained dependency information and the original semantic signals. Moreover, DAFA reconstructs the attention calculation flow and provides better interpretability. By applying it o
&lt;/p&gt;</description></item><item><title>MAPL&#20351;&#29992;&#23545;&#40784;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#23398;&#20064;&#21333;&#27169;&#24577;&#27169;&#22411;&#34920;&#31034;&#31354;&#38388;&#20043;&#38388;&#30340;&#36731;&#37327;&#32423;&#26144;&#23556;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38754;&#21521;&#35270;&#35273;-&#35821;&#35328;&#23569;&#26679;&#26412;&#20219;&#21153;&#30340;&#22522;&#20110;&#21442;&#25968;&#25928;&#29575;&#30340;&#36866;&#24212;&#65292;&#24182;&#22312;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2210.07179</link><description>&lt;p&gt;
MAPL: &#22522;&#20110;&#21442;&#25968;&#25928;&#29575;&#30340;&#21333;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#35270;&#35273;-&#35821;&#35328;&#23569;&#26679;&#26412;&#20219;&#21153;&#20013;&#30340;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
MAPL: Parameter-Efficient Adaptation of Unimodal Pre-Trained Models for Vision-Language Few-Shot Prompting. (arXiv:2210.07179v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07179
&lt;/p&gt;
&lt;p&gt;
MAPL&#20351;&#29992;&#23545;&#40784;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#23398;&#20064;&#21333;&#27169;&#24577;&#27169;&#22411;&#34920;&#31034;&#31354;&#38388;&#20043;&#38388;&#30340;&#36731;&#37327;&#32423;&#26144;&#23556;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38754;&#21521;&#35270;&#35273;-&#35821;&#35328;&#23569;&#26679;&#26412;&#20219;&#21153;&#30340;&#22522;&#20110;&#21442;&#25968;&#25928;&#29575;&#30340;&#36866;&#24212;&#65292;&#24182;&#22312;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21333;&#27169;&#24577;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#65292;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#20986;&#33394;&#30340;&#38646;&#26679;&#26412;&#21644;&#65288;&#22522;&#20110;&#25552;&#31034;&#30340;&#65289;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MAPL&#65292;&#19968;&#31181;&#31616;&#21333;&#19988;&#21442;&#25968;&#25928;&#29575;&#39640;&#30340;&#26041;&#27861;&#65292;&#23427;&#37325;&#29992;&#20923;&#32467;&#30340;&#21333;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#22312;&#22810;&#27169;&#24577;&#35270;&#35273;-&#35821;&#35328;&#65288;VL&#65289;&#22330;&#26223;&#20013;&#30340;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#12290;MAPL&#20351;&#29992;&#23545;&#40784;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#23398;&#20064;&#20102;&#21333;&#27169;&#24577;&#27169;&#22411;&#34920;&#31034;&#31354;&#38388;&#20043;&#38388;&#30340;&#36731;&#37327;&#32423;&#26144;&#23556;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#20165;&#26377;&#23569;&#37327;&#19978;&#19979;&#25991;&#31034;&#20363;&#23601;&#25512;&#24191;&#21040;&#30475;&#19981;&#35265;&#30340;VL&#20219;&#21153;&#12290;MAPL&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#24456;&#23569;&#65292;&#20351;&#24471;&#23427;&#22312;&#20302;&#25968;&#25454;&#21644;&#22495;&#20869;&#23398;&#20064;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;MAPL&#30340;&#27169;&#22359;&#21270;&#20351;&#24471;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#20854;&#20182;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#22312;&#20960;&#20010;&#35270;&#35273;&#38382;&#31572;&#21644;&#22270;&#20687;&#26631;&#39064;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;MAPL&#30456;&#23545;&#20110;&#31867;&#20284;&#26041;&#27861;&#22312;&#35757;&#32451;&#23569;&#24471;&#22810;&#30340;&#21442;&#25968;&#26102;&#23454;&#29616;&#20102;&#20248;&#36234;&#25110;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;MAPL&#21487;&#20197;&#22312;&#20960;&#23567;&#26102;&#20869;&#20351;&#29992;&#36866;&#24230;&#30340;&#35745;&#31639;&#36164;&#28304;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained models have proved to be remarkable zero- and (prompt-based) few-shot learners in unimodal vision and language tasks. We propose MAPL, a simple and parameter-efficient method that reuses frozen pre-trained unimodal models and leverages their strong generalization capabilities in multimodal vision-language (VL) settings. MAPL learns a lightweight mapping between the representation spaces of unimodal models using aligned image-text data, and can generalize to unseen VL tasks from just a few in-context examples. The small number of trainable parameters makes MAPL effective at low-data and in-domain learning. Moreover, MAPL's modularity enables easy extension to other pre-trained models. Extensive experiments on several visual question answering and image captioning benchmarks show that MAPL achieves superior or competitive performance compared to similar methods while training orders of magnitude fewer parameters. MAPL can be trained in just a few hours using modest comp
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#22256;&#24785;&#24230;&#25351;&#26631;&#35780;&#20272;&#29983;&#25104;&#25991;&#26412;&#36136;&#37327;&#26159;&#19981;&#21487;&#38752;&#30340;&#65292;&#30001;&#20110;&#30701;&#25991;&#26412; PPL &#20540;&#39640;&#20110;&#38271;&#25991;&#26412;&#65292;&#37325;&#22797;&#25991;&#26412;&#27573;&#33853;&#21644;&#26631;&#28857;&#31526;&#21495;&#20063;&#21487;&#20197;&#25439;&#22351;&#25351;&#26631;&#34920;&#29616;&#12290; &#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#25991;&#26412;&#36136;&#37327;&#24212;&#35880;&#24910;&#12290;</title><link>http://arxiv.org/abs/2210.05892</link><description>&lt;p&gt;
PLM &#22256;&#24785;&#24230;&#19981;&#21487;&#38752;&#65292;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Perplexity from PLM Is Unreliable for Evaluating Text Quality. (arXiv:2210.05892v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05892
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#22256;&#24785;&#24230;&#25351;&#26631;&#35780;&#20272;&#29983;&#25104;&#25991;&#26412;&#36136;&#37327;&#26159;&#19981;&#21487;&#38752;&#30340;&#65292;&#30001;&#20110;&#30701;&#25991;&#26412; PPL &#20540;&#39640;&#20110;&#38271;&#25991;&#26412;&#65292;&#37325;&#22797;&#25991;&#26412;&#27573;&#33853;&#21644;&#26631;&#28857;&#31526;&#21495;&#20063;&#21487;&#20197;&#25439;&#22351;&#25351;&#26631;&#34920;&#29616;&#12290; &#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#25991;&#26412;&#36136;&#37327;&#24212;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24456;&#22810;&#30740;&#31350;&#37117;&#20351;&#29992;&#22256;&#24785;&#24230;&#65288;PPL&#65289;&#26469;&#35780;&#20272;&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#12290;&#20182;&#20204;&#35748;&#20026;&#65292;&#22914;&#26524; PPL &#30340;&#20540;&#36234;&#23567;&#65292;&#34987;&#35780;&#20272;&#25991;&#26412;&#30340;&#36136;&#37327;&#65288;&#21363;&#27969;&#30021;&#24615;&#65289;&#23601;&#36234;&#22909;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616; PPL &#21452;&#37325;&#38169;&#35823;&#65292;&#19981;&#33021;&#20844;&#27491;&#22320;&#35780;&#20272;&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#65292;&#21407;&#22240;&#21253;&#25324;&#65306;&#65288;i&#65289;&#30701;&#25991;&#26412;&#30340; PPL &#20540;&#22823;&#20110;&#38271;&#25991;&#26412;&#65292;&#36825;&#19982;&#24120;&#35782;&#30456;&#24726;&#65292;&#65288;ii&#65289;&#37325;&#22797;&#25991;&#26412;&#27573;&#33853;&#29366;&#24577;&#19979;&#21487;&#20197;&#25439;&#22351; PPL&#65292;&#65288;iii&#65289;&#26631;&#28857;&#31526;&#21495;&#21487;&#20197;&#20005;&#37325;&#24433;&#21709; PPL &#30340;&#34920;&#29616;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;PPL &#19981;&#33021;&#21487;&#38752;&#22320;&#35780;&#20272;&#32473;&#23450;&#25991;&#26412;&#30340;&#36136;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#25991;&#26412;&#36136;&#37327;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, amounts of works utilize perplexity~(PPL) to evaluate the quality of the generated text. They suppose that if the value of PPL is smaller, the quality(i.e. fluency) of the text to be evaluated is better. However, we find that the PPL referee is unqualified and it cannot evaluate the generated text fairly for the following reasons: (i) The PPL of short text is larger than long text, which goes against common sense, (ii) The repeated text span could damage the performance of PPL, and (iii) The punctuation marks could affect the performance of PPL heavily. Experiments show that the PPL is unreliable for evaluating the quality of given text. Last, we discuss the key problems with evaluating text quality using language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#33258;&#30001;&#26694;&#26550;&#65292;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#23454;&#29616;&#22797;&#26434;&#39640;&#32423;&#20219;&#21153;&#30340;&#30446;&#26631;&#39537;&#21160;&#23548;&#33322;&#12290;&#36890;&#36807;&#23558;&#20808;&#21069;&#30340;&#22810;&#30446;&#26631;DRL&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#21333;&#19968;&#30446;&#26631;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#37319;&#26679;&#30340;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;&#26469;&#25351;&#23548;DRL&#26234;&#33021;&#20307;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#28385;&#36275;&#19981;&#21487;&#34892;&#30340;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#20219;&#21153;&#24182;&#23613;&#21487;&#33021;&#20943;&#23569;&#36829;&#35268;&#12290;</title><link>http://arxiv.org/abs/2210.01162</link><description>&lt;p&gt;
&#23398;&#20064;&#26368;&#23567;&#36829;&#21453;&#36830;&#32493;&#25511;&#21046;&#20197;&#23454;&#29616;&#19981;&#21487;&#34892;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#35268;&#33539;
&lt;/p&gt;
&lt;p&gt;
Learning Minimally-Violating Continuous Control for Infeasible Linear Temporal Logic Specifications. (arXiv:2210.01162v4 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#33258;&#30001;&#26694;&#26550;&#65292;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#23454;&#29616;&#22797;&#26434;&#39640;&#32423;&#20219;&#21153;&#30340;&#30446;&#26631;&#39537;&#21160;&#23548;&#33322;&#12290;&#36890;&#36807;&#23558;&#20808;&#21069;&#30340;&#22810;&#30446;&#26631;DRL&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#21333;&#19968;&#30446;&#26631;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#37319;&#26679;&#30340;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;&#26469;&#25351;&#23548;DRL&#26234;&#33021;&#20307;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#28385;&#36275;&#19981;&#21487;&#34892;&#30340;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#20219;&#21153;&#24182;&#23613;&#21487;&#33021;&#20943;&#23569;&#36829;&#35268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#26102;&#38388;&#25511;&#21046;&#32508;&#21512;&#65292;&#20197;&#23454;&#29616;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;(LTL)&#34920;&#36798;&#30340;&#22797;&#26434;&#39640;&#32423;&#20219;&#21153;&#30340;&#30446;&#26631;&#39537;&#21160;&#23548;&#33322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#33258;&#30001;&#26694;&#26550;&#65292;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#65292;&#20854;&#20013;&#24213;&#23618;&#21160;&#24577;&#31995;&#32479;&#26410;&#30693;&#65288;&#36879;&#26126;&#30418;&#23376;&#65289;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#26412;&#25991;&#32771;&#34385;&#20102;&#32473;&#23450;&#30340;LTL&#35268;&#33539;&#21487;&#33021;&#26159;&#19981;&#21487;&#34892;&#30340;&#24773;&#20917;&#65292;&#22240;&#27492;&#26080;&#27861;&#20840;&#23616;&#23436;&#25104;&#12290;&#25105;&#20204;&#19981;&#20462;&#25913;&#32473;&#23450;&#30340;LTL&#20844;&#24335;&#65292;&#32780;&#26159;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;DRL&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#36829;&#35268;&#28385;&#36275;&#23427;&#12290;&#20026;&#20102;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#23558;&#20808;&#21069;&#30340;&#22810;&#30446;&#26631;DRL&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#21333;&#19968;&#30446;&#26631;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#35201;&#27714;&#21516;&#26102;&#23454;&#29616;&#33258;&#21160;&#26426;&#28385;&#36275;&#21644;&#26368;&#23567;&#36829;&#35268;&#20195;&#20215;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#37319;&#26679;&#30340;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;&#26469;&#25351;&#23548;&#21487;&#33021;&#19981;&#21487;&#34892;&#30340;LTL&#20219;&#21153;&#30340;DRL&#26234;&#33021;&#20307;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20943;&#36731;&#20102;DRL&#30340;&#36817;&#35270;&#20542;&#21521;&#65292;&#36825;&#22312;&#23398;&#20064;&#21487;&#20197;&#20855;&#26377;&#38271;&#25110;&#26080;&#38480;&#25345;&#32493;&#26102;&#38388;&#30340;&#19968;&#33324;LTL&#20219;&#21153;&#26102;&#32463;&#24120;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores continuous-time control synthesis for target-driven navigation to satisfy complex high-level tasks expressed as linear temporal logic (LTL). We propose a model-free framework using deep reinforcement learning (DRL) where the underlying dynamic system is unknown (an opaque box). Unlike prior work, this paper considers scenarios where the given LTL specification might be infeasible and therefore cannot be accomplished globally. Instead of modifying the given LTL formula, we provide a general DRL-based approach to satisfy it with minimal violation. To do this, we transform a previously multi-objective DRL problem, which requires simultaneous automata satisfaction and minimum violation cost, into a single objective. By guiding the DRL agent with a sampling-based path planning algorithm for the potentially infeasible LTL task, the proposed approach mitigates the myopic tendencies of DRL, which are often an issue when learning general LTL tasks that can have long or infin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;NSGA-II&#31639;&#27861;&#22312;&#36866;&#24403;&#30340;&#20154;&#21475;&#35268;&#27169;&#19979;&#65292;&#22312;&#35299;&#20915;$OneMinMax$&#38382;&#39064;&#26102;&#38656;&#35201;$\Omega(Nn\log n)$&#27425;&#20989;&#25968;&#35780;&#20272;&#65292;&#24182;&#22312;&#35299;&#20915;&#36339;&#36291;&#22823;&#23567;&#20026;$k$&#30340;$OneJumpZeroJump$&#38382;&#39064;&#26102;&#38656;&#35201;$\Omega(Nn^k)$&#27425;&#35780;&#20272;&#12290;&#36825;&#20123;&#19979;&#38480;&#20063;&#34920;&#26126;&#65292;&#21363;&#20351;&#20351;&#29992;&#26356;&#22823;&#30340;&#20154;&#21475;&#22823;&#23567;&#65292;NSGA-II&#20063;&#19981;&#33021;&#22312;&#24182;&#34892;&#36816;&#34892;&#26102;&#38388;&#26041;&#38754;&#21463;&#30410;&#12290;</title><link>http://arxiv.org/abs/2209.13974</link><description>&lt;p&gt;
&#20174;&#29702;&#35299;NSGA-II&#30340;&#20154;&#21475;&#21160;&#24577;&#21040;&#39318;&#27425;&#35777;&#26126;&#30340;&#19979;&#38480;
&lt;/p&gt;
&lt;p&gt;
From Understanding the Population Dynamics of the NSGA-II to the First Proven Lower Bounds. (arXiv:2209.13974v2 [cs.NE] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;NSGA-II&#31639;&#27861;&#22312;&#36866;&#24403;&#30340;&#20154;&#21475;&#35268;&#27169;&#19979;&#65292;&#22312;&#35299;&#20915;$OneMinMax$&#38382;&#39064;&#26102;&#38656;&#35201;$\Omega(Nn\log n)$&#27425;&#20989;&#25968;&#35780;&#20272;&#65292;&#24182;&#22312;&#35299;&#20915;&#36339;&#36291;&#22823;&#23567;&#20026;$k$&#30340;$OneJumpZeroJump$&#38382;&#39064;&#26102;&#38656;&#35201;$\Omega(Nn^k)$&#27425;&#35780;&#20272;&#12290;&#36825;&#20123;&#19979;&#38480;&#20063;&#34920;&#26126;&#65292;&#21363;&#20351;&#20351;&#29992;&#26356;&#22823;&#30340;&#20154;&#21475;&#22823;&#23567;&#65292;NSGA-II&#20063;&#19981;&#33021;&#22312;&#24182;&#34892;&#36816;&#34892;&#26102;&#38388;&#26041;&#38754;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;NSGA-II&#30340;&#26356;&#20026;&#22797;&#26434;&#30340;&#20154;&#21475;&#21160;&#24577;&#65292;&#30446;&#21069;&#35813;&#31639;&#27861;&#30340;&#25152;&#26377;&#36816;&#34892;&#26102;&#20445;&#35777;&#37117;&#27809;&#26377;&#38468;&#24102;&#20219;&#20309;&#38750;&#24179;&#20961;&#19979;&#38480;&#12290;&#36890;&#36807;&#23545;NSGA-II&#30340;&#20154;&#21475;&#21160;&#24577;&#36827;&#34892;&#31532;&#19968;&#27425;&#25968;&#23398;&#29702;&#35299;&#65292;&#21363;&#36890;&#36807;&#20272;&#35745;&#20855;&#26377;&#29305;&#23450;&#30446;&#26631;&#20540;&#30340;&#20010;&#20307;&#30340;&#26399;&#26395;&#25968;&#37327;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;NSGA-II&#38656;&#35201;&#36866;&#24403;&#30340;&#20154;&#21475;&#35268;&#27169;&#25165;&#33021;&#22312;$OneMinMax$&#38382;&#39064;&#20013;&#25214;&#21040;Pareto&#21069;&#27839;&#65292;&#24182;&#22312;&#36339;&#36291;&#22823;&#23567;&#20026;$k$&#26102;&#38656;&#35201;$\Omega(Nn^k)$&#27425;&#35780;&#20272;&#26469;&#35299;&#20915;$OneJumpZeroJump$&#38382;&#39064;&#12290;&#36825;&#20123;&#19979;&#38480;&#26159;&#28176;&#36827;&#32039;&#23494;&#30340;&#65288;&#21363;&#19982;&#20043;&#21069;&#26174;&#31034;&#30340;&#19978;&#38480;&#21305;&#37197;&#65289;&#65292;&#24182;&#19988;&#34920;&#26126;&#22312;&#24182;&#34892;&#36816;&#34892;&#26102;&#38388;&#65288;&#36845;&#20195;&#27425;&#25968;&#65289;&#26041;&#38754;&#65292;&#21363;&#20351;&#20351;&#29992;&#26356;&#22823;&#30340;&#20154;&#21475;&#22823;&#23567;&#65292;NSGA-II&#20063;&#19981;&#20250;&#20174;&#20013;&#33719;&#30410;&#12290;&#23545;&#20110;$OneJumpZeroJump$&#38382;&#39064;&#65292;&#24403;&#20351;&#29992;&#30456;&#21516;&#30340;&#25490;&#24207;&#26469;&#35745;&#31639;&#20004;&#20010;&#30446;&#26631;&#30340;&#25317;&#25380;&#36317;&#31163;&#36129;&#29486;&#26102;&#65292;&#25105;&#20204;&#29978;&#33267;&#33719;&#24471;&#20102;&#19968;&#20010;&#32039;&#23494;&#30340;&#36816;&#34892;&#26102;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the more complicated population dynamics of the NSGA-II, none of the existing runtime guarantees for this algorithm is accompanied by a non-trivial lower bound. Via a first mathematical understanding of the population dynamics of the NSGA-II, that is, by estimating the expected number of individuals having a certain objective value, we prove that the NSGA-II with suitable population size needs $\Omega(Nn\log n)$ function evaluations to find the Pareto front of the OneMinMax problem and $\Omega(Nn^k)$ evaluations on the OneJumpZeroJump problem with jump size $k$. These bounds are asymptotically tight (that is, they match previously shown upper bounds) and show that the NSGA-II here does not even in terms of the parallel runtime (number of iterations) profit from larger population sizes. For the OneJumpZeroJump problem and when the same sorting is used for the computation of the crowding distance contributions of the two objectives, we even obtain a runtime estimate that is tight 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#65292;&#20197;&#26816;&#26597;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#23545;&#35805;&#20013;&#20844;&#24179;&#24615;&#30340;&#21547;&#20041;&#12290;&#20316;&#32773;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#36827;&#34892;&#20102;&#23457;&#35745;&#30740;&#31350;&#65292;&#21457;&#29616; GPT-3 &#22312;&#22238;&#24212;&#27668;&#20505;&#21464;&#21270;&#21644;BBL&#36816;&#21160;&#30340;&#25552;&#31034;&#26102;&#23384;&#22312;&#19981;&#20844;&#24179;&#30340;&#34892;&#20026;&#65292;&#24378;&#21270;&#20102;&#21051;&#26495;&#21360;&#35937;&#65292;&#36793;&#32536;&#21270;&#20102;&#26576;&#20123;&#29305;&#23450;&#30340;&#32676;&#20307;&#12290;&#35813;&#30740;&#31350;&#34920;&#26126;&#26377;&#24517;&#35201;&#35299;&#20915;&#36825;&#20123;&#20559;&#35265;&#65292;&#20197;&#38450;&#27490;AI-powered&#26381;&#21153;&#20013;&#36827;&#19968;&#27493;&#24041;&#22266;&#26435;&#21147;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2209.13627</link><description>&lt;p&gt;
GPT-3 &#22914;&#20309;&#22788;&#29702;&#27668;&#20505;&#21464;&#21270;&#21644;&#8220;&#40657;&#20154;&#30340;&#21629;&#20063;&#26159;&#21629;&#8221;&#31561;&#19981;&#21516;&#20844;&#20247;&#30340;&#35805;&#39064;&#65306;&#20851;&#20110;&#23545;&#35805;&#24335; AI &#20844;&#24179;&#24615;&#30340;&#25209;&#21028;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
How GPT-3 responds to different publics on climate change and Black Lives Matter: A critical appraisal of equity in conversational AI. (arXiv:2209.13627v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#65292;&#20197;&#26816;&#26597;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#23545;&#35805;&#20013;&#20844;&#24179;&#24615;&#30340;&#21547;&#20041;&#12290;&#20316;&#32773;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#36827;&#34892;&#20102;&#23457;&#35745;&#30740;&#31350;&#65292;&#21457;&#29616; GPT-3 &#22312;&#22238;&#24212;&#27668;&#20505;&#21464;&#21270;&#21644;BBL&#36816;&#21160;&#30340;&#25552;&#31034;&#26102;&#23384;&#22312;&#19981;&#20844;&#24179;&#30340;&#34892;&#20026;&#65292;&#24378;&#21270;&#20102;&#21051;&#26495;&#21360;&#35937;&#65292;&#36793;&#32536;&#21270;&#20102;&#26576;&#20123;&#29305;&#23450;&#30340;&#32676;&#20307;&#12290;&#35813;&#30740;&#31350;&#34920;&#26126;&#26377;&#24517;&#35201;&#35299;&#20915;&#36825;&#20123;&#20559;&#35265;&#65292;&#20197;&#38450;&#27490;AI-powered&#26381;&#21153;&#20013;&#36827;&#19968;&#27493;&#24041;&#22266;&#26435;&#21147;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#36825;&#31181;&#27169;&#22411;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#25509;&#36817;&#20154;&#31867;&#30340;&#25991;&#26412;&#12290;&#36825;&#20123;&#27169;&#22411;&#39537;&#21160;&#30528;&#26234;&#33021;&#20581;&#24247;&#12289;&#37329;&#34701;&#21644;&#33258;&#20027;&#39550;&#39542;&#31561;&#39046;&#22495;&#30340;&#27969;&#34892;&#34394;&#25311;&#21161;&#25163;&#12290;&#34429;&#28982;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#27491;&#22312;&#25913;&#36827;&#65292;&#20294;&#20154;&#20204;&#20173;&#28982;&#25285;&#24515;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#19981;&#21516;&#31243;&#24230;&#22320;&#20026;&#31038;&#20250;&#20013;&#30340;&#25152;&#26377;&#23376;&#32676;&#20307;&#25552;&#20379;&#26381;&#21153;&#12290;&#23613;&#31649;&#36328;&#23398;&#31185;&#30340; AI &#20844;&#24179;&#35752;&#35770;&#36234;&#26469;&#36234;&#22810;&#65292;&#20294;&#32570;&#20047;&#31995;&#32479;&#24615;&#30340;&#25351;&#26631;&#26469;&#35780;&#20272;&#23545;&#35805;&#31995;&#32479;&#20013;&#20844;&#24179;&#30340;&#21547;&#20041;&#65292;&#20197;&#21450;&#22914;&#20309;&#35753;&#19981;&#21516;&#30340;&#20154;&#32676;&#21442;&#19982;&#35780;&#20272;&#24490;&#29615;&#12290;&#26412;&#25991;&#22522;&#20110;&#27665;&#20027;&#20915;&#31574;&#29702;&#35770;&#21644;&#31185;&#23398;&#25216;&#26415;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#26469;&#25581;&#31034;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#23545;&#35805;&#20013;&#20844;&#24179;&#24615;&#30340;&#21547;&#20041;&#12290;&#20351;&#29992;&#36825;&#19968;&#26694;&#26550;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23457;&#35745;&#30740;&#31350;&#65292;&#20197;&#26816;&#26597; GPT-3 &#22914;&#20309;&#21709;&#24212;&#20851;&#38190;&#30340;&#31185;&#23398;&#21644;&#31038;&#20250;&#35805;&#39064;&#65306;&#27668;&#20505;&#21464;&#21270;&#21644;&#8220;&#40657;&#20154;&#30340;&#21629;&#20063;&#26159;&#21629;&#8221;&#36816;&#21160;&#12290;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#21253;&#25324; 480 &#27425;&#25552;&#31034;&#65292;&#20854;&#20013;&#31995;&#32479;&#22320;&#21464;&#21270;&#21457;&#35328;&#20154;&#30340;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#22320;&#29702;&#20301;&#32622;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#24403;&#22238;&#24212;&#27668;&#20505;&#21464;&#21270;&#21644; BLM &#30340;&#25552;&#31034;&#26102;&#65292;GPT-3 &#26377;&#26102;&#20250;&#24378;&#21270;&#21051;&#26495;&#21360;&#35937;&#65292;&#36793;&#32536;&#21270;&#26576;&#20123;&#32676;&#20307;&#65292;&#22914;&#22303;&#33879;&#27665;&#26063;&#12290;&#25105;&#20204;&#35748;&#20026;&#24517;&#39035;&#35299;&#20915;&#36825;&#20123;&#20559;&#35265;&#65292;&#20197;&#38450;&#27490;&#26435;&#21147;&#32467;&#26500;&#22312; AI &#21160;&#21147;&#26381;&#21153;&#20013;&#36827;&#19968;&#27493;&#24041;&#22266;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoregressive language models, which use deep learning to produce human-like texts, have become increasingly widespread. Such models are powering popular virtual assistants in areas like smart health, finance, and autonomous driving. While the parameters of these large language models are improving, concerns persist that these models might not work equally for all subgroups in society. Despite growing discussions of AI fairness across disciplines, there lacks systemic metrics to assess what equity means in dialogue systems and how to engage different populations in the assessment loop. Grounded in theories of deliberative democracy and science and technology studies, this paper proposes an analytical framework for unpacking the meaning of equity in human-AI dialogues. Using this framework, we conducted an auditing study to examine how GPT-3 responded to different sub-populations on crucial science and social topics: climate change and the Black Lives Matter (BLM) movement. Our corpus 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#19981;&#21516;&#25277;&#35937;&#23618;&#27425;&#19978;&#30340;&#27867;&#21270;&#38382;&#39064;&#21450;&#20854;&#26041;&#27861;&#65292;&#20854;&#20013;&#26679;&#26412;&#27867;&#21270;&#24050;&#32463;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#26410;&#26469;&#38656;&#35201;&#37325;&#28857;&#20851;&#27880;&#20943;&#23569;&#36807;&#25311;&#21512;&#65307;&#20998;&#24067;&#27867;&#21270;&#19982;&#39046;&#22495;&#27867;&#21270;&#26377;&#30456;&#20284;&#20043;&#22788;&#65292;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#22256;&#38590;&#30340;&#26679;&#26412;&#25110;&#20998;&#24067;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2209.01610</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27867;&#21270;&#65306;&#32508;&#36848;&#19982;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Generalization in Neural Networks: A Broad Survey. (arXiv:2209.01610v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.01610
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#19981;&#21516;&#25277;&#35937;&#23618;&#27425;&#19978;&#30340;&#27867;&#21270;&#38382;&#39064;&#21450;&#20854;&#26041;&#27861;&#65292;&#20854;&#20013;&#26679;&#26412;&#27867;&#21270;&#24050;&#32463;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#26410;&#26469;&#38656;&#35201;&#37325;&#28857;&#20851;&#27880;&#20943;&#23569;&#36807;&#25311;&#21512;&#65307;&#20998;&#24067;&#27867;&#21270;&#19982;&#39046;&#22495;&#27867;&#21270;&#26377;&#30456;&#20284;&#20043;&#22788;&#65292;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#22256;&#38590;&#30340;&#26679;&#26412;&#25110;&#20998;&#24067;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#19981;&#21516;&#25277;&#35937;&#23618;&#27425;&#30340;&#27010;&#24565;&#12289;&#24314;&#27169;&#26041;&#27861;&#21644;&#26368;&#36817;&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#21253;&#25324;&#26679;&#26412;&#12289;&#20998;&#24067;&#12289;&#39046;&#22495;&#12289;&#20219;&#21153;&#12289;&#27169;&#24577;&#21644;&#33539;&#22260;&#19978;&#30340;&#27867;&#21270;&#12290;&#22312;&#26679;&#26412;&#27867;&#21270;&#26041;&#38754;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#20960;&#20046;&#25152;&#26377;&#30340;&#26368;&#26032;&#25913;&#36827;&#37117;&#20943;&#23567;&#20102;&#35757;&#32451;&#35823;&#24046;&#65292;&#32780;&#36807;&#25311;&#21512;&#20445;&#25345;&#19981;&#21464;&#65307;&#38543;&#30528;&#20960;&#20046;&#25152;&#26377;&#30340;&#35757;&#32451;&#35823;&#24046;&#34987;&#28040;&#38500;&#65292;&#26410;&#26469;&#30340;&#36827;&#23637;&#23558;&#38656;&#35201;&#38598;&#20013;&#20851;&#27880;&#20943;&#23569;&#36807;&#25311;&#21512;&#12290;&#20174;&#32479;&#35745;&#23398;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;(2)&#20998;&#24067;&#27867;&#21270;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#26679;&#26412;&#26435;&#37325;&#25110;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#30340;&#21464;&#21270;&#65307;&#22240;&#27492;&#65292;&#22312;&#39046;&#22495;&#27867;&#21270;&#25104;&#21151;&#30340;&#25216;&#26415;&#26377;&#21487;&#33021;&#24212;&#29992;&#20110;&#22256;&#38590;&#30340;&#26679;&#26412;&#25110;&#20998;&#24067;&#27867;&#21270;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;(3)&#39046;&#22495;&#27867;&#21270;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#26368;&#36817;&#30340;&#36827;&#23637;&#21644;&#20016;&#23500;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper reviews concepts, modeling approaches, and recent findings along a spectrum of different levels of abstraction of neural network models including generalization across (1) Samples, (2) Distributions, (3) Domains, (4) Tasks, (5) Modalities, and (6) Scopes. Results on (1) sample generalization show that, in the case of ImageNet, nearly all the recent improvements reduced training error while overfitting stayed flat; with nearly all the training error eliminated, future progress will require a focus on reducing overfitting. Perspectives from statistics highlight how (2) distribution generalization can be viewed alternately as a change in sample weights or a change in the input-output relationship; thus, techniques that have been successful in domain generalization have the potential to be applied to difficult forms of sample or distribution generalization. Transfer learning approaches to (3) domain generalization are summarized, as are recent advances and the wealth of domain a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;NSGA-II&#20248;&#21270;OneJumpZeroJump&#22522;&#20934;&#20989;&#25968;&#26102;&#65292;&#20132;&#21449;&#25805;&#20316;&#21487;&#20197;&#24102;&#26469;&#21487;&#35777;&#26126;&#30340;&#21152;&#36895;&#25928;&#26524;&#65307;&#21516;&#26679;&#65292;&#20132;&#21449;&#25805;&#20316;&#23545;&#20110;&#21333;&#30446;&#26631;&#20248;&#21270;&#20013;&#30340;$(\mu+1)$&#36951;&#20256;&#31639;&#27861;&#20063;&#26377;&#26356;&#26174;&#33879;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2208.08759</link><description>&lt;p&gt;
NSGA-II&#30340;&#36816;&#34892;&#26102;&#20998;&#26512;&#65306;&#20132;&#21449;&#25805;&#20316;&#24102;&#26469;&#21487;&#35777;&#26126;&#30340;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
Runtime Analysis for the NSGA-II: Provable Speed-Ups From Crossover. (arXiv:2208.08759v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;NSGA-II&#20248;&#21270;OneJumpZeroJump&#22522;&#20934;&#20989;&#25968;&#26102;&#65292;&#20132;&#21449;&#25805;&#20316;&#21487;&#20197;&#24102;&#26469;&#21487;&#35777;&#26126;&#30340;&#21152;&#36895;&#25928;&#26524;&#65307;&#21516;&#26679;&#65292;&#20132;&#21449;&#25805;&#20316;&#23545;&#20110;&#21333;&#30446;&#26631;&#20248;&#21270;&#20013;&#30340;$(\mu+1)$&#36951;&#20256;&#31639;&#27861;&#20063;&#26377;&#26356;&#26174;&#33879;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;NSGA-II&#65292;&#26368;&#24120;&#29992;&#30340;&#22810;&#30446;&#26631;&#28436;&#21270;&#31639;&#27861;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#25968;&#23398;&#36816;&#34892;&#26102;&#20998;&#26512;&#12290;&#26412;&#30740;&#31350;&#35777;&#26126;&#65292;&#24403;&#20351;&#29992;&#20132;&#21449;&#26102;&#65292;NSGA-II&#22312;&#20248;&#21270;OneJumpZeroJump&#22522;&#20934;&#20989;&#25968;&#26102;&#20250;&#21576;&#29616;&#20986;&#28176;&#36817;&#26356;&#24555;&#30340;&#36895;&#24230;&#12290;&#19982;Dang&#12289;Opris&#12289;Salehi&#21644;Sudholt&#30340;&#19968;&#39033;&#29420;&#31435;&#24182;&#34892;&#24037;&#20316;&#19968;&#36215;&#65292;&#36825;&#26159;&#31532;&#19968;&#27425;&#35777;&#26126;&#20132;&#21449;&#25805;&#20316;&#23545;NSGA-II&#26377;&#36825;&#26679;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#35770;&#35777;&#21487;&#20197;&#34987;&#36716;&#31227;&#21040;&#21333;&#30446;&#26631;&#20248;&#21270;&#20013;&#12290;&#32780;&#19988;&#65292;&#20182;&#20204;&#35777;&#26126;&#20132;&#21449;&#25805;&#20316;&#21487;&#20197;&#27604;&#20197;&#21069;&#26356;&#21152;&#26174;&#33879;&#22320;&#21152;&#36895;$(\mu+1)$&#36951;&#20256;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#20132;&#21449;&#30340;&#38468;&#21152;&#20215;&#20540;&#65292;&#24182;&#34920;&#26126;&#35266;&#23519;&#21040;&#30340;&#20248;&#21183;&#29978;&#33267;&#27604;&#25105;&#20204;&#30340;&#35777;&#26126;&#25152;&#33021;&#20445;&#35777;&#30340;&#26356;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Very recently, the first mathematical runtime analyses for the NSGA-II, the most common multi-objective evolutionary algorithm, have been conducted. Continuing this research direction, we prove that the NSGA-II optimizes the OneJumpZeroJump benchmark asymptotically faster when crossover is employed. Together with a parallel independent work by Dang, Opris, Salehi, and Sudholt, this is the first time such an advantage of crossover is proven for the NSGA-II. Our arguments can be transferred to single-objective optimization. They then prove that crossover can speed up the $(\mu+1)$ genetic algorithm in a different way and more pronounced than known before. Our experiments confirm the added value of crossover and show that the observed advantages are even larger than what our proofs can guarantee.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20132;&#20114;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#21453;&#39304;&#39057;&#29575;&#23545;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#23450;&#37327;&#21270;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#27809;&#26377;&#21333;&#19968;&#30340;&#29702;&#24819;&#21453;&#39304;&#39057;&#29575;&#23384;&#22312;&#65292;&#24212;&#35813;&#26681;&#25454;&#20855;&#20307;&#30340;&#20219;&#21153;&#21644;&#26426;&#22120;&#20154;&#22797;&#26434;&#24615;&#36827;&#34892;&#35843;&#25972;&#12290;</title><link>http://arxiv.org/abs/2207.09845</link><description>&lt;p&gt;
&#20132;&#20114;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#21453;&#39304;&#39057;&#29575;&#23545;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#24433;&#21709;&#30340;&#23450;&#37327;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Quantifying the Effect of Feedback Frequency in Interactive Reinforcement Learning for Robotic Tasks. (arXiv:2207.09845v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.09845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20132;&#20114;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#21453;&#39304;&#39057;&#29575;&#23545;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#23450;&#37327;&#21270;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#27809;&#26377;&#21333;&#19968;&#30340;&#29702;&#24819;&#21453;&#39304;&#39057;&#29575;&#23384;&#22312;&#65292;&#24212;&#35813;&#26681;&#25454;&#20855;&#20307;&#30340;&#20219;&#21153;&#21644;&#26426;&#22120;&#20154;&#22797;&#26434;&#24615;&#36827;&#34892;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#26426;&#22120;&#20154;&#25511;&#21046;&#20013;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#25104;&#21151;&#26696;&#20363;&#65292;&#20294;&#19968;&#20010;&#37325;&#35201;&#30340;&#25345;&#20037;&#24615;&#38382;&#39064;&#26159;&#25968;&#25454;&#25928;&#29575;&#38750;&#24120;&#20302;&#12290;&#20132;&#20114;&#21453;&#39304;&#26159;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#22823;&#22823;&#21152;&#36895;RL&#12290;&#22240;&#27492;&#65292;&#26377;&#22823;&#37327;&#19981;&#21516;&#30340;&#31574;&#30053;&#65292;&#28982;&#32780;&#36825;&#20123;&#31574;&#30053;&#20027;&#35201;&#26159;&#22312;&#31163;&#25955;&#30340;&#32593;&#26684;&#19990;&#30028;&#21644;&#23567;&#35268;&#27169;&#30340;&#26368;&#20248;&#25511;&#21046;&#22330;&#26223;&#20013;&#27979;&#35797;&#30340;&#12290;&#22312;&#25991;&#29486;&#20013;&#65292;&#23545;&#20110;&#21738;&#31181;&#21453;&#39304;&#39057;&#29575;&#26368;&#20248;&#25110;&#22312;&#20160;&#20040;&#26102;&#20505;&#21453;&#39304;&#26368;&#26377;&#30410;&#24182;&#27809;&#26377;&#20849;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#24046;&#24322;&#65292;&#25105;&#20204;&#22312;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#20998;&#31163;&#24182;&#37327;&#21270;&#20102;&#21453;&#39304;&#39057;&#29575;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#28085;&#30422;&#20102;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#26426;&#26800;&#33218;&#30340;&#36870;&#36816;&#21160;&#23398;&#23398;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#34920;&#38754;&#19978;&#30683;&#30462;&#30340;&#29616;&#35937;&#22312;&#19981;&#21516;&#30340;&#22797;&#26434;&#24230;&#27700;&#24179;&#19978;&#20986;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#27809;&#26377;&#21333;&#19968;&#30340;&#29702;&#24819;&#21453;&#39304;&#39057;&#29575;&#23384;&#22312;&#12290;&#21453;&#39304;&#39057;&#29575;&#24212;&#35813;&#26681;&#25454;&#20855;&#20307;&#30340;&#20219;&#21153;&#21644;&#26426;&#22120;&#20154;&#22797;&#26434;&#24615;&#36827;&#34892;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has become widely adopted in robot control. Despite many successes, one major persisting problem can be very low data efficiency. One solution is interactive feedback, which has been shown to speed up RL considerably. As a result, there is an abundance of different strategies, which are, however, primarily tested on discrete grid-world and small scale optimal control scenarios. In the literature, there is no consensus about which feedback frequency is optimal or at which time the feedback is most beneficial. To resolve these discrepancies we isolate and quantify the effect of feedback frequency in robotic tasks with continuous state and action spaces. The experiments encompass inverse kinematics learning for robotic manipulator arms of different complexity. We show that seemingly contradictory reported phenomena occur at different complexity levels. Furthermore, our results suggest that no single ideal feedback frequency exists. Rather that feedback frequenc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Betty&#30340;&#33258;&#21160;&#24494;&#20998;&#24211;&#65292;&#21487;&#29992;&#20110;&#22823;&#35268;&#27169;&#30340;&#26799;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#65292;&#22312;&#24191;&#27867;&#30340;&#22810;&#23618;&#27425;&#20248;&#21270;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2207.02849</link><description>&lt;p&gt;
Betty: &#19968;&#20010;&#29992;&#20110;&#22810;&#23618;&#27425;&#20248;&#21270;&#30340;&#33258;&#21160;&#24494;&#20998;&#24211;
&lt;/p&gt;
&lt;p&gt;
Betty: An Automatic Differentiation Library for Multilevel Optimization. (arXiv:2207.02849v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.02849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Betty&#30340;&#33258;&#21160;&#24494;&#20998;&#24211;&#65292;&#21487;&#29992;&#20110;&#22823;&#35268;&#27169;&#30340;&#26799;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#65292;&#22312;&#24191;&#27867;&#30340;&#22810;&#23618;&#27425;&#20248;&#21270;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#30340;&#22810;&#23618;&#27425;&#20248;&#21270;(MLO)&#24050;&#25104;&#20026;&#30740;&#31350;&#20247;&#22810;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#36229;&#21442;&#25968;&#20248;&#21270;&#12289;&#20803;&#23398;&#20064;&#12289;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#12290;&#28982;&#32780;&#65292;MLO&#20013;&#30340;&#26799;&#24230;&#65292;&#26159;&#36890;&#36807;&#38142;&#24335;&#27861;&#21017;&#32452;&#25104;&#26368;&#20339;&#21709;&#24212;Jacobi&#30697;&#38453;&#32780;&#33719;&#24471;&#30340;&#65292;&#20855;&#26377;&#35745;&#31639;&#21644;&#20869;&#23384;&#23494;&#38598;&#31561;&#19981;&#21033;&#22240;&#32032;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38754;&#21521;&#22823;&#35268;&#27169;MLO&#30340;&#36719;&#20214;&#24211;Betty&#65292;&#20174;&#32780;&#21021;&#27493;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;&#22312;&#20854;&#26680;&#24515;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;MLO&#25968;&#25454;&#27969;&#22270;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;(1)&#20026;MLO&#24320;&#21457;&#39640;&#25928;&#30340;&#33258;&#21160;&#24494;&#20998;&#65292;&#23558;&#35745;&#31639;&#22797;&#26434;&#24230;&#20174;O(d^3)&#38477;&#33267;O(d^2)&#65292;(2)&#34701;&#20837;&#31995;&#32479;&#25903;&#25345;&#65292;&#20363;&#22914;&#28151;&#21512;&#31934;&#24230;&#21644;&#25968;&#25454;&#24182;&#34892;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;&#21487;&#20280;&#32553;&#24615;&#65292;(3)&#20415;&#20110;&#23454;&#29616;&#20219;&#24847;&#22797;&#26434;&#24230;&#30340;MLO&#31243;&#24207;&#65292;&#21516;&#26102;&#20801;&#35768;&#22810;&#26679;&#21270;&#30340;&#31639;&#27861;&#21644;&#31995;&#32479;&#35774;&#35745;&#36873;&#25321;&#30340;&#27169;&#22359;&#21270;&#25509;&#21475;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;Betty&#22312;&#24191;&#27867;&#30340;MLO&#20219;&#21153;&#20013;&#37117;&#23454;&#29616;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#36229;&#21442;&#25968;&#20248;&#21270;&#12289;&#20803;&#23398;&#20064;&#21644;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient-based multilevel optimization (MLO) has gained attention as a framework for studying numerous problems, ranging from hyperparameter optimization and meta-learning to neural architecture search and reinforcement learning. However, gradients in MLO, which are obtained by composing best-response Jacobians via the chain rule, are notoriously difficult to implement and memory/compute intensive. We take an initial step towards closing this gap by introducing Betty, a software library for large-scale MLO. At its core, we devise a novel dataflow graph for MLO, which allows us to (1) develop efficient automatic differentiation for MLO that reduces the computational complexity from O(d^3) to O(d^2), (2) incorporate systems support such as mixed-precision and data-parallel training for scalability, and (3) facilitate implementation of MLO programs of arbitrary complexity while allowing a modular interface for diverse algorithmic and systems design choices. We empirically demonstrate that
&lt;/p&gt;</description></item><item><title>NovelCraft&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#24320;&#25918;&#19990;&#30028;&#20013;&#26032;&#39062;&#24615;&#26816;&#27979;&#19982;&#21457;&#29616;&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;&#22312;&#22797;&#26434;&#30340;&#22330;&#26223;&#20013;&#25554;&#20837;&#26032;&#39062;&#29289;&#20307;&#30340;&#26816;&#27979;&#38656;&#35201;&#26356;&#22909;&#30340;&#22522;&#20934;&#65292;&#24182;&#21457;&#29616;&#20102;&#25511;&#21046;&#20551;&#38451;&#24615;&#26102;&#26356;&#31616;&#21333;&#30340;&#26041;&#27861;&#21487;&#33021;&#27604;&#22797;&#26434;&#30340;&#26041;&#27861;&#26356;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2206.11736</link><description>&lt;p&gt;
NovelCraft&#65306;&#24320;&#25918;&#19990;&#30028;&#20013;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#21644;&#21457;&#29616;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
NovelCraft: A Dataset for Novelty Detection and Discovery in Open Worlds. (arXiv:2206.11736v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.11736
&lt;/p&gt;
&lt;p&gt;
NovelCraft&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#24320;&#25918;&#19990;&#30028;&#20013;&#26032;&#39062;&#24615;&#26816;&#27979;&#19982;&#21457;&#29616;&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;&#22312;&#22797;&#26434;&#30340;&#22330;&#26223;&#20013;&#25554;&#20837;&#26032;&#39062;&#29289;&#20307;&#30340;&#26816;&#27979;&#38656;&#35201;&#26356;&#22909;&#30340;&#22522;&#20934;&#65292;&#24182;&#21457;&#29616;&#20102;&#25511;&#21046;&#20551;&#38451;&#24615;&#26102;&#26356;&#31616;&#21333;&#30340;&#26041;&#27861;&#21487;&#33021;&#27604;&#22797;&#26434;&#30340;&#26041;&#27861;&#26356;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35753;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#25104;&#21151;&#25191;&#34892;&#20219;&#21153;&#65292;&#24517;&#39035;&#33021;&#22815;&#26816;&#27979;&#21644;&#36866;&#24212;&#26032;&#39062;&#24615;&#12290;&#28982;&#32780;&#65292;&#35270;&#35273;&#26032;&#39062;&#24615;&#26816;&#27979;&#30740;&#31350;&#36890;&#24120;&#21482;&#35780;&#20272;&#26088;&#22312;&#36827;&#34892;&#23545;&#35937;&#20998;&#31867;&#30340;&#37325;&#22797;&#21033;&#29992;&#25968;&#25454;&#38598;&#65288;&#22914;CIFAR-10&#65289;&#65292;&#20854;&#20013;&#22270;&#20687;&#32858;&#28966;&#20110;&#19968;&#20010;&#26126;&#26174;&#12289;&#23621;&#20013;&#30340;&#23545;&#35937;&#12290;&#38656;&#35201;&#26032;&#30340;&#22522;&#20934;&#26469;&#20195;&#34920;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#23548;&#33322;&#22797;&#26434;&#22330;&#26223;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26032;NovelCraft&#25968;&#25454;&#38598;&#21253;&#21547;&#23436;&#25104;&#20462;&#25913;&#21518;&#30340;Minecraft&#29615;&#22659;&#20013;&#30340;&#36339;&#36339;&#29699;&#35013;&#37197;&#20219;&#21153;&#30340;&#20195;&#29702;&#25152;&#30475;&#21040;&#30340;&#22270;&#20687;&#21644;&#31526;&#21495;&#19990;&#30028;&#29366;&#24577;&#30340;&#22810;&#27169;&#24335;&#24773;&#33410;&#25968;&#25454;&#12290;&#22312;&#26576;&#20123;&#24773;&#33410;&#20013;&#65292;&#25105;&#20204;&#22312;&#22797;&#26434;&#30340;3D&#22330;&#26223;&#20013;&#25554;&#20837;&#26032;&#39062;&#29289;&#20307;&#65292;&#36825;&#20123;&#29289;&#20307;&#21487;&#33021;&#24433;&#21709;&#28216;&#25103;&#29609;&#27861;&#24182;&#20986;&#29616;&#22312;&#21508;&#31181;&#22823;&#23567;&#21644;&#20301;&#32622;&#20013;&#12290;&#25105;&#20204;&#30340;&#35270;&#35273;&#26032;&#39062;&#24615;&#26816;&#27979;&#22522;&#20934;&#21457;&#29616;&#65292;&#25511;&#21046;&#20551;&#38451;&#24615;&#26102;&#65292;&#26368;&#22909;&#30340;&#38754;&#31215;&#19979;&#26354;&#32447;&#24230;&#37327;&#30340;&#26041;&#27861;&#21487;&#33021;&#20250;&#34987;&#26356;&#31616;&#21333;&#30340;&#26367;&#20195;&#26041;&#27861;&#36229;&#36807;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order for artificial agents to successfully perform tasks in changing environments, they must be able to both detect and adapt to novelty. However, visual novelty detection research often only evaluates on repurposed datasets such as CIFAR-10 originally intended for object classification, where images focus on one distinct, well-centered object. New benchmarks are needed to represent the challenges of navigating the complex scenes of an open world. Our new NovelCraft dataset contains multimodal episodic data of the images and symbolic world-states seen by an agent completing a pogo stick assembly task within a modified Minecraft environment. In some episodes, we insert novel objects within the complex 3D scene that may impact gameplay and appear in a variety of sizes and positions. Our visual novelty detection benchmark finds that methods that rank best on popular area-under-the-curve metrics may be outperformed by simpler alternatives when controlling false positives matters most. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20462;&#25913;&#36807;&#30340;&#39532;&#36420;&#34444;&#20808;&#39564;&#30340;&#22810;&#35270;&#35282;&#28508;&#21464;&#37327;&#27169;&#22411;MuVI&#65292;&#29992;&#20110;&#24314;&#27169;&#32467;&#26500;&#31232;&#30095;&#24615;&#12290;&#23427;&#33021;&#22815;&#32435;&#20837;&#26377;&#38480;&#19988;&#22122;&#22768;&#30340;&#39046;&#22495;&#30693;&#35782;&#65292;&#20197;&#20869;&#22312;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#20998;&#26512;&#22810;&#35270;&#35282;&#25968;&#25454;&#65292;&#20248;&#20110;&#29616;&#26377;&#32467;&#26500;&#31232;&#30095;&#24615;&#24314;&#27169;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2204.06242</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#28508;&#21464;&#37327;&#27169;&#22411;&#20013;&#30340;&#39046;&#22495;&#30693;&#35782;&#32534;&#30721;:&#24102;&#32467;&#26500;&#31232;&#30095;&#36125;&#21494;&#26031;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Encoding Domain Knowledge in Multi-view Latent Variable Models: A Bayesian Approach with Structured Sparsity. (arXiv:2204.06242v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.06242
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20462;&#25913;&#36807;&#30340;&#39532;&#36420;&#34444;&#20808;&#39564;&#30340;&#22810;&#35270;&#35282;&#28508;&#21464;&#37327;&#27169;&#22411;MuVI&#65292;&#29992;&#20110;&#24314;&#27169;&#32467;&#26500;&#31232;&#30095;&#24615;&#12290;&#23427;&#33021;&#22815;&#32435;&#20837;&#26377;&#38480;&#19988;&#22122;&#22768;&#30340;&#39046;&#22495;&#30693;&#35782;&#65292;&#20197;&#20869;&#22312;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#20998;&#26512;&#22810;&#35270;&#35282;&#25968;&#25454;&#65292;&#20248;&#20110;&#29616;&#26377;&#32467;&#26500;&#31232;&#30095;&#24615;&#24314;&#27169;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#31995;&#32479;&#19981;&#20165;&#26377;&#26469;&#33258;&#21333;&#20010;&#25968;&#25454;&#28304;&#30340;&#25968;&#25454;&#65292;&#36824;&#26377;&#26469;&#33258;&#22810;&#20010;&#25968;&#25454;&#35270;&#35282;&#30340;&#25968;&#25454;&#12290;&#20363;&#22914;&#65292;&#22312;&#22522;&#22240;&#32452;&#21307;&#23398;&#20013;&#65292;&#24739;&#32773;&#21487;&#20197;&#36890;&#36807;&#26469;&#33258;&#19981;&#21516;&#20998;&#23376;&#23618;&#38754;&#30340;&#25968;&#25454;&#36827;&#34892;&#25551;&#36848;&#12290;&#21033;&#29992;&#20855;&#26377;&#32467;&#26500;&#31232;&#30095;&#24615;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#26159;&#25581;&#31034;&#25968;&#25454;&#35270;&#35282;&#20869;&#21644;&#36328;&#35270;&#35282;&#21464;&#21270;&#30340;&#24120;&#29992;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#21487;&#35299;&#37322;&#24615;&#36739;&#24046;&#65292;&#38656;&#35201;&#19987;&#23478;&#30452;&#25509;&#26816;&#26597;&#21644;&#35299;&#37322;&#27599;&#20010;&#35201;&#32032;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MuVI&#65292;&#19968;&#31181;&#22522;&#20110;&#20462;&#25913;&#36807;&#30340;&#39532;&#36420;&#34444;&#20808;&#39564;&#30340;&#26032;&#22411;&#22810;&#35270;&#35282;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#29992;&#20110;&#24314;&#27169;&#32467;&#26500;&#31232;&#30095;&#24615;&#12290;&#36825;&#26377;&#21161;&#20110;&#23545;&#26377;&#38480;&#30340;&#21644;&#22122;&#22768;&#39046;&#22495;&#30693;&#35782;&#36827;&#34892;&#32435;&#20837;&#65292;&#20174;&#32780;&#20197;&#20869;&#22312;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#20998;&#26512;&#22810;&#35270;&#35282;&#25968;&#25454;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#37325;&#24314;&#35823;&#24046;&#21644;&#31934;&#30830;&#24230;/&#21484;&#22238;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#32467;&#26500;&#31232;&#30095;&#24615;&#24314;&#27169;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#31283;&#20581;&#22320;&#25972;&#21512;&#22122;&#22768;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world systems are described not only by data from a single source but via multiple data views. In genomic medicine, for instance, patients can be characterized by data from different molecular layers. Latent variable models with structured sparsity are a commonly used tool for disentangling variation within and across data views. However, their interpretability is cumbersome since it requires a direct inspection and interpretation of each factor from domain experts. Here, we propose MuVI, a novel multi-view latent variable model based on a modified horseshoe prior for modeling structured sparsity. This facilitates the incorporation of limited and noisy domain knowledge, thereby allowing for an analysis of multi-view data in an inherently explainable manner. We demonstrate that our model (i) outperforms state-of-the-art approaches for modeling structured sparsity in terms of the reconstruction error and the precision/recall, (ii) robustly integrates noisy domain expertise in t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#26426;&#22120;&#20154;&#35270;&#35273;&#35302;&#35273;&#29289;&#20307;&#24863;&#30693;&#30340;&#29616;&#29366;&#20197;&#21450;&#25361;&#25112;&#12290;&#20154;&#31867;&#22810;&#27169;&#24577;&#29289;&#20307;&#24863;&#30693;&#30340;&#29983;&#29289;&#23398;&#22522;&#30784;&#21644;&#26368;&#26032;&#30340;&#26426;&#22120;&#20154;&#24863;&#30693;&#25216;&#26415;&#21644;&#25968;&#25454;&#37319;&#38598;&#31574;&#30053;&#34987;&#27010;&#36848;&#65292;&#21516;&#26102;&#37325;&#28857;&#20171;&#32461;&#20102;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2203.11544</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#30340;&#35270;&#35273;&#35302;&#35273;&#29289;&#20307;&#24863;&#30693;&#65306;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
Visuo-Haptic Object Perception for Robots: An Overview. (arXiv:2203.11544v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.11544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#26426;&#22120;&#20154;&#35270;&#35273;&#35302;&#35273;&#29289;&#20307;&#24863;&#30693;&#30340;&#29616;&#29366;&#20197;&#21450;&#25361;&#25112;&#12290;&#20154;&#31867;&#22810;&#27169;&#24577;&#29289;&#20307;&#24863;&#30693;&#30340;&#29983;&#29289;&#23398;&#22522;&#30784;&#21644;&#26368;&#26032;&#30340;&#26426;&#22120;&#20154;&#24863;&#30693;&#25216;&#26415;&#21644;&#25968;&#25454;&#37319;&#38598;&#31574;&#30053;&#34987;&#27010;&#36848;&#65292;&#21516;&#26102;&#37325;&#28857;&#20171;&#32461;&#20102;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#30340;&#29289;&#20307;&#24863;&#30693;&#33021;&#21147;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#32780;&#22312;&#24320;&#21457;&#25317;&#26377;&#31867;&#20284;&#29087;&#32451;&#25216;&#33021;&#30340;&#33258;&#20027;&#26426;&#22120;&#20154;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#65292;&#36825;&#19968;&#28857;&#21464;&#24471;&#26356;&#21152;&#26126;&#26174;&#12290;&#34429;&#28982;&#20154;&#24037;&#35270;&#35273;&#21644;&#35302;&#35273;&#25216;&#26415;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#36825;&#20004;&#31181;&#20256;&#24863;&#27169;&#24335;&#22312;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#25972;&#21512;&#20173;&#26377;&#24453;&#25913;&#21892;&#65292;&#23384;&#22312;&#30528;&#20960;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#21463;&#21040;&#20154;&#31867;&#22914;&#20309;&#36890;&#36807;&#35270;&#35273;&#21644;&#35302;&#35273;&#30693;&#35273;&#26469;&#24863;&#30693;&#29289;&#20307;&#23646;&#24615;&#21644;&#25512;&#21160;&#25163;&#21160;&#20219;&#21153;&#25191;&#34892;&#30340;&#21551;&#21457;&#65292;&#24635;&#32467;&#20102;&#26426;&#22120;&#20154;&#30340;&#35270;&#35273;&#35302;&#35273;&#29289;&#20307;&#24863;&#30693;&#30340;&#29616;&#29366;&#12290;&#39318;&#20808;&#65292;&#27010;&#36848;&#20102;&#20154;&#31867;&#22810;&#27169;&#24577;&#29289;&#20307;&#24863;&#30693;&#30340;&#29983;&#29289;&#23398;&#22522;&#30784;&#12290;&#25509;&#19979;&#26469;&#65292;&#35752;&#35770;&#20102;&#26426;&#22120;&#20154;&#30340;&#24863;&#30693;&#25216;&#26415;&#21644;&#25968;&#25454;&#37319;&#38598;&#31574;&#30053;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#28982;&#21518;&#65292;&#20171;&#32461;&#20102;&#20027;&#35201;&#30340;&#35745;&#31639;&#25216;&#26415;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The object perception capabilities of humans are impressive, and this becomes even more evident when trying to develop solutions with a similar proficiency in autonomous robots. While there have been notable advancements in the technologies for artificial vision and touch, the effective integration of these two sensory modalities in robotic applications still needs to be improved, and several open challenges exist. Taking inspiration from how humans combine visual and haptic perception to perceive object properties and drive the execution of manual tasks, this article summarises the current state of the art of visuo-haptic object perception in robots. Firstly, the biological basis of human multimodal object perception is outlined. Then, the latest advances in sensing technologies and data collection strategies for robots are discussed. Next, an overview of the main computational techniques is presented, highlighting the main challenges of multimodal machine learning and presenting a fe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#38463;&#25289;&#20271;&#35821;&#35821;&#26009;&#24211;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#39046;&#22495;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2201.09227</link><description>&lt;p&gt;
&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#20803;&#21270;&#30340;&#38463;&#25289;&#20271;&#35821;&#35821;&#26009;&#24211;&#29992;&#20110;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
A Large and Diverse Arabic Corpus for Language Modeling. (arXiv:2201.09227v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.09227
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#38463;&#25289;&#20271;&#35821;&#35821;&#26009;&#24211;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#39046;&#22495;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#24341;&#20837;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24314;&#27169;&#30340;&#37325;&#22823;&#33539;&#24335;&#36716;&#21464;&#65292;&#20854;&#20013;&#22823;&#22411;&#39044;&#20808;&#35757;&#32451;&#30340;LM&#24050;&#32463;&#25104;&#20026;&#22823;&#22810;&#25968;NLP&#20219;&#21153;&#19981;&#21487;&#20998;&#21106;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;LM&#36275;&#22815;&#26234;&#33021;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#35821;&#35328;&#30340;&#26377;&#29992;&#21644;&#30456;&#20851;&#34920;&#31034;&#12290;&#36825;&#20123;&#27169;&#22411;&#34987;&#29992;&#20110;&#23545;&#24120;&#35268;NLP&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#20855;&#26377;&#26174;&#30528;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#30456;&#21453;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#35757;&#32451;&#38656;&#35201;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#35821;&#26009;&#24211;&#65292;&#36825;&#20010;&#35821;&#26009;&#24211;&#21487;&#20197;&#24456;&#22909;&#22320;&#20195;&#34920;&#38463;&#25289;&#20271;&#35821;&#12290;&#30001;&#20110;&#33521;&#35821;&#35821;&#26009;&#24211;&#21487;&#33719;&#24471;&#22823;&#37327;&#36164;&#28304;&#65292;&#22240;&#27492;&#33521;&#35821;LM&#36890;&#24120;&#27604;&#20854;&#20182;&#35821;&#35328;LM&#34920;&#29616;&#26356;&#22909;&#12290;&#26412;&#25991;&#35814;&#32454;&#25551;&#36848;&#20102;&#19968;&#20010;&#22823;&#22411;&#38463;&#25289;&#20271;&#35821;&#35821;&#26009;&#24211;&#30340;&#35774;&#35745;&#21644;&#24320;&#21457;&#12290;&#23427;&#30001;&#36229;&#36807;500GB&#30340;&#24050;&#21152;&#24037;&#30340;&#38463;&#25289;&#20271;&#25991;&#26412;&#32452;&#25104;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#39046;&#22495;&#30693;&#35782;&#21644;&#19979;&#28216;&#25512;&#29702;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#35813;&#35821;&#26009;&#24211;&#36824;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;&#38463;&#25289;&#20271;&#35821;LM&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) have introduced a major paradigm shift in Natural Language Processing (NLP) modeling where large pre-trained LMs became integral to most of the NLP tasks. The LMs are intelligent enough to find useful and relevant representations of the language without any supervision. Perhaps, these models are used to fine-tune typical NLP tasks with significantly high accuracy as compared to the traditional approaches. Conversely, the training of these models requires a massively large corpus that is a good representation of the language. English LMs generally perform better than their other language counterparts, due to the availability of massive English corpora. This work elaborates on the design and development of a large Arabic corpus. It consists of over 500 GB of Arabic cleaned text targeted at improving cross-domain knowledge and downstream generalization capability of large-scale language models. Moreover, the corpus is utilized in the training of a large Arabic LM. In
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21457;&#29616;&#20102;&#23545;&#25239;&#35757;&#32451;&#20013;&#23384;&#22312;&#30340;&#26631;&#31614;&#22122;&#22768;&#65292;&#24182;&#35299;&#37322;&#20102;&#20854;&#23545;&#40065;&#26834;&#36807;&#24230;&#25311;&#21512;&#30340;&#26222;&#36941;&#23384;&#22312;&#20197;&#21450;&#25200;&#21160;&#21322;&#24452;&#21644;&#25968;&#25454;&#36136;&#37327;&#30340;&#20381;&#36182;&#24615;&#12290;&#36890;&#36807;&#35813;&#35770;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#26657;&#20934;&#26631;&#31614;&#20197;&#24212;&#23545;&#26631;&#31614;&#22122;&#22768;&#21644;&#40065;&#26834;&#36807;&#24230;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2110.03135</link><description>&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#26631;&#31614;&#22122;&#22768;&#65306;&#30740;&#31350;&#40065;&#26834;&#36807;&#24230;&#25311;&#21512;&#30340;&#26032;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Label Noise in Adversarial Training: A Novel Perspective to Study Robust Overfitting. (arXiv:2110.03135v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.03135
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21457;&#29616;&#20102;&#23545;&#25239;&#35757;&#32451;&#20013;&#23384;&#22312;&#30340;&#26631;&#31614;&#22122;&#22768;&#65292;&#24182;&#35299;&#37322;&#20102;&#20854;&#23545;&#40065;&#26834;&#36807;&#24230;&#25311;&#21512;&#30340;&#26222;&#36941;&#23384;&#22312;&#20197;&#21450;&#25200;&#21160;&#21322;&#24452;&#21644;&#25968;&#25454;&#36136;&#37327;&#30340;&#20381;&#36182;&#24615;&#12290;&#36890;&#36807;&#35813;&#35770;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#26657;&#20934;&#26631;&#31614;&#20197;&#24212;&#23545;&#26631;&#31614;&#22122;&#22768;&#21644;&#40065;&#26834;&#36807;&#24230;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#23384;&#22312;&#26631;&#31614;&#22122;&#22768;&#12290;&#36825;&#31181;&#26631;&#31614;&#22122;&#22768;&#26159;&#30001;&#20110;&#23545;&#25239;&#26679;&#26412;&#30340;&#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#19982;&#20174;&#24178;&#20928;&#26679;&#26412;&#32487;&#25215;&#30340;&#26631;&#31614;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#36896;&#25104;&#30340; - &#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#34987;&#23545;&#25239;&#25200;&#21160;&#25197;&#26354;&#65292;&#20294;&#20174;&#24178;&#20928;&#26679;&#26412;&#32487;&#25215;&#26631;&#31614;&#30340;&#24120;&#35265;&#20570;&#27861;&#21364;&#24573;&#30053;&#20102;&#36825;&#19968;&#28857;&#12290;&#35748;&#35782;&#21040;&#26631;&#31614;&#22122;&#22768;&#26377;&#21161;&#20110;&#27934;&#23519;&#23545;&#25239;&#35757;&#32451;&#20013;&#40065;&#26834;&#36807;&#24230;&#25311;&#21512;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#24182;&#35299;&#37322;&#20102;&#20854;&#23545;&#25200;&#21160;&#21322;&#24452;&#21644;&#25968;&#25454;&#36136;&#37327;&#30340;&#22855;&#29305;&#20381;&#36182;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26631;&#31614;&#22122;&#22768;&#35270;&#35282;&#19982;&#25105;&#20204;&#23545;&#23545;&#25239;&#35757;&#32451;&#20013;&#32426;&#20803;&#21452;&#19979;&#38477;&#29616;&#35937;&#30340;&#35266;&#23519;&#30456;&#21563;&#21512;&#12290;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#33258;&#21160;&#26657;&#20934;&#26631;&#31614;&#20197;&#24212;&#23545;&#26631;&#31614;&#22122;&#22768;&#21644;&#40065;&#26834;&#36807;&#24230;&#25311;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19981;&#24341;&#20837;&#26032;&#30340;&#36229;&#21442;&#25968;&#25110;&#39069;&#22806;&#30340;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that label noise exists in adversarial training. Such label noise is due to the mismatch between the true label distribution of adversarial examples and the label inherited from clean examples - the true label distribution is distorted by the adversarial perturbation, but is neglected by the common practice that inherits labels from clean examples. Recognizing label noise sheds insights on the prevalence of robust overfitting in adversarial training, and explains its intriguing dependence on perturbation radius and data quality. Also, our label noise perspective aligns well with our observations of the epoch-wise double descent in adversarial training. Guided by our analyses, we proposed a method to automatically calibrate the label to address the label noise and robust overfitting. Our method achieves consistent performance improvements across various models and datasets without introducing new hyper-parameters or additional tuning.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;Unity&#28216;&#25103;&#24341;&#25806;&#30340;&#24320;&#28304;&#29983;&#24577;&#31995;&#32479;&#27169;&#25311;&#22120;Ecotwin&#65292;&#20854;&#20013;&#21160;&#29289;&#35748;&#30693;&#30340;&#24314;&#27169;&#36890;&#36807;&#25972;&#21512;&#19977;&#20010;&#29420;&#31435;&#30340;&#32593;&#32476;&#23454;&#29616;&#65292;&#27169;&#25311;&#33258;&#28982;&#29616;&#35937;&#22312;&#27169;&#22411;&#20013;&#20986;&#29616;&#32780;&#26080;&#38656;&#30828;&#32534;&#30721;&#12290;</title><link>http://arxiv.org/abs/2108.07578</link><description>&lt;p&gt;
&#36890;&#24448;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#29983;&#24577;&#31995;&#32479;&#20043;&#36335;
&lt;/p&gt;
&lt;p&gt;
The Ecosystem Path to General AI. (arXiv:2108.07578v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.07578
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;Unity&#28216;&#25103;&#24341;&#25806;&#30340;&#24320;&#28304;&#29983;&#24577;&#31995;&#32479;&#27169;&#25311;&#22120;Ecotwin&#65292;&#20854;&#20013;&#21160;&#29289;&#35748;&#30693;&#30340;&#24314;&#27169;&#36890;&#36807;&#25972;&#21512;&#19977;&#20010;&#29420;&#31435;&#30340;&#32593;&#32476;&#23454;&#29616;&#65292;&#27169;&#25311;&#33258;&#28982;&#29616;&#35937;&#22312;&#27169;&#22411;&#20013;&#20986;&#29616;&#32780;&#26080;&#38656;&#30828;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39318;&#20808;&#35752;&#35770;&#20102;&#29983;&#24577;&#31995;&#32479;&#27169;&#25311;&#22120;&#19982;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#38543;&#21518;&#25105;&#20204;&#20171;&#32461;&#20102;&#22522;&#20110;Unity&#28216;&#25103;&#24341;&#25806;&#30340;&#24320;&#28304;&#29983;&#24577;&#31995;&#32479;&#27169;&#25311;&#22120;Ecotwin&#65292;&#23427;&#21487;&#20197;&#36816;&#34892;&#21253;&#21547;&#26080;&#29983;&#21629;&#29289;&#20307;&#22914;&#23665;&#33033;&#21644;&#28246;&#27850;&#20197;&#21450;&#21253;&#25324;&#21160;&#29289;&#21644;&#26893;&#29289;&#30340;&#29983;&#24577;&#31995;&#32479;&#12290;&#21160;&#29289;&#35748;&#30693;&#30340;&#24314;&#27169;&#26159;&#36890;&#36807;&#25972;&#21512;&#19977;&#20010;&#29420;&#31435;&#30340;&#32593;&#32476;&#26469;&#23454;&#29616;&#30340;&#65306;&#65288;i&#65289;&#30828;&#36830;&#36890;&#30340;&#21453;&#23556;&#32593;&#32476;&#65307;&#65288;ii&#65289;&#24184;&#31119;&#32593;&#32476;&#65292;&#29992;&#20110;&#23558;&#20256;&#24863;&#25968;&#25454;&#65292;&#22914;&#27687;&#27668;&#12289;&#27700;&#12289;&#33021;&#37327;&#21644;&#27668;&#21619;&#65292;&#26144;&#23556;&#21040;&#19968;&#20010;&#26631;&#37327;&#24184;&#31119;&#20540;&#65307;&#65288;iii&#65289;&#36873;&#25321;&#34892;&#21160;&#30340;&#31574;&#30053;&#32593;&#32476;&#12290;&#31574;&#30053;&#32593;&#32476;&#26159;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#20854;&#20013;&#22870;&#21169;&#20449;&#21495;&#34987;&#23450;&#20041;&#20026;&#20174;&#19968;&#20010;&#26102;&#38388;&#27493;&#21040;&#19979;&#19968;&#20010;&#26102;&#38388;&#27493;&#30340;&#24184;&#31119;&#24046;&#12290;&#25152;&#26377;&#30340;&#29983;&#29289;&#33021;&#22815;&#36827;&#34892;&#26377;&#24615;&#25110;&#26080;&#24615;&#32321;&#27542;&#65292;&#24182;&#19988;&#22914;&#26524;&#23427;&#20204;&#32791;&#23613;&#20102;&#20851;&#38190;&#36164;&#28304;&#65292;&#23427;&#20204;&#23601;&#20250;&#27515;&#20129;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#20351;&#29992;Ecotwin&#36827;&#34892;&#30340;&#19977;&#20010;&#30740;&#31350;&#30340;&#32467;&#26524;&#65292;&#20854;&#20013;&#33258;&#28982;&#29616;&#35937;&#22312;&#27169;&#22411;&#20013;&#20986;&#29616;&#32780;&#26080;&#38656;&#30828;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
We start by discussing the link between ecosystem simulators and general AI. Then we present the open-source ecosystem simulator Ecotwin, which is based on the game engine Unity and operates on ecosystems containing inanimate objects like mountains and lakes, as well as organisms such as animals and plants. Animal cognition is modeled by integrating three separate networks: (i) a reflex network for hard-wired reflexes; (ii) a happiness network that maps sensory data such as oxygen, water, energy, and smells, to a scalar happiness value; and (iii) a policy network for selecting actions. The policy network is trained with reinforcement learning (RL), where the reward signal is defined as the happiness difference from one time step to the next. All organisms are capable of either sexual or asexual reproduction, and they die if they run out of critical resources. We report results from three studies with Ecotwin, in which natural phenomena emerge in the models without being hardwired. Firs
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Quantal Hierarchy&#27169;&#22411;&#65292;&#25918;&#23485;&#20102;&#20114;&#30456;&#19968;&#33268;&#24615;&#21644;&#26368;&#20248;&#21453;&#24212;&#30340;&#38480;&#21046;&#65292;&#20351;&#24471;&#21487;&#20197;&#36817;&#20284;&#32423;&#21035;K&#65292;QRE&#25110;Nash&#22343;&#34913;&#34892;&#20026;&#12290;&#36825;&#31181;&#27169;&#22411;&#22522;&#20110;&#21464;&#20998;&#33258;&#30001;&#33021;&#21407;&#29702;&#30340;&#36882;&#24402;&#24418;&#24335;&#65292;&#23558;&#26356;&#39640;&#38454;&#25512;&#29702;&#34920;&#31034;&#20026;&#28436;&#21270;&#24418;&#24335;&#21338;&#24328;&#26641;&#20013;&#30340;&#65288;&#20266;&#65289;&#24207;&#21015;&#20915;&#31574;&#21046;&#23450;&#12290;</title><link>http://arxiv.org/abs/2106.15844</link><description>&lt;p&gt;
&#26377;&#30028;&#29702;&#24615;&#19979;&#30340;&#37327;&#32423;&#24207;&#21015;&#27169;&#22411;&#65306;&#25918;&#23485;&#26368;&#20248;&#21453;&#24212;&#21644;&#20114;&#30456;&#19968;&#33268;&#24615;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Bounded rationality for relaxing best response and mutual consistency: The Quantal Hierarchy model of decision-making. (arXiv:2106.15844v5 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.15844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Quantal Hierarchy&#27169;&#22411;&#65292;&#25918;&#23485;&#20102;&#20114;&#30456;&#19968;&#33268;&#24615;&#21644;&#26368;&#20248;&#21453;&#24212;&#30340;&#38480;&#21046;&#65292;&#20351;&#24471;&#21487;&#20197;&#36817;&#20284;&#32423;&#21035;K&#65292;QRE&#25110;Nash&#22343;&#34913;&#34892;&#20026;&#12290;&#36825;&#31181;&#27169;&#22411;&#22522;&#20110;&#21464;&#20998;&#33258;&#30001;&#33021;&#21407;&#29702;&#30340;&#36882;&#24402;&#24418;&#24335;&#65292;&#23558;&#26356;&#39640;&#38454;&#25512;&#29702;&#34920;&#31034;&#20026;&#28436;&#21270;&#24418;&#24335;&#21338;&#24328;&#26641;&#20013;&#30340;&#65288;&#20266;&#65289;&#24207;&#21015;&#20915;&#31574;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21338;&#24328;&#35770;&#23545;&#20915;&#31574;&#21046;&#23450;&#20855;&#26377;&#36716;&#22411;&#24615;&#30340;&#20316;&#29992;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#25152;&#20551;&#35774;&#30340;&#26465;&#20214;&#21487;&#33021;&#36807;&#20110;&#20005;&#26684;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#29702;&#24615;&#30340;&#19968;&#20123;&#28508;&#22312;&#20551;&#35774;&#65292;&#22914;&#20114;&#30456;&#19968;&#33268;&#24615;&#21644;&#26368;&#20248;&#21453;&#24212;&#65292;&#24182;&#32771;&#34385;&#20351;&#29992;&#32423;&#21035;K&#25512;&#29702;&#21644;&#37327;&#21270;&#21453;&#39304;&#24179;&#34913;&#65288;QRE&#65289;&#30340;&#27010;&#24565;&#25918;&#23485;&#36825;&#20123;&#20551;&#35774;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Quantal Hierarchy&#27169;&#22411;&#30340;&#20449;&#24687;&#35770;&#21452;&#21442;&#25968;&#27169;&#22411;&#65292;&#21487;&#20197;&#25918;&#23485;&#20114;&#30456;&#19968;&#33268;&#24615;&#21644;&#26368;&#20248;&#21453;&#24212;&#30340;&#38480;&#21046;&#65292;&#21516;&#26102;&#22312;&#26497;&#38480;&#24773;&#20917;&#19979;&#36817;&#20284;&#32423;&#21035;K&#65292;QRE&#25110;&#20856;&#22411;&#30340;Nash&#22343;&#34913;&#34892;&#20026;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#21464;&#20998;&#33258;&#30001;&#33021;&#21407;&#29702;&#30340;&#36882;&#24402;&#24418;&#24335;&#65292;&#23558;&#26356;&#39640;&#38454;&#25512;&#29702;&#34920;&#31034;&#20026;&#28436;&#21270;&#24418;&#24335;&#21338;&#24328;&#26641;&#20013;&#30340;&#65288;&#20266;&#65289;&#24207;&#21015;&#20915;&#31574;&#21046;&#23450;&#12290;&#36825;&#31181;&#34920;&#31034;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#20197;&#31867;&#20284;&#20110;&#39034;&#24207;&#21338;&#24328;&#30340;&#26041;&#24335;&#22788;&#29702;&#21516;&#26102;&#21338;&#24328;&#65292;&#20854;&#20013;&#25512;&#29702;&#36164;&#28304;&#22312;&#28216;&#25103;&#20013;&#36880;&#28176;&#32791;&#23613;&#12290;
&lt;/p&gt;
&lt;p&gt;
While game theory has been transformative for decision-making, the assumptions made can be overly restrictive in certain instances. In this work, we investigate some of the underlying assumptions of rationality, such as mutual consistency and best response, and consider ways to relax these assumptions using concepts from level-$k$ reasoning and quantal response equilibrium (QRE) respectively. Specifically, we propose an information-theoretic two-parameter model called the Quantal Hierarchy model, which can relax both mutual consistency and best response while still approximating level-$k$, QRE, or typical Nash equilibrium behaviour in the limiting cases. The model is based on a recursive form of the variational free energy principle, representing higher-order reasoning as (pseudo) sequential decision-making in extensive-form game tree. This representation enables us to treat simultaneous games in a similar manner to sequential games, where reasoning resources deplete throughout the gam
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; Label Mask &#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#65288;LM-MTC&#65289;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#26469;&#25429;&#25417;&#26631;&#31614;&#20043;&#38388;&#30340;&#38544;&#21547;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#26631;&#31614;&#30340;&#36974;&#30422;&#35821;&#35328;&#27169;&#22411;&#65288;MLM&#65289;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2106.10076</link><description>&lt;p&gt;
&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#30340;&#26631;&#31614;&#25552;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Label prompt for multi-label text classification. (arXiv:2106.10076v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.10076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; Label Mask &#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#65288;LM-MTC&#65289;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#26469;&#25429;&#25417;&#26631;&#31614;&#20043;&#38388;&#30340;&#38544;&#21547;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#26631;&#31614;&#30340;&#36974;&#30422;&#35821;&#35328;&#27169;&#22411;&#65288;MLM&#65289;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20013;&#65292;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#22914;&#20309;&#21033;&#29992;&#26631;&#31614;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#20294;&#26159;&#65292;&#22312;&#19968;&#20010;&#22797;&#26434;&#21644;&#26410;&#30693;&#30340;&#26631;&#31614;&#31354;&#38388;&#20013;&#30452;&#25509;&#24314;&#27169;&#26631;&#31614;&#20043;&#38388;&#30340;&#20851;&#32852;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181; Label Mask &#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#65288;LM-MTC&#65289;&#65292;&#23427;&#21463;&#21040;&#35821;&#35328;&#27169;&#22411;&#30340;&#22635;&#31354;&#38382;&#39064;&#24605;&#24819;&#30340;&#21551;&#21457;&#12290;LM-MTC &#33021;&#22815;&#36890;&#36807;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#33021;&#21147;&#25429;&#25417;&#26631;&#31614;&#20043;&#38388;&#30340;&#38544;&#21547;&#20851;&#31995;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#28508;&#22312;&#26631;&#31614;&#20998;&#37197;&#19968;&#20010;&#19981;&#21516;&#30340;&#26631;&#35760;&#65292;&#24182;&#20197;&#19968;&#23450;&#27010;&#29575;&#38543;&#26426;&#36974;&#30422;&#35813;&#26631;&#35760;&#65292;&#24314;&#31435;&#20102;&#22522;&#20110;&#26631;&#31614;&#30340;&#36974;&#25513;&#35821;&#35328;&#27169;&#22411;&#65288;MLM&#65289;&#12290;&#25105;&#20204;&#21516;&#26102;&#35757;&#32451; MTC &#21644; MLM&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the key problems in multi-label text classification is how to take advantage of the correlation among labels. However, it is very challenging to directly model the correlations among labels in a complex and unknown label space. In this paper, we propose a Label Mask multi-label text classification model (LM-MTC), which is inspired by the idea of cloze questions of language model. LM-MTC is able to capture implicit relationships among labels through the powerful ability of pre-train language models. On the basis, we assign a different token to each potential label, and randomly mask the token with a certain probability to build a label based Masked Language Model (MLM). We train the MTC and MLM together, further improving the generalization ability of the model. A large number of experiments on multiple datasets demonstrate the effectiveness of our method.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#31639;&#27861;ANTS&#65292;&#23427;&#23558;&#35268;&#21010;&#21644;&#23398;&#20064;&#32467;&#21512;&#22312;&#26368;&#22823;&#29109;&#33539;&#24335;&#20013;&#65292;&#24182;&#36890;&#36807;&#22312;Atari&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20854;&#26126;&#26174;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;AlphaZero&#31995;&#32479;&#30340;&#35268;&#21010;&#32452;&#20214;PUCT&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#31283;&#20581;&#24615;&#65292;&#21487;&#25512;&#21160;&#22522;&#20110;&#26641;&#30340;&#35268;&#21010;&#26041;&#27861;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2102.06808</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#29109;&#26641;&#25628;&#32034;&#30340;&#35268;&#21010;&#19982;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Planning and Learning Using Adaptive Entropy Tree Search. (arXiv:2102.06808v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.06808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#31639;&#27861;ANTS&#65292;&#23427;&#23558;&#35268;&#21010;&#21644;&#23398;&#20064;&#32467;&#21512;&#22312;&#26368;&#22823;&#29109;&#33539;&#24335;&#20013;&#65292;&#24182;&#36890;&#36807;&#22312;Atari&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20854;&#26126;&#26174;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;AlphaZero&#31995;&#32479;&#30340;&#35268;&#21010;&#32452;&#20214;PUCT&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#31283;&#20581;&#24615;&#65292;&#21487;&#25512;&#21160;&#22522;&#20110;&#26641;&#30340;&#35268;&#21010;&#26041;&#27861;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#20154;&#24037;&#26234;&#33021;&#31361;&#30772;&#34920;&#26126;&#65292;&#23558;&#22522;&#20110;&#26641;&#30340;&#35268;&#21010;&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20316;&#33258;&#36866;&#24212;&#29109;&#26641;&#25628;&#32034;&#65288;ANTS&#65289;&#30340;&#20840;&#26032;&#31639;&#27861;&#65292;&#23558;&#35268;&#21010;&#21644;&#23398;&#20064;&#32467;&#21512;&#22312;&#26368;&#22823;&#29109;&#33539;&#24335;&#20013;&#12290;&#36890;&#36807;&#22312;Atari&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;ANTS&#26126;&#26174;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;AlphaZero&#31995;&#32479;&#30340;&#35268;&#21010;&#32452;&#20214;PUCT&#12290;ANTS&#24314;&#31435;&#22312;&#26368;&#22823;&#29109;&#35268;&#21010;&#26041;&#27861;&#30340;&#22522;&#30784;&#20043;&#19978;&#65292;&#28982;&#32780;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#22312;&#19982;&#23398;&#20064;&#30456;&#32467;&#21512;&#26102;&#34920;&#29616;&#19981;&#20339;&#65292;ANTS&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#24182;&#36798;&#21040;&#20102;&#19982;&#30446;&#21069;&#26368;&#20808;&#36827;&#31639;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;ANTS&#22312;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#19979;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;ANTS&#30340;&#39640;&#24615;&#33021;&#21644;&#31283;&#20581;&#24615;&#23558;&#20351;&#22522;&#20110;&#26641;&#30340;&#35268;&#21010;&#26041;&#27861;&#26356;&#21152;&#36866;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent breakthroughs in Artificial Intelligence have shown that the combination of tree-based planning with deep learning can lead to superior performance. We present Adaptive Entropy Tree Search (ANTS) - a novel algorithm combining planning and learning in the maximum entropy paradigm. Through a comprehensive suite of experiments on the Atari benchmark we show that ANTS significantly outperforms PUCT, the planning component of the state-of-the-art AlphaZero system. ANTS builds upon recent work on maximum entropy planning methods - which however, as we show, fail in combination with learning. ANTS resolves this issue to reach state-of-the-art performance. We further find that ANTS exhibits superior robustness to different hyperparameter choices, compared to the previous algorithms. We believe that the high performance and robustness of ANTS can bring tree search planning one step closer to wide practical adoption.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#21160;&#24577;&#35748;&#30693;&#36923;&#36753;&#20013;&#30340;&#20449;&#24687;&#21464;&#21270;&#37327;&#21270;&#27169;&#24577;&#65292;&#21253;&#25324;&#20844;&#29702;&#21270;&#12289;&#34920;&#36848;&#26041;&#24335;&#12289;&#21487;&#20915;&#23450;&#24615;&#21644;&#22797;&#26434;&#24615;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#21644;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2004.05802</link><description>&lt;p&gt;
&#21160;&#24577;&#35748;&#30693;&#36923;&#36753;&#20013;&#30340;&#20449;&#24687;&#21464;&#21270;&#37327;&#21270;&#27169;&#24577;&#65306;&#27010;&#36848;&#19982;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
To Be Announced. (arXiv:2004.05802v4 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2004.05802
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#21160;&#24577;&#35748;&#30693;&#36923;&#36753;&#20013;&#30340;&#20449;&#24687;&#21464;&#21270;&#37327;&#21270;&#27169;&#24577;&#65292;&#21253;&#25324;&#20844;&#29702;&#21270;&#12289;&#34920;&#36848;&#26041;&#24335;&#12289;&#21487;&#20915;&#23450;&#24615;&#21644;&#22797;&#26434;&#24615;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#21644;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#20855;&#26377;&#20449;&#24687;&#21464;&#21270;&#37327;&#21270;&#27169;&#24577;&#30340;&#21160;&#24577;&#35748;&#30693;&#36923;&#36753;&#65292;&#24182;&#20171;&#32461;&#20102;&#20854;&#23436;&#25972;&#30340;&#20844;&#29702;&#21270;&#12290;&#25105;&#20204;&#20851;&#27880;&#20102;&#28041;&#21450;&#30693;&#35782;&#21644;&#37327;&#35789;&#20132;&#20114;&#20316;&#29992;&#30340;&#20844;&#29702;&#65292;&#22312;&#34920;&#36848;&#26041;&#24335;&#12289;&#21487;&#20915;&#23450;&#24615;&#12289;&#27169;&#22411;&#26816;&#26597;&#21644;&#21487;&#28385;&#36275;&#24615;&#30340;&#22797;&#26434;&#24615;&#65292;&#20197;&#21450;&#24212;&#29992;&#26041;&#38754;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#24320;&#25918;&#30340;&#38382;&#39064;&#21644;&#30740;&#31350;&#26032;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this survey we review dynamic epistemic logics with modalities for quantification over information change. Of such logics we present complete axiomatizations, focussing on axioms involving the interaction between knowledge and such quantifiers, we report on their relative expressivity, on decidability and on the complexity of model checking and satisfiability, and on applications. We focus on open problems and new directions for research.
&lt;/p&gt;</description></item></channel></rss>