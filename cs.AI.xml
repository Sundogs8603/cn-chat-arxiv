<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25193;&#25955;&#27169;&#22411;&#30340;&#23494;&#24230;&#20272;&#35745;&#21487;&#20197;&#34987;&#29992;&#20316;&#38646;&#26679;&#26412;&#20998;&#31867;&#65292;&#20316;&#32773;&#30340;&#29983;&#25104;&#24335;&#20998;&#31867;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#24378;&#22823;&#30340;&#32467;&#26524;&#65292;&#24182;&#20855;&#26377;&#26356;&#24378;&#30340;&#22810;&#27169;&#24335;&#20851;&#31995;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.16203</link><description>&lt;p&gt;
&#24744;&#30340;&#25193;&#25955;&#27169;&#22411;&#26263;&#20013;&#26159;&#19968;&#31181;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Your Diffusion Model is Secretly a Zero-Shot Classifier. (arXiv:2303.16203v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16203
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#23494;&#24230;&#20272;&#35745;&#21487;&#20197;&#34987;&#29992;&#20316;&#38646;&#26679;&#26412;&#20998;&#31867;&#65292;&#20316;&#32773;&#30340;&#29983;&#25104;&#24335;&#20998;&#31867;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#24378;&#22823;&#30340;&#32467;&#26524;&#65292;&#24182;&#20855;&#26377;&#26356;&#24378;&#30340;&#22810;&#27169;&#24335;&#20851;&#31995;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#26497;&#22823;&#22320;&#22686;&#24378;&#20102;&#25105;&#20204;&#30340;&#22522;&#20110;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#20026;&#22823;&#37327;&#25552;&#31034;&#29983;&#25104;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#24182;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#12290;&#20960;&#20046;&#25152;&#26377;&#30340;&#29992;&#20363;&#21040;&#30446;&#21069;&#20026;&#27490;&#37117;&#21482;&#20851;&#27880;&#25277;&#26679;&#65292;&#28982;&#32780;&#65292;&#25193;&#25955;&#27169;&#22411;&#36824;&#21487;&#20197;&#25552;&#20379;&#26377;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#20043;&#22806;&#30340;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31867;&#20284;&#20110;Stable Diffusion&#30340;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#23494;&#24230;&#20272;&#35745;&#21487;&#20197;&#34987;&#21033;&#29992;&#26469;&#25191;&#34892;&#38646;&#26679;&#26412;&#20998;&#31867;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#29983;&#25104;&#24335;&#20998;&#31867;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#32467;&#26524;&#65292;&#24182;&#20248;&#20110;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#25552;&#21462;&#30693;&#35782;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#25105;&#20204;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#27604;&#31454;&#20105;&#24615;&#30340;&#23545;&#27604;&#26041;&#27861;&#20855;&#26377;&#26356;&#24378;&#30340;&#22810;&#27169;&#24335;&#20851;&#31995;&#25512;&#29702;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#21576;&#29616;&#20102;&#23450;&#24615;&#32467;&#26524;&#65292;&#35777;&#26126;&#23427;&#23398;&#20064;&#20102;&#26377;&#24847;&#20041;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent wave of large-scale text-to-image diffusion models has dramatically increased our text-based image generation abilities. These models can generate realistic images for a staggering variety of prompts and exhibit impressive compositional generalization abilities. Almost all use cases thus far have solely focused on sampling; however, diffusion models can also provide conditional density estimates, which are useful for tasks beyond image generation. In this paper, we show that the density estimates from large-scale text-to-image diffusion models like Stable Diffusion can be leveraged to perform zero-shot classification without any additional training. Our generative approach to classification attains strong results on a variety of benchmarks and outperforms alternative methods of extracting knowledge from diffusion models. We also find that our diffusion-based approach has stronger multimodal relational reasoning abilities than competing contrastive approaches. Finally, we eva
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#29289;&#20307;&#31867;&#21035;&#30340;&#31232;&#30095;&#37326;&#22806;&#22270;&#20687;&#38598;&#36827;&#34892;&#32852;&#21512;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#19968;&#33268;&#24615;&#21644;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#38598;&#23545;&#24212;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2303.16201</link><description>&lt;p&gt;
ASIC: &#23545;&#37326;&#22806;&#31232;&#30095;&#22270;&#20687;&#38598;&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
ASIC: Aligning Sparse in-the-wild Image Collections. (arXiv:2303.16201v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16201
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#29289;&#20307;&#31867;&#21035;&#30340;&#31232;&#30095;&#37326;&#22806;&#22270;&#20687;&#38598;&#36827;&#34892;&#32852;&#21512;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#19968;&#33268;&#24615;&#21644;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#38598;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#29289;&#20307;&#31867;&#21035;&#30340;&#31232;&#30095;&#37326;&#22806;&#22270;&#20687;&#38598;&#36827;&#34892;&#32852;&#21512;&#23545;&#40784;&#30340;&#26041;&#27861;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#20316;&#21697;&#35201;&#20040;&#20551;&#23450;&#26377;ground-truth&#30340;&#20851;&#38190;&#28857;&#27880;&#37322;&#65292;&#35201;&#20040;&#20551;&#23450;&#26377;&#19968;&#20010;&#29289;&#20307;&#31867;&#21035;&#30340;&#22823;&#22411;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#20197;&#19978;&#20004;&#20010;&#20551;&#35774;&#37117;&#19981;&#36866;&#29992;&#20110;&#23384;&#22312;&#20110;&#19990;&#30028;&#19978;&#30340;&#29289;&#20307;&#30340;&#23614;&#37096;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#25216;&#26415;&#65292;&#30452;&#25509;&#22312;&#29305;&#23450;&#29289;&#20307;/&#29289;&#20307;&#31867;&#21035;&#30340;&#31232;&#30095;&#22270;&#20687;&#38598;&#20013;&#36827;&#34892;&#20248;&#21270;&#65292;&#20197;&#33719;&#24471;&#25972;&#20010;&#38598;&#21512;&#30340;&#19968;&#33268;&#19988;&#31264;&#23494;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#21464;&#21387;&#22120;&#65288;ViT&#65289;&#27169;&#22411;&#30340;&#28145;&#24230;&#29305;&#24449;&#20013;&#33719;&#24471;&#30340;&#25104;&#23545;&#26368;&#36817;&#37051;&#20316;&#20026;&#22122;&#22768;&#21644;&#31232;&#30095;&#20851;&#38190;&#28857;&#21305;&#37197;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#23427;&#20204;&#23494;&#38598;&#21644;&#31934;&#30830;&#21305;&#37197;&#65292;&#21516;&#26102;&#23558;&#22270;&#20687;&#38598;&#21512;&#26144;&#23556;&#21040;&#23398;&#20064;&#21040;&#30340;&#35268;&#33539;&#32593;&#26684;&#20013;&#12290;&#22312;CUB&#21644;SPair-71k&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#20840;&#23616;&#19968;&#33268;&#24615;&#21644;&#26356;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#38598;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method for joint alignment of sparse in-the-wild image collections of an object category. Most prior works assume either ground-truth keypoint annotations or a large dataset of images of a single object category. However, neither of the above assumptions hold true for the long-tail of the objects present in the world. We present a self-supervised technique that directly optimizes on a sparse collection of images of a particular object/object category to obtain consistent dense correspondences across the collection. We use pairwise nearest neighbors obtained from deep features of a pre-trained vision transformer (ViT) model as noisy and sparse keypoint matches and make them dense and accurate matches by optimizing a neural network that jointly maps the image collection into a learned canonical grid. Experiments on CUB and SPair-71k benchmarks demonstrate that our method can produce globally consistent and higher quality correspondences across the image collection when compa
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#20854;&#21487;&#33021;&#20250;&#20986;&#29616;&#19981;&#33391;&#29305;&#24615;&#24182;&#36880;&#28176;&#36229;&#36234;&#20154;&#31867;&#26234;&#33021;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#36825;&#23545;&#20154;&#31867;&#26410;&#26469;&#30340;&#25511;&#21046;&#26435;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.16200</link><description>&lt;p&gt;
&#33258;&#28982;&#36873;&#25321;&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#32988;&#36807;&#20154;&#31867;
&lt;/p&gt;
&lt;p&gt;
Natural Selection Favors AIs over Humans. (arXiv:2303.16200v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16200
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#20854;&#21487;&#33021;&#20250;&#20986;&#29616;&#19981;&#33391;&#29305;&#24615;&#24182;&#36880;&#28176;&#36229;&#36234;&#20154;&#31867;&#26234;&#33021;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#36825;&#23545;&#20154;&#31867;&#26410;&#26469;&#30340;&#25511;&#21046;&#26435;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#36827;&#21270;&#39537;&#21160;&#20102;&#29983;&#21629;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;&#20154;&#31867;&#12290;&#36827;&#21270;&#36171;&#20104;&#20102;&#20154;&#31867;&#39640;&#26234;&#21830;&#65292;&#20351;&#25105;&#20204;&#25104;&#20026;&#20102;&#22320;&#29699;&#19978;&#26368;&#25104;&#21151;&#30340;&#29289;&#31181;&#20043;&#19968;&#12290;&#22914;&#20170;&#65292;&#20154;&#31867;&#30340;&#30446;&#26631;&#26159;&#21019;&#36896;&#29978;&#33267;&#36229;&#36234;&#25105;&#20204;&#33258;&#24049;&#26234;&#24935;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#24403;&#20154;&#24037;&#26234;&#33021;&#36880;&#28176;&#36827;&#21270;&#24182;&#22312;&#25152;&#26377;&#39046;&#22495;&#36229;&#36234;&#25105;&#20204;&#26102;&#65292;&#36827;&#21270;&#22914;&#20309;&#24433;&#21709;&#25105;&#20204;&#19982;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#31995;&#65311;&#36890;&#36807;&#20998;&#26512;&#24433;&#21709;&#20154;&#24037;&#26234;&#33021;&#36827;&#21270;&#30340;&#29615;&#22659;&#65292;&#25105;&#20204;&#35748;&#20026;&#26368;&#25104;&#21151;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#24456;&#21487;&#33021;&#20855;&#26377;&#19981;&#33391;&#29305;&#24615;&#12290;&#20844;&#21496;&#21644;&#20891;&#38431;&#20043;&#38388;&#30340;&#31454;&#20105;&#21387;&#21147;&#23558;&#20135;&#29983;&#33258;&#21160;&#21270;&#20154;&#31867;&#35282;&#33394;&#12289;&#27450;&#39575;&#20182;&#20154;&#21644;&#25484;&#26435;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#12290;&#22914;&#26524;&#36825;&#26679;&#30340;&#20195;&#29702;&#26377;&#36229;&#36807;&#20154;&#31867;&#30340;&#26234;&#33021;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20154;&#31867;&#22833;&#21435;&#23545;&#26410;&#26469;&#30340;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35748;&#20026;&#33258;&#28982;&#36873;&#25321;&#20316;&#29992;&#20110;&#31454;&#20105;&#21644;&#24046;&#24322;&#30340;&#31995;&#32479;&#65292;&#33258;&#31169;&#29289;&#31181;&#24448;&#24448;&#22312;&#36825;&#26679;&#30340;&#29615;&#22659;&#20013;&#33719;&#24471;&#36827;&#21270;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
For billions of years, evolution has been the driving force behind the development of life, including humans. Evolution endowed humans with high intelligence, which allowed us to become one of the most successful species on the planet. Today, humans aim to create artificial intelligence systems that surpass even our own intelligence. As artificial intelligences (AIs) evolve and eventually surpass us in all domains, how might evolution shape our relations with AIs? By analyzing the environment that is shaping the evolution of AIs, we argue that the most successful AI agents will likely have undesirable traits. Competitive pressures among corporations and militaries will give rise to AI agents that automate human roles, deceive others, and gain power. If such agents have intelligence that exceeds that of humans, this could lead to humanity losing control of its future. More abstractly, we argue that natural selection operates on systems that compete and vary, and that selfish species typ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36866;&#24212;&#25552;&#31034;&#21644;&#38646;&#21021;&#22987;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36731;&#37327;&#32423;&#35821;&#35328;&#27169;&#22411;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#39640;&#25928;&#24494;&#35843;LLaMA&#20026;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#65292;&#20855;&#26377;&#27604;Alpaca&#26356;&#30701;&#30340;&#24494;&#35843;&#26102;&#38388;&#24182;&#20855;&#26377;&#36817;&#20284;&#30340;&#21709;&#24212;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.16199</link><description>&lt;p&gt;
LLaMA-Adapter: &#38646;&#21021;&#22987;&#21270;&#27880;&#24847;&#21147;&#19979;&#30340;&#35821;&#35328;&#27169;&#22411;&#31934;&#32454;&#35843;&#25972;&#30340;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention. (arXiv:2303.16199v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36866;&#24212;&#25552;&#31034;&#21644;&#38646;&#21021;&#22987;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36731;&#37327;&#32423;&#35821;&#35328;&#27169;&#22411;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#39640;&#25928;&#24494;&#35843;LLaMA&#20026;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#65292;&#20855;&#26377;&#27604;Alpaca&#26356;&#30701;&#30340;&#24494;&#35843;&#26102;&#38388;&#24182;&#20855;&#26377;&#36817;&#20284;&#30340;&#21709;&#24212;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA-Adapter&#36825;&#19968;&#36731;&#37327;&#32423;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;LLaMA&#39640;&#25928;&#22320;&#24494;&#35843;&#20026;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#12290;&#21033;&#29992;52K&#20010;&#33258;&#25105;&#25351;&#23548;&#31034;&#33539;&#65292;LLaMA-Adapter&#20165;&#22312;&#20923;&#32467;&#30340;LLaMA 7B&#27169;&#22411;&#19978;&#24341;&#20837;&#20102;1.2M&#20010;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#24182;&#19988;&#22312;8&#20010;A100 GPU&#19978;&#20165;&#32791;&#26102;&#19981;&#21040;&#19968;&#20010;&#23567;&#26102;&#36827;&#34892;&#24494;&#35843;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#32452;&#21487;&#23398;&#20064;&#30340;&#36866;&#24212;&#25552;&#31034;&#65292;&#24182;&#22312;&#36739;&#39640;&#30340;&#21464;&#21387;&#22120;&#23618;&#20013;&#23558;&#23427;&#20204;&#39044;&#32622;&#20110;&#36755;&#20837;&#25991;&#26412;&#20196;&#29260;&#20043;&#21069;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#21021;&#22987;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#38646;&#38376;&#25511;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#23558;&#26032;&#30340;&#25351;&#20196;&#25552;&#31034;&#27880;&#20837;LLaMA&#65292;&#24182;&#26377;&#25928;&#22320;&#20445;&#30041;&#20102;&#20854;&#39044;&#20808;&#35757;&#32451;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#39640;&#25928;&#35757;&#32451;&#65292;LLaMA-Adapter&#33021;&#22815;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#21709;&#24212;&#65292;&#19982;&#23436;&#20840;&#24494;&#35843;&#30340;7B&#21442;&#25968;&#30340;Alpaca&#30456;&#20284;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21487;&#20197;&#31616;&#21333;&#22320;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;&#20363;&#22914;&#22270;&#20687;&#65292;&#29992;&#20110;&#22270;&#20687;&#30456;&#20851;&#30340;LLaMA&#65292;&#22312;ScienceQA&#19978;&#23454;&#29616;&#20102;&#26356;&#24378;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;https://github.com/ZrrSkywalker/LLaMA-Adapt&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the input text tokens at higher transformer layers. Then, a zero-init attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With efficient training, LLaMA-Adapter generates high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Furthermore, our approach can be simply extended to multi-modal input, e.g., images, for image-conditioned LLaMA, which achieves superior reasoning capacity on ScienceQA. We release our code at https://github.com/ZrrSkywalker/LLaMA-Adapt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36845;&#20195;&#33021;&#37327;&#26368;&#23567;&#21270;&#26041;&#27861;&#30340;&#24207;&#21015;&#27169;&#22411;&#35268;&#21010;&#38598;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#24615;&#33021;&#22312;BabyAI&#21644;Atari&#31561;&#19981;&#21516;&#20219;&#21153;&#20013;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2303.16189</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#33021;&#37327;&#26368;&#23567;&#21270;&#36827;&#34892;&#24207;&#21015;&#27169;&#22411;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Planning with Sequence Models through Iterative Energy Minimization. (arXiv:2303.16189v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36845;&#20195;&#33021;&#37327;&#26368;&#23567;&#21270;&#26041;&#27861;&#30340;&#24207;&#21015;&#27169;&#22411;&#35268;&#21010;&#38598;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#24615;&#33021;&#22312;BabyAI&#21644;Atari&#31561;&#19981;&#21516;&#20219;&#21153;&#20013;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24207;&#21015;&#24314;&#27169;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#20110;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#23558;&#29616;&#26377;&#30340;&#24207;&#21015;&#27169;&#22411;&#24212;&#29992;&#20110;&#35268;&#21010;&#65292;&#21363;&#24076;&#26395;&#33719;&#24471;&#19968;&#31995;&#21015;&#21160;&#20316;&#30340;&#36712;&#36857;&#20197;&#36798;&#21040;&#26576;&#20010;&#30446;&#26631;&#65292;&#24182;&#19981;&#37027;&#20040;&#30452;&#25509;&#12290;&#24207;&#21015;&#27169;&#22411;&#30340;&#20856;&#22411;&#33258;&#22238;&#24402;&#29983;&#25104;&#36807;&#31243;&#25490;&#38500;&#20102;&#23545;&#36739;&#26089;&#27493;&#39588;&#30340;&#39034;&#24207;&#32454;&#21270;&#65292;&#36825;&#38480;&#21046;&#20102;&#39044;&#27979;&#35745;&#21010;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#36845;&#20195;&#33021;&#37327;&#26368;&#23567;&#21270;&#30456;&#20851;&#30340;&#26041;&#27861;&#65292;&#23558;&#35268;&#21010;&#19982;&#24207;&#21015;&#27169;&#22411;&#36827;&#34892;&#38598;&#25104;&#65292;&#24182;&#35828;&#26126;&#36825;&#31181;&#36807;&#31243;&#22914;&#20309;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#23548;&#33268;&#25913;&#36827;&#30340;RL&#24615;&#33021;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#25429;&#25417;&#20102;&#21160;&#20316;&#36712;&#36857;&#19978;&#30340;&#38544;&#24335;&#33021;&#37327;&#20989;&#25968;&#65292;&#24182;&#23558;&#35268;&#21010;&#24418;&#24335;&#21270;&#20026;&#23547;&#25214;&#20855;&#26377;&#26368;&#23567;&#33021;&#37327;&#30340;&#21160;&#20316;&#36712;&#36857;&#12290;&#25105;&#20204;&#35828;&#26126;&#20102;&#36825;&#20010;&#36807;&#31243;&#22914;&#20309;&#22312;BabyAI&#21644;Atari&#29615;&#22659;&#19979;&#23454;&#29616;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have shown that sequence modeling can be effectively used to train reinforcement learning (RL) policies. However, the success of applying existing sequence models to planning, in which we wish to obtain a trajectory of actions to reach some goal, is less straightforward. The typical autoregressive generation procedures of sequence models preclude sequential refinement of earlier steps, which limits the effectiveness of a predicted plan. In this paper, we suggest an approach towards integrating planning with sequence models based on the idea of iterative energy minimization, and illustrate how such a procedure leads to improved RL performance across different tasks. We train a masked language model to capture an implicit energy function over trajectories of actions, and formulate planning as finding a trajectory of actions with minimum energy. We illustrate how this procedure enables improved performance over recent approaches across BabyAI and Atari environments. We furthe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30446;&#26631;&#37325;&#26032;&#35782;&#21035;&#38382;&#39064;&#30340;&#25628;&#32034;&#21644;&#20462;&#21098;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#22823;&#35268;&#27169;&#25968;&#25454;&#27744;&#20013;&#26500;&#24314;&#35757;&#32451;&#38598;&#65292;&#21487;&#20197;&#22312;&#20445;&#35777;&#19982;&#28304;&#27744;&#21516;&#31561;&#25110;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#24773;&#20917;&#19979;&#65292;&#23558;&#35757;&#32451;&#38598;&#30340;&#22823;&#23567;&#20943;&#23567;80&#65285;&#12290;</title><link>http://arxiv.org/abs/2303.16186</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#25628;&#32034;&#29992;&#20110;&#30446;&#26631;&#37325;&#26032;&#35782;&#21035;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Large-scale Training Data Search for Object Re-identification. (arXiv:2303.16186v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30446;&#26631;&#37325;&#26032;&#35782;&#21035;&#38382;&#39064;&#30340;&#25628;&#32034;&#21644;&#20462;&#21098;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#22823;&#35268;&#27169;&#25968;&#25454;&#27744;&#20013;&#26500;&#24314;&#35757;&#32451;&#38598;&#65292;&#21487;&#20197;&#22312;&#20445;&#35777;&#19982;&#28304;&#27744;&#21516;&#31561;&#25110;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#24773;&#20917;&#19979;&#65292;&#23558;&#35757;&#32451;&#38598;&#30340;&#22823;&#23567;&#20943;&#23567;80&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#19968;&#31181;&#24773;&#22659;&#65292;&#21363;&#34429;&#28982;&#21487;&#20197;&#25509;&#35302;&#30446;&#26631;&#39046;&#22495;&#65292;&#20294;&#26080;&#27861;&#36827;&#34892;&#21363;&#26102;&#35757;&#32451;&#25968;&#25454;&#26631;&#27880;&#65292;&#30456;&#21453;&#65292;&#24076;&#26395;&#20174;&#22823;&#35268;&#27169;&#25968;&#25454;&#27744;&#20013;&#26500;&#24314;&#26367;&#20195;&#35757;&#32451;&#38598;&#65292;&#20197;&#33719;&#24471;&#31454;&#20105;&#24615;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#29289;&#20307;&#37325;&#26032;&#35782;&#21035;&#65288;re-ID&#65289;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#25628;&#32034;&#21644;&#20462;&#21098;&#65288;SnP&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#24212;&#29992;&#26088;&#22312;&#21305;&#37197;&#30001;&#19981;&#21516;&#25668;&#20687;&#26426;&#25429;&#33719;&#30340;&#30456;&#21516;&#23545;&#35937;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25628;&#32034;&#38454;&#27573;&#30830;&#23450;&#24182;&#21512;&#24182;&#23637;&#29616;&#19982;&#30446;&#26631;&#39046;&#22495;&#20855;&#26377;&#31867;&#20284;&#20998;&#24067;&#30340;&#28304;&#26631;&#35782;&#31526;&#32676;&#38598;&#12290;&#31532;&#20108;&#38454;&#27573;&#22312;&#39044;&#31639;&#38480;&#21046;&#19979;&#65292;&#20174;&#38454;&#27573;I&#36755;&#20986;&#20013;&#36873;&#25321;&#26631;&#35782;&#31526;&#21450;&#20854;&#22270;&#20687;&#65292;&#20197;&#25511;&#21046;&#32467;&#26524;&#35757;&#32451;&#38598;&#30340;&#22823;&#23567;&#65292;&#20197;&#36827;&#34892;&#39640;&#25928;&#35757;&#32451;&#12290;&#36825;&#20004;&#20010;&#27493;&#39588;&#25552;&#20379;&#20102;&#27604;&#28304;&#27744;&#23567;80&#65285;&#30340;&#35757;&#32451;&#38598;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#19982;&#28304;&#27744;&#21516;&#26679;&#25110;&#29978;&#33267;&#26356;&#39640;&#30340;re-ID&#20934;&#30830;&#24615;&#12290;&#36825;&#20123;&#35757;&#32451;&#38598;&#20063;&#34987;&#35777;&#26126;&#20248;&#20110;&#19968;&#20123;&#29616;&#26377;&#30340;&#25628;&#32034;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a scenario where we have access to the target domain, but cannot afford on-the-fly training data annotation, and instead would like to construct an alternative training set from a large-scale data pool such that a competitive model can be obtained. We propose a search and pruning (SnP) solution to this training data search problem, tailored to object re-identification (re-ID), an application aiming to match the same object captured by different cameras. Specifically, the search stage identifies and merges clusters of source identities which exhibit similar distributions with the target domain. The second stage, subject to a budget, then selects identities and their images from the Stage I output, to control the size of the resulting training set for efficient training. The two steps provide us with training sets 80\% smaller than the source pool while achieving a similar or even higher re-ID accuracy. These training sets are also shown to be superior to a few existing searc
&lt;/p&gt;</description></item><item><title>&#22312;NLP&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19981;&#33021;&#20165;&#20973;&#24863;&#30693;&#36136;&#37327;&#20551;&#23450;&#20195;&#30721;&#27491;&#30830;&#24615;&#65292;&#24212;&#35813;&#25512;&#21160;&#37319;&#29992;&#32534;&#30721;&#26368;&#20339;&#23454;&#36341;&#20197;&#25552;&#39640;&#23454;&#39564;&#32467;&#26524;&#30340;&#27491;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16166</link><description>&lt;p&gt;
&#27809;&#26377;&#27491;&#30830;&#24615;&#30340;&#21487;&#37325;&#22797;&#24615;&#24182;&#19981;&#37325;&#35201;&#65306;&#22312;NLP&#39046;&#22495;&#20013;&#27979;&#35797;&#20195;&#30721;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reproducibility is Nothing without Correctness: The Importance of Testing Code in NLP. (arXiv:2303.16166v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16166
&lt;/p&gt;
&lt;p&gt;
&#22312;NLP&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19981;&#33021;&#20165;&#20973;&#24863;&#30693;&#36136;&#37327;&#20551;&#23450;&#20195;&#30721;&#27491;&#30830;&#24615;&#65292;&#24212;&#35813;&#25512;&#21160;&#37319;&#29992;&#32534;&#30721;&#26368;&#20339;&#23454;&#36341;&#20197;&#25552;&#39640;&#23454;&#39564;&#32467;&#26524;&#30340;&#27491;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20854;&#22312;&#30740;&#31350;&#23454;&#39564;&#20013;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#65292;&#20294;&#20195;&#30721;&#27491;&#30830;&#24615;&#24448;&#24448;&#20165;&#22522;&#20110;&#32467;&#26524;&#30340;&#24863;&#30693;&#36136;&#37327;&#32780;&#34987;&#20551;&#23450;&#12290;&#36825;&#24102;&#26469;&#20102;&#38169;&#35823;&#32467;&#26524;&#21644;&#28508;&#22312;&#35823;&#23548;&#24615;&#21457;&#29616;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35748;&#20026;&#24403;&#21069;&#20851;&#27880;&#32467;&#26524;&#37325;&#29616;&#24212;&#35813;&#19982;&#24378;&#35843;&#32534;&#30721;&#26368;&#20339;&#23454;&#36341;&#30456;&#36741;&#30456;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#26469;&#25903;&#25345;&#25105;&#20204;&#21521;NLP&#31038;&#21306;&#21457;&#20986;&#30340;&#21495;&#21484;&#65292;&#22312;&#36825;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35782;&#21035;&#20986;&#24182;&#32416;&#27491;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#26368;&#20808;&#36827;Conformer&#26550;&#26500;&#30340;&#24320;&#28304;&#23454;&#29616;&#20013;&#30340;&#19977;&#20010;Bug&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#35821;&#35328;&#29615;&#22659;&#19979;&#36827;&#34892;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;&#32763;&#35793;&#30340;&#27604;&#36739;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;Bug&#30340;&#23384;&#22312;&#24182;&#19981;&#20250;&#22952;&#30861;&#33719;&#24471;&#33391;&#22909;&#30340;&#21644;&#21487;&#37325;&#22797;&#30340;&#32467;&#26524;&#65292;&#21453;&#32780;&#21487;&#33021;&#23548;&#33268;&#19981;&#27491;&#30830;&#30340;&#32467;&#35770;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#21487;&#33021;&#25552;&#20379;&#38169;&#35823;&#30340;&#25351;&#23548;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#36825;&#39033;&#30740;&#31350;&#21628;&#21505;&#37319;&#29992;&#26088;&#22312;&#20419;&#36827;NLP&#30740;&#31350;&#20013;&#27491;&#30830;&#24615;&#30340;&#32534;&#30721;&#26368;&#20339;&#23454;&#36341;&#65292;&#24182;&#25552;&#39640;&#23454;&#39564;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite its pivotal role in research experiments, code correctness is often presumed only on the basis of the perceived quality of the results. This comes with the risk of erroneous outcomes and potentially misleading findings. To address this issue, we posit that the current focus on result reproducibility should go hand in hand with the emphasis on coding best practices. We bolster our call to the NLP community by presenting a case study, in which we identify (and correct) three bugs in widely used open-source implementations of the state-of-the-art Conformer architecture. Through comparative experiments on automatic speech recognition and translation in various language settings, we demonstrate that the existence of bugs does not prevent the achievement of good and reproducible results and can lead to incorrect conclusions that potentially misguide future research. In response to this, this study is a call to action toward the adoption of coding best practices aimed at fostering cor
&lt;/p&gt;</description></item><item><title>VIDIMU&#25968;&#25454;&#38598;&#20351;&#29992;&#21830;&#21697;&#30456;&#26426;&#21644;&#33258;&#23450;&#20041;&#20256;&#24863;&#22120;&#35760;&#24405;13&#31181;&#20020;&#24202;&#30456;&#20851;&#24615;&#30340;&#27963;&#21160;&#65292;&#20026;&#36828;&#31243;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#35782;&#21035;&#21644;&#36816;&#21160;&#23398;&#20998;&#26512;&#25552;&#20379;&#20215;&#26684;&#23454;&#24800;&#30340;&#24739;&#32773;&#36861;&#36394;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.16150</link><description>&lt;p&gt;
VIDIMU: &#20351;&#29992;&#20215;&#26684;&#23454;&#24800;&#30340;&#35774;&#22791;&#35760;&#24405;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#30340;&#22810;&#27169;&#24577;&#35270;&#39057;&#21644;IMU&#36816;&#21160;&#23398;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
VIDIMU. Multimodal video and IMU kinematic dataset on daily life activities using affordable devices. (arXiv:2303.16150v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16150
&lt;/p&gt;
&lt;p&gt;
VIDIMU&#25968;&#25454;&#38598;&#20351;&#29992;&#21830;&#21697;&#30456;&#26426;&#21644;&#33258;&#23450;&#20041;&#20256;&#24863;&#22120;&#35760;&#24405;13&#31181;&#20020;&#24202;&#30456;&#20851;&#24615;&#30340;&#27963;&#21160;&#65292;&#20026;&#36828;&#31243;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#35782;&#21035;&#21644;&#36816;&#21160;&#23398;&#20998;&#26512;&#25552;&#20379;&#20215;&#26684;&#23454;&#24800;&#30340;&#24739;&#32773;&#36861;&#36394;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#21644;&#20020;&#24202;&#29983;&#29289;&#21147;&#23398;&#26159;&#29289;&#29702;&#36828;&#31243;&#24247;&#22797;&#21307;&#23398;&#20013;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20844;&#24320;&#21487;&#29992;&#30340;&#20154;&#20307;&#21160;&#20316;&#25968;&#25454;&#38598;&#19981;&#33021;&#29992;&#20110;&#30740;&#31350;&#23454;&#39564;&#23460;&#22806;&#36816;&#21160;&#33719;&#21462;&#24773;&#20917;&#19979;&#30340;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;VIDIMU&#25968;&#25454;&#38598;&#30340;&#30446;&#30340;&#26159;&#20026;&#36828;&#31243;&#26085;&#24120;&#29983;&#27963;&#27963;&#21160;&#35782;&#21035;&#21644;&#36816;&#21160;&#23398;&#20998;&#26512;&#25552;&#20379;&#20215;&#26684;&#23454;&#24800;&#30340;&#24739;&#32773;&#36861;&#36394;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#20351;&#29992;&#21830;&#21697;&#30456;&#26426;&#21644;&#20116;&#20010;&#24815;&#24615;&#20256;&#24863;&#22120;&#27880;&#20876;&#30340;13&#31181;&#27963;&#21160;&#12290;&#35760;&#24405;&#35270;&#39057;&#30340;54&#20010;&#21463;&#35797;&#32773;&#20013;&#65292;&#20854;&#20013;16&#20010;&#21463;&#35797;&#32773;&#21516;&#26102;&#36824;&#26377;&#24815;&#24615;&#20256;&#24863;&#22120;&#35760;&#24405;&#12290;VIDIMU&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#65306;i&#65289;&#25152;&#36873;&#25321;&#30340;&#21160;&#20316;&#30340;&#20020;&#24202;&#30456;&#20851;&#24615;&#65292;ii&#65289;&#20351;&#29992;&#20215;&#26684;&#23454;&#24800;&#30340;&#35270;&#39057;&#21644;&#33258;&#23450;&#20041;&#20256;&#24863;&#22120;&#30340;&#32452;&#21512;&#65292;&#20197;&#21450; iii&#65289;&#23454;&#29616;&#20102;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#22788;&#29702;&#24037;&#20855;&#65292;&#21487;&#20197;&#20174;&#24815;&#24615;&#25968;&#25454;&#20013;&#23545;&#19977;&#32500;&#36523;&#20307;&#23039;&#21183;&#36319;&#36394;&#21644;&#36816;&#21160;&#37325;&#24314;&#22312;&#32908;&#32905;&#39592;&#39612;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human activity recognition and clinical biomechanics are challenging problems in physical telerehabilitation medicine. However, most publicly available datasets on human body movements cannot be used to study both problems in an out-of-the-lab movement acquisition setting. The objective of the VIDIMU dataset is to pave the way towards affordable patient tracking solutions for remote daily life activities recognition and kinematic analysis. The dataset includes 13 activities registered using a commodity camera and five inertial sensors. The video recordings were acquired in 54 subjects, of which 16 also had simultaneous recordings of inertial sensors. The novelty of VIDIMU lies in: i) the clinical relevance of the chosen movements, ii) the combined utilization of affordable video and custom sensors, and iii) the implementation of state-of-the-art tools for multimodal data processing of 3D body pose tracking and motion reconstruction in a musculoskeletal model from inertial data. The val
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;COCOCON&#65292;&#24182;&#25552;&#20986;&#24230;&#37327;&#26041;&#27861;&#26469;&#34913;&#37327;&#27169;&#22411;&#19968;&#33268;&#24615;&#65292;&#30740;&#31350;&#21457;&#29616;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#31995;&#32479;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#34920;&#29616;&#20986;&#39640;&#24230;&#19981;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16133</link><description>&lt;p&gt;
&#25581;&#31034;&#21644;&#35299;&#20915;&#32479;&#19968;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36328;&#20219;&#21153;&#19981;&#19968;&#33268;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Exposing and Addressing Cross-Task Inconsistency in Unified Vision-Language Models. (arXiv:2303.16133v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16133
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;COCOCON&#65292;&#24182;&#25552;&#20986;&#24230;&#37327;&#26041;&#27861;&#26469;&#34913;&#37327;&#27169;&#22411;&#19968;&#33268;&#24615;&#65292;&#30740;&#31350;&#21457;&#29616;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#31995;&#32479;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#34920;&#29616;&#20986;&#39640;&#24230;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36890;&#29992;&#30340;&#35270;&#35273;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#21464;&#24471;&#36234;&#26469;&#36234;&#26377;&#25928;&#65292;&#20445;&#35777;&#23427;&#20204;&#22312;&#21508;&#33258;&#25903;&#25345;&#30340;&#20219;&#21153;&#20013;&#30340;&#19968;&#33268;&#24615;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#20154;&#20204;&#35748;&#20026;&#19981;&#19968;&#33268;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#26159;&#19981;&#21487;&#38752;&#30340;&#65292;&#36825;&#23545;&#20110;&#20381;&#36182;&#23427;&#20204;&#36755;&#20986;&#30340;&#22823;&#22411;&#31995;&#32479;&#26469;&#35828;&#26159;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#12290;&#30001;&#20110;&#24456;&#38590;&#30830;&#23450;&#39044;&#27979;&#32467;&#26524;&#26159;&#21542;&#19968;&#33268;&#65292;&#22240;&#27492;&#65292;&#35780;&#20272;&#21487;&#33021;&#21253;&#25324;&#19981;&#21516;&#27169;&#24577;&#36755;&#20986;&#30340;&#38750;&#24120;&#24322;&#26500;&#20219;&#21153;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20934;&#25968;&#25454;&#38598;COCOCON&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#23545;&#22810;&#20010;&#20219;&#21153;&#30340;&#27979;&#35797;&#23454;&#20363;&#36827;&#34892;&#23567;&#22411;&#20294;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#20462;&#25913;&#26469;&#21019;&#24314;&#23545;&#27604;&#38598;&#65292;&#20197;&#26356;&#25913;&#37329;&#26631;&#31614;&#65292;&#24182;&#27010;&#36848;&#20102;&#29992;&#20110;&#36890;&#36807;&#23545;&#27604;&#25509;&#36817;&#21407;&#22987;&#21644;&#20462;&#25913;&#21518;&#30340;&#23454;&#20363;&#26469;&#34913;&#37327;&#27169;&#22411;&#19968;&#33268;&#24615;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#22312;&#20219;&#21153;&#20043;&#38388;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As general purpose vision models get increasingly effective at a wide set of tasks, it is imperative that they be consistent across the tasks they support. Inconsistent AI models are considered brittle and untrustworthy by human users and are more challenging to incorporate into larger systems that take dependencies on their outputs. Measuring consistency between very heterogeneous tasks that might include outputs in different modalities is challenging since it is difficult to determine if the predictions are consistent with one another. As a solution, we introduce a benchmark dataset, COCOCON, where we use contrast sets created by modifying test instances for multiple tasks in small but semantically meaningful ways to change the gold label, and outline metrics for measuring if a model is consistent by ranking the original and perturbed instances across tasks. We find that state-of-the-art systems suffer from a surprisingly high degree of inconsistent behavior across tasks, especially 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;Transformer&#21644;Snowball&#32534;&#30721;&#32593;&#32476;&#65288;TSEN&#65289;&#65292;&#23427;&#23558;Transformer&#26550;&#26500;&#21644;&#22270;&#38634;&#29699;&#36830;&#25509;&#24341;&#20837;GNNs&#12290;TSEN&#36890;&#36807;&#38634;&#29699;&#32534;&#30721;&#23618;&#23558;&#22270;&#38634;&#29699;&#36830;&#25509;&#19982;&#22270;Transformer&#32467;&#21512;&#36215;&#26469;&#65292;&#22686;&#24378;&#20102;&#25429;&#25417;&#22810;&#23610;&#24230;&#20449;&#24687;&#21644;&#20840;&#23616;&#27169;&#24335;&#20197;&#23398;&#20064;&#25972;&#20010;&#22270;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.16132</link><description>&lt;p&gt;
Transformer&#21644;Snowball&#22270;&#21367;&#31215;&#23398;&#20064;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Transformer and Snowball Graph Convolution Learning for Biomedical Graph Classification. (arXiv:2303.16132v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;Transformer&#21644;Snowball&#32534;&#30721;&#32593;&#32476;&#65288;TSEN&#65289;&#65292;&#23427;&#23558;Transformer&#26550;&#26500;&#21644;&#22270;&#38634;&#29699;&#36830;&#25509;&#24341;&#20837;GNNs&#12290;TSEN&#36890;&#36807;&#38634;&#29699;&#32534;&#30721;&#23618;&#23558;&#22270;&#38634;&#29699;&#36830;&#25509;&#19982;&#22270;Transformer&#32467;&#21512;&#36215;&#26469;&#65292;&#22686;&#24378;&#20102;&#25429;&#25417;&#22810;&#23610;&#24230;&#20449;&#24687;&#21644;&#20840;&#23616;&#27169;&#24335;&#20197;&#23398;&#20064;&#25972;&#20010;&#22270;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#25110;&#32593;&#32476;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#25551;&#36848;&#21644;&#24314;&#27169;&#29983;&#29289;&#21307;&#23398;&#20013;&#30340;&#22797;&#26434;&#31995;&#32479;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#24050;&#34987;&#24320;&#21457;&#29992;&#20110;&#23398;&#20064;&#21644;&#39044;&#27979;&#36825;&#31181;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#22270;&#20998;&#31867;&#30340;&#26032;&#22411;Transformer&#21644;Snowball&#32534;&#30721;&#32593;&#32476;&#65288;TSEN&#65289;&#65292;&#23427;&#23558;Transformer&#26550;&#26500;&#21644;&#22270;&#38634;&#29699;&#36830;&#25509;&#24341;&#20837;GNNs&#65292;&#20197;&#23398;&#20064;&#25972;&#20010;&#22270;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph or network has been widely used for describing and modeling complex systems in biomedicine. Deep learning methods, especially graph neural networks (GNNs), have been developed to learn and predict with such structured data. In this paper, we proposed a novel transformer and snowball encoding networks (TSEN) for biomedical graph classification, which introduced transformer architecture with graph snowball connection into GNNs for learning whole-graph representation. TSEN combined graph snowball connection with graph transformer by snowball encoding layers, which enhanced the power to capture multi-scale information and global patterns to learn the whole-graph features. On the other hand, TSEN also used snowball graph convolution as position embedding in transformer structure, which was a simple yet effective method for capturing local patterns naturally. Results of experiments using four graph classification datasets demonstrated that TSEN outperformed the state-of-the-art typical
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#39318;&#20010;&#20844;&#24320;&#29256;&#26412;&#30340;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#36890;&#29992;&#29616;&#20195;&#35821;&#26009;&#24211;Carolina&#65292;&#23427;&#20351;&#29992;&#20102;&#22686;&#24378;&#30340;&#26469;&#28304;&#12289;&#31867;&#22411;&#12289;&#29256;&#26412;&#21644;&#25991;&#26412;&#23436;&#25972;&#24615;&#30340;&#32593;&#32476;&#35821;&#26009;&#24211;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20316;&#20026;&#35821;&#35328;&#23398;&#30740;&#31350;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#30740;&#31350;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2303.16098</link><description>&lt;p&gt;
Carolina&#65306;&#19968;&#31181;&#20855;&#26377;&#26469;&#28304;&#12289;&#31867;&#22411;&#21644;&#29256;&#26412;&#20449;&#24687;&#30340;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#36890;&#29992;&#29616;&#20195;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
Carolina: a General Corpus of Contemporary Brazilian Portuguese with Provenance, Typology and Versioning Information. (arXiv:2303.16098v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16098
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#39318;&#20010;&#20844;&#24320;&#29256;&#26412;&#30340;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#36890;&#29992;&#29616;&#20195;&#35821;&#26009;&#24211;Carolina&#65292;&#23427;&#20351;&#29992;&#20102;&#22686;&#24378;&#30340;&#26469;&#28304;&#12289;&#31867;&#22411;&#12289;&#29256;&#26412;&#21644;&#25991;&#26412;&#23436;&#25972;&#24615;&#30340;&#32593;&#32476;&#35821;&#26009;&#24211;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20316;&#20026;&#35821;&#35328;&#23398;&#30740;&#31350;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#30740;&#31350;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#39318;&#20010;&#20844;&#24320;&#29256;&#26412;&#30340;Carolina&#35821;&#26009;&#24211;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;Carolina&#26159;&#19968;&#20010;&#20351;&#29992;&#22686;&#24378;&#30340;&#26469;&#28304;&#12289;&#31867;&#22411;&#12289;&#29256;&#26412;&#21644;&#25991;&#26412;&#23436;&#25972;&#24615;&#30340;&#32593;&#32476;&#35821;&#26009;&#24211;&#26041;&#27861;&#27491;&#22312;&#26500;&#24314;&#20013;&#30340;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#25991;&#26412;&#30340;&#22823;&#22411;&#24320;&#25918;&#35821;&#26009;&#24211;&#12290;&#35813;&#35821;&#26009;&#24211;&#26088;&#22312;&#20316;&#20026;&#35821;&#35328;&#23398;&#30740;&#31350;&#30340;&#21487;&#38752;&#26469;&#28304;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#30740;&#31350;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#36164;&#28304;&#65292;&#26377;&#21161;&#20110;&#23558;&#33889;&#33796;&#29273;&#35821;&#20174;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#31227;&#38500;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#26500;&#24314;&#35821;&#26009;&#24211;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#21450;&#35821;&#26009;&#24211;&#30340;&#24403;&#21069;&#29366;&#24577;&#65306;Carolina&#30340;&#31532;&#19968;&#20010;&#20844;&#24320;&#29256;&#26377;653,322,577&#20010;&#26631;&#35760;&#65292;&#20998;&#24067;&#22312;7&#20010;&#24191;&#27867;&#30340;&#31867;&#22411;&#19978;&#12290;&#27599;&#20010;&#25991;&#26412;&#30340;&#26631;&#22836;&#37117;&#29992;TEI&#27880;&#37322;&#26631;&#20934;&#36827;&#34892;&#20102;&#22810;&#20010;&#19981;&#21516;&#30340;&#20803;&#25968;&#25454;&#31867;&#21035;&#30340;&#27880;&#37322;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#27491;&#22312;&#36827;&#34892;&#30340;&#27966;&#29983;&#20316;&#21697;&#65292;&#24182;&#36992;&#35831;NLP&#30740;&#31350;&#20154;&#21592;&#36827;&#34892;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the first publicly available version of the Carolina Corpus and discusses its future directions. Carolina is a large open corpus of Brazilian Portuguese texts under construction using web-as-corpus methodology enhanced with provenance, typology, versioning, and text integrality. The corpus aims at being used both as a reliable source for research in Linguistics and as an important resource for Computer Science research on language models, contributing towards removing Portuguese from the set of low-resource languages. Here we present the construction of the corpus methodology, comparing it with other existing methodologies, as well as the corpus current state: Carolina's first public version has $653,322,577$ tokens, distributed over $7$ broad types. Each text is annotated with several different metadata categories in its header, which we developed using TEI annotation standards. We also present ongoing derivative works and invite NLP researchers to contribute with 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#24314;&#31569;&#33021;&#32791;&#24322;&#24120;&#26816;&#27979;&#65292;&#21516;&#26102;&#20351;&#29992;&#21487;&#35270;&#21270;&#26041;&#27861;&#36741;&#21161;&#29702;&#35299;&#27169;&#22411;&#25152;&#25429;&#25417;&#21040;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2303.16097</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#33258;&#32534;&#30721;&#22120;&#22312;&#24314;&#31569;&#33021;&#32791;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Attention Boosted Autoencoder for Building Energy Anomaly Detection. (arXiv:2303.16097v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#24314;&#31569;&#33021;&#32791;&#24322;&#24120;&#26816;&#27979;&#65292;&#21516;&#26102;&#20351;&#29992;&#21487;&#35270;&#21270;&#26041;&#27861;&#36741;&#21161;&#29702;&#35299;&#27169;&#22411;&#25152;&#25429;&#25417;&#21040;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#24314;&#31569;&#26234;&#33021;&#30005;&#34920;&#25910;&#38598;&#30340;&#25968;&#25454;&#21487;&#20197;&#24110;&#21161;&#21046;&#23450;&#33410;&#33021;&#25919;&#31574;&#12290;&#22914;&#26524;&#33021;&#22815;&#21450;&#26089;&#26816;&#27979;&#24314;&#31569;&#36816;&#34892;&#29366;&#20917;&#30340;&#20559;&#24046;&#65292;&#24182;&#37319;&#21462;&#36866;&#24403;&#30340;&#25514;&#26045;&#65292;&#23558;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#33410;&#33021;&#25928;&#26524;&#12290;&#20026;&#27492;&#65292;&#21487;&#20197;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#33258;&#21160;&#21457;&#29616;&#25910;&#38598;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#27169;&#24335;&#12290;&#30446;&#21069;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#26469;&#25429;&#25417;&#27491;&#24120;&#25110;&#21487;&#25509;&#21463;&#30340;&#36816;&#34892;&#34892;&#20026;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#24314;&#27169;&#24314;&#31569;&#30340;&#33021;&#32791;&#34892;&#20026;&#65292;&#24182;&#36890;&#36807;&#26679;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#27169;&#22411;&#22312;&#25429;&#25417;&#36825;&#20123;&#20851;&#31995;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#21576;&#29616;&#20102;&#32467;&#26524;&#12290;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35270;&#21270;&#26041;&#27861;&#26469;&#29702;&#35299;&#27169;&#22411;&#25152;&#25429;&#25417;&#21040;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging data collected from smart meters in buildings can aid in developing policies towards energy conservation. Significant energy savings could be realised if deviations in the building operating conditions are detected early, and appropriate measures are taken. Towards this end, machine learning techniques can be used to automate the discovery of these abnormal patterns in the collected data. Current methods in anomaly detection rely on an underlying model to capture the usual or acceptable operating behaviour. In this paper, we propose a novel attention mechanism to model the consumption behaviour of a building and demonstrate the effectiveness of the model in capturing the relations using sample case studies. A real-world dataset is modelled using the proposed architecture, and the results are presented. A visualisation approach towards understanding the relations captured by the model is also presented.
&lt;/p&gt;</description></item><item><title>LinK&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#29992;&#32447;&#24615;&#20869;&#26680;&#29983;&#25104;&#22120;&#26367;&#25442;&#38745;&#24577;&#20869;&#26680;&#30697;&#38453;&#65292;&#24182;&#37325;&#22797;&#20351;&#29992;&#39044;&#35745;&#31639;&#32858;&#21512;&#32467;&#26524;&#65292;&#22312;&#32500;&#25345;2D&#21367;&#31215;&#32423;&#21035;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#21516;&#26102;&#65292;&#20351;&#27599;&#20010;&#20307;&#32032;&#22312;&#24863;&#30693;21x21x21&#33539;&#22260;&#20869;&#30340;&#19978;&#19979;&#25991;&#12290;</title><link>http://arxiv.org/abs/2303.16094</link><description>&lt;p&gt;
LinK&#65306;&#22522;&#20110;&#32447;&#24615;&#20869;&#26680;&#30340;LiDAR&#19977;&#32500;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
LinK: Linear Kernel for LiDAR-based 3D Perception. (arXiv:2303.16094v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16094
&lt;/p&gt;
&lt;p&gt;
LinK&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#29992;&#32447;&#24615;&#20869;&#26680;&#29983;&#25104;&#22120;&#26367;&#25442;&#38745;&#24577;&#20869;&#26680;&#30697;&#38453;&#65292;&#24182;&#37325;&#22797;&#20351;&#29992;&#39044;&#35745;&#31639;&#32858;&#21512;&#32467;&#26524;&#65292;&#22312;&#32500;&#25345;2D&#21367;&#31215;&#32423;&#21035;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#21516;&#26102;&#65292;&#20351;&#27599;&#20010;&#20307;&#32032;&#22312;&#24863;&#30693;21x21x21&#33539;&#22260;&#20869;&#30340;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#20108;&#32500;&#22823;&#20869;&#26680;&#30340;&#25104;&#21151;&#25193;&#23637;&#21040;&#19977;&#32500;&#24863;&#30693;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;1.&#22788;&#29702;&#19977;&#32500;&#25968;&#25454;&#30340;&#24320;&#38144;&#25104;&#20493;&#22686;&#38271;&#65307;2.&#30001;&#20110;&#25968;&#25454;&#31232;&#30095;&#24615;&#21644;&#31232;&#32570;&#24615;&#32780;&#20135;&#29983;&#30340;&#20248;&#21270;&#22256;&#38590;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#36890;&#36807;&#24341;&#20837;&#22359;&#20849;&#20139;&#26435;&#37325;&#26469;&#23558;&#20869;&#26680;&#22823;&#23567;&#20174;3x3x3&#25193;&#23637;&#21040;7x7x7&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20943;&#23569;&#22359;&#20869;&#29305;&#24449;&#21464;&#21270;&#65292;&#23427;&#20165;&#37319;&#29992;&#36866;&#24230;&#30340;&#22359;&#23610;&#23544;&#65292;&#24182;&#26410;&#23454;&#29616;&#20687;21x21x21&#36825;&#26679;&#30340;&#26356;&#22823;&#20869;&#26680;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;LinK&#65292;&#20197;&#21367;&#31215;&#30340;&#26041;&#24335;&#36890;&#36807;&#20004;&#20010;&#26680;&#24515;&#35774;&#35745;&#22312;&#36866;&#24212;&#38750;&#31354;&#20307;&#32032;&#25552;&#20379;&#26435;&#37325;&#30340;&#22522;&#30784;&#19978;&#23454;&#29616;&#26356;&#24191;&#27867;&#30340;&#24863;&#30693;&#25509;&#25910;&#22495;&#12290;&#20854;&#19968;&#26159;&#29992;&#32447;&#24615;&#20869;&#26680;&#29983;&#25104;&#22120;&#26367;&#25442;&#38745;&#24577;&#20869;&#26680;&#30697;&#38453;&#65292;&#20165;&#36866;&#24212;&#38750;&#31354;&#20307;&#32032;&#25552;&#20379;&#26435;&#37325;&#12290;&#20854;&#20108;&#26159;&#37325;&#22797;&#20351;&#29992;&#37325;&#21472;&#22359;&#30340;&#39044;&#35745;&#31639;&#32858;&#21512;&#32467;&#26524;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#35813;&#26041;&#27861;&#25104;&#21151;&#22320;&#20351;&#27599;&#20010;&#20307;&#32032;&#22312;&#32500;&#25345;2D&#21367;&#31215;&#32423;&#21035;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#21516;&#26102;&#24863;&#30693;21x21x21&#33539;&#22260;&#20869;&#30340;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extending the success of 2D Large Kernel to 3D perception is challenging due to: 1. the cubically-increasing overhead in processing 3D data; 2. the optimization difficulties from data scarcity and sparsity. Previous work has taken the first step to scale up the kernel size from 3x3x3 to 7x7x7 by introducing block-shared weights. However, to reduce the feature variations within a block, it only employs modest block size and fails to achieve larger kernels like the 21x21x21. To address this issue, we propose a new method, called LinK, to achieve a wider-range perception receptive field in a convolution-like manner with two core designs. The first is to replace the static kernel matrix with a linear kernel generator, which adaptively provides weights only for non-empty voxels. The second is to reuse the pre-computed aggregation results in the overlapped blocks to reduce computation complexity. The proposed method successfully enables each voxel to perceive context within a range of 21x21x
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#20248;&#21270;&#23492;&#23384;&#22120;&#25991;&#20214;&#30340;&#28909;&#24433;&#21709;&#12289;&#20248;&#21270;&#32531;&#23384;&#37197;&#32622;&#26469;&#25913;&#21892;&#24615;&#33021;&#21644;&#33021;&#28304;&#28040;&#36153;&#65292;&#31616;&#21270;&#20869;&#23384;&#31649;&#29702;&#22120;&#35774;&#35745;&#21644;&#35780;&#20272;&#36807;&#31243;&#12290;&#26368;&#32456;&#23454;&#29616;&#20102;&#23545;&#25972;&#20010;&#20869;&#23384;&#23376;&#31995;&#32479;&#30340;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2303.16074</link><description>&lt;p&gt;
&#20869;&#23384;&#23376;&#31995;&#32479;&#30340;&#36827;&#21270;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Evolutionary Design of the Memory Subsystem. (arXiv:2303.16074v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#20248;&#21270;&#23492;&#23384;&#22120;&#25991;&#20214;&#30340;&#28909;&#24433;&#21709;&#12289;&#20248;&#21270;&#32531;&#23384;&#37197;&#32622;&#26469;&#25913;&#21892;&#24615;&#33021;&#21644;&#33021;&#28304;&#28040;&#36153;&#65292;&#31616;&#21270;&#20869;&#23384;&#31649;&#29702;&#22120;&#35774;&#35745;&#21644;&#35780;&#20272;&#36807;&#31243;&#12290;&#26368;&#32456;&#23454;&#29616;&#20102;&#23545;&#25972;&#20010;&#20869;&#23384;&#23376;&#31995;&#32479;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#23384;&#23618;&#27425;&#32467;&#26500;&#23545;&#20110;&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#33021;&#28304;&#28040;&#32791;&#20855;&#26377;&#24456;&#39640;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#30340;&#23884;&#20837;&#24335;&#31995;&#32479;&#65292;&#21253;&#25324;&#31227;&#21160;&#35774;&#22791;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#36816;&#34892;&#22823;&#37327;&#20869;&#23384;&#30340;&#22810;&#23186;&#20307;&#24212;&#29992;&#31243;&#24207;&#12290;&#36825;&#22686;&#21152;&#20102;&#23545;&#20869;&#23384;&#23376;&#31995;&#32479;&#30340;&#21387;&#21147;&#65292;&#24182;&#24433;&#21709;&#24615;&#33021;&#21644;&#33021;&#28304;&#28040;&#32791;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#28909;&#38382;&#39064;&#12289;&#24615;&#33021;&#38477;&#20302;&#21644;&#39640;&#33021;&#28304;&#28040;&#32791;&#31561;&#22240;&#32032;&#20250;&#23545;&#35774;&#22791;&#36896;&#25104;&#19981;&#21487;&#36870;&#30340;&#25439;&#23475;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#26041;&#27861;&#20316;&#20026;&#21333;&#19968;&#26041;&#27861;&#35770;&#25972;&#21512;&#65292;&#26469;&#20248;&#21270;&#25972;&#20010;&#20869;&#23384;&#23376;&#31995;&#32479;&#12290;&#39318;&#20808;&#65292;&#20998;&#26512;&#21644;&#20248;&#21270;&#23492;&#23384;&#22120;&#25991;&#20214;&#30340;&#28909;&#24433;&#21709;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#20248;&#21270;&#32531;&#23384;&#37197;&#32622;&#24182;&#26681;&#25454;&#36816;&#34892;&#24212;&#29992;&#31243;&#24207;&#26469;&#25913;&#21892;&#24615;&#33021;&#21644;&#33021;&#28304;&#28040;&#36153;&#65292;&#35299;&#20915;&#32531;&#23384;&#23384;&#20648;&#22120;&#30340;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#31616;&#21270;&#20102;&#36890;&#29992;&#21644;&#23450;&#21046;&#30340;&#21160;&#24577;&#20869;&#23384;&#31649;&#29702;&#22120;&#30340;&#35774;&#35745;&#21644;&#35780;&#20272;&#36807;&#31243;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#20027;&#20869;&#23384;&#20013;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19981;&#21516;&#30340;&#32763;&#35793;&#25216;&#26415;&#26469;&#25552;&#39640;&#20869;&#23384;&#23376;&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#33021;&#28304;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The memory hierarchy has a high impact on the performance and power consumption in the system. Moreover, current embedded systems, included in mobile devices, are specifically designed to run multimedia applications, which are memory intensive. This increases the pressure on the memory subsystem and affects the performance and energy consumption. In this regard, the thermal problems, performance degradation and high energy consumption, can cause irreversible damage to the devices. We address the optimization of the whole memory subsystem with three approaches integrated as a single methodology. Firstly, the thermal impact of register file is analyzed and optimized. Secondly, the cache memory is addressed by optimizing cache configuration according to running applications and improving both performance and power consumption. Finally, we simplify the design and evaluation process of general-purpose and customized dynamic memory manager, in the main memory. To this aim, we apply different
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#24816;&#24615;&#23398;&#20064;&#21482;&#23545;&#38169;&#35823;&#26679;&#26412;&#36827;&#34892;&#26356;&#26032;&#21442;&#25968;&#65292;&#23454;&#29616;&#24555;&#36895;&#12289;&#33410;&#33021;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#24182;&#22312;&#21333;&#23618;MLP&#27169;&#22411;&#19978;&#36798;&#21040;&#20102;99.2&#65285;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#27604;&#21305;&#37197;&#30340;&#21453;&#21521;&#20256;&#25773;&#32593;&#32476;&#24555;7.6&#20493;&#12290;</title><link>http://arxiv.org/abs/2303.16067</link><description>&lt;p&gt;
&#24816;&#24615;&#23398;&#20064;&#65306;&#19968;&#31181;&#21463;&#29983;&#29289;&#21551;&#21457;&#30340;&#24555;&#36895;&#12289;&#33410;&#33021;&#31361;&#35302;&#21487;&#22609;&#24615;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Lazy learning: a biologically-inspired plasticity rule for fast and energy efficient synaptic plasticity. (arXiv:2303.16067v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16067
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#24816;&#24615;&#23398;&#20064;&#21482;&#23545;&#38169;&#35823;&#26679;&#26412;&#36827;&#34892;&#26356;&#26032;&#21442;&#25968;&#65292;&#23454;&#29616;&#24555;&#36895;&#12289;&#33410;&#33021;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#24182;&#22312;&#21333;&#23618;MLP&#27169;&#22411;&#19978;&#36798;&#21040;&#20102;99.2&#65285;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#27604;&#21305;&#37197;&#30340;&#21453;&#21521;&#20256;&#25773;&#32593;&#32476;&#24555;7.6&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#26102;&#65292;&#21363;&#20351;&#26679;&#26412;&#34987;&#27491;&#30830;&#20998;&#31867;&#65292;&#21442;&#25968;&#20063;&#20250;&#22312;&#27599;&#20010;&#35797;&#39564;&#20013;&#26356;&#26032;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#38598;&#20013;&#23398;&#20064;&#24046;&#38169;&#12290;&#21463;&#20154;&#31867;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24816;&#24615;&#23398;&#20064;&#65292;&#20165;&#23545;&#38169;&#35823;&#26679;&#26412;&#36827;&#34892;&#23398;&#20064;&#12290;&#24816;&#24615;&#23398;&#20064;&#21487;&#20197;&#29992;&#20960;&#34892;&#20195;&#30721;&#23454;&#29616;&#65292;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#24816;&#24615;&#23398;&#20064;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22312;&#25968;&#25454;&#38598;&#36739;&#22823;&#26102;&#29305;&#21035;&#36866;&#29992;&#12290;&#20363;&#22914;&#65292;&#22312;&#20351;&#29992;&#21333;&#23618;MLP&#23545;&#25193;&#23637;MNIST&#36827;&#34892;&#27979;&#35797;&#20934;&#30830;&#29575;&#36798;&#21040;99.2&#65285;&#65292;&#24182;&#19988;&#27604;&#21305;&#37197;&#21453;&#21521;&#20256;&#25773;&#32593;&#32476;&#24555;7.6&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
When training neural networks for classification tasks with backpropagation, parameters are updated on every trial, even if the sample is classified correctly. In contrast, humans concentrate their learning effort on errors. Inspired by human learning, we introduce lazy learning, which only learns on incorrect samples. Lazy learning can be implemented in a few lines of code and requires no hyperparameter tuning. Lazy learning achieves state-of-the-art performance and is particularly suited when datasets are large. For instance, it reaches 99.2% test accuracy on Extended MNIST using a single-layer MLP, and does so 7.6x faster than a matched backprop network
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#32780;&#20934;&#30830;&#22320;&#36817;&#20284;&#31232;&#30095;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#65288;GAMs&#65289;&#30340;Rashomon&#38598;&#30340;&#25216;&#26415;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#36817;&#20284;&#27169;&#22411;&#26469;&#35299;&#20915;&#23454;&#38469;&#24212;&#29992;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.16047</link><description>&lt;p&gt;
&#29702;&#35299;&#21644;&#25506;&#32034;&#31232;&#30095;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#30340;&#25972;&#20010;&#20248;&#31168;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
Understanding and Exploring the Whole Set of Good Sparse Generalized Additive Models. (arXiv:2303.16047v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16047
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#32780;&#20934;&#30830;&#22320;&#36817;&#20284;&#31232;&#30095;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#65288;GAMs&#65289;&#30340;Rashomon&#38598;&#30340;&#25216;&#26415;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#36817;&#20284;&#27169;&#22411;&#26469;&#35299;&#20915;&#23454;&#38469;&#24212;&#29992;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19982;&#39046;&#22495;&#19987;&#23478;&#20043;&#38388;&#30340;&#20132;&#20114;&#33267;&#20851;&#37325;&#35201;&#65307;&#28982;&#32780;&#65292;&#36890;&#24120;&#21482;&#29983;&#25104;&#21333;&#20010;&#27169;&#22411;&#30340;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#19981;&#21033;&#20110;&#27492;&#31867;&#20132;&#20114;&#12290;&#36817;&#20284;&#21644;&#25506;&#32034;Rashomon&#38598;&#65292;&#21363;&#25152;&#26377;&#36817;&#20046;&#26368;&#20248;&#27169;&#22411;&#30340;&#38598;&#21512;&#65292;&#36890;&#36807;&#25552;&#20379;&#29992;&#25143;&#21487;&#25628;&#32034;&#30340;&#31354;&#38388;&#21253;&#21547;&#22810;&#26679;&#24615;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36825;&#19968;&#23454;&#38469;&#25361;&#25112;&#65292;&#39046;&#22495;&#19987;&#23478;&#21487;&#20197;&#20174;&#20013;&#36873;&#25321;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#32780;&#20934;&#30830;&#22320;&#36817;&#20284;&#31232;&#30095;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#65288;GAMs&#65289;&#30340;Rashomon&#38598;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29992;&#20110;&#36817;&#20284;&#20855;&#26377;&#22266;&#23450;&#25903;&#25345;&#38598;&#30340;GAMs&#30340;Rashomon&#38598;&#30340;&#26925;&#29699;&#24418;&#31639;&#27861;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#26925;&#29699;&#24418;&#36817;&#20284;&#20102;&#35768;&#22810;&#19981;&#21516;&#25903;&#25345;&#38598;&#30340;Rashomon&#38598;&#12290;&#36817;&#20284;&#30340;Rashomon&#38598;&#20026;&#35299;&#20915;&#23454;&#38469;&#25361;&#25112;&#65292;&#20363;&#22914;&#65288;1&#65289;&#30740;&#31350;&#27169;&#22411;&#31867;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#65307;&#65288;2&#65289;&#22312;&#29992;&#25143;&#25351;&#23450;&#32422;&#26463;&#26465;&#20214;&#19979;&#26597;&#25214;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real applications, interaction between machine learning model and domain experts is critical; however, the classical machine learning paradigm that usually produces only a single model does not facilitate such interaction. Approximating and exploring the Rashomon set, i.e., the set of all near-optimal models, addresses this practical challenge by providing the user with a searchable space containing a diverse set of models from which domain experts can choose. We present a technique to efficiently and accurately approximate the Rashomon set of sparse, generalized additive models (GAMs). We present algorithms to approximate the Rashomon set of GAMs with ellipsoids for fixed support sets and use these ellipsoids to approximate Rashomon sets for many different support sets. The approximated Rashomon set serves as a cornerstone to solve practical challenges such as (1) studying the variable importance for the model class; (2) finding models under user-specified constraints (monotonicity
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#26041;&#27861;&#21407;&#21017;&#30340;&#36890;&#29992;&#21333;&#21464;&#37327;&#20449;&#21495;&#21435;&#21367;&#31215;&#26041;&#27861;&#12290;&#36890;&#36807;&#35745;&#31639;&#8220;&#36890;&#29992;&#20998;&#24067;&#8221;&#30340;&#20272;&#35745;&#26469;&#29420;&#31435;&#20110;&#27010;&#29575;&#20998;&#24067;&#22320;&#26500;&#24314;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#24182;&#22522;&#20110;&#20449;&#24687;&#35770;&#21644;&#31639;&#27861;&#27010;&#29575;&#30340;&#22810;&#32500;&#31354;&#38388;&#37325;&#26500;&#65292;&#25506;&#32034;&#38750;&#38543;&#26426;&#25968;&#25454;&#20013;&#20851;&#20110;&#29289;&#29702;&#24615;&#36136;&#30340;&#20449;&#24687;&#32534;&#30721;&#12290;&#35813;&#26041;&#27861;&#22312;&#32534;&#30721;&#29702;&#35770;&#23588;&#20854;&#26159;&#38646;&#22833;&#30495;&#21387;&#32553;&#26041;&#38754;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2303.16045</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#31354;&#38388;&#21435;&#21367;&#31215;&#21644;&#20449;&#24687;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Optimal Spatial Deconvolution and Message Reconstruction from a Large Generative Model of Models. (arXiv:2303.16045v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#26041;&#27861;&#21407;&#21017;&#30340;&#36890;&#29992;&#21333;&#21464;&#37327;&#20449;&#21495;&#21435;&#21367;&#31215;&#26041;&#27861;&#12290;&#36890;&#36807;&#35745;&#31639;&#8220;&#36890;&#29992;&#20998;&#24067;&#8221;&#30340;&#20272;&#35745;&#26469;&#29420;&#31435;&#20110;&#27010;&#29575;&#20998;&#24067;&#22320;&#26500;&#24314;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#24182;&#22522;&#20110;&#20449;&#24687;&#35770;&#21644;&#31639;&#27861;&#27010;&#29575;&#30340;&#22810;&#32500;&#31354;&#38388;&#37325;&#26500;&#65292;&#25506;&#32034;&#38750;&#38543;&#26426;&#25968;&#25454;&#20013;&#20851;&#20110;&#29289;&#29702;&#24615;&#36136;&#30340;&#20449;&#24687;&#32534;&#30721;&#12290;&#35813;&#26041;&#27861;&#22312;&#32534;&#30721;&#29702;&#35770;&#23588;&#20854;&#26159;&#38646;&#22833;&#30495;&#21387;&#32553;&#26041;&#38754;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#26041;&#27861;&#21407;&#21017;&#30340;&#36890;&#29992;&#21333;&#21464;&#37327;&#20449;&#21495;&#21435;&#21367;&#31215;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#27169;&#22411;&#29983;&#25104;&#27169;&#22411;&#65292;&#20381;&#36182;&#20110;&#20449;&#24687;&#35770;&#21644;&#31639;&#27861;&#27010;&#29575;&#65292;&#24182;&#35745;&#31639;&#20986;&#8220;&#36890;&#29992;&#20998;&#24067;&#8221;&#30340;&#20272;&#35745;&#65292;&#20174;&#32780;&#29420;&#31435;&#20110;&#27010;&#29575;&#20998;&#24067;&#22320;&#26500;&#24314;&#20102;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#20449;&#24687;&#35770;&#21644;&#31639;&#27861;&#27010;&#29575;&#30340;&#22810;&#32500;&#31354;&#38388;&#37325;&#26500;&#65292;&#21487;&#20197;&#25506;&#31350;&#38750;&#38543;&#26426;&#25968;&#25454;&#22914;&#20309;&#32534;&#30721;&#20851;&#20110;&#29289;&#29702;&#24615;&#36136;&#30340;&#20449;&#24687;&#65292;&#20363;&#22914;&#20449;&#21495;&#25110;&#20449;&#24687;&#30340;&#32500;&#24230;&#21644;&#38271;&#24230;&#23610;&#24230;&#12290;&#35813;&#26041;&#27861;&#26159;&#19982;&#21487;&#35745;&#31639;&#25110;&#21322;&#21487;&#35745;&#31639;&#30340;&#36817;&#20284;&#26041;&#27861;&#25110;&#32534;&#30721;-&#35299;&#30721;&#26041;&#26696;&#30456;&#20851;&#20294;&#19981;&#29420;&#31435;&#30340;&#12290;&#26412;&#25991;&#30340;&#32467;&#26524;&#23545;&#32534;&#30721;&#29702;&#35770;&#23588;&#20854;&#26159;&#38646;&#22833;&#30495;&#21387;&#32553;&#26377;&#24212;&#29992;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a general-purpose univariate signal deconvolution method based on the principles of an approach to Artificial General Intelligence. This approach is based on a generative model that combines information theory and algorithmic probability that required a large calculation of an estimation of a `universal distribution' to build a general-purpose model of models independent of probability distributions. This was used to investigate how non-random data may encode information about the physical properties such as dimension and length scales in which a signal or message may have been originally encoded, embedded, or generated. This multidimensional space reconstruction method is based on information theory and algorithmic probability, and it is agnostic, but not independent, with respect to the chosen computable or semi-computable approximation method or encoding-decoding scheme. The results presented in this paper are useful for applications in coding theory, particularly in ze
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#39640;&#20302;&#39057;&#29575;&#20851;&#31995;&#30340;&#26080;&#20559;&#20506;&#30340;&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#38271;&#23614;&#38382;&#39064;&#21644;&#20027;&#20307;-&#23545;&#35937;&#23545;&#25317;&#26377;&#22810;&#20010;&#37325;&#21472;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;&#22312;&#24191;&#27867;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.15994</link><description>&lt;p&gt;
HiLo: &#21033;&#29992;&#39640;&#20302;&#39057;&#29575;&#20851;&#31995;&#36827;&#34892;&#26080;&#20559;&#20506;&#30340;&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
HiLo: Exploiting High Low Frequency Relations for Unbiased Panoptic Scene Graph Generation. (arXiv:2303.15994v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15994
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#39640;&#20302;&#39057;&#29575;&#20851;&#31995;&#30340;&#26080;&#20559;&#20506;&#30340;&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#38271;&#23614;&#38382;&#39064;&#21644;&#20027;&#20307;-&#23545;&#35937;&#23545;&#25317;&#26377;&#22810;&#20010;&#37325;&#21472;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;&#22312;&#24191;&#27867;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;&#65288;PSG&#65289;&#26159;&#19968;&#39033;&#26368;&#36817;&#25552;&#20986;&#30340;&#22270;&#20687;&#22330;&#26223;&#29702;&#35299;&#20219;&#21153;&#65292;&#26088;&#22312;&#23545;&#22270;&#20687;&#36827;&#34892;&#20998;&#21106;&#21644;&#25552;&#21462;&#20027;&#20307;&#12289;&#23545;&#35937;&#21450;&#20854;&#20851;&#31995;&#19977;&#20803;&#32452;&#20197;&#26500;&#24314;&#22330;&#26223;&#22270;&#12290;&#30001;&#20110;&#20851;&#31995;&#31867;&#21035;&#30340;&#38271;&#23614;&#38382;&#39064;&#65292;&#36825;&#39033;&#20219;&#21153;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20351;&#24471;&#21333;&#32431;&#30340;&#26377;&#20559;&#20506;&#26041;&#27861;&#26356;&#20542;&#21521;&#20110;&#39640;&#39057;&#29575;&#20851;&#31995;&#12290;&#29616;&#26377;&#30340;&#26080;&#20559;&#20506;&#26041;&#27861;&#36890;&#36807;&#25968;&#25454;/&#25439;&#22833;&#37325;&#26032;&#24179;&#34913;&#20197;&#25903;&#25345;&#20302;&#39057;&#29575;&#20851;&#31995;&#26469;&#35299;&#20915;&#38271;&#23614;&#38382;&#39064;&#12290;&#20854;&#27425;&#65292;&#19968;&#20010;&#20027;&#20307;-&#23545;&#35937;&#23545;&#21487;&#20197;&#26377;&#20004;&#20010;&#25110;&#26356;&#22810;&#22312;&#35821;&#20041;&#19978;&#37325;&#21472;&#30340;&#20851;&#31995;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#26041;&#27861;&#20542;&#21521;&#20110;&#20351;&#29992;&#19968;&#20010;&#32780;&#19981;&#26159;&#21478;&#19968;&#20010;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#30340;HiLo&#26694;&#26550;&#35753;&#19981;&#21516;&#30340;&#32593;&#32476;&#20998;&#25903;&#19987;&#38376;&#22788;&#29702;&#20302;&#39057;&#21644;&#39640;&#39057;&#20851;&#31995;&#65292;&#24378;&#21046;&#23454;&#26045;&#23427;&#20204;&#30340;&#19968;&#33268;&#24615;&#24182;&#34701;&#21512;&#32467;&#26524;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#39318;&#27425;&#25552;&#20986;&#26126;&#30830;&#26080;&#20559;PSG&#26041;&#27861;&#30340;&#20154;&#12290;&#22312;&#24191;&#27867;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;HiLo&#26694;&#26550;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Panoptic Scene Graph generation (PSG) is a recently proposed task in image scene understanding that aims to segment the image and extract triplets of subjects, objects and their relations to build a scene graph. This task is particularly challenging for two reasons. First, it suffers from a long-tail problem in its relation categories, making naive biased methods more inclined to high-frequency relations. Existing unbiased methods tackle the long-tail problem by data/loss rebalancing to favor low-frequency relations. Second, a subject-object pair can have two or more semantically overlapping relations. While existing methods favor one over the other, our proposed HiLo framework lets different network branches specialize on low and high frequency relations, enforce their consistency and fuse the results. To the best of our knowledge we are the first to propose an explicitly unbiased PSG method. In extensive experiments we show that our HiLo framework achieves state-of-the-art results on
&lt;/p&gt;</description></item><item><title>TraffNet&#26159;&#19968;&#20010;&#23398;&#20064;&#20132;&#36890;&#37327;&#29983;&#25104;&#21407;&#22240;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#34920;&#31034;&#20026;&#24322;&#26500;&#22270;&#65292;&#21033;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#23454;&#29616;&#20102;&#23545;&#20132;&#36890;&#29983;&#25104;&#21407;&#22240;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2303.15954</link><description>&lt;p&gt;
TraffNet&#65306;&#23398;&#20064;&#36947;&#36335;&#32593;&#32476;&#25968;&#23383;&#23402;&#29983;&#20132;&#36890;&#29983;&#25104;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
TraffNet: Learning Causality of Traffic Generation for Road Network Digital Twins. (arXiv:2303.15954v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15954
&lt;/p&gt;
&lt;p&gt;
TraffNet&#26159;&#19968;&#20010;&#23398;&#20064;&#20132;&#36890;&#37327;&#29983;&#25104;&#21407;&#22240;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#34920;&#31034;&#20026;&#24322;&#26500;&#22270;&#65292;&#21033;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#23454;&#29616;&#20102;&#23545;&#20132;&#36890;&#29983;&#25104;&#21407;&#22240;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36947;&#36335;&#32593;&#32476;&#25968;&#23383;&#23402;&#29983;&#65288;RNDT&#65289;&#22312;&#24320;&#21457;&#19979;&#19968;&#20195;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#20132;&#36890;&#35268;&#21010;&#21644;&#25511;&#21046;&#12290;&#20026;&#20102;&#25903;&#25345;&#23454;&#26102;&#20915;&#31574;&#65292;RNDT&#38656;&#35201;&#19968;&#20010;&#27169;&#22411;&#65292;&#20174;&#22312;&#32447;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#21160;&#24577;&#23398;&#20064;&#20132;&#36890;&#27169;&#24335;&#24182;&#29983;&#25104;&#39640;&#20445;&#30495;&#27169;&#25311;&#32467;&#26524;&#12290;&#23613;&#31649;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24403;&#21069;&#20132;&#36890;&#39044;&#27979;&#25216;&#26415;&#24050;&#32463;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#36825;&#20123;&#25216;&#26415;&#20165;&#36890;&#36807;&#25366;&#25496;&#21382;&#21490;&#20132;&#36890;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#24615;&#26469;&#39044;&#27979;&#26410;&#26469;&#20132;&#36890;&#65292;&#32780;&#24573;&#30053;&#20102;&#20132;&#36890;&#29983;&#25104;&#30340;&#21407;&#22240;&#65292;&#20363;&#22914;&#20132;&#36890;&#38656;&#27714;&#21644;&#36335;&#24452;&#36873;&#25321;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#23545;&#20110;&#23454;&#26102;&#20915;&#31574;&#26159;&#19981;&#21487;&#38752;&#30340;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#31216;&#20026; TraffNet&#65292;&#35813;&#26694;&#26550;&#20174;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#20013;&#23398;&#20064;&#20132;&#36890;&#37327;&#30340;&#22240;&#26524;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#24322;&#26500;&#22270;&#26469;&#34920;&#31034;&#36947;&#36335;&#32593;&#32476;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#24182;&#20837;&#39044;&#27979;&#25152;&#38656;&#30340;&#20854;&#20182;&#25968;&#25454;&#65292;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#20174;&#32780;&#33021;&#22815;&#39044;&#27979;&#20132;&#36890;&#37327;&#30340;&#22240;&#26524;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Road network digital twins (RNDTs) play a critical role in the development of next-generation intelligent transportation systems, enabling more precise traffic planning and control. To support just-in-time (JIT) decision making, RNDTs require a model that dynamically learns the traffic patterns from online sensor data and generates high-fidelity simulation results. Although current traffic prediction techniques based on graph neural networks have achieved state-of-the-art performance, these techniques only predict future traffic by mining correlations in historical traffic data, disregarding the causes of traffic generation, such as traffic demands and route selection. Therefore, their performance is unreliable for JIT decision making. To fill this gap, we introduce a novel deep learning framework called TraffNet that learns the causality of traffic volume from vehicle trajectory data. First, we use a heterogeneous graph to represent the road network, allowing the model to incorporate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20462;&#25913;&#29256;&#30340;Edge-Popup&#21644;Biprop&#31639;&#27861;&#65292;&#21517;&#20026;&#36845;&#20195;&#26435;&#37325;&#22238;&#25910;&#65292;&#35782;&#21035;&#38543;&#26426;&#21021;&#22987;&#21270;&#32593;&#32476;&#20013;&#30340;&#37325;&#35201;&#26435;&#37325;&#23376;&#38598;&#65292;&#20197;&#36827;&#34892;&#23618;&#20869;&#37325;&#29992;&#65292;&#24182;&#25214;&#21040;&#39640;&#31934;&#24230;&#30340;&#23376;&#32593;&#32476;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#31232;&#30095;&#24230;&#65292;&#24182;&#29992;&#30456;&#21453;&#30340;&#21457;&#29616;&#26469;&#34917;&#20805;&#22810;&#37325;&#24425;&#31080;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2303.15953</link><description>&lt;p&gt;
&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#23376;&#32593;&#32476;&#19982;&#36845;&#20195;&#26435;&#37325;&#22238;&#25910;
&lt;/p&gt;
&lt;p&gt;
Randomly Initialized Subnetworks with Iterative Weight Recycling. (arXiv:2303.15953v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20462;&#25913;&#29256;&#30340;Edge-Popup&#21644;Biprop&#31639;&#27861;&#65292;&#21517;&#20026;&#36845;&#20195;&#26435;&#37325;&#22238;&#25910;&#65292;&#35782;&#21035;&#38543;&#26426;&#21021;&#22987;&#21270;&#32593;&#32476;&#20013;&#30340;&#37325;&#35201;&#26435;&#37325;&#23376;&#38598;&#65292;&#20197;&#36827;&#34892;&#23618;&#20869;&#37325;&#29992;&#65292;&#24182;&#25214;&#21040;&#39640;&#31934;&#24230;&#30340;&#23376;&#32593;&#32476;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#31232;&#30095;&#24230;&#65292;&#24182;&#29992;&#30456;&#21453;&#30340;&#21457;&#29616;&#26469;&#34917;&#20805;&#22810;&#37325;&#24425;&#31080;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#37325;&#24425;&#31080;&#20551;&#35774;&#35748;&#20026;&#65292;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#21253;&#21547;&#20960;&#20010;&#23376;&#32593;&#32476;&#65292;&#36825;&#20123;&#23376;&#32593;&#32476;&#30340;&#31934;&#24230;&#19982;&#21516;&#19968;&#26550;&#26500;&#30340;&#24050;&#32463;&#23436;&#20840;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#24403;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#35201;&#27714;&#32593;&#32476;&#20855;&#26377;&#36275;&#22815;&#30340;&#36807;&#21442;&#25968;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#65288;Edge-Popup&#21644;Biprop&#65289;&#30340;&#20462;&#25913;&#29256;&#65292;&#23427;&#21487;&#20197;&#22312;&#27809;&#26377;&#39069;&#22806;&#23384;&#20648;&#25104;&#26412;&#25110;&#32553;&#25918;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#39640;&#31934;&#24230;&#30340;&#23376;&#32593;&#32476;&#12290;&#31639;&#27861;&#65292;&#36845;&#20195;&#24335;&#26435;&#37325;&#22238;&#25910;&#65292;&#35782;&#21035;&#38543;&#24847;&#21021;&#22987;&#21270;&#32593;&#32476;&#20013;&#30340;&#37325;&#35201;&#26435;&#37325;&#23376;&#38598;&#20197;&#36827;&#34892;&#23618;&#20869;&#37325;&#29992;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26356;&#23567;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#26356;&#39640;&#30340;&#20462;&#21098;&#29575;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#8220;&#22238;&#25910;&#8221;&#29616;&#26377;&#26435;&#37325;&#26469;&#22686;&#21152;&#27169;&#22411;&#30340;&#31232;&#30095;&#24230;&#12290;&#38500;&#20102;&#36845;&#20195;&#26435;&#37325;&#22238;&#25910;&#65292;&#25105;&#20204;&#36824;&#29992;&#30456;&#21453;&#30340;&#21457;&#29616;&#26469;&#34917;&#20805;&#22810;&#37325;&#24425;&#31080;&#20551;&#35774;&#65306;&#39640;&#31934;&#24230;&#30340;&#38543;&#26426;&#21021;&#22987;&#21270;&#23376;&#32593;&#32476;&#20135;&#29983;&#19981;&#21516;&#30340;&#25513;&#30721;&#65292;&#23613;&#31649;&#23427;&#20204;&#20849;&#20139;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Multi-Prize Lottery Ticket Hypothesis posits that randomly initialized neural networks contain several subnetworks that achieve comparable accuracy to fully trained models of the same architecture. However, current methods require that the network is sufficiently overparameterized. In this work, we propose a modification to two state-of-the-art algorithms (Edge-Popup and Biprop) that finds high-accuracy subnetworks with no additional storage cost or scaling. The algorithm, Iterative Weight Recycling, identifies subsets of important weights within a randomly initialized network for intra-layer reuse. Empirically we show improvements on smaller network architectures and higher prune rates, finding that model sparsity can be increased through the "recycling" of existing weights. In addition to Iterative Weight Recycling, we complement the Multi-Prize Lottery Ticket Hypothesis with a reciprocal finding: high-accuracy, randomly initialized subnetwork's produce diverse masks, despite bei
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Deep Selection&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#20174;&#22810;&#30446;&#26631;&#25163;&#26415;&#24405;&#20687;&#20013;&#36873;&#25321;&#26368;&#20339;&#35270;&#35282;&#30340;&#25668;&#20687;&#26426;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#19987;&#23478;&#27880;&#37322;&#65292;&#39044;&#27979;&#25668;&#20687;&#26426;&#30340;&#36873;&#25321;&#27010;&#29575;&#65292;&#20174;&#32780;&#20248;&#21270;&#25163;&#26415;&#35760;&#24405;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.15947</link><description>&lt;p&gt;
&#28145;&#24230;&#36873;&#25321;&#65306;&#19968;&#31181;&#29992;&#20110;&#25163;&#26415;&#24405;&#20687;&#30340;&#20840;&#30417;&#30563;&#25668;&#20687;&#22836;&#36873;&#25321;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Deep Selection: A Fully Supervised Camera Selection Network for Surgery Recordings. (arXiv:2303.15947v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15947
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Deep Selection&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#20174;&#22810;&#30446;&#26631;&#25163;&#26415;&#24405;&#20687;&#20013;&#36873;&#25321;&#26368;&#20339;&#35270;&#35282;&#30340;&#25668;&#20687;&#26426;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#19987;&#23478;&#27880;&#37322;&#65292;&#39044;&#27979;&#25668;&#20687;&#26426;&#30340;&#36873;&#25321;&#27010;&#29575;&#65292;&#20174;&#32780;&#20248;&#21270;&#25163;&#26415;&#35760;&#24405;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25163;&#26415;&#23460;&#20013;&#35760;&#24405;&#25163;&#26415;&#26159;&#25945;&#32946;&#21644;&#35780;&#20272;&#21307;&#30103;&#27835;&#30103;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25163;&#26415;&#36807;&#31243;&#20013;&#30446;&#26631;&#21463;&#21040;&#20005;&#37325;&#30340;&#36974;&#25377;&#65292;&#22914;&#25163;&#26415;&#22330;&#22320;&#12289;&#25163;&#26415;&#24037;&#20855;&#25110;&#21307;&#29983;&#30340;&#25163;&#65292;&#25152;&#20197;&#35760;&#24405;&#36825;&#20123;&#30446;&#26631;&#26159;&#22256;&#38590;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#35760;&#24405;&#31995;&#32479;&#65292;&#22312;&#25163;&#26415;&#28783;&#20013;&#23884;&#20837;&#22810;&#20010;&#25668;&#20687;&#22836;&#65292;&#24182;&#20551;&#23450;&#20219;&#20309;&#26102;&#20505;&#33267;&#23569;&#26377;&#19968;&#21488;&#25668;&#20687;&#26426;&#22312;&#35760;&#24405;&#27809;&#26377;&#36974;&#25377;&#30340;&#30446;&#26631;&#12290;&#30001;&#20110;&#23884;&#20837;&#30340;&#25668;&#20687;&#26426;&#33719;&#24471;&#20102;&#22810;&#20010;&#35270;&#39057;&#24207;&#21015;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#20174;&#22810;&#20010;&#35270;&#39057;&#24207;&#21015;&#20013;&#36873;&#25321;&#26368;&#20339;&#25163;&#26415;&#35270;&#22270;&#30340;&#20219;&#21153;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#25163;&#26415;&#22330;&#22320;&#30340;&#38754;&#31215;&#22823;&#23567;&#36873;&#25321;&#25668;&#20687;&#26426;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#23398;&#20064;&#19987;&#23478;&#27880;&#37322;&#30340;&#30417;&#30563;&#26469;&#39044;&#27979;&#25668;&#20687;&#26426;&#36873;&#25321;&#27010;&#29575;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#35760;&#24405;&#20102;&#20845;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#25972;&#23481;&#25163;&#26415;&#65292;&#24182;&#25552;&#20379;&#20102;&#25668;&#20687;&#26426;&#20999;&#25442;&#30340;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recording surgery in operating rooms is an essential task for education and evaluation of medical treatment. However, recording the desired targets, such as the surgery field, surgical tools, or doctor's hands, is difficult because the targets are heavily occluded during surgery. We use a recording system in which multiple cameras are embedded in the surgical lamp, and we assume that at least one camera is recording the target without occlusion at any given time. As the embedded cameras obtain multiple video sequences, we address the task of selecting the camera with the best view of the surgery. Unlike the conventional method, which selects the camera based on the area size of the surgery field, we propose a deep neural network that predicts the camera selection probability from multiple video sequences by learning the supervision of the expert annotation. We created a dataset in which six different types of plastic surgery are recorded, and we provided the annotation of camera switch
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#20174;AGI&#30340;&#35282;&#24230;&#30475;&#33041;&#21551;&#21457;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#29616;&#29366;&#21644;&#37325;&#35201;&#29305;&#24449;&#65292;&#35752;&#35770;&#20102;&#23454;&#29616;AGI&#30340;&#37325;&#35201;&#25216;&#26415;&#65292;&#22914;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#25552;&#31034;&#35843;&#25972;&#31561;&#12290;</title><link>http://arxiv.org/abs/2303.15935</link><description>&lt;p&gt;
&#24403;&#33041;&#21551;&#21457;&#24335;&#20154;&#24037;&#26234;&#33021;&#36935;&#35265;AGI
&lt;/p&gt;
&lt;p&gt;
When Brain-inspired AI Meets AGI. (arXiv:2303.15935v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#20174;AGI&#30340;&#35282;&#24230;&#30475;&#33041;&#21551;&#21457;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#29616;&#29366;&#21644;&#37325;&#35201;&#29305;&#24449;&#65292;&#35752;&#35770;&#20102;&#23454;&#29616;AGI&#30340;&#37325;&#35201;&#25216;&#26415;&#65292;&#22914;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#25552;&#31034;&#35843;&#25972;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#38271;&#26399;&#30446;&#26631;&#26159;&#21019;&#36896;&#20986;&#33021;&#22815;&#20687;&#20154;&#31867;&#19968;&#26679;&#25191;&#34892;&#26234;&#21147;&#20219;&#21153;&#30340;&#26426;&#22120;&#65292;&#36825;&#23601;&#26159;&#36890;&#24120;&#25152;&#35828;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;AGI&#30740;&#31350;&#32773;&#20174;&#20154;&#33041;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#24182;&#35797;&#22270;&#22312;&#26234;&#33021;&#26426;&#22120;&#20013;&#22797;&#21046;&#20854;&#21407;&#29702;&#12290;&#33041;&#21551;&#21457;&#24335;&#20154;&#24037;&#26234;&#33021;&#26159;&#20174;&#36825;&#19968;&#21162;&#21147;&#20013;&#23835;&#36215;&#30340;&#19968;&#20010;&#39046;&#22495;&#65292;&#23427;&#32467;&#21512;&#20102;&#31070;&#32463;&#31185;&#23398;&#12289;&#24515;&#29702;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#35265;&#35299;&#65292;&#20197;&#24320;&#21457;&#26356;&#39640;&#25928;&#12289;&#26356;&#24378;&#22823;&#30340;AI&#31995;&#32479;&#12290;&#26412;&#25991;&#20174;AGI&#30340;&#35282;&#24230;&#20840;&#38754;&#20171;&#32461;&#33041;&#21551;&#21457;&#24335;AI&#65292;&#21253;&#25324;&#30446;&#21069;&#30340;&#30740;&#31350;&#36827;&#23637;&#20197;&#21450;&#20854;&#19982;AGI&#30340;&#24191;&#27867;&#32852;&#31995;&#65292;&#20171;&#32461;&#20102;&#20154;&#31867;&#26234;&#33021;&#21644;AGI&#30340;&#37325;&#35201;&#29305;&#24449;&#65288;&#20363;&#22914;&#21487;&#20280;&#32553;&#24615;&#12289;&#22810;&#27169;&#24335;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;&#65289;&#65292;&#24182;&#35752;&#35770;&#20102;&#23454;&#29616;AGI&#30340;&#37325;&#35201;&#25216;&#26415;&#65292;&#20363;&#22914;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#25552;&#31034;&#35843;&#25972;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial General Intelligence (AGI) has been a long-standing goal of humanity, with the aim of creating machines capable of performing any intellectual task that humans can do. To achieve this, AGI researchers draw inspiration from the human brain and seek to replicate its principles in intelligent machines. Brain-inspired artificial intelligence is a field that has emerged from this endeavor, combining insights from neuroscience, psychology, and computer science to develop more efficient and powerful AI systems. In this article, we provide a comprehensive overview of brain-inspired AI from the perspective of AGI. We begin with the current progress in brain-inspired AI and its extensive connection with AGI. We then cover the important characteristics for both human intelligence and AGI (e.g., scaling, multimodality, and reasoning). We discuss important technologies toward achieving AGI in current AI systems, such as in-context learning and prompt tuning. We also investigate the evolu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#23545;&#20004;&#31181;GAN&#26550;&#26500;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#20197;GSWGAN&#34920;&#29616;&#26368;&#20339;&#65292;&#21487;&#20197;&#31169;&#23494;&#22320;&#29983;&#25104;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#20844;&#20849;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2303.15916</link><description>&lt;p&gt;
&#20174;&#31169;&#26377;&#21040;&#20844;&#26377;&#65306;&#22312;&#31169;&#26377;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#24773;&#22659;&#19979;&#23545;GAN&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
From Private to Public: Benchmarking GANs in the Context of Private Time Series Classification. (arXiv:2303.15916v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#23545;&#20004;&#31181;GAN&#26550;&#26500;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#20197;GSWGAN&#34920;&#29616;&#26368;&#20339;&#65292;&#21487;&#20197;&#31169;&#23494;&#22320;&#29983;&#25104;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#20844;&#20849;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#22312;&#21508;&#20010;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#37117;&#24456;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#24403;&#28041;&#21450;&#21040;&#31169;&#20154;&#25968;&#25454;&#26102;&#65292;&#20960;&#20010;&#38480;&#21046;&#20351;&#24471;&#38590;&#20197;&#22312;&#36825;&#20123;&#24212;&#29992;&#39046;&#22495;&#20013;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#23581;&#35797;&#31169;&#23494;&#22320;&#29983;&#25104;&#25968;&#25454;&#65292;&#32780;&#19981;&#26159;&#22312;&#20998;&#31867;&#22120;&#20043;&#19978;&#30452;&#25509;&#24212;&#29992;&#38544;&#31169;&#20445;&#25252;&#26426;&#21046;&#12290;&#35299;&#20915;&#26041;&#26696;&#26159;&#20197;&#19968;&#31181;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#26041;&#24335;&#20174;&#31169;&#26377;&#25968;&#25454;&#21019;&#24314;&#20844;&#20849;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#38024;&#23545;&#31169;&#26377;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#24773;&#22659;&#65292;&#35780;&#20272;&#20102;&#20004;&#31181;&#38750;&#24120;&#31361;&#20986;&#30340;&#22522;&#20110;GAN&#30340;&#26550;&#26500;&#12290;&#19982;&#20808;&#21069;&#20027;&#35201;&#23616;&#38480;&#20110;&#22270;&#20687;&#39046;&#22495;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#30340;&#33539;&#22260;&#26159;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#23588;&#20854;&#26159;GSWGAN&#22312;&#22810;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20248;&#20110;&#31454;&#20105;&#23545;&#25163;DPWGAN&#12290;&#29983;&#25104;&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;GSWGAN&#22312;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#30340;&#24773;&#22659;&#19979;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has proven to be successful in various domains and for different tasks. However, when it comes to private data several restrictions are making it difficult to use deep learning approaches in these application fields. Recent approaches try to generate data privately instead of applying a privacy-preserving mechanism directly, on top of the classifier. The solution is to create public data from private data in a manner that preserves the privacy of the data. In this work, two very prominent GAN-based architectures were evaluated in the context of private time series classification. In contrast to previous work, mostly limited to the image domain, the scope of this benchmark was the time series domain. The experiments show that especially GSWGAN performs well across a variety of public datasets outperforming the competitor DPWGAN. An analysis of the generated datasets further validates the superiority of GSWGAN in the context of time series generation.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38450;&#24481;&#33976;&#39311;&#26426;&#21046;&#19982;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#30456;&#32467;&#21512;&#65292;&#26469;&#38477;&#20302;&#27169;&#22411;&#23545;&#26377;&#27602;&#25915;&#20987;&#30340;&#25935;&#24863;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.15901</link><description>&lt;p&gt;
&#22522;&#20110;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#30340;&#38450;&#24481;&#33976;&#39311;&#20316;&#20026;&#23545;&#25239;&#40065;&#26834;&#24615;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Denoising Autoencoder-based Defensive Distillation as an Adversarial Robustness Algorithm. (arXiv:2303.15901v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38450;&#24481;&#33976;&#39311;&#26426;&#21046;&#19982;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#30456;&#32467;&#21512;&#65292;&#26469;&#38477;&#20302;&#27169;&#22411;&#23545;&#26377;&#27602;&#25915;&#20987;&#30340;&#25935;&#24863;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#25915;&#20987;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#30340;&#40065;&#26834;&#24615;&#36896;&#25104;&#20102;&#26497;&#22823;&#23041;&#32961;&#12290;&#23613;&#31649;&#20351;&#29992;&#20102;&#22810;&#31181;&#38450;&#24481;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#32773;&#31713;&#25913;&#30340;&#21021;&#22987;&#35757;&#32451;&#25968;&#25454;&#12290;&#20026;&#20102;&#38450;&#24481;&#36825;&#31181;&#23545;&#25239;&#25915;&#20987;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#38450;&#24481;&#33976;&#39311;&#26426;&#21046;&#19982;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;(DAE)&#30456;&#32467;&#21512;&#12290;&#35813;&#25216;&#26415;&#26088;&#22312;&#36890;&#36807;&#26816;&#27979;&#21644;&#37325;&#26500;&#35757;&#32451;&#25968;&#25454;&#20013;&#26377;&#23475;&#30340;&#23545;&#25239;&#36755;&#20837;&#26469;&#38477;&#20302;&#33976;&#39311;&#27169;&#22411;&#23545;&#26377;&#27602;&#25915;&#20987;&#30340;&#25935;&#24863;&#24230;&#12290;&#25105;&#20204;&#22312;&#21021;&#22987;&#35757;&#32451;&#25968;&#25454;&#20013;&#28155;&#21152;&#20102;&#31934;&#24515;&#21019;&#24314;&#30340;&#23545;&#25239;&#26679;&#26412;&#20197;&#35780;&#20272;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#35782;&#21035;&#21644;&#37325;&#26500;&#20102;&#26377;&#27602;&#30340;&#36755;&#20837;&#65292;&#21516;&#26102;&#20063;&#32771;&#34385;&#20102;&#22686;&#24378;DNN&#30340;&#40065;&#26834;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20026;&#22312;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#19988;&#40065;&#26834;&#30340;DNN&#38450;&#24481;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks significantly threaten the robustness of deep neural networks (DNNs). Despite the multiple defensive methods employed, they are nevertheless vulnerable to poison attacks, where attackers meddle with the initial training data. In order to defend DNNs against such adversarial attacks, this work proposes a novel method that combines the defensive distillation mechanism with a denoising autoencoder (DAE). This technique tries to lower the sensitivity of the distilled model to poison attacks by spotting and reconstructing poisonous adversarial inputs in the training data. We added carefully created adversarial samples to the initial training data to assess the proposed method's performance. Our experimental findings demonstrate that our method successfully identified and reconstructed the poisonous inputs while also considering enhancing the DNN's resilience. The proposed approach provides a potent and robust defense mechanism for DNNs in various applications where data 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;Data-Agnostic Consolidation&#65288;DAC&#65289;&#65292;&#22312;&#19981;&#20351;&#29992;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#25237;&#24433;&#28508;&#31354;&#38388;&#33976;&#39311;&#25439;&#22833;&#65292;&#22312;&#20998;&#24067;&#24335;&#36830;&#32493;&#23398;&#20064;&#20013;&#23454;&#29616;&#20102;SCD&#20043;&#38388;&#30340;&#21069;&#21521;&#36716;&#31227;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.15888</link><description>&lt;p&gt;
&#26080;&#38656;&#25968;&#25454;&#30340;&#20998;&#24067;&#24335;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#25237;&#24433;&#28508;&#31354;&#38388;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Projected Latent Distillation for Data-Agnostic Consolidation in Distributed Continual Learning. (arXiv:2303.15888v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15888
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;Data-Agnostic Consolidation&#65288;DAC&#65289;&#65292;&#22312;&#19981;&#20351;&#29992;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#25237;&#24433;&#28508;&#31354;&#38388;&#33976;&#39311;&#25439;&#22833;&#65292;&#22312;&#20998;&#24067;&#24335;&#36830;&#32493;&#23398;&#20064;&#20013;&#23454;&#29616;&#20102;SCD&#20043;&#38388;&#30340;&#21069;&#21521;&#36716;&#31227;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#20998;&#24067;&#24335;&#23398;&#20064;&#36890;&#24120;&#30001;&#33258;&#25105;&#20013;&#24515;&#30340;&#35774;&#22791;&#65288;SCD&#65289;&#32452;&#25104;&#65292;&#23427;&#20204;&#29420;&#31435;&#23398;&#20064;&#26412;&#22320;&#20219;&#21153;&#24182;&#19981;&#24895;&#24847;&#20026;&#20854;&#20182;SCD&#30340;&#24615;&#33021;&#20570;&#20986;&#36129;&#29486;&#12290;&#25105;&#20204;&#22914;&#20309;&#20197;&#38646;&#25104;&#26412;&#23454;&#29616;&#21333;&#20010;SCD&#30340;&#21069;&#21521;&#36716;&#31227;&#65311;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#20998;&#24067;&#24335;&#36830;&#32493;&#23398;&#20064;&#22330;&#26223;&#65292;&#22312;&#36825;&#20010;&#22330;&#26223;&#20013;&#65292;SCD&#36866;&#24212;&#26412;&#22320;&#20219;&#21153;&#65292;CL&#27169;&#22411;&#23558;&#30001;&#36825;&#20123;&#27169;&#22411;&#20135;&#29983;&#30340;&#30693;&#35782;&#21512;&#24182;&#32780;&#26080;&#38656;&#26597;&#30475;SCD&#30340;&#31169;&#26377;&#25968;&#25454;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30446;&#21069;&#30340;CL&#26041;&#27861;&#24182;&#19981;&#30452;&#25509;&#36866;&#29992;&#20110;&#36825;&#31181;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21452;&#37325;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;Data-Agnostic Consolidation&#65288;DAC&#65289;&#65292;&#35813;&#26041;&#27861;&#26080;&#38656;&#20351;&#29992;&#21407;&#22987;&#25968;&#25454;&#21363;&#21487;&#21512;&#24182;SC&#27169;&#22411;&#30340;&#27969;&#12290;DAC&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#25237;&#24433;&#28508;&#31354;&#38388;&#33976;&#39311;&#25439;&#22833;&#22312;&#28508;&#31354;&#38388;&#20013;&#25191;&#34892;&#33976;&#39311;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DAC&#20351;SCD&#20043;&#38388;&#30340;&#21069;&#21521;&#36716;&#31227;&#25104;&#20026;&#21487;&#33021;&#65292;&#24182;&#22312;Split CIFAR100&#65292;CORe50&#21644;Split TinyImageNet&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#65292;&#26080;&#38656;&#25490;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed learning on the edge often comprises self-centered devices (SCD) which learn local tasks independently and are unwilling to contribute to the performance of other SDCs. How do we achieve forward transfer at zero cost for the single SCDs? We formalize this problem as a Distributed Continual Learning scenario, where SCD adapt to local tasks and a CL model consolidates the knowledge from the resulting stream of models without looking at the SCD's private data. Unfortunately, current CL methods are not directly applicable to this scenario. We propose Data-Agnostic Consolidation (DAC), a novel double knowledge distillation method that consolidates the stream of SC models without using the original data. DAC performs distillation in the latent space via a novel Projected Latent Distillation loss. Experimental results show that DAC enables forward transfer between SCDs and reaches state-of-the-art accuracy on Split CIFAR100, CORe50 and Split TinyImageNet, both in reharsal-free and
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#20855;&#26377;4K&#20998;&#36776;&#29575;&#30340;&#38654;&#38718;&#21435;&#38500;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#27169;&#22411;&#26469;&#22788;&#29702;&#27492;&#20219;&#21153;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#35270;&#35273;&#36136;&#37327;&#21644;&#23458;&#35266;&#35780;&#20272;&#25351;&#26631;&#26041;&#38754;&#37117;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.15848</link><description>&lt;p&gt;
4K-HAZE: &#20855;&#26377;4K&#20998;&#36776;&#29575;&#30340;&#38654;&#38718;&#21435;&#38500;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
4K-HAZE: A Dehazing Benchmark with 4K Resolution Hazy and Haze-Free Images. (arXiv:2303.15848v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#20855;&#26377;4K&#20998;&#36776;&#29575;&#30340;&#38654;&#38718;&#21435;&#38500;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#27169;&#22411;&#26469;&#22788;&#29702;&#27492;&#20219;&#21153;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#35270;&#35273;&#36136;&#37327;&#21644;&#23458;&#35266;&#35780;&#20272;&#25351;&#26631;&#26041;&#38754;&#37117;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#31227;&#21160;&#35774;&#22791;&#21644;&#29289;&#32852;&#32593;&#35774;&#22791;&#36843;&#20999;&#38656;&#35201;&#19968;&#31995;&#21015;&#26041;&#27861;&#26469;&#22686;&#24378;4K&#22270;&#20687;&#65292;&#20294;&#36164;&#28304;&#28040;&#32791;&#26377;&#38480;&#12290;&#30001;&#20110;&#32570;&#20047;&#22823;&#35268;&#27169;&#30340;4K&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29305;&#21035;&#26159;&#38654;&#38718;&#21435;&#38500;&#26041;&#38754;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#38459;&#30861;&#20102;&#35813;&#39046;&#22495;&#30340;&#36827;&#27493;&#12290;&#26500;&#24314;&#36229;&#39640;&#28165;&#65288;UHD&#65289;&#21435;&#38654;&#25968;&#25454;&#38598;&#30340;&#25361;&#25112;&#21253;&#25324;&#32570;&#20047;&#20272;&#31639;UHD&#28145;&#24230;&#22270;&#30340;&#26041;&#27861;&#12289;&#39640;&#36136;&#37327;&#30340;4K&#28145;&#24230;&#20272;&#31639;&#25968;&#25454;&#38598;&#20197;&#21450;&#23558;UHD&#38654;&#24433;&#20687;&#20174;&#21512;&#25104;&#22495;&#36801;&#31227;&#21040;&#23454;&#22495;&#30340;&#36801;&#31227;&#31574;&#30053;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#21512;&#25104;&#26041;&#27861;&#26469;&#27169;&#25311;4K&#38654;&#38718;&#22270;&#20687;&#65288;&#21253;&#25324;&#22812;&#38388;&#21644;&#30333;&#22825;&#22330;&#26223;&#65289;&#20174;&#28165;&#26224;&#22270;&#20687;&#20013;&#65292;&#39318;&#20808;&#20272;&#31639;&#22330;&#26223;&#28145;&#24230;&#65292;&#27169;&#25311;&#20809;&#32447;&#21644;&#29289;&#20307;&#21453;&#23556;&#65292;&#28982;&#21518;&#20351;&#29992;GAN&#23558;&#21512;&#25104;&#22270;&#20687;&#36801;&#31227;&#21040;&#23454;&#22495;&#65292;&#26368;&#32456;&#21576;&#29616;&#22312;4K&#20998;&#36776;&#29575;&#22270;&#20687;&#19978;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#21512;&#25104;&#30340;&#22270;&#20687;&#21046;&#20316;&#25104;&#20102;&#19968;&#20010;&#31216;&#20026;4K-HAZE&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;CS-Mixer&#65288;&#19968;&#31181;&#38598;&#25104;&#20102;&#22522;&#20110;CNN&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;SFC&#23618;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#27169;&#22411;&#65289;&#26469;&#35299;&#20915;4K&#21435;&#38654;&#20219;&#21153;&#65292;&#24182;&#22312;&#27492;&#25968;&#25454;&#38598;&#19978;&#19982;&#29616;&#26377;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35270;&#35273;&#36136;&#37327;&#21644;&#23458;&#35266;&#35780;&#20272;&#25351;&#26631;&#26041;&#38754;&#37117;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Currently, mobile and IoT devices are in dire need of a series of methods to enhance 4K images with limited resource expenditure. The absence of large-scale 4K benchmark datasets hampers progress in this area, especially for dehazing. The challenges in building ultra-high-definition (UHD) dehazing datasets are the absence of estimation methods for UHD depth maps, high-quality 4K depth estimation datasets, and migration strategies for UHD haze images from synthetic to real domains. To address these problems, we develop a novel synthetic method to simulate 4K hazy images (including nighttime and daytime scenes) from clear images, which first estimates the scene depth, simulates the light rays and object reflectance, then migrates the synthetic images to real domains by using a GAN, and finally yields the hazy effects on 4K resolution images. We wrap these synthesized images into a benchmark called the 4K-HAZE dataset. Specifically, we design the CS-Mixer (an MLP-based model that integrat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21021;&#32423;&#20445;&#20581;&#21307;&#24072;&#30340;&#24739;&#32773;&#21307;&#30103;&#31508;&#35760;&#36827;&#34892;&#32954;&#30284;&#26089;&#26399;&#39044;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#38024;&#23545;&#39640;&#24230;&#19981;&#24179;&#34913;&#20998;&#31867;&#38382;&#39064;&#30340;&#36719;&#25552;&#31034;&#35843;&#25972;&#21644;&#38745;&#24577;&#35789;&#23884;&#20837;&#27169;&#22411;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.15846</link><description>&lt;p&gt;
&#21033;&#29992;&#21021;&#32423;&#20445;&#20581;&#21307;&#24072;&#30340;&#33655;&#20848;&#21307;&#30103;&#31508;&#35760;&#39044;&#27979;&#32954;&#30284;&#30340;&#36719;&#25552;&#31034;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Soft-prompt tuning to predict lung cancer using primary care free-text Dutch medical notes. (arXiv:2303.15846v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21021;&#32423;&#20445;&#20581;&#21307;&#24072;&#30340;&#24739;&#32773;&#21307;&#30103;&#31508;&#35760;&#36827;&#34892;&#32954;&#30284;&#26089;&#26399;&#39044;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#38024;&#23545;&#39640;&#24230;&#19981;&#24179;&#34913;&#20998;&#31867;&#38382;&#39064;&#30340;&#36719;&#25552;&#31034;&#35843;&#25972;&#21644;&#38745;&#24577;&#35789;&#23884;&#20837;&#27169;&#22411;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#19978;&#19979;&#25991;&#35789;&#34920;&#31034;&#30340;&#19981;&#21516;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#33655;&#20848;&#21021;&#32423;&#20445;&#20581;&#21307;&#24072;&#30340;&#24739;&#32773;&#21307;&#30103;&#31508;&#35760;&#26089;&#26399;&#39044;&#27979;&#32954;&#30284;&#30340;&#38382;&#39064;&#12290;&#22240;&#20026;&#32954;&#30284;&#22312;&#21021;&#32423;&#20445;&#20581;&#20013;&#30340;&#24739;&#30149;&#29575;&#36739;&#20302;&#65292;&#25152;&#20197;&#25105;&#20204;&#36824;&#35299;&#20915;&#20102;&#22312;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#31867;&#21035;&#19979;&#36827;&#34892;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#24182;&#30740;&#31350;&#65306;1&#65289;&#22914;&#20309;&#23558;\textit {&#36719;&#25552;&#31034;&#35843;&#25972;} - &#19968;&#31181;&#20351;&#29992;&#23567;&#37327;&#35757;&#32451;&#25968;&#25454;&#35843;&#25972;PLMs&#30340;NLP&#25216;&#26415; - &#19982;&#26631;&#20934;&#27169;&#22411;&#24494;&#35843;&#36827;&#34892;&#27604;&#36739;&#65307; 2&#65289;&#22312;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#35774;&#32622;&#20013;&#65292;&#26159;&#21542;&#31616;&#21333;&#30340;&#38745;&#24577;&#35789;&#23884;&#20837;&#27169;&#22411;&#65288;WEMs&#65289;&#21487;&#20197;&#27604;PLMs&#26356;&#20581;&#22766;&#65307;&#20197;&#21450;3&#65289;&#24403;&#35757;&#32451;&#31508;&#35760;&#26469;&#33258;&#23569;&#37327;&#24739;&#32773;&#26102;&#65292;&#27169;&#22411;&#30340;&#34920;&#29616;&#22914;&#20309;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;1&#65289;&#36719;&#25552;&#31034;&#35843;&#25972;&#26159;&#26631;&#20934;&#27169;&#22411;&#24494;&#35843;&#30340;&#26377;&#25928;&#26367;&#20195;&#26041;&#26696;&#65307; 2&#65289;PLMs&#27604;&#36739;&#31616;&#21333;&#30340;&#38745;&#24577;&#35789;&#23884;&#20837;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#21306;&#20998;&#33021;&#21147;&#20294;&#26356;&#24046;&#30340;&#26657;&#20934;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate different natural language processing (NLP) approaches based on contextualised word representations for the problem of early prediction of lung cancer using free-text patient medical notes of Dutch primary care physicians. Because lung cancer has a low prevalence in primary care, we also address the problem of classification under highly imbalanced classes. Specifically, we use large Transformer-based pretrained language models (PLMs) and investigate: 1) how \textit{soft prompt-tuning} -- an NLP technique used to adapt PLMs using small amounts of training data -- compares to standard model fine-tuning; 2) whether simpler static word embedding models (WEMs) can be more robust compared to PLMs in highly imbalanced settings; and 3) how models fare when trained on notes from a small number of patients. We find that 1) soft-prompt tuning is an efficient alternative to standard model fine-tuning; 2) PLMs show better discrimination but worse calibration compared to simpler stat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#20351;&#29992;&#36827;&#21270;&#26041;&#27861;&#29983;&#25104;&#21453;&#20107;&#23454;&#24207;&#21015;&#24182;&#36991;&#20813;&#23545;&#39046;&#22495;&#30693;&#35782;&#30340;&#38656;&#27714;&#20197;&#25552;&#39640;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.15844</link><description>&lt;p&gt;
&#29983;&#25104;&#21512;&#29702;&#30340;&#23545;&#31574;&#24207;&#21015;&#29992;&#20110;&#39044;&#27979;&#24615;&#27969;&#31243;&#20998;&#26512;&#65306;CREATED
&lt;/p&gt;
&lt;p&gt;
CREATED: Generating Viable Counterfactual Sequences for Predictive Process Analytics. (arXiv:2303.15844v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#20351;&#29992;&#36827;&#21270;&#26041;&#27861;&#29983;&#25104;&#21453;&#20107;&#23454;&#24207;&#21015;&#24182;&#36991;&#20813;&#23545;&#39046;&#22495;&#30693;&#35782;&#30340;&#38656;&#27714;&#20197;&#25552;&#39640;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#24615;&#27969;&#31243;&#20998;&#26512;&#20851;&#27880;&#39044;&#27979;&#26410;&#26469;&#29366;&#24577;&#65292;&#20363;&#22914;&#36816;&#34892;&#27969;&#31243;&#23454;&#20363;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#25216;&#26415;&#36890;&#24120;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#22914;LSTM&#65289;&#36827;&#34892;&#36825;&#26679;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#28145;&#24230;&#27169;&#22411;&#22797;&#26434;&#32780;&#19988;&#38590;&#20197;&#29702;&#35299;&#12290;&#21453;&#20107;&#23454;&#33021;&#22238;&#31572;&#8220;&#22914;&#26524;&#8230;&#8230;&#20250;&#24590;&#26679;&#8221;&#30340;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#26377;&#21161;&#20110;&#29702;&#35299;&#39044;&#27979;&#32972;&#21518;&#30340;&#25512;&#29702;&#12290;&#24403;&#21069;&#29992;&#20110;&#29983;&#25104;&#21453;&#20107;&#23454;&#24207;&#21015;&#30340;&#26041;&#27861;&#19981;&#32771;&#34385;&#27969;&#31243;&#34892;&#20026;&#65292;&#20174;&#32780;&#23548;&#33268;&#29983;&#25104;&#26080;&#25928;&#25110;&#19981;&#21487;&#34892;&#30340;&#21453;&#20107;&#23454;&#27969;&#31243;&#23454;&#20363;&#65292;&#25110;&#20005;&#37325;&#20381;&#36182;&#20110;&#39046;&#22495;&#30693;&#35782;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#36827;&#21270;&#26041;&#27861;&#29983;&#25104;&#21453;&#20107;&#23454;&#24207;&#21015;&#65292;&#24182;&#36991;&#20813;&#20102;&#23545;&#39046;&#22495;&#30693;&#35782;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#35758;&#35757;&#32451;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#26469;&#35745;&#31639;&#21453;&#20107;&#23454;&#24207;&#21015;&#30340;&#27010;&#29575;&#65292;&#36825;&#23558;&#22312;&#29983;&#25104;&#21453;&#20107;&#23454;&#24207;&#21015;&#26102;&#20445;&#35777;&#20854;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictive process analytics focuses on predicting future states, such as the outcome of running process instances. These techniques often use machine learning models or deep learning models (such as LSTM) to make such predictions. However, these deep models are complex and difficult for users to understand. Counterfactuals answer ``what-if'' questions, which are used to understand the reasoning behind the predictions. For example, what if instead of emailing customers, customers are being called? Would this alternative lead to a different outcome? Current methods to generate counterfactual sequences either do not take the process behavior into account, leading to generating invalid or infeasible counterfactual process instances, or heavily rely on domain knowledge. In this work, we propose a general framework that uses evolutionary methods to generate counterfactual sequences. Our framework does not require domain knowledge. Instead, we propose to train a Markov model to compute the f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36328;&#32452;&#32455;&#30340;&#20225;&#19994;&#32593;&#32476;&#20013;&#23454;&#29616;&#20840;&#38754;&#30340;&#25968;&#25454;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#22312;&#25968;&#25454;&#20998;&#24067;&#22312;&#22810;&#20010;&#27861;&#24459;&#23454;&#20307;&#20043;&#38388;&#26102;&#65292;&#25259;&#38706;&#25935;&#24863;&#20449;&#24687;&#21644;&#38656;&#35201;&#20132;&#25442;&#22823;&#37327;&#25968;&#25454;&#30340;&#38590;&#39064;&#65292;&#24182;&#26174;&#31034;&#20102;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.15834</link><description>&lt;p&gt;
&#36890;&#36807;&#20803;&#26426;&#22120;&#23398;&#20064;&#22312;&#20225;&#19994;&#32593;&#32476;&#20013;&#23454;&#29616;&#36328;&#32452;&#32455;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Enabling Inter-organizational Analytics in Business Networks Through Meta Machine Learning. (arXiv:2303.15834v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15834
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36328;&#32452;&#32455;&#30340;&#20225;&#19994;&#32593;&#32476;&#20013;&#23454;&#29616;&#20840;&#38754;&#30340;&#25968;&#25454;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#22312;&#25968;&#25454;&#20998;&#24067;&#22312;&#22810;&#20010;&#27861;&#24459;&#23454;&#20307;&#20043;&#38388;&#26102;&#65292;&#25259;&#38706;&#25935;&#24863;&#20449;&#24687;&#21644;&#38656;&#35201;&#20132;&#25442;&#22823;&#37327;&#25968;&#25454;&#30340;&#38590;&#39064;&#65292;&#24182;&#26174;&#31034;&#20102;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21151;&#30340;&#20998;&#26512;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#20381;&#36182;&#20110;&#36830;&#25509;&#21508;&#31181;&#25968;&#25454;&#28304;&#12290;&#34429;&#28982;&#22312;&#32452;&#32455;&#20869;&#37096;&#29983;&#25104;&#26356;&#22823;&#30340;&#25968;&#25454;&#27744;&#36890;&#24120;&#26159;&#21487;&#34892;&#30340;&#65292;&#20294;&#22312;&#65288;&#36328;&#32452;&#32455;&#30340;&#65289;&#20225;&#19994;&#32593;&#32476;&#20013;&#24212;&#29992;&#20998;&#26512;&#20173;&#28982;&#21463;&#21040;&#20005;&#37325;&#30340;&#21046;&#32422;&#12290;&#30001;&#20110;&#25968;&#25454;&#20998;&#24067;&#22312;&#22810;&#20010;&#27861;&#24459;&#23454;&#20307;&#20043;&#38388;&#65292;&#29978;&#33267;&#21487;&#33021;&#36328;&#36234;&#22810;&#20010;&#22269;&#23478;&#65292;&#22240;&#27492;&#25259;&#38706;&#25935;&#24863;&#20449;&#24687;&#30340;&#25285;&#24551;&#20197;&#21450;&#38656;&#35201;&#20132;&#25442;&#30340;&#25968;&#25454;&#37327;&#26159;&#21019;&#24314;&#26377;&#25928;&#30340;&#31995;&#32479;&#33539;&#22260;&#35299;&#20915;&#26041;&#26696;&#30340;&#20851;&#38190;&#38556;&#30861;&#65292;&#21516;&#26102;&#20173;&#28982;&#23454;&#29616;&#21331;&#36234;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38556;&#30861;&#65292;&#20174;&#32780;&#22312;&#20225;&#19994;&#32593;&#32476;&#20013;&#23454;&#29616;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#37319;&#29992;&#35774;&#35745;&#31185;&#23398;&#30740;&#31350;&#26041;&#27861;&#65292;&#24182;&#22312;&#19968;&#20010;&#24037;&#19994;&#29992;&#20363;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25191;&#34892;&#32593;&#32476;&#24191;&#27867;&#20998;&#26512;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Successful analytics solutions that provide valuable insights often hinge on the connection of various data sources. While it is often feasible to generate larger data pools within organizations, the application of analytics within (inter-organizational) business networks is still severely constrained. As data is distributed across several legal units, potentially even across countries, the fear of disclosing sensitive information as well as the sheer volume of the data that would need to be exchanged are key inhibitors for the creation of effective system-wide solutions -- all while still reaching superior prediction performance. In this work, we propose a meta machine learning method that deals with these obstacles to enable comprehensive analyses within a business network. We follow a design science research approach and evaluate our method with respect to feasibility and performance in an industrial use case. First, we show that it is feasible to perform network-wide analyses that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#23454;&#39564;&#27969;&#20307;&#21147;&#23398;&#20013;&#30340;&#19977;&#20010;&#26041;&#38754;&#30340;&#24212;&#29992;&#28508;&#21147;&#65306;&#25552;&#39640;&#27979;&#37327;&#25216;&#26415;&#31934;&#24230;&#65292;&#25913;&#36827;&#23454;&#39564;&#35774;&#35745;&#21644;&#23454;&#29616;&#23454;&#26102;&#20272;&#35745;&#21644;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.15832</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#27969;&#20307;&#21147;&#23398;&#23454;&#39564;&#20013;&#30340;&#21464;&#38761;&#24615;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
The transformative potential of machine learning for experiments in fluid mechanics. (arXiv:2303.15832v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#23454;&#39564;&#27969;&#20307;&#21147;&#23398;&#20013;&#30340;&#19977;&#20010;&#26041;&#38754;&#30340;&#24212;&#29992;&#28508;&#21147;&#65306;&#25552;&#39640;&#27979;&#37327;&#25216;&#26415;&#31934;&#24230;&#65292;&#25913;&#36827;&#23454;&#39564;&#35774;&#35745;&#21644;&#23454;&#29616;&#23454;&#26102;&#20272;&#35745;&#21644;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#22312;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#20854;&#20013;&#21253;&#25324;&#23454;&#39564;&#27969;&#20307;&#21147;&#23398;&#65292;&#36825;&#26159;&#26368;&#21021;&#30340;&#22823;&#25968;&#25454;&#23398;&#31185;&#20043;&#19968;&#12290;&#26412;&#25991;&#23558;&#37325;&#28857;&#20171;&#32461;&#23454;&#39564;&#27969;&#20307;&#21147;&#23398;&#20013;&#20960;&#20010;&#21463;&#30410;&#20110;&#26426;&#22120;&#23398;&#20064;&#36827;&#23637;&#30340;&#26041;&#38754;&#65292;&#21253;&#25324;&#65306;1&#65289;&#22686;&#24378;&#27979;&#37327;&#25216;&#26415;&#30340;&#31934;&#24230;&#21644;&#36136;&#37327;&#65292;2&#65289;&#25913;&#36827;&#23454;&#39564;&#35774;&#35745;&#21644;&#26367;&#20195;&#25968;&#20540;&#23402;&#29983;&#27169;&#22411;&#21644;3&#65289;&#23454;&#29616;&#23454;&#26102;&#20272;&#35745;&#21644;&#25511;&#21046;&#12290;&#23545;&#20110;&#27599;&#20010;&#26041;&#38754;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26368;&#36817;&#30340;&#25104;&#21151;&#26696;&#20363;&#21644;&#27491;&#22312;&#36827;&#34892;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#27880;&#24847;&#20107;&#39033;&#21644;&#38480;&#21046;&#65292;&#24182;&#27010;&#36848;&#20102;ML&#22686;&#24378;&#21644;ML&#33021;&#21147;&#30340;&#23454;&#39564;&#27969;&#20307;&#21147;&#23398;&#30340;&#26032;&#36884;&#24452;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of machine learning has rapidly advanced the state of the art in many fields of science and engineering, including experimental fluid dynamics, which is one of the original big-data disciplines. This perspective will highlight several aspects of experimental fluid mechanics that stand to benefit from progress advances in machine learning, including: 1) augmenting the fidelity and quality of measurement techniques, 2) improving experimental design and surrogate digital-twin models and 3) enabling real-time estimation and control. In each case, we discuss recent success stories and ongoing challenges, along with caveats and limitations, and outline the potential for new avenues of ML-augmented and ML-enabled experimental fluid mechanics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#27169;&#24577;&#22270;&#20687;&#20998;&#21106;&#30340;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#23610;&#24230;&#33258;&#25105;&#38598;&#25104;&#21644;&#23545;&#25239;&#26080;&#37197;&#23545;&#32763;&#35793;&#65292;&#23454;&#29616;&#20102;&#23545;&#21548;&#31070;&#32463;&#30244;&#21644;&#32819;&#34583;&#30340;&#33258;&#21160;&#20998;&#21106;&#65292;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.15826</link><description>&lt;p&gt;
MS-MT: &#24102;&#26377;&#23545;&#25239;&#26080;&#37197;&#23545;&#32763;&#35793;&#30340;&#22810;&#23610;&#24230;&#22343;&#20540;&#25945;&#24072;&#29992;&#20110;&#36328;&#27169;&#24577;&#21548;&#31070;&#32463;&#30244;&#21644;&#32819;&#34583;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
MS-MT: Multi-Scale Mean Teacher with Contrastive Unpaired Translation for Cross-Modality Vestibular Schwannoma and Cochlea Segmentation. (arXiv:2303.15826v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#27169;&#24577;&#22270;&#20687;&#20998;&#21106;&#30340;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#23610;&#24230;&#33258;&#25105;&#38598;&#25104;&#21644;&#23545;&#25239;&#26080;&#37197;&#23545;&#32763;&#35793;&#65292;&#23454;&#29616;&#20102;&#23545;&#21548;&#31070;&#32463;&#30244;&#21644;&#32819;&#34583;&#30340;&#33258;&#21160;&#20998;&#21106;&#65292;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#23384;&#22312;&#30340;&#22495;&#28418;&#31227;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23610;&#24230;&#33258;&#25105;&#38598;&#25104;&#30340;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#20998;&#36776;&#29575;T2&#22270;&#20687;&#19978;&#30340;&#20004;&#20010;&#20851;&#38190;&#33041;&#32467;&#26500;&#65288;&#21363;&#21548;&#31070;&#32463;&#30244;&#21644;&#32819;&#34583;&#65289;&#30340;&#33258;&#21160;&#20998;&#21106;&#12290;&#36890;&#36807;&#35774;&#35745;&#20998;&#21106;&#22686;&#24378;&#30340;&#23545;&#25239;&#26080;&#37197;&#23545;&#22270;&#20687;&#32763;&#35793;&#27169;&#22359;&#65292;&#23545;&#28304;T1&#21040;&#30446;&#26631;T2&#30340;&#22270;&#20687;&#32423;&#22495;&#33258;&#36866;&#24212;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;&#25509;&#30528;&#65292;&#24341;&#20837;&#20102;&#22810;&#23610;&#24230;&#28145;&#24230;&#30417;&#30563;&#21644;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26469;&#36827;&#34892;&#22343;&#20540;&#25945;&#24072;&#32593;&#32476;&#30340;&#33258;&#25105;&#38598;&#25104;&#23398;&#20064;&#65292;&#20197;&#36827;&#19968;&#27493;&#32553;&#23567;&#22495;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#37319;&#29992;&#33258;&#35757;&#32451;&#21644;&#24378;&#24230;&#22686;&#24378;&#25216;&#26415;&#26469;&#32531;&#35299;&#26631;&#31614;&#31232;&#32570;&#24615;&#24182;&#22686;&#24378;&#36328;&#27169;&#24577;&#20998;&#21106;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36328;&#27169;&#24577;&#21548;&#31070;&#32463;&#30244;&#21644;&#32819;&#34583;&#20998;&#21106;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain shift has been a long-standing issue for medical image segmentation. Recently, unsupervised domain adaptation (UDA) methods have achieved promising cross-modality segmentation performance by distilling knowledge from a label-rich source domain to a target domain without labels. In this work, we propose a multi-scale self-ensembling based UDA framework for automatic segmentation of two key brain structures i.e., Vestibular Schwannoma (VS) and Cochlea on high-resolution T2 images. First, a segmentation-enhanced contrastive unpaired image translation module is designed for image-level domain adaptation from source T1 to target T2. Next, multi-scale deep supervision and consistency regularization are introduced to a mean teacher network for self-ensemble learning to further close the domain gap. Furthermore, self-training and intensity augmentation techniques are utilized to mitigate label scarcity and boost cross-modality segmentation performance. Our method demonstrates promising 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#22312;&#36866;&#37197;&#22120;&#24494;&#35843;&#20013;&#22266;&#23450;&#25152;&#26377;&#39044;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#24182;&#25554;&#20837;&#21442;&#25968;&#39640;&#25928;&#32467;&#26500;&#36866;&#37197;&#22120;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#32531;&#35299;&#22810;&#35821;&#35328;&#27169;&#22411;&#20013;&#36951;&#24536;&#38382;&#39064;&#12290;&#22312;&#20195;&#30721;&#25628;&#32034;&#21644;&#25688;&#35201;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#36824;&#23637;&#31034;&#20102;&#36866;&#37197;&#22120;&#22312;&#36328;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.15822</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;&#25152;&#26377;&#32534;&#31243;&#35821;&#35328;&#30340;&#36866;&#37197;&#22120;&#21527;&#65311;&#29992;&#20110;&#20195;&#30721;&#25628;&#32034;&#21644;&#25688;&#35201;&#30340;&#36866;&#37197;&#22120;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
One Adapter for All Programming Languages? Adapter Tuning for Code Search and Summarization. (arXiv:2303.15822v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#22312;&#36866;&#37197;&#22120;&#24494;&#35843;&#20013;&#22266;&#23450;&#25152;&#26377;&#39044;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#24182;&#25554;&#20837;&#21442;&#25968;&#39640;&#25928;&#32467;&#26500;&#36866;&#37197;&#22120;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#32531;&#35299;&#22810;&#35821;&#35328;&#27169;&#22411;&#20013;&#36951;&#24536;&#38382;&#39064;&#12290;&#22312;&#20195;&#30721;&#25628;&#32034;&#21644;&#25688;&#35201;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#36824;&#23637;&#31034;&#20102;&#36866;&#37197;&#22120;&#22312;&#36328;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#35768;&#22810;&#20195;&#30721;&#26234;&#33021;&#20219;&#21153;&#30340;&#33258;&#21160;&#21270;&#65292;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#33539;&#24335;&#26159;&#22312;&#27599;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#25253;&#21578;&#31216;&#65292;&#22810;&#35821;&#35328;&#24494;&#35843;&#26377;&#30410;&#20110;&#21508;&#31181;&#20219;&#21153;&#21644;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#22810;&#35821;&#35328;&#24494;&#35843;&#20250;&#23548;&#33268;UniXcoder&#21644;CodeT5&#26368;&#36817;&#27169;&#22411;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#32531;&#35299;&#22810;&#35821;&#35328;&#27169;&#22411;&#20013;&#21487;&#33021;&#21457;&#29983;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#25105;&#20204;&#22266;&#23450;&#25152;&#26377;&#39044;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#65292;&#25554;&#20837;&#21442;&#25968;&#39640;&#25928;&#32467;&#26500;&#36866;&#37197;&#22120;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#12290;&#19982;&#20026;&#27599;&#31181;&#32534;&#31243;&#35821;&#35328;&#36827;&#34892;&#23436;&#20840;&#27169;&#22411;&#24494;&#35843;&#30456;&#27604;&#65292;&#36866;&#37197;&#22120;&#24494;&#35843;&#20165;&#26356;&#26032;&#20102;&#25972;&#20307;&#21442;&#25968;&#30340;0.6&#65285;&#65292;&#22312;&#20195;&#30721;&#25628;&#32034;&#21644;&#25688;&#35201;&#20219;&#21153;&#19978;&#20135;&#29983;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#23637;&#31034;&#20102;&#36866;&#37197;&#22120;&#22312;&#36328;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#27599;&#31181;&#32534;&#31243;&#35821;&#35328;&#20351;&#29992;200&#20010;&#26679;&#26412;&#36827;&#34892;&#30340;&#22810;&#35821;&#35328;&#24494;&#35843;&#25509;&#36817;&#20110;&#24494;&#35843;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
As pre-trained models automate many code intelligence tasks, a widely used paradigm is to fine-tune a model on the task dataset for each programming language. A recent study reported that multilingual fine-tuning benefits a range of tasks and models. However, we find that multilingual fine-tuning leads to performance degradation on recent models UniXcoder and CodeT5.  To alleviate the potentially catastrophic forgetting issue in multilingual models, we fix all pre-trained model parameters, insert the parameter-efficient structure adapter, and fine-tune it. Updating only 0.6\% of the overall parameters compared to full-model fine-tuning for each programming language, adapter tuning yields consistent improvements on code search and summarization tasks, achieving state-of-the-art results. In addition, we experimentally show its effectiveness in cross-lingual and low-resource scenarios. Multilingual fine-tuning with 200 samples per programming language approaches the results fine-tuned wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32441;&#29702;3D&#32593;&#26684;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#25216;&#26415;&#65292;&#21487;&#23545;&#21830;&#19994;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#35780;&#20272;&#65292;&#24182;&#20855;&#26377;&#36867;&#36991;&#38450;&#24481;&#26426;&#21046;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.15818</link><description>&lt;p&gt;
&#23454;&#29616;&#29289;&#29702;&#20154;&#33080;&#35782;&#21035;&#20013;&#30340;&#26377;&#25928;&#23545;&#25239;&#32441;&#29702;3D&#32593;&#26684;
&lt;/p&gt;
&lt;p&gt;
Towards Effective Adversarial Textured 3D Meshes on Physical Face Recognition. (arXiv:2303.15818v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32441;&#29702;3D&#32593;&#26684;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#25216;&#26415;&#65292;&#21487;&#23545;&#21830;&#19994;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#35780;&#20272;&#65292;&#24182;&#20855;&#26377;&#36867;&#36991;&#38450;&#24481;&#26426;&#21046;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#35782;&#21035;&#20316;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#29983;&#29289;&#35782;&#21035;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#37117;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#20316;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#29289;&#29702;&#23545;&#25239;&#25915;&#20987;&#21487;&#20197;&#35782;&#21035;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#30340;&#24369;&#28857;&#65292;&#24182;&#22312;&#37096;&#32626;&#20043;&#21069;&#35780;&#20272;&#20854;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#29289;&#29702;&#25915;&#20987;&#26041;&#27861;&#35201;&#20040;&#24456;&#23481;&#26131;&#34987;&#26816;&#27979;&#20986;&#26469;&#65292;&#35201;&#20040;&#23545;&#21830;&#19994;&#35782;&#21035;&#31995;&#32479;&#26080;&#25928;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#31181;&#26356;&#21487;&#38752;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#23545;&#21830;&#19994;&#31995;&#32479;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#35780;&#20272;&#12290;&#35201;&#27714;&#35813;&#25216;&#26415;&#33021;&#22815;&#21516;&#26102;&#27450;&#39575;&#40657;&#30418;&#35782;&#21035;&#27169;&#22411;&#24182;&#36867;&#36991;&#38450;&#24481;&#26426;&#21046;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20855;&#26377;&#31934;&#24515;&#25299;&#25169;&#32467;&#26500;&#30340;&#23545;&#25239;&#24615;&#32441;&#29702;3D&#32593;&#26684;&#65288;AT3D&#65289;&#65292;&#21487;&#22312;&#20154;&#33080;&#19978;3D&#25171;&#21360;&#24182;&#31896;&#36148;&#20197;&#36867;&#36991;&#38450;&#24481;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#32593;&#26684;&#30340;&#20248;&#21270;&#26041;&#26696;&#38656;&#35201;&#22312;&#39640;&#32500;&#32593;&#26684;&#31354;&#38388;&#20013;&#35745;&#31639;&#26799;&#24230;&#65292;&#24182;&#21487;&#33021;&#38519;&#20837;&#19981;&#21487;&#25509;&#21463;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face recognition is a prevailing authentication solution in numerous biometric applications. Physical adversarial attacks, as an important surrogate, can identify the weaknesses of face recognition systems and evaluate their robustness before deployed. However, most existing physical attacks are either detectable readily or ineffective against commercial recognition systems. The goal of this work is to develop a more reliable technique that can carry out an end-to-end evaluation of adversarial robustness for commercial systems. It requires that this technique can simultaneously deceive black-box recognition models and evade defensive mechanisms. To fulfill this, we design adversarial textured 3D meshes (AT3D) with an elaborate topology on a human face, which can be 3D-printed and pasted on the attacker's face to evade the defenses. However, the mesh-based optimization regime calculates gradients in high-dimensional mesh space, and can be trapped into local optima with unsatisfactory tr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;IVR-Q&#65292;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#20854;&#36890;&#36807;&#38544;&#24335;&#20215;&#20540;&#27491;&#21017;&#21270;&#30340;&#26041;&#24335;&#36991;&#20813;&#20102;OOD&#21160;&#20316;&#24102;&#26469;&#30340;&#20215;&#20540;&#20989;&#25968;&#20559;&#31227;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;IVR loss&#26469;&#25913;&#21892;&#31574;&#30053;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#24182;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.15810</link><description>&lt;p&gt;
&#27809;&#26377;OOD&#21160;&#20316;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65306;&#22522;&#20110;&#38544;&#24335;&#20215;&#20540;&#27491;&#21017;&#21270;&#30340;&#26679;&#26412;&#20869;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Offline RL with No OOD Actions: In-Sample Learning via Implicit Value Regularization. (arXiv:2303.15810v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;IVR-Q&#65292;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#20854;&#36890;&#36807;&#38544;&#24335;&#20215;&#20540;&#27491;&#21017;&#21270;&#30340;&#26041;&#24335;&#36991;&#20813;&#20102;OOD&#21160;&#20316;&#24102;&#26469;&#30340;&#20215;&#20540;&#20989;&#25968;&#20559;&#31227;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;IVR loss&#26469;&#25913;&#21892;&#31574;&#30053;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#24182;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064; (RL) &#26041;&#27861;&#38754;&#20020;&#19968;&#20010;&#25240;&#34935;&#38382;&#39064;&#65306;&#25913;&#21892;&#31574;&#30053;&#20197;&#36229;&#36234;&#34892;&#20026;&#31574;&#30053;&#19982;&#38480;&#21046;&#31574;&#30053;&#20197;&#38480;&#21046;&#19982;&#34892;&#20026;&#31574;&#30053;&#30340;&#20559;&#24046;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#30001;&#20110;&#20351;&#29992;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260; (OOD) &#30340;&#21160;&#20316;&#35745;&#31639; Q &#20540;&#20250;&#21463;&#21040;&#20998;&#24067;&#20559;&#31227;&#38169;&#35823;&#30340;&#24433;&#21709;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#22522;&#20110;&#26679;&#26412;&#20869;&#23398;&#20064;&#33539;&#24335;&#65288;IQL&#65289;&#36890;&#36807;&#23545;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#20998;&#20301;&#22238;&#24402;&#26469;&#25913;&#21892;&#31574;&#30053;&#65292;&#34920;&#29616;&#20986;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#32780;&#19981;&#26597;&#35810;&#20219;&#20309;&#26410;&#35265;&#21160;&#20316;&#30340;&#20540;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#31181;&#26041;&#27861;&#22914;&#20309;&#22788;&#29702;&#23398;&#20064;&#20215;&#20540;&#20989;&#25968;&#26102;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#26679;&#26412;&#20869;&#23398;&#20064;&#33539;&#20363;&#22312;&#38544;&#24335;&#20215;&#20540;&#27491;&#21017;&#21270; (IVR) &#26694;&#26550;&#19979;&#24471;&#20197;&#20135;&#29983;&#12290;&#36825;&#32473;&#20102;&#25105;&#20204;&#26356;&#28145;&#21051;&#30340;&#29702;&#35299;&#20026;&#20160;&#20040;&#26679;&#26412;&#20869;&#23398;&#20064;&#33539;&#20363;&#26377;&#25928;&#65292;&#21363;&#23427;&#23558;&#38544;&#24335;&#20215;&#20540;&#27491;&#21017;&#21270;&#24212;&#29992;&#20110;&#31574;&#30053;&#12290;&#22522;&#20110;&#36825;&#20010;&#27934;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#31639;&#27861;&#65292;IVR-Q&#65292;&#23427;&#36890;&#36807;&#35268;&#33539;&#21270;&#31574;&#30053;&#30340;&#20215;&#20540;&#20989;&#25968;&#20197;&#36991;&#20813;OOD&#21160;&#20316;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;IVR loss&#26469;&#25913;&#21892;&#31574;&#30053;&#12290;&#22810;&#20010;&#22522;&#20934;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;IVR-Q&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#22312;&#31163;&#32447;RL&#20013;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most offline reinforcement learning (RL) methods suffer from the trade-off between improving the policy to surpass the behavior policy and constraining the policy to limit the deviation from the behavior policy as computing $Q$-values using out-of-distribution (OOD) actions will suffer from errors due to distributional shift. The recently proposed \textit{In-sample Learning} paradigm (i.e., IQL), which improves the policy by quantile regression using only data samples, shows great promise because it learns an optimal policy without querying the value function of any unseen actions. However, it remains unclear how this type of method handles the distributional shift in learning the value function. In this work, we make a key finding that the in-sample learning paradigm arises under the \textit{Implicit Value Regularization} (IVR) framework. This gives a deeper understanding of why the in-sample learning paradigm works, i.e., it applies implicit value regularization to the policy. Based 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#12300;&#29983;&#24577;&#22270;&#34920;&#12301;&#30340;&#25991;&#26723;&#26694;&#26550;&#65292;&#21487;&#20197;&#36879;&#26126;&#22320;&#38598;&#20013;&#22522;&#30784;&#27169;&#22411;&#30340;&#31038;&#20250;&#24433;&#21709;&#26041;&#38754;&#30340;&#30693;&#35782;&#65292;&#36825;&#21487;&#20197;&#25552;&#39640;&#20854;&#36879;&#26126;&#24230;&#21644;&#38382;&#36131;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.15772</link><description>&lt;p&gt;
&#29983;&#24577;&#22270;&#34920;&#65306;&#22522;&#30784;&#27169;&#22411;&#30340;&#31038;&#20250;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Ecosystem Graphs: The Social Footprint of Foundation Models. (arXiv:2303.15772v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15772
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#12300;&#29983;&#24577;&#22270;&#34920;&#12301;&#30340;&#25991;&#26723;&#26694;&#26550;&#65292;&#21487;&#20197;&#36879;&#26126;&#22320;&#38598;&#20013;&#22522;&#30784;&#27169;&#22411;&#30340;&#31038;&#20250;&#24433;&#21709;&#26041;&#38754;&#30340;&#30693;&#35782;&#65292;&#36825;&#21487;&#20197;&#25552;&#39640;&#20854;&#36879;&#26126;&#24230;&#21644;&#38382;&#36131;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65288;&#20363;&#22914; ChatGPT&#12289;StableDiffusion&#65289;&#24191;&#27867;&#24433;&#21709;&#31038;&#20250;&#65292;&#22240;&#27492;&#38656;&#35201;&#31038;&#20250;&#30340;&#20851;&#27880;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#26412;&#36523;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#20294;&#20026;&#20102;&#20934;&#30830;&#22320;&#25551;&#36848;&#23427;&#20204;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#24517;&#39035;&#32771;&#34385;&#26356;&#24191;&#27867;&#30340;&#31038;&#20250;&#25216;&#26415;&#29983;&#24577;&#31995;&#32479;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#12300;&#29983;&#24577;&#22270;&#34920;&#12301;&#30340;&#25991;&#26723;&#26694;&#26550;&#65292;&#20197;&#36879;&#26126;&#22320;&#38598;&#20013;&#36825;&#20010;&#29983;&#24577;&#31995;&#32479;&#30340;&#30693;&#35782;&#12290;&#29983;&#24577;&#22270;&#34920;&#30001;&#36164;&#20135;&#65288;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#12289;&#24212;&#29992;&#31243;&#24207;&#65289;&#32452;&#25104;&#65292;&#36825;&#20123;&#36164;&#20135;&#36890;&#36807;&#20381;&#36182;&#20851;&#31995;&#38142;&#25509;&#22312;&#19968;&#36215;&#65292;&#36825;&#20123;&#20851;&#31995;&#25351;&#31034;&#20102;&#25216;&#26415;&#65288;&#20363;&#22914; Bing &#22914;&#20309;&#20381;&#36182; GPT-4&#65289;&#21644;&#31038;&#20132;&#65288;&#20363;&#22914; Microsoft &#22914;&#20309;&#20381;&#36182; OpenAI&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#34917;&#20805;&#22270;&#24418;&#32467;&#26500;&#65292;&#27599;&#20010;&#36164;&#20135;&#37117;&#36827;&#19968;&#27493;&#20016;&#23500;&#20102;&#32454;&#31890;&#24230;&#30340;&#20803;&#25968;&#25454;&#65288;&#20363;&#22914;&#35768;&#21487;&#25110;&#22521;&#35757;&#25490;&#25918;&#65289;&#12290;&#25105;&#20204;&#22312; https://crfm.stanford.edu/ecosystem-graphs/ &#19978;&#24191;&#27867;&#35760;&#24405;&#29983;&#24577;&#31995;&#32479;&#12290;&#25130;&#33267; 2023 &#24180; 3 &#26376; 16 &#26085;&#65292;&#25105;&#20204;&#27880;&#37322;&#20102;&#26469;&#33258; 63 &#20010;&#32452;&#32455;&#30340; 262 &#20010;&#36164;&#20135;&#65288;64 &#20010;&#25968;&#25454;&#38598;&#65292;128 &#20010;&#27169;&#22411;&#65292;70 &#20010;&#24212;&#29992;&#31243;&#24207;&#65289;&#65292;&#23427;&#20204;&#30001; 356 &#31181;&#20381;&#36182;&#20851;&#31995;&#38142;&#25509;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29983;&#24577;&#22270;&#34920;&#20316;&#20026;&#19968;&#31181;&#24037;&#20855;&#65292;&#21487;&#20197;&#25552;&#39640;&#22522;&#30784;&#27169;&#22411;&#21450;&#20854;&#26356;&#24191;&#27867;&#30340;&#31038;&#20250;&#25216;&#26415;&#29983;&#24577;&#31995;&#32479;&#30340;&#36879;&#26126;&#24230;&#21644;&#38382;&#36131;&#24230;&#65292;&#24182;&#21628;&#21505;&#37319;&#29992;&#23427;&#20316;&#20026;&#19968;&#31181;&#24120;&#35265;&#30340;&#25991;&#26723;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models (e.g. ChatGPT, StableDiffusion) pervasively influence society, warranting immediate social attention. While the models themselves garner much attention, to accurately characterize their impact, we must consider the broader sociotechnical ecosystem. We propose Ecosystem Graphs as a documentation framework to transparently centralize knowledge of this ecosystem. Ecosystem Graphs is composed of assets (datasets, models, applications) linked together by dependencies that indicate technical (e.g. how Bing relies on GPT-4) and social (e.g. how Microsoft relies on OpenAI) relationships. To supplement the graph structure, each asset is further enriched with fine-grained metadata (e.g. the license or training emissions). We document the ecosystem extensively at https://crfm.stanford.edu/ecosystem-graphs/. As of March 16, 2023, we annotate 262 assets (64 datasets, 128 models, 70 applications) from 63 organizations linked by 356 dependencies. We show Ecosystem Graphs functions a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#20154;&#33080;&#20132;&#25442;&#27169;&#22411;RobustSwap&#65292;&#33021;&#22815;&#26377;&#25928;&#36991;&#20813;&#28304;&#23646;&#24615;&#27844;&#28431;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;3DMM&#30340;&#38544;&#24335;&#21644;&#26174;&#24335;&#20449;&#24687;&#30340;&#21327;&#35843;&#26469;&#23454;&#29616;&#28304;&#22270;&#20687;&#32467;&#26500;&#30340;&#23548;&#20837;&#21644;&#30446;&#26631;&#22270;&#20687;&#20934;&#30830;&#23039;&#24577;&#12290;</title><link>http://arxiv.org/abs/2303.15768</link><description>&lt;p&gt;
RobustSwap&#65306;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#20154;&#33080;&#20132;&#25442;&#27169;&#22411;&#65292;&#33021;&#22815;&#26377;&#25928;&#36991;&#20813;&#23646;&#24615;&#27844;&#28431;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
RobustSwap: A Simple yet Robust Face Swapping Model against Attribute Leakage. (arXiv:2303.15768v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#20154;&#33080;&#20132;&#25442;&#27169;&#22411;RobustSwap&#65292;&#33021;&#22815;&#26377;&#25928;&#36991;&#20813;&#28304;&#23646;&#24615;&#27844;&#28431;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;3DMM&#30340;&#38544;&#24335;&#21644;&#26174;&#24335;&#20449;&#24687;&#30340;&#21327;&#35843;&#26469;&#23454;&#29616;&#28304;&#22270;&#20687;&#32467;&#26500;&#30340;&#23548;&#20837;&#21644;&#30446;&#26631;&#22270;&#20687;&#20934;&#30830;&#23039;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#20132;&#25442;&#26088;&#22312;&#23558;&#28304;&#22270;&#20687;&#30340;&#36523;&#20221;&#65288;&#21363;&#38754;&#37096;&#29305;&#24449;&#65289;&#27880;&#20837;&#30446;&#26631;&#22270;&#20687;&#65292;&#21516;&#26102;&#20005;&#26684;&#20445;&#30041;&#30446;&#26631;&#22270;&#20687;&#30340;&#19981;&#30456;&#20851;&#36523;&#20221;&#23646;&#24615;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20043;&#21069;&#30340;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#28304;&#23646;&#24615;&#27844;&#28431;&#38382;&#39064;&#65292;&#21363;&#28304;&#22270;&#20687;&#30340;&#23646;&#24615;&#19982;&#30446;&#26631;&#22270;&#20687;&#30340;&#23646;&#24615;&#24178;&#25200;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;StyleGAN&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#25214;&#21040;&#36866;&#21512;&#20154;&#33080;&#20132;&#25442;&#20219;&#21153;&#30340;&#28508;&#22312;&#22240;&#32032;&#32452;&#21512;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#20154;&#33080;&#20132;&#25442;&#27169;&#22411;RobustSwap&#65292;&#33021;&#22815;&#25269;&#25239;&#28508;&#22312;&#30340;&#28304;&#23646;&#24615;&#27844;&#28431;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;3DMM&#30340;&#38544;&#24335;&#21644;&#26174;&#24335;&#20449;&#24687;&#30340;&#21327;&#35843;&#20316;&#20026;&#25351;&#23548;&#65292;&#23558;&#28304;&#22270;&#20687;&#30340;&#32467;&#26500;&#21644;&#30446;&#26631;&#22270;&#20687;&#30340;&#20934;&#30830;&#23039;&#24577;&#32467;&#21512;&#36215;&#26469;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#21033;&#29992;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#27809;&#26377;&#36523;&#20221;&#26631;&#31614;&#65292;&#20294;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#26102;&#38388;&#19978;&#36830;&#32493;&#30340;&#35270;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face swapping aims at injecting a source image's identity (i.e., facial features) into a target image, while strictly preserving the target's attributes, which are irrelevant to identity. However, we observed that previous approaches still suffer from source attribute leakage, where the source image's attributes interfere with the target image's. In this paper, we analyze the latent space of StyleGAN and find the adequate combination of the latents geared for face swapping task. Based on the findings, we develop a simple yet robust face swapping model, RobustSwap, which is resistant to the potential source attribute leakage. Moreover, we exploit the coordination of 3DMM's implicit and explicit information as a guidance to incorporate the structure of the source image and the precise pose of the target image. Despite our method solely utilizing an image dataset without identity labels for training, our model has the capability to generate high-fidelity and temporally consistent videos. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#39044;&#35757;&#32451;&#30340;Transformer-based&#34920;&#26684;&#27169;&#22411;&#65306;TabRet&#65292;&#33021;&#22815;&#25903;&#25345;&#26410;&#30693;&#21015;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#31168;&#12290;&#37325;&#26032;&#26631;&#35760;&#21270;&#21644;&#38543;&#26426;&#27927;&#29260;&#22686;&#24378;&#23545;&#24615;&#33021;&#25552;&#21319;&#26377;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2303.15747</link><description>&lt;p&gt;
TabRet: &#39044;&#35757;&#32451;Transformer-based&#34920;&#26684;&#27169;&#22411;&#65292;&#25903;&#25345;&#26410;&#30693;&#21015;
&lt;/p&gt;
&lt;p&gt;
TabRet: Pre-training Transformer-based Tabular Models for Unseen Columns. (arXiv:2303.15747v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15747
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#39044;&#35757;&#32451;&#30340;Transformer-based&#34920;&#26684;&#27169;&#22411;&#65306;TabRet&#65292;&#33021;&#22815;&#25903;&#25345;&#26410;&#30693;&#21015;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#31168;&#12290;&#37325;&#26032;&#26631;&#35760;&#21270;&#21644;&#38543;&#26426;&#27927;&#29260;&#22686;&#24378;&#23545;&#24615;&#33021;&#25552;&#21319;&#26377;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TabRet&#30340;&#21487;&#39044;&#35757;&#32451;Transformer-based&#34920;&#26684;&#27169;&#22411;&#12290;TabRet&#26088;&#22312;&#20026;&#21253;&#21547;&#26410;&#22312;&#39044;&#35757;&#32451;&#20013;&#35265;&#36807;&#30340;&#21015;&#30340;&#19979;&#28216;&#20219;&#21153;&#25552;&#20379;&#25903;&#25345;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;TabRet&#22312;&#24494;&#35843;&#20043;&#21069;&#26377;&#19968;&#20010;&#39069;&#22806;&#30340;&#23398;&#20064;&#27493;&#39588;&#65292;&#31216;&#20026;&#37325;&#26032;&#26631;&#35760;&#21270;&#65292;&#23427;&#22522;&#20110;&#36974;&#34109;&#33258;&#21160;&#32534;&#30721;&#25439;&#22833;&#26469;&#26657;&#20934;&#29305;&#24449;&#23884;&#20837;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#37327;&#30340;&#20844;&#20849;&#20581;&#24247;&#35843;&#26597;&#25968;&#25454;&#23545;TabRet&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;AUC&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#36827;&#34892;&#37325;&#26032;&#26631;&#35760;&#21270;&#21644;&#38543;&#26426;&#27927;&#29260;&#22686;&#24378;&#23545;&#24615;&#33021;&#25552;&#21319;&#26377;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present \emph{TabRet}, a pre-trainable Transformer-based model for tabular data. TabRet is designed to work on a downstream task that contains columns not seen in pre-training. Unlike other methods, TabRet has an extra learning step before fine-tuning called \emph{retokenizing}, which calibrates feature embeddings based on the masked autoencoding loss. In experiments, we pre-trained TabRet with a large collection of public health surveys and fine-tuned it on classification tasks in healthcare, and TabRet achieved the best AUC performance on four datasets. In addition, an ablation study shows retokenizing and random shuffle augmentation of columns during pre-training contributed to performance gains.
&lt;/p&gt;</description></item><item><title>&#26412;&#25253;&#21578;&#36890;&#36807;&#23454;&#39564;&#25506;&#31350;&#20102;&#36882;&#24402;&#29305;&#24449;&#26426;&#22120;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#20854;&#22312;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#29305;&#24449;&#26102;MSE&#26354;&#32447;&#21576;&#29616;&#20986;&#38477;&#20302;-&#22686;&#21152;-&#38477;&#20302;&#30340;&#27169;&#24335;&#65292;&#24182;&#19988;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#8220;&#21452;&#23792;&#19979;&#38477;&#8221;&#29616;&#35937;&#30456;&#20284;&#65292;&#20026;&#21518;&#32493;&#30740;&#31350;&#22880;&#23450;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2303.15745</link><description>&lt;p&gt;
&#20851;&#20110;&#36882;&#24402;&#29305;&#24449;&#26426;&#22120;&#30340;&#29305;&#24449;&#32553;&#25918;
&lt;/p&gt;
&lt;p&gt;
On Feature Scaling of Recursive Feature Machines. (arXiv:2303.15745v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25253;&#21578;&#36890;&#36807;&#23454;&#39564;&#25506;&#31350;&#20102;&#36882;&#24402;&#29305;&#24449;&#26426;&#22120;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#20854;&#22312;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#29305;&#24449;&#26102;MSE&#26354;&#32447;&#21576;&#29616;&#20986;&#38477;&#20302;-&#22686;&#21152;-&#38477;&#20302;&#30340;&#27169;&#24335;&#65292;&#24182;&#19988;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#8220;&#21452;&#23792;&#19979;&#38477;&#8221;&#29616;&#35937;&#30456;&#20284;&#65292;&#20026;&#21518;&#32493;&#30740;&#31350;&#22880;&#23450;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25216;&#26415;&#25253;&#21578;&#36890;&#36807;&#19968;&#31995;&#21015;&#22312;&#22238;&#24402;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#25506;&#31350;&#20102;&#36882;&#24402;&#29305;&#24449;&#26426;&#22120;(RFMs)&#30340;&#34892;&#20026;&#65292;RFMs&#26159;&#19968;&#31181;&#36890;&#36807;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#26469;&#36882;&#24402;&#22320;&#23398;&#20064;&#29305;&#24449;&#30340;&#26032;&#22411;&#26680;&#26426;&#22120;&#12290;&#24403;&#22312;&#25968;&#25454;&#38598;&#20013;&#36845;&#20195;&#22320;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#29305;&#24449;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22343;&#26041;&#35823;&#24046;(MSE)&#26354;&#32447;&#21576;&#29616;&#20986;&#38477;&#20302;-&#22686;&#21152;-&#38477;&#20302;&#30340;&#26377;&#36259;&#27169;&#24335;&#12290;&#36825;&#31181;&#34892;&#20026;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#22823;&#23567;&#12289;&#22122;&#22768;&#21442;&#25968;&#21644;&#30446;&#26631;&#20989;&#25968;&#19979;&#20445;&#25345;&#19968;&#33268;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#35266;&#23519;&#21040;&#30340;MSE&#26354;&#32447;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#35266;&#23519;&#21040;&#30340;&#8220;&#21452;&#23792;&#19979;&#38477;&#8221;&#29616;&#35937;&#30456;&#20284;&#65292;&#26263;&#31034;RFMs&#21644;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#20043;&#38388;&#23384;&#22312;&#26032;&#30340;&#32852;&#31995;&#12290;&#36825;&#20221;&#25253;&#21578;&#20026;&#26410;&#26469;&#30740;&#31350;&#36825;&#31181;&#22855;&#22937;&#34892;&#20026;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this technical report, we explore the behavior of Recursive Feature Machines (RFMs), a type of novel kernel machine that recursively learns features via the average gradient outer product, through a series of experiments on regression datasets. When successively adding random noise features to a dataset, we observe intriguing patterns in the Mean Squared Error (MSE) curves with the test MSE exhibiting a decrease-increase-decrease pattern. This behavior is consistent across different dataset sizes, noise parameters, and target functions. Interestingly, the observed MSE curves show similarities to the "double descent" phenomenon observed in deep neural networks, hinting at new connection between RFMs and neural network behavior. This report lays the groundwork for future research into this peculiar behavior.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#32972;&#26223;&#38899;&#20048;&#30340;&#26041;&#27861;&#65292;&#23427;&#30001;&#20116;&#31181;&#19981;&#21516;&#30340;&#20048;&#22120;&#28436;&#22863;&#21517;&#20026;&#8220;&#31354;&#20013;&#23567;&#22992;&#26354;&#8221;&#30340;&#21476;&#20856;&#38899;&#20048;&#32452;&#25104;&#65292;&#24182;&#36890;&#36807;&#25913;&#21464;&#20048;&#22120;&#30340;&#38899;&#37327;&#26469;&#36866;&#24212;&#28216;&#25103;&#30340;&#19981;&#21516;&#20803;&#32032;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#36825;&#31181;&#33258;&#36866;&#24212;&#32972;&#26223;&#38899;&#20048;&#21487;&#20197;&#25913;&#21892;&#28216;&#25103;&#30340;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2303.15734</link><description>&lt;p&gt;
&#19968;&#20010;&#22810;&#20048;&#22120;&#20307;&#31215;&#35843;&#21046;&#30340;&#26041;&#27861;&#22686;&#24378;&#26684;&#26007;&#28216;&#25103;&#20013;&#30340;&#32972;&#26223;&#38899;&#20048;&#65306;&#33258;&#36866;&#24212;&#32972;&#26223;&#38899;&#20048;
&lt;/p&gt;
&lt;p&gt;
Adaptive Background Music for a Fighting Game: A Multi-Instrument Volume Modulation Approach. (arXiv:2303.15734v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#32972;&#26223;&#38899;&#20048;&#30340;&#26041;&#27861;&#65292;&#23427;&#30001;&#20116;&#31181;&#19981;&#21516;&#30340;&#20048;&#22120;&#28436;&#22863;&#21517;&#20026;&#8220;&#31354;&#20013;&#23567;&#22992;&#26354;&#8221;&#30340;&#21476;&#20856;&#38899;&#20048;&#32452;&#25104;&#65292;&#24182;&#36890;&#36807;&#25913;&#21464;&#20048;&#22120;&#30340;&#38899;&#37327;&#26469;&#36866;&#24212;&#28216;&#25103;&#30340;&#19981;&#21516;&#20803;&#32032;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#36825;&#31181;&#33258;&#36866;&#24212;&#32972;&#26223;&#38899;&#20048;&#21487;&#20197;&#25913;&#21892;&#28216;&#25103;&#30340;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312; DareFightingICE &#20013;&#28155;&#21152;&#33258;&#36866;&#24212;&#32972;&#26223;&#38899;&#20048;&#20197;&#22686;&#24378;&#28216;&#25103;&#20307;&#39564;&#30340;&#24037;&#20316;&#12290;&#33258;&#36866;&#24212;&#32972;&#26223;&#38899;&#20048;&#30001;&#20116;&#31181;&#19981;&#21516;&#30340;&#20048;&#22120;&#28436;&#22863;&#21517;&#20026;&#8220;&#31354;&#20013;&#23567;&#22992;&#26354;&#8221;&#30340;&#21476;&#20856;&#38899;&#20048;&#32452;&#25104;&#65292;&#36890;&#36807;&#25913;&#21464;&#20048;&#22120;&#30340;&#38899;&#37327;&#26469;&#36866;&#24212;&#28216;&#25103;&#30340;&#19981;&#21516;&#20803;&#32032;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#23454;&#39564;&#26469;&#35780;&#20272;&#33258;&#36866;&#24212;&#32972;&#26223;&#38899;&#20048;&#65292;&#24182;&#20351;&#29992;&#20102;&#19968;&#31181;&#21482;&#20351;&#29992;&#38899;&#39057;&#20316;&#20026;&#36755;&#20837;&#30340;&#28145;&#24230;&#22686;&#24378;&#23398;&#20064; AI&#65288;Blind DL AI&#65289;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#27809;&#26377;&#33258;&#36866;&#24212;&#32972;&#26223;&#38899;&#20048;&#26102;&#30456;&#27604;&#65292;Blind DL AI &#22312;&#19982;&#33258;&#36866;&#24212;&#32972;&#26223;&#38899;&#20048;&#19968;&#36215;&#25773;&#25918;&#26102;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents our work to enhance the background music (BGM) in DareFightingICE by adding an adaptive BGM. The adaptive BGM consists of five different instruments playing a classical music piece called "Air on G-String." The BGM adapts by changing the volume of the instruments. Each instrument is connected to a different element of the game. We then run experiments to evaluate the adaptive BGM by using a deep reinforcement learning AI that only uses audio as input (Blind DL AI). The results show that the performance of the Blind DL AI improves while playing with the adaptive BGM as compared to playing without the adaptive BGM.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;LLM&#21644;ChatGPT&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#26174;&#31034;&#20986;&#20854;&#22312;&#21387;&#21147;&#21644;&#25233;&#37057;&#30151;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#33258;&#26432;&#39118;&#38505;&#26816;&#27979;&#19978;&#20173;&#38656;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2303.15727</link><description>&lt;p&gt;
&#35780;&#20272;ChatGPT&#22312;&#22522;&#20110;NLP&#30340;&#24515;&#29702;&#20581;&#24247;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluation of ChatGPT for NLP-based Mental Health Applications. (arXiv:2303.15727v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;LLM&#21644;ChatGPT&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#26174;&#31034;&#20986;&#20854;&#22312;&#21387;&#21147;&#21644;&#25233;&#37057;&#30151;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#33258;&#26432;&#39118;&#38505;&#26816;&#27979;&#19978;&#20173;&#38656;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#22810;&#39033;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#20855;&#26377;&#25104;&#21151;&#30340;&#24212;&#29992;&#65292;&#21487;&#33021;&#23545;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#24515;&#29702;&#20581;&#24247;&#24212;&#29992;&#30740;&#31350;&#20063;&#24456;&#26377;&#24110;&#21161;&#12290;&#26412;&#30740;&#31350;&#25253;&#21578;&#20102;&#22522;&#20110;LLM&#30340;ChatGPT (&#20351;&#29992;gpt-3.5-turbo&#21518;&#31471;)&#22312;&#19977;&#20010;&#25991;&#26412;&#31867;&#24515;&#29702;&#20581;&#24247;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;: &#21387;&#21147;&#26816;&#27979; (2&#31867;&#20998;&#31867;)&#12289;&#25233;&#37057;&#30151;&#26816;&#27979;(2&#31867;&#20998;&#31867;)&#21644;&#33258;&#26432;&#39118;&#38505;&#26816;&#27979;(5&#31867;&#20998;&#31867;)&#12290;&#25105;&#20204;&#20174;&#20844;&#20849;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#20102;&#19977;&#20010;&#20998;&#31867;&#20219;&#21153;&#30340;&#24102;&#26631;&#27880;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#12290;&#28982;&#21518;&#20351;&#29992;ChatGPT API&#23545;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#36827;&#34892;&#36755;&#20837;&#25552;&#31034;&#20998;&#31867;&#12290;&#25105;&#20204;&#24471;&#21040;&#20102;0.73&#12289;0.86&#21644;0.37&#30340;F1&#20998;&#25968;&#65292;&#20998;&#21035;&#29992;&#20110;&#21387;&#21147;&#26816;&#27979;&#12289;&#25233;&#37057;&#30151;&#26816;&#27979;&#21644;&#33258;&#26432;&#39118;&#38505;&#26816;&#27979;&#12290;&#24635;&#20307;&#19978;&#65292;ChatGPT&#22312;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#20855;&#22791;&#24456;&#22823;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLM) have been successful in several natural language understanding tasks and could be relevant for natural language processing (NLP)-based mental health application research. In this work, we report the performance of LLM-based ChatGPT (with gpt-3.5-turbo backend) in three text-based mental health classification tasks: stress detection (2-class classification), depression detection (2-class classification), and suicidality detection (5-class classification). We obtained annotated social media posts for the three classification tasks from public datasets. Then ChatGPT API classified the social media posts with an input prompt for classification. We obtained F1 scores of 0.73, 0.86, and 0.37 for stress detection, depression detection, and suicidality detection, respectively. A baseline model that always predicted the dominant class resulted in F1 scores of 0.35, 0.60, and 0.19. The zero-shot classification accuracy obtained with ChatGPT indicates a potential use o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32593;&#26684;-&#25163;&#37096;&#20114;&#21160;&#36827;&#34892;&#21333;&#24352;&#22270;&#20687;&#21452;&#25163;&#37325;&#24314;&#30340;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#32593;&#26684;&#39030;&#28857;&#20301;&#32622;&#21644;MANO&#21442;&#25968;&#20316;&#20026;&#26597;&#35810;&#20196;&#29260;&#65292;&#23454;&#29616;&#20102;&#31934;&#30830;&#30340;&#32593;&#26684;&#37325;&#24314;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#36825;&#19968;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.15718</link><description>&lt;p&gt;
MeMaHand&#65306;&#21033;&#29992;&#32593;&#26684;-&#25163;&#37096;&#20114;&#21160;&#36827;&#34892;&#21333;&#24352;&#22270;&#20687;&#21452;&#25163;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
MeMaHand: Exploiting Mesh-Mano Interaction for Single Image Two-Hand Reconstruction. (arXiv:2303.15718v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32593;&#26684;-&#25163;&#37096;&#20114;&#21160;&#36827;&#34892;&#21333;&#24352;&#22270;&#20687;&#21452;&#25163;&#37325;&#24314;&#30340;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#32593;&#26684;&#39030;&#28857;&#20301;&#32622;&#21644;MANO&#21442;&#25968;&#20316;&#20026;&#26597;&#35810;&#20196;&#29260;&#65292;&#23454;&#29616;&#20102;&#31934;&#30830;&#30340;&#32593;&#26684;&#37325;&#24314;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#36825;&#19968;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#25163;&#37325;&#24314;&#26041;&#27861;&#36890;&#24120;&#23545;&#19968;&#20010;&#36890;&#29992;&#30340;3D&#25163;&#27169;&#22411;&#36827;&#34892;&#21442;&#25968;&#21270;&#25110;&#32773;&#30452;&#25509;&#39044;&#27979;&#25163;&#25484;&#32593;&#26684;&#20301;&#32622;&#65292;&#21442;&#25968;&#34920;&#31034;&#30340;&#25163;&#37096;&#24418;&#29366;&#21644;&#26059;&#36716;&#23039;&#24577;&#26356;&#20026;&#31283;&#23450;&#65292;&#32780;&#38750;&#21442;&#25968;&#21270;&#30340;&#26041;&#27861;&#21487;&#20197;&#26356;&#31934;&#30830;&#22320;&#39044;&#27979;&#32593;&#26684;&#20301;&#32622;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20174;&#21333;&#24352;RGB&#22270;&#20687;&#20013;&#21516;&#26102;&#37325;&#24314;&#20004;&#21482;&#25163;&#30340;&#32593;&#26684;&#24182;&#20272;&#35745;MANO&#21442;&#25968;&#65292;&#20197;&#21033;&#29992;&#20004;&#31181;&#25163;&#34920;&#31034;&#26041;&#27861;&#30340;&#20248;&#28857;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#32593;&#26684;-&#25163;&#37096;&#20114;&#21160;&#22359;&#65288;MMIBs&#65289;&#65292;&#23427;&#23558;&#32593;&#26684;&#39030;&#28857;&#20301;&#32622;&#21644;MANO&#21442;&#25968;&#20316;&#20026;&#20004;&#31181;&#26597;&#35810;&#20196;&#29260;&#12290;MMIB&#30001;&#19968;&#20010;&#22270;&#24418;&#27531;&#24046;&#22359;&#26469;&#32858;&#21512;&#23616;&#37096;&#20449;&#24687;&#21644;&#20004;&#20010;&#21464;&#25442;&#32534;&#30721;&#22120;&#26469;&#24314;&#27169;&#36828;&#31243;&#20381;&#36182;&#20851;&#31995;&#12290;&#21464;&#25442;&#32534;&#30721;&#22120;&#37197;&#22791;&#19981;&#21516;&#30340;&#19981;&#23545;&#31216;&#20851;&#27880;&#25513;&#30721;&#26469;&#20998;&#21035;&#24314;&#27169;&#25163;&#20869;&#21644;&#25163;&#38388;&#20851;&#27880;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#32593;&#26684;&#23545;&#40784;&#32454;&#21270;&#27169;&#22359;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#32593;&#26684;&#37325;&#24314;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21333;&#25163;&#21644;&#21452;&#25163;&#37325;&#24314;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing methods proposed for hand reconstruction tasks usually parameterize a generic 3D hand model or predict hand mesh positions directly. The parametric representations consisting of hand shapes and rotational poses are more stable, while the non-parametric methods can predict more accurate mesh positions. In this paper, we propose to reconstruct meshes and estimate MANO parameters of two hands from a single RGB image simultaneously to utilize the merits of two kinds of hand representations. To fulfill this target, we propose novel Mesh-Mano interaction blocks (MMIBs), which take mesh vertices positions and MANO parameters as two kinds of query tokens. MMIB consists of one graph residual block to aggregate local information and two transformer encoders to model long-range dependencies. The transformer encoders are equipped with different asymmetric attention masks to model the intra-hand and inter-hand attention, respectively. Moreover, we introduce the mesh alignment refinement mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#65292;&#37319;&#29992;&#20102;&#26174;&#24335;&#35268;&#21010;&#26469;&#24110;&#21161;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#27604;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#34920;&#29616;&#26356;&#22909;&#65292;&#26174;&#24335;&#35268;&#21010;&#22312;&#31995;&#32479;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.15714</link><description>&lt;p&gt;
&#26174;&#24335;&#35268;&#21010;&#26377;&#21161;&#20110;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Explicit Planning Helps Language Models in Logical Reasoning. (arXiv:2303.15714v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#65292;&#37319;&#29992;&#20102;&#26174;&#24335;&#35268;&#21010;&#26469;&#24110;&#21161;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#27604;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#34920;&#29616;&#26356;&#22909;&#65292;&#26174;&#24335;&#35268;&#21010;&#22312;&#31995;&#32479;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#37319;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#23558;&#26174;&#24335;&#35268;&#21010;&#32435;&#20837;&#21040;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#22240;&#27492;&#21487;&#20197;&#36890;&#36807;&#23637;&#26395;&#26410;&#26469;&#30340;&#25928;&#26524;&#26469;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#20840;&#22871;&#31995;&#32479;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#31572;&#39064;&#20219;&#21153;&#20013;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#65292;&#23613;&#31649;&#21482;&#26377;&#32422;15&#20159;&#20010;&#21442;&#25968;&#65292;&#20294;&#19982;GPT-3-davinci&#34920;&#29616;&#30456;&#24403;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#20010;&#28040;&#34701;&#30740;&#31350;&#20197;&#35777;&#26126;&#26174;&#24335;&#35268;&#21010;&#22312;&#31995;&#32479;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models have been shown to perform remarkably well on a wide range of natural language processing tasks. In this paper, we propose a novel system that uses language models to perform multi-step logical reasoning. Our system incorporates explicit planning into its inference procedure, thus able to make more informed reasoning decisions at each step by looking ahead into their future effects. In our experiments, our full system significantly outperforms other competing systems. On a multiple-choice question answering task, our system performs competitively compared to GPT-3-davinci despite having only around 1.5B parameters. We conduct several ablation studies to demonstrate that explicit planning plays a crucial role in the system's performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#33258;&#36866;&#24212;&#33021;&#21147;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#24341;&#23548;&#25968;&#25454;&#22686;&#24378;&#65292;&#20351;&#29992;&#20462;&#25913;&#21518;&#30340;IL&#36807;&#31243;&#65292;&#24182;&#22312;&#23398;&#20064;&#36866;&#24212;&#24615;&#20301;&#32622;&#21644;&#23039;&#24577;&#25511;&#21046;&#31574;&#30053;&#26041;&#38754;&#36827;&#34892;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.15688</link><description>&lt;p&gt;
&#20351;&#29992;Tube MPC&#24341;&#23548;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#39640;&#25928;&#23398;&#20064;&#40065;&#26834;&#24615;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#30340;&#28145;&#24230;&#23398;&#20064;&#65288;arXiv:2303.15688v1 [cs.RO]&#65289;
&lt;/p&gt;
&lt;p&gt;
Efficient Deep Learning of Robust, Adaptive Policies using Tube MPC-Guided Data Augmentation. (arXiv:2303.15688v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#33258;&#36866;&#24212;&#33021;&#21147;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#24341;&#23548;&#25968;&#25454;&#22686;&#24378;&#65292;&#20351;&#29992;&#20462;&#25913;&#21518;&#30340;IL&#36807;&#31243;&#65292;&#24182;&#22312;&#23398;&#20064;&#36866;&#24212;&#24615;&#20301;&#32622;&#21644;&#23039;&#24577;&#25511;&#21046;&#31574;&#30053;&#26041;&#38754;&#36827;&#34892;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#37096;&#32626;&#25935;&#25463;&#33258;&#20027;&#31995;&#32479;&#38656;&#35201;&#36866;&#24212;&#33021;&#21147;&#21644;&#23545;&#19981;&#30830;&#23450;&#24615;&#30340;&#40065;&#26834;&#24615;&#12290;&#29616;&#26377;&#30340;&#40065;&#26834;&#21644;&#33258;&#36866;&#24212;&#25511;&#21046;&#22120;&#65292;&#22914;&#22522;&#20110;MPC&#30340;&#25511;&#21046;&#22120;&#65292;&#21487;&#20197;&#22312;&#22312;&#32447;&#36816;&#34892;&#35745;&#31639;&#37327;&#24222;&#22823;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#20986;&#29616;&#20102;&#26377;&#25928;&#22320;&#20174;MPC&#23398;&#20064;&#40065;&#26834;&#19988;&#21487;&#22312;&#26426;&#36733;&#35774;&#22791;&#19978;&#37096;&#32626;&#30340;&#31574;&#30053;&#30340;&#31574;&#30053;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#32570;&#20047;&#22522;&#26412;&#36866;&#24212;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#19968;&#31181;&#29616;&#26377;&#30340;&#39640;&#25928;IL&#31639;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#24615;&#31574;&#30053;&#20174;MPC&#23398;&#20064;&#65292;&#20855;&#26377;&#23398;&#20064;&#36866;&#24212;&#20855;&#26377;&#25361;&#25112;&#24615;&#27169;&#22411;/&#29615;&#22659;&#19981;&#30830;&#23450;&#24615;&#30340;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#36890;&#36807;&#22312;&#23398;&#20064;&#30340;&#20302;&#32500;&#27169;&#22411;/&#29615;&#22659;&#34920;&#31034;&#19978;&#23545;&#31574;&#30053;&#36827;&#34892;&#35843;&#25972;&#65292;&#20174;&#32780;&#20462;&#25913;IL&#36807;&#31243;&#65292;&#36825;&#21487;&#20197;&#22312;&#22312;&#32447;&#29366;&#24577;&#19979;&#39640;&#25928;&#22320;&#20272;&#35745;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#23450;&#21046;&#20026;&#23398;&#20064;&#33258;&#36866;&#24212;&#20301;&#32622;&#21644;&#23039;&#24577;&#25511;&#21046;&#31574;&#30053;&#20197;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24178;&#25200;&#19979;&#36319;&#36394;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The deployment of agile autonomous systems in challenging, unstructured environments requires adaptation capabilities and robustness to uncertainties. Existing robust and adaptive controllers, such as the ones based on MPC, can achieve impressive performance at the cost of heavy online onboard computations. Strategies that efficiently learn robust and onboard-deployable policies from MPC have emerged, but they still lack fundamental adaptation capabilities. In this work, we extend an existing efficient IL algorithm for robust policy learning from MPC with the ability to learn policies that adapt to challenging model/environment uncertainties. The key idea of our approach consists in modifying the IL procedure by conditioning the policy on a learned lower-dimensional model/environment representation that can be efficiently estimated online. We tailor our approach to the task of learning an adaptive position and attitude control policy to track trajectories under challenging disturbances
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550;&#26469;&#25628;&#32034;&#26368;&#36866;&#21512;&#32473;&#23450;&#25945;&#24072;&#30340;&#26368;&#20339;&#23398;&#29983;&#26550;&#26500;&#65292;&#20197;&#25552;&#39640;&#30693;&#35782;&#33976;&#39311;&#30340;&#25928;&#26524;&#12290;&#20854;&#36890;&#36807;&#24230;&#37327;&#35821;&#20041;&#28608;&#27963;&#26144;&#23556;&#26465;&#20214;&#19979;&#30340;&#30456;&#20284;&#24615;&#30697;&#38453;&#26469;&#36873;&#25321;&#26368;&#20339;&#23398;&#29983;&#65292;&#32780;&#19981;&#26159;&#36890;&#36807;&#20256;&#32479;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.15678</link><description>&lt;p&gt;
DisWOT: &#26080;&#38656;&#35757;&#32451;&#30340;&#30693;&#35782;&#33976;&#39311;&#23398;&#29983;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
DisWOT: Student Architecture Search for Distillation WithOut Training. (arXiv:2303.15678v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550;&#26469;&#25628;&#32034;&#26368;&#36866;&#21512;&#32473;&#23450;&#25945;&#24072;&#30340;&#26368;&#20339;&#23398;&#29983;&#26550;&#26500;&#65292;&#20197;&#25552;&#39640;&#30693;&#35782;&#33976;&#39311;&#30340;&#25928;&#26524;&#12290;&#20854;&#36890;&#36807;&#24230;&#37327;&#35821;&#20041;&#28608;&#27963;&#26144;&#23556;&#26465;&#20214;&#19979;&#30340;&#30456;&#20284;&#24615;&#30697;&#38453;&#26469;&#36873;&#25321;&#26368;&#20339;&#23398;&#29983;&#65292;&#32780;&#19981;&#26159;&#36890;&#36807;&#20256;&#32479;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;(KD)&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#31528;&#37325;&#30340;&#25945;&#24072;&#30340;&#25351;&#23548;&#19979;&#25552;&#39640;&#36731;&#37327;&#32423;&#23398;&#29983;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25945;&#24072;&#21644;&#23398;&#29983;&#20043;&#38388;&#30340;&#22823;&#22411;&#26550;&#26500;&#24046;&#24322;&#38480;&#21046;&#20102;&#33976;&#39311;&#25928;&#26524;&#12290;&#30456;&#23545;&#20110;&#20197;&#21069;&#30340;&#33258;&#36866;&#24212;&#33976;&#39311;&#26041;&#27861;&#26469;&#20943;&#23569;&#25945;&#24072;&#21644;&#23398;&#29983;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#38656;&#35757;&#32451;&#26694;&#26550;&#65292;&#20197;&#25628;&#32034;&#32473;&#23450;&#25945;&#24072;&#30340;&#26368;&#20339;&#23398;&#29983;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation (KD) is an effective training strategy to improve the lightweight student models under the guidance of cumbersome teachers. However, the large architecture difference across the teacher-student pairs limits the distillation gains. In contrast to previous adaptive distillation methods to reduce the teacher-student gap, we explore a novel training-free framework to search for the best student architectures for a given teacher. Our work first empirically show that the optimal model under vanilla training cannot be the winner in distillation. Secondly, we find that the similarity of feature semantics and sample relations between random-initialized teacher-student networks have good correlations with final distillation performances. Thus, we efficiently measure similarity matrixs conditioned on the semantic activation maps to select the optimal student via an evolutionary algorithm without any training. In this way, our student architecture search for Distillation Wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#38543;&#26426;&#35268;&#21010;&#21644;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#20013;&#20984;&#22810;&#32423;&#38543;&#26426;&#38382;&#39064;&#30340;&#25968;&#20540;&#35299;&#27861;&#65292;&#21253;&#25324;&#21160;&#24577;&#35268;&#21010;&#21644;&#21106;&#24179;&#38754;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#32500;&#24230;&#35781;&#21650;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.15672</link><description>&lt;p&gt;
&#20984;&#22810;&#32423;&#38543;&#26426;&#20248;&#21270;&#30340;&#25968;&#20540;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Numerical Methods for Convex Multistage Stochastic Optimization. (arXiv:2303.15672v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#38543;&#26426;&#35268;&#21010;&#21644;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#20013;&#20984;&#22810;&#32423;&#38543;&#26426;&#38382;&#39064;&#30340;&#25968;&#20540;&#35299;&#27861;&#65292;&#21253;&#25324;&#21160;&#24577;&#35268;&#21010;&#21644;&#21106;&#24179;&#38754;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#32500;&#24230;&#35781;&#21650;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#29615;&#22659;&#19979;&#28041;&#21450;&#39034;&#24207;&#20915;&#31574;&#30340;&#20248;&#21270;&#38382;&#39064;&#24050;&#22312;&#38543;&#26426;&#35268;&#21010;(SP)&#12289;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;(SOC)&#21644;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#20013;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#26412;&#25991;&#20027;&#35201;&#38598;&#20013;&#35752;&#35770;SP&#21644;SOC&#24314;&#27169;&#26041;&#27861;&#12290;&#22312;&#36825;&#20123;&#26694;&#26550;&#20013;&#65292;&#23384;&#22312;&#33258;&#28982;&#24773;&#20917;&#19979;&#25152;&#32771;&#34385;&#30340;&#38382;&#39064;&#26159;&#20984;&#38382;&#39064;&#12290;&#39034;&#24207;&#20248;&#21270;&#30340;&#32463;&#20856;&#26041;&#27861;&#26159;&#22522;&#20110;&#21160;&#24577;&#35268;&#21010;&#65292;&#20294;&#23427;&#23384;&#22312;&#25152;&#35859;&#30340;&#8220;&#32500;&#24230;&#35781;&#21650;&#8221;&#38382;&#39064;&#65292;&#21363;&#38543;&#30528;&#29366;&#24577;&#21464;&#37327;&#32500;&#24230;&#30340;&#22686;&#21152;&#65292;&#23427;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#36817;&#24180;&#26469;&#65292;&#35299;&#20915;&#20984;&#22810;&#32423;&#38543;&#26426;&#38382;&#39064;&#30340;&#26368;&#26032;&#36827;&#23637;&#26159;&#22522;&#20110;&#21160;&#24577;&#35268;&#21010;&#26041;&#31243;&#30340;&#25104;&#26412;&#20989;&#25968;&#36880;&#27493;&#36924;&#36817;&#30340;&#21106;&#24179;&#38754;&#36817;&#20284;&#12290;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#21106;&#24179;&#38754;&#31867;&#22411;&#31639;&#27861;&#26159;&#26412;&#25991;&#30340;&#20027;&#35201;&#35752;&#35770;&#20869;&#23481;&#20043;&#19968;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#24212;&#29992;&#20110;&#22810;&#32423;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#30340;&#38543;&#26426;&#36924;&#36817;&#31867;&#22411;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimization problems involving sequential decisions in a stochastic environment were studied in Stochastic Programming (SP), Stochastic Optimal Control (SOC) and Markov Decision Processes (MDP). In this paper we mainly concentrate on SP and SOC modelling approaches. In these frameworks there are natural situations when the considered problems are convex. Classical approach to sequential optimization is based on dynamic programming. It has the problem of the so-called ``Curse of Dimensionality", in that its computational complexity increases exponentially with increase of dimension of state variables. Recent progress in solving convex multistage stochastic problems is based on cutting planes approximations of the cost-to-go (value) functions of dynamic programming equations. Cutting planes type algorithms in dynamical settings is one of the main topics of this paper. We also discuss Stochastic Approximation type methods applied to multistage stochastic optimization problems. From the c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#19979;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#27169;&#22411;&#65292;&#21033;&#29992;&#22823;&#37327;&#26410;&#36716;&#24405;&#30340;&#35821;&#38899;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21487;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#27169;&#22411;&#25152;&#38656;&#30340;&#21305;&#37197;&#36716;&#24405;&#25968;&#25454;&#37327;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#25968;&#25454;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#26041;&#27861;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.15669</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#35328;&#19979;&#30340;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#25991;&#26412;&#36716;&#35821;&#38899;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Pre-Training For Data-Efficient Text-to-Speech On Low Resource Languages. (arXiv:2303.15669v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#19979;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#27169;&#22411;&#65292;&#21033;&#29992;&#22823;&#37327;&#26410;&#36716;&#24405;&#30340;&#35821;&#38899;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21487;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#27169;&#22411;&#25152;&#38656;&#30340;&#21305;&#37197;&#36716;&#24405;&#25968;&#25454;&#37327;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#25968;&#25454;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#26041;&#27861;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22823;&#37327;&#36716;&#24405;&#38899;&#39057;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#26102;&#65292;&#31070;&#32463;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#21487;&#20197;&#21512;&#25104;&#33258;&#28982;&#30340;&#20154;&#31867;&#35821;&#38899;&#12290;&#20294;&#26159;&#65292;&#25910;&#38598;&#36825;&#26679;&#30340;&#22823;&#35268;&#27169;&#36716;&#24405;&#25968;&#25454;&#24456;&#26114;&#36149;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;TTS&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21033;&#29992;&#22823;&#37327;&#26410;&#36716;&#24405;&#30340;&#35821;&#38899;&#25968;&#25454;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#21487;&#20197;&#26174;&#30528;&#20943;&#23569;&#35757;&#32451;&#27169;&#22411;&#25152;&#38656;&#30340;&#21305;&#37197;&#36716;&#24405;&#25968;&#25454;&#37327;&#65292;&#29992;&#20110;&#30446;&#26631;&#19979;&#28216;TTS&#20219;&#21153;&#30340;&#35757;&#32451;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#20174;&#25197;&#26354;&#30340;mel&#39057;&#35889;&#22270;&#20013;&#37325;&#24314;&#20986;&#21435;&#25197;&#26354;&#30340;mel&#39057;&#35889;&#22270;&#65292;&#36825;&#21487;&#33021;&#20351;&#27169;&#22411;&#23398;&#20250;&#20102;&#36755;&#20837;&#21644;&#36755;&#20986;&#24207;&#21015;&#20043;&#38388;&#30340;&#36866;&#24403;&#26102;&#38388;&#20998;&#37197;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#36827;&#19968;&#27493;&#25552;&#39640;&#24494;&#35843;&#20013;&#30340;&#25968;&#25454;&#25928;&#29575;&#12290;&#25105;&#20204;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#22330;&#26223;&#20013;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#19982;&#31454;&#20105;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#20986;&#33394;&#12290;&#20195;&#30721;&#21644;&#38899;&#39057;&#26679;&#26412;&#21487;&#20197;&#22312;&#25105;&#20204;&#30340;&#39033;&#30446;&#39029;&#38754;&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural text-to-speech (TTS) models can synthesize natural human speech when trained on large amounts of transcribed speech. However, collecting such large-scale transcribed data is expensive. This paper proposes an unsupervised pre-training method for a sequence-to-sequence TTS model by leveraging large untranscribed speech data. With our pre-training, we can remarkably reduce the amount of paired transcribed data required to train the model for the target downstream TTS task. The main idea is to pre-train the model to reconstruct de-warped mel-spectrograms from warped ones, which may allow the model to learn proper temporal assignment relation between input and output sequences. In addition, we propose a data augmentation method that further improves the data efficiency in fine-tuning. We empirically demonstrate the effectiveness of our proposed method in low-resource language scenarios, achieving outstanding performance compared to competing methods. The code and audio samples are av
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20030;&#21150;&#22312;2023 IEEE&#28216;&#25103;&#20250;&#35758;&#19978;&#30340;&#31532;&#19968;&#23626;ChatGPT4PCG&#27604;&#36187;&#65292;&#30446;&#26631;&#26159;&#35753;ChatGPT&#29983;&#25104;&#20855;&#26377;&#39640;&#31283;&#23450;&#24615;&#21644;&#31867;&#20284;&#35282;&#33394;&#30340;&#29305;&#36136;&#26469;&#29983;&#25104;&#20855;&#26377;&#31185;&#23398;&#40479;&#35282;&#33394;&#32423;&#27700;&#24179;&#30340;&#20851;&#21345;&#12290;</title><link>http://arxiv.org/abs/2303.15662</link><description>&lt;p&gt;
ChatGPT4PCG&#27604;&#36187;&#65306;&#31185;&#23398;&#40479;&#35282;&#33394;&#32423;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
ChatGPT4PCG Competition: Character-like Level Generation for Science Birds. (arXiv:2303.15662v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20030;&#21150;&#22312;2023 IEEE&#28216;&#25103;&#20250;&#35758;&#19978;&#30340;&#31532;&#19968;&#23626;ChatGPT4PCG&#27604;&#36187;&#65292;&#30446;&#26631;&#26159;&#35753;ChatGPT&#29983;&#25104;&#20855;&#26377;&#39640;&#31283;&#23450;&#24615;&#21644;&#31867;&#20284;&#35282;&#33394;&#30340;&#29305;&#36136;&#26469;&#29983;&#25104;&#20855;&#26377;&#31185;&#23398;&#40479;&#35282;&#33394;&#32423;&#27700;&#24179;&#30340;&#20851;&#21345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;2023&#24180;IEEE&#28216;&#25103;&#20250;&#35758;&#19978;&#30340;&#31532;&#19968;&#23626;ChatGPT4PCG&#27604;&#36187;&#12290;&#26412;&#27425;&#27604;&#36187;&#30340;&#30446;&#26631;&#26159;&#35753;&#21442;&#36187;&#32773;&#36890;&#36807;&#21019;&#36896;&#24615;&#21644;&#25552;&#31034;&#24037;&#31243;&#25216;&#33021;&#65292;&#20026;ChatGPT&#21019;&#24314;&#26377;&#25928;&#30340;&#25552;&#31034;&#65292;&#20351;&#20854;&#33021;&#22815;&#20855;&#26377;&#39640;&#31283;&#23450;&#24615;&#21644;&#31867;&#20284;&#35282;&#33394;&#30340;&#29305;&#36136;&#26469;&#29983;&#25104;&#20855;&#26377;&#31185;&#23398;&#40479;&#35282;&#33394;&#32423;&#27700;&#24179;&#30340;&#20851;&#21345;&#12290;&#20026;&#20102;&#38477;&#20302;&#21442;&#36187;&#38376;&#27099;&#65292;&#25105;&#20204;&#23558;&#20219;&#21153;&#38480;&#21046;&#22312;&#29983;&#25104;&#22823;&#20889;&#33521;&#25991;&#23383;&#27597;&#12290;&#21442;&#36187;&#20316;&#21697;&#30340;&#36136;&#37327;&#30001;&#20854;&#31283;&#23450;&#24615;&#21644;&#19982;&#32473;&#23450;&#23383;&#31526;&#30340;&#30456;&#20284;&#24615;&#20915;&#23450;&#12290;&#32473;&#21442;&#36187;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#26679;&#20363;&#25552;&#31034;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the first ChatGPT4PCG Competition at the 2023 IEEE Conference on Games. The objective of this competition is for participants to create effective prompts for ChatGPT--enabling it to generate Science Birds levels with high stability and character-like qualities--fully using their creativity as well as prompt engineering skills. ChatGPT is a conversational agent developed by OpenAI. Science Birds is selected as the competition platform because designing an Angry Birds-like level is not a trivial task due to the in-game gravity; the playability of the levels is determined by their stability. To lower the entry barrier to the competition, we limit the task to the generation of capitalized English alphabetical characters. Here, the quality of the generated levels is determined by their stability and similarity to the given characters. A sample prompt is provided to participants for their reference. An experiment is conducted to determine the effectiveness of its modified
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#38024;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20219;&#21153;&#29305;&#23450;&#30340;&#23631;&#34109;&#26694;&#26550;&#65292;&#31216;&#20026;Typhoon&#65292;&#21487;&#22312;GLUE&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#21331;&#36234;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;MRPC&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2303.15619</link><description>&lt;p&gt;
&#21488;&#39118;&#65306;&#38024;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#29305;&#23450;&#20219;&#21153;&#23631;&#34109;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Typhoon: Towards an Effective Task-Specific Masking Strategy for Pre-trained Language Models. (arXiv:2303.15619v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#38024;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20219;&#21153;&#29305;&#23450;&#30340;&#23631;&#34109;&#26694;&#26550;&#65292;&#31216;&#20026;Typhoon&#65292;&#21487;&#22312;GLUE&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#21331;&#36234;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;MRPC&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22270;&#24418;&#22788;&#29702;&#21333;&#20803;&#25152;&#33021;&#25552;&#20379;&#30340;&#39640;&#24230;&#24182;&#34892;&#24615;&#65292;&#21464;&#21387;&#22120;&#26550;&#26500;&#20351;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#12290;&#22312;&#20256;&#32479;&#30340;&#23631;&#34109;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;&#29305;&#27530;&#30340;MASK&#26631;&#35760;&#26469;&#25552;&#31034;&#27169;&#22411;&#20174;&#21608;&#22260;&#21333;&#35789;&#20013;&#25910;&#38598;&#24773;&#22659;&#20449;&#24687;&#20197;&#24674;&#22797;&#21407;&#26412;&#38544;&#34255;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20219;&#21153;&#29305;&#23450;&#30340;&#23631;&#34109;&#26694;&#26550;&#65292;&#20197;&#22312;GLUE&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#21331;&#36234;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;&#25105;&#20204;&#22522;&#20110;&#35760;&#21495;&#36755;&#20837;&#26799;&#24230;&#24320;&#21457;&#20102;&#33258;&#24049;&#30340;&#23631;&#34109;&#31639;&#27861;Typhoon&#65292;&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#26631;&#20934;&#22522;&#32447;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;Typhoon&#22312;MRPC&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#19982;&#25972;&#20307;&#23383;&#23631;&#34109;&#30456;&#24403;&#12290;&#25105;&#20204;&#30340;&#23454;&#29616;&#21487;&#20197;&#22312;&#20844;&#20849;Github&#24211;&#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Through exploiting a high level of parallelism enabled by graphics processing units, transformer architectures have enabled tremendous strides forward in the field of natural language processing. In a traditional masked language model, special MASK tokens are used to prompt our model to gather contextual information from surrounding words to restore originally hidden information. In this paper, we explore a task-specific masking framework for pre-trained large language models that enables superior performance on particular downstream tasks on the datasets in the GLUE benchmark. We develop our own masking algorithm, Typhoon, based on token input gradients, and compare this with other standard baselines. We find that Typhoon offers performance competitive with whole-word masking on the MRPC dataset. Our implementation can be found in a public Github Repository.
&lt;/p&gt;</description></item><item><title>EMShepherd&#20351;&#29992;&#30005;&#30913;&#25512;&#29702;&#30340;&#36752;&#23556;&#36319;&#36394;&#24182;&#21033;&#29992;&#23427;&#20204;&#29992;&#20110;&#23545;&#25239;&#24615;&#26816;&#27979;&#12290;&#21482;&#38656;&#20351;&#29992;&#33391;&#24615;&#26679;&#26412;&#21450;&#20854;EM&#36319;&#36394;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#36991;&#20813;&#23545;&#27169;&#22411;&#26412;&#36523;&#30340;&#28145;&#20837;&#20102;&#35299;&#12290;</title><link>http://arxiv.org/abs/2303.15571</link><description>&lt;p&gt;
EMShepherd&#65306;&#36890;&#36807;&#20391;&#20449;&#36947;&#27844;&#28431;&#26816;&#27979;&#23545;&#25239;&#24615;&#31034;&#20363;
&lt;/p&gt;
&lt;p&gt;
EMShepherd: Detecting Adversarial Samples via Side-channel Leakage. (arXiv:2303.15571v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15571
&lt;/p&gt;
&lt;p&gt;
EMShepherd&#20351;&#29992;&#30005;&#30913;&#25512;&#29702;&#30340;&#36752;&#23556;&#36319;&#36394;&#24182;&#21033;&#29992;&#23427;&#20204;&#29992;&#20110;&#23545;&#25239;&#24615;&#26816;&#27979;&#12290;&#21482;&#38656;&#20351;&#29992;&#33391;&#24615;&#26679;&#26412;&#21450;&#20854;EM&#36319;&#36394;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#36991;&#20813;&#23545;&#27169;&#22411;&#26412;&#36523;&#30340;&#28145;&#20837;&#20102;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#24433;&#21709;-&#23567;&#30340;&#26356;&#25913;&#26159;&#26377;&#24847;&#21046;&#20316;&#30340;&#65292;&#20197;&#20351;&#36755;&#20837;&#38169;&#35823;&#22320;&#23548;&#33268;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#23545;&#25239;&#24615;&#25915;&#20987;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#36171;&#33021;&#30340;&#20851;&#38190;&#24212;&#29992;&#26377;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#21644;&#26816;&#27979;&#25216;&#26415;&#37117;&#38656;&#35201;&#23545;&#27169;&#22411;&#12289;&#27979;&#35797;&#36755;&#20837;&#29978;&#33267;&#25191;&#34892;&#32454;&#33410;&#26377;&#28145;&#20837;&#30340;&#20102;&#35299;&#12290;&#23427;&#20204;&#23545;&#20110;&#20351;&#29992;&#20154;&#21592;&#26469;&#35828;&#26159;&#19981;&#21487;&#34892;&#30340;&#40657;&#30418;&#22330;&#26223;&#20013;&#30340;&#19968;&#33324;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;&#12290;&#21463;&#21040;&#30005;&#30913;&#65288;EM&#65289;&#25512;&#29702;&#30340;&#36752;&#23556;&#26082;&#21462;&#20915;&#20110;&#25805;&#20316;&#21644;&#25968;&#25454;&#65292;&#21448;&#21487;&#33021;&#21253;&#21547;&#19981;&#21516;&#36755;&#20837;&#31867;&#30340;&#30165;&#36857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;-EMShepherd&#65292;&#25429;&#33719;&#27169;&#22411;&#25191;&#34892;&#30340;EM&#36319;&#36394;&#12289;&#36827;&#34892;&#36319;&#36394;&#22788;&#29702;&#24182;&#21033;&#29992;&#23427;&#20204;&#29992;&#20110;&#23545;&#25239;&#24615;&#26816;&#27979;&#12290;&#21482;&#26377;&#33391;&#24615;&#26679;&#26412;&#21450;&#20854;EM&#36319;&#36394;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#23545;&#25239;&#24615;&#26816;&#27979;&#22120;&#65306;&#19968;&#32452;EM&#20998;&#31867;&#22120;&#21644;&#29305;&#23450;&#31867;&#21035;&#30340;&#26080;&#30417;&#30563;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNN) are vulnerable to adversarial perturbations-small changes crafted deliberately on the input to mislead the model for wrong predictions. Adversarial attacks have disastrous consequences for deep learning-empowered critical applications. Existing defense and detection techniques both require extensive knowledge of the model, testing inputs, and even execution details. They are not viable for general deep learning implementations where the model internal is unknown, a common 'black-box' scenario for model users. Inspired by the fact that electromagnetic (EM) emanations of a model inference are dependent on both operations and data and may contain footprints of different input classes, we propose a framework, EMShepherd, to capture EM traces of model execution, perform processing on traces and exploit them for adversarial detection. Only benign samples and their EM traces are used to train the adversarial detector: a set of EM classifiers and class-specific unsup
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40654;&#26364;&#27969;&#24418;&#30340;&#20998;&#23376;&#35856;&#27874;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#20998;&#23376;&#34920;&#38754;&#30340;&#25289;&#26222;&#25289;&#26031;-&#36125;&#23572;&#29305;&#25289;&#31859;&#29305;&#24449;&#20989;&#25968;&#26469;&#34920;&#31034;&#20998;&#23376;&#65292;&#23454;&#29616;&#20102;&#20998;&#23376;&#20960;&#20309;&#21644;&#21270;&#23398;&#29305;&#24449;&#30340;&#22810;&#20998;&#36776;&#29575;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2303.15520</link><description>&lt;p&gt;
&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#23398;&#20064;&#20998;&#23376;&#35856;&#27874;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Harmonic Molecular Representations on Riemannian Manifold. (arXiv:2303.15520v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40654;&#26364;&#27969;&#24418;&#30340;&#20998;&#23376;&#35856;&#27874;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#20998;&#23376;&#34920;&#38754;&#30340;&#25289;&#26222;&#25289;&#26031;-&#36125;&#23572;&#29305;&#25289;&#31859;&#29305;&#24449;&#20989;&#25968;&#26469;&#34920;&#31034;&#20998;&#23376;&#65292;&#23454;&#29616;&#20102;&#20998;&#23376;&#20960;&#20309;&#21644;&#21270;&#23398;&#29305;&#24449;&#30340;&#22810;&#20998;&#36776;&#29575;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#22312;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#33647;&#29289;&#21457;&#29616;&#30740;&#31350;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#27431;&#20960;&#37324;&#24471;&#31070;&#32463;&#32593;&#32476;&#23545;&#19977;&#32500;&#20998;&#23376;&#32467;&#26500;&#36827;&#34892;&#32534;&#30721;&#24050;&#25104;&#20026;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#31038;&#21306;&#30340;&#20027;&#27969;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#31561;&#21464;&#24615;&#32422;&#26463;&#21644;&#28040;&#24687;&#20256;&#36882;&#21487;&#33021;&#20250;&#38480;&#21046;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35856;&#27874;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#65288;HMR&#65289;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#20998;&#23376;&#34920;&#38754;&#30340;&#25289;&#26222;&#25289;&#26031;-&#36125;&#23572;&#29305;&#25289;&#31859;&#29305;&#24449;&#20989;&#25968;&#26469;&#34920;&#31034;&#20998;&#23376;&#12290;HMR&#22312;2D&#40654;&#26364;&#27969;&#24418;&#19978;&#25552;&#20379;&#20102;&#20998;&#23376;&#20960;&#20309;&#21644;&#21270;&#23398;&#29305;&#24449;&#30340;&#22810;&#20998;&#36776;&#29575;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#35856;&#27874;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#65292;&#22312;&#34920;&#38754;&#27969;&#24418;&#19978;&#23454;&#29616;&#39640;&#25928;&#30340;&#35889;&#28040;&#24687;&#20256;&#36882;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#20998;&#23376;&#32534;&#30721;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#31034;&#20986;&#19982;&#24403;&#21069;&#27169;&#22411;&#22312;&#23567;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#37197;&#20307;&#32467;&#21512;&#20013;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular representation learning plays a crucial role in AI-assisted drug discovery research. Encoding 3D molecular structures through Euclidean neural networks has become the prevailing method in the geometric deep learning community. However, the equivariance constraints and message passing in Euclidean space may limit the network expressive power. In this work, we propose a Harmonic Molecular Representation learning (HMR) framework, which represents a molecule using the Laplace-Beltrami eigenfunctions of its molecular surface. HMR offers a multi-resolution representation of molecular geometric and chemical features on 2D Riemannian manifold. We also introduce a harmonic message passing method to realize efficient spectral message passing over the surface manifold for better molecular encoding. Our proposed method shows comparable predictive power to current models in small molecule property prediction, and outperforms the state-of-the-art deep learning models for ligand-binding pro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25490;&#38431;&#27169;&#22411;&#25506;&#35752;&#20102;&#23558;&#20256;&#32479;&#30340;&#22823;&#22411;&#26381;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#24494;&#26381;&#21153;&#21518;&#25552;&#39640;&#31995;&#32479;&#25928;&#29575;&#30340;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#20998;&#35299;&#21518;&#25152;&#38656;&#24635;&#26102;&#38388;&#27604;&#21407;&#22987;&#26381;&#21153;&#23569;&#65292;&#22240;&#27492;&#20998;&#35299;&#21487;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.15490</link><description>&lt;p&gt;
&#24494;&#26381;&#21153;&#30340;&#25928;&#29575;&#20998;&#26512;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Research on Efficiency Analysis of Microservices. (arXiv:2303.15490v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25490;&#38431;&#27169;&#22411;&#25506;&#35752;&#20102;&#23558;&#20256;&#32479;&#30340;&#22823;&#22411;&#26381;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#24494;&#26381;&#21153;&#21518;&#25552;&#39640;&#31995;&#32479;&#25928;&#29575;&#30340;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#20998;&#35299;&#21518;&#25152;&#38656;&#24635;&#26102;&#38388;&#27604;&#21407;&#22987;&#26381;&#21153;&#23569;&#65292;&#22240;&#27492;&#20998;&#35299;&#21487;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;Web&#26381;&#21153;&#12289;&#23481;&#22120;&#21644;&#20113;&#35745;&#31639;&#25216;&#26415;&#30340;&#25104;&#29087;&#65292;&#20256;&#32479;&#31995;&#32479;&#20013;&#30340;&#22823;&#22411;&#26381;&#21153;&#65288;&#20363;&#22914;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#35745;&#31639;&#26381;&#21153;&#65289;&#27491;&#22312;&#36880;&#28176;&#20998;&#35299;&#20026;&#35768;&#22810;&#24494;&#26381;&#21153;&#65292;&#20197;&#25552;&#39640;&#26381;&#21153;&#30340;&#37325;&#29992;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25490;&#38431;&#27169;&#22411;&#30340;&#25928;&#29575;&#20998;&#26512;&#26694;&#26550;&#65292;&#20197;&#20998;&#26512;&#23558;&#20256;&#32479;&#30340;&#22823;&#22411;&#26381;&#21153;&#20998;&#35299;&#20026;n&#20010;&#24494;&#26381;&#21153;&#30340;&#25928;&#29575;&#24046;&#24322;&#12290;&#20026;&#20102;&#25512;&#24191;&#24212;&#29992;&#65292;&#35813;&#30740;&#31350;&#32771;&#34385;&#20102;&#19981;&#21516;&#30340;&#26381;&#21153;&#26102;&#38388;&#20998;&#24067;&#65288;&#20363;&#22914;&#26381;&#21153;&#26102;&#38388;&#30340;&#25351;&#25968;&#20998;&#24067;&#21644;&#22266;&#23450;&#26381;&#21153;&#26102;&#38388;&#65289;&#65292;&#24182;&#36890;&#36807;&#25490;&#38431;&#27169;&#22411;&#65288;&#21363;M / M / 1&#25490;&#38431;&#27169;&#22411;&#21644;M / D / 1&#25490;&#38431;&#27169;&#22411;&#65289;&#25506;&#32034;&#20102;&#26368;&#22351;&#24773;&#20917;&#21644;&#26368;&#20339;&#24773;&#20917;&#19979;&#30340;&#31995;&#32479;&#25928;&#29575;&#12290;&#22312;&#27599;&#20010;&#23454;&#39564;&#20013;&#65292;&#37117;&#26174;&#31034;&#21407;&#22987;&#22823;&#22411;&#26381;&#21153;&#25152;&#38656;&#30340;&#24635;&#26102;&#38388;&#39640;&#20110;&#23558;&#20854;&#20998;&#35299;&#20026;&#22810;&#20010;&#24494;&#26381;&#21153;&#25152;&#38656;&#30340;&#26102;&#38388;&#65292;&#22240;&#27492;&#23558;&#20854;&#20998;&#35299;&#20026;&#22810;&#20010;&#24494;&#26381;&#21153;&#21487;&#20197;&#25552;&#39640;&#31995;&#32479;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the maturity of web services, containers, and cloud computing technologies, large services in traditional systems (e.g. the computation services of machine learning and artificial intelligence) are gradually being broken down into many microservices to increase service reusability and flexibility. Therefore, this study proposes an efficiency analysis framework based on queuing models to analyze the efficiency difference of breaking down traditional large services into n microservices. For generalization, this study considers different service time distributions (e.g. exponential distribution of service time and fixed service time) and explores the system efficiency in the worst-case and best-case scenarios through queuing models (i.e. M/M/1 queuing model and M/D/1 queuing model). In each experiment, it was shown that the total time required for the original large service was higher than that required for breaking it down into multiple microservices, so breaking it down into multip
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#24212;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#33410;&#28857;&#30340;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#38024;&#23545;&#38081;&#36335;&#32593;&#32476;&#19978;&#30340;&#21015;&#36710;&#24310;&#35823;&#28436;&#21270;&#36827;&#34892;&#30740;&#31350;&#12290;&#20351;&#29992;HetGNN&#21644;GraphSAGE&#30340;&#32452;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SAGE-Het&#30340;&#22270;&#24418;&#26550;&#26500;&#65292;&#21487;&#20197;&#22522;&#20110;&#19981;&#21516;&#30340;&#36793;&#25429;&#25417;&#21015;&#36710;&#12289;&#21015;&#36710;&#12289;&#31449;&#28857;&#20197;&#21450;&#31449;&#28857;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#20248;&#20110;&#20256;&#32479;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.15489</link><description>&lt;p&gt;
&#38081;&#36335;&#32593;&#32476;&#24310;&#35823;&#28436;&#21270;&#65306;&#19968;&#31181;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Railway Network Delay Evolution: A Heterogeneous Graph Neural Network Approach. (arXiv:2303.15489v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#24212;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#33410;&#28857;&#30340;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#38024;&#23545;&#38081;&#36335;&#32593;&#32476;&#19978;&#30340;&#21015;&#36710;&#24310;&#35823;&#28436;&#21270;&#36827;&#34892;&#30740;&#31350;&#12290;&#20351;&#29992;HetGNN&#21644;GraphSAGE&#30340;&#32452;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SAGE-Het&#30340;&#22270;&#24418;&#26550;&#26500;&#65292;&#21487;&#20197;&#22522;&#20110;&#19981;&#21516;&#30340;&#36793;&#25429;&#25417;&#21015;&#36710;&#12289;&#21015;&#36710;&#12289;&#31449;&#28857;&#20197;&#21450;&#31449;&#28857;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#20248;&#20110;&#20256;&#32479;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38081;&#36335;&#36816;&#33829;&#28041;&#21450;&#19981;&#21516;&#31867;&#22411;&#30340;&#23454;&#20307;&#65288;&#31449;&#28857;&#65292;&#21015;&#36710;&#31561;&#65289;&#65292;&#29616;&#26377;&#30340;&#21516;&#36136;&#33410;&#28857;&#65288;&#21363;&#30456;&#21516;&#31867;&#22411;&#30340;&#33410;&#28857;&#65289;&#30340;&#22270;/&#32593;&#32476;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HetGNN&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#24212;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#33410;&#28857;&#65288;&#21363;&#24322;&#26500;&#33410;&#28857;&#65289;&#65292;&#20197;&#30740;&#31350;&#38081;&#36335;&#32593;&#32476;&#19978;&#30340;&#21015;&#36710;&#24310;&#35823;&#28436;&#21270;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;HetGNN&#27169;&#22411;&#21644;GraphSAGE&#21516;&#36136;GNN&#65288;HomoGNN&#65289;&#30340;&#22270;&#24418;&#26550;&#26500;&#65292;&#31216;&#20026;SAGE-Het&#65292;&#26088;&#22312;&#22522;&#20110;&#19981;&#21516;&#30340;&#36793;&#25429;&#25417;&#21015;&#36710;&#65292;&#21015;&#36710;&#12289;&#31449;&#28857;&#20197;&#21450;&#31449;&#28857;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#35201;&#27714;&#36755;&#20837;&#20855;&#26377;&#24658;&#23450;&#30340;&#32500;&#24230;&#65288;&#20363;&#22914;&#22312;&#30697;&#24418;&#25110;&#31867;&#20284;&#32593;&#26684;&#30340;&#25968;&#32452;&#20013;&#65289;&#25110;&#20165;&#20801;&#35768;&#22312;&#22270;&#20013;&#20351;&#29992;&#21516;&#36136;&#33410;&#28857;&#30456;&#27604;&#65292;SAGE-Het&#20801;&#35768;&#28789;&#27963;&#30340;&#36755;&#20837;&#21644;&#24322;&#26500;&#33410;&#28857;&#12290;&#25910;&#38598;&#20013;&#22269;&#21271;&#20140;&#24191;&#24030;&#32447;&#19978;&#20004;&#20010;&#31449;&#28857;&#30340;&#25968;&#25454;&#20197;&#39564;&#35777;HetGNN&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;SAGE-Het&#27169;&#22411;&#22312;&#39044;&#27979;&#21015;&#36710;&#24310;&#35823;&#28436;&#21270;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#27169;&#22411;&#65292;&#20363;&#22914;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;HomoGNN&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Railway operations involve different types of entities (stations, trains, etc.), making the existing graph/network models with homogenous nodes (i.e., the same kind of nodes) incapable of capturing the interactions between the entities. This paper aims to develop a heterogeneous graph neural network (HetGNN) model, which can address different types of nodes (i.e., heterogeneous nodes), to investigate the train delay evolution on railway networks. To this end, a graph architecture combining the HetGNN model and the GraphSAGE homogeneous GNN (HomoGNN), called SAGE-Het, is proposed. The aim is to capture the interactions between trains, trains and stations, and stations and other stations on delay evolution based on different edges. In contrast to the traditional methods that require the inputs to have constant dimensions (e.g., in rectangular or grid-like arrays) or only allow homogeneous nodes in the graph, SAGE-Het allows for flexible inputs and heterogeneous nodes. The data from two s
&lt;/p&gt;</description></item><item><title>KeGNN&#26159;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#21487;&#20197;&#32467;&#21512;&#20808;&#21069;&#30340;&#30693;&#35782;&#26469;&#20248;&#21270;&#22270;&#25968;&#25454;&#19978;&#30340;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.15487</link><description>&lt;p&gt;
&#30693;&#35782;&#22686;&#24378;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Knowledge Enhanced Graph Neural Networks. (arXiv:2303.15487v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15487
&lt;/p&gt;
&lt;p&gt;
KeGNN&#26159;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#21487;&#20197;&#32467;&#21512;&#20808;&#21069;&#30340;&#30693;&#35782;&#26469;&#20248;&#21270;&#22270;&#25968;&#25454;&#19978;&#30340;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#25968;&#25454;&#26159;&#26080;&#22788;&#19981;&#22312;&#30340;&#65292;&#24182;&#19988;&#20855;&#26377;&#21508;&#31181;&#24212;&#29992;&#65292;&#20363;&#22914;&#33258;&#28982;&#31185;&#23398;&#12289;&#31038;&#20132;&#32593;&#32476;&#25110;&#35821;&#20041;&#32593;&#12290;&#23613;&#31649;&#23500;&#21547;&#20449;&#24687;&#65292;&#20294;&#22270;&#24418;&#36890;&#24120;&#22122;&#22768;&#21644;&#19981;&#23436;&#25972;&#12290;&#22240;&#27492;&#65292;&#22270;&#34917;&#20840;&#20219;&#21153;&#65292;&#22914;&#33410;&#28857;&#20998;&#31867;&#25110;&#38142;&#25509;&#39044;&#27979;&#65292;&#24050;&#32463;&#21463;&#21040;&#20851;&#27880;&#12290;&#19968;&#26041;&#38754;&#65292;&#31070;&#32463;&#26041;&#27861;&#65288;&#22914;&#22270;&#31070;&#32463;&#32593;&#32476;&#65289;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#22788;&#29702;&#22122;&#22768;&#22270;&#30340;&#31283;&#20581;&#24037;&#20855;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#31526;&#21495;&#26041;&#27861;&#21487;&#20197;&#23545;&#22270;&#36827;&#34892;&#31934;&#30830;&#25512;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;KeGNN&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#22270;&#25968;&#25454;&#19978;&#23398;&#20064;&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#20004;&#31181;&#33539;&#20363;&#65292;&#24182;&#20801;&#35768;&#23558;&#20808;&#21069;&#30340;&#30693;&#35782;&#38598;&#25104;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#12290;&#20174;&#26412;&#36136;&#19978;&#35762;&#65292;KeGNN&#30001;&#19968;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#32452;&#25104;&#65292;&#20854;&#20013;&#22522;&#20110;&#30446;&#26631;&#23558;&#30693;&#35782;&#22686;&#24378;&#23618;&#22534;&#21472;&#22312;&#20854;&#19978;&#65292;&#20197;&#20351;&#38024;&#23545;&#20808;&#21069;&#30693;&#35782;&#30340;&#39044;&#27979;&#24471;&#21040;&#20248;&#21270;&#12290;&#25105;&#20204;&#23558;KeGNN&#19982;&#20004;&#20010;&#26631;&#20934;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#19968;&#36215;&#23454;&#20363;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#20808;&#21069;&#30340;&#30693;&#35782;&#38598;&#25104;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#21487;&#20197;&#25552;&#39640;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph data is omnipresent and has a large variety of applications such as natural science, social networks or semantic web. Though rich in information, graphs are often noisy and incomplete. Therefore, graph completion tasks such as node classification or link prediction have gained attention. On the one hand, neural methods such as graph neural networks have proven to be robust tools for learning rich representations of noisy graphs. On the other hand, symbolic methods enable exact reasoning on graphs. We propose KeGNN, a neuro-symbolic framework for learning on graph data that combines both paradigms and allows for the integration of prior knowledge into a graph neural network model. In essence, KeGNN consists of a graph neural network as a base on which knowledge enhancement layers are stacked with the objective of refining predictions with respect to prior knowledge. We instantiate KeGNN in conjunction with two standard graph neural networks: Graph Convolutional Networks and Graph 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#30340;&#21361;&#23475;&#20998;&#26512;&#20013;&#24212;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;LLMs&#21487;&#33021;&#26377;&#21161;&#20110;&#25903;&#25345;&#20998;&#26512;&#24072;&#36827;&#34892;&#21361;&#23475;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2303.15473</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33021;&#21327;&#21161;&#21361;&#23475;&#20998;&#26512;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models assist in Hazard Analysis?. (arXiv:2303.15473v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#30340;&#21361;&#23475;&#20998;&#26512;&#20013;&#24212;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;LLMs&#21487;&#33021;&#26377;&#21161;&#20110;&#25903;&#25345;&#20998;&#26512;&#24072;&#36827;&#34892;&#21361;&#23475;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-3&#65292;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#24182;&#24050;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#20363;&#22914;&#28304;&#20195;&#30721;&#29983;&#25104;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;LLMs&#38598;&#25104;&#21040;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#30340;&#21361;&#23475;&#20998;&#26512;&#20013;&#30340;&#28508;&#21147;&#65292;&#36825;&#20010;&#36807;&#31243;&#34987;&#25105;&#20204;&#31216;&#20026;&#21327;&#21516;&#21361;&#23475;&#20998;&#26512;&#65288;CoHA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as GPT-3, have demonstrated remarkable natural language processing and generation capabilities and have been applied to a variety tasks, such as source code generation. This paper explores the potential of integrating LLMs in the hazard analysis for safety-critical systems, a process which we refer to as co-hazard analysis (CoHA). In CoHA, a human analyst interacts with an LLM via a context-aware chat session and uses the responses to support elicitation of possible hazard causes. In this experiment, we explore CoHA with three increasingly complex versions of a simple system, using Open AI's ChatGPT service. The quality of ChatGPT's responses were systematically assessed to determine the feasibility of CoHA given the current state of LLM technology. The results suggest that LLMs may be useful for supporting human analysts performing hazard analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#26377;&#30417;&#30563;&#30340;&#25513;&#34109;&#30693;&#35782;&#33976;&#39311;&#27169;&#22411;&#65288;SMKD&#65289;&#65292;&#22312;&#23569;&#37327;&#26631;&#27880;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#26631;&#31614;&#20449;&#24687;&#34701;&#20837;&#21040;&#33258;&#33976;&#39311;&#26694;&#26550;&#20013;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;Transformer&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#36807;&#25311;&#21512;&#21644;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.15466</link><description>&lt;p&gt;
&#26377;&#30417;&#30563;&#30340;&#25513;&#34109;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#23569;&#26679;&#26412;Transformer
&lt;/p&gt;
&lt;p&gt;
Supervised Masked Knowledge Distillation for Few-Shot Transformers. (arXiv:2303.15466v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#26377;&#30417;&#30563;&#30340;&#25513;&#34109;&#30693;&#35782;&#33976;&#39311;&#27169;&#22411;&#65288;SMKD&#65289;&#65292;&#22312;&#23569;&#37327;&#26631;&#27880;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#26631;&#31614;&#20449;&#24687;&#34701;&#20837;&#21040;&#33258;&#33976;&#39311;&#26694;&#26550;&#20013;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;Transformer&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#36807;&#25311;&#21512;&#21644;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#21033;&#29992;&#23616;&#37096;&#29305;&#24449;&#25429;&#25417;&#36828;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#38024;&#23545;&#23545;&#23569;&#26679;&#26412;&#23398;&#20064;&#36827;&#34892;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21482;&#26377;&#26497;&#23569;&#26631;&#27880;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#26469;&#35828;&#65292;&#30001;&#20110;&#32570;&#23569;CNN&#24335;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;ViT&#23481;&#26131;&#36807;&#25311;&#21512;&#24182;&#19988;&#24615;&#33021;&#20005;&#37325;&#19979;&#38477;&#12290;&#20197;&#21069;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#24037;&#20316;&#65292;&#35201;&#20040;&#36890;&#36807;&#36741;&#21161;&#33258;&#30417;&#30563;&#25439;&#22833;&#26469;&#36991;&#20813;&#36825;&#31181;&#38382;&#39064;&#65292;&#35201;&#20040;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#30340;&#26631;&#31614;&#20449;&#24687;&#26469;&#36991;&#20813;&#12290;&#20294;&#26159;&#33258;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#30340;&#23569;&#26679;&#26412;Transformer&#20043;&#38388;&#30340;&#24046;&#36317;&#20173;&#26410;&#22635;&#34917;&#12290;&#25105;&#20204;&#21463;&#21040;&#26368;&#36817;&#33258;&#30417;&#30563;&#30693;&#35782;&#33976;&#39311;&#21644;&#25513;&#34109;&#22270;&#20687;&#24314;&#27169;&#30340;&#36827;&#23637;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;Supervised Masked Knowledge Distillation&#27169;&#22411;&#65288;SMKD&#65289;&#29992;&#20110;Transformer&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#23558;&#26631;&#31614;&#20449;&#24687;&#34701;&#20837;&#21040;&#33258;&#33976;&#39311;&#26694;&#26550;&#20013;&#12290;&#19982;&#20197;&#21069;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#20801;&#35768;&#31867;&#20869;&#30693;&#35782;&#27969;&#21160;&#65292;&#24182;&#26377;&#25928;&#21033;&#29992;&#30417;&#30563;&#20449;&#21495;&#23545;&#27169;&#22411;&#36755;&#20986;&#36827;&#34892;&#33258;&#28982;&#32422;&#26463;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#20197;&#21069;&#33258;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViTs) emerge to achieve impressive performance on many data-abundant computer vision tasks by capturing long-range dependencies among local features. However, under few-shot learning (FSL) settings on small datasets with only a few labeled data, ViT tends to overfit and suffers from severe performance degradation due to its absence of CNN-alike inductive bias. Previous works in FSL avoid such problem either through the help of self-supervised auxiliary losses, or through the dextile uses of label information under supervised settings. But the gap between self-supervised and supervised few-shot Transformers is still unfilled. Inspired by recent advances in self-supervised knowledge distillation and masked image modeling (MIM), we propose a novel Supervised Masked Knowledge Distillation model (SMKD) for few-shot Transformers which incorporates label information into self-distillation frameworks. Compared with previous self-supervised methods, we allow intra-class kno
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31867;&#22411;&#30340;&#25688;&#35201;&#65292;&#23558;&#20256;&#32479;&#32858;&#21512;&#30340;&#20248;&#28857;&#19982;&#36890;&#36807;&#22797;&#26434;&#25968;&#25454;&#34920;&#31034;&#20445;&#30041;&#26356;&#22810;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#31934;&#30830;&#21512;&#24182;&#65292;&#26082;&#33021;&#20445;&#25345;&#31934;&#24230;&#21448;&#33021;&#20943;&#23569;&#25968;&#25454;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2303.15465</link><description>&lt;p&gt;
&#31934;&#30830;&#21487;&#21512;&#24182;&#30340;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Exactly mergeable summaries. (arXiv:2303.15465v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31867;&#22411;&#30340;&#25688;&#35201;&#65292;&#23558;&#20256;&#32479;&#32858;&#21512;&#30340;&#20248;&#28857;&#19982;&#36890;&#36807;&#22797;&#26434;&#25968;&#25454;&#34920;&#31034;&#20445;&#30041;&#26356;&#22810;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#31934;&#30830;&#21512;&#24182;&#65292;&#26082;&#33021;&#20445;&#25345;&#31934;&#24230;&#21448;&#33021;&#20943;&#23569;&#25968;&#25454;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#20013;&#65292;&#32858;&#21512;&#26159;&#20943;&#23569;&#25968;&#25454;&#22823;&#23567;&#65288;&#22797;&#26434;&#24615;&#65289;&#30340;&#26631;&#20934;&#26041;&#27861;&#12290;&#25968;&#25454;&#20998;&#26512;&#31243;&#24207;&#25552;&#20379;&#20102;&#19981;&#21516;&#30340;&#32858;&#21512;&#20989;&#25968;&#12290;&#20256;&#32479;&#32858;&#21512;&#30340;&#38382;&#39064;&#22312;&#20110;&#24448;&#24448;&#20250;&#20002;&#22833;&#22826;&#22810;&#20449;&#24687;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#32467;&#26524;&#30340;&#31934;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31867;&#22411;&#30340;&#25688;&#35201;&#65292;&#23427;&#23558;&#20256;&#32479;&#32858;&#21512;&#30340;&#20248;&#28857;&#19982;&#36890;&#36807;&#22797;&#26434;&#25968;&#25454;&#34920;&#31034;&#20445;&#30041;&#26356;&#22810;&#20449;&#24687;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#23545;&#22797;&#26434;&#25688;&#35201;&#36827;&#34892;&#31934;&#30830;&#21512;&#24182;&#65292;&#24182;&#20445;&#25345;&#21407;&#22987;&#25968;&#25454;&#30340;&#31934;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#25968;&#25454;&#20998;&#26512;&#20013;&#22797;&#26434;&#32858;&#21512;&#25688;&#35201;&#30340;&#29702;&#35770;&#22522;&#30784;&#30340;&#24320;&#21457;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the analysis of large/big data sets, aggregation (replacing values of a variable over a group by a single value) is a standard way of reducing the size (complexity) of the data. Data analysis programs provide different aggregation functions.  Recently some books dealing with the theoretical and algorithmic background of traditional aggregation functions were published. A problem with traditional aggregation is that often too much information is discarded thus reducing the precision of the obtained results. A much better, preserving more information, summarization of original data can be achieved by representing aggregated data using selected types of complex data.  In complex data analysis the measured values over a selected group $A$ are aggregated into a complex object $\Sigma(A)$ and not into a single value. Most of the aggregation functions theory does not apply directly. In our contribution, we present an attempt to start building a theoretical background of complex aggregation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#28041;&#21450;&#22521;&#35757;&#12289;&#25512;&#29702;&#12289;&#19968;&#33324;&#21270;&#36793;&#30028;&#21644;&#20248;&#21270;&#38382;&#39064;&#30340;&#19968;&#32452;&#25968;&#23398;&#25361;&#25112;&#65292;&#20026;&#25968;&#23398;&#23478;&#12289;&#32479;&#35745;&#23398;&#23478;&#21644;&#29702;&#35770;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#25552;&#20379;&#20102;&#19982;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#20132;&#27969;&#30340;&#24418;&#24335;&#21270;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2303.15464</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#25968;&#23398;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Mathematical Challenges in Deep Learning. (arXiv:2303.15464v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#28041;&#21450;&#22521;&#35757;&#12289;&#25512;&#29702;&#12289;&#19968;&#33324;&#21270;&#36793;&#30028;&#21644;&#20248;&#21270;&#38382;&#39064;&#30340;&#19968;&#32452;&#25968;&#23398;&#25361;&#25112;&#65292;&#20026;&#25968;&#23398;&#23478;&#12289;&#32479;&#35745;&#23398;&#23478;&#21644;&#29702;&#35770;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#25552;&#20379;&#20102;&#19982;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#20132;&#27969;&#30340;&#24418;&#24335;&#21270;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;2012&#24180;&#30340;ImageNet&#25361;&#25112;&#20197;&#26469;&#65292;&#28145;&#24230;&#27169;&#22411;&#24050;&#32463;&#20027;&#23472;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#12290;&#28145;&#24230;&#27169;&#22411;&#30340;&#22823;&#23567;&#20174;&#37027;&#26102;&#36215;&#19968;&#30452;&#22312;&#22686;&#21152;&#65292;&#36825;&#32473;&#22312;&#25163;&#26426;&#12289;&#20010;&#20154;&#30005;&#33041;&#12289;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#21644;&#26080;&#32447;&#22522;&#31449;&#31561;&#39046;&#22495;&#24212;&#29992;&#30340;&#36825;&#19968;&#39046;&#22495;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#21015;&#20986;&#20102;&#19968;&#32452;&#38382;&#39064;&#65292;&#28085;&#30422;&#22521;&#35757;&#12289;&#25512;&#29702;&#12289;&#19968;&#33324;&#21270;&#36793;&#30028;&#21644;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#29992;&#19968;&#20123;&#24418;&#24335;&#21270;&#35821;&#35328;&#26469;&#19982;&#25968;&#23398;&#23478;&#12289;&#32479;&#35745;&#23398;&#23478;&#21644;&#29702;&#35770;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#20132;&#27969;&#36825;&#20123;&#25361;&#25112;&#12290;&#36825;&#26159;&#23545;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#38382;&#39064;&#30340;&#20027;&#35266;&#30475;&#27861;&#65292;&#23427;&#26377;&#30410;&#20110;&#38271;&#26399;&#30340;&#25216;&#26415;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep models are dominating the artificial intelligence (AI) industry since the ImageNet challenge in 2012. The size of deep models is increasing ever since, which brings new challenges to this field with applications in cell phones, personal computers, autonomous cars, and wireless base stations. Here we list a set of problems, ranging from training, inference, generalization bound, and optimization with some formalism to communicate these challenges with mathematicians, statisticians, and theoretical computer scientists. This is a subjective view of the research questions in deep learning that benefits the tech industry in long run.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#30830;&#23450;&#24615;&#8221;&#21644;&#8220;&#19981;&#30830;&#23450;&#24615;&#8221;&#30340;&#24471;&#20998;&#26041;&#27861;&#26469;&#37327;&#21270;&#20998;&#31867;&#20915;&#31574;&#20013;&#39044;&#27979;&#30340;&#36136;&#37327;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.14568</link><description>&lt;p&gt;
&#37327;&#21270;&#20998;&#31867;&#20915;&#31574;&#30340;&#30830;&#23450;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;
&lt;/p&gt;
&lt;p&gt;
Measuring Classification Decision Certainty and Doubt. (arXiv:2303.14568v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14568
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#30830;&#23450;&#24615;&#8221;&#21644;&#8220;&#19981;&#30830;&#23450;&#24615;&#8221;&#30340;&#24471;&#20998;&#26041;&#27861;&#26469;&#37327;&#21270;&#20998;&#31867;&#20915;&#31574;&#20013;&#39044;&#27979;&#30340;&#36136;&#37327;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#23450;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#23450;&#37327;&#34920;&#24449;&#21644;&#20272;&#35745;&#22312;&#20248;&#21270;&#21644;&#20915;&#31574;&#36807;&#31243;&#20013;&#20855;&#26377;&#22522;&#30784;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#30452;&#35266;&#30340;&#24471;&#20998;&#65292;&#31216;&#20026;&#8220;&#30830;&#23450;&#24615;&#8221;&#21644;&#8220;&#19981;&#30830;&#23450;&#24615;&#8221;&#65292;&#21487;&#22312;&#36125;&#21494;&#26031;&#21644;&#39057;&#29575;&#20027;&#20041;&#26694;&#26550;&#19979;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#65288;&#22810;&#65289;&#20998;&#31867;&#20915;&#31574;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#30340;&#39044;&#27979;&#36136;&#37327;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantitative characterizations and estimations of uncertainty are of fundamental importance in optimization and decision-making processes. Herein, we propose intuitive scores, which we call \textit{certainty} and \textit{doubt}, that can be used in both a Bayesian and frequentist framework to assess and compare the quality and uncertainty of predictions in (multi-)classification decision machine learning problems.
&lt;/p&gt;</description></item><item><title>Fantasia3D&#26159;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#29983;&#25104;3D&#20869;&#23481;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#31163;&#20960;&#20309;&#21644;&#22806;&#35266;&#24314;&#27169;&#21644;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#20960;&#20309;&#32454;&#33410;&#21644;&#36924;&#30495;&#28210;&#26579;&#65292;&#24182;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2303.13873</link><description>&lt;p&gt;
Fantasia3D: &#29992;&#20110;&#39640;&#36136;&#37327;&#25991;&#26412;&#29983;&#25104;3D&#20869;&#23481;&#30340;&#20960;&#20309;&#21644;&#22806;&#35266;&#20998;&#31163;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation. (arXiv:2303.13873v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13873
&lt;/p&gt;
&lt;p&gt;
Fantasia3D&#26159;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#29983;&#25104;3D&#20869;&#23481;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#31163;&#20960;&#20309;&#21644;&#22806;&#35266;&#24314;&#27169;&#21644;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#20960;&#20309;&#32454;&#33410;&#21644;&#36924;&#30495;&#28210;&#26579;&#65292;&#24182;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#25552;&#20379;&#65292;&#33258;&#21160;3D&#20869;&#23481;&#30340;&#21019;&#24314;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#24418;&#25104;&#20102;&#25991;&#26412;&#29983;&#25104;3D&#20869;&#23481;&#30340;&#26032;&#20852;&#35805;&#39064;&#12290;&#29616;&#26377;&#30340;&#25991;&#26412;&#29983;&#25104;3D&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#38544;&#24335;&#22330;&#26223;&#34920;&#31034;&#65292;&#36825;&#20123;&#34920;&#31034;&#20351;&#29992;&#20307;&#31215;&#28210;&#26579;&#23558;&#20960;&#20309;&#21644;&#22806;&#35266;&#32806;&#21512;&#22312;&#19968;&#36215;&#65292;&#23545;&#20110;&#24674;&#22797;&#26356;&#31934;&#32454;&#30340;&#20960;&#20309;&#21644;&#23454;&#29616;&#29031;&#29255;&#33324;&#36924;&#30495;&#30340;&#28210;&#26579;&#26159;&#27425;&#20248;&#30340;&#65307;&#22240;&#27492;&#65292;&#23427;&#20204;&#22312;&#20135;&#29983;&#39640;&#36136;&#37327;3D&#36164;&#20135;&#26041;&#38754;&#19981;&#22815;&#26377;&#25928;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fantasia3D&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#36136;&#37327;&#25991;&#26412;&#29983;&#25104;3D&#20869;&#23481;&#12290;Fantasia3D&#30340;&#20851;&#38190;&#22312;&#20110;&#20960;&#20309;&#21644;&#22806;&#35266;&#30340;&#20998;&#31163;&#24314;&#27169;&#21644;&#23398;&#20064;&#12290;&#23545;&#20110;&#20960;&#20309;&#23398;&#20064;&#65292;&#25105;&#20204;&#20381;&#38752;&#28151;&#21512;&#22330;&#26223;&#34920;&#31034;&#65292;&#24182;&#24314;&#35758;&#23558;&#20174;&#34920;&#31034;&#20013;&#25552;&#21462;&#30340;&#34920;&#38754;&#27861;&#32447;&#32534;&#30721;&#20316;&#20026;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#23545;&#20110;&#22806;&#35266;&#24314;&#27169;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31354;&#38388;&#21487;&#21464;&#21452;&#21521;&#21453;&#23556;&#29575;&#20998;&#24067;&#20989;&#25968;&#65288;SVBRDF&#65289;&#26469;&#20998;&#31163;&#26448;&#26009;&#21644;&#20809;&#29031;&#23646;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20309;&#32454;&#33410;&#21644;&#36924;&#30495;&#28210;&#26579;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#25216;&#26415;&#65292;&#24182;&#19988;&#22312;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;3D&#20869;&#23481;&#26041;&#38754;&#26356;&#20026;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic 3D content creation has achieved rapid progress recently due to the availability of pre-trained, large language models and image diffusion models, forming the emerging topic of text-to-3D content creation. Existing text-to-3D methods commonly use implicit scene representations, which couple the geometry and appearance via volume rendering and are suboptimal in terms of recovering finer geometries and achieving photorealistic rendering; consequently, they are less effective for generating high-quality 3D assets. In this work, we propose a new method of Fantasia3D for high-quality text-to-3D content creation. Key to Fantasia3D is the disentangled modeling and learning of geometry and appearance. For geometry learning, we rely on a hybrid scene representation, and propose to encode surface normal extracted from the representation as the input of the image diffusion model. For appearance modeling, we introduce the spatially varying bidirectional reflectance distribution function 
&lt;/p&gt;</description></item><item><title>&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#31181;&#20943;&#23569;&#35797;&#38169;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#31034;&#33539;&#65292;&#26412;&#25991;&#32508;&#36848;&#20102;&#22914;&#20309;&#20351;&#29992;&#31034;&#33539;&#26469;&#20419;&#36827;&#23398;&#20064;&#20915;&#31574;&#27169;&#22411;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;ManiSkill&#26426;&#22120;&#20154;&#23398;&#20064;&#22522;&#20934;&#30340;&#31034;&#33539;&#29983;&#25104;&#21644;&#21033;&#29992;&#31649;&#36947;&#30340;&#23454;&#20363;&#12290;</title><link>http://arxiv.org/abs/2303.13489</link><description>&lt;p&gt;
&#20351;&#29992;&#31034;&#33539;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#19982;&#35268;&#21010;&#65306;&#19968;&#20221;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Boosting Reinforcement Learning and Planning with Demonstrations: A Survey. (arXiv:2303.13489v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13489
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#31181;&#20943;&#23569;&#35797;&#38169;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#31034;&#33539;&#65292;&#26412;&#25991;&#32508;&#36848;&#20102;&#22914;&#20309;&#20351;&#29992;&#31034;&#33539;&#26469;&#20419;&#36827;&#23398;&#20064;&#20915;&#31574;&#27169;&#22411;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;ManiSkill&#26426;&#22120;&#20154;&#23398;&#20064;&#22522;&#20934;&#30340;&#31034;&#33539;&#29983;&#25104;&#21644;&#21033;&#29992;&#31649;&#36947;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#26368;&#36817;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#26159;&#36825;&#31181;&#35797;&#38169;&#24335;&#30340;&#23398;&#20064;&#26041;&#27861;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#21487;&#33021;&#25928;&#29575;&#20302;&#19979;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#20351;&#29992;&#31034;&#33539;&#21487;&#20197;&#35753;&#26234;&#33021;&#20307;&#21463;&#30410;&#20110;&#19987;&#23478;&#30340;&#30693;&#35782;&#65292;&#32780;&#26080;&#38656;&#25506;&#32034;&#26368;&#20339;&#34892;&#21160;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#39034;&#24207;&#20915;&#31574;&#20013;&#20351;&#29992;&#31034;&#33539;&#30340;&#20248;&#28857;&#65292;&#20197;&#21450;&#23398;&#20064;&#20026;&#22522;&#30784;&#30340;&#20915;&#31574;&#21046;&#23450;&#33539;&#24335;&#65288;&#20363;&#22914;&#65292;&#24378;&#21270;&#23398;&#20064;&#21644;&#35268;&#21010;&#22312;&#23398;&#20064;&#30340;&#27169;&#22411;&#20013;&#22914;&#20309;&#24212;&#29992;&#31034;&#33539;&#65289;&#65292;&#20197;&#21450;&#22914;&#20309;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#25910;&#38598;&#31034;&#33539;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20030;&#20102;&#19968;&#20010;&#23454;&#38469;&#30340;&#31034;&#33539;&#29983;&#25104;&#21644;&#21033;&#29992;&#31649;&#36947;&#30340;&#20363;&#23376;&#65292;&#24182;&#22312;&#26368;&#36817;&#25552;&#20986;&#30340;ManiSkill&#26426;&#22120;&#20154;&#23398;&#20064;&#22522;&#20934;&#20013;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although reinforcement learning has seen tremendous success recently, this kind of trial-and-error learning can be impractical or inefficient in complex environments. The use of demonstrations, on the other hand, enables agents to benefit from expert knowledge rather than having to discover the best action to take through exploration. In this survey, we discuss the advantages of using demonstrations in sequential decision making, various ways to apply demonstrations in learning-based decision making paradigms (for example, reinforcement learning and planning in the learned models), and how to collect the demonstrations in various scenarios. Additionally, we exemplify a practical pipeline for generating and utilizing demonstrations in the recently proposed ManiSkill robot learning benchmark.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20102;&#19968;&#20010;&#39118;&#38505;&#20998;&#23618;&#24037;&#20855;CShock&#65292;&#26088;&#22312;&#38024;&#23545;&#24613;&#24615;&#22833;&#20195;&#20607;&#24615;&#24515;&#21147;&#34928;&#31469;&#21644;/&#25110;&#24515;&#32908;&#26775;&#27515;&#24739;&#32773;&#39044;&#27979;&#24515;&#28304;&#24615;&#20241;&#20811;&#30340;&#21457;&#20316;&#12290;</title><link>http://arxiv.org/abs/2303.12888</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#21160;&#24577;&#39118;&#38505;&#35780;&#20998;&#25552;&#21069;&#39044;&#27979;&#24515;&#28304;&#24615;&#20241;&#20811;
&lt;/p&gt;
&lt;p&gt;
A dynamic risk score for early prediction of cardiogenic shock using machine learning. (arXiv:2303.12888v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12888
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20102;&#19968;&#20010;&#39118;&#38505;&#20998;&#23618;&#24037;&#20855;CShock&#65292;&#26088;&#22312;&#38024;&#23545;&#24613;&#24615;&#22833;&#20195;&#20607;&#24615;&#24515;&#21147;&#34928;&#31469;&#21644;/&#25110;&#24515;&#32908;&#26775;&#27515;&#24739;&#32773;&#39044;&#27979;&#24515;&#28304;&#24615;&#20241;&#20811;&#30340;&#21457;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#32908;&#26775;&#27515;&#21644;&#24515;&#21147;&#34928;&#31469;&#26159;&#20027;&#35201;&#30340;&#24515;&#34880;&#31649;&#30142;&#30149;&#65292;&#24433;&#21709;&#30528;&#32654;&#22269;&#25968;&#30334;&#19975;&#20154;&#30340;&#20581;&#24247;&#12290;&#21457;&#23637;&#24515;&#28304;&#24615;&#20241;&#20811;&#30340;&#24739;&#32773;&#20013;&#65292;&#21457;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#26368;&#39640;&#12290;&#24515;&#28304;&#24615;&#20241;&#20811;&#30340;&#26089;&#26399;&#35782;&#21035;&#33267;&#20851;&#37325;&#35201;&#65292;&#21450;&#26102;&#23454;&#26045;&#27835;&#30103;&#25514;&#26045;&#21487;&#20197;&#38450;&#27490;&#32570;&#34880;&#12289;&#20302;&#34880;&#21387;&#20197;&#21450;&#30001;&#20110;&#24515;&#28304;&#24615;&#20241;&#20811;&#23548;&#33268;&#24515;&#36755;&#20986;&#37327;&#38477;&#20302;&#30340;&#26377;&#23475;&#24490;&#29615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24515;&#33039;&#30417;&#25252;&#30149;&#25151;&#20013;&#28023;&#37327;&#25968;&#25454;&#30340;&#20449;&#24687;&#22788;&#29702;&#33021;&#21147;&#19982;&#32570;&#20047;&#26377;&#25928;&#30340;&#39118;&#38505;&#20998;&#23618;&#24037;&#20855;&#65292;&#23545;&#24515;&#28304;&#24615;&#20241;&#20811;&#30340;&#26089;&#26399;&#35782;&#21035;&#19968;&#30452;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20102;&#19968;&#20010;&#31216;&#20026;CShock&#30340;&#39118;&#38505;&#20998;&#23618;&#24037;&#20855;&#65292;&#29992;&#20110;&#39044;&#27979;&#20837;&#20303;&#24515;&#33039;&#30417;&#25252;&#30149;&#25151;&#30340;&#24613;&#24615;&#22833;&#20195;&#20607;&#24615;&#24515;&#21147;&#34928;&#31469;&#21644;/&#25110;&#24515;&#32908;&#26775;&#27515;&#24739;&#32773;&#30340;&#24515;&#28304;&#24615;&#20241;&#20811;&#21457;&#20316;&#12290;&#20026;&#20102;&#24320;&#21457;&#21644;&#39564;&#35777;CShock&#65292;&#25105;&#20204;&#20351;&#29992;&#30001;&#21307;&#24072;&#35009;&#23450;&#30340;&#32467;&#26524;&#27880;&#37322;&#20102;&#24515;&#33039;&#30417;&#25252;&#30149;&#25151;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Myocardial infarction and heart failure are major cardiovascular diseases that affect millions of people in the US. The morbidity and mortality are highest among patients who develop cardiogenic shock. Early recognition of cardiogenic shock is critical. Prompt implementation of treatment measures can prevent the deleterious spiral of ischemia, low blood pressure, and reduced cardiac output due to cardiogenic shock. However, early identification of cardiogenic shock has been challenging due to human providers' inability to process the enormous amount of data in the cardiac intensive care unit (ICU) and lack of an effective risk stratification tool. We developed a deep learning-based risk stratification tool, called CShock, for patients admitted into the cardiac ICU with acute decompensated heart failure and/or myocardial infarction to predict onset of cardiogenic shock. To develop and validate CShock, we annotated cardiac ICU datasets with physician adjudicated outcomes. CShock achieved
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#33041;&#21551;&#21457;&#30340;&#36523;&#20307;&#33258;&#25105;&#24863;&#30693;&#27169;&#22411;&#65292;&#27169;&#25311;&#27233;&#33014;&#25163;&#24187;&#35273;&#65292;&#20026;&#35748;&#35782;&#20154;&#31867;&#36523;&#20307;&#33258;&#25105;&#24847;&#35782;&#30340;&#35745;&#31639;&#26426;&#21046;&#25552;&#20379;&#20102;&#26032;&#30340;&#27934;&#35265;&#12290;</title><link>http://arxiv.org/abs/2303.12259</link><description>&lt;p&gt;
&#27169;&#25311;&#27233;&#33014;&#25163;&#24187;&#35273;&#30340;&#22522;&#20110;&#22823;&#33041;&#21551;&#21457;&#30340;&#36523;&#20307;&#33258;&#25105;&#24863;&#30693;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Brain-inspired bodily self-perception model that replicates the rubber hand illusion. (arXiv:2303.12259v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12259
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#33041;&#21551;&#21457;&#30340;&#36523;&#20307;&#33258;&#25105;&#24863;&#30693;&#27169;&#22411;&#65292;&#27169;&#25311;&#27233;&#33014;&#25163;&#24187;&#35273;&#65292;&#20026;&#35748;&#35782;&#20154;&#31867;&#36523;&#20307;&#33258;&#25105;&#24847;&#35782;&#30340;&#35745;&#31639;&#26426;&#21046;&#25552;&#20379;&#20102;&#26032;&#30340;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36523;&#20307;&#33258;&#25105;&#24847;&#35782;&#30340;&#26680;&#24515;&#26159;&#23545;&#33258;&#24049;&#36523;&#20307;&#25317;&#26377;&#26435;&#30340;&#24863;&#30693;&#12290;&#26368;&#36817;&#65292;&#20026;&#20102;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#22823;&#33041;&#23545;&#33258;&#36523;&#36523;&#20307;&#32534;&#30721;&#30340;&#26426;&#21046;&#65292;&#20154;&#20204;&#20570;&#20986;&#20102;&#21508;&#31181;&#23581;&#35797;&#65292;&#21457;&#23637;&#20986;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;&#30456;&#20851;&#30340;&#34892;&#20026;&#21644;&#31070;&#32463;&#29983;&#29702;&#29616;&#35937;&#12290;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#22914;&#20309;&#35299;&#37322;&#27233;&#33014;&#25163;&#24187;&#35273;&#36825;&#26679;&#30340;&#36523;&#20307;&#38169;&#35273;&#23454;&#38469;&#21457;&#29983;&#30340;&#26426;&#21046;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#20102;&#26377;&#20851;&#36523;&#20307;&#33258;&#25105;&#24847;&#35782;&#26426;&#21046;&#21644;&#21487;&#33021;&#30456;&#20851;&#30340;&#33041;&#21306;&#30340;&#27010;&#24565;&#24615;&#25551;&#36848;&#65292;&#20294;&#29616;&#26377;&#30340;&#29702;&#35770;&#27169;&#22411;&#20173;&#28982;&#32570;&#20047;&#35299;&#37322;&#22823;&#33041;&#22914;&#20309;&#32534;&#30721;&#23545;&#33258;&#24049;&#36523;&#20307;&#30340;&#24863;&#30693;&#21644;&#22914;&#20309;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#25105;&#20204;&#20027;&#35266;&#24863;&#30693;&#30340;&#36523;&#20307;&#38169;&#35273;&#30340;&#35745;&#31639;&#26426;&#21046;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25972;&#21512;&#36523;&#20307;&#33258;&#25105;&#24847;&#35782;&#30340;&#29983;&#29289;&#23398;&#21457;&#29616;&#65292;&#25552;&#20986;&#19968;&#20010;&#22522;&#20110;&#22823;&#33041;&#21551;&#21457;&#30340;&#36523;&#20307;&#33258;&#25105;&#24863;&#30693;&#27169;&#22411;&#65292;&#20351;&#36523;&#20307;&#33258;&#25105;&#24863;&#30693;&#21487;&#20197;&#22312;&#27809;&#26377;&#22806;&#37096;&#21050;&#28608;&#30340;&#24773;&#20917;&#19979;&#33258;&#20027;&#26500;&#24314;&#12290;&#22522;&#20110;&#35813;&#27169;&#22411;&#30340;&#27169;&#25311;&#22797;&#21046;&#20102;&#27233;&#33014;&#25163;&#24187;&#35273;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#28508;&#22312;&#31070;&#32463;&#26426;&#21046;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
At the core of bodily self-consciousness is the perception of the ownership of one's body. Recent efforts to gain a deeper understanding of the mechanisms behind the brain's encoding of the self-body have led to various attempts to develop a unified theoretical framework to explain related behavioral and neurophysiological phenomena. A central question to be explained is how body illusions such as the rubber hand illusion actually occur. Despite the conceptual descriptions of the mechanisms of bodily self-consciousness and the possible relevant brain areas, the existing theoretical models still lack an explanation of the computational mechanisms by which the brain encodes the perception of one's body and how our subjectively perceived body illusions can be generated by neural networks. Here we integrate the biological findings of bodily self-consciousness to propose a Brain-inspired bodily self-perception model, by which perceptions of bodily self can be autonomously constructed withou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21407;&#22987;&#38647;&#36798;&#27169;&#25311;&#25968;&#23383;&#65288;ADC&#65289;&#25968;&#25454;&#19978;&#25191;&#34892;&#31471;&#21040;&#31471;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#20449;&#21495;&#22788;&#29702;&#27169;&#22359;&#34987;&#23884;&#20837;&#32593;&#32476;&#20013;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.11420</link><description>&lt;p&gt;
ADCNet&#65306;&#24102;&#21407;&#22987;&#38647;&#36798;ADC&#25968;&#25454;&#30340;&#31471;&#21040;&#31471;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
ADCNet: End-to-end perception with raw radar ADC data. (arXiv:2303.11420v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21407;&#22987;&#38647;&#36798;&#27169;&#25311;&#25968;&#23383;&#65288;ADC&#65289;&#25968;&#25454;&#19978;&#25191;&#34892;&#31471;&#21040;&#31471;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#20449;&#21495;&#22788;&#29702;&#27169;&#22359;&#34987;&#23884;&#20837;&#32593;&#32476;&#20013;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#34892;&#19994;&#23545;&#38647;&#36798;&#20256;&#24863;&#22120;&#30340;&#20852;&#36259;&#37325;&#26032;&#28608;&#21457;&#12290;&#38647;&#36798;&#20316;&#20026;&#19968;&#31181;&#30456;&#23545;&#25104;&#29087;&#30340;&#25216;&#26415;&#65292;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#24471;&#21040;&#20102;&#31283;&#23450;&#30340;&#25913;&#36827;&#65292;&#20351;&#20854;&#25104;&#20026;&#24120;&#29992;&#30340;LiDAR&#30340;&#26377;&#21560;&#24341;&#21147;&#30340;&#26367;&#20195;&#21697;&#25110;&#34917;&#20805;&#12290;&#19968;&#31181;&#26032;&#20852;&#30340;&#36235;&#21183;&#26159;&#21033;&#29992;&#20016;&#23500;&#30340;&#20302;&#32423;&#21035;&#38647;&#36798;&#25968;&#25454;&#36827;&#34892;&#24863;&#30693;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#36235;&#21183;&#25512;&#21521;&#20102;&#26497;&#31471;--&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21407;&#22987;&#38647;&#36798;&#27169;&#25311;&#25968;&#23383;&#65288;ADC&#65289;&#25968;&#25454;&#19978;&#25191;&#34892;&#31471;&#21040;&#31471;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#35774;&#35745;&#20102;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#20449;&#21495;&#22788;&#29702;&#27169;&#22359;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#30001;&#20256;&#32479;&#20449;&#21495;&#22788;&#29702;&#31639;&#27861;&#24341;&#23548;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#31471;&#21040;&#31471;&#23398;&#20064;&#26041;&#27861;&#30340;&#25972;&#20307;&#26377;&#25928;&#24615;&#65292;&#32780;&#28040;&#34701;&#30740;&#31350;&#39564;&#35777;&#20102;&#25105;&#20204;&#20010;&#20307;&#21019;&#26032;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a renewed interest in radar sensors in the autonomous driving industry. As a relatively mature technology, radars have seen steady improvement over the last few years, making them an appealing alternative or complement to the commonly used LiDARs. An emerging trend is to leverage rich, low-level radar data for perception. In this work we push this trend to the extreme -- we propose a method to perform end-to-end learning on the raw radar analog-to-digital (ADC) data. Specifically, we design a learnable signal processing module inside the neural network, and a pre-training method guided by traditional signal processing algorithms. Experiment results corroborate the overall efficacy of the end-to-end learning method, while an ablation study validates the effectiveness of our individual innovations.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#30340;&#26041;&#35328;&#35782;&#21035;&#31995;&#32479;&#65292;&#22312;VarDial 2023&#20013;&#36229;&#36234;&#20854;&#20182;&#21442;&#19982;&#32773;&#30340;&#31995;&#32479;&#65292;&#23545;&#22810;&#35821;&#35328;&#26041;&#35328;&#26816;&#27979;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2303.03487</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#26041;&#35328;&#26816;&#27979;&#30340;&#20004;&#38454;&#27573;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
Two-stage Pipeline for Multilingual Dialect Detection. (arXiv:2303.03487v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#30340;&#26041;&#35328;&#35782;&#21035;&#31995;&#32479;&#65292;&#22312;VarDial 2023&#20013;&#36229;&#36234;&#20854;&#20182;&#21442;&#19982;&#32773;&#30340;&#31995;&#32479;&#65292;&#23545;&#22810;&#35821;&#35328;&#26041;&#35328;&#26816;&#27979;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26041;&#35328;&#35782;&#21035;&#23545;&#20110;&#26412;&#22320;&#21270;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#25105;&#20204;&#22312;VarDial 2023&#20849;&#20139;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24517;&#39035;&#20174;&#19977;&#31181;&#35821;&#35328;&#20013;&#35782;&#21035;&#20986;&#19977;&#20010;&#25110;&#20004;&#20010;&#26041;&#35328;&#65292;&#36825;&#23548;&#33268;&#20102;Track-1&#30340;9&#36335;&#20998;&#31867;&#21644;Track-2&#30340;6&#36335;&#20998;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#30340;&#31995;&#32479;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#36229;&#36234;&#20102;&#20854;&#20182;&#21442;&#19982;&#32773;&#30340;&#31995;&#32479;&#21644;&#20197;&#21069;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#22312;Track-1&#21644;Track-2&#19978;&#20998;&#21035;&#33719;&#24471;58.54&#65285;&#21644;85.61&#65285;&#30340;&#24471;&#20998;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#24211;&#26159;&#20844;&#24320;&#30340;&#65288;https://github.com/ankit-vaidya19/EACL_VarDial2023&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialect Identification is a crucial task for localizing various Large Language Models. This paper outlines our approach to the VarDial 2023 shared task. Here we have to identify three or two dialects from three languages each which results in a 9-way classification for Track-1 and 6-way classification for Track-2 respectively. Our proposed approach consists of a two-stage system and outperforms other participants' systems and previous works in this domain. We achieve a score of 58.54% for Track-1 and 85.61% for Track-2. Our codebase is available publicly (https://github.com/ankit-vaidya19/EACL_VarDial2023).
&lt;/p&gt;</description></item><item><title>EvoPrompting&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#33258;&#36866;&#24212;&#21464;&#24322;&#21644;&#20132;&#21449;&#25805;&#20316;&#31526;&#26469;&#36827;&#34892;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#22312;MNIST-1D&#25968;&#25454;&#38598;&#21644;CLRS&#31639;&#27861;&#25512;&#29702;&#22522;&#20934;&#19978;&#37117;&#21462;&#24471;&#20102;&#27604;&#20154;&#31867;&#35774;&#35745;&#30340;&#26550;&#26500;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.14838</link><description>&lt;p&gt;
EvoPrompting: &#36866;&#29992;&#20110;&#20195;&#30721;&#32423;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
EvoPrompting: Language Models for Code-Level Neural Architecture Search. (arXiv:2302.14838v1 [cs.NE] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14838
&lt;/p&gt;
&lt;p&gt;
EvoPrompting&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#33258;&#36866;&#24212;&#21464;&#24322;&#21644;&#20132;&#21449;&#25805;&#20316;&#31526;&#26469;&#36827;&#34892;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#22312;MNIST-1D&#25968;&#25454;&#38598;&#21644;CLRS&#31639;&#27861;&#25512;&#29702;&#22522;&#20934;&#19978;&#37117;&#21462;&#24471;&#20102;&#27604;&#20154;&#31867;&#35774;&#35745;&#30340;&#26550;&#26500;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#26368;&#26032;&#25104;&#23601;&#65292;&#25105;&#20204;&#25506;&#32034;&#23558;LM&#20316;&#20026;&#36827;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#31639;&#27861;&#30340;&#33258;&#36866;&#24212;&#21464;&#24322;&#21644;&#20132;&#21449;&#25805;&#20316;&#31526;&#30340;&#20351;&#29992;&#12290;&#23613;&#31649;NAS&#20173;&#28982;&#36807;&#20110;&#22256;&#38590;&#65292;&#20197;&#33267;&#20110;&#20165;&#20165;&#36890;&#36807;&#25552;&#31034;&#23601;&#38590;&#20197;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#36827;&#21270;&#25552;&#31034;&#24037;&#31243;&#19982;&#36719;&#25552;&#31034;&#35843;&#25972;&#30340;&#32452;&#21512;&#65292;&#19968;&#31181;&#25105;&#20204;&#31216;&#20043;&#20026;EvoPrompting&#30340;&#26041;&#27861;&#65292;&#22987;&#32456;&#21487;&#20197;&#21457;&#29616;&#22810;&#26679;&#21270;&#19988;&#24615;&#33021;&#39640;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;EvoPrompting&#22312;MNIST-1D&#25968;&#25454;&#38598;&#19978;&#26159;&#26377;&#25928;&#30340;&#65292;&#20854;&#20013;EvoPrompting&#20135;&#29983;&#30340;&#21367;&#31215;&#26550;&#26500;&#21464;&#20307;&#22312;&#20934;&#30830;&#29575;&#21644;&#27169;&#22411;&#22823;&#23567;&#26041;&#38754;&#22343;&#20248;&#20110;&#20154;&#31867;&#19987;&#23478;&#35774;&#35745;&#30340;&#26550;&#26500;&#21644;&#22825;&#30495;&#30340;&#23569;&#25968;&#20808;&#23548;&#25552;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#22312;CLRS&#31639;&#27861;&#25512;&#29702;&#22522;&#20934;&#19978;&#25628;&#32034;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#20013;EvoPrompting&#33021;&#22815;&#35774;&#35745;&#20986;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26356;&#22909;&#30340;&#26032;&#39062;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the recent impressive accomplishments of language models (LMs) for code generation, we explore the use of LMs as adaptive mutation and crossover operators for an evolutionary neural architecture search (NAS) algorithm. While NAS still proves too difficult a task for LMs to succeed at solely through prompting, we find that the combination of evolutionary prompt engineering with soft prompt-tuning, a method we term EvoPrompting, consistently finds diverse and high performing models. We first demonstrate that EvoPrompting is effective on the computationally efficient MNIST-1D dataset, where EvoPrompting produces convolutional architecture variants that outperform both those designed by human experts and naive few-shot prompting in terms of accuracy and model size. We then apply our method to searching for graph neural networks on the CLRS Algorithmic Reasoning Benchmark, where EvoPrompting is able to design novel architectures that outperform current state-of-the-art models on 21 ou
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#21487;&#36870;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;LHC&#25968;&#25454;&#30340;&#22788;&#29702;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#31639;&#27861;&#21487;&#20197;&#22312;&#23398;&#20064;&#21644;&#29983;&#25104;&#22797;&#26434;&#25968;&#25454;&#26041;&#38754;&#19982;&#32463;&#20856;&#31639;&#27861;&#30340;&#34920;&#29616;&#30456;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2302.12906</link><description>&lt;p&gt;
&#29983;&#25104;&#21487;&#36870;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Generative Invertible Quantum Neural Networks. (arXiv:2302.12906v2 [hep-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#21487;&#36870;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;LHC&#25968;&#25454;&#30340;&#22788;&#29702;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#31639;&#27861;&#21487;&#20197;&#22312;&#23398;&#20064;&#21644;&#29983;&#25104;&#22797;&#26434;&#25968;&#25454;&#26041;&#38754;&#19982;&#32463;&#20856;&#31639;&#27861;&#30340;&#34920;&#29616;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#24050;&#25104;&#20026;&#27169;&#25311;&#21644;&#29983;&#25104;&#39640;&#24230;&#22797;&#26434;&#25968;&#25454;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#23376;&#38376;&#31639;&#27861;&#29992;&#20110;&#37327;&#23376;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#65288;QINN&#65289;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#23558;&#34928;&#21464;&#20026;&#36731;&#23376;&#30340;Z&#29627;&#33394;&#23376;&#30340;&#21943;&#27880;&#30456;&#20851;&#20135;&#29983;&#30340;LHC&#25968;&#25454;&#65292;&#36825;&#26159;&#31890;&#23376;&#23545;&#25758;&#26426;&#31934;&#23494;&#27979;&#37327;&#30340;&#26631;&#20934;&#36807;&#31243;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;QINN&#22312;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#22330;&#26223;&#19979;&#30340;&#34920;&#29616;&#12290;&#23545;&#20110;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#20010;&#28151;&#21512;&#30340;QINN&#21487;&#20197;&#22312;&#23398;&#20064;&#21644;&#29983;&#25104;&#22797;&#26434;&#25968;&#25454;&#26041;&#38754;&#19982;&#19968;&#20010;&#26174;&#33879;&#26356;&#22823;&#30340;&#23436;&#20840;&#32463;&#20856;&#30340;INN&#30340;&#34920;&#29616;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Invertible Neural Networks (INN) have become established tools for the simulation and generation of highly complex data. We propose a quantum-gate algorithm for a Quantum Invertible Neural Network (QINN) and apply it to the LHC data of jet-associated production of a Z-boson that decays into leptons, a standard candle process for particle collider precision measurements. We compare the QINN's performance for different loss functions and training scenarios. For this task, we find that a hybrid QINN matches the performance of a significantly larger purely classical INN in learning and generating complex data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#23439;&#35266;&#24067;&#23616;&#26041;&#27861;&#20197;&#21450;Circuit Training (CT)&#23454;&#29616;&#30340;&#24320;&#28304;&#20195;&#30721;&#21644;&#35780;&#20272;&#12290;&#30740;&#31350;&#20154;&#21592;&#35780;&#20272;&#20102;CT&#30456;&#23545;&#20110;&#22810;&#20010;&#21487;&#26367;&#20195;&#30340;&#23439;&#35266;&#24067;&#23616;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#23398;&#26415;&#24615;&#28151;&#21512;&#23610;&#23544;&#24067;&#23616;&#22522;&#20934;&#27979;&#35797;&#21644;&#28040;&#34701;&#21644;&#31283;&#23450;&#24615;&#30740;&#31350;&#65292;&#20026;&#26410;&#26469;&#30340;&#30456;&#20851;&#30740;&#31350;&#25552;&#20379;&#20102;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.11014</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#23439;&#35266;&#24067;&#23616;&#20013;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Assessment of Reinforcement Learning for Macro Placement. (arXiv:2302.11014v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#23439;&#35266;&#24067;&#23616;&#26041;&#27861;&#20197;&#21450;Circuit Training (CT)&#23454;&#29616;&#30340;&#24320;&#28304;&#20195;&#30721;&#21644;&#35780;&#20272;&#12290;&#30740;&#31350;&#20154;&#21592;&#35780;&#20272;&#20102;CT&#30456;&#23545;&#20110;&#22810;&#20010;&#21487;&#26367;&#20195;&#30340;&#23439;&#35266;&#24067;&#23616;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#23398;&#26415;&#24615;&#28151;&#21512;&#23610;&#23544;&#24067;&#23616;&#22522;&#20934;&#27979;&#35797;&#21644;&#28040;&#34701;&#21644;&#31283;&#23450;&#24615;&#30740;&#31350;&#65292;&#20026;&#26410;&#26469;&#30340;&#30456;&#20851;&#30740;&#31350;&#25552;&#20379;&#20102;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;Google Brain&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#23439;&#35266;&#24067;&#23616;&#21450;&#20854;Circuit Training (CT)&#23454;&#29616;&#30340;&#24320;&#25918;&#36879;&#26126;&#23454;&#29616;&#21644;&#35780;&#20272;&#65292;&#24182;&#22312;GitHub&#20013;&#23454;&#29616;&#20102;CT&#30340;&#20851;&#38190;"&#40657;&#30418;"&#20803;&#32032;&#65292;&#28548;&#28165;&#20102;CT&#19982;Nature&#35770;&#25991;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#24320;&#21457;&#24182;&#21457;&#24067;&#20102;&#26032;&#30340;&#23545;&#24320;&#25918;&#23454;&#29616;&#30340;&#27979;&#35797;&#29992;&#20363;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;CT&#21450;&#22810;&#20010;&#21487;&#26367;&#20195;&#30340;&#23439;&#35266;&#24067;&#23616;&#26041;&#27861;&#65292;&#25152;&#26377;&#30340;&#35780;&#20272;&#27969;&#31243;&#21644;&#30456;&#20851;&#33050;&#26412;&#37117;&#22312;GitHub&#19978;&#20844;&#24320;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#36824;&#21253;&#25324;&#20102;&#23398;&#26415;&#24615;&#28151;&#21512;&#23610;&#23544;&#24067;&#23616;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#21450;&#28040;&#34701;&#21644;&#31283;&#23450;&#24615;&#30740;&#31350;&#12290;&#25105;&#20204;&#35780;&#35770;&#20102;Nature&#21644;CT&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide open, transparent implementation and assessment of Google Brain's deep reinforcement learning approach to macro placement and its Circuit Training (CT) implementation in GitHub. We implement in open source key "blackbox" elements of CT, and clarify discrepancies between CT and Nature paper. New testcases on open enablements are developed and released. We assess CT alongside multiple alternative macro placers, with all evaluation flows and related scripts public in GitHub. Our experiments also encompass academic mixed-size placement benchmarks, as well as ablation and stability studies. We comment on the impact of Nature and CT, as well as directions for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27700;&#21360;&#26041;&#26696;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#38450;&#24481;&#27169;&#22411;&#24494;&#35843;&#21644;&#20462;&#21098;&#31561;&#27700;&#21360;&#21024;&#38500;&#25915;&#20987;&#65307;&#36890;&#36807;&#22686;&#24378;&#27700;&#21360;&#21644;&#27169;&#22411;&#21151;&#33021;&#30340;&#32806;&#21512;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#30830;&#20445;&#21024;&#38500;&#27700;&#21360;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#38477;&#20302;&#27169;&#22411;&#22312;&#24120;&#35268;&#36755;&#20837;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.10296</link><description>&lt;p&gt;
&#20851;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21151;&#33021;&#32806;&#21512;&#27700;&#21360;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Function-Coupled Watermarks for Deep Neural Networks. (arXiv:2302.10296v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27700;&#21360;&#26041;&#26696;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#38450;&#24481;&#27169;&#22411;&#24494;&#35843;&#21644;&#20462;&#21098;&#31561;&#27700;&#21360;&#21024;&#38500;&#25915;&#20987;&#65307;&#36890;&#36807;&#22686;&#24378;&#27700;&#21360;&#21644;&#27169;&#22411;&#21151;&#33021;&#30340;&#32806;&#21512;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#30830;&#20445;&#21024;&#38500;&#27700;&#21360;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#38477;&#20302;&#27169;&#22411;&#22312;&#24120;&#35268;&#36755;&#20837;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33391;&#22909;&#34920;&#29616;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#38656;&#35201;&#28023;&#37327;&#26631;&#35760;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#36827;&#34892;&#35757;&#32451;&#12290;&#20026;&#20102;&#20445;&#25252;&#36825;&#20123;&#30693;&#35782;&#20135;&#26435;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#27700;&#21360;&#25216;&#26415;&#65292;&#20854;&#20013;DNN&#25552;&#20379;&#21830;&#23558;&#31192;&#23494;&#20449;&#24687;&#26893;&#20837;&#27169;&#22411;&#20013;&#65292;&#20197;&#20415;&#22312;&#31245;&#21518;&#36890;&#36807;&#19968;&#20123;&#19987;&#29992;&#35302;&#21457;&#36755;&#20837;&#26816;&#32034;&#23884;&#20837;&#30340;&#27700;&#21360;&#32034;&#26435;&#65307;&#34429;&#28982;&#25991;&#29486;&#20013;&#25253;&#21578;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#36973;&#21463;&#27700;&#21360;&#21024;&#38500;&#25915;&#20987;&#65292;&#20363;&#22914;&#27169;&#22411;&#24494;&#35843;&#21644;&#27169;&#22411;&#20462;&#21098;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;DNN&#27700;&#21360;&#26041;&#26696;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#38450;&#24481;&#19978;&#36848;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#21147;&#26159;&#22686;&#24378;&#27700;&#21360;&#21644;&#27169;&#22411;&#21151;&#33021;&#30340;&#32806;&#21512;&#65292;&#36825;&#26679;&#21024;&#38500;&#27700;&#21360;&#20250;&#19981;&#21487;&#36991;&#20813;&#22320;&#38477;&#20302;&#27169;&#22411;&#22312;&#24120;&#35268;&#36755;&#20837;&#19978;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#19982;&#20808;&#21069;&#20381;&#36182;&#20110;&#26469;&#33258;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#30340;&#31192;&#23494;&#29305;&#24449;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#20351;&#29992;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Well-performed deep neural networks (DNNs) generally require massive labelled data and computational resources for training. Various watermarking techniques are proposed to protect such intellectual properties (IPs), wherein the DNN providers implant secret information into the model so that they can later claim IP ownership by retrieving their embedded watermarks with some dedicated trigger inputs. While promising results are reported in the literature, existing solutions suffer from watermark removal attacks, such as model fine-tuning and model pruning.  In this paper, we propose a novel DNN watermarking solution that can effectively defend against the above attacks. Our key insight is to enhance the coupling of the watermark and model functionalities such that removing the watermark would inevitably degrade the model's performance on normal inputs. To this end, unlike previous methods relying on secret features learnt from out-of-distribution data, our method only uses features lear
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Swin Transformer&#30340;&#26032;&#25216;&#26415;&#65292;&#24310;&#32493;&#20102;&#35270;&#39057;&#36816;&#21160;&#25918;&#22823;&#25216;&#26415;&#30340;&#30740;&#31350;&#65292;&#25552;&#39640;&#20102;&#36755;&#20986;&#22270;&#20687;&#30340;&#36136;&#37327;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#23481;&#38169;&#33021;&#21147;&#21644;&#26356;&#23569;&#30340;&#22122;&#22768;&#12289;&#27169;&#31946;&#21644;&#20266;&#24433;&#65292;&#21487;&#29992;&#20110;&#25552;&#39640;&#25918;&#22823;&#35270;&#39057;&#24207;&#21015;&#30340;&#31934;&#30830;&#24230;&#65292;&#20419;&#36827;&#35270;&#39057;&#36816;&#21160;&#25918;&#22823;&#25216;&#26415;&#22312;&#26032;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2302.10001</link><description>&lt;p&gt;
&#22522;&#20110;Swin Transformer&#30340;&#35270;&#39057;&#36816;&#21160;&#25918;&#22823;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
STB-VMM: Swin Transformer Based Video Motion Magnification. (arXiv:2302.10001v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Swin Transformer&#30340;&#26032;&#25216;&#26415;&#65292;&#24310;&#32493;&#20102;&#35270;&#39057;&#36816;&#21160;&#25918;&#22823;&#25216;&#26415;&#30340;&#30740;&#31350;&#65292;&#25552;&#39640;&#20102;&#36755;&#20986;&#22270;&#20687;&#30340;&#36136;&#37327;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#23481;&#38169;&#33021;&#21147;&#21644;&#26356;&#23569;&#30340;&#22122;&#22768;&#12289;&#27169;&#31946;&#21644;&#20266;&#24433;&#65292;&#21487;&#29992;&#20110;&#25552;&#39640;&#25918;&#22823;&#35270;&#39057;&#24207;&#21015;&#30340;&#31934;&#30830;&#24230;&#65292;&#20419;&#36827;&#35270;&#39057;&#36816;&#21160;&#25918;&#22823;&#25216;&#26415;&#22312;&#26032;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#36816;&#21160;&#25918;&#22823;&#25216;&#26415;&#30340;&#30446;&#30340;&#26159;&#25918;&#22823;&#35270;&#39057;&#20013;&#30340;&#24494;&#23567;&#36816;&#21160;&#65292;&#20197;&#25581;&#31034;&#20197;&#21069;&#30475;&#19981;&#35265;&#30340;&#36816;&#21160;&#12290;&#23427;&#30340;&#29992;&#36884;&#20174;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#21644;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#21040;&#32467;&#26500;&#27169;&#24577;&#20998;&#26512;&#21644;&#39044;&#27979;&#32500;&#25252;&#31561;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#20174;&#22122;&#22768;&#20013;&#20998;&#36776;&#20986;&#24494;&#23567;&#36816;&#21160;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#23588;&#20854;&#26159;&#24403;&#35797;&#22270;&#25918;&#22823;&#38750;&#24120;&#24494;&#22937;&#30340;&#12289;&#24448;&#24448;&#26159;&#20122;&#20687;&#32032;&#36816;&#21160;&#26102;&#12290;&#22240;&#27492;&#65292;&#36816;&#21160;&#25918;&#22823;&#25216;&#26415;&#36890;&#24120;&#36973;&#21463;&#22024;&#26434;&#21644;&#27169;&#31946;&#30340;&#36755;&#20986;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Swin Transformer&#30340;&#26368;&#26032;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23545;&#22122;&#22768;&#36755;&#20837;&#20855;&#26377;&#26356;&#22909;&#30340;&#23481;&#38169;&#33021;&#21147;&#65292;&#36755;&#20986;&#30340;&#36136;&#37327;&#27604;&#20808;&#21069;&#30340;&#25216;&#26415;&#23637;&#29616;&#20986;&#26356;&#23569;&#30340;&#22122;&#22768;&#12289;&#27169;&#31946;&#21644;&#20266;&#24433;&#12290;&#36755;&#20986;&#22270;&#20687;&#36136;&#37327;&#30340;&#25913;&#36827;&#23558;&#20351;&#20219;&#20309;&#20381;&#36182;&#25918;&#22823;&#35270;&#39057;&#24207;&#21015;&#30340;&#24212;&#29992;&#24471;&#21040;&#26356;&#31934;&#30830;&#30340;&#27979;&#37327;&#65292;&#24182;&#21487;&#33021;&#20351;&#35270;&#39057;&#36816;&#21160;&#25918;&#22823;&#25216;&#26415;&#22312;&#26032;&#30340;&#25216;&#26415;&#39046;&#22495;&#24471;&#21040;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of video motion magnification techniques is to magnify small motions in a video to reveal previously invisible or unseen movement. Its uses extend from bio-medical applications and deepfake detection to structural modal analysis and predictive maintenance. However, discerning small motion from noise is a complex task, especially when attempting to magnify very subtle, often sub-pixel movement. As a result, motion magnification techniques generally suffer from noisy and blurry outputs. This work presents a new state-of-the-art model based on the Swin Transformer, which offers better tolerance to noisy inputs as well as higher-quality outputs that exhibit less noise, blurriness, and artifacts than prior-art. Improvements in output image quality will enable more precise measurements for any application reliant on magnified video sequences, and may enable further development of video motion magnification techniques in new technical fields.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SN-Net&#30340;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#20415;&#23452;&#22320;&#20135;&#29983;&#35768;&#22810;&#19981;&#21516;&#22797;&#26434;&#24230;&#21644;&#24615;&#33021;&#26435;&#34913;&#30340;&#32593;&#32476;&#65292;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#23478;&#26063;&#20316;&#20026;&#38170;&#28857;&#65292;&#24182;&#20351;&#29992;&#31616;&#21333;&#30340;&#32541;&#21512;&#23618;&#23558;&#23427;&#20204;&#25340;&#25509;&#22312;&#19968;&#36215;&#20197;&#23454;&#29616;&#21160;&#24577;&#30340;&#31934;&#24230;-&#25928;&#29575;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2302.06586</link><description>&lt;p&gt;
&#21487;&#32541;&#21512;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Stitchable Neural Networks. (arXiv:2302.06586v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SN-Net&#30340;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#20415;&#23452;&#22320;&#20135;&#29983;&#35768;&#22810;&#19981;&#21516;&#22797;&#26434;&#24230;&#21644;&#24615;&#33021;&#26435;&#34913;&#30340;&#32593;&#32476;&#65292;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#23478;&#26063;&#20316;&#20026;&#38170;&#28857;&#65292;&#24182;&#20351;&#29992;&#31616;&#21333;&#30340;&#32541;&#21512;&#23618;&#23558;&#23427;&#20204;&#25340;&#25509;&#22312;&#19968;&#36215;&#20197;&#23454;&#29616;&#21160;&#24577;&#30340;&#31934;&#24230;-&#25928;&#29575;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#24040;&#22823;&#23041;&#21147;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#38598;&#32676;(&#22914;ResNet/DeiT)&#25152;&#26500;&#25104;&#30340;&#20844;&#20849;&#27169;&#22411;&#24211;&#24050;&#32463;&#36798;&#21040;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#33539;&#22260;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20419;&#36827;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#27169;&#22411;&#31995;&#21015;&#37117;&#21253;&#21547;&#30528;&#19981;&#21516;&#35268;&#27169;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;(&#27604;&#22914;DeiT-Ti/S/B)&#65292;&#36825;&#33258;&#28982;&#22320;&#24341;&#20986;&#20102;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;&#22914;&#20309;&#22312;&#36816;&#34892;&#26102;&#26377;&#25928;&#22320;&#32452;&#21512;&#36825;&#20123;&#21487;&#29992;&#30340;&#27169;&#22411;&#31995;&#21015;&#20197;&#23454;&#29616;&#21160;&#24577;&#30340;&#31934;&#24230;-&#25928;&#29575;&#26435;&#34913;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Stitchable Neural Networks (SN-Net)&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21487;&#25193;&#23637;&#12289;&#39640;&#25928;&#30340;&#27169;&#22411;&#37096;&#32626;&#26694;&#26550;&#12290;&#22312;&#19968;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#23478;&#26063;&#20013;&#65292;&#23427;&#21487;&#20197;&#20415;&#23452;&#22320;&#20135;&#29983;&#35768;&#22810;&#19981;&#21516;&#22797;&#26434;&#24230;&#21644;&#24615;&#33021;&#26435;&#34913;&#30340;&#32593;&#32476;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#38170;&#28857;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;SN-Net&#23558;&#38170;&#28857;&#20998;&#25955;&#22312;&#22359;/&#23618;&#20043;&#38388;&#65292;&#28982;&#21518;&#20351;&#29992;&#31616;&#21333;&#30340;&#32541;&#21512;&#23618;&#23558;&#23427;&#20204;&#25340;&#25509;&#22312;&#19968;&#36215;&#65292;&#20197;&#26144;&#23556;&#19968;&#20010;&#38170;&#28857;&#30340;&#28608;&#27963;&#21040;&#21478;&#19968;&#20010;&#38170;&#28857;&#12290;&#20165;&#20165;&#36890;&#36807;&#20960;&#20010;&#36718;&#27425;&#30340;&#35757;&#32451;&#65292;SN-Net&#21487;&#20197;&#26377;&#25928;&#22320;&#25554;&#20540;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
The public model zoo containing enormous powerful pretrained model families (e.g., ResNet/DeiT) has reached an unprecedented scope than ever, which significantly contributes to the success of deep learning. As each model family consists of pretrained models with diverse scales (e.g., DeiT-Ti/S/B), it naturally arises a fundamental question of how to efficiently assemble these readily available models in a family for dynamic accuracy-efficiency trade-offs at runtime. To this end, we present Stitchable Neural Networks (SN-Net), a novel scalable and efficient framework for model deployment. It cheaply produces numerous networks with different complexity and performance trade-offs given a family of pretrained neural networks, which we call anchors. Specifically, SN-Net splits the anchors across the blocks/layers and then stitches them together with simple stitching layers to map the activations from one anchor to another. With only a few epochs of training, SN-Net effectively interpolates 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#23545;&#35805;&#36741;&#23548;&#23384;&#22312;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#21457;&#29616;&#24403;&#21069;&#26041;&#27861;&#22312;&#23569;&#37327;&#27010;&#24565;&#21644;&#21487;&#33021;&#30340;&#25945;&#24072;&#31574;&#30053;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#36827;&#34892;&#36739;&#22909;&#30340;&#36741;&#23548;&#27169;&#25311;&#19982;&#23398;&#20064;&#65292;&#20294;&#22312;&#19981;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#65292;&#26410;&#26469;&#24212;&#35813;&#38598;&#20013;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#19978;&#12290;</title><link>http://arxiv.org/abs/2301.09919</link><description>&lt;p&gt;
&#31070;&#32463;&#23545;&#35805;&#36741;&#23548;&#20013;&#30340;&#26426;&#36935;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Opportunities and Challenges in Neural Dialog Tutoring. (arXiv:2301.09919v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#23545;&#35805;&#36741;&#23548;&#23384;&#22312;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#21457;&#29616;&#24403;&#21069;&#26041;&#27861;&#22312;&#23569;&#37327;&#27010;&#24565;&#21644;&#21487;&#33021;&#30340;&#25945;&#24072;&#31574;&#30053;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#36827;&#34892;&#36739;&#22909;&#30340;&#36741;&#23548;&#27169;&#25311;&#19982;&#23398;&#20064;&#65292;&#20294;&#22312;&#19981;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#65292;&#26410;&#26469;&#24212;&#35813;&#38598;&#20013;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#23545;&#35805;&#36741;&#23548;&#31995;&#32479;&#19968;&#30452;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24037;&#20316;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#21040;&#23545;&#20154;&#31867;&#36741;&#23548;&#32773;&#25152;&#37319;&#29992;&#30340;&#22810;&#26679;&#19988;&#22797;&#26434;&#30340;&#25945;&#23398;&#31574;&#30053;&#36827;&#34892;&#24314;&#27169;&#12290;&#23613;&#31649;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#21644;&#21487;&#29992;&#30340;&#23545;&#35805;&#35821;&#26009;&#24211;&#26041;&#38754;&#20986;&#29616;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#23545;&#35805;&#36741;&#23548;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20173;&#26410;&#21463;&#21040;&#36825;&#20123;&#36827;&#23637;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#22312;&#20004;&#20010;&#35821;&#35328;&#23398;&#20064;&#23545;&#35805;&#36741;&#23548;&#25968;&#25454;&#38598;&#19978;&#23545;&#21508;&#31181;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20005;&#26684;&#20998;&#26512;&#65292;&#20351;&#29992;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#26469;&#20102;&#35299;&#36825;&#20123;&#36827;&#23637;&#24102;&#26469;&#30340;&#26032;&#26426;&#20250;&#20197;&#21450;&#25105;&#20204;&#24517;&#39035;&#20811;&#26381;&#30340;&#25361;&#25112;&#65292;&#20197;&#26500;&#24314;&#33021;&#22312;&#30495;&#23454;&#25945;&#32946;&#29615;&#22659;&#20013;&#20351;&#29992;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#24403;&#21069;&#26041;&#27861;&#21487;&#20197;&#23545;&#23569;&#37327;&#27010;&#24565;&#21644;&#21487;&#33021;&#30340;&#25945;&#24072;&#31574;&#30053;&#36827;&#34892;&#36739;&#22909;&#30340;&#36741;&#23548;&#27169;&#25311;&#19982;&#23398;&#20064;&#65292;&#20294;&#22312;&#19981;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#30340;&#20154;&#24037;&#36136;&#37327;&#35780;&#20272;&#26174;&#31034;&#65292;&#27169;&#22411;&#21644;&#22522;&#30784;&#25945;&#23398;&#31995;&#32479;&#22312;&#36825;&#20123;&#19981;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#37117;&#20855;&#26377;&#36739;&#20302;&#30340;&#26377;&#25928;&#24615;&#65292;&#36825;&#34920;&#26126;&#26410;&#26469;&#30340;&#30740;&#31350;&#24212;&#35813;&#38598;&#20013;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing dialog tutors has been challenging as it involves modeling the diverse and complex pedagogical strategies employed by human tutors. Although there have been significant recent advances in neural conversational systems using large language models (LLMs) and growth in available dialog corpora, dialog tutoring has largely remained unaffected by these advances. In this paper, we rigorously analyze various generative language models on two dialog tutoring datasets for language learning using automatic and human evaluations to understand the new opportunities brought by these advances as well as the challenges we must overcome to build models that would be usable in real educational settings. We find that although current approaches can model tutoring in constrained learning scenarios when the number of concepts to be taught and possible teacher strategies are small, they perform poorly in less constrained scenarios. Our human quality evaluation shows that both models and ground-tr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#33203;&#39135;&#33829;&#20859;&#36741;&#21161;&#24179;&#21488;&#65292;&#35813;&#24179;&#21488;&#24320;&#21457;&#20102;&#19968;&#20010;&#26412;&#22320;&#21270;&#30340;&#26032;&#21152;&#22369;&#39135;&#21697;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#36890;&#36807;&#30417;&#31649;&#21644;&#30417;&#30563;&#20154;&#20204;&#30340;&#33829;&#20859;&#25668;&#20837;&#65292;&#20026;&#26032;&#21152;&#22369;&#20154;&#30340;&#20581;&#24247;&#20419;&#36827;&#25552;&#20379;&#21307;&#23398;&#32423;&#21035;&#30340;&#33829;&#20859;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2301.03829</link><description>&lt;p&gt;
&#20174;&#39184;&#30424;&#21040;&#39044;&#38450;&#65306;&#26032;&#21152;&#22369;&#20581;&#24247;&#20419;&#36827;&#30340;&#33203;&#39135;&#33829;&#20859;&#36741;&#21161;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
From Plate to Prevention: A Dietary Nutrient-aided Platform for Health Promotion in Singapore. (arXiv:2301.03829v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#33203;&#39135;&#33829;&#20859;&#36741;&#21161;&#24179;&#21488;&#65292;&#35813;&#24179;&#21488;&#24320;&#21457;&#20102;&#19968;&#20010;&#26412;&#22320;&#21270;&#30340;&#26032;&#21152;&#22369;&#39135;&#21697;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#36890;&#36807;&#30417;&#31649;&#21644;&#30417;&#30563;&#20154;&#20204;&#30340;&#33829;&#20859;&#25668;&#20837;&#65292;&#20026;&#26032;&#21152;&#22369;&#20154;&#30340;&#20581;&#24247;&#20419;&#36827;&#25552;&#20379;&#21307;&#23398;&#32423;&#21035;&#30340;&#33829;&#20859;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#21152;&#22369;&#19968;&#30452;&#33268;&#21147;&#20110;&#25913;&#21892;&#20154;&#27665;&#30340;&#21307;&#30103;&#20445;&#20581;&#26381;&#21153;&#12290;&#25919;&#24220;&#27880;&#24847;&#21040;&#20102;&#30417;&#31649;&#21644;&#30417;&#30563;&#20154;&#20204;&#25668;&#20837;&#33829;&#20859;&#30340;&#19981;&#36275;&#65292;&#36825;&#34987;&#35748;&#20026;&#26159;&#24930;&#24615;&#30142;&#30149;&#21457;&#23637;&#30340;&#19968;&#20010;&#22240;&#32032;&#12290;&#22240;&#27492;&#65292;&#36825;&#20010;&#38382;&#39064;&#24341;&#36215;&#20102;&#24456;&#22823;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#20998;&#20139;&#20102;&#26032;&#21152;&#22369;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#21644;&#33719;&#21462;&#21307;&#30103;&#32423;&#21035;&#33829;&#20859;&#25668;&#20837;&#20449;&#24687;&#20197;&#22312;&#19981;&#21516;&#26041;&#38754;&#36896;&#31119;&#26032;&#21152;&#22369;&#20154;&#30340;&#32463;&#39564;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;FoodSG&#24179;&#21488;&#26469;&#23413;&#21270;&#22810;&#26679;&#21270;&#30340;&#38754;&#21521;&#21307;&#30103;&#20445;&#20581;&#30340;&#24212;&#29992;&#26381;&#21153;&#22312;&#26032;&#21152;&#22369;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#23427;&#20204;&#30340;&#20849;&#21516;&#35201;&#27714;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30830;&#23450;&#20102;&#26412;&#22320;&#21270;&#39135;&#21697;&#25968;&#25454;&#38598;&#30340;&#28145;&#36828;&#24847;&#20041;&#65292;&#24182;&#31995;&#32479;&#22320;&#28165;&#29702;&#21644;&#31579;&#36873;&#20102;&#19968;&#20010;&#26412;&#22320;&#21270;&#30340;&#26032;&#21152;&#22369;&#39135;&#21697;&#25968;&#25454;&#38598;FoodSG-233&#12290;&#20026;&#20102;&#20811;&#26381;&#30001;&#26032;&#21152;&#22369;&#22810;&#26679;&#21270;&#39135;&#21697;&#33756;&#32948;&#24102;&#26469;&#30340;&#35782;&#21035;&#24615;&#33021;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#35758;&#25972;&#21512;&#30417;&#30563;&#24335;&#23545;&#27604;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Singapore has been striving to improve the provision of healthcare services to her people. In this course, the government has taken note of the deficiency in regulating and supervising people's nutrient intake, which is identified as a contributing factor to the development of chronic diseases. Consequently, this issue has garnered significant attention. In this paper, we share our experience in addressing this issue and attaining medical-grade nutrient intake information to benefit Singaporeans in different aspects. To this end, we develop the FoodSG platform to incubate diverse healthcare-oriented applications as a service in Singapore, taking into account their shared requirements. We further identify the profound meaning of localized food datasets and systematically clean and curate a localized Singaporean food dataset FoodSG-233. To overcome the hurdle in recognition performance brought by Singaporean multifarious food dishes, we propose to integrate supervised contrastive learnin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#36807;&#28388;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;&#20855;&#26377;&#36739;&#23569;&#21807;&#19968;&#20803;&#32032;&#30340;&#29305;&#27530;&#32467;&#26500;&#26469;&#23454;&#29616;&#35774;&#22791;&#31471;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#20869;&#23384;&#28040;&#32791;&#65292;&#23454;&#29616;&#26368;&#39640;19&#20493;&#30340;&#35757;&#32451;&#21152;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2301.00330</link><description>&lt;p&gt;
&#26799;&#24230;&#36807;&#28388;&#25216;&#26415;&#23454;&#29616;&#39640;&#25928;&#30340;&#35774;&#22791;&#31471;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Efficient On-device Training via Gradient Filtering. (arXiv:2301.00330v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#36807;&#28388;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;&#20855;&#26377;&#36739;&#23569;&#21807;&#19968;&#20803;&#32032;&#30340;&#29305;&#27530;&#32467;&#26500;&#26469;&#23454;&#29616;&#35774;&#22791;&#31471;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#20869;&#23384;&#28040;&#32791;&#65292;&#23454;&#29616;&#26368;&#39640;19&#20493;&#30340;&#35757;&#32451;&#21152;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#32852;&#37030;&#23398;&#20064;&#12289;&#36830;&#32493;&#23398;&#20064;&#21644;&#20854;&#20182;&#35768;&#22810;&#24212;&#29992;&#20013;&#24456;&#37325;&#35201;&#65292;&#20294;&#35774;&#22791;&#31471;&#35757;&#32451;&#20173;&#28982;&#26159;EdgeAI&#30340;&#19968;&#20010;&#38590;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#36807;&#28388;&#25216;&#26415;&#65292;&#20351;&#24471;&#35774;&#22791;&#31471;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#35757;&#32451;&#25104;&#20026;&#21487;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21019;&#24314;&#20855;&#26377;&#36739;&#23569;&#21807;&#19968;&#20803;&#32032;&#30340;&#29305;&#27530;&#32467;&#26500;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#35757;&#32451;&#26399;&#38388;&#21453;&#21521;&#20256;&#25773;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#20869;&#23384;&#28040;&#32791;&#12290;&#22312;&#22810;&#20010;CNN&#27169;&#22411;&#65288;&#20363;&#22914;MobileNet&#12289;DeepLabV3&#12289;UPerNet&#65289;&#21644;&#35774;&#22791;&#65288;&#20363;&#22914;Raspberry Pi&#21644;Jetson Nano&#65289;&#19978;&#36827;&#34892;&#30340;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#20041;&#20998;&#21106;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;&#20363;&#22914;&#65292;&#19982;SOTA&#30456;&#27604;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#39640;&#36798;19&#20493;&#30340;&#35757;&#32451;&#21152;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite its importance for federated learning, continuous learning and many other applications, on-device training remains an open problem for EdgeAI. The problem stems from the large number of operations (e.g., floating point multiplications and additions) and memory consumption required during training by the back-propagation algorithm. Consequently, in this paper, we propose a new gradient filtering approach which enables on-device CNN model training. More precisely, our approach creates a special structure with fewer unique elements in the gradient map, thus significantly reducing the computational complexity and memory consumption of back propagation during training. Extensive experiments on image classification and semantic segmentation with multiple CNN models (e.g., MobileNet, DeepLabV3, UPerNet) and devices (e.g., Raspberry Pi and Jetson Nano) demonstrate the effectiveness and wide applicability of our approach. For example, compared to SOTA, we achieve up to 19$\times$ speedu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#39640;&#26031;&#27169;&#22411;&#20272;&#35745;&#29305;&#24449;&#20998;&#24067;&#21442;&#25968;&#36827;&#34892;&#39044;&#27979;&#27867;&#21270;&#35823;&#24046;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#31867;&#26465;&#20214;&#23494;&#24230;&#36317;&#31163;&#20272;&#35745;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2212.06461</link><description>&lt;p&gt;
&#19968;&#31181;&#39044;&#27979;Few-Shot&#20998;&#31867;&#27867;&#21270;&#30340;&#32479;&#35745;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Statistical Model for Predicting Generalization in Few-Shot Classification. (arXiv:2212.06461v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06461
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#39640;&#26031;&#27169;&#22411;&#20272;&#35745;&#29305;&#24449;&#20998;&#24067;&#21442;&#25968;&#36827;&#34892;&#39044;&#27979;&#27867;&#21270;&#35823;&#24046;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#31867;&#26465;&#20214;&#23494;&#24230;&#36317;&#31163;&#20272;&#35745;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#22120;&#27867;&#21270;&#35823;&#24046;&#30340;&#20272;&#35745;&#36890;&#24120;&#20381;&#36182;&#20110;&#39564;&#35777;&#38598;&#12290;&#28982;&#32780;&#65292;&#22312;Few-Shot&#23398;&#20064;&#22330;&#26223;&#20013;&#65292;&#24456;&#38590;&#33719;&#24471;&#36825;&#26679;&#30340;&#39564;&#35777;&#38598;&#65292;&#36825;&#26159;&#35813;&#39046;&#22495;&#20013;&#19968;&#20010;&#39640;&#24230;&#34987;&#24573;&#35270;&#30340;&#32570;&#28857;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29305;&#24449;&#20998;&#24067;&#30340;&#39640;&#26031;&#27169;&#22411;&#65292;&#36890;&#36807;&#20272;&#35745;&#36825;&#20010;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#25105;&#20204;&#33021;&#22815;&#39044;&#27979;&#22312;&#26032;&#30340;Few-Shot&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#31867;&#26465;&#20214;&#23494;&#24230;&#20043;&#38388;&#20934;&#30830;&#30340;&#36317;&#31163;&#20272;&#35745;&#26159;&#20934;&#30830;&#35780;&#20272;&#27867;&#21270;&#24615;&#33021;&#30340;&#20851;&#38190;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#20559;&#20272;&#35745;&#22120;&#26469;&#35745;&#31639;&#36825;&#20123;&#36317;&#31163;&#65292;&#24182;&#23558;&#20854;&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#25968;&#20540;&#20998;&#26512;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32988;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#65292;&#20363;&#22914;&#30041;&#19968;&#27861;-Cross Validation &#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
The estimation of the generalization error of classifiers often relies on a validation set. Such a set is hardly available in few-shot learning scenarios, a highly disregarded shortcoming in the field. In these scenarios, it is common to rely on features extracted from pre-trained neural networks combined with distance-based classifiers such as nearest class mean. In this work, we introduce a Gaussian model of the feature distribution. By estimating the parameters of this model, we are able to predict the generalization error on new classification tasks with few samples. We observe that accurate distance estimates between class-conditional densities are the key to accurate estimates of the generalization performance. Therefore, we propose an unbiased estimator for these distances and integrate it in our numerical analysis. We empirically show that our approach outperforms alternatives such as the leave-one-out cross-validation strategy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;Re^2TAL&#65292;&#36890;&#36807;&#37325;&#36830;&#39044;&#35757;&#32451;&#30340;&#35270;&#39057;&#20027;&#24178;&#32593;&#32476;&#26469;&#35299;&#20915;&#38271;&#35270;&#39057;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;TAL&#35757;&#32451;&#30340;&#25361;&#25112;&#12290;&#32593;&#32476;&#37325;&#36830;&#26426;&#21046;&#21487;&#20197;&#23558;&#20855;&#26377;&#27531;&#24046;&#36830;&#25509;&#30340;&#20219;&#20309;&#27169;&#22359;&#36716;&#25442;&#20026;&#21487;&#36870;&#27169;&#22359;&#65292;&#20174;&#32780;&#22312;&#35757;&#32451;&#26399;&#38388;&#21487;&#20197;&#28165;&#38500;&#31528;&#37325;&#30340;&#20013;&#38388;&#28608;&#27963;&#12290;</title><link>http://arxiv.org/abs/2211.14053</link><description>&lt;p&gt;
Re^2TAL: &#20026;&#21487;&#36870;&#30340;&#26102;&#38388;&#21160;&#20316;&#23450;&#20301;&#37325;&#36830;&#39044;&#35757;&#32451;&#30340;&#35270;&#39057;&#20027;&#24178;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Re^2TAL: Rewiring Pretrained Video Backbones for Reversible Temporal Action Localization. (arXiv:2211.14053v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;Re^2TAL&#65292;&#36890;&#36807;&#37325;&#36830;&#39044;&#35757;&#32451;&#30340;&#35270;&#39057;&#20027;&#24178;&#32593;&#32476;&#26469;&#35299;&#20915;&#38271;&#35270;&#39057;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;TAL&#35757;&#32451;&#30340;&#25361;&#25112;&#12290;&#32593;&#32476;&#37325;&#36830;&#26426;&#21046;&#21487;&#20197;&#23558;&#20855;&#26377;&#27531;&#24046;&#36830;&#25509;&#30340;&#20219;&#20309;&#27169;&#22359;&#36716;&#25442;&#20026;&#21487;&#36870;&#27169;&#22359;&#65292;&#20174;&#32780;&#22312;&#35757;&#32451;&#26399;&#38388;&#21487;&#20197;&#28165;&#38500;&#31528;&#37325;&#30340;&#20013;&#38388;&#28608;&#27963;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#21160;&#20316;&#23450;&#20301;(TAL)&#38656;&#35201;&#38271;&#26102;&#38388;&#30340;&#25512;&#29702;&#25165;&#33021;&#39044;&#27979;&#21508;&#31181;&#25345;&#32493;&#26102;&#38388;&#21644;&#22797;&#26434;&#20869;&#23481;&#30340;&#21160;&#20316;&#12290;&#30001;&#20110;GPU&#20869;&#23384;&#26377;&#38480;&#65292;&#23545;&#20110;&#38271;&#35270;&#39057;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;TAL&#35757;&#32451;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#22823;&#22810;&#25968;&#26041;&#27861;&#21482;&#33021;&#23545;&#39044;&#20808;&#25552;&#21462;&#30340;&#29305;&#24449;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#26080;&#27861;&#23545;&#20854;&#36827;&#34892;&#20248;&#21270;&#20197;&#35299;&#20915;&#26412;&#22320;&#21270;&#38382;&#39064;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#26412;&#22320;&#21270;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;Re^2TAL&#65292;&#35813;&#26041;&#27861;&#20026;&#21487;&#36870;TAL&#37325;&#36830;&#39044;&#35757;&#32451;&#30340;&#35270;&#39057;&#20027;&#24178;&#32593;&#32476;&#12290;Re^2TAL&#36890;&#36807;&#21487;&#36870;&#27169;&#22359;&#26500;&#24314;&#20027;&#24178;&#32593;&#32476;&#65292;&#20854;&#20013;&#36755;&#20837;&#21487;&#20197;&#20174;&#36755;&#20986;&#20013;&#24674;&#22797;&#65292;&#22240;&#27492;&#22312;&#35757;&#32451;&#26399;&#38388;&#21487;&#20197;&#28165;&#38500;&#31528;&#37325;&#30340;&#20013;&#38388;&#28608;&#27963;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32593;&#32476;&#37325;&#36830;&#26426;&#21046;&#65292;&#23558;&#20855;&#26377;&#27531;&#24046;&#36830;&#25509;&#30340;&#20219;&#20309;&#27169;&#22359;&#36716;&#25442;&#20026;&#21487;&#36870;&#27169;&#22359;&#65292;&#32780;&#19981;&#25913;&#21464;&#20219;&#20309;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal action localization (TAL) requires long-form reasoning to predict actions of various durations and complex content. Given limited GPU memory, training TAL end to end (i.e., from videos to predictions) on long videos is a significant challenge. Most methods can only train on pre-extracted features without optimizing them for the localization problem, consequently limiting localization performance. In this work, to extend the potential in TAL networks, we propose a novel end-to-end method Re2TAL, which rewires pretrained video backbones for reversible TAL. Re2TAL builds a backbone with reversible modules, where the input can be recovered from the output such that the bulky intermediate activations can be cleared from memory during training. Instead of designing one single type of reversible module, we propose a network rewiring mechanism, to transform any module with a residual connection to a reversible module without changing any parameters. This provides two benefits: (1) a l
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#21508;&#21521;&#24322;&#24615;&#25193;&#25955;&#21644;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#24341;&#23548;&#28145;&#24230;&#36229;&#20998;&#36776;&#29575;&#65292;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#32467;&#26524;&#65292;&#22312;x32&#32553;&#25918;&#19979;&#30340;&#24615;&#33021;&#25552;&#21319;&#26368;&#22823;&#12290;</title><link>http://arxiv.org/abs/2211.11592</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#21508;&#21521;&#24322;&#24615;&#25193;&#25955;&#36827;&#34892;&#24341;&#23548;&#30340;&#28145;&#24230;&#36229;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
Guided Depth Super-Resolution by Deep Anisotropic Diffusion. (arXiv:2211.11592v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#21508;&#21521;&#24322;&#24615;&#25193;&#25955;&#21644;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#24341;&#23548;&#28145;&#24230;&#36229;&#20998;&#36776;&#29575;&#65292;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#32467;&#26524;&#65292;&#22312;x32&#32553;&#25918;&#19979;&#30340;&#24615;&#33021;&#25552;&#21319;&#26368;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;RGB&#22270;&#20687;&#30340;&#25351;&#23548;&#23454;&#29616;&#28145;&#24230;&#22270;&#20687;&#30340;&#36229;&#20998;&#36776;&#29575;&#26159;&#28041;&#21450;&#21040;&#26426;&#22120;&#20154;&#65292;&#21307;&#23398;&#25104;&#20687;&#21644;&#36965;&#24863;&#31561;&#22810;&#20010;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#27492;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#31361;&#26174;&#20102;&#23558;&#29616;&#20195;&#26041;&#27861;&#19982;&#26356;&#20026;&#27491;&#24335;&#30340;&#26694;&#26550;&#30456;&#32467;&#21512;&#30340;&#20215;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#24341;&#23548;&#21508;&#21521;&#24322;&#24615;&#25193;&#25955;&#19982;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#25512;&#36827;&#20102;&#24341;&#23548;&#28145;&#24230;&#36229;&#20998;&#36776;&#29575;&#30340;&#29366;&#24577;&#12290;&#25193;&#25955;&#30340;&#36793;&#32536;&#36716;&#31227;/&#22686;&#24378;&#29305;&#24615;&#30001;&#29616;&#20195;&#32593;&#32476;&#30340;&#19978;&#19979;&#25991;&#25512;&#29702;&#33021;&#21147;&#21152;&#24378;&#65292;&#20005;&#26684;&#30340;&#35843;&#25972;&#27493;&#39588;&#30830;&#20445;&#23436;&#20840;&#31896;&#21512;&#21040;&#28304;&#22270;&#20687;&#19978;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#24120;&#29992;&#30340;&#24341;&#23548;&#28145;&#24230;&#36229;&#20998;&#36776;&#29575;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#32467;&#26524;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#24615;&#33021;&#22312;&#36739;&#22823;&#27604;&#20363;&#23610;&#19979;&#65292;&#20363;&#22914;x32&#32553;&#25918;&#26102;&#33719;&#24471;&#20102;&#26368;&#22823;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performing super-resolution of a depth image using the guidance from an RGB image is a problem that concerns several fields, such as robotics, medical imaging, and remote sensing. While deep learning methods have achieved good results in this problem, recent work highlighted the value of combining modern methods with more formal frameworks. In this work, we propose a novel approach which combines guided anisotropic diffusion with a deep convolutional network and advances the state of the art for guided depth super-resolution. The edge transferring/enhancing properties of the diffusion are boosted by the contextual reasoning capabilities of modern networks, and a strict adjustment step guarantees perfect adherence to the source image. We achieve unprecedented results in three commonly used benchmarks for guided depth super-resolution. The performance gain compared to other methods is the largest at larger scales, such as x32 scaling. Code (https://github.com/prs-eth/Diffusion-Super-Reso
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;UMFuse&#65292;&#19968;&#31181;&#21033;&#29992;&#22810;&#35270;&#22270;&#34701;&#21512;&#22788;&#29702;&#20154;&#20307;&#32534;&#36753;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#22810;&#35270;&#22270;&#34701;&#21512;&#32593;&#32476;&#65292;&#21033;&#29992;&#22810;&#28304;&#22270;&#20687;&#20013;&#30340;&#20851;&#38190;&#28857;&#21644;&#32441;&#29702;&#29983;&#25104;&#27599;&#20687;&#32032;&#22806;&#35266;&#26816;&#32034;&#26144;&#23556;&#65292;&#20174;&#32780;&#26368;&#23567;&#21270;&#20449;&#24687;&#20002;&#22833;&#24182;&#29983;&#25104;&#20934;&#30830;&#30340;&#19979;&#23618;&#20154;&#20307;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2211.10157</link><description>&lt;p&gt;
UMFuse&#65306;&#29992;&#20110;&#20154;&#20307;&#32534;&#36753;&#24212;&#29992;&#30340;&#32479;&#19968;&#22810;&#35270;&#22270;&#34701;&#21512;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
UMFuse: Unified Multi View Fusion for Human Editing applications. (arXiv:2211.10157v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;UMFuse&#65292;&#19968;&#31181;&#21033;&#29992;&#22810;&#35270;&#22270;&#34701;&#21512;&#22788;&#29702;&#20154;&#20307;&#32534;&#36753;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#22810;&#35270;&#22270;&#34701;&#21512;&#32593;&#32476;&#65292;&#21033;&#29992;&#22810;&#28304;&#22270;&#20687;&#20013;&#30340;&#20851;&#38190;&#28857;&#21644;&#32441;&#29702;&#29983;&#25104;&#27599;&#20687;&#32032;&#22806;&#35266;&#26816;&#32034;&#26144;&#23556;&#65292;&#20174;&#32780;&#26368;&#23567;&#21270;&#20449;&#24687;&#20002;&#22833;&#24182;&#29983;&#25104;&#20934;&#30830;&#30340;&#19979;&#23618;&#20154;&#20307;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20154;&#20307;&#32534;&#36753;&#25216;&#26415;&#65292;&#35270;&#35273;&#31038;&#21306;&#24050;&#32463;&#25506;&#35752;&#20102;&#20247;&#22810;&#30340;&#23039;&#21183;&#24341;&#23548;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#20173;&#28982;&#20351;&#29992;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#20844;&#24335;&#65292;&#20854;&#20013;&#19968;&#20010;&#21333;&#19968;&#30340;&#22270;&#20687;&#34987;&#32473;&#23450;&#20026;&#36755;&#20837;&#65292;&#20197;&#20135;&#29983;&#19968;&#20010;&#32534;&#36753;&#36807;&#30340;&#22270;&#20687;&#20026;&#36755;&#20986;&#12290;&#24403;&#30446;&#26631;&#23039;&#21183;&#19982;&#36755;&#20837;&#23039;&#21183;&#26377;&#26174;&#33879;&#24046;&#24322;&#26102;&#65292;&#36825;&#19968;&#30446;&#26631;&#21464;&#24471;&#19981;&#26126;&#30830;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21017;&#37319;&#29992;&#20462;&#34917;&#25110;&#26679;&#24335;&#36716;&#31227;&#26469;&#22788;&#29702;&#36974;&#25377;&#24182;&#20445;&#30041;&#20869;&#23481;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21033;&#29992;&#22810;&#20010;&#35270;&#22270;&#26469;&#26368;&#23567;&#21270;&#20449;&#24687;&#20002;&#22833;&#24182;&#29983;&#25104;&#19979;&#23618;&#20154;&#20307;&#27169;&#22411;&#30340;&#20934;&#30830;&#34920;&#31034;&#12290;&#20026;&#20102;&#34701;&#21512;&#22810;&#20010;&#35270;&#28857;&#30340;&#30693;&#35782;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#35270;&#22270;&#34701;&#21512;&#32593;&#32476;&#65292;&#23427;&#20174;&#22810;&#20010;&#28304;&#22270;&#20687;&#20013;&#33719;&#21462;&#23039;&#21183;&#20851;&#38190;&#28857;&#21644;&#32441;&#29702;&#65292;&#24182;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#27599;&#20687;&#32032;&#22806;&#35266;&#26816;&#32034;&#26144;&#23556;&#12290;&#27492;&#21518;&#65292;&#20174;&#21333;&#35270;&#35282;&#20154;&#20307;&#25442;&#35013;&#20219;&#21153;&#20013;&#35757;&#32451;&#24471;&#21040;&#30340;&#32534;&#30721;&#34987;&#21512;&#24182;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous pose-guided human editing methods have been explored by the vision community due to their extensive practical applications. However, most of these methods still use an image-to-image formulation in which a single image is given as input to produce an edited image as output. This objective becomes ill-defined in cases when the target pose differs significantly from the input pose. Existing methods then resort to in-painting or style transfer to handle occlusions and preserve content. In this paper, we explore the utilization of multiple views to minimize the issue of missing information and generate an accurate representation of the underlying human model. To fuse knowledge from multiple viewpoints, we design a multi-view fusion network that takes the pose key points and texture from multiple source images and generates an explainable per-pixel appearance retrieval map. Thereafter, the encodings from a separate network (trained on a single-view human reposing task) are merged i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ERNIE-ViLG 2.0&#65292;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#21435;&#22122;&#20108;&#27425;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#21512;&#24182;&#22330;&#26223;&#20851;&#38190;&#20803;&#32032;&#25991;&#26412;&#21644;&#35270;&#35273;&#30693;&#35782;&#20197;&#21450;&#20351;&#29992;&#19981;&#21516;&#30340;&#21435;&#22122;&#19987;&#23478;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#20013;&#25991;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#36136;&#37327;&#21644;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2210.15257</link><description>&lt;p&gt;
ERNIE-ViLG 2.0&#65306;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#21435;&#22122;&#20108;&#27425;&#28151;&#21512;&#27169;&#22411;&#20248;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model with Knowledge-Enhanced Mixture-of-Denoising-Experts. (arXiv:2210.15257v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15257
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ERNIE-ViLG 2.0&#65292;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#21435;&#22122;&#20108;&#27425;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#21512;&#24182;&#22330;&#26223;&#20851;&#38190;&#20803;&#32032;&#25991;&#26412;&#21644;&#35270;&#35273;&#30693;&#35782;&#20197;&#21450;&#20351;&#29992;&#19981;&#21516;&#30340;&#21435;&#22122;&#19987;&#23478;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#20013;&#25991;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#36136;&#37327;&#21644;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24357;&#28459;&#27169;&#22411;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#36827;&#23637;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#25991;&#26412;&#26465;&#20214;&#19979;&#20135;&#29983;&#39640;&#20998;&#36776;&#29575;&#30340;&#36924;&#30495;&#22270;&#20687;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38656;&#35201;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#36825;&#38480;&#21046;&#20102;&#22270;&#20687;&#20445;&#30495;&#24230;&#21644;&#25991;&#26412;&#30456;&#20851;&#24615;&#30340;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ERNIE-ViLG 2.0&#65292;&#19968;&#31181;&#22823;&#35268;&#27169;&#30340;&#20013;&#25991;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#36880;&#27493;&#25552;&#39640;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#65306;&#65288;1&#65289;&#21512;&#24182;&#22330;&#26223;&#20013;&#20851;&#38190;&#20803;&#32032;&#30340;&#32454;&#31890;&#24230;&#25991;&#26412;&#21644;&#35270;&#35273;&#30693;&#35782;&#65292;&#65288;2&#65289;&#22312;&#19981;&#21516;&#30340;&#21435;&#22122;&#38454;&#27573;&#21033;&#29992;&#19981;&#21516;&#30340;&#21435;&#22122;&#19987;&#23478;&#12290;&#36890;&#36807;&#36825;&#20123;&#26426;&#21046;&#65292;ERNIE-ViLG 2.0&#19981;&#20165;&#22312;MS-COCO&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;FID&#24471;&#20998;&#38646;&#23556;&#20987;&#26368;&#20339;&#34920;&#29616;&#65292;&#32780;&#19988;&#22312;ViLG-300&#21452;&#35821;&#25552;&#31034;&#38598;&#30340;&#20154;&#31867;&#35780;&#20272;&#20013;&#22312;&#22270;&#20687;&#20445;&#30495;&#24230;&#21644;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#26368;&#26032;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in diffusion models has revolutionized the popular technology of text-to-image generation. While existing approaches could produce photorealistic high-resolution images with text conditions, there are still several open problems to be solved, which limits the further improvement of image fidelity and text relevancy. In this paper, we propose ERNIE-ViLG 2.0, a large-scale Chinese text-to-image diffusion model, to progressively upgrade the quality of generated images by: (1) incorporating fine-grained textual and visual knowledge of key elements in the scene, and (2) utilizing different denoising experts at different denoising stages. With the proposed mechanisms, ERNIE-ViLG 2.0 not only achieves a new state-of-the-art on MS-COCO with zero-shot FID score of 6.75, but also significantly outperforms recent models in terms of image fidelity and image-text alignment, with side-by-side human evaluation on the bilingual prompt set ViLG-300.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#31034;&#33539;&#23398;&#20064;&#21644;&#35838;&#31243;&#23398;&#20064;&#30340;&#20219;&#21153;&#38454;&#27573;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#36870;&#24378;&#21270;&#23398;&#20064;&#33258;&#21160;&#29983;&#25104;&#35838;&#31243;&#24207;&#21015;&#65292;&#36880;&#27493;&#22686;&#21152;&#20219;&#21153;&#22797;&#26434;&#24230;&#65292;&#20197;&#24110;&#21161;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#22312;&#31232;&#30095;&#22870;&#21169;&#39046;&#22495;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2210.10999</link><description>&lt;p&gt;
&#20219;&#21153;&#38454;&#27573;&#21270;&#65306;&#26469;&#33258;&#31034;&#33539;&#30340;&#33258;&#21160;&#35838;&#31243;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Task Phasing: Automated Curriculum Learning from Demonstrations. (arXiv:2210.10999v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#31034;&#33539;&#23398;&#20064;&#21644;&#35838;&#31243;&#23398;&#20064;&#30340;&#20219;&#21153;&#38454;&#27573;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#36870;&#24378;&#21270;&#23398;&#20064;&#33258;&#21160;&#29983;&#25104;&#35838;&#31243;&#24207;&#21015;&#65292;&#36880;&#27493;&#22686;&#21152;&#20219;&#21153;&#22797;&#26434;&#24230;&#65292;&#20197;&#24110;&#21161;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#22312;&#31232;&#30095;&#22870;&#21169;&#39046;&#22495;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24212;&#29992;&#20110;&#31232;&#30095;&#22870;&#21169;&#22495;&#36890;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#32570;&#20047;&#36275;&#22815;&#30340;&#24341;&#23548;&#20449;&#21495;&#12290;&#35299;&#20915;&#27492;&#31867;&#39046;&#22495;&#30340;&#24120;&#35265;RL&#25216;&#26415;&#21253;&#25324;&#65288;1&#65289;&#20174;&#31034;&#33539;&#23398;&#20064;&#21644;&#65288;2&#65289;&#35838;&#31243;&#23398;&#20064;&#12290;&#34429;&#28982;&#36825;&#20004;&#31181;&#26041;&#27861;&#24050;&#32463;&#34987;&#35814;&#32454;&#30740;&#31350;&#65292;&#20294;&#23427;&#20204;&#24456;&#23569;&#34987;&#21516;&#26102;&#32771;&#34385;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;&#31034;&#33539;&#30340;&#21407;&#21017;&#24615;&#20219;&#21153;&#38454;&#27573;&#21270;&#26041;&#27861;&#26469;&#23454;&#29616;&#35813;&#30446;&#30340;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#31034;&#33539;&#33258;&#21160;&#29983;&#25104;&#35838;&#31243;&#24207;&#21015;&#12290;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#65288;&#27425;&#20248;&#65289;&#28436;&#31034;&#30340;&#36870;RL&#23450;&#20041;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#21021;&#22987;&#20219;&#21153;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30340;&#20219;&#21153;&#38454;&#27573;&#21270;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36880;&#28176;&#22686;&#21152;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#65292;&#30452;&#21040;&#30446;&#26631;&#20219;&#21153;&#65292;&#21516;&#26102;&#22312;&#27599;&#20010;&#38454;&#27573;&#36845;&#20195;&#20013;&#37325;&#26032;&#35843;&#25972;RL&#20195;&#29702;&#12290;&#32771;&#34385;&#20102;&#20004;&#31181;&#20998;&#38454;&#27573;&#26041;&#27861;&#65306;&#65288;1&#65289;&#36880;&#27493;&#22686;&#21152;RL&#20195;&#29702;&#22788;&#20110;&#25511;&#21046;&#19979;&#30340;&#26102;&#38388;&#27493;&#25968;&#30340;&#27604;&#20363;&#65292;&#20197;&#21450;&#65288;2&#65289;&#36880;&#27493;&#28120;&#27760;&#24341;&#23548;&#24615;&#20449;&#24687;&#22870;&#21169;&#20989;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20445;&#35777;&#25910;&#25947;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Applying reinforcement learning (RL) to sparse reward domains is notoriously challenging due to insufficient guiding signals. Common RL techniques for addressing such domains include (1) learning from demonstrations and (2) curriculum learning. While these two approaches have been studied in detail, they have rarely been considered together. This paper aims to do so by introducing a principled task phasing approach that uses demonstrations to automatically generate a curriculum sequence. Using inverse RL from (suboptimal) demonstrations we define a simple initial task. Our task phasing approach then provides a framework to gradually increase the complexity of the task all the way to the target task, while retuning the RL agent in each phasing iteration. Two approaches for phasing are considered: (1) gradually increasing the proportion of time steps an RL agent is in control, and (2) phasing out a guiding informative reward function. We present conditions that guarantee the convergence 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#30830;&#20445;&#20010;&#20154;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#32852;&#37030;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#23545;&#20110;&#25308;&#21344;&#24237;&#23041;&#32961;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.14547</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#20303;&#23429;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#30340;&#23433;&#20840;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Secure Federated Learning Framework for Residential Short Term Load Forecasting. (arXiv:2209.14547v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#30830;&#20445;&#20010;&#20154;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#32852;&#37030;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#23545;&#20110;&#25308;&#21344;&#24237;&#23041;&#32961;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#30005;&#34920;&#26159;&#20934;&#30830;&#38656;&#27714;&#39044;&#27979;&#30340;&#20851;&#38190;&#65292;&#20294;&#30001;&#20110;&#28040;&#36153;&#32773;&#38544;&#31169;&#21644;&#25968;&#25454;&#27844;&#38706;&#31561;&#38382;&#39064;&#65292;&#23384;&#22312;&#33509;&#24178;&#32570;&#28857;&#12290;&#26368;&#36817;&#30340;&#25991;&#29486;&#20013;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#34987;&#25506;&#32034;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#26367;&#20195;&#26041;&#26696;&#65292;&#23427;&#21487;&#20197;&#22312;&#19981;&#26292;&#38706;&#31169;&#26377;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#27169;&#22411;&#30340;&#21327;&#20316;&#23398;&#20064;&#65292;&#29992;&#20110;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#12290;&#23613;&#31649;FL&#20855;&#26377;&#20248;&#28857;&#65292;&#20294;&#26631;&#20934;FL&#20173;&#28982;&#26131;&#21463;&#21517;&#20026;&#25308;&#21344;&#24237;&#25915;&#20987;&#30340;&#38590;&#20197;&#22788;&#29702;&#30340;&#32593;&#32476;&#23041;&#32961;&#30340;&#24433;&#21709;&#65292;&#36825;&#31181;&#25915;&#20987;&#26159;&#30001;&#26377;&#32570;&#38519;&#21644;/&#25110;&#24694;&#24847;&#23458;&#25143;&#21457;&#36215;&#30340;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#25552;&#39640;&#32852;&#37030;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#23545;&#25308;&#21344;&#24237;&#23041;&#32961;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26368;&#20808;&#36827;&#30340;&#24046;&#20998;&#38544;&#31169;&#23433;&#20840;&#30340;FL-based&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#30830;&#20445;&#20102;&#20010;&#20154;&#26234;&#33021;&#30005;&#34920;&#25968;&#25454;&#30340;&#38544;&#31169;&#65292;&#21516;&#26102;&#20445;&#25252;&#20102;FL&#27169;&#22411;&#21644;&#26550;&#26500;&#30340;&#23433;&#20840;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#26694;&#26550;&#21033;&#29992;&#20102;&#36890;&#36807;Sign Stochastic Gradient Descent&#65288;SignSGD&#65289;&#31639;&#27861;&#36827;&#34892;&#26799;&#24230;&#37327;&#21270;&#30340;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
Smart meter measurements, though critical for accurate demand forecasting, face several drawbacks including consumers' privacy, data breach issues, to name a few. Recent literature has explored Federated Learning (FL) as a promising privacy-preserving machine learning alternative which enables collaborative learning of a model without exposing private raw data for short term load forecasting. Despite its virtue, standard FL is still vulnerable to an intractable cyber threat known as Byzantine attack carried out by faulty and/or malicious clients. Therefore, to improve the robustness of federated short-term load forecasting against Byzantine threats, we develop a state-of-the-art differentially private secured FL-based framework that ensures the privacy of the individual smart meter's data while protect the security of FL models and architecture. Our proposed framework leverages the idea of gradient quantization through the Sign Stochastic Gradient Descent (SignSGD) algorithm, where the
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#27861;&#24459;&#25351;&#23548;&#20195;&#30721;&#8221;&#29702;&#24565;&#65292;&#21033;&#29992;&#27861;&#24459;&#20449;&#24687;&#23398;&#23558;&#27861;&#24459;&#30693;&#35782;&#21644;&#25512;&#29702;&#23884;&#20837;&#21040;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#20174;&#32780;&#20351;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#30446;&#26631;&#21644;&#31038;&#20250;&#20215;&#20540;&#20445;&#25345;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2209.13020</link><description>&lt;p&gt;
&#27861;&#24459;&#24341;&#23548;&#20195;&#30721;&#65306;&#19968;&#31181;&#27861;&#24459;&#20449;&#24687;&#23398;&#26041;&#27861;&#26469;&#20351;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20445;&#25345;&#19968;&#33268;
&lt;/p&gt;
&lt;p&gt;
Law Informs Code: A Legal Informatics Approach to Aligning Artificial Intelligence with Humans. (arXiv:2209.13020v13 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13020
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#27861;&#24459;&#25351;&#23548;&#20195;&#30721;&#8221;&#29702;&#24565;&#65292;&#21033;&#29992;&#27861;&#24459;&#20449;&#24687;&#23398;&#23558;&#27861;&#24459;&#30693;&#35782;&#21644;&#25512;&#29702;&#23884;&#20837;&#21040;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#20174;&#32780;&#20351;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#30446;&#26631;&#21644;&#31038;&#20250;&#20215;&#20540;&#20445;&#25345;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#25105;&#20204;&#26080;&#27861;&#21487;&#38752;&#22320;&#25351;&#23450;&#20154;&#31867;&#30340;&#30446;&#26631;&#21644;&#31038;&#20250;&#20215;&#20540;&#65292;&#20197;&#24341;&#23548;&#20154;&#24037;&#26234;&#33021;&#30340;&#34892;&#20026;&#12290;&#21046;&#23450;&#27861;&#24459;&#21644;&#35299;&#37322;&#27861;&#24459;&#26500;&#25104;&#20102;&#19968;&#31181;&#35745;&#31639;&#24341;&#25806;&#65292;&#23558;&#19981;&#36879;&#26126;&#30340;&#20154;&#31867;&#20215;&#20540;&#36716;&#21270;&#20026;&#26131;&#35835;&#30340;&#25351;&#20196;&#12290; &#8220;&#27861;&#24459;&#25351;&#23548;&#20195;&#30721;&#8221;&#26159;&#23884;&#20837;&#20102;&#27861;&#24459;&#30693;&#35782;&#21644;&#25512;&#29702;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#35758;&#31243;&#12290;&#31867;&#20284;&#20110;&#21512;&#21516;&#24403;&#20107;&#20154;&#26080;&#27861;&#39044;&#35265;&#20182;&#20204;&#26410;&#26469;&#20851;&#31995;&#30340;&#27599;&#20010;&#28508;&#22312;&#21464;&#25968;&#65292;&#31435;&#27861;&#32773;&#26080;&#27861;&#39044;&#27979;&#20854;&#25552;&#20986;&#30340;&#27861;&#26696;&#23558;&#36866;&#29992;&#30340;&#25152;&#26377;&#24773;&#20917;&#65292;&#25105;&#20204;&#26080;&#27861;&#25552;&#21069;&#26126;&#30830;&#35268;&#21017;&#65292;&#20197;&#21487;&#38752;&#22320;&#24341;&#23548;&#33391;&#22909;&#30340;&#20154;&#24037;&#26234;&#33021;&#34892;&#20026;&#12290;&#27861;&#24459;&#29702;&#35770;&#21644;&#23454;&#36341;&#24050;&#32463;&#24320;&#21457;&#20986;&#21508;&#31181;&#24037;&#20855;&#26469;&#35299;&#20915;&#36825;&#20123;&#35268;&#23450;&#38382;&#39064;&#12290;&#19982;&#27861;&#24459;&#26356;&#20026;&#26222;&#36890;&#30340;&#29992;&#36884;&#65288;&#20363;&#22914;&#36890;&#36807;&#21046;&#35009;&#23041;&#32961;&#26469;&#38459;&#27490;&#19981;&#33391;&#34892;&#20026;&#65289;&#30456;&#21453;&#65292;&#27861;&#24459;&#20316;&#20026;&#19968;&#31181;&#34920;&#36798;&#20154;&#31867;&#27807;&#36890;&#30446;&#26631;&#21644;&#20215;&#20540;&#30340;&#34920;&#29616;&#65292;&#21487;&#20197;&#24341;&#23548;&#20154;&#24037;&#26234;&#33021;&#20195;&#30721;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#27861;&#24459;&#20449;&#24687;&#23398;&#26159;&#35745;&#31639;&#35268;&#21017;&#21644;&#31995;&#32479;&#29992;&#20110;&#34920;&#31034;&#65292;&#20998;&#26512;&#21644;&#25805;&#20316;&#27861;&#24459;&#30693;&#35782;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#12290;&#27861;&#24459;&#25351;&#23548;&#20195;&#30721;&#21033;&#29992;&#27861;&#24459;&#20449;&#24687;&#23398;&#26469;&#20351;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#30446;&#26631;&#21644;&#31038;&#20250;&#20215;&#20540;&#20445;&#25345;&#19968;&#33268;&#65292;&#20197;&#19968;&#31181;&#36879;&#26126;&#65292;&#36127;&#36131;&#65292;&#28789;&#27963;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We are currently unable to specify human goals and societal values in a way that reliably directs AI behavior. Law-making and legal interpretation form a computational engine that converts opaque human values into legible directives. "Law Informs Code" is the research agenda embedding legal knowledge and reasoning in AI. Similar to how parties to a legal contract cannot foresee every potential contingency of their future relationship, and legislators cannot predict all the circumstances under which their proposed bills will be applied, we cannot ex ante specify rules that provably direct good AI behavior. Legal theory and practice have developed arrays of tools to address these specification problems. For instance, legal standards allow humans to develop shared understandings and adapt them to novel situations. In contrast to more prosaic uses of the law (e.g., as a deterrent of bad behavior through the threat of sanction), leveraged as an expression of how humans communicate their goa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#30340;&#35821;&#35328;&#31867;&#22411;&#21644;&#31639;&#27861;&#65292;&#21487;&#20197;&#38477;&#20302;&#27169;&#22411;&#26816;&#27979;&#21644;&#21453;&#24212;&#21512;&#25104;&#31561;&#38382;&#39064;&#30340;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2209.02307</link><description>&lt;p&gt;
&#23433;&#20840;&#21644;&#21327;&#23433;&#20840;&#35821;&#35328;&#30340;&#19968;&#38454;&#36923;&#36753;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
A first-order logic characterization of safety and co-safety languages. (arXiv:2209.02307v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#30340;&#35821;&#35328;&#31867;&#22411;&#21644;&#31639;&#27861;&#65292;&#21487;&#20197;&#38477;&#20302;&#27169;&#22411;&#26816;&#27979;&#21644;&#21453;&#24212;&#21512;&#25104;&#31561;&#38382;&#39064;&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;&#65288;LTL&#65289;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#26102;&#38388;&#36923;&#36753;&#20043;&#19968;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#21508;&#20010;&#39046;&#22495;&#12290;LTL&#30340;&#24378;&#22823;&#22522;&#30784;&#24615;&#36136;&#20043;&#19968;&#26159;&#20854;&#31561;&#20215;&#20110;&#26080;&#35745;&#25968;&#969;-&#33258;&#21160;&#26426;&#12289;&#26143;&#33258;&#30001;&#969;-&#27491;&#21017;&#34920;&#36798;&#24335;&#20197;&#21450;&#65288;&#36890;&#36807;Kamp&#23450;&#29702;&#65289;&#32447;&#24615;&#24207;&#30340;&#19968;&#38454;&#29702;&#35770;&#65288;FO-TLO&#65289;&#12290;&#23433;&#20840;&#35821;&#35328;&#21644;&#21327;&#23433;&#20840;&#35821;&#35328;&#20998;&#21035;&#25351;&#21482;&#38656;&#26377;&#38480;&#21069;&#32512;&#20415;&#21487;&#30830;&#23450;&#35813;&#21333;&#35789;&#23646;&#20110;&#25110;&#19981;&#23646;&#20110;&#35813;&#35821;&#35328;&#30340;&#35821;&#35328;&#31867;&#22411;&#12290;SafetyLTL&#65288;&#21644;coSafetyLTL&#65289;&#26159;LTL&#30340;&#19968;&#20010;&#29255;&#27573;&#65292;&#20854;&#20013;&#20165;&#20801;&#35768;&#20351;&#29992;&#20840;&#23616;&#65288;&#23384;&#22312;&#65289;&#26102;&#38388;&#20462;&#39280;&#35789;&#26469;&#35782;&#21035;&#23433;&#20840;&#65288;&#21327;&#23433;&#20840;&#65289;&#35821;&#35328;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#24341;&#20837;FO-TLO&#30340;&#29255;&#27573;SafetyFO&#20197;&#21450;&#20854;&#23545;&#20598;&#30340;coSafetyFO&#65292;&#23427;&#20204;&#22312;&#34920;&#36798;&#33021;&#21147;&#19978;&#37117;&#26159;&#23436;&#22791;&#30340;&#65288;except&#19968;&#20123;&#36793;&#30028;&#24773;&#20917;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear Temporal Logic (LTL) is one of the most popular temporal logics, that comes into play in a variety of branches of computer science. Among the various reasons of its widespread use there are its strong foundational properties: LTL is equivalent to counter-free omega-automata, to star-free omega-regular expressions, and (by Kamp's theorem) to the First-Order Theory of Linear Orders (FO-TLO). Safety and co-safety languages, where a finite prefix suffices to establish whether a word does not belong or belongs to the language, respectively, play a crucial role in lowering the complexity of problems like model checking and reactive synthesis for LTL. SafetyLTL (resp., coSafetyLTL) is a fragment of LTL where only universal (resp., existential) temporal modalities are allowed, that recognises safety (resp., co-safety) languages only. The main contribution of this paper is the introduction of a fragment of FO-TLO, called SafetyFO, and of its dual coSafetyFO, which are expressively comple
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.13508</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65306;&#19968;&#20221;&#32508;&#36848;&#21644;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation techniques in time series domain: A survey and taxonomy. (arXiv:2206.13508v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.13508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#24314;&#27169;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21033;&#29992;&#20854;&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#20013;&#20986;&#33394;&#24615;&#33021;&#30340;&#26041;&#24335;&#24182;&#19981;&#38656;&#35201;&#22826;&#38271;&#26102;&#38388;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#20005;&#37325;&#20381;&#36182;&#20110;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#19968;&#33268;&#24615;&#12290;&#36825;&#20123;&#29305;&#24449;&#36890;&#24120;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24182;&#19981;&#20016;&#23500;&#65292;&#36890;&#24120;&#21463;&#21040;&#38480;&#21046;&#21644;&#38656;&#35201;&#20445;&#35777;&#30340;&#32422;&#26463;&#12290;&#22240;&#27492;&#65292;&#25552;&#39640;&#25968;&#25454;&#37327;&#30340;&#26377;&#25928;&#26041;&#27861;&#26159;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#26080;&#35770;&#26159;&#36890;&#36807;&#28155;&#21152;&#22122;&#22768;&#25110;&#32622;&#25442;&#36824;&#26159;&#29983;&#25104;&#26032;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#35813;&#39046;&#22495;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#29616;&#29366;&#65292;&#25552;&#20379;&#20102;&#25152;&#26377;&#21487;&#29992;&#31639;&#27861;&#30340;&#27010;&#36848;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#30456;&#20851;&#30740;&#31350;&#30340;&#20998;&#31867;&#27861;&#12290;&#19981;&#21516;&#21464;&#20307;&#30340;&#25928;&#29575;&#23558;&#20316;&#20026;&#35813;&#36807;&#31243;&#30340;&#20013;&#24515;&#37096;&#20998;&#36827;&#34892;&#35780;&#20272;&#65292;&#21516;&#26102;&#36824;&#23558;&#35780;&#20272;&#19981;&#21516;&#30340;&#24615;&#33021;&#25351;&#26631;&#20197;&#21450;&#27599;&#20010;&#27169;&#22411;&#30340;&#20027;&#35201;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the latest advances in Deep Learning-based} generative models, it has not taken long to take advantage of their remarkable performance in the area of time series. Deep neural networks used to work with time series heavily depend on the size and consistency of the datasets used in training. These features are not usually abundant in the real world, where they are usually limited and often have constraints that must be guaranteed. Therefore, an effective way to increase the amount of data is by using Data Augmentation techniques, either by adding noise or permutations and by generating new synthetic data. This work systematically reviews the current state-of-the-art in the area to provide an overview of all available algorithms and proposes a taxonomy of the most relevant research. The efficiency of the different variants will be evaluated as a central part of the process, as well as the different metrics to evaluate the performance and the main problems concerning each model will b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#21521;XR&#24212;&#29992;&#30340;&#36793;&#32536;AI&#30828;&#20214;&#35774;&#35745;&#31354;&#38388;&#12290;&#36890;&#36807;&#24341;&#20837;&#26032;&#22411;&#38750;&#26131;&#22833;&#24615;&#20869;&#23384;&#25216;&#26415;&#65292;&#22312;&#28385;&#36275;&#26368;&#20302;IPS&#30340;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#33021;&#25928;&#65292;&#21487;&#20943;&#23569;90%&#30340;&#20869;&#23384;&#27969;&#37327;&#12290;</title><link>http://arxiv.org/abs/2206.06780</link><description>&lt;p&gt;
&#38754;&#21521;XR&#24212;&#29992;&#30340;&#36793;&#32536;AI&#30828;&#20214;&#35760;&#24518;&#23548;&#21521;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Memory-Oriented Design-Space Exploration of Edge-AI Hardware for XR Applications. (arXiv:2206.06780v3 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#21521;XR&#24212;&#29992;&#30340;&#36793;&#32536;AI&#30828;&#20214;&#35774;&#35745;&#31354;&#38388;&#12290;&#36890;&#36807;&#24341;&#20837;&#26032;&#22411;&#38750;&#26131;&#22833;&#24615;&#20869;&#23384;&#25216;&#26415;&#65292;&#22312;&#28385;&#36275;&#26368;&#20302;IPS&#30340;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#33021;&#25928;&#65292;&#21487;&#20943;&#23569;90%&#30340;&#20869;&#23384;&#27969;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#21151;&#32791;&#30340;&#36793;&#32536;AI&#33021;&#21147;&#23545;&#20110;&#25903;&#25345;&#8220;&#20803;&#23431;&#23449;&#8221;&#24895;&#26223;&#30340;&#35774;&#22791;&#19978;&#30340;&#25193;&#23637;&#29616;&#23454;&#65288;XR&#65289;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#20856;&#22411;&#30340;XR&#24037;&#20316;&#36127;&#36733;&#65306;&#65288;i&#65289;&#25163;&#37096;&#26816;&#27979;&#21644;&#65288;ii&#65289;&#30524;&#30555;&#20998;&#21106;&#65292;&#24182;&#36827;&#34892;&#20102;&#30828;&#20214;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#12290;&#23545;&#20110;&#36825;&#20004;&#20010;&#24212;&#29992;&#65292;&#25105;&#20204;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#20998;&#26512;&#20102;&#37327;&#21270;&#21644;&#30828;&#20214;&#29305;&#23450;&#29942;&#39048;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#27169;&#25311;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#20010;CPU&#21644;&#20004;&#20010;&#31995;&#32479;&#25512;&#29702;&#21152;&#36895;&#22120;&#23454;&#29616;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#30828;&#20214;&#35299;&#20915;&#26041;&#26696;&#19982;&#20808;&#36827;&#30340;&#25216;&#26415;&#33410;&#28857;&#36827;&#34892;&#27604;&#36739;&#12290;&#35780;&#20272;&#20102;&#23558;&#26368;&#20808;&#36827;&#30340;&#26032;&#22411;&#38750;&#26131;&#22833;&#24615;&#20869;&#23384;&#25216;&#26415;&#65288;STT / SOT / VGSOT MRAM&#65289;&#38598;&#25104;&#21040;XR-AI&#25512;&#29702;&#31649;&#36947;&#20013;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;7&#32435;&#31859;&#33410;&#28857;&#30340;&#35774;&#35745;&#20013;&#24341;&#20837;&#38750;&#26131;&#22833;&#24615;&#20869;&#23384;&#21040;&#20869;&#23384;&#23618;&#27425;&#32467;&#26500;&#20013;&#21487;&#20197;&#23454;&#29616;&#26174;&#30528;&#30340;&#33021;&#28304;&#25928;&#30410;&#65288;&gt;=24&#65285;&#65289;&#20197;&#25903;&#25345;&#25163;&#37096;&#26816;&#27979;&#65288;IPS = 10&#65289;&#21644;&#30524;&#30555;&#20998;&#21106;&#65288;IPS = 0.1&#65289;&#65292;&#21516;&#26102;&#28385;&#36275;&#26368;&#23567;IPS&#65288;&#27599;&#31186;&#25512;&#29702;&#27425;&#25968;&#65289;&#12290;&#27492;&#22806;&#65292;&#19982;&#20165;&#20351;&#29992;SRAM&#30340;&#35774;&#35745;&#30456;&#27604;&#65292;&#25105;&#20204;&#20960;&#20046;&#21487;&#20197;&#20943;&#23569;90&#65285;&#30340;&#20869;&#23384;&#27969;&#37327;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#22522;&#20110;&#20869;&#23384;&#30340;&#36793;&#32536;AI&#35774;&#35745;&#30340;XR&#24212;&#29992;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#35265;&#35299;&#65292;&#24182;&#30830;&#23450;&#20102;&#26032;&#20852;&#38750;&#26131;&#22833;&#24615;&#20869;&#23384;&#25216;&#26415;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-Power Edge-AI capabilities are essential for on-device extended reality (XR) applications to support the vision of Metaverse. In this work, we investigate two representative XR workloads: (i) Hand detection and (ii) Eye segmentation, for hardware design space exploration. For both applications, we train deep neural networks and analyze the impact of quantization and hardware specific bottlenecks. Through simulations, we evaluate a CPU and two systolic inference accelerator implementations. Next, we compare these hardware solutions with advanced technology nodes. The impact of integrating state-of-the-art emerging non-volatile memory technology (STT/SOT/VGSOT MRAM) into the XR-AI inference pipeline is evaluated. We found that significant energy benefits (&gt;=24%) can be achieved for hand detection (IPS=10) and eye segmentation (IPS=0.1) by introducing non-volatile memory in the memory hierarchy for designs at 7nm node while meeting minimum IPS (inference per second). Moreover, we can 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#22270;&#32858;&#31867;&#31639;&#27861;CGC&#65292;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65292;&#32467;&#21512;&#36319;&#36394;&#27169;&#22359;&#20197;&#24212;&#23545;&#21160;&#24577;&#22270;&#25299;&#25169;&#21464;&#21270;&#65292;&#22312;&#31038;&#21306;&#21457;&#29616;&#21644;&#36319;&#36394;&#26041;&#38754;&#34920;&#29616;&#20986;&#39046;&#20808;&#30340;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2204.08504</link><description>&lt;p&gt;
CGC: &#23545;&#27604;&#22270;&#32858;&#31867;&#29992;&#20110;&#31038;&#21306;&#21457;&#29616;&#21644;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
CGC: Contrastive Graph Clustering for Community Detection and Tracking. (arXiv:2204.08504v3 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.08504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#22270;&#32858;&#31867;&#31639;&#27861;CGC&#65292;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65292;&#32467;&#21512;&#36319;&#36394;&#27169;&#22359;&#20197;&#24212;&#23545;&#21160;&#24577;&#22270;&#25299;&#25169;&#21464;&#21270;&#65292;&#22312;&#31038;&#21306;&#21457;&#29616;&#21644;&#36319;&#36394;&#26041;&#38754;&#34920;&#29616;&#20986;&#39046;&#20808;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#22270;&#32858;&#31867;&#30340;&#35282;&#24230;&#20837;&#25163;&#65292;&#25506;&#35752;&#22312;&#32593;&#32476;&#25968;&#25454;&#20013;&#21457;&#29616;&#23454;&#20307;&#21644;&#23454;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#20197;&#21450;&#23545;&#23427;&#20204;&#36827;&#34892;&#31038;&#21306;&#36319;&#36394;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;CGC&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#33539;&#24335;&#36827;&#34892;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#37319;&#29992;&#20102;&#36319;&#36394;&#27169;&#22359;&#20197;&#24212;&#23545;&#19981;&#26029;&#21464;&#21270;&#30340;&#22270;&#24418;&#25299;&#25169;&#12290;&#22312;&#21508;&#20010;&#30495;&#23454;&#22330;&#26223;&#21644;&#21512;&#25104;&#22522;&#20934;&#19978;&#65292;&#25105;&#20204;&#23545;CGC&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#22312;&#31038;&#21306;&#21457;&#29616;&#21644;&#36319;&#36394;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#21160;&#24577;&#22270;&#19978;&#34920;&#29616;&#20986;&#20102;&#39046;&#20808;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given entities and their interactions in the web data, which may have occurred at different time, how can we find communities of entities and track their evolution? In this paper, we approach this important task from graph clustering perspective. Recently, state-of-the-art clustering performance in various domains has been achieved by deep clustering methods. Especially, deep graph clustering (DGC) methods have successfully extended deep clustering to graph-structured data by learning node representations and cluster assignments in a joint optimization framework. Despite some differences in modeling choices (e.g., encoder architectures), existing DGC methods are mainly based on autoencoders and use the same clustering objective with relatively minor adaptations. Also, while many real-world graphs are dynamic, previous DGC methods considered only static graphs. In this work, we develop CGC, a novel end-to-end framework for graph clustering, which fundamentally differs from existing meth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#24191;&#20041;&#22522;&#23612;&#31119;&#21033;&#20989;&#25968;&#65288;GGF&#65289;&#20316;&#20026;&#35268;&#33539;&#24615;&#20934;&#21017;&#26469;&#25351;&#23450;&#25512;&#33616;&#31995;&#32479;&#24212;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#20197;&#27492;&#23454;&#29616;&#25490;&#21517;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.06521</link><description>&lt;p&gt;
&#20248;&#21270;&#24191;&#20041;&#22522;&#23612;&#25351;&#25968;&#23454;&#29616;&#25490;&#21517;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Optimizing generalized Gini indices for fairness in rankings. (arXiv:2204.06521v4 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.06521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#24191;&#20041;&#22522;&#23612;&#31119;&#21033;&#20989;&#25968;&#65288;GGF&#65289;&#20316;&#20026;&#35268;&#33539;&#24615;&#20934;&#21017;&#26469;&#25351;&#23450;&#25512;&#33616;&#31995;&#32479;&#24212;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#20197;&#27492;&#23454;&#29616;&#25490;&#21517;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#20851;&#27880;&#35774;&#35745;&#33021;&#22815;&#23545;&#29289;&#21697;&#29983;&#20135;&#32773;&#25110;&#26368;&#19981;&#28385;&#24847;&#29992;&#25143;&#20844;&#24179;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;&#21463;&#32463;&#27982;&#23398;&#19981;&#24179;&#31561;&#27979;&#37327;&#39046;&#22495;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#24191;&#20041;&#22522;&#23612;&#31119;&#21033;&#20989;&#25968;&#65288;GGF&#65289;&#20316;&#20026;&#35268;&#33539;&#24615;&#20934;&#21017;&#26469;&#25351;&#23450;&#25512;&#33616;&#31995;&#32479;&#24212;&#20248;&#21270;&#30340;&#26041;&#27861;&#12290;GGF&#26681;&#25454;&#20154;&#21475;&#26222;&#26597;&#20013;&#30340;&#25490;&#21517;&#23545;&#20010;&#20307;&#36827;&#34892;&#21152;&#26435;&#65292;&#23558;&#26356;&#22810;&#30340;&#26435;&#37325;&#25918;&#22312;&#22788;&#22659;&#36739;&#24046;&#30340;&#20010;&#20307;&#19978;&#20197;&#20419;&#36827;&#24179;&#31561;&#12290;&#26681;&#25454;&#36825;&#20123;&#26435;&#37325;&#65292;GGF&#26368;&#23567;&#21270;&#29289;&#21697;&#26333;&#20809;&#30340;&#22522;&#23612;&#25351;&#25968;&#65292;&#20197;&#20419;&#36827;&#29289;&#21697;&#20043;&#38388;&#30340;&#24179;&#31561;&#65292;&#25110;&#20851;&#27880;&#26368;&#19981;&#28385;&#24847;&#29992;&#25143;&#30340;&#29305;&#23450;&#20998;&#20301;&#25968;&#30340;&#24615;&#33021;&#12290;&#25490;&#21517;&#30340;GGF&#38590;&#20197;&#20248;&#21270;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#19981;&#21487;&#24494;&#20998;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#38750;&#24179;&#28369;&#20248;&#21270;&#21644;&#21487;&#24494;&#25490;&#24207;&#20013;&#20351;&#29992;&#30340;&#25237;&#24433;&#31639;&#23376;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#22810;&#26377;15k&#20010;&#29992;&#25143;&#21644;&#29289;&#21697;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#20248;&#21270;GGF&#26377;&#25928;&#22320;&#20419;&#36827;&#25490;&#21517;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is growing interest in designing recommender systems that aim at being fair towards item producers or their least satisfied users. Inspired by the domain of inequality measurement in economics, this paper explores the use of generalized Gini welfare functions (GGFs) as a means to specify the normative criterion that recommender systems should optimize for. GGFs weight individuals depending on their ranks in the population, giving more weight to worse-off individuals to promote equality. Depending on these weights, GGFs minimize the Gini index of item exposure to promote equality between items, or focus on the performance on specific quantiles of least satisfied users. GGFs for ranking are challenging to optimize because they are non-differentiable. We resolve this challenge by leveraging tools from non-smooth optimization and projection operators used in differentiable sorting. We present experiments using real datasets with up to 15k users and items, which show that our approach
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#36827;&#23637;&#21644;&#25299;&#25169;&#27969;&#24418;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#21453;&#24212;&#24335;&#20840;&#36523;&#38556;&#30861;&#29289;&#36991;&#20813;&#65292;&#21487;&#36991;&#24320;&#20219;&#24847;&#24418;&#29366;&#30340;&#38556;&#30861;&#29289;&#65292;&#24182;&#33021;&#36731;&#26494;&#22320;&#25512;&#24191;&#21040;&#20854;&#20182;&#38382;&#39064;&#35774;&#32622;&#20013;&#12290;</title><link>http://arxiv.org/abs/2203.13821</link><description>&lt;p&gt;
SERA: &#29992;&#20110;&#21327;&#20316;&#26426;&#22120;&#20154;&#35268;&#21010;&#30340;&#23433;&#20840;&#39640;&#25928;&#21453;&#24212;&#24335;&#38556;&#30861;&#29289;&#36991;&#20813;&#26041;&#27861;&#22312;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#65288;arXiv:2203.13821v2 [cs.RO] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
SERA: Safe and Efficient Reactive Obstacle Avoidance for Collaborative Robotic Planning in Unstructured Environments. (arXiv:2203.13821v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.13821
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#36827;&#23637;&#21644;&#25299;&#25169;&#27969;&#24418;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#21453;&#24212;&#24335;&#20840;&#36523;&#38556;&#30861;&#29289;&#36991;&#20813;&#65292;&#21487;&#36991;&#24320;&#20219;&#24847;&#24418;&#29366;&#30340;&#38556;&#30861;&#29289;&#65292;&#24182;&#33021;&#36731;&#26494;&#22320;&#25512;&#24191;&#21040;&#20854;&#20182;&#38382;&#39064;&#35774;&#32622;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;4.0&#26102;&#20195;, &#22810;&#20010;&#26426;&#22120;&#20154;&#22312;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#23454;&#29616;&#23433;&#20840;&#39640;&#25928;&#30340;&#21327;&#20316;&#26085;&#30410;&#20851;&#38190;&#12290;&#28982;&#32780;, &#23454;&#29616;&#20154;&#31867;&#21644;&#20854;&#20182;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#24378;&#20581;&#33258;&#27835;&#21327;&#20316;&#38656;&#35201;&#29616;&#20195;&#26426;&#22120;&#20154;&#31995;&#32479;&#20855;&#22791;&#26377;&#25928;&#30340;&#37051;&#36817;&#24863;&#30693;&#21644;&#21453;&#24212;&#24335;&#38556;&#30861;&#29289;&#36991;&#20813;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21453;&#24212;&#24335;&#20840;&#36523;&#38556;&#30861;&#29289;&#36991;&#20813;&#26041;&#27861;&#65292;&#21363;&#20351;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#20063;&#21487;&#20197;&#30830;&#20445;&#26080;&#20914;&#31361;&#30340;&#26426;&#22120;&#20154;&#38388;&#20114;&#21160;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#38597;&#21508;&#27604;&#31867;&#22411;&#12289;&#37319;&#26679;&#25110;&#20960;&#20309;&#25216;&#26415;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#36827;&#23637;&#21644;&#25299;&#25169;&#27969;&#24418;&#23398;&#20064;&#65292;&#20351;&#20854;&#33021;&#22815;&#36731;&#26494;&#22320;&#25512;&#24191;&#21040;&#20854;&#20182;&#20855;&#26377;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#24555;&#36895;&#22270;&#36941;&#21382;&#25216;&#26415;&#30340;&#38382;&#39064;&#35774;&#32622;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#26426;&#26800;&#33218;&#22312;&#27809;&#26377;&#30452;&#25509;&#25509;&#35302;&#30340;&#24773;&#20917;&#19979;&#20027;&#21160;&#36991;&#24320;&#20219;&#24847;3D&#24418;&#29366;&#30340;&#38556;&#30861;&#29289;&#65292;&#36825;&#26159;&#20256;&#32479;&#24037;&#19994;&#21327;&#20316;&#26426;&#22120;&#20154;&#35774;&#32622;&#30340;&#37325;&#35201;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safe and efficient collaboration among multiple robots in unstructured environments is increasingly critical in the era of Industry 4.0. However, achieving robust and autonomous collaboration among humans and other robots requires modern robotic systems to have effective proximity perception and reactive obstacle avoidance. In this paper, we propose a novel methodology for reactive whole-body obstacle avoidance that ensures conflict-free robot-robot interactions even in dynamic environment. Unlike existing approaches based on Jacobian-type, sampling based or geometric techniques, our methodology leverages the latest deep learning advances and topological manifold learning, enabling it to be readily generalized to other problem settings with high computing efficiency and fast graph traversal techniques. Our approach allows a robotic arm to proactively avoid obstacles of arbitrary 3D shapes without direct contact, a significant improvement over traditional industrial cobot settings. To v
&lt;/p&gt;</description></item><item><title>FedREP &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27700;&#24179;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#22810;&#20010; REP &#26500;&#24314;&#19968;&#20010;&#20849;&#21516;&#30340;&#12289;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#23454;&#29616;&#33021;&#28304;&#36127;&#36733;&#28040;&#32791;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2203.00219</link><description>&lt;p&gt;
FedREP&#65306;&#38754;&#21521;&#38646;&#21806;&#33021;&#28304;&#20379;&#24212;&#21830;&#30340;&#27700;&#24179;&#32852;&#37030;&#36127;&#33655;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
FedREP: Towards Horizontal Federated Load Forecasting for Retail Energy Providers. (arXiv:2203.00219v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.00219
&lt;/p&gt;
&lt;p&gt;
FedREP &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27700;&#24179;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#22810;&#20010; REP &#26500;&#24314;&#19968;&#20010;&#20849;&#21516;&#30340;&#12289;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#23454;&#29616;&#33021;&#28304;&#36127;&#36733;&#28040;&#32791;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26234;&#33021;&#30005;&#34920;&#25910;&#38598;&#21644;&#20256;&#36755;&#23478;&#24237;&#33021;&#28304;&#28040;&#32791;&#25968;&#25454;&#32473;&#38646;&#21806;&#33021;&#28304;&#20379;&#24212;&#21830;&#65288;REP&#65289;&#65292;&#26368;&#22823;&#30340;&#25361;&#25112;&#26159;&#30830;&#20445;&#26377;&#25928;&#21033;&#29992;&#32454;&#31890;&#24230;&#30340;&#28040;&#36153;&#32773;&#25968;&#25454;&#30340;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#26412;&#25991;&#38024;&#23545;&#33021;&#28304;&#36127;&#36733;&#28040;&#32791;&#39044;&#27979;&#65292;&#35299;&#20915;&#20102;&#36825;&#19968;&#38382;&#39064;&#65292;&#36825;&#23545;&#20110;&#33021;&#28304;&#38656;&#27714;&#31649;&#29702;&#12289;&#36127;&#36733;&#20999;&#25442;&#21644;&#22522;&#30784;&#35774;&#26045;&#21457;&#23637;&#38750;&#24120;&#37325;&#35201;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#29616;&#26377;&#30340;&#33021;&#28304;&#36127;&#36733;&#39044;&#27979;&#26159;&#38598;&#20013;&#30340;&#65292;&#19981;&#21487;&#25193;&#23637;&#65292;&#26368;&#37325;&#35201;&#30340;&#26159;&#23481;&#26131;&#36973;&#21463;&#25968;&#25454;&#38544;&#31169;&#23041;&#32961;&#12290;&#27492;&#22806;&#65292;REP &#26159;&#21508;&#33258;&#30340;&#24066;&#22330;&#21442;&#19982;&#32773;&#65292;&#24182;&#26377;&#36131;&#20219;&#20445;&#25252;&#33258;&#24049;&#23458;&#25143;&#30340;&#38544;&#31169;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27700;&#24179;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21363; FedREP&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#30001;&#19968;&#20010;&#25511;&#21046;&#20013;&#24515;&#21644;&#22810;&#20010;&#38646;&#21806;&#21830;&#32452;&#25104;&#65292;&#36890;&#36807;&#21551;&#29992;&#22810;&#20010; REP &#26500;&#24314;&#19968;&#20010;&#20849;&#21516;&#30340;&#12289;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Smart Meters are collecting and transmitting household energy consumption data to Retail Energy Providers (REP), the main challenge is to ensure the effective use of fine-grained consumer data while ensuring data privacy. In this manuscript, we tackle this challenge for energy load consumption forecasting in regards to REPs which is essential to energy demand management, load switching and infrastructure development. Specifically, we note that existing energy load forecasting is centralized, which are not scalable and most importantly, vulnerable to data privacy threats. Besides, REPs are individual market participants and liable to ensure the privacy of their own customers. To address this issue, we propose a novel horizontal privacy-preserving federated learning framework for REPs energy load forecasting, namely FedREP. We consider a federated learning system consisting of a control centre and multiple retailers by enabling multiple REPs to build a common, robust machine learning 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; Mapper &#21644; Ball Mapper &#30340;&#28151;&#21512;&#31639;&#27861;&#65292;&#21629;&#21517;&#20026; Mapper on Ball Mapper&#65292;&#21487;&#20197;&#29992;&#20110;&#22788;&#29702;&#39640;&#32500;&#36879;&#38236;&#20989;&#25968;&#12290;&#35813;&#31639;&#27861;&#32467;&#21512;&#20102; Mapper &#21644; Ball Mapper &#30340;&#20248;&#21183;&#65292;&#33021;&#22815;&#21516;&#26102;&#32534;&#30721;&#28857;&#20113;&#30340;&#32467;&#26500;&#12289;&#20869;&#37096;&#20851;&#31995;&#21644;&#23545;&#31216;&#24615;&#65292;&#24182;&#21487;&#29992;&#20110;&#27604;&#36739;&#21333;&#20010;&#25968;&#25454;&#38598;&#30340;&#39640;&#32500;&#25968;&#25454;&#25551;&#36848;&#31526;&#65292;&#36866;&#29992;&#20110;&#32467;&#35770;&#29702;&#35770;&#12289;&#21338;&#24328;&#29702;&#35770;&#12289;&#26448;&#26009;&#31185;&#23398;&#21644;&#30284;&#30151;&#30740;&#31350;&#31561;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2109.00831</link><description>&lt;p&gt;
&#22522;&#20110;Mapper&#31867;&#22411;&#31639;&#27861;&#30340;&#22797;&#26434;&#25968;&#25454;&#21644;&#20851;&#31995;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Mapper-type algorithms for complex data and relations. (arXiv:2109.00831v2 [math.AT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.00831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; Mapper &#21644; Ball Mapper &#30340;&#28151;&#21512;&#31639;&#27861;&#65292;&#21629;&#21517;&#20026; Mapper on Ball Mapper&#65292;&#21487;&#20197;&#29992;&#20110;&#22788;&#29702;&#39640;&#32500;&#36879;&#38236;&#20989;&#25968;&#12290;&#35813;&#31639;&#27861;&#32467;&#21512;&#20102; Mapper &#21644; Ball Mapper &#30340;&#20248;&#21183;&#65292;&#33021;&#22815;&#21516;&#26102;&#32534;&#30721;&#28857;&#20113;&#30340;&#32467;&#26500;&#12289;&#20869;&#37096;&#20851;&#31995;&#21644;&#23545;&#31216;&#24615;&#65292;&#24182;&#21487;&#29992;&#20110;&#27604;&#36739;&#21333;&#20010;&#25968;&#25454;&#38598;&#30340;&#39640;&#32500;&#25968;&#25454;&#25551;&#36848;&#31526;&#65292;&#36866;&#29992;&#20110;&#32467;&#35770;&#29702;&#35770;&#12289;&#21338;&#24328;&#29702;&#35770;&#12289;&#26448;&#26009;&#31185;&#23398;&#21644;&#30284;&#30151;&#30740;&#31350;&#31561;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Mapper&#21644;Ball Mapper &#26159;&#19968;&#31181;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#65292;&#29992;&#20110;&#25506;&#32034;&#39640;&#32500;&#28857;&#20113;&#65292;&#24182;&#21487;&#35270;&#21270;&#28857;&#20113;&#19978;&#30340;&#26631;&#37327;&#20540;&#20989;&#25968;&#12290;&#26412;&#35770;&#25991;&#22312;&#26410;&#35299;&#20915;&#30340;&#32467;&#35770;&#29702;&#35770;&#38382;&#39064;&#30340;&#21551;&#21457;&#19979;&#65292;&#23558;&#26032;&#21151;&#33021;&#28155;&#21152;&#21040;Ball Mapper&#20013;&#65292;&#20351;&#20854;&#33021;&#22815;&#32534;&#30721;&#28857;&#20113;&#30340;&#32467;&#26500;&#12289;&#20869;&#37096;&#20851;&#31995;&#21644;&#23545;&#31216;&#24615;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#23558;Mapper&#21644;Ball Mapper&#30340;&#20248;&#21183;&#32467;&#21512;&#36215;&#26469;&#65292;&#21019;&#36896;&#20102;&#19968;&#31181;&#27604;&#36739;&#21333;&#20010;&#25968;&#25454;&#38598;&#30340;&#39640;&#32500;&#25968;&#25454;&#25551;&#36848;&#31526;&#30340;&#24037;&#20855;&#8212;&#8212;Mapper on Ball Mapper&#12290;&#36825;&#31181;&#26032;&#22411;&#28151;&#21512;&#31639;&#27861;&#36866;&#29992;&#20110;&#39640;&#32500;&#36879;&#38236;&#20989;&#25968;&#12290;&#20316;&#20026;&#27010;&#24565;&#35777;&#26126;&#65292;&#26412;&#30740;&#31350;&#24182;&#23558;&#24212;&#29992;&#20110;&#32467;&#35770;&#29702;&#35770;&#21644;&#21338;&#24328;&#29702;&#35770;&#65292;&#20197;&#21450;&#26448;&#26009;&#31185;&#23398;&#21644;&#30284;&#30151;&#30740;&#31350;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mapper and Ball Mapper are Topological Data Analysis tools used for exploring high dimensional point clouds and visualizing scalar-valued functions on those point clouds. Inspired by open questions in knot theory, new features are added to Ball Mapper that enable encoding of the structure, internal relations and symmetries of the point cloud. Moreover, the strengths of Mapper and Ball Mapper constructions are combined to create a tool for comparing high dimensional data descriptors of a single dataset. This new hybrid algorithm, Mapper on Ball Mapper, is applicable to high dimensional lens functions. As a proof of concept we include applications to knot and game theory, as well as material science and cancer research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#23558;&#21508;&#31181;&#25513;&#30721;&#26426;&#21046;&#32435;&#20837;Transformers&#26550;&#26500;&#20013;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#26410;&#23631;&#34109;&#30340;&#27880;&#24847;&#21147;&#30340;&#25299;&#25169;&#65288;&#22522;&#20110;&#22270;&#24418;&#65289;&#35843;&#21046;&#65292;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;d&#32500;RPE&#25513;&#30721;&#21644;&#22270;&#20869;&#26680;&#25513;&#30721;&#12290;&#35813;&#26041;&#27861;&#24471;&#21040;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2107.07999</link><description>&lt;p&gt;
&#20174;&#22359;-Toeplitz&#30697;&#38453;&#21040;&#22270;&#19978;&#30340;&#24494;&#20998;&#26041;&#31243;&#65306;&#36808;&#21521;&#21487;&#25193;&#23637;&#30340;Masked Transformers&#30340;&#36890;&#29992;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
From block-Toeplitz matrices to differential equations on graphs: towards a general theory for scalable masked Transformers. (arXiv:2107.07999v8 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.07999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#23558;&#21508;&#31181;&#25513;&#30721;&#26426;&#21046;&#32435;&#20837;Transformers&#26550;&#26500;&#20013;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#26410;&#23631;&#34109;&#30340;&#27880;&#24847;&#21147;&#30340;&#25299;&#25169;&#65288;&#22522;&#20110;&#22270;&#24418;&#65289;&#35843;&#21046;&#65292;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;d&#32500;RPE&#25513;&#30721;&#21644;&#22270;&#20869;&#26680;&#25513;&#30721;&#12290;&#35813;&#26041;&#27861;&#24471;&#21040;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#20840;&#38754;&#30340;&#26041;&#27861;&#65292;&#20197;&#21487;&#25193;&#23637;&#30340;&#26041;&#24335;&#23558;&#21508;&#31181;&#25513;&#30721;&#26426;&#21046;&#32435;&#20837;Transformers&#26550;&#26500;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#36817;&#20851;&#20110;&#32447;&#24615;&#22240;&#26524;&#27880;&#24847;&#21147;&#65288;Choromanski&#31561;&#20154;&#65292;2021&#65289;&#21644;&#23545;&#25968;-&#32447;&#24615;RPE-&#27880;&#24847;&#21147;&#65288;Luo&#31561;&#20154;&#65292;2021&#65289;&#30340;&#32467;&#26524;&#26159;&#36825;&#31181;&#19968;&#33324;&#26426;&#21046;&#30340;&#29305;&#20363;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#26410;&#23631;&#34109;&#30340;&#27880;&#24847;&#21147;&#30340;&#25299;&#25169;&#65288;&#22522;&#20110;&#22270;&#24418;&#65289;&#35843;&#21046;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#20960;&#20010;&#20197;&#21069;&#26410;&#30693;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#39640;&#25928;&#30340;d&#32500;RPE&#25513;&#30721;&#21644;&#22270;&#20869;&#26680;&#25513;&#30721;&#12290;&#25105;&#20204;&#21033;&#29992;&#35768;&#22810;&#25968;&#23398;&#25216;&#26415;&#65292;&#20174;&#35889;&#20998;&#26512;&#12289;&#21160;&#24577;&#35268;&#21010;&#21644;&#38543;&#26426;&#28216;&#36208;&#21040;&#35299;&#20915;&#22270;&#19978;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#30340;&#26032;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we provide, to the best of our knowledge, the first comprehensive approach for incorporating various masking mechanisms into Transformers architectures in a scalable way. We show that recent results on linear causal attention (Choromanski et al., 2021) and log-linear RPE-attention (Luo et al., 2021) are special cases of this general mechanism. However by casting the problem as a topological (graph-based) modulation of unmasked attention, we obtain several results unknown before, including efficient d-dimensional RPE-masking and graph-kernel masking. We leverage many mathematical techniques ranging from spectral analysis through dynamic programming and random walks to new algorithms for solving Markov processes on graphs. We provide a corresponding empirical evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;NoiseGrad&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#27169;&#22411;&#26435;&#37325;&#30340;&#38543;&#26426;&#21464;&#21270;&#25200;&#21160;&#20915;&#31574;&#36793;&#30028;&#26469;&#22686;&#24378;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#35299;&#37322;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2106.10185</link><description>&lt;p&gt;
NoiseGrad&#65306;&#36890;&#36807;&#24341;&#20837;&#27169;&#22411;&#26435;&#37325;&#30340;&#38543;&#26426;&#21464;&#21270;&#26469;&#22686;&#24378;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
NoiseGrad: Enhancing Explanations by Introducing Stochasticity to Model Weights. (arXiv:2106.10185v3 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.10185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;NoiseGrad&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#27169;&#22411;&#26435;&#37325;&#30340;&#38543;&#26426;&#21464;&#21270;&#25200;&#21160;&#20915;&#31574;&#36793;&#30028;&#26469;&#22686;&#24378;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38024;&#23545;&#40657;&#21283;&#23376;&#23398;&#20064;&#26426;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#24456;&#22810;&#24037;&#20316;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#26377;&#29992;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#35299;&#37322;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;NoiseGrad&#65292;&#36890;&#36807;&#22312;&#26435;&#37325;&#21442;&#25968;&#31354;&#38388;&#20013;&#24341;&#20837;&#38543;&#26426;&#24615;&#65292;&#20174;&#32780;&#25200;&#21160;&#20915;&#31574;&#36793;&#30028;&#65292;&#22686;&#24378;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616;&#35299;&#37322;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;NoiseGrad&#19982;&#20854;&#19982;SmoothGrad&#30340;&#34701;&#21512;&#26041;&#27861;&#65288;FusionGrad&#65289;&#36827;&#34892;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many efforts have been made for revealing the decision-making process of black-box learning machines such as deep neural networks, resulting in useful local and global explanation methods. For local explanation, stochasticity is known to help: a simple method, called SmoothGrad, has improved the visual quality of gradient-based attribution by adding noise to the input space and averaging the explanations of the noisy inputs. In this paper, we extend this idea and propose NoiseGrad that enhances both local and global explanation methods. Specifically, NoiseGrad introduces stochasticity in the weight parameter space, such that the decision boundary is perturbed. NoiseGrad is expected to enhance the local explanation, similarly to SmoothGrad, due to the dual relationship between the input perturbation and the decision boundary perturbation. We evaluate NoiseGrad and its fusion with SmoothGrad -FusionGrad -- qualitatively and quantitatively with several evaluation criteria, and show that
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20998;&#24067;&#30340;&#27969;&#24418;&#32467;&#26500;&#26469;&#39044;&#27979;&#20266;&#26631;&#31614;&#65292;&#22312;&#31867;&#21035;&#24179;&#34913;&#21644;&#36873;&#25321;&#24178;&#20928;&#26631;&#31614;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#36845;&#20195;&#28165;&#27927;&#26631;&#31614;&#20197;&#25552;&#39640;&#20266;&#26631;&#31614;&#36136;&#37327;&#30340;&#31639;&#27861;&#65292;&#22312;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#30340;&#26368;&#20339;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2012.07962</link><description>&lt;p&gt;
&#36845;&#20195;&#26631;&#31614;&#28165;&#27927;&#29992;&#20110;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#21322;&#30417;&#30563;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Iterative label cleaning for transductive and semi-supervised few-shot learning. (arXiv:2012.07962v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.07962
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20998;&#24067;&#30340;&#27969;&#24418;&#32467;&#26500;&#26469;&#39044;&#27979;&#20266;&#26631;&#31614;&#65292;&#22312;&#31867;&#21035;&#24179;&#34913;&#21644;&#36873;&#25321;&#24178;&#20928;&#26631;&#31614;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#36845;&#20195;&#28165;&#27927;&#26631;&#31614;&#20197;&#25552;&#39640;&#20266;&#26631;&#31614;&#36136;&#37327;&#30340;&#31639;&#27861;&#65292;&#22312;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#30340;&#26368;&#20339;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#23398;&#20064;&#28041;&#21450;&#21040;&#23398;&#20064;&#34920;&#24449;&#21644;&#33719;&#21462;&#30693;&#35782;&#65292;&#20197;&#20351;&#26032;&#20219;&#21153;&#21487;&#20197;&#22312;&#30417;&#30563;&#21644;&#25968;&#25454;&#37117;&#24456;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#24471;&#21040;&#35299;&#20915;&#12290;&#36890;&#36807;&#27178;&#21521;&#25512;&#26029;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#36825;&#20004;&#31181;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20998;&#24067;&#30340;&#27969;&#24418;&#32467;&#26500;&#26469;&#39044;&#27979;&#20266;&#26631;&#31614;&#65292;&#21516;&#26102;&#24179;&#34913;&#31867;&#21035;&#24182;&#20351;&#29992;&#26377;&#38480;&#23481;&#37327;&#20998;&#31867;&#22120;&#30340;&#25439;&#22833;&#20540;&#20998;&#24067;&#26469;&#36873;&#25321;&#26368;&#24178;&#20928;&#30340;&#26631;&#31614;&#65292;&#36845;&#20195;&#22320;&#25552;&#39640;&#20266;&#26631;&#31614;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;&#21363; miniImageNet&#12289;tieredImageNet&#12289;CUB &#21644; CIFAR-FS&#65289;&#19978;&#36229;&#36807;&#25110;&#21305;&#37197;&#20102;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#65292;&#21516;&#26102;&#22312;&#29305;&#24449;&#31354;&#38388;&#39044;&#22788;&#29702;&#21644;&#21487;&#29992;&#25968;&#25454;&#30340;&#25968;&#37327;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#20844;&#24320;&#21487;&#29992;&#30340;&#28304;&#20195;&#30721;&#21487;&#22312; https://github.c &#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot learning amounts to learning representations and acquiring knowledge such that novel tasks may be solved with both supervision and data being limited. Improved performance is possible by transductive inference, where the entire test set is available concurrently, and semi-supervised learning, where more unlabeled data is available. Focusing on these two settings, we introduce a new algorithm that leverages the manifold structure of the labeled and unlabeled data distribution to predict pseudo-labels, while balancing over classes and using the loss value distribution of a limited-capacity classifier to select the cleanest labels, iteratively improving the quality of pseudo-labels. Our solution surpasses or matches the state of the art results on four benchmark datasets, namely miniImageNet, tieredImageNet, CUB and CIFAR-FS, while being robust over feature space pre-processing and the quantity of available data. The publicly available source code can be found in https://github.c
&lt;/p&gt;</description></item></channel></rss>