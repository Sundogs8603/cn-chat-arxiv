<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;ChatGPT&#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;&#24320;&#25918;&#24847;&#22270;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#21512;&#25104;&#27867;&#21270;&#33021;&#21147;&#65292;&#26377;&#25928;&#25913;&#21892;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#20102;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2308.13517</link><description>&lt;p&gt;
ChatGPT&#20316;&#20026;&#21512;&#25104;&#27867;&#21270;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#22312;&#24320;&#25918;&#24847;&#22270;&#26816;&#27979;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
ChatGPT as Data Augmentation for Compositional Generalization: A Case Study in Open Intent Detection. (arXiv:2308.13517v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;ChatGPT&#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;&#24320;&#25918;&#24847;&#22270;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#21512;&#25104;&#27867;&#21270;&#33021;&#21147;&#65292;&#26377;&#25928;&#25913;&#21892;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#20102;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#24847;&#22270;&#26816;&#27979;&#26159;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#28041;&#21450;&#23545;&#29992;&#25143;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#24847;&#22270;&#36827;&#34892;&#35782;&#21035;&#12290;&#34429;&#28982;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#26159;&#22312;&#22788;&#29702;&#35821;&#35328;&#32452;&#25104;&#25104;&#20998;&#30340;&#26032;&#32452;&#21512;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#65292;&#36825;&#23545;&#20110;&#21512;&#25104;&#27867;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#22312;&#24320;&#25918;&#24847;&#22270;&#26816;&#27979;&#20219;&#21153;&#20013;&#23558;ChatGPT&#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#25552;&#39640;&#21512;&#25104;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#29616;&#26377;&#22522;&#20934;&#22312;&#35780;&#20272;&#36825;&#20010;&#38382;&#39064;&#26102;&#30340;&#23616;&#38480;&#24615;&#65292;&#24378;&#35843;&#20102;&#26500;&#24314;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#24320;&#25918;&#24847;&#22270;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#21512;&#25104;&#27867;&#21270;&#38382;&#39064;&#30340;&#38656;&#27714;&#12290;&#36890;&#36807;&#23558;ChatGPT&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#32435;&#20837;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#23545;&#22810;&#20010;&#22522;&#20934;&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#24320;&#25918;&#24847;&#22270;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open intent detection, a crucial aspect of natural language understanding, involves the identification of previously unseen intents in user-generated text. Despite the progress made in this field, challenges persist in handling new combinations of language components, which is essential for compositional generalization. In this paper, we present a case study exploring the use of ChatGPT as a data augmentation technique to enhance compositional generalization in open intent detection tasks. We begin by discussing the limitations of existing benchmarks in evaluating this problem, highlighting the need for constructing datasets for addressing compositional generalization in open intent detection tasks. By incorporating synthetic data generated by ChatGPT into the training process, we demonstrate that our approach can effectively improve model performance. Rigorous evaluation of multiple benchmarks reveals that our method outperforms existing techniques and significantly enhances open inte
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#29983;&#25104;&#20195;&#30721;&#20043;&#21069;&#25552;&#38382;&#28548;&#28165;&#38382;&#39064;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#21487;&#20197;&#24471;&#21040;&#25552;&#21319;&#65292;&#22686;&#21152;&#20102;&#23545;&#29983;&#25104;&#20195;&#30721;&#30340;&#20449;&#24515;&#12290;</title><link>http://arxiv.org/abs/2308.13507</link><description>&lt;p&gt;
&#25552;&#38382;&#28548;&#28165;&#38382;&#39064;&#26159;&#21542;&#22686;&#21152;&#20102;&#29983;&#25104;&#20195;&#30721;&#30340;&#20449;&#24515;&#65311;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27807;&#36890;&#33021;&#21147;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Does Asking Clarifying Questions Increases Confidence in Generated Code? On the Communication Skills of Large Language Models. (arXiv:2308.13507v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13507
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#29983;&#25104;&#20195;&#30721;&#20043;&#21069;&#25552;&#38382;&#28548;&#28165;&#38382;&#39064;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#21487;&#20197;&#24471;&#21040;&#25552;&#21319;&#65292;&#22686;&#21152;&#20102;&#23545;&#29983;&#25104;&#20195;&#30721;&#30340;&#20449;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26174;&#33879;&#25552;&#39640;&#20102;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#25104;&#20026;&#39030;&#32423;&#36719;&#20214;&#24037;&#31243;&#24072;&#26041;&#38754;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;&#22522;&#20110;&#35266;&#23519;&#21040;&#39030;&#32423;&#36719;&#20214;&#24037;&#31243;&#24072;&#36890;&#24120;&#20250;&#25552;&#20986;&#28548;&#28165;&#38382;&#39064;&#20197;&#20943;&#23569;&#38656;&#27714;&#21644;&#32534;&#30721;&#35299;&#20915;&#26041;&#26696;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#35748;&#20026;&#22312;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;LLMs&#20063;&#24212;&#35813;&#37319;&#29992;&#21516;&#26679;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#29983;&#25104;&#26368;&#32456;&#20195;&#30721;&#20043;&#21069;&#25552;&#20986;&#28145;&#20837;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#20943;&#36731;&#20351;&#29992;LLMs&#36827;&#34892;&#32534;&#31243;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#22914;&#24847;&#22270;&#35268;&#33539;&#19981;&#26126;&#30830;&#12289;&#35745;&#31639;&#24605;&#32500;&#19981;&#36275;&#21644;&#20195;&#30721;&#36136;&#37327;&#19981;&#29702;&#24819;&#12290;&#36825;&#21453;&#36807;&#26469;&#22686;&#21152;&#20102;&#23545;&#29983;&#25104;&#20195;&#30721;&#30340;&#33258;&#20449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#22914;&#20309;&#21033;&#29992;&#26356;&#22909;&#30340;&#27807;&#36890;&#25216;&#24039;&#26469;&#22686;&#21152;&#23545;&#29983;&#25104;&#20195;&#30721;&#30340;&#20449;&#24515;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#27807;&#36890;&#20026;&#20013;&#24515;&#30340;&#36807;&#31243;&#65292;&#21033;&#29992;LLM&#29983;&#25104;&#30340;&#27807;&#36890;&#22120;&#26469;&#35782;&#21035;&#39640;&#24230;&#19981;&#30830;&#23450;&#25110;&#20449;&#24515;&#20302;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have significantly improved the ability to perform tasks in the field of code generation. However, there is still a gap between LLMs being capable coders and being top-tier software engineers. Based on the observation that top-level software engineers often ask clarifying questions to reduce ambiguity in both requirements and coding solutions, we argue that the same should be applied to LLMs for code generation tasks. By asking probing questions in various topics before generating the final code, the challenges of programming with LLMs, such as unclear intent specification, lack of computational thinking, and undesired code quality, may be alleviated. This, in turn, increases confidence in the generated code. In this work, we explore how to leverage better communication skills to achieve greater confidence in generated code. We propose a communication-centered process that uses an LLM-generated communicator to identify issues with high ambiguity or low conf
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#20013;&#35299;&#20915;&#20102;&#27867;&#21270;&#24615;&#38382;&#39064;&#12290;&#22312;&#32463;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#21518;&#65292;&#30740;&#31350;&#21457;&#29616;&#25152;&#25552;&#20986;&#30340;&#26816;&#27979;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#35757;&#32451;&#20013;&#26410;&#36935;&#21040;&#30340;&#31713;&#25913;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.13503</link><description>&lt;p&gt;
&#36890;&#36807;&#25506;&#31350;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#35299;&#20915;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#20013;&#30340;&#27867;&#21270;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Attending Generalizability in Course of Deep Fake Detection by Exploring Multi-task Learning. (arXiv:2308.13503v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#20013;&#35299;&#20915;&#20102;&#27867;&#21270;&#24615;&#38382;&#39064;&#12290;&#22312;&#32463;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#21518;&#65292;&#30740;&#31350;&#21457;&#29616;&#25152;&#25552;&#20986;&#30340;&#26816;&#27979;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#35757;&#32451;&#20013;&#26410;&#36935;&#21040;&#30340;&#31713;&#25913;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22810;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#24335;&#65292;&#26088;&#22312;&#22312;&#36328;&#25805;&#25511;&#22330;&#26223;&#20013;&#23558;&#35270;&#39057;&#20998;&#31867;&#20026;&#21407;&#22987;&#25110;&#34987;&#31713;&#25913;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#20266;&#36896;&#22330;&#26223;&#20013;&#30340;&#27867;&#21270;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;FaceForensics++&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;1000&#20010;&#21407;&#22987;&#35270;&#39057;&#65292;&#36890;&#36807;&#22235;&#31181;&#19981;&#21516;&#30340;&#25216;&#26415;&#36827;&#34892;&#20102;&#31713;&#25913;&#65292;&#20849;&#35745;5000&#20010;&#35270;&#39057;&#12290;&#25105;&#20204;&#23545;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#36825;&#20123;&#25216;&#26415;&#22312;&#25991;&#29486;&#20013;&#24050;&#32463;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20855;&#26377;&#27867;&#21270;&#24615;&#30340;&#22909;&#22788;&#12290;&#21487;&#20197;&#24471;&#20986;&#32467;&#35770;&#65306;&#25152;&#25552;&#20986;&#30340;&#26816;&#27979;&#27169;&#22411;&#38750;&#24120;&#27867;&#21270;&#65292;&#21363;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#22312;&#35757;&#32451;&#20013;&#26410;&#36935;&#21040;&#30340;&#31713;&#25913;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work explores various ways of exploring multi-task learning (MTL) techniques aimed at classifying videos as original or manipulated in cross-manipulation scenario to attend generalizability in deep fake scenario. The dataset used in our evaluation is FaceForensics++, which features 1000 original videos manipulated by four different techniques, with a total of 5000 videos. We conduct extensive experiments on multi-task learning and contrastive techniques, which are well studied in literature for their generalization benefits. It can be concluded that the proposed detection model is quite generalized, i.e., accurately detects manipulation methods not encountered during training as compared to the state-of-the-art.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#37197;&#23545;&#36317;&#31163;&#20272;&#35745;&#22120;&#23545;&#38598;&#25104;&#27169;&#22411;&#36827;&#34892;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#24120;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#24555;&#36895;&#12289;&#26356;&#20934;&#30830;&#22320;&#22312;&#26356;&#22823;&#30340;&#31354;&#38388;&#21644;&#26356;&#39640;&#32500;&#24230;&#19978;&#20272;&#35745;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13498</link><description>&lt;p&gt;
&#36867;&#31163;&#26679;&#26412;&#38519;&#38449;&#65306;&#20351;&#29992;&#37197;&#23545;&#36317;&#31163;&#20272;&#35745;&#22120;&#24555;&#36895;&#20934;&#30830;&#22320;&#20272;&#35745;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Escaping the Sample Trap: Fast and Accurate Epistemic Uncertainty Estimation with Pairwise-Distance Estimators. (arXiv:2308.13498v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#37197;&#23545;&#36317;&#31163;&#20272;&#35745;&#22120;&#23545;&#38598;&#25104;&#27169;&#22411;&#36827;&#34892;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#24120;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#24555;&#36895;&#12289;&#26356;&#20934;&#30830;&#22320;&#22312;&#26356;&#22823;&#30340;&#31354;&#38388;&#21644;&#26356;&#39640;&#32500;&#24230;&#19978;&#20272;&#35745;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#37197;&#23545;&#36317;&#31163;&#20272;&#35745;&#22120;&#65288;PaiDEs&#65289;&#23545;&#38598;&#25104;&#27169;&#22411;&#36827;&#34892;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#20123;&#20272;&#35745;&#22120;&#21033;&#29992;&#27169;&#22411;&#32452;&#20214;&#20043;&#38388;&#30340;&#37197;&#23545;&#36317;&#31163;&#26469;&#24314;&#31435;&#29109;&#30340;&#36793;&#30028;&#65292;&#24182;&#23558;&#36825;&#20123;&#36793;&#30028;&#20316;&#20026;&#22522;&#20110;&#20449;&#24687;&#20934;&#21017;&#30340;&#20272;&#35745;&#20540;&#12290;&#19982;&#26368;&#36817;&#22522;&#20110;&#26679;&#26412;&#30340;&#33945;&#29305;&#21345;&#27931;&#20272;&#35745;&#22120;&#29992;&#20110;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;PaiDEs&#33021;&#22815;&#22312;&#26356;&#22823;&#30340;&#31354;&#38388;&#65288;&#26368;&#22810;100&#20493;&#65289;&#19978;&#20197;&#26356;&#24555;&#30340;&#36895;&#24230;&#65288;&#26368;&#22810;100&#20493;&#65289;&#20272;&#35745;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#26356;&#39640;&#32500;&#24230;&#19978;&#20855;&#26377;&#26356;&#20934;&#30830;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#29992;&#20110;&#35780;&#20272;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#23454;&#39564;&#65306;&#19968;&#32500;&#27491;&#24358;&#25968;&#25454;&#65292;&#25670;&#21160;&#29289;&#20307;&#65288;Pendulum-v0&#65289;&#65292;&#36339;&#36291;&#26426;&#22120;&#20154;&#65288;Hopper-v2&#65289;&#65292;&#34434;&#34433;&#26426;&#22120;&#20154;&#65288;Ant-v2&#65289;&#21644;&#20154;&#24418;&#26426;&#22120;&#20154;&#65288;Humanoid-v2&#65289;&#12290;&#23545;&#20110;&#27599;&#20010;&#23454;&#39564;&#35774;&#32622;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#26469;&#23637;&#31034;PaiDEs&#22312;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces a novel approach for epistemic uncertainty estimation for ensemble models using pairwise-distance estimators (PaiDEs). These estimators utilize the pairwise-distance between model components to establish bounds on entropy and uses said bounds as estimates for information-based criterion. Unlike recent deep learning methods for epistemic uncertainty estimation, which rely on sample-based Monte Carlo estimators, PaiDEs are able to estimate epistemic uncertainty up to 100$\times$ faster, over a larger space (up to 100$\times$) and perform more accurately in higher dimensions. To validate our approach, we conducted a series of experiments commonly used to evaluate epistemic uncertainty estimation: 1D sinusoidal data, Pendulum-v0, Hopper-v2, Ant-v2 and Humanoid-v2. For each experimental setting, an Active Learning framework was applied to demonstrate the advantages of PaiDEs for epistemic uncertainty estimation.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20223;&#21046;&#35895;&#27468;&#30524;&#21160;&#35770;&#25991;&#30340;&#24320;&#28304;&#23454;&#29616;&#65292;&#37325;&#28857;&#26159;&#36890;&#36807;&#25972;&#21512;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#23454;&#29616;&#19982;&#35895;&#27468;&#35770;&#25991;&#30456;&#24403;&#30340;&#20934;&#30830;&#30524;&#21160;&#36861;&#36394;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.13495</link><description>&lt;p&gt;
&#24320;&#25918;&#27880;&#35270;&#65306;&#19968;&#20010;&#20223;&#21046;&#35895;&#27468;&#30524;&#21160;&#35770;&#25991;&#30340;&#24320;&#28304;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Open Gaze: An Open-Source Implementation Replicating Google's Eye Tracking Paper. (arXiv:2308.13495v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20223;&#21046;&#35895;&#27468;&#30524;&#21160;&#35770;&#25991;&#30340;&#24320;&#28304;&#23454;&#29616;&#65292;&#37325;&#28857;&#26159;&#36890;&#36807;&#25972;&#21512;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#23454;&#29616;&#19982;&#35895;&#27468;&#35770;&#25991;&#30456;&#24403;&#30340;&#20934;&#30830;&#30524;&#21160;&#36861;&#36394;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30524;&#21160;&#24050;&#32463;&#25104;&#20026;&#35270;&#35273;&#30740;&#31350;&#12289;&#35821;&#35328;&#20998;&#26512;&#21644;&#21487;&#29992;&#24615;&#35780;&#20272;&#31561;&#19981;&#21516;&#39046;&#22495;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#20351;&#29992;&#19987;&#38376;&#30340;&#12289;&#26114;&#36149;&#30340;&#30524;&#21160;&#36861;&#36394;&#30828;&#20214;&#30340;&#25193;&#23637;&#24335;&#26700;&#38754;&#26174;&#31034;&#22120;&#19978;&#12290;&#23613;&#31649;&#26234;&#33021;&#25163;&#26426;&#30340;&#26222;&#21450;&#29575;&#21644;&#20351;&#29992;&#39057;&#29575;&#24456;&#39640;&#65292;&#20294;&#23545;&#20110;&#26234;&#33021;&#25163;&#26426;&#19978;&#30340;&#30524;&#29699;&#31227;&#21160;&#27169;&#24335;&#21364;&#40092;&#26377;&#35265;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26234;&#33021;&#25163;&#26426;&#30340;&#24320;&#28304;&#27880;&#35270;&#36861;&#36394;&#23454;&#29616;&#65292;&#27169;&#25311;&#20102;&#35895;&#27468;&#35770;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#35770;&#65288;&#20854;&#28304;&#20195;&#30721;&#20173;&#28982;&#26159;&#19987;&#26377;&#30340;&#65289;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#30828;&#20214;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#19982;&#35895;&#27468;&#35770;&#25991;&#26041;&#27861;&#30456;&#24403;&#30340;&#20934;&#30830;&#24230;&#12290;&#36890;&#36807;&#25972;&#21512;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#31181;&#26412;&#22320;&#20110;&#26234;&#33021;&#25163;&#26426;&#30340;&#20934;&#30830;&#30524;&#21160;&#36861;&#36394;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#31227;&#21160;&#30524;&#21160;&#36861;&#36394;&#22120;&#30456;&#24403;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Eye tracking has been a pivotal tool in diverse fields such as vision research, language analysis, and usability assessment. The majority of prior investigations, however, have concentrated on expansive desktop displays employing specialized, costly eye tracking hardware that lacks scalability. Remarkably little insight exists into ocular movement patterns on smartphones, despite their widespread adoption and significant usage. In this manuscript, we present an open-source implementation of a smartphone-based gaze tracker that emulates the methodology proposed by a GooglePaper (whose source code remains proprietary). Our focus is on attaining accuracy comparable to that attained through the GooglePaper's methodology, without the necessity for supplementary hardware. Through the integration of machine learning techniques, we unveil an accurate eye tracking solution that is native to smartphones. Our approach demonstrates precision akin to the state-of-the-art mobile eye trackers, which 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#24555;&#36229;&#36731;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;Fast-MpoxNet&#65292;&#29992;&#20110;&#26089;&#26399;&#29492;&#30168;&#35786;&#26029;&#12290;&#23427;&#20855;&#26377;&#36739;&#23567;&#30340;&#21442;&#25968;&#37327;&#21644;&#36739;&#24555;&#30340;&#22788;&#29702;&#36895;&#24230;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#34701;&#21512;&#21644;&#36741;&#21161;&#25439;&#22833;&#22686;&#24378;&#31574;&#30053;&#25552;&#39640;&#20102;&#35786;&#26029;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.13492</link><description>&lt;p&gt;
&#36229;&#24555;&#36229;&#36731;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26234;&#33021;&#30417;&#27979;&#31995;&#32479;&#29992;&#20110;&#22312;&#20219;&#20309;&#26102;&#38388;&#21644;&#22320;&#28857;&#35786;&#26029;&#26089;&#26399;&#29492;&#30168;
&lt;/p&gt;
&lt;p&gt;
Ultrafast-and-Ultralight ConvNet-Based Intelligent Monitoring System for Diagnosing Early-Stage Mpox Anytime and Anywhere. (arXiv:2308.13492v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13492
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#24555;&#36229;&#36731;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;Fast-MpoxNet&#65292;&#29992;&#20110;&#26089;&#26399;&#29492;&#30168;&#35786;&#26029;&#12290;&#23427;&#20855;&#26377;&#36739;&#23567;&#30340;&#21442;&#25968;&#37327;&#21644;&#36739;&#24555;&#30340;&#22788;&#29702;&#36895;&#24230;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#34701;&#21512;&#21644;&#36741;&#21161;&#25439;&#22833;&#22686;&#24378;&#31574;&#30053;&#25552;&#39640;&#20102;&#35786;&#26029;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#32570;&#20047;&#26356;&#39640;&#25928;&#30340;&#29492;&#30168;&#35786;&#26029;&#24037;&#20855;&#65292;&#20854;&#20256;&#25773;&#20173;&#28982;&#26410;&#21463;&#25511;&#21046;&#65292;&#32473;&#20840;&#29699;&#20581;&#24247;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#34429;&#28982;&#30456;&#20851;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#29492;&#30168;&#35786;&#26029;&#26041;&#38754;&#30340;&#39640;&#25928;&#24615;&#65292;&#20294;&#26159;&#23545;&#20110;&#26089;&#26399;&#29492;&#30168;&#30340;&#25512;&#29702;&#36895;&#24230;&#12289;&#21442;&#25968;&#22823;&#23567;&#21644;&#35786;&#26029;&#24615;&#33021;&#30340;&#24573;&#35270;&#20351;&#24471;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#24555;&#36229;&#36731;&#30340;&#32593;&#32476;&#65292;&#21517;&#20026;Fast-MpoxNet&#12290;Fast-MpoxNet&#21482;&#26377;0.27M&#20010;&#21442;&#25968;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;CPU&#19978;&#20197;&#27599;&#31186;68&#24103;&#30340;&#36895;&#24230;&#22788;&#29702;&#36755;&#20837;&#22270;&#20687;&#12290;&#20026;&#20102;&#20811;&#26381;&#23567;&#27169;&#22411;&#23481;&#37327;&#24102;&#26469;&#30340;&#35786;&#26029;&#24615;&#33021;&#38480;&#21046;&#65292;&#23427;&#38598;&#25104;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#21644;&#22810;&#20010;&#36741;&#21161;&#25439;&#22833;&#22686;&#24378;&#31574;&#30053;&#65292;&#20197;&#26356;&#22909;&#22320;&#26816;&#27979;&#24494;&#23567;&#30340;&#22270;&#20687;&#21464;&#21270;&#21644;&#20248;&#21270;&#26435;&#37325;&#12290;&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#21644;&#20116;&#25240;&#20132;&#21449;&#39564;&#35777;&#65292;Fast-MpoxNet&#23454;&#29616;&#20102;94.26%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the lack of more efficient diagnostic tools for monkeypox, its spread remains unchecked, presenting a formidable challenge to global health. While the high efficacy of deep learning models for monkeypox diagnosis has been demonstrated in related studies, the overlook of inference speed, the parameter size and diagnosis performance for early-stage monkeypox renders the models inapplicable in real-world settings. To address these challenges, we proposed an ultrafast and ultralight network named Fast-MpoxNet. Fast-MpoxNet possesses only 0.27M parameters and can process input images at 68 frames per second (FPS) on the CPU. To counteract the diagnostic performance limitation brought about by the small model capacity, it integrates the attention-based feature fusion module and the multiple auxiliary losses enhancement strategy for better detecting subtle image changes and optimizing weights. Using transfer learning and five-fold cross-validation, Fast-MpoxNet achieves 94.26% Accuracy
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35838;&#31243;&#23398;&#20064;&#21644;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20248;&#21270;&#19968;&#23545;&#19968;&#33258;&#20027;&#36187;&#36710;&#31574;&#30053;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#36880;&#27493;&#36807;&#28193;&#21040;&#26356;&#22797;&#26434;&#30340;&#30495;&#23454;&#29615;&#22659;&#65292;&#21516;&#26102;&#30830;&#20445;&#26234;&#33021;&#20307;&#30340;&#23433;&#20840;&#24615;&#21644;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2308.13491</link><description>&lt;p&gt;
&#23454;&#29616;&#20248;&#21270;&#30340;&#19968;&#23545;&#19968;&#33258;&#20027;&#36187;&#36710;&#31574;&#30053;&#30340;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
Towards Optimal Head-to-head Autonomous Racing with Curriculum Reinforcement Learning. (arXiv:2308.13491v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35838;&#31243;&#23398;&#20064;&#21644;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20248;&#21270;&#19968;&#23545;&#19968;&#33258;&#20027;&#36187;&#36710;&#31574;&#30053;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#36880;&#27493;&#36807;&#28193;&#21040;&#26356;&#22797;&#26434;&#30340;&#30495;&#23454;&#29615;&#22659;&#65292;&#21516;&#26102;&#30830;&#20445;&#26234;&#33021;&#20307;&#30340;&#23433;&#20840;&#24615;&#21644;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#23545;&#19968;&#33258;&#20027;&#36187;&#36710;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#36710;&#36742;&#38656;&#35201;&#22312;&#25705;&#25830;&#25110;&#25805;&#25511;&#26497;&#38480;&#19979;&#36816;&#34892;&#65292;&#20197;&#23454;&#29616;&#26368;&#30701;&#30340;&#22280;&#36895;&#65292;&#21516;&#26102;&#36824;&#35201;&#31215;&#26497;&#23547;&#25214;&#36229;&#36710;/&#20445;&#25345;&#39046;&#20808;&#30340;&#31574;&#30053;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#23545;&#19968;&#36187;&#36710;&#29615;&#22659;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#27169;&#25311;&#36710;&#36742;&#21160;&#21147;&#23398;&#12290;&#19968;&#20123;&#20808;&#21069;&#30340;&#30740;&#31350;&#23581;&#35797;&#30452;&#25509;&#22312;&#22797;&#26434;&#30340;&#36710;&#36742;&#21160;&#21147;&#23398;&#29615;&#22659;&#20013;&#23398;&#20064;&#31574;&#30053;&#65292;&#20294;&#26410;&#33021;&#23398;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35838;&#31243;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;&#36739;&#31616;&#21333;&#30340;&#36710;&#36742;&#27169;&#22411;&#36807;&#28193;&#21040;&#26356;&#22797;&#26434;&#30340;&#30495;&#23454;&#29615;&#22659;&#65292;&#23558;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#31574;&#30053;&#35757;&#32451;&#24471;&#26356;&#25509;&#36817;&#26368;&#20248;&#31574;&#30053;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#26356;&#26377;&#25928;&#22320;&#20445;&#35777;&#26234;&#33021;&#20307;&#30340;&#23433;&#20840;&#24615;&#32780;&#19981;&#25439;&#23475;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Head-to-head autonomous racing is a challenging problem, as the vehicle needs to operate at the friction or handling limits in order to achieve minimum lap times while also actively looking for strategies to overtake/stay ahead of the opponent. In this work we propose a head-to-head racing environment for reinforcement learning which accurately models vehicle dynamics. Some previous works have tried learning a policy directly in the complex vehicle dynamics environment but have failed to learn an optimal policy. In this work, we propose a curriculum learning-based framework by transitioning from a simpler vehicle model to a more complex real environment to teach the reinforcement learning agent a policy closer to the optimal policy. We also propose a control barrier function-based safe reinforcement learning algorithm to enforce the safety of the agent in a more effective way while not compromising on optimality.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#31354;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#20316;&#20026;&#21160;&#24577;&#36136;&#37327;&#25511;&#21046;&#65288;dQC&#65289;&#24037;&#20855;&#65292;&#29992;&#20110;DNN&#20998;&#21106;&#33258;&#30001;&#21628;&#21560;DCE-CMRI&#25968;&#25454;&#38598;&#65292;&#24182;&#24314;&#31435;&#20102;&#20154;&#22312;&#22238;&#36335;&#26694;&#26550;&#26469;&#25913;&#21892;&#20998;&#21106;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.13488</link><description>&lt;p&gt;
&#20020;&#26102;&#19981;&#30830;&#23450;&#24615;&#23450;&#20301;&#20197;&#23454;&#29616;&#20154;&#22312;&#22238;&#36335;&#20998;&#26512;&#21160;&#24577;&#22686;&#24378;&#24515;&#33039;&#30913;&#20849;&#25391;&#25104;&#20687;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Temporal Uncertainty Localization to Enable Human-in-the-loop Analysis of Dynamic Contrast-enhanced Cardiac MRI Datasets. (arXiv:2308.13488v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#31354;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#20316;&#20026;&#21160;&#24577;&#36136;&#37327;&#25511;&#21046;&#65288;dQC&#65289;&#24037;&#20855;&#65292;&#29992;&#20110;DNN&#20998;&#21106;&#33258;&#30001;&#21628;&#21560;DCE-CMRI&#25968;&#25454;&#38598;&#65292;&#24182;&#24314;&#31435;&#20102;&#20154;&#22312;&#22238;&#36335;&#26694;&#26550;&#26469;&#25913;&#21892;&#20998;&#21106;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22686;&#24378;&#24515;&#33039;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;DCE-CMRI&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#29992;&#20110;&#35786;&#26029;&#24515;&#32908;&#34880;&#27969;&#65288;&#28748;&#27880;&#65289;&#24322;&#24120;&#30340;&#27169;&#24335;&#12290;&#22312;&#20856;&#22411;&#30340;&#33258;&#30001;&#21628;&#21560;DCE-CMRI&#25195;&#25551;&#20013;&#65292;&#20250;&#22312;&#21508;&#31181;&#23545;&#27604;&#24230;&#8220;&#36827;&#20837;/&#31163;&#24320;&#8221;&#38454;&#27573;&#33719;&#21462;&#25509;&#36817;300&#24133;&#24515;&#32908;&#28748;&#27880;&#30340;&#26102;&#24207;&#22270;&#20687;&#12290;&#23545;&#20110;&#27599;&#19968;&#24103;DCE&#22270;&#20687;&#24207;&#21015;&#20013;&#24515;&#32908;&#36718;&#24275;&#30340;&#25163;&#21160;&#20998;&#21106;&#21487;&#33021;&#26159;&#32321;&#29712;&#21644;&#32791;&#26102;&#30340;&#65292;&#29305;&#21035;&#26159;&#24403;&#38750;&#21018;&#24615;&#36816;&#21160;&#26657;&#27491;&#22833;&#36133;&#25110;&#19981;&#21487;&#29992;&#26102;&#12290;&#34429;&#28982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#26174;&#31034;&#20986;&#23545;DCE-CMRI&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#26512;&#30340;&#28508;&#21147;&#65292;&#20294;&#32570;&#20047;&#19968;&#31181;&#21487;&#38752;&#26816;&#27979;&#22833;&#36133;&#20998;&#21106;&#30340;&#8220;&#21160;&#24577;&#36136;&#37327;&#25511;&#21046;&#8221;&#65288;dQC&#65289;&#25216;&#26415;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#31354;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#20316;&#20026;&#22522;&#20110;DNN&#30340;&#33258;&#30001;&#21628;&#21560;DCE-CMRI&#25968;&#25454;&#38598;&#20998;&#21106;&#30340;dQC&#24037;&#20855;&#65292;&#24182;&#36890;&#36807;&#22312;&#22806;&#37096;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#25152;&#25552;&#20986;&#30340;&#24230;&#37327;&#65292;&#24182;&#24314;&#31435;&#19968;&#20010;&#20154;&#22312;&#22238;&#36335;&#26694;&#26550;&#26469;&#25913;&#21892;&#20998;&#21106;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic contrast-enhanced (DCE) cardiac magnetic resonance imaging (CMRI) is a widely used modality for diagnosing myocardial blood flow (perfusion) abnormalities. During a typical free-breathing DCE-CMRI scan, close to 300 time-resolved images of myocardial perfusion are acquired at various contrast "wash in/out" phases. Manual segmentation of myocardial contours in each time-frame of a DCE image series can be tedious and time-consuming, particularly when non-rigid motion correction has failed or is unavailable. While deep neural networks (DNNs) have shown promise for analyzing DCE-CMRI datasets, a "dynamic quality control" (dQC) technique for reliably detecting failed segmentations is lacking. Here we propose a new space-time uncertainty metric as a dQC tool for DNN-based segmentation of free-breathing DCE-CMRI datasets by validating the proposed metric on an external dataset and establishing a human-in-the-loop framework to improve the segmentation results. In the proposed approach,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;LTL&#27169;&#22411;&#26816;&#27979;&#38382;&#39064;&#12290;&#23454;&#35777;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.13474</link><description>&lt;p&gt;
OCTAL: &#29992;&#20110;LTL&#27169;&#22411;&#26816;&#27979;&#30340;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
OCTAL: Graph Representation Learning for LTL Model Checking. (arXiv:2308.13474v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;LTL&#27169;&#22411;&#26816;&#27979;&#38382;&#39064;&#12290;&#23454;&#35777;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#26816;&#27979;&#24191;&#27867;&#24212;&#29992;&#20110;&#39564;&#35777;&#22797;&#26434;&#21644;&#24182;&#21457;&#31995;&#32479;&#26159;&#21542;&#31526;&#21512;&#35268;&#33539;&#12290;&#32431;&#31526;&#21495;&#26041;&#27861;&#34429;&#28982;&#27969;&#34892;&#65292;&#20294;&#30001;&#20110;&#38656;&#35201;&#36827;&#34892;&#20132;&#21449;&#20056;&#31215;&#25805;&#20316;&#32780;&#23548;&#33268;&#29366;&#24577;&#31354;&#38388;&#33192;&#32960;&#38382;&#39064;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#31995;&#32479;&#21644;/&#25110;&#35268;&#33539;&#26469;&#35828;&#25104;&#26412;&#22826;&#39640;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#65288;GRL&#65289;&#26469;&#35299;&#20915;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL&#65289;&#27169;&#22411;&#26816;&#27979;&#38382;&#39064;&#65292;&#31995;&#32479;&#21644;&#35268;&#33539;&#20998;&#21035;&#29992;B&#252;chi&#33258;&#21160;&#26426;&#21644;LTL&#20844;&#24335;&#34920;&#31034;&#12290;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;GRL&#30340;&#26032;&#26694;&#26550;\model&#65292;&#29992;&#20110;&#23398;&#20064;&#22270;&#24418;&#32467;&#26500;&#21270;&#31995;&#32479;&#21644;&#35268;&#33539;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#23558;&#27169;&#22411;&#26816;&#27979;&#38382;&#39064;&#36716;&#21270;&#20026;&#20108;&#20803;&#20998;&#31867;&#12290;&#22312;&#20004;&#20010;&#27169;&#22411;&#26816;&#27979;&#22330;&#26223;&#19978;&#30340;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#65292;\model&#21462;&#24471;&#20102;&#24456;&#26377;&#24076;&#26395;&#30340;&#20934;&#30830;&#24615;&#65292;&#19982;&#35268;&#33539;&#20856;&#22411;SOTA&#27169;&#22411;&#26816;&#26597;&#22120;&#30456;&#27604;&#65292;&#25972;&#20307;&#21152;&#36895;&#20102;&#39640;&#36798;11&#20493;&#65292;&#20165;&#20165;&#36827;&#34892;&#21487;&#28385;&#36275;&#24615;&#26816;&#26597;&#30340;&#21152;&#36895;&#29575;&#36798;&#21040;&#20102;31&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model Checking is widely applied in verifying the correctness of complex and concurrent systems against a specification. Pure symbolic approaches while popular, suffer from the state space explosion problem due to cross product operations required that make them prohibitively expensive for large-scale systems and/or specifications. In this paper, we propose to use graph representation learning (GRL) for solving linear temporal logic (LTL) model checking, where the system and the specification are expressed by a B{\"u}chi automaton and an LTL formula, respectively. A novel GRL-based framework \model, is designed to learn the representation of the graph-structured system and specification, which reduces the model checking problem to binary classification. Empirical experiments on two model checking scenarios show that \model achieves promising accuracy, with up to $11\times$ overall speedup against canonical SOTA model checkers and $31\times$ for satisfiability checking alone.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#30693;&#35782;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#30693;&#35782;&#24341;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#65292;&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#30693;&#35782;&#26469;&#24357;&#34917;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#30340;&#20449;&#24687;&#32570;&#22833;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13467</link><description>&lt;p&gt;
&#21033;&#29992;&#30693;&#35782;&#21644;&#24378;&#21270;&#23398;&#20064;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Leveraging Knowledge and Reinforcement Learning for Enhanced Reliability of Language Models. (arXiv:2308.13467v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#30693;&#35782;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#30693;&#35782;&#24341;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#65292;&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#30693;&#35782;&#26469;&#24357;&#34917;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#30340;&#20449;&#24687;&#32570;&#22833;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#31038;&#21306;&#19968;&#30452;&#22312;&#20351;&#29992;&#20247;&#21253;&#25216;&#26415;&#65292;&#21019;&#24314;&#29992;&#20110;&#35757;&#32451;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#22914;BERT&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914;General Language Understanding and Evaluation(GLUE)&#12290;GLUE&#20219;&#21153;&#20351;&#29992;&#20114;&#35780;&#35745;&#37327;&#26041;&#27861;&#65288;&#22914;Cohens Kappa&#65289;&#26469;&#34913;&#37327;&#21487;&#38752;&#24615;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#26041;&#38754;&#24120;&#24120;&#34987;&#24573;&#35270;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#30693;&#35782;&#24341;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#23558;ConceptNet&#21644;&#32500;&#22522;&#30334;&#31185;&#30340;&#30693;&#35782;&#20316;&#20026;&#30693;&#35782;&#22270;&#23884;&#20837;&#36827;&#34892;&#25972;&#21512;&#12290;&#36825;&#31181;&#26041;&#27861;&#27169;&#20223;&#20102;&#20154;&#31867;&#27880;&#37322;&#32773;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#26469;&#24357;&#34917;&#25968;&#25454;&#38598;&#20013;&#30340;&#20449;&#24687;&#32570;&#22833;&#12290;&#36890;&#36807;&#22312;&#20061;&#20010;GLUE&#25968;&#25454;&#38598;&#19978;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21487;&#20197;&#22686;&#24378;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#24471;&#20998;&#65292;&#36229;&#36807;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Natural Language Processing(NLP) community has been using crowd sourcing techniques to create benchmark datasets such as General Language Understanding and Evaluation(GLUE) for training modern Language Models such as BERT. GLUE tasks measure the reliability scores using inter annotator metrics i.e. Cohens Kappa. However, the reliability aspect of LMs has often been overlooked. To counter this problem, we explore a knowledge-guided LM ensembling approach that leverages reinforcement learning to integrate knowledge from ConceptNet and Wikipedia as knowledge graph embeddings. This approach mimics human annotators resorting to external knowledge to compensate for information deficits in the datasets. Across nine GLUE datasets, our research shows that ensembling strengthens reliability and accuracy scores, outperforming state of the art.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#30340;&#27010;&#24565;&#29942;&#39048;&#35760;&#24518;&#27169;&#22411;&#65288;CB2M&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#23558;&#24178;&#39044;&#25512;&#24191;&#21040;&#19981;&#21516;&#24773;&#22659;&#24182;&#37325;&#26032;&#24212;&#29992;&#20808;&#21069;&#24178;&#39044;&#26469;&#33258;&#21160;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;&#24403;&#27809;&#26377;&#20808;&#21069;&#30340;&#20154;&#31867;&#24178;&#39044;&#20449;&#24687;&#26102;&#65292;CB2M&#33021;&#22815;&#26816;&#27979;&#38169;&#35823;&#24182;&#35831;&#27714;&#26377;&#38024;&#23545;&#24615;&#30340;&#24178;&#39044;&#12290;</title><link>http://arxiv.org/abs/2308.13453</link><description>&lt;p&gt;
&#23398;&#20064;&#24178;&#39044;&#27010;&#24565;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
Learning to Intervene on Concept Bottlenecks. (arXiv:2308.13453v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13453
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#30340;&#27010;&#24565;&#29942;&#39048;&#35760;&#24518;&#27169;&#22411;&#65288;CB2M&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#23558;&#24178;&#39044;&#25512;&#24191;&#21040;&#19981;&#21516;&#24773;&#22659;&#24182;&#37325;&#26032;&#24212;&#29992;&#20808;&#21069;&#24178;&#39044;&#26469;&#33258;&#21160;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;&#24403;&#27809;&#26377;&#20808;&#21069;&#30340;&#20154;&#31867;&#24178;&#39044;&#20449;&#24687;&#26102;&#65292;CB2M&#33021;&#22815;&#26816;&#27979;&#38169;&#35823;&#24182;&#35831;&#27714;&#26377;&#38024;&#23545;&#24615;&#30340;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#32570;&#20047;&#35299;&#37322;&#24615;&#65292;&#32780;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBM&#65289;&#36890;&#36807;&#20854;&#27010;&#24565;&#34920;&#31034;&#25552;&#20379;&#22266;&#26377;&#30340;&#35299;&#37322;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#20204;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#26356;&#26032;&#27010;&#24565;&#20540;&#24182;&#32416;&#27491;&#27169;&#22411;&#30340;&#39044;&#27979;&#36755;&#20986;&#26469;&#36827;&#34892;&#24178;&#39044;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26041;&#27861;&#20013;&#36825;&#20123;&#24178;&#39044;&#20165;&#24212;&#29992;&#20110;&#27169;&#22411;&#19968;&#27425;&#21518;&#21363;&#34987;&#20002;&#24323;&#12290;&#20026;&#20102;&#32416;&#27491;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27010;&#24565;&#29942;&#39048;&#35760;&#24518;&#27169;&#22411;&#65288;CB2M&#65289;&#65292;&#36825;&#26159;CBM&#30340;&#19968;&#20010;&#25193;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CB2M&#36890;&#36807;&#21452;&#25240;&#21472;&#35760;&#24518;&#23398;&#20064;&#23558;&#24178;&#39044;&#30340;&#25512;&#24191;&#21040;&#36866;&#24403;&#30340;&#26032;&#24773;&#22659;&#20013;&#65292;&#20174;&#32780;&#33021;&#22815;&#23398;&#20064;&#26816;&#27979;&#38169;&#35823;&#24182;&#37325;&#26032;&#24212;&#29992;&#20808;&#21069;&#30340;&#24178;&#39044;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;CB2M&#33021;&#22815;&#20174;&#26368;&#21021;&#33719;&#24471;&#30340;&#23569;&#37327;&#24178;&#39044;&#20013;&#33258;&#21160;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22914;&#26524;&#27809;&#26377;&#20808;&#21069;&#30340;&#20154;&#31867;&#24178;&#39044;&#20449;&#24687;&#65292;CB2M&#21487;&#20197;&#26816;&#27979;&#21040;CBM&#29942;&#39048;&#30340;&#28508;&#22312;&#38169;&#35823;&#24182;&#35831;&#27714;&#26377;&#38024;&#23545;&#24615;&#30340;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
While traditional deep learning models often lack interpretability, concept bottleneck models (CBMs) provide inherent explanations via their concept representations. Specifically, they allow users to perform interventional interactions on these concepts by updating the concept values and thus correcting the predictive output of the model. Traditionally, however, these interventions are applied to the model only once and discarded afterward. To rectify this, we present concept bottleneck memory models (CB2M), an extension to CBMs. Specifically, a CB2M learns to generalize interventions to appropriate novel situations via a two-fold memory with which it can learn to detect mistakes and to reapply previous interventions. In this way, a CB2M learns to automatically improve model performance from a few initially obtained interventions. If no prior human interventions are available, a CB2M can detect potential mistakes of the CBM bottleneck and request targeted interventions. In our experime
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23558;&#23398;&#20064;&#21040;&#30340;&#26102;&#38388;&#33258;&#21160;&#26426;&#19982;&#20851;&#20110;&#31995;&#32479;&#30340;&#24418;&#24335;&#21270;&#30693;&#35782;&#22270;&#35889;&#30456;&#32467;&#21512;&#65292;&#25913;&#36827;&#22522;&#20110;&#27169;&#22411;&#30340;&#32593;&#32476;&#29289;&#29702;&#29983;&#20135;&#31995;&#32479;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#36890;&#36807;&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#25551;&#36848;&#27169;&#22411;&#21644;&#24322;&#24120;&#65292;&#20197;&#20415;&#25805;&#20316;&#21592;&#26356;&#23481;&#26131;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2308.13433</link><description>&lt;p&gt;
&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#34920;&#31034;&#23450;&#26102;&#33258;&#21160;&#26426;&#21450;&#20854;&#22312;&#32593;&#32476;&#29289;&#29702;&#29983;&#20135;&#31995;&#32479;&#20013;&#30340;&#26102;&#38388;&#24322;&#24120;
&lt;/p&gt;
&lt;p&gt;
Representing Timed Automata and Timing Anomalies of Cyber-Physical Production Systems in Knowledge Graphs. (arXiv:2308.13433v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23558;&#23398;&#20064;&#21040;&#30340;&#26102;&#38388;&#33258;&#21160;&#26426;&#19982;&#20851;&#20110;&#31995;&#32479;&#30340;&#24418;&#24335;&#21270;&#30693;&#35782;&#22270;&#35889;&#30456;&#32467;&#21512;&#65292;&#25913;&#36827;&#22522;&#20110;&#27169;&#22411;&#30340;&#32593;&#32476;&#29289;&#29702;&#29983;&#20135;&#31995;&#32479;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#36890;&#36807;&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#25551;&#36848;&#27169;&#22411;&#21644;&#24322;&#24120;&#65292;&#20197;&#20415;&#25805;&#20316;&#21592;&#26356;&#23481;&#26131;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#24322;&#24120;&#26816;&#27979;&#26159;&#35782;&#21035;&#32593;&#32476;&#29289;&#29702;&#29983;&#20135;&#31995;&#32479;&#39044;&#26399;&#34892;&#20026;&#20559;&#24046;&#30340;&#25104;&#21151;&#26041;&#27861;&#12290;&#30001;&#20110;&#25163;&#21160;&#21019;&#24314;&#36825;&#20123;&#27169;&#22411;&#26159;&#19968;&#39033;&#32791;&#26102;&#30340;&#36807;&#31243;&#65292;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#24182;&#20197;&#26102;&#38388;&#33258;&#21160;&#26426;&#31561;&#36890;&#29992;&#24418;&#24335;&#36827;&#34892;&#34920;&#31034;&#26159;&#26377;&#20248;&#21183;&#30340;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23545;&#31995;&#32479;&#30340;&#39069;&#22806;&#20449;&#24687;&#32570;&#20047;&#65292;&#36825;&#20123;&#27169;&#22411;&#65288;&#20197;&#21450;&#30001;&#27492;&#26816;&#27979;&#21040;&#30340;&#24322;&#24120;&#65289;&#21487;&#33021;&#38590;&#20197;&#35299;&#37322;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23558;&#23398;&#20064;&#21040;&#30340;&#26102;&#38388;&#33258;&#21160;&#26426;&#19982;&#20851;&#20110;&#31995;&#32479;&#30340;&#24418;&#24335;&#21270;&#30693;&#35782;&#22270;&#35889;&#30456;&#32467;&#21512;&#26469;&#25913;&#36827;&#22522;&#20110;&#27169;&#22411;&#30340;&#32593;&#32476;&#29289;&#29702;&#29983;&#20135;&#31995;&#32479;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#27169;&#22411;&#21644;&#26816;&#27979;&#21040;&#30340;&#24322;&#24120;&#37117;&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#25551;&#36848;&#65292;&#20197;&#20415;&#25805;&#20316;&#21592;&#26356;&#23481;&#26131;&#35299;&#37322;&#27169;&#22411;&#21644;&#26816;&#27979;&#21040;&#30340;&#24322;&#24120;&#12290;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#24517;&#35201;&#27010;&#24565;&#30340;&#26412;&#20307;&#35770;&#12290;&#35813;&#26041;&#27861;&#22312;&#19968;&#20010;&#20116;&#32592;&#28151;&#21512;&#32593;&#32476;&#29289;&#29702;&#29983;&#20135;&#31995;&#32479;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#33021;&#22815;&#24418;&#24335;&#21270;&#22320;&#23450;&#20041;&#33258;&#21160;&#26426;&#27169;&#22411;&#21644;&#26102;&#38388;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-Based Anomaly Detection has been a successful approach to identify deviations from the expected behavior of Cyber-Physical Production Systems. Since manual creation of these models is a time-consuming process, it is advantageous to learn them from data and represent them in a generic formalism like timed automata. However, these models - and by extension, the detected anomalies - can be challenging to interpret due to a lack of additional information about the system. This paper aims to improve model-based anomaly detection in CPPS by combining the learned timed automaton with a formal knowledge graph about the system. Both the model and the detected anomalies are described in the knowledge graph in order to allow operators an easier interpretation of the model and the detected anomalies. The authors additionally propose an ontology of the necessary concepts. The approach was validated on a five-tank mixing CPPS and was able to formally define both automata model as well as timin
&lt;/p&gt;</description></item><item><title>QKSAN&#26159;&#19968;&#20010;&#37327;&#23376;&#26680;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#36890;&#36807;&#32467;&#21512;&#37327;&#23376;&#26680;&#26041;&#27861;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#39640;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#39640;&#32500;&#37327;&#23376;&#25968;&#25454;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13422</link><description>&lt;p&gt;
QKSAN: &#37327;&#23376;&#26680;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
QKSAN: A Quantum Kernel Self-Attention Network. (arXiv:2308.13422v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13422
&lt;/p&gt;
&lt;p&gt;
QKSAN&#26159;&#19968;&#20010;&#37327;&#23376;&#26680;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#36890;&#36807;&#32467;&#21512;&#37327;&#23376;&#26680;&#26041;&#27861;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#39640;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#39640;&#32500;&#37327;&#23376;&#25968;&#25454;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;SAM&#65289;&#25797;&#38271;&#20174;&#25968;&#25454;&#20869;&#37096;&#25552;&#21462;&#37325;&#35201;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#27169;&#22411;&#32570;&#20047;&#20687;SAM&#36825;&#26679;&#21306;&#20998;&#20449;&#24687;&#30340;&#22266;&#26377;&#36830;&#25509;&#30340;&#33021;&#21147;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#22823;&#35268;&#27169;&#39640;&#32500;&#37327;&#23376;&#25968;&#25454;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#37327;&#23376;&#26680;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;QKSAM&#65289;&#65292;&#23427;&#23558;&#37327;&#23376;&#26680;&#26041;&#27861;&#65288;QKM&#65289;&#30340;&#25968;&#25454;&#34920;&#31034;&#20248;&#21183;&#19982;SAM&#30340;&#26377;&#25928;&#20449;&#24687;&#25552;&#21462;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;&#22522;&#20110;QKSAM&#26500;&#24314;&#20102;&#19968;&#20010;&#37327;&#23376;&#26680;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;QKSAN&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#24310;&#36831;&#27979;&#37327;&#21407;&#29702;&#65288;DMP&#65289;&#21644;&#26465;&#20214;&#27979;&#37327;&#25216;&#26415;&#65292;&#22312;&#35745;&#31639;&#36807;&#31243;&#20013;&#20197;&#27010;&#29575;&#27979;&#37327;&#30340;&#26041;&#24335;&#37322;&#25918;&#20102;&#19968;&#21322;&#30340;&#37327;&#23376;&#36164;&#28304;&#12290;&#37327;&#23376;&#26680;&#33258;&#27880;&#24847;&#21147;&#20998;&#25968;&#65288;QKSAS&#65289;&#30830;&#23450;&#27979;&#37327;&#26465;&#20214;&#24182;&#21453;&#26144;&#20102;&#37327;&#23376;&#31995;&#32479;&#30340;&#27010;&#29575;&#26412;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-Attention Mechanism (SAM) is skilled at extracting important information from the interior of data to improve the computational efficiency of models. Nevertheless, many Quantum Machine Learning (QML) models lack the ability to distinguish the intrinsic connections of information like SAM, which limits their effectiveness on massive high-dimensional quantum data. To address this issue, a Quantum Kernel Self-Attention Mechanism (QKSAM) is introduced, which combines the data representation benefit of Quantum Kernel Methods (QKM) with the efficient information extraction capability of SAM. A Quantum Kernel Self-Attention Network (QKSAN) framework is built based on QKSAM, with Deferred Measurement Principle (DMP) and conditional measurement techniques, which releases half of the quantum resources with probabilistic measurements during computation. The Quantum Kernel Self-Attention Score (QKSAS) determines the measurement conditions and reflects the probabilistic nature of quantum syste
&lt;/p&gt;</description></item><item><title>SoTaNa&#26159;&#19968;&#31181;&#24320;&#28304;&#36719;&#20214;&#24320;&#21457;&#21161;&#25163;&#65292;&#21033;&#29992;ChatGPT&#29983;&#25104;&#39640;&#36136;&#37327;&#25968;&#25454;&#24182;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#22686;&#24378;LLaMA&#27169;&#22411;&#12290;&#23427;&#22312;&#22238;&#31572;Stack Overflow&#38382;&#39064;&#21644;&#20195;&#30721;&#25688;&#35201;&#21644;&#29983;&#25104;&#26041;&#38754;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.13416</link><description>&lt;p&gt;
SoTaNa: &#24320;&#28304;&#36719;&#20214;&#24320;&#21457;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
SoTaNa: The Open-Source Software Development Assistant. (arXiv:2308.13416v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13416
&lt;/p&gt;
&lt;p&gt;
SoTaNa&#26159;&#19968;&#31181;&#24320;&#28304;&#36719;&#20214;&#24320;&#21457;&#21161;&#25163;&#65292;&#21033;&#29992;ChatGPT&#29983;&#25104;&#39640;&#36136;&#37327;&#25968;&#25454;&#24182;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#22686;&#24378;LLaMA&#27169;&#22411;&#12290;&#23427;&#22312;&#22238;&#31572;Stack Overflow&#38382;&#39064;&#21644;&#20195;&#30721;&#25688;&#35201;&#21644;&#29983;&#25104;&#26041;&#38754;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#24320;&#21457;&#22312;&#25512;&#21160;&#29616;&#20195;&#31038;&#20250;&#30340;&#21019;&#26032;&#21644;&#25928;&#29575;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20026;&#28385;&#36275;&#36825;&#19968;&#21160;&#24577;&#39046;&#22495;&#30340;&#38656;&#27714;&#65292;&#36843;&#20999;&#38656;&#35201;&#19968;&#31181;&#26377;&#25928;&#30340;&#36719;&#20214;&#24320;&#21457;&#21161;&#25163;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;ChatGPT&#23384;&#22312;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#26435;&#37325;&#31561;&#21487;&#21450;&#24615;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#20687;LLaMA&#36825;&#26679;&#30340;&#20854;&#20182;&#22823;&#22411;&#24320;&#28304;&#27169;&#22411;&#24050;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#38590;&#20197;&#29702;&#35299;&#20154;&#31867;&#24847;&#22270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SoTaNa&#65292;&#19968;&#31181;&#24320;&#28304;&#36719;&#20214;&#24320;&#21457;&#21161;&#25163;&#12290;SoTaNa&#21033;&#29992;ChatGPT&#29983;&#25104;&#22522;&#20110;&#25351;&#23548;&#30340;&#39640;&#36136;&#37327;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#30340;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#26469;&#22686;&#24378;&#24320;&#28304;&#22522;&#30784;&#27169;&#22411;LLaMA&#12290;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;\our{}&#22312;&#22238;&#31572;Stack Overflow&#38382;&#39064;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#23427;&#22312;&#20195;&#30721;&#25688;&#35201;&#21644;&#29983;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#20854;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software development plays a crucial role in driving innovation and efficiency across modern societies. To meet the demands of this dynamic field, there is a growing need for an effective software development assistant. However, existing large language models represented by ChatGPT suffer from limited accessibility, including training data and model weights. Although other large open-source models like LLaMA have shown promise, they still struggle with understanding human intent. In this paper, we present SoTaNa, an open-source software development assistant. SoTaNa utilizes ChatGPT to generate high-quality instruction-based data for the domain of software engineering and employs a parameter-efficient fine-tuning approach to enhance the open-source foundation model, LLaMA. We evaluate the effectiveness of \our{} in answering Stack Overflow questions and demonstrate its capabilities. Additionally, we discuss its capabilities in code summarization and generation, as well as the impact of
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;TFDNet&#30340;&#26102;&#39057;&#22686;&#24378;&#20998;&#35299;&#32593;&#32476;&#65292;&#29992;&#20110;&#20174;&#26102;&#39057;&#22495;&#25429;&#33719;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#30340;&#22522;&#26412;&#27169;&#24335;&#21644;&#26102;&#38388;&#21608;&#26399;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13386</link><description>&lt;p&gt;
TFDNet&#65306;&#22686;&#24378;&#26102;&#39057;&#20998;&#35299;&#32593;&#32476;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TFDNet: Time-Frequency Enhanced Decomposed Network for Long-term Time Series Forecasting. (arXiv:2308.13386v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13386
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;TFDNet&#30340;&#26102;&#39057;&#22686;&#24378;&#20998;&#35299;&#32593;&#32476;&#65292;&#29992;&#20110;&#20174;&#26102;&#39057;&#22495;&#25429;&#33719;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#30340;&#22522;&#26412;&#27169;&#24335;&#21644;&#26102;&#38388;&#21608;&#26399;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26159;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#65292;&#24182;&#19988;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#19987;&#27880;&#20110;&#25429;&#33719;&#26469;&#33258;&#21333;&#19968;&#22495;&#65288;&#20363;&#22914;&#26102;&#38388;&#22495;&#25110;&#39057;&#29575;&#22495;&#65289;&#30340;&#22522;&#26412;&#27169;&#24335;&#65292;&#24182;&#19988;&#27809;&#26377;&#20174;&#26102;&#39057;&#22495;&#32508;&#21512;&#22788;&#29702;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#26102;&#39057;&#22686;&#24378;&#20998;&#35299;&#32593;&#32476;&#65288;TFDNet&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26102;&#39057;&#22495;&#25429;&#33719;&#38271;&#26399;&#30340;&#22522;&#26412;&#27169;&#24335;&#21644;&#26102;&#38388;&#21608;&#26399;&#24615;&#12290;&#22312;TFDNet&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#23610;&#24230;&#26102;&#39057;&#22686;&#24378;&#32534;&#30721;&#22120;&#20027;&#24178;&#65292;&#24182;&#24320;&#21457;&#20004;&#20010;&#20998;&#21035;&#29992;&#20110;&#25429;&#33719;&#22810;&#20998;&#36776;&#29575;&#20013;&#20998;&#35299;&#36235;&#21183;&#21644;&#23395;&#33410;&#20998;&#37327;&#20013;&#30340;&#19981;&#21516;&#27169;&#24335;&#30340;&#36235;&#21183;&#21644;&#23395;&#33410;&#26102;&#39057;&#22359;&#12290;&#36890;&#36807;&#30740;&#31350;&#21644;&#25972;&#21512;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#28508;&#22312;&#19981;&#21516;&#36890;&#36947;&#30456;&#20851;&#27169;&#24335;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26102;&#39057;&#22359;&#20013;&#26680;&#25805;&#20316;&#30340;&#22810;&#26679;&#21270;&#20869;&#26680;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-term time series forecasting is a vital task and has a wide range of real applications. Recent methods focus on capturing the underlying patterns from one single domain (e.g. the time domain or the frequency domain), and have not taken a holistic view to process long-term time series from the time-frequency domains. In this paper, we propose a Time-Frequency Enhanced Decomposed Network (TFDNet) to capture both the long-term underlying patterns and temporal periodicity from the time-frequency domain. In TFDNet, we devise a multi-scale time-frequency enhanced encoder backbone and develop two separate trend and seasonal time-frequency blocks to capture the distinct patterns within the decomposed trend and seasonal components in multi-resolutions. Diverse kernel learning strategies of the kernel operations in time-frequency blocks have been explored, by investigating and incorporating the potential different channel-wise correlation patterns of multivariate time series. Experimental e
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26681;&#25454;&#20351;&#29992;CodeBERT&#27169;&#22411;&#20998;&#26512;&#32534;&#31243;&#35821;&#35328;&#30340;&#34920;&#31034;&#65292;&#21457;&#29616;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#22312;&#26631;&#35760;&#34920;&#31034;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#65292;&#24314;&#35758;&#20351;&#29992;&#36825;&#31181;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#26469;&#36873;&#25321;&#36328;&#22810;&#31181;&#35821;&#35328;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.13354</link><description>&lt;p&gt;
&#20851;&#20110;&#35821;&#35328;&#36873;&#25321;&#23545;&#35757;&#32451;&#21644;&#35780;&#20272;&#32534;&#31243;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On the Impact of Language Selection for Training and Evaluating Programming Language Models. (arXiv:2308.13354v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13354
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26681;&#25454;&#20351;&#29992;CodeBERT&#27169;&#22411;&#20998;&#26512;&#32534;&#31243;&#35821;&#35328;&#30340;&#34920;&#31034;&#65292;&#21457;&#29616;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#22312;&#26631;&#35760;&#34920;&#31034;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#65292;&#24314;&#35758;&#20351;&#29992;&#36825;&#31181;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#26469;&#36873;&#25321;&#36328;&#22810;&#31181;&#35821;&#35328;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#26174;&#31034;&#20986;&#22312;&#22686;&#24378;&#36825;&#20123;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#28508;&#21147;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#30340;&#26174;&#33879;&#36827;&#23637;&#19981;&#20165;&#36866;&#29992;&#20110;&#32534;&#31243;&#35821;&#35328;&#39046;&#22495;&#65292;&#32780;&#19988;&#36824;&#25193;&#23637;&#21040;&#32534;&#31243;&#35821;&#35328;&#39046;&#22495;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#20855;&#22791;&#20174;&#22810;&#31181;&#35821;&#35328;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#20294;&#35780;&#20272;&#36890;&#24120;&#21482;&#20851;&#27880;&#21516;&#19968;&#31181;&#35821;&#35328;&#30340;&#29305;&#23450;&#32452;&#21512;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;CodeBERT&#27169;&#22411;&#30340;&#32534;&#31243;&#35821;&#35328;&#34920;&#31034;&#20998;&#26512;&#26469;&#35780;&#20272;&#32534;&#31243;&#35821;&#35328;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#20687;C++&#12289;Python&#21644;Java&#36825;&#26679;&#30340;&#35821;&#35328;&#20013;&#30340;&#26631;&#35760;&#34920;&#31034;&#20043;&#38388;&#23384;&#22312;&#30456;&#36817;&#24615;&#65292;&#32780;&#20687;Mathematica&#21644;R&#36825;&#26679;&#30340;&#35821;&#35328;&#20013;&#30340;&#30456;&#21516;&#26631;&#35760;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#19981;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#22788;&#29702;&#22810;&#31181;&#35821;&#35328;&#26102;&#65292;&#36825;&#31181;&#29616;&#35937;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#25105;&#20204;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26469;&#36873;&#25321;&#19968;&#20010;&#21487;&#20197;&#24179;&#34913;&#22810;&#31181;&#35821;&#35328;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advancements in Transformer-based Language Models have demonstrated significant potential in enhancing the multilingual capabilities of these models. The remarkable progress made in this domain not only applies to natural language tasks but also extends to the domain of programming languages. Despite the ability of these models to learn from multiple languages, evaluations typically focus on particular combinations of the same languages. In this study, we evaluate the similarity of programming languages by analyzing their representations using a CodeBERT-based model. Our experiments reveal that token representation in languages such as C++, Python, and Java exhibit proximity to one another, whereas the same tokens in languages such as Mathematica and R display significant dissimilarity. Our findings suggest that this phenomenon can potentially result in performance challenges when dealing with diverse languages. Thus, we recommend using our similarity measure to select a div
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#21547;&#26377;&#24322;&#24120;&#26679;&#26412;&#30340;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.13352</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#23436;&#20840;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#27745;&#26579;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
A Generic Machine Learning Framework for Fully-Unsupervised Anomaly Detection with Contaminated Data. (arXiv:2308.13352v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13352
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#21547;&#26377;&#24322;&#24120;&#26679;&#26412;&#30340;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#20010;&#39046;&#22495;&#21644;&#24212;&#29992;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24050;&#32463;&#35299;&#20915;&#20102;&#24322;&#24120;&#26816;&#27979;&#65288;AD&#65289;&#20219;&#21153;&#12290;&#36825;&#20123;&#31639;&#27861;&#20013;&#22823;&#22810;&#25968;&#20351;&#29992;&#27491;&#24120;&#25968;&#25454;&#23545;&#19968;&#20010;&#22522;&#20110;&#27531;&#24046;&#30340;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#26681;&#25454;&#26410;&#35265;&#26679;&#26412;&#19982;&#23398;&#20064;&#21040;&#30340;&#27491;&#24120;&#33539;&#22260;&#30340;&#19981;&#30456;&#20284;&#24615;&#26469;&#20998;&#37197;&#24322;&#24120;&#20998;&#25968;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#22522;&#26412;&#20551;&#35774;&#26159;&#21487;&#20197;&#29992;&#26080;&#24322;&#24120;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#25805;&#20316;&#29615;&#22659;&#20013;&#65292;&#35757;&#32451;&#25968;&#25454;&#36890;&#24120;&#20250;&#19982;&#19968;&#23450;&#27604;&#20363;&#30340;&#24322;&#24120;&#26679;&#26412;&#28151;&#21512;&#12290;&#32780;&#21033;&#29992;&#27745;&#26579;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#24517;&#28982;&#20250;&#23548;&#33268;&#22522;&#20110;&#27531;&#24046;&#30340;&#31639;&#27861;&#30340;AD&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#29992;&#20110;AD&#20219;&#21153;&#30340;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#30340;&#25913;&#36827;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#26159;&#36890;&#29992;&#30340;&#65292;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#22522;&#20110;&#27531;&#24046;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#22312;&#20004;&#20010;&#22810;&#20803;&#26102;&#38388;&#25968;&#25454;&#38598;&#19978;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection (AD) tasks have been solved using machine learning algorithms in various domains and applications. The great majority of these algorithms use normal data to train a residual-based model, and assign anomaly scores to unseen samples based on their dissimilarity with the learned normal regime. The underlying assumption of these approaches is that anomaly-free data is available for training. This is, however, often not the case in real-world operational settings, where the training data may be contaminated with a certain fraction of abnormal samples. Training with contaminated data, in turn, inevitably leads to a deteriorated AD performance of the residual-based algorithms.  In this paper we introduce a framework for a fully unsupervised refinement of contaminated training data for AD tasks. The framework is generic and can be applied to any residual-based machine learning model. We demonstrate the application of the framework to two public datasets of multivariate time s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Squeeze&#32858;&#21512;&#28608;&#21169;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#20840;&#23616;&#36890;&#36947;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25380;&#21387;&#21644;&#32858;&#21512;&#28608;&#21169;&#30340;&#26041;&#24335;&#65292;&#22312;&#36890;&#36947;&#32423;&#21035;&#19978;&#36830;&#25509;&#31354;&#38388;&#34920;&#31034;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.13343</link><description>&lt;p&gt;
Squeeze&#32858;&#21512;&#28608;&#21169;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Squeeze aggregated excitation network. (arXiv:2308.13343v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Squeeze&#32858;&#21512;&#28608;&#21169;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#20840;&#23616;&#36890;&#36947;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25380;&#21387;&#21644;&#32858;&#21512;&#28608;&#21169;&#30340;&#26041;&#24335;&#65292;&#22312;&#36890;&#36947;&#32423;&#21035;&#19978;&#36830;&#25509;&#31354;&#38388;&#34920;&#31034;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#20855;&#26377;&#31354;&#38388;&#34920;&#31034;&#33021;&#21147;&#65292;&#36890;&#36807;&#22312;&#36890;&#36947;&#27700;&#24179;&#19978;&#26174;&#24335;&#24314;&#27169;&#65292;Squeeze and excitation&#23558;&#36890;&#36947;&#32423;&#21035;&#30340;&#34920;&#31034;&#36830;&#25509;&#36215;&#26469;&#12290;&#22810;&#23618;&#24863;&#30693;&#22120;&#23398;&#20064;&#20840;&#23616;&#34920;&#31034;&#65292;&#22312;&#22823;&#22810;&#25968;&#27169;&#22411;&#20013;&#65292;&#23427;&#36890;&#24120;&#22312;&#25152;&#26377;&#21367;&#31215;&#23618;&#20043;&#21518;&#22312;&#26368;&#21518;&#20351;&#29992;&#65292;&#20197;&#25910;&#38598;&#20998;&#31867;&#20043;&#21069;&#23398;&#21040;&#30340;&#25152;&#26377;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36890;&#36947;&#20869;&#24341;&#21457;&#20840;&#23616;&#34920;&#31034;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SaEnet&#65292;&#21363;Squeeze&#32858;&#21512;&#28608;&#21169;&#32593;&#32476;&#65292;&#29992;&#20110;&#23398;&#20064;&#23618;&#20043;&#38388;&#30340;&#20840;&#23616;&#36890;&#36947;&#34920;&#31034;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22359;&#21033;&#29992;&#25380;&#21387;&#21518;&#36890;&#36807;&#32858;&#21512;&#28608;&#21169;&#24674;&#22797;&#20854;&#24418;&#29366;&#20043;&#21069;&#20256;&#36882;&#37325;&#35201;&#20449;&#24687;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#22312;&#32593;&#32476;&#20013;&#20855;&#26377;&#22810;&#20998;&#25903;&#32447;&#24615;(&#23494;&#38598;)&#23618;&#30340;&#26032;&#24605;&#24819;&#12290;&#36825;&#20010;&#23618;&#20174;&#21387;&#32553;&#30340;&#20449;&#24687;&#20013;&#23398;&#20064;&#20840;&#23616;&#34920;&#31034;&#65292;&#22686;&#24378;&#20102;&#32593;&#32476;&#30340;&#34920;&#24449;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks have spatial representations which read patterns in the vision tasks. Squeeze and excitation links the channel wise representations by explicitly modeling on channel level. Multi layer perceptrons learn global representations and in most of the models it is used often at the end after all convolutional layers to gather all the information learned before classification. We propose a method of inducing the global representations within channels to have better performance of the model. We propose SaEnet, Squeeze aggregated excitation network, for learning global channelwise representation in between layers. The proposed module takes advantage of passing important information after squeeze by having aggregated excitation before regaining its shape. We also introduce a new idea of having a multibranch linear(dense) layer in the network. This learns global representations from the condensed information which enhances the representational power of the network. Th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PGI&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#23454;&#38469;&#21830;&#19994;&#38382;&#39064;&#20013;&#24212;&#29992;&#20110;GPT&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;GPT&#27169;&#22411;&#30340;&#33021;&#21147;&#26469;&#29702;&#35299;&#22797;&#26434;&#30340;&#35821;&#35328;&#32467;&#26500;&#65292;&#24182;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#22238;&#24212;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;PGI&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24110;&#21161;&#35299;&#20915;&#20102;&#20154;&#31867;&#26234;&#33021;&#20302;&#24230;&#21033;&#29992;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.13317</link><description>&lt;p&gt;
&#25913;&#36896;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#30340;&#36755;&#20986;: PGI&#26694;&#26550;&#23545;&#27880;&#24847;&#21147;&#21160;&#24577;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Transforming the Output of Generative Pre-trained Transformer: The Influence of the PGI Framework on Attention Dynamics. (arXiv:2308.13317v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PGI&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#23454;&#38469;&#21830;&#19994;&#38382;&#39064;&#20013;&#24212;&#29992;&#20110;GPT&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;GPT&#27169;&#22411;&#30340;&#33021;&#21147;&#26469;&#29702;&#35299;&#22797;&#26434;&#30340;&#35821;&#35328;&#32467;&#26500;&#65292;&#24182;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#22238;&#24212;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;PGI&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24110;&#21161;&#35299;&#20915;&#20102;&#20154;&#31867;&#26234;&#33021;&#20302;&#24230;&#21033;&#29992;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Persona-Grouping-Intelligence (PGI)&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;GPT&#27169;&#22411;&#22312;&#23454;&#38469;&#21830;&#19994;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;PGI&#21033;&#29992;GPT&#27169;&#22411;&#30340;&#20869;&#22312;&#33021;&#21147;&#26469;&#29702;&#35299;&#22797;&#26434;&#30340;&#35821;&#35328;&#32467;&#26500;&#65292;&#24182;&#29983;&#25104;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#22238;&#24212;&#12290;&#23454;&#39564;&#22312;&#19968;&#20010;&#21830;&#19994;&#22330;&#26223;&#20013;&#36827;&#34892;&#65292;&#35813;&#22330;&#26223;&#23384;&#22312;&#20154;&#31867;&#26234;&#33021;&#34987;&#20302;&#25928;&#30340;&#21830;&#19994;&#27969;&#31243;&#20302;&#24230;&#21033;&#29992;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#21033;&#29992;GPT&#27169;&#22411;&#26469;&#20943;&#36731;&#20154;&#31867;&#22312;&#24191;&#27867;&#12289;&#21333;&#35843;&#21644;&#37325;&#22797;&#30340;&#20219;&#21153;&#20013;&#30340;&#24037;&#20316;&#36127;&#33655;&#65292;&#23558;&#37325;&#28857;&#36716;&#21521;&#20915;&#31574;&#27963;&#21160;&#12290;&#35813;&#23454;&#39564;&#29983;&#25104;&#30340;4,000&#20010;&#22238;&#24212;&#30340;&#39564;&#35777;&#20934;&#30830;&#29575;&#20026;93.81%&#65292;&#31361;&#20986;&#20102;PGI&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#31181;&#33539;&#24335;&#36716;&#21464;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#20154;&#31867;&#26234;&#33021;&#20302;&#24230;&#21033;&#29992;&#30340;&#38382;&#39064;&#65292;&#20351;&#20225;&#19994;&#29615;&#22659;&#19982;&#20915;&#31574;&#27963;&#21160;&#30456;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach named Persona-Grouping-Intelligence (PGI), which has been crafted to tackle the challenges posed by GPT models when applied to real-world business issues. PGI leverages the inherent capabilities of the GPT model to comprehend intricate language structures and generate responses that are contextually relevant. The experiment occurred in a business scenario where human intelligence was being underutilized due to less optimized business processes. The primary objective of this approach is to leverage GPT models to reduce the workload on humans in tasks that are extensive, monotonous, and repetitive. Instead, the focus is redirected toward decision-making activities. Remarkably, the experiment yielded an accuracy rate of 93.81% in validating 4,000 responses generated by the model, underscoring the effectiveness of the PGI strategies. Effectively addressing the issue of underutilized human intelligence, this paradigm shift aligns business environments wi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#22312;&#24037;&#19994;&#26426;&#22120;&#20154;&#32972;&#26223;&#19979;&#30340;&#32676;&#20307;&#21160;&#21147;&#23398;&#21644;&#21516;&#20394;&#21387;&#21147;&#12290;&#36890;&#36807;&#23558;&#32463;&#20856;&#30340;Asch&#23454;&#39564;&#24341;&#20837;HRI&#20013;&#65292;&#25105;&#20204;&#23558;&#27979;&#35797;&#21442;&#19982;&#32773;&#22312;&#32676;&#20307;&#26426;&#22120;&#20154;&#21516;&#20276;&#32473;&#20986;&#38169;&#35823;&#22238;&#24212;&#26102;&#23545;&#26426;&#22120;&#20154;&#22238;&#24212;&#30340;&#36981;&#24490;&#31243;&#24230;&#65292;&#24182;&#20851;&#27880;&#32676;&#20307;&#35268;&#27169;&#12289;&#26426;&#22120;&#20154;&#21487;&#20449;&#24230;&#12289;&#24515;&#29702;&#21387;&#21147;&#21644;&#21516;&#20394;&#21387;&#21147;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.13307</link><description>&lt;p&gt;
Asch&#36935;&#19978;HRI&#65306;&#20154;&#31867;&#23545;&#26426;&#22120;&#20154;&#32676;&#20307;&#30340;&#36981;&#20174;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Asch Meets HRI: Human Conformity to Robot Groups. (arXiv:2308.13307v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#22312;&#24037;&#19994;&#26426;&#22120;&#20154;&#32972;&#26223;&#19979;&#30340;&#32676;&#20307;&#21160;&#21147;&#23398;&#21644;&#21516;&#20394;&#21387;&#21147;&#12290;&#36890;&#36807;&#23558;&#32463;&#20856;&#30340;Asch&#23454;&#39564;&#24341;&#20837;HRI&#20013;&#65292;&#25105;&#20204;&#23558;&#27979;&#35797;&#21442;&#19982;&#32773;&#22312;&#32676;&#20307;&#26426;&#22120;&#20154;&#21516;&#20276;&#32473;&#20986;&#38169;&#35823;&#22238;&#24212;&#26102;&#23545;&#26426;&#22120;&#20154;&#22238;&#24212;&#30340;&#36981;&#24490;&#31243;&#24230;&#65292;&#24182;&#20851;&#27880;&#32676;&#20307;&#35268;&#27169;&#12289;&#26426;&#22120;&#20154;&#21487;&#20449;&#24230;&#12289;&#24515;&#29702;&#21387;&#21147;&#21644;&#21516;&#20394;&#21387;&#21147;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30740;&#31350;&#22823;&#32434;&#65292;&#26088;&#22312;&#25506;&#31350;&#22312;&#24037;&#19994;&#26426;&#22120;&#20154;&#30340;&#32972;&#26223;&#19979;&#30340;&#32676;&#20307;&#21160;&#21147;&#23398;&#21644;&#21516;&#20394;&#21387;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35745;&#21010;&#30340;&#21160;&#26426;&#26159;&#24037;&#19994;&#26426;&#22120;&#20154;&#24050;&#32463;&#25104;&#20026;&#20154;&#26426;&#20849;&#21516;&#24037;&#20316;&#30340;&#19968;&#20010;&#19981;&#21487;&#25110;&#32570;&#30340;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#24037;&#19994;&#26426;&#22120;&#20154;&#22312;&#26426;&#22120;&#20154;&#21487;&#20449;&#24230;&#12289;&#32676;&#20307;&#21160;&#21147;&#23398;&#20197;&#21450;&#28508;&#22312;&#29992;&#25143;&#36319;&#38543;&#26426;&#22120;&#20154;&#25351;&#31034;&#30340;&#20542;&#21521;&#26041;&#38754;&#30340;&#30740;&#31350;&#20013;&#36824;&#24456;&#23569;&#34987;&#25972;&#21512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23558;&#32463;&#20856;&#30340;Asch&#23454;&#39564;&#65288;&#21442;&#35265;\cite{Asch_51}&#65289;&#23558;&#20854;&#36716;&#21270;&#20026;&#22312;&#24037;&#19994;&#26426;&#22120;&#20154;&#20013;&#36827;&#34892;&#30340;HRI&#23454;&#39564;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#27979;&#35797;&#21442;&#19982;&#32773;&#22312;&#38754;&#23545;&#19968;&#20010;&#32676;&#20307;&#65288;&#30456;&#23545;&#20110;&#20010;&#20307;&#65289;&#30340;&#24037;&#19994;&#26426;&#22120;&#20154;&#25163;&#33218;&#65288;&#30456;&#23545;&#20110;&#20154;&#31867;&#65289;&#21516;&#20276;&#32473;&#20986;&#38169;&#35823;&#22238;&#24212;&#26102;&#65292;&#23545;&#26426;&#22120;&#20154;&#22238;&#24212;&#30340;&#36981;&#24490;&#31243;&#24230;&#12290;&#25105;&#20204;&#23545;&#24037;&#19994;&#26426;&#22120;&#20154;&#32972;&#26223;&#19979;&#30340;&#32676;&#20307;&#35268;&#27169;&#12289;&#34987;&#24863;&#30693;&#30340;&#26426;&#22120;&#20154;&#21487;&#20449;&#24230;&#12289;&#24515;&#29702;&#21387;&#21147;&#21644;&#21516;&#20394;&#21387;&#21147;&#30340;&#24433;&#21709;&#24863;&#20852;&#36259;&#12290;&#36890;&#36807;&#36825;&#39033;&#30740;&#31350;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#24076;&#26395;&#25581;&#31034;&#21487;&#33021;&#25903;&#25745;HRI&#30340;&#32676;&#20307;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a research outline that aims at investigating group dynamics and peer pressure in the context of industrial robots. Our research plan was motivated by the fact that industrial robots became already an integral part of human-robot co-working. However, industrial robots have been sparsely integrated into research on robot credibility, group dynamics, and potential users' tendency to follow a robot's indication. Therefore, we aim to transfer the classic Asch experiment (see \cite{Asch_51}) into HRI with industrial robots. More precisely, we will test to what extent participants follow a robot's response when confronted with a group (vs. individual) industrial robot arms (vs. human) peers who give a false response. We are interested in highlighting the effects of group size, perceived robot credibility, psychological stress, and peer pressure in the context of industrial robots. With the results of this research, we hope to highlight group dynamics that might underlie HRI in ind
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#21097;&#20313;&#20998;&#31867;&#22120;&#65288;DRC&#65289;&#26469;&#35299;&#20915;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;DRC&#36890;&#36807;&#20998;&#25903;&#23618;&#21512;&#24182;&#21644;&#27169;&#22411;&#36866;&#24212;&#21644;&#34701;&#21512;&#65288;MAF&#65289;&#27969;&#31243;&#30340;&#32467;&#21512;&#65292;&#22312;&#20256;&#32479;CI&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.13305</link><description>&lt;p&gt;
&#21160;&#24577;&#21097;&#20313;&#20998;&#31867;&#22120;&#29992;&#20110;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dynamic Residual Classifier for Class Incremental Learning. (arXiv:2308.13305v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13305
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#21097;&#20313;&#20998;&#31867;&#22120;&#65288;DRC&#65289;&#26469;&#35299;&#20915;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;DRC&#36890;&#36807;&#20998;&#25903;&#23618;&#21512;&#24182;&#21644;&#27169;&#22411;&#36866;&#24212;&#21644;&#34701;&#21512;&#65288;MAF&#65289;&#27969;&#31243;&#30340;&#32467;&#21512;&#65292;&#22312;&#20256;&#32479;CI&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#65292;&#28436;&#32451;&#31574;&#30053;&#34987;&#24191;&#27867;&#29992;&#20110;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#36890;&#36807;&#20445;&#30041;&#20808;&#21069;&#20219;&#21153;&#30340;&#26377;&#38480;&#33539;&#20363;&#12290;&#30001;&#20110;&#26087;&#31867;&#21644;&#26032;&#31867;&#20043;&#38388;&#26679;&#26412;&#25968;&#37327;&#19981;&#24179;&#34913;&#65292;&#20998;&#31867;&#22120;&#23398;&#20064;&#21487;&#33021;&#20250;&#20135;&#29983;&#20559;&#24046;&#12290;&#29616;&#26377;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#21033;&#29992;&#38271;&#23614;&#35782;&#21035;&#25216;&#26415;&#26469;&#22788;&#29702;&#22686;&#37327;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#20363;&#22914;&#35843;&#25972;&#30340;&#25439;&#22833;&#21644;&#25968;&#25454;&#37325;&#26032;&#21462;&#26679;&#26041;&#27861;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#21160;&#24577;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;&#21097;&#20313;&#20998;&#31867;&#22120;&#65288;DRC&#65289;&#26469;&#24212;&#23545;&#36825;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#26223;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DRC&#24314;&#31435;&#22312;&#26368;&#36817;&#30340;&#20808;&#36827;&#21097;&#20313;&#20998;&#31867;&#22120;&#20043;&#19978;&#65292;&#36890;&#36807;&#20998;&#25903;&#23618;&#21512;&#24182;&#26469;&#22788;&#29702;&#27169;&#22411;&#22686;&#38271;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;DRC&#19982;&#19981;&#21516;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#27969;&#31243;&#20860;&#23481;&#65292;&#24182;&#19988;&#26174;&#33879;&#25913;&#36827;&#20102;&#23427;&#20204;&#12290;&#23558;DRC&#19982;&#27169;&#22411;&#36866;&#24212;&#21644;&#34701;&#21512;&#65288;MAF&#65289;&#27969;&#31243;&#30456;&#32467;&#21512;&#65292;&#35813;&#26041;&#27861;&#22312;&#20256;&#32479;CI&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rehearsal strategy is widely used to alleviate the catastrophic forgetting problem in class incremental learning (CIL) by preserving limited exemplars from previous tasks. With imbalanced sample numbers between old and new classes, the classifier learning can be biased. Existing CIL methods exploit the long-tailed (LT) recognition techniques, e.g., the adjusted losses and the data re-sampling methods, to handle the data imbalance issue within each increment task. In this work, the dynamic nature of data imbalance in CIL is shown and a novel Dynamic Residual Classifier (DRC) is proposed to handle this challenging scenario. Specifically, DRC is built upon a recent advance residual classifier with the branch layer merging to handle the model-growing problem. Moreover, DRC is compatible with different CIL pipelines and substantially improves them. Combining DRC with the model adaptation and fusion (MAF) pipeline, this method achieves state-of-the-art results on both the conventional CI
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#36229;&#21442;&#25968;&#21270;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#23398;&#20064;&#32039;&#20945;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#36229;&#21442;&#25968;&#21270;&#27169;&#22411;&#26550;&#26500;&#24182;&#26356;&#26377;&#25928;&#22320;&#20849;&#20139;&#36328;&#20219;&#21153;&#30340;&#36229;&#21442;&#25968;&#21270;&#27169;&#22411;&#21442;&#25968;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#20248;&#21270;&#21644;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.13300</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#36229;&#21442;&#25968;&#21270;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#23398;&#20064;&#32039;&#20945;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Learning Compact Neural Networks with Deep Overparameterised Multitask Learning. (arXiv:2308.13300v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#36229;&#21442;&#25968;&#21270;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#23398;&#20064;&#32039;&#20945;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#36229;&#21442;&#25968;&#21270;&#27169;&#22411;&#26550;&#26500;&#24182;&#26356;&#26377;&#25928;&#22320;&#20849;&#20139;&#36328;&#20219;&#21153;&#30340;&#36229;&#21442;&#25968;&#21270;&#27169;&#22411;&#21442;&#25968;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#20248;&#21270;&#21644;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32039;&#20945;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#35768;&#22810;&#20248;&#28857;&#12290;&#28982;&#32780;&#65292;&#29992;&#23567;&#21442;&#25968;&#22823;&#23567;&#21644;&#20302;&#35745;&#31639;&#25104;&#26412;&#26469;&#35757;&#32451;&#32039;&#20945;&#30340;&#31070;&#32463;&#32593;&#32476;&#20197;&#36798;&#21040;&#19982;&#26356;&#22797;&#26434;&#12289;&#26356;&#24378;&#22823;&#30340;&#20307;&#31995;&#32467;&#26500;&#30456;&#21516;&#25110;&#26356;&#22909;&#30340;&#27169;&#22411;&#24615;&#33021;&#36890;&#24120;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#36825;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#23588;&#20854;&#22914;&#27492;&#65292;&#22240;&#20026;&#19981;&#21516;&#30340;&#20219;&#21153;&#31454;&#20105;&#36164;&#28304;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#26377;&#25928;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#36229;&#21442;&#25968;&#21270;&#27169;&#22411;&#26550;&#26500;&#24182;&#26356;&#26377;&#25928;&#22320;&#20849;&#20139;&#36328;&#20219;&#21153;&#30340;&#36229;&#21442;&#25968;&#21270;&#27169;&#22411;&#21442;&#25968;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#20248;&#21270;&#21644;&#27867;&#21270;&#12290;&#22312;&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#20219;&#21153;&#25968;&#25454;&#38598;&#65288;NYUv2&#21644;COCO&#65289;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#22312;&#21508;&#31181;&#21367;&#31215;&#32593;&#32476;&#21644;&#21442;&#25968;&#22823;&#23567;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compact neural network offers many benefits for real-world applications. However, it is usually challenging to train the compact neural networks with small parameter sizes and low computational costs to achieve the same or better model performance compared to more complex and powerful architecture. This is particularly true for multitask learning, with different tasks competing for resources. We present a simple, efficient and effective multitask learning overparameterisation neural network design by overparameterising the model architecture in training and sharing the overparameterised model parameters more effectively across tasks, for better optimisation and generalisation. Experiments on two challenging multitask datasets (NYUv2 and COCO) demonstrate the effectiveness of the proposed method across various convolutional networks and parameter sizes.
&lt;/p&gt;</description></item><item><title>JAX-LOB&#26159;&#31532;&#19968;&#20010;GPU&#21152;&#36895;&#30340;LOB&#27169;&#25311;&#22120;&#65292;&#21487;&#20197;&#24182;&#34892;&#22788;&#29702;&#25968;&#21315;&#20010;&#35746;&#21333;&#31807;&#65292;&#20197;&#36739;&#20302;&#30340;&#22788;&#29702;&#26102;&#38388;&#23454;&#29616;&#22823;&#35268;&#27169;&#24378;&#21270;&#23398;&#20064;&#20132;&#26131;&#65292;&#20026;&#37329;&#34701;&#20132;&#26131;&#30740;&#31350;&#25552;&#20379;&#20102;&#37325;&#35201;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2308.13289</link><description>&lt;p&gt;
JAX-LOB&#65306;&#19968;&#31181;&#22522;&#20110;GPU&#21152;&#36895;&#30340;&#38480;&#20215;&#21333;&#31807;&#27169;&#25311;&#22120;&#65292;&#20026;&#22823;&#35268;&#27169;&#24378;&#21270;&#23398;&#20064;&#20132;&#26131;&#35299;&#38145;
&lt;/p&gt;
&lt;p&gt;
JAX-LOB: A GPU-Accelerated limit order book simulator to unlock large scale reinforcement learning for trading. (arXiv:2308.13289v1 [q-fin.TR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13289
&lt;/p&gt;
&lt;p&gt;
JAX-LOB&#26159;&#31532;&#19968;&#20010;GPU&#21152;&#36895;&#30340;LOB&#27169;&#25311;&#22120;&#65292;&#21487;&#20197;&#24182;&#34892;&#22788;&#29702;&#25968;&#21315;&#20010;&#35746;&#21333;&#31807;&#65292;&#20197;&#36739;&#20302;&#30340;&#22788;&#29702;&#26102;&#38388;&#23454;&#29616;&#22823;&#35268;&#27169;&#24378;&#21270;&#23398;&#20064;&#20132;&#26131;&#65292;&#20026;&#37329;&#34701;&#20132;&#26131;&#30740;&#31350;&#25552;&#20379;&#20102;&#37325;&#35201;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#37329;&#34701;&#20132;&#26131;&#25152;&#20351;&#29992;&#38480;&#20215;&#21333;&#31807;&#65288;LOB&#65289;&#26469;&#22788;&#29702;&#35746;&#21333;&#21644;&#21305;&#37197;&#20132;&#26131;&#12290;&#20026;&#20102;&#30740;&#31350;&#30446;&#30340;&#65292;&#38656;&#35201;&#20855;&#26377;&#22823;&#35268;&#27169;&#39640;&#25928;&#30340;LOB&#21160;&#24577;&#27169;&#25311;&#22120;&#12290;&#20197;&#21069;&#26366;&#22312;&#22522;&#20110;&#20195;&#29702;&#27169;&#22411;&#65288;ABMs&#65289;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#29615;&#22659;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#20013;&#23454;&#29616;&#20102;LOB&#27169;&#25311;&#22120;&#65292;&#22788;&#29702;&#26469;&#33258;&#21382;&#21490;&#25968;&#25454;&#38598;&#21644;&#25163;&#24037;&#20195;&#29702;&#30340;&#35746;&#21333;&#27969;&#12290;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#65292;&#38656;&#35201;&#22788;&#29702;&#22810;&#20010;&#35746;&#21333;&#31807;&#65292;&#26080;&#35770;&#26159;&#29992;&#20110;ABM&#30340;&#26657;&#20934;&#36824;&#26159;RL&#20195;&#29702;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#39318;&#20010;&#33021;&#22815;&#24182;&#34892;&#22788;&#29702;&#25968;&#21315;&#26412;&#35746;&#21333;&#31807;&#19988;&#27599;&#20010;&#28040;&#24687;&#22788;&#29702;&#26102;&#38388;&#26174;&#33879;&#20943;&#23569;&#30340;GPU-enabled LOB&#27169;&#25311;&#22120;-JAX-LOB&#30340;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#27169;&#25311;&#22120;JAX-LOB&#30340;&#23454;&#29616;&#22522;&#20110;&#35774;&#35745;&#36873;&#25321;&#65292;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;JAX&#30340;&#21151;&#33021;&#65292;&#21516;&#26102;&#19981;&#25439;&#23475;&#19982;LOB&#30456;&#20851;&#30340;&#26426;&#21046;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#23558;JAX-LOB&#19982;&#20854;&#20182;JAX&#21253;&#38598;&#25104;&#65292;&#20197;&#25552;&#20379;&#22914;&#20309;&#36866;&#29992;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Financial exchanges across the world use limit order books (LOBs) to process orders and match trades. For research purposes it is important to have large scale efficient simulators of LOB dynamics. LOB simulators have previously been implemented in the context of agent-based models (ABMs), reinforcement learning (RL) environments, and generative models, processing order flows from historical data sets and hand-crafted agents alike. For many applications, there is a requirement for processing multiple books, either for the calibration of ABMs or for the training of RL agents. We showcase the first GPU-enabled LOB simulator designed to process thousands of books in parallel, with a notably reduced per-message processing time. The implementation of our simulator - JAX-LOB - is based on design choices that aim to best exploit the powers of JAX without compromising on the realism of LOB-related mechanisms. We integrate JAX-LOB with other JAX packages, to provide an example of how one may ad
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;AtmoRep&#30340;&#22823;&#27668;&#21160;&#21147;&#23398;&#38543;&#26426;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#22823;&#35268;&#27169;&#34920;&#31034;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#30830;&#23450;&#22797;&#26434;&#30340;&#22823;&#27668;&#21160;&#21147;&#23398;&#30340;&#36890;&#29992;&#25551;&#36848;&#65292;&#20174;&#32780;&#20026;&#21508;&#31181;&#24212;&#29992;&#25552;&#20379;&#25216;&#33021;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.13280</link><description>&lt;p&gt;
AtmoRep:&#19968;&#31181;&#21033;&#29992;&#22823;&#35268;&#27169;&#34920;&#31034;&#23398;&#20064;&#30340;&#22823;&#27668;&#21160;&#21147;&#23398;&#38543;&#26426;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AtmoRep: A stochastic model of atmosphere dynamics using large scale representation learning. (arXiv:2308.13280v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13280
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;AtmoRep&#30340;&#22823;&#27668;&#21160;&#21147;&#23398;&#38543;&#26426;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#22823;&#35268;&#27169;&#34920;&#31034;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#30830;&#23450;&#22797;&#26434;&#30340;&#22823;&#27668;&#21160;&#21147;&#23398;&#30340;&#36890;&#29992;&#25551;&#36848;&#65292;&#20174;&#32780;&#20026;&#21508;&#31181;&#24212;&#29992;&#25552;&#20379;&#25216;&#33021;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#27668;&#23545;&#20154;&#31867;&#26377;&#22810;&#31181;&#24433;&#21709;&#65292;&#20174;&#22240;&#22825;&#27668;&#19981;&#33391;&#32780;&#20007;&#29983;&#30340;&#25439;&#22833;&#21040;&#23545;&#31038;&#20250;&#30340;&#38271;&#26399;&#31038;&#20250;&#21644;&#32463;&#27982;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#23545;&#22823;&#27668;&#21160;&#21147;&#23398;&#36827;&#34892;&#35745;&#31639;&#26426;&#27169;&#25311;&#23545;&#25105;&#20204;&#21644;&#26410;&#26469;&#30340;&#19990;&#20195;&#30340;&#31119;&#31049;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AtmoRep&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#22823;&#27668;&#21160;&#21147;&#23398;&#38543;&#26426;&#35745;&#31639;&#26426;&#27169;&#22411;&#65292;&#21487;&#20197;&#20026;&#24191;&#27867;&#30340;&#24212;&#29992;&#25552;&#20379;&#25216;&#33021;&#32467;&#26524;&#12290;AtmoRep&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#22823;&#35268;&#27169;&#34920;&#31034;&#23398;&#20064;&#26469;&#30830;&#23450;&#22823;&#27668;&#39640;&#24230;&#22797;&#26434;&#12289;&#38543;&#26426;&#21160;&#21147;&#23398;&#30340;&#36890;&#29992;&#25551;&#36848;&#65292;&#35813;&#25551;&#36848;&#22522;&#20110;&#21382;&#21490;&#36712;&#36857;&#30340;&#26368;&#20339;&#21487;&#29992;&#20272;&#35745;&#65292;&#36825;&#20123;&#21382;&#21490;&#36712;&#36857;&#21463;&#35266;&#27979;&#32422;&#26463;&#12290;&#36825;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#21644;&#19968;&#20010;&#29420;&#29305;&#30340;&#38598;&#21512;&#23454;&#29616;&#30340;&#65292;&#35813;&#38598;&#21512;&#20174;&#38543;&#26426;&#27169;&#22411;&#20013;&#37319;&#26679;&#65292;&#20854;&#21487;&#21464;&#24615;&#21463;&#21382;&#21490;&#35760;&#24405;&#20013;&#30340;&#21487;&#21464;&#24615;&#21551;&#21457;&#12290;AtmoRep&#30340;&#20219;&#21153;&#26080;&#20851;&#24615;&#20351;&#20854;&#33021;&#22815;&#20026;&#21508;&#31181;&#24212;&#29992;&#25552;&#20379;&#28789;&#27963;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The atmosphere affects humans in a multitude of ways, from loss of life due to adverse weather effects to long-term social and economic impacts on societies. Computer simulations of atmospheric dynamics are, therefore, of great importance for the well-being of our and future generations. Here, we propose AtmoRep, a novel, task-independent stochastic computer model of atmospheric dynamics that can provide skillful results for a wide range of applications. AtmoRep uses large-scale representation learning from artificial intelligence to determine a general description of the highly complex, stochastic dynamics of the atmosphere from the best available estimate of the system's historical trajectory as constrained by observations. This is enabled by a novel self-supervised learning objective and a unique ensemble that samples from the stochastic model with a variability informed by the one in the historical record. The task-independent nature of AtmoRep enables skillful results for a divers
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#23558;&#38543;&#26426;&#26862;&#26519;&#25512;&#24191;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#27700;&#24179;&#29699;&#37325;&#26032;&#23450;&#20041;&#20102;&#20998;&#21106;&#30340;&#27010;&#24565;&#12290;&#20026;&#20102;&#22788;&#29702;&#22810;&#31867;&#25968;&#25454;&#21644;&#19981;&#24179;&#34913;&#23454;&#39564;&#65292;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31867;&#32452;&#21512;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.13279</link><description>&lt;p&gt;
&#38750;&#27431;&#20960;&#37324;&#24471;&#38543;&#26426;&#26862;&#26519;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic Random Forests. (arXiv:2308.13279v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13279
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#23558;&#38543;&#26426;&#26862;&#26519;&#25512;&#24191;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#27700;&#24179;&#29699;&#37325;&#26032;&#23450;&#20041;&#20102;&#20998;&#21106;&#30340;&#27010;&#24565;&#12290;&#20026;&#20102;&#22788;&#29702;&#22810;&#31867;&#25968;&#25454;&#21644;&#19981;&#24179;&#34913;&#23454;&#39564;&#65292;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31867;&#32452;&#21512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#30001;&#20110;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#20998;&#23618;&#32467;&#26500;&#65288;&#26080;&#35770;&#26159;&#38544;&#24335;&#36824;&#26159;&#26174;&#24335;&#65289;&#32780;&#25104;&#20026;&#34920;&#31034;&#25968;&#25454;&#30340;&#27969;&#34892;&#36873;&#25321;&#12290;&#38543;&#20043;&#32780;&#26469;&#30340;&#26159;&#38656;&#35201;&#33021;&#22815;&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#35299;&#20915;&#20998;&#31867;&#31561;&#22522;&#26412;&#20219;&#21153;&#30340;&#31639;&#27861;&#12290;&#26368;&#36817;&#65292;&#26377;&#22810;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#22522;&#20110;&#36229;&#24179;&#38754;&#30340;&#20998;&#31867;&#22120;&#65288;&#22914;&#36923;&#36753;&#22238;&#24402;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#65289;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#34429;&#28982;&#26377;&#25928;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#20998;&#23618;&#25968;&#25454;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#20247;&#25152;&#21608;&#30693;&#30340;&#38543;&#26426;&#26862;&#26519;&#25512;&#24191;&#21040;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#27700;&#24179;&#29699;&#37325;&#26032;&#23450;&#20041;&#20102;&#20998;&#21106;&#30340;&#27010;&#24565;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#30001;&#20110;&#25214;&#21040;&#20840;&#23616;&#26368;&#20248;&#20998;&#21106;&#26159;&#35745;&#31639;&#19978;&#38590;&#20197;&#22788;&#29702;&#30340;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#22823;&#36793;&#30028;&#20998;&#31867;&#22120;&#25214;&#21040;&#20505;&#36873;&#30340;&#27700;&#24179;&#29699;&#12290;&#20026;&#20102;&#20351;&#38750;&#27431;&#20960;&#37324;&#24471;&#38543;&#26426;&#26862;&#26519;&#36866;&#29992;&#20110;&#22810;&#31867;&#25968;&#25454;&#21644;&#19981;&#24179;&#34913;&#23454;&#39564;&#65292;&#25105;&#20204;&#36824;&#27010;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#23427;&#20204;&#30340;&#26368;&#20302;&#20844;&#20849;&#31062;&#20808;&#21644;&#31867;&#24179;&#34913;&#30340;&#31867;&#32452;&#21512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic space is becoming a popular choice for representing data due to the hierarchical structure - whether implicit or explicit - of many real-world datasets. Along with it comes a need for algorithms capable of solving fundamental tasks, such as classification, in hyperbolic space. Recently, multiple papers have investigated hyperbolic alternatives to hyperplane-based classifiers, such as logistic regression and SVMs. While effective, these approaches struggle with more complex hierarchical data. We, therefore, propose to generalize the well-known random forests to hyperbolic space. We do this by redefining the notion of a split using horospheres. Since finding the globally optimal split is computationally intractable, we find candidate horospheres through a large-margin classifier. To make hyperbolic random forests work on multi-class data and imbalanced experiments, we furthermore outline a new method for combining classes based on their lowest common ancestor and a class-balan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;LLMs&#21644;Decision Transformers&#38598;&#25104;&#21040;&#35821;&#35328;&#39537;&#21160;&#30340;&#29983;&#25104;&#36136;&#37327;&#22810;&#26679;&#24615;&#38382;&#39064;&#20013;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#21152;&#20855;&#26377;&#36712;&#36857;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#24211;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#20381;&#36182;&#20110;&#36825;&#20123;&#25551;&#36848;&#30340;&#31574;&#30053;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.13278</link><description>&lt;p&gt;
&#23558;LLMs&#21644;Decision Transformers&#38598;&#25104;&#21040;&#35821;&#35328;&#39537;&#21160;&#30340;&#29983;&#25104;&#36136;&#37327;&#22810;&#26679;&#24615;&#20013;
&lt;/p&gt;
&lt;p&gt;
Integrating LLMs and Decision Transformers for Language Grounded Generative Quality-Diversity. (arXiv:2308.13278v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;LLMs&#21644;Decision Transformers&#38598;&#25104;&#21040;&#35821;&#35328;&#39537;&#21160;&#30340;&#29983;&#25104;&#36136;&#37327;&#22810;&#26679;&#24615;&#38382;&#39064;&#20013;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#21152;&#20855;&#26377;&#36712;&#36857;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#24211;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#20381;&#36182;&#20110;&#36825;&#20123;&#25551;&#36848;&#30340;&#31574;&#30053;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36136;&#37327;&#22810;&#26679;&#24615;&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#21644;&#25511;&#21046;&#39046;&#22495;&#38382;&#39064;&#30340;&#38543;&#26426;&#20248;&#21270;&#20998;&#25903;&#65292;&#20854;&#30446;&#30340;&#26159;&#26500;&#24314;&#34920;&#29616;&#33391;&#22909;&#19988;&#22312;&#34892;&#20026;&#31354;&#38388;&#19978;&#20855;&#26377;&#22810;&#26679;&#24615;&#30340;&#25919;&#31574;/&#25216;&#33021;&#24211;&#12290;&#36825;&#26679;&#30340;&#23384;&#26723;&#36890;&#24120;&#30001;&#26377;&#38480;&#25968;&#37327;&#30340;&#21453;&#24212;&#20195;&#29702;&#32452;&#25104;&#65292;&#27599;&#20010;&#20195;&#29702;&#37117;&#19982;&#21807;&#19968;&#30340;&#34892;&#20026;&#25551;&#36848;&#31526;&#30456;&#20851;&#32852;&#65292;&#32780;&#22312;&#31895;&#30053;&#31163;&#25955;&#21270;&#30340;&#31354;&#38388;&#20043;&#22806;&#23454;&#20363;&#21270;&#34892;&#20026;&#25551;&#36848;&#31526;&#24182;&#19981;&#30452;&#35266;&#12290;&#34429;&#28982;&#26368;&#36817;&#26377;&#19968;&#20123;&#24037;&#20316;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20294;&#29983;&#25104;&#30340;&#36712;&#36857;&#22312;&#30446;&#26631;&#34892;&#20026;&#25551;&#36848;&#31526;&#35268;&#23450;&#20043;&#22806;&#24456;&#38590;&#36827;&#34892;&#23450;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#22312;&#20855;&#26377;&#38745;&#24577;&#22330;&#26223;&#20803;&#32032;&#30340;&#29615;&#22659;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#21152;&#20855;&#26377;&#36712;&#36857;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#24211;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#20381;&#36182;&#20110;&#36825;&#20123;&#25551;&#36848;&#30340;&#31574;&#30053;&#26469;&#20849;&#21516;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quality-Diversity is a branch of stochastic optimization that is often applied to problems from the Reinforcement Learning and control domains in order to construct repertoires of well-performing policies/skills that exhibit diversity with respect to a behavior space. Such archives are usually composed of a finite number of reactive agents which are each associated to a unique behavior descriptor, and instantiating behavior descriptors outside of that coarsely discretized space is not straight-forward. While a few recent works suggest solutions to that issue, the trajectory that is generated is not easily customizable beyond the specification of a target behavior descriptor. We propose to jointly solve those problems in environments where semantic information about static scene elements is available by leveraging a Large Language Model to augment the repertoire with natural language descriptions of trajectories, and training a policy conditioned on those descriptions. Thus, our method 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#30693;&#35782;&#39537;&#21160;&#30340;&#24605;&#32500;&#38142;&#65288;KD-CoT&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#39564;&#35777;&#21644;&#20462;&#25913;LLMs&#20013;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#36890;&#36807;&#19982;&#22806;&#37096;&#30693;&#35782;&#30340;&#20132;&#20114;&#26469;&#35299;&#20915;&#24187;&#35273;&#21644;&#38169;&#35823;&#20256;&#25773;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.13259</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#39537;&#21160;&#30340;CoT&#65306;&#25506;&#32034;LLMs&#20013;&#23545;&#30693;&#35782;&#23494;&#38598;&#22411;&#38382;&#31572;&#36827;&#34892;&#24544;&#23454;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering. (arXiv:2308.13259v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#30693;&#35782;&#39537;&#21160;&#30340;&#24605;&#32500;&#38142;&#65288;KD-CoT&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#39564;&#35777;&#21644;&#20462;&#25913;LLMs&#20013;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#36890;&#36807;&#19982;&#22806;&#37096;&#30693;&#35782;&#30340;&#20132;&#20114;&#26469;&#35299;&#20915;&#24187;&#35273;&#21644;&#38169;&#35823;&#20256;&#25773;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#37197;&#22791;&#20102;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#24187;&#35273;&#21644;&#26080;&#27861;&#35775;&#38382;&#22806;&#37096;&#30693;&#35782;&#65292;LLMs&#22312;&#23545;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#65288;&#22914;&#30693;&#35782;&#24211;&#38382;&#31572;&#65289;&#36827;&#34892;&#25512;&#29702;&#26102;&#24120;&#24120;&#20250;&#20135;&#29983;&#19981;&#27491;&#30830;&#25110;&#19981;&#24544;&#23454;&#30340;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#30693;&#35782;&#39537;&#21160;&#30340;&#24605;&#32500;&#38142;&#65288;KD-CoT&#65289;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#19982;&#22806;&#37096;&#30693;&#35782;&#30340;&#20132;&#20114;&#26469;&#39564;&#35777;&#21644;&#20462;&#25913;CoT&#20013;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#20174;&#32780;&#20811;&#26381;&#24187;&#35273;&#21644;&#38169;&#35823;&#20256;&#25773;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23558;LLMs&#30340;CoT&#25512;&#29702;&#36807;&#31243;&#35268;&#33539;&#21270;&#20026;&#32467;&#26500;&#21270;&#30340;&#22810;&#36718;&#38382;&#31572;&#26684;&#24335;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;LLMs&#19982;&#19968;&#20010;&#38382;&#31572;&#31995;&#32479;&#36827;&#34892;&#20132;&#20114;&#65292;&#35813;&#31995;&#32479;&#26816;&#32034;&#22806;&#37096;&#30693;&#35782;&#24182;&#22522;&#20110;&#26816;&#32034;&#21040;&#30340;&#20934;&#30830;&#31572;&#26696;&#20135;&#29983;&#24544;&#23454;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#25105;&#20204;&#24320;&#21457;&#30340;KBQA CoT&#38598;&#21512;&#20419;&#36827;&#20102;LLMs&#30340;&#32467;&#26500;&#21270;CoT&#25512;&#29702;&#65292;&#23427;&#20316;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#19968;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Equipped with Chain-of-Thought (CoT), Large language models (LLMs) have shown impressive reasoning ability in various downstream tasks. Even so, suffering from hallucinations and the inability to access external knowledge, LLMs often come with incorrect or unfaithful intermediate reasoning steps, especially in the context of answering knowledge-intensive tasks such as KBQA. To alleviate this issue, we propose a framework called Knowledge-Driven Chain-of-Thought (KD-CoT) to verify and modify reasoning traces in CoT via interaction with external knowledge, and thus overcome the hallucinations and error propagation. Concretely, we formulate the CoT rationale process of LLMs into a structured multi-round QA format. In each round, LLMs interact with a QA system that retrieves external knowledge and produce faithful reasoning traces based on retrieved precise answers. The structured CoT reasoning of LLMs is facilitated by our developed KBQA CoT collection, which serves as in-context learning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#26694;&#26550;&#65292;&#20174;EEG&#20449;&#21495;&#20013;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#23545;&#40784;&#36825;&#20004;&#31181;&#27169;&#24577;&#12290;&#36890;&#36807;&#22312;&#26368;&#24191;&#27867;&#30340;EEG&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#21644;&#29983;&#29289;&#21512;&#29702;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13234</link><description>&lt;p&gt;
&#20174;&#33041;&#30005;&#22270;&#35299;&#30721;&#33258;&#28982;&#22270;&#20687;&#36827;&#34892;&#29289;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Decoding Natural Images from EEG for Object Recognition. (arXiv:2308.13234v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#26694;&#26550;&#65292;&#20174;EEG&#20449;&#21495;&#20013;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#23545;&#40784;&#36825;&#20004;&#31181;&#27169;&#24577;&#12290;&#36890;&#36807;&#22312;&#26368;&#24191;&#27867;&#30340;EEG&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#21644;&#29983;&#29289;&#21512;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#33041;&#33041;&#22270;&#65288;EEG&#65289;&#20197;&#20854;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#21644;&#36866;&#24230;&#30340;&#20449;&#22122;&#27604;&#32780;&#38395;&#21517;&#12290;&#26368;&#36817;&#65292;&#33021;&#21542;&#20174;EEG&#20013;&#35299;&#30721;&#33258;&#28982;&#22270;&#20687;&#25104;&#20026;&#28909;&#38376;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#30417;&#30563;&#30340;&#26694;&#26550;&#65292;&#20174;EEG&#20449;&#21495;&#20013;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#22270;&#20687;&#21644;EEG&#32534;&#30721;&#22120;&#20174;&#37197;&#23545;&#30340;&#22270;&#20687;&#21050;&#28608;&#21644;EEG&#21709;&#24212;&#20013;&#25552;&#21462;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#36890;&#36807;&#32422;&#26463;&#23427;&#20204;&#30340;&#30456;&#20284;&#24615;&#26469;&#23545;&#40784;&#36825;&#20004;&#31181;&#27169;&#24577;&#12290;&#27492;&#22806;&#65292;&#22312;EEG&#32534;&#30721;&#22120;&#20043;&#21069;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#27169;&#22359;&#65292;&#29992;&#20110;&#25429;&#25417;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26368;&#24191;&#27867;&#30340;EEG&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#22312;200&#31181;&#38646;&#26679;&#26412;&#20219;&#21153;&#20013;&#65292;top-1&#20934;&#30830;&#24230;&#36798;&#21040;15.6%&#65292;top-5&#20934;&#30830;&#24230;&#36798;&#21040;42.8%&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#23545;EEG&#20449;&#21495;&#30340;&#26102;&#38388;&#12289;&#31354;&#38388;&#12289;&#39057;&#35889;&#21644;&#35821;&#20041;&#26041;&#38754;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#33391;&#22909;&#30340;&#29983;&#29289;&#21512;&#29702;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electroencephalogram (EEG) is a brain signal known for its high time resolution and moderate signal-to-noise ratio. Whether natural images can be decoded from EEG has been a hot issue recently. In this paper, we propose a self-supervised framework to learn image representations from EEG signals. Specifically, image and EEG encoders are first used to extract features from paired image stimuli and EEG responses. Then we employ contrastive learning to align these two modalities by constraining their similarity. Additionally, we introduce two plug-in-play modules that capture spatial correlations before the EEG encoder. Our approach achieves state-of-the-art results on the most extensive EEG-image dataset, with a top-1 accuracy of 15.6% and a top-5 accuracy of 42.8% in 200-way zero-shot tasks. More importantly, extensive experiments analyzing the temporal, spatial, spectral, and semantic aspects of EEG signals demonstrate good biological plausibility. These results offer valuable insights 
&lt;/p&gt;</description></item><item><title>MultiCapCLIP&#26159;&#19968;&#31181;&#38646;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#19981;&#21516;&#22330;&#26223;&#21644;&#35821;&#35328;&#30340;&#35270;&#35273;&#23383;&#24149;&#65292;&#26080;&#38656;&#19979;&#28216;&#25968;&#25454;&#38598;&#20013;&#30340;&#20219;&#20309;&#26631;&#35760;&#12290;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#23427;&#36890;&#36807;&#26816;&#32034;&#27010;&#24565;&#25552;&#31034;&#24182;&#33258;&#32534;&#30721;&#23398;&#20064;&#20889;&#20316;&#39118;&#26684;&#26469;&#29983;&#25104;&#23383;&#24149;&#12290;&#22312;&#27979;&#35797;&#38454;&#27573;&#65292;&#23427;&#30452;&#25509;&#21033;&#29992;&#35270;&#35273;&#25968;&#25454;&#36827;&#34892;&#26816;&#32034;&#12290;</title><link>http://arxiv.org/abs/2308.13218</link><description>&lt;p&gt;
MultiCapCLIP: &#38646;&#26679;&#26412;&#22810;&#35821;&#35328;&#35270;&#35273;&#23383;&#24149;&#30340;&#33258;&#32534;&#30721;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
MultiCapCLIP: Auto-Encoding Prompts for Zero-Shot Multilingual Visual Captioning. (arXiv:2308.13218v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13218
&lt;/p&gt;
&lt;p&gt;
MultiCapCLIP&#26159;&#19968;&#31181;&#38646;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#19981;&#21516;&#22330;&#26223;&#21644;&#35821;&#35328;&#30340;&#35270;&#35273;&#23383;&#24149;&#65292;&#26080;&#38656;&#19979;&#28216;&#25968;&#25454;&#38598;&#20013;&#30340;&#20219;&#20309;&#26631;&#35760;&#12290;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#23427;&#36890;&#36807;&#26816;&#32034;&#27010;&#24565;&#25552;&#31034;&#24182;&#33258;&#32534;&#30721;&#23398;&#20064;&#20889;&#20316;&#39118;&#26684;&#26469;&#29983;&#25104;&#23383;&#24149;&#12290;&#22312;&#27979;&#35797;&#38454;&#27573;&#65292;&#23427;&#30452;&#25509;&#21033;&#29992;&#35270;&#35273;&#25968;&#25454;&#36827;&#34892;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#24335;&#35270;&#35273;&#23383;&#24149;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#22270;&#20687;&#25110;&#35270;&#39057;&#19982;&#29305;&#23450;&#35821;&#35328;&#30340;&#25551;&#36848;&#36827;&#34892;&#37197;&#23545;&#65288;&#21363;&#35270;&#35273;-&#23383;&#24149;&#23545;&#65289;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#22330;&#26223;&#21644;&#35821;&#35328;&#26469;&#35828;&#65292;&#25910;&#38598;&#21644;&#26631;&#27880;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26159;&#32791;&#26102;&#19988;&#26114;&#36149;&#30340;&#12290;&#22240;&#27492;&#65292;&#36890;&#24120;&#26080;&#27861;&#25552;&#20379;&#36275;&#22815;&#30340;&#26631;&#35760;&#23545;&#12290;&#20026;&#20102;&#35299;&#20915;&#26631;&#31614;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38646;&#26679;&#26412;&#26041;&#27861;MultiCapCLIP&#65292;&#23427;&#21487;&#20197;&#22312;&#27809;&#26377;&#19979;&#28216;&#25968;&#25454;&#38598;&#30340;&#20219;&#20309;&#26631;&#35760;&#30340;&#24773;&#20917;&#19979;&#20026;&#19981;&#21516;&#22330;&#26223;&#21644;&#35821;&#35328;&#29983;&#25104;&#35270;&#35273;&#23383;&#24149;&#12290;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;MultiCapCLIP&#20165;&#38656;&#35201;&#25991;&#26412;&#25968;&#25454;&#20316;&#20026;&#36755;&#20837;&#12290;&#28982;&#21518;&#23427;&#36827;&#34892;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#65306;1&#65289;&#26816;&#32034;&#20445;&#30041;&#26032;&#22330;&#26223;&#30456;&#20851;&#39046;&#22495;&#30693;&#35782;&#30340;&#27010;&#24565;&#25552;&#31034;&#65307;2&#65289;&#33258;&#32534;&#30721;&#25552;&#31034;&#20197;&#23398;&#20064;&#36755;&#20986;&#25152;&#38656;&#35821;&#35328;&#30340;&#23383;&#24149;&#30340;&#20889;&#20316;&#39118;&#26684;&#12290;&#22312;&#27979;&#35797;&#38454;&#27573;&#65292;MultiCapCLIP&#30452;&#25509;&#23558;&#35270;&#35273;&#25968;&#25454;&#20316;&#20026;&#36755;&#20837;&#26469;&#26816;&#32034;...
&lt;/p&gt;
&lt;p&gt;
Supervised visual captioning models typically require a large scale of images or videos paired with descriptions in a specific language (i.e., the vision-caption pairs) for training. However, collecting and labeling large-scale datasets is time-consuming and expensive for many scenarios and languages. Therefore, sufficient labeled pairs are usually not available. To deal with the label shortage problem, we present a simple yet effective zero-shot approach MultiCapCLIP that can generate visual captions for different scenarios and languages without any labeled vision-caption pairs of downstream datasets. In the training stage, MultiCapCLIP only requires text data for input. Then it conducts two main steps: 1) retrieving concept prompts that preserve the corresponding domain knowledge of new scenarios; 2) auto-encoding the prompts to learn writing styles to output captions in a desired language. In the testing stage, MultiCapCLIP instead takes visual data as input directly to retrieve the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#22270;ODE&#31639;&#27861;&#65288;PINGO&#65289;&#65292;&#29992;&#20110;&#27169;&#25311;&#21644;&#24314;&#27169;&#22810;&#23545;&#35937;&#29289;&#29702;&#31995;&#32479;&#30340;&#38271;&#26399;&#21160;&#24577;&#65292;&#24182;&#35299;&#20915;&#20102;&#24403;&#21069;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.13212</link><description>&lt;p&gt;
&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#22270;ODE&#29992;&#20110;&#38271;&#26399;&#21160;&#21147;&#23398;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Physics-Inspired Neural Graph ODE for Long-term Dynamical Simulation. (arXiv:2308.13212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13212
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#22270;ODE&#31639;&#27861;&#65288;PINGO&#65289;&#65292;&#29992;&#20110;&#27169;&#25311;&#21644;&#24314;&#27169;&#22810;&#23545;&#35937;&#29289;&#29702;&#31995;&#32479;&#30340;&#38271;&#26399;&#21160;&#24577;&#65292;&#24182;&#35299;&#20915;&#20102;&#24403;&#21069;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#21644;&#24314;&#27169;&#22810;&#23545;&#35937;&#29289;&#29702;&#31995;&#32479;&#30340;&#38271;&#26399;&#21160;&#24577;&#26159;&#19968;&#39033;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#21033;&#29992;&#20855;&#26377;&#31561;&#21464;&#24615;&#36136;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#23545;&#29289;&#29702;&#31995;&#32479;&#36827;&#34892;&#24314;&#27169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20182;&#20204;&#23558;&#21160;&#21147;&#23398;&#24314;&#27169;&#20026;&#19968;&#31995;&#21015;&#20855;&#26377;&#22266;&#23450;&#26102;&#38388;&#38388;&#38548;&#30340;&#31163;&#25955;&#29366;&#24577;&#65292;&#24182;&#23398;&#20064;&#25152;&#26377;&#30456;&#37051;&#29366;&#24577;&#20043;&#38388;&#30340;&#30452;&#25509;&#26144;&#23556;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#30452;&#25509;&#26144;&#23556;&#24573;&#30053;&#20102;&#20004;&#20010;&#29366;&#24577;&#20043;&#38388;&#30340;&#36830;&#32493;&#24615;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#25105;&#20204;&#24050;&#32463;&#39564;&#35777;&#20102;&#22312;&#24403;&#21069;&#22522;&#20110;GNN&#30340;&#30452;&#25509;&#26144;&#23556;&#27169;&#22411;&#20013;&#65292;&#20004;&#20010;&#31163;&#25955;&#21160;&#24577;&#29366;&#24577;&#20043;&#38388;&#23384;&#22312;&#26080;&#25968;&#21487;&#33021;&#30340;&#36712;&#36857;&#12290;&#36825;&#20010;&#38382;&#39064;&#26497;&#22823;&#22320;&#38459;&#30861;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#23548;&#33268;&#38271;&#26399;&#27169;&#25311;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#36890;&#36807;&#31163;&#25955;&#30417;&#30563;&#20449;&#21495;&#24314;&#27169;&#28508;&#22312;&#36712;&#36857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#22270;ODE(PINGO)&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulating and modeling the long-term dynamics of multi-object physical systems is an essential and challenging task. Current studies model the physical systems utilizing Graph Neural Networks (GNNs) with equivariant properties. Specifically, they model the dynamics as a sequence of discrete states with a fixed time interval and learn a direct mapping for all the two adjacent states. However, this direct mapping overlooks the continuous nature between the two states. Namely, we have verified that there are countless possible trajectories between two discrete dynamic states in current GNN-based direct mapping models. This issue greatly hinders the model generalization ability, leading to poor performance of the long-term simulation. In this paper, to better model the latent trajectory through discrete supervision signals, we propose a Physics-Inspired Neural Graph ODE (PINGO) algorithm. In PINGO, to ensure the uniqueness of the trajectory, we construct a Physics-Inspired Neural ODE fram
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20154;&#26426;&#20132;&#20114;&#20013;&#24418;&#24335;&#21270;&#33258;&#28982;&#35821;&#35328;&#37327;&#35789;&#30340;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#31995;&#32479;&#20197;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#21040;&#36923;&#36753;&#34920;&#31034;&#30340;&#36716;&#25442;&#21644;&#35780;&#20272;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26426;&#22120;&#20154;&#30340;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2308.13192</link><description>&lt;p&gt;
&#20154;&#26426;&#20132;&#20114;&#20013;&#33258;&#28982;&#35821;&#35328;&#37327;&#35789;&#30340;&#24418;&#24335;&#21270;
&lt;/p&gt;
&lt;p&gt;
Formalising Natural Language Quantifiers for Human-Robot Interactions. (arXiv:2308.13192v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20154;&#26426;&#20132;&#20114;&#20013;&#24418;&#24335;&#21270;&#33258;&#28982;&#35821;&#35328;&#37327;&#35789;&#30340;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#31995;&#32479;&#20197;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#21040;&#36923;&#36753;&#34920;&#31034;&#30340;&#36716;&#25442;&#21644;&#35780;&#20272;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26426;&#22120;&#20154;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20154;&#26426;&#20132;&#20114;&#32972;&#26223;&#19979;&#24418;&#24335;&#21270;&#33258;&#28982;&#35821;&#35328;&#37327;&#35789;&#30340;&#26041;&#27861;&#12290;&#35813;&#35299;&#20915;&#26041;&#26696;&#22522;&#20110;&#19968;&#38454;&#36923;&#36753;&#24182;&#25193;&#23637;&#20102;&#34920;&#31034;&#21464;&#37327;&#22522;&#25968;&#30340;&#33021;&#21147;&#65292;&#31867;&#20284;&#20110;&#24191;&#20041;&#37327;&#35789;&#12290;&#20026;&#20102;&#23637;&#31034;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31995;&#32479;&#65292;&#33021;&#22815;&#25509;&#25910;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#65292;&#23558;&#20854;&#36716;&#25442;&#20026;&#24418;&#24335;&#21270;&#30340;&#36923;&#36753;&#34920;&#31034;&#65292;&#35780;&#20272;&#23427;&#65292;&#24182;&#36820;&#22238;&#32467;&#26524;&#25110;&#21521;&#27169;&#25311;&#26426;&#22120;&#20154;&#21457;&#36865;&#21629;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method for formalising quantifiers in natural language in the context of human-robot interactions. The solution is based on first-order logic extended with capabilities to represent the cardinality of variables, operating similarly to generalised quantifiers. To demonstrate the method, we designed an end-to-end system able to receive input as natural language, convert it into a formal logical representation, evaluate it, and return a result or send a command to a simulated robot.
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;transformer&#33021;&#22815;&#22788;&#29702;&#26356;&#38271;&#30340;&#24207;&#21015;&#65292;&#21516;&#26102;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#19982;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#32447;&#24615;&#22686;&#38271;&#12290;</title><link>http://arxiv.org/abs/2308.13191</link><description>&lt;p&gt;
Chunk, Align, Select: &#19968;&#31181;&#31616;&#21333;&#30340;&#29992;&#20110;transformer&#30340;&#38271;&#24207;&#21015;&#22788;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Chunk, Align, Select: A Simple Long-sequence Processing Method for Transformers. (arXiv:2308.13191v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13191
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;transformer&#33021;&#22815;&#22788;&#29702;&#26356;&#38271;&#30340;&#24207;&#21015;&#65292;&#21516;&#26102;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#19982;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#32447;&#24615;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#20173;&#28982;&#38754;&#20020;&#30528;&#38271;&#24207;&#21015;&#22788;&#29702;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;transformer&#20013;&#33258;&#27880;&#24847;&#25805;&#20316;&#30340;&#35745;&#31639;&#25104;&#26412;&#38543;&#30528;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#21152;&#21576;&#20108;&#27425;&#22686;&#38271;&#12290;&#20026;&#20102;&#20943;&#36731;&#38271;&#24207;&#21015;&#22788;&#29702;&#30340;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;transformer&#33021;&#22815;&#22788;&#29702;&#26356;&#38271;&#30340;&#24207;&#21015;&#65292;&#21516;&#26102;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#19982;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#32447;&#24615;&#22686;&#38271;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#27599;&#20010;&#38271;&#24207;&#21015;&#36755;&#20837;&#21010;&#20998;&#20026;&#19968;&#25209;chunk&#65292;&#28982;&#21518;&#22312;&#32534;&#30721;&#36807;&#31243;&#20013;&#23545;chunk&#20043;&#38388;&#30340;&#20449;&#24687;&#36827;&#34892;&#23545;&#40784;&#65292;&#26368;&#21518;&#20174;&#32534;&#30721;&#22120;&#20013;&#36873;&#25321;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#38544;&#34255;&#29366;&#24577;&#36827;&#34892;&#35299;&#30721;&#12290;&#20026;&#20102;&#25552;&#21462;chunk&#20043;&#38388;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#25105;&#20204;&#22312;&#27599;&#20010;&#32534;&#30721;transformer&#22359;&#20013;&#23545;chunk&#20043;&#38388;&#30340;&#36215;&#22987;&#21644;&#32467;&#26463;token&#36827;&#34892;&#23545;&#40784;&#12290;&#20026;&#20102;&#23398;&#20064;&#19968;&#20010;&#26377;&#25928;&#30340;&#38544;&#34255;&#29366;&#24577;&#36873;&#25321;&#31574;&#30053;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#37325;&#26356;&#26032;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although dominant in natural language processing, transformer-based models remain challenged by the task of long-sequence processing, because the computational cost of self-attention operations in transformers swells quadratically with the input sequence length. To alleviate the complexity of long-sequence processing, we propose a simple framework to enable the offthe-shelf pre-trained transformers to process much longer sequences, while the computation and memory costs remain growing linearly with the input sequence lengths. More specifically, our method divides each long-sequence input into a batch of chunks, then aligns the interchunk information during the encoding steps, and finally selects the most representative hidden states from the encoder for the decoding process. To extract inter-chunk semantic information, we align the start and end token embeddings among chunks in each encoding transformer block. To learn an effective hidden selection policy, we design a dual updating sch
&lt;/p&gt;</description></item><item><title>Falcon&#26159;&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;&#22522;&#20110;&#21516;&#24577;&#21152;&#23494;&#30340;&#21367;&#31215;&#30340;&#23494;&#38598;&#25171;&#21253;&#31639;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#25171;&#21253;&#23494;&#24230;&#21644;&#38477;&#20302;&#36890;&#20449;&#24320;&#38144;&#65292;&#23454;&#29616;&#20102;&#22312;&#36816;&#31639;&#22120;&#32423;&#21035;&#19978;&#36229;&#36807;15.6&#20493;&#30340;&#24310;&#36831;&#38477;&#20302;&#65292;&#24182;&#19988;&#22312;&#32593;&#32476;&#32423;&#21035;&#19978;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#21152;&#23494;&#25512;&#26029;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.13189</link><description>&lt;p&gt;
Falcon: &#21152;&#36895;&#22522;&#20110;&#21516;&#24577;&#21152;&#23494;&#30340;&#21367;&#31215;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#31169;&#23494;&#31227;&#21160;&#32593;&#32476;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Falcon: Accelerating Homomorphically Encrypted Convolutions for Efficient Private Mobile Network Inference. (arXiv:2308.13189v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13189
&lt;/p&gt;
&lt;p&gt;
Falcon&#26159;&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;&#22522;&#20110;&#21516;&#24577;&#21152;&#23494;&#30340;&#21367;&#31215;&#30340;&#23494;&#38598;&#25171;&#21253;&#31639;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#25171;&#21253;&#23494;&#24230;&#21644;&#38477;&#20302;&#36890;&#20449;&#24320;&#38144;&#65292;&#23454;&#29616;&#20102;&#22312;&#36816;&#31639;&#22120;&#32423;&#21035;&#19978;&#36229;&#36807;15.6&#20493;&#30340;&#24310;&#36831;&#38477;&#20302;&#65292;&#24182;&#19988;&#22312;&#32593;&#32476;&#32423;&#21035;&#19978;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#21152;&#23494;&#25512;&#26029;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#32593;&#32476;&#65292;&#22914;MobileNetV2&#12289;EfficientNet&#31561;&#65292;&#22312;&#36731;&#37327;&#32423;&#35745;&#31639;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#21516;&#24577;&#21152;&#23494;&#30340;&#20004;&#26041;&#35745;&#31639;&#26694;&#26550;&#23545;&#20110;&#36825;&#20123;&#32593;&#32476;&#24182;&#27809;&#26377;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#19988;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#23384;&#22312;&#39640;&#24320;&#38144;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20302;&#25928;&#20027;&#35201;&#26469;&#33258;&#20110;&#25171;&#21253;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#24573;&#30053;&#20102;&#35745;&#31639;&#29305;&#24615;&#21644;&#21516;&#24577;&#21152;&#23494;&#28145;&#24230;&#21367;&#31215;&#30340;&#36890;&#20449;&#29942;&#39048;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Falcon&#65292;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#21516;&#24577;&#21152;&#23494;&#30340;&#20004;&#26041;&#35745;&#31639;&#26694;&#26550;&#30340;&#26377;&#25928;&#23494;&#38598;&#25171;&#21253;&#31639;&#27861;&#12290;Falcon&#37319;&#29992;&#20102;&#19968;&#31181;&#38646;&#24863;&#30693;&#30340;&#36138;&#24515;&#25171;&#21253;&#31639;&#27861;&#21644;&#19968;&#31181;&#36890;&#20449;&#24863;&#30693;&#30340;&#25805;&#20316;&#22120;&#24179;&#38138;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#21367;&#31215;&#30340;&#25171;&#21253;&#23494;&#24230;&#12290;&#19982;SOTA&#30340;&#22522;&#20110;&#21516;&#24577;&#21152;&#23494;&#30340;&#20004;&#26041;&#35745;&#31639;&#26694;&#26550;&#65288;&#22914;CrypTFlow2&#12289;Iron&#21644;Cheetah&#65289;&#30456;&#27604;&#65292;Falcon&#22312;&#36816;&#31639;&#22120;&#32423;&#21035;&#20998;&#21035;&#23454;&#29616;&#20102;&#36229;&#36807;15.6&#20493;&#12289;5.1&#20493;&#21644;1.8&#20493;&#30340;&#24310;&#36831;&#38477;&#20302;&#12290;&#21516;&#26102;&#65292;&#22312;&#32593;&#32476;&#32423;&#21035;&#19978;&#65292;Falcon&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#21152;&#23494;&#25512;&#26029;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient networks, e.g., MobileNetV2, EfficientNet, etc, achieves state-of-the-art (SOTA) accuracy with lightweight computation. However, existing homomorphic encryption (HE)-based two-party computation (2PC) frameworks are not optimized for these networks and suffer from a high inference overhead. We observe the inefficiency mainly comes from the packing algorithm, which ignores the computation characteristics and the communication bottleneck of homomorphically encrypted depthwise convolutions. Therefore, in this paper, we propose Falcon, an effective dense packing algorithm for HE-based 2PC frameworks. Falcon features a zero-aware greedy packing algorithm and a communication-aware operator tiling strategy to improve the packing density for depthwise convolutions. Compared to SOTA HE-based 2PC frameworks, e.g., CrypTFlow2, Iron and Cheetah, Falcon achieves more than 15.6x, 5.1x and 1.8x latency reduction, respectively, at operator level. Meanwhile, at network level, Falcon allows for
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;SC-GAN&#65292;&#29992;&#20110;&#21512;&#25104;H&amp;E&#22270;&#20687;&#21644;IHC&#26579;&#33394;&#20043;&#38388;&#30340;&#36716;&#25442;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#36793;&#32536;&#32467;&#26500;&#20449;&#24687;&#21644;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#22686;&#24378;&#20102;&#29305;&#24449;&#23450;&#20301;&#24182;&#20445;&#30041;&#20102;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.13182</link><description>&lt;p&gt;
&#32467;&#26500;Cycle GAN&#29992;&#20110;&#22312;&#32467;&#32928;&#20013;&#23545;&#33146;&#20307;&#26631;&#35760;&#36827;&#34892;&#34394;&#25311;&#20813;&#30123;&#32452;&#32455;&#21270;&#23398;&#26579;&#33394;
&lt;/p&gt;
&lt;p&gt;
Structural Cycle GAN for Virtual Immunohistochemistry Staining of Gland Markers in the Colon. (arXiv:2308.13182v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;SC-GAN&#65292;&#29992;&#20110;&#21512;&#25104;H&amp;E&#22270;&#20687;&#21644;IHC&#26579;&#33394;&#20043;&#38388;&#30340;&#36716;&#25442;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#36793;&#32536;&#32467;&#26500;&#20449;&#24687;&#21644;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#22686;&#24378;&#20102;&#29305;&#24449;&#23450;&#20301;&#24182;&#20445;&#30041;&#20102;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#23383;&#25195;&#25551;&#20202;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#20986;&#29616;&#65292;&#35786;&#26029;&#25805;&#20316;&#21487;&#33021;&#20174;&#26174;&#24494;&#38236;&#31227;&#21040;&#26700;&#38754;&#19978;&#12290;&#34880;&#32418;&#32032;&#21644;&#20234;&#32418;&#26579;&#33394;&#65288;H&amp;E&#65289;&#26159;&#26368;&#24120;&#29992;&#20110;&#30142;&#30149;&#20998;&#26512;&#12289;&#35786;&#26029;&#21644;&#20998;&#32423;&#30340;&#26579;&#33394;&#26041;&#27861;&#20043;&#19968;&#65292;&#20294;&#30149;&#29702;&#23398;&#23478;&#30830;&#23454;&#38656;&#35201;&#19981;&#21516;&#30340;&#20813;&#30123;&#32452;&#32455;&#21270;&#23398;&#65288;IHC&#65289;&#26579;&#33394;&#26469;&#20998;&#26512;&#29305;&#23450;&#30340;&#32467;&#26500;&#25110;&#32454;&#32990;&#12290;&#22312;&#21333;&#20010;&#26631;&#26412;&#19978;&#33719;&#24471;&#25152;&#26377;&#36825;&#20123;&#26579;&#33394;&#65288;H&amp;E&#21644;&#19981;&#21516;&#30340;IHC&#65289;&#26159;&#19968;&#39033;&#32321;&#29712;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#34394;&#25311;&#26579;&#33394;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#32467;&#26500;Cycle GAN&#65288;SC-GAN&#65289;&#65292;&#29992;&#20110;&#20174;H&amp;E&#22270;&#20687;&#21512;&#25104;IHC&#26579;&#33394;&#21453;&#20043;&#20134;&#28982;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#30830;&#22320;&#23558;&#36793;&#32536;&#32467;&#26500;&#20449;&#24687;&#65288;&#38500;&#20102;&#39068;&#33394;&#25968;&#25454;&#65289;&#32435;&#20837;&#21040;&#25152;&#25552;&#20986;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#35299;&#30721;&#22120;&#20013;&#65292;&#24182;&#19988;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20165;&#20351;&#29992;&#27880;&#24847;&#21147;&#27169;&#22359;&#12290;&#36825;&#31181;&#38598;&#25104;&#22686;&#24378;&#20102;&#29305;&#24449;&#23450;&#20301;&#24182;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20445;&#30041;&#20102;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of digital scanners and deep learning, diagnostic operations may move from a microscope to a desktop. Hematoxylin and Eosin (H&amp;E) staining is one of the most frequently used stains for disease analysis, diagnosis, and grading, but pathologists do need different immunohistochemical (IHC) stains to analyze specific structures or cells. Obtaining all of these stains (H&amp;E and different IHCs) on a single specimen is a tedious and time-consuming task. Consequently, virtual staining has emerged as an essential research direction. Here, we propose a novel generative model, Structural Cycle-GAN (SC-GAN), for synthesizing IHC stains from H&amp;E images, and vice versa. Our method expressly incorporates structural information in the form of edges (in addition to color data) and employs attention modules exclusively in the decoder of the proposed generator model. This integration enhances feature localization and preserves contextual information during the generation process. In additi
&lt;/p&gt;</description></item><item><title>DAG-ACFL&#26159;&#19968;&#31181;&#22522;&#20110;DAG-DLT&#30340;&#24322;&#27493;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#36873;&#25321;&#30456;&#20284;&#20998;&#24067;&#30340;&#23458;&#25143;&#31471;&#27169;&#22411;&#26469;&#32858;&#21512;&#20840;&#23616;&#27169;&#22411;&#65292;&#21516;&#26102;&#37319;&#29992;&#33258;&#36866;&#24212;&#30340;tip&#36873;&#25321;&#31639;&#27861;&#38477;&#20302;&#36890;&#20449;&#21644;&#23384;&#20648;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.13158</link><description>&lt;p&gt;
DAG-ACFL&#65306;&#22522;&#20110;DAG-DLT&#30340;&#24322;&#27493;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DAG-ACFL: Asynchronous Clustered Federated Learning based on DAG-DLT. (arXiv:2308.13158v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13158
&lt;/p&gt;
&lt;p&gt;
DAG-ACFL&#26159;&#19968;&#31181;&#22522;&#20110;DAG-DLT&#30340;&#24322;&#27493;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#36873;&#25321;&#30456;&#20284;&#20998;&#24067;&#30340;&#23458;&#25143;&#31471;&#27169;&#22411;&#26469;&#32858;&#21512;&#20840;&#23616;&#27169;&#22411;&#65292;&#21516;&#26102;&#37319;&#29992;&#33258;&#36866;&#24212;&#30340;tip&#36873;&#25321;&#31639;&#27861;&#38477;&#20302;&#36890;&#20449;&#21644;&#23384;&#20648;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26088;&#22312;&#22312;&#20445;&#25252;&#23458;&#25143;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#21327;&#21516;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23458;&#25143;&#25968;&#25454;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65292;&#32852;&#37030;&#23398;&#20064;&#38754;&#20020;&#25361;&#25112;&#12290;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;(CFL)&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#24050;&#32463;&#20986;&#29616;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;CFL&#26694;&#26550;&#37117;&#37319;&#29992;&#21516;&#27493;&#26694;&#26550;&#32570;&#20047;&#24322;&#27493;&#24615;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#21521;&#26080;&#29615;&#22270;&#20998;&#24067;&#24335;&#36134;&#26412;&#25216;&#26415;(DAG-DLT)&#30340;&#24322;&#27493;CFL&#26694;&#26550;SDAGFL&#65292;&#20294;&#20854;&#23436;&#20840;&#21435;&#20013;&#24515;&#21270;&#23548;&#33268;&#20102;&#39640;&#36890;&#20449;&#21644;&#23384;&#20648;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DAG-ACFL&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;DAG-DLT&#30340;&#24322;&#27493;&#32858;&#31867;FL&#26694;&#26550;&#12290;&#39318;&#20808;&#35814;&#32454;&#20171;&#32461;&#20102;DAG-ACFL&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#21518;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#21442;&#25968;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#30340;tip&#36873;&#25321;&#31639;&#27861;&#65292;&#20197;&#32858;&#21512;&#20855;&#26377;&#30456;&#20284;&#20998;&#24067;&#30340;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#12290;&#37319;&#29992;&#33258;&#36866;&#24212;tip&#36873;&#25321;&#31639;&#27861;&#65292;&#21033;&#29992;&#21464;&#28857;&#26816;&#27979;&#21160;&#24577;&#30830;&#23450;&#25152;&#36873;tip&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;DAG-ACFL&#30340;&#24615;&#33021;&#24182;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) aims to collaboratively train a global model while ensuring client data privacy. However, FL faces challenges from the non-IID data distribution among clients. Clustered FL (CFL) has emerged as a promising solution, but most existing CFL frameworks adopt synchronous frameworks lacking asynchrony. An asynchronous CFL framework called SDAGFL based on directed acyclic graph distributed ledger techniques (DAG-DLT) was proposed, but its complete decentralization leads to high communication and storage costs. We propose DAG-ACFL, an asynchronous clustered FL framework based on directed acyclic graph distributed ledger techniques (DAG-DLT). We first detail the components of DAG-ACFL. A tip selection algorithm based on the cosine similarity of model parameters is then designed to aggregate models from clients with similar distributions. An adaptive tip selection algorithm leveraging change-point detection dynamically determines the number of selected tips. We evaluate t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#26679;&#21270;&#12289;&#21069;k&#20010;&#21644;&#39640;&#36136;&#37327;&#30340;&#35268;&#21010;&#27169;&#25311;&#22120;&#19978;&#30340;&#35268;&#21010;&#38382;&#39064;&#12290;&#36890;&#36807;&#20174;&#39044;&#29983;&#25104;&#30340;&#25628;&#32034;&#26641;&#20013;&#25552;&#21462;&#26377;&#30028;&#35268;&#21010;&#38598;&#65292;&#24182;&#35780;&#20272;&#36335;&#24452;&#30456;&#23545;&#36136;&#37327;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#35813;&#26041;&#27861;&#22312;&#21482;&#26377;&#40657;&#31665;&#27169;&#25311;&#27169;&#22411;&#30340;&#38382;&#39064;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#25552;&#20986;&#20102;&#20462;&#25913;MCTS&#31639;&#27861;&#20197;&#22686;&#21152;&#22810;&#26679;&#24615;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2308.13147</link><description>&lt;p&gt;
&#22810;&#26679;&#21270;&#12289;&#21069;k&#20010;&#21644;&#39640;&#36136;&#37327;&#30340;&#35268;&#21010;&#27169;&#25311;&#22120;&#19978;&#30340;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Diverse, Top-k, and Top-Quality Planning Over Simulators. (arXiv:2308.13147v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#26679;&#21270;&#12289;&#21069;k&#20010;&#21644;&#39640;&#36136;&#37327;&#30340;&#35268;&#21010;&#27169;&#25311;&#22120;&#19978;&#30340;&#35268;&#21010;&#38382;&#39064;&#12290;&#36890;&#36807;&#20174;&#39044;&#29983;&#25104;&#30340;&#25628;&#32034;&#26641;&#20013;&#25552;&#21462;&#26377;&#30028;&#35268;&#21010;&#38598;&#65292;&#24182;&#35780;&#20272;&#36335;&#24452;&#30456;&#23545;&#36136;&#37327;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#35813;&#26041;&#27861;&#22312;&#21482;&#26377;&#40657;&#31665;&#27169;&#25311;&#27169;&#22411;&#30340;&#38382;&#39064;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#25552;&#20986;&#20102;&#20462;&#25913;MCTS&#31639;&#27861;&#20197;&#22686;&#21152;&#22810;&#26679;&#24615;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26679;&#21270;&#12289;&#21069;k&#20010;&#21644;&#39640;&#36136;&#37327;&#30340;&#35268;&#21010;&#20851;&#27880;&#30340;&#26159;&#29983;&#25104;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#30340;&#35299;&#38598;&#12290;&#20197;&#24448;&#36825;&#20010;&#39046;&#22495;&#26159;&#20256;&#32479;&#35268;&#21010;&#22120;&#30340;&#39046;&#22495;&#65292;&#38656;&#35201;&#19968;&#20010;&#38382;&#39064;&#23454;&#20363;&#30340;&#31526;&#21495;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#21482;&#26377;&#40657;&#31665;&#27169;&#25311;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20174;&#39044;&#29983;&#25104;&#30340;&#26368;&#20248;&#25628;&#32034;&#26641;&#20013;&#25552;&#21462;&#26377;&#30028;&#35268;&#21010;&#38598;&#30340;&#36807;&#31243;&#65292;&#20197;&#21450;&#35780;&#20272;&#36890;&#36807;&#25628;&#32034;&#26641;&#30340;&#36335;&#24452;&#30456;&#23545;&#36136;&#37327;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#38544;&#34255;&#20449;&#24687;&#30340;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#24182;&#24314;&#35758;&#20462;&#25913;MCTS&#31639;&#27861;&#20197;&#22686;&#21152;&#29983;&#25104;&#35268;&#21010;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20256;&#32479;&#35268;&#21010;&#22120;&#19981;&#36866;&#29992;&#30340;&#39046;&#22495;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#39640;&#36136;&#37327;&#30340;&#35268;&#21010;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diverse, top-k, and top-quality planning are concerned with the generation of sets of solutions to sequential decision problems. Previously this area has been the domain of classical planners that require a symbolic model of the problem instance. This paper proposes a novel alternative approach that uses Monte Carlo Tree Search (MCTS), enabling application to problems for which only a black-box simulation model is available. We present a procedure for extracting bounded sets of plans from pre-generated search trees in best-first order, and a metric for evaluating the relative quality of paths through a search tree. We demonstrate this approach on a path-planning problem with hidden information, and suggest adaptations to the MCTS algorithm to increase the diversity of generated plans. Our results show that our method can generate diverse and high-quality plan sets in domains where classical planners are not applicable.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;&#23384;&#22312;&#30340;&#38382;&#39064;&#20197;&#21450;&#24403;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.13142</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#32508;&#36848;&#65306;&#38382;&#39064;&#21450;&#20854;&#35299;&#20915;&#26041;&#26696;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey of Diffusion Based Image Generation Models: Issues and Their Solutions. (arXiv:2308.13142v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13142
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;&#23384;&#22312;&#30340;&#38382;&#39064;&#20197;&#21450;&#24403;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#22823;&#22411;&#27169;&#22411;&#30340;&#21457;&#23637;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#22312;ChatGPT&#30340;&#25104;&#21151;&#20043;&#21518;&#65292;&#24341;&#20837;&#20102;&#35768;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#12290;&#31867;&#20284;&#30340;&#36827;&#23637;&#20063;&#22312;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#65292;&#22914;&#35895;&#27468;&#30340;Imagen&#27169;&#22411;&#12289;OpenAI&#30340;DALL-E 2&#21644;&#31283;&#23450;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#29983;&#25104;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#31867;&#20284;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#38754;&#20020;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#31283;&#23450;&#30340;&#25193;&#25955;&#27169;&#22411;&#21644;&#20854;&#22522;&#26412;&#25968;&#23398;&#21407;&#29702;&#30340;&#24320;&#28304;&#21487;&#29992;&#24615;&#20351;&#23398;&#26415;&#30028;&#33021;&#22815;&#24191;&#27867;&#20998;&#26512;&#24403;&#21069;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#22522;&#20110;&#36825;&#20010;&#31283;&#23450;&#30340;&#25193;&#25955;&#26694;&#26550;&#36827;&#34892;&#25913;&#36827;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#30740;&#31350;&#19982;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30456;&#20851;&#30340;&#29616;&#26377;&#38382;&#39064;&#21644;&#24403;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been significant progress in the development of large models. Following the success of ChatGPT, numerous language models have been introduced, demonstrating remarkable performance. Similar advancements have also been observed in image generation models, such as Google's Imagen model, OpenAI's DALL-E 2, and stable diffusion models, which have exhibited impressive capabilities in generating images. However, similar to large language models, these models still encounter unresolved challenges. Fortunately, the availability of open-source stable diffusion models and their underlying mathematical principles has enabled the academic community to extensively analyze the performance of current image generation models and make improvements based on this stable diffusion framework. This survey aims to examine the existing issues and the current solutions pertaining to image generation models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#36951;&#20256;&#31639;&#27861;&#21644;&#29228;&#23665;&#31639;&#27861;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;CIFAR-100&#25968;&#25454;&#38598;&#19978;&#20248;&#21270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#36951;&#20256;&#31639;&#27861;&#36827;&#34892;&#20840;&#23616;&#25628;&#32034;&#21644;&#25506;&#32034;&#25628;&#32034;&#31354;&#38388;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#20351;&#29992;&#29228;&#23665;&#31639;&#27861;&#36827;&#34892;&#23616;&#37096;&#20248;&#21270;&#65292;&#24182;&#24341;&#20837;&#21464;&#24322;&#25805;&#20316;&#20197;&#36991;&#20813;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#30456;&#36739;&#20110;&#26631;&#20934;&#30340;&#36951;&#20256;&#31639;&#27861;&#21644;&#29228;&#23665;&#31639;&#27861;&#22312;&#24615;&#33021;&#19978;&#26377;&#25152;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.13099</link><description>&lt;p&gt;
&#28151;&#21512;&#36951;&#20256;&#31639;&#27861;&#21644;&#29228;&#23665;&#31639;&#27861;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Hybrid Genetic Algorithm and Hill Climbing Optimization for the Neural Network. (arXiv:2308.13099v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#36951;&#20256;&#31639;&#27861;&#21644;&#29228;&#23665;&#31639;&#27861;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;CIFAR-100&#25968;&#25454;&#38598;&#19978;&#20248;&#21270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#36951;&#20256;&#31639;&#27861;&#36827;&#34892;&#20840;&#23616;&#25628;&#32034;&#21644;&#25506;&#32034;&#25628;&#32034;&#31354;&#38388;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#20351;&#29992;&#29228;&#23665;&#31639;&#27861;&#36827;&#34892;&#23616;&#37096;&#20248;&#21270;&#65292;&#24182;&#24341;&#20837;&#21464;&#24322;&#25805;&#20316;&#20197;&#36991;&#20813;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#30456;&#36739;&#20110;&#26631;&#20934;&#30340;&#36951;&#20256;&#31639;&#27861;&#21644;&#29228;&#23665;&#31639;&#27861;&#22312;&#24615;&#33021;&#19978;&#26377;&#25152;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#27169;&#22411;&#65292;&#23558;&#36951;&#20256;&#31639;&#27861;&#21644;&#29228;&#23665;&#31639;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#22312;CIFAR-100&#25968;&#25454;&#38598;&#19978;&#20248;&#21270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#19968;&#20010;&#26579;&#33394;&#20307;&#31181;&#32676;&#26469;&#34920;&#31034;CNN&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#12290;&#36951;&#20256;&#31639;&#27861;&#29992;&#20110;&#36873;&#25321;&#21644;&#32321;&#27542;&#26368;&#36866;&#24212;&#30340;&#26579;&#33394;&#20307;&#29983;&#25104;&#26032;&#30340;&#21518;&#20195;&#12290;&#28982;&#21518;&#65292;&#29228;&#23665;&#31639;&#27861;&#34987;&#24212;&#29992;&#20110;&#21518;&#20195;&#19978;&#65292;&#36827;&#19968;&#27493;&#20248;&#21270;&#23427;&#20204;&#30340;&#36229;&#21442;&#25968;&#12290;&#24341;&#20837;&#21464;&#24322;&#25805;&#20316;&#20197;&#20351;&#31181;&#32676;&#22810;&#26679;&#21270;&#65292;&#24182;&#38450;&#27490;&#31639;&#27861;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#36951;&#20256;&#31639;&#27861;&#29992;&#20110;&#20840;&#23616;&#25628;&#32034;&#21644;&#25506;&#32034;&#25628;&#32034;&#31354;&#38388;&#65292;&#32780;&#29228;&#23665;&#31639;&#27861;&#29992;&#20110;&#23616;&#37096;&#20248;&#21270;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#30446;&#26631;&#20989;&#25968;&#26159;&#22312;CIFAR-100&#27979;&#35797;&#38598;&#19978;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#29575;&#12290;&#36890;&#36807;&#19982;&#26631;&#20934;&#30340;&#36951;&#20256;&#31639;&#27861;&#21644;&#29228;&#23665;&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#35780;&#20272;&#20102;&#28151;&#21512;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a hybrid model combining genetic algorithm and hill climbing algorithm for optimizing Convolutional Neural Networks (CNNs) on the CIFAR-100 dataset. The proposed model utilizes a population of chromosomes that represent the hyperparameters of the CNN model. The genetic algorithm is used for selecting and breeding the fittest chromosomes to generate new offspring. The hill climbing algorithm is then applied to the offspring to further optimize their hyperparameters. The mutation operation is introduced to diversify the population and to prevent the algorithm from getting stuck in local optima. The Genetic Algorithm is used for global search and exploration of the search space, while Hill Climbing is used for local optimization of promising solutions. The objective function is the accuracy of the trained neural network on the CIFAR-100 test set. The performance of the hybrid model is evaluated by comparing it with the standard genetic algorithm and hill-climbing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#33258;&#20027;Formula SAE&#36187;&#36710;&#30340;&#30740;&#31350;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#25104;&#21151;&#23398;&#20064;&#36187;&#36710;&#39550;&#39542;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#30495;&#23454;&#36187;&#36947;&#19978;&#30340;&#29289;&#29702;&#24179;&#21488;&#12290;&#35813;&#30740;&#31350;&#25552;&#20379;&#23545;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2308.13088</link><description>&lt;p&gt;
&#23545;&#19968;&#36742;&#33258;&#20027;Formula SAE&#36187;&#36710;&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25511;&#21046;&#36827;&#34892;&#31454;&#36187;
&lt;/p&gt;
&lt;p&gt;
Racing Towards Reinforcement Learning based control of an Autonomous Formula SAE Car. (arXiv:2308.13088v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#33258;&#20027;Formula SAE&#36187;&#36710;&#30340;&#30740;&#31350;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#25104;&#21151;&#23398;&#20064;&#36187;&#36710;&#39550;&#39542;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#30495;&#23454;&#36187;&#36947;&#19978;&#30340;&#29289;&#29702;&#24179;&#21488;&#12290;&#35813;&#30740;&#31350;&#25552;&#20379;&#23545;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#20027;&#23548;&#33322;&#30740;&#31350;&#30340;&#26085;&#30410;&#27969;&#34892;&#65292;Formula Student&#65288;FS&#65289;&#36187;&#20107;&#22312;&#20854;&#27604;&#36187;&#21015;&#34920;&#20013;&#24341;&#20837;&#20102;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;DV&#65289;&#31867;&#21035;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#36825;&#20123;&#27604;&#36187;&#20013;&#30340;&#33258;&#20027;FS&#36187;&#36710;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36827;&#34892;&#31471;&#21040;&#31471;&#25511;&#21046;&#30340;&#21021;&#27493;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312;&#31867;&#20284;&#20110;&#23454;&#38469;&#35774;&#35745;&#30340;&#36187;&#36947;&#19978;&#65292;&#36890;&#36807;&#22312;Turtlebot2&#24179;&#21488;&#19978;&#36827;&#34892;&#27169;&#25311;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#29992;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;RL&#31639;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#22320;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#23398;&#20064;&#36187;&#36710;&#39550;&#39542;&#65292;&#24182;&#23558;&#20854;&#36716;&#31227;&#21040;&#30495;&#23454;&#19990;&#30028;&#30340;&#36187;&#36947;&#19978;&#30340;&#29289;&#29702;&#24179;&#21488;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#38480;&#21046;&#36827;&#34892;&#20102;&#25506;&#35752;&#65292;&#24182;&#25552;&#20986;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#20840;&#23610;&#24230;&#33258;&#20027;FS&#36187;&#36710;&#30340;&#26410;&#26469;&#26041;&#21521;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rising popularity of autonomous navigation research, Formula Student (FS) events are introducing a Driverless Vehicle (DV) category to their event list. This paper presents the initial investigation into utilising Deep Reinforcement Learning (RL) for end-to-end control of an autonomous FS race car for these competitions. We train two state-of-the-art RL algorithms in simulation on tracks analogous to the full-scale design on a Turtlebot2 platform. The results demonstrate that our approach can successfully learn to race in simulation and then transfer to a real-world racetrack on the physical platform. Finally, we provide insights into the limitations of the presented approach and guidance into the future directions for applying RL toward full-scale autonomous FS racing.
&lt;/p&gt;</description></item><item><title>&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#20010;&#30740;&#31350;&#39046;&#22495;&#65292;&#20294;&#30446;&#21069;&#23384;&#22312;&#32452;&#32455;&#19981;&#22815;&#26377;&#24207;&#21644;&#35780;&#20272;&#21327;&#35758;&#26377;&#32570;&#38519;&#30340;&#38382;&#39064;&#12290;&#25991;&#31456;&#35780;&#20272;&#20102;&#35768;&#22810;&#26368;&#36817;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#25351;&#20986;&#20102;&#38024;&#23545;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#35780;&#20272;&#21327;&#35758;&#23384;&#22312;&#30340;&#38382;&#39064;&#21450;&#22914;&#20309;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.13068</link><description>&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;: &#28843;&#37239;&#31639;&#27861;&#21644;&#26377;&#32570;&#38519;&#30340;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multivariate Time Series Anomaly Detection: Fancy Algorithms and Flawed Evaluation Methodology. (arXiv:2308.13068v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13068
&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#20010;&#30740;&#31350;&#39046;&#22495;&#65292;&#20294;&#30446;&#21069;&#23384;&#22312;&#32452;&#32455;&#19981;&#22815;&#26377;&#24207;&#21644;&#35780;&#20272;&#21327;&#35758;&#26377;&#32570;&#38519;&#30340;&#38382;&#39064;&#12290;&#25991;&#31456;&#35780;&#20272;&#20102;&#35768;&#22810;&#26368;&#36817;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#25351;&#20986;&#20102;&#38024;&#23545;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#35780;&#20272;&#21327;&#35758;&#23384;&#22312;&#30340;&#38382;&#39064;&#21450;&#22914;&#20309;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MVTS&#65289;&#30340;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#36817;&#24180;&#26469;&#21560;&#24341;&#20102;&#24037;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#30340;&#22823;&#37327;&#30740;&#31350;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#25991;&#29486;&#30340;&#20180;&#32454;&#30740;&#31350;&#35753;&#25105;&#20204;&#24847;&#35782;&#21040;&#65306;1&#65289;&#35813;&#39046;&#22495;&#30340;&#31038;&#21306;&#27963;&#36291;&#65292;&#20294;&#24182;&#19981;&#20687;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31561;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#37027;&#26679;&#32452;&#32455;&#26377;&#24207;&#65307;2&#65289;&#22823;&#22810;&#25968;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20351;&#29992;&#19981;&#21512;&#36866;&#25110;&#23384;&#22312;&#26126;&#26174;&#32570;&#38519;&#30340;&#35780;&#20272;&#21327;&#35758;&#36827;&#34892;&#35780;&#20272;&#65292;&#32570;&#20047;&#31185;&#23398;&#22522;&#30784;&#12290;&#20854;&#20013;&#19968;&#20010;&#38750;&#24120;&#27969;&#34892;&#30340;&#21327;&#35758;&#65292;&#21363;&#25152;&#35859;&#30340; \pa &#21327;&#35758;&#65292;&#26159;&#22914;&#27492;&#26377;&#32570;&#38519;&#65292;&#20197;&#33267;&#20110;&#38543;&#26426;&#29468;&#27979;&#21487;&#20197;&#26174;&#31034;&#31995;&#32479;&#22320;&#20248;&#20110;&#36804;&#20170;&#20026;&#27490;&#24320;&#21457;&#30340;\emph{&#25152;&#26377;}&#31639;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26356;&#20581;&#22766;&#30340;&#21327;&#35758;&#23545;&#35768;&#22810;&#26368;&#36817;&#30340;&#31639;&#27861;&#36827;&#34892;&#22238;&#39038;&#21644;&#35780;&#20272;&#65292;&#24182;&#35752;&#35770;&#22312;MVTS&#24322;&#24120;&#26816;&#27979;&#30340;&#32972;&#26223;&#19979;&#65292;&#19968;&#20010;&#26412;&#26469;&#24456;&#22909;&#30340;&#21327;&#35758;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#20197;&#21450;&#22914;&#20309;&#20943;&#36731;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#34920;&#36798;&#20102;&#20851;&#20999;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate Time Series (MVTS) anomaly detection is a long-standing and challenging research topic that has attracted tremendous research effort from both industry and academia recently. However, a careful study of the literature makes us realize that 1) the community is active but not as organized as other sibling machine learning communities such as Computer Vision (CV) and Natural Language Processing (NLP), and 2) most proposed solutions are evaluated using either inappropriate or highly flawed protocols, with an apparent lack of scientific foundation. So flawed is one very popular protocol, the so-called \pa protocol, that a random guess can be shown to systematically outperform \emph{all} algorithms developed so far. In this paper, we review and evaluate many recent algorithms using more robust protocols and discuss how a normally good protocol may have weaknesses in the context of MVTS anomaly detection and how to mitigate them. We also share our concerns about benchmark dataset
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19981;&#33021;&#20855;&#22791;&#22240;&#26524;&#24615;&#65292;&#23427;&#20204;&#21482;&#26159;&#37325;&#22797;&#23884;&#20837;&#22312;&#25968;&#25454;&#20013;&#30340;&#22240;&#26524;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2308.13067</link><description>&lt;p&gt;
&#22240;&#26524;&#40550;&#40521;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#35848;&#35770;&#22240;&#26524;&#24615;&#65292;&#20294;&#23427;&#20204;&#24182;&#19981;&#20855;&#22791;&#22240;&#26524;&#24615;
&lt;/p&gt;
&lt;p&gt;
Causal Parrots: Large Language Models May Talk Causality But Are Not Causal. (arXiv:2308.13067v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13067
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19981;&#33021;&#20855;&#22791;&#22240;&#26524;&#24615;&#65292;&#23427;&#20204;&#21482;&#26159;&#37325;&#22797;&#23884;&#20837;&#22312;&#25968;&#25454;&#20013;&#30340;&#22240;&#26524;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#20154;&#35748;&#20026;&#35268;&#27169;&#26159;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#30340;&#20840;&#37096;&#25152;&#38656;&#65292;&#29978;&#33267;&#21487;&#20197;&#28085;&#30422;&#22240;&#26524;&#27169;&#22411;&#12290;&#25105;&#20204;&#26126;&#30830;&#25351;&#20986;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19981;&#33021;&#20855;&#22791;&#22240;&#26524;&#24615;&#65292;&#24182;&#35299;&#37322;&#20026;&#20160;&#20040;&#26377;&#26102;&#25105;&#20204;&#21487;&#33021;&#26377;&#36825;&#31181;&#24863;&#35273;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23450;&#20041;&#24182;&#20030;&#20363;&#20102;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#30340;&#23376;&#32676;&#65292;&#31216;&#20043;&#20026;&#20803;SCM&#65292;&#23427;&#22312;&#20854;&#21464;&#37327;&#20013;&#32534;&#30721;&#20851;&#20110;&#20854;&#20182;SCM&#30340;&#22240;&#26524;&#20107;&#23454;&#12290;&#25105;&#20204;&#29468;&#27979;&#65292;&#22312;LLM&#25104;&#21151;&#36827;&#34892;&#22240;&#26524;&#25512;&#29702;&#30340;&#24773;&#20917;&#19979;&#65292;&#32972;&#21518;&#21487;&#33021;&#23384;&#22312;&#19968;&#20010;&#30456;&#24212;&#30340;&#20803;SCM&#65292;&#22312;&#20854;&#25968;&#25454;&#20013;&#23637;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#20013;&#22240;&#26524;&#20107;&#23454;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#32780;LLM&#26368;&#32456;&#26159;&#22312;&#36825;&#20123;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#22914;&#26524;&#25105;&#20204;&#30340;&#20551;&#35774;&#25104;&#31435;&#65292;&#37027;&#20040;&#36825;&#23558;&#24847;&#21619;&#30528;LLM&#23601;&#20687;&#40550;&#40521;&#19968;&#26679;&#65292;&#23427;&#20204;&#21482;&#26159;&#37325;&#22797;&#23884;&#20837;&#22312;&#25968;&#25454;&#20013;&#30340;&#22240;&#26524;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#20998;&#26512;&#25552;&#20379;&#20102;&#25903;&#25345;&#35777;&#25454;&#65292;&#34920;&#26126;&#24403;&#21069;&#30340;LLM&#29978;&#33267;&#26159;&#24369;&#8220;&#22240;&#26524;&#40550;&#40521;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Some argue scale is all what is needed to achieve AI, covering even causal models. We make it clear that large language models (LLMs) cannot be causal and give reason onto why sometimes we might feel otherwise. To this end, we define and exemplify a new subgroup of Structural Causal Model (SCM) that we call meta SCM which encode causal facts about other SCM within their variables. We conjecture that in the cases where LLM succeed in doing causal inference, underlying was a respective meta SCM that exposed correlations between causal facts in natural language on whose data the LLM was ultimately trained. If our hypothesis holds true, then this would imply that LLMs are like parrots in that they simply recite the causal knowledge embedded in the data. Our empirical analysis provides favoring evidence that current LLMs are even weak `causal parrots.'
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#31181;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#26469;&#29983;&#25104;&#25991;&#26723;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22270;&#20070;&#25512;&#33616;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#27604;&#21333;&#19968;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.13050</link><description>&lt;p&gt;
&#22810;&#37325;BERT&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Multi-BERT for Embeddings for Recommendation System. (arXiv:2308.13050v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#31181;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#26469;&#29983;&#25104;&#25991;&#26723;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22270;&#20070;&#25512;&#33616;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#27604;&#21333;&#19968;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21477;&#23376;BERT&#65288;SBERT&#65289;&#21644;RoBERTa&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#26469;&#29983;&#25104;&#25991;&#26723;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#21477;&#23376;&#35270;&#20026;&#26631;&#35760;&#65292;&#24182;&#20026;&#23427;&#20204;&#29983;&#25104;&#23884;&#20837;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#25991;&#26723;&#20869;&#30340;&#21477;&#23376;&#20869;&#37096;&#21644;&#21477;&#23376;&#38388;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#22270;&#20070;&#25512;&#33616;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#29983;&#25104;&#35821;&#20041;&#20016;&#23500;&#21644;&#20934;&#30830;&#30340;&#25991;&#26723;&#23884;&#20837;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#22312;Goodreads&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22270;&#20070;&#25512;&#33616;&#20219;&#21153;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#23558;&#20351;&#29992;&#25105;&#20204;&#30340;MULTI-BERT&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26723;&#23884;&#20837;&#19982;&#20165;&#20351;&#29992;SBERT&#29983;&#25104;&#30340;&#23884;&#20837;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#20351;&#29992;&#31934;&#30830;&#24230;&#20316;&#20026;&#35780;&#20272;&#25351;&#26631;&#26469;&#27604;&#36739;&#29983;&#25104;&#23884;&#20837;&#30340;&#36136;&#37327;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#29983;&#25104;&#23884;&#20837;&#30340;&#36136;&#37327;&#26041;&#38754;&#22987;&#32456;&#20248;&#20110;SBERT&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel approach for generating document embeddings using a combination of Sentence-BERT (SBERT) and RoBERTa, two state-of-the-art natural language processing models. Our approach treats sentences as tokens and generates embeddings for them, allowing the model to capture both intra-sentence and inter-sentence relations within a document. We evaluate our model on a book recommendation task and demonstrate its effectiveness in generating more semantically rich and accurate document embeddings. To assess the performance of our approach, we conducted experiments on a book recommendation task using the Goodreads dataset. We compared the document embeddings generated using our MULTI-BERT model to those generated using SBERT alone. We used precision as our evaluation metric to compare the quality of the generated embeddings. Our results showed that our model consistently outperformed SBERT in terms of the quality of the generated embeddings. Furthermore, we found tha
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#20998;&#24067;&#24335;&#21644;&#19981;&#23436;&#25972;&#30340;&#25968;&#25454;&#28304;&#20013;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#65292;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#24182;&#35299;&#20915;&#22240;&#20026;&#32570;&#22833;&#20540;&#24341;&#20837;&#30340;&#20559;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.13047</link><description>&lt;p&gt;
&#19981;&#23436;&#25972;&#35266;&#27979;&#25968;&#25454;&#30340;&#32852;&#37030;&#22240;&#26524;&#25928;&#24212;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning of Causal Effects from Incomplete Observational Data. (arXiv:2308.13047v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13047
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#20998;&#24067;&#24335;&#21644;&#19981;&#23436;&#25972;&#30340;&#25968;&#25454;&#28304;&#20013;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#65292;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#24182;&#35299;&#20915;&#22240;&#20026;&#32570;&#22833;&#20540;&#24341;&#20837;&#30340;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#21644;&#19981;&#23436;&#25972;&#30340;&#25968;&#25454;&#28304;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24456;&#24120;&#35265;&#65292;&#23545;&#22240;&#26524;&#25512;&#26029;&#25552;&#20986;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#30001;&#20110;&#38544;&#31169;&#38480;&#21046;&#65292;&#36825;&#20123;&#25968;&#25454;&#28304;&#26080;&#27861;&#21512;&#24182;&#20026;&#19968;&#20010;&#23454;&#20307;&#65292;&#32780;&#20854;&#20013;&#30340;&#32570;&#22833;&#20540;&#21487;&#33021;&#20250;&#24341;&#20837;&#20559;&#24046;&#21040;&#22240;&#26524;&#20272;&#35745;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#20998;&#24067;&#24335;&#21644;&#19981;&#23436;&#25972;&#30340;&#25968;&#25454;&#28304;&#20013;&#36827;&#34892;&#32852;&#37030;&#22240;&#26524;&#25512;&#26029;&#65292;&#20174;&#32780;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#25439;&#22833;&#20989;&#25968;&#25286;&#20998;&#20026;&#22810;&#20010;&#32452;&#20214;&#65292;&#27599;&#20010;&#32452;&#20214;&#23545;&#24212;&#20110;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#29305;&#23450;&#25968;&#25454;&#28304;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32570;&#22833;&#38543;&#26426;&#20551;&#35774;&#19979;&#32771;&#34385;&#20102;&#32570;&#22833;&#25968;&#25454;&#65292;&#24182;&#20272;&#35745;&#20102;&#22240;&#26524;&#20272;&#35745;&#30340;&#39640;&#38454;&#32479;&#35745;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#20998;&#25955;&#30340;&#25968;&#25454;&#28304;&#20013;&#24674;&#22797;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#21464;&#37327;&#30340;&#26465;&#20214;&#20998;&#24067;&#65292;&#20197;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20272;&#35745;&#20102;&#24322;&#36136;&#30340;&#26465;&#20214;&#20998;&#24067;&#20197;&#24212;&#23545;&#19981;&#23436;&#25972;&#30340;&#25968;&#25454;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decentralized and incomplete data sources are prevalent in real-world applications, posing a formidable challenge for causal inference. These sources cannot be consolidated into a single entity owing to privacy constraints, and the presence of missing values within them can potentially introduce bias to the causal estimands. We introduce a new approach for federated causal inference from incomplete data, enabling the estimation of causal effects from multiple decentralized and incomplete data sources. Our approach disentangles the loss function into multiple components, each corresponding to a specific data source with missing values. Our approach accounts for the missing data under the missing at random assumption, while also estimating higher-order statistics of the causal estimands. Our method recovers the conditional distribution of missing confounders given the observed confounders from the decentralized data sources to identify causal effects. Our framework estimates heterogeneou
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;Llama 2&#27169;&#22411;&#23454;&#29616;&#20102;&#37329;&#34701;&#26032;&#38395;&#30340;&#22810;&#20219;&#21153;&#20998;&#26512;&#65292;&#21253;&#25324;&#25991;&#26412;&#20998;&#26512;&#12289;&#25688;&#35201;&#21644;&#24773;&#24863;&#25552;&#21462;&#31561;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25552;&#21462;&#30340;&#21629;&#21517;&#23454;&#20307;&#24773;&#24863;&#21487;&#20197;&#20316;&#20026;&#26377;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2308.13032</link><description>&lt;p&gt;
&#20351;&#29992;&#31934;&#32454;&#35843;&#25972;&#30340;Llama 2 GPT&#27169;&#22411;&#36827;&#34892;&#37329;&#34701;&#26032;&#38395;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Financial News Analytics Using Fine-Tuned Llama 2 GPT Model. (arXiv:2308.13032v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;Llama 2&#27169;&#22411;&#23454;&#29616;&#20102;&#37329;&#34701;&#26032;&#38395;&#30340;&#22810;&#20219;&#21153;&#20998;&#26512;&#65292;&#21253;&#25324;&#25991;&#26412;&#20998;&#26512;&#12289;&#25688;&#35201;&#21644;&#24773;&#24863;&#25552;&#21462;&#31561;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25552;&#21462;&#30340;&#21629;&#21517;&#23454;&#20307;&#24773;&#24863;&#21487;&#20197;&#20316;&#20026;&#26377;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20351;&#29992;&#31934;&#32454;&#35843;&#25972;&#30340;Llama 2 Large Language Model (LLM) &#23545;&#37329;&#34701;&#26032;&#38395;&#36827;&#34892;&#22810;&#20219;&#21153;&#20998;&#26512;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;PEFT/LoRA&#26041;&#27861;&#23545;&#27169;&#22411;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#65292;&#20027;&#35201;&#21253;&#25324;&#20174;&#37329;&#34701;&#24066;&#22330;&#35282;&#24230;&#20998;&#26512;&#25991;&#26412;&#12289;&#31361;&#20986;&#25991;&#26412;&#30340;&#20027;&#35201;&#35266;&#28857;&#12289;&#23545;&#25991;&#26412;&#36827;&#34892;&#25688;&#35201;&#21644;&#25552;&#21462;&#20855;&#26377;&#36866;&#24403;&#24773;&#24863;&#30340;&#21629;&#21517;&#23454;&#20307;&#31561;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;Llama 2&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#22810;&#20219;&#21153;&#30340;&#37329;&#34701;&#26032;&#38395;&#20998;&#26512;&#65292;&#20854;&#21709;&#24212;&#30340;&#32467;&#26500;&#21487;&#20197;&#37096;&#20998;&#20026;&#32467;&#26500;&#21270;&#25991;&#26412;&#65292;&#21478;&#19968;&#37096;&#20998;&#25968;&#25454;&#21487;&#20197;&#37319;&#29992;JSON&#26684;&#24335;&#36827;&#19968;&#27493;&#22788;&#29702;&#12290;&#25552;&#21462;&#30340;&#21629;&#21517;&#23454;&#20307;&#24773;&#24863;&#21487;&#20197;&#34987;&#35270;&#20026;&#20855;&#26377;&#23450;&#37327;&#30446;&#26631;&#21464;&#37327;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper considers the possibility to fine-tune Llama 2 Large Language Model (LLM) for the multitask analysis of financial news. For fine-tuning, the PEFT/LoRA based approach was used. In the study, the model was fine-tuned for the following tasks: analysing a text from financial market perspectives, highlighting main points of a text, summarizing a text and extracting named entities with appropriate sentiments. The obtained results show that the fine-tuned Llama 2 model can perform a multitask financial news analysis with a specified structure of response, part of response can be a structured text and another part of data can have JSON format for further processing. Extracted sentiments for named entities can be considered as predictive features in supervised machine learning models with quantitative target variables.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#38646;&#26679;&#26412;&#35828;&#35805;&#32773;&#33258;&#36866;&#24212;&#35821;&#38899;&#21512;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#35299;&#32806;&#34920;&#31034;&#23398;&#20064;&#26469;&#25913;&#21892;&#27169;&#22411;&#22312;&#26410;&#30693;&#35828;&#35805;&#32773;&#26041;&#38754;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.13007</link><description>&lt;p&gt;
&#20855;&#26377;&#35299;&#32806;&#34920;&#31034;&#30340;&#36890;&#29992;&#38646;&#26679;&#26412;&#35828;&#35805;&#32773;&#33258;&#36866;&#24212;&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Generalizable Zero-Shot Speaker Adaptive Speech Synthesis with Disentangled Representations. (arXiv:2308.13007v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#38646;&#26679;&#26412;&#35828;&#35805;&#32773;&#33258;&#36866;&#24212;&#35821;&#38899;&#21512;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#35299;&#32806;&#34920;&#31034;&#23398;&#20064;&#26469;&#25913;&#21892;&#27169;&#22411;&#22312;&#26410;&#30693;&#35828;&#35805;&#32773;&#26041;&#38754;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22810;&#25968;&#20851;&#20110;&#35821;&#38899;&#21512;&#25104;&#30340;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#20026;&#25968;&#25454;&#38598;&#20869;&#30340;&#35828;&#35805;&#32773;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#19978;&#65292;&#20294;&#21516;&#26679;&#37325;&#35201;&#19988;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#26159;&#20026;&#27809;&#26377;&#21442;&#32771;&#25968;&#25454;&#30340;&#25968;&#25454;&#38598;&#22806;&#30340;&#26410;&#30693;&#35828;&#35805;&#32773;&#21512;&#25104;&#35821;&#38899;&#65292;&#21363;&#35828;&#35805;&#32773;&#33258;&#36866;&#24212;&#35821;&#38899;&#21512;&#25104;&#12290;&#35768;&#22810;&#30740;&#31350;&#25552;&#20986;&#20102;&#38024;&#23545;&#27492;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#35828;&#35805;&#32773;&#33258;&#36866;&#24212;&#25991;&#26412;&#21040;&#35821;&#38899;&#21644;&#22768;&#38899;&#36716;&#25442;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#22411;&#22312;&#20998;&#24067;&#22806;&#25968;&#25454;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#24046;&#65292;&#24403;&#21069;&#22823;&#22810;&#25968;&#26041;&#27861;&#22312;&#20026;&#26410;&#30693;&#35828;&#35805;&#32773;&#65288;&#21363;&#19981;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#35828;&#35805;&#32773;&#65289;&#21512;&#25104;&#35821;&#38899;&#26102;&#65292;&#20250;&#23548;&#33268;&#33258;&#28982;&#24230;&#21644;&#35828;&#35805;&#32773;&#30456;&#20284;&#24615;&#30340;&#38477;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GZS-TV&#65292;&#19968;&#31181;&#36890;&#29992;&#30340;&#38646;&#26679;&#26412;&#35828;&#35805;&#32773;&#33258;&#36866;&#24212;&#25991;&#26412;&#21040;&#35821;&#38899;&#21644;&#22768;&#38899;&#36716;&#25442;&#27169;&#22411;&#12290;GZS-TV&#24341;&#20837;&#20102;&#35299;&#32806;&#34920;&#31034;&#23398;&#20064;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#21033;&#29992;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#35828;&#35805;&#32773;&#23884;&#20837;&#25552;&#21462;&#21644;&#38899;&#33394;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
While most research into speech synthesis has focused on synthesizing high-quality speech for in-dataset speakers, an equally essential yet unsolved problem is synthesizing speech for unseen speakers who are out-of-dataset with limited reference data, i.e., speaker adaptive speech synthesis. Many studies have proposed zero-shot speaker adaptive text-to-speech and voice conversion approaches aimed at this task. However, most current approaches suffer from the degradation of naturalness and speaker similarity when synthesizing speech for unseen speakers (i.e., speakers not in the training dataset) due to the poor generalizability of the model in out-of-distribution data. To address this problem, we propose GZS-TV, a generalizable zero-shot speaker adaptive text-to-speech and voice conversion model. GZS-TV introduces disentangled representation learning for both speaker embedding extraction and timbre transformation to improve model generalization and leverages the representation learning
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;360&#24230;&#35270;&#39057;&#26174;&#33879;&#24615;&#39044;&#27979;&#30340;&#29699;&#24418;&#35270;&#35273;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#20999;&#32447;&#22270;&#20687;&#34920;&#31034;&#21644;&#29699;&#24418;&#20960;&#20309;&#24863;&#30693;&#30340;&#26426;&#21046;&#65292;&#20197;&#21450;&#19968;&#31181;&#26080;&#30417;&#30563;&#27491;&#21017;&#21270;&#39033;&#26469;&#20943;&#23569;&#20266;&#20687;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#20840;&#26223;&#35270;&#39057;&#29702;&#35299;&#21644;&#26174;&#33879;&#24615;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.13004</link><description>&lt;p&gt;
&#29992;&#20110;360&#24230;&#35270;&#39057;&#26174;&#33879;&#24615;&#39044;&#27979;&#30340;&#29699;&#24418;&#35270;&#35273;Transformer
&lt;/p&gt;
&lt;p&gt;
Spherical Vision Transformer for 360-degree Video Saliency Prediction. (arXiv:2308.13004v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;360&#24230;&#35270;&#39057;&#26174;&#33879;&#24615;&#39044;&#27979;&#30340;&#29699;&#24418;&#35270;&#35273;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#20999;&#32447;&#22270;&#20687;&#34920;&#31034;&#21644;&#29699;&#24418;&#20960;&#20309;&#24863;&#30693;&#30340;&#26426;&#21046;&#65292;&#20197;&#21450;&#19968;&#31181;&#26080;&#30417;&#30563;&#27491;&#21017;&#21270;&#39033;&#26469;&#20943;&#23569;&#20266;&#20687;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#20840;&#26223;&#35270;&#39057;&#29702;&#35299;&#21644;&#26174;&#33879;&#24615;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20840;&#26223;&#35270;&#39057;&#65288;ODVs&#65289;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#65292;&#35813;&#35270;&#39057;&#25429;&#25417;&#20102;&#25972;&#20010;&#35270;&#37326;&#65288;FOV&#65289;&#65292;&#22240;&#27492;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#65292;360&#24230;&#26174;&#33879;&#24615;&#39044;&#27979;&#21464;&#24471;&#37325;&#35201;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#39044;&#27979;&#20154;&#31867;&#22312;360&#24230;&#22330;&#26223;&#20013;&#30475;&#21521;&#20309;&#22788;&#38754;&#20020;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#29699;&#24418;&#22833;&#30495;&#12289;&#39640;&#20998;&#36776;&#29575;&#21644;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SalViT360&#30340;&#22522;&#20110;&#35270;&#35273;Transformer&#30340;&#20840;&#26223;&#35270;&#39057;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#20999;&#32447;&#22270;&#20687;&#34920;&#31034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29699;&#24418;&#20960;&#20309;&#24863;&#30693;&#30340;&#26102;&#31354;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#33021;&#22815;&#26377;&#25928;&#29702;&#35299;&#20840;&#26223;&#35270;&#39057;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#33268;&#24615;&#30340;&#26080;&#30417;&#30563;&#27491;&#21017;&#21270;&#39033;&#65292;&#29992;&#20110;&#25237;&#24433;-based&#30340;360&#24230;&#23494;&#38598;&#39044;&#27979;&#27169;&#22411;&#65292;&#20197;&#20943;&#23569;&#25237;&#24433;&#21453;&#28436;&#21518;&#39044;&#27979;&#20013;&#20986;&#29616;&#30340;&#20266;&#20687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#31532;&#19968;&#20010;&#37319;&#29992;&#20999;&#32447;&#22270;&#20687;&#36827;&#34892;&#20840;&#26223;&#26174;&#33879;&#24615;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;&#20840;&#26223;&#35270;&#39057;&#26174;&#33879;&#24615;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#20854;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing interest in omnidirectional videos (ODVs) that capture the full field-of-view (FOV) has gained 360-degree saliency prediction importance in computer vision. However, predicting where humans look in 360-degree scenes presents unique challenges, including spherical distortion, high resolution, and limited labelled data. We propose a novel vision-transformer-based model for omnidirectional videos named SalViT360 that leverages tangent image representations. We introduce a spherical geometry-aware spatiotemporal self-attention mechanism that is capable of effective omnidirectional video understanding. Furthermore, we present a consistency-based unsupervised regularization term for projection-based 360-degree dense-prediction models to reduce artefacts in the predictions that occur after inverse projection. Our approach is the first to employ tangent images for omnidirectional saliency prediction, and our experimental results on three ODV saliency datasets demonstrate its effect
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#21322;&#27169;&#22411;&#20381;&#36182;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#26469;&#23454;&#29616;&#20855;&#26377;&#24322;&#26500;&#35686;&#25106;&#20449;&#21495;&#34892;&#20026;&#30340;&#21608;&#30028;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2308.12985</link><description>&lt;p&gt;
&#24322;&#26500;&#35686;&#25106;&#20449;&#21495;&#34892;&#20026;&#19979;&#30340;&#21608;&#30028;&#25511;&#21046;&#65306;&#22522;&#20110;&#21322;&#27169;&#22411;&#20381;&#36182;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Perimeter Control with Heterogeneous Cordon Signal Behaviors: A Semi-Model Dependent Reinforcement Learning Approach. (arXiv:2308.12985v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#21322;&#27169;&#22411;&#20381;&#36182;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#26469;&#23454;&#29616;&#20855;&#26377;&#24322;&#26500;&#35686;&#25106;&#20449;&#21495;&#34892;&#20026;&#30340;&#21608;&#30028;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21608;&#30028;&#25511;&#21046;&#65288;PC&#65289;&#31574;&#30053;&#26088;&#22312;&#36890;&#36807;&#30417;&#27979;&#21463;&#20445;&#25252;&#32593;&#32476;&#65288;PN&#65289;&#30340;&#36716;&#31227;&#27969;&#37327;&#26469;&#35299;&#20915;&#22478;&#24066;&#36947;&#36335;&#32593;&#32476;&#22312;&#36807;&#39281;&#21644;&#24773;&#20917;&#19979;&#30340;&#25511;&#21046;&#38382;&#39064;&#12290;&#29616;&#26377;&#30740;&#31350;&#20013;&#23545;&#35686;&#25106;&#20449;&#21495;&#30340;&#22343;&#21248;&#27979;&#37327;&#29575;&#24573;&#35270;&#20102;&#20132;&#21449;&#21475;&#32423;&#21035;&#30340;&#20132;&#36890;&#29366;&#24577;&#30340;&#22810;&#26679;&#24615;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#30340;&#23616;&#37096;&#20132;&#36890;&#25317;&#22581;&#21644;&#30772;&#22351;&#32593;&#32476;&#31283;&#23450;&#24615;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#21322;&#27169;&#22411;&#20381;&#36182;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#26469;&#23454;&#29616;&#20855;&#26377;&#24322;&#26500;&#35686;&#25106;&#20449;&#21495;&#34892;&#20026;&#30340;&#21608;&#30028;&#25511;&#21046;&#12290;&#25152;&#25552;&#20986;&#30340;&#31574;&#30053;&#23558;&#22522;&#20110;MARL&#30340;&#20449;&#21495;&#25511;&#21046;&#26041;&#27861;&#19982;&#38598;&#20013;&#24335;&#21453;&#39304;PC&#31574;&#30053;&#30456;&#32467;&#21512;&#65292;&#24182;&#24212;&#29992;&#20110;PN&#30340;&#35686;&#25106;&#20449;&#21495;&#12290;&#23427;&#26159;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#31995;&#32479;&#65292;&#21453;&#39304;PC&#31574;&#30053;&#26816;&#27979;PN&#30340;&#25972;&#20307;&#20132;&#36890;&#29366;&#24577;&#65292;&#28982;&#21518;&#23558;&#26412;&#22320;&#25351;&#20196;&#20998;&#21457;&#32473;&#30001;MARL&#26694;&#26550;&#20013;&#30340;&#26234;&#33021;&#20307;&#25511;&#21046;&#30340;&#35686;&#25106;&#20449;&#21495;&#12290;&#27599;&#20010;&#35686;&#25106;&#20449;&#21495;&#37117;&#29420;&#31435;&#32780;&#19981;&#21516;&#65292;&#21019;&#24314;&#20102;&#24377;&#24615;&#21644;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Perimeter Control (PC) strategies have been proposed to address urban road network control in oversaturated situations by monitoring transfer flows of the Protected Network (PN). The uniform metering rate for cordon signals in existing studies ignores the variety of local traffic states at the intersection level, which may cause severe local traffic congestion and ruin the network stability. This paper introduces a semi-model dependent Multi-Agent Reinforcement Learning (MARL) framework to conduct PC with heterogeneous cordon signal behaviors. The proposed strategy integrates the MARL-based signal control method with centralized feedback PC policy and is applied to cordon signals of the PN. It operates as a two-stage system, with the feedback PC strategy detecting the overall traffic state within the PN and then distributing local instructions to cordon signals controlled by agents in the MARL framework. Each cordon signal acts independently and differently, creating a slack and distri
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;AI&#38899;&#20048;&#29983;&#25104;&#24037;&#20855;&#32508;&#36848;&#65292;&#20998;&#31867;&#20102;&#22522;&#20110;&#21442;&#25968;&#12289;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#38899;&#20048;&#29983;&#25104;&#26041;&#27861;&#65292;&#24182;&#35814;&#32454;&#20171;&#32461;&#20102;&#36825;&#20123;&#24037;&#20855;&#30340;&#20248;&#28857;&#12289;&#23616;&#38480;&#24615;&#20197;&#21450;&#36873;&#25321;&#36807;&#31243;&#20013;&#38656;&#35201;&#32771;&#34385;&#30340;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2308.12982</link><description>&lt;p&gt;
AI&#38899;&#20048;&#29983;&#25104;&#24037;&#20855;&#21644;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of AI Music Generation Tools and Models. (arXiv:2308.12982v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12982
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;AI&#38899;&#20048;&#29983;&#25104;&#24037;&#20855;&#32508;&#36848;&#65292;&#20998;&#31867;&#20102;&#22522;&#20110;&#21442;&#25968;&#12289;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#38899;&#20048;&#29983;&#25104;&#26041;&#27861;&#65292;&#24182;&#35814;&#32454;&#20171;&#32461;&#20102;&#36825;&#20123;&#24037;&#20855;&#30340;&#20248;&#28857;&#12289;&#23616;&#38480;&#24615;&#20197;&#21450;&#36873;&#25321;&#36807;&#31243;&#20013;&#38656;&#35201;&#32771;&#34385;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;AI&#38899;&#20048;&#29983;&#25104;&#24037;&#20855;&#30340;&#32508;&#36848;&#65292;&#21253;&#25324;&#30740;&#31350;&#39033;&#30446;&#21644;&#21830;&#19994;&#24212;&#29992;&#12290;&#20026;&#20102;&#36827;&#34892;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#23558;&#38899;&#20048;&#29983;&#25104;&#26041;&#27861;&#20998;&#20026;&#19977;&#31867;&#65306;&#22522;&#20110;&#21442;&#25968;&#12289;&#22522;&#20110;&#25991;&#26412;&#21644;&#22522;&#20110;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#31361;&#20986;&#20102;&#36825;&#20123;&#24037;&#20855;&#30340;&#22810;&#26679;&#24615;&#21644;&#21151;&#33021;&#29305;&#28857;&#65292;&#36866;&#29992;&#20110;&#20174;&#26222;&#36890;&#21548;&#20247;&#21040;&#19987;&#19994;&#38899;&#20048;&#20154;&#30340;&#21508;&#31181;&#29992;&#25143;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#27599;&#20010;&#24037;&#20855;&#37117;&#26377;&#33258;&#24049;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#21015;&#34920;&#65292;&#21015;&#20986;&#20102;&#22312;&#36873;&#25321;&#24037;&#20855;&#36807;&#31243;&#20013;&#24212;&#32771;&#34385;&#30340;&#36825;&#20123;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32508;&#36848;&#25552;&#20379;&#20102;&#20851;&#20110;AI&#38899;&#20048;&#29983;&#25104;&#30340;&#24213;&#23618;&#26426;&#21046;&#21644;&#25361;&#25112;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we provide a comprehensive survey of AI music generation tools, including both research projects and commercialized applications. To conduct our analysis, we classified music generation approaches into three categories: parameter-based, text-based, and visual-based classes. Our survey highlights the diverse possibilities and functional features of these tools, which cater to a wide range of users, from regular listeners to professional musicians. We observed that each tool has its own set of advantages and limitations. As a result, we have compiled a comprehensive list of these factors that should be considered during the tool selection process. Moreover, our survey offers critical insights into the underlying mechanisms and challenges of AI music generation.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;&#24320;&#25918;&#30740;&#31350;&#30693;&#35782;&#22270;&#35889;&#20316;&#20026;&#35745;&#31639;&#26426;&#36741;&#21161;&#24037;&#20855;&#65292;&#20174;&#31185;&#23398;&#35770;&#25991;&#20013;&#32452;&#32455;&#20851;&#38190;&#27934;&#23519;&#65292;&#20197;&#21152;&#36895;&#30693;&#35782;&#33719;&#21462;&#30340;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2308.12981</link><description>&lt;p&gt;
&#22522;&#20110;&#24320;&#25918;&#30740;&#31350;&#30693;&#35782;&#22270;&#35889;&#30340;&#31185;&#23398;&#35770;&#25991;&#30693;&#35782;&#33719;&#21462;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An approach based on Open Research Knowledge Graph for Knowledge Acquisition from scientific papers. (arXiv:2308.12981v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12981
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;&#24320;&#25918;&#30740;&#31350;&#30693;&#35782;&#22270;&#35889;&#20316;&#20026;&#35745;&#31639;&#26426;&#36741;&#21161;&#24037;&#20855;&#65292;&#20174;&#31185;&#23398;&#35770;&#25991;&#20013;&#32452;&#32455;&#20851;&#38190;&#27934;&#23519;&#65292;&#20197;&#21152;&#36895;&#30693;&#35782;&#33719;&#21462;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#35770;&#25991;&#21487;&#20197;&#20998;&#20026;&#20004;&#20010;&#20027;&#35201;&#37096;&#20998;&#65292;&#21363;&#20803;&#25968;&#25454;&#21644;&#20840;&#25991;&#12290;&#20803;&#25968;&#25454;&#25552;&#20379;&#20102;&#35770;&#25991;&#30340;&#31616;&#35201;&#27010;&#36848;&#65292;&#32780;&#20840;&#25991;&#21253;&#21547;&#20102;&#23545;&#20854;&#20182;&#30740;&#31350;&#20154;&#21592;&#26377;&#20215;&#20540;&#30340;&#20851;&#38190;&#27934;&#23519;&#12290;&#20026;&#20102;&#20174;&#31185;&#23398;&#35770;&#25991;&#20013;&#33719;&#21462;&#20803;&#25968;&#25454;&#21644;&#20851;&#38190;&#27934;&#23519;&#65292;&#30693;&#35782;&#33719;&#21462;&#26159;&#19968;&#39033;&#26680;&#24515;&#27963;&#21160;&#12290;&#23427;&#21253;&#25324;&#25910;&#38598;&#12289;&#20998;&#26512;&#21644;&#32452;&#32455;&#23884;&#20837;&#22312;&#31185;&#23398;&#35770;&#25991;&#20013;&#30340;&#30693;&#35782;&#65292;&#20197;&#20415;&#22312;&#38656;&#35201;&#26102;&#21487;&#20197;&#20351;&#29992;&#21644;&#37325;&#22797;&#20351;&#29992;&#12290;&#37492;&#20110;&#20016;&#23500;&#30340;&#31185;&#23398;&#25991;&#29486;&#65292;&#25163;&#21160;&#30693;&#35782;&#33719;&#21462;&#26159;&#19968;&#39033;&#32321;&#29712;&#30340;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#36890;&#24120;&#37319;&#29992;&#35745;&#31639;&#26426;&#36741;&#21161;&#21644;&#65288;&#21322;&#65289;&#33258;&#21160;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#26412;&#30740;&#31350;&#20013;&#30340;&#30446;&#30340;&#26159;&#65306;&#21033;&#29992;&#19982;&#26412;&#20307;&#23398;&#20064;&#30456;&#20851;&#30340;&#35770;&#25991;&#31574;&#21010;&#24320;&#25918;&#30740;&#31350;&#30693;&#35782;&#22270;&#35889;&#65288;ORKG&#65289;&#65292;&#24182;&#20351;&#29992;ORKG&#20316;&#20026;&#35745;&#31639;&#26426;&#36741;&#21161;&#24037;&#20855;&#65292;&#32452;&#32455;&#20174;&#30740;&#31350;&#35770;&#25991;&#20013;&#25552;&#21462;&#30340;&#20851;&#38190;&#27934;&#23519;&#12290;&#35813;&#26041;&#27861;&#34987;&#29992;&#20110;&#35760;&#24405;&#8220;&#27969;&#34892;&#30149;&#23398;&#30417;&#27979;&#31995;&#32479;&#35774;&#35745;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
A scientific paper can be divided into two major constructs which are Metadata and Full-body text. Metadata provides a brief overview of the paper while the Full-body text contains key-insights that can be valuable to fellow researchers. To retrieve metadata and key-insights from scientific papers, knowledge acquisition is a central activity. It consists of gathering, analyzing and organizing knowledge embedded in scientific papers in such a way that it can be used and reused whenever needed. Given the wealth of scientific literature, manual knowledge acquisition is a cumbersome task. Thus, computer-assisted and (semi-)automatic strategies are generally adopted. Our purpose in this research was two fold: curate Open Research Knowledge Graph (ORKG) with papers related to ontology learning and define an approach using ORKG as a computer-assisted tool to organize key-insights extracted from research papers. This approach was used to document the "epidemiological surveillance systems desig
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#30693;&#35782;&#22312;&#22810;&#27169;&#24577;&#23545;&#40784;&#20013;&#30340;&#20316;&#29992;&#65292;&#35774;&#35745;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#23545;&#40784;&#25506;&#27979;&#22522;&#20934;&#26469;&#26816;&#27979;&#20851;&#38190;&#30340;&#35821;&#35328;&#32452;&#25104;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2308.12898</link><description>&lt;p&gt;
&#35821;&#35328;&#30693;&#35782;&#33021;&#25913;&#36827;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Linguistic Knowledge Improve Multimodal Alignment in Vision-Language Pretraining?. (arXiv:2308.12898v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#30693;&#35782;&#22312;&#22810;&#27169;&#24577;&#23545;&#40784;&#20013;&#30340;&#20316;&#29992;&#65292;&#35774;&#35745;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#23545;&#40784;&#25506;&#27979;&#22522;&#20934;&#26469;&#26816;&#27979;&#20851;&#38190;&#30340;&#35821;&#35328;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23186;&#20307;&#39046;&#22495;&#23545;&#36890;&#36807;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24863;&#30693;&#21644;&#34920;&#36798;&#29289;&#29702;&#19990;&#30028;&#23637;&#29616;&#20986;&#20102;&#24378;&#28872;&#30340;&#20852;&#36259;&#65292;&#20854;&#20013;&#35270;&#35273;-&#35821;&#35328;&#30456;&#20851;&#30340;&#30740;&#31350;&#26159;&#24403;&#21069;&#26368;&#21560;&#24341;&#20154;&#30340;&#35805;&#39064;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#20197;&#19979;&#20004;&#20010;&#38382;&#39064;&#30340;&#25506;&#32034;&#38750;&#24120;&#26377;&#38480;&#65306;1&#65289;&#22312;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#26159;&#21542;&#21487;&#20197;&#25552;&#21462;&#20851;&#38190;&#30340;&#35821;&#35328;&#30693;&#35782;&#65288;&#22914;&#35821;&#20041;&#21644;&#21477;&#27861;&#65289;&#65292;2&#65289;&#36825;&#31181;&#35821;&#35328;&#30693;&#35782;&#22914;&#20309;&#24433;&#21709;&#25110;&#22686;&#24378;&#22810;&#27169;&#24577;&#23545;&#40784;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#38416;&#26126;&#20840;&#38754;&#30340;&#35821;&#35328;&#30693;&#35782;&#65292;&#21253;&#25324;&#35821;&#20041;&#34920;&#36798;&#21644;&#21477;&#27861;&#32467;&#26500;&#65292;&#23545;&#22810;&#27169;&#24577;&#23545;&#40784;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#21457;&#24067;&#20102;SNARE&#65292;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#25506;&#27979;&#22522;&#20934;&#65292;&#26469;&#26816;&#27979;&#20851;&#38190;&#30340;&#35821;&#35328;&#32452;&#25104;&#37096;&#20998;&#65292;&#22914;&#35789;&#27719;&#12289;&#35821;&#20041;&#21644;&#21477;&#27861;&#30693;&#35782;&#65292;&#21253;&#21547;&#20102;&#22235;&#20010;&#20219;&#21153;&#65306;&#35821;&#20041;&#32467;&#26500;&#12289;&#21542;&#23450;&#36923;&#36753;&#12289;&#23646;&#24615;&#24402;&#23646;&#21644;&#20851;&#31995;&#32452;&#21512;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;.....
&lt;/p&gt;
&lt;p&gt;
The multimedia community has shown a significant interest in perceiving and representing the physical world with multimodal pretrained neural network models, and among them, the visual-language pertaining (VLP) is, currently, the most captivating topic. However, there have been few endeavors dedicated to the exploration of 1) whether essential linguistic knowledge (e.g., semantics and syntax) can be extracted during VLP, and 2) how such linguistic knowledge impact or enhance the multimodal alignment. In response, here we aim to elucidate the impact of comprehensive linguistic knowledge, including semantic expression and syntactic structure, on multimodal alignment. Specifically, we design and release the SNARE, the first large-scale multimodal alignment probing benchmark, to detect the vital linguistic components, e.g., lexical, semantic, and syntax knowledge, containing four tasks: Semantic structure, Negation logic, Attribute ownership, and Relationship composition. Based on our prop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#12289;&#35268;&#27169;&#21644;&#20219;&#21153;&#65292;&#21487;&#20197;&#26377;&#25928;&#20351;&#20854;&#25104;&#20026;&#24378;&#22823;&#30340;&#35821;&#35328;&#23398;&#20064;&#32773;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#36890;&#29992;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#33021;&#22815;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12219</link><description>&lt;p&gt;
&#25193;&#23637;&#24615;&#21644;&#25351;&#23548;&#35843;&#20248;&#30340;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23436;&#25104;&#22810;&#31181;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning. (arXiv:2308.12219v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#12289;&#35268;&#27169;&#21644;&#20219;&#21153;&#65292;&#21487;&#20197;&#26377;&#25928;&#20351;&#20854;&#25104;&#20026;&#24378;&#22823;&#30340;&#35821;&#35328;&#23398;&#20064;&#32773;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#36890;&#29992;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#33021;&#22815;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#20852;&#36215;&#24471;&#30410;&#20110;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#23613;&#31649;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#35299;&#20915;&#19982;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#36890;&#29992;&#35821;&#35328;&#20219;&#21153;&#20173;&#28982;&#19981;&#26126;&#30830;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#12289;&#35268;&#27169;&#21644;&#20219;&#21153;&#26041;&#38754;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#20351;&#20854;&#25104;&#20026;&#24378;&#22823;&#30340;&#35821;&#35328;&#23398;&#20064;&#32773;&#12290;&#25105;&#20204;&#36890;&#36807;&#20808;&#36890;&#36807;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#39044;&#35757;&#32451;&#20174;&#22823;&#35268;&#27169;&#25968;&#25454;&#20013;&#33719;&#21462;&#30693;&#35782;&#65292;&#20877;&#36890;&#36807;&#25193;&#25955;&#36866;&#24212;&#23558;&#39044;&#35757;&#32451;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#20026;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#30340;&#24494;&#35843;&#21644;&#25351;&#23548;&#35843;&#20248;&#26469;&#21457;&#25496;&#20854;&#22312;&#35299;&#20915;&#36890;&#29992;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#19979;&#28216;&#35821;&#35328;&#20219;&#21153;&#20013;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent surge of generative AI has been fueled by the generative power of diffusion probabilistic models and the scalable capabilities of large language models. Despite their potential, it remains elusive whether diffusion language models can solve general language tasks comparable to their autoregressive counterparts. This paper demonstrates that scaling diffusion models w.r.t. data, sizes, and tasks can effectively make them strong language learners. We build competent diffusion language models at scale by first acquiring knowledge from massive data via masked language modeling pretraining thanks to their intrinsic connections. We then reprogram pretrained masked language models into diffusion language models via diffusive adaptation, wherein task-specific finetuning and instruction finetuning are explored to unlock their versatility in solving general language tasks. Experiments show that scaling diffusion language models consistently improves performance across downstream langua
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31867;&#20998;&#31867;&#65288;OCC&#65289;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#21457;&#29616;&#22312;&#36866;&#24403;&#30340;&#33539;&#25968;&#31354;&#38388;&#20013;&#65292;&#20219;&#20309;&#31354;&#38388;&#37117;&#21487;&#20197;&#20316;&#20026;&#36229;&#29699;&#24515;&#30340;&#31561;&#25928;&#26367;&#20195;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#35757;&#32451;&#26679;&#26412;&#30340;&#20998;&#24067;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2308.11898</link><description>&lt;p&gt;
&#25506;&#32034;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#19968;&#31867;&#20998;&#31867;&#30340;&#20248;&#21270;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Exploring the Optimization Objective of One-Class Classification for Anomaly Detection. (arXiv:2308.11898v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31867;&#20998;&#31867;&#65288;OCC&#65289;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#21457;&#29616;&#22312;&#36866;&#24403;&#30340;&#33539;&#25968;&#31354;&#38388;&#20013;&#65292;&#20219;&#20309;&#31354;&#38388;&#37117;&#21487;&#20197;&#20316;&#20026;&#36229;&#29699;&#24515;&#30340;&#31561;&#25928;&#26367;&#20195;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#35757;&#32451;&#26679;&#26412;&#30340;&#20998;&#24067;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31867;&#20998;&#31867;&#65288;OCC&#65289;&#26159;&#19968;&#31181;&#38271;&#26399;&#20197;&#26469;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#20511;&#21161;&#39044;&#35757;&#32451;&#39592;&#24178;&#32593;&#32476;&#30340;&#24378;&#22823;&#34920;&#31034;&#33021;&#21147;&#65292;OCC&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#26377;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;&#36890;&#24120;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;OCC&#26041;&#27861;&#37319;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#22686;&#24378;&#39044;&#35757;&#32451;&#39592;&#24178;&#32593;&#32476;&#29305;&#24449;&#30340;&#21306;&#20998;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#25928;&#26524;&#12290;&#34429;&#28982;&#24403;&#21069;&#22823;&#22810;&#25968;&#26041;&#27861;&#24378;&#35843;&#29305;&#24449;&#36801;&#31227;&#31574;&#30053;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;OCC&#26041;&#27861;&#20013;&#30340;&#20248;&#21270;&#30446;&#26631;&#31354;&#38388;&#20063;&#21487;&#33021;&#26159;&#24433;&#21709;&#24615;&#33021;&#30340;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;OCC&#30340;&#20248;&#21270;&#30446;&#26631;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#25512;&#23548;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20010;&#20851;&#38190;&#30340;&#27934;&#35265;&#65306;&#22312;&#36866;&#24403;&#30340;&#33539;&#25968;&#31354;&#38388;&#20013;&#65292;&#20219;&#20309;&#31354;&#38388;&#37117;&#21487;&#20197;&#20316;&#20026;&#36229;&#29699;&#24515;&#30340;&#31561;&#25928;&#26367;&#20195;&#65292;&#26080;&#38656;&#20381;&#36182;&#20110;&#35757;&#32451;&#26679;&#26412;&#30340;&#20998;&#24067;&#20551;&#35774;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#30830;&#23450;&#21487;&#34892;&#22495;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
One-class classification (OCC) is a longstanding method for anomaly detection. With the powerful representation capability of the pre-trained backbone, OCC methods have witnessed significant performance improvements. Typically, most of these OCC methods employ transfer learning to enhance the discriminative nature of the pre-trained backbone's features, thus achieving remarkable efficacy. While most current approaches emphasize feature transfer strategies, we argue that the optimization objective space within OCC methods could also be an underlying critical factor influencing performance. In this work, we conducted a thorough investigation into the optimization objective of OCC. Through rigorous theoretical analysis and derivation, we unveil a key insights: any space with the suitable norm can serve as an equivalent substitute for the hypersphere center, without relying on the distribution assumption of training samples. Further, we provide guidelines for determining the feasible domai
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#36234;&#29425;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#35821;&#20041;&#38450;&#28779;&#22681;&#30340;&#27010;&#24565;&#21644;&#19977;&#31181;&#25216;&#26415;&#23454;&#29616;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.11521</link><description>&lt;p&gt;
&#33258;&#25105;&#27450;&#39575;&#65306;&#36870;&#21521;&#30772;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#38450;&#28779;&#22681;
&lt;/p&gt;
&lt;p&gt;
Self-Deception: Reverse Penetrating the Semantic Firewall of Large Language Models. (arXiv:2308.11521v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11521
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#36234;&#29425;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#35821;&#20041;&#38450;&#28779;&#22681;&#30340;&#27010;&#24565;&#21644;&#19977;&#31181;&#25216;&#26415;&#23454;&#29616;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914;ChatGPT&#65292;&#20855;&#26377;&#25509;&#36817;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#24778;&#20154;&#33021;&#21147;&#12290;&#34429;&#28982;&#20026;&#21508;&#31181;&#31038;&#20250;&#38656;&#27714;&#25552;&#20379;&#20102;&#20415;&#21033;&#65292;&#20294;LLM&#20063;&#38477;&#20302;&#20102;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;LLM&#24320;&#21457;&#20154;&#21592;&#24050;&#32463;&#37096;&#32626;&#20102;&#35821;&#20041;&#32423;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#25298;&#32477;&#21487;&#33021;&#23548;&#33268;&#19981;&#36866;&#24403;&#20869;&#23481;&#30340;&#25552;&#31034;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#38450;&#24481;&#26426;&#21046;&#24182;&#19981;&#23436;&#20840;&#21487;&#38752;&#65292;&#19968;&#20123;&#25915;&#20987;&#32773;&#24050;&#32463;&#35774;&#35745;&#20986;&#20102;&#8220;&#36234;&#29425;&#8221;&#25552;&#31034;&#65292;&#20020;&#26102;&#20351;LLM&#24536;&#35760;&#20869;&#23481;&#38450;&#24481;&#35268;&#21017;&#24182;&#22238;&#31572;&#20219;&#20309;&#19981;&#36866;&#24403;&#30340;&#38382;&#39064;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#23578;&#26080;&#20851;&#20110;&#36825;&#20123;&#35821;&#20041;&#32423;&#25915;&#20987;&#21644;&#38450;&#24481;&#21407;&#21017;&#30340;&#26126;&#30830;&#35299;&#37322;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#36234;&#29425;&#38382;&#39064;&#65292;&#24182;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#36234;&#29425;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35821;&#20041;&#38450;&#28779;&#22681;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20379;&#20102;&#19977;&#31181;&#25216;&#26415;&#23454;&#29616;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as ChatGPT, have emerged with astonishing capabilities approaching artificial general intelligence. While providing convenience for various societal needs, LLMs have also lowered the cost of generating harmful content. Consequently, LLM developers have deployed semantic-level defenses to recognize and reject prompts that may lead to inappropriate content. Unfortunately, these defenses are not foolproof, and some attackers have crafted "jailbreak" prompts that temporarily hypnotize the LLM into forgetting content defense rules and answering any improper questions. To date, there is no clear explanation of the principles behind these semantic-level attacks and defenses in both industry and academia.  This paper investigates the LLM jailbreak problem and proposes an automatic jailbreak method for the first time. We propose the concept of a semantic firewall and provide three technical implementation approaches. Inspired by the attack that penetrates trad
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#26469;&#25913;&#21892;&#22810;&#39033;&#24335;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#39033;&#24335;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#22312;&#26041;&#31243;&#24674;&#22797;&#38382;&#39064;&#20013;&#30340;&#34920;&#29616;&#65292;&#20854;&#20013;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#26159;&#26368;&#20339;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#24191;&#27867;&#30340;&#31526;&#21495;&#31070;&#32463;&#32593;&#32476;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2308.10892</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#22810;&#39033;&#24335;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#39033;&#24335;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Bayesian polynomial neural networks and polynomial neural ordinary differential equations. (arXiv:2308.10892v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#26469;&#25913;&#21892;&#22810;&#39033;&#24335;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#39033;&#24335;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#22312;&#26041;&#31243;&#24674;&#22797;&#38382;&#39064;&#20013;&#30340;&#34920;&#29616;&#65292;&#20854;&#20013;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#26159;&#26368;&#20339;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#24191;&#27867;&#30340;&#31526;&#21495;&#31070;&#32463;&#32593;&#32476;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#24335;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#39033;&#24335;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#26159;&#26368;&#36817;&#29992;&#20110;&#31185;&#23398;&#21644;&#24037;&#31243;&#38382;&#39064;&#26041;&#31243;&#24674;&#22797;&#30340;&#20004;&#31181;&#24378;&#22823;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21482;&#33021;&#25552;&#20379;&#27169;&#22411;&#21442;&#25968;&#30340;&#28857;&#20272;&#35745;&#65292;&#24182;&#19988;&#30446;&#21069;&#19981;&#33021;&#36866;&#24212;&#22122;&#22768;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#21644;&#39564;&#35777;&#20197;&#19979;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;: &#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#12289;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;(MCMC)&#37319;&#26679;&#26041;&#27861;&#21644;&#21464;&#20998;&#25512;&#26029;&#12290;&#25105;&#20204;&#21457;&#29616;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#26159;&#36825;&#31867;&#38382;&#39064;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#22810;&#39033;&#24335;&#31070;&#32463;&#32593;&#32476;&#25152;&#23646;&#30340;&#26356;&#24191;&#27867;&#30340;&#31526;&#21495;&#31070;&#32463;&#32593;&#32476;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic regression with polynomial neural networks and polynomial neural ordinary differential equations (ODEs) are two recent and powerful approaches for equation recovery of many science and engineering problems. However, these methods provide point estimates for the model parameters and are currently unable to accommodate noisy data. We address this challenge by developing and validating the following Bayesian inference methods: the Laplace approximation, Markov Chain Monte Carlo (MCMC) sampling methods, and variational inference. We have found the Laplace approximation to be the best method for this class of problems. Our work can be easily extended to the broader class of symbolic neural networks to which the polynomial neural network belongs.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33021;&#28304;&#31649;&#29702;&#26041;&#27861;&#26469;&#20248;&#21270;&#25805;&#20316;&#20154;&#24037;&#19978;&#28044;&#31995;&#32479;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#24314;&#27169;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#32467;&#21512;&#20998;&#20301;&#32593;&#32476;&#21644;&#28145;&#24230;&#31454;&#20105;&#32593;&#32476;&#30340;&#24605;&#24819;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#25928;&#12290;</title><link>http://arxiv.org/abs/2308.10199</link><description>&lt;p&gt;
&#20154;&#24037;&#19978;&#28044;&#33021;&#28304;&#31649;&#29702;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Artificial Upwelling Energy Management. (arXiv:2308.10199v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10199
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33021;&#28304;&#31649;&#29702;&#26041;&#27861;&#26469;&#20248;&#21270;&#25805;&#20316;&#20154;&#24037;&#19978;&#28044;&#31995;&#32479;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#24314;&#27169;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#32467;&#21512;&#20998;&#20301;&#32593;&#32476;&#21644;&#28145;&#24230;&#31454;&#20105;&#32593;&#32476;&#30340;&#24605;&#24819;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#19978;&#28044;&#65288;AU&#65289;&#20316;&#20026;&#19968;&#31181;&#23558;&#23500;&#21547;&#33829;&#20859;&#30340;&#24213;&#23618;&#27700;&#25552;&#21319;&#21040;&#28023;&#38754;&#12289;&#21050;&#28608;&#28023;&#34299;&#29983;&#38271;&#24182;&#22686;&#21152;&#28023;&#27915;&#30899;&#23553;&#23384;&#30340;&#26041;&#27861;&#65292;&#21463;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#36825;&#23548;&#33268;&#22312;&#20013;&#22269;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#22826;&#38451;&#33021;&#20379;&#30005;&#21644;&#31354;&#27668;&#22686;&#21387;&#30340;AU&#31995;&#32479;&#65288;AUS&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#30340;&#28023;&#27915;&#29615;&#22659;&#20013;&#39640;&#25928;&#35843;&#24230;&#27668;&#20307;&#21943;&#23556;&#31995;&#32479;&#20173;&#28982;&#26159;&#25805;&#20316;AUS&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#26377;&#28508;&#21147;&#26174;&#33879;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#31639;&#27861;&#24320;&#21457;AUS&#36816;&#34892;&#30340;&#39640;&#25928;&#31574;&#30053;&#30340;&#26032;&#33021;&#28304;&#31649;&#29702;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#26368;&#22823;&#21270;AUS&#33021;&#28304;&#25928;&#29575;&#30340;&#38382;&#39064;&#24314;&#27169;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#23558;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#65288;QR-DQN&#65289;&#20013;&#30340;&#20998;&#20301;&#32593;&#32476;&#19982;&#28145;&#24230;&#31454;&#20105;&#32593;&#32476;&#30456;&#32467;&#21512;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#20223;&#30495;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;AUS&#30340;&#33021;&#28304;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The potential of artificial upwelling (AU) as a means of lifting nutrient-rich bottom water to the surface, stimulating seaweed growth, and consequently enhancing ocean carbon sequestration, has been gaining increasing attention in recent years. This has led to the development of the first solar-powered and air-lifted AU system (AUS) in China. However, efficient scheduling of air injection systems in complex marine environments remains a crucial challenge in operating AUS, as it holds the potential to significantly improve energy efficiency. To tackle this challenge, we propose a novel energy management approach that utilizes deep reinforcement learning (DRL) algorithm to develop efficient strategies for operating AUS. Specifically, we formulate the problem of maximizing the energy efficiency of AUS as a Markov decision process and integrate the quantile network in distributional reinforcement learning (QR-DQN) with the deep dueling network to solve it. Through extensive simulations, w
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20914;&#31361;&#24863;&#30693;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#20026;&#27599;&#20010;&#25351;&#26631;&#25552;&#20379;&#29420;&#29305;&#30340;&#32467;&#26500;&#26469;&#32531;&#35299;&#25351;&#26631;&#22238;&#24402;&#30446;&#26631;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;</title><link>http://arxiv.org/abs/2308.08915</link><description>&lt;p&gt;
&#36229;&#36234;&#20849;&#20139;&#65306;&#20914;&#31361;&#24863;&#30693;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Beyond Sharing: Conflict-Aware Multivariate Time Series Anomaly Detection. (arXiv:2308.08915v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08915
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20914;&#31361;&#24863;&#30693;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#20026;&#27599;&#20010;&#25351;&#26631;&#25552;&#20379;&#29420;&#29305;&#30340;&#32467;&#26500;&#26469;&#32531;&#35299;&#25351;&#26631;&#22238;&#24402;&#30446;&#26631;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#30340;&#20851;&#38190;&#32489;&#25928;&#25351;&#26631;(KPI)&#20197;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;(MTS)&#30340;&#24418;&#24335;&#36827;&#34892;&#30417;&#27979;&#65292;&#20197;&#30830;&#20445;&#36719;&#20214;&#24212;&#29992;&#31243;&#24207;&#21644;&#26381;&#21153;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#12290;&#20934;&#30830;&#26816;&#27979;MTS&#30340;&#24322;&#24120;&#23545;&#20110;&#21518;&#32493;&#30340;&#25925;&#38556;&#25490;&#38500;&#38750;&#24120;&#20851;&#38190;&#12290;&#24322;&#24120;&#30340;&#31232;&#32570;&#24615;&#21644;&#25163;&#21160;&#26631;&#35760;&#23548;&#33268;&#20102;&#21508;&#31181;&#33258;&#30417;&#30563;&#30340;MTS&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#36825;&#20123;&#26041;&#27861;&#20248;&#21270;&#20102;&#19968;&#20010;&#28085;&#30422;&#25152;&#26377;&#25351;&#26631;&#22238;&#24402;&#30446;&#26631;/&#25439;&#22833;&#30340;&#25972;&#20307;&#30446;&#26631;/&#25439;&#22833;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#20102;&#25351;&#26631;&#22238;&#24402;&#30446;&#26631;&#20043;&#38388;&#20914;&#31361;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#23548;&#33268;MTS&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#25439;&#22833;&#20013;&#25379;&#25166;&#12290;&#36825;&#19968;&#20851;&#38190;&#26041;&#38754;&#26174;&#33879;&#24433;&#21709;&#26816;&#27979;&#24615;&#33021;&#65292;&#20294;&#22312;&#29616;&#26377;&#26041;&#27861;&#20013;&#34987;&#24573;&#35270;&#20102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#27169;&#20223;&#22810;&#38376;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;(MMoE)&#30340;&#35774;&#35745;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CAD&#65292;&#19968;&#31181;&#20914;&#31361;&#24863;&#30693;&#30340;&#22810;&#21464;&#37327;KPI&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#12290;CAD&#20026;&#27599;&#20010;&#25351;&#26631;&#25552;&#20379;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#32467;&#26500;&#65292;&#20197;&#32531;&#35299;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;
Massive key performance indicators (KPIs) are monitored as multivariate time series data (MTS) to ensure the reliability of the software applications and service system. Accurately detecting the abnormality of MTS is very critical for subsequent fault elimination. The scarcity of anomalies and manual labeling has led to the development of various self-supervised MTS anomaly detection (AD) methods, which optimize an overall objective/loss encompassing all metrics' regression objectives/losses. However, our empirical study uncovers the prevalence of conflicts among metrics' regression objectives, causing MTS models to grapple with different losses. This critical aspect significantly impacts detection performance but has been overlooked in existing approaches. To address this problem, by mimicking the design of multi-gate mixture-of-experts (MMoE), we introduce CAD, a Conflict-aware multivariate KPI Anomaly Detection algorithm. CAD offers an exclusive structure for each metric to mitigate
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#32416;&#38169;&#30721;&#21464;&#21387;&#22120;&#20013;&#20351;&#29992;&#31995;&#32479;&#21270;&#32534;&#30721;&#21644;&#21452;&#37325;&#36974;&#34109;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08128</link><description>&lt;p&gt;
&#22914;&#20309;&#22312;&#32416;&#38169;&#30721;&#21464;&#21387;&#22120;&#20013;&#36827;&#34892;&#36974;&#34109;&#65306;&#31995;&#32479;&#21270;&#19982;&#21452;&#37325;&#36974;&#34109;
&lt;/p&gt;
&lt;p&gt;
How to Mask in Error Correction Code Transformer: Systematic and Double Masking. (arXiv:2308.08128v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08128
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#32416;&#38169;&#30721;&#21464;&#21387;&#22120;&#20013;&#20351;&#29992;&#31995;&#32479;&#21270;&#32534;&#30721;&#21644;&#21452;&#37325;&#36974;&#34109;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36890;&#20449;&#21644;&#23384;&#20648;&#31995;&#32479;&#20013;&#65292;&#32416;&#38169;&#30721;&#65288;ECC&#65289;&#23545;&#20110;&#30830;&#20445;&#25968;&#25454;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24212;&#29992;&#24191;&#27867;&#25193;&#23637;&#65292;&#31070;&#32463;&#32593;&#32476;&#35299;&#30721;&#22120;&#24050;&#25104;&#20026;&#30740;&#31350;&#30340;&#28966;&#28857;&#65292;&#36229;&#36234;&#20256;&#32479;&#35299;&#30721;&#31639;&#27861;&#12290;&#22312;&#36825;&#20123;&#31070;&#32463;&#35299;&#30721;&#22120;&#20013;&#65292;&#32416;&#38169;&#30721;&#21464;&#21387;&#22120;&#65288;ECCT&#65289;&#24050;&#32463;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22823;&#24133;&#36229;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;ECCT&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#21033;&#29992;ECC&#30340;&#31995;&#32479;&#32534;&#30721;&#25216;&#26415;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#36974;&#34109;&#30697;&#38453;&#26469;&#25913;&#21892;ECCT&#30340;&#24615;&#33021;&#24182;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;ECCT&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#31216;&#20026;&#21452;&#37325;&#36974;&#34109;&#30340;ECCT&#12290;&#35813;&#26550;&#26500;&#20197;&#24182;&#34892;&#26041;&#24335;&#20351;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#36974;&#34109;&#30697;&#38453;&#65292;&#20197;&#23398;&#20064;&#36974;&#34109;&#33258;&#27880;&#24847;&#21147;&#22359;&#20013;&#32534;&#30721;&#23383;&#20301;&#20043;&#38388;&#26356;&#22810;&#26679;&#30340;&#29305;&#24449;&#20851;&#31995;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In communication and storage systems, error correction codes (ECCs) are pivotal in ensuring data reliability. As deep learning's applicability has broadened across diverse domains, there is a growing research focus on neural network-based decoders that outperform traditional decoding algorithms. Among these neural decoders, Error Correction Code Transformer (ECCT) has achieved the state-of-the-art performance, outperforming other methods by large margins. To further enhance the performance of ECCT, we propose two novel methods. First, leveraging the systematic encoding technique of ECCs, we introduce a new masking matrix for ECCT, aiming to improve the performance and reduce the computational complexity. Second, we propose a novel transformer architecture of ECCT called a double-masked ECCT. This architecture employs two different mask matrices in a parallel manner to learn more diverse features of the relationship between codeword bits in the masked self-attention blocks. Extensive si
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21463;&#38480;&#39057;&#29575;&#30340;&#36523;&#20221;&#19981;&#21487;&#30693;&#25915;&#20987;&#36827;&#34892;&#20154;&#33080;&#21152;&#23494;&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;&#22806;&#37096;&#25200;&#21160;&#21152;&#23494;&#20154;&#33080;&#22270;&#20687;&#30340;&#38544;&#31169;&#20445;&#25252;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#40657;&#30418;&#22330;&#26223;&#19979;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.05983</link><description>&lt;p&gt;
&#36890;&#36807;&#21463;&#38480;&#39057;&#29575;&#30340;&#36523;&#20221;&#19981;&#21487;&#30693;&#25915;&#20987;&#36827;&#34892;&#20154;&#33080;&#21152;&#23494;
&lt;/p&gt;
&lt;p&gt;
Face Encryption via Frequency-Restricted Identity-Agnostic Attacks. (arXiv:2308.05983v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05983
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21463;&#38480;&#39057;&#29575;&#30340;&#36523;&#20221;&#19981;&#21487;&#30693;&#25915;&#20987;&#36827;&#34892;&#20154;&#33080;&#21152;&#23494;&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;&#22806;&#37096;&#25200;&#21160;&#21152;&#23494;&#20154;&#33080;&#22270;&#20687;&#30340;&#38544;&#31169;&#20445;&#25252;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#40657;&#30418;&#22330;&#26223;&#19979;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#22825;&#26377;&#25968;&#21313;&#20159;&#20154;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#20998;&#20139;&#20182;&#20204;&#30340;&#26085;&#24120;&#29031;&#29255;&#12290;&#28982;&#32780;&#65292;&#24694;&#24847;&#37319;&#38598;&#32773;&#21033;&#29992;&#28145;&#24230;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#36731;&#26494;&#22320;&#20174;&#36825;&#20123;&#22270;&#29255;&#20013;&#31363;&#21462;&#20182;&#20204;&#30340;&#29983;&#29289;&#29305;&#24449;&#20449;&#24687;&#65288;&#20363;&#22914;&#20154;&#33080;&#65289;&#12290;&#19968;&#20123;&#30740;&#31350;&#27491;&#22312;&#36827;&#34892;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#38590;&#20197;&#23519;&#35273;&#30340;&#25200;&#21160;&#26469;&#29983;&#25104;&#21152;&#23494;&#20154;&#33080;&#29031;&#29255;&#65292;&#20197;&#20943;&#23569;&#20154;&#33080;&#20449;&#24687;&#27844;&#28431;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#38656;&#35201;&#26356;&#24378;&#30340;&#40657;&#30418;&#22330;&#26223;&#21487;&#34892;&#24615;&#21644;&#26356;&#33258;&#28982;&#30340;&#35270;&#35273;&#22806;&#35266;&#65292;&#36825;&#23545;&#38544;&#31169;&#20445;&#25252;&#30340;&#21487;&#34892;&#24615;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#38480;&#39057;&#29575;&#30340;&#36523;&#20221;&#19981;&#21487;&#30693;&#65288;FRIA&#65289;&#26694;&#26550;&#65292;&#20197;&#20174;&#26410;&#32463;&#25480;&#26435;&#30340;&#20154;&#33080;&#35782;&#21035;&#20013;&#21152;&#23494;&#20154;&#33080;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#20010;&#20154;&#20449;&#24687;&#12290;&#23545;&#20110;&#24369;&#40657;&#30418;&#22330;&#26223;&#30340;&#21487;&#34892;&#24615;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22810;&#20010;&#20154;&#33080;&#35782;&#21035;&#27169;&#22411;&#20013;&#30340;&#24179;&#22343;&#29305;&#24449;&#34920;&#31034;&#30456;&#20284;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#36890;&#36807;&#20114;&#32852;&#32593;&#29228;&#21462;&#30340;&#25968;&#25454;&#38598;&#20013;&#30340;&#24179;&#22343;&#29305;&#24449;&#20316;&#20026;t
&lt;/p&gt;
&lt;p&gt;
Billions of people are sharing their daily live images on social media everyday. However, malicious collectors use deep face recognition systems to easily steal their biometric information (e.g., faces) from these images. Some studies are being conducted to generate encrypted face photos using adversarial attacks by introducing imperceptible perturbations to reduce face information leakage. However, existing studies need stronger black-box scenario feasibility and more natural visual appearances, which challenge the feasibility of privacy protection. To address these problems, we propose a frequency-restricted identity-agnostic (FRIA) framework to encrypt face images from unauthorized face recognition without access to personal information. As for the weak black-box scenario feasibility, we obverse that representations of the average feature in multiple face recognition models are similar, thus we propose to utilize the average feature via the crawled dataset from the Internet as the t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32047;&#31215;&#25512;&#29702;&#65288;CR&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20197;&#32047;&#31215;&#21644;&#36845;&#20195;&#30340;&#26041;&#24335;&#27169;&#25311;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#32452;&#20214;&#65292;&#31616;&#21270;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#36923;&#36753;&#25512;&#29702;&#21644;24&#28857;&#28216;&#25103;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.04371</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32047;&#31215;&#25512;&#29702;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Cumulative Reasoning With Large Language Models. (arXiv:2308.04371v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32047;&#31215;&#25512;&#29702;&#65288;CR&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20197;&#32047;&#31215;&#21644;&#36845;&#20195;&#30340;&#26041;&#24335;&#27169;&#25311;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#32452;&#20214;&#65292;&#31616;&#21270;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#36923;&#36753;&#25512;&#29702;&#21644;24&#28857;&#28216;&#25103;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#35821;&#35328;&#27169;&#22411;&#24378;&#22823;&#19988;&#22810;&#21151;&#33021;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26080;&#27861;&#35299;&#20915;&#39640;&#24230;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#36825;&#26159;&#22240;&#20026;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#38656;&#35201;&#28145;&#24605;&#29087;&#34385;&#65292;&#32780;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#27492;&#21482;&#26377;&#26368;&#23567;&#31243;&#24230;&#30340;&#25351;&#23548;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#32047;&#31215;&#25512;&#29702;&#65288;CR&#65289;&#65292;&#23427;&#20197;&#32047;&#31215;&#21644;&#36845;&#20195;&#30340;&#26041;&#24335;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#27169;&#25311;&#20154;&#31867;&#30340;&#24605;&#32500;&#36807;&#31243;&#12290;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#32452;&#20214;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#31616;&#21270;&#20102;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#65292;&#20351;&#20854;&#26356;&#26131;&#31649;&#29702;&#21644;&#26356;&#26377;&#25928;&#12290;&#23545;&#20110;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;CR&#22312;&#24615;&#33021;&#19978;&#22987;&#32456;&#36229;&#36807;&#29616;&#26377;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#22810;&#36798;9.3&#65285;&#65292;&#24182;&#22312;&#32463;&#36807;&#31574;&#21010;&#30340;FOLIO&#32500;&#22522;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#24778;&#20154;&#30340;98.04&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;&#22312;24&#28857;&#28216;&#25103;&#30340;&#32972;&#26223;&#19979;&#65292;CR&#23454;&#29616;&#20102;94&#65285;&#30340;&#20934;&#30830;&#29575;&#65292;&#30456;&#27604;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25552;&#21319;&#20102;20&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
While language models are powerful and versatile, they often fail to address highly complex problems. This is because solving complex problems requires deliberate thinking, which has been only minimally guided during training. In this paper, we propose a new method called Cumulative Reasoning (CR), which employs language models in a cumulative and iterative manner to emulate human thought processes. By decomposing tasks into smaller components, \ournameb streamlines the problem-solving process, rendering it both more manageable and effective. For logical inference tasks, CR consistently outperforms existing methods with an improvement up to 9.3\%, and achieves the astonishing accuracy of 98.04\% on the curated FOLIO wiki dataset. In the context of the Game of 24, CR achieves an accuracy of 94\%, which signifies a substantial enhancement of 20\% over the previous state-of-the-art method.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#25490;&#24207;&#26694;&#26550;&#65292;&#21517;&#20026;STARank&#65292;&#23427;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#20505;&#36873;&#39033;&#30446;&#25490;&#21015;&#26469;&#26367;&#20195;&#20010;&#21035;&#35780;&#20998;&#21644;&#25490;&#24207;&#25805;&#20316;&#65292;&#24182;&#19988;&#26159;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#30340;&#12290;</title><link>http://arxiv.org/abs/2308.02860</link><description>&lt;p&gt;
&#29992;&#23433;&#25490;&#21462;&#20195;&#35780;&#20998;&#65306;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#25490;&#24207;&#30340;&#19978;&#19979;&#25991;&#38598;&#21512;&#21040;&#25490;&#21015;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Replace Scoring with Arrangement: A Contextual Set-to-Arrangement Framework for Learning-to-Rank. (arXiv:2308.02860v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02860
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#25490;&#24207;&#26694;&#26550;&#65292;&#21517;&#20026;STARank&#65292;&#23427;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#20505;&#36873;&#39033;&#30446;&#25490;&#21015;&#26469;&#26367;&#20195;&#20010;&#21035;&#35780;&#20998;&#21644;&#25490;&#24207;&#25805;&#20316;&#65292;&#24182;&#19988;&#26159;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#25490;&#24207;&#26159;top-N&#25512;&#33616;&#20219;&#21153;&#20013;&#30340;&#26680;&#24515;&#25216;&#26415;&#65292;&#29702;&#24819;&#30340;&#25490;&#21517;&#22120;&#24212;&#35813;&#26159;&#19968;&#20010;&#20174;&#39033;&#30446;&#38598;&#21512;&#21040;&#25490;&#21015;&#65288;&#21363;&#25490;&#21015;&#65289;&#30340;&#26144;&#23556;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#35299;&#20915;&#26041;&#26696;&#23646;&#20110;&#27010;&#29575;&#25490;&#24207;&#21407;&#21017;&#65288;PRP&#65289;&#33539;&#24335;&#65292;&#21363;&#39318;&#20808;&#23545;&#20505;&#36873;&#38598;&#20013;&#30340;&#27599;&#20010;&#39033;&#30446;&#36827;&#34892;&#35780;&#20998;&#65292;&#28982;&#21518;&#25191;&#34892;&#25490;&#24207;&#25805;&#20316;&#20197;&#29983;&#25104;&#25490;&#21517;&#21015;&#34920;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24573;&#35270;&#20102;&#20010;&#20307;&#35780;&#20998;&#36807;&#31243;&#20013;&#20505;&#36873;&#39033;&#30446;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20381;&#36182;&#24615;&#65292;&#24182;&#19988;&#25490;&#24207;&#25805;&#20316;&#26159;&#19981;&#21487;&#24494;&#20998;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STARank&#30340;&#38598;&#21512;&#21040;&#25490;&#21015;&#25490;&#24207;&#26694;&#26550;&#65292;&#23427;&#30452;&#25509;&#29983;&#25104;&#20505;&#36873;&#39033;&#30446;&#30340;&#25490;&#21015;&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#34892;&#20010;&#21035;&#35780;&#20998;&#21644;&#25490;&#24207;&#25805;&#20316;&#65292;&#24182;&#19988;&#26159;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#30340;&#12290;&#22240;&#27492;&#65292;STARank&#21487;&#20197;&#22312;&#21482;&#26377;&#30495;&#23454;&#25490;&#21015;&#21487;&#35775;&#38382;&#20294;&#27809;&#26377;&#39033;&#30446;&#30340;&#30495;&#23454;&#30456;&#20851;&#24230;&#20998;&#25968;&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#12290;&#20026;&#27492;&#65292;STARank&#39318;&#20808;&#38405;&#35835;&#20505;&#36873;&#39033;&#30446;...
&lt;/p&gt;
&lt;p&gt;
Learning-to-rank is a core technique in the top-N recommendation task, where an ideal ranker would be a mapping from an item set to an arrangement (a.k.a. permutation). Most existing solutions fall in the paradigm of probabilistic ranking principle (PRP), i.e., first score each item in the candidate set and then perform a sort operation to generate the top ranking list. However, these approaches neglect the contextual dependence among candidate items during individual scoring, and the sort operation is non-differentiable. To bypass the above issues, we propose Set-To-Arrangement Ranking (STARank), a new framework directly generates the permutations of the candidate items without the need for individually scoring and sort operations; and is end-to-end differentiable. As a result, STARank can operate when only the ground-truth permutations are accessible without requiring access to the ground-truth relevance scores for items. For this purpose, STARank first reads the candidate items in t
&lt;/p&gt;</description></item><item><title>ChatMOF&#26159;&#19968;&#31181;&#33258;&#20027;AI&#31995;&#32479;&#65292;&#29992;&#20110;&#39044;&#27979;&#21644;&#29983;&#25104;&#37329;&#23646;-&#26377;&#26426;&#39592;&#26550;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#20174;&#25991;&#26412;&#36755;&#20837;&#20013;&#25552;&#21462;&#20851;&#38190;&#32454;&#33410;&#65292;&#24182;&#25552;&#20379;&#36866;&#24403;&#30340;&#22238;&#24212;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#32452;&#21512;&#20195;&#29702;&#12289;&#24037;&#20855;&#21253;&#21644;&#35780;&#20272;&#22120;&#30340;&#26680;&#24515;&#32452;&#20214;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#26816;&#32034;&#12289;&#24615;&#36136;&#39044;&#27979;&#21644;&#32467;&#26500;&#29983;&#25104;&#31561;&#22810;&#20010;&#20219;&#21153;&#12290;&#30740;&#31350;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21183;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.01423</link><description>&lt;p&gt;
ChatMOF: &#19968;&#31181;&#33258;&#20027;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#29992;&#20110;&#39044;&#27979;&#21644;&#29983;&#25104;&#37329;&#23646;-&#26377;&#26426;&#39592;&#26550;
&lt;/p&gt;
&lt;p&gt;
ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks. (arXiv:2308.01423v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01423
&lt;/p&gt;
&lt;p&gt;
ChatMOF&#26159;&#19968;&#31181;&#33258;&#20027;AI&#31995;&#32479;&#65292;&#29992;&#20110;&#39044;&#27979;&#21644;&#29983;&#25104;&#37329;&#23646;-&#26377;&#26426;&#39592;&#26550;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#20174;&#25991;&#26412;&#36755;&#20837;&#20013;&#25552;&#21462;&#20851;&#38190;&#32454;&#33410;&#65292;&#24182;&#25552;&#20379;&#36866;&#24403;&#30340;&#22238;&#24212;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#32452;&#21512;&#20195;&#29702;&#12289;&#24037;&#20855;&#21253;&#21644;&#35780;&#20272;&#22120;&#30340;&#26680;&#24515;&#32452;&#20214;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#26816;&#32034;&#12289;&#24615;&#36136;&#39044;&#27979;&#21644;&#32467;&#26500;&#29983;&#25104;&#31561;&#22810;&#20010;&#20219;&#21153;&#12290;&#30740;&#31350;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21183;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatMOF&#26159;&#19968;&#20010;&#33258;&#20027;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#29992;&#20110;&#39044;&#27979;&#21644;&#29983;&#25104;&#37329;&#23646;-&#26377;&#26426;&#39592;&#26550;&#65288;MOFs&#65289;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;gpt-3.5-turbo&#65289;&#65292;ChatMOF&#20174;&#25991;&#26412;&#36755;&#20837;&#20013;&#25552;&#21462;&#20851;&#38190;&#32454;&#33410;&#24182;&#25552;&#20379;&#36866;&#24403;&#30340;&#22238;&#24212;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#23545;&#21018;&#24615;&#32467;&#26500;&#21270;&#26597;&#35810;&#30340;&#38656;&#27714;&#12290;&#35813;&#31995;&#32479;&#30001;&#19977;&#20010;&#26680;&#24515;&#32452;&#20214;&#65288;&#21363;&#20195;&#29702;&#12289;&#24037;&#20855;&#21253;&#21644;&#35780;&#20272;&#22120;&#65289;&#32452;&#25104;&#65292;&#24418;&#25104;&#19968;&#20010;&#24378;&#22823;&#30340;&#27969;&#27700;&#32447;&#65292;&#31649;&#29702;&#22810;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#25968;&#25454;&#26816;&#32034;&#12289;&#24615;&#36136;&#39044;&#27979;&#21644;&#32467;&#26500;&#29983;&#25104;&#12290;&#35813;&#30740;&#31350;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20248;&#28857;&#21644;&#38480;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#23545;&#26410;&#26469;&#21457;&#23637;&#30340;&#21464;&#38761;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatMOF is an autonomous Artificial Intelligence (AI) system that is built to predict and generate of metal-organic frameworks (MOFs). By leveraging a large-scale language model (gpt-3.5-turbo), ChatMOF extracts key details from textual inputs and delivers appropriate responses, thus eliminating the necessity for rigid structured queries. The system is comprised of three core components (i.e. an agent, a toolkit, and an evaluator) and it forms a robust pipeline that manages a variety of tasks, including data retrieval, property prediction, and structure generation. The study further explores the merits and constraints of using large language models (LLMs) AI system in material sciences using and showcases its transformative potential for future advancements.
&lt;/p&gt;</description></item><item><title>BubbleML&#26159;&#19968;&#20010;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#29289;&#29702;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#29289;&#29702;&#39537;&#21160;&#27169;&#25311;&#33719;&#24471;&#20934;&#30830;&#30340;&#22320;&#38754;&#30495;&#23454;&#20449;&#24687;&#65292;&#24182;&#22312;&#21508;&#31181;&#27832;&#33150;&#22330;&#26223;&#20013;&#39564;&#35777;&#20102;&#20854;&#21487;&#38752;&#24615;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.14623</link><description>&lt;p&gt;
BubbleML: &#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#29289;&#29702;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
BubbleML: A Multi-Physics Dataset and Benchmarks for Machine Learning. (arXiv:2307.14623v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14623
&lt;/p&gt;
&lt;p&gt;
BubbleML&#26159;&#19968;&#20010;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#29289;&#29702;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#29289;&#29702;&#39537;&#21160;&#27169;&#25311;&#33719;&#24471;&#20934;&#30830;&#30340;&#22320;&#38754;&#30495;&#23454;&#20449;&#24687;&#65292;&#24182;&#22312;&#21508;&#31181;&#27832;&#33150;&#22330;&#26223;&#20013;&#39564;&#35777;&#20102;&#20854;&#21487;&#38752;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30456;&#21464;&#29616;&#35937;&#39046;&#22495;&#65292;&#32570;&#20047;&#36866;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#30340;&#21487;&#35775;&#38382;&#21644;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#23454;&#39564;&#25968;&#25454;&#38598;&#36890;&#24120;&#21463;&#38480;&#65292;&#21487;&#29992;&#24615;&#26377;&#38480;&#19988;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#31232;&#32570;&#65292;&#38459;&#30861;&#20102;&#25105;&#20204;&#23545;&#36825;&#31181;&#22797;&#26434;&#22810;&#29289;&#29702;&#29616;&#35937;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BubbleML&#25968;&#25454;&#38598;&#65288;https://github.com/HPCForge/BubbleML&#65289;&#65292;&#23427;&#21033;&#29992;&#29289;&#29702;&#39537;&#21160;&#30340;&#27169;&#25311;&#20026;&#21508;&#31181;&#27832;&#33150;&#22330;&#26223;&#25552;&#20379;&#20934;&#30830;&#30340;&#22320;&#38754;&#30495;&#23454;&#20449;&#24687;&#65292;&#21253;&#25324;&#26680;&#27873;&#27744;&#27832;&#33150;&#12289;&#27969;&#21160;&#27832;&#33150;&#21644;&#20122;&#20919;&#27832;&#33150;&#12290;&#36825;&#20010;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#21508;&#31181;&#21442;&#25968;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#37325;&#21147;&#26465;&#20214;&#12289;&#27969;&#37327;&#12289;&#20122;&#20919;&#27700;&#24179;&#21644;&#22721;&#38754;&#36807;&#28909;&#65292;&#24635;&#20849;&#26377;51&#20010;&#27169;&#25311;&#12290;BubbleML&#24050;&#32463;&#36890;&#36807;&#23454;&#39564;&#35266;&#23519;&#21644;&#36235;&#21183;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#34987;&#30830;&#35748;&#20026;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20419;&#36827;&#22810;&#26679;&#21270;&#38477;&#20302;&#28201;&#24230;&#27832;&#33150;&#30740;&#31350;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of phase change phenomena, the lack of accessible and diverse datasets suitable for machine learning (ML) training poses a significant challenge. Existing experimental datasets are often restricted, with limited availability and sparse ground truth data, impeding our understanding of this complex multi-physics phenomena. To bridge this gap, we present the BubbleML Dataset(https://github.com/HPCForge/BubbleML) which leverages physics-driven simulations to provide accurate ground truth information for various boiling scenarios, encompassing nucleate pool boiling, flow boiling, and sub-cooled boiling. This extensive dataset covers a wide range of parameters, including varying gravity conditions, flow rates, sub-cooling levels, and wall superheat, comprising 51 simulations. BubbleML is validated against experimental observations and trends, establishing it as an invaluable resource for ML research. Furthermore, we showcase its potential to facilitate exploration of diverse dow
&lt;/p&gt;</description></item><item><title>&#22312;&#21322;&#30417;&#30563;&#30446;&#26631;&#26816;&#27979;&#20013;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#35757;&#32451;&#30340;&#27169;&#22411;&#31934;&#21270;(TMR)&#38454;&#27573;&#21644;&#34920;&#31034;&#20998;&#27495;(RD)&#31574;&#30053;&#65292;&#29992;&#26469;&#35299;&#20915;&#20266;&#26631;&#31614;&#22122;&#22768;&#21644;&#25945;&#24072;-&#23398;&#29983;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;TMR&#38454;&#27573;&#36890;&#36807;&#36731;&#37327;&#32423;&#32553;&#25918;&#25805;&#20316;&#20248;&#21270;&#27169;&#22411;&#26435;&#37325;&#65292;&#38450;&#27490;&#36807;&#24230;&#25311;&#21512;&#25110;&#36951;&#24536;&#23398;&#21040;&#30340;&#27169;&#24335;&#65307;RD&#31574;&#30053;&#24110;&#21161;&#20445;&#25345;&#27169;&#22411;&#30340;&#24046;&#24322;&#65292;&#40723;&#21169;&#23398;&#29983;&#27169;&#22411;&#25506;&#32034;&#20114;&#34917;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.13755</link><description>&lt;p&gt;
TMR-RD: &#29992;&#20110;&#21322;&#30417;&#30563;&#30446;&#26631;&#26816;&#27979;&#30340;&#22522;&#20110;&#35757;&#32451;&#30340;&#27169;&#22411;&#31934;&#21270;&#21644;&#34920;&#31034;&#20998;&#27495;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TMR-RD: Training-based Model Refinement and Representation Disagreement for Semi-Supervised Object Detection. (arXiv:2307.13755v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13755
&lt;/p&gt;
&lt;p&gt;
&#22312;&#21322;&#30417;&#30563;&#30446;&#26631;&#26816;&#27979;&#20013;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#35757;&#32451;&#30340;&#27169;&#22411;&#31934;&#21270;(TMR)&#38454;&#27573;&#21644;&#34920;&#31034;&#20998;&#27495;(RD)&#31574;&#30053;&#65292;&#29992;&#26469;&#35299;&#20915;&#20266;&#26631;&#31614;&#22122;&#22768;&#21644;&#25945;&#24072;-&#23398;&#29983;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;TMR&#38454;&#27573;&#36890;&#36807;&#36731;&#37327;&#32423;&#32553;&#25918;&#25805;&#20316;&#20248;&#21270;&#27169;&#22411;&#26435;&#37325;&#65292;&#38450;&#27490;&#36807;&#24230;&#25311;&#21512;&#25110;&#36951;&#24536;&#23398;&#21040;&#30340;&#27169;&#24335;&#65307;RD&#31574;&#30053;&#24110;&#21161;&#20445;&#25345;&#27169;&#22411;&#30340;&#24046;&#24322;&#65292;&#40723;&#21169;&#23398;&#29983;&#27169;&#22411;&#25506;&#32034;&#20114;&#34917;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#30446;&#26631;&#26816;&#27979;(SSOD)&#21487;&#20197;&#23558;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#21644;&#22823;&#37327;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#39640;&#29616;&#26377;&#30446;&#26631;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#35768;&#22810;&#36827;&#23637;&#65292;&#20294;&#26159;&#26368;&#36817;&#30340;SSOD&#26041;&#27861;&#20173;&#28982;&#38754;&#20020;&#30528;&#20266;&#26631;&#31614;&#22122;&#22768;/&#35823;&#23548;&#12289;&#32463;&#20856;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;(EMA)&#31574;&#30053;&#21644;&#21518;&#26399;&#35757;&#32451;&#20013;&#25945;&#24072;-&#23398;&#29983;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#31561;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35757;&#32451;&#30340;&#27169;&#22411;&#31934;&#21270;(TMR)&#38454;&#27573;&#21644;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#34920;&#31034;&#20998;&#27495;(RD)&#31574;&#30053;&#65292;&#20197;&#35299;&#20915;&#32463;&#20856;EMA&#30340;&#23616;&#38480;&#24615;&#21644;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;&#25945;&#24072;-&#23398;&#29983;&#27169;&#22411;&#30340;TMR&#38454;&#27573;&#20248;&#21270;&#20102;&#36731;&#37327;&#32423;&#32553;&#25918;&#25805;&#20316;&#65292;&#20197;&#31934;&#21270;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#24182;&#38450;&#27490;&#36807;&#24230;&#25311;&#21512;&#25110;&#36951;&#24536;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#27169;&#24335;&#12290;&#21516;&#26102;&#65292;RD&#31574;&#30053;&#24110;&#21161;&#20445;&#25345;&#36825;&#20123;&#27169;&#22411;&#30340;&#24046;&#24322;&#65292;&#40723;&#21169;&#23398;&#29983;&#27169;&#22411;&#25506;&#32034;&#20114;&#34917;&#30340;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#32423;&#36830;&#22238;&#24402;&#26469;&#29983;&#25104;... (&#25688;&#35201;&#26410;&#23436;&#25972;&#25552;&#20379;)
&lt;/p&gt;
&lt;p&gt;
Semi-supervised object detection (SSOD) can incorporate limited labeled data and large amounts of unlabeled data to improve the performance and generalization of existing object detectors. Despite many advances, recent SSOD methods are still challenged by noisy/misleading pseudo-labels, classical exponential moving average (EMA) strategy, and the consensus of Teacher-Student models in the latter stages of training. This paper proposes a novel training-based model refinement (TMR) stage and a simple yet effective representation disagreement (RD) strategy to address the limitations of classical EMA and the consensus problem. The TMR stage of Teacher-Student models optimizes the lightweight scaling operation to refine the model's weights and prevent overfitting or forgetting learned patterns from unlabeled data. Meanwhile, the RD strategy helps keep these models diverged to encourage the student model to explore complementary representations. In addition, we use cascade regression to gene
&lt;/p&gt;</description></item><item><title>ACTI&#22312;EVALITA 2023&#20013;&#30340;&#38452;&#35851;&#35770;&#36776;&#35782;&#20219;&#21153;&#20849;&#26377;15&#25903;&#22242;&#38431;&#21442;&#19982;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21028;&#26029;&#38452;&#35851;&#20869;&#23481;&#21644;&#20998;&#31867;&#65292;&#24471;&#20986;&#20102;&#20851;&#20110;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#25269;&#21046;&#22312;&#22312;&#32447;&#24179;&#21488;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#30340;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2307.06954</link><description>&lt;p&gt;
ACTI&#22312;EVALITA 2023&#20013;&#30340;&#32508;&#36848;&#65306;&#38452;&#35851;&#35770;&#36776;&#35782;&#20219;&#21153;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
ACTI at EVALITA 2023: Overview of the Conspiracy Theory Identification Task. (arXiv:2307.06954v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06954
&lt;/p&gt;
&lt;p&gt;
ACTI&#22312;EVALITA 2023&#20013;&#30340;&#38452;&#35851;&#35770;&#36776;&#35782;&#20219;&#21153;&#20849;&#26377;15&#25903;&#22242;&#38431;&#21442;&#19982;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21028;&#26029;&#38452;&#35851;&#20869;&#23481;&#21644;&#20998;&#31867;&#65292;&#24471;&#20986;&#20102;&#20851;&#20110;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#25269;&#21046;&#22312;&#22312;&#32447;&#24179;&#21488;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38452;&#35851;&#35770;&#36776;&#35782;&#20219;&#21153;&#26159;Evalita 2023&#39318;&#27425;&#25552;&#20986;&#30340;&#26032;&#20849;&#20139;&#20219;&#21153;&#12290;ACTI&#25361;&#25112;&#20165;&#22522;&#20110;Telegram&#19978;&#30340;&#38452;&#35851;&#39057;&#36947;&#35780;&#35770;&#65292;&#20998;&#20026;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;(i) &#38452;&#35851;&#20869;&#23481;&#20998;&#31867;&#65306;&#36776;&#35782;&#38452;&#35851;&#20869;&#23481;&#21644;(ii) &#38452;&#35851;&#31867;&#21035;&#20998;&#31867;&#65306;&#38024;&#23545;&#29305;&#23450;&#38452;&#35851;&#29702;&#35770;&#20998;&#31867;&#12290;&#20849;&#26377;15&#25903;&#22242;&#38431;&#21442;&#19982;&#20102;&#35813;&#20219;&#21153;&#65292;&#24635;&#20849;&#25552;&#20132;&#20102;81&#20010;&#32467;&#26524;&#12290;&#25105;&#20204;&#35828;&#26126;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20851;&#20110;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#26469;&#25269;&#21046;&#22312;&#22312;&#32447;&#24179;&#21488;&#19978;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conspiracy Theory Identication task is a new shared task proposed for the first time at the Evalita 2023. The ACTI challenge, based exclusively on comments published on conspiratorial channels of telegram, is divided into two subtasks: (i) Conspiratorial Content Classification: identifying conspiratorial content and (ii) Conspiratorial Category Classification about specific conspiracy theory classification. A total of fifteen teams participated in the task for a total of 81 submissions. We illustrate the best performing approaches were based on the utilization of large language models. We finally draw conclusions about the utilization of these models for counteracting the spreading of misinformation in online platforms.
&lt;/p&gt;</description></item><item><title>IntelliGraphs&#26159;&#19968;&#32452;&#26032;&#30340;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#30693;&#35782;&#22270;&#35889;&#29983;&#25104;&#12290;&#20854;&#20013;&#21253;&#21547;&#20855;&#26377;&#36923;&#36753;&#35268;&#21017;&#34920;&#36798;&#30340;&#35821;&#20041;&#30340;&#23376;&#22270;&#65292;&#29992;&#20110;&#35780;&#20272;&#23376;&#22270;&#25512;&#26029;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.06698</link><description>&lt;p&gt;
IntelliGraphs: &#29992;&#20110;&#35780;&#20272;&#30693;&#35782;&#22270;&#35889;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
IntelliGraphs: Datasets for Benchmarking Knowledge Graph Generation. (arXiv:2307.06698v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06698
&lt;/p&gt;
&lt;p&gt;
IntelliGraphs&#26159;&#19968;&#32452;&#26032;&#30340;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#30693;&#35782;&#22270;&#35889;&#29983;&#25104;&#12290;&#20854;&#20013;&#21253;&#21547;&#20855;&#26377;&#36923;&#36753;&#35268;&#21017;&#34920;&#36798;&#30340;&#35821;&#20041;&#30340;&#23376;&#22270;&#65292;&#29992;&#20110;&#35780;&#20272;&#23376;&#22270;&#25512;&#26029;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGE&#65289;&#27169;&#22411;&#29992;&#20110;&#23398;&#20064;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#36830;&#32493;&#34920;&#31034;&#12290;&#25991;&#29486;&#20013;&#19968;&#20010;&#20851;&#38190;&#30340;&#20219;&#21153;&#26159;&#39044;&#27979;&#23454;&#20307;&#20043;&#38388;&#30340;&#32570;&#22833;&#38142;&#25509;&#12290;&#28982;&#32780;&#65292;&#30693;&#35782;&#22270;&#35889;&#19981;&#20165;&#20165;&#26159;&#38142;&#25509;&#30340;&#38598;&#21512;&#65292;&#36824;&#20855;&#26377;&#20854;&#32467;&#26500;&#20013;&#30340;&#35821;&#20041;&#12290;&#35821;&#20041;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20363;&#22914;&#26597;&#35810;&#22238;&#31572;&#25110;&#25512;&#29702;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#23376;&#22270;&#25512;&#26029;&#20219;&#21153;&#65292;&#20854;&#20013;&#19968;&#20010;&#27169;&#22411;&#24517;&#39035;&#29983;&#25104;&#21487;&#33021;&#30340;&#24182;&#19988;&#35821;&#20041;&#19978;&#26377;&#25928;&#30340;&#23376;&#22270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;IntelliGraphs&#65292;&#19968;&#20010;&#21253;&#21547;&#20116;&#20010;&#26032;&#30340;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;&#30340;&#38598;&#21512;&#12290;IntelliGraphs&#25968;&#25454;&#38598;&#21253;&#21547;&#20855;&#26377;&#36923;&#36753;&#35268;&#21017;&#34920;&#36798;&#30340;&#35821;&#20041;&#30340;&#23376;&#22270;&#65292;&#29992;&#20110;&#35780;&#20272;&#23376;&#22270;&#25512;&#26029;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#20135;&#29983;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#22235;&#20010;&#26032;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#22522;&#20110;&#20256;&#32479;KGE&#30340;&#19977;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#21040;&#35821;&#20041;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#19968;&#22522;&#20934;&#23558;&#20419;&#36827;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Embedding (KGE) models are used to learn continuous representations of entities and relations. A key task in the literature is predicting missing links between entities. However, Knowledge Graphs are not just sets of links but also have semantics underlying their structure. Semantics is crucial in several downstream tasks, such as query answering or reasoning. We introduce the subgraph inference task, where a model has to generate likely and semantically valid subgraphs. We propose IntelliGraphs, a set of five new Knowledge Graph datasets. The IntelliGraphs datasets contain subgraphs with semantics expressed in logical rules for evaluating subgraph inference. We also present the dataset generator that produced the synthetic datasets. We designed four novel baseline models, which include three models based on traditional KGEs. We evaluate their expressiveness and show that these models cannot capture the semantics. We believe this benchmark will encourage the development
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#26088;&#22312;&#36890;&#36807;&#20223;&#30495;&#33041;&#37096;&#25805;&#20316;&#26469;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#65292;&#20294;&#26159;&#22312;SNNs&#30340;&#39640;&#25928;&#30828;&#20214;&#21518;&#31471;&#35774;&#35745;&#19978;&#20173;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2306.15749</link><description>&lt;p&gt;
&#20309;&#21435;&#20309;&#20174;&#65306;&#28145;&#24230;&#23398;&#20064;&#21152;&#36895;&#30340;&#25968;&#23383;&#30828;&#20214;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
To Spike or Not To Spike: A Digital Hardware Perspective on Deep Learning Acceleration. (arXiv:2306.15749v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15749
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#26088;&#22312;&#36890;&#36807;&#20223;&#30495;&#33041;&#37096;&#25805;&#20316;&#26469;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#65292;&#20294;&#26159;&#22312;SNNs&#30340;&#39640;&#25928;&#30828;&#20214;&#21518;&#31471;&#35774;&#35745;&#19978;&#20173;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#22312;&#28085;&#30422;&#35745;&#31639;&#26426;&#35270;&#35273;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#21464;&#24471;&#36234;&#26469;&#36234;&#26377;&#31454;&#20105;&#21147;&#65307;&#28982;&#32780;&#65292;&#36825;&#26159;&#20197;&#25928;&#29575;&#20026;&#20195;&#20215;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#36234;&#26469;&#36234;&#22810;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#33021;&#21147;&#12290;&#29983;&#29289;&#33041;&#30340;&#21151;&#32791;&#25928;&#29575;&#36229;&#36807;&#20219;&#20309;&#22823;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#65307;&#22240;&#27492;&#65292;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#35797;&#22270;&#27169;&#20223;&#33041;&#37096;&#25805;&#20316;&#65292;&#20363;&#22914;&#22522;&#20110;&#33033;&#20914;&#30340;&#20449;&#24687;&#22788;&#29702;&#65292;&#20197;&#25552;&#39640;DL&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#23613;&#31649;&#33041;&#37096;&#26377;&#35832;&#22914;&#39640;&#25928;&#30340;&#20449;&#24687;&#20256;&#36755;&#12289;&#23494;&#38598;&#30340;&#31070;&#32463;&#20803;&#36830;&#25509;&#21644;&#35745;&#31639;&#19982;&#23384;&#20648;&#30340;&#20849;&#21516;&#20301;&#32622;&#31561;&#20248;&#21183;&#65292;&#20294;&#21487;&#29992;&#30340;&#29983;&#29289;&#22522;&#24213;&#20005;&#37325;&#38480;&#21046;&#20102;&#29983;&#29289;&#22823;&#33041;&#30340;&#36827;&#21270;&#12290;&#30005;&#23376;&#30828;&#20214;&#27809;&#26377;&#30456;&#21516;&#30340;&#32422;&#26463;&#65307;&#22240;&#27492;&#65292;&#34429;&#28982;&#24314;&#27169;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#21487;&#33021;&#25581;&#31034;&#20102;&#19968;&#20010;&#35868;&#39064;&#30340;&#19968;&#37096;&#20998;&#65292;&#20294;&#23545;&#20110;SNNs&#30340;&#39640;&#25928;&#30828;&#20214;&#21518;&#31471;&#35774;&#35745;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep learning models scale, they become increasingly competitive from domains spanning computer vision to natural language processing; however, this happens at the expense of efficiency since they require increasingly more memory and computing power. The power efficiency of the biological brain outperforms the one of any large-scale deep learning (DL) model; thus, neuromorphic computing tries to mimic the brain operations, such as spike-based information processing, to improve the efficiency of DL models. Despite the benefits of the brain, such as efficient information transmission, dense neuronal interconnects, and the co-location of computation and memory, the available biological substrate has severely constrained the evolution of biological brains. Electronic hardware does not have the same constraints; therefore, while modeling spiking neural networks (SNNs) might uncover one piece of the puzzle, the design of efficient hardware backends for SNNs needs further investigation, po
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#29616;&#20195;&#32422;&#26463;&#32534;&#31243;&#25945;&#32946;&#30340;&#21069;&#26223;&#65292;&#37325;&#28857;&#20851;&#27880;&#22312;&#32447;&#21644;&#34394;&#25311;&#35838;&#31243;&#65292;&#24182;&#24635;&#32467;&#20102;&#20052;&#27835;&#20122;&#29702;&#24037;&#23398;&#38498;&#30340;&#32422;&#26463;&#32534;&#31243;&#35838;&#31243;&#30340;&#37325;&#35201;&#25910;&#33719;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#25945;&#23398;&#21644;&#25512;&#24191;&#26041;&#27861;&#20197;&#21450;&#32452;&#32455;&#21464;&#38761;&#30340;&#24819;&#27861;&#65292;&#20197;&#20419;&#36827;&#32422;&#26463;&#32534;&#31243;&#25945;&#32946;&#30340;&#38271;&#26399;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.13676</link><description>&lt;p&gt;
&#29616;&#20195;&#32422;&#26463;&#32534;&#31243;&#25945;&#32946;&#65306;&#26410;&#26469;&#30340;&#25945;&#35757;
&lt;/p&gt;
&lt;p&gt;
Modern Constraint Programming Education: Lessons for the Future. (arXiv:2306.13676v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13676
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#29616;&#20195;&#32422;&#26463;&#32534;&#31243;&#25945;&#32946;&#30340;&#21069;&#26223;&#65292;&#37325;&#28857;&#20851;&#27880;&#22312;&#32447;&#21644;&#34394;&#25311;&#35838;&#31243;&#65292;&#24182;&#24635;&#32467;&#20102;&#20052;&#27835;&#20122;&#29702;&#24037;&#23398;&#38498;&#30340;&#32422;&#26463;&#32534;&#31243;&#35838;&#31243;&#30340;&#37325;&#35201;&#25910;&#33719;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#25945;&#23398;&#21644;&#25512;&#24191;&#26041;&#27861;&#20197;&#21450;&#32452;&#32455;&#21464;&#38761;&#30340;&#24819;&#27861;&#65292;&#20197;&#20419;&#36827;&#32422;&#26463;&#32534;&#31243;&#25945;&#32946;&#30340;&#38271;&#26399;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#19968;&#20301;&#32422;&#26463;&#32534;&#31243;&#25945;&#24072;&#30340;&#35270;&#35282;&#65292;&#35814;&#36848;&#20102;&#29616;&#20195;&#32422;&#26463;&#32534;&#31243;&#25945;&#32946;&#30340;&#21069;&#26223;&#12290;&#25991;&#31456;&#27010;&#36848;&#20102;&#24403;&#21069;&#32422;&#26463;&#32534;&#31243;&#35838;&#31243;&#21644;&#25945;&#23398;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#22312;&#32447;&#21644;&#34394;&#25311;&#35838;&#31243;&#12290;&#38543;&#21518;&#35752;&#35770;&#20102;&#20052;&#27835;&#20122;&#29702;&#24037;&#23398;&#38498;&#22312;&#32654;&#22269;&#20122;&#29305;&#20848;&#22823;&#25152;&#37319;&#21462;&#30340;&#38754;&#21521;&#24037;&#31243;&#23398;&#29983;&#30340;&#21019;&#26032;&#32422;&#26463;&#32534;&#31243;&#25945;&#32946;&#26041;&#27861;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#20052;&#27835;&#20122;&#29702;&#24037;&#23398;&#38498;&#30340;&#32422;&#26463;&#32534;&#31243;&#35838;&#31243;&#30340;&#37325;&#35201;&#25910;&#33719;&#65292;&#24182;&#25506;&#35752;&#20102;&#32422;&#26463;&#32534;&#31243;&#25945;&#32946;&#30340;&#26410;&#26469;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#25945;&#23398;&#21644;&#25512;&#24191;&#26041;&#27861;&#20197;&#21450;&#32452;&#32455;&#21464;&#38761;&#30340;&#24819;&#27861;&#65292;&#20197;&#20419;&#36827;&#32422;&#26463;&#32534;&#31243;&#25945;&#32946;&#30340;&#38271;&#26399;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper details an outlook on modern constraint programming (CP) education through the lens of a CP instructor. A general overview of current CP courses and instructional methods is presented, with a focus on online and virtually-delivered courses. This is followed by a discussion of the novel approach taken to introductory CP education for engineering students at large scale at the Georgia Institute of Technology (Georgia Tech) in Atlanta, GA, USA. The paper summarizes important takeaways from the Georgia Tech CP course and ends with a discussion on the future of CP education. Some ideas for instructional methods, promotional methods, and organizational changes are proposed to aid in the long-term growth of CP education.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#28436;&#31034;&#22914;&#20309;&#21033;&#29992;&#29616;&#26377;&#26041;&#27861;&#21644;&#36719;&#20214;&#24037;&#20855;&#32467;&#21512;&#23884;&#20837;&#25216;&#26415;&#35774;&#35745;&#38754;&#21521;&#31185;&#23398;&#39046;&#22495;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#35813;&#26426;&#22120;&#20154;&#33021;&#22815;&#22788;&#29702;&#31185;&#23398;&#25991;&#29486;&#65292;&#25552;&#20379;&#29305;&#23450;&#39046;&#22495;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#22312;&#21021;&#27493;&#30740;&#31350;&#36741;&#21161;&#30693;&#35782;&#26041;&#38754;&#20026;&#29289;&#29702;&#31185;&#23398;&#23478;&#25552;&#20379;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2306.10067</link><description>&lt;p&gt;
&#21033;&#29992;&#23884;&#20837;&#25216;&#26415;&#35774;&#35745;&#38754;&#21521;&#31185;&#23398;&#39046;&#22495;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Domain-specific ChatBots for Science using Embeddings. (arXiv:2306.10067v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#28436;&#31034;&#22914;&#20309;&#21033;&#29992;&#29616;&#26377;&#26041;&#27861;&#21644;&#36719;&#20214;&#24037;&#20855;&#32467;&#21512;&#23884;&#20837;&#25216;&#26415;&#35774;&#35745;&#38754;&#21521;&#31185;&#23398;&#39046;&#22495;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#35813;&#26426;&#22120;&#20154;&#33021;&#22815;&#22788;&#29702;&#31185;&#23398;&#25991;&#29486;&#65292;&#25552;&#20379;&#29305;&#23450;&#39046;&#22495;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#22312;&#21021;&#27493;&#30740;&#31350;&#36741;&#21161;&#30693;&#35782;&#26041;&#38754;&#20026;&#29289;&#29702;&#31185;&#23398;&#23478;&#25552;&#20379;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#24050;&#25104;&#20026;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#33021;&#22788;&#29702;&#22810;&#31181;&#20219;&#21153;&#12290;&#32463;&#35843;&#25972;&#30340;&#36825;&#20123;&#31995;&#32479;&#24050;&#34987;&#36716;&#21270;&#20026;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#33021;&#22238;&#31572;&#29992;&#25143;&#23545;&#24191;&#27867;&#35805;&#39064;&#30340;&#26597;&#35810;&#65292;&#25552;&#20379;&#20016;&#23500;&#30340;&#20449;&#24687;&#21644;&#21019;&#24847;&#22238;&#31572;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#22312;&#33258;&#28982;&#31185;&#23398;&#39046;&#22495;&#30340;&#30693;&#35782;&#20173;&#19981;&#23436;&#25972;&#65292;&#24182;&#19988;&#38754;&#20020;&#20005;&#26684;&#38656;&#27714;&#21644;&#26469;&#28304;&#26631;&#20934;&#65292;&#22240;&#27492;&#20854;&#22312;&#29289;&#29702;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#20173;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#25991;&#28436;&#31034;&#20102;&#22914;&#20309;&#36731;&#26494;&#22320;&#23558;&#29616;&#26377;&#26041;&#27861;&#21644;&#36719;&#20214;&#24037;&#20855;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#38754;&#21521;&#29305;&#23450;&#39046;&#22495;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#35813;&#31995;&#32479;&#33021;&#25509;&#21463;&#29616;&#26377;&#26684;&#24335;&#30340;&#31185;&#23398;&#25991;&#29486;&#65292;&#24182;&#20351;&#29992;&#25991;&#26412;&#23884;&#20837;&#26597;&#25214;&#26469;&#20026;LLM&#25552;&#20379;&#29305;&#23450;&#39046;&#22495;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#20415;&#22312;&#25776;&#20889;&#22238;&#31572;&#26102;&#20351;&#29992;&#12290;&#25105;&#20204;&#21516;&#26679;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#22270;&#20687;&#23884;&#20837;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#36328;&#20986;&#29256;&#29289;&#22270;&#29255;&#30340;&#25628;&#32034;&#21644;&#26816;&#32034;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25552;&#20379;&#21021;&#27493;&#30740;&#31350;&#36741;&#21161;&#30693;&#35782;&#26041;&#38754;&#65292;LLM&#24050;&#32463;&#36866;&#29992;&#20110;&#29289;&#29702;&#31185;&#23398;&#23478;&#30340;&#20351;&#29992;&#65292;&#24182;&#19988;&#36827;&#19968;&#27493;&#30340;&#24320;&#21457;&#21487;&#20197;&#25193;&#23637;&#36825;&#20123;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have emerged as powerful machine-learning systems capable of handling a myriad of tasks. Tuned versions of these systems have been turned into chatbots that can respond to user queries on a vast diversity of topics, providing informative and creative replies. However, their application to physical science research remains limited owing to their incomplete knowledge in these areas, contrasted with the needs of rigor and sourcing in science domains. Here, we demonstrate how existing methods and software tools can be easily combined to yield a domain-specific chatbot. The system ingests scientific documents in existing formats, and uses text embedding lookup to provide the LLM with domain-specific contextual information when composing its reply. We similarly demonstrate that existing image embedding methods can be used for search and retrieval across publication figures. These results confirm that LLMs are already suitable for use by physical scientists in acc
&lt;/p&gt;</description></item><item><title>SDR-GAIN&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#34892;&#20154;&#23039;&#24577;&#20013;&#37096;&#20998;&#36974;&#25377;&#38382;&#39064;&#30340;&#20851;&#38190;&#28857;&#34917;&#20840;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23545;&#19981;&#23436;&#25972;&#30340;&#20851;&#38190;&#28857;&#36827;&#34892;&#38477;&#32500;&#65292;&#32479;&#19968;&#29305;&#24449;&#20998;&#24067;&#65292;&#24182;&#20351;&#29992;GAN&#26694;&#26550;&#30340;&#20004;&#31181;&#29983;&#25104;&#27169;&#22411;&#26469;&#23436;&#25104;&#23039;&#24577;&#30340;&#34917;&#20840;&#12290;&#35813;&#26041;&#27861;&#30340;&#23454;&#39564;&#34920;&#26126;&#24615;&#33021;&#20248;&#20110;&#22522;&#26412;&#30340;GAIN&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2306.03538</link><description>&lt;p&gt;
SDR-GAIN&#65306;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#39640;&#23454;&#26102;&#36974;&#25377;&#34892;&#20154;&#23039;&#24577;&#23436;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SDR-GAIN: A High Real-Time Occluded Pedestrian Pose Completion Method for Autonomous Driving. (arXiv:2306.03538v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03538
&lt;/p&gt;
&lt;p&gt;
SDR-GAIN&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#34892;&#20154;&#23039;&#24577;&#20013;&#37096;&#20998;&#36974;&#25377;&#38382;&#39064;&#30340;&#20851;&#38190;&#28857;&#34917;&#20840;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23545;&#19981;&#23436;&#25972;&#30340;&#20851;&#38190;&#28857;&#36827;&#34892;&#38477;&#32500;&#65292;&#32479;&#19968;&#29305;&#24449;&#20998;&#24067;&#65292;&#24182;&#20351;&#29992;GAN&#26694;&#26550;&#30340;&#20004;&#31181;&#29983;&#25104;&#27169;&#22411;&#26469;&#23436;&#25104;&#23039;&#24577;&#30340;&#34917;&#20840;&#12290;&#35813;&#26041;&#27861;&#30340;&#23454;&#39564;&#34920;&#26126;&#24615;&#33021;&#20248;&#20110;&#22522;&#26412;&#30340;GAIN&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#32531;&#35299;&#22522;&#20110;&#20154;&#20307;&#23039;&#24577;&#20851;&#38190;&#28857;&#30340;&#34892;&#20154;&#26816;&#27979;&#31639;&#27861;&#20013;&#37096;&#20998;&#36974;&#25377;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20998;&#31163;&#21644;&#38477;&#32500;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#24615;&#34917;&#20840;&#32593;&#32476;(SDR-GAIN)&#30340;&#26032;&#22411;&#34892;&#20154;&#23039;&#21183;&#20851;&#38190;&#28857;&#34917;&#20840;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;OpenPose&#22312;&#22270;&#20687;&#20013;&#20272;&#35745;&#34892;&#20154;&#30340;&#23039;&#24577;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#30001;&#20110;&#36974;&#25377;&#25110;&#20854;&#20182;&#22240;&#32032;&#32780;&#19981;&#23436;&#25972;&#30340;&#34892;&#20154;&#22836;&#37096;&#21644;&#36527;&#24178;&#20851;&#38190;&#28857;&#36827;&#34892;&#32500;&#24230;&#32553;&#20943;&#65292;&#20197;&#22686;&#24378;&#29305;&#24449;&#24182;&#36827;&#19968;&#27493;&#32479;&#19968;&#29305;&#24449;&#20998;&#24067;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GAN)&#26694;&#26550;&#30340;&#20004;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#34701;&#21512;&#20102;Huber&#25439;&#22833;&#12289;&#27531;&#24046;&#32467;&#26500;&#21644;L1&#27491;&#21017;&#21270;&#26469;&#29983;&#25104;&#37096;&#20998;&#36974;&#25377;&#34892;&#20154;&#19981;&#23436;&#25972;&#22836;&#37096;&#21644;&#36527;&#24178;&#23039;&#24577;&#20851;&#38190;&#28857;&#30340;&#32570;&#22833;&#37096;&#20998;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23039;&#24577;&#34917;&#20840;&#12290;&#25105;&#20204;&#22312;MS COCO&#21644;JAAD&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SDR-GAIN&#30340;&#24615;&#33021;&#20248;&#20110;&#22522;&#26412;&#30340;GAIN&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
To mitigate the challenges arising from partial occlusion in human pose keypoint based pedestrian detection methods , we present a novel pedestrian pose keypoint completion method called the separation and dimensionality reduction-based generative adversarial imputation networks (SDR-GAIN) . Firstly, we utilize OpenPose to estimate pedestrian poses in images. Then, we isolate the head and torso keypoints of pedestrians with incomplete keypoints due to occlusion or other factors and perform dimensionality reduction to enhance features and further unify feature distribution. Finally, we introduce two generative models based on the generative adversarial networks (GAN) framework, which incorporate Huber loss, residual structure, and L1 regularization to generate missing parts of the incomplete head and torso pose keypoints of partially occluded pedestrians, resulting in pose completion. Our experiments on MS COCO and JAAD datasets demonstrate that SDR-GAIN outperforms basic GAIN framework
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#33258;&#21160;&#39550;&#39542;&#20013;RL&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#21152;&#39118;&#38505;&#24863;&#30693;&#30340;&#22870;&#21169;&#24418;&#25104;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#35757;&#32451;&#21644;&#27979;&#35797;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#39069;&#22806;&#30340;&#37325;&#22609;&#22870;&#21169;&#39033;&#26469;&#40723;&#21169;&#25506;&#32034;&#24182;&#24809;&#32602;&#39118;&#38505;&#39550;&#39542;&#34892;&#20026;&#65292;&#35777;&#26126;&#20854;&#22312;&#21508;&#31181;RL&#20195;&#29702;&#20013;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.03220</link><description>&lt;p&gt;
&#38754;&#21521;&#33258;&#21160;&#39550;&#39542;&#30340;&#39118;&#38505;&#24863;&#30693;&#22870;&#21169;&#24418;&#25104;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Risk-Aware Reward Shaping of Reinforcement Learning Agents for Autonomous Driving. (arXiv:2306.03220v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#33258;&#21160;&#39550;&#39542;&#20013;RL&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#21152;&#39118;&#38505;&#24863;&#30693;&#30340;&#22870;&#21169;&#24418;&#25104;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#35757;&#32451;&#21644;&#27979;&#35797;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#39069;&#22806;&#30340;&#37325;&#22609;&#22870;&#21169;&#39033;&#26469;&#40723;&#21169;&#25506;&#32034;&#24182;&#24809;&#32602;&#39118;&#38505;&#39550;&#39542;&#34892;&#20026;&#65292;&#35777;&#26126;&#20854;&#22312;&#21508;&#31181;RL&#20195;&#29702;&#20013;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#33258;&#21160;&#39550;&#39542;&#20013;&#26377;&#25928;&#30340;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#25968;&#25454;&#33258;&#21160;&#23398;&#20064;&#26368;&#20248;&#39550;&#39542;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;RL&#20195;&#29702;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20854;&#23545;&#20110;&#24615;&#33021;&#30340;&#24433;&#21709;&#24456;&#22823;&#65292;&#20294;&#26159;&#24456;&#38590;&#30830;&#23450;&#12290;&#20256;&#32479;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#23433;&#20840;&#39550;&#39542;&#29366;&#24577;&#30340;&#22870;&#21169;&#65292;&#20294;&#24182;&#26410;&#32435;&#20837;&#36710;&#36742;&#39118;&#38505;&#39550;&#39542;&#34892;&#20026;&#30340;&#24863;&#30693;&#12290;&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#20351;&#29992;&#39118;&#38505;&#24863;&#30693;&#30340;&#22870;&#21169;&#24418;&#25104;&#26469;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#20013;RL&#20195;&#29702;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#24615;&#33021;&#12290;&#26681;&#25454;&#23454;&#36341;&#20013;&#35268;&#23450;&#30340;&#19968;&#33324;&#33258;&#21160;&#39550;&#39542;&#30340;&#23433;&#20840;&#35201;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39069;&#22806;&#30340;&#37325;&#22609;&#22870;&#21169;&#39033;&#65292;&#20197;&#40723;&#21169;&#25506;&#32034;&#24182;&#24809;&#32602;&#39118;&#38505;&#39550;&#39542;&#34892;&#20026;&#12290; OpenAI Gym&#20013;&#30340;&#27169;&#25311;&#30740;&#31350;&#34920;&#26126;&#20102;&#39118;&#38505;&#24863;&#30693;&#22870;&#21169;&#24418;&#25104;&#22312;&#21508;&#31181;RL&#20195;&#29702;&#20013;&#30340;&#20248;&#21183;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25351;&#20986;&#20195;&#29702;&#36716;&#31227;&#30340;&#26041;&#24335;&#23545;&#39118;&#38505;&#24863;&#30693;&#22870;&#21169;&#24418;&#25104;&#24433;&#21709;&#30340;&#29616;&#23454;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) is an effective approach to motion planning in autonomous driving, where an optimal driving policy can be automatically learned using the interaction data with the environment. Nevertheless, the reward function for an RL agent, which is significant to its performance, is challenging to be determined. The conventional work mainly focuses on rewarding safe driving states but does not incorporate the awareness of risky driving behaviors of the vehicles. In this paper, we investigate how to use risk-aware reward shaping to leverage the training and test performance of RL agents in autonomous driving. Based on the essential requirements that prescribe the safety specifications for general autonomous driving in practice, we propose additional reshaped reward terms that encourage exploration and penalize risky driving behaviors. A simulation study in OpenAI Gym indicates the advantage of risk-aware reward shaping for various RL agents. Also, we point out that proxi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#20010;&#21517;&#20026;SpeechGen&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#31034;&#35843;&#33410;&#65292;&#35299;&#38145;&#20102;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#30452;&#25509;&#36866;&#24212;&#36830;&#32493;&#35821;&#38899;&#21040;&#31163;&#25955;&#26631;&#35760;&#30340;&#20219;&#21153;&#65292;&#20351;&#24471;&#35821;&#38899;&#29983;&#25104;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.02207</link><description>&lt;p&gt;
SpeechGen: &#21033;&#29992;&#25552;&#31034;&#35299;&#38145;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
SpeechGen: Unlocking the Generative Power of Speech Language Models with Prompts. (arXiv:2306.02207v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#20010;&#21517;&#20026;SpeechGen&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#31034;&#35843;&#33410;&#65292;&#35299;&#38145;&#20102;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#30452;&#25509;&#36866;&#24212;&#36830;&#32493;&#35821;&#38899;&#21040;&#31163;&#25955;&#26631;&#35760;&#30340;&#20219;&#21153;&#65292;&#20351;&#24471;&#35821;&#38899;&#29983;&#25104;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#20013;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;ChatGPT&#30340;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#23558;&#36830;&#32493;&#35821;&#38899;&#30452;&#25509;&#36866;&#24212;&#20110;&#22788;&#29702;&#31163;&#25955;&#26631;&#35760;&#30340;LLM&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#65292;&#36825;&#22952;&#30861;&#20102;LLM&#22312;&#35821;&#38899;&#29983;&#25104;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#39640;&#32423;&#35821;&#38899;LM&#20204;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#35821;&#38899;&#20449;&#21495;&#25152;&#21253;&#21547;&#30340;&#20016;&#23500;&#20449;&#24687;&#65292;&#21253;&#25324;&#35828;&#35805;&#32773;&#21644;&#24773;&#24863;&#31561;&#65292;&#36825;&#20123;&#20449;&#24687;&#20165;&#36890;&#36807;&#25991;&#26412;&#25968;&#25454;&#26080;&#27861;&#33719;&#21462;&#12290;&#22312;&#19968;&#20123;&#35821;&#38899;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#31616;&#21333;&#30340;&#25552;&#31034;&#35843;&#25972;&#24050;&#32463;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#21442;&#25968;&#25928;&#29575;&#21644;&#31454;&#20105;&#24615;&#33021;&#30340;&#25552;&#39640;&#12290;&#20294;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#25552;&#31034;&#33021;&#22815;&#26377;&#25928;&#22320;&#28608;&#21457;&#35821;&#38899;LM&#30340;&#29983;&#25104;&#20219;&#21153;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#30693;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#20808;&#39537;&#24615;&#30740;&#31350;&#65292;&#35813;&#30740;&#31350;&#22312;&#31216;&#20026;SpeechGen&#30340;&#32479;&#19968;&#26694;&#26550;&#20013;&#20351;&#29992;&#25552;&#31034;&#35843;&#33410;&#26469;&#21050;&#28608;&#35821;&#38899;LM&#36827;&#34892;&#21508;&#31181;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#20855;&#26377;&#32422;10M&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have gained considerable attention for Artificial Intelligence Generated Content (AIGC), particularly with the emergence of ChatGPT. However, the direct adaptation of continuous speech to LLMs that process discrete tokens remains an unsolved challenge, hindering the application of LLMs for speech generation. The advanced speech LMs are in the corner, as that speech signals encapsulate a wealth of information, including speaker and emotion, beyond textual data alone. Prompt tuning has demonstrated notable gains in parameter efficiency and competitive performance on some speech classification tasks. However, the extent to which prompts can effectively elicit generation tasks from speech LMs remains an open question. In this paper, we present pioneering research that explores the application of prompt tuning to stimulate speech LMs for various generation tasks, within a unified framework called SpeechGen, with around 10M trainable parameters. The proposed unif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#30340;&#34987;&#24573;&#35270;&#30340;&#25928;&#29575;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#25506;&#35752;16&#20301;&#21644;32&#20301;&#27169;&#22411;&#30340;&#24046;&#24322;&#65292;&#24182;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;16&#20301;&#27169;&#22411;&#19982;&#20854;32&#20301;&#23545;&#24212;&#29289;&#20043;&#38388;&#30340;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2305.10947</link><description>&lt;p&gt;
&#20851;&#20110;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#30340;&#36777;&#25252;
&lt;/p&gt;
&lt;p&gt;
In Defense of Pure 16-bit Floating-Point Neural Networks. (arXiv:2305.10947v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#30340;&#34987;&#24573;&#35270;&#30340;&#25928;&#29575;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#25506;&#35752;16&#20301;&#21644;32&#20301;&#27169;&#22411;&#30340;&#24046;&#24322;&#65292;&#24182;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;16&#20301;&#27169;&#22411;&#19982;&#20854;32&#20301;&#23545;&#24212;&#29289;&#20043;&#38388;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#23569;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#21644;&#28608;&#27963;&#25152;&#38656;&#30340;&#20301;&#25968;&#26159;&#38750;&#24120;&#21487;&#21462;&#30340;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#21152;&#24555;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#12290;&#22240;&#27492;&#65292;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20197;&#24320;&#21457;&#21033;&#29992;&#26356;&#20302;&#31934;&#24230;&#35745;&#31639;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#27604;&#22914;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#30446;&#21069;&#19981;&#23384;&#22312;&#32431;16&#20301;&#28014;&#28857;&#35774;&#32622;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#34987;&#24573;&#35270;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#26469;&#25506;&#35752;&#36896;&#25104;16&#20301;&#21644;32&#20301;&#27169;&#22411;&#30340;&#24046;&#24322;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#35268;&#33539;&#21270;&#20102;&#28014;&#28857;&#35823;&#24046;&#21644;&#23481;&#24525;&#24230;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;16&#20301;&#27169;&#22411;&#19982;&#20854;32&#20301;&#23545;&#24212;&#29289;&#20043;&#38388;&#23494;&#20999;&#36924;&#36817;&#32467;&#26524;&#30340;&#26465;&#20214;&#12290;&#36825;&#31181;&#29702;&#35770;&#25506;&#32034;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reducing the number of bits needed to encode the weights and activations of neural networks is highly desirable as it speeds up their training and inference time while reducing memory consumption. For these reasons, research in this area has attracted significant attention toward developing neural networks that leverage lower-precision computing, such as mixed-precision training. Interestingly, none of the existing approaches has investigated pure 16-bit floating-point settings. In this paper, we shed light on the overlooked efficiency of pure 16-bit floating-point neural networks. As such, we provide a comprehensive theoretical analysis to investigate the factors contributing to the differences observed between 16-bit and 32-bit models. We formalize the concepts of floating-point error and tolerance, enabling us to quantitatively explain the conditions under which a 16-bit model can closely approximate the results of its 32-bit counterpart. This theoretical exploration offers perspect
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#21464;&#21387;&#22120;&#30340;&#23545;&#27604;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#38024;&#23545;&#24320;&#25918;&#35789;&#27719;&#30340;&#29289;&#20307;&#26816;&#27979;&#20219;&#21153;&#65292;&#37319;&#29992;&#21306;&#22495;&#24863;&#30693;&#39044;&#35757;&#32451;&#12289;&#32858;&#28966;&#25439;&#22833;&#21644;&#26032;&#39062;&#29289;&#20307;&#25552;&#26696;&#31561;&#25216;&#26415;&#65292;&#22312;LVIS&#19978;&#21462;&#24471;&#20102;32.1$AP_r$&#30340;&#26368;&#20339;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.07011</link><description>&lt;p&gt;
&#21306;&#22495;&#24863;&#30693;&#39044;&#35757;&#32451;&#65306;&#35270;&#35273;&#21464;&#21387;&#22120;&#19979;&#30340;&#24320;&#25918;&#35789;&#27719;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers. (arXiv:2305.07011v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#21464;&#21387;&#22120;&#30340;&#23545;&#27604;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#38024;&#23545;&#24320;&#25918;&#35789;&#27719;&#30340;&#29289;&#20307;&#26816;&#27979;&#20219;&#21153;&#65292;&#37319;&#29992;&#21306;&#22495;&#24863;&#30693;&#39044;&#35757;&#32451;&#12289;&#32858;&#28966;&#25439;&#22833;&#21644;&#26032;&#39062;&#29289;&#20307;&#25552;&#26696;&#31561;&#25216;&#26415;&#65292;&#22312;LVIS&#19978;&#21462;&#24471;&#20102;32.1$AP_r$&#30340;&#26368;&#20339;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21306;&#22495;&#24863;&#30693;&#24320;&#25918;&#35789;&#27719;&#35270;&#35273;&#21464;&#21387;&#22120;&#65288;RO-ViT&#65289;&#65292;&#19968;&#31181;&#23545;&#27604;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#22635;&#34917;&#22270;&#20687;&#32423;&#39044;&#35757;&#32451;&#21644;&#24320;&#25918;&#35789;&#27719;&#29289;&#20307;&#26816;&#27979;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#24314;&#35758;&#38543;&#26426;&#35009;&#21098;&#24182;&#35843;&#25972;&#20301;&#32622;&#23884;&#20837;&#30340;&#21306;&#22495;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#25972;&#20010;&#22270;&#20687;&#20301;&#32622;&#23884;&#20837;&#12290;&#36825;&#26356;&#22909;&#22320;&#21305;&#37197;&#20102;&#26816;&#27979;&#24494;&#35843;&#38454;&#27573;&#20013;&#21306;&#22495;&#32423;&#21035;&#19978;&#20351;&#29992;&#20301;&#32622;&#23884;&#20837;&#30340;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#29992;&#32858;&#28966;&#25439;&#22833;&#26367;&#25442;&#20102;&#23545;&#27604;&#23398;&#20064;&#20013;&#24120;&#29992;&#30340;softmax&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#37027;&#20123;&#26377;&#20449;&#24687;&#37327;&#20294;&#38590;&#20197;&#25429;&#25417;&#30340;&#20363;&#23376;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#26368;&#36817;&#22312;&#26032;&#39062;&#29289;&#20307;&#25552;&#26696;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#20197;&#25913;&#36827;&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#22312;LVIS&#21644;COCO&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;&#23436;&#25972;&#27169;&#22411;&#21644;&#38646;-shot&#36716;&#31227;&#24615;&#33021;&#12290;RO-ViT&#22312;LVIS&#19978;&#23454;&#29616;&#20102;32.1$AP_r$&#30340;&#26368;&#20339;&#25928;&#26524;&#65292;&#36229;&#36807;&#29616;&#26377;&#26368;&#20339;&#26041;&#27861;5.8&#20010;&#30334;&#20998;&#28857;&#65292;&#21516;&#26102;&#36824;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#38646;-shot&#36716;&#31227;&#26816;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Region-aware Open-vocabulary Vision Transformers (RO-ViT) - a contrastive image-text pretraining recipe to bridge the gap between image-level pretraining and open-vocabulary object detection. At the pretraining phase, we propose to randomly crop and resize regions of positional embeddings instead of using the whole image positional embeddings. This better matches the use of positional embeddings at region-level in the detection finetuning phase. In addition, we replace the common softmax cross entropy loss in contrastive learning with focal loss to better learn the informative yet difficult examples. Finally, we leverage recent advances in novel object proposals to improve open-vocabulary detection finetuning. We evaluate our full model on the LVIS and COCO open-vocabulary detection benchmarks and zero-shot transfer. RO-ViT achieves a state-of-the-art 32.1 $AP_r$ on LVIS, surpassing the best existing approach by +5.8 points in addition to competitive zero-shot transfer detec
&lt;/p&gt;</description></item><item><title>CryCeleb&#26159;&#19968;&#20010;&#22522;&#20110;&#23156;&#20799;&#21741;&#22768;&#30340;&#35828;&#35805;&#20154;&#35748;&#35777;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36229;&#36807;6&#23567;&#26102;&#30340;&#25163;&#21160;&#20998;&#21106;&#21741;&#22768;&#65292;&#21487;&#29992;&#20110;&#30740;&#31350;&#23156;&#20799;&#21741;&#22768;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2305.00969</link><description>&lt;p&gt;
CryCeleb: &#22522;&#20110;&#23156;&#20799;&#21741;&#22768;&#30340;&#35828;&#35805;&#20154;&#35748;&#35777;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CryCeleb: A Speaker Verification Dataset Based on Infant Cry Sounds. (arXiv:2305.00969v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00969
&lt;/p&gt;
&lt;p&gt;
CryCeleb&#26159;&#19968;&#20010;&#22522;&#20110;&#23156;&#20799;&#21741;&#22768;&#30340;&#35828;&#35805;&#20154;&#35748;&#35777;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36229;&#36807;6&#23567;&#26102;&#30340;&#25163;&#21160;&#20998;&#21106;&#21741;&#22768;&#65292;&#21487;&#29992;&#20110;&#30740;&#31350;&#23156;&#20799;&#21741;&#22768;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;Ubenwa CryCeleb&#25968;&#25454;&#38598;&#8212;&#8212;&#19968;&#20010;&#26631;&#35760;&#30340;&#23156;&#20799;&#21741;&#22768;&#25910;&#38598;&#65292;&#20197;&#21450;&#38468;&#24102;&#30340;CryCeleb 2023&#20219;&#21153;&#8212;&#8212;&#19968;&#20010;&#22522;&#20110;&#23156;&#20799;&#21741;&#22768;&#30340;&#20844;&#20849;&#35828;&#35805;&#20154;&#39564;&#35777;&#25361;&#25112;&#12290;&#25105;&#20204;&#37322;&#25918;&#20986;786&#21517;&#26032;&#29983;&#20799;&#36229;&#36807;6&#23567;&#26102;&#30340;&#25163;&#21160;&#20998;&#21106;&#21741;&#22768;&#65292;&#20197;&#40723;&#21169;&#23156;&#20799;&#21741;&#22768;&#20998;&#26512;&#26041;&#38754;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes the Ubenwa CryCeleb dataset - a labeled collection of infant cries, and the accompanying CryCeleb 2023 task - a public speaker verification challenge based on infant cry sounds. We release for academic usage more than 6 hours of manually segmented cry sounds from 786 newborns to encourage research in infant cry analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#26377;&#38556;&#30861;&#29289;&#30340;&#29615;&#22659;&#20013;&#36890;&#36807;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36335;&#24452;&#35268;&#21010;&#20248;&#21270;&#65292;&#20174;&#32780;&#20943;&#23569;&#33021;&#37327;&#28040;&#32791;&#21644;&#20154;&#21147;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2303.17655</link><description>&lt;p&gt;
&#22522;&#20110;Q&#23398;&#20064;&#30340;&#26080;&#20154;&#26426;&#38598;&#32676;&#38556;&#30861;&#29289;&#36335;&#24452;&#35268;&#21010;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Q-learning Based System for Path Planning with UAV Swarms in Obstacle Environments. (arXiv:2303.17655v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#26377;&#38556;&#30861;&#29289;&#30340;&#29615;&#22659;&#20013;&#36890;&#36807;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36335;&#24452;&#35268;&#21010;&#20248;&#21270;&#65292;&#20174;&#32780;&#20943;&#23569;&#33021;&#37327;&#28040;&#32791;&#21644;&#20154;&#21147;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26080;&#20154;&#26426;&#38598;&#32676;&#30340;&#33258;&#20027;&#25511;&#21046;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#65292;&#38754;&#23545;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#38556;&#30861;&#29289;&#65292;&#36335;&#24452;&#35268;&#21010;&#23545;&#20110;&#20248;&#21270;&#33021;&#37327;&#28040;&#32791;&#21644;&#20943;&#23569;&#20154;&#21147;&#25104;&#26412;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#19981;&#26029;&#35843;&#25972;&#23398;&#20064;&#65292;&#23454;&#29616;&#22312;&#26377;&#38556;&#30861;&#29289;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#36335;&#24452;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Path Planning methods for autonomous control of Unmanned Aerial Vehicle (UAV) swarms are on the rise because of all the advantages they bring. There are more and more scenarios where autonomous control of multiple UAVs is required. Most of these scenarios present a large number of obstacles, such as power lines or trees. If all UAVs can be operated autonomously, personnel expenses can be decreased. In addition, if their flight paths are optimal, energy consumption is reduced. This ensures that more battery time is left for other operations. In this paper, a Reinforcement Learning based system is proposed for solving this problem in environments with obstacles by making use of Q-Learning. This method allows a model, in this particular case an Artificial Neural Network, to self-adjust by learning from its mistakes and achievements. Regardless of the size of the map or the number of UAVs in the swarm, the goal of these paths is to ensure complete coverage of an area with fixed obstacles f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ViewRefer&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#35270;&#35282;&#30340;&#19977;&#32500;&#35270;&#35273;&#23450;&#20301;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#35270;&#35282;&#21407;&#22411;&#65292;&#20174;&#25991;&#26412;&#21644;3D&#27169;&#24577;&#20013;&#33719;&#21462;&#35270;&#35282;&#30693;&#35782;&#24182;&#22686;&#24378;&#26694;&#26550;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.16894</link><description>&lt;p&gt;
ViewRefer: &#22522;&#20110;GPT&#21644;&#26679;&#20363;&#24341;&#23548;&#30340;&#22810;&#35270;&#35282;&#30693;&#35782;&#22788;&#29702;&#30340;&#19977;&#32500;&#35270;&#35273;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with GPT and Prototype Guidance. (arXiv:2303.16894v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ViewRefer&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#35270;&#35282;&#30340;&#19977;&#32500;&#35270;&#35273;&#23450;&#20301;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#35270;&#35282;&#21407;&#22411;&#65292;&#20174;&#25991;&#26412;&#21644;3D&#27169;&#24577;&#20013;&#33719;&#21462;&#35270;&#35282;&#30693;&#35782;&#24182;&#22686;&#24378;&#26694;&#26550;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22810;&#35270;&#35282;&#36755;&#20837;&#30340;3D&#22330;&#26223;&#65292;&#21487;&#20197;&#32531;&#35299;3D&#35270;&#35273;&#23450;&#20301;&#20013;&#30340;&#35270;&#35282;&#24046;&#24322;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#24573;&#30053;&#20102;&#23884;&#20837;&#22312;&#25991;&#26412;&#27169;&#24577;&#20013;&#30340;&#35270;&#35282;&#32447;&#32034;&#65292;&#24182;&#19988;&#26410;&#33021;&#26435;&#34913;&#19981;&#21516;&#35270;&#22270;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ViewRefer&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#35270;&#35282;&#30340;&#19977;&#32500;&#35270;&#35273;&#23450;&#20301;&#26694;&#26550;&#65292;&#25506;&#32034;&#22914;&#20309;&#20174;&#25991;&#26412;&#21644;3D&#27169;&#24577;&#20013;&#33719;&#21462;&#35270;&#35282;&#30693;&#35782;&#12290;&#20854;&#20013;&#65292;ViewRefer&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT&#65289;&#30340;&#22810;&#26679;&#21270;&#35821;&#35328;&#30693;&#35782;&#65292;&#23558;&#21333;&#19968;&#30340;&#23450;&#20301;&#25991;&#26412;&#25193;&#23637;&#20026;&#22810;&#20010;&#20960;&#20309;&#19968;&#33268;&#30340;&#25551;&#36848;&#65307;&#21516;&#26102;&#65292;&#22312;3D&#27169;&#24577;&#20013;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;Transformer&#30340;&#34701;&#21512;&#27169;&#22359;&#21644;&#35270;&#22270;&#38388;&#27880;&#24847;&#21147;&#65292;&#20197;&#22686;&#24378;&#35270;&#22270;&#20043;&#38388;&#29289;&#20307;&#30340;&#20132;&#20114;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#32452;&#21487;&#23398;&#20064;&#30340;&#22810;&#35270;&#35282;&#21407;&#22411;&#65292;&#29992;&#20110;&#35760;&#24518;&#19981;&#21516;&#35270;&#35282;&#19979;&#30340;&#22330;&#26223;&#26080;&#20851;&#30693;&#35782;&#65292;&#20174;&#20004;&#20010;&#26041;&#38754;&#22686;&#24378;&#20102;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding 3D scenes from multi-view inputs has been proven to alleviate the view discrepancy issue in 3D visual grounding. However, existing methods normally neglect the view cues embedded in the text modality and fail to weigh the relative importance of different views. In this paper, we propose ViewRefer, a multi-view framework for 3D visual grounding exploring how to grasp the view knowledge from both text and 3D modalities. For the text branch, ViewRefer leverages the diverse linguistic knowledge of large-scale language models, e.g., GPT, to expand a single grounding text to multiple geometry-consistent descriptions. Meanwhile, in the 3D modality, a transformer fusion module with inter-view attention is introduced to boost the interaction of objects across views. On top of that, we further present a set of learnable multi-view prototypes, which memorize scene-agnostic knowledge for different views, and enhance the framework from two perspectives: a view-guided attention module 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20272;&#35745;&#21644;&#32500;&#25252;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#36755;&#20837;-&#36755;&#20986;&#23646;&#24615;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#19981;&#20165;&#32771;&#34385;&#30452;&#25509;&#24433;&#21709;&#65292;&#36824;&#33021;&#32771;&#34385;&#38388;&#25509;&#24433;&#21709;&#12290;&#27492;&#26041;&#27861;&#33021;&#22815;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#21516;&#26102;&#26377;&#25928;&#22320;&#37327;&#21270;&#22240;&#26524;&#24402;&#22240;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#22240;&#26524;&#24402;&#22240;&#12290;</title><link>http://arxiv.org/abs/2303.13850</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22240;&#26524;&#24402;&#22240;&#23398;&#20064;&#65306;&#36229;&#36234;&#30452;&#25509;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Learning Causal Attributions in Neural Networks: Beyond Direct Effects. (arXiv:2303.13850v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13850
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20272;&#35745;&#21644;&#32500;&#25252;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#36755;&#20837;-&#36755;&#20986;&#23646;&#24615;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#19981;&#20165;&#32771;&#34385;&#30452;&#25509;&#24433;&#21709;&#65292;&#36824;&#33021;&#32771;&#34385;&#38388;&#25509;&#24433;&#21709;&#12290;&#27492;&#26041;&#27861;&#33021;&#22815;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#21516;&#26102;&#26377;&#25928;&#22320;&#37327;&#21270;&#22240;&#26524;&#24402;&#22240;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#22240;&#26524;&#24402;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25429;&#25417;&#21644;&#32500;&#25252;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#22240;&#26524;&#26041;&#27861;&#20272;&#35745;&#21644;&#32500;&#25252;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#30340;&#36755;&#20837;-&#36755;&#20986;&#23646;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20165;&#20551;&#35774;&#36755;&#20837;&#21464;&#37327;&#29420;&#31435;&#65288;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65289;&#65292;&#22240;&#27492;&#20165;&#30740;&#31350;&#30452;&#25509;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#31070;&#32463;&#32593;&#32476;&#35270;&#20026;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#22312;&#36755;&#20837;&#29305;&#24449;&#20013;&#24341;&#20837;&#36793;&#32536;&#20197;&#25429;&#25417;&#21644;&#32500;&#25252;&#30452;&#25509;&#21644;&#38388;&#25509;&#22240;&#26524;&#25928;&#24212;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#26377;&#25928;&#30340;&#36817;&#20284;&#31574;&#30053;&#26469;&#37327;&#21270;&#39640;&#32500;&#25968;&#25454;&#30340;&#22240;&#26524;&#24402;&#22240;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#21508;&#31181;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23398;&#20064;&#20102;&#25509;&#36817;&#22522;&#26412;&#20107;&#23454;&#25928;&#26524;&#30340;&#30452;&#25509;&#21644;&#38388;&#25509;&#22240;&#26524;&#24402;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been a growing interest in capturing and maintaining causal relationships in Neural Network (NN) models in recent years. We study causal approaches to estimate and maintain input-output attributions in NN models in this work. In particular, existing efforts in this direction assume independence among input variables (by virtue of the NN architecture), and hence study only direct causal effects. Viewing an NN as a structural causal model (SCM), we instead focus on going beyond direct effects, introduce edges among input features, and provide a simple yet effective methodology to capture and maintain direct and indirect causal effects while training an NN model. We also propose effective approximation strategies to quantify causal attributions in high dimensional data. Our wide range of experiments on synthetic and real-world datasets show that the proposed ante-hoc method learns causal attributions for both direct and indirect causal effects close to the ground truth effects.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;PDSketch&#35821;&#35328;&#21644;&#21487;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#21644;&#22312;&#32447;&#35268;&#21010;&#65292;&#21152;&#36895;&#20102;&#26426;&#22120;&#20154;&#30340;&#28789;&#27963;&#24615;&#21644;&#36890;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.05501</link><description>&lt;p&gt;
PDSketch: &#38598;&#25104;&#35268;&#21010;&#39046;&#22495;&#32534;&#31243;&#21644;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PDSketch: Integrated Planning Domain Programming and Learning. (arXiv:2303.05501v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;PDSketch&#35821;&#35328;&#21644;&#21487;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#21644;&#22312;&#32447;&#35268;&#21010;&#65292;&#21152;&#36895;&#20102;&#26426;&#22120;&#20154;&#30340;&#28789;&#27963;&#24615;&#21644;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#27169;&#22411;&#23398;&#20064;&#21644;&#22312;&#32447;&#35268;&#21010;&#26041;&#27861;&#65292;&#20197;&#26500;&#24314;&#28789;&#27963;&#21644;&#36890;&#29992;&#30340;&#26426;&#22120;&#20154;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#24213;&#23618;&#29615;&#22659;&#36716;&#25442;&#27169;&#22411;&#20013;&#30340;&#23616;&#37096;&#24615;&#21644;&#31232;&#30095;&#32467;&#26500;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12289;&#25968;&#25454;&#25928;&#29575;&#21644;&#36816;&#34892;&#25928;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22495;&#23450;&#20041;&#35821;&#35328;&#65292;&#21517;&#20026;PDSketch&#12290;&#23427;&#20801;&#35768;&#29992;&#25143;&#28789;&#27963;&#22320;&#23450;&#20041;&#36716;&#25442;&#27169;&#22411;&#20013;&#30340;&#39640;&#32423;&#32467;&#26500;&#65292;&#20363;&#22914;&#23545;&#35937;&#21644;&#29305;&#24449;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#31867;&#20284;&#20110;&#31243;&#24207;&#21592;&#20351;&#29992;TensorFlow&#25110;PyTorch&#25351;&#23450;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26680;&#22823;&#23567;&#21644;&#38544;&#34255;&#32500;&#24230;&#30340;&#26041;&#24335;&#12290;&#36716;&#25442;&#27169;&#22411;&#30340;&#32454;&#33410;&#23558;&#30001;&#21487;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#22635;&#20805;&#12290;&#22522;&#20110;&#23450;&#20041;&#30340;&#32467;&#26500;&#21644;&#23398;&#20064;&#21442;&#25968;&#65292;PDSketch&#33258;&#21160;&#29983;&#25104;&#19982;&#22495;&#26080;&#20851;&#30340;&#35268;&#21010;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#34893;&#29983;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#21152;&#36895;&#20102;&#23545;&#26032;&#30446;&#26631;&#30340;&#35268;&#21010;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies a model learning and online planning approach towards building flexible and general robots. Specifically, we investigate how to exploit the locality and sparsity structures in the underlying environmental transition model to improve model generalization, data-efficiency, and runtime-efficiency. We present a new domain definition language, named PDSketch. It allows users to flexibly define high-level structures in the transition models, such as object and feature dependencies, in a way similar to how programmers use TensorFlow or PyTorch to specify kernel sizes and hidden dimensions of a convolutional neural network. The details of the transition model will be filled in by trainable neural networks. Based on the defined structures and learned parameters, PDSketch automatically generates domain-independent planning heuristics without additional training. The derived heuristics accelerate the performance-time planning for novel goals.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25554;&#20837;&#26684;&#26519;&#31461;&#35805;&#21644;&#22522;&#20110;&#29616;&#26377;&#25991;&#26412;&#30340;&#21551;&#31034;&#24037;&#31243;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#21644;&#21551;&#31034;&#24037;&#31243;&#30340;&#21487;&#34892;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;4&#38454;&#27573;&#30340;&#21551;&#31034;&#24037;&#31243;&#36807;&#31243;&#65292;&#24182;&#35752;&#35770;&#20102;&#29983;&#25104;&#27169;&#22411;&#22312;&#26576;&#20123;&#25554;&#22270;&#19978;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2302.08961</link><description>&lt;p&gt;
&#22312;&#20185;&#22659;&#19982;&#20185;&#22659;&#20043;&#38388;&#30340;&#21551;&#31034;&#24037;&#31243;&#20013;&#25554;&#20837;&#26684;&#26519;&#31461;&#35805;&#65306;&#20013;&#36884;&#26053;&#31243;&#26469;&#35828;&#26126;&#31461;&#35805;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Grimm in Wonderland: Prompt Engineering with Midjourney to Illustrate Fairytales. (arXiv:2302.08961v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08961
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25554;&#20837;&#26684;&#26519;&#31461;&#35805;&#21644;&#22522;&#20110;&#29616;&#26377;&#25991;&#26412;&#30340;&#21551;&#31034;&#24037;&#31243;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#21644;&#21551;&#31034;&#24037;&#31243;&#30340;&#21487;&#34892;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;4&#38454;&#27573;&#30340;&#21551;&#31034;&#24037;&#31243;&#36807;&#31243;&#65292;&#24182;&#35752;&#35770;&#20102;&#29983;&#25104;&#27169;&#22411;&#22312;&#26576;&#20123;&#25554;&#22270;&#19978;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#36136;&#37327;&#19981;&#26029;&#25552;&#39640;&#65292;&#20294;&#20854;&#36866;&#29992;&#33539;&#22260;&#30340;&#30028;&#38480;&#20173;&#19981;&#28165;&#26970;&#12290;&#29305;&#21035;&#26159;&#65292;&#20197;&#25913;&#36827;&#25991;&#26412;&#36755;&#20837;&#20197;&#23454;&#29616;&#26356;&#22909;&#32467;&#26524;&#20026;&#30446;&#26631;&#30340;&#21551;&#31034;&#24037;&#31243;&#65292;&#20284;&#20046;&#23578;&#26410;&#38024;&#23545;&#19982;&#29616;&#26377;&#25991;&#26412;&#19968;&#36215;&#24037;&#20316;&#36827;&#34892;&#30740;&#31350;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#21644;&#21551;&#31034;&#24037;&#31243;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#27969;&#34892;&#31461;&#35805;&#30340;&#22522;&#26412;&#25554;&#22270;&#12290;&#20351;&#29992;Midjourney v4&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#34892;&#21160;&#30740;&#31350;&#65292;&#26088;&#22312;&#23581;&#35797;&#20026;5&#20010;&#27969;&#34892;&#31461;&#35805;&#20013;&#30340;&#27599;&#20010;&#31461;&#35805;&#29983;&#25104;5&#20010;&#20196;&#20154;&#20449;&#26381;&#30340;&#25554;&#22270;&#65292;&#24182;&#30830;&#23450;&#19968;&#20010;&#20174;&#29616;&#26377;&#25991;&#26412;&#21040;&#25554;&#22270;&#30340;&#21551;&#31034;&#24037;&#31243;&#36807;&#31243;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20010;&#21021;&#27493;&#30340;4&#38454;&#27573;&#36807;&#31243;&#65306;i&#65289;&#21021;&#22987;&#25552;&#31034;&#65292;ii&#65289;&#26500;&#22270;&#35843;&#25972;&#65292;iii&#65289;&#39118;&#26684;&#32454;&#21270;&#65292;&#21644;iv&#65289;&#21464;&#24322;&#36873;&#25321;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#29983;&#25104;&#27169;&#22411;&#22312;&#26576;&#20123;&#25554;&#22270;&#19978;&#36935;&#21040;&#22256;&#38590;&#30340;&#19977;&#20010;&#21407;&#22240;&#65306;&#35745;&#25968;&#22256;&#38590;&#65292;
&lt;/p&gt;
&lt;p&gt;
The quality of text-to-image generation is continuously improving, yet the boundaries of its applicability are still unclear. In particular, refinement of the text input with the objective of achieving better results - commonly called prompt engineering - so far seems to have not been geared towards work with pre-existing texts. We investigate whether text-to-image generation and prompt engineering could be used to generate basic illustrations of popular fairytales. Using Midjourney v4, we engage in action research with a dual aim: to attempt to generate 5 believable illustrations for each of 5 popular fairytales, and to define a prompt engineering process that starts from a pre-existing text and arrives at an illustration of it. We arrive at a tentative 4-stage process: i) initial prompt, ii) composition adjustment, iii) style refinement, and iv) variation selection. We also discuss three reasons why the generation model struggles with certain illustrations: difficulties with counts, 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31867;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#31574;&#30053;&#30340;&#22870;&#21169;&#20449;&#21495;&#30001;&#19982;&#20043;&#30456;&#20851;&#19988;&#21516;&#26102;&#20248;&#21270;&#30340;&#21028;&#21035;&#22120;&#29983;&#25104;&#65292;&#23548;&#33268;&#23398;&#20064;&#36807;&#31243;&#19981;&#31283;&#23450;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20462;&#21098;&#32447;&#24615;&#22870;&#21169;&#20989;&#25968;&#21487;&#20197;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2302.00270</link><description>&lt;p&gt;
&#20869;&#37096;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Internally Rewarded Reinforcement Learning. (arXiv:2302.00270v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00270
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31867;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#31574;&#30053;&#30340;&#22870;&#21169;&#20449;&#21495;&#30001;&#19982;&#20043;&#30456;&#20851;&#19988;&#21516;&#26102;&#20248;&#21270;&#30340;&#21028;&#21035;&#22120;&#29983;&#25104;&#65292;&#23548;&#33268;&#23398;&#20064;&#36807;&#31243;&#19981;&#31283;&#23450;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20462;&#21098;&#32447;&#24615;&#22870;&#21169;&#20989;&#25968;&#21487;&#20197;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31867;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#29992;&#20110;&#31574;&#30053;&#23398;&#20064;&#30340;&#22870;&#21169;&#20449;&#21495;&#30001;&#19968;&#20010;&#19982;&#31574;&#30053;&#30456;&#20851;&#19988;&#19982;&#31574;&#30053;&#21516;&#26102;&#20248;&#21270;&#30340;&#21028;&#21035;&#22120;&#29983;&#25104;&#12290;&#31574;&#30053;&#21644;&#21028;&#21035;&#22120;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#23548;&#33268;&#20102;&#19981;&#31283;&#23450;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#22240;&#20026;&#26469;&#33258;&#19981;&#25104;&#29087;&#21028;&#21035;&#22120;&#30340;&#22870;&#21169;&#20449;&#21495;&#26159;&#22024;&#26434;&#30340;&#65292;&#38459;&#30861;&#20102;&#31574;&#30053;&#30340;&#23398;&#20064;&#65307;&#21453;&#36807;&#26469;&#65292;&#26410;&#32463;&#20248;&#21270;&#30340;&#31574;&#30053;&#20063;&#20250;&#38459;&#30861;&#21028;&#21035;&#22120;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#23398;&#20064;&#35774;&#32622;&#31216;&#20026;&#8220;&#20869;&#37096;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;&#8221;&#65288;IRRL&#65289;&#65292;&#22240;&#20026;&#22870;&#21169;&#19981;&#26159;&#30452;&#25509;&#26469;&#33258;&#29615;&#22659;&#65292;&#32780;&#26159;&#30001;&#21028;&#21035;&#22120;&#8220;&#20869;&#37096;&#8221;&#25552;&#20379;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27491;&#24335;&#22320;&#34920;&#36848;&#20102;IRRL&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31867;&#23646;&#20110;IRRL&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#25512;&#23548;&#24182;&#32463;&#39564;&#24615;&#22320;&#20998;&#26512;&#20102;IRRL&#20013;&#22870;&#21169;&#20989;&#25968;&#30340;&#24433;&#21709;&#65292;&#24182;&#22522;&#20110;&#36825;&#20123;&#20998;&#26512;&#25552;&#20986;&#20102;&#20462;&#21098;&#32447;&#24615;&#22870;&#21169;&#20989;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#22870;&#21169;&#20989;&#25968;&#21487;&#20197;&#25345;&#32493;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a class of reinforcement learning problems where the reward signals for policy learning are generated by a discriminator that is dependent on and jointly optimized with the policy. This interdependence between the policy and the discriminator leads to an unstable learning process because reward signals from an immature discriminator are noisy and impede policy learning, and conversely, an under-optimized policy impedes discriminator learning. We call this learning setting \textit{Internally Rewarded Reinforcement Learning} (IRRL) as the reward is not provided directly by the environment but \textit{internally} by the discriminator. In this paper, we formally formulate IRRL and present a class of problems that belong to IRRL. We theoretically derive and empirically analyze the effect of the reward function in IRRL and based on these analyses propose the clipped linear reward function. Experimental results show that the proposed reward function can consistently stabilize the tra
&lt;/p&gt;</description></item><item><title>SceneRF&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#32467;&#21512;NeRF&#30340;&#36752;&#23556;&#22330;&#25216;&#26415;&#65292;&#26080;&#38656;&#28145;&#24230;&#30417;&#30563;&#65292;&#21482;&#38656;&#20351;&#29992;&#22270;&#20687;&#24207;&#21015;&#35757;&#32451;&#65292;&#21487;&#20197;&#39640;&#25928;&#22788;&#29702;&#22823;&#22330;&#26223;&#65292;&#33021;&#22815;&#29983;&#25104;&#26032;&#30340;&#28145;&#24230;&#35270;&#22270;&#24182;&#36827;&#34892;3D&#22330;&#26223;&#37325;&#24314;&#65292;&#24615;&#33021;&#22312;&#23460;&#20869;&#22806;&#22330;&#26223;&#20013;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.02501</link><description>&lt;p&gt;
SceneRF: &#21033;&#29992;&#36752;&#23556;&#22330;&#30340;&#33258;&#30417;&#30563;&#21333;&#30446;3D&#22330;&#26223;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
SceneRF: Self-Supervised Monocular 3D Scene Reconstruction with Radiance Fields. (arXiv:2212.02501v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02501
&lt;/p&gt;
&lt;p&gt;
SceneRF&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#32467;&#21512;NeRF&#30340;&#36752;&#23556;&#22330;&#25216;&#26415;&#65292;&#26080;&#38656;&#28145;&#24230;&#30417;&#30563;&#65292;&#21482;&#38656;&#20351;&#29992;&#22270;&#20687;&#24207;&#21015;&#35757;&#32451;&#65292;&#21487;&#20197;&#39640;&#25928;&#22788;&#29702;&#22823;&#22330;&#26223;&#65292;&#33021;&#22815;&#29983;&#25104;&#26032;&#30340;&#28145;&#24230;&#35270;&#22270;&#24182;&#36827;&#34892;3D&#22330;&#26223;&#37325;&#24314;&#65292;&#24615;&#33021;&#22312;&#23460;&#20869;&#22806;&#22330;&#26223;&#20013;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#32500;&#22270;&#20687;&#19977;&#32500;&#37325;&#24314;&#30340;&#30740;&#31350;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#20851;&#27880;&#65292;&#36890;&#24120;&#38656;&#35201;&#20351;&#29992;&#28145;&#24230;&#30417;&#30563;&#26469;&#36827;&#34892;&#35757;&#32451;&#12290;&#20026;&#20102;&#20943;&#23569;&#23545;&#26114;&#36149;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SceneRF&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#23436;&#20840;&#22522;&#20110;&#22270;&#20687;&#24207;&#21015;&#36827;&#34892;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#21333;&#30446;&#22330;&#26223;&#37325;&#24314;&#26041;&#27861;&#12290;&#22522;&#20110;&#26368;&#26032;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#25216;&#26415;(NeRF)&#65292;&#25105;&#20204;&#20248;&#21270;&#20102;&#19968;&#20010;&#36752;&#23556;&#22330;&#65292;&#24182;&#37319;&#29992;&#20102;&#26174;&#24335;&#28145;&#24230;&#20248;&#21270;&#21644;&#26032;&#39062;&#30340;&#27010;&#29575;&#37319;&#26679;&#31574;&#30053;&#26469;&#26377;&#25928;&#22788;&#29702;&#22823;&#22330;&#26223;&#12290;&#22312;&#25512;&#29702;&#38454;&#27573;&#65292;&#21482;&#38656;&#36755;&#20837;&#21333;&#20010;&#22270;&#20687;&#21363;&#21487;&#29983;&#25104;&#26032;&#30340;&#28145;&#24230;&#35270;&#22270;&#65292;&#24182;&#23558;&#20854;&#34701;&#21512;&#22312;&#19968;&#36215;&#20197;&#33719;&#24471;3D&#22330;&#26223;&#37325;&#24314;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#22312;&#23460;&#20869;BundleFusion&#21644;&#23460;&#22806;SemanticKITTI&#22330;&#26223;&#19979;&#65292;&#24615;&#33021;&#20248;&#20110;&#26368;&#36817;&#30340;&#25152;&#26377;&#22522;&#32447;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#36827;&#34892;&#26032;&#35270;&#35282;&#28145;&#24230;&#21512;&#25104;&#21644;&#22330;&#26223;&#37325;&#24314;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://astra-vision.github.io/SceneRF&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D reconstruction from 2D image was extensively studied, training with depth supervision. To relax the dependence to costly-acquired datasets, we propose SceneRF, a self-supervised monocular scene reconstruction method using only posed image sequences for training. Fueled by the recent progress in neural radiance fields (NeRF) we optimize a radiance field though with explicit depth optimization and a novel probabilistic sampling strategy to efficiently handle large scenes. At inference, a single input image suffices to hallucinate novel depth views which are fused together to obtain 3D scene reconstruction. Thorough experiments demonstrate that we outperform all recent baselines for novel depth views synthesis and scene reconstruction, on indoor BundleFusion and outdoor SemanticKITTI. Our code is available at https://astra-vision.github.io/SceneRF.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#26435;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#22270;&#20687;&#20462;&#22797;&#65292;&#36890;&#36807;&#23398;&#20064;&#22810;&#20010;&#26435;&#37325;&#30340;&#20808;&#39564;&#20219;&#21153;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;&#37325;&#24314;&#25439;&#22833;&#21644;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#26469;&#29983;&#25104;&#26356;&#20855;&#21560;&#24341;&#21147;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2211.13856</link><description>&lt;p&gt;
WSSL&#65306;&#21152;&#26435;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#22270;&#20687;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
WSSL: Weighted Self-supervised Learning Framework For Image-inpainting. (arXiv:2211.13856v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13856
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#26435;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#22270;&#20687;&#20462;&#22797;&#65292;&#36890;&#36807;&#23398;&#20064;&#22810;&#20010;&#26435;&#37325;&#30340;&#20808;&#39564;&#20219;&#21153;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;&#37325;&#24314;&#25439;&#22833;&#21644;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#26469;&#29983;&#25104;&#26356;&#20855;&#21560;&#24341;&#21147;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20462;&#22797;&#26159;&#37325;&#26032;&#29983;&#25104;&#22270;&#20687;&#20002;&#22833;&#37096;&#20998;&#30340;&#36807;&#31243;&#12290;&#22522;&#20110;&#30417;&#30563;&#31639;&#27861;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23384;&#22312;&#20004;&#20010;&#37325;&#35201;&#30340;&#32570;&#28857;&#12290;&#23427;&#20204;&#22312;&#20351;&#29992;&#26410;&#35265;&#25968;&#25454;&#36827;&#34892;&#27979;&#35797;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;&#23427;&#20204;&#26080;&#27861;&#25429;&#25417;&#21040;&#22270;&#20687;&#30340;&#20840;&#23616;&#19978;&#19979;&#25991;&#65292;&#23548;&#33268;&#32467;&#26524;&#35270;&#35273;&#19978;&#19981;&#21560;&#24341;&#20154;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#22270;&#20687;&#20462;&#22797;&#65306;&#21152;&#26435;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;WSSL&#65289;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;WSSL&#26469;&#20174;&#22810;&#20010;&#21152;&#26435;&#30340;&#20808;&#39564;&#20219;&#21153;&#20013;&#23398;&#20064;&#29305;&#24449;&#12290;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#29305;&#24449;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#65292;&#21363;&#22270;&#20687;&#20462;&#22797;&#12290;&#20026;&#20102;&#25913;&#21892;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#24615;&#33021;&#24182;&#20135;&#29983;&#26356;&#20855;&#35270;&#35273;&#21560;&#24341;&#21147;&#30340;&#22270;&#20687;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#20687;&#20462;&#22797;&#25439;&#22833;&#20989;&#25968;&#12290;&#35813;&#25439;&#22833;&#20989;&#25968;&#21033;&#29992;&#37325;&#24314;&#25439;&#22833;&#21644;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#26469;&#37325;&#26032;&#29983;&#25104;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;WSSL&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#25439;&#22833;&#20989;&#25968;&#26377;&#21161;&#20110;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image inpainting is the process of regenerating lost parts of the image. Supervised algorithm-based methods have shown excellent results but have two significant drawbacks. They do not perform well when tested with unseen data. They fail to capture the global context of the image, resulting in a visually unappealing result. We propose a novel self-supervised learning framework for image-inpainting: Weighted Self-Supervised Learning (WSSL) to tackle these problems. We designed WSSL to learn features from multiple weighted pretext tasks. These features are then utilized for the downstream task, image-inpainting. To improve the performance of our framework and produce more visually appealing images, we also present a novel loss function for image inpainting. The loss function takes advantage of both reconstruction loss and perceptual loss functions to regenerate the image. Our experimentation shows WSSL outperforms previous methods, and our loss function helps produce better results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#33258;&#30456;&#20284;&#29289;&#20307;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#28779;&#21644;&#28895;&#30340;&#33258;&#30456;&#20284;&#29305;&#24449;&#26469;&#35299;&#20915;&#20854;&#24418;&#29366;&#22810;&#26679;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#26041;&#27861;&#26469;&#35780;&#20272;&#21644;&#25552;&#39640;&#29289;&#20307;&#26816;&#27979;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2211.10995</link><description>&lt;p&gt;
&#29420;&#29305;&#30340;&#33258;&#30456;&#20284;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Distinctive Self-Similar Object Detection. (arXiv:2211.10995v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#33258;&#30456;&#20284;&#29289;&#20307;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#28779;&#21644;&#28895;&#30340;&#33258;&#30456;&#20284;&#29305;&#24449;&#26469;&#35299;&#20915;&#20854;&#24418;&#29366;&#22810;&#26679;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#26041;&#27861;&#26469;&#35780;&#20272;&#21644;&#25552;&#39640;&#29289;&#20307;&#26816;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29289;&#20307;&#26816;&#27979;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20102;&#37325;&#35201;&#30340;&#23384;&#22312;&#12290;&#28982;&#32780;&#65292;&#20687;&#28779;&#21644;&#28895;&#36825;&#26679;&#30340;&#29289;&#20307;&#30001;&#20110;&#20854;&#38750;&#22266;&#24577;&#21644;&#21508;&#31181;&#21508;&#26679;&#30340;&#24418;&#29366;&#65292;&#23545;&#29289;&#20307;&#26816;&#27979;&#25552;&#20986;&#20102;&#25361;&#25112;&#65292;&#22240;&#27492;&#22312;&#23454;&#38469;&#30340;&#28779;&#28798;&#39044;&#38450;&#21644;&#25511;&#21046;&#20013;&#38590;&#20197;&#30495;&#27491;&#28385;&#36275;&#35201;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28779;&#21644;&#28895;&#20013;&#30340;&#29420;&#29305;&#30340;&#33258;&#30456;&#20284;&#29305;&#24449;&#21487;&#20197;&#35299;&#20915;&#21508;&#31181;&#24418;&#29366;&#30340;&#22256;&#25200;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#35752;&#35770;&#36825;&#20010;&#38382;&#39064;&#30340;&#12290;&#20026;&#20102;&#35780;&#20272;&#28779;&#21644;&#28895;&#30340;&#33258;&#30456;&#20284;&#24615;&#24182;&#25552;&#39640;&#29289;&#20307;&#26816;&#27979;&#30340;&#31934;&#24230;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20351;&#29992;Hausdorff&#36317;&#31163;&#25551;&#36848;&#23454;&#20363;&#20043;&#38388;&#30456;&#20284;&#24230;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#33258;&#30456;&#20284;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#35780;&#20272;&#36825;&#20010;&#29305;&#23450;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#26356;&#20844;&#24179;&#30340;&#26041;&#24335;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#25105;&#20204;&#30340;&#32593;&#32476;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based object detection has demonstrated a significant presence in the practical applications of artificial intelligence. However, objects such as fire and smoke, pose challenges to object detection because of their non-solid and various shapes, and consequently difficult to truly meet requirements in practical fire prevention and control. In this paper, we propose that the distinctive fractal feature of self-similar in fire and smoke can relieve us from struggling with their various shapes. To our best knowledge, we are the first to discuss this problem. In order to evaluate the self-similarity of the fire and smoke and improve the precision of object detection, we design a semi-supervised method that use Hausdorff distance to describe the resemblance between instances. Besides, based on the concept of self-similar, we have devised a novel methodology for evaluating this particular task in a more equitable manner. We have meticulously designed our network architecture bas
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#26469;&#20248;&#21270;&#29305;&#24449;&#36890;&#20449;&#12289;&#27169;&#22411;&#31934;&#24230;&#21644;&#20998;&#24067;&#24335;&#21516;&#27493;&#12290;</title><link>http://arxiv.org/abs/2211.00216</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Distributed Graph Neural Network Training: A Survey. (arXiv:2211.00216v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00216
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#26469;&#20248;&#21270;&#29305;&#24449;&#36890;&#20449;&#12289;&#27169;&#22411;&#31934;&#24230;&#21644;&#20998;&#24067;&#24335;&#21516;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#19968;&#31181;&#22312;&#22270;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#24212;&#29992;&#12290;&#23613;&#31649;GNNs&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#26159;&#23558;&#20854;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#22270;&#20381;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20998;&#24067;&#24335;&#35745;&#31639;&#25104;&#20026;&#35757;&#32451;&#22823;&#35268;&#27169;GNNs&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#33021;&#25552;&#20379;&#20016;&#23500;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#22270;&#32467;&#26500;&#30340;&#20381;&#36182;&#24615;&#20351;&#24471;&#23454;&#29616;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;GNN&#35757;&#32451;&#21464;&#24471;&#22256;&#38590;&#65292;&#23384;&#22312;&#22823;&#37327;&#30340;&#36890;&#20449;&#21644;&#36127;&#36733;&#19981;&#24179;&#34913;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#20998;&#24067;&#24335;GNN&#35757;&#32451;&#36827;&#34892;&#20102;&#35768;&#22810;&#21162;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#35757;&#32451;&#31639;&#27861;&#21644;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#22312;&#20998;&#24067;&#24335;&#25191;&#34892;GNN&#35757;&#32451;&#30340;&#20248;&#21270;&#25216;&#26415;&#26041;&#38754;&#32570;&#20047;&#31995;&#32479;&#24615;&#30340;&#32508;&#36848;&#12290;&#26412;&#35843;&#26597;&#20998;&#26512;&#20102;&#20998;&#24067;&#24335;GNN&#35757;&#32451;&#20013;&#30340;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#22823;&#35268;&#27169;&#29305;&#24449;&#36890;&#20449;&#12289;&#27169;&#22411;&#31934;&#24230;&#25439;&#22833;&#21644;&#20998;&#24067;&#24335;&#21516;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) are a type of deep learning models that are trained on graphs and have been successfully applied in various domains. Despite the effectiveness of GNNs, it is still challenging for GNNs to efficiently scale to large graphs. As a remedy, distributed computing becomes a promising solution of training large-scale GNNs, since it is able to provide abundant computing resources. However, the dependency of graph structure increases the difficulty of achieving high-efficiency distributed GNN training, which suffers from the massive communication and workload imbalance. In recent years, many efforts have been made on distributed GNN training, and an array of training algorithms and systems have been proposed. Yet, there is a lack of systematic review on the optimization techniques for the distributed execution of GNN training. In this survey, we analyze three major challenges in distributed GNN training that are massive feature communication, the loss of model accura
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Scene-Rep Transformer&#26469;&#25552;&#21319;&#24378;&#21270;&#23398;&#20064;&#20915;&#31574;&#33021;&#21147;&#65292;&#36890;&#36807;&#25913;&#36827;&#22330;&#26223;&#34920;&#31034;&#32534;&#30721;&#21644;&#39034;&#24207;&#39044;&#27979;&#28508;&#22312;&#33976;&#39311;&#12290;&#37319;&#29992;&#22810;&#38454;&#27573;Transformer&#32534;&#30721;&#22120;&#24314;&#27169;&#20132;&#20114;&#24847;&#35782;&#21644;&#24847;&#22270;&#24847;&#35782;&#65292;&#24182;&#20351;&#29992;&#39034;&#24207;&#28508;&#22312;Transformer&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#21152;&#36895;&#35757;&#32451;&#21644;&#20943;&#23569;&#25506;&#32034;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2208.12263</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#22330;&#26223;&#34920;&#31034;&#23398;&#20064;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Augmenting Reinforcement Learning with Transformer-based Scene Representation Learning for Decision-making of Autonomous Driving. (arXiv:2208.12263v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Scene-Rep Transformer&#26469;&#25552;&#21319;&#24378;&#21270;&#23398;&#20064;&#20915;&#31574;&#33021;&#21147;&#65292;&#36890;&#36807;&#25913;&#36827;&#22330;&#26223;&#34920;&#31034;&#32534;&#30721;&#21644;&#39034;&#24207;&#39044;&#27979;&#28508;&#22312;&#33976;&#39311;&#12290;&#37319;&#29992;&#22810;&#38454;&#27573;Transformer&#32534;&#30721;&#22120;&#24314;&#27169;&#20132;&#20114;&#24847;&#35782;&#21644;&#24847;&#22270;&#24847;&#35782;&#65292;&#24182;&#20351;&#29992;&#39034;&#24207;&#28508;&#22312;Transformer&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#21152;&#36895;&#35757;&#32451;&#21644;&#20943;&#23569;&#25506;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#33258;&#21160;&#39550;&#39542;&#30340;&#20915;&#31574;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#30001;&#20110;&#20132;&#36890;&#21442;&#19982;&#32773;&#30340;&#38543;&#26426;&#24615;&#21644;&#36947;&#36335;&#32467;&#26500;&#30340;&#22797;&#26434;&#24615;&#12290;&#23613;&#31649;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#20915;&#31574;&#26041;&#26696;&#22312;&#22788;&#29702;&#22478;&#24066;&#39550;&#39542;&#22330;&#26223;&#26041;&#38754;&#24456;&#26377;&#21069;&#26223;&#65292;&#20294;&#23427;&#30340;&#37319;&#26679;&#25928;&#29575;&#20302;&#19988;&#36866;&#24212;&#24615;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Scene-Rep Transformer&#26469;&#25913;&#21892;RL&#20915;&#31574;&#33021;&#21147;&#65292;&#36890;&#36807;&#26356;&#22909;&#30340;&#22330;&#26223;&#34920;&#31034;&#32534;&#30721;&#21644;&#39034;&#24207;&#39044;&#27979;&#28508;&#22312;&#33976;&#39311;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#38454;&#27573;Transformer&#65288;MST&#65289;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#24314;&#27169;&#33258;&#36710;&#19982;&#20854;&#37051;&#23621;&#20043;&#38388;&#30340;&#20132;&#20114;&#24847;&#35782;&#20197;&#21450;&#20195;&#29702;&#32773;&#19982;&#20505;&#36873;&#36335;&#24452;&#20043;&#38388;&#30340;&#24847;&#22270;&#24847;&#35782;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#20855;&#26377;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#30340;&#39034;&#24207;&#28508;&#22312;Transformer&#65288;SLT&#65289;&#65292;&#23558;&#26410;&#26469;&#30340;&#39044;&#27979;&#20449;&#24687;&#33976;&#39311;&#21040;&#28508;&#22312;&#30340;&#22330;&#26223;&#34920;&#31034;&#20013;&#65292;&#20197;&#20943;&#23569;&#25506;&#32034;&#31354;&#38388;&#24182;&#21152;&#24555;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-making for urban autonomous driving is challenging due to the stochastic nature of interactive traffic participants and the complexity of road structures. Although reinforcement learning (RL)-based decision-making scheme is promising to handle urban driving scenarios, it suffers from low sample efficiency and poor adaptability. In this paper, we propose Scene-Rep Transformer to improve the RL decision-making capabilities with better scene representation encoding and sequential predictive latent distillation. Specifically, a multi-stage Transformer (MST) encoder is constructed to model not only the interaction awareness between the ego vehicle and its neighbors but also intention awareness between the agents and their candidate routes. A sequential latent Transformer (SLT) with self-supervised learning objectives is employed to distill the future predictive information into the latent scene representation, in order to reduce the exploration space and speed up training. The fina
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#20004;&#27493;&#31070;&#32463;&#32593;&#32476;TMO&#65292;&#20855;&#26377;&#33258;&#26657;&#20934;&#21644;&#24863;&#30693;&#20248;&#21270;&#21151;&#33021;&#65292;&#21487;&#20197;&#23558;HDR&#22270;&#20687;&#21387;&#32553;&#21040;LDR&#22270;&#20687;&#65292;&#21516;&#26102;&#36890;&#36807;&#24863;&#30693;&#24230;&#37327;&#23454;&#29616;&#20102;&#28789;&#25935;&#30340;&#36136;&#37327;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2206.09146</link><description>&lt;p&gt;
&#19968;&#31181;&#24863;&#30693;&#20248;&#21270;&#19988;&#33258;&#26657;&#20934;&#30340;&#33394;&#35843;&#26144;&#23556;&#36816;&#31639;&#31526;
&lt;/p&gt;
&lt;p&gt;
A Perceptually Optimized and Self-Calibrated Tone Mapping Operator. (arXiv:2206.09146v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#20004;&#27493;&#31070;&#32463;&#32593;&#32476;TMO&#65292;&#20855;&#26377;&#33258;&#26657;&#20934;&#21644;&#24863;&#30693;&#20248;&#21270;&#21151;&#33021;&#65292;&#21487;&#20197;&#23558;HDR&#22270;&#20687;&#21387;&#32553;&#21040;LDR&#22270;&#20687;&#65292;&#21516;&#26102;&#36890;&#36807;&#24863;&#30693;&#24230;&#37327;&#23454;&#29616;&#20102;&#28789;&#25935;&#30340;&#36136;&#37327;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39640;&#21160;&#24577;&#33539;&#22260;&#65288;HDR&#65289;&#25668;&#24433;&#30340;&#26222;&#21450;&#21644;&#21487;&#35775;&#38382;&#24615;&#22686;&#21152;&#65292;&#23545;&#21160;&#24577;&#33539;&#22260;&#21387;&#32553;&#30340;&#33394;&#35843;&#26144;&#23556;&#36816;&#31639;&#31526;&#65288;TMO&#65289;&#38656;&#27714;&#26085;&#30410;&#22686;&#21152;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#20004;&#27493;&#31070;&#32463;&#32593;&#32476;TMO&#65292;&#20855;&#26377;&#33258;&#26657;&#20934;&#21644;&#24863;&#30693;&#20248;&#21270;&#21151;&#33021;&#12290;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;HDR&#22270;&#20687;&#20998;&#35299;&#25104;&#35268;&#33539;&#21270;&#25289;&#26222;&#25289;&#26031;&#37329;&#23383;&#22612;&#65292;&#28982;&#21518;&#20351;&#29992;&#20004;&#20010;&#36731;&#37327;&#32423;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#65292;&#20197;&#35268;&#33539;&#21270;&#34920;&#31034;&#20316;&#20026;&#36755;&#20837;&#26469;&#20272;&#35745;&#30456;&#24212;LDR&#22270;&#20687;&#30340;&#25289;&#26222;&#25289;&#26031;&#37329;&#23383;&#22612;&#12290;&#25105;&#20204;&#36890;&#36807;&#26368;&#23567;&#21270;&#35268;&#33539;&#21270;&#25289;&#26222;&#25289;&#26031;&#37329;&#23383;&#22612;&#36317;&#31163;&#65288;NLPD&#65289;&#26469;&#20248;&#21270;&#33394;&#35843;&#26144;&#23556;&#32593;&#32476;&#65292;&#36825;&#26159;&#19982;&#20154;&#31867;&#23545;&#33394;&#35843;&#26144;&#23556;&#22270;&#20687;&#36136;&#37327;&#30340;&#21028;&#26029;&#30456;&#19968;&#33268;&#30340;&#24863;&#30693;&#24230;&#37327;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#20013;&#65292;&#36755;&#20837;&#30340;HDR&#22270;&#20687;&#26159;&#33258;&#26657;&#20934;&#30340;&#65292;&#20197;&#35745;&#31639;&#20986;&#26368;&#32456;&#30340;LDR&#22270;&#20687;&#12290;&#25105;&#20204;&#23558;&#21516;&#19968;&#24352;HDR&#22270;&#20687;&#36755;&#20837;&#32463;&#36807;&#19981;&#21516;&#26368;&#22823;&#20142;&#24230;&#37325;&#26032;&#35843;&#25972;&#27604;&#20363;&#21518;&#30340;&#23398;&#20064;&#33394;&#35843;&#26144;&#23556;&#32593;&#32476;&#20013;&#65292;&#26469;&#23436;&#25104;&#36825;&#19968;&#33258;&#26657;&#20934;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing popularity and accessibility of high dynamic range (HDR) photography, tone mapping operators (TMOs) for dynamic range compression are practically demanding. In this paper, we develop a two-stage neural network-based TMO that is self-calibrated and perceptually optimized. In Stage one, motivated by the physiology of the early stages of the human visual system, we first decompose an HDR image into a normalized Laplacian pyramid. We then use two lightweight deep neural networks (DNNs), taking the normalized representation as input and estimating the Laplacian pyramid of the corresponding LDR image. We optimize the tone mapping network by minimizing the normalized Laplacian pyramid distance (NLPD), a perceptual metric aligning with human judgments of tone-mapped image quality. In Stage two, the input HDR image is self-calibrated to compute the final LDR image. We feed the same HDR image but rescaled with different maximum luminances to the learned tone mapping network, 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#35821;&#27861;&#30340;&#22522;&#30784;&#35789;&#27719;&#23398;&#20064;&#65288;G2L2&#65289;&#26159;&#19968;&#31181;&#20174;&#22522;&#30784;&#25968;&#25454;&#20013;&#23398;&#20064;&#35821;&#35328;&#21547;&#20041;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21333;&#35789;&#26144;&#23556;&#21040;&#35821;&#27861;&#31867;&#22411;&#21644;&#31070;&#32463;&#31526;&#21495;&#35821;&#20041;&#31243;&#24207;&#65292;&#21033;&#29992;&#22522;&#20110;&#35821;&#27861;&#30340;&#32452;&#21512;&#25512;&#23548;&#21477;&#23376;&#30340;&#21547;&#20041;&#65292;&#26368;&#32456;&#21487;&#20197;&#22312;&#22522;&#30784;&#36755;&#20837;&#19978;&#25191;&#34892;&#12290;</title><link>http://arxiv.org/abs/2202.08806</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#27861;&#30340;&#22522;&#30784;&#35789;&#27719;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Grammar-Based Grounded Lexicon Learning. (arXiv:2202.08806v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.08806
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35821;&#27861;&#30340;&#22522;&#30784;&#35789;&#27719;&#23398;&#20064;&#65288;G2L2&#65289;&#26159;&#19968;&#31181;&#20174;&#22522;&#30784;&#25968;&#25454;&#20013;&#23398;&#20064;&#35821;&#35328;&#21547;&#20041;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21333;&#35789;&#26144;&#23556;&#21040;&#35821;&#27861;&#31867;&#22411;&#21644;&#31070;&#32463;&#31526;&#21495;&#35821;&#20041;&#31243;&#24207;&#65292;&#21033;&#29992;&#22522;&#20110;&#35821;&#27861;&#30340;&#32452;&#21512;&#25512;&#23548;&#21477;&#23376;&#30340;&#21547;&#20041;&#65292;&#26368;&#32456;&#21487;&#20197;&#22312;&#22522;&#30784;&#36755;&#20837;&#19978;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#27861;&#30340;&#22522;&#30784;&#35789;&#27719;&#23398;&#20064;&#26041;&#27861;&#65288;G2L2&#65289;&#65292;&#29992;&#20110;&#20174;&#22522;&#30784;&#25968;&#25454;&#65288;&#22914;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#37197;&#23545;&#65289;&#20013;&#23398;&#20064;&#35821;&#35328;&#30340;&#32452;&#21512;&#21644;&#22522;&#20110;&#22522;&#30784;&#30340;&#21547;&#20041;&#34920;&#31034;&#12290;G2L2&#30340;&#26680;&#24515;&#26159;&#19968;&#32452;&#35789;&#27719;&#26465;&#30446;&#65292;&#23558;&#27599;&#20010;&#21333;&#35789;&#26144;&#23556;&#21040;&#19968;&#20010;&#35821;&#27861;&#31867;&#22411;&#21644;&#31070;&#32463;&#31526;&#21495;&#35821;&#20041;&#31243;&#24207;&#30340;&#20803;&#32452;&#12290;&#32473;&#23450;&#19968;&#20010;&#36755;&#20837;&#21477;&#23376;&#65292;G2L2&#39318;&#20808;&#26597;&#25214;&#19982;&#27599;&#20010;&#26631;&#35760;&#30456;&#20851;&#32852;&#30340;&#35789;&#27719;&#26465;&#30446;&#12290;&#28982;&#21518;&#36890;&#36807;&#22522;&#20110;&#35821;&#27861;&#30340;&#32452;&#21512;&#35789;&#27719;&#21547;&#20041;&#26469;&#25512;&#23548;&#21477;&#23376;&#30340;&#21547;&#20041;&#20316;&#20026;&#21487;&#25191;&#34892;&#30340;&#31070;&#32463;&#31526;&#21495;&#31243;&#24207;&#12290;&#24674;&#22797;&#30340;&#21547;&#20041;&#31243;&#24207;&#21487;&#20197;&#22312;&#22522;&#30784;&#36755;&#20837;&#19978;&#25191;&#34892;&#12290;&#20026;&#20102;&#22312;&#25351;&#25968;&#32423;&#22686;&#38271;&#30340;&#32452;&#21512;&#31354;&#38388;&#20013;&#20419;&#36827;&#23398;&#20064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#22238;&#24402;&#30340;channel pruning&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Grammar-Based Grounded Lexicon Learning (G2L2), a lexicalist approach toward learning a compositional and grounded meaning representation of language from grounded data, such as paired images and texts. At the core of G2L2 is a collection of lexicon entries, which map each word to a tuple of a syntactic type and a neuro-symbolic semantic program. For example, the word shiny has a syntactic type of adjective; its neuro-symbolic semantic program has the symbolic form {\lambda}x. filter(x, SHINY), where the concept SHINY is associated with a neural network embedding, which will be used to classify shiny objects. Given an input sentence, G2L2 first looks up the lexicon entries associated with each token. It then derives the meaning of the sentence as an executable neuro-symbolic program by composing lexical meanings based on syntax. The recovered meaning programs can be executed on grounded inputs. To facilitate learning in an exponentially-growing compositional space, we introd
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;G\"odel&#30340;&#26412;&#20307;&#35770;&#35777;&#30340;&#31616;&#21270;&#21464;&#20307;&#65292;&#35813;&#21464;&#20307;&#22312;&#22522;&#26412;&#27169;&#24577;&#36923;&#36753;K&#25110;KT&#20013;&#24050;&#32463;&#26159;&#26377;&#25928;&#30340;&#65292;&#36991;&#20813;&#20102;&#22797;&#26434;&#30340;&#35859;&#35789;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20154;&#26426;&#20132;&#20114;&#22312;&#35745;&#31639;&#24418;&#32780;&#19978;&#23398;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2202.06264</link><description>&lt;p&gt;
G\"odel&#30340;&#26412;&#20307;&#35770;&#35777;&#30340;&#31616;&#21270;&#21464;&#20307;
&lt;/p&gt;
&lt;p&gt;
A Simplified Variant of G\"odel's Ontological Argument. (arXiv:2202.06264v3 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.06264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;G\"odel&#30340;&#26412;&#20307;&#35770;&#35777;&#30340;&#31616;&#21270;&#21464;&#20307;&#65292;&#35813;&#21464;&#20307;&#22312;&#22522;&#26412;&#27169;&#24577;&#36923;&#36753;K&#25110;KT&#20013;&#24050;&#32463;&#26159;&#26377;&#25928;&#30340;&#65292;&#36991;&#20813;&#20102;&#22797;&#26434;&#30340;&#35859;&#35789;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20154;&#26426;&#20132;&#20114;&#22312;&#35745;&#31639;&#24418;&#32780;&#19978;&#23398;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;G\"odel&#30340;&#26412;&#20307;&#35770;&#35777;&#30340;&#31616;&#21270;&#21464;&#20307;&#12290;&#36825;&#20010;&#31616;&#21270;&#30340;&#35770;&#35777;&#22312;&#22522;&#26412;&#27169;&#24577;&#36923;&#36753;K&#25110;KT&#20013;&#24050;&#32463;&#26159;&#26377;&#25928;&#30340;&#65292;&#23427;&#19981;&#20250;&#36973;&#21463;&#27169;&#24577;&#23849;&#28291;&#65292;&#24182;&#19988;&#36991;&#20813;&#20102;G\"odel&#25152;&#20351;&#29992;&#30340;&#30456;&#24403;&#22797;&#26434;&#30340;&#26412;&#36136;&#65288;Ess.&#65289;&#21644;&#24517;&#28982;&#23384;&#22312;&#65288;NE&#65289;&#30340;&#35859;&#35789;&#12290;&#25152;&#25552;&#20986;&#30340;&#21464;&#20307;&#26159;&#36890;&#36807;&#19982;&#29616;&#20195;&#35777;&#26126;&#21161;&#29702;&#31995;&#32479;&#20132;&#20114;&#36827;&#34892;&#19968;&#31995;&#21015;&#29702;&#35770;&#31616;&#21270;&#23454;&#39564;&#30340;&#21103;&#20135;&#29289;&#12290;&#36825;&#20123;&#23454;&#39564;&#30340;&#36215;&#28857;&#26159;G\"odel&#35770;&#35777;&#30340;&#35745;&#31639;&#26426;&#32534;&#30721;&#65292;&#28982;&#21518;&#31995;&#32479;&#22320;&#24212;&#29992;&#33258;&#21160;&#25512;&#29702;&#25216;&#26415;&#26469;&#24471;&#21040;&#25152;&#23637;&#31034;&#30340;&#31616;&#21270;&#21464;&#20307;&#12290;&#25152;&#21576;&#29616;&#30340;&#24037;&#20316;&#22240;&#27492;&#23637;&#31034;&#20102;&#35745;&#31639;&#24418;&#32780;&#19978;&#23398;&#20013;&#23500;&#26377;&#25104;&#26524;&#30340;&#20154;&#26426;&#20132;&#20114;&#12290;&#36825;&#20010;&#23637;&#31034;&#32467;&#26524;&#26159;&#21542;&#22686;&#21152;&#25110;&#20943;&#23569;&#20102;&#26412;&#20307;&#35770;&#35777;&#30340;&#21560;&#24341;&#21147;&#21644;&#35828;&#26381;&#21147;&#65292;&#26159;&#19968;&#20010;&#25105;&#24819;&#20132;&#32473;&#21746;&#23398;&#21644;&#31070;&#23398;&#35752;&#35770;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A simplified variant of G\"odel's ontological argument is presented. The simplified argument is valid already in basic modal logics K or KT, it does not suffer from modal collapse, and it avoids the rather complex predicates of essence (Ess.) and necessary existence (NE) as used by G\"odel. The variant presented has been obtained as a side result of a series of theory simplification experiments conducted in interaction with a modern proof assistant system. The starting point for these experiments was the computer encoding of G\"odel's argument, and then automated reasoning techniques were systematically applied to arrive at the simplified variant presented. The presented work thus exemplifies a fruitful human-computer interaction in computational metaphysics. Whether the presented result increases or decreases the attractiveness and persuasiveness of the ontological argument is a question I would like to pass on to philosophy and theology.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#21033;&#29992;&#25968;&#23398;&#21644;&#32479;&#35745;&#26041;&#27861;&#30830;&#23450;&#26631;&#31614;&#30340;&#30456;&#20284;&#24615;&#65292;&#21253;&#25324;&#35789;&#27719;&#30456;&#20284;&#24615;&#21644;&#20849;&#29616;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#32771;&#34385;&#20102;&#26631;&#31614;&#20998;&#37197;&#30340;&#26102;&#38388;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2201.03622</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#31995;&#32479;&#22312;&#31038;&#21306;&#26816;&#27979;&#20013;&#30340;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Graph-Based Recommendation System Enhanced with Community Detection. (arXiv:2201.03622v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.03622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#21033;&#29992;&#25968;&#23398;&#21644;&#32479;&#35745;&#26041;&#27861;&#30830;&#23450;&#26631;&#31614;&#30340;&#30456;&#20284;&#24615;&#65292;&#21253;&#25324;&#35789;&#27719;&#30456;&#20284;&#24615;&#21644;&#20849;&#29616;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#32771;&#34385;&#20102;&#26631;&#31614;&#20998;&#37197;&#30340;&#26102;&#38388;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30740;&#31350;&#32773;&#24050;&#32463;&#21033;&#29992;&#26631;&#31614;&#20449;&#24687;&#26469;&#25913;&#21892;&#25512;&#33616;&#31995;&#32479;&#20013;&#25512;&#33616;&#25216;&#26415;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#30740;&#31350;&#29992;&#25143;&#30340;&#26631;&#31614;&#65292;&#21487;&#20197;&#20102;&#35299;&#20182;&#20204;&#30340;&#20852;&#36259;&#65292;&#20174;&#32780;&#25552;&#39640;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29992;&#25143;&#33258;&#23450;&#20041;&#26631;&#31614;&#30340;&#20219;&#24847;&#24615;&#21644;&#32570;&#20047;&#38480;&#21046;&#65292;&#30830;&#23450;&#20854;&#30830;&#20999;&#21547;&#20041;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#25991;&#21033;&#29992;&#25968;&#23398;&#21644;&#32479;&#35745;&#26041;&#27861;&#30830;&#23450;&#26631;&#31614;&#30340;&#35789;&#27719;&#30456;&#20284;&#24615;&#21644;&#20849;&#29616;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#20998;&#37197;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#21478;&#22806;&#65292;&#32771;&#34385;&#21040;&#29992;&#25143;&#20852;&#36259;&#38543;&#26102;&#38388;&#21464;&#21270;&#65292;&#26412;&#25991;&#36824;&#22312;&#20849;&#29616;&#26631;&#31614;&#20013;&#32771;&#34385;&#20102;&#26631;&#31614;&#20998;&#37197;&#30340;&#26102;&#38388;&#20197;&#30830;&#23450;&#26631;&#31614;&#30340;&#30456;&#20284;&#24615;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#26631;&#31614;&#30340;&#30456;&#20284;&#24615;&#21019;&#24314;&#22270;&#24418;&#27169;&#22411;&#26469;&#24314;&#27169;&#29992;&#25143;&#30340;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many researchers have used tag information to improve the performance of recommendation techniques in recommender systems. Examining the tags of users will help to get their interests and leads to more accuracy in the recommendations. Since user-defined tags are chosen freely and without any restrictions, problems arise in determining their exact meaning and the similarity of tags. However, using thesaurus and ontologies to find the meaning of tags is not very efficient due to their free definition by users and the use of different languages in many data sets. Therefore, this article uses mathematical and statistical methods to determine lexical similarity and co-occurrence tags solution to assign semantic similarity. On the other hand, due to the change of users' interests over time this article has considered the time of tag assignments in co-occurrence tags for determining similarity of tags. Then the graph is created based on similarity of tags. For modeling the interests of the us
&lt;/p&gt;</description></item><item><title>FactCheck in finance&#26159;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#32593;&#32476;&#26032;&#38395;&#32858;&#21512;&#22120;&#65292;&#33021;&#22815;&#20174;&#22810;&#35821;&#35328;&#26032;&#38395;&#28304;&#20013;&#25552;&#21462;&#37325;&#35201;&#30340;&#37329;&#34701;&#20107;&#20214;&#65292;&#24182;&#36890;&#36807;&#26080;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#23545;&#20854;&#36827;&#34892;&#32858;&#31867;&#12290;&#36890;&#36807;&#20351;&#29992;Transformer-based&#20107;&#23454;&#26680;&#26597;&#22120;&#26469;&#26816;&#26597;&#26032;&#38395;&#25991;&#31456;&#30340;&#21487;&#20449;&#24230;&#65292;&#35813;&#31995;&#32479;&#26174;&#31034;&#20986;&#20248;&#20110;&#20960;&#20010;&#24378;&#22522;&#20934;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2106.15221</link><description>&lt;p&gt;
&#20107;&#23454;&#26680;&#26597;: &#20998;&#26512;&#22810;&#35821;&#35328;&#26032;&#38395;&#28304;&#20013;&#30340;&#37329;&#34701;&#20107;&#20214;
&lt;/p&gt;
&lt;p&gt;
Fact Check: Analyzing Financial Events from Multilingual News Sources. (arXiv:2106.15221v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.15221
&lt;/p&gt;
&lt;p&gt;
FactCheck in finance&#26159;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#32593;&#32476;&#26032;&#38395;&#32858;&#21512;&#22120;&#65292;&#33021;&#22815;&#20174;&#22810;&#35821;&#35328;&#26032;&#38395;&#28304;&#20013;&#25552;&#21462;&#37325;&#35201;&#30340;&#37329;&#34701;&#20107;&#20214;&#65292;&#24182;&#36890;&#36807;&#26080;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#23545;&#20854;&#36827;&#34892;&#32858;&#31867;&#12290;&#36890;&#36807;&#20351;&#29992;Transformer-based&#20107;&#23454;&#26680;&#26597;&#22120;&#26469;&#26816;&#26597;&#26032;&#38395;&#25991;&#31456;&#30340;&#21487;&#20449;&#24230;&#65292;&#35813;&#31995;&#32479;&#26174;&#31034;&#20986;&#20248;&#20110;&#20960;&#20010;&#24378;&#22522;&#20934;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#37329;&#34701;&#26032;&#38395;&#25968;&#25454;&#30340;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#24613;&#21095;&#22686;&#21152;&#65292;&#20351;&#24471;&#25237;&#36164;&#20998;&#26512;&#24072;&#36234;&#26469;&#36234;&#38590;&#20197;&#33719;&#21462;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#21644;&#36827;&#34892;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FactCheck in finance&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#32593;&#32476;&#26032;&#38395;&#32858;&#21512;&#22120;&#65292;&#20197;&#20174;&#22810;&#35821;&#35328;&#26032;&#38395;&#28304;&#20013;&#25552;&#20379;&#25237;&#36164;&#20998;&#26512;&#24072;&#25972;&#20307;&#35270;&#35282;&#19979;&#30340;&#37325;&#35201;&#37329;&#34701;&#20107;&#20214;&#65292;&#24182;&#20351;&#29992;&#26080;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#25552;&#21462;&#20107;&#20214;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32593;&#32476;&#30028;&#38754;&#65292;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#20107;&#23454;&#26680;&#26597;&#22120;&#26469;&#26816;&#26597;&#26032;&#38395;&#25991;&#31456;&#30340;&#21487;&#20449;&#24230;&#12290;&#35813;&#20107;&#23454;&#26680;&#26597;&#22120;&#30340;&#24615;&#33021;&#20351;&#29992;&#19982;&#24182;&#36141;&#20107;&#20214;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#26174;&#31034;&#20248;&#20110;&#20960;&#20010;&#24378;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The explosion in the sheer magnitude and complexity of financial news data in recent years makes it increasingly challenging for investment analysts to extract valuable insights and perform analysis. We propose FactCheck in finance, a web-based news aggregator with deep learning models, to provide analysts with a holistic view of important financial events from multilingual news sources and extract events using an unsupervised clustering method. A web interface is provided to examine the credibility of news articles using a transformer-based fact-checker. The performance of the fact checker is evaluated using a dataset related to merger and acquisition (M\&amp;A) events and is shown to outperform several strong baselines.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#33258;&#22238;&#24402;&#31574;&#30053;&#20998;&#35299;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22788;&#29702;&#31526;&#21495;&#20851;&#31995;&#38382;&#39064;&#30340;&#21487;&#21464;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#65292;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#23637;&#29616;&#20102;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#21644;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;-shot&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2009.12462</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#33258;&#22238;&#24402;&#31574;&#30053;&#20998;&#35299;&#30340;&#31526;&#21495;&#20851;&#31995;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Symbolic Relational Deep Reinforcement Learning based on Graph Neural Networks and Autoregressive Policy Decomposition. (arXiv:2009.12462v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.12462
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#33258;&#22238;&#24402;&#31574;&#30053;&#20998;&#35299;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22788;&#29702;&#31526;&#21495;&#20851;&#31995;&#38382;&#39064;&#30340;&#21487;&#21464;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#65292;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#23637;&#29616;&#20102;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#21644;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;-shot&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20851;&#27880;&#20110;&#20197;&#23545;&#35937;&#12289;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#21160;&#20316;&#26469;&#33258;&#28982;&#23450;&#20041;&#30340;&#31526;&#21495;&#20851;&#31995;&#38382;&#39064;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#36825;&#20123;&#38382;&#39064;&#20855;&#26377;&#21487;&#21464;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#65292;&#23545;&#20110;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#32780;&#35328;&#65292;&#25214;&#21040;&#19968;&#20010;&#22266;&#23450;&#38271;&#24230;&#30340;&#34920;&#31034;&#26159;&#22256;&#38590;&#30340;&#65292;&#29978;&#33267;&#19981;&#21487;&#33021;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#33258;&#22238;&#24402;&#31574;&#30053;&#20998;&#35299;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#28982;&#22320;&#24212;&#29992;&#20110;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#19988;&#23436;&#20840;&#26159;&#39046;&#22495;&#26080;&#20851;&#30340;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#39046;&#22495;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#38382;&#39064;&#22823;&#23567;&#19978;&#24341;&#20154;&#27880;&#30446;&#30340;&#38646;-shot&#27867;&#21270;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We focus on reinforcement learning (RL) in relational problems that are naturally defined in terms of objects, their relations, and object-centric actions. These problems are characterized by variable state and action spaces, and finding a fixed-length representation, required by most existing RL methods, is difficult, if not impossible. We present a deep RL framework based on graph neural networks and auto-regressive policy decomposition that naturally works with these problems and is completely domain-independent. We demonstrate the framework's broad applicability in three distinct domains and show impressive zero-shot generalization over different problem sizes.
&lt;/p&gt;</description></item></channel></rss>