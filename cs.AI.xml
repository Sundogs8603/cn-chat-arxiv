<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#32852;&#21512;&#35268;&#21010;&#26041;&#27861;&#65292;&#23558;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#19982;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#32467;&#21512;&#65292;&#23454;&#29616;&#22312;&#39640;&#24230;&#20132;&#20114;&#30340;&#39550;&#39542;&#22330;&#26223;&#20013;&#20026;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#35268;&#21010;&#23433;&#20840;&#30340;&#36816;&#21160;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2310.18301</link><description>&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#30340;&#20132;&#20114;&#24335;&#36816;&#21160;&#35268;&#21010;&#19982;&#32852;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Interactive Motion Planning for Autonomous Vehicles with Joint Optimization. (arXiv:2310.18301v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#32852;&#21512;&#35268;&#21010;&#26041;&#27861;&#65292;&#23558;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#19982;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#32467;&#21512;&#65292;&#23454;&#29616;&#22312;&#39640;&#24230;&#20132;&#20114;&#30340;&#39550;&#39542;&#22330;&#26223;&#20013;&#20026;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#35268;&#21010;&#23433;&#20840;&#30340;&#36816;&#21160;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#24230;&#20132;&#20114;&#30340;&#39550;&#39542;&#22330;&#26223;&#20013;&#65292;&#19968;&#20010;&#36710;&#36742;&#30340;&#34892;&#21160;&#20250;&#26497;&#22823;&#22320;&#24433;&#21709;&#21040;&#20854;&#21608;&#22260;&#36710;&#36742;&#30340;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#26679;&#30340;&#20132;&#20114;&#29615;&#22659;&#20013;&#20026;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#35268;&#21010;&#23433;&#20840;&#30340;&#36816;&#21160;&#36335;&#24452;&#38656;&#35201;&#32771;&#34385;&#33258;&#36523;&#24847;&#22270;&#34892;&#21160;&#23545;&#21608;&#22260;&#36710;&#36742;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#22312;&#30456;&#20851;&#30740;&#31350;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#35768;&#22810;&#27169;&#22411;&#37117;&#25903;&#25345;&#20197;&#33258;&#36523;&#26465;&#20214;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#65292;&#21033;&#29992;&#33258;&#36523;&#26465;&#20214;&#30340;&#39044;&#27979;&#22312;&#19979;&#28216;&#35268;&#21010;&#20013;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38480;&#21046;&#20102;&#35268;&#21010;&#22120;&#30340;&#32467;&#26500;&#65292;&#20363;&#22914;&#37319;&#26679;&#22411;&#35268;&#21010;&#22120;&#12290;&#23613;&#31649;&#37319;&#26679;&#22411;&#35268;&#21010;&#22120;&#33021;&#22815;&#29983;&#25104;&#31934;&#32454;&#30340;&#39640;&#36136;&#37327;&#36816;&#21160;&#36335;&#24452;&#65292;&#20294;&#22522;&#20110;&#26799;&#24230;&#30340;&#35268;&#21010;&#31639;&#27861;&#65292;&#22914;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#65292;&#30001;&#20110;&#20854;&#36845;&#20195;&#24615;&#36136;&#21644;&#23545;&#26799;&#24230;&#30340;&#38656;&#27714;&#65292;&#24456;&#38590;&#21033;&#29992;&#33258;&#36523;&#26465;&#20214;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20132;&#20114;&#24335;&#32852;&#21512;&#35268;&#21010;&#65288;IJP&#65289;&#65292;&#23558;MPC&#19982;
&lt;/p&gt;
&lt;p&gt;
In highly interactive driving scenarios, the actions of one agent greatly influences those of its neighbors. Planning safe motions for autonomous vehicles in such interactive environments, therefore, requires reasoning about the impact of the ego's intended motion plan on nearby agents' behavior. Deep-learning-based models have recently achieved great success in trajectory prediction and many models in the literature allow for ego-conditioned prediction. However, leveraging ego-conditioned prediction remains challenging in downstream planning due to the complex nature of neural networks, limiting the planner structure to simple ones, e.g., sampling-based planner. Despite their ability to generate fine-grained high-quality motion plans, it is difficult for gradient-based planning algorithms, such as model predictive control (MPC), to leverage ego-conditioned prediction due to their iterative nature and need for gradient. We present Interactive Joint Planning (IJP) that bridges MPC with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#32771;&#34385;&#20102;&#24403;&#21069;&#36890;&#36947;&#29420;&#31435;&#31574;&#30053;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#26159;&#21542;&#26159;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CSC&#30340;&#36890;&#36947;&#33258;&#32858;&#31867;&#31574;&#30053;&#26469;&#22686;&#24378;&#24615;&#33021;&#24182;&#20943;&#23567;&#21442;&#25968;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2310.17658</link><description>&lt;p&gt;
&#36890;&#36947;&#29420;&#31435;&#31574;&#30053;&#26159;&#21542;&#26159;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26368;&#20339;&#35299;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Channel Independent strategy optimal for Time Series Forecasting?. (arXiv:2310.17658v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#32771;&#34385;&#20102;&#24403;&#21069;&#36890;&#36947;&#29420;&#31435;&#31574;&#30053;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#26159;&#21542;&#26159;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CSC&#30340;&#36890;&#36947;&#33258;&#32858;&#31867;&#31574;&#30053;&#26469;&#22686;&#24378;&#24615;&#33021;&#24182;&#20943;&#23567;&#21442;&#25968;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#20986;&#29616;&#20102;&#35768;&#22810;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#21333;&#19968;&#32447;&#24615;&#23618;&#30340;&#36890;&#36947;&#30456;&#20851;(CD)&#25110;&#36890;&#36947;&#29420;&#31435;(CI)&#24314;&#27169;&#65292;&#29978;&#33267;&#21487;&#20197;&#36229;&#36807;&#35768;&#22810;&#22797;&#26434;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#23558;CD&#21644;CI&#35270;&#20026;&#20004;&#31181;&#20114;&#34917;&#20294;&#20114;&#26021;&#30340;&#26041;&#27861;&#65292;&#26080;&#27861;&#21516;&#26102;&#21033;&#29992;&#36825;&#20004;&#20010;&#26497;&#31471;&#12290;&#32780;&#19988;&#65292;CD&#21644;CI&#37117;&#26159;&#38745;&#24577;&#31574;&#30053;&#65292;&#26080;&#27861;&#22312;&#27809;&#26377;&#22823;&#37327;&#23454;&#39564;&#30340;&#24773;&#20917;&#19979;&#30830;&#23450;&#26159;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;&#26368;&#20339;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20102;&#24403;&#21069;CI&#31574;&#30053;&#26159;&#21542;&#26159;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#31216;&#20026;CSC&#65288;&#36890;&#36947;&#33258;&#32858;&#31867;&#31574;&#30053;&#65289;&#65292;&#29992;&#20110;&#32447;&#24615;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#36890;&#36947;&#33258;&#32858;&#31867;&#31574;&#30053;&#22686;&#24378;&#20102;CI&#31574;&#30053;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#20943;&#23567;&#20102;&#21442;&#25968;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been an emergence of various models for long-term time series forecasting. Recent studies have demonstrated that a single linear layer, using Channel Dependent (CD) or Channel Independent (CI) modeling, can even outperform a large number of sophisticated models. However, current research primarily considers CD and CI as two complementary yet mutually exclusive approaches, unable to harness these two extremes simultaneously. And it is also a challenging issue that both CD and CI are static strategies that cannot be determined to be optimal for a specific dataset without extensive experiments. In this paper, we reconsider whether the current CI strategy is the best solution for time series forecasting. First, we propose a simple yet effective strategy called CSC, which stands for $\mathbf{C}$hannel $\mathbf{S}$elf-$\mathbf{C}$lustering strategy, for linear models. Our Channel Self-Clustering (CSC) enhances CI strategy's performance improvements while reducing parameter size, fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;AI&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#24230;&#37327;&#30340;&#23384;&#22312;&#24615;&#12289;&#21807;&#19968;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#25552;&#20986;&#20102;&#21487;&#20197;&#39564;&#35777;&#30340;&#25968;&#23398;&#26465;&#20214;&#65292;&#24182;&#22312;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#21644;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#36827;&#34892;&#20102;&#23454;&#38469;&#35745;&#31639;&#21644;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2310.14421</link><description>&lt;p&gt;
&#23545;AI&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#24230;&#37327;&#30340;&#23384;&#22312;&#24615;&#65292;&#21807;&#19968;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On existence, uniqueness and scalability of adversarial robustness measures for AI classifiers. (arXiv:2310.14421v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;AI&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#24230;&#37327;&#30340;&#23384;&#22312;&#24615;&#12289;&#21807;&#19968;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#25552;&#20986;&#20102;&#21487;&#20197;&#39564;&#35777;&#30340;&#25968;&#23398;&#26465;&#20214;&#65292;&#24182;&#22312;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#21644;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#36827;&#34892;&#20102;&#23454;&#38469;&#35745;&#31639;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#24182;&#35777;&#26126;&#20102;&#38024;&#23545;&#65288;&#23616;&#37096;&#65289;&#21807;&#19968;&#21487;&#36870;&#20998;&#31867;&#22120;&#12289;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65288;GLM&#65289;&#21644;&#29109;AI&#65288;EAI&#65289;&#20855;&#26377;&#26368;&#23567;&#23545;&#25239;&#36335;&#24452;&#65288;MAP&#65289;&#21644;&#26368;&#23567;&#23545;&#25239;&#36317;&#31163;&#65288;MAD&#65289;&#30340;&#23384;&#22312;&#24615;&#12289;&#21807;&#19968;&#24615;&#21644;&#26126;&#30830;&#30340;&#20998;&#26512;&#35745;&#31639;&#30340;&#31616;&#21333;&#21487;&#39564;&#35777;&#30340;&#25968;&#23398;&#26465;&#20214;&#12290;&#22312;&#24120;&#35265;&#30340;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#65292;&#38024;&#23545;&#31070;&#32463;&#32593;&#32476;&#12289;&#25552;&#21319;&#38543;&#26426;&#26862;&#26519;&#12289;GLM&#21644;EAI&#31561;&#21508;&#31867;AI&#24037;&#20855;&#36827;&#34892;MAP&#21644;MAD&#30340;&#23454;&#38469;&#35745;&#31639;&#12289;&#27604;&#36739;&#21644;&#35299;&#37322;&#65292;&#21253;&#25324;&#21452;&#21367;&#29366;&#34746;&#26059;&#32447;&#21450;&#20854;&#25193;&#23637;&#20197;&#21450;&#20004;&#20010;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38382;&#39064;&#65288;&#29992;&#20110;&#20581;&#24247;&#20445;&#38505;&#29702;&#36180;&#39044;&#27979;&#21644;&#24515;&#33039;&#30149;&#21457;&#20316;&#33268;&#27515;&#29575;&#20998;&#31867;&#65289;&#12290;&#22312;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#65292;&#23637;&#31034;&#20102;MAP&#22914;&#20309;&#22312;&#39044;&#23450;&#20041;&#30340;&#21487;&#35775;&#38382;&#25511;&#21046;&#21464;&#37327;&#23376;&#38598;&#20013;&#25552;&#20379;&#21807;&#19968;&#30340;&#26368;&#23567;&#24739;&#32773;&#29305;&#23450;&#39118;&#38505;&#32531;&#35299;&#24178;&#39044;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simply-verifiable mathematical conditions for existence, uniqueness and explicit analytical computation of minimal adversarial paths (MAP) and minimal adversarial distances (MAD) for (locally) uniquely-invertible classifiers, for generalized linear models (GLM), and for entropic AI (EAI) are formulated and proven. Practical computation of MAP and MAD, their comparison and interpretations for various classes of AI tools (for neuronal networks, boosted random forests, GLM and EAI) are demonstrated on the common synthetic benchmarks: on a double Swiss roll spiral and its extensions, as well as on the two biomedical data problems (for the health insurance claim predictions, and for the heart attack lethality classification). On biomedical applications it is demonstrated how MAP provides unique minimal patient-specific risk-mitigating interventions in the predefined subsets of accessible control variables.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#25209;&#21028;&#24615;&#35843;&#26597;&#20998;&#26512;&#20102;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#19982;&#20844;&#24179;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25351;&#20986;XAI&#22312;&#23454;&#29616;&#20844;&#24179;&#29702;&#24819;&#26041;&#38754;&#23384;&#22312;&#28508;&#21147;&#21644;&#38480;&#21046;&#65292;&#21628;&#21505;&#26356;&#20855;&#20307;&#22320;&#35828;&#26126;XAI&#26041;&#27861;&#22914;&#20309;&#24110;&#21161;&#35299;&#20915;&#20844;&#24179;&#29702;&#24819;&#12290;</title><link>http://arxiv.org/abs/2310.13007</link><description>&lt;p&gt;
XAI&#30340;&#20844;&#24179;&#25928;&#30410;&#30340;&#25209;&#21028;&#24615;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Critical Survey on Fairness Benefits of XAI. (arXiv:2310.13007v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13007
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#25209;&#21028;&#24615;&#35843;&#26597;&#20998;&#26512;&#20102;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#19982;&#20844;&#24179;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25351;&#20986;XAI&#22312;&#23454;&#29616;&#20844;&#24179;&#29702;&#24819;&#26041;&#38754;&#23384;&#22312;&#28508;&#21147;&#21644;&#38480;&#21046;&#65292;&#21628;&#21505;&#26356;&#20855;&#20307;&#22320;&#35828;&#26126;XAI&#26041;&#27861;&#22914;&#20309;&#24110;&#21161;&#35299;&#20915;&#20844;&#24179;&#29702;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20010;&#25209;&#21028;&#24615;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20851;&#20110;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#21644;&#20844;&#24179;&#20043;&#38388;&#20851;&#31995;&#30340;&#20856;&#22411;&#35770;&#36848;&#65292;&#20197;&#35299;&#24320;&#36825;&#20004;&#20010;&#27010;&#24565;&#20043;&#38388;&#30340;&#22810;&#32500;&#20851;&#31995;&#12290;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#21644;&#38543;&#21518;&#30340;&#23450;&#24615;&#20869;&#23481;&#20998;&#26512;&#65292;&#25105;&#20204;&#20174;175&#31687;&#35770;&#25991;&#20013;&#35782;&#21035;&#20986;&#20851;&#20110;XAI&#30340;&#20844;&#24179;&#25928;&#30410;&#30340;&#19971;&#20010;&#20856;&#22411;&#35770;&#26029;&#12290;&#25105;&#20204;&#25552;&#20986;&#20851;&#20110;&#36825;&#20123;&#35770;&#26029;&#30340;&#37325;&#35201;&#35686;&#21578;&#65292;&#24182;&#20026;&#26410;&#26469;&#22260;&#32469;XAI&#22312;&#29305;&#23450;&#20844;&#24179;&#29702;&#24819;&#20013;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#36827;&#34892;&#35752;&#35770;&#25552;&#20379;&#20102;&#19968;&#20010;&#20999;&#20837;&#28857;&#12290;&#34429;&#28982;&#25991;&#29486;&#36890;&#24120;&#35748;&#20026;XAI&#26159;&#23454;&#29616;&#22810;&#20010;&#20844;&#24179;&#29702;&#24819;&#30340;&#19968;&#31181;&#25163;&#27573;&#65292;&#20294;&#25105;&#20204;&#27880;&#24847;&#21040;&#36825;&#20123;&#29702;&#24819;&#19982;XAI&#30340;&#33021;&#21147;&#20043;&#38388;&#23384;&#22312;&#19981;&#19968;&#33268;&#12290;&#25105;&#20204;&#40723;&#21169;&#23558;XAI&#35270;&#20026;&#24212;&#23545;&#31639;&#27861;&#20844;&#24179;&#36825;&#19968;&#22810;&#32500;&#31038;&#20250;&#25216;&#26415;&#25361;&#25112;&#30340;&#20247;&#22810;&#24037;&#20855;&#20043;&#19968;&#65292;&#24182;&#26356;&#20855;&#20307;&#22320;&#35828;&#26126;&#21738;&#31181;XAI&#26041;&#27861;&#22914;&#20309;&#24110;&#21161;&#21738;&#20123;&#20154;&#35299;&#20915;&#21738;&#20123;&#20844;&#24179;&#29702;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this critical survey, we analyze typical claims on the relationship between explainable AI (XAI) and fairness to disentangle the multidimensional relationship between these two concepts. Based on a systematic literature review and a subsequent qualitative content analysis, we identify seven archetypal claims from 175 papers on the alleged fairness benefits of XAI. We present crucial caveats with respect to these claims and provide an entry point for future discussions around the potentials and limitations of XAI for specific fairness desiderata. While the literature often suggests XAI to be an enabler for several fairness desiderata, we notice a misalignment between these desiderata and the capabilities of XAI. We encourage to conceive XAI as one of many tools to approach the multidimensional, sociotechnical challenge of algorithmic fairness and to be more specific about how exactly what kind of XAI method enables whom to address which fairness desideratum.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25512;&#26029;&#24615;&#25490;&#38500;&#25552;&#31034;&#65288;IEP&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#25490;&#38500;&#21644;&#25512;&#29702;&#30340;&#21407;&#21017;&#65292;&#24341;&#23548;LLM&#36827;&#34892;&#38750;&#32447;&#24615;&#24605;&#32771;&#12290;IEP&#36890;&#36807;&#35268;&#21010;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65292;&#21487;&#20197;&#27169;&#25311;&#22797;&#26434;&#30340;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#65292;&#27604;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2310.12342</link><description>&lt;p&gt;
&#36890;&#36807;&#25512;&#29702;&#19982;&#35268;&#21010;&#28040;&#38500;&#25512;&#29702;&#65306;&#19968;&#31181;&#24341;&#23548;LLMs&#38750;&#32447;&#24615;&#24605;&#32500;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Eliminating Reasoning via Inferring with Planning: A New Framework to Guide LLMs' Non-linear Thinking. (arXiv:2310.12342v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25512;&#26029;&#24615;&#25490;&#38500;&#25552;&#31034;&#65288;IEP&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#25490;&#38500;&#21644;&#25512;&#29702;&#30340;&#21407;&#21017;&#65292;&#24341;&#23548;LLM&#36827;&#34892;&#38750;&#32447;&#24615;&#24605;&#32771;&#12290;IEP&#36890;&#36807;&#35268;&#21010;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65292;&#21487;&#20197;&#27169;&#25311;&#22797;&#26434;&#30340;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#65292;&#27604;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Thought Chain&#65288;CoT&#65289;&#25552;&#31034;&#21450;&#20854;&#21464;&#20307;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#32447;&#24615;&#35748;&#30693;&#21644;&#36923;&#36753;&#65292;&#25506;&#32034;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35013;&#22791;&#39640;&#32423;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#24605;&#32500;&#22797;&#26434;&#19988;&#28151;&#21512;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#24605;&#32500;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#24335;&#65292;&#31216;&#20026;&#25512;&#26029;&#24615;&#25490;&#38500;&#25552;&#31034;&#65288;IEP&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#25490;&#38500;&#21644;&#25512;&#29702;&#30340;&#21407;&#21017;&#65292;&#20197;&#24341;&#23548;LLM&#36827;&#34892;&#38750;&#32447;&#24615;&#24605;&#32771;&#12290;IEP&#25351;&#23548;LLM&#36827;&#34892;&#35268;&#21010;&#65292;&#24182;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#25512;&#26029;&#27599;&#20010;&#21487;&#33021;&#35299;&#19982;&#19978;&#19979;&#25991;&#12289;&#24120;&#35782;&#25110;&#20107;&#23454;&#30340;&#25512;&#29702;&#20851;&#31995;&#65292;&#20174;&#32780;&#36890;&#36807;&#22238;&#28335;&#25512;&#29702;&#33719;&#24471;&#26356;&#24191;&#27867;&#30340;&#35270;&#35282;&#12290;&#30456;&#27604;&#20854;&#20182;&#22522;&#20110;CoT&#30340;&#26041;&#27861;&#65292;IEP&#30340;&#21069;&#21521;&#35268;&#21010;&#21644;&#21518;&#21521;&#25490;&#38500;&#36807;&#31243;&#26356;&#22909;&#22320;&#27169;&#25311;&#20102;&#22797;&#26434;&#30340;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#65292;&#21518;&#32773;&#20165;&#21453;&#26144;&#32447;&#24615;&#35748;&#30693;&#36807;&#31243;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#39564;&#35777;&#20102;IEP&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought(CoT) prompting and its variants explore equipping large language models (LLMs) with high-level reasoning abilities by emulating human-like linear cognition and logic. However, the human mind is complicated and mixed with both linear and nonlinear thinking. In this work, we propose \textbf{I}nferential \textbf{E}xclusion \textbf{P}rompting (IEP), a novel prompting that combines the principles of elimination and inference in order to guide LLMs to think non-linearly. IEP guides LLMs to plan and then utilize Natural Language Inference (NLI) to deduce each possible solution's entailment relation with context, commonsense, or facts, therefore yielding a broader perspective by thinking back for inferring. This forward planning and backward eliminating process allows IEP to better simulate the complex human thinking processes compared to other CoT-based methods, which only reflect linear cognitive processes. We conducted a series of empirical studies and have corroborated tha
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;ChatGPT&#26725;&#25509;&#29702;&#35770;&#25299;&#25169;&#27010;&#24565;&#21644;&#35745;&#31639;&#25299;&#25169;&#23454;&#29616;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#32534;&#30721;&#25216;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;ChatGPT&#30340;&#24110;&#21161;&#65292;&#32431;&#31929;&#30340;&#29702;&#35770;&#23478;&#22914;&#20309;&#23558;&#25968;&#23398;&#20844;&#24335;&#21644;&#27010;&#24565;&#36716;&#21270;&#20026;&#21151;&#33021;&#24615;&#30340;&#35745;&#31639;&#25299;&#25169;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2310.07570</link><description>&lt;p&gt;
ChatGPT&#29992;&#20110;&#35745;&#31639;&#25299;&#25169;&#23398;
&lt;/p&gt;
&lt;p&gt;
ChatGPT for Computational Topology. (arXiv:2310.07570v2 [math.AT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07570
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;ChatGPT&#26725;&#25509;&#29702;&#35770;&#25299;&#25169;&#27010;&#24565;&#21644;&#35745;&#31639;&#25299;&#25169;&#23454;&#29616;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#32534;&#30721;&#25216;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;ChatGPT&#30340;&#24110;&#21161;&#65292;&#32431;&#31929;&#30340;&#29702;&#35770;&#23478;&#22914;&#20309;&#23558;&#25968;&#23398;&#20844;&#24335;&#21644;&#27010;&#24565;&#36716;&#21270;&#20026;&#21151;&#33021;&#24615;&#30340;&#35745;&#31639;&#25299;&#25169;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#37324;&#31243;&#30865;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22312;&#25968;&#23398;&#29615;&#22659;&#20013;&#65292;&#23427;&#24448;&#24448;&#21463;&#21040;&#27010;&#24565;&#38169;&#35823;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#26159;&#19968;&#20010;&#30456;&#23545;&#36739;&#26032;&#30340;&#23398;&#31185;&#65292;&#22312;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35745;&#31639;&#31639;&#27861;&#21644;&#32534;&#30721;&#25216;&#33021;&#22312;&#29702;&#35770;&#23478;&#20013;&#30340;&#29702;&#35299;&#36824;&#26377;&#38480;&#65292;&#36825;&#38459;&#30861;&#20102;&#25299;&#25169;&#23398;&#30340;&#36827;&#23637;&#12290;&#36825;&#39033;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;ChatGPT&#26469;&#24357;&#21512;&#29702;&#35770;&#25299;&#25169;&#27010;&#24565;&#19982;&#35745;&#31639;&#25299;&#25169;&#23454;&#29616;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#27809;&#26377;&#35745;&#31639;&#32463;&#39564;&#21644;&#32534;&#30721;&#25216;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#32431;&#31929;&#30340;&#29702;&#35770;&#23478;&#22914;&#20309;&#20511;&#21161;ChatGPT&#23558;&#25968;&#23398;&#20844;&#24335;&#21644;&#27010;&#24565;&#26377;&#25928;&#22320;&#36716;&#21270;&#20026;&#21151;&#33021;&#24615;&#30340;&#35745;&#31639;&#25299;&#25169;&#20195;&#30721;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#27010;&#36848;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT represents a significant milestone in the field of artificial intelligence (AI), finding widespread applications across diverse domains. However, its effectiveness in mathematical contexts has been somewhat constrained by its susceptibility to conceptual errors. Concurrently, topological data analysis (TDA), a relatively new discipline, has garnered substantial interest in recent years. Nonetheless, the advancement of TDA is impeded by the limited understanding of computational algorithms and coding proficiency among theoreticians. This work endeavors to bridge the gap between theoretical topological concepts and their practical implementation in computational topology through the utilization of ChatGPT. We showcase how a pure theoretician, devoid of computational experience and coding skills, can effectively transform mathematical formulations and concepts into functional code for computational topology with the assistance of ChatGPT. Our strategy outlines a productive process
&lt;/p&gt;</description></item><item><title>AdaptNet&#26159;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#35282;&#33394;&#25511;&#21046;&#30340;&#31574;&#30053;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#29616;&#26377;&#31574;&#30053;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#21487;&#20197;&#20174;&#31867;&#20284;&#20219;&#21153;&#20013;&#24555;&#36895;&#23398;&#20064;&#21040;&#26032;&#30340;&#34892;&#20026;&#65292;&#26174;&#33879;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.00239</link><description>&lt;p&gt;
AdaptNet: &#22522;&#20110;&#29289;&#29702;&#30340;&#35282;&#33394;&#25511;&#21046;&#30340;&#31574;&#30053;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
AdaptNet: Policy Adaptation for Physics-Based Character Control. (arXiv:2310.00239v2 [cs.GR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00239
&lt;/p&gt;
&lt;p&gt;
AdaptNet&#26159;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#35282;&#33394;&#25511;&#21046;&#30340;&#31574;&#30053;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#29616;&#26377;&#31574;&#30053;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#21487;&#20197;&#20174;&#31867;&#20284;&#20219;&#21153;&#20013;&#24555;&#36895;&#23398;&#20064;&#21040;&#26032;&#30340;&#34892;&#20026;&#65292;&#26174;&#33879;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#20154;&#31867;&#22312;&#23398;&#20064;&#26032;&#25216;&#33021;&#26102;&#33021;&#22815;&#36866;&#24212;&#29616;&#26377;&#25216;&#33021;&#30340;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdaptNet&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20462;&#25913;&#29616;&#26377;&#31574;&#30053;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#20351;&#20854;&#33021;&#22815;&#20174;&#31867;&#20284;&#20219;&#21153;&#20013;&#24555;&#36895;&#23398;&#20064;&#21040;&#26032;&#30340;&#34892;&#20026;&#65292;&#30456;&#27604;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#12290;AdaptNet&#22312;&#32473;&#23450;&#30340;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#22120;&#22522;&#30784;&#19978;&#26500;&#24314;&#20102;&#19968;&#20010;&#20004;&#23618;&#27425;&#32467;&#26500;&#65292;&#36890;&#36807;&#22686;&#21152;&#21407;&#22987;&#29366;&#24577;&#23884;&#20837;&#26469;&#25903;&#25345;&#34892;&#20026;&#30340;&#36866;&#24230;&#21464;&#21270;&#65292;&#24182;&#36827;&#19968;&#27493;&#20462;&#25913;&#31574;&#30053;&#32593;&#32476;&#23618;&#26469;&#23454;&#29616;&#26356;&#28145;&#36828;&#30340;&#21464;&#21270;&#12290;&#35813;&#25216;&#26415;&#34987;&#35777;&#26126;&#21487;&#20197;&#26377;&#25928;&#22320;&#36866;&#24212;&#29616;&#26377;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#25511;&#21046;&#22120;&#20197;&#36866;&#24212;&#24191;&#27867;&#30340;&#26032;&#30340;&#36816;&#21160;&#39118;&#26684;&#12289;&#26032;&#30340;&#20219;&#21153;&#30446;&#26631;&#12289;&#35282;&#33394;&#24418;&#24577;&#30340;&#21464;&#21270;&#20197;&#21450;&#29615;&#22659;&#30340;&#24191;&#27867;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#19982;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#25110;&#20351;&#29992;&#20854;&#20182;&#20462;&#25913;&#29616;&#26377;&#31574;&#30053;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#26174;&#31034;&#20986;&#26174;&#33879;&#25552;&#39640;&#30340;&#23398;&#20064;&#25928;&#29575;&#65292;&#34920;&#29616;&#20026;&#22823;&#22823;&#32553;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#20195;&#30721;&#21487;&#22312;https://motion-&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by humans' ability to adapt skills in the learning of new ones, this paper presents AdaptNet, an approach for modifying the latent space of existing policies to allow new behaviors to be quickly learned from like tasks in comparison to learning from scratch. Building on top of a given reinforcement learning controller, AdaptNet uses a two-tier hierarchy that augments the original state embedding to support modest changes in a behavior and further modifies the policy network layers to make more substantive changes. The technique is shown to be effective for adapting existing physics-based controllers to a wide range of new styles for locomotion, new task targets, changes in character morphology and extensive changes in environment. Furthermore, it exhibits significant increase in learning efficiency, as indicated by greatly reduced training times when compared to training from scratch or using other approaches that modify existing policies. Code is available at https://motion-
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#22312;CT&#25195;&#25551;&#20998;&#31867;&#20013;&#30340;&#32467;&#26524;&#24448;&#24448;&#21482;&#20851;&#27880;&#20934;&#30830;&#24615;&#65292;&#32780;&#24573;&#35270;&#20102;&#20844;&#27491;&#24615;&#21644;&#35299;&#37322;&#24615;&#65292;&#23548;&#33268;&#27169;&#22411;&#19981;&#21487;&#20449;&#21644;&#19981;&#36866;&#29992;&#20110;&#30495;&#23454;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2309.12632</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;CT&#25195;&#25551;&#20998;&#31867;&#20013;&#30340;&#32467;&#26524;&#26159;&#21542;&#20844;&#27491;&#21487;&#35299;&#37322;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Deep Learning Classification Results Obtained on CT Scans Fair and Interpretable?. (arXiv:2309.12632v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12632
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;CT&#25195;&#25551;&#20998;&#31867;&#20013;&#30340;&#32467;&#26524;&#24448;&#24448;&#21482;&#20851;&#27880;&#20934;&#30830;&#24615;&#65292;&#32780;&#24573;&#35270;&#20102;&#20844;&#27491;&#24615;&#21644;&#35299;&#37322;&#24615;&#65292;&#23548;&#33268;&#27169;&#22411;&#19981;&#21487;&#20449;&#21644;&#19981;&#36866;&#29992;&#20110;&#30495;&#23454;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#22270;&#20687;&#21644;&#29289;&#20307;&#20998;&#31867;&#20013;&#30340;&#24040;&#22823;&#25104;&#21151;&#65292;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#20063;&#38754;&#20020;&#30528;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#21160;&#35786;&#26029;&#26696;&#20363;&#30340;&#21387;&#21147;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#25991;&#29486;&#20013;&#22823;&#22810;&#25968;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20998;&#31867;&#23581;&#35797;&#20165;&#20165;&#20851;&#27880;&#26497;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#19981;&#32771;&#34385;&#21487;&#35299;&#37322;&#24615;&#25110;&#32773;&#24739;&#32773;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#20998;&#31163;&#12290;&#20363;&#22914;&#65292;&#22823;&#37096;&#20998;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#32954;&#32467;&#33410;&#20998;&#31867;&#35770;&#25991;&#20250;&#23545;&#25968;&#25454;&#36827;&#34892;&#38543;&#26426;&#27927;&#29260;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#27979;&#35797;&#38598;&#65292;&#23548;&#33268;&#19968;&#20010;&#20154;&#30340;CT&#25195;&#25551;&#22270;&#20687;&#20013;&#30340;&#26576;&#20123;&#22270;&#20687;&#20301;&#20110;&#35757;&#32451;&#38598;&#20013;&#65292;&#32780;&#20854;&#20182;&#22270;&#20687;&#21017;&#20301;&#20110;&#39564;&#35777;&#25110;&#27979;&#35797;&#22270;&#20687;&#38598;&#20013;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#35823;&#23548;&#24615;&#30340;&#20934;&#30830;&#29575;&#25253;&#21578;&#21644;&#23398;&#20064;&#21040;&#30340;&#26080;&#20851;&#29305;&#24449;&#65292;&#26368;&#32456;&#38477;&#20302;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following the great success of various deep learning methods in image and object classification, the biomedical image processing society is also overwhelmed with their applications to various automatic diagnosis cases. Unfortunately, most of the deep learning-based classification attempts in the literature solely focus on the aim of extreme accuracy scores, without considering interpretability, or patient-wise separation of training and test data. For example, most lung nodule classification papers using deep learning randomly shuffle data and split it into training, validation, and test sets, causing certain images from the CT scan of a person to be in the training set, while other images of the exact same person to be in the validation or testing image sets. This can result in reporting misleading accuracy rates and the learning of irrelevant features, ultimately reducing the real-life usability of these models. When the deep neural networks trained on the traditional, unfair data sh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#20154;&#31867;&#21161;&#21147;&#28789;&#24039;&#25235;&#21462;&#8221;&#30340;&#26032;&#22411;&#20219;&#21153;&#65292;&#36890;&#36807;&#20351;&#29992;Grasping Gradient Field&#21644;&#22522;&#20110;&#21382;&#21490;&#26465;&#20214;&#30340;&#27531;&#24046;&#31574;&#30053;&#65292;&#35757;&#32451;&#25511;&#21046;&#26426;&#22120;&#20154;&#25163;&#25351;&#20197;&#36866;&#24212;&#19981;&#21516;&#29992;&#25143;&#24847;&#22270;&#21644;&#29289;&#20307;&#20960;&#20309;&#24418;&#29366;&#30340;&#28789;&#24039;&#25235;&#21462;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2309.06038</link><description>&lt;p&gt;
&#20026;&#20154;&#31867;&#21161;&#21147;&#28789;&#24039;&#25235;&#21462;&#23398;&#20064;&#22522;&#20110;&#24471;&#20998;&#30340;&#25235;&#21462;&#21407;&#35821;
&lt;/p&gt;
&lt;p&gt;
Learning Score-based Grasping Primitive for Human-assisting Dexterous Grasping. (arXiv:2309.06038v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#20154;&#31867;&#21161;&#21147;&#28789;&#24039;&#25235;&#21462;&#8221;&#30340;&#26032;&#22411;&#20219;&#21153;&#65292;&#36890;&#36807;&#20351;&#29992;Grasping Gradient Field&#21644;&#22522;&#20110;&#21382;&#21490;&#26465;&#20214;&#30340;&#27531;&#24046;&#31574;&#30053;&#65292;&#35757;&#32451;&#25511;&#21046;&#26426;&#22120;&#20154;&#25163;&#25351;&#20197;&#36866;&#24212;&#19981;&#21516;&#29992;&#25143;&#24847;&#22270;&#21644;&#29289;&#20307;&#20960;&#20309;&#24418;&#29366;&#30340;&#28789;&#24039;&#25235;&#21462;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#20154;&#31867;&#21161;&#21147;&#28789;&#24039;&#25235;&#21462;&#8221;&#30340;&#26032;&#22411;&#20219;&#21153;&#65292;&#26088;&#22312;&#35757;&#32451;&#25511;&#21046;&#26426;&#22120;&#20154;&#25163;&#25351;&#20197;&#24110;&#21161;&#29992;&#25143;&#25235;&#21462;&#29289;&#20307;&#30340;&#31574;&#30053;&#12290;&#19982;&#20256;&#32479;&#30340;&#28789;&#24039;&#25235;&#21462;&#19981;&#21516;&#65292;&#36825;&#20010;&#20219;&#21153;&#38754;&#20020;&#30528;&#26356;&#22797;&#26434;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#31574;&#30053;&#38656;&#35201;&#36866;&#24212;&#19981;&#21516;&#30340;&#29992;&#25143;&#24847;&#22270;&#21644;&#29289;&#20307;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#30001;&#20004;&#20010;&#23376;&#27169;&#22359;&#32452;&#25104;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65306;&#19968;&#31181;&#25163;-&#29289;&#20307;&#26465;&#20214;&#25235;&#21462;&#21407;&#35821;&#31216;&#20026;Grasping Gradient Field&#65288;GraspGF&#65289;&#65292;&#20197;&#21450;&#19968;&#31181;&#22522;&#20110;&#21382;&#21490;&#26465;&#20214;&#30340;&#27531;&#24046;&#31574;&#30053;&#12290;GraspGF&#36890;&#36807;&#20272;&#35745;&#26469;&#33258;&#25104;&#21151;&#25235;&#21462;&#31034;&#20363;&#38598;&#30340;&#26799;&#24230;&#26469;&#23398;&#20064;&#8220;&#22914;&#20309;&#8221;&#25235;&#21462;&#65292;&#32780;&#27531;&#24046;&#31574;&#30053;&#26681;&#25454;&#36712;&#36857;&#21382;&#21490;&#30830;&#23450;&#8220;&#20309;&#26102;&#8221;&#21644;&#20197;&#20309;&#31181;&#36895;&#24230;&#25191;&#34892;&#25235;&#21462;&#21160;&#20316;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of anthropomorphic robotic hands for assisting individuals in situations where human hands may be unavailable or unsuitable has gained significant importance. In this paper, we propose a novel task called human-assisting dexterous grasping that aims to train a policy for controlling a robotic hand's fingers to assist users in grasping objects. Unlike conventional dexterous grasping, this task presents a more complex challenge as the policy needs to adapt to diverse user intentions, in addition to the object's geometry. We address this challenge by proposing an approach consisting of two sub-modules: a hand-object-conditional grasping primitive called Grasping Gradient Field~(GraspGF), and a history-conditional residual policy. GraspGF learns `how' to grasp by estimating the gradient from a success grasping example set, while the residual policy determines `when' and at what speed the grasping action should be executed based on the trajectory history. Experimental results demons
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#26469;&#34913;&#37327;&#32534;&#31243;&#35821;&#35328;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;&#23398;&#20064;&#25110;&#29702;&#35299;&#65292;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2308.15452</link><description>&lt;p&gt;
&#20160;&#20040;&#26102;&#20505;&#32534;&#31243;&#24605;&#32500;&#23545;&#25512;&#29702;&#36215;&#20316;&#29992;?
&lt;/p&gt;
&lt;p&gt;
When Do Program-of-Thoughts Work for Reasoning?. (arXiv:2308.15452v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15452
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#26469;&#34913;&#37327;&#32534;&#31243;&#35821;&#35328;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;&#23398;&#20064;&#25110;&#29702;&#35299;&#65292;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#22312;&#20307;&#29616;&#20986;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23613;&#31649;&#20687;&#32534;&#31243;&#24605;&#32500;&#25552;&#31034;&#36825;&#26679;&#30340;&#26041;&#27861;&#23545;&#20110;&#20351;&#29992;&#32534;&#31243;&#35821;&#35328;&#26469;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;LLM&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#20195;&#30721;&#25968;&#25454;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#20855;&#20307;&#24433;&#21709;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#32467;&#26500;&#21644;&#36923;&#36753;&#23646;&#24615;&#65292;&#20197;&#34913;&#37327;&#20195;&#30721;&#21644;&#25512;&#29702;&#33021;&#21147;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#26469;&#32534;&#30721;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#38590;&#24230;&#21644;&#22280;&#22797;&#26434;&#24230;&#26469;&#35745;&#31639;&#36923;&#36753;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;LLM&#23398;&#20064;&#25110;&#29702;&#35299;&#12290;&#26368;&#20339;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#36890;&#36807;&#32534;&#31243;&#36741;&#21161;&#25552;&#31034;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#21518;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#21160;&#21512;&#25104;&#30340;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
The reasoning capabilities of Large Language Models (LLMs) play a pivotal role in the realm of embodied artificial intelligence. Although there are effective methods like program-of-thought prompting for LLMs which uses programming language to tackle complex reasoning tasks, the specific impact of code data on the improvement of reasoning capabilities remains under-explored. To address this gap, we propose complexity-impacted reasoning score (CIRS), which combines structural and logical attributes, to measure the correlation between code and reasoning abilities. Specifically, we use the abstract syntax tree to encode the structural information and calculate logical complexity by considering the difficulty and the cyclomatic complexity. Through an empirical analysis, we find not all code data of complexity can be learned or understood by LLMs. Optimal level of complexity is critical to the improvement of reasoning abilities by program-aided prompting. Then we design an auto-synthesizing
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#24490;&#29615;&#31070;&#32463;&#30005;&#36335;&#22914;&#20309;&#20174;&#22797;&#26434;&#27010;&#29575;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#65292;&#24182;&#35777;&#26126;&#20102;&#24102;&#26377;&#21333;&#29420;&#36755;&#20986;&#21333;&#20803;&#30340;&#31070;&#32463;&#30005;&#36335;&#30340;&#21457;&#25918;&#29575;&#21160;&#21147;&#23398;&#21487;&#20197;&#20174;&#20219;&#24847;&#27010;&#29575;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#12290;</title><link>http://arxiv.org/abs/2308.11809</link><description>&lt;p&gt;
&#22312;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#34920;&#36798;&#24615;&#27010;&#29575;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Expressive probabilistic sampling in recurrent neural networks. (arXiv:2308.11809v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11809
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#24490;&#29615;&#31070;&#32463;&#30005;&#36335;&#22914;&#20309;&#20174;&#22797;&#26434;&#27010;&#29575;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#65292;&#24182;&#35777;&#26126;&#20102;&#24102;&#26377;&#21333;&#29420;&#36755;&#20986;&#21333;&#20803;&#30340;&#31070;&#32463;&#30005;&#36335;&#30340;&#21457;&#25918;&#29575;&#21160;&#21147;&#23398;&#21487;&#20197;&#20174;&#20219;&#24847;&#27010;&#29575;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#37319;&#26679;&#30340;&#22823;&#33041;&#21151;&#33021;&#36125;&#21494;&#26031;&#27169;&#22411;&#20013;&#65292;&#20551;&#35774;&#31070;&#32463;&#27963;&#21160;&#26159;&#26469;&#33258;&#22823;&#33041;&#29992;&#20110;&#27010;&#29575;&#35745;&#31639;&#30340;&#27010;&#29575;&#20998;&#24067;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#31070;&#32463;&#21160;&#21147;&#23398;&#26426;&#21046;&#27169;&#22411;&#22914;&#20309;&#20174;&#20219;&#24847;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#20173;&#28982;&#32570;&#20047;&#20840;&#38754;&#29702;&#35299;&#12290;&#25105;&#20204;&#20351;&#29992;&#20989;&#25968;&#20998;&#26512;&#21644;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#24037;&#20855;&#26469;&#25506;&#32034;$\textit{&#24490;&#29615;}$&#31070;&#32463;&#30005;&#36335;&#20174;&#22797;&#26434;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#30340;&#26368;&#23567;&#26550;&#26500;&#35201;&#27714;&#12290;&#39318;&#20808;&#25105;&#20204;&#32771;&#34385;&#20256;&#32479;&#30340;&#37319;&#26679;&#27169;&#22411;&#65292;&#23427;&#30001;&#19968;&#20010;&#31070;&#32463;&#20803;&#32593;&#32476;&#32452;&#25104;&#65292;&#20854;&#36755;&#20986;&#30452;&#25509;&#34920;&#31034;&#26679;&#26412;&#65288;&#20165;&#37319;&#26679;&#22120;&#32593;&#32476;&#65289;&#12290;&#25105;&#20204;&#35748;&#20026;&#20256;&#32479;&#27169;&#22411;&#20013;&#30340;&#31361;&#35302;&#30005;&#27969;&#21644;&#21457;&#25918;&#29575;&#21160;&#21147;&#23398;&#33021;&#22815;&#20174;&#22797;&#26434;&#27010;&#29575;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#24102;&#26377;&#21333;&#29420;&#30340;&#36755;&#20986;&#21333;&#20803;&#38598;&#30340;&#24490;&#29615;&#31070;&#32463;&#30005;&#36335;&#30340;&#21457;&#25918;&#29575;&#21160;&#21147;&#23398;&#21487;&#20197;&#20174;&#20219;&#24847;&#27010;&#29575;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
In sampling-based Bayesian models of brain function, neural activities are assumed to be samples from probability distributions that the brain uses for probabilistic computation. However, a comprehensive understanding of how mechanistic models of neural dynamics can sample from arbitrary distributions is still lacking. We use tools from functional analysis and stochastic differential equations to explore the minimum architectural requirements for $\textit{recurrent}$ neural circuits to sample from complex distributions. We first consider the traditional sampling model consisting of a network of neurons whose outputs directly represent the samples (sampler-only network). We argue that synaptic current and firing-rate dynamics in the traditional model have limited capacity to sample from a complex probability distribution. We show that the firing rate dynamics of a recurrent neural circuit with a separate set of output units can sample from an arbitrary probability distribution. We call 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#22270;&#20687;&#21644;&#30495;&#23454;&#22270;&#20687;&#65292;&#25552;&#39640;&#20102;&#38750;&#37202;&#31934;&#24615;&#33026;&#32938;&#24615;&#32925;&#30149;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.06507</link><description>&lt;p&gt;
&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#25913;&#36827;&#38750;&#37202;&#31934;&#24615;&#33026;&#32938;&#24615;&#32925;&#30149;&#20998;&#31867;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Improving Nonalcoholic Fatty Liver Disease Classification Performance With Latent Diffusion Models. (arXiv:2307.06507v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#22270;&#20687;&#21644;&#30495;&#23454;&#22270;&#20687;&#65292;&#25552;&#39640;&#20102;&#38750;&#37202;&#31934;&#24615;&#33026;&#32938;&#24615;&#32925;&#30149;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#28145;&#24230;&#23398;&#20064;&#19982;&#20020;&#24202;&#19987;&#19994;&#30693;&#35782;&#30456;&#32467;&#21512;&#22312;&#35299;&#20915;&#21307;&#30103;&#25361;&#25112;&#21644;&#25552;&#20379;&#25913;&#36827;&#35786;&#26029;&#24037;&#20855;&#26041;&#38754;&#20855;&#26377;&#24456;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24448;&#24448;&#38656;&#35201;&#26377;&#27880;&#37322;&#30340;&#21307;&#23398;&#22270;&#20687;&#25104;&#20026;&#20805;&#20998;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24378;&#22823;&#33021;&#21147;&#30340;&#38556;&#30861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#32467;&#21512;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#22270;&#20687;&#21644;&#30495;&#23454;&#22270;&#20687;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#39640;&#38750;&#37202;&#31934;&#24615;&#33026;&#32938;&#24615;&#32925;&#30149;&#65288;NAFLD&#65289;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#20004;&#20010;&#25351;&#26631;&#26469;&#35780;&#20272;&#21512;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#65306;&#22522;&#20110;&#25193;&#25955;&#29983;&#25104;&#30340;&#22270;&#20687;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#29983;&#25104;&#30340;&#22270;&#20687;&#35745;&#31639;&#30340;Inception Score&#65288;IS&#65289;&#21644;Fr\'{e}chet Inception Distance&#65288;FID&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#25193;&#25955;&#29983;&#25104;&#30340;&#22270;&#20687;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#26368;&#22823;IS&#24471;&#20998;&#20026;1.90&#65292;&#32780;GANs&#20026;1.67&#65292;&#26368;&#23567;FID&#24471;&#20998;&#20026;69.45&#65292;&#32780;GANs&#20026;99.53&#12290;&#21033;&#29992;&#37096;&#20998;&#20923;&#32467;&#30340;CNN&#39592;&#24178;&#65288;EfficientNet v1&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating deep learning with clinical expertise holds great potential for addressing healthcare challenges and empowering medical professionals with improved diagnostic tools. However, the need for annotated medical images is often an obstacle to leveraging the full power of machine learning models. Our research demonstrates that by combining synthetic images, generated using diffusion models, with real images, we can enhance nonalcoholic fatty liver disease (NAFLD) classification performance. We evaluate the quality of the synthetic images by comparing two metrics: Inception Score (IS) and Fr\'{e}chet Inception Distance (FID), computed on diffusion-generated images and generative adversarial networks (GANs)-generated images. Our results show superior performance for the diffusion-generated images, with a maximum IS score of $1.90$ compared to $1.67$ for GANs, and a minimum FID score of $69.45$ compared to $99.53$ for GANs. Utilizing a partially frozen CNN backbone (EfficientNet v1),
&lt;/p&gt;</description></item><item><title>&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24403;&#20248;&#21270;&#38382;&#39064;&#30340;&#39118;&#26223;&#22312;&#19981;&#21516;&#36845;&#20195;&#20013;&#24046;&#24322;&#36739;&#22823;&#26102;&#65292;&#37325;&#32622;&#20248;&#21270;&#22120;&#30340;&#20869;&#37096;&#21442;&#25968;&#21487;&#20197;&#36991;&#20813;&#27745;&#26579;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17833</link><description>&lt;p&gt;
&#37325;&#32622;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20248;&#21270;&#22120;&#65306;&#19968;&#20010;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Resetting the Optimizer in Deep RL: An Empirical Study. (arXiv:2306.17833v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17833
&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24403;&#20248;&#21270;&#38382;&#39064;&#30340;&#39118;&#26223;&#22312;&#19981;&#21516;&#36845;&#20195;&#20013;&#24046;&#24322;&#36739;&#22823;&#26102;&#65292;&#37325;&#32622;&#20248;&#21270;&#22120;&#30340;&#20869;&#37096;&#21442;&#25968;&#21487;&#20197;&#36991;&#20813;&#27745;&#26579;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#36817;&#20284;&#35745;&#31639;&#26368;&#20248;&#20540;&#20989;&#25968;&#30340;&#20219;&#21153;&#12290;&#36825;&#20010;&#36845;&#20195;&#36807;&#31243;&#21253;&#25324;&#22312;&#27599;&#20010;&#36845;&#20195;&#20013;&#35299;&#20915;&#19968;&#31995;&#21015;&#19981;&#21516;&#36845;&#20195;&#20013;&#30446;&#26631;&#20989;&#25968;&#21487;&#33021;&#25913;&#21464;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#20351;&#29992;&#29616;&#20195;&#21464;&#31181;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#22914;Adam&#12290;&#36825;&#20123;&#20248;&#21270;&#22120;&#20445;&#25345;&#33258;&#24049;&#30340;&#20869;&#37096;&#21442;&#25968;&#65292;&#22914;&#26799;&#24230;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#30697;&#20272;&#35745;&#65292;&#24182;&#38543;&#26102;&#38388;&#26356;&#26032;&#36825;&#20123;&#21442;&#25968;&#12290;&#22240;&#27492;&#65292;&#20043;&#21069;&#36845;&#20195;&#30340;&#20449;&#24687;&#34987;&#29992;&#26469;&#22312;&#24403;&#21069;&#36845;&#20195;&#20013;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#20551;&#35774;&#22312;&#20043;&#21069;&#36845;&#20195;&#30340;&#20248;&#21270;&#39118;&#26223;&#19982;&#24403;&#21069;&#36845;&#20195;&#30456;&#24046;&#36739;&#22823;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#21487;&#33021;&#20250;&#27745;&#26579;&#25152;&#20351;&#29992;&#20248;&#21270;&#22120;&#30340;&#20869;&#37096;&#21442;&#25968;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#31181;&#24433;&#21709;&#65292;&#19968;&#20010;&#31616;&#21333;&#30340;&#24819;&#27861;&#26159;&#22312;&#24320;&#22987;&#26032;&#30340;&#36845;&#20195;&#26102;&#37325;&#32622;&#20248;&#21270;&#22120;&#30340;&#20869;&#37096;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We focus on the task of approximating the optimal value function in deep reinforcement learning. This iterative process is comprised of approximately solving a sequence of optimization problems where the objective function can change per iteration. The common approach to solving the problem is to employ modern variants of the stochastic gradient descent algorithm such as Adam. These optimizers maintain their own internal parameters such as estimates of the first and the second moment of the gradient, and update these parameters over time. Therefore, information obtained in previous iterations is being used to solve the optimization problem in the current iteration. We hypothesize that this can contaminate the internal parameters of the employed optimizer in situations where the optimization landscape of the previous iterations is quite different from the current iteration. To hedge against this effect, a simple idea is to reset the internal parameters of the optimizer when starting a n
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#22797;&#26434;&#24230;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#26368;&#23567;&#21270;&#21487;&#25511;&#22240;&#32032;&#30340;&#25968;&#37327;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#21464;&#39118;&#37327;&#24320;&#25918;&#24335;&#21150;&#20844;&#23460; HVAC &#31995;&#32479;&#30340;&#33021;&#28304;&#20248;&#21270;&#65292;&#19982;&#20256;&#32479;&#31995;&#32479;&#30456;&#27604;&#33021;&#28304;&#28040;&#32791;&#20943;&#23569;&#20102; 37%&#65292;&#19988;&#28201;&#24230;&#33539;&#22260;&#36829;&#35268;&#29575;&#26497;&#20302; (&lt;1%)&#12290;</title><link>http://arxiv.org/abs/2306.13333</link><description>&lt;p&gt;
&#22810;&#21464;&#39118;&#37327;&#24320;&#25918;&#24335;&#21150;&#20844;&#23460;&#20013; HVAC &#31995;&#32479;&#33021;&#28304;&#20248;&#21270;&#65306;&#19968;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Energy Optimization for HVAC Systems in Multi-VAV Open Offices: A Deep Reinforcement Learning Approach. (arXiv:2306.13333v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#22797;&#26434;&#24230;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#26368;&#23567;&#21270;&#21487;&#25511;&#22240;&#32032;&#30340;&#25968;&#37327;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#21464;&#39118;&#37327;&#24320;&#25918;&#24335;&#21150;&#20844;&#23460; HVAC &#31995;&#32479;&#30340;&#33021;&#28304;&#20248;&#21270;&#65292;&#19982;&#20256;&#32479;&#31995;&#32479;&#30456;&#27604;&#33021;&#28304;&#28040;&#32791;&#20943;&#23569;&#20102; 37%&#65292;&#19988;&#28201;&#24230;&#33539;&#22260;&#36829;&#35268;&#29575;&#26497;&#20302; (&lt;1%)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#36229;&#36807; 32% &#30340;&#33021;&#28304;&#29992;&#20110;&#21830;&#19994;&#21644;&#20303;&#23429;&#24314;&#31569;&#65292;&#36843;&#20999;&#38656;&#35201;&#37325;&#26032;&#23457;&#35270;&#20256;&#32479;&#30340;&#24314;&#31569;&#33021;&#28304;&#31649;&#29702;&#26041;&#27861;&#12290;&#30001;&#20110; HVAC &#31995;&#32479;&#21344;&#21830;&#19994;&#37096;&#38376;&#24635;&#33021;&#32791;&#30340;&#32422; 40%&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#22797;&#26434;&#24230; DRL &#27169;&#22411;&#30340;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#23454;&#29616;&#24320;&#25918;&#24335;&#21150;&#20844;&#23460; HVAC &#33021;&#28304;&#20248;&#21270;&#65292;&#20165;&#20351;&#29992;&#23569;&#37327;&#21487;&#25511;&#21644;&#21487;&#35775;&#38382;&#22240;&#32032;&#12290;&#36890;&#36807;&#19982;&#30495;&#23454;&#24314;&#31569;&#20013;&#22522;&#20110;&#29616;&#26377; HVAC &#35745;&#21010;&#30340;&#22522;&#32447;&#31995;&#32479;&#36827;&#34892;&#27604;&#36739;&#65292;&#35780;&#20272;&#20102;&#25105;&#20204;&#35299;&#20915;&#26041;&#26696;&#30340;&#25972;&#20307;&#33021;&#28304;&#28040;&#32791;&#21644;&#28909;&#33298;&#36866;&#24230;&#27700;&#24179;&#12290;&#35813;&#27604;&#36739;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24037;&#20316;&#26102;&#38388;&#20869;&#23454;&#29616;&#20102; 37% &#30340;&#33021;&#28304;&#28040;&#32791;&#33410;&#32422;&#65292;&#36829;&#35268;&#28201;&#24230;&#33539;&#22260;&#21344;&#27604;&#26368;&#20302; (&lt;1%)&#12290;&#35757;&#32451;&#19968;&#20010;&#24615;&#33021;&#20248;&#36234;&#30340;&#35206;&#30422; d
&lt;/p&gt;
&lt;p&gt;
With more than 32% of the global energy used by commercial and residential buildings, there is an urgent need to revisit traditional approaches to Building Energy Management (BEM). With HVAC systems accounting for about 40% of the total energy cost in the commercial sector, we propose a low-complexity DRL-based model with multi-input multi-output architecture for the HVAC energy optimization of open-plan offices, which uses only a handful of controllable and accessible factors. The efficacy of our solution is evaluated through extensive analysis of the overall energy consumption and thermal comfort levels compared to a baseline system based on the existing HVAC schedule in a real building. This comparison shows that our method achieves 37% savings in energy consumption with minimum violation (&lt;1%) of the desired temperature range during work hours. It takes only a total of 40 minutes for 5 epochs (about 7.75 minutes per epoch) to train a network with superior performance and covering d
&lt;/p&gt;</description></item><item><title>AdCraft&#26159;&#19968;&#31181;&#39640;&#32423;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#29615;&#22659;&#65292;&#29992;&#20110;&#27169;&#25311;&#20986;&#20215;&#21644;&#39044;&#31639;&#21464;&#21270;&#30340;&#25628;&#32034;&#24341;&#25806;&#33829;&#38144;(SEM)&#27963;&#21160;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#21644;&#25552;&#39640;SEM&#20986;&#20215;&#21644;&#39044;&#31639;&#31649;&#29702;&#30456;&#20851;&#30340;RL&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.11971</link><description>&lt;p&gt;
AdCraft&#65306;&#19968;&#31181;&#29992;&#20110;&#25628;&#32034;&#24341;&#25806;&#33829;&#38144;&#20248;&#21270;&#30340;&#39640;&#32423;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
AdCraft: An Advanced Reinforcement Learning Benchmark Environment for Search Engine Marketing Optimization. (arXiv:2306.11971v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11971
&lt;/p&gt;
&lt;p&gt;
AdCraft&#26159;&#19968;&#31181;&#39640;&#32423;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#29615;&#22659;&#65292;&#29992;&#20110;&#27169;&#25311;&#20986;&#20215;&#21644;&#39044;&#31639;&#21464;&#21270;&#30340;&#25628;&#32034;&#24341;&#25806;&#33829;&#38144;(SEM)&#27963;&#21160;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#21644;&#25552;&#39640;SEM&#20986;&#20215;&#21644;&#39044;&#31639;&#31649;&#29702;&#30456;&#20851;&#30340;RL&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#29615;&#22659;&#8212;&#8212; AdCraft&#65292;&#20854;&#20855;&#26377;&#38543;&#26426;&#21644;&#38750;&#38745;&#24577;&#29305;&#24615;&#12290;&#35813;&#29615;&#22659;&#27169;&#25311;&#20102;&#25628;&#32034;&#24341;&#25806;&#33829;&#38144;&#20013;&#20986;&#20215;&#21644;&#39044;&#31639;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;SEM&#26159;&#19968;&#31181;&#21033;&#29992;&#20184;&#36153;&#24191;&#21578;&#26469;&#22686;&#21152;&#32593;&#31449;&#22312;&#25628;&#32034;&#24341;&#25806;&#32467;&#26524;&#39029;&#38754;&#19978;&#30340;&#21487;&#35265;&#24615;&#30340;&#25968;&#23383;&#33829;&#38144;&#25216;&#26415;&#12290;SEM&#24191;&#21578;&#27963;&#21160;&#30340;&#34920;&#29616;&#21462;&#20915;&#20110;&#22810;&#20010;&#22240;&#32032;&#65292;&#21253;&#25324;&#20851;&#38190;&#23383;&#36873;&#25321;&#12289;&#24191;&#21578;&#35774;&#35745;&#12289;&#20986;&#20215;&#31649;&#29702;&#12289;&#39044;&#31639;&#35843;&#25972;&#21644;&#34920;&#29616;&#30417;&#25511;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26368;&#36817;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#20248;&#21270;SEM&#24191;&#21578;&#25237;&#25918;&#27963;&#21160;&#30340;&#28508;&#22312;&#31574;&#30053;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#65292;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#25104;&#26412;&#39640;&#26114;&#25110;&#19981;&#21487;&#34892;&#12290;&#25105;&#20204;&#30340;&#21487;&#23450;&#21046;&#29615;&#22659;&#20351;&#20174;&#19994;&#32773;&#33021;&#22815;&#35780;&#20272;&#21644;&#25552;&#39640;&#19982;SEM&#20986;&#20215;&#21644;&#39044;&#31639;&#31649;&#29702;&#30456;&#20851;&#30340;RL&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#26080;&#38656;&#20184;&#20986;&#36825;&#20123;&#25104;&#26412;&#12290;&#36890;&#36807;&#22312;AdCraft&#29615;&#22659;&#19979;&#36827;&#34892;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;
&lt;/p&gt;
&lt;p&gt;
We introduce \env{}, a novel benchmark environment for the Reinforcement Learning (RL) community distinguished by its stochastic and non-stationary properties. The environment simulates bidding and budgeting dynamics within Search Engine Marketing (SEM), a digital marketing technique utilizing paid advertising to enhance the visibility of websites on search engine results pages (SERPs). The performance of SEM advertisement campaigns depends on several factors, including keyword selection, ad design, bid management, budget adjustments, and performance monitoring. Deep RL recently emerged as a potential strategy to optimize campaign profitability within the complex and dynamic landscape of SEM but it requires substantial data, which may be costly or infeasible to acquire in practice. Our customizable environment enables practitioners to assess and enhance the robustness of RL algorithms pertinent to SEM bid and budget management without such costs. Through a series of experiments within 
&lt;/p&gt;</description></item><item><title>BackpropTools&#26159;&#19968;&#27454;&#24555;&#36895;&#12289;&#21487;&#31227;&#26893;&#30340;&#36830;&#32493;&#25511;&#21046;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24211;&#65292;&#23427;&#36890;&#36807;&#27169;&#26495;&#20803;&#32534;&#31243;&#25552;&#20379;&#32039;&#23494;&#38598;&#25104;&#30340;&#21487;&#32452;&#21512;&#32452;&#20214;&#65292;&#24182;&#22312;&#24322;&#26500;&#24179;&#21488;&#38598;&#21512;&#19978;&#26080;&#32541;&#20351;&#29992;&#65292;&#21516;&#26102;&#22312;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#30340;&#28145;&#24230;RL&#20195;&#29702;&#39640;&#25928;&#21487;&#25193;&#23637;&#35757;&#32451;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#30001;&#20110;&#20854;&#21487;&#31227;&#26893;&#24615;&#21644;&#23454;&#26102;&#20445;&#35777;&#65292;&#23427;&#25104;&#20026;&#20102;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#23398;&#26469;&#30340;&#31574;&#30053;&#30340;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2306.03530</link><description>&lt;p&gt;
BackpropTools: &#19968;&#27454;&#24555;&#36895;&#12289;&#21487;&#31227;&#26893;&#30340;&#36830;&#32493;&#25511;&#21046;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24211;
&lt;/p&gt;
&lt;p&gt;
BackpropTools: A Fast, Portable Deep Reinforcement Learning Library for Continuous Control. (arXiv:2306.03530v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03530
&lt;/p&gt;
&lt;p&gt;
BackpropTools&#26159;&#19968;&#27454;&#24555;&#36895;&#12289;&#21487;&#31227;&#26893;&#30340;&#36830;&#32493;&#25511;&#21046;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24211;&#65292;&#23427;&#36890;&#36807;&#27169;&#26495;&#20803;&#32534;&#31243;&#25552;&#20379;&#32039;&#23494;&#38598;&#25104;&#30340;&#21487;&#32452;&#21512;&#32452;&#20214;&#65292;&#24182;&#22312;&#24322;&#26500;&#24179;&#21488;&#38598;&#21512;&#19978;&#26080;&#32541;&#20351;&#29992;&#65292;&#21516;&#26102;&#22312;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#30340;&#28145;&#24230;RL&#20195;&#29702;&#39640;&#25928;&#21487;&#25193;&#23637;&#35757;&#32451;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#30001;&#20110;&#20854;&#21487;&#31227;&#26893;&#24615;&#21644;&#23454;&#26102;&#20445;&#35777;&#65292;&#23427;&#25104;&#20026;&#20102;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#23398;&#26469;&#30340;&#31574;&#30053;&#30340;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#20135;&#29983;&#20986;&#20855;&#26377;&#33021;&#21147;&#30340;&#20195;&#29702;&#21644;&#25511;&#21046;&#31574;&#30053;&#65292;&#20294;&#24120;&#24120;&#21463;&#21040;&#35757;&#32451;&#26102;&#38388;&#36807;&#38271;&#30340;&#22256;&#25200;&#12290;&#27492;&#22806;&#65292;&#22312;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#28145;&#24230;&#23398;&#20064;&#24211;&#30340;&#23454;&#26102;&#24615;&#21644;&#21487;&#31227;&#26893;&#24615;&#30340;&#32570;&#20047;&#38480;&#21046;&#20102;&#23398;&#20064;&#31574;&#30053;&#22312;&#23454;&#38469;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BackpropTools&#65292;&#19968;&#31181;&#20381;&#36182;&#24615;-free&#12289;header-only&#12289;pure C++&#30340;&#28145;&#24230;&#30417;&#30563;&#21644;&#24378;&#21270;&#23398;&#20064;&#24211;&#12290;&#21033;&#29992;&#26368;&#36817;C++&#26631;&#20934;&#30340;&#27169;&#26495;&#20803;&#32534;&#31243;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21487;&#20197;&#30001;&#32534;&#35793;&#22120;&#32039;&#23494;&#38598;&#25104;&#30340;&#21487;&#32452;&#21512;&#32452;&#20214;&#12290;&#20854;&#26032;&#39062;&#30340;&#26550;&#26500;&#20801;&#35768;BackpropTools&#22312;&#24322;&#26500;&#24179;&#21488;&#38598;&#21512;&#19978;&#26080;&#32541;&#20351;&#29992;&#65292;&#20174;HPC&#38598;&#32676;&#12289;&#24037;&#20316;&#31449;&#21644;&#31508;&#35760;&#26412;&#30005;&#33041;&#21040;&#26234;&#33021;&#25163;&#26426;&#12289;&#26234;&#33021;&#25163;&#34920;&#21644;&#24494;&#25511;&#21046;&#22120;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#30001;&#20110;RL&#31639;&#27861;&#19982;&#27169;&#25311;&#29615;&#22659;&#30340;&#32039;&#23494;&#38598;&#25104;&#65292;BackpropTools&#22312;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#30340;&#28145;&#24230;RL&#20195;&#29702;&#30340;&#39640;&#25928;&#21487;&#25193;&#23637;&#35757;&#32451;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#23427;&#30340;&#21487;&#31227;&#26893;&#24615;&#21644;&#23454;&#26102;&#20445;&#35777;&#20351;&#20854;&#25104;&#20026;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#23398;&#26469;&#30340;&#31574;&#30053;&#30340;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning (RL) has been demonstrated to yield capable agents and control policies in several domains but is commonly plagued by prohibitively long training times. Additionally, in the case of continuous control problems, the applicability of learned policies on real-world embedded devices is limited due to the lack of real-time guarantees and portability of existing deep learning libraries. To address these challenges, we present BackpropTools, a dependency-free, header-only, pure C++ library for deep supervised and reinforcement learning. Leveraging the template meta-programming capabilities of recent C++ standards, we provide composable components that can be tightly integrated by the compiler. Its novel architecture allows BackpropTools to be used seamlessly on a heterogeneous set of platforms, from HPC clusters over workstations and laptops to smartphones, smartwatches, and microcontrollers. Specifically, due to the tight integration of the RL algorithms with simu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33539;&#30068;&#35770;&#30340;&#19990;&#30028;&#29366;&#24577;&#34920;&#31034;&#21644;&#36716;&#25442;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#20351;&#29992;&#30693;&#35782;&#22270;&#21644;&#20851;&#31995;&#25968;&#25454;&#24211;&#26469;&#24314;&#27169;&#35268;&#21010;&#20013;&#19990;&#30028;&#29366;&#24577;&#21644;&#26356;&#26032;&#30340;&#27491;&#24335;&#35821;&#20041;&#12290;</title><link>http://arxiv.org/abs/2305.17208</link><description>&lt;p&gt;
&#30693;&#35782;&#20026;&#22522;&#30784;&#30340;&#35268;&#21010;&#30340;&#33539;&#30068;&#34920;&#36798;&#35821;&#35328;&#21644;&#35745;&#31639;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Categorical Representation Language and Computational System for Knowledge-Based Planning. (arXiv:2305.17208v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33539;&#30068;&#35770;&#30340;&#19990;&#30028;&#29366;&#24577;&#34920;&#31034;&#21644;&#36716;&#25442;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#20351;&#29992;&#30693;&#35782;&#22270;&#21644;&#20851;&#31995;&#25968;&#25454;&#24211;&#26469;&#24314;&#27169;&#35268;&#21010;&#20013;&#19990;&#30028;&#29366;&#24577;&#21644;&#26356;&#26032;&#30340;&#27491;&#24335;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#19968;&#38454;&#36923;&#36753;&#30340;&#32463;&#20856;&#35268;&#21010;&#34920;&#36798;&#35821;&#35328;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24314;&#27169;&#21644;&#35299;&#20915;&#35268;&#21010;&#38382;&#39064;&#65292;&#20294;&#26159;&#22312;&#22797;&#26434;&#30340;&#35268;&#21010;&#22330;&#26223;&#20013;&#24448;&#24448;&#38590;&#20197;&#25429;&#25417;&#21040;&#38544;&#21547;&#30340;&#21069;&#25552;&#21644;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#31034;&#21644;&#36716;&#25442;&#35268;&#21010;&#36807;&#31243;&#20013;&#19990;&#30028;&#29366;&#24577;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#22522;&#20110;&#33539;&#30068;&#35770;&#30340;C&#38598;&#21644;&#21452;&#25512;&#25104;&#37325;&#20889;&#65288;DPO&#65289;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#34920;&#31034;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#36825;&#20123;&#30693;&#35782;&#25903;&#25345;&#21508;&#20010;&#23618;&#27425;&#30340;&#39046;&#22495;&#25277;&#35937;&#12290;&#23427;&#26681;&#25454;&#29992;&#25143;&#25552;&#20379;&#30340;&#26412;&#20307;&#23398;&#34920;&#31034;&#35859;&#35789;&#30340;&#35821;&#20041;&#65292;&#24182;&#22312;&#29366;&#24577;&#36716;&#25442;&#26102;&#20445;&#25345;&#35821;&#20041;&#30340;&#19968;&#33268;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#20026;&#20351;&#29992;&#30693;&#35782;&#22270;&#21644;&#20851;&#31995;&#25968;&#25454;&#24211;&#26469;&#24314;&#27169;&#35268;&#21010;&#20013;&#30340;&#19990;&#30028;&#29366;&#24577;&#21644;&#26356;&#26032;&#25552;&#20379;&#20102;&#27491;&#24335;&#30340;&#35821;&#20041;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#33539;&#30068;&#35770;&#34920;&#31034;&#19982;&#32463;&#20856;&#35268;&#21010;&#34920;&#31034;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical planning representation languages based on first-order logic have been extensively used to model and solve planning problems, but they struggle to capture implicit preconditions and effects that arise in complex planning scenarios. To address this problem, we propose an alternative approach to representing and transforming world states during planning. Based on the category-theoretic concepts of $\mathsf{C}$-sets and double-pushout rewriting (DPO), our proposed representation can effectively handle structured knowledge about world states that support domain abstractions at all levels. It formalizes the semantics of predicates according to a user-provided ontology and preserves the semantics when transitioning between world states. This method provides a formal semantics for using knowledge graphs and relational databases to model world states and updates in planning. In this paper, we compare our category-theoretic representation with the classical planning representation. We
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#35299;&#20915;&#27169;&#31946;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#37327;&#27979;&#37327;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#37325;&#22797;&#24615;&#65292;&#25214;&#20986;&#20102;&#22312;&#21547;&#31946;&#38382;&#39064;&#38598;&#20013;&#22238;&#31572;&#39640;&#31934;&#24230;&#23376;&#38598;&#38382;&#39064;&#30340;&#26368;&#21487;&#38752;&#26041;&#27861;&#12290;&#36825;&#31181;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#27495;&#20041;&#65292;&#24182;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14613</link><description>&lt;p&gt;
&#23545;&#27169;&#31946;&#38382;&#39064;&#30340;&#26377;&#36873;&#25321;&#24615;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Selectively Answering Ambiguous Questions. (arXiv:2305.14613v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#35299;&#20915;&#27169;&#31946;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#37327;&#27979;&#37327;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#37325;&#22797;&#24615;&#65292;&#25214;&#20986;&#20102;&#22312;&#21547;&#31946;&#38382;&#39064;&#38598;&#20013;&#22238;&#31572;&#39640;&#31934;&#24230;&#23376;&#38598;&#38382;&#39064;&#30340;&#26368;&#21487;&#38752;&#26041;&#27861;&#12290;&#36825;&#31181;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#27495;&#20041;&#65292;&#24182;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#30340;&#35821;&#35328;&#27169;&#22411;&#24212;&#35813;&#22312;&#19981;&#30693;&#36947;&#31572;&#26696;&#30340;&#24773;&#20917;&#19979;&#25918;&#24323;&#22238;&#31572;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38382;&#38382;&#32773;&#24847;&#22270;&#25110;&#19978;&#19979;&#25991;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#38382;&#39064;&#30340;&#31572;&#26696;&#20063;&#21487;&#33021;&#19981;&#28165;&#26970;&#12290;&#26412;&#30740;&#31350;&#20174;&#36825;&#20010;&#35282;&#24230;&#35843;&#26597;&#20102;&#38382;&#39064;&#22238;&#31572;&#65292;&#19987;&#27880;&#20110;&#22312;&#20247;&#22810;&#26412;&#36136;&#19978;&#21547;&#31946;&#30340;&#38382;&#39064;&#38598;&#20013;&#22238;&#31572;&#39640;&#31934;&#24230;&#23376;&#38598;&#30340;&#38382;&#39064;&#12290;&#22312;&#27492;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#23450;&#37327;&#27979;&#37327;&#19968;&#32452;&#37319;&#26679;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#37325;&#22797;&#24615;&#26159;&#26368;&#21487;&#38752;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#32780;&#38750;&#20808;&#21069;&#24037;&#20316;&#20013;&#20351;&#29992;&#30340;&#27169;&#22411;&#30340;&#27010;&#29575;&#25110;&#33258;&#25105;&#39564;&#35777;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#26041;&#27861;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#19981;&#21516;&#30340;&#27169;&#22411;&#35268;&#27169;&#65292;&#20197;&#21450;&#24102;&#25110;&#19981;&#24102;&#25351;&#23548;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#27495;&#20041;&#65292;&#24182;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trustworthy language models should abstain from answering questions when they do not know the answer. However, the answer to a question can be unknown for a variety of reasons. Prior research has focused on the case in which the question is clear and the answer is unambiguous but possibly unknown. However, the answer to a question can also be unclear due to uncertainty of the questioner's intent or context. We investigate question answering from this perspective, focusing on answering a subset of questions with a high degree of accuracy, from a set of questions in which many are inherently ambiguous. In this setting, we find that the most reliable approach to calibration involves quantifying repetition within a set of sampled model outputs, rather than the model's likelihood or self-verification as used in prior work. % We find this to be the case across different types of uncertainty, varying model scales and both with or without instruction tuning. Our results suggest that sampling-b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#30340;&#36755;&#20837;&#36873;&#25321;&#20250;&#23545;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;&#22312;&#22522;&#20934;&#27979;&#35797;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#8220;&#33258;&#25105;&#22686;&#24378;&#8221;&#25216;&#26415;&#20197;&#25913;&#21892;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.13062</link><description>&lt;p&gt;
GPT4Table&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#29702;&#35299;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#21527;&#65311;&#19968;&#39033;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
GPT4Table: Can Large Language Models Understand Structured Table Data? A Benchmark and Empirical Study. (arXiv:2305.13062v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#30340;&#36755;&#20837;&#36873;&#25321;&#20250;&#23545;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;&#22312;&#22522;&#20934;&#27979;&#35797;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#8220;&#33258;&#25105;&#22686;&#24378;&#8221;&#25216;&#26415;&#20197;&#25913;&#21892;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23569;&#26679;&#26412;&#25512;&#29702;&#22120;&#26469;&#35299;&#20915;&#19982;&#33258;&#28982;&#35821;&#35328;&#30456;&#20851;&#30340;&#20219;&#21153;&#36234;&#26469;&#36234;&#20855;&#21560;&#24341;&#21147;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;LLMs&#23545;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#20363;&#22914;&#34920;&#26684;&#65289;&#30340;&#29702;&#35299;&#31243;&#24230;&#36824;&#26377;&#24456;&#22810;&#38656;&#35201;&#23398;&#20064;&#30340;&#22320;&#26041;&#12290;&#23613;&#31649;&#21487;&#20197;&#20351;&#29992;&#34920;&#26684;&#24207;&#21015;&#21270;&#20316;&#20026;LLMs&#30340;&#36755;&#20837;&#65292;&#20294;&#30446;&#21069;&#36824;&#32570;&#20047;&#23545;LLMs&#26159;&#21542;&#30495;&#27491;&#33021;&#22815;&#29702;&#35299;&#36825;&#31867;&#25968;&#25454;&#30340;&#20840;&#38754;&#30740;&#31350;&#12290;&#26412;&#25991;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;LLMs&#30340;&#32467;&#26500;&#29702;&#35299;&#33021;&#21147;&#65288;SUC&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21019;&#24314;&#30340;&#22522;&#20934;&#27979;&#35797;&#21253;&#25324;&#19971;&#20010;&#20219;&#21153;&#65292;&#27599;&#20010;&#20219;&#21153;&#37117;&#26377;&#20854;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#21333;&#20803;&#26684;&#26597;&#25214;&#12289;&#34892;&#26816;&#32034;&#21644;&#22823;&#23567;&#26816;&#27979;&#12290;&#25105;&#20204;&#23545;GPT-3.5&#21644;GPT-4&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#24615;&#33021;&#22240;&#22810;&#31181;&#36755;&#20837;&#36873;&#25321;&#32780;&#24322;&#65292;&#21253;&#25324;&#34920;&#26684;&#36755;&#20837;&#26684;&#24335;&#12289;&#20869;&#23481;&#39034;&#24207;&#12289;&#35282;&#33394;&#25552;&#31034;&#21644;&#20998;&#21306;&#26631;&#35760;&#31561;&#12290;&#26681;&#25454;&#22522;&#20934;&#27979;&#35797;&#35780;&#20272;&#25152;&#24471;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#33258;&#25105;&#22686;&#24378;&#8221;&#25216;&#26415;&#20197;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are becoming attractive as few-shot reasoners to solve Natural Language (NL)-related tasks. However, there is still much to learn about how well LLMs understand structured data, such as tables. While it is true that tables can be used as inputs to LLMs with serialization, there lack of comprehensive studies examining whether LLMs can truly comprehend such data. In this paper, we try to understand this by designing a benchmark to evaluate the structural understanding capabilities (SUC) of LLMs. The benchmark we create includes seven tasks, each with its own unique challenges, \eg, cell lookup, row retrieval, and size detection. We run a series of evaluations on GPT-3.5 and GPT-4. We discover that the performance varied depending on a number of input choices, including table input format, content order, role prompting, and partition marks. Drawing from the insights gained through the benchmark evaluations, we then propose \textit{self-augmentation} for effect
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Summarize and Score&#65288;SASC&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#33719;&#21462;&#40657;&#30418;&#25991;&#26412;&#27169;&#22359;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20197;&#21450;&#35299;&#37322;&#21487;&#38752;&#31243;&#24230;&#30340;&#20998;&#25968;&#12290;&#30740;&#31350;&#32773;&#20204;&#24050;&#32463;&#22312;&#21512;&#25104;&#27169;&#22359;&#21644;BERT&#27169;&#22411;&#20013;&#20351;&#29992;SASC&#65292;&#35753;&#25105;&#20204;&#21487;&#20197;&#35299;&#37322;&#27169;&#22359;&#30340;&#36873;&#25321;&#24615;&#65292;&#36825;&#23545;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2305.09863</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#29992;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#40657;&#30418;&#25991;&#26412;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Explaining black box text modules in natural language with language models. (arXiv:2305.09863v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Summarize and Score&#65288;SASC&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#33719;&#21462;&#40657;&#30418;&#25991;&#26412;&#27169;&#22359;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20197;&#21450;&#35299;&#37322;&#21487;&#38752;&#31243;&#24230;&#30340;&#20998;&#25968;&#12290;&#30740;&#31350;&#32773;&#20204;&#24050;&#32463;&#22312;&#21512;&#25104;&#27169;&#22359;&#21644;BERT&#27169;&#22411;&#20013;&#20351;&#29992;SASC&#65292;&#35753;&#25105;&#20204;&#21487;&#20197;&#35299;&#37322;&#27169;&#22359;&#30340;&#36873;&#25321;&#24615;&#65292;&#36825;&#23545;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#35777;&#26126;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24555;&#36895;&#22686;&#38271;&#21644;&#19981;&#36879;&#26126;&#24615;&#24050;&#32463;&#24341;&#36215;&#20102;&#23545;&#21487;&#35299;&#37322;&#24615;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#35810;&#38382;&#26159;&#21542;&#21487;&#20197;&#33258;&#21160;&#33719;&#21462;&#40657;&#30418;&#25991;&#26412;&#27169;&#22359;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;&#19968;&#20010;&#8220;&#25991;&#26412;&#27169;&#22359;&#8221;&#26159;&#23558;&#25991;&#26412;&#26144;&#23556;&#21040;&#26631;&#37327;&#36830;&#32493;&#20540;&#30340;&#20219;&#20309;&#20989;&#25968;&#65292;&#20363;&#22914;LLM&#20869;&#30340;&#23376;&#27169;&#22359;&#25110;&#22823;&#33041;&#21306;&#22495;&#30340;&#25311;&#21512;&#27169;&#22411;&#12290;&#8220;&#40657;&#30418;&#8221;&#34920;&#31034;&#25105;&#20204;&#21482;&#33021;&#35775;&#38382;&#27169;&#22359;&#30340;&#36755;&#20837;/&#36755;&#20986;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Summarize and Score&#65288;SASC&#65289;&#26041;&#27861;&#65292;&#23427;&#25509;&#21463;&#25991;&#26412;&#27169;&#22359;&#24182;&#36820;&#22238;&#27169;&#22359;&#36873;&#25321;&#24615;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20197;&#21450;&#35299;&#37322;&#21487;&#38752;&#31243;&#24230;&#30340;&#20998;&#25968;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19978;&#19979;&#25991;&#20013;&#30740;&#31350;SASC&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#27169;&#22359;&#19978;&#35780;&#20272;SASC&#65292;&#24182;&#21457;&#29616;&#23427;&#32463;&#24120;&#24674;&#22797;&#22522;&#26412;&#30495;&#30456;&#35828;&#26126;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;SASC&#26469;&#35299;&#37322;&#39044;&#35757;&#32451;BERT&#27169;&#22411;&#20013;&#30340;&#27169;&#22359;&#65292;&#20351;&#24471;&#26816;&#26597;BERT&#30340;&#27169;&#22359;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable prediction performance for a growing array of tasks. However, their rapid proliferation and increasing opaqueness have created a growing need for interpretability. Here, we ask whether we can automatically obtain natural language explanations for black box text modules. A "text module" is any function that maps text to a scalar continuous value, such as a submodule within an LLM or a fitted model of a brain region. "Black box" indicates that we only have access to the module's inputs/outputs.  We introduce Summarize and Score (SASC), a method that takes in a text module and returns a natural language explanation of the module's selectivity along with a score for how reliable the explanation is. We study SASC in 3 contexts. First, we evaluate SASC on synthetic modules and find that it often recovers ground truth explanations. Second, we use SASC to explain modules found within a pre-trained BERT model, enabling inspection of the 
&lt;/p&gt;</description></item><item><title>&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#65292;&#27169;&#22411;&#30340;&#39044;&#27979;&#26041;&#24046;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#35823;&#24046;&#26469;&#28304;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#27965;&#24615;&#26631;&#20934;&#26469;&#34913;&#37327;&#27979;&#37327;&#21644;&#20943;&#23569;&#38543;&#24847;&#24615;&#12290;&#20316;&#32773;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#22788;&#29702;&#38543;&#24847;&#24615;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25581;&#31034;&#20102;&#24403;&#21069;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#26576;&#20123;&#31867;&#22411;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.11562</link><description>&lt;p&gt;
&#39044;&#27979;&#26159;&#21542;&#38543;&#24847;&#65311;&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#35780;&#20272;&#33258;&#27965;&#24615;
&lt;/p&gt;
&lt;p&gt;
Is My Prediction Arbitrary? Measuring Self-Consistency in Fair Classification. (arXiv:2301.11562v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11562
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#65292;&#27169;&#22411;&#30340;&#39044;&#27979;&#26041;&#24046;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#35823;&#24046;&#26469;&#28304;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#27965;&#24615;&#26631;&#20934;&#26469;&#34913;&#37327;&#27979;&#37327;&#21644;&#20943;&#23569;&#38543;&#24847;&#24615;&#12290;&#20316;&#32773;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#22788;&#29702;&#38543;&#24847;&#24615;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25581;&#31034;&#20102;&#24403;&#21069;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#26576;&#20123;&#31867;&#22411;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#65292;&#19981;&#21516;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#39044;&#27979;&#26041;&#24046;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#35823;&#24046;&#26469;&#28304;&#38382;&#39064;&#12290; &#23454;&#35777;&#34920;&#26126;&#65292;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#39044;&#27979;&#30340;&#26041;&#24046;&#24046;&#24322;&#38750;&#24120;&#22823;&#65292;&#20197;&#33267;&#20110;&#20915;&#31574;&#23454;&#38469;&#19978;&#26159;&#38543;&#24847;&#30340;&#12290; &#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#20570;&#20986;&#20102;&#22235;&#20010;&#24635;&#20307;&#36129;&#29486;&#65306;&#25105;&#20204;1&#65289;&#23450;&#20041;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#24046;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#31216;&#20026;&#33258;&#27965;&#24615;&#65292;&#22312;&#27979;&#37327;&#21644;&#20943;&#23569;&#38543;&#24847;&#24615;&#26102;&#20351;&#29992;&#65307; 2&#65289;&#24320;&#21457;&#20102;&#19968;&#31181;&#21512;&#29702;&#30340;&#31639;&#27861;&#65292;&#24403;&#39044;&#27979;&#26080;&#27861;&#20570;&#20986;&#20915;&#31574;&#26102;&#65292;&#21487;&#20197;&#25918;&#24323;&#20998;&#31867;&#65307; 3&#65289;&#36827;&#34892;&#20102;&#36804;&#20170;&#20026;&#27490;&#26377;&#20851;&#20844;&#24179;&#20998;&#31867;&#20013;&#26041;&#24046;&#65288;&#30456;&#23545;&#20110;&#33258;&#27965;&#24615;&#21644;&#38543;&#24847;&#24615;&#65289;&#20316;&#29992;&#30340;&#26368;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#65307; 4&#65289;&#25512;&#20986;&#20102;&#19968;&#20010;&#24037;&#20855;&#21253;&#65292;&#20351;&#32654;&#22269;&#20303;&#25151;&#25269;&#25276;&#36151;&#27454;&#25259;&#38706;&#27861;&#26696;&#65288;HMDA&#65289;&#25968;&#25454;&#38598;&#26131;&#20110;&#29992;&#20110;&#26410;&#26469;&#30740;&#31350;&#12290; &#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#25581;&#31034;&#20102;&#20851;&#20110;&#21487;&#37325;&#22797;&#24615;&#30340;&#20196;&#20154;&#38663;&#24778;&#30340;&#35265;&#35299;&#12290;&#24403;&#32771;&#34385;&#21040;&#26041;&#24046;&#21644;&#38543;&#24847;&#39044;&#27979;&#30340;&#21487;&#33021;&#24615;&#26102;&#65292;&#22823;&#22810;&#25968;&#20844;&#24179;&#20998;&#31867;&#22522;&#20934;&#25509;&#36817;&#20844;&#24179;&#12290; &#20294;&#26159;&#65292;&#19968;&#23567;&#37096;&#20998;&#23454;&#20363;&#26174;&#31034;&#20986;&#26497;&#22823;&#30340;&#38543;&#24847;&#24615;&#27700;&#24179;&#65292;&#36825;&#34920;&#26126;&#24403;&#21069;&#30340;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#22788;&#29702;&#26576;&#20123;&#31867;&#22411;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variance in predictions across different trained models is a significant, under-explored source of error in fair classification. Empirically, the variance on some instances is so large that decisions can be effectively arbitrary. To study this problem, we perform a large-scale empirical study and make four overarching contributions: We 1) Define a metric called self-consistency, derived from variance, which we use as a proxy for measuring and reducing arbitrariness; 2) Develop an ensembling algorithm that abstains from classification when a prediction would be arbitrary; 3) Conduct the largest to-date empirical study of the role of variance (vis-a-vis self-consistency and arbitrariness) in fair classification; and, 4) Release a toolkit that makes the US Home Mortgage Disclosure Act (HMDA) datasets easily usable for future research. Altogether, our empirical results reveal shocking insights about reproducibility. Most fairness classification benchmarks are close-to-fair when taking into
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#12290;&#38024;&#23545;&#36825;&#19968;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#26696;&#8212;&#8212;KGEditor&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.10405</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Editing Language Model-based Knowledge Graph Embeddings. (arXiv:2301.10405v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#12290;&#38024;&#23545;&#36825;&#19968;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#26696;&#8212;&#8212;KGEditor&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20960;&#21313;&#24180;&#26469;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23884;&#20837;&#24050;&#32463;&#21462;&#24471;&#20102;&#23454;&#35777;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;KG&#23884;&#20837;&#36890;&#24120;&#20316;&#20026;&#38745;&#24577;&#24037;&#20214;&#37096;&#32626;&#65292;&#20462;&#25913;&#36215;&#26469;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;KG&#23884;&#20837;&#12290;&#35813;&#20219;&#21153;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#65292;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#22235;&#20010;&#26032;&#25968;&#25454;&#38598;&#65306;E-FB15k237&#12289;A-FB15k237&#12289;E-WN18RR &#21644; A-WN18RR&#65292;&#24182;&#35780;&#20272;&#20102;&#20960;&#31181;&#30693;&#35782;&#32534;&#36753;&#22522;&#32447;&#65292;&#35777;&#26126;&#20102;&#20043;&#21069;&#30340;&#27169;&#22411;&#22788;&#29702;&#35813;&#20219;&#21153;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#24378;&#22823;&#30340;&#22522;&#32447;&#8212;&#8212;KGEditor&#65292;&#23427;&#21033;&#29992;&#36229;&#32593;&#32476;&#30340;&#38468;&#21152;&#21442;&#25968;&#23618;&#26469;&#32534;&#36753;/&#28155;&#21152;&#20107;&#23454;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#26102;&#65292;KGEditor &#30340;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently decades have witnessed the empirical success of framing Knowledge Graph (KG) embeddings via language models. However, language model-based KG embeddings are usually deployed as static artifacts, which are challenging to modify without re-training after deployment. To address this issue, we propose a new task of editing language model-based KG embeddings in this paper. The proposed task aims to enable data-efficient and fast updates to KG embeddings without damaging the performance of the rest. We build four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and evaluate several knowledge editing baselines demonstrating the limited ability of previous models to handle the proposed challenging task. We further propose a simple yet strong baseline dubbed KGEditor, which utilizes additional parametric layers of the hyper network to edit/add facts. Comprehensive experimental results demonstrate that KGEditor can perform better when updating specific facts while not affec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#20851;&#20110;&#20351;&#29992;Transformer&#26550;&#26500;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;LaMDA&#26159;&#21542;&#20855;&#26377;&#24847;&#35782;&#30340;&#35828;&#27861;&#12290;&#20316;&#32773;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#19981;&#21487;&#33021;&#20855;&#26377;&#24847;&#35782;&#65292;&#32780;LaMDA&#27809;&#26377;&#27604;&#20854;&#20182;&#31867;&#20284;&#27169;&#22411;&#26356;&#20855;&#20808;&#36827;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.11483</link><description>&lt;p&gt;
Deanthropomorphising NLP&#65306;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#24847;&#35782;&#21040;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Deanthropomorphising NLP: Can a Language Model Be Conscious?. (arXiv:2211.11483v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20851;&#20110;&#20351;&#29992;Transformer&#26550;&#26500;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;LaMDA&#26159;&#21542;&#20855;&#26377;&#24847;&#35782;&#30340;&#35828;&#27861;&#12290;&#20316;&#32773;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#19981;&#21487;&#33021;&#20855;&#26377;&#24847;&#35782;&#65292;&#32780;LaMDA&#27809;&#26377;&#27604;&#20854;&#20182;&#31867;&#20284;&#27169;&#22411;&#26356;&#20855;&#20808;&#36827;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23545;&#26368;&#36817;&#26377;&#20851;&#20351;&#29992;Transformer&#27169;&#22411;&#26550;&#26500;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;LaMDA&#20855;&#26377;&#24847;&#35782;&#30340;&#35828;&#27861;&#36827;&#34892;&#35752;&#35770;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26679;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#21487;&#33021;&#20855;&#26377;&#24847;&#35782;&#65292;&#32780;LaMDA&#24182;&#27809;&#26377;&#27604;&#20854;&#20182;&#31867;&#20284;&#27169;&#22411;&#26356;&#20855;&#20808;&#36827;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#32508;&#21512;&#20449;&#24687;&#29702;&#35770;&#23545;Transformer&#26550;&#26500;&#36827;&#34892;&#20998;&#26512;&#26469;&#35777;&#26126;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#26377;&#24847;&#35782;&#30340;&#35828;&#27861;&#26159;NLP&#25253;&#36947;&#20013;&#20351;&#29992;&#25311;&#20154;&#21270;&#35821;&#35328;&#30340;&#26356;&#24191;&#27867;&#20542;&#21521;&#30340;&#19968;&#37096;&#20998;&#12290;&#26080;&#35770;&#36825;&#20123;&#35828;&#27861;&#30340;&#30495;&#23454;&#24615;&#22914;&#20309;&#65292;&#25105;&#20204;&#35748;&#20026;&#29616;&#22312;&#26159;&#35780;&#20272;&#35821;&#35328;&#24314;&#27169;&#36827;&#23637;&#24182;&#32771;&#34385;&#35813;&#20219;&#21153;&#30340;&#20262;&#29702;&#24433;&#21709;&#30340;&#36866;&#24403;&#26102;&#26426;&#12290;&#20026;&#20102;&#20351;&#26412;&#25991;&#26377;&#21161;&#20110;NLP&#31038;&#21306;&#20197;&#22806;&#30340;&#35835;&#32773;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20123;NLP&#22522;&#30784;&#30693;&#35782;&#30340;&#20171;&#32461;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work is intended as a voice in the discussion over the recent claims that LaMDA, a pretrained language model based on the Transformer model architecture, is sentient. This claim, if confirmed, would have serious ramifications in the Natural Language Processing (NLP) community due to wide-spread use of similar models. However, here we take the position that such a language model cannot be sentient, or conscious, and that LaMDA in particular exhibits no advances over other similar models that would qualify it. We justify this by analysing the Transformer architecture through Integrated Information Theory. We see the claims of consciousness as part of a wider tendency to use anthropomorphic language in NLP reporting. Regardless of the veracity of the claims, we consider this an opportune moment to take stock of progress in language modelling and consider the ethical implications of the task. In order to make this work helpful for readers outside the NLP community, we also present the
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#20869;&#22312;&#21160;&#26426;&#34913;&#37327;&#26631;&#20934;&#32780;&#19981;&#20381;&#36182;&#20110;&#22870;&#21169;&#26469;&#21019;&#24314;&#19968;&#20010;&#20855;&#26377;&#33258;&#25105;&#20445;&#25252;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#12290;</title><link>http://arxiv.org/abs/2211.10851</link><description>&lt;p&gt;
&#22870;&#21169;&#24182;&#38750;&#24517;&#35201;&#65306;&#22914;&#20309;&#20026;&#32456;&#36523;&#23398;&#20064;&#21019;&#24314;&#19968;&#20010;&#32452;&#21512;&#24615;&#33258;&#25105;&#20445;&#25252;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Reward is not Necessary: How to Create a Compositional Self-Preserving Agent for Life-Long Learning. (arXiv:2211.10851v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10851
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#20869;&#22312;&#21160;&#26426;&#34913;&#37327;&#26631;&#20934;&#32780;&#19981;&#20381;&#36182;&#20110;&#22870;&#21169;&#26469;&#21019;&#24314;&#19968;&#20010;&#20855;&#26377;&#33258;&#25105;&#20445;&#25252;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#35748;&#20026;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#36991;&#20813;&#24809;&#32602;&#26159;&#35299;&#37322;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#30340;&#26680;&#24515;&#12290;&#28982;&#32780;&#65292;&#22312;&#19968;&#29983;&#20013;&#65292;&#29983;&#29289;&#38656;&#35201;&#23398;&#20064;&#20851;&#20110;&#19990;&#30028;&#32467;&#26500;&#30340;&#35768;&#22810;&#19981;&#21516;&#26041;&#38754;&#65306;&#19990;&#30028;&#29366;&#24577;&#21644;&#29366;&#24577;&#36716;&#31227;&#21160;&#21147;&#23398;&#12290;&#38543;&#30528;&#26234;&#33021;&#20307;&#34701;&#20837;&#26032;&#30693;&#35782;&#65292;&#29366;&#24577;&#32452;&#21512;&#30340;&#25968;&#37327;&#20197;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#24182;&#19988;&#23545;&#20110;&#32473;&#23450;&#30340;&#29366;&#24577;&#32452;&#21512;&#65292;&#27809;&#26377;&#26126;&#26174;&#23450;&#20041;&#30340;&#39044;&#35774;&#22870;&#21169;&#25110;&#25104;&#26412;&#30340;&#21152;&#26435;&#32452;&#21512;&#65292;&#22240;&#20026;&#36825;&#26679;&#30340;&#21152;&#26435;&#38656;&#35201;&#22312;&#26234;&#33021;&#20307;&#22312;&#19990;&#30028;&#20013;&#30340;&#32463;&#39564;&#20043;&#21069;&#23545;&#22909;&#30340;&#21644;&#22351;&#30340;&#32452;&#21512;&#36827;&#34892;&#32534;&#30721;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24517;&#39035;&#22312;&#22823;&#29366;&#24577;&#31354;&#38388;&#20013;&#24320;&#21457;&#26356;&#33258;&#28982;&#30340;&#34892;&#20026;&#21644;&#21160;&#26426;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20165;&#20351;&#29992;&#20869;&#22312;&#21160;&#26426;&#34913;&#37327;&#26631;&#20934;&#65288;&#21363;&#36171;&#20104;&#33021;&#21147;&#65289;&#26159;&#21487;&#33021;&#30340;&#65292;&#35813;&#26631;&#20934;&#34913;&#37327;&#26234;&#33021;&#20307;&#22312;&#36716;&#31227;&#25805;&#20316;&#32773;&#19979;&#23454;&#29616;&#35768;&#22810;&#21487;&#33021;&#26410;&#26469;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24314;&#35758;&#23558;&#36171;&#20104;&#33021;&#21147;&#25193;&#23637;&#21040;&#20998;&#23618;&#29366;&#24577;&#31354;&#38388;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning views the maximization of rewards and avoidance of punishments as central to explaining goal-directed behavior. However, over a life, organisms will need to learn about many different aspects of the world's structure: the states of the world and state-vector transition dynamics. The number of combinations of states grows exponentially as an agent incorporates new knowledge, and there is no obvious weighted combination of pre-existing rewards or costs defined for a given combination of states, as such a weighting would need to encode information about good and bad combinations prior to an agent's experience in the world. Therefore, we must develop more naturalistic accounts of behavior and motivation in large state-spaces. We show that it is possible to use only the intrinsic motivation metric of empowerment, which measures the agent's capacity to realize many possible futures under a transition operator. We propose to scale empowerment to hierarchical state-space
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22240;&#26524;&#20851;&#31995;&#22312;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24517;&#35201;&#24615;&#21644;&#36866;&#29992;&#24615;&#65292;&#24378;&#35843;&#20102;&#38750;&#22240;&#26524;&#39044;&#27979;&#30340;&#31038;&#20250;&#24433;&#21709;&#21644;&#27861;&#24459;&#21453;&#27495;&#35270;&#36807;&#31243;&#20381;&#36182;&#20110;&#22240;&#26524;&#20027;&#24352;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#24212;&#29992;&#22240;&#26524;&#20851;&#31995;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2207.04053</link><description>&lt;p&gt;
&#35770;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#20013;&#22240;&#26524;&#20851;&#31995;&#30340;&#24517;&#35201;&#24615;&#21644;&#36866;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Need and Applicability of Causality for Fair Machine Learning. (arXiv:2207.04053v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22240;&#26524;&#20851;&#31995;&#22312;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24517;&#35201;&#24615;&#21644;&#36866;&#29992;&#24615;&#65292;&#24378;&#35843;&#20102;&#38750;&#22240;&#26524;&#39044;&#27979;&#30340;&#31038;&#20250;&#24433;&#21709;&#21644;&#27861;&#24459;&#21453;&#27495;&#35270;&#36807;&#31243;&#20381;&#36182;&#20110;&#22240;&#26524;&#20027;&#24352;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#24212;&#29992;&#22240;&#26524;&#20851;&#31995;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38500;&#20102;&#22312;&#27969;&#34892;&#30149;&#23398;&#12289;&#25919;&#27835;&#21644;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#24120;&#35265;&#24212;&#29992;&#26696;&#20363;&#22806;&#65292;&#20107;&#23454;&#35777;&#26126;&#22240;&#26524;&#20851;&#31995;&#22312;&#35780;&#20272;&#33258;&#21160;&#20915;&#31574;&#30340;&#20844;&#27491;&#24615;&#26041;&#38754;&#21313;&#20998;&#37325;&#35201;&#65292;&#26080;&#35770;&#26159;&#22312;&#27861;&#24459;&#19978;&#36824;&#26159;&#26085;&#24120;&#29983;&#27963;&#20013;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#20026;&#20309;&#22240;&#26524;&#20851;&#31995;&#23545;&#20844;&#24179;&#24615;&#35780;&#20272;&#23588;&#20026;&#37325;&#35201;&#30340;&#35770;&#28857;&#21644;&#31034;&#20363;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;&#38750;&#22240;&#26524;&#39044;&#27979;&#30340;&#31038;&#20250;&#24433;&#21709;&#20197;&#21450;&#20381;&#36182;&#22240;&#26524;&#20027;&#24352;&#30340;&#27861;&#24459;&#21453;&#27495;&#35270;&#36807;&#31243;&#12290;&#25105;&#20204;&#26368;&#21518;&#35752;&#35770;&#20102;&#24212;&#29992;&#22240;&#26524;&#20851;&#31995;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#65292;&#20197;&#21450;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Besides its common use cases in epidemiology, political, and social sciences, causality turns out to be crucial in evaluating the fairness of automated decisions, both in a legal and everyday sense. We provide arguments and examples, of why causality is particularly important for fairness evaluation. In particular, we point out the social impact of non-causal predictions and the legal anti-discrimination process that relies on causal claims. We conclude with a discussion about the challenges and limitations of applying causality in practical scenarios as well as possible solutions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;Gogioso&#21644;Pinzani&#22312;QPL 2021&#20013;&#25552;&#20986;&#30340;&#26463;&#29702;&#35770;&#27169;&#22411;&#65292;&#20026;&#35821;&#20041;&#27495;&#20041;&#30340;&#20004;&#20010;&#29305;&#24449;&#65288;&#19981;&#21516;&#21487;&#33021;&#35299;&#37322;&#30340;&#32852;&#21512;&#21487;&#20449;&#24230;&#21644;&#26576;&#20123;&#35789;&#22312;&#36807;&#31243;&#20013;&#25198;&#28436;&#26356;&#37325;&#35201;&#35282;&#33394;&#30340;&#22240;&#26524;&#32467;&#26500;&#65289;&#36827;&#34892;&#24314;&#27169;&#12290;&#36890;&#36807;&#23545;&#24515;&#29702;&#35821;&#35328;&#23398;&#25991;&#29486;&#20013;&#30340;&#27495;&#20041;&#30701;&#35821;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;&#30740;&#31350;&#20154;&#21592;&#23545;&#20154;&#31867;&#23545;&#20110;&#36825;&#20123;&#27495;&#20041;&#30340;&#21028;&#26029;&#36827;&#34892;&#20102;&#23454;&#35777;&#27979;&#37327;&#12290;</title><link>http://arxiv.org/abs/2206.06807</link><description>&lt;p&gt;
&#35821;&#20041;&#27495;&#20041;&#30340;&#22240;&#26524;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
The Causal Structure of Semantic Ambiguities. (arXiv:2206.06807v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;Gogioso&#21644;Pinzani&#22312;QPL 2021&#20013;&#25552;&#20986;&#30340;&#26463;&#29702;&#35770;&#27169;&#22411;&#65292;&#20026;&#35821;&#20041;&#27495;&#20041;&#30340;&#20004;&#20010;&#29305;&#24449;&#65288;&#19981;&#21516;&#21487;&#33021;&#35299;&#37322;&#30340;&#32852;&#21512;&#21487;&#20449;&#24230;&#21644;&#26576;&#20123;&#35789;&#22312;&#36807;&#31243;&#20013;&#25198;&#28436;&#26356;&#37325;&#35201;&#35282;&#33394;&#30340;&#22240;&#26524;&#32467;&#26500;&#65289;&#36827;&#34892;&#24314;&#27169;&#12290;&#36890;&#36807;&#23545;&#24515;&#29702;&#35821;&#35328;&#23398;&#25991;&#29486;&#20013;&#30340;&#27495;&#20041;&#30701;&#35821;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;&#30740;&#31350;&#20154;&#21592;&#23545;&#20154;&#31867;&#23545;&#20110;&#36825;&#20123;&#27495;&#20041;&#30340;&#21028;&#26029;&#36827;&#34892;&#20102;&#23454;&#35777;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27495;&#20041;&#26159;&#33258;&#28982;&#35821;&#35328;&#29616;&#35937;&#65292;&#22312;&#19981;&#21516;&#30340;&#35821;&#27861;&#12289;&#35821;&#20041;&#21644;&#35821;&#29992;&#23618;&#38754;&#19978;&#21457;&#29983;&#12290;&#23427;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65307;&#20363;&#22914;&#65292;&#22312;&#24515;&#29702;&#35821;&#35328;&#23398;&#39046;&#22495;&#65292;&#25105;&#20204;&#26377;&#22810;&#31181;&#31454;&#20105;&#24615;&#30340;&#30740;&#31350;&#20154;&#31867;&#28040;&#27495;&#36807;&#31243;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#30740;&#31350;&#26159;&#32463;&#39564;&#24615;&#30340;&#65292;&#22522;&#20110;&#30524;&#21160;&#36319;&#36394;&#31561;&#27979;&#37327;&#26041;&#27861;&#12290;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#20026;&#35821;&#20041;&#27495;&#20041;&#24418;&#24335;&#21270;&#36825;&#20123;&#36827;&#31243;&#65292;&#20854;&#20013;&#25105;&#20204;&#30830;&#23450;&#20102;&#20004;&#20010;&#29305;&#24449;&#65306;(1)&#19981;&#21516;&#21487;&#33021;&#35299;&#37322;&#20043;&#38388;&#30340;&#32852;&#21512;&#21487;&#20449;&#24230;&#65292;(2)&#26681;&#25454;&#26576;&#20123;&#35789;&#22312;&#36807;&#31243;&#20013;&#25198;&#28436;&#26356;&#37325;&#35201;&#35282;&#33394;&#30340;&#22240;&#26524;&#32467;&#26500;&#12290;Gogioso&#21644;Pinzani&#22312;QPL 2021&#20013;&#25552;&#20986;&#30340;&#26032;&#22411;&#26463;&#29702;&#35770;&#30830;&#23450;&#22240;&#26524;&#24615;&#27169;&#22411;&#24182;&#23545;&#36825;&#20123;&#29305;&#24449;&#36827;&#34892;&#25512;&#29702;&#25552;&#20379;&#20102;&#24037;&#20855;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#29702;&#35770;&#24212;&#29992;&#20110;&#20174;&#24515;&#29702;&#35821;&#35328;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#30340;&#27495;&#20041;&#30701;&#35821;&#25968;&#25454;&#38598;&#21644;&#25105;&#20204;&#20351;&#29992;Amazon&#30340;&#26426;&#26800;&#22303;&#32819;&#20854;&#24341;&#25806;&#25910;&#38598;&#30340;&#20154;&#31867;&#21487;&#20449;&#24230;&#21028;&#26029;&#20013;&#12290;&#25105;&#20204;&#27979;&#37327;&#20102;&#20854;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#12289;&#27495;&#20041;&#27700;&#24179;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ambiguity is a natural language phenomenon occurring at different levels of syntax, semantics, and pragmatics. It is widely studied; in Psycholinguistics, for instance, we have a variety of competing studies for the human disambiguation processes. These studies are empirical and based on eyetracking measurements. Here we take first steps towards formalizing these processes for semantic ambiguities where we identified the presence of two features: (1) joint plausibility degrees of different possible interpretations, (2) causal structures according to which certain words play a more substantial role in the processes. The novel sheaf-theoretic model of definite causality developed by Gogioso and Pinzani in QPL 2021 offers tools to model and reason about these features. We applied this theory to a dataset of ambiguous phrases extracted from Psycholinguistics literature and their human plausibility judgements collected by us using the Amazon Mechanical Turk engine. We measured the causal fr
&lt;/p&gt;</description></item></channel></rss>