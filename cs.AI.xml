<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#24247;&#31185;&#36842;&#20122;&#26694;&#26550;&#26159;&#19968;&#20010;&#25903;&#25345;&#21508;&#31181;&#27010;&#29575;&#29702;&#35770;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#38480;&#21046;&#30340;&#24182;&#34892;&#30340;&#31070;&#32463;&#31526;&#21495;&#32467;&#26500;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#38598;&#21512;&#27963;&#21160;&#26816;&#27979;&#12289;&#23454;&#20307;&#38142;&#25509;&#21644;&#25512;&#33616;&#20219;&#21153;&#65292;&#25552;&#39640;&#20102;&#26368;&#26032;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.00480</link><description>&lt;p&gt;
&#19982;&#24247;&#31185;&#36842;&#20122;&#24182;&#34892;&#30340;&#31070;&#32463;&#31526;&#21495;&#19968;&#20307;&#21270;
&lt;/p&gt;
&lt;p&gt;
Parallel Neurosymbolic Integration with Concordia. (arXiv:2306.00480v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00480
&lt;/p&gt;
&lt;p&gt;
&#24247;&#31185;&#36842;&#20122;&#26694;&#26550;&#26159;&#19968;&#20010;&#25903;&#25345;&#21508;&#31181;&#27010;&#29575;&#29702;&#35770;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#38480;&#21046;&#30340;&#24182;&#34892;&#30340;&#31070;&#32463;&#31526;&#21495;&#32467;&#26500;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#38598;&#21512;&#27963;&#21160;&#26816;&#27979;&#12289;&#23454;&#20307;&#38142;&#25509;&#21644;&#25512;&#33616;&#20219;&#21153;&#65292;&#25552;&#39640;&#20102;&#26368;&#26032;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24182;&#34892;&#30340;&#31070;&#32463;&#31526;&#21495;&#32467;&#26500;&#36890;&#36807;&#23558;&#36923;&#36753;&#29702;&#35770;&#30340;&#30693;&#35782;&#25552;&#21462;&#21040;&#28145;&#24230;&#27169;&#22411;&#20013;&#65292;&#22312;NLP&#20013;&#24471;&#21040;&#26377;&#25928;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25216;&#26415;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#21253;&#25324;&#25903;&#25345;&#21463;&#38480;&#24418;&#24335;&#30340;&#36923;&#36753;&#29702;&#35770;&#65292;&#24182;&#20381;&#36182;&#20110;&#36923;&#36753;&#21644;&#28145;&#24230;&#32593;&#32476;&#20043;&#38388;&#30340;&#29420;&#31435;&#24615;&#20551;&#35774;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#24247;&#31185;&#36842;&#20122;&#26694;&#26550;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#38480;&#21046;&#12290;&#24247;&#31185;&#36842;&#20122;&#26694;&#26550;&#26082;&#19981;&#20851;&#27880;&#28145;&#24230;&#32593;&#32476;&#65292;&#20063;&#19981;&#20851;&#27880;&#36923;&#36753;&#29702;&#35770;&#65292;&#25903;&#25345;&#21508;&#31181;&#27010;&#29575;&#29702;&#35770;&#12290;&#26694;&#26550;&#21487;&#20197;&#25903;&#25345;&#20004;&#20010;&#32452;&#20214;&#30340;&#30417;&#30563;&#35757;&#32451;&#21644;&#31070;&#32463;&#32452;&#20214;&#30340;&#26080;&#30417;&#30563;&#35757;&#32451;&#12290;&#24247;&#31185;&#36842;&#20122;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;NLP&#21644;&#25968;&#25454;&#20998;&#31867;&#20197;&#22806;&#30340;&#20219;&#21153;&#65292;&#25552;&#39640;&#20102;&#38598;&#21512;&#27963;&#21160;&#26816;&#27979;&#12289;&#23454;&#20307;&#38142;&#25509;&#21644;&#25512;&#33616;&#20219;&#21153;&#30340;&#26368;&#26032;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parallel neurosymbolic architectures have been applied effectively in NLP by distilling knowledge from a logic theory into a deep model.However, prior art faces several limitations including supporting restricted forms of logic theories and relying on the assumption of independence between the logic and the deep network. We present Concordia, a framework overcoming the limitations of prior art. Concordia is agnostic both to the deep network and the logic theory offering support for a wide range of probabilistic theories. Our framework can support supervised training of both components and unsupervised training of the neural component. Concordia has been successfully applied to tasks beyond NLP and data classification, improving the accuracy of state-of-the-art on collective activity detection, entity linking and recommendation tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23581;&#35797;&#23454;&#29616;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36816;&#29992;&#21487;&#36870;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#24494;&#35843;&#65292;&#24182;&#21457;&#29616;&#22312;&#21021;&#22987;&#21270;&#24494;&#35843;&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2306.00477</link><description>&lt;p&gt;
&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#20855;&#26377;&#21487;&#36870;&#24615;&#65306;&#20174;&#21442;&#25968;&#21040;&#20869;&#23384;&#39640;&#25928;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning. (arXiv:2306.00477v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23581;&#35797;&#23454;&#29616;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36816;&#29992;&#21487;&#36870;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#24494;&#35843;&#65292;&#24182;&#21457;&#29616;&#22312;&#21021;&#22987;&#21270;&#24494;&#35843;&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#38750;&#24120;&#25104;&#21151;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#35757;&#32451;&#23569;&#37327;&#21442;&#25968;&#32780;&#19981;&#20250;&#38477;&#20302;&#24615;&#33021;&#65292;&#24182;&#38543;&#30528;PLM&#36234;&#26469;&#36234;&#22823;&#32780;&#25104;&#20026;&#20107;&#23454;&#19978;&#30340;&#23398;&#20064;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PEFT&#26041;&#27861;&#19981;&#20855;&#22791;&#20869;&#23384;&#25928;&#29575;&#65292;&#22240;&#20026;&#23427;&#20204;&#20173;&#38656;&#35201;&#23384;&#20648;&#22823;&#37096;&#20998;&#20013;&#38388;&#28608;&#27963;&#20540;&#20197;&#20415;&#35745;&#31639;&#26799;&#24230;&#65292;&#31867;&#20284;&#20110;&#24494;&#35843;&#12290;&#19968;&#20010;&#20943;&#23569;&#28608;&#27963;&#20869;&#23384;&#30340;&#26377;&#25928;&#26041;&#27861;&#26159;&#24212;&#29992;&#21487;&#36870;&#27169;&#22411;&#65292;&#36825;&#26679;&#20013;&#38388;&#28608;&#27963;&#20540;&#23601;&#26080;&#38656;&#32531;&#23384;&#65292;&#21487;&#20197;&#37325;&#26032;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#23558;PLM&#20462;&#25913;&#20026;&#23427;&#30340;&#21487;&#36870;&#21464;&#20307;&#24182;&#36827;&#34892;PEFT&#24182;&#19981;&#26159;&#19968;&#20214;&#23481;&#26131;&#30340;&#20107;&#65292;&#22240;&#20026;&#21487;&#36870;&#27169;&#22411;&#20855;&#26377;&#19982;&#24403;&#21069;&#21457;&#24067;&#30340;PLM&#19981;&#21516;&#30340;&#20307;&#31995;&#32467;&#26500;&#12290;&#26412;&#25991;&#39318;&#20808;&#35843;&#26597;&#29616;&#26377;PEFT&#26041;&#27861;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#35748;&#35782;&#21040;&#22312;&#21021;&#22987;&#21270;PEFT&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs) has emerged as a highly successful approach, with training only a small number of parameters without sacrificing performance and becoming the de-facto learning paradigm with the increasing size of PLMs. However, existing PEFT methods are not memory-efficient, because they still require caching most of the intermediate activations for the gradient calculation, akin to fine-tuning. One effective way to reduce the activation memory is to apply a reversible model, so the intermediate activations are not necessary to be cached and can be recomputed. Nevertheless, modifying a PLM to its reversible variant with PEFT is not straightforward, since the reversible model has a distinct architecture from the currently released PLMs. In this paper, we first investigate what is a key factor for the success of existing PEFT methods, and realize that it's essential to preserve the PLM's starting point when initializing a PEFT 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#20420;&#35821;&#25991;&#26412;&#30340;&#22823;&#25968;&#25454;&#26041;&#27861;&#65292;&#26088;&#22312;&#26816;&#27979;&#25991;&#26412;&#20013;&#30340;&#35773;&#21050;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#32452;&#21487;&#29992;&#20110;&#23624;&#25240;&#21644;&#25991;&#26412;&#21512;&#25104;&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#32593;&#32476;&#26381;&#21153;&#20013;&#23454;&#29616;&#12290;&#23545;&#36825;&#20123;&#31639;&#27861;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27979;&#35797;&#21644;&#27604;&#36739;&#65292;&#21516;&#26102;&#29992;&#20110;&#20272;&#35745;&#20420;&#35821;&#19981;&#21516;&#35789;&#24615;&#30340;&#24418;&#24577;&#21464;&#24322;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.00445</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#22823;&#25968;&#25454;&#30340;&#20420;&#35821;&#35773;&#21050;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A big data approach towards sarcasm detection in Russian. (arXiv:2306.00445v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#20420;&#35821;&#25991;&#26412;&#30340;&#22823;&#25968;&#25454;&#26041;&#27861;&#65292;&#26088;&#22312;&#26816;&#27979;&#25991;&#26412;&#20013;&#30340;&#35773;&#21050;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#32452;&#21487;&#29992;&#20110;&#23624;&#25240;&#21644;&#25991;&#26412;&#21512;&#25104;&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#32593;&#32476;&#26381;&#21153;&#20013;&#23454;&#29616;&#12290;&#23545;&#36825;&#20123;&#31639;&#27861;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27979;&#35797;&#21644;&#27604;&#36739;&#65292;&#21516;&#26102;&#29992;&#20110;&#20272;&#35745;&#20420;&#35821;&#19981;&#21516;&#35789;&#24615;&#30340;&#24418;&#24577;&#21464;&#24322;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#32452;&#38024;&#23545;&#20420;&#32599;&#26031;&#35821;&#35328;&#23624;&#25240;&#21644;&#33258;&#21160;&#25991;&#26412;&#21512;&#25104;&#30340;&#30830;&#23450;&#24615;&#31639;&#27861;&#65292;&#24182;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#32593;&#32476;&#26381;&#21153;www.passare.ru&#20013;&#23454;&#29616;&#12290;&#35813;&#26381;&#21153;&#25552;&#20379;&#20102;&#21333;&#35789;&#23624;&#25240;&#12289;&#35789;&#21305;&#37197;&#21644;&#29983;&#25104;&#35821;&#27861;&#27491;&#30830;&#30340;&#20420;&#35821;&#25991;&#26412;&#30340;&#21151;&#33021;&#12290;&#25152;&#36873;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/passare-ru/PassareFunctions/&#20013;&#33719;&#24471;&#12290;&#38024;&#23545;OpenCorpora&#27880;&#37322;&#35821;&#26009;&#24211;&#23545;&#23624;&#25240;&#21151;&#33021;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#19982;&#20854;&#20182;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#29992;&#20110;&#20272;&#35745;&#20420;&#35821;&#19981;&#21516;&#35789;&#24615;&#30340;&#24418;&#24577;&#21464;&#24322;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a set of deterministic algorithms for Russian inflection and automated text synthesis. These algorithms are implemented in a publicly available web-service www.passare.ru. This service provides functions for inflection of single words, word matching and synthesis of grammatically correct Russian text. Selected code and datasets are available at https://github.com/passare-ru/PassareFunctions/ Performance of the inflectional functions has been tested against the annotated corpus of Russian language OpenCorpora, compared with that of other solutions, and used for estimating the morphological variability and complexity of different parts of speech in Russian.
&lt;/p&gt;</description></item><item><title>&#36830;&#32493;&#23398;&#20064;&#20013;&#23384;&#22312;&#19968;&#31181;&#29305;&#27530;&#24418;&#24335;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#8212;&#8212;&#36234;&#30028;&#36951;&#24536;&#65292;&#24403;&#32473;&#23450;&#31867;&#21035;&#24341;&#20837;&#31867;&#20869;&#20998;&#24067;&#36716;&#31227;&#26102;&#65292;&#23427;&#20250;&#26174;&#30528;&#21066;&#24369;&#35813;&#31867;&#21035;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.00427</link><description>&lt;p&gt;
&#38024;&#23545;&#31867;&#20869;&#20998;&#24067;&#36716;&#31227;&#30340;&#36807;&#24230;&#36951;&#24536;&#65306;&#36830;&#32493;&#23398;&#20064;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution forgetting: vulnerability of continual learning to intra-class distribution shift. (arXiv:2306.00427v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00427
&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#20013;&#23384;&#22312;&#19968;&#31181;&#29305;&#27530;&#24418;&#24335;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#8212;&#8212;&#36234;&#30028;&#36951;&#24536;&#65292;&#24403;&#32473;&#23450;&#31867;&#21035;&#24341;&#20837;&#31867;&#20869;&#20998;&#24067;&#36716;&#31227;&#26102;&#65292;&#23427;&#20250;&#26174;&#30528;&#21066;&#24369;&#35813;&#31867;&#21035;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#26159;&#35753;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#24320;&#25918;&#29615;&#22659;&#20013;&#24037;&#20316;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#22312;&#32852;&#21512;&#23398;&#20064;&#20013;&#65292;&#20154;&#20204;&#24050;&#32463;&#30693;&#36947;&#30001;&#24847;&#22270;&#25915;&#20987;&#25110;&#29615;&#22659;&#25200;&#21160;&#24341;&#36215;&#30340;&#36234;&#30028;&#38382;&#39064;&#20005;&#37325;&#24433;&#21709;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#36830;&#32493;&#23398;&#20064;&#35774;&#32622;&#20013;&#30001;&#36234;&#30028;&#38382;&#39064;&#24341;&#36215;&#30340;&#19968;&#31181;&#29305;&#27530;&#24418;&#24335;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#36234;&#30028;&#36951;&#24536;&#65288;OODF&#65289;&#12290;&#22312;&#36830;&#32493;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#38024;&#23545;&#32473;&#23450;&#31867;&#21035;&#65292;&#24341;&#20837;&#31867;&#20869;&#20998;&#24067;&#36716;&#31227;&#26174;&#30528;&#21066;&#24369;&#20102;&#21518;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#35813;&#31867;&#21035;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36825;&#31181;&#29616;&#35937;&#23545;&#20110;&#36830;&#32493;&#23398;&#20064;&#32780;&#35328;&#26159;&#29305;&#27530;&#30340;&#65292;&#22240;&#20026;&#21516;&#26679;&#32423;&#21035;&#30340;&#20998;&#24067;&#36716;&#31227;&#21482;&#26377;&#24494;&#19981;&#36275;&#36947;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) is an important technique to allow artificial neural networks to work in open environments. CL enables a system to learn new tasks without severe interference to its performance on old tasks, i.e., overcome the problems of catastrophic forgetting. In joint learning, it is well known that the out-of-distribution (OOD) problem caused by intentional attacks or environmental perturbations will severely impair the ability of networks to generalize. In this work, we reported a special form of catastrophic forgetting raised by the OOD problem in continual learning settings, and we named it out-of-distribution forgetting (OODF). In continual image classification tasks, we found that for a given category, introducing an intra-class distribution shift significantly impaired the recognition accuracy of CL methods for that category during subsequent learning. Interestingly, this phenomenon is special for CL as the same level of distribution shift had only negligible effects
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;AIGC&#30340;&#27010;&#24565;&#12289;&#20998;&#31867;&#20197;&#21450;&#22522;&#30784;&#25216;&#26415;&#65292;&#35814;&#32454;&#38416;&#36848;&#20102;&#22312;AIGC&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#25361;&#25112;&#65292;&#25552;&#20379;&#20102;&#38544;&#31169;&#35745;&#31639;&#21644;&#21306;&#22359;&#38142;&#31561;&#25216;&#26415;&#30456;&#24212;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#30830;&#23450;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.00419</link><description>&lt;p&gt;
AIGC&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#21450;&#20854;&#35299;&#20915;&#26041;&#26696;&#65306;&#25506;&#32034;&#38544;&#31169;&#35745;&#31639;&#12289;&#21306;&#22359;&#38142;&#31561;&#25216;&#26415;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Challenges and Remedies to Privacy and Security in AIGC: Exploring the Potential of Privacy Computing, Blockchain, and Beyond. (arXiv:2306.00419v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;AIGC&#30340;&#27010;&#24565;&#12289;&#20998;&#31867;&#20197;&#21450;&#22522;&#30784;&#25216;&#26415;&#65292;&#35814;&#32454;&#38416;&#36848;&#20102;&#22312;AIGC&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#25361;&#25112;&#65292;&#25552;&#20379;&#20102;&#38544;&#31169;&#35745;&#31639;&#21644;&#21306;&#22359;&#38142;&#31561;&#25216;&#26415;&#30456;&#24212;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#30830;&#23450;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#26159;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#30340;&#26368;&#26032;&#25104;&#26524;&#20043;&#19968;&#12290;&#30456;&#20851;&#24212;&#29992;&#29983;&#25104;&#30340;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#38899;&#39057;&#31561;&#20869;&#23481;&#24341;&#21457;&#20102;&#28608;&#28872;&#30340;&#35752;&#35770;&#12290;&#21508;&#31181;&#34893;&#29983;&#30340;AIGC&#24212;&#29992;&#20063;&#36880;&#28176;&#36827;&#20837;&#21508;&#34892;&#21508;&#19994;&#65292;&#32473;&#20154;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#24102;&#26469;&#20102;&#38590;&#20197;&#24819;&#35937;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29983;&#25104;&#24615;&#24037;&#20855;&#30340;&#24555;&#36895;&#21457;&#23637;&#20063;&#24341;&#21457;&#20102;&#20851;&#20110;AIGC&#38544;&#31169;&#12289;&#23433;&#20840;&#29978;&#33267;&#29256;&#26435;&#38382;&#39064;&#30340;&#25285;&#24551;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#21306;&#22359;&#38142;&#21644;&#38544;&#31169;&#35745;&#31639;&#31561;&#20808;&#36827;&#25216;&#26415;&#21487;&#20197;&#19982;AIGC&#24037;&#20855;&#30456;&#32467;&#21512;&#65292;&#20294;&#23578;&#26410;&#26377;&#30740;&#31350;&#23545;&#20854;&#30456;&#20851;&#24615;&#21644;&#21069;&#26223;&#36827;&#34892;&#31995;&#32479;&#21644;&#35814;&#32454;&#30340;&#35843;&#26597;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#36890;&#36807;&#20805;&#20998;&#25506;&#32034;&#19978;&#36848;&#25216;&#26415;&#65292;&#30740;&#31350;&#22914;&#20309;&#21033;&#29992;&#23427;&#20204;&#26469;&#20445;&#25252;AIGC&#20013;&#25968;&#25454;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#12290;&#26412;&#25991;&#39318;&#20808;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;AIGC&#30340;&#27010;&#24565;&#12289;&#20998;&#31867;&#21644;&#22522;&#30784;&#25216;&#26415;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35814;&#32454;&#38416;&#36848;&#20102;AIGC&#20013;&#30340;&#28508;&#22312;&#38544;&#31169;&#21644;&#23433;&#20840;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#30456;&#24212;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#29305;&#21035;&#35752;&#35770;&#20102;&#38544;&#31169;&#35745;&#31639;&#21644;&#21306;&#22359;&#38142;&#22312;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#21644;&#23616;&#38480;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26412;&#39046;&#22495;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence Generated Content (AIGC) is one of the latest achievements in AI development. The content generated by related applications, such as text, images and audio, has sparked a heated discussion. Various derived AIGC applications are also gradually entering all walks of life, bringing unimaginable impact to people's daily lives. However, the rapid development of such generative tools has also raised concerns about privacy and security issues, and even copyright issues in AIGC. We note that advanced technologies such as blockchain and privacy computing can be combined with AIGC tools, but no work has yet been done to investigate their relevance and prospect in a systematic and detailed way. Therefore it is necessary to investigate how they can be used to protect the privacy and security of data in AIGC by fully exploring the aforementioned technologies. In this paper, we first systematically review the concept, classification and underlying technologies of AIGC. Then, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25511;&#21046;&#26631;&#35760;&#32423;&#30340;&#29983;&#25104;&#12289;&#25552;&#39640;&#21407;&#22987;&#23398;&#20064;&#21644;&#20943;&#23569;&#38169;&#35823;&#65292;&#20854;&#20013;&#21253;&#25324;&#33945;&#29305;&#21345;&#27931;dropout&#12289;&#36793;&#32536;&#38750;&#20284;&#28982;&#23398;&#20064;&#21644;&#26368;&#23567;&#21270;&#29109;&#12290;&#22312;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#24773;&#24863;&#22235;&#20803;&#32452;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.00418</link><description>&lt;p&gt;
&#8220;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#38750;&#20284;&#28982;&#23398;&#20064;&#25552;&#39640;&#29983;&#25104;&#24335;&#24773;&#24863;&#22235;&#20803;&#32452;&#39044;&#27979;&#8221;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Unlikelihood Learning Improves Generative Aspect Sentiment Quad Prediction. (arXiv:2306.00418v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25511;&#21046;&#26631;&#35760;&#32423;&#30340;&#29983;&#25104;&#12289;&#25552;&#39640;&#21407;&#22987;&#23398;&#20064;&#21644;&#20943;&#23569;&#38169;&#35823;&#65292;&#20854;&#20013;&#21253;&#25324;&#33945;&#29305;&#21345;&#27931;dropout&#12289;&#36793;&#32536;&#38750;&#20284;&#28982;&#23398;&#20064;&#21644;&#26368;&#23567;&#21270;&#29109;&#12290;&#22312;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#24773;&#24863;&#22235;&#20803;&#32452;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#24191;&#27867;&#20851;&#27880;&#20102;&#24773;&#24863;&#22235;&#20803;&#32452;&#39044;&#27979;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#20986;&#22235;&#20803;&#32452;&#65292;&#23558;&#21407;&#22987;&#21477;&#23376;&#36716;&#21270;&#20026;&#27169;&#26495;&#21270;&#30340;&#30446;&#26631;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#21482;&#20851;&#27880;&#29983;&#25104;&#20160;&#20040;&#65292;&#32780;&#24573;&#30053;&#20102;&#19981;&#38656;&#35201;&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#35748;&#20026;&#32771;&#34385;&#36127;&#26679;&#26412;&#20063;&#20250;&#24102;&#26469;&#28508;&#22312;&#30340;&#22909;&#22788;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#26495;&#26080;&#20851;&#30340;&#26041;&#27861;&#26469;&#25511;&#21046;&#26631;&#35760;&#32423;&#30340;&#29983;&#25104;&#65292;&#21516;&#26102;&#25552;&#39640;&#21407;&#22987;&#23398;&#20064;&#21644;&#20943;&#23569;&#38169;&#35823;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33945;&#29305;&#21345;&#27931;dropout&#26469;&#29702;&#35299;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#32622;&#19981;&#30830;&#23450;&#24615;&#65292;&#33719;&#21462;&#22122;&#22768;&#21644;&#38169;&#35823;&#20449;&#24687;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#36793;&#32536;&#38750;&#20284;&#28982;&#23398;&#20064;&#26469;&#25233;&#21046;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#38169;&#35823;&#26631;&#35760;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26368;&#23567;&#21270;&#29109;&#26469;&#24179;&#34913;&#36793;&#32536;&#38750;&#20284;&#28982;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;&#22312;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25552;&#39640;&#24773;&#24863;&#22235;&#20803;&#32452;&#39044;&#27979;&#30340;&#24615;&#33021;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, aspect sentiment quad prediction has received widespread attention in the field of aspect-based sentiment analysis. Existing studies extract quadruplets via pre-trained generative language models to paraphrase the original sentence into a templated target sequence. However, previous works only focus on what to generate but ignore what not to generate. We argue that considering the negative samples also leads to potential benefits. In this work, we propose a template-agnostic method to control the token-level generation, which boosts original learning and reduces mistakes simultaneously. Specifically, we introduce Monte Carlo dropout to understand the built-in uncertainty of pre-trained language models, acquiring the noises and errors. We further propose marginalized unlikelihood learning to suppress the uncertainty-aware mistake tokens. Finally, we introduce minimization entropy to balance the effects of marginalized unlikelihood learning. Extensive experiments on four public
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21487;&#25511;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;COMODO&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#22238;&#24402;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;A-MDM&#65289;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#12289;&#38271;&#26102;&#38388;&#20869;&#30340;&#36816;&#21160;&#24207;&#21015;&#65292;&#20197;&#23454;&#29616;&#22312;&#21709;&#24212;&#20110;&#26102;&#21464;&#25511;&#21046;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23454;&#26102;&#36816;&#21160;&#21512;&#25104;&#12290;</title><link>http://arxiv.org/abs/2306.00416</link><description>&lt;p&gt;
&#21487;&#25511;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Controllable Motion Diffusion Model. (arXiv:2306.00416v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00416
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21487;&#25511;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;COMODO&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#22238;&#24402;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;A-MDM&#65289;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#12289;&#38271;&#26102;&#38388;&#20869;&#30340;&#36816;&#21160;&#24207;&#21015;&#65292;&#20197;&#23454;&#29616;&#22312;&#21709;&#24212;&#20110;&#26102;&#21464;&#25511;&#21046;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23454;&#26102;&#36816;&#21160;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#21160;&#30011;&#20013;&#65292;&#20026;&#34394;&#25311;&#35282;&#33394;&#29983;&#25104;&#36924;&#30495;&#19988;&#21487;&#25511;&#30340;&#36816;&#21160;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20174;&#22270;&#20687;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#25104;&#21151;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#23637;&#31034;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#22823;&#22810;&#38480;&#20110;&#31163;&#32447;&#24212;&#29992;&#65292;&#30446;&#26631;&#26159;&#29983;&#25104;&#21516;&#26102;&#29983;&#25104;&#25152;&#26377;&#27493;&#39588;&#30340;&#24207;&#21015;&#32423;&#29983;&#25104;&#12290;&#20026;&#20102;&#33021;&#22815;&#22312;&#21709;&#24212;&#20110;&#26102;&#21464;&#25511;&#21046;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#23454;&#26102;&#36816;&#21160;&#21512;&#25104;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#25511;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;COMODO&#65289;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20197;&#33258;&#22238;&#24402;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;A-MDM&#65289;&#20026;&#22522;&#30784;&#65292;&#36880;&#27493;&#29983;&#25104;&#36816;&#21160;&#24207;&#21015;&#12290;&#36890;&#36807;&#31616;&#21333;&#22320;&#20351;&#29992;&#26631;&#20934;DDPM&#31639;&#27861;&#32780;&#26080;&#38656;&#20219;&#20309;&#39069;&#22806;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#20135;&#29983;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#36816;&#21160;&#25511;&#21046;&#19979;&#38271;&#26102;&#38388;&#20869;&#30340;&#39640;&#20445;&#30495;&#24230;&#36816;&#21160;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating realistic and controllable motions for virtual characters is a challenging task in computer animation, and its implications extend to games, simulations, and virtual reality. Recent studies have drawn inspiration from the success of diffusion models in image generation, demonstrating the potential for addressing this task. However, the majority of these studies have been limited to offline applications that target at sequence-level generation that generates all steps simultaneously. To enable real-time motion synthesis with diffusion models in response to time-varying control signals, we propose the framework of the Controllable Motion Diffusion Model (COMODO). Our framework begins with an auto-regressive motion diffusion model (A-MDM), which generates motion sequences step by step. In this way, simply using the standard DDPM algorithm without any additional complexity, our framework is able to generate high-fidelity motion sequences over extended periods with different type
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#23545;&#29616;&#26377;&#30340;&#20844;&#27491;&#24863;&#30693;&#25512;&#33616;&#31995;&#32479;&#26041;&#27861;&#21644;&#23454;&#36341;&#36827;&#34892;&#20102;&#24635;&#32467;&#20998;&#26512;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#30456;&#20851;&#30340;&#27010;&#24565;&#23450;&#20041;&#12289;&#20998;&#31867;&#12289;&#26041;&#27861;&#21644;&#38656;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.00403</link><description>&lt;p&gt;
&#20844;&#27491;&#24863;&#30693;&#25512;&#33616;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Fairness-aware Recommender Systems. (arXiv:2306.00403v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#23545;&#29616;&#26377;&#30340;&#20844;&#27491;&#24863;&#30693;&#25512;&#33616;&#31995;&#32479;&#26041;&#27861;&#21644;&#23454;&#36341;&#36827;&#34892;&#20102;&#24635;&#32467;&#20998;&#26512;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#30456;&#20851;&#30340;&#27010;&#24565;&#23450;&#20041;&#12289;&#20998;&#31867;&#12289;&#26041;&#27861;&#21644;&#38656;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#20449;&#24687;&#36807;&#28388;&#26381;&#21153;&#65292;&#25512;&#33616;&#31995;&#32479;&#36890;&#36807;&#25552;&#20379;&#20010;&#24615;&#21270;&#24314;&#35758;&#21644;&#24110;&#21161;&#20154;&#20204;&#20570;&#20986;&#20915;&#31574;&#26497;&#22823;&#22320;&#20016;&#23500;&#20102;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#65292;&#20351;&#23427;&#20204;&#22312;&#20449;&#24687;&#26102;&#20195;&#23545;&#20154;&#31867;&#31038;&#20250;&#33267;&#20851;&#37325;&#35201;&#21644;&#19981;&#21487;&#25110;&#32570;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#20154;&#20204;&#23545;&#23427;&#20204;&#30340;&#20381;&#36182;&#31243;&#24230;&#22686;&#21152;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#30001;&#20110;&#20854;&#19981;&#20844;&#24179;&#24615;&#65288;&#20363;&#22914;&#24037;&#20316;&#25512;&#33616;&#20013;&#30340;&#24615;&#21035;&#27495;&#35270;&#65289;&#65292;&#25512;&#33616;&#31995;&#32479;&#23545;&#31038;&#20250;&#21644;&#20010;&#20154;&#21487;&#33021;&#25317;&#26377;&#26080;&#24847;&#35782;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#24320;&#21457;&#21487;&#20449;&#36182;&#30340;&#26381;&#21153;&#65292;&#35774;&#35745;&#20844;&#27491;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#20197;&#32531;&#35299;&#36825;&#20123;&#20559;&#35265;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#32508;&#36848;&#27010;&#36848;&#20102;&#29616;&#26377;&#30340;&#20844;&#27491;&#24615;&#25512;&#33616;&#31995;&#32479;&#26041;&#27861;&#21644;&#23454;&#36341;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19981;&#21516;&#25512;&#33616;&#22330;&#26223;&#19979;&#30340;&#20844;&#27491;&#24615;&#27010;&#24565;&#65292;&#20840;&#38754;&#20998;&#31867;&#24403;&#21069;&#30340;&#36827;&#23637;&#24182;&#20171;&#32461;&#20102;&#20419;&#36827;&#25512;&#33616;&#31995;&#32479;&#19981;&#21516;&#38454;&#27573;&#30340;&#20844;&#27491;&#24615;&#30340;&#20856;&#22411;&#26041;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#22312;&#20171;&#32461;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#20123;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
As information filtering services, recommender systems have extremely enriched our daily life by providing personalized suggestions and facilitating people in decision-making, which makes them vital and indispensable to human society in the information era. However, as people become more dependent on them, recent studies show that recommender systems potentially own unintentional impacts on society and individuals because of their unfairness (e.g., gender discrimination in job recommendations). To develop trustworthy services, it is crucial to devise fairness-aware recommender systems that can mitigate these bias issues. In this survey, we summarise existing methodologies and practices of fairness in recommender systems. Firstly, we present concepts of fairness in different recommendation scenarios, comprehensively categorize current advances, and introduce typical methods to promote fairness in different stages of recommender systems. Next, after introducing datasets and evaluation me
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25945;&#24072;&#20195;&#29702;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#20808;&#21069;&#23398;&#20064;&#30340;&#30693;&#35782;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#20351;&#23398;&#29983;&#32593;&#32476;&#21487;&#20197;&#20174;&#26679;&#26412;&#30340;&#22810;&#31181;&#35266;&#28857;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#22312;&#26631;&#20934;&#22522;&#20934;&#19978;&#20248;&#20110;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.00393</link><description>&lt;p&gt;
Teacher Agent&#65306;&#19968;&#31181;&#22522;&#20110;&#37325;&#22797;&#35757;&#32451;&#30340;&#35270;&#39057;&#22686;&#37327;&#23398;&#20064;&#38750;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Teacher Agent: A Non-Knowledge Distillation Method for Rehearsal-based Video Incremental Learning. (arXiv:2306.00393v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00393
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25945;&#24072;&#20195;&#29702;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#20808;&#21069;&#23398;&#20064;&#30340;&#30693;&#35782;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#20351;&#23398;&#29983;&#32593;&#32476;&#21487;&#20197;&#20174;&#26679;&#26412;&#30340;&#22810;&#31181;&#35266;&#28857;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#22312;&#26631;&#20934;&#22522;&#20934;&#19978;&#20248;&#20110;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;&#35270;&#39057;&#30340;&#31038;&#20132;&#23186;&#20307;&#30340;&#26222;&#21450;&#65292;&#19981;&#26029;&#26377;&#26032;&#30340;&#35270;&#39057;&#31867;&#21035;&#34987;&#29983;&#25104;&#65292;&#36843;&#20999;&#38656;&#35201;&#31283;&#20581;&#30340;&#22686;&#37327;&#23398;&#20064;&#25216;&#26415;&#26469;&#29702;&#35299;&#36825;&#20123;&#35270;&#39057;&#12290;&#20854;&#20013;&#26368;&#22823;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#22312;&#23398;&#20064;&#26032;&#31867;&#21035;&#30340;&#21516;&#26102;&#65292;&#32593;&#32476;&#24448;&#24448;&#20250;&#24536;&#35760;&#20808;&#21069;&#23398;&#20064;&#36807;&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#30693;&#35782;&#33976;&#39311;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#25216;&#26415;&#65292;&#23427;&#36890;&#36807;&#23558;&#19981;&#21516;&#31867;&#21035;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#30340;&#37325;&#35201;&#20449;&#24687;&#20256;&#36755;&#21040;&#23398;&#29983;&#27169;&#22411;&#20013;&#26469;&#22686;&#24378;&#20854;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#26368;&#22909;&#26377;&#19968;&#20010;&#24378;&#22823;&#30340;&#25945;&#24072;&#27169;&#22411;&#26469;&#25351;&#23548;&#23398;&#29983;&#12290;&#28982;&#32780;&#65292;&#32593;&#32476;&#26412;&#36523;&#30340;&#26377;&#38480;&#34920;&#29616;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#21457;&#29983;&#21487;&#33021;&#23548;&#33268;&#25945;&#24072;&#32593;&#32476;&#23545;&#26576;&#20123;&#35760;&#24518;&#26679;&#26412;&#20570;&#20986;&#19981;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23398;&#29983;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25945;&#24072;&#20195;&#29702;&#65292;&#33021;&#22815;&#20174;&#20808;&#21069;&#23398;&#20064;&#30340;&#30693;&#35782;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#20351;&#23398;&#29983;&#32593;&#32476;&#21487;&#20197;&#20174;&#26679;&#26412;&#30340;&#22810;&#31181;&#35266;&#28857;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35270;&#39057;&#20998;&#31867;&#20219;&#21153;&#30340;&#26631;&#20934;&#22522;&#20934;&#19978;&#20248;&#20110;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rise in popularity of video-based social media, new categories of videos are constantly being generated, creating an urgent need for robust incremental learning techniques for video understanding. One of the biggest challenges in this task is catastrophic forgetting, where the network tends to forget previously learned data while learning new categories. To overcome this issue, knowledge distillation is a widely used technique for rehearsal-based video incremental learning that involves transferring important information on similarities among different categories to enhance the student model. Therefore, it is preferable to have a strong teacher model to guide the students. However, the limited performance of the network itself and the occurrence of catastrophic forgetting can result in the teacher network making inaccurate predictions for some memory exemplars, ultimately limiting the student network's performance. Based on these observations, we propose a teacher agent capabl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#20351;&#29992;AI&#31995;&#32479;&#36827;&#34892;&#20915;&#31574;&#26102;&#30340;&#21487;&#20449;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#25324;&#34920;&#36798;&#12289;&#30495;&#23454;&#21644;&#22522;&#26412;&#27700;&#24179;&#30340;&#19981;&#21516;&#20449;&#20219;&#32423;&#21035;&#30340;TAI&#20998;&#31867;&#31995;&#32479;&#25110;&#26694;&#26550;&#65292;&#20351;&#29992;&#21313;&#20010;&#32500;&#24230;&#26469;&#34913;&#37327;&#20449;&#20219;&#65292;&#24182;&#25552;&#20379;&#20102;&#29616;&#26377;TAI&#30740;&#31350;&#30340;&#35843;&#26597;&#21644;&#20803;&#20998;&#26512;&#65292;&#36824;&#30830;&#23450;&#20102;TAI&#30740;&#31350;&#30340;&#26410;&#26469;&#26041;&#21521;&#21644;&#28508;&#22312;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.00380</link><description>&lt;p&gt;
&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#30340;&#35843;&#26597;&#12289;&#20998;&#31867;&#21450;&#26410;&#26469;&#26041;&#21521;&#65306;&#20803;&#20915;&#31574;&#30340;&#25112;&#30053;&#20915;&#31574;&#20803;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
The Survey, Taxonomy, and Future Directions of Trustworthy AI: A Meta Decision of Strategic Decisions. (arXiv:2306.00380v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#20351;&#29992;AI&#31995;&#32479;&#36827;&#34892;&#20915;&#31574;&#26102;&#30340;&#21487;&#20449;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#25324;&#34920;&#36798;&#12289;&#30495;&#23454;&#21644;&#22522;&#26412;&#27700;&#24179;&#30340;&#19981;&#21516;&#20449;&#20219;&#32423;&#21035;&#30340;TAI&#20998;&#31867;&#31995;&#32479;&#25110;&#26694;&#26550;&#65292;&#20351;&#29992;&#21313;&#20010;&#32500;&#24230;&#26469;&#34913;&#37327;&#20449;&#20219;&#65292;&#24182;&#25552;&#20379;&#20102;&#29616;&#26377;TAI&#30740;&#31350;&#30340;&#35843;&#26597;&#21644;&#20803;&#20998;&#26512;&#65292;&#36824;&#30830;&#23450;&#20102;TAI&#30740;&#31350;&#30340;&#26410;&#26469;&#26041;&#21521;&#21644;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21046;&#23450;&#25112;&#30053;&#20915;&#31574;&#26102;&#65292;&#25105;&#20204;&#24120;&#24120;&#38754;&#20020;&#30528;&#22823;&#37327;&#38656;&#35201;&#22788;&#29702;&#30340;&#20449;&#24687;&#12290;&#24403;&#19968;&#20123;&#35777;&#25454;&#30456;&#20114;&#30683;&#30462;&#25110;&#33258;&#30456;&#30683;&#30462;&#26102;&#65292;&#24773;&#20917;&#21487;&#33021;&#20250;&#26356;&#21152;&#22797;&#26434;&#12290;&#27492;&#26102;&#65292;&#38382;&#39064;&#22312;&#20110;&#22914;&#20309;&#30830;&#23450;&#21738;&#20123;&#20449;&#24687;&#26159;&#26377;&#29992;&#30340;&#65292;&#21738;&#20123;&#24212;&#35813;&#34987;&#25490;&#38500;&#12290;&#36825;&#20010;&#36807;&#31243;&#34987;&#31216;&#20026;&#20803;&#20915;&#31574;&#12290;&#21516;&#26679;&#65292;&#22312;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#36827;&#34892;&#25112;&#30053;&#20915;&#31574;&#26102;&#65292;&#23545;AI&#26412;&#36523;&#30340;&#20449;&#20219;&#23601;&#25104;&#20026;&#20102;&#19968;&#20010;&#20803;&#20915;&#31574;&#65292;&#22240;&#20026;&#35768;&#22810;AI&#31995;&#32479;&#34987;&#35270;&#20026;&#22788;&#29702;&#22823;&#37327;&#25968;&#25454;&#30340;&#19981;&#36879;&#26126;&#8220;&#40657;&#21283;&#23376;&#8221;&#12290;&#20449;&#20219;&#19968;&#20010;&#19981;&#36879;&#26126;&#30340;&#31995;&#32479;&#28041;&#21450;&#20915;&#23450;&#20160;&#20040;&#26679;&#30340;&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#65288;TAI&#65289;&#27700;&#24179;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21363;&#24341;&#20837;&#19968;&#20010;&#26032;&#39062;&#30340;TAI&#20998;&#31867;&#31995;&#32479;&#25110;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#39046;&#22495;&#65306;&#34920;&#36798;&#12289;&#30495;&#23454;&#21644;&#22522;&#26412;&#27700;&#24179;&#30340;&#19981;&#21516;&#20449;&#20219;&#32423;&#21035;&#12290;&#20026;&#20102;&#25903;&#25745;&#36825;&#20123;&#39046;&#22495;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#21313;&#20010;&#32500;&#24230;&#26469;&#34913;&#37327;&#20449;&#20219;&#65306;&#21487;&#35299;&#37322;&#24615;/&#36879;&#26126;&#24615;&#12289;&#26080;&#20559;&#24615;&#12289;&#38382;&#36131;&#21046;&#12289;&#20934;&#30830;&#24615;&#12289;&#21487;&#38752;&#24615;&#12289;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#20844;&#24179;&#24615;&#21644;&#20154;&#31867;&#25511;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29616;&#26377;TAI&#30740;&#31350;&#30340;&#35843;&#26597;&#21644;&#20803;&#20998;&#26512;&#65292;&#20197;&#20840;&#38754;&#20102;&#35299;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#30340;&#30740;&#31350;&#29616;&#29366;&#12290;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;TAI&#30740;&#31350;&#30340;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#28508;&#22312;&#24212;&#29992;&#65292;&#36825;&#20123;&#24212;&#29992;&#21487;&#33021;&#20174;TAI&#30340;&#20351;&#29992;&#20013;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
When making strategic decisions, we are often confronted with overwhelming information to process. The situation can be further complicated when some pieces of evidence are contradicted each other or paradoxical. The challenge then becomes how to determine which information is useful and which ones should be eliminated. This process is known as meta-decision. Likewise, when it comes to using Artificial Intelligence (AI) systems for strategic decision-making, placing trust in the AI itself becomes a meta-decision, given that many AI systems are viewed as opaque "black boxes" that process large amounts of data. Trusting an opaque system involves deciding on the level of Trustworthy AI (TAI). We propose a new approach to address this issue by introducing a novel taxonomy or framework of TAI, which encompasses three crucial domains: articulate, authentic, and basic for different levels of trust. To underpin these domains, we create ten dimensions to measure trust: explainability/transparen
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#26412;&#20307;&#35770;&#65292;&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#26377;&#20851;&#32593;&#32476;&#23433;&#20840;&#26412;&#20307;&#35770;&#39046;&#22495;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.00377</link><description>&lt;p&gt;
&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#24320;&#21457;&#21644;&#26500;&#24314;&#26412;&#20307;&#35770;
&lt;/p&gt;
&lt;p&gt;
Developing and Building Ontologies in Cyber Security. (arXiv:2306.00377v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00377
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#26412;&#20307;&#35770;&#65292;&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#26377;&#20851;&#32593;&#32476;&#23433;&#20840;&#26412;&#20307;&#35770;&#39046;&#22495;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#23433;&#20840;&#26159;&#25105;&#20204;&#29616;&#20195;&#31038;&#20250;&#20013;&#26368;&#21463;&#20851;&#27880;&#30340;&#23398;&#31185;&#20043;&#19968;&#12290;&#25105;&#20204;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#24037;&#20316;&#65292;&#36873;&#25321;&#20102;&#32593;&#32476;&#23433;&#20840;&#26412;&#20307;&#35770;&#20316;&#20026;&#30740;&#31350;&#20027;&#39064;&#12290;&#25105;&#20204;&#27719;&#38598;&#20102;&#26368;&#26032;&#21644;&#20197;&#21069;&#30340;&#26412;&#20307;&#35770;&#65292;&#24182;&#26681;&#25454;&#19981;&#21516;&#30340;&#20998;&#26512;&#22240;&#32032;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#25214;&#21040;&#26368;&#20339;&#26041;&#26696;&#12290;&#36873;&#25321;&#27492;&#20027;&#39064;&#30340;&#21407;&#22240;&#26159;&#27719;&#38598;&#26469;&#33258;&#19981;&#21516;&#26102;&#20195;&#30340;&#19981;&#21516;&#26412;&#20307;&#35770;&#12290;&#22240;&#20026;&#30740;&#31350;&#20013;&#21253;&#25324;&#30340;SLR&#22823;&#22810;&#30740;&#31350;&#21333;&#20010;&#26412;&#20307;&#35770;&#12290;&#22914;&#26524;&#20219;&#20309;&#30740;&#31350;&#20154;&#21592;&#24819;&#35201;&#30740;&#31350;&#26412;&#20307;&#35770;&#65292;&#20182;&#24517;&#39035;&#30740;&#31350;&#27599;&#20010;&#21333;&#29420;&#30340;&#26412;&#20307;&#35770;&#24182;&#36873;&#25321;&#26368;&#36866;&#21512;&#20182;&#30740;&#31350;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#27719;&#38598;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#26412;&#20307;&#35770;&#65292;&#24182;&#30456;&#20114;&#27604;&#36739;&#20197;&#24471;&#21040;&#26368;&#20339;&#26041;&#26696;&#12290;&#36890;&#36807;&#31995;&#32479;&#24615;&#36807;&#31243;&#35748;&#30495;&#36873;&#25321;&#20102;2010&#24180;&#33267;2020&#24180;&#38388;&#30340;24&#31687;&#35770;&#25991;&#65292;&#24182;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#26368;&#21518;&#65292;&#26412;SLR&#21576;&#29616;&#32473;&#30740;&#31350;&#20154;&#21592;&#26377;&#20851;&#32593;&#32476;&#23433;&#20840;&#26412;&#20307;&#35770;&#39046;&#22495;&#30340;&#26377;&#21069;&#36884;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cyber Security is one of the most arising disciplines in our modern society. We work on Cybersecurity domain and in this the topic we chose is Cyber Security Ontologies. In this we gather all latest and previous ontologies and compare them on the basis of different analyzing factors to get best of them. Reason to select this topic is to assemble different ontologies from different era of time. Because, researches that included in this SLR is mostly studied single ontology. If any researcher wants to study ontologies, he has to study every single ontology and select which one is best for his research. So, we assemble different types of ontology and compare them against each other to get best of them. A total 24 papers between years 2010-2020 are carefully selected through systematic process and classified accordingly. Lastly, this SLR have been presented to provide the researchers promising future directions in the domain of cybersecurity ontologies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22240;&#26524;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;ATE&#65289;&#20998;&#25968;&#21644;&#21453;&#20107;&#23454;&#22686;&#24378;&#26041;&#27861;&#30340;CFL&#20307;&#31995;&#26550;&#26500;&#65292;&#36890;&#36807;&#25554;&#20837;&#24335;&#26041;&#24335;&#23545;&#39044;&#35757;&#32451;&#30340;LM&#36827;&#34892;&#35299;&#27602;&#65292;&#23454;&#29616;&#20102;&#22240;&#26524;&#20844;&#24179;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#27492;&#26041;&#27861;&#19981;&#20250;&#23545;&#27169;&#22411;&#22256;&#24785;&#20135;&#29983;&#22826;&#22823;&#24433;&#21709;&#65292;&#36824;&#20943;&#36731;&#20102;&#24847;&#22806;&#30340;&#20559;&#32622;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.00374</link><description>&lt;p&gt;
CFL&#65306;&#36890;&#36807;&#22522;&#20110;Token&#32423;&#23646;&#24615;&#25511;&#21046;&#30340;&#29983;&#25104;&#23454;&#29616;&#22240;&#26524;&#20844;&#24179;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CFL: Causally Fair Language Models Through Token-level Attribute Controlled Generation. (arXiv:2306.00374v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22240;&#26524;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;ATE&#65289;&#20998;&#25968;&#21644;&#21453;&#20107;&#23454;&#22686;&#24378;&#26041;&#27861;&#30340;CFL&#20307;&#31995;&#26550;&#26500;&#65292;&#36890;&#36807;&#25554;&#20837;&#24335;&#26041;&#24335;&#23545;&#39044;&#35757;&#32451;&#30340;LM&#36827;&#34892;&#35299;&#27602;&#65292;&#23454;&#29616;&#20102;&#22240;&#26524;&#20844;&#24179;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#27492;&#26041;&#27861;&#19981;&#20250;&#23545;&#27169;&#22411;&#22256;&#24785;&#20135;&#29983;&#22826;&#22823;&#24433;&#21709;&#65292;&#36824;&#20943;&#36731;&#20102;&#24847;&#22806;&#30340;&#20559;&#32622;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#23646;&#24615;&#65292;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#22240;&#26524;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;ATE&#65289;&#20998;&#25968;&#21644;&#21453;&#20107;&#23454;&#22686;&#24378;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#22312;LM&#35299;&#27602;&#30340;&#32972;&#26223;&#19979;&#25552;&#20986;&#20102;CFL&#65288;causally fair language&#65289;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#25554;&#20837;&#24335;&#26041;&#24335;&#23545;&#39044;&#35757;&#32451;&#30340;LM&#36827;&#34892;&#35299;&#27602;&#12290;&#25105;&#20204;&#30340;&#20307;&#31995;&#32467;&#26500;&#22522;&#20110;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#65292;&#25968;&#23398;&#36879;&#26126;&#19988;&#19982;&#35768;&#22810;&#29616;&#26377;&#35299;&#27602;&#25216;&#26415;&#30456;&#27604;&#35745;&#31639;&#25928;&#29575;&#39640;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20960;&#20010;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#29702;&#35299;LM&#22312;&#26377;&#23475;&#25991;&#26412;&#29983;&#25104;&#29615;&#22659;&#20013;&#30340;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;\RTP(RTP)&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#27602;&#24615;&#36864;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CFL&#23454;&#29616;&#20102;&#36825;&#31181;&#35299;&#27602;&#65292;&#23545;&#27169;&#22411;&#22256;&#24785;&#27809;&#26377;&#22826;&#22823;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23545;BOLD&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;CFL&#30340;&#20943;&#36731;&#24847;&#22806;&#20559;&#32622;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method to control the attributes of Language Models (LMs) for the text generation task using Causal Average Treatment Effect (ATE) scores and counterfactual augmentation. We explore this method, in the context of LM detoxification, and propose the Causally Fair Language (CFL) architecture for detoxifying pre-trained LMs in a plug-and-play manner. Our architecture is based on a Structural Causal Model (SCM) that is mathematically transparent and computationally efficient as compared with many existing detoxification techniques. We also propose several new metrics that aim to better understand the behaviour of LMs in the context of toxic text generation. Further, we achieve state of the art performance for toxic degeneration, which are computed using \RTP (RTP) benchmark. Our experiments show that CFL achieves such a detoxification without much impact on the model perplexity. We also show that CFL mitigates the unintended bias problem through experiments on the BOLD dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24314;&#31435;&#20102;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#19968;&#33268;&#24615;&#25193;&#25955;&#27169;&#22411;&#21644;Fokker-Planck&#27491;&#21017;&#21270;&#20043;&#38388;&#30340;&#29702;&#35770;&#32852;&#31995;&#65292;&#20026;&#19981;&#21516;&#30446;&#26631;&#22686;&#24378;&#25193;&#25955;&#27169;&#22411;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#21644;&#21253;&#23481;&#24615;&#30340;&#26694;&#26550;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.00367</link><description>&lt;p&gt;
&#20851;&#20110;&#19968;&#33268;&#24615;&#27169;&#22411;&#30340;&#31561;&#20215;&#24615;: &#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#19968;&#33268;&#24615;&#25193;&#25955;&#27169;&#22411;&#21644;Fokker-Planck&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
On the Equivalence of Consistency-Type Models: Consistency Models, Consistent Diffusion Models, and Fokker-Planck Regularization. (arXiv:2306.00367v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24314;&#31435;&#20102;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#19968;&#33268;&#24615;&#25193;&#25955;&#27169;&#22411;&#21644;Fokker-Planck&#27491;&#21017;&#21270;&#20043;&#38388;&#30340;&#29702;&#35770;&#32852;&#31995;&#65292;&#20026;&#19981;&#21516;&#30446;&#26631;&#22686;&#24378;&#25193;&#25955;&#27169;&#22411;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#21644;&#21253;&#23481;&#24615;&#30340;&#26694;&#26550;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20013;&#21508;&#31181;&#8220;&#19968;&#33268;&#24615;&#8221;&#27010;&#24565;&#30340;&#20986;&#29616;&#24341;&#36215;&#20102;&#24456;&#22823;&#20851;&#27880;&#65292;&#24110;&#21161;&#25913;&#21892;&#20102;&#26679;&#26412;&#36136;&#37327;&#12289;&#20284;&#28982;&#20272;&#35745;&#21644;&#21152;&#36895;&#25277;&#26679;&#12290;&#34429;&#28982;&#22312;&#25991;&#29486;&#20013;&#31867;&#20284;&#30340;&#27010;&#24565;&#24050;&#32463;&#34987;&#25552;&#20986;&#65292;&#20294;&#23427;&#20204;&#20043;&#38388;&#30340;&#30830;&#20999;&#20851;&#31995;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19977;&#20010;&#26368;&#36817;&#30340;&#8220;&#19968;&#33268;&#24615;&#8221;&#27010;&#24565;&#20043;&#38388;&#30340;&#29702;&#35770;&#32852;&#31995;&#65292;&#26088;&#22312;&#20026;&#19981;&#21516;&#30446;&#26631;&#22686;&#24378;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#35265;&#35299;&#20026;&#19968;&#33268;&#24615;&#31867;&#22411;&#27169;&#22411;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#21644;&#21253;&#23481;&#24615;&#30340;&#26694;&#26550;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of various notions of ``consistency'' in diffusion models has garnered considerable attention and helped achieve improved sample quality, likelihood estimation, and accelerated sampling. Although similar concepts have been proposed in the literature, the precise relationships among them remain unclear. In this study, we establish theoretical connections between three recent ``consistency'' notions designed to enhance diffusion models for distinct objectives. Our insights offer the potential for a more comprehensive and encompassing framework for consistency-type models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#22810;&#20219;&#21153;&#23398;&#20064;&#35282;&#24230;&#20986;&#21457;&#65292;&#30740;&#31350;&#20102;&#25193;&#25955;&#35757;&#32451;&#20013;&#30340;&#36127;&#36801;&#31227;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#27491;&#21017;&#21270;&#25216;&#26415;&#22686;&#24378;&#25193;&#25955;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#36127;&#36801;&#31227;&#24182;&#25552;&#39640;&#21435;&#22122;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.00354</link><description>&lt;p&gt;
&#35299;&#20915;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing Negative Transfer in Diffusion Models. (arXiv:2306.00354v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#22810;&#20219;&#21153;&#23398;&#20064;&#35282;&#24230;&#20986;&#21457;&#65292;&#30740;&#31350;&#20102;&#25193;&#25955;&#35757;&#32451;&#20013;&#30340;&#36127;&#36801;&#31227;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#27491;&#21017;&#21270;&#25216;&#26415;&#22686;&#24378;&#25193;&#25955;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#36127;&#36801;&#31227;&#24182;&#25552;&#39640;&#21435;&#22122;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#23427;&#22312;&#21516;&#26102;&#28085;&#30422;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#30340;&#21435;&#22122;&#20219;&#21153;&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#20195;&#34920;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#30340;&#24418;&#24335;&#12290;&#28982;&#32780;&#65292;&#20174;MTL&#30340;&#35282;&#24230;&#20998;&#26512;&#21644;&#25913;&#21892;&#25193;&#25955;&#27169;&#22411;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#29305;&#21035;&#22320;&#65292;MTL&#26377;&#26102;&#20250;&#23548;&#33268;&#20247;&#25152;&#21608;&#30693;&#30340;$\textit{&#36127;&#36801;&#31227;}$&#29616;&#35937;&#65292;&#36825;&#31181;&#29616;&#35937;&#26159;&#30001;&#20110;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#20914;&#31361;&#32780;&#23548;&#33268;&#26576;&#20123;&#20219;&#21153;&#30340;&#24615;&#33021;&#38477;&#20302;&#12290;&#26412;&#25991;&#26088;&#22312;&#20174;MTL&#30340;&#35282;&#24230;&#20998;&#26512;&#25193;&#25955;&#35757;&#32451;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#35266;&#23519;&#65306;$\textbf{(O1)}$ &#38543;&#30528;&#22122;&#22768;&#27700;&#24179;&#20043;&#38388;&#30340;&#24046;&#36317;&#21152;&#22823;&#65292;&#21435;&#22122;&#20219;&#21153;&#20043;&#38388;&#30340;&#20219;&#21153;&#20146;&#21644;&#21147;&#20943;&#24369;&#65292; $\textbf{(O2)}$ &#22312;&#25193;&#25955;&#35757;&#32451;&#30340;&#32972;&#26223;&#19979;&#65292;&#36127;&#36801;&#31227;&#20063;&#21487;&#33021;&#20250;&#20986;&#29616;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20943;&#36731;&#36127;&#36801;&#31227;&#26469;&#22686;&#24378;&#25193;&#25955;&#35757;&#32451;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;&#29616;&#26377;&#30340;MTL&#26041;&#27861;&#12289;&#20855;&#20307;&#26159;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#26469;&#40723;&#21169;&#20219;&#21153;&#29305;&#23450;&#30340;&#29305;&#24449;&#25552;&#21462;&#24182;&#20943;&#23569;&#20219;&#21153;&#24178;&#25200;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#20943;&#36731;&#20102;&#36127;&#36801;&#31227;&#65292;&#25552;&#39640;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#21435;&#22122;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based generative models have achieved remarkable success in various domains. It trains a model on denoising tasks that encompass different noise levels simultaneously, representing a form of multi-task learning (MTL). However, analyzing and improving diffusion models from an MTL perspective remains under-explored. In particular, MTL can sometimes lead to the well-known phenomenon of $\textit{negative transfer}$, which results in the performance degradation of certain tasks due to conflicts between tasks. In this paper, we aim to analyze diffusion training from an MTL standpoint, presenting two key observations: $\textbf{(O1)}$ the task affinity between denoising tasks diminishes as the gap between noise levels widens, and $\textbf{(O2)}$ negative transfer can arise even in the context of diffusion training. Building upon these observations, our objective is to enhance diffusion training by mitigating negative transfer. To achieve this, we propose leveraging existing MTL metho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26174;&#24335;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#19982;&#38544;&#24335;&#27491;&#21017;&#21270;&#32467;&#21512;&#65292;&#21487;&#20197;&#20351;&#21333;&#23618;&#32593;&#32476;&#23454;&#29616;&#19982;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#30456;&#24403;&#30340;&#20302;&#31209;&#36817;&#20284;&#21644;&#27867;&#21270;&#35823;&#24046;&#65292;&#20351;&#28145;&#24230;&#19981;&#20877;&#26159;&#23398;&#20064;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2306.00342</link><description>&lt;p&gt;
&#32467;&#21512;&#26174;&#24335;&#21644;&#38544;&#24335;&#27491;&#21017;&#21270;&#30340;&#28145;&#24230;&#32593;&#32476;&#39640;&#25928;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Combining Explicit and Implicit Regularization for Efficient Learning in Deep Networks. (arXiv:2306.00342v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26174;&#24335;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#19982;&#38544;&#24335;&#27491;&#21017;&#21270;&#32467;&#21512;&#65292;&#21487;&#20197;&#20351;&#21333;&#23618;&#32593;&#32476;&#23454;&#29616;&#19982;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#30456;&#24403;&#30340;&#20302;&#31209;&#36817;&#20284;&#21644;&#27867;&#21270;&#35823;&#24046;&#65292;&#20351;&#28145;&#24230;&#19981;&#20877;&#26159;&#23398;&#20064;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#27491;&#21017;&#21270;&#30740;&#31350;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#26799;&#24230;&#36712;&#36857;&#65292;&#20197;&#35299;&#37322;&#20026;&#20160;&#20040;&#28145;&#24230;&#32593;&#32476;&#26356;&#20542;&#21521;&#20110;&#26576;&#20123;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#20013;&#65292;&#24050;&#32463;&#35777;&#26126;&#26799;&#24230;&#19979;&#38477;&#38544;&#24335;&#22320;&#26397;&#21521;&#30697;&#38453;&#34917;&#20840;/&#22240;&#24335;&#20998;&#35299;&#20219;&#21153;&#19978;&#30340;&#20302;&#31209;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;&#28155;&#21152;&#23618;&#25968;&#19981;&#20165;&#21487;&#20197;&#25552;&#39640;&#36825;&#20123;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#20316;&#20026;&#19968;&#31181;&#21152;&#36895;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#36825;&#31181;&#20302;&#31209;&#20559;&#21521;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26174;&#24335;&#24809;&#32602;&#26469;&#21453;&#26144;&#36825;&#31181;&#38544;&#24335;&#20559;&#24046;&#65292;&#21482;&#22312;&#26576;&#20123;&#33258;&#36866;&#24212;&#26799;&#24230;&#20248;&#21270;&#22120;&#65288;&#20363;&#22914;Adam&#65289;&#36215;&#20316;&#29992;&#12290;&#36825;&#31181;&#32452;&#21512;&#21487;&#20197;&#20351;&#36864;&#21270;&#30340;&#21333;&#23618;&#32593;&#32476;&#23454;&#29616;&#19982;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#30456;&#24403;&#30340;&#20302;&#31209;&#36817;&#20284;&#21644;&#27867;&#21270;&#35823;&#24046;&#65292;&#20351;&#28145;&#24230;&#19981;&#20877;&#26159;&#23398;&#20064;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#21333;&#23618;&#32593;&#32476;&#36824;&#22312;&#19968;&#31995;&#21015;&#21442;&#25968;&#21644;&#25968;&#25454;&#38598;&#19978;&#33021;&#22815;&#34920;&#29616;&#20248;&#24322;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#21508;&#31181;&#30697;&#38453;&#34917;&#20840;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Works on implicit regularization have studied gradient trajectories during the optimization process to explain why deep networks favor certain kinds of solutions over others. In deep linear networks, it has been shown that gradient descent implicitly regularizes toward low-rank solutions on matrix completion/factorization tasks. Adding depth not only improves performance on these tasks but also acts as an accelerative pre-conditioning that further enhances this bias towards low-rankedness. Inspired by this, we propose an explicit penalty to mirror this implicit bias which only takes effect with certain adaptive gradient optimizers (e.g. Adam). This combination can enable a degenerate single-layer network to achieve low-rank approximations with generalization error comparable to deep linear networks, making depth no longer necessary for learning. The single-layer network also performs competitively or out-performs various approaches for matrix completion over a range of parameter and da
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;IBIA&#26694;&#26550;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;PGM&#36716;&#21270;&#20026;&#26377;&#30028;&#22242;&#22823;&#23567;&#30340;SLCTF&#24207;&#21015;&#65292;&#24182;&#20351;&#29992;&#21551;&#21457;&#24335;&#32622;&#20449;&#24230;&#26356;&#26032;&#31639;&#27861;&#26469;&#25512;&#23548;&#36793;&#32536;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#26356;&#21152;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2306.00335</link><description>&lt;p&gt;
&#20351;&#29992;IBIA&#26694;&#26550;&#30340;&#36793;&#32536;&#36817;&#20284;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Approximate inference of marginals using the IBIA framework. (arXiv:2306.00335v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00335
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;IBIA&#26694;&#26550;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;PGM&#36716;&#21270;&#20026;&#26377;&#30028;&#22242;&#22823;&#23567;&#30340;SLCTF&#24207;&#21015;&#65292;&#24182;&#20351;&#29992;&#21551;&#21457;&#24335;&#32622;&#20449;&#24230;&#26356;&#26032;&#31639;&#27861;&#26469;&#25512;&#23548;&#36793;&#32536;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#26356;&#21152;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#22270;&#27169;&#22411;&#65288;PGM&#65289;&#20013;&#36793;&#32536;&#30340;&#31934;&#30830;&#25512;&#26029;&#34987;&#35748;&#20026;&#26159;&#19981;&#21487;&#35745;&#31639;&#30340;&#65292;&#22240;&#27492;&#38656;&#35201;&#20351;&#29992;&#36817;&#20284;&#26041;&#27861;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#21464;&#20998;&#25216;&#26415;&#22312;&#29615;&#36335;&#22270;&#20013;&#25191;&#34892;&#36845;&#20195;&#20449;&#24687;&#20256;&#36882;&#65292;&#36825;&#23545;&#20110;&#35768;&#22810;&#22522;&#20934;&#27979;&#35797;&#26469;&#35828;&#25910;&#25947;&#36895;&#24230;&#24930;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22686;&#37327;&#26500;&#24314;-&#25512;&#29702;-&#36817;&#20284;&#65288;IBIA&#65289;&#33539;&#20363;&#30340;&#26032;&#22411;&#36793;&#32536;&#25512;&#26029;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#23558;PGM&#36716;&#21270;&#20026;&#20855;&#26377;&#26377;&#30028;&#22242;&#22823;&#23567;&#30340;&#19968;&#31995;&#21015;&#38142;&#25509;&#30340;&#22242;&#26641;&#26862;&#26519;&#65288;SLCTF&#65289;&#65292;&#28982;&#21518;&#20351;&#29992;&#21551;&#21457;&#24335;&#32622;&#20449;&#24230;&#26356;&#26032;&#31639;&#27861;&#26469;&#25512;&#23548;&#36793;&#32536;&#12290;&#23545;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#25105;&#20204;&#26174;&#31034;&#22914;&#26524;IBIA&#20013;&#22686;&#37327;&#26500;&#24314;&#27493;&#39588;&#20351;&#29992;&#21464;&#37327;&#30340;&#25299;&#25169;&#39034;&#24207;&#65292;&#21017;&#65288;a&#65289;&#25152;&#26377;SLCTF&#20013;&#30340;CTF&#30340;&#20808;&#39564;&#36793;&#32536;&#19968;&#33268;&#65292;&#65288;b&#65289;&#19968;&#26086;&#23558;&#25152;&#26377;&#35777;&#25454;&#21464;&#37327;&#28155;&#21152;&#21040;SLCTF&#20013;&#65292;&#21518;&#39564;&#36793;&#32536;&#23601;&#26159;&#19968;&#33268;&#30340;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#32622;&#20449;&#20256;&#36882;&#27493;&#39588;&#26159;&#38750;&#36845;&#20195;&#30340;&#65292;&#20934;&#30830;&#24230;&#21644;&#22797;&#26434;&#24230;&#20043;&#38388;&#30340;&#26435;&#34913;&#21487;&#20197;&#36890;&#36807;&#35843;&#25972;&#22242;&#30340;&#22823;&#23567;&#26469;&#25511;&#21046;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#25105;&#20204;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exact inference of marginals in probabilistic graphical models (PGM) is known to be intractable, necessitating the use of approximate methods. Most of the existing variational techniques perform iterative message passing in loopy graphs which is slow to converge for many benchmarks. In this paper, we propose a new algorithm for marginal inference that is based on the incremental build-infer-approximate (IBIA) paradigm. Our algorithm converts the PGM into a sequence of linked clique tree forests (SLCTF) with bounded clique sizes, and then uses a heuristic belief update algorithm to infer the marginals. For the special case of Bayesian networks, we show that if the incremental build step in IBIA uses the topological order of variables then (a) the prior marginals are consistent in all CTFs in the SLCTF and (b) the posterior marginals are consistent once all evidence variables are added to the SLCTF. In our approach, the belief propagation step is non-iterative and the accuracy-complexity
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#22411;&#27169;&#22411;&#30340;&#22810;&#32500;&#28145;&#24230;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#26041;&#27861;&#65292;&#22312;&#35821;&#38899;&#22686;&#24378;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#30001;&#20110;&#20351;&#29992;&#30340;&#21442;&#25968;&#27604;&#24120;&#35268;&#21367;&#31215;&#23618;&#23569;&#65292;&#25552;&#20986;&#30340;TF&#22495;S4&#27169;&#22411;&#22823;&#23567;&#20943;&#23569;&#20102;78.6&#65285;&#65292;&#21516;&#26102;&#22312;&#25968;&#25454;&#22686;&#24191;&#21518;&#33719;&#24471;3.15&#30340;PESQ&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2306.00331</link><description>&lt;p&gt;
&#22810;&#32500;&#28145;&#24230;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#26041;&#27861;&#22312;&#35821;&#38899;&#22686;&#24378;&#20013;&#30340;&#24212;&#29992;&#65306;&#22522;&#20110;&#23567;&#22411;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Multi-dimensional Deep Structured State Space Approach to Speech Enhancement Using Small-footprint Models. (arXiv:2306.00331v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00331
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#22411;&#27169;&#22411;&#30340;&#22810;&#32500;&#28145;&#24230;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#26041;&#27861;&#65292;&#22312;&#35821;&#38899;&#22686;&#24378;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#30001;&#20110;&#20351;&#29992;&#30340;&#21442;&#25968;&#27604;&#24120;&#35268;&#21367;&#31215;&#23618;&#23569;&#65292;&#25552;&#20986;&#30340;TF&#22495;S4&#27169;&#22411;&#22823;&#23567;&#20943;&#23569;&#20102;78.6&#65285;&#65292;&#21516;&#26102;&#22312;&#25968;&#25454;&#22686;&#24191;&#21518;&#33719;&#24471;3.15&#30340;PESQ&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32500;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#65288;S4&#65289;&#26041;&#27861;&#26469;&#36827;&#34892;&#35821;&#38899;&#22686;&#24378;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#25429;&#25417;&#39057;&#29575;&#36724;&#19978;&#30340;&#35889;&#20381;&#36182;&#24615;&#65292;&#25105;&#20204;&#30528;&#37325;&#20110;&#20351;&#29992;&#30333;&#21270;&#21464;&#25442;&#20462;&#25913;&#22810;&#32500;S4&#23618;&#65292;&#20197;&#26500;&#24314;&#20855;&#26377;&#33391;&#22909;&#24615;&#33021;&#30340;&#26032;&#23567;&#22411;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#26102;&#38388;&#65288;T&#65289;&#21644;&#26102;&#39057;&#65288;TF&#65289;&#22495;&#25506;&#32034;&#20102;&#20960;&#20010;&#22522;&#20110;S4&#30340;&#28145;&#24230;&#26550;&#26500;&#12290;&#23613;&#31649;&#20351;&#29992;&#30340;&#21442;&#25968;&#27604;&#24120;&#35268;&#21367;&#31215;&#23618;&#23569;&#65292;&#20294;2D S4&#23618;&#21487;&#20197;&#34987;&#35270;&#20026;&#20855;&#26377;&#26080;&#38480;&#25509;&#25910;&#22495;&#30340;&#29305;&#27530;&#21367;&#31215;&#23618;&#12290;&#22312;VoiceBank-DEMAND&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#26102;&#65292;&#19982;&#22522;&#20110;&#21367;&#31215;&#23618;&#30340;&#24120;&#35268;U-net&#27169;&#22411;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;TF&#22495;S4&#27169;&#22411;&#22823;&#23567;&#20943;&#23569;&#20102;78.6&#65285;&#65292;&#20294;&#20173;&#22312;&#25968;&#25454;&#22686;&#24191;&#21518;&#33719;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#65292;PESQ&#24471;&#20998;&#20026;3.15&#12290;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#65292;&#25105;&#20204;&#29978;&#33267;&#21487;&#20197;&#36798;&#21040;3.18&#30340;PESQ&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a multi-dimensional structured state space (S4) approach to speech enhancement. To better capture the spectral dependencies across the frequency axis, we focus on modifying the multi-dimensional S4 layer with whitening transformation to build new small-footprint models that also achieve good performance. We explore several S4-based deep architectures in time (T) and time-frequency (TF) domains. The 2-D S4 layer can be considered a particular convolutional layer with an infinite receptive field although it utilizes fewer parameters than a conventional convolutional layer. Evaluated on the VoiceBank-DEMAND data set, when compared with the conventional U-net model based on convolutional layers, the proposed TF-domain S4-based model is 78.6% smaller in size, yet it still achieves competitive results with a PESQ score of 3.15 with data augmentation. By increasing the model size, we can even reach a PESQ score of 3.18.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#8220;&#24605;&#32500;&#20811;&#38534;&#8221;&#65292;&#36890;&#36807;&#23398;&#20064;&#20154;&#31867;&#30340;&#24605;&#32500;&#26469;&#35757;&#32451;AI&#20195;&#29702;&#65292;&#20197;&#22312;&#27867;&#21270;&#12289;&#25506;&#32034;&#12289;&#35268;&#21010;&#31561;&#33021;&#21147;&#26041;&#38754;&#23454;&#29616;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.00323</link><description>&lt;p&gt;
&#8220;&#24605;&#32500;&#20811;&#38534;&#65306;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#24605;&#32500;&#23398;&#20064;&#24605;&#32771;&#24182;&#34892;&#21160;&#8221;&#12290;&#65288;arXiv:2306.00323v1 [cs.AI]&#65289;
&lt;/p&gt;
&lt;p&gt;
Thought Cloning: Learning to Think while Acting by Imitating Human Thinking. (arXiv:2306.00323v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#8220;&#24605;&#32500;&#20811;&#38534;&#8221;&#65292;&#36890;&#36807;&#23398;&#20064;&#20154;&#31867;&#30340;&#24605;&#32500;&#26469;&#35757;&#32451;AI&#20195;&#29702;&#65292;&#20197;&#22312;&#27867;&#21270;&#12289;&#25506;&#32034;&#12289;&#35268;&#21010;&#31561;&#33021;&#21147;&#26041;&#38754;&#23454;&#29616;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#20154;&#31867;&#24605;&#32500;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#23427;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#38750;&#20961;&#30340;&#27867;&#21270;&#12289;&#25506;&#32034;&#12289;&#35268;&#21010;&#12289;&#37325;&#26032;&#35268;&#21010;&#21644;&#36866;&#24212;&#26032;&#24773;&#20917;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#22312;&#36825;&#20123;&#33021;&#21147;&#20013;&#36828;&#26410;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#20551;&#35774;&#20854;&#20013;&#19968;&#20010;&#35748;&#30693;&#32570;&#38519;&#30340;&#21407;&#22240;&#26159;&#20182;&#20204;&#32570;&#20047;&#20351;&#29992;&#35821;&#35328;&#24605;&#32771;&#25152;&#24102;&#26469;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#35748;&#20026;&#36890;&#36807;&#35757;&#32451;AI&#20195;&#29702;&#20154;&#20687;&#20154;&#31867;&#19968;&#26679;&#24605;&#32771;&#65292;&#21487;&#20197;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#8220;&#24605;&#32500;&#20811;&#38534;&#8221;&#65292;&#20854;&#24819;&#27861;&#19981;&#20165;&#26159;&#20811;&#38534;&#20154;&#31867;&#31034;&#33539;&#32773;&#30340;&#34892;&#20026;&#65292;&#32780;&#19988;&#36824;&#21253;&#25324;&#20154;&#31867;&#22312;&#25191;&#34892;&#36825;&#20123;&#34892;&#20026;&#26102;&#25152;&#20135;&#29983;&#30340;&#24819;&#27861;&#12290;&#34429;&#28982;&#25105;&#20204;&#24076;&#26395;&#8220;&#24605;&#32500;&#20811;&#38534;&#8221;&#22312;&#22788;&#29702;&#32593;&#32476;&#35268;&#27169;&#30340;&#20154;&#31867;&#24605;&#32500;&#21644;&#34892;&#20026;&#25968;&#25454;&#26102;&#33021;&#22815;&#21457;&#25381;&#20986;&#33394;&#65288;&#20363;&#22914;&#65292;&#24102;&#26377;&#21095;&#26412;&#30340;&#22312;&#32447;&#35270;&#39057;&#65289;&#65292;&#20294;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22312;&#24605;&#32771;&#21644;&#34892;&#21160;&#25968;&#25454;&#20026;&#21512;&#25104;&#29983;&#25104;&#30340;&#39046;&#22495;&#30340;&#23454;&#39564;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#8220;&#24605;&#32500;&#20811;&#38534;&#8221;&#23398;&#20064;&#36895;&#24230;&#27604;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24555;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is often considered a key aspect of human thinking, providing us with exceptional abilities to generalize, explore, plan, replan, and adapt to new situations. However, Reinforcement Learning (RL) agents are far from human-level performance in any of these abilities. We hypothesize one reason for such cognitive deficiencies is that they lack the benefits of thinking in language and that we can improve AI agents by training them to think like humans do. We introduce a novel Imitation Learning framework, Thought Cloning, where the idea is to not just clone the behaviors of human demonstrators, but also the thoughts humans have as they perform these behaviors. While we expect Thought Cloning to truly shine at scale on internet-sized datasets of humans thinking out loud while acting (e.g. online videos with transcripts), here we conduct experiments in a domain where the thinking and action data are synthetically generated. Results reveal that Thought Cloning learns much faster than
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20803;&#32032;&#38500;&#27861;&#30340;&#21487;&#23398;&#20064;&#33293;&#20837;&#26426;&#21046;FlexRound&#65292;&#20351;&#24471;&#21518;&#35757;&#32451;&#37327;&#21270;&#26102;&#26356;&#22909;&#22320;&#37325;&#26500;&#27599;&#20010;&#23618;&#25110;&#22359;&#30340;&#36755;&#20986;&#65292;&#24182;&#19988;&#33021;&#22815;&#23398;&#20064;&#20844;&#20849;&#37327;&#21270;&#32593;&#26684;&#22823;&#23567;&#20197;&#21450;&#27599;&#20010;&#39044;&#35757;&#32451;&#26435;&#37325;&#30340;&#19981;&#21516;&#27604;&#20363;&#23610;&#12290;</title><link>http://arxiv.org/abs/2306.00317</link><description>&lt;p&gt;
&#22522;&#20110;&#20803;&#32032;&#38500;&#27861;&#30340;&#21487;&#23398;&#20064;&#33293;&#20837;&#29992;&#20110;&#21518;&#35757;&#32451;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
FlexRound: Learnable Rounding based on Element-wise Division for Post-Training Quantization. (arXiv:2306.00317v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00317
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20803;&#32032;&#38500;&#27861;&#30340;&#21487;&#23398;&#20064;&#33293;&#20837;&#26426;&#21046;FlexRound&#65292;&#20351;&#24471;&#21518;&#35757;&#32451;&#37327;&#21270;&#26102;&#26356;&#22909;&#22320;&#37325;&#26500;&#27599;&#20010;&#23618;&#25110;&#22359;&#30340;&#36755;&#20986;&#65292;&#24182;&#19988;&#33021;&#22815;&#23398;&#20064;&#20844;&#20849;&#37327;&#21270;&#32593;&#26684;&#22823;&#23567;&#20197;&#21450;&#27599;&#20010;&#39044;&#35757;&#32451;&#26435;&#37325;&#30340;&#19981;&#21516;&#27604;&#20363;&#23610;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#24050;&#32463;&#22312;&#36164;&#28304;&#26377;&#38480;&#35774;&#22791;&#19978;&#37096;&#32626;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#22240;&#20026;&#19982;&#37327;&#21270;&#24863;&#30693;&#22521;&#35757;&#19981;&#21516;&#65292;&#23436;&#20840;&#19981;&#38656;&#35201;&#20840;&#38754;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#25110;&#31471;&#21040;&#31471;&#22521;&#35757;&#12290;&#22240;&#20026;&#22522;&#20110;&#37325;&#26500;&#27599;&#20010;&#23618;&#25110;&#22359;&#36755;&#20986;&#30340;PTQ&#26041;&#26696;&#25928;&#26524;&#26174;&#30528;&#20197;&#22686;&#24378;&#37327;&#21270;&#27169;&#22411;&#24615;&#33021;&#65292;&#25152;&#20197;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#24320;&#21457;&#20102;&#31639;&#27861;&#26469;&#35774;&#35745;&#21644;&#23398;&#20064;&#19968;&#31181;&#26032;&#30340;&#26435;&#37325;&#33293;&#20837;&#26041;&#26696;&#65292;&#20197;&#26356;&#22909;&#22320;&#37325;&#26500;&#27599;&#20010;&#23618;&#25110;&#22359;&#30340;&#36755;&#20986;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26032;&#30340;PTQ&#26435;&#37325;&#33293;&#20837;&#26426;&#21046;&#65292;&#21517;&#20026;FlexRound&#65292;&#20854;&#22522;&#20110;&#20803;&#32032;&#38500;&#27861;&#32780;&#19981;&#26159;&#20856;&#22411;&#30340;&#20803;&#32032;&#21152;&#27861;&#65292;&#20174;&#32780;&#20351;FlexRound&#33021;&#22815;&#21516;&#26102;&#23398;&#20064;&#20844;&#20849;&#37327;&#21270;&#32593;&#26684;&#22823;&#23567;&#20197;&#21450;&#27599;&#20010;&#39044;&#35757;&#32451;&#26435;&#37325;&#30340;&#19981;&#21516;&#27604;&#20363;&#23610;&#12290;&#30001;&#20110;&#20803;&#32032;&#38500;&#27861;&#20135;&#29983;&#30340;&#23548;&#25968;&#30340;&#20114;&#34917;&#35268;&#21017;&#65292;FlexRound&#22312;&#26356;&#26032;&#20854;&#30456;&#20851;&#39044;&#35757;&#32451;&#26435;&#37325;&#26102;&#22825;&#29983;&#33021;&#22815;&#21033;&#29992;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization (PTQ) has been gaining popularity for the deployment of deep neural networks on resource-limited devices since unlike quantization-aware training, neither a full training dataset nor end-to-end training is required at all. As PTQ schemes based on reconstructing each layer or block output turn out to be effective to enhance quantized model performance, recent works have developed algorithms to devise and learn a new weight-rounding scheme so as to better reconstruct each layer or block output. In this work, we propose a simple yet effective new weight-rounding mechanism for PTQ, coined FlexRound, based on element-wise division instead of typical element-wise addition such that FlexRound enables jointly learning a common quantization grid size as well as a different scale for each pre-trained weight. Thanks to the reciprocal rule of derivatives induced by element-wise division, FlexRound is inherently able to exploit pre-trained weights when updating their corr
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36741;&#21161;&#39564;&#35777;&#31995;&#32479;&#65292;&#21487;&#20197;&#26377;&#25928;&#38450;&#24481;&#21508;&#31181;&#23545;&#25239;&#25915;&#20987;&#65292;&#25104;&#21151;&#29575;&#39640;&#36798;97.32%&#12290;</title><link>http://arxiv.org/abs/2306.00314</link><description>&lt;p&gt;
&#22522;&#20110;&#31532;&#20108;&#31867;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#39564;&#35777;&#26041;&#27861;&#30340;&#23545;&#25239;&#24863;&#30693;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Adversarial-Aware Deep Learning System based on a Secondary Classical Machine Learning Verification Approach. (arXiv:2306.00314v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00314
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36741;&#21161;&#39564;&#35777;&#31995;&#32479;&#65292;&#21487;&#20197;&#26377;&#25928;&#38450;&#24481;&#21508;&#31181;&#23545;&#25239;&#25915;&#20987;&#65292;&#25104;&#21151;&#29575;&#39640;&#36798;97.32%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#34987;&#29992;&#20110;&#21019;&#24314;&#21508;&#31181;&#26377;&#25928;&#30340;&#22270;&#20687;&#20998;&#31867;&#24212;&#29992;&#31243;&#24207;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#35797;&#22270;&#23558;&#27169;&#22411;&#24341;&#23548;&#21040;&#39044;&#27979;&#38169;&#35823;&#30340;&#31867;&#21035;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20027;&#35201;&#30340;&#23545;&#25239;&#25915;&#20987;&#27169;&#22411;&#65292;&#21457;&#29616;&#23427;&#20204;&#37117;&#29305;&#21035;&#38024;&#23545;&#21644;&#21033;&#29992;&#20854;&#35774;&#35745;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#12290;&#36825;&#31181;&#29702;&#35299;&#20351;&#25105;&#20204;&#21457;&#23637;&#20102;&#19968;&#31181;&#20551;&#35774;&#65292;&#35748;&#20026;&#22823;&#22810;&#25968;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22914;&#38543;&#26426;&#26862;&#26519;(RF)&#65292;&#23545;&#25239;&#25915;&#20987;&#27169;&#22411;&#20813;&#30123;&#65292;&#22240;&#20026;&#23427;&#20204;&#26681;&#26412;&#19981;&#20381;&#36182;&#20110;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#12290;&#25105;&#20204;&#23545;&#21463;&#27426;&#36814;&#30340;&#23545;&#25239;&#25915;&#20987;&#36827;&#34892;&#30340;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23454;&#39564;&#30740;&#31350;&#25903;&#25345;&#20102;&#36825;&#19968;&#20551;&#35774;&#12290;&#22522;&#20110;&#36825;&#20010;&#20551;&#35774;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24863;&#30693;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20316;&#20026;&#36741;&#21161;&#39564;&#35777;&#31995;&#32479;&#26469;&#34917;&#20805;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#20027;&#35201;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#34429;&#28982;&#31532;&#20108;&#20010;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#19981;&#22914;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39640;&#65292;&#20294;&#23427;&#20316;&#20026;&#24378;&#26377;&#21147;&#30340;&#38450;&#24481;&#25514;&#26045;&#65292;&#23545;&#25239;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#39640;&#36798;97.32%&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models have been used in creating various effective image classification applications. However, they are vulnerable to adversarial attacks that seek to misguide the models into predicting incorrect classes. Our study of major adversarial attack models shows that they all specifically target and exploit the neural networking structures in their designs. This understanding makes us develop a hypothesis that most classical machine learning models, such as Random Forest (RF), are immune to adversarial attack models because they do not rely on neural network design at all. Our experimental study of classical machine learning models against popular adversarial attacks supports this hypothesis. Based on this hypothesis, we propose a new adversarial-aware deep learning system by using a classical machine learning model as the secondary verification system to complement the primary deep learning model in image classification. Although the secondary classical machine learning model
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#32447;&#24615;transformers&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#19968;&#20010;&#27880;&#24847;&#21147;&#23618;&#65292;transformers&#33021;&#22815;&#23454;&#29616;&#19968;&#27425;&#39044;&#22788;&#29702;&#26799;&#24230;&#19979;&#38477;&#65292;&#32780;&#23545;&#20110;&#19968;&#20010;$k$&#20010;&#27880;&#24847;&#21147;&#23618;&#30340;transformer&#65292;&#23427;&#21487;&#20197;&#23454;&#29616;&#22810;&#27425;&#39044;&#22788;&#29702;&#26799;&#24230;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2306.00297</link><description>&lt;p&gt;
Transformers&#23398;&#20064;&#23454;&#29616;&#39044;&#22788;&#29702;&#26799;&#24230;&#19979;&#38477;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transformers learn to implement preconditioned gradient descent for in-context learning. (arXiv:2306.00297v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#32447;&#24615;transformers&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#19968;&#20010;&#27880;&#24847;&#21147;&#23618;&#65292;transformers&#33021;&#22815;&#23454;&#29616;&#19968;&#27425;&#39044;&#22788;&#29702;&#26799;&#24230;&#19979;&#38477;&#65292;&#32780;&#23545;&#20110;&#19968;&#20010;$k$&#20010;&#27880;&#24847;&#21147;&#23618;&#30340;transformer&#65292;&#23427;&#21487;&#20197;&#23454;&#29616;&#22810;&#27425;&#39044;&#22788;&#29702;&#26799;&#24230;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;transformers&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#30340;&#39537;&#21160;&#65292;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;transformers&#21487;&#20197;&#23454;&#29616;&#20687;&#26799;&#24230;&#19979;&#38477;&#36825;&#26679;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#31934;&#24515;&#30340;&#26435;&#37325;&#26500;&#36896;&#65292;&#36825;&#20123;&#30740;&#31350;&#34920;&#26126;&#22810;&#23618;transformers&#20855;&#26377;&#36275;&#22815;&#30340;&#34920;&#36798;&#33021;&#21147;&#26469;&#27169;&#25311;&#26799;&#24230;&#19979;&#38477;&#36845;&#20195;&#12290;&#36229;&#36234;&#34920;&#36798;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#38382;&#65306;transformers&#33021;&#21542;&#36890;&#36807;&#22312;&#38543;&#26426;&#38382;&#39064;&#23454;&#20363;&#19978;&#35757;&#32451;&#26469;&#23398;&#20064;&#23454;&#29616;&#36825;&#26679;&#30340;&#31639;&#27861;&#65311;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36890;&#36807;&#23545;&#32447;&#24615;&#22238;&#24402;&#30340;&#38543;&#26426;&#23454;&#20363;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#31532;&#19968;&#27425;&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#29702;&#35770;&#36827;&#23637;&#65292;&#36890;&#36807;&#23545;&#32447;&#24615;transformers&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#20998;&#26512;&#12290;&#23545;&#20110;&#19968;&#20010;&#27880;&#24847;&#21147;&#23618;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35757;&#32451;&#30446;&#26631;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#23454;&#29616;&#20102;&#19968;&#27425;&#39044;&#22788;&#29702;&#26799;&#24230;&#19979;&#38477;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#39044;&#22788;&#29702;&#30697;&#38453;&#19981;&#20165;&#36866;&#24212;&#36755;&#20837;&#20998;&#24067;&#65292;&#32780;&#19988;&#36824;&#36866;&#24212;&#20110;&#25968;&#25454;&#19981;&#20805;&#20998;&#24341;&#36215;&#30340;&#26041;&#24046;&#12290;&#23545;&#20110;&#19968;&#20010;&#20855;&#26377;$k$&#20010;&#27880;&#24847;&#21147;&#23618;&#30340;transformer&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#23450;&#26465;&#20214;&#19979;&#23427;&#21487;&#20197;&#23454;&#29616;&#22810;&#27425;&#39044;&#22788;&#29702;&#26799;&#24230;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the striking ability of transformers for in-context learning, several works demonstrate that transformers can implement algorithms like gradient descent. By a careful construction of weights, these works show that multiple layers of transformers are expressive enough to simulate gradient descent iterations. Going beyond the question of expressivity, we ask: Can transformers learn to implement such algorithms by training over random problem instances? To our knowledge, we make the first theoretical progress toward this question via analysis of the loss landscape for linear transformers trained over random instances of linear regression. For a single attention layer, we prove the global minimum of the training objective implements a single iteration of preconditioned gradient descent. Notably, the preconditioning matrix not only adapts to the input distribution but also to the variance induced by data inadequacy. For a transformer with $k$ attention layers, we prove certain 
&lt;/p&gt;</description></item><item><title>EMOTE&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#24314;&#27169;&#26550;&#26500;&#65292;&#29992;&#20110;&#27169;&#25311;&#21478;&#19968;&#20010;&#26234;&#33021;&#20307;&#30340;&#21160;&#20316;&#20215;&#20540;&#20989;&#25968;&#65292;&#36890;&#36807;&#20849;&#24773;&#30340;&#24819;&#35937;&#32593;&#32476;&#23558;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#35266;&#23519;&#29366;&#24577;&#36716;&#25442;&#25104;&#21487;&#35299;&#37322;&#30340;&#8220;&#20849;&#24773;&#29366;&#24577;&#8221;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#26234;&#33021;&#20307;&#28216;&#25103;&#20013;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.00295</link><description>&lt;p&gt;
&#27169;&#25311;&#20849;&#24773;&#36807;&#31243;&#30340;&#21487;&#35299;&#37322;&#24314;&#27169;&#26550;&#26500;EMOTE
&lt;/p&gt;
&lt;p&gt;
EMOTE: An Explainable architecture for Modelling the Other Through Empathy. (arXiv:2306.00295v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00295
&lt;/p&gt;
&lt;p&gt;
EMOTE&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#24314;&#27169;&#26550;&#26500;&#65292;&#29992;&#20110;&#27169;&#25311;&#21478;&#19968;&#20010;&#26234;&#33021;&#20307;&#30340;&#21160;&#20316;&#20215;&#20540;&#20989;&#25968;&#65292;&#36890;&#36807;&#20849;&#24773;&#30340;&#24819;&#35937;&#32593;&#32476;&#23558;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#35266;&#23519;&#29366;&#24577;&#36716;&#25442;&#25104;&#21487;&#35299;&#37322;&#30340;&#8220;&#20849;&#24773;&#29366;&#24577;&#8221;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#26234;&#33021;&#20307;&#28216;&#25103;&#20013;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#24120;&#21487;&#20197;&#20551;&#35774;&#20182;&#20154;&#19982;&#25105;&#20204;&#26377;&#31867;&#20284;&#30340;&#30446;&#26631;&#12290;&#36825;&#31181;&#20551;&#35774;&#26377;&#26102;&#20063;&#36866;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#28216;&#25103;&#65292;&#20363;&#22914;&#65292;Agent 1&#23545;&#32511;&#33394;&#39063;&#31890;&#30340;&#21560;&#24341;&#31867;&#27604;&#20110;Agent 2&#23545;&#32418;&#33394;&#39063;&#31890;&#30340;&#21560;&#24341;&#12290;&#36825;&#31181;&#8220;&#31867;&#27604;&#8221;&#20551;&#35774;&#19982;&#20849;&#24773;&#35748;&#30693;&#36807;&#31243;&#23494;&#20999;&#30456;&#20851;&#12290;&#21463;&#21040;&#20849;&#24773;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#19988;&#21487;&#35299;&#37322;&#30340;&#24314;&#27169;&#26550;&#26500;&#65292;&#29992;&#20110;&#27169;&#25311;&#21478;&#19968;&#20010;&#26234;&#33021;&#20307;&#30340;&#21160;&#20316;&#20215;&#20540;&#20989;&#25968;&#12290;&#36825;&#28041;&#21450;&#23398;&#20064;&#19968;&#20010;&#8220;&#24819;&#35937;&#32593;&#32476;&#8221;&#65292;&#20197;&#36716;&#25442;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#35266;&#23519;&#29366;&#24577;&#65292;&#20174;&#32780;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#8220;&#20849;&#24773;&#29366;&#24577;&#8221;&#65292;&#24403;&#21576;&#29616;&#32473;&#23398;&#20064;&#26234;&#33021;&#20307;&#26102;&#65292;&#20250;&#20135;&#29983;&#27169;&#20223;&#20854;&#20182;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#30001;&#21333;&#20010;&#23398;&#20064;&#26234;&#33021;&#20307;&#21644;&#26681;&#25454;&#22266;&#23450;&#31574;&#30053;&#34892;&#21160;&#30340;&#20854;&#20182;&#65288;&#29420;&#31435;&#65289;&#26234;&#33021;&#20307;&#32452;&#25104;&#30340;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#12290;&#35813;&#26550;&#26500;&#29305;&#21035;&#36866;&#29992;&#20110;&#20351;&#29992;&#22797;&#21512;&#20540;&#25110;&#22870;&#21169;&#20989;&#25968;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#26234;&#33021;&#20307;&#28216;&#25103;&#29615;&#22659;&#20013;&#20135;&#29983;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We can usually assume others have goals analogous to our own. This assumption can also, at times, be applied to multi-agent games - e.g. Agent 1's attraction to green pellets is analogous to Agent 2's attraction to red pellets. This "analogy" assumption is tied closely to the cognitive process known as empathy. Inspired by empathy, we design a simple and explainable architecture to model another agent's action-value function. This involves learning an "Imagination Network" to transform the other agent's observed state in order to produce a human-interpretable "empathetic state" which, when presented to the learning agent, produces behaviours that mimic the other agent. Our approach is applicable to multi-agent scenarios consisting of a single learning agent and other (independent) agents acting according to fixed policies. This architecture is particularly beneficial for (but not limited to) algorithms using a composite value or reward function. We show our method produces better perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36866;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#20219;&#21153;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#30340;&#26080;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#24615;&#33021;&#39044;&#27979;&#25351;&#26631;hidden covariance&#65292;&#21487;&#26174;&#33879;&#20248;&#21270;&#29616;&#26377;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2306.00288</link><description>&lt;p&gt;
&#26080;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#22312;RNN&#21644;Transformer&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Training-free Neural Architecture Search for RNNs and Transformers. (arXiv:2306.00288v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36866;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#20219;&#21153;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#30340;&#26080;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#24615;&#33021;&#39044;&#27979;&#25351;&#26631;hidden covariance&#65292;&#21487;&#26174;&#33879;&#20248;&#21270;&#29616;&#26377;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;(NAS)&#21487;&#20197;&#33258;&#21160;&#21019;&#24314;&#26032;&#30340;&#26377;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#20026;&#25163;&#21160;&#35774;&#35745;&#22797;&#26434;&#26550;&#26500;&#25552;&#20379;&#20102;&#19968;&#31181;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;NAS&#31639;&#27861;&#36895;&#24230;&#36739;&#24930;&#65292;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#26550;&#26500;&#30340;&#26080;&#35757;&#32451;NAS&#25351;&#26631;&#65292;&#26497;&#22823;&#22320;&#21152;&#36895;&#20102;&#25628;&#32034;&#31639;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#38024;&#23545;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(RNN)&#21644;BERT-based transformer&#26550;&#26500;&#30340;&#26080;&#35757;&#32451;NAS&#25351;&#26631;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;hidden covariance&#30340;&#26032;&#30340;&#26080;&#35757;&#32451;&#25351;&#26631;&#65292;&#21487;&#20197;&#39044;&#27979;&#35757;&#32451;&#21518;RNN&#26550;&#26500;&#30340;&#24615;&#33021;&#65292;&#24182;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26080;&#35757;&#32451;&#25351;&#26631;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#30340;&#36716;&#25442;&#22120;&#25628;&#32034;&#31354;&#38388;&#33539;&#20363;
&lt;/p&gt;
&lt;p&gt;
Neural architecture search (NAS) has allowed for the automatic creation of new and effective neural network architectures, offering an alternative to the laborious process of manually designing complex architectures. However, traditional NAS algorithms are slow and require immense amounts of computing power. Recent research has investigated training-free NAS metrics for image classification architectures, drastically speeding up search algorithms. In this paper, we investigate training-free NAS metrics for recurrent neural network (RNN) and BERT-based transformer architectures, targeted towards language modeling tasks. First, we develop a new training-free metric, named hidden covariance, that predicts the trained performance of an RNN architecture and significantly outperforms existing training-free metrics. We experimentally evaluate the effectiveness of the hidden covariance metric on the NAS-Bench-NLP benchmark. Second, we find that the current search space paradigm for transformer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#31283;&#20581;&#33258;&#25105;&#35757;&#32451;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#20266;&#26631;&#31614;&#19981;&#20934;&#30830;&#21644;&#23436;&#20840;&#20934;&#30830;&#26102;&#20998;&#21035;&#37319;&#21462;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;ImageNet&#21644;nuScenes&#25968;&#25454;&#38598;&#19978;&#22343;&#27604;&#26631;&#20934;&#33258;&#25105;&#35757;&#32451;&#24635;&#32467;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.00265</link><description>&lt;p&gt;
&#21452;&#37325;&#31283;&#20581;&#33258;&#25105;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Doubly Robust Self-Training. (arXiv:2306.00265v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#31283;&#20581;&#33258;&#25105;&#35757;&#32451;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#20266;&#26631;&#31614;&#19981;&#20934;&#30830;&#21644;&#23436;&#20840;&#20934;&#30830;&#26102;&#20998;&#21035;&#37319;&#21462;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;ImageNet&#21644;nuScenes&#25968;&#25454;&#38598;&#19978;&#22343;&#27604;&#26631;&#20934;&#33258;&#25105;&#35757;&#32451;&#24635;&#32467;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#35757;&#32451;&#26159;&#35299;&#20915;&#21322;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#30340;&#19968;&#31181;&#37325;&#35201;&#25216;&#26415;&#12290;&#23427;&#36890;&#36807;&#29983;&#25104;&#20266;&#26631;&#31614;&#24182;&#23558;&#20854;&#19982;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#32467;&#21512;&#20351;&#29992;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;&#33258;&#25105;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#36825;&#20123;&#20266;&#26631;&#31614;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#21452;&#37325;&#31283;&#20581;&#33258;&#25105;&#35757;&#32451;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#21322;&#30417;&#30563;&#31639;&#27861;&#65292;&#21487;&#20197;&#20445;&#35777;&#22312;&#20004;&#20010;&#26497;&#31471;&#20043;&#38388;&#24179;&#34913;&#12290;&#24403;&#20266;&#26631;&#31614;&#23436;&#20840;&#19981;&#27491;&#30830;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#34987;&#20943;&#23569;&#21040;&#20165;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#30456;&#21453;&#65292;&#24403;&#20266;&#26631;&#31614;&#23436;&#20840;&#20934;&#30830;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#21464;&#25104;&#21033;&#29992;&#25152;&#26377;&#20266;&#26631;&#31614;&#25968;&#25454;&#21644;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#36807;&#31243;&#65292;&#20174;&#32780;&#22686;&#21152;&#26377;&#25928;&#30340;&#26679;&#26412;&#37327;&#12290;&#36890;&#36807;&#22312;ImageNet&#22270;&#20687;&#20998;&#31867;&#21644;nuScenes&#33258;&#20027;&#39550;&#39542;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21452;&#37325;&#31283;&#20581;&#25439;&#22833;&#20248;&#20110;&#26631;&#20934;&#33258;&#25105;&#35757;&#32451;&#22522;&#32447;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-training is an important technique for solving semi-supervised learning problems. It leverages unlabeled data by generating pseudo-labels and combining them with a limited labeled dataset for training. The effectiveness of self-training heavily relies on the accuracy of these pseudo-labels. In this paper, we introduce doubly robust self-training, a novel semi-supervised algorithm that provably balances between two extremes. When the pseudo-labels are entirely incorrect, our method reduces to a training process solely using labeled data. Conversely, when the pseudo-labels are completely accurate, our method transforms into a training process utilizing all pseudo-labeled data and labeled data, thus increasing the effective sample size. Through empirical evaluations on both the ImageNet dataset for image classification and the nuScenes autonomous driving dataset for 3D object detection, we demonstrate the superiority of the doubly robust loss over the standard self-training baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;BetaZero&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#20351;&#29992;&#23398;&#20064;&#36817;&#20284;&#31639;&#27861;&#30340;&#32622;&#20449;&#29366;&#24577;&#35268;&#21010;&#31639;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#38271;&#26102;&#38388;&#36328;&#24230;&#30340;POMDP&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.00249</link><description>&lt;p&gt;
BetaZero&#65306;&#22522;&#20110;&#23398;&#20064;&#30340;&#36817;&#20284;&#31639;&#27861;&#30340;&#32622;&#20449;&#29366;&#24577;&#35268;&#21010;&#29992;&#20110;&#38271;&#26102;&#38388;&#36328;&#24230;&#30340;POMDPs
&lt;/p&gt;
&lt;p&gt;
BetaZero: Belief-State Planning for Long-Horizon POMDPs using Learned Approximations. (arXiv:2306.00249v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;BetaZero&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#20351;&#29992;&#23398;&#20064;&#36817;&#20284;&#31639;&#27861;&#30340;&#32622;&#20449;&#29366;&#24577;&#35268;&#21010;&#31639;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#38271;&#26102;&#38388;&#36328;&#24230;&#30340;POMDP&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#30340;&#35268;&#21010;&#38382;&#39064;&#65292;&#21253;&#25324;&#33258;&#21160;&#39550;&#39542;&#12289;&#30899;&#20648;&#23384;&#21644;&#36164;&#28304;&#21208;&#25506;&#31561;&#21487;&#25345;&#32493;&#33021;&#28304;&#24212;&#29992;&#65292;&#26368;&#36817;&#34987;&#24314;&#27169;&#20026;&#37096;&#20998;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDPs&#65289;&#24182;&#20351;&#29992;&#36817;&#20284;&#26041;&#27861;&#35299;&#20915;&#12290;&#20026;&#20102;&#22312;&#23454;&#36341;&#20013;&#35299;&#20915;&#39640;&#32500;&#24230;POMDPs&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20351;&#29992;&#20102;&#38382;&#39064;&#29305;&#23450;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#36827;&#34892;&#22312;&#32447;&#35268;&#21010;&#65292;&#20197;&#20943;&#23569;&#35268;&#21010;&#26102;&#38388;&#36328;&#24230;&#24182;&#20351;&#38382;&#39064;&#26131;&#20110;&#35299;&#20915;&#12290;&#26368;&#36817;&#25104;&#21151;&#22320;&#22312;&#23436;&#20840;&#21487;&#35266;&#23519;&#30340;&#39046;&#22495;&#20013;&#25214;&#21040;&#20102;&#29992;&#20110;&#26367;&#25442;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#23398;&#20064;&#36817;&#20284;&#31639;&#27861;&#12290;&#20851;&#38190;&#27934;&#35265;&#26159;&#23558;&#22312;&#32447;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#19982;&#31163;&#32447;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#30456;&#32467;&#21512;&#65292;&#20197;&#20248;&#21270;&#31574;&#30053;&#21644;&#20540;&#20989;&#25968;&#12290;&#26412;&#25991;&#23558;&#36825;&#19968;&#27934;&#35265;&#24212;&#29992;&#21040;&#20102;&#37096;&#20998;&#35266;&#23519;&#22495;&#65292;&#24182;&#25552;&#20986;&#20102;BetaZero&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;POMDP&#30340;&#32622;&#20449;&#29366;&#24577;&#35268;&#21010;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world planning problems$\unicode{x2014}$including autonomous driving and sustainable energy applications like carbon storage and resource exploration$\unicode{x2014}$have recently been modeled as partially observable Markov decision processes (POMDPs) and solved using approximate methods. To solve high-dimensional POMDPs in practice, state-of-the-art methods use online planning with problem-specific heuristics to reduce planning horizons and make the problems tractable. Algorithms that learn approximations to replace heuristics have recently found success in large-scale problems in the fully observable domain. The key insight is the combination of online Monte Carlo tree search with offline neural network approximations of the optimal policy and value function. In this work, we bring this insight to partially observed domains and propose BetaZero, a belief-state planning algorithm for POMDPs. BetaZero learns offline approximations based on accurate belief models to enable online d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Pinterest&#25512;&#33616;&#31995;&#32479;&#30340;&#26550;&#26500;&#21644;TransAct&#27169;&#22411;&#12290;TransAct&#26159;&#19968;&#20010;&#20174;&#29992;&#25143;&#23454;&#26102;&#27963;&#21160;&#20013;&#25552;&#21462;&#30701;&#26399;&#20559;&#22909;&#30340;&#24207;&#21015;&#27169;&#22411;&#12290;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#36890;&#36807;&#28151;&#21512;&#25490;&#21517;&#26041;&#27861;&#32467;&#21512;&#30452;&#25509;&#22312;&#23454;&#26102;&#29992;&#25143;&#27963;&#21160;&#19978;&#23398;&#20064;&#21644;&#22312;&#36739;&#38271;&#26102;&#38388;&#27573;&#20869;&#23398;&#20064;&#25209;&#37327;&#29992;&#25143;&#34920;&#31034;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.00248</link><description>&lt;p&gt;
TransAct: Pinterest&#23454;&#26102;&#29992;&#25143;&#34892;&#20026;&#27169;&#22411;&#20013;&#30340;Transformer-Based&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
TransAct: Transformer-based Realtime User Action Model for Recommendation at Pinterest. (arXiv:2306.00248v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Pinterest&#25512;&#33616;&#31995;&#32479;&#30340;&#26550;&#26500;&#21644;TransAct&#27169;&#22411;&#12290;TransAct&#26159;&#19968;&#20010;&#20174;&#29992;&#25143;&#23454;&#26102;&#27963;&#21160;&#20013;&#25552;&#21462;&#30701;&#26399;&#20559;&#22909;&#30340;&#24207;&#21015;&#27169;&#22411;&#12290;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#36890;&#36807;&#28151;&#21512;&#25490;&#21517;&#26041;&#27861;&#32467;&#21512;&#30452;&#25509;&#22312;&#23454;&#26102;&#29992;&#25143;&#27963;&#21160;&#19978;&#23398;&#20064;&#21644;&#22312;&#36739;&#38271;&#26102;&#38388;&#27573;&#20869;&#23398;&#20064;&#25209;&#37327;&#29992;&#25143;&#34920;&#31034;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32534;&#30721;&#29992;&#25143;&#27963;&#21160;&#20197;&#36827;&#34892;&#19979;&#19968;&#27493;&#34892;&#21160;&#39044;&#27979;&#30340;&#24207;&#21015;&#27169;&#22411;&#24050;&#25104;&#20026;&#26500;&#24314;&#22823;&#35268;&#27169;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#30340;&#24120;&#35265;&#35774;&#35745;&#36873;&#25321;&#12290; &#20256;&#32479;&#30340;&#24207;&#21015;&#25512;&#33616;&#26041;&#27861;&#35201;&#20040;&#21033;&#29992;&#23454;&#26102;&#29992;&#25143;&#25805;&#20316;&#36827;&#34892;&#31471;&#21040;&#31471;&#23398;&#20064;&#65292;&#35201;&#20040;&#20197;&#33073;&#26426;&#25209;&#37327;&#29983;&#25104;&#30340;&#26041;&#24335;&#21333;&#29420;&#23398;&#20064;&#29992;&#25143;&#34920;&#31034;&#12290; &#26412;&#25991;(1)&#20171;&#32461;&#20102;Pinterest Homefeed&#30340;&#25490;&#21517;&#26550;&#26500;&#65292;&#21363;&#25105;&#20204;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#20135;&#21697;&#21644;&#26368;&#22823;&#30340;&#21442;&#19982;&#38754;&#65307;(2)&#25552;&#20986;&#20102;TransAct&#65292;&#19968;&#31181;&#20174;&#29992;&#25143;&#23454;&#26102;&#27963;&#21160;&#20013;&#25552;&#21462;&#30701;&#26399;&#20559;&#22909;&#30340;&#24207;&#21015;&#27169;&#22411;&#65307;(3)&#25551;&#36848;&#20102;&#25105;&#20204;&#30340;&#28151;&#21512;&#25490;&#21517;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;TransAct&#30340;&#31471;&#21040;&#31471;&#24207;&#21015;&#24314;&#27169;&#19982;&#25209;&#37327;&#29983;&#25104;&#30340;&#29992;&#25143;&#23884;&#20837;&#28151;&#21512;&#12290; &#28151;&#21512;&#26041;&#27861;&#20801;&#35768;&#25105;&#20204;&#32467;&#21512;&#30452;&#25509;&#22312;&#23454;&#26102;&#29992;&#25143;&#27963;&#21160;&#19978;&#23398;&#20064;&#20197;&#33719;&#24471;&#21709;&#24212;&#24615;&#30340;&#20248;&#28857;&#21644;&#22312;&#36739;&#38271;&#26102;&#38388;&#27573;&#20869;&#23398;&#20064;&#30340;&#25209;&#37327;&#29992;&#25143;&#34920;&#31034;&#30340;&#25104;&#26412;&#25928;&#30410;&#12290; &#25105;&#20204;&#25551;&#36848;&#20102;&#23454;&#39564;&#32467;&#26524;......&#65288;&#21407;&#25991;&#20869;&#23481;&#30465;&#30053;&#65289;
&lt;/p&gt;
&lt;p&gt;
Sequential models that encode user activity for next action prediction have become a popular design choice for building web-scale personalized recommendation systems. Traditional methods of sequential recommendation either utilize end-to-end learning on realtime user actions, or learn user representations separately in an offline batch-generated manner. This paper (1) presents Pinterest's ranking architecture for Homefeed, our personalized recommendation product and the largest engagement surface; (2) proposes TransAct, a sequential model that extracts users' short-term preferences from their realtime activities; (3) describes our hybrid approach to ranking, which combines end-to-end sequential modeling via TransAct with batch-generated user embeddings. The hybrid approach allows us to combine the advantages of responsiveness from learning directly on realtime user activity with the cost-effectiveness of batch user representations learned over a longer time period. We describe the resu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#35270;&#35273;&#35009;&#21098;&#26469;&#25552;&#39640;BLIP&#27169;&#22411;&#22312;&#32454;&#33410;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.00228</link><description>&lt;p&gt;
&#20351;&#29992;&#35270;&#35273;&#35009;&#21098;&#26469;&#22686;&#24378;BLIP&#27169;&#22411;&#32454;&#33410;&#38382;&#39064;&#22238;&#31572;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Using Visual Cropping to Enhance Fine-Detail Question Answering of BLIP-Family Models. (arXiv:2306.00228v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#35270;&#35273;&#35009;&#21098;&#26469;&#25552;&#39640;BLIP&#27169;&#22411;&#22312;&#32454;&#33410;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38382;&#31572;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#24863;&#30693;&#12289;&#35821;&#35328;&#21644;&#32972;&#26223;&#30693;&#35782;&#31995;&#32479;&#20043;&#38388;&#30340;&#26080;&#32541;&#20132;&#20114;&#12290;&#34429;&#28982;&#26368;&#36817;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#22914;BLIP&#24050;&#32463;&#25552;&#39640;&#20102;&#36825;&#39033;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#32570;&#20047;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#38382;&#39064;&#21644;&#25512;&#29702;&#31867;&#22411;&#19978;&#34920;&#29616;&#30340;&#33021;&#21147;&#12290;&#37492;&#20110;&#25105;&#20204;&#23545;BLIP&#23478;&#26063;&#27169;&#22411;&#36827;&#34892;&#30340;&#21021;&#27493;&#20998;&#26512;&#26174;&#31034;&#20986;&#22238;&#31572;&#32454;&#33410;&#38382;&#39064;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#25105;&#20204;&#30740;&#31350;&#20197;&#19979;&#38382;&#39064;&#65306;&#33021;&#21542;&#20351;&#29992;&#35270;&#35273;&#35009;&#21098;&#26469;&#25552;&#39640;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#22312;&#32454;&#33410;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#65311;&#32771;&#34385;&#21040;BLIP&#23478;&#26063;&#27169;&#22411;&#30340;&#26368;&#36817;&#25104;&#21151;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#38646;&#26679;&#26412;&#21644;&#19968;&#20010;&#24494;&#35843;&#30340;BLIP&#27169;&#22411;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19977;&#20010;&#21463;&#25511;&#30340;&#23376;&#38598;&#65292;&#20197;&#34913;&#37327;&#35009;&#21098;&#26159;&#21542;&#26377;&#21161;&#20110;&#27169;&#22411;&#24615;&#33021;&#12290;&#38500;&#20102;&#20154;&#24037;&#35009;&#21098;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#20004;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#23884;&#20837;&#30340;&#33258;&#21160;&#35009;&#21098;&#31574;&#30053;&#65292;&#36825;&#20123;&#31574;&#30053;&#20197;CLIP&#20026;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Question Answering is a challenging task, as it requires seamless interaction between perceptual, linguistic, and background knowledge systems. While the recent progress of visual and natural language models like BLIP has led to improved performance on this task, we lack understanding of the ability of such models to perform on different kinds of questions and reasoning types. As our initial analysis of BLIP-family models revealed difficulty with answering fine-detail questions, we investigate the following question: Can visual cropping be employed to improve the performance of state-of-the-art visual question answering models on fine-detail questions? Given the recent success of the BLIP-family models, we study a zero-shot and a fine-tuned BLIP model. We define three controlled subsets of the popular VQA-v2 benchmark to measure whether cropping can help model performance. Besides human cropping, we devise two automatic cropping strategies based on multi-modal embedding by CLIP 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25193;&#25955;&#30011;&#31508;&#30340;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;AI&#22270;&#20687;&#24494;&#35843;&#24037;&#20855;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26681;&#25454;&#30446;&#26631;&#21306;&#22495;&#20462;&#25913;AI&#21512;&#25104;&#22270;&#20687;&#24182;&#20445;&#30041;&#21407;&#22987;&#19978;&#19979;&#25991;&#12290;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#20462;&#22797;&#25216;&#26415;&#36827;&#34892;&#27604;&#36739;&#65292;&#35813;&#26041;&#27861;&#22312;&#29992;&#25143;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#21487;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.00219</link><description>&lt;p&gt;
&#25193;&#25955;&#30011;&#31508;&#65306;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;AI&#29983;&#25104;&#22270;&#20687;&#32534;&#36753;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Diffusion Brush: A Latent Diffusion Model-based Editing Tool for AI-generated Images. (arXiv:2306.00219v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25193;&#25955;&#30011;&#31508;&#30340;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;AI&#22270;&#20687;&#24494;&#35843;&#24037;&#20855;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26681;&#25454;&#30446;&#26631;&#21306;&#22495;&#20462;&#25913;AI&#21512;&#25104;&#22270;&#20687;&#24182;&#20445;&#30041;&#21407;&#22987;&#19978;&#19979;&#25991;&#12290;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#20462;&#22797;&#25216;&#26415;&#36827;&#34892;&#27604;&#36739;&#65292;&#35813;&#26041;&#27861;&#22312;&#29992;&#25143;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#21487;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#22411;&#38480;&#21046;&#65292;&#29983;&#25104;&#30340;&#22270;&#20687;&#32463;&#24120;&#21253;&#21547;&#19981;&#33391;&#30340;&#20266;&#24433;&#25110;&#20854;&#20182;&#38169;&#35823;&#12290;&#29616;&#26377;&#30340;&#24494;&#35843;&#29983;&#25104;&#22270;&#20687;&#30340;&#25216;&#26415;&#35201;&#20040;&#32791;&#26102;&#65288;&#25163;&#21160;&#32534;&#36753;&#65289;&#65292;&#35201;&#20040;&#20135;&#29983;&#19981;&#22815;&#23436;&#32654;&#30340;&#32467;&#26524;&#65288;&#20462;&#34917;&#65289;&#65292;&#35201;&#20040;&#20250;&#23548;&#33268;&#25972;&#20307;&#22270;&#20687;&#20135;&#29983;&#24847;&#24819;&#19981;&#21040;&#30340;&#21464;&#21270;&#65288;&#21464;&#20307;&#36873;&#25321;&#21644;&#25552;&#31034;&#24494;&#35843;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25193;&#25955;&#30011;&#31508;&#30340;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;AI&#22270;&#20687;&#24494;&#35843;&#24037;&#20855;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24494;&#35843;AI&#21512;&#25104;&#22270;&#20687;&#20013;&#25152;&#38656;&#30340;&#21306;&#22495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#20013;&#22312;&#30446;&#26631;&#21306;&#22495;&#24341;&#20837;&#20102;&#26032;&#30340;&#38543;&#26426;&#22122;&#22768;&#27169;&#24335;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#20445;&#30041;&#20854;&#20182;&#21306;&#22495;&#21407;&#22987;&#19978;&#19979;&#25991;&#30340;&#21516;&#26102;&#65292;&#39640;&#25928;&#22320;&#23545;&#25351;&#23450;&#21306;&#22495;&#36827;&#34892;&#26356;&#25913;&#12290;&#25105;&#20204;&#36890;&#36807;&#33402;&#26415;&#23478;&#36827;&#34892;&#30340;&#29992;&#25143;&#30740;&#31350;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#23558;&#25105;&#20204;&#30340;&#25216;&#26415;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#20462;&#22797;&#25216;&#26415;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image generative models have made remarkable advancements in generating high-quality images. However, generated images often contain undesirable artifacts or other errors due to model limitations. Existing techniques to fine-tune generated images are time-consuming (manual editing), produce poorly-integrated results (inpainting), or result in unexpected changes across the entire image (variation selection and prompt fine-tuning). In this work, we present Diffusion Brush, a Latent Diffusion Model-based (LDM) tool to efficiently fine-tune desired regions within an AI-synthesized image. Our method introduces new random noise patterns at targeted regions during the reverse diffusion process, enabling the model to efficiently make changes to the specified regions while preserving the original context for the rest of the image. We evaluate our method's usability and effectiveness through a user study with artists, comparing our technique against other state-of-the-art image inpaintin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#65292;&#22810;&#20010;&#26234;&#33021;&#20307;&#36890;&#36807;&#26368;&#22823;&#21270;&#24635;&#20307;&#22870;&#21169;&#24182;&#22312;&#23545;&#24635;&#20307;&#25928;&#29992;&#30340;&#26399;&#26395;&#20540;&#19978;&#35774;&#32622;&#32422;&#26463;&#26465;&#20214;&#36827;&#34892;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#27867;&#21270;Lagrangian&#31574;&#30053;&#20248;&#21270;&#65292;&#37319;&#29992;&#22522;&#20110;&#21344;&#29992;&#27979;&#37327;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#32622;&#20449;&#19978;&#38480;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#23454;&#29616;&#23545;&#32422;&#26463;&#30340;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2306.00212</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23433;&#20840;&#24615;&#20248;&#21270;&#25216;&#26415;&#8212;&#8212;&#27867;&#21270;Lagrangian&#31574;&#30053;&#20248;&#21270;&#30340;&#21487;&#35777;&#26126;&#39640;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Provably Efficient Generalized Lagrangian Policy Optimization for Safe Multi-Agent Reinforcement Learning. (arXiv:2306.00212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#65292;&#22810;&#20010;&#26234;&#33021;&#20307;&#36890;&#36807;&#26368;&#22823;&#21270;&#24635;&#20307;&#22870;&#21169;&#24182;&#22312;&#23545;&#24635;&#20307;&#25928;&#29992;&#30340;&#26399;&#26395;&#20540;&#19978;&#35774;&#32622;&#32422;&#26463;&#26465;&#20214;&#36827;&#34892;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#27867;&#21270;Lagrangian&#31574;&#30053;&#20248;&#21270;&#65292;&#37319;&#29992;&#22522;&#20110;&#21344;&#29992;&#27979;&#37327;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#32622;&#20449;&#19978;&#38480;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#23454;&#29616;&#23545;&#32422;&#26463;&#30340;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#26377;&#32422;&#26463;&#30340;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#20013;&#65292;&#20114;&#30456;&#31454;&#20105;&#30340;&#22810;&#20010;&#26234;&#33021;&#20307;&#36890;&#36807;&#26368;&#22823;&#21270;&#24635;&#20307;&#22870;&#21169;&#24182;&#22312;&#23545;&#24635;&#20307;&#25928;&#29992;&#30340;&#26399;&#26395;&#20540;&#19978;&#35774;&#32622;&#32422;&#26463;&#26465;&#20214;&#36827;&#34892;&#22312;&#32447;&#23433;&#20840;&#24615;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#19968;&#20010;&#25317;&#26377;&#20004;&#20010;&#29420;&#31435;&#36716;&#31227;&#20989;&#25968;&#65292;&#23545;&#26234;&#33021;&#20307;&#26410;&#30693;&#30340;&#19988;&#23384;&#22312;&#23545;&#25239;&#24615;&#22870;&#21169;&#20989;&#25968;&#21644;&#38543;&#26426;&#25928;&#29992;&#20989;&#25968;&#30340;&#21452;&#20154;&#38646;&#21644;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#12290;&#20026;&#20102;&#35299;&#20915;&#35813;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#21344;&#29992;&#27979;&#37327;&#30340;&#26041;&#27861;&#23558;&#20854;&#25551;&#36848;&#20026;&#24102;&#26377;&#26174;&#24335;&#32422;&#26463;&#30340;&#22312;&#32447;&#32422;&#26463;&#38797;&#28857;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#22312;&#32422;&#26463;&#20248;&#21270;&#20013;&#25512;&#24191;Lagrange&#20056;&#25968;&#26041;&#27861;&#24182;&#21019;&#24314;&#19968;&#20010;&#24102;&#26377;Minimax&#20915;&#31574;&#21407;&#22987;&#21464;&#37327;&#21644;&#21452;&#21464;&#37327;&#30340;&#24191;&#20041;Lagrangian&#22788;&#29702;&#32422;&#26463;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#32622;&#20449;&#19978;&#38480;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23454;&#29616;&#25506;&#32034;&#21644;&#21033;&#29992;&#30340;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#36807;&#22312;&#32447;&#38236;&#20687;&#26356;&#26032;Minimax&#20915;&#31574;&#21407;&#22987;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine online safe multi-agent reinforcement learning using constrained Markov games in which agents compete by maximizing their expected total rewards under a constraint on expected total utilities. Our focus is confined to an episodic two-player zero-sum constrained Markov game with independent transition functions that are unknown to agents, adversarial reward functions, and stochastic utility functions. For such a Markov game, we employ an approach based on the occupancy measure to formulate it as an online constrained saddle-point problem with an explicit constraint. We extend the Lagrange multiplier method in constrained optimization to handle the constraint by creating a generalized Lagrangian with minimax decision primal variables and a dual variable. Next, we develop an upper confidence reinforcement learning algorithm to solve this Lagrangian problem while balancing exploration and exploitation. Our algorithm updates the minimax decision primal variables via online mirror
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Adam&#22312;Transformer&#30340;&#35757;&#32451;&#20013;&#20026;&#20160;&#20040;&#27604;SGD&#26356;&#24555;&#25910;&#25947;&#65292;&#25552;&#20986;&#20102;&#26041;&#21521;&#38160;&#24230;&#30340;&#27010;&#24565;&#24182;&#36890;&#36807;&#27604;&#36739;&#35777;&#26126;&#20102;SGD&#30456;&#23545;&#20110;&#33258;&#36866;&#24212;&#31639;&#27861;&#20855;&#26377;&#26356;&#24046;&#30340;&#26041;&#21521;&#38160;&#24230;&#65292;&#36827;&#32780;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#21482;&#38656;&#35201;&#35009;&#21098;&#19968;&#23567;&#37096;&#20998;&#22352;&#26631;&#21363;&#21487;&#25552;&#39640;SGD&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.00204</link><description>&lt;p&gt;
&#25506;&#31350;Adam&#22312;Transformers&#19978;&#27604;SGD&#26356;&#24555;&#25910;&#25947;&#30340;&#21407;&#22240;
&lt;/p&gt;
&lt;p&gt;
Toward Understanding Why Adam Converges Faster Than SGD for Transformers. (arXiv:2306.00204v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Adam&#22312;Transformer&#30340;&#35757;&#32451;&#20013;&#20026;&#20160;&#20040;&#27604;SGD&#26356;&#24555;&#25910;&#25947;&#65292;&#25552;&#20986;&#20102;&#26041;&#21521;&#38160;&#24230;&#30340;&#27010;&#24565;&#24182;&#36890;&#36807;&#27604;&#36739;&#35777;&#26126;&#20102;SGD&#30456;&#23545;&#20110;&#33258;&#36866;&#24212;&#31639;&#27861;&#20855;&#26377;&#26356;&#24046;&#30340;&#26041;&#21521;&#38160;&#24230;&#65292;&#36827;&#32780;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#21482;&#38656;&#35201;&#35009;&#21098;&#19968;&#23567;&#37096;&#20998;&#22352;&#26631;&#21363;&#21487;&#25552;&#39640;SGD&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20173;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#21463;&#27426;&#36814;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#20294;&#26159;&#20687;Adam&#36825;&#26679;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#22312;&#19968;&#20123;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#24050;&#32463;&#35777;&#26126;&#20102;&#20248;&#20110;SGD&#30340;&#32463;&#39564;&#20248;&#21183;&#65292;&#29305;&#21035;&#26159;&#22312;Transformer&#30340;&#35757;&#32451;&#19978;&#12290;&#28982;&#32780;&#65292;&#20026;&#20160;&#20040;Adam&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#20250;&#26174;&#33879;&#27604;SGD&#26356;&#24555;&#22320;&#25910;&#25947;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#24565;&#8212;&#8212;&#26041;&#21521;&#38160;&#24230;&#65292;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;Adam&#27604;SGD&#26356;&#24555;&#22320;&#25910;&#25947;&#12290;&#25105;&#20204;&#35748;&#20026;&#20248;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#19982;&#26356;&#26032;&#27493;&#39588;&#30340;&#26041;&#21521;&#38160;&#24230;&#23494;&#20999;&#30456;&#20851;&#65292;&#35777;&#26126;&#20102;SGD&#30456;&#23545;&#20110;&#33258;&#36866;&#24212;&#31639;&#27861;&#20855;&#26377;&#26356;&#24046;&#30340;&#26041;&#21521;&#38160;&#24230;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35266;&#23519;&#21040;&#21482;&#26377;&#19968;&#23567;&#37096;&#20998;&#22352;&#26631;&#23548;&#33268;&#20102;SGD&#30340;&#38160;&#24230;&#19981;&#36275;&#21644;&#25910;&#25947;&#36895;&#24230;&#32531;&#24930;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#22352;&#26631;&#32423;&#21035;&#30340;&#35009;&#21098;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22352;&#26631;&#32423;&#21035;&#35009;&#21098;&#23545;&#20110;&#38160;&#24230;&#38477;&#20302;&#21644;&#21152;&#36895;&#25910;&#25947;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
While stochastic gradient descent (SGD) is still the most popular optimization algorithm in deep learning, adaptive algorithms such as Adam have established empirical advantages over SGD in some deep learning applications such as training transformers. However, it remains a question that why Adam converges significantly faster than SGD in these scenarios. In this paper, we propose one explanation of why Adam converges faster than SGD using a new concept directional sharpness. We argue that the performance of optimization algorithms is closely related to the directional sharpness of the update steps, and show SGD has much worse directional sharpness compared to adaptive algorithms. We further observe that only a small fraction of the coordinates causes the bad sharpness and slow convergence of SGD, and propose to use coordinate-wise clipping as a solution to SGD and other optimization algorithms. We demonstrate the effect of coordinate-wise clipping on sharpness reduction and speeding u
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SSL-CPCD&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38024;&#23545;&#34917;&#19969;&#32423;&#21035;&#30340;&#23454;&#20363;&#32452;&#21306;&#20998;&#21644;&#36328;&#31867;&#21035;&#21464;&#21270;&#30340;&#24809;&#32602;&#65292;&#22312;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#25552;&#39640;&#20869;&#31397;&#38236;&#22270;&#20687;&#20998;&#26512;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.00197</link><description>&lt;p&gt;
SSL-CPCD&#65306;&#33258;&#25105;&#30417;&#30563;&#30340;&#32452;&#21512;&#39044;&#25991;&#26412;&#31867;&#21035;&#21306;&#20998;&#23398;&#20064;&#65292;&#25552;&#39640;&#20869;&#31397;&#38236;&#22270;&#20687;&#20998;&#26512;&#30340;&#27867;&#21270;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
SSL-CPCD: Self-supervised learning with composite pretext-class discrimination for improved generalisability in endoscopic image analysis. (arXiv:2306.00197v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SSL-CPCD&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38024;&#23545;&#34917;&#19969;&#32423;&#21035;&#30340;&#23454;&#20363;&#32452;&#21306;&#20998;&#21644;&#36328;&#31867;&#21035;&#21464;&#21270;&#30340;&#24809;&#32602;&#65292;&#22312;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#25552;&#39640;&#20869;&#31397;&#38236;&#22270;&#20687;&#20998;&#26512;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#27493;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30417;&#30563;&#26041;&#27861;&#24191;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#38754;&#20020;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#36825;&#24433;&#21709;&#20102;&#20020;&#24202;&#36716;&#21270;&#12290;&#20869;&#31397;&#38236;&#25104;&#20687;&#25968;&#25454;&#20855;&#26377;&#22823;&#30340;&#24739;&#32773;&#20869;&#22806;&#21464;&#24322;&#24615;&#65292;&#36825;&#20351;&#24471;&#36825;&#20123;&#27169;&#22411;&#26356;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#26080;&#27861;&#23398;&#20064;&#20195;&#34920;&#19979;&#28216;&#20219;&#21153;&#30340;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#23613;&#31649;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#21644;&#21307;&#38498;&#20869;&#37096;&#21487;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#22823;&#22810;&#25968;&#30417;&#30563;&#27169;&#22411;&#20173;&#34920;&#29616;&#19981;&#20339;&#12290;&#32780;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#22312;&#33258;&#28982;&#22330;&#26223;&#25968;&#25454;&#20013;&#23545;&#35813;&#38382;&#39064;&#20570;&#20102;&#19968;&#23450;&#31243;&#24230;&#30340;&#35299;&#20915;&#65292;&#20294;&#22312;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25506;&#32034;&#22522;&#20110;&#34917;&#19969;&#30340;&#23454;&#20363;&#32452;&#21306;&#20998;&#21644;&#20351;&#29992;&#20313;&#24358;&#30456;&#20284;&#24230;&#24230;&#37327;&#20013;&#30340;&#21152;&#24615;&#35282;&#24230;&#36793;&#36317;&#23545;&#36328;&#31867;&#21035;&#21464;&#21270;&#30340;&#24809;&#32602;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#20351;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#23398;&#20064;&#20195;&#34920;&#19979;&#28216;&#20219;&#21153;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20869;&#31397;&#38236;&#22270;&#20687;&#20998;&#26512;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven methods have shown tremendous progress in medical image analysis. In this context, deep learning-based supervised methods are widely popular. However, they require a large amount of training data and face issues in generalisability to unseen datasets that hinder clinical translation. Endoscopic imaging data incorporates large inter- and intra-patient variability that makes these models more challenging to learn representative features for downstream tasks. Thus, despite the publicly available datasets and datasets that can be generated within hospitals, most supervised models still underperform. While self-supervised learning has addressed this problem to some extent in natural scene data, there is a considerable performance gap in the medical image domain. In this paper, we propose to explore patch-level instance-group discrimination and penalisation of inter-class variation using additive angular margin within the cosine similarity metrics. Our novel approach enables mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#23384;&#22312;&#19968;&#23450;&#31243;&#24230;&#30340;&#25193;&#25955;&#20887;&#20313;&#65292;&#21363;&#38543;&#26426;&#36873;&#25321;&#19968;&#37096;&#20998;&#31070;&#32463;&#20803;&#19982;&#20840;&#37096;&#31070;&#32463;&#20803;&#30456;&#20284;&#24230;&#39640;&#19988;&#24615;&#33021;&#30456;&#20284;&#65292;&#21487;&#29992;&#20110;&#38477;&#20302;&#23454;&#38469;&#37096;&#32626;&#20013;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2306.00183</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#30340;&#25193;&#25955;&#20887;&#20313;
&lt;/p&gt;
&lt;p&gt;
Diffused Redundancy in Pre-trained Representations. (arXiv:2306.00183v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#23384;&#22312;&#19968;&#23450;&#31243;&#24230;&#30340;&#25193;&#25955;&#20887;&#20313;&#65292;&#21363;&#38543;&#26426;&#36873;&#25321;&#19968;&#37096;&#20998;&#31070;&#32463;&#20803;&#19982;&#20840;&#37096;&#31070;&#32463;&#20803;&#30456;&#20284;&#24230;&#39640;&#19988;&#24615;&#33021;&#30456;&#20284;&#65292;&#21487;&#29992;&#20110;&#38477;&#20302;&#23454;&#38469;&#37096;&#32626;&#20013;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#33719;&#24471;&#30340;&#34920;&#31034;&#24050;&#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26356;&#21152;&#28145;&#20837;&#22320;&#30740;&#31350;&#20102;&#36825;&#31181;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#30340;&#29305;&#24449;&#26159;&#22914;&#20309;&#34987;&#32534;&#30721;&#30340;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#32473;&#23450;&#23618;&#20013;&#23398;&#21040;&#30340;&#34920;&#31034;&#23637;&#29616;&#20986;&#19968;&#23450;&#31243;&#24230;&#30340;&#25193;&#25955;&#20887;&#20313;&#65292;&#21363;&#23545;&#20110;&#36229;&#36807;&#19968;&#20010;&#38408;&#20540;&#22823;&#23567;&#30340;&#20219;&#20309;&#38543;&#26426;&#23376;&#38598;&#31070;&#32463;&#20803;&#65292;&#37117;&#19982;&#23436;&#25972;&#23618;&#20855;&#26377;&#24456;&#39640;&#30340;&#30456;&#20284;&#24230;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#33021;&#22815;&#34920;&#29616;&#20986;&#19982;&#25972;&#20010;&#23618;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#19981;&#21516;&#30340;&#31070;&#32463;&#26550;&#26500;&#65288;&#21253;&#25324;CNN&#21644;Transformer&#65289;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;ImageNet1k&#21644;ImageNet21k&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#35780;&#20272;&#20102;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#22914;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#20197;&#21033;&#29992;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#30340;&#20887;&#20313;&#26469;&#38477;&#20302;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#65292;&#21516;&#26102;&#20173;&#28982;&#20445;&#25345;&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representations learned by pre-training a neural network on a large dataset are increasingly used successfully to perform a variety of downstream tasks. In this work, we take a closer look at how features are encoded in such pre-trained representations. We find that learned representations in a given layer exhibit a degree of diffuse redundancy, i.e., any randomly chosen subset of neurons in the layer that is larger than a threshold size shares a large degree of similarity with the full layer and is able to perform similarly as the whole layer on a variety of downstream tasks. For example, a linear probe trained on $20\%$ of randomly picked neurons from a ResNet50 pre-trained on ImageNet1k achieves an accuracy within $5\%$ of a linear probe trained on the full layer of neurons for downstream CIFAR10 classification. We conduct experiments on different neural architectures (including CNNs and Transformers) pre-trained on both ImageNet1k and ImageNet21k and evaluate a variety of downstrea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21487;&#24494;&#20998;&#28210;&#26579;&#23558;&#24103;&#38388;&#20809;&#27969;&#26144;&#23556;&#21040;&#19977;&#32500;&#22330;&#26223;&#27969;&#20197;&#21516;&#27493;&#37325;&#24314;&#30456;&#26426;&#20301;&#23039;&#21644;&#19977;&#32500;&#31070;&#32463;&#22330;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#31934;&#30830;&#30456;&#26426;&#20301;&#23039;&#65292;&#21487;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#35270;&#39057;&#25968;&#25454;&#19978;&#36827;&#34892;&#20840;&#33258;&#30417;&#30563;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2306.00180</link><description>&lt;p&gt;
FlowCam: &#36890;&#36807;&#20687;&#32032;&#23545;&#40784;&#22330;&#26223;&#27969;&#65292;&#26080;&#38656;&#30456;&#26426;&#20301;&#23039;&#35757;&#32451;&#20855;&#26377;&#36890;&#29992;&#24615;&#30340;&#19977;&#32500;&#36752;&#23556;&#22330;
&lt;/p&gt;
&lt;p&gt;
FlowCam: Training Generalizable 3D Radiance Fields without Camera Poses via Pixel-Aligned Scene Flow. (arXiv:2306.00180v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21487;&#24494;&#20998;&#28210;&#26579;&#23558;&#24103;&#38388;&#20809;&#27969;&#26144;&#23556;&#21040;&#19977;&#32500;&#22330;&#26223;&#27969;&#20197;&#21516;&#27493;&#37325;&#24314;&#30456;&#26426;&#20301;&#23039;&#21644;&#19977;&#32500;&#31070;&#32463;&#22330;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#31934;&#30830;&#30456;&#26426;&#20301;&#23039;&#65292;&#21487;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#35270;&#39057;&#25968;&#25454;&#19978;&#36827;&#34892;&#20840;&#33258;&#30417;&#30563;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#23450;&#20301;&#22270;&#20687;&#20013;&#37325;&#24314;&#19977;&#32500;&#31070;&#32463;&#22330;&#24050;&#25104;&#20026;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#20294;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#35270;&#39057;&#25968;&#25454;&#19978;&#32467;&#26500;&#20809;&#23398;&#31934;&#30830;&#30456;&#26426;&#20301;&#23039;&#65292;&#36825;&#20063;&#26159;&#20854;&#26080;&#27861;&#25512;&#24191;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#22312;&#32447;&#37325;&#24314;&#30456;&#26426;&#20301;&#23039;&#21644;&#19977;&#32500;&#31070;&#32463;&#22330;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#26469;&#23436;&#25104;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21487;&#24494;&#20998;&#28210;&#26579;&#23558;&#24103;&#38388;&#20809;&#27969;&#26144;&#23556;&#21040;&#19977;&#32500;&#22330;&#26223;&#27969;&#65292;&#20197;&#20272;&#35745;&#20301;&#23039;&#12290;&#28982;&#21518;&#20877;&#36890;&#36807;&#23545;&#22330;&#26223;&#27969;&#22330;&#36827;&#34892;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#25311;&#21512;&#26469;&#25191;&#34892;SE&#65288;3&#65289;&#30456;&#26426;&#20301;&#23039;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#37325;&#26032;&#28210;&#26579;&#36755;&#20837;&#35270;&#39057;&#26469;&#32852;&#21512;&#30417;&#30563;&#20301;&#23039;&#20272;&#35745;&#21644;&#36890;&#29992;&#30340;&#31070;&#32463;&#22330;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#35270;&#39057;&#25968;&#25454;&#19978;&#36827;&#34892;&#20840;&#33258;&#30417;&#30563;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reconstruction of 3D neural fields from posed images has emerged as a promising method for self-supervised representation learning. The key challenge preventing the deployment of these 3D scene learners on large-scale video data is their dependence on precise camera poses from structure-from-motion, which is prohibitively expensive to run at scale. We propose a method that jointly reconstructs camera poses and 3D neural scene representations online and in a single forward pass. We estimate poses by first lifting frame-to-frame optical flow to 3D scene flow via differentiable rendering, preserving locality and shift-equivariance of the image processing backbone. SE(3) camera pose estimation is then performed via a weighted least-squares fit to the scene flow field. This formulation enables us to jointly supervise pose estimation and a generalizable neural scene representation via re-rendering the input video, and thus, train end-to-end and fully self-supervised on real-world video datas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#27880;&#37322;&#30340;&#33258;&#21160;&#21270;&#27969;&#31243;&#65292;&#24378;&#35843;&#24517;&#39035;&#38024;&#23545;&#20154;&#31867;&#29983;&#25104;&#30340;&#26631;&#31614;&#39564;&#35777;&#23427;&#20204;&#30340;&#24615;&#33021;&#65292;&#39564;&#35777;&#32467;&#26524;&#34920;&#26126;LLM&#30340;&#24615;&#33021;&#24456;&#26377;&#21069;&#36884;&#65292;&#20294;&#39640;&#24230;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#21644;&#27880;&#37322;&#20219;&#21153;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.00176</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#33258;&#21160;&#26631;&#27880;&#38656;&#35201;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Automated Annotation with Generative AI Requires Validation. (arXiv:2306.00176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#27880;&#37322;&#30340;&#33258;&#21160;&#21270;&#27969;&#31243;&#65292;&#24378;&#35843;&#24517;&#39035;&#38024;&#23545;&#20154;&#31867;&#29983;&#25104;&#30340;&#26631;&#31614;&#39564;&#35777;&#23427;&#20204;&#30340;&#24615;&#33021;&#65292;&#39564;&#35777;&#32467;&#26524;&#34920;&#26126;LLM&#30340;&#24615;&#33021;&#24456;&#26377;&#21069;&#36884;&#65292;&#20294;&#39640;&#24230;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#21644;&#27880;&#37322;&#20219;&#21153;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#20197;&#25104;&#20026;&#25991;&#26412;&#27880;&#37322;&#36807;&#31243;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20294;&#30001;&#20110;&#25552;&#31034;&#36136;&#37327;&#65292;&#25991;&#26412;&#25968;&#25454;&#29305;&#23450;&#24615;&#21644;&#27010;&#24565;&#38590;&#24230;&#31561;&#21407;&#22240;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#22312;&#27880;&#37322;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#12290;&#22240;&#20026;&#21363;&#20351;LLM&#25216;&#26415;&#24471;&#21040;&#25913;&#36827;&#65292;&#36825;&#20123;&#25361;&#25112;&#20173;&#23558;&#23384;&#22312;&#65292;&#25152;&#20197;&#25105;&#20204;&#35748;&#20026;&#20351;&#29992;LLM&#30340;&#20219;&#20309;&#33258;&#21160;&#26631;&#27880;&#36807;&#31243;&#37117;&#24517;&#39035;&#38024;&#23545;&#20154;&#31867;&#29983;&#25104;&#30340;&#26631;&#31614;&#39564;&#35777;LLM&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#19968;&#20010;&#24037;&#20316;&#27969;&#31243;&#65292;&#20197;&#19968;&#31181;&#21407;&#21017;&#24615;&#21644;&#26377;&#25928;&#29575;&#30340;&#26041;&#24335;&#21033;&#29992;LLM&#30340;&#27880;&#37322;&#28508;&#21147;&#12290;&#20351;&#29992;GPT-4&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#39640;&#24433;&#21709;&#26399;&#21002;&#30340;&#26368;&#26032;&#31038;&#20250;&#31185;&#23398;&#25991;&#31456;&#20013;&#22797;&#21046;11&#20010;&#25968;&#25454;&#38598;&#30340;27&#20010;&#26631;&#27880;&#20219;&#21153;&#26469;&#39564;&#35777;&#36825;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#25991;&#26412;&#27880;&#37322;&#65292;LLM&#30340;&#24615;&#33021;&#24456;&#26377;&#21069;&#36884;&#65292;&#20294;&#39640;&#24230;&#21462;&#20915;&#20110;&#25968;&#25454;&#38598;&#21644;&#27880;&#37322;&#20219;&#21153;&#31867;&#22411;&#65292;&#36825;&#24378;&#35843;&#20102;&#25353;&#20219;&#21153;&#39564;&#35777;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#26131;&#20110;&#20351;&#29992;&#30340;&#36719;&#20214;&#65292;&#26088;&#22312;&#23454;&#29616;&#25105;&#20204;&#30340;&#24037;&#20316;&#27969;&#31243;&#24182;&#31616;&#21270;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative large language models (LLMs) can be a powerful tool for augmenting text annotation procedures, but their performance varies across annotation tasks due to prompt quality, text data idiosyncrasies, and conceptual difficulty. Because these challenges will persist even as LLM technology improves, we argue that any automated annotation process using an LLM must validate the LLM's performance against labels generated by humans. To this end, we outline a workflow to harness the annotation potential of LLMs in a principled, efficient way. Using GPT-4, we validate this approach by replicating 27 annotation tasks across 11 datasets from recent social science articles in high-impact journals. We find that LLM performance for text annotation is promising but highly contingent on both the dataset and the type of annotation task, which reinforces the necessity to validate on a task-by-task basis. We make available easy-to-use software designed to implement our workflow and streamline the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#19977;&#31181;&#20915;&#31574;&#31639;&#27861;&#22312;&#26032;&#24247;&#22982;&#31867;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#21457;&#29616;&#26080;&#26102;&#38388;&#30340;&#20915;&#31574;&#29702;&#35770;&#22312;&#36825;&#20123;&#38382;&#39064;&#19978;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.00175</link><description>&lt;p&gt;
&#26032;&#24247;&#22982;&#31867;&#38382;&#39064;&#19978;&#20915;&#31574;&#31639;&#27861;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
A Comparison of Decision Algorithms on Newcomblike Problems. (arXiv:2306.00175v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#19977;&#31181;&#20915;&#31574;&#31639;&#27861;&#22312;&#26032;&#24247;&#22982;&#31867;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#21457;&#29616;&#26080;&#26102;&#38388;&#30340;&#20915;&#31574;&#29702;&#35770;&#22312;&#36825;&#20123;&#38382;&#39064;&#19978;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#37319;&#29992;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#24418;&#24335;&#36827;&#34892;&#34920;&#36848;&#26102;&#65292;&#20004;&#31181;&#26631;&#20934;&#30340;&#20915;&#31574;&#31639;&#27861;&#65288;&#35777;&#25454;&#20915;&#31574;&#29702;&#35770;&#21644;&#22240;&#26524;&#20915;&#31574;&#29702;&#35770;&#65289;&#22312;&#29359;&#20154;&#22256;&#22659;&#21644;&#25152;&#35859;&#30340;&#8220;&#26032;&#24247;&#22982;&#31867;&#8221;&#38382;&#39064;&#26041;&#38754;&#38754;&#20020;&#31995;&#32479;&#24615;&#22833;&#36133;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#26032;&#30340;&#20915;&#31574;&#31639;&#27861;&#65292;&#31216;&#20026;&#26080;&#26102;&#38388;&#30340;&#20915;&#31574;&#29702;&#35770;&#65292;&#35813;&#29702;&#35770;&#22312;&#36825;&#20123;&#38382;&#39064;&#19978;&#22987;&#32456;&#33719;&#32988;&#12290;
&lt;/p&gt;
&lt;p&gt;
When formulated using Bayesian networks, two standard decision algorithms (Evidential Decision Theory and Causal Decision Theory) can be shown to fail systematically when faced with aspects of the prisoner's dilemma and so-called "Newcomblike" problems. We describe a new form of decision algorithm, called Timeless Decision Theory, which consistently wins on these problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25945;&#31243;&#35770;&#25991;&#20171;&#32461;&#20102;&#31526;&#21495;&#22238;&#24402;&#25216;&#26415;&#65292;&#20197;&#20154;&#31867;&#20581;&#24247;&#21644;&#33829;&#20859;&#39046;&#22495;&#30340;&#23454;&#38469;&#24212;&#29992;&#20026;&#20363;&#65292;&#23558;&#21508;&#31181;&#20154;&#20307;&#27979;&#37327;&#25351;&#26631;&#34701;&#21512;&#25104;&#19968;&#20010;&#31616;&#21333;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#20197;&#20272;&#31639;&#20307;&#33026;&#30334;&#20998;&#27604;&#12290;</title><link>http://arxiv.org/abs/2306.00153</link><description>&lt;p&gt;
&#22522;&#20110;&#31526;&#21495;&#22238;&#24402;&#30340;&#20449;&#24687;&#34701;&#21512;&#65306;&#20197;&#20154;&#31867;&#20581;&#24247;&#20026;&#32972;&#26223;&#30340;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
Information Fusion via Symbolic Regression: A Tutorial in the Context of Human Health. (arXiv:2306.00153v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00153
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25945;&#31243;&#35770;&#25991;&#20171;&#32461;&#20102;&#31526;&#21495;&#22238;&#24402;&#25216;&#26415;&#65292;&#20197;&#20154;&#31867;&#20581;&#24247;&#21644;&#33829;&#20859;&#39046;&#22495;&#30340;&#23454;&#38469;&#24212;&#29992;&#20026;&#20363;&#65292;&#23558;&#21508;&#31181;&#20154;&#20307;&#27979;&#37327;&#25351;&#26631;&#34701;&#21512;&#25104;&#19968;&#20010;&#31616;&#21333;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#20197;&#20272;&#31639;&#20307;&#33026;&#30334;&#20998;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25945;&#31243;&#35770;&#25991;&#25552;&#20379;&#20102;&#31526;&#21495;&#22238;&#24402;&#65288;SR&#65289;&#30340;&#19968;&#33324;&#27010;&#36848;&#65292;&#37325;&#28857;&#20851;&#27880;&#21487;&#35299;&#37322;&#24615;&#26631;&#20934;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#23613;&#31649;&#25991;&#29486;&#20013;&#23545;&#21487;&#35299;&#37322;&#24615;&#24314;&#27169;&#30340;&#23450;&#20041;&#20173;&#26377;&#20105;&#35758;&#65292;&#20294;&#21487;&#35299;&#37322;&#24615;&#27169;&#22411;&#26159;&#25903;&#25345;&#35780;&#20272;&#25104;&#21151;&#20449;&#24687;&#34701;&#21512;&#30340;&#23454;&#29992;&#26041;&#24335;&#12290;&#20026;&#20102;&#20256;&#36798;SR&#20316;&#20026;&#24314;&#27169;&#25216;&#26415;&#30340;&#20248;&#28857;&#65292;&#25105;&#20204;&#21033;&#29992;&#26469;&#33258;&#30142;&#30149;&#25511;&#21046;&#21644;&#39044;&#38450;&#20013;&#24515;&#65288;CDC&#65289;&#30340;&#20844;&#24320;&#21487;&#29992;&#30340;&#22269;&#23478;&#20581;&#24247;&#21644;&#33829;&#20859;&#35843;&#26597;&#65288;NHANES&#65289;&#25968;&#25454;&#65292;&#22312;&#20581;&#24247;&#21644;&#33829;&#20859;&#39046;&#22495;&#23637;&#31034;&#20102;&#19968;&#20010;&#24212;&#29992;&#65292;&#23558;&#21508;&#31181;&#20154;&#20307;&#27979;&#37327;&#25351;&#26631;&#34701;&#21512;&#25104;&#19968;&#20010;&#31616;&#21333;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#20197;&#20272;&#31639;&#20307;&#33026;&#30334;&#20998;&#27604;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;SR&#24314;&#27169;&#30340;&#20248;&#28857;&#21644;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#25152;&#23398;&#27169;&#22411;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
This tutorial paper provides a general overview of symbolic regression (SR) with specific focus on standards of interpretability. We posit that interpretable modeling, although its definition is still disputed in the literature, is a practical way to support the evaluation of successful information fusion. In order to convey the benefits of SR as a modeling technique, we demonstrate an application within the field of health and nutrition using publicly available National Health and Nutrition Examination Survey (NHANES) data from the Centers for Disease Control and Prevention (CDC), fusing together anthropometric markers into a simple mathematical expression to estimate body fat percentage. We discuss the advantages and challenges associated with SR modeling and provide qualitative and quantitative analyses of the learned models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#26576;&#20123;&#38382;&#39064;&#22312;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#26410;&#24471;&#21040;&#35299;&#20915;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#38382;&#39064;&#30340;&#22238;&#31572;&#65292;&#21253;&#25324;&#24191;&#32780;&#27973;&#30340;ReLU&#32593;&#32476;&#19981;&#33021;&#34987;&#28145;&#32780;&#31364;&#30340;ReLU&#32593;&#32476;&#24456;&#22909;&#22320;&#36924;&#36817;&#31561;&#12290;</title><link>http://arxiv.org/abs/2306.00145</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#34920;&#36798;&#33021;&#21147;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Expressive Power of Neural Networks. (arXiv:2306.00145v1 [math.CA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#26576;&#20123;&#38382;&#39064;&#22312;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#26410;&#24471;&#21040;&#35299;&#20915;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#38382;&#39064;&#30340;&#22238;&#31572;&#65292;&#21253;&#25324;&#24191;&#32780;&#27973;&#30340;ReLU&#32593;&#32476;&#19981;&#33021;&#34987;&#28145;&#32780;&#31364;&#30340;ReLU&#32593;&#32476;&#24456;&#22909;&#22320;&#36924;&#36817;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
1989&#24180;&#65292;George Cybenko&#22312;&#19968;&#31687;&#37324;&#31243;&#30865;&#24335;&#30340;&#35770;&#25991;&#20013;&#35777;&#26126;&#20102;&#23485;&#32780;&#27973;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#32039;&#33268;&#38598;&#19978;&#36924;&#36817;&#20219;&#24847;&#36830;&#32493;&#20989;&#25968;&#65292;&#36825;&#20010;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#24341;&#21457;&#20102;&#24456;&#22810;&#21518;&#32493;&#30740;&#31350;&#12290;&#26412;&#25991;&#23558;&#36890;&#36807;&#19968;&#20010;&#26694;&#26550;&#22238;&#31572;&#8220;&#26377;&#27809;&#26377;&#19968;&#20123;&#24191;&#32780;&#27973;&#30340;ReLU&#32593;&#32476;&#26080;&#27861;&#34987;&#28145;&#32780;&#31364;&#30340;ReLU&#32593;&#32476;&#24456;&#22909;&#22320;&#36924;&#36817;&#65311;&#8221;&#8220;&#26222;&#36941;&#36924;&#36817;&#23450;&#29702;&#26159;&#21542;&#20173;&#36866;&#29992;&#20110;Sobolev&#31354;&#38388;&#33539;&#25968;W 1,1&#65311;&#8221;&#8220;&#36825;&#20123;&#32467;&#26524;&#26159;&#21542;&#36866;&#29992;&#20110;&#38500;ReLU&#20043;&#22806;&#30340;&#28608;&#27963;&#20989;&#25968;&#65311;&#8221;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In 1989 George Cybenko proved in a landmark paper that wide shallow neural networks can approximate arbitrary continuous functions on a compact set. This universal approximation theorem sparked a lot of follow-up research.  Shen, Yang and Zhang determined optimal approximation rates for ReLU-networks in $L^p$-norms with $p \in [1,\infty)$. Kidger and Lyons proved a universal approximation theorem for deep narrow ReLU-networks. Telgarsky gave an example of a deep narrow ReLU-network that cannot be approximated by a wide shallow ReLU-network unless it has exponentially many neurons.  However, there are even more questions that still remain unresolved. Are there any wide shallow ReLU-networks that cannot be approximated well by deep narrow ReLU-networks? Is the universal approximation theorem still true for other norms like the Sobolev norm $W^{1,1}$? Do these results hold for activation functions other than ReLU?  We will answer all of those questions and more with a framework of two exp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26102;&#38388;&#34917;&#19969;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;&#21152;&#25343;&#22823;&#20892;&#30000;&#30340;&#22810;&#26102;&#30456;&#36965;&#24863;&#24433;&#20687;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#25163;&#21160;&#32463;&#36807;&#30830;&#35748;&#21644;&#31579;&#36873;&#30340;&#39640;&#20998;&#36776;&#29575;&#22320;&#29702;&#21442;&#32771;&#22270;&#20687;&#65292;&#35206;&#30422;&#22235;&#20010;&#20892;&#20316;&#29289;&#29983;&#20135;&#24180;&#24230;&#21644;&#20116;&#20010;&#26376;&#20221;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#20110;&#25552;&#39640;&#22303;&#22320;&#35206;&#30422;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.00114</link><description>&lt;p&gt;
&#21152;&#25343;&#22823;&#20892;&#30000;&#25968;&#25454;&#38598;&#65306;&#29992;&#20110;&#20892;&#19994;&#22810;&#26102;&#30456;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#30340;&#26032;&#22320;&#34920;&#35206;&#30422;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
The Canadian Cropland Dataset: A New Land Cover Dataset for Multitemporal Deep Learning Classification in Agriculture. (arXiv:2306.00114v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00114
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26102;&#38388;&#34917;&#19969;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;&#21152;&#25343;&#22823;&#20892;&#30000;&#30340;&#22810;&#26102;&#30456;&#36965;&#24863;&#24433;&#20687;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#25163;&#21160;&#32463;&#36807;&#30830;&#35748;&#21644;&#31579;&#36873;&#30340;&#39640;&#20998;&#36776;&#29575;&#22320;&#29702;&#21442;&#32771;&#22270;&#20687;&#65292;&#35206;&#30422;&#22235;&#20010;&#20892;&#20316;&#29289;&#29983;&#20135;&#24180;&#24230;&#21644;&#20116;&#20010;&#26376;&#20221;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#20110;&#25552;&#39640;&#22303;&#22320;&#35206;&#30422;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#36965;&#24863;&#30417;&#27979;&#22303;&#22320;&#35206;&#30422;&#26159;&#30740;&#31350;&#29615;&#22659;&#21464;&#21270;&#21644;&#36890;&#36807;&#31918;&#39135;&#20135;&#37327;&#39044;&#27979;&#30830;&#20445;&#20840;&#29699;&#31918;&#39135;&#23433;&#20840;&#30340;&#20851;&#38190;&#12290;&#23588;&#20854;&#26159;&#65292;&#22810;&#26102;&#30456;&#36965;&#24863;&#24433;&#20687;&#25552;&#20379;&#20102;&#20851;&#20110;&#22330;&#26223;&#21160;&#24577;&#30340;&#30456;&#20851;&#20449;&#24687;&#65292;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#24102;&#26469;&#26356;&#22909;&#30340;&#22303;&#22320;&#35206;&#30422;&#20998;&#31867;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38590;&#20197;&#33719;&#21462;&#21487;&#38752;&#12289;&#32454;&#31890;&#24230;&#21644;&#39640;&#36136;&#37327;&#30340;&#27880;&#37322;&#26679;&#26412;&#25903;&#25345;&#20182;&#20204;&#30340;&#20551;&#35774;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#21463;&#30410;&#20110;&#39640;&#31354;&#38388;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21152;&#25343;&#22823;&#20892;&#30000;&#30340;&#26102;&#38388;&#34917;&#19969;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#26469;&#33258;10&#20010;&#20892;&#20316;&#29289;&#31867;&#21035;&#30340;78,536&#20010;&#25163;&#21160;&#32463;&#36807;&#30830;&#35748;&#21644;&#31579;&#36873;&#30340;&#39640;&#20998;&#36776;&#29575;(10&#31859;/&#20687;&#32032;&#65292;640 x 640&#31859;)&#22320;&#29702;&#21442;&#32771;&#22270;&#20687;&#65292;&#35206;&#30422;&#20102;&#22235;&#20010;&#20892;&#20316;&#29289;&#29983;&#20135;&#24180;&#24230;(2017-2020)&#21644;&#20116;&#20010;&#26376;&#20221;(&#20845;&#26376;-&#21313;&#26376;)&#12290;&#27599;&#20010;&#23454;&#20363;&#37117;&#21253;&#21547;12&#20010;&#20809;&#35889;&#27874;&#27573;&#12289;&#19968;&#24352;RGB&#22270;&#20687;&#21644;&#39069;&#22806;&#30340;&#26893;&#34987;&#25351;&#25968;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monitoring land cover using remote sensing is vital for studying environmental changes and ensuring global food security through crop yield forecasting. Specifically, multitemporal remote sensing imagery provides relevant information about the dynamics of a scene, which has proven to lead to better land cover classification results. Nevertheless, few studies have benefited from high spatial and temporal resolution data due to the difficulty of accessing reliable, fine-grained and high-quality annotated samples to support their hypotheses. Therefore, we introduce a temporal patch-based dataset of Canadian croplands, enriched with labels retrieved from the Canadian Annual Crop Inventory. The dataset contains 78,536 manually verified and curated high-resolution (10 m/pixel, 640 x 640 m) geo-referenced images from 10 crop classes collected over four crop production years (2017-2020) and five months (June-October). Each instance contains 12 spectral bands, an RGB image, and additional veget
&lt;/p&gt;</description></item><item><title>MuseCoco&#26159;&#19968;&#31181;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#31526;&#21495;&#38899;&#20048;&#30340;&#31995;&#32479;&#65292;&#20855;&#22791;&#39640;&#25928;&#21644;&#28789;&#27963;&#31561;&#29305;&#28857;&#65292;&#20026;&#38899;&#20048;&#23478;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#38899;&#20048;&#29983;&#25104;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.00110</link><description>&lt;p&gt;
MuseCoco: &#20174;&#25991;&#26412;&#29983;&#25104;&#31526;&#21495;&#38899;&#20048;
&lt;/p&gt;
&lt;p&gt;
MuseCoco: Generating Symbolic Music from Text. (arXiv:2306.00110v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00110
&lt;/p&gt;
&lt;p&gt;
MuseCoco&#26159;&#19968;&#31181;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#31526;&#21495;&#38899;&#20048;&#30340;&#31995;&#32479;&#65292;&#20855;&#22791;&#39640;&#25928;&#21644;&#28789;&#27963;&#31561;&#29305;&#28857;&#65292;&#20026;&#38899;&#20048;&#23478;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#38899;&#20048;&#29983;&#25104;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#38899;&#20048;&#26159;&#19968;&#31181;&#29992;&#25143;&#21451;&#22909;&#30340;&#26041;&#24335;&#65292;&#22240;&#20026;&#25991;&#26412;&#26159;&#30456;&#23545;&#26131;&#20110;&#29992;&#25143;&#21442;&#19982;&#30340;&#30028;&#38754;&#12290;&#32780;&#26377;&#20123;&#26041;&#27861;&#21033;&#29992;&#25991;&#26412;&#26469;&#25511;&#21046;&#38899;&#20048;&#38899;&#39057;&#30340;&#29983;&#25104;&#65292;&#20294;&#26159;&#32534;&#36753;&#29983;&#25104;&#38899;&#39057;&#30340;&#38899;&#20048;&#20803;&#32032;&#23545;&#20110;&#29992;&#25143;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#31526;&#21495;&#38899;&#20048;&#20855;&#26377;&#26131;&#20110;&#32534;&#36753;&#30340;&#20248;&#28857;&#65292;&#20351;&#29992;&#25143;&#26356;&#23481;&#26131;&#25805;&#20316;&#29305;&#23450;&#30340;&#38899;&#20048;&#20803;&#32032;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MuseCoco&#65292;&#23427;&#21033;&#29992;&#38899;&#20048;&#23646;&#24615;&#20316;&#20026;&#26725;&#26753;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#25991;&#26412;&#21040;&#23646;&#24615;&#29702;&#35299;&#21644;&#23646;&#24615;&#21040;&#38899;&#20048;&#29983;&#25104;&#30340;&#20004;&#20010;&#38454;&#27573;&#65292;&#20174;&#32780;&#29983;&#25104;&#31526;&#21495;&#38899;&#20048;&#12290;MuseCoCo&#20195;&#34920;&#38899;&#20048;&#20316;&#26354;&#21103;&#39550;&#39542;&#65292;&#20351;&#38899;&#20048;&#23478;&#21487;&#20197;&#30452;&#25509;&#20174;&#32473;&#23450;&#30340;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#38899;&#20048;&#65292;&#19982;&#20174;&#22836;&#24320;&#22987;&#21019;&#20316;&#30456;&#27604;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;&#35813;&#31995;&#32479;&#20855;&#26377;&#20004;&#20010;&#20027;&#35201;&#20248;&#28857;&#65306;&#25968;&#25454;&#39640;&#25928;&#12290;&#22312;&#23646;&#24615;&#21040;&#38899;&#20048;&#29983;&#25104;&#38454;&#27573;&#65292;&#23646;&#24615;&#21487;&#20197;&#30452;&#25509;&#36827;&#34892;&#32534;&#30721;&#65292;&#32780;&#19981;&#38656;&#35201;&#22823;&#37327;&#30340;&#38899;&#20048;&#25968;&#25454;&#12290;&#20854;&#27425;&#65292;&#27492;&#31995;&#32479;&#20855;&#26377;&#39640;&#32423;&#21035;&#30340;&#28789;&#27963;&#24615;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#36890;&#36807;&#21464;&#26356;&#25991;&#26412;&#36755;&#20837;&#26469;&#29983;&#25104;&#22810;&#26679;&#21270;&#19988;&#26377;&#20010;&#24615;&#30340;&#31526;&#21495;&#38899;&#20048;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating music from text descriptions is a user-friendly mode since the text is a relatively easy interface for user engagement. While some approaches utilize texts to control music audio generation, editing musical elements in generated audio is challenging for users. In contrast, symbolic music offers ease of editing, making it more accessible for users to manipulate specific musical elements. In this paper, we propose MuseCoco, which generates symbolic music from text descriptions with musical attributes as the bridge to break down the task into text-to-attribute understanding and attribute-to-music generation stages. MuseCoCo stands for Music Composition Copilot that empowers musicians to generate music directly from given text descriptions, offering a significant improvement in efficiency compared to creating music entirely from scratch. The system has two main advantages: Firstly, it is data efficient. In the attribute-to-music generation stage, the attributes can be directly e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;MERT&#65292;&#21033;&#29992;&#20102;&#25945;&#24072;&#27169;&#22411;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#20248;&#20110;&#20256;&#32479;&#30340;&#35821;&#38899;&#21644;&#38899;&#39057;&#26041;&#27861;&#30340;&#32452;&#21512;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.00107</link><description>&lt;p&gt;
MERT:&#24102;&#26377;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#22768;&#23398;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training. (arXiv:2306.00107v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00107
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;MERT&#65292;&#21033;&#29992;&#20102;&#25945;&#24072;&#27169;&#22411;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#20248;&#20110;&#20256;&#32479;&#30340;&#35821;&#38899;&#21644;&#38899;&#39057;&#26041;&#27861;&#30340;&#32452;&#21512;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26368;&#36817;&#22312;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#35821;&#38899;&#39046;&#22495;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#35757;&#32451;&#36890;&#29992;&#27169;&#22411;&#30340;&#19968;&#31181;&#24456;&#26377;&#21069;&#26223;&#30340;&#33539;&#20363;&#65292;&#23545;&#20110;&#36328;&#36234;&#38899;&#20048;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#35843;&#24615;&#21644;&#38899;&#39640;&#36825;&#26679;&#30340;&#29305;&#27530;&#38899;&#20048;&#30693;&#35782;&#30340;&#24314;&#27169;&#39047;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#22768;&#23398;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;&#65292;&#21363;MERT&#12290;&#22312;&#25105;&#20204;&#30340;&#25506;&#32034;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26356;&#20248;&#31168;&#30340;&#25945;&#24072;&#27169;&#22411;&#32452;&#21512;&#65292;&#36825;&#31181;&#32452;&#21512;&#26041;&#27861;&#22312;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#35821;&#38899;&#21644;&#38899;&#39057;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has recently emerged as a promising paradigm for training generalisable models on large-scale data in the fields of vision, text, and speech. Although SSL has been proven effective in speech and audio, its application to music audio has yet to be thoroughly explored. This is primarily due to the distinctive challenges associated with modelling musical knowledge, particularly its tonal and pitched characteristics of music. To address this research gap, we propose an acoustic Music undERstanding model with large-scale self-supervised Training (MERT), which incorporates teacher models to provide pseudo labels in the masked language modelling (MLM) style acoustic pre-training. In our exploration, we identified a superior combination of teacher models, which outperforms conventional speech and audio approaches in terms of performance. This combination includes an acoustic teacher based on Residual Vector Quantization - Variational AutoEncoder (RVQ-VAE) and a m
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#25216;&#26415;&#30340;&#24555;&#36895;&#22686;&#38271;&#35753;&#25163;&#24037;&#33402;&#26415;&#21697;&#21644;AI&#29983;&#25104;&#22270;&#20687;&#21306;&#20998;&#26085;&#28176;&#22256;&#38590;&#12290;&#28982;&#32780;&#25552;&#39640;&#20154;&#31867;&#29983;&#27963;&#21644;&#24037;&#20316;&#26631;&#20934;&#20197;&#21450;&#21033;&#29992;&#19968;&#32676;&#20154;&#26469;&#20805;&#23454;&#21478;&#19968;&#32676;&#20154;&#20043;&#38388;&#23547;&#27714;&#24179;&#34913;&#26159;&#20851;&#38190;&#12290;&#35813;&#39046;&#22495;&#38754;&#20020;&#30528;&#34987;AI&#22522;&#30784;&#35774;&#26045;&#25509;&#31649;&#30340;&#39118;&#38505;&#65292;&#21516;&#26102;&#23384;&#22312;&#36523;&#20221;&#30423;&#31363;&#12289;&#25968;&#25454;&#27927;&#30333;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.00080</link><description>&lt;p&gt;
AI&#22270;&#20687;&#21644;Overton Window
&lt;/p&gt;
&lt;p&gt;
AI Imagery and the Overton Window. (arXiv:2306.00080v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00080
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#25216;&#26415;&#30340;&#24555;&#36895;&#22686;&#38271;&#35753;&#25163;&#24037;&#33402;&#26415;&#21697;&#21644;AI&#29983;&#25104;&#22270;&#20687;&#21306;&#20998;&#26085;&#28176;&#22256;&#38590;&#12290;&#28982;&#32780;&#25552;&#39640;&#20154;&#31867;&#29983;&#27963;&#21644;&#24037;&#20316;&#26631;&#20934;&#20197;&#21450;&#21033;&#29992;&#19968;&#32676;&#20154;&#26469;&#20805;&#23454;&#21478;&#19968;&#32676;&#20154;&#20043;&#38388;&#23547;&#27714;&#24179;&#34913;&#26159;&#20851;&#38190;&#12290;&#35813;&#39046;&#22495;&#38754;&#20020;&#30528;&#34987;AI&#22522;&#30784;&#35774;&#26045;&#25509;&#31649;&#30340;&#39118;&#38505;&#65292;&#21516;&#26102;&#23384;&#22312;&#36523;&#20221;&#30423;&#31363;&#12289;&#25968;&#25454;&#27927;&#30333;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22312;&#36807;&#21435;&#19968;&#24180;&#20013;&#22312;&#35270;&#35273;&#32508;&#21512;&#21644;&#32654;&#23398;&#24418;&#35937;&#30340;&#29983;&#20135;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#27493;&#65292;&#21040;&#20102;&#21306;&#20998;&#25163;&#24037;&#33402;&#26415;&#21697;&#21644;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#24840;&#21457;&#22256;&#38590;&#30340;&#22320;&#27493;&#12290;&#20363;&#22914;&#31283;&#24577;&#25193;&#25955;&#12289;Midjourney&#31561;&#29983;&#25104;&#27169;&#22411;&#26377;&#26395;&#22312;&#25216;&#26415;&#21644;&#20262;&#29702;&#26041;&#38754;&#24433;&#21709;&#20960;&#20010;&#20027;&#35201;&#34892;&#19994;&#12290;&#22312;&#25552;&#39640;&#20154;&#31867;&#29983;&#27963;&#21644;&#24037;&#20316;&#26631;&#20934;&#20197;&#21450;&#21033;&#29992;&#19968;&#32676;&#20154;&#26469;&#20805;&#23454;&#21478;&#19968;&#32676;&#20154;&#20043;&#38388;&#23547;&#27714;&#24179;&#34913;&#26159;&#35752;&#35770;&#30340;&#22797;&#26434;&#21644;&#20851;&#38190;&#37096;&#20998;&#12290;&#30001;&#20110;&#36825;&#31181;&#25216;&#26415;&#30340;&#24555;&#36895;&#22686;&#38271;&#12289;&#27169;&#22411;&#36816;&#34892;&#26041;&#24335;&#21644;&#28784;&#33394;&#27861;&#24459;&#30340;&#23384;&#22312;&#65292;&#21253;&#25324;&#35270;&#39057;&#28216;&#25103;&#34892;&#19994;&#22312;&#20869;&#30340;&#35270;&#35273;&#21644;&#33402;&#26415;&#39046;&#22495;&#38754;&#20020;&#34987;AI&#22522;&#30784;&#35774;&#26045;&#25152;&#26377;&#32773;&#25509;&#31649;&#30340;&#39118;&#38505;&#12290;&#35813;&#25991;&#31456;&#26159;&#19968;&#31687;&#25991;&#29486;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#24403;&#20170;AI&#24320;&#21457;&#32773;&#21644;&#29992;&#25143;&#38754;&#20020;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#36523;&#20221;&#30423;&#31363;&#12289;&#25968;&#25454;&#27927;&#30333;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI-based text-to-image generation has undergone a significant leap in the production of visually comprehensive and aesthetic imagery over the past year, to the point where differentiating between a man-made piece of art and an AI-generated image is becoming more difficult. Generative Models such as Stable Diffusion, Midjourney and others are expected to affect several major industries in technological and ethical aspects. Striking the balance between raising human standard of life and work vs exploiting one group of people to enrich another is a complex and crucial part of the discussion. Due to the rapid growth of this technology, the way in which its models operate, and gray area legalities, visual and artistic domains - including the video game industry, are at risk of being taken over from creators by AI infrastructure owners. This paper is a literature review examining the concerns facing both AI developers and users today, including identity theft, data laundering and more. It di
&lt;/p&gt;</description></item><item><title>&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#20351;&#29992;&#37327;&#23376;&#35745;&#31639;&#26426;&#36827;&#34892;&#35780;&#20272;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#22312;&#35757;&#32451;&#23436;&#21518;&#65292;&#20351;&#29992;&#37327;&#23376;&#35745;&#31639;&#26426;&#29983;&#25104;&#19968;&#20010;&#32463;&#20856;&#38452;&#24433;&#27169;&#22411;&#26469;&#35745;&#31639;&#20989;&#25968;&#30340;&#32463;&#20856;&#35745;&#31639;&#36817;&#20284;&#65292;&#36991;&#20813;&#20102;&#23545;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2306.00061</link><description>&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#38452;&#24433;
&lt;/p&gt;
&lt;p&gt;
Shadows of quantum machine learning. (arXiv:2306.00061v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00061
&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#20351;&#29992;&#37327;&#23376;&#35745;&#31639;&#26426;&#36827;&#34892;&#35780;&#20272;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#22312;&#35757;&#32451;&#23436;&#21518;&#65292;&#20351;&#29992;&#37327;&#23376;&#35745;&#31639;&#26426;&#29983;&#25104;&#19968;&#20010;&#32463;&#20856;&#38452;&#24433;&#27169;&#22411;&#26469;&#35745;&#31639;&#20989;&#25968;&#30340;&#32463;&#20856;&#35745;&#31639;&#36817;&#20284;&#65292;&#36991;&#20813;&#20102;&#23545;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#32463;&#24120;&#34987;&#35748;&#20026;&#26159;&#21033;&#29992;&#37327;&#23376;&#35745;&#31639;&#26426;&#35299;&#20915;&#23454;&#38469;&#38382;&#39064;&#30340;&#26368;&#26377;&#21069;&#36884;&#30340;&#24212;&#29992;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#38459;&#30861;&#20854;&#22312;&#23454;&#36341;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#20027;&#35201;&#38556;&#30861;&#26159;&#36825;&#20123;&#27169;&#22411;&#21363;&#20351;&#22312;&#35757;&#32451;&#36807;&#31243;&#21518;&#65292;&#20173;&#38656;&#35201;&#35775;&#38382;&#37327;&#23376;&#35745;&#31639;&#26426;&#25165;&#33021;&#23545;&#26032;&#25968;&#25454;&#36827;&#34892;&#35780;&#20272;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#37327;&#23376;&#27169;&#22411;&#30340;&#35757;&#32451;&#38454;&#27573;&#20043;&#21518;&#65292;&#37327;&#23376;&#35745;&#31639;&#26426;&#21487;&#20197;&#29992;&#26469;&#29983;&#25104;&#25105;&#20204;&#25152;&#35859;&#30340;&#35813;&#27169;&#22411;&#30340;&#8220;&#32463;&#20856;&#38452;&#24433;&#8221;&#65292;&#21363;&#24050;&#23398;&#20064;&#20989;&#25968;&#30340;&#32463;&#20856;&#35745;&#31639;&#36817;&#20284;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#35752;&#20102;&#36825;&#20010;&#24819;&#27861;&#24182;&#25552;&#20986;&#20102;&#26500;&#24314;&#36825;&#31181;&#24433;&#23376;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#20063;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#32463;&#20856;&#27169;&#22411;&#21487;&#33021;&#20195;&#26367;&#30340;&#21487;&#33021;&#24615;&#65292;&#20174;&#32780;&#39318;&#20808;&#22238;&#36991;&#20102;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#38656;&#35201;&#12290;&#26412;&#25991;&#37319;&#29992;&#26032;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#37327;&#23376;&#32447;&#24615;&#27169;&#22411;&#21644;&#32463;&#20856;&#38452;&#24433;&#37325;&#26500;&#30340;&#26694;&#26550;&#26469;&#23450;&#20041;&#38452;&#24433;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum machine learning is often highlighted as one of the most promising uses for a quantum computer to solve practical problems. However, a major obstacle to the widespread use of quantum machine learning models in practice is that these models, even once trained, still require access to a quantum computer in order to be evaluated on new data. To solve this issue, we suggest that following the training phase of a quantum model, a quantum computer could be used to generate what we call a classical shadow of this model, i.e., a classically computable approximation of the learned function. While recent works already explore this idea and suggest approaches to construct such shadow models, they also raise the possibility that a completely classical model could be trained instead, thus circumventing the need for a quantum computer in the first place. In this work, we take a novel approach to define shadow models based on the frameworks of quantum linear models and classical shadow tomogr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#36827;&#21270;&#31574;&#30053;&#31639;&#27861;&#20855;&#26377;&#39640;&#24230;&#31232;&#30095;&#21487;&#35757;&#32451;&#30340;&#21021;&#22987;&#21442;&#25968;&#65292;&#24182;&#27604;&#36739;&#20102;&#19982;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#31232;&#30095;&#35757;&#32451;&#30340;&#24046;&#24322;&#12290;&#36827;&#21270;&#31574;&#30053;&#31639;&#27861;&#21487;&#20197;&#25506;&#32034;&#21508;&#31181;&#19981;&#21516;&#30340;&#12289;&#24179;&#22374;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#24182;&#19988;&#19981;&#20250;&#20445;&#30041;&#32447;&#24615;&#27169;&#24335;&#30340;&#36830;&#25509;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.00045</link><description>&lt;p&gt;
&#36827;&#21270;&#20248;&#21270;&#20013;&#30340;&#24425;&#31080;&#31080;&#29616;&#35937;&#65306;&#31232;&#30095;&#19988;&#26080;&#38656;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Lottery Tickets in Evolutionary Optimization: On Sparse Backpropagation-Free Trainability. (arXiv:2306.00045v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#36827;&#21270;&#31574;&#30053;&#31639;&#27861;&#20855;&#26377;&#39640;&#24230;&#31232;&#30095;&#21487;&#35757;&#32451;&#30340;&#21021;&#22987;&#21442;&#25968;&#65292;&#24182;&#27604;&#36739;&#20102;&#19982;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#31232;&#30095;&#35757;&#32451;&#30340;&#24046;&#24322;&#12290;&#36827;&#21270;&#31574;&#30053;&#31639;&#27861;&#21487;&#20197;&#25506;&#32034;&#21508;&#31181;&#19981;&#21516;&#30340;&#12289;&#24179;&#22374;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#24182;&#19988;&#19981;&#20250;&#20445;&#30041;&#32447;&#24615;&#27169;&#24335;&#30340;&#36830;&#25509;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24425;&#31080;&#31080;&#29616;&#35937;&#26159;&#21542;&#21482;&#23384;&#22312;&#20110;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#20013;&#65292;&#36824;&#26159;&#21487;&#20197;&#25512;&#24191;&#21040;&#36827;&#21270;&#20248;&#21270;&#31639;&#27861;&#20013;&#65311;&#26412;&#25991;&#35777;&#26126;&#20102;&#36827;&#21270;&#31574;&#30053;&#31639;&#27861;&#20855;&#26377;&#39640;&#24230;&#31232;&#30095;&#21487;&#35757;&#32451;&#30340;&#21021;&#22987;&#21442;&#25968;&#65292;&#24182;&#27604;&#36739;&#20102;&#19982;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#31232;&#30095;&#35757;&#32451;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#22122;&#27604;&#36845;&#20195;&#21098;&#26525;&#36807;&#31243;&#65292;&#23558;&#25439;&#22833;&#26354;&#29575;&#20449;&#24687;&#34701;&#20837;&#21040;&#32593;&#32476;&#21098;&#26525;&#27493;&#39588;&#20013;&#65292;&#21487;&#20197;&#21457;&#29616;&#19982;&#26799;&#24230;&#19979;&#38477;&#30456;&#27604;&#65292;&#40657;&#30418;&#36827;&#21270;&#31639;&#27861;&#26041;&#27861;&#26377;&#21487;&#33021;&#21457;&#29616;&#26356;&#31232;&#30095;&#19988;&#21487;&#35757;&#32451;&#30340;&#32593;&#32476;&#21021;&#22987;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#21021;&#22987;&#21442;&#25968;&#21253;&#21547;&#20102;&#24402;&#32435;&#20559;&#35265;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#36827;&#21270;&#31574;&#30053;&#20219;&#21153;&#21450;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#20013;&#36827;&#34892;&#20256;&#36882;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#20248;&#21270;&#31639;&#27861;&#21644;&#31232;&#30095;&#27700;&#24179;&#20135;&#29983;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#19982;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#19981;&#21516;&#65292;&#36827;&#21270;&#31574;&#30053;&#31639;&#27861;&#21487;&#20197;&#25506;&#32034;&#21508;&#31181;&#19981;&#21516;&#30340;&#12289;&#24179;&#22374;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#24182;&#19988;&#19981;&#20250;&#20445;&#30041;&#32447;&#24615;&#27169;&#24335;&#30340;&#36830;&#25509;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is the lottery ticket phenomenon an idiosyncrasy of gradient-based training or does it generalize to evolutionary optimization? In this paper we establish the existence of highly sparse trainable initializations for evolution strategies (ES) and characterize qualitative differences compared to gradient descent (GD)-based sparse training. We introduce a novel signal-to-noise iterative pruning procedure, which incorporates loss curvature information into the network pruning step. This can enable the discovery of even sparser trainable network initializations when using black-box evolution as compared to GD-based optimization. Furthermore, we find that these initializations encode an inductive bias, which transfers across different ES, related tasks and even to GD-based training. Finally, we compare the local optima resulting from the different optimization paradigms and sparsity levels. In contrast to GD, ES explore diverse and flat local optima and do not preserve linear mode connectivi
&lt;/p&gt;</description></item><item><title>SNO&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#20854;&#20351;&#29992;&#31354;&#38388;&#32593;&#26426;&#21046;&#26469;&#32472;&#21046;&#35299;&#31354;&#38388;&#30340;&#26223;&#35266;&#65292;&#21487;&#20197;&#20351;&#29992;&#22823;&#37096;&#20998;&#25628;&#32034;&#35299;&#25552;&#20379;&#30340;&#20449;&#24687;&#26469;&#26367;&#20195;&#36817;&#20284;&#29468;&#27979;&#25628;&#32034;&#65292;&#23427;&#22312;&#21333;&#30446;&#26631;&#26377;&#32422;&#26463;&#38382;&#39064;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.00043</link><description>&lt;p&gt;
&#31354;&#38388;&#32593;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Space Net Optimization. (arXiv:2306.00043v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00043
&lt;/p&gt;
&lt;p&gt;
SNO&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#20854;&#20351;&#29992;&#31354;&#38388;&#32593;&#26426;&#21046;&#26469;&#32472;&#21046;&#35299;&#31354;&#38388;&#30340;&#26223;&#35266;&#65292;&#21487;&#20197;&#20351;&#29992;&#22823;&#37096;&#20998;&#25628;&#32034;&#35299;&#25552;&#20379;&#30340;&#20449;&#24687;&#26469;&#26367;&#20195;&#36817;&#20284;&#29468;&#27979;&#25628;&#32034;&#65292;&#23427;&#22312;&#21333;&#30446;&#26631;&#26377;&#32422;&#26463;&#38382;&#39064;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#20381;&#36182;&#20110;&#19968;&#20123;&#34987;&#25628;&#32034;&#30340;&#35299;&#26469;&#24341;&#23548;&#21518;&#32493;&#25628;&#32034;&#65292;&#20197;&#22312;&#25910;&#25947;&#36807;&#31243;&#20013;&#20351;&#29992;&#65292;&#20854;&#21407;&#22240;&#24456;&#31616;&#21333;&#65306;&#35745;&#31639;&#26426;&#30340;&#26377;&#38480;&#35745;&#31639;&#36164;&#28304;&#20351;&#24471;&#26080;&#27861;&#20445;&#30041;&#25152;&#26377;&#24050;&#25628;&#32034;&#30340;&#35299;&#12290;&#36825;&#20063;&#25581;&#31034;&#20986;&#22823;&#22810;&#25968;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#27599;&#19968;&#27425;&#25628;&#32034;&#37117;&#20687;&#26159;&#19968;&#20010;&#36817;&#20284;&#30340;&#29468;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65306;&#31354;&#38388;&#32593;&#20248;&#21270;&#65288;SNO&#65289;&#12290;&#23427;&#37197;&#22791;&#20102;&#19968;&#31181;&#31216;&#20026;&#31354;&#38388;&#32593;&#30340;&#26032;&#26426;&#21046;&#65292;&#20174;&#32780;&#20351;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#33021;&#22815;&#20351;&#29992;&#25152;&#26377;&#25628;&#32034;&#35299;&#25552;&#20379;&#30340;&#22823;&#37096;&#20998;&#20449;&#24687;&#26469;&#25551;&#36848;&#35299;&#31354;&#38388;&#30340;&#26223;&#35266;&#12290;&#36890;&#36807;&#31354;&#38388;&#32593;&#65292;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#23601;&#20687;&#22312;&#35299;&#31354;&#38388;&#19978;&#25317;&#26377;&#20102;&#8220;&#35270;&#35273;&#8221;&#12290;&#27169;&#25311;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#35813;&#30740;&#31350;&#20013;&#19982;&#19968;&#32452;&#33879;&#21517;&#30340;&#21333;&#30446;&#26631;&#26377;&#32422;&#26463;&#38382;&#39064;&#30456;&#27604;&#65292;SNO&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#37117;&#20248;&#20110;&#20854;&#20182;&#25152;&#26377;&#27604;&#36739;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most metaheuristic algorithms rely on a few searched solutions to guide later searches during the convergence process for a simple reason: the limited computing resource of a computer makes it impossible to retain all the searched solutions. This also reveals that each search of most metaheuristic algorithms is just like a ballpark guess. To help address this issue, we present a novel metaheuristic algorithm called space net optimization (SNO). It is equipped with a new mechanism called space net; thus, making it possible for a metaheuristic algorithm to use most information provided by all searched solutions to depict the landscape of the solution space. With the space net, a metaheuristic algorithm is kind of like having a ``vision'' on the solution space. Simulation results show that SNO outperforms all the other metaheuristic algorithms compared in this study for a set of well-known single objective bound constrained problems in most cases.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#24037;&#20316;&#27969;&#31243;&#26469;&#35780;&#20272;&#24615;&#33021;&#39044;&#27979;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#19978;&#36827;&#34892;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.00040</link><description>&lt;p&gt;
&#35780;&#20272;&#24615;&#33021;&#39044;&#27979;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;
&lt;/p&gt;
&lt;p&gt;
Assessing the Generalizability of a Performance Predictive Model. (arXiv:2306.00040v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#24037;&#20316;&#27969;&#31243;&#26469;&#35780;&#20272;&#24615;&#33021;&#39044;&#27979;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#19978;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#31639;&#27861;&#36873;&#25321;&#21644;&#37197;&#32622;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#20248;&#31168;&#30340;&#39044;&#27979;&#24615;&#33021;&#27169;&#22411;&#65292;&#32780;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#36825;&#26159;&#36890;&#36807;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#30340;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#38382;&#39064;&#23454;&#20363;&#30340;&#29305;&#24449;&#34920;&#31034;&#20316;&#20026;&#36755;&#20837;&#25968;&#25454;&#65292;&#24182;&#39044;&#27979;&#20854;&#34920;&#29616;&#25152;&#20351;&#29992;&#30340;&#31639;&#27861;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24120;&#35265;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24456;&#38590;&#23545;&#26410;&#34987;&#35757;&#32451;&#25968;&#25454;&#35206;&#30422;&#30340;&#29305;&#24449;&#34920;&#31034;&#36827;&#34892;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#23548;&#33268;&#23545;&#26410;&#35265;&#38382;&#39064;&#30340;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24037;&#20316;&#27969;&#31243;&#65292;&#29992;&#20110;&#35780;&#20272;&#22312;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#19978;&#35757;&#32451;&#30340;&#39044;&#27979;&#24615;&#33021;&#27169;&#22411;&#23545;&#21478;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#30340;&#26222;&#36866;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#20043;&#38388;&#35757;&#32451;&#39044;&#27979;&#24615;&#33021;&#27169;&#22411;&#26469;&#27979;&#35797;&#24037;&#20316;&#27969;&#31243;&#65292;&#24182;&#21457;&#29616;&#26223;&#35266;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#26222;&#36866;&#24615;&#27169;&#24335;&#22312;&#24615;&#33021;&#31354;&#38388;&#20013;&#24471;&#21040;&#20102;&#20307;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key component of automated algorithm selection and configuration, which in most cases are performed using supervised machine learning (ML) methods is a good-performing predictive model. The predictive model uses the feature representation of a set of problem instances as input data and predicts the algorithm performance achieved on them. Common machine learning models struggle to make predictions for instances with feature representations not covered by the training data, resulting in poor generalization to unseen problems. In this study, we propose a workflow to estimate the generalizability of a predictive model for algorithm performance, trained on one benchmark suite to another. The workflow has been tested by training predictive models across benchmark suites and the results show that generalizability patterns in the landscape feature space are reflected in the performance space.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCSD&#30340;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20195;&#30721;&#24322;&#21619;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#35753;&#32452;&#32455;&#21327;&#20316;&#35757;&#32451;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#23454;&#39564;&#20013;&#20998;&#21035;&#20351;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#30340;&#26816;&#27979;&#25928;&#26524;&#65292;&#24182;&#19988;&#27604;&#38598;&#20013;&#24335;&#21644;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.00038</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20195;&#30721;&#24322;&#21619;&#26816;&#27979;&#26041;&#27861;(FedCSD)
&lt;/p&gt;
&lt;p&gt;
FedCSD: A Federated Learning Based Approach for Code-Smell Detection. (arXiv:2306.00038v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCSD&#30340;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20195;&#30721;&#24322;&#21619;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#35753;&#32452;&#32455;&#21327;&#20316;&#35757;&#32451;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#23454;&#39564;&#20013;&#20998;&#21035;&#20351;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#30340;&#26816;&#27979;&#25928;&#26524;&#65292;&#24182;&#19988;&#27604;&#38598;&#20013;&#24335;&#21644;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCSD&#30340;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20195;&#30721;&#24322;&#21619;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#35753;&#32452;&#32455;&#21327;&#20316;&#35757;&#32451;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#19977;&#20010;&#23454;&#39564;&#26469;&#25903;&#25345;&#36825;&#20123;&#26029;&#35328;&#65292;&#36825;&#20123;&#23454;&#39564;&#21033;&#29992;&#20102;&#19977;&#20010;&#25163;&#21160;&#39564;&#35777;&#30340;&#25968;&#25454;&#38598;&#65292;&#26469;&#26816;&#27979;&#21644;&#30740;&#31350;&#19981;&#21516;&#30340;&#20195;&#30721;&#24322;&#21619;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a Federated Learning Code Smell Detection (FedCSD) approach that allows organizations to collaboratively train federated ML models while preserving their data privacy. These assertions have been supported by three experiments that have significantly leveraged three manually validated datasets aimed at detecting and examining different code smell scenarios. In experiment 1, which was concerned with a centralized training experiment, dataset two achieved the lowest accuracy (92.30%) with fewer smells, while datasets one and three achieved the highest accuracy with a slight difference (98.90% and 99.5%, respectively). This was followed by experiment 2, which was concerned with cross-evaluation, where each ML model was trained using one dataset, which was then evaluated over the other two datasets. Results from this experiment show a significant drop in the model's accuracy (lowest accuracy: 63.80\%) where fewer smells exist in the training dataset, which has a noticeab
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#31216;&#24615;&#25628;&#32034;&#30340;&#26426;&#22120;&#20154;&#35774;&#35745;&#26041;&#27861;&#65292;&#20351;&#29992;&#20108;&#38754;&#20307;&#32676;&#30340;&#23376;&#32676;&#34920;&#31034;&#23545;&#31216;&#24615;&#65292;&#24182;&#22312;&#32467;&#26500;&#21270;&#23376;&#32676;&#20013;&#25628;&#32034;&#26368;&#20248;&#23545;&#31216;&#24615;&#65292;&#24182;&#22312;&#27492;&#23545;&#31216;&#24615;&#19979;&#23436;&#25104;&#26426;&#22120;&#20154;&#35774;&#35745;&#12290;&#29702;&#35770;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#33021;&#22312;&#20445;&#25345;&#35774;&#35745;&#31354;&#38388;&#30340;&#21516;&#26102;&#65292;&#35774;&#35745;&#20986;&#39640;&#25928;&#30340;&#23545;&#31216;&#26426;&#22120;&#20154;&#65292;&#23454;&#35777;&#35780;&#20272;&#20063;&#34920;&#26126;&#20854;&#20855;&#26377;&#21331;&#36234;&#30340;&#25928;&#29575;&#21644;&#36890;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.00036</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#31216;&#32467;&#26500;&#23376;&#32676;&#30340;&#26426;&#22120;&#20154;&#35774;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Symmetry-Aware Robot Design with Structured Subgroups. (arXiv:2306.00036v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00036
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#31216;&#24615;&#25628;&#32034;&#30340;&#26426;&#22120;&#20154;&#35774;&#35745;&#26041;&#27861;&#65292;&#20351;&#29992;&#20108;&#38754;&#20307;&#32676;&#30340;&#23376;&#32676;&#34920;&#31034;&#23545;&#31216;&#24615;&#65292;&#24182;&#22312;&#32467;&#26500;&#21270;&#23376;&#32676;&#20013;&#25628;&#32034;&#26368;&#20248;&#23545;&#31216;&#24615;&#65292;&#24182;&#22312;&#27492;&#23545;&#31216;&#24615;&#19979;&#23436;&#25104;&#26426;&#22120;&#20154;&#35774;&#35745;&#12290;&#29702;&#35770;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#33021;&#22312;&#20445;&#25345;&#35774;&#35745;&#31354;&#38388;&#30340;&#21516;&#26102;&#65292;&#35774;&#35745;&#20986;&#39640;&#25928;&#30340;&#23545;&#31216;&#26426;&#22120;&#20154;&#65292;&#23454;&#35777;&#35780;&#20272;&#20063;&#34920;&#26126;&#20854;&#20855;&#26377;&#21331;&#36234;&#30340;&#25928;&#29575;&#21644;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#30340;&#35774;&#35745;&#26088;&#22312;&#23398;&#20064;&#22914;&#20309;&#21019;&#24314;&#26131;&#20110;&#25511;&#21046;&#19988;&#39640;&#25928;&#23436;&#25104;&#20219;&#21153;&#30340;&#26426;&#22120;&#20154;&#12290;&#20197;&#24448;&#30340;&#26426;&#22120;&#20154;&#35774;&#35745;&#30740;&#31350;&#24050;&#35777;&#26126;&#20854;&#33021;&#29983;&#25104;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#30340;&#26426;&#22120;&#20154;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#30452;&#25509;&#20174;&#24222;&#22823;&#30340;&#35774;&#35745;&#31354;&#38388;&#20013;&#25628;&#32034;&#26426;&#22120;&#20154;&#65292;&#32780;&#24573;&#30053;&#20102;&#20849;&#21516;&#30340;&#32467;&#26500;&#65292;&#23548;&#33268;&#26426;&#22120;&#20154;&#24322;&#24120;&#19988;&#24615;&#33021;&#19981;&#20339;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#22522;&#20110;&#23545;&#31216;&#32467;&#26500;&#23376;&#32676;&#30340;&#26426;&#22120;&#20154;&#35774;&#35745;&#8221;&#65288;SARD&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#25226;&#23545;&#31216;&#24615;&#25628;&#32034;&#24341;&#20837;&#26426;&#22120;&#20154;&#35774;&#35745;&#36807;&#31243;&#20013;&#65292;&#21033;&#29992;&#35774;&#35745;&#31354;&#38388;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#20855;&#20307;&#35828;&#65292;&#25105;&#20204;&#29992;&#20108;&#38754;&#20307;&#32676;&#30340;&#23376;&#32676;&#20195;&#34920;&#23545;&#31216;&#24615;&#65292;&#24182;&#22312;&#32467;&#26500;&#21270;&#23376;&#32676;&#20013;&#25628;&#32034;&#26368;&#20248;&#23545;&#31216;&#24615;&#12290;&#28982;&#21518;&#65292;&#22312;&#25628;&#32034;&#21040;&#30340;&#23545;&#31216;&#24615;&#19979;&#23436;&#25104;&#26426;&#22120;&#20154;&#35774;&#35745;&#12290;&#36825;&#26679;&#65292;SARD&#33021;&#22815;&#35774;&#35745;&#20986;&#39640;&#25928;&#30340;&#23545;&#31216;&#26426;&#22120;&#20154;&#65292;&#21516;&#26102;&#35206;&#30422;&#21407;&#22987;&#35774;&#35745;&#31354;&#38388;&#65292;&#29702;&#35770;&#19978;&#26159;&#21487;&#34892;&#30340;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;SARD&#20855;&#26377;&#21331;&#36234;&#30340;&#25928;&#29575;&#21644;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robot design aims at learning to create robots that can be easily controlled and perform tasks efficiently. Previous works on robot design have proven its ability to generate robots for various tasks. However, these works searched the robots directly from the vast design space and ignored common structures, resulting in abnormal robots and poor performance. To tackle this problem, we propose a Symmetry-Aware Robot Design (SARD) framework that exploits the structure of the design space by incorporating symmetry searching into the robot design process. Specifically, we represent symmetries with the subgroups of the dihedral group and search for the optimal symmetry in structured subgroups. Then robots are designed under the searched symmetry. In this way, SARD can design efficient symmetric robots while covering the original design space, which is theoretically analyzed. We further empirically evaluate SARD on various tasks, and the results show its superior efficiency and generalizabili
&lt;/p&gt;</description></item><item><title>CodeTF&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;Transformer&#24211;&#65292;&#25552;&#20379;&#20102;&#21253;&#25324;&#39044;&#35757;&#32451;&#30340;Code LLM&#27169;&#22411;&#21644;&#26631;&#20934;&#21270;&#25509;&#21475;&#31561;&#19968;&#31995;&#21015;&#21151;&#33021;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#23558;&#26368;&#20808;&#36827;&#30340;Code LLM&#27169;&#22411;&#24212;&#29992;&#20110;&#21508;&#31181;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2306.00029</link><description>&lt;p&gt;
CodeTF&#65306;&#19968;&#31449;&#24335;Transformer&#24211;&#65292;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#20195;&#30721;LLM
&lt;/p&gt;
&lt;p&gt;
CodeTF: One-stop Transformer Library for State-of-the-art Code LLM. (arXiv:2306.00029v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00029
&lt;/p&gt;
&lt;p&gt;
CodeTF&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;Transformer&#24211;&#65292;&#25552;&#20379;&#20102;&#21253;&#25324;&#39044;&#35757;&#32451;&#30340;Code LLM&#27169;&#22411;&#21644;&#26631;&#20934;&#21270;&#25509;&#21475;&#31561;&#19968;&#31995;&#21015;&#21151;&#33021;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#23558;&#26368;&#20808;&#36827;&#30340;Code LLM&#27169;&#22411;&#24212;&#29992;&#20110;&#21508;&#31181;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#26234;&#33021;&#22312;&#36716;&#22411;&#29616;&#20195;&#36719;&#20214;&#24037;&#31243;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#21033;&#29992;&#22823;&#37327;&#24320;&#28304;&#20195;&#30721;&#21644;&#32534;&#31243;&#35821;&#35328;&#29305;&#24449;&#30340;Transformer-based&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#24050;&#32463;&#23637;&#31034;&#20986;&#20102;&#23545;&#36825;&#20123;&#20219;&#21153;&#30340;&#26174;&#33879;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#37096;&#32626;&#36890;&#24120;&#38656;&#35201;&#23545;&#26426;&#22120;&#23398;&#20064;&#21644;&#36719;&#20214;&#24037;&#31243;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#20174;&#32780;&#20026;&#27169;&#22411;&#24212;&#29992;&#24102;&#26469;&#20102;&#19968;&#23450;&#30340;&#38556;&#30861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CodeTF&#65292;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#24320;&#25918;&#28304;&#20195;&#30721;&#24211;&#65292;&#29992;&#20110;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;Code LLM&#21644;&#20195;&#30721;&#26234;&#33021;&#12290;&#25105;&#20204;&#37319;&#29992;&#27169;&#22359;&#21270;&#35774;&#35745;&#21644;&#21487;&#25193;&#23637;&#26694;&#26550;&#30340;&#21407;&#21017;&#65292;&#35774;&#35745;CodeTF&#24182;&#25552;&#20379;&#32479;&#19968;&#25509;&#21475;&#65292;&#20197;&#20415;&#24555;&#36895;&#35775;&#38382;&#21644;&#24320;&#21457;&#19981;&#21516;&#31867;&#22411;&#30340;&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#24211;&#25903;&#25345;&#39044;&#35757;&#32451;&#30340;Code LLM&#27169;&#22411;&#21644;&#27969;&#34892;&#30340;&#20195;&#30721;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#26631;&#20934;&#21270;&#25509;&#21475;&#20197;&#26377;&#25928;&#22320;&#35757;&#32451;&#21644;&#26381;&#21153;&#20195;&#30721;LLMs&#65292;&#24182;&#25903;&#25345;&#21452;GPU&#35757;&#32451;&#21644;&#25512;&#29702;&#12290;&#20351;&#29992;CodeTF&#65292;&#29992;&#25143;&#21487;&#20197;&#36731;&#26494;&#23558;&#26368;&#20808;&#36827;&#30340;Code LLM&#27169;&#22411;&#24212;&#29992;&#20110;&#21508;&#31181;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#20013;&#65292;&#20943;&#23569;&#35757;&#32451;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code intelligence plays a key role in transforming modern software engineering. Recently, deep learning-based models, especially Transformer-based large language models (LLMs), have demonstrated remarkable potential in tackling these tasks by leveraging massive open-source code data and programming language features. However, the development and deployment of such models often require expertise in both machine learning and software engineering, creating a barrier for the model adoption. In this paper, we present CodeTF, an open-source Transformer-based library for state-of-the-art Code LLMs and code intelligence. Following the principles of modular design and extensible framework, we design CodeTF with a unified interface to enable rapid access and development across different types of models, datasets and tasks. Our library supports a collection of pretrained Code LLM models and popular code benchmarks, including a standardized interface to train and serve code LLMs efficiently, and d
&lt;/p&gt;</description></item><item><title>&#39046;&#33521;&#20351;&#29992;&#20844;&#24179;&#24615;&#26694;&#26550;&#26469;&#23454;&#29616;AI&#20844;&#24179;&#24615;&#65292;&#35813;&#26694;&#26550;&#23558;AI&#30340;&#20844;&#24179;&#24615;&#20998;&#31163;&#20986;&#20844;&#24179;&#24453;&#36935;&#21644;&#20844;&#24179;&#20135;&#21697;&#26399;&#26395;&#65292;&#24182;&#25552;&#20379;&#20102;&#25805;&#20316;&#25351;&#21335;&#26469;&#23454;&#29616;&#24179;&#31561;AI&#24453;&#36935;&#24182;&#37197;&#21512;&#20135;&#21697;&#24179;&#31561;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.00025</link><description>&lt;p&gt;
&#22312;&#39046;&#33521;&#24179;&#21488;&#19978;&#23454;&#29616;AI&#20844;&#24179;&#24615;&#30340;&#20998;&#31163;&#19982;&#25805;&#20316;&#21270;
&lt;/p&gt;
&lt;p&gt;
Disentangling and Operationalizing AI Fairness at LinkedIn. (arXiv:2306.00025v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00025
&lt;/p&gt;
&lt;p&gt;
&#39046;&#33521;&#20351;&#29992;&#20844;&#24179;&#24615;&#26694;&#26550;&#26469;&#23454;&#29616;AI&#20844;&#24179;&#24615;&#65292;&#35813;&#26694;&#26550;&#23558;AI&#30340;&#20844;&#24179;&#24615;&#20998;&#31163;&#20986;&#20844;&#24179;&#24453;&#36935;&#21644;&#20844;&#24179;&#20135;&#21697;&#26399;&#26395;&#65292;&#24182;&#25552;&#20379;&#20102;&#25805;&#20316;&#25351;&#21335;&#26469;&#23454;&#29616;&#24179;&#31561;AI&#24453;&#36935;&#24182;&#37197;&#21512;&#20135;&#21697;&#24179;&#31561;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#39046;&#33521;&#24179;&#21488;&#19978;&#23454;&#29616;&#35268;&#27169;&#21270;&#30340;AI&#20844;&#24179;&#24615;&#19981;&#20165;&#20855;&#26377;&#22810;&#20010;&#30456;&#20114;&#20914;&#31361;&#30340;&#20844;&#24179;&#23450;&#20041;&#65292;&#32780;&#19988;&#30830;&#23450;&#20844;&#24179;&#24615;&#36824;&#21462;&#20915;&#20110;AI&#23454;&#29616;&#30340;&#20135;&#21697;&#30340;&#20855;&#20307;&#32454;&#33410;&#21644;&#32972;&#26223;&#65292;&#36825;&#23545;&#20110;AI&#20174;&#19994;&#20154;&#21592;&#30340;&#20844;&#24179;&#24615;&#26399;&#26395;&#20540;&#20063;&#38656;&#35201;&#26126;&#30830;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#39046;&#33521;&#27491;&#22312;&#20351;&#29992;&#30340;AI&#20844;&#24179;&#24615;&#26694;&#26550;&#26469;&#24212;&#23545;&#36825;&#19977;&#20010;&#25361;&#25112;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#21306;&#20998;&#24179;&#31561;&#23545;&#24453;&#21644;&#20844;&#24179;&#20135;&#21697;&#26399;&#26395;&#65292;&#20174;&#32780;&#20998;&#31163;&#20102;AI&#30340;&#20844;&#24179;&#24615;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#26126;&#30830;&#30340;&#25351;&#23548;&#26041;&#38024;&#26469;&#25805;&#20316;&#21270;&#23454;&#29616;&#24179;&#31561;&#30340;AI&#24453;&#36935;&#65292;&#37197;&#21512;&#20135;&#21697;&#24179;&#31561;&#30340;&#31574;&#30053;&#65292;&#32780;&#19981;&#26159;&#22312;&#36825;&#20004;&#31181;&#24120;&#35265;&#30340;&#20844;&#24179;&#24615;&#35299;&#37322;&#20043;&#38388;&#24378;&#21046;&#36827;&#34892;&#26435;&#34913;&#12290;&#26412;&#25991;&#37325;&#28857;&#20171;&#32461;&#20102;&#39046;&#33521;AI&#20844;&#24179;&#24615;&#26694;&#26550;&#20013;&#30340;&#24179;&#31561;AI&#24453;&#36935;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#20998;&#20139;&#20102;&#25903;&#25345;&#36825;&#20123;&#21407;&#21017;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Operationalizing AI fairness at LinkedIn's scale is challenging not only because there are multiple mutually incompatible definitions of fairness but also because determining what is fair depends on the specifics and context of the product where AI is deployed. Moreover, AI practitioners need clarity on what fairness expectations need to be addressed at the AI level. In this paper, we present the evolving AI fairness framework used at LinkedIn to address these three challenges. The framework disentangles AI fairness by separating out equal treatment and equitable product expectations. Rather than imposing a trade-off between these two commonly opposing interpretations of fairness, the framework provides clear guidelines for operationalizing equal AI treatment complemented with a product equity strategy. This paper focuses on the equal AI treatment component of LinkedIn's AI fairness framework, shares the principles that support it, and illustrates their application through a case study
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#32654;&#22269;CDC&#30340;&#24515;&#33039;&#30149;&#35843;&#26597;&#36827;&#34892;&#20102;&#20934;&#30830;&#24615;&#35843;&#26597;&#65292;&#30830;&#23450;&#20102;&#26368;&#30456;&#20851;&#30340;&#38382;&#39064;&#23376;&#38598;&#65292;&#24182;&#35777;&#26126;&#20351;&#29992;&#26377;&#38480;&#30340;&#38382;&#39064;&#38598;&#20173;&#28982;&#21487;&#20197;&#20445;&#25345;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#35843;&#26597;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.00023</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#39044;&#27979;&#24515;&#33039;&#30149;&#24182;&#20943;&#23569;&#35843;&#26597;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;
Predicting Heart Disease and Reducing Survey Time Using Machine Learning Algorithms. (arXiv:2306.00023v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#32654;&#22269;CDC&#30340;&#24515;&#33039;&#30149;&#35843;&#26597;&#36827;&#34892;&#20102;&#20934;&#30830;&#24615;&#35843;&#26597;&#65292;&#30830;&#23450;&#20102;&#26368;&#30456;&#20851;&#30340;&#38382;&#39064;&#23376;&#38598;&#65292;&#24182;&#35777;&#26126;&#20351;&#29992;&#26377;&#38480;&#30340;&#38382;&#39064;&#38598;&#20173;&#28982;&#21487;&#20197;&#20445;&#25345;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#35843;&#26597;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#21644;&#20998;&#26512;&#24072;&#27491;&#33268;&#21147;&#20110;&#25913;&#21892;&#21508;&#31181;&#30142;&#30149;&#30340;&#21307;&#23398;&#35786;&#26029;&#12290;&#24515;&#33039;&#30149;&#26159;&#19968;&#31181;&#24120;&#35265;&#30142;&#30149;&#65292;&#21487;&#35270;&#20026;&#20840;&#29699;&#37325;&#35201;&#27515;&#22240;&#12290;&#26089;&#26399;&#21457;&#29616;&#24515;&#33039;&#30149;&#26377;&#21161;&#20110;&#26174;&#33879;&#38477;&#20302;&#24515;&#21147;&#34928;&#31469;&#30340;&#39118;&#38505;&#12290;&#22240;&#27492;&#65292;&#30142;&#30149;&#25511;&#21046;&#21644;&#39044;&#38450;&#20013;&#24515;&#27599;&#24180;&#20174;400,000&#22810;&#21517;&#21442;&#19982;&#32773;&#20013;&#36827;&#34892;&#19982;&#20581;&#24247;&#30456;&#20851;&#30340;&#30005;&#35805;&#35843;&#26597;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#23545;&#20110;&#39044;&#27979;&#24515;&#33039;&#30149;&#30340;&#25968;&#25454;&#21487;&#38752;&#24615;&#20197;&#21450;&#26159;&#21542;&#25152;&#26377;&#35843;&#26597;&#38382;&#39064;&#37117;&#20855;&#26377;&#24378;&#30456;&#20851;&#24615;&#25552;&#20986;&#20102;&#19968;&#20123;&#25285;&#24551;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#36923;&#36753;&#22238;&#24402;&#31561;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20197;&#35843;&#26597;&#32654;&#22269;&#30142;&#30149;&#25511;&#21046;&#21644;&#39044;&#38450;&#20013;&#24515;&#30340;&#24515;&#33039;&#30149;&#35843;&#26597;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#26469;&#30830;&#23450;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#24515;&#33039;&#30149;&#30340;&#26368;&#30456;&#20851;&#30340;&#38382;&#39064;&#23376;&#38598;&#12290;&#20026;&#20102;&#24471;&#20986;&#26377;&#21147;&#30340;&#32467;&#35770;&#65292;&#25105;&#20204;&#22312;&#20840;&#22269;&#20581;&#24247;&#21644;&#33829;&#20859;&#35843;&#26597;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#24320;&#21457;&#30340;&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#26377;&#38480;&#30340;&#38382;&#39064;&#38598;&#21487;&#20197;&#26174;&#33879;&#32553;&#30701;&#35843;&#26597;&#26102;&#38388;&#65292;&#21516;&#26102;&#20173;&#28982;&#20445;&#25345;&#39640;&#30340;&#24515;&#33039;&#30149;&#39044;&#27979;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Currently, many researchers and analysts are working toward medical diagnosis enhancement for various diseases. Heart disease is one of the common diseases that can be considered a significant cause of mortality worldwide. Early detection of heart disease significantly helps in reducing the risk of heart failure. Consequently, the Centers for Disease Control and Prevention (CDC) conducts a health-related telephone survey yearly from over 400,000 participants. However, several concerns arise regarding the reliability of the data in predicting heart disease and whether all of the survey questions are strongly related. This study aims to utilize several machine learning techniques, such as support vector machines and logistic regression, to investigate the accuracy of the CDC's heart disease survey in the United States. Furthermore, we use various feature selection methods to identify the most relevant subset of questions that can be utilized to forecast heart conditions. To reach a robus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#35299;&#37322;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#65292;&#37319;&#29992;&#21452;&#21521;&#21464;&#24418;&#22120;&#27169;&#22411;BERT&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#25552;&#20379;&#21518;&#39564;&#30340;&#27169;&#22411;&#26080;&#20851;&#21644;&#20195;&#29702;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#20197;&#25903;&#25345;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#24182;&#38450;&#27490;&#27169;&#22411;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2306.00021</link><description>&lt;p&gt;
&#37319;&#29992;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#35299;&#37322;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Explaining Hate Speech Classification with Model Agnostic Methods. (arXiv:2306.00021v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35299;&#37322;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#65292;&#37319;&#29992;&#21452;&#21521;&#21464;&#24418;&#22120;&#27169;&#22411;BERT&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#25552;&#20379;&#21518;&#39564;&#30340;&#27169;&#22411;&#26080;&#20851;&#21644;&#20195;&#29702;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#20197;&#25903;&#25345;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#24182;&#38450;&#27490;&#27169;&#22411;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#31361;&#30772;&#12290;&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#23545;&#35805;&#20013;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20063;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20154;&#21592;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#27491;&#22914;&#26368;&#36817;&#30340;&#36235;&#21183;&#25152;&#34920;&#26126;&#30340;&#37027;&#26679;&#65292;&#22312;AI&#27169;&#22411;&#20013;&#21152;&#20837;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#32500;&#24230;&#30340;&#38656;&#27714;&#24050;&#32463;&#24471;&#21040;&#28145;&#21051;&#30340;&#35748;&#35782;&#12290;&#26412;&#25991;&#30340;&#30740;&#31350;&#30446;&#26631;&#26159;&#35201;&#22312;&#20167;&#24680;&#35328;&#35770;&#39044;&#27979;&#19982;&#31995;&#32479;&#29983;&#25104;&#30340;&#35299;&#37322;&#20043;&#38388;&#25645;&#24314;&#26725;&#26753;&#12290;&#36890;&#36807;&#39318;&#20808;&#39044;&#27979;&#25991;&#26412;&#30340;&#20998;&#31867;&#65292;&#28982;&#21518;&#25552;&#20379;&#19968;&#20010;&#21518;&#39564;&#30340;&#27169;&#22411;&#26080;&#20851;&#21644;&#20195;&#29702;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#26469;&#25903;&#25345;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#24182;&#38450;&#27490;&#27169;&#22411;&#20559;&#24046;&#12290;&#37319;&#29992;&#21452;&#21521;&#21464;&#24418;&#22120;&#27169;&#22411;BERT&#36827;&#34892;&#39044;&#27979;&#65292;&#22240;&#20026;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
There have been remarkable breakthroughs in Machine Learning and Artificial Intelligence, notably in the areas of Natural Language Processing and Deep Learning. Additionally, hate speech detection in dialogues has been gaining popularity among Natural Language Processing researchers with the increased use of social media. However, as evidenced by the recent trends, the need for the dimensions of explainability and interpretability in AI models has been deeply realised. Taking note of the factors above, the research goal of this paper is to bridge the gap between hate speech prediction and the explanations generated by the system to support its decision. This has been achieved by first predicting the classification of a text and then providing a posthoc, model agnostic and surrogate interpretability approach for explainability and to prevent model bias. The bidirectional transformer model BERT has been used for prediction because of its state of the art efficiency over other Machine Lea
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#35843;&#26597;&#20102;GPT-4&#30340;&#22320;&#29702;&#30693;&#35782;&#27700;&#24179;&#65292;&#24182;&#25506;&#35752;&#20854;&#22312;&#22320;&#29702;&#25968;&#25454;&#20998;&#26512;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.00020</link><description>&lt;p&gt;
GPT4GEO&#65306;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#30475;&#24453;&#19990;&#30028;&#22320;&#29702;
&lt;/p&gt;
&lt;p&gt;
GPT4GEO: How a Language Model Sees the World's Geography. (arXiv:2306.00020v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00020
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35843;&#26597;&#20102;GPT-4&#30340;&#22320;&#29702;&#30693;&#35782;&#27700;&#24179;&#65292;&#24182;&#25506;&#35752;&#20854;&#22312;&#22320;&#29702;&#25968;&#25454;&#20998;&#26512;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#28041;&#21450;&#38382;&#39064;&#22238;&#31572;&#12289;&#29983;&#25104;&#36830;&#36143;&#25991;&#26412;&#21644;&#20195;&#30721;&#30340;&#24191;&#27867;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#20840;&#38754;&#29702;&#35299;LLM&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#23545;&#20110;&#23433;&#20840;&#12289;&#19979;&#28216;&#24212;&#29992;&#21644;&#24615;&#33021;&#25913;&#36827;&#37117;&#26377;&#30410;&#22788;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;GPT-4&#33719;&#24471;&#20107;&#23454;&#22320;&#29702;&#30693;&#35782;&#30340;&#31243;&#24230;&#65292;&#24182;&#33021;&#21542;&#23558;&#36825;&#20123;&#30693;&#35782;&#29992;&#20110;&#35299;&#37322;&#24615;&#25512;&#29702;&#65292;&#36825;&#23545;&#20110;&#28041;&#21450;&#22320;&#29702;&#25968;&#25454;&#30340;&#24212;&#29992;&#65288;&#22914;&#22320;&#29702;&#31354;&#38388;&#20998;&#26512;&#12289;&#20379;&#24212;&#38142;&#31649;&#29702;&#21644;&#28798;&#38590;&#21709;&#24212;&#65289;&#23588;&#20854;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#22810;&#26679;&#21270;&#30340;&#23454;&#39564;&#65292;&#20174;&#23450;&#20301;&#12289;&#36317;&#31163;&#21644;&#39640;&#24230;&#20272;&#35745;&#31561;&#20107;&#23454;&#20219;&#21153;&#24320;&#22987;&#65292;&#21040;&#29983;&#25104;&#22269;&#23478;&#36718;&#24275;&#21644;&#26053;&#28216;&#32593;&#32476;&#12289;&#22312;&#32422;&#26463;&#26465;&#20214;&#19979;&#23547;&#25214;&#36335;&#32447;&#21644;&#20379;&#24212;&#38142;&#20998;&#26512;&#31561;&#26356;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;GPT-4&#65288;&#27809;&#26377;&#25554;&#20214;&#25110;Internet&#35775;&#38382;&#65289;&#20102;&#35299;&#21644;&#19981;&#20102;&#35299;&#19990;&#30028;&#22320;&#29702;&#30340;&#24191;&#27867;&#25551;&#36848;&#65292;&#24182;&#35752;&#35770;&#20102;&#22312;&#22320;&#29702;&#31354;&#38388;&#39046;&#22495;&#24212;&#29992;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown remarkable capabilities across a broad range of tasks involving question answering and the generation of coherent text and code. Comprehensively understanding the strengths and weaknesses of LLMs is beneficial for safety, downstream applications and improving performance. In this work, we investigate the degree to which GPT-4 has acquired factual geographic knowledge and is capable of using this knowledge for interpretative reasoning, which is especially important for applications that involve geographic data, such as geospatial analysis, supply chain management, and disaster response. To this end, we design and conduct a series of diverse experiments, starting from factual tasks such as location, distance and elevation estimation to more complex questions such as generating country outlines and travel networks, route finding under constraints and supply chain analysis. We provide a broad characterisation of what GPT-4 (without plugins or Interne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#32467;&#21512;&#31526;&#21495;&#34920;&#31034;&#21644;&#33258;&#19979;&#32780;&#19978;&#30340;&#36870;&#21521;&#24037;&#31243;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#30495;&#27491;&#35821;&#35328;&#29702;&#35299;&#19978;&#30340;&#23616;&#38480;&#24615;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#12289;&#35821;&#35328;&#26080;&#20851;&#30340;LLMs&#12290;</title><link>http://arxiv.org/abs/2306.00017</link><description>&lt;p&gt;
&#21521;&#21487;&#35299;&#37322;&#30340;&#12289;&#35821;&#35328;&#26080;&#20851;&#30340;LLMs&#36808;&#36827;&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#31526;&#21495;&#36870;&#21521;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse Engineering of Language at Scale. (arXiv:2306.00017v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#32467;&#21512;&#31526;&#21495;&#34920;&#31034;&#21644;&#33258;&#19979;&#32780;&#19978;&#30340;&#36870;&#21521;&#24037;&#31243;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#30495;&#27491;&#35821;&#35328;&#29702;&#35299;&#19978;&#30340;&#23616;&#38480;&#24615;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#12289;&#35821;&#35328;&#26080;&#20851;&#30340;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#19968;&#20010;&#37324;&#31243;&#30865;&#65292;&#26080;&#21487;&#21542;&#35748;&#22320;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20013;&#35768;&#22810;&#20449;&#20208;&#12290;&#28982;&#32780;&#65292;&#24403;&#28041;&#21450;&#30495;&#27491;&#30340;&#35821;&#35328;&#29702;&#35299;&#26102;&#65292;&#36825;&#20123;LLM&#30340;&#35768;&#22810;&#38480;&#21046;&#20173;&#28982;&#23384;&#22312;&#65292;&#36825;&#20123;&#38480;&#21046;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24213;&#23618;&#26550;&#26500;&#30340;&#21103;&#20135;&#21697;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#20122;&#31526;&#21495;&#24615;&#36136;&#65292;&#36825;&#20123;&#27169;&#22411;&#33719;&#24471;&#26377;&#20851;&#35821;&#35328;&#22914;&#20309;&#36816;&#20316;&#30340;&#20219;&#20309;&#30693;&#35782;&#37117;&#23558;&#34987;&#22475;&#22312;&#25968;&#21313;&#20159;&#20010;&#24494;&#29305;&#24449;&#65288;&#26435;&#37325;&#65289;&#20013;&#65292;&#20854;&#20013;&#27809;&#26377;&#19968;&#20010;&#21333;&#29420;&#30340;&#29305;&#24449;&#26377;&#24847;&#20041;&#65292;&#20351;&#24471;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#31526;&#21495;&#34920;&#31034;&#30340;&#24378;&#24230;&#19982;&#25105;&#20204;&#35748;&#20026;&#26159;LLMs&#25104;&#21151;&#30340;&#20851;&#38190;&#32467;&#21512;&#36215;&#26469;&#65292;&#21363;&#22312;&#35268;&#27169;&#19978;&#25104;&#21151;&#22320;&#36827;&#34892;&#33258;&#19979;&#32780;&#19978;&#30340;&#35821;&#35328;&#36870;&#21521;&#24037;&#31243;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20027;&#24352;&#22312;&#31526;&#21495;&#35774;&#32622;&#19979;&#23545;&#35821;&#35328;&#36827;&#34892;&#33258;&#19979;&#32780;&#19978;&#30340;&#36870;&#21521;&#24037;&#31243;&#12290;&#19968;&#20123;&#20316;&#32773;&#25552;&#20986;&#20102;&#36825;&#20010;&#39033;&#30446;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#23558;&#36827;&#34892;&#35814;&#32454;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved a milestone that undenia-bly changed many held beliefs in artificial intelligence (AI). However, there remains many limitations of these LLMs when it comes to true language understanding, limitations that are a byproduct of the under-lying architecture of deep neural networks. Moreover, and due to their subsymbolic nature, whatever knowledge these models acquire about how language works will always be buried in billions of microfeatures (weights), none of which is meaningful on its own, making such models hopelessly unexplainable. To address these limitations, we suggest com-bining the strength of symbolic representations with what we believe to be the key to the success of LLMs, namely a successful bottom-up re-verse engineering of language at scale. As such we argue for a bottom-up reverse engineering of language in a symbolic setting. Hints on what this project amounts to have been suggested by several authors, and we discuss in some detail
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#21644;&#20808;&#39564;&#20449;&#24565;&#32435;&#20837;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#65288;DCM&#65289;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20013;&#65292;&#20197;&#25193;&#23637;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;DCM&#20013;&#30340;&#28508;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26082;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#21448;&#20445;&#30041;&#20102;DCM&#30340;&#20248;&#28857;&#21644;DNNs&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.00016</link><description>&lt;p&gt;
&#23558;&#39046;&#22495;&#30693;&#35782;&#32435;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#20013;
&lt;/p&gt;
&lt;p&gt;
Incorporating Domain Knowledge in Deep Neural Networks for Discrete Choice Models. (arXiv:2306.00016v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#21644;&#20808;&#39564;&#20449;&#24565;&#32435;&#20837;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#65288;DCM&#65289;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20013;&#65292;&#20197;&#25193;&#23637;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;DCM&#20013;&#30340;&#28508;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26082;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#21448;&#20445;&#30041;&#20102;DCM&#30340;&#20248;&#28857;&#21644;DNNs&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#65288;DCM&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#26053;&#34892;&#38656;&#27714;&#20998;&#26512;&#20013;&#65292;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#29702;&#35770;&#35745;&#37327;&#26694;&#26550;&#26469;&#29702;&#35299;&#21644;&#39044;&#27979;&#36873;&#25321;&#34892;&#20026;&#12290;DCM&#26159;&#38543;&#26426;&#25928;&#29992;&#27169;&#22411;&#65288;RUM&#65289;&#65292;&#20854;&#20027;&#35201;&#20248;&#28857;&#26159;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#20272;&#35745;&#36825;&#20123;&#27169;&#22411;&#30340;&#26680;&#24515;&#35201;&#27714;&#26159;&#20808;&#39564;&#25351;&#23450;&#30456;&#20851;&#30340;&#25928;&#29992;&#20989;&#25968;&#65292;&#20351;&#20854;&#23545;&#27169;&#22411;&#20154;&#21592;&#30340;&#20027;&#35266;&#20449;&#24565;&#25935;&#24863;&#12290;&#26368;&#36817;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#23398;&#20064;DCM&#20013;&#26410;&#35266;&#27979;&#21040;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;ML&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#8220;&#40657;&#30418;&#23376;&#8221;&#65292;&#21487;&#33021;&#19982;&#39044;&#26399;&#30340;&#20851;&#31995;&#19981;&#31526;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#32422;&#26463;&#25903;&#25345;&#24320;&#21457;&#24182;&#23558;&#39046;&#22495;&#30693;&#35782;&#21644;&#20808;&#39564;&#20449;&#24565;&#32435;&#20837;&#21487;&#35299;&#37322;&#27169;&#22411;&#20013;&#65292;&#20197;&#25193;&#23637;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;DCM&#20013;&#30340;&#28508;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21253;&#25324;&#20195;&#34920;&#25152;&#38656;&#20851;&#31995;&#30340;&#20266;&#25968;&#25454;&#26679;&#26412;&#21644;&#23558;DCM&#36716;&#25442;&#20026;&#20855;&#26377;&#36719;&#32422;&#26463;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#12290;&#30001;&#27492;&#24471;&#21040;&#30340;&#27169;&#22411;&#26082;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#21448;&#20445;&#30041;&#20102;RUM&#30340;&#20248;&#28857;&#21644;DNNs&#30340;&#28789;&#27963;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discrete choice models (DCM) are widely employed in travel demand analysis as a powerful theoretical econometric framework for understanding and predicting choice behaviors. DCMs are formed as random utility models (RUM), with their key advantage of interpretability. However, a core requirement for the estimation of these models is a priori specification of the associated utility functions, making them sensitive to modelers' subjective beliefs. Recently, machine learning (ML) approaches have emerged as a promising avenue for learning unobserved non-linear relationships in DCMs. However, ML models are considered "black box" and may not correspond with expected relationships. This paper proposes a framework that expands the potential of data-driven approaches for DCM by supporting the development of interpretable models that incorporate domain knowledge and prior beliefs through constraints. The proposed framework includes pseudo data samples that represent required relationships and a l
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#26399;&#26041;&#27861;GraphCleaner&#65292;&#29992;&#20110;&#22312;&#27969;&#34892;&#30340;&#22270;&#23398;&#20064;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#26816;&#27979;&#21644;&#32416;&#27491;&#38169;&#35823;&#26631;&#27880;&#33410;&#28857;&#12290;GraphCleaner &#32452;&#21512;&#20102;&#21512;&#25104;&#38169;&#35823;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#29983;&#25104;&#21644;&#37051;&#22495;&#24863;&#30693;&#38169;&#35823;&#26631;&#27880;&#26816;&#27979;&#20004;&#31181;&#26032;&#39062;&#24605;&#24819;&#65292;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20854;&#24179;&#22343;F1&#24471;&#20998;&#25552;&#39640;&#20102;0.14&#12290;</title><link>http://arxiv.org/abs/2306.00015</link><description>&lt;p&gt;
GraphCleaner: &#22312;&#27969;&#34892;&#30340;&#22270;&#23398;&#20064;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#26816;&#27979;&#38169;&#35823;&#26631;&#27880;&#30340;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
GraphCleaner: Detecting Mislabelled Samples in Popular Graph Learning Benchmarks. (arXiv:2306.00015v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00015
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#26399;&#26041;&#27861;GraphCleaner&#65292;&#29992;&#20110;&#22312;&#27969;&#34892;&#30340;&#22270;&#23398;&#20064;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#26816;&#27979;&#21644;&#32416;&#27491;&#38169;&#35823;&#26631;&#27880;&#33410;&#28857;&#12290;GraphCleaner &#32452;&#21512;&#20102;&#21512;&#25104;&#38169;&#35823;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#29983;&#25104;&#21644;&#37051;&#22495;&#24863;&#30693;&#38169;&#35823;&#26631;&#27880;&#26816;&#27979;&#20004;&#31181;&#26032;&#39062;&#24605;&#24819;&#65292;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20854;&#24179;&#22343;F1&#24471;&#20998;&#25552;&#39640;&#20102;0.14&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#27969;&#34892;&#30340;&#25991;&#26412;&#65292;&#22270;&#20687;&#21644;&#38899;&#39057;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#26631;&#31614;&#38169;&#35823;&#65292;&#36825;&#20005;&#37325;&#24433;&#21709;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24320;&#21457;&#21644;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#12290;&#23613;&#31649;&#38024;&#23545;&#36890;&#29992;&#25968;&#25454;&#31867;&#22411;&#65288;&#22914;&#22270;&#20687;&#21644;&#25991;&#26412;&#65289;&#30340;&#36136;&#37327;&#25913;&#36827;&#24037;&#20316;&#26377;&#25152;&#22686;&#21152;&#65292;&#20294;&#22312;&#22270;&#25968;&#25454;&#20013;&#26816;&#27979;&#38169;&#35823;&#26631;&#27880;&#30340;&#38382;&#39064;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#12290;&#35813;&#35770;&#25991;&#26088;&#22312;&#25506;&#32034;&#27969;&#34892;&#30340;&#23454;&#38469;&#22270;&#25968;&#25454;&#38598;&#20013;&#30340;&#38169;&#35823;&#26631;&#27880;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;GraphCleaner&#65292;&#19968;&#31181;&#21518;&#26399;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#32416;&#27491;&#36825;&#20123;&#22270;&#25968;&#25454;&#38598;&#20013;&#30340;&#38169;&#35823;&#26631;&#27880;&#33410;&#28857;&#12290;GraphCleaner&#32467;&#21512;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#24605;&#24819;&#65306;1&#65289;&#21512;&#25104;&#38169;&#35823;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#29983;&#25104;&#65292;&#26088;&#22312;&#29983;&#25104;&#36924;&#30495;&#30340;&#38169;&#35823;&#26631;&#27880;&#65307;2&#65289;&#37051;&#22495;&#24863;&#30693;&#38169;&#35823;&#26631;&#27880;&#26816;&#27979;&#65292;&#20854;&#20013;&#21033;&#29992;&#20102;&#26631;&#31614;&#21644;&#22522;&#20998;&#31867;&#22120;&#39044;&#27979;&#20013;&#30340;&#37051;&#22495;&#20381;&#36182;&#24615;&#12290;&#22312;6&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;6&#20010;&#23454;&#39564;&#35774;&#32622;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;GraphCleaner&#20248;&#20110;&#26368;&#25509;&#36817;&#30340;&#22522;&#20934;&#32447;&#65292;&#24182;&#20855;&#26377;&#24179;&#22343;F1&#24471;&#20998;&#25913;&#36827;0.14&#12290;
&lt;/p&gt;
&lt;p&gt;
Label errors have been found to be prevalent in popular text, vision, and audio datasets, which heavily influence the safe development and evaluation of machine learning algorithms. Despite increasing efforts towards improving the quality of generic data types, such as images and texts, the problem of mislabel detection in graph data remains underexplored. To bridge the gap, we explore mislabelling issues in popular real-world graph datasets and propose GraphCleaner, a post-hoc method to detect and correct these mislabelled nodes in graph datasets. GraphCleaner combines the novel ideas of 1) Synthetic Mislabel Dataset Generation, which seeks to generate realistic mislabels; and 2) Neighborhood-Aware Mislabel Detection, where neighborhood dependency is exploited in both labels and base classifier predictions. Empirical evaluations on 6 datasets and 6 experimental settings demonstrate that GraphCleaner outperforms the closest baseline, with an average improvement of 0.14 in F1 score, and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#31354;&#25968;&#25454;&#26041;&#27861;&#21450;&#24212;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;GNN&#30340;&#20998;&#31867;&#21450;&#20854;&#24212;&#29992;&#39046;&#22495;&#12290;&#36825;&#20010;&#32508;&#36848;&#31361;&#20986;&#20102;&#20351;&#29992;GNN&#20998;&#26512;&#26102;&#31354;&#25968;&#25454;&#30340;&#24378;&#22823;&#24037;&#20855;&#21450;&#20854;&#23616;&#38480;&#24615;&#65292;&#24182;&#25351;&#20986;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#21644;&#30446;&#21069;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.00012</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#31354;&#25968;&#25454;&#26041;&#27861;&#21450;&#24212;&#29992;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network for spatiotemporal data: methods and applications. (arXiv:2306.00012v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#31354;&#25968;&#25454;&#26041;&#27861;&#21450;&#24212;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;GNN&#30340;&#20998;&#31867;&#21450;&#20854;&#24212;&#29992;&#39046;&#22495;&#12290;&#36825;&#20010;&#32508;&#36848;&#31361;&#20986;&#20102;&#20351;&#29992;GNN&#20998;&#26512;&#26102;&#31354;&#25968;&#25454;&#30340;&#24378;&#22823;&#24037;&#20855;&#21450;&#20854;&#23616;&#38480;&#24615;&#65292;&#24182;&#25351;&#20986;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#21644;&#30446;&#21069;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#65292;&#34164;&#21547;&#30528;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#30340;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#24613;&#21095;&#22686;&#21152;&#65292;&#20026;&#22825;&#27668;&#39044;&#25253;&#12289;&#33258;&#28982;&#28798;&#23475;&#31649;&#29702;&#12289;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#21644;&#31934;&#20934;&#20892;&#19994;&#31561;&#24212;&#29992;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#35265;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#29992;&#20110;&#23545;&#20855;&#26377;&#20381;&#36182;&#20851;&#31995;&#30340;&#25968;&#25454;&#65288;&#22914;&#31354;&#38388;&#21644;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65289;&#36827;&#34892;&#24314;&#27169;&#21644;&#29702;&#35299;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#24050;&#26377;&#22823;&#37327;&#24037;&#20316;&#33268;&#21147;&#20110;&#21033;&#29992;GNN&#35299;&#20915;&#26102;&#31354;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#31354;&#38388;&#21644;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#26102;&#31354;&#25968;&#25454;&#30340;&#24378;&#36328;&#23398;&#31185;&#24615;&#36136;&#23548;&#33268;&#20247;&#22810;&#29305;&#23450;&#20110;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#30340;GNN&#21464;&#20307;&#30340;&#20986;&#29616;&#12290;&#23613;&#31649;&#36825;&#20123;&#25216;&#26415;&#36890;&#24120;&#36866;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#20851;&#20110;&#26102;&#31354;&#25968;&#25454;GNN&#30340;&#20840;&#38754;&#25991;&#29486;&#32508;&#36848;&#65292;&#22240;&#27492;&#36328;&#39046;&#22495;&#21442;&#32771;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#26159;&#24517;&#35201;&#30340;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#29616;&#26377;&#20851;&#20110;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#31354;&#25968;&#25454;&#30740;&#31350;&#65292;&#24182;&#23545;&#20854;&#20248;&#28857;&#12289;&#23616;&#38480;&#24615;&#21644;&#28508;&#22312;&#24212;&#29992;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;&#25105;&#20204;&#20063;&#25552;&#20379;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#36335;&#32447;&#22270;&#21644;&#24378;&#35843;&#20102;&#19968;&#20123;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of big data, there has been a surge in the availability of data containing rich spatial and temporal information, offering valuable insights into dynamic systems and processes for applications such as weather forecasting, natural disaster management, intelligent transport systems, and precision agriculture. Graph neural networks (GNNs) have emerged as a powerful tool for modeling and understanding data with dependencies to each other such as spatial and temporal dependencies. There is a large amount of existing work that focuses on addressing the complex spatial and temporal dependencies in spatiotemporal data using GNNs. However, the strong interdisciplinary nature of spatiotemporal data has created numerous GNNs variants specifically designed for distinct application domains. Although the techniques are generally applicable across various domains, cross-referencing these methods remains essential yet challenging due to the absence of a comprehensive literature review on GN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31616;&#21333;&#24418;&#24335;&#26144;&#23556;&#31070;&#32463;&#32593;&#32476;&#65288;SMNN&#65289;&#30340;&#35757;&#32451;&#36807;&#31243;&#21644;&#26367;&#20195;&#20984;&#22810;&#38754;&#20307;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#39318;&#27425;&#24341;&#20837;&#20102; SMNN &#30340;&#21487;&#35299;&#37322;&#24615;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.00010</link><description>&lt;p&gt;
&#31616;&#21333;&#24418;&#24335;&#26144;&#23556;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Explainability in Simplicial Map Neural Networks. (arXiv:2306.00010v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31616;&#21333;&#24418;&#24335;&#26144;&#23556;&#31070;&#32463;&#32593;&#32476;&#65288;SMNN&#65289;&#30340;&#35757;&#32451;&#36807;&#31243;&#21644;&#26367;&#20195;&#20984;&#22810;&#38754;&#20307;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#39318;&#27425;&#24341;&#20837;&#20102; SMNN &#30340;&#21487;&#35299;&#37322;&#24615;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31616;&#21333;&#24418;&#24335;&#26144;&#23556;&#31070;&#32463;&#32593;&#32476;&#65288;SMNN&#65289;&#26159;&#22522;&#20110;&#25299;&#25169;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20855;&#26377;&#26222;&#36866;&#36924;&#36817;&#33021;&#21147;&#21644;&#22312;&#36866;&#24403;&#26465;&#20214;&#19979;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#32500;&#20013;&#24212;&#29992; SMNN &#23384;&#22312;&#19968;&#20123;&#29942;&#39048;&#65292;&#39318;&#20808;&#27809;&#26377;&#23450;&#20041; SMNN &#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#20854;&#27425;&#23545;&#20110;&#36755;&#20837;&#25968;&#25454;&#38598;&#38656;&#35201;&#26500;&#24314;&#19968;&#20010;&#21253;&#22260;&#20984;&#22810;&#38754;&#20307;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#32473;&#23450;&#25968;&#25454;&#38598;&#30340;&#25903;&#25345;&#23376;&#38598;&#21644;&#25237;&#24433;&#21040;&#36229;&#29699;&#38754;&#30340;&#26041;&#27861;&#20316;&#20026;&#26367;&#20195;&#20984;&#22810;&#38754;&#20307;&#30340; SMNN &#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#39318;&#27425;&#24341;&#20837;&#20102; SMNN &#30340;&#21487;&#35299;&#37322;&#24615;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simplicial map neural networks (SMNNs) are topology-based neural networks with interesting properties such as universal approximation capability and robustness to adversarial examples under appropriate conditions. However, SMNNs present some bottlenecks for their possible application in high dimensions. First, no SMNN training process has been defined so far. Second, SMNNs require the construction of a convex polytope surrounding the input dataset. In this paper, we propose a SMNN training procedure based on a support subset of the given dataset and a method based on projection to a hypersphere as a replacement for the convex polytope construction. In addition, the explainability capacity of SMNNs is also introduced for the first time in this paper.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22270;&#24418;&#24322;&#24120;&#30417;&#27979;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#19968;&#31867;&#21516;&#22411;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#35780;&#20998;&#24230;&#37327;&#8212;&#8212;&#24403;&#21069;&#33410;&#28857;&#20146;&#21644;&#21147;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#37327;&#36523;&#23450;&#21046;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#25130;&#26029;&#20146;&#21644;&#21147;&#26368;&#22823;&#21270;&#65288;TAM&#65289;&#26041;&#27861;&#65292;&#20248;&#21270;&#22312;&#21407;&#22987;&#22270;&#24418;&#32467;&#26500;&#19978;&#36827;&#34892;&#65292;&#33021;&#22815;&#26377;&#25928;&#36827;&#34892;&#21452;&#37325;One-Class&#30340;GAD&#12290;</title><link>http://arxiv.org/abs/2306.00006</link><description>&lt;p&gt;
&#25130;&#26029;&#20146;&#21644;&#21147;&#26368;&#22823;&#21270;&#65306;&#29992;&#20110;&#22270;&#24418;&#24322;&#24120;&#30417;&#27979;&#30340;&#21333;&#31867;&#21516;&#22411;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Truncated Affinity Maximization: One-class Homophily Modeling for Graph Anomaly Detection. (arXiv:2306.00006v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22270;&#24418;&#24322;&#24120;&#30417;&#27979;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#19968;&#31867;&#21516;&#22411;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#35780;&#20998;&#24230;&#37327;&#8212;&#8212;&#24403;&#21069;&#33410;&#28857;&#20146;&#21644;&#21147;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#37327;&#36523;&#23450;&#21046;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#25130;&#26029;&#20146;&#21644;&#21147;&#26368;&#22823;&#21270;&#65288;TAM&#65289;&#26041;&#27861;&#65292;&#20248;&#21270;&#22312;&#21407;&#22987;&#22270;&#24418;&#32467;&#26500;&#19978;&#36827;&#34892;&#65292;&#33021;&#22815;&#26377;&#25928;&#36827;&#34892;&#21452;&#37325;One-Class&#30340;GAD&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#24418;&#24322;&#24120;&#30417;&#27979;&#65288;GAD&#65289;&#25968;&#25454;&#38598;&#20013;&#32463;&#24120;&#21457;&#29616;&#19968;&#31181;&#26222;&#36941;&#30340;&#23646;&#24615;......&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#35780;&#20998;&#24230;&#37327; - &#24403;&#21069;&#33410;&#28857;&#20146;&#21644;&#21147;......&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#25130;&#26029;&#20146;&#21644;&#21147;&#26368;&#22823;&#21270; (TAM)&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#19982;_neighbors&#30340;&#26412;&#22320;&#20146;&#21644;&#21147;&#26469;&#23398;&#20064;&#37327;&#36523;&#23450;&#21046;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;&#26412;&#25991;&#25152;&#25552;&#26041;&#27861;&#22312;&#21407;&#22987;&#22270;&#24418;&#32467;&#26500;&#19978;&#36827;&#34892;&#20248;&#21270;&#65292;&#21487;&#20197;&#36827;&#34892;&#21452;&#37325;One-Class&#30340;GAD&#12290;
&lt;/p&gt;
&lt;p&gt;
One prevalent property we find empirically in real-world graph anomaly detection (GAD) datasets is a one-class homophily, i.e., normal nodes tend to have strong connection/affinity with each other, while the homophily in abnormal nodes is significantly weaker than normal nodes. However, this anomaly-discriminative property is ignored by existing GAD methods that are typically built using a conventional anomaly detection objective, such as data reconstruction. In this work, we explore this property to introduce a novel unsupervised anomaly scoring measure for GAD -- local node affinity -- that assigns a larger anomaly score to nodes that are less affiliated with their neighbors, with the affinity defined as similarity on node attributes/representations. We further propose Truncated Affinity Maximization (TAM) that learns tailored node representations for our anomaly measure by maximizing the local affinity of nodes to their neighbors. Optimizing on the original graph structure can be bi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30417;&#30563;&#24335;&#27880;&#24847;&#21147;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#20998;&#26512;&#36229;&#22768;&#22270;&#20687;&#65292;&#23454;&#29616;AS&#30340;&#31934;&#30830;&#31579;&#26597;&#19988;&#31934;&#24230;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.00003</link><description>&lt;p&gt;
&#36890;&#36807;&#30417;&#30563;&#24335;&#27880;&#24847;&#21147;&#22810;&#23454;&#20363;&#23398;&#20064;&#20174;&#22810;&#35270;&#35282;&#36229;&#22768;&#22270;&#20687;&#26816;&#27979;&#24515;&#33039;&#30149;
&lt;/p&gt;
&lt;p&gt;
Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning. (arXiv:2306.00003v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30417;&#30563;&#24335;&#27880;&#24847;&#21147;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#20998;&#26512;&#36229;&#22768;&#22270;&#20687;&#65292;&#23454;&#29616;AS&#30340;&#31934;&#30830;&#31579;&#26597;&#19988;&#31934;&#24230;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#33033;&#29923;&#29421;&#31364;(AS)&#26159;&#19968;&#31181;&#23548;&#33268;&#20005;&#37325;&#21457;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#30340;&#36864;&#34892;&#24615;&#29923;&#33180;&#30142;&#30149;&#12290;&#36825;&#31181;&#24773;&#20917;&#32463;&#24120;&#34987;&#20302;&#20272;&#21644;&#20302;&#27835;&#30103;&#12290;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#65292;AS&#26159;&#36890;&#36807;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;&#19987;&#23478;&#23457;&#26597;&#26469;&#35786;&#26029;&#30340;&#65292;&#36825;&#20250;&#20135;&#29983;&#25968;&#21313;&#20010;&#19979;&#32954;&#37319;&#26679;&#30340;&#36229;&#22768;&#22270;&#20687;&#12290;&#21482;&#26377;&#19968;&#20123;&#35270;&#22270;&#26174;&#31034;&#20027;&#21160;&#33033;&#29923;&#12290;&#20026;&#20102;&#33258;&#21160;&#21270;&#31579;&#26597;AS&#65292;&#28145;&#24230;&#32593;&#32476;&#24517;&#39035;&#23398;&#20064;&#27169;&#20223;&#20154;&#31867;&#19987;&#23478;&#35782;&#21035;&#20027;&#21160;&#33033;&#29923;&#35270;&#22270;&#30340;&#33021;&#21147;&#65292;&#28982;&#21518;&#27719;&#24635;&#36825;&#20123;&#30456;&#20851;&#22270;&#20687;&#20197;&#20135;&#29983;&#30740;&#31350;&#32423;&#35786;&#26029;&#12290;&#25105;&#20204;&#21457;&#29616;&#20808;&#21069;&#30340;AS&#26816;&#27979;&#26041;&#27861;&#30001;&#20110;&#20381;&#36182;&#20110;&#36328;&#22270;&#20687;&#30340;&#19981;&#28789;&#27963;&#24179;&#22343;&#20540;&#32780;&#23548;&#33268;&#31934;&#24230;&#19981;&#36275;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#65292;&#29616;&#25104;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22810;&#23454;&#20363;(MIL)&#23398;&#20064;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;MIL&#26041;&#27861;&#65292;&#21253;&#21547;&#20004;&#20010;&#20851;&#38190;&#26041;&#27861;&#21019;&#26032;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#30417;&#30563;&#24335;&#27880;&#24847;&#25216;&#26415;&#65292;&#24341;&#23548;&#23398;&#20064;&#30340;&#27880;&#24847;&#26426;&#21046;&#20559;&#29233;&#30456;&#20851;&#35270;&#22270;&#12290;&#20854;&#27425;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#25552;&#39640;&#20102;&#27599;&#20010;&#21333;&#29420;&#22270;&#20687;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#20010;&#30495;&#23454;&#30340;&#20020;&#24202;&#25968;&#25454;&#38598;&#65288;4569&#21517;&#24739;&#32773;&#65289;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22312;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20043;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aortic stenosis (AS) is a degenerative valve condition that causes substantial morbidity and mortality. This condition is under-diagnosed and under-treated. In clinical practice, AS is diagnosed with expert review of transthoracic echocardiography, which produces dozens of ultrasound images of the heart. Only some of these views show the aortic valve. To automate screening for AS, deep networks must learn to mimic a human expert's ability to identify views of the aortic valve then aggregate across these relevant images to produce a study-level diagnosis. We find previous approaches to AS detection yield insufficient accuracy due to relying on inflexible averages across images. We further find that off-the-shelf attention-based multiple instance learning (MIL) performs poorly. We contribute a new end-to-end MIL approach with two key methodological innovations. First, a supervised attention technique guides the learned attention mechanism to favor relevant views. Second, a novel self-sup
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#19968;&#31867;&#20197;&#20915;&#31574;&#20026;&#23548;&#21521;&#30340;&#20154;&#26426;&#23545;&#35805;&#20219;&#21153;&#65292;&#20197;&#21450;&#22312;&#20250;&#35758;&#35770;&#25991;&#23457;&#31295;&#20154;&#20998;&#37197;&#12289;&#22478;&#24066;&#22810;&#27493;&#34892;&#31243;&#35268;&#21010;&#21644;&#26053;&#34892;&#35745;&#21010;&#21327;&#21830;&#31561;&#22330;&#26223;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#21644;&#29992;&#25143;&#19981;&#21516;&#30340;&#33021;&#21147;&#22914;&#20309;&#32467;&#21512;&#20197;&#36798;&#21040;&#26368;&#20339;&#20915;&#31574;&#12290;&#35770;&#25991;&#36890;&#36807;&#26500;&#24314;&#23545;&#35805;&#29615;&#22659;&#24182;&#36827;&#34892;&#20154;&#26426;&#23545;&#35805;&#25910;&#38598;&#25968;&#25454;&#65292;&#21457;&#29616;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#22312;&#27492;&#31867;&#20219;&#21153;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.20076</link><description>&lt;p&gt;
&#20197;&#20915;&#31574;&#20026;&#23548;&#21521;&#30340;&#20154;&#26426;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
Decision-Oriented Dialogue for Human-AI Collaboration. (arXiv:2305.20076v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.20076
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#19968;&#31867;&#20197;&#20915;&#31574;&#20026;&#23548;&#21521;&#30340;&#20154;&#26426;&#23545;&#35805;&#20219;&#21153;&#65292;&#20197;&#21450;&#22312;&#20250;&#35758;&#35770;&#25991;&#23457;&#31295;&#20154;&#20998;&#37197;&#12289;&#22478;&#24066;&#22810;&#27493;&#34892;&#31243;&#35268;&#21010;&#21644;&#26053;&#34892;&#35745;&#21010;&#21327;&#21830;&#31561;&#22330;&#26223;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#21644;&#29992;&#25143;&#19981;&#21516;&#30340;&#33021;&#21147;&#22914;&#20309;&#32467;&#21512;&#20197;&#36798;&#21040;&#26368;&#20339;&#20915;&#31574;&#12290;&#35770;&#25991;&#36890;&#36807;&#26500;&#24314;&#23545;&#35805;&#29615;&#22659;&#24182;&#36827;&#34892;&#20154;&#26426;&#23545;&#35805;&#25910;&#38598;&#25968;&#25454;&#65292;&#21457;&#29616;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#22312;&#27492;&#31867;&#20219;&#21153;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31867;&#20219;&#21153;&#65292;&#31216;&#20026;&#20197;&#20915;&#31574;&#20026;&#23548;&#21521;&#30340;&#23545;&#35805;&#65292;&#20854;&#20013;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#24517;&#39035;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#19982;&#19968;&#21517;&#25110;&#22810;&#21517;&#20154;&#31867;&#21512;&#20316;&#65292;&#24110;&#21161;&#20182;&#20204;&#20570;&#20986;&#22797;&#26434;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#39046;&#22495;&#20013;&#24418;&#24335;&#21270;&#29992;&#25143;&#38754;&#20020;&#26085;&#24120;&#20915;&#31574;&#30340;&#36807;&#31243;&#65306;&#65288;1&#65289;&#36873;&#25321;&#20250;&#35758;&#35770;&#25991;&#30340;&#23457;&#31295;&#20154;&#20998;&#37197;&#65292;&#65288;2&#65289;&#22312;&#22478;&#24066;&#20013;&#35268;&#21010;&#22810;&#27493;&#34892;&#31243;&#65292;&#20197;&#21450;&#65288;3&#65289;&#20026;&#19968;&#32676;&#26379;&#21451;&#21327;&#21830;&#26053;&#34892;&#35745;&#21010;&#12290;&#22312;&#27599;&#20010;&#35774;&#32622;&#20013;&#65292;AI&#21161;&#25163;&#21644;&#29992;&#25143;&#20855;&#26377;&#19981;&#21516;&#30340;&#33021;&#21147;&#65292;&#20182;&#20204;&#24517;&#39035;&#32467;&#21512;&#36215;&#26469;&#24471;&#20986;&#26368;&#20339;&#20915;&#31574;&#65306;&#21161;&#25163;&#21487;&#20197;&#35775;&#38382;&#21644;&#22788;&#29702;&#22823;&#37327;&#20449;&#24687;&#65292;&#32780;&#29992;&#25143;&#20855;&#26377;&#31995;&#32479;&#22806;&#30340;&#20559;&#22909;&#21644;&#38480;&#21046;&#12290;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#23545;&#35805;&#29615;&#22659;&#65292;&#20854;&#20013;&#20195;&#29702;&#26681;&#25454;&#20182;&#20204;&#36798;&#21040;&#30340;&#26368;&#32456;&#20915;&#31574;&#30340;&#36136;&#37327;&#33719;&#24471;&#22870;&#21169;&#12290;&#20351;&#29992;&#36825;&#20123;&#29615;&#22659;&#65292;&#25105;&#20204;&#19982;&#20154;&#20204;&#25198;&#28436;&#21161;&#25163;&#30340;&#20154;&#36827;&#34892;&#20102;&#20154;&#26426;&#23545;&#35805;&#12290;&#20026;&#20102;&#27604;&#36739;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#30340;&#20132;&#27969;&#26041;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#19982;&#20154;&#31867;-&#20154;&#31867;&#23545;&#35805;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#20915;&#31574;&#23548;&#21521;&#23545;&#35805;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#20984;&#26174;&#20102;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe a class of tasks called decision-oriented dialogues, in which AI assistants must collaborate with one or more humans via natural language to help them make complex decisions. We formalize three domains in which users face everyday decisions: (1) choosing an assignment of reviewers to conference papers, (2) planning a multi-step itinerary in a city, and (3) negotiating travel plans for a group of friends. In each of these settings, AI assistants and users have disparate abilities that they must combine to arrive at the best decision: assistants can access and process large amounts of information, while users have preferences and constraints external to the system. For each task, we build a dialogue environment where agents receive a reward based on the quality of the final decision they reach. Using these environments, we collect human-human dialogues with humans playing the role of assistant. To compare how current AI assistants communicate in these settings, we present bas
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25903;&#25345;&#21453;&#21521;&#20256;&#25773;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#8212;&#8212;&#26463;&#25628;&#32034;&#36882;&#24402;&#21333;&#20803;&#65288;BT-Cell&#65289;&#65292;&#29992;&#20110;&#25193;&#23637;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#23545;&#28508;&#22312;&#32467;&#26500;&#30340;&#24863;&#30693;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#26494;&#26463;&#25628;&#32034;&#20013;&#30828;&#21069;k&#31639;&#23376;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26799;&#24230;&#20449;&#21495;&#20256;&#36882;&#12290;&#22312;&#35780;&#20272;&#20013;&#21457;&#29616;&#65292;BT-Cell&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#30340;&#22810;&#20010;&#20855;&#26377;&#32467;&#26500;&#25935;&#24863;&#24615;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.19999</link><description>&lt;p&gt;
&#26463;&#25628;&#32034;&#36882;&#24402;&#21333;&#20803;&#65306;&#19968;&#31181;&#25903;&#25345;&#21453;&#21521;&#20256;&#25773;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Beam Tree Recursive Cells. (arXiv:2305.19999v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25903;&#25345;&#21453;&#21521;&#20256;&#25773;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#8212;&#8212;&#26463;&#25628;&#32034;&#36882;&#24402;&#21333;&#20803;&#65288;BT-Cell&#65289;&#65292;&#29992;&#20110;&#25193;&#23637;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#23545;&#28508;&#22312;&#32467;&#26500;&#30340;&#24863;&#30693;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#26494;&#26463;&#25628;&#32034;&#20013;&#30828;&#21069;k&#31639;&#23376;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26799;&#24230;&#20449;&#21495;&#20256;&#36882;&#12290;&#22312;&#35780;&#20272;&#20013;&#21457;&#29616;&#65292;BT-Cell&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#30340;&#22810;&#20010;&#20855;&#26377;&#32467;&#26500;&#25935;&#24863;&#24615;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#26463;&#25628;&#32034;&#36882;&#24402;&#21333;&#20803;&#65288;BT-Cell&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25193;&#23637;&#25903;&#25345;&#20351;&#29992;&#26463;&#25628;&#32034;&#36827;&#34892;&#28508;&#22312;&#32467;&#26500;&#24863;&#30693;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RvNN&#65289;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#25552;&#20986;&#22312;&#26463;&#25628;&#32034;&#20013;&#23545;&#30828;&#24615;&#21069;k&#31639;&#23376;&#30340;&#25918;&#26494;&#26469;&#25193;&#23637;&#27492;&#26694;&#26550;&#65292;&#20197;&#26356;&#22909;&#22320;&#20256;&#36882;&#26799;&#24230;&#20449;&#21495;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#30340;&#19981;&#21516;&#20195;&#34920;&#24615;&#20998;&#24067;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;BT-Cell&#22312;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20307;&#29616;&#32467;&#26500;&#25935;&#24863;&#24615;&#30340;&#20219;&#21153;&#65288;&#22914;ListOps&#21644;&#36923;&#36753;&#25512;&#29702;&#65289;&#19978;&#36798;&#21040;&#20102;&#20960;&#20046;&#23436;&#32654;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#23454;&#38469;&#25968;&#25454;&#19978;&#19982;&#20854;&#20182;&#22522;&#20110;RvNN&#30340;&#27169;&#22411;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;ListOps&#20013;&#30830;&#23450;&#20102;&#31070;&#32463;&#27169;&#22411;&#22312;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#21442;&#25968;&#25968;&#37327;&#19978;&#30340;&#26410;&#30693;&#22833;&#25928;&#26696;&#20363;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/JRC1995/BeamTreeRecursiveCells&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Beam Tree Recursive Cell (BT-Cell) - a backpropagation-friendly framework to extend Recursive Neural Networks (RvNNs) with beam search for latent structure induction. We further extend this framework by proposing a relaxation of the hard top-k operators in beam search for better propagation of gradient signals. We evaluate our proposed models in different out-of-distribution splits in both synthetic and realistic data. Our experiments show that BTCell achieves near-perfect performance on several challenging structure-sensitive synthetic tasks like ListOps and logical inference while maintaining comparable performance in realistic data against other RvNN-based models. Additionally, we identify a previously unknown failure case for neural models in generalization to unseen number of arguments in ListOps. The code is available at: https://github.com/JRC1995/BeamTreeRecursiveCells.
&lt;/p&gt;</description></item><item><title>InGram&#26159;&#19968;&#31181;&#26032;&#30340;&#24402;&#32435;&#24335;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#29983;&#25104;&#26032;&#20851;&#31995;&#21644;&#23454;&#20307;&#30340;&#23884;&#20837;&#65292;&#24182;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#27719;&#24635;&#37051;&#23621;&#23884;&#20837;&#29983;&#25104;&#20851;&#31995;&#21644;&#23454;&#20307;&#23884;&#20837;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.19987</link><description>&lt;p&gt;
InGram&#65306;&#36890;&#36807;&#20851;&#31995;&#22270;&#36827;&#34892;&#24402;&#32435;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
InGram: Inductive Knowledge Graph Embedding via Relation Graphs. (arXiv:2305.19987v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19987
&lt;/p&gt;
&lt;p&gt;
InGram&#26159;&#19968;&#31181;&#26032;&#30340;&#24402;&#32435;&#24335;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#29983;&#25104;&#26032;&#20851;&#31995;&#21644;&#23454;&#20307;&#30340;&#23884;&#20837;&#65292;&#24182;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#27719;&#24635;&#37051;&#23621;&#23884;&#20837;&#29983;&#25104;&#20851;&#31995;&#21644;&#23454;&#20307;&#23884;&#20837;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#32435;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#34987;&#35270;&#20026;&#39044;&#27979;&#35757;&#32451;&#26399;&#38388;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#23454;&#20307;&#20043;&#38388;&#30340;&#32570;&#22833;&#19977;&#20803;&#32452;&#30340;&#20219;&#21153;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;InGram&#65292;&#23427;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#29983;&#25104;&#26032;&#20851;&#31995;&#21644;&#23454;&#20307;&#30340;&#23884;&#20837;&#65292;&#24182;&#22522;&#20110;&#20851;&#31995;&#22270;&#21644;&#21407;&#22987;&#30693;&#35782;&#22270;&#35889;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#27719;&#24635;&#37051;&#23621;&#23884;&#20837;&#20197;&#29983;&#25104;&#20851;&#31995;&#21644;&#23454;&#20307;&#23884;&#20837;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;InGram&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inductive knowledge graph completion has been considered as the task of predicting missing triplets between new entities that are not observed during training. While most inductive knowledge graph completion methods assume that all entities can be new, they do not allow new relations to appear at inference time. This restriction prohibits the existing methods from appropriately handling real-world knowledge graphs where new entities accompany new relations. In this paper, we propose an INductive knowledge GRAph eMbedding method, InGram, that can generate embeddings of new relations as well as new entities at inference time. Given a knowledge graph, we define a relation graph as a weighted graph consisting of relations and the affinity weights between them. Based on the relation graph and the original knowledge graph, InGram learns how to aggregate neighboring embeddings to generate relation and entity embeddings using an attention mechanism. Experimental results show that InGram outper
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#21028;&#21035;&#24335;LLMs&#21644;&#29983;&#25104;&#24335;LLMs&#20004;&#31181;&#27169;&#22411;&#33539;&#24335;&#65292;&#24635;&#32467;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24378;&#35843;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.19860</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Large Language Models for Recommendation. (arXiv:2305.19860v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#21028;&#21035;&#24335;LLMs&#21644;&#29983;&#25104;&#24335;LLMs&#20004;&#31181;&#27169;&#22411;&#33539;&#24335;&#65292;&#24635;&#32467;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24378;&#35843;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#24182;&#22312;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#24341;&#36215;&#20102;&#37325;&#35270;&#12290;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#28023;&#37327;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24050;&#22312;&#23398;&#20064;&#36890;&#29992;&#34920;&#31034;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#25104;&#21151;&#65292;&#24182;&#26377;&#21487;&#33021;&#36890;&#36807;&#19968;&#20123;&#26377;&#25928;&#30340;&#36716;&#31227;&#25216;&#26415;&#65288;&#22914;&#24494;&#35843;&#21644;&#25552;&#31034;&#35843;&#25972;&#65289;&#31561;&#25163;&#27573;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#21508;&#20010;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#25512;&#33616;&#36136;&#37327;&#30340;&#20851;&#38190;&#26159;&#21033;&#29992;&#23427;&#20204;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#29305;&#24449;&#34920;&#31034;&#21644;&#22823;&#37327;&#30340;&#22806;&#37096;&#30693;&#35782;&#35206;&#30422;&#65292;&#24314;&#31435;&#39033;&#30446;&#21644;&#29992;&#25143;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#20840;&#38754;&#20102;&#35299;&#29616;&#26377;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#26412;&#32508;&#36848;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#27861;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#20998;&#20026;&#20004;&#31181;&#20027;&#35201;&#33539;&#24335;&#65292;&#20998;&#21035;&#26159;&#21028;&#21035;&#24335;LLMs&#21644;&#29983;&#25104;&#24335;LLMs&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#36825;&#20123;&#33539;&#24335;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#24378;&#35843;&#20102;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#24320;&#25918;&#24615;&#30740;&#31350;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have emerged as powerful tools in the field of Natural Language Processing (NLP) and have recently gained significant attention in the domain of Recommendation Systems (RS). These models, trained on massive amounts of data using self-supervised learning, have demonstrated remarkable success in learning universal representations and have the potential to enhance various aspects of recommendation systems by some effective transfer techniques such as fine-tuning and prompt tuning, and so on. The crucial aspect of harnessing the power of language models in enhancing recommendation quality is the utilization of their high-quality representations of textual features and their extensive coverage of external knowledge to establish correlations between items and users. To provide a comprehensive understanding of the existing LLM-based recommendation systems, this survey presents a taxonomy that categorizes these models into two major paradigms, respectively Discrimi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FineRewards&#65292;&#36890;&#36807;&#24341;&#20837;&#32454;&#31890;&#24230;&#30340;&#35821;&#20041;&#22870;&#21169;&#65292;&#21363;&#26631;&#39064;&#22870;&#21169;&#21644;SAM&#22870;&#21169;&#65292;&#26469;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2305.19599</link><description>&lt;p&gt;
&#32454;&#31890;&#24230;&#35821;&#20041;&#22870;&#21169;&#22686;&#24378;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Boosting Text-to-Image Diffusion Models with Fine-Grained Semantic Rewards. (arXiv:2305.19599v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FineRewards&#65292;&#36890;&#36807;&#24341;&#20837;&#32454;&#31890;&#24230;&#30340;&#35821;&#20041;&#22870;&#21169;&#65292;&#21363;&#26631;&#39064;&#22870;&#21169;&#21644;SAM&#22870;&#21169;&#65292;&#26469;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#30740;&#31350;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#22312;&#32473;&#23450;&#30340;&#25991;&#26412;&#25552;&#31034;&#19979;&#29983;&#25104;&#20102;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#32454;&#31890;&#24230;&#35821;&#20041;&#25351;&#23548;&#65292;&#20197;&#25104;&#21151;&#35786;&#26029;&#24418;&#24577;&#24046;&#24322;&#20026;&#27490;&#65292;&#20197;&#21069;&#30340;&#26041;&#27861;&#26080;&#27861;&#25191;&#34892;&#25991;&#26412;&#27010;&#24565;&#21644;&#29983;&#25104;&#30340;&#22270;&#20687;&#20043;&#38388;&#30340;&#20934;&#30830;&#24418;&#24577;&#23545;&#40784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FineRewards&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#26032;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#22870;&#21169;--&#26631;&#39064;&#22870;&#21169;&#21644;&#35821;&#20041;&#20998;&#21106;&#20219;&#20309;&#20107;&#29289;&#65288;SAM&#65289;&#22870;&#21169;&#65292;&#26469;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in text-to-image diffusion models have achieved remarkable success in generating high-quality, realistic images from given text prompts. However, previous methods fail to perform accurate modality alignment between text concepts and generated images due to the lack of fine-level semantic guidance that successfully diagnoses the modality discrepancy. In this paper, we propose FineRewards to improve the alignment between text and images in text-to-image diffusion models by introducing two new fine-grained semantic rewards: the caption reward and the Semantic Segment Anything (SAM) reward. From the global semantic view, the caption reward generates a corresponding detailed caption that depicts all important contents in the synthetic image via a BLIP-2 model and then calculates the reward score by measuring the similarity between the generated caption and the given prompt. From the local semantic view, the SAM reward segments the generated images into local parts with categ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#20869;&#37096;&#20998;&#24067;&#22312;&#32447;&#36866;&#24212;(IDAQ)&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#31574;&#30053;&#21518;&#39564;&#38598;&#21512;&#21644;&#20449;&#24565;&#26356;&#26032;&#32593;&#32476;&#37327;&#21270;&#31574;&#30053;&#19981;&#30830;&#23450;&#24615;&#24182;&#29983;&#25104;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#22788;&#29702;&#26032;&#20219;&#21153;&#65292;&#22312;&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#19978;&#20855;&#26377;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.19529</link><description>&lt;p&gt;
&#24102;&#26377;&#20869;&#37096;&#20998;&#24067;&#22312;&#32447;&#36866;&#24212;&#30340;&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Offline Meta Reinforcement Learning with In-Distribution Online Adaptation. (arXiv:2305.19529v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#20869;&#37096;&#20998;&#24067;&#22312;&#32447;&#36866;&#24212;(IDAQ)&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#31574;&#30053;&#21518;&#39564;&#38598;&#21512;&#21644;&#20449;&#24565;&#26356;&#26032;&#32593;&#32476;&#37327;&#21270;&#31574;&#30053;&#19981;&#30830;&#23450;&#24615;&#24182;&#29983;&#25104;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#22788;&#29702;&#26032;&#20219;&#21153;&#65292;&#22312;&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#19978;&#20855;&#26377;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#20219;&#21153;&#30456;&#20851;&#30340;&#34892;&#20026;&#31574;&#30053;(&#20363;&#22914;&#65292;&#23545;&#27599;&#20010;&#20010;&#20307;&#20219;&#21153;&#36827;&#34892;RL&#26234;&#33021;&#20307;&#30340;&#35757;&#32451;)&#26469;&#25910;&#38598;&#22810;&#20219;&#21153;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24635;&#26159;&#38656;&#35201;&#39069;&#22806;&#30340;&#20449;&#24687;&#36827;&#34892;&#24555;&#36895;&#35843;&#25972;&#65292;&#20363;&#22914;&#27979;&#35797;&#20219;&#21153;&#30340;&#31163;&#32447;&#19978;&#19979;&#25991;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#27491;&#24335;&#22320;&#34920;&#24449;&#20102;&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#29420;&#29305;&#25361;&#25112;&#65306;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#22312;&#32447;&#36866;&#24212;&#20043;&#38388;&#30340;&#36716;&#25442;-&#22870;&#21169;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#65292;&#26469;&#33258;&#20998;&#24067;&#20043;&#22806;&#30340;&#36866;&#24212;&#24773;&#20917;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#21487;&#38752;&#30340;&#31574;&#30053;&#35780;&#20272;&#65292;&#24182;&#19988;&#20351;&#29992;&#20998;&#24067;&#20869;&#30340;&#24773;&#20917;&#36827;&#34892;&#22312;&#32447;&#36866;&#24212;&#21487;&#20197;&#30830;&#20445;&#36866;&#24212;&#24615;&#33021;&#20445;&#35777;&#12290;&#22522;&#20110;&#36825;&#20123;&#29702;&#35770;&#27934;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#24212;&#26694;&#26550;&#65292;&#31216;&#20026;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#20869;&#37096;&#20998;&#24067;&#22312;&#32447;&#36866;&#24212;(IDAQ)&#65292;&#23427;&#21033;&#29992;&#31574;&#30053;&#21518;&#39564;&#38598;&#21512;&#21644;&#20449;&#24565;&#26356;&#26032;&#32593;&#32476;&#37327;&#21270;&#31574;&#30053;&#19981;&#30830;&#23450;&#24615;&#24182;&#29983;&#25104;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#22788;&#29702;&#26032;&#20219;&#21153;&#12290; &#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;IDAQ&#30340;&#25928;&#26524;&#20248;&#20110;&#29616;&#26377;&#30340;&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#19988;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#22312;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent offline meta-reinforcement learning (meta-RL) methods typically utilize task-dependent behavior policies (e.g., training RL agents on each individual task) to collect a multi-task dataset. However, these methods always require extra information for fast adaptation, such as offline context for testing tasks. To address this problem, we first formally characterize a unique challenge in offline meta-RL: transition-reward distribution shift between offline datasets and online adaptation. Our theory finds that out-of-distribution adaptation episodes may lead to unreliable policy evaluation and that online adaptation with in-distribution episodes can ensure adaptation performance guarantee. Based on these theoretical insights, we propose a novel adaptation framework, called In-Distribution online Adaptation with uncertainty Quantification (IDAQ), which generates in-distribution context using a given uncertainty quantification and performs effective task belief inference to address new
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;$K^2$-&#26641;&#30340;&#22270;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#32039;&#20945;&#29983;&#25104;&#65292;&#24182;&#21516;&#26102;&#25429;&#33719;&#22270;&#30340;&#20869;&#22312;&#20998;&#23618;&#32467;&#26500;&#12290;&#36890;&#36807;&#25552;&#20986;&#39034;&#24207;$K^2$-&#26641;&#34920;&#31034;&#21644;&#24341;&#20837;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#26412;&#25991;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.19125</link><description>&lt;p&gt;
&#22522;&#20110;$K^2$-&#26641;&#30340;&#20998;&#32423;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Graph Generation with $K^2$-trees. (arXiv:2305.19125v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;$K^2$-&#26641;&#30340;&#22270;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#32039;&#20945;&#29983;&#25104;&#65292;&#24182;&#21516;&#26102;&#25429;&#33719;&#22270;&#30340;&#20869;&#22312;&#20998;&#23618;&#32467;&#26500;&#12290;&#36890;&#36807;&#25552;&#20986;&#39034;&#24207;$K^2$-&#26641;&#34920;&#31034;&#21644;&#24341;&#20837;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#26412;&#25991;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#30446;&#26631;&#20998;&#24067;&#29983;&#25104;&#22270;&#26159;&#35768;&#22810;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#21253;&#25324;&#33647;&#29289;&#21457;&#29616;&#21644;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#21407;&#26412;&#35774;&#35745;&#29992;&#20110;&#26080;&#25439;&#22270;&#21387;&#32553;&#30340;$K^2$-&#26641;&#34920;&#31034;&#30340;&#26032;&#39062;&#22270;&#29983;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#21160;&#26426;&#28304;&#20110;$K^2$-&#26641;&#33021;&#22815;&#22312;&#36827;&#34892;&#32039;&#20945;&#29983;&#25104;&#30340;&#21516;&#26102;&#65292;&#25429;&#33719;&#22270;&#30340;&#20869;&#22312;&#20998;&#23618;&#32467;&#26500;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;(1)&#25552;&#20986;&#20102;&#19968;&#31181;&#21253;&#21547;&#21098;&#26525;&#12289;&#25153;&#24179;&#21270;&#21644;&#35760;&#21495;&#21270;&#36807;&#31243;&#30340;&#39034;&#24207;K2&#26641;&#34920;&#31034;&#21644;(2)&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#26088;&#22312;&#36890;&#36807;&#32467;&#21512;&#19987;&#19994;&#26641;&#24418;&#20301;&#32622;&#32534;&#30721;&#26041;&#26696;&#26469;&#29983;&#25104;&#24207;&#21015;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#22235;&#20010;&#24120;&#35268;&#21644;&#20004;&#20010;&#20998;&#23376;&#22270;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#20197;&#35777;&#23454;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22270;&#29983;&#25104;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating graphs from a target distribution is a significant challenge across many domains, including drug discovery and social network analysis. In this work, we introduce a novel graph generation method leveraging $K^2$-tree representation which was originally designed for lossless graph compression. Our motivation stems from the ability of the $K^2$-trees to enable compact generation while concurrently capturing the inherent hierarchical structure of a graph. In addition, we make further contributions by (1) presenting a sequential $K^2$-tree representation that incorporates pruning, flattening, and tokenization processes and (2) introducing a Transformer-based architecture designed to generate the sequence by incorporating a specialized tree positional encoding scheme. Finally, we extensively evaluate our algorithm on four general and two molecular graph datasets to confirm its superiority for graph generation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#21333;&#19968;&#32508;&#21512;&#38450;&#27450;&#39575;&#35828;&#35805;&#20154;&#39564;&#35777;&#23884;&#20837;&#65292;&#22312;&#27450;&#39575;&#21644;&#38750;&#30446;&#26631;&#35828;&#35805;&#20154;&#36755;&#20837;&#26041;&#38754;&#36798;&#21040;&#21331;&#36234;&#24615;&#33021;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#23884;&#20837;&#26041;&#26696;&#65292;&#35813;&#23884;&#20837;&#36890;&#36807;&#22810;&#38454;&#27573;&#35757;&#32451;&#21644;&#22797;&#21046;&#21512;&#25104;&#25216;&#26415;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#22312; SASV2022 &#25361;&#25112;&#21327;&#35758;&#20013;&#21462;&#24471;&#20102;1.06%&#30340;SASV-EER&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.19051</link><description>&lt;p&gt;
&#21521;&#21333;&#19968;&#32508;&#21512;&#38450;&#27450;&#39575;&#35828;&#35805;&#20154;&#39564;&#35777;&#23884;&#20837;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Towards single integrated spoofing-aware speaker verification embeddings. (arXiv:2305.19051v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#21333;&#19968;&#32508;&#21512;&#38450;&#27450;&#39575;&#35828;&#35805;&#20154;&#39564;&#35777;&#23884;&#20837;&#65292;&#22312;&#27450;&#39575;&#21644;&#38750;&#30446;&#26631;&#35828;&#35805;&#20154;&#36755;&#20837;&#26041;&#38754;&#36798;&#21040;&#21331;&#36234;&#24615;&#33021;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#23884;&#20837;&#26041;&#26696;&#65292;&#35813;&#23884;&#20837;&#36890;&#36807;&#22810;&#38454;&#27573;&#35757;&#32451;&#21644;&#22797;&#21046;&#21512;&#25104;&#25216;&#26415;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#22312; SASV2022 &#25361;&#25112;&#21327;&#35758;&#20013;&#21462;&#24471;&#20102;1.06%&#30340;SASV-EER&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#21333;&#19968;&#32508;&#21512;&#38450;&#27450;&#39575;&#35828;&#35805;&#20154;&#39564;&#35777;&#23884;&#20837;&#65292;&#28385;&#36275;&#20197;&#19979;&#20004;&#20010;&#26041;&#38754;&#30340;&#35201;&#27714;&#65306;&#39318;&#20808;&#65292;&#38656;&#35201;&#25298;&#32477;&#38750;&#30446;&#26631;&#35828;&#35805;&#20154;&#30340;&#36755;&#20837;&#20197;&#21450;&#30446;&#26631;&#35828;&#35805;&#20154;&#30340;&#27450;&#39575;&#36755;&#20837;&#12290;&#20854;&#27425;&#65292;&#38656;&#35201;&#35777;&#26126;&#20854;&#19982;&#33258;&#21160;&#35828;&#35805;&#20154;&#39564;&#35777; (ASV) &#21644;&#21453;&#27450;&#39575;&#23884;&#20837; (CM) &#34701;&#21512;&#30340;&#24615;&#33021;&#30456;&#24403;&#65292;&#32780;&#21518;&#32773;&#22312; SASV2022 &#25361;&#25112;&#20013;&#34920;&#29616;&#36828;&#22909;&#20110;&#21333;&#19968;&#23884;&#20837;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#20998;&#26512;&#35748;&#20026;&#21333;&#19968; SASV &#23884;&#20837;&#30340;&#21155;&#36136;&#24615;&#33021;&#26469;&#33258;&#20110;&#35757;&#32451;&#25968;&#25454;&#37327;&#19981;&#36275;&#21644; ASV &#19982; CM &#20219;&#21153;&#30340;&#19981;&#21516;&#24615;&#36136;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#22810;&#38454;&#27573;&#35757;&#32451;&#21644;&#25439;&#22833;&#20989;&#25968;&#30340;&#32452;&#21512;&#12290;&#21516;&#26102;&#21033;&#29992; Copy synthesis &#21644;&#20960;&#20010;&#22768;&#30721;&#22120;&#35299;&#20915;&#27450;&#39575;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#35780;&#20272; SASV2022 &#25361;&#25112;&#21327;&#35758;&#26102;&#65292;&#36798;&#21040;&#20102; SASV-EER 1.06%&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aims to develop a single integrated spoofing-aware speaker verification (SASV) embeddings that satisfy two aspects. First, rejecting non-target speakers' input as well as target speakers' spoofed inputs should be addressed. Second, competitive performance should be demonstrated compared to the fusion of automatic speaker verification (ASV) and countermeasure (CM) embeddings, which outperformed single embedding solutions by a large margin in the SASV2022 challenge. We analyze that the inferior performance of single SASV embeddings comes from insufficient amount of training data and distinct nature of ASV and CM tasks. To this end, we propose a novel framework that includes multi-stage training and a combination of loss functions. Copy synthesis, combined with several vocoders, is also exploited to address the lack of spoofed data. Experimental results show dramatic improvements, achieving a SASV-EER of 1.06% on the evaluation protocol of the SASV2022 challenge.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#20934;&#21017;&#25512;&#33616;&#30340;&#26631;&#20934;&#20559;&#22909;&#24863;&#30693;&#36731;&#37327;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;MC&#25193;&#23637;&#22270;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#25429;&#25417;&#29992;&#25143;&#30340;&#26631;&#20934;&#20559;&#22909;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#29992;&#25143;&#23545;&#21508;&#20010;&#26631;&#20934;&#30340;&#20559;&#22909;&#21512;&#24182;&#21040;&#26368;&#32456;&#30340;&#25512;&#33616;&#21015;&#34920;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.18885</link><description>&lt;p&gt;
&#26631;&#20934;&#27604;&#35780;&#20998;&#26356;&#37325;&#35201;&#65306;&#38754;&#21521;&#22810;&#20934;&#21017;&#25512;&#33616;&#30340;&#26631;&#20934;&#20559;&#22909;&#24863;&#30693;&#36731;&#37327;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Criteria Tell You More than Ratings: Criteria Preference-Aware Light Graph Convolution for Effective Multi-Criteria Recommendation. (arXiv:2305.18885v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#20934;&#21017;&#25512;&#33616;&#30340;&#26631;&#20934;&#20559;&#22909;&#24863;&#30693;&#36731;&#37327;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;MC&#25193;&#23637;&#22270;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#25429;&#25417;&#29992;&#25143;&#30340;&#26631;&#20934;&#20559;&#22909;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#29992;&#25143;&#23545;&#21508;&#20010;&#26631;&#20934;&#30340;&#20559;&#22909;&#21512;&#24182;&#21040;&#26368;&#32456;&#30340;&#25512;&#33616;&#21015;&#34920;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20934;&#21017;&#25512;&#33616;&#31995;&#32479;&#29616;&#22312;&#22312;&#24191;&#27867;&#30340;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#20013;&#21033;&#29992;&#22810;&#20934;&#21017; (MC) &#35780;&#20998;&#20449;&#24687;&#65292;&#32780;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476; (GNN) &#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#25512;&#33616;&#31995;&#32479;&#30340;&#24320;&#21457;&#20013;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;GNN&#36741;&#21161;&#35774;&#35745;MC&#25512;&#33616;&#31995;&#32479;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26631;&#20934;&#20559;&#22909;&#24863;&#30693;&#36731;&#37327;&#22270;&#21367;&#31215;&#26041;&#27861;(CPA-LGC),&#21487;&#20197;&#20934;&#30830;&#25429;&#25417;&#29992;&#25143;&#30340;&#26631;&#20934;&#20559;&#22909;&#20197;&#21450;&#22797;&#26434;&#39640;&#38454;&#36830;&#25509;&#20013;&#30340;&#21327;&#20316;&#20449;&#21495;&#12290;&#26412;&#25991;&#22312;MC&#25193;&#23637;&#22270;&#19978;&#26500;&#24314;&#20102;&#19968;&#20010;&#33021;&#22815;&#23558;&#29992;&#25143;-&#29289;&#21697;MC&#35780;&#20998;&#36716;&#25442;&#20026;&#25193;&#23637;&#20108;&#20998;&#22270;&#30340;MC&#25193;&#23637;&#22270;&#65292;&#20877;&#36827;&#19968;&#27493;&#23558;&#26631;&#20934;&#37325;&#35201;&#24615;&#32534;&#30721;&#21040;&#22270;&#21367;&#31215;&#36807;&#31243;&#20013;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#20934;&#20559;&#22909;&#24863;&#30693;&#32858;&#21512;&#26041;&#27861;&#26469;&#23558;&#29992;&#25143;&#23545;&#19981;&#21516;&#26631;&#20934;&#30340;&#20559;&#22909;&#21512;&#24182;&#21040;&#26368;&#32456;&#30340;&#25512;&#33616;&#21015;&#34920;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The multi-criteria (MC) recommender system, which leverages MC rating information in a wide range of e-commerce areas, is ubiquitous nowadays. Surprisingly, although graph neural networks (GNNs) have been widely applied to develop various recommender systems due to GNN's high expressive capability in learning graph representations, it has been still unexplored how to design MC recommender systems with GNNs. In light of this, we make the first attempt towards designing a GNN-aided MC recommender system. Specifically, rather than straightforwardly adopting existing GNN-based recommendation methods, we devise a novel criteria preference-aware light graph convolution CPA-LGC method, which is capable of precisely capturing the criteria preference of users as well as the collaborative signal in complex high-order connectivities. To this end, we first construct an MC expansion graph that transforms user--item MC ratings into an expanded bipartite graph to potentially learn from the collaborat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#31561;&#21464;&#22270;Few-shot&#23398;&#20064;&#65288;TEG&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31561;&#21464;&#24615;&#36136;&#26469;&#20351;&#27169;&#22411;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#20219;&#21153;&#36866;&#24212;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;Few-shot&#20998;&#31867;&#22522;&#20934;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18758</link><description>&lt;p&gt;
&#20219;&#21153;&#31561;&#21464;&#22270;Few-shot&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Task-Equivariant Graph Few-shot Learning. (arXiv:2305.18758v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#31561;&#21464;&#22270;Few-shot&#23398;&#20064;&#65288;TEG&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31561;&#21464;&#24615;&#36136;&#26469;&#20351;&#27169;&#22411;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#20219;&#21153;&#36866;&#24212;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;Few-shot&#20998;&#31867;&#22522;&#20934;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20854;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#27599;&#31867;&#20855;&#26377;&#36275;&#22815;&#26631;&#35760;&#33410;&#28857;&#30340;&#21487;&#29992;&#24615;&#12290;&#22312;&#29616;&#23454;&#24773;&#20917;&#19979;&#65292;&#19981;&#26159;&#25152;&#26377;&#31867;&#37117;&#26377;&#35768;&#22810;&#26631;&#35760;&#33410;&#28857;&#65292;&#27169;&#22411;&#21487;&#33021;&#38656;&#35201;&#20998;&#31867;&#26032;&#31867;&#21035;&#65292;&#20351;&#24471;&#25163;&#21160;&#26631;&#35760;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;GNN&#38656;&#35201;&#33021;&#22815;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#26631;&#35760;&#33410;&#28857;&#23545;&#33410;&#28857;&#36827;&#34892;&#20998;&#31867;&#65292;&#31216;&#20026;Few-shot&#33410;&#28857;&#20998;&#31867;&#12290;&#20808;&#21069;&#30340;&#22522;&#20110;&#21095;&#38598;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;Few-shot&#33410;&#28857;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#20165;&#26377;&#22810;&#26679;&#30340;&#35757;&#32451;&#20803;&#20219;&#21153;&#25165;&#33021;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#12290;&#20026;&#20102;&#24212;&#23545;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;Few-shot&#23398;&#20064;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#20219;&#21153;&#31561;&#21464;&#22270;Few-shot&#23398;&#20064;&#65288;TEG&#65289;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;TEG&#26694;&#26550;&#36890;&#36807;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31561;&#21464;&#24615;&#36136;&#26469;&#20351;&#27169;&#22411;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#20219;&#21153;&#36866;&#24212;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;Few-shot&#20998;&#31867;&#22522;&#20934;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Graph Neural Networks (GNNs) have been successful in node classification tasks, their performance heavily relies on the availability of a sufficient number of labeled nodes per class. In real-world situations, not all classes have many labeled nodes and there may be instances where the model needs to classify new classes, making manual labeling difficult. To solve this problem, it is important for GNNs to be able to classify nodes with a limited number of labeled nodes, known as few-shot node classification. Previous episodic meta-learning based methods have demonstrated success in few-shot node classification, but our findings suggest that optimal performance can only be achieved with a substantial amount of diverse training meta-tasks. To address this challenge of meta-learning based few-shot learning (FSL), we propose a new approach, the Task-Equivariant Graph few-shot learning (TEG) framework. Our TEG framework enables the model to learn transferable task-adaptation strate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;GRAM-ODENNs&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#24403;&#21069;&#22270;ODE&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#21253;&#25324;&#23616;&#37096;&#27169;&#24335;&#30340;&#24573;&#30053;&#21644;&#32570;&#20047;&#21160;&#24577;&#35821;&#20041;&#36793;&#32536;&#65292;&#29992;&#20110;&#26356;&#20934;&#30830;&#30340;&#38271;&#31243;&#20132;&#36890;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.18687</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#22810;ODE&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26102;&#31354;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Graph-based Multi-ODE Neural Networks for Spatio-Temporal Traffic Forecasting. (arXiv:2305.18687v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;GRAM-ODENNs&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#24403;&#21069;&#22270;ODE&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#21253;&#25324;&#23616;&#37096;&#27169;&#24335;&#30340;&#24573;&#30053;&#21644;&#32570;&#20047;&#21160;&#24577;&#35821;&#20041;&#36793;&#32536;&#65292;&#29992;&#20110;&#26356;&#20934;&#30830;&#30340;&#38271;&#31243;&#20132;&#36890;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#20132;&#36890;&#39046;&#22495;&#20013;&#20986;&#29616;&#20102;&#35768;&#22810;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20132;&#36890;&#32593;&#32476;&#20013;&#35266;&#23519;&#21040;&#30340;&#22797;&#26434;&#32780;&#24191;&#27867;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#65292;&#38271;&#31243;&#20132;&#36890;&#39044;&#27979;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#30446;&#21069;&#30340;&#20316;&#21697;&#20027;&#35201;&#20381;&#36182;&#20110;&#20855;&#26377;&#22270;&#24418;&#32467;&#26500;&#30340;&#36947;&#36335;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23398;&#20064;&#34920;&#31034;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#22312;&#28145;&#24230;&#26550;&#26500;&#20013;&#23384;&#22312;&#36807;&#24230;&#24179;&#28369;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#20171;&#32461;&#20102;&#23558;GNN&#19982;&#27531;&#24046;&#36830;&#25509;&#25110;&#31070;&#32463;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22270;&#24418;ODE&#27169;&#22411;&#22312;&#29305;&#24449;&#25552;&#21462;&#26041;&#38754;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;&#65288;1&#65289;&#23427;&#20204;&#20542;&#21521;&#20110;&#20840;&#23616;&#26102;&#38388;&#27169;&#24335;&#65292;&#24573;&#30053;&#20102;&#23545;&#20110;&#24847;&#22806;&#20107;&#20214;&#24456;&#37325;&#35201;&#30340;&#23616;&#37096;&#27169;&#24335;&#65307;&#65288;2&#65289;&#23427;&#20204;&#22312;&#20854;&#26550;&#26500;&#35774;&#35745;&#20013;&#32570;&#20047;&#21160;&#24577;&#35821;&#20041;&#36793;&#32536;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22522;&#20110;&#22270;&#30340;&#22810;ODE&#31070;&#32463;&#32593;&#32476;&#65288;GRAM-ODENNs&#65289;&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#24403;&#21069;&#22270;&#24418;ODE&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#12290;GRAM-ODENNs&#20351;&#29992;&#22810;&#20010;&#24182;&#34892;ODE&#32534;&#30721;&#22120;&#21644;&#21160;&#24577;&#22270;&#32467;&#26500;&#65292;&#36866;&#24212;&#20132;&#36890;&#25968;&#25454;&#30340;&#26102;&#38388;&#28436;&#21464;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#38271;&#31243;&#20132;&#36890;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a recent surge in the development of spatio-temporal forecasting models in the transportation domain. Long-range traffic forecasting, however, remains a challenging task due to the intricate and extensive spatio-temporal correlations observed in traffic networks. Current works primarily rely on road networks with graph structures and learn representations using graph neural networks (GNNs), but this approach suffers from over-smoothing problem in deep architectures. To tackle this problem, recent methods introduced the combination of GNNs with residual connections or neural ordinary differential equations (ODE). However, current graph ODE models face two key limitations in feature extraction: (1) they lean towards global temporal patterns, overlooking local patterns that are important for unexpected events; and (2) they lack dynamic semantic edges in their architectural design. In this paper, we propose a novel architecture called Graph-based Multi-ODE Neural Networks (GRAM-OD
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21487;&#25511;&#27585;&#28781;&#36335;&#24452;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#36845;&#20195;&#29983;&#25104;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21521;&#20462;&#22797;&#36712;&#36857;&#30340;&#29366;&#24577;-&#21160;&#20316;&#23545;&#28155;&#21152;&#26465;&#20214;&#36755;&#20837;&#26469;&#23454;&#29616;&#21487;&#25511;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18553</link><description>&lt;p&gt;
&#21487;&#25511;&#27585;&#28781;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Controllable Path of Destruction. (arXiv:2305.18553v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21487;&#25511;&#27585;&#28781;&#36335;&#24452;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#36845;&#20195;&#29983;&#25104;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21521;&#20462;&#22797;&#36712;&#36857;&#30340;&#29366;&#24577;-&#21160;&#20316;&#23545;&#28155;&#21152;&#26465;&#20214;&#36755;&#20837;&#26469;&#23454;&#29616;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27585;&#28781;&#36335;&#24452;&#65288;PoD&#65289;&#26159;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#36845;&#20195;&#29983;&#25104;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#30772;&#22351;&#19968;&#32452;&#29289;&#21697;&#26469;&#20135;&#29983;&#19968;&#20010;&#35757;&#32451;&#38598;&#65292;&#20026;&#27599;&#20010;&#30772;&#22351;&#27493;&#39588;&#21019;&#24314;&#19968;&#20010;&#19982;&#30456;&#24212;&#20462;&#22797;&#21160;&#20316;&#30456;&#20851;&#30340;&#35757;&#32451;&#23454;&#20363;&#12290;&#22312;&#27492;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#29983;&#25104;&#22120;&#21487;&#20197;&#36890;&#36807;&#20174;&#20219;&#24847;&#29366;&#24577;&#8220;&#20462;&#22797;&#8221;&#26469;&#29983;&#25104;&#26032;&#30340;&#29289;&#21697;&#12290;PoD&#26041;&#27861;&#22312;&#21407;&#22987;&#35757;&#32451;&#31034;&#20363;&#26041;&#38754;&#38750;&#24120;&#33410;&#30465;&#65292;&#24182;&#19988;&#38750;&#24120;&#36866;&#21512;&#30001;&#20998;&#31867;&#25968;&#25454;&#32452;&#25104;&#30340;&#21151;&#33021;&#37096;&#20214;&#65292;&#20363;&#22914;&#28216;&#25103;&#20851;&#21345;&#21644;&#31163;&#25955;&#30340;3D&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#27585;&#28781;&#36335;&#24452;&#26041;&#27861;&#25193;&#23637;&#21040;&#20801;&#35768;&#35774;&#35745;&#24072;&#25511;&#21046;&#29983;&#25104;&#30340;&#29289;&#21697;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#36890;&#36807;&#21521;&#26500;&#25104;&#20462;&#22797;&#36712;&#36857;&#30340;&#29366;&#24577;-&#21160;&#20316;&#23545;&#28155;&#21152;&#26465;&#20214;&#36755;&#20837;&#26469;&#24341;&#20837;&#21487;&#25511;&#24615;&#12290;&#25105;&#20204;&#22312;2D&#22320;&#29282;&#35774;&#32622;&#20197;&#21450;&#23567;&#22411;3D&#20048;&#39640;&#27773;&#36710;&#39046;&#22495;&#27979;&#35797;&#20102;&#21487;&#25511;PoD&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Path of Destruction (PoD) is a self-supervised method for learning iterative generators. The core idea is to produce a training set by destroying a set of artifacts, and for each destructive step create a training instance based on the corresponding repair action. A generator trained on this dataset can then generate new artifacts by ``repairing'' from arbitrary states. The PoD method is very data-efficient in terms of original training examples and well-suited to functional artifacts composed of categorical data, such as game levels and discrete 3D structures. In this paper, we extend the Path of Destruction method to allow designer control over aspects of the generated artifacts. Controllability is introduced by adding conditional inputs to the state-action pairs that make up the repair trajectories. We test the controllable PoD method in a 2D dungeon setting, as well as in the domain of small 3D Lego cars.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35843;&#25972;&#21487;&#20449;&#32852;&#37030;&#23398;&#20064;&#20445;&#25252;&#26426;&#21046;&#30340;&#21442;&#25968;&#65292;&#20197;&#22312;&#38544;&#31169;&#27844;&#38706;&#12289;&#25928;&#29992;&#25439;&#22833;&#21644;&#25928;&#29575;&#38477;&#20302;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.18400</link><description>&lt;p&gt;
&#19968;&#31181;&#20803;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#35843;&#25972;&#21487;&#20449;&#32852;&#37030;&#23398;&#20064;&#20445;&#25252;&#26426;&#21046;&#30340;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
A Meta-learning Framework for Tuning Parameters of Protection Mechanisms in Trustworthy Federated Learning. (arXiv:2305.18400v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18400
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35843;&#25972;&#21487;&#20449;&#32852;&#37030;&#23398;&#20064;&#20445;&#25252;&#26426;&#21046;&#30340;&#21442;&#25968;&#65292;&#20197;&#22312;&#38544;&#31169;&#27844;&#38706;&#12289;&#25928;&#29992;&#25439;&#22833;&#21644;&#25928;&#29575;&#38477;&#20302;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20449;&#32852;&#37030;&#23398;&#20064;&#65288;TFL&#65289;&#36890;&#24120;&#21033;&#29992;&#20445;&#25252;&#26426;&#21046;&#26469;&#20445;&#35777;&#38544;&#31169;&#23433;&#20840;&#12290;&#28982;&#32780;&#65292;&#20445;&#25252;&#26426;&#21046;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#24341;&#20837;&#25928;&#29992;&#25439;&#22833;&#25110;&#25928;&#29575;&#38477;&#20302;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#22240;&#27492;&#65292;&#20445;&#25252;&#26426;&#21046;&#21450;&#20854;&#21442;&#25968;&#24212;&#35813;&#20180;&#32454;&#36873;&#25321;&#65292;&#20197;&#22312;&#20445;&#25252;&#38544;&#31169;&#27844;&#38706;&#12289;&#25928;&#29992;&#25439;&#22833;&#21644;&#25928;&#29575;&#38477;&#20302;&#20043;&#38388;&#21462;&#24471;&#26368;&#20339;&#24179;&#34913;&#12290;&#20026;&#27492;&#65292;&#32852;&#37030;&#23398;&#20064;&#20174;&#19994;&#32773;&#38656;&#35201;&#24037;&#20855;&#26469;&#34913;&#37327;&#36825;&#19977;&#20010;&#22240;&#32032;&#65292;&#24182;&#20248;&#21270;&#23427;&#20204;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#36873;&#25321;&#26368;&#36866;&#21512;&#25163;&#22836;&#24212;&#29992;&#30340;&#20445;&#25252;&#26426;&#21046;&#12290;&#22522;&#20110;&#36825;&#20010;&#35201;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23427;(1)&#23558;TFL&#23450;&#20041;&#20026;&#25214;&#21040;&#20445;&#25252;&#26426;&#21046;&#26469;&#20248;&#21270;&#38544;&#31169;&#27844;&#38706;&#12289;&#25928;&#29992;&#25439;&#22833;&#21644;&#25928;&#29575;&#38477;&#20302;&#19977;&#32773;&#20043;&#38388;&#30340;&#26435;&#34913;&#30340;&#38382;&#39064;&#65307;(2)&#27491;&#24335;&#23450;&#20041;&#20102;&#36825;&#19977;&#20010;&#22240;&#32032;&#30340;&#26377;&#30028;&#27979;&#37327;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#31639;&#27861;&#26469;&#36817;&#20284;&#35299;&#20915;&#27492;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trustworthy Federated Learning (TFL) typically leverages protection mechanisms to guarantee privacy. However, protection mechanisms inevitably introduce utility loss or efficiency reduction while protecting data privacy. Therefore, protection mechanisms and their parameters should be carefully chosen to strike an optimal tradeoff between \textit{privacy leakage}, \textit{utility loss}, and \textit{efficiency reduction}. To this end, federated learning practitioners need tools to measure the three factors and optimize the tradeoff between them to choose the protection mechanism that is most appropriate to the application at hand. Motivated by this requirement, we propose a framework that (1) formulates TFL as a problem of finding a protection mechanism to optimize the tradeoff between privacy leakage, utility loss, and efficiency reduction and (2) formally defines bounded measurements of the three factors. We then propose a meta-learning algorithm to approximate this optimization proble
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415;&#30340;&#21487;&#35270;&#21270;&#32534;&#31243;&#20219;&#21153;&#21512;&#25104;&#26041;&#27861;NeurTaskSyn&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#38024;&#23545;&#35268;&#33539;&#20013;&#32473;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20195;&#30721;&#25152;&#38656;&#35201;&#30340;&#32534;&#31243;&#27010;&#24565;&#21644;&#23545;&#21487;&#35270;&#21270;&#20219;&#21153;&#30340;&#38480;&#21046;&#65292;&#33258;&#21160;&#29983;&#25104;&#32534;&#31243;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.18342</link><description>&lt;p&gt;
&#21487;&#35270;&#21270;&#32534;&#31243;&#20013;&#31070;&#32463;&#20219;&#21153;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Neural Task Synthesis for Visual Programming. (arXiv:2305.18342v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18342
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415;&#30340;&#21487;&#35270;&#21270;&#32534;&#31243;&#20219;&#21153;&#21512;&#25104;&#26041;&#27861;NeurTaskSyn&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#38024;&#23545;&#35268;&#33539;&#20013;&#32473;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20195;&#30721;&#25152;&#38656;&#35201;&#30340;&#32534;&#31243;&#27010;&#24565;&#21644;&#23545;&#21487;&#35270;&#21270;&#20219;&#21153;&#30340;&#38480;&#21046;&#65292;&#33258;&#21160;&#29983;&#25104;&#32534;&#31243;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21512;&#25104;&#26032;&#30340;&#20869;&#23481;&#65292;&#29983;&#25104;&#24335;&#31070;&#32463;&#27169;&#22411;&#22312;&#22686;&#24378;&#32534;&#31243;&#25945;&#32946;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#26088;&#22312;&#35774;&#35745;&#31070;&#32463;&#27169;&#22411;&#65292;&#33021;&#22815;&#26681;&#25454;&#21487;&#35270;&#21270;&#32534;&#31243;&#29615;&#22659;&#19979;&#32473;&#23450;&#30340;&#35268;&#33539;&#33258;&#21160;&#29983;&#25104;&#32534;&#31243;&#20219;&#21153;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#20687; GPT-4 &#36825;&#26679;&#30340;&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#33719;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#21512;&#25104;&#21487;&#35270;&#21270;&#32534;&#31243;&#20219;&#21153;&#26041;&#38754;&#25928;&#26524;&#19981;&#20339;&#65292;&#24182;&#19988;&#22312;&#36923;&#36753;&#21644;&#31354;&#38388;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415; NeurTaskSyn&#65292;&#35813;&#25216;&#26415;&#33021;&#22815;&#38024;&#23545;&#35268;&#33539;&#20013;&#32473;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20195;&#30721;&#25152;&#38656;&#35201;&#30340;&#32534;&#31243;&#27010;&#24565;&#21644;&#23545;&#21487;&#35270;&#21270;&#20219;&#21153;&#30340;&#38480;&#21046;&#65292;&#21512;&#25104;&#32534;&#31243;&#20219;&#21153;&#12290;NeurTaskSyn &#30001;&#20004;&#20010;&#37096;&#20998;&#26500;&#25104;&#65306;&#31532;&#19968;&#20010;&#37096;&#20998;&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#31243;&#24207;&#36827;&#34892;&#35757;&#32451;&#65292;&#29983;&#25104;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#20195;&#30721;&#65292;&#31532;&#20108;&#20010;&#37096;&#20998;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31243;&#24207;&#36827;&#34892;&#35757;&#32451;&#65292;&#25351;&#23548;&#24213;&#23618;&#31526;&#21495;&#25191;&#34892;&#24341;&#25806;&#29983;&#25104;&#21487;&#35270;&#21270;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative neural models hold great promise in enhancing programming education by synthesizing new content for students. We seek to design neural models that can automatically generate programming tasks for a given specification in the context of visual programming domains. Despite the recent successes of large generative models like GPT-4, our initial results show that these models are ineffective in synthesizing visual programming tasks and struggle with logical and spatial reasoning. We propose a novel neuro-symbolic technique, NeurTaskSyn, that can synthesize programming tasks for a specification given in the form of desired programming concepts exercised by its solution code and constraints on the visual task. NeurTaskSyn has two components: the first component is trained via imitation learning procedure to generate possible solution codes, and the second component is trained via reinforcement learning procedure to guide an underlying symbolic execution engine that generates visua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HyNT&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#36229;&#20851;&#31995;&#22411;&#30693;&#35782;&#22270;&#30340;&#34920;&#31034;&#65292;&#21253;&#25324;&#25968;&#20540;&#25991;&#23383;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#19978;&#19979;&#25991;Transformer&#21644;&#39044;&#27979;Transformer&#65292;&#36890;&#36807;&#23398;&#20064;&#19977;&#20803;&#32452;&#21644;&#20854;&#38480;&#23450;&#35789;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20197;&#21450;&#25968;&#20540;&#20449;&#24687;&#26469;&#33719;&#24471;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.18256</link><description>&lt;p&gt;
&#29992;Transformer&#23398;&#20064;&#36229;&#20851;&#31995;&#22411;&#21644;&#25968;&#20540;&#30693;&#35782;&#22270;&#20013;&#30340;&#34920;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning on Hyper-Relational and Numeric Knowledge Graphs with Transformers. (arXiv:2305.18256v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HyNT&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#36229;&#20851;&#31995;&#22411;&#30693;&#35782;&#22270;&#30340;&#34920;&#31034;&#65292;&#21253;&#25324;&#25968;&#20540;&#25991;&#23383;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#19978;&#19979;&#25991;Transformer&#21644;&#39044;&#27979;Transformer&#65292;&#36890;&#36807;&#23398;&#20064;&#19977;&#20803;&#32452;&#21644;&#20854;&#38480;&#23450;&#35789;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20197;&#21450;&#25968;&#20540;&#20449;&#24687;&#26469;&#33719;&#24471;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#20102;&#19968;&#20010;&#36229;&#20851;&#31995;&#22411;&#30693;&#35782;&#22270;&#35889;&#65292;&#20854;&#20013;&#19977;&#20803;&#32452;&#19982;&#38480;&#23450;&#35789;&#38598;&#21512;&#30456;&#20851;&#32852;; &#19968;&#20010;&#38480;&#23450;&#35789;&#30001;&#20851;&#31995;&#21644;&#23454;&#20307;&#32452;&#25104;&#65292;&#20026;&#19977;&#20803;&#32452;&#25552;&#20379;&#36741;&#21161;&#20449;&#24687;&#12290;&#29616;&#26377;&#30340;&#36229;&#20851;&#31995;&#22411;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#20551;&#23450;&#23454;&#20307;&#26159;&#31163;&#25955;&#23545;&#35937;&#65292;&#20294;&#26377;&#20123;&#20449;&#24687;&#24212;&#20351;&#29992;&#25968;&#20540;&#34920;&#31034;&#65292;&#20363;&#22914;(J.R.R.&#65292;&#20986;&#29983;&#20110;&#65292;1892)&#12290;&#21516;&#26102;&#65292;&#19977;&#20803;&#32452;(J.R.R.&#65292;&#23601;&#35835;&#20110;&#65292;&#29275;&#27941;&#22823;&#23398;)&#21487;&#20197;&#19982;&#38480;&#23450;&#35789;(&#24320;&#22987;&#26102;&#38388;&#65292;1911)&#30456;&#20851;&#32852;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HyNT&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#21253;&#21547;&#19977;&#20803;&#32452;&#25110;&#38480;&#23450;&#35789;&#20013;&#25968;&#20540;&#25991;&#23383;&#30340;&#36229;&#20851;&#31995;&#22411;&#30693;&#35782;&#22270;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;Transformer&#21644;&#19968;&#20010;&#39044;&#27979;Transformer&#65292;&#26469;&#23398;&#20064;&#34920;&#31034;&#65292;&#19981;&#20165;&#22522;&#20110;&#19977;&#20803;&#32452;&#21644;&#20854;&#38480;&#23450;&#35789;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#36824;&#22522;&#20110;&#25968;&#20540;&#20449;&#24687;&#12290;&#36890;&#36807;&#23398;&#20064;&#19977;&#20803;&#32452;&#21644;&#38480;&#23450;&#35789;&#30340;&#32039;&#20945;&#34920;&#31034;&#65292;&#24182;&#23558;&#23427;&#20204;&#39304;&#36865;&#32473;Transformer&#26469;&#33719;&#24471;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A hyper-relational knowledge graph has been recently studied where a triplet is associated with a set of qualifiers; a qualifier is composed of a relation and an entity, providing auxiliary information for a triplet. While existing hyper-relational knowledge graph embedding methods assume that the entities are discrete objects, some information should be represented using numeric values, e.g., (J.R.R., was born in, 1892). Also, a triplet (J.R.R., educated at, Oxford Univ.) can be associated with a qualifier such as (start time, 1911). In this paper, we propose a unified framework named HyNT that learns representations of a hyper-relational knowledge graph containing numeric literals in either triplets or qualifiers. We define a context transformer and a prediction transformer to learn the representations based not only on the correlations between a triplet and its qualifiers but also on the numeric information. By learning compact representations of triplets and qualifiers and feeding 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35270;&#38169;&#35273;&#30340;&#29420;&#29305;&#25968;&#25454;&#38598;InDL&#65292;&#29992;&#20110;&#27979;&#35797;&#21644;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22270;&#20013;&#36923;&#36753;&#35299;&#37322;&#33021;&#21147;&#12290;&#21033;&#29992;&#20960;&#20309;&#20809;&#23398;&#35270;&#38169;&#35273;&#65292;&#24314;&#31435;&#21487;&#27604;&#24615;&#26694;&#26550;&#29992;&#20110;&#38416;&#26126;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#32570;&#38519;&#21644;&#25552;&#20379;&#25913;&#36827;&#27169;&#22411;&#30340;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.17716</link><description>&lt;p&gt;
InDL: &#22522;&#20110;&#35270;&#38169;&#35273;&#30340;&#22270;&#20013;&#36923;&#36753;&#35299;&#37322;&#26032;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
InDL: A New Datasets and Benchmark for In-Diagram Logic Interpreting based on Visual Illusion. (arXiv:2305.17716v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35270;&#38169;&#35273;&#30340;&#29420;&#29305;&#25968;&#25454;&#38598;InDL&#65292;&#29992;&#20110;&#27979;&#35797;&#21644;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22270;&#20013;&#36923;&#36753;&#35299;&#37322;&#33021;&#21147;&#12290;&#21033;&#29992;&#20960;&#20309;&#20809;&#23398;&#35270;&#38169;&#35273;&#65292;&#24314;&#31435;&#21487;&#27604;&#24615;&#26694;&#26550;&#29992;&#20110;&#38416;&#26126;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#32570;&#38519;&#21644;&#25552;&#20379;&#25913;&#36827;&#27169;&#22411;&#30340;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22270;&#20013;&#36923;&#36753;&#35299;&#37322;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#26377;&#36259;&#39046;&#22495;&#30340;&#35270;&#38169;&#35273;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#25968;&#25454;&#38598;InDL&#65292;&#26088;&#22312;&#20005;&#26684;&#27979;&#35797;&#21644;&#22522;&#20934;&#36825;&#20123;&#27169;&#22411;&#12290;&#25105;&#20204;&#21033;&#29992;&#20845;&#20010;&#32463;&#20856;&#30340;&#20960;&#20309;&#35270;&#35273;&#38169;&#35273;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#20154;&#31867;&#21644;&#26426;&#22120;&#35270;&#35273;&#24863;&#30693;&#30340;&#21487;&#27604;&#24615;&#26694;&#26550;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#37327;&#21270;&#30340;&#34913;&#37327;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#38416;&#26126;&#20102;&#21487;&#33021;&#23384;&#22312;&#30340;&#32570;&#38519;&#65292;&#24182;&#25552;&#20379;&#20102;&#25913;&#36827;&#27169;&#22411;&#30340;&#34892;&#21160;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel approach to evaluating deep learning models' capacity for in-diagram logic interpretation. Leveraging the intriguing realm of visual illusions, we establish a unique dataset, InDL, designed to rigorously test and benchmark these models. Deep learning has witnessed remarkable progress in domains such as computer vision and natural language processing. However, models often stumble in tasks requiring logical reasoning due to their inherent 'black box' characteristics, which obscure the decision-making process. Our work presents a new lens to understand these models better by focusing on their handling of visual illusions -- a complex interplay of perception and logic. We utilize six classic geometric optical illusions to create a comparative framework between human and machine visual perception. This methodology offers a quantifiable measure to rank models, elucidating potential weaknesses and providing actionable insights for model improvements. Our experim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#30142;&#30149;&#24739;&#32773;&#20010;&#20307;&#30340;&#26681;&#26412;&#21407;&#22240;&#30340;&#26032;&#20844;&#24335;&#65292;&#21487;&#20197;&#29992;&#20110;&#33258;&#21160;&#20174;&#25968;&#25454;&#20013;&#26816;&#27979;&#26681;&#26412;&#21407;&#22240;&#65292;&#24182;&#32771;&#34385;&#20102;&#22122;&#22768;&#26631;&#31614;&#21644;&#30142;&#30149;&#27969;&#34892;&#29575;&#31561;&#22240;&#32032;&#65292;&#21516;&#26102;&#20855;&#26377;&#24555;&#36895;&#35745;&#31639;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.17574</link><description>&lt;p&gt;
&#30142;&#30149;&#24739;&#32773;&#20010;&#20307;&#26681;&#26412;&#21407;&#22240;&#30340;&#21453;&#20107;&#23454;&#20844;&#24335;&#21270;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Formulation of Patient-Specific Root Causes of Disease. (arXiv:2305.17574v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#30142;&#30149;&#24739;&#32773;&#20010;&#20307;&#30340;&#26681;&#26412;&#21407;&#22240;&#30340;&#26032;&#20844;&#24335;&#65292;&#21487;&#20197;&#29992;&#20110;&#33258;&#21160;&#20174;&#25968;&#25454;&#20013;&#26816;&#27979;&#26681;&#26412;&#21407;&#22240;&#65292;&#24182;&#32771;&#34385;&#20102;&#22122;&#22768;&#26631;&#31614;&#21644;&#30142;&#30149;&#27969;&#34892;&#29575;&#31561;&#22240;&#32032;&#65292;&#21516;&#26102;&#20855;&#26377;&#24555;&#36895;&#35745;&#31639;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30142;&#30149;&#30340;&#26681;&#26412;&#21407;&#22240;&#30452;&#35266;&#22320;&#23545;&#24212;&#20110;&#22686;&#21152;&#35786;&#26029;&#21487;&#33021;&#24615;&#30340;&#26681;&#26412;&#39030;&#28857;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26681;&#26412;&#21407;&#22240;&#30340;&#25551;&#36848;&#32570;&#20047;&#35745;&#31639;&#26426;&#31639;&#27861;&#21457;&#23637;&#25152;&#38656;&#30340;&#20005;&#26684;&#25968;&#23398;&#20844;&#24335;&#12290;&#22312;&#20197;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#20351;&#29992;&#24178;&#39044;&#20027;&#20041;&#32773;&#24080;&#25143;&#23450;&#20041;&#20102;&#30142;&#30149;&#30340;&#30149;&#20154;&#29305;&#23450;&#26681;&#26412;&#21407;&#22240;&#65292;&#35813;&#24080;&#25143;&#20165;&#25856;&#21319;&#21040;&#29645;&#29664;&#30340;&#22240;&#26524;Ladder&#30340;&#31532;&#20108;&#23618;&#12290;&#22312;&#36825;&#20010;&#29702;&#35770;&#24615;&#30340;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#21453;&#20107;&#23454;&#30340;&#23450;&#20041;&#26469;&#25856;&#21319;&#21040;&#31532;&#19977;&#23618;&#65292;&#20197;&#21305;&#37197;&#22522;&#20110;&#22266;&#23450;&#20107;&#23454;&#25968;&#25454;&#30340;&#20020;&#24202;&#30452;&#35273;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;Shapley&#20540;&#20026;&#27599;&#20010;&#21464;&#37327;&#20998;&#37197;&#26681;&#22240;&#36129;&#29486;&#24471;&#20998;&#12290;&#25552;&#20986;&#30340;&#30142;&#30149;&#24739;&#32773;&#20010;&#20307;&#26681;&#26412;&#21407;&#22240;&#30340;&#21453;&#20107;&#23454;&#20844;&#24335;&#21270;&#32771;&#34385;&#20102;&#22122;&#22768;&#26631;&#31614;&#65292;&#36866;&#24212;&#20102;&#30142;&#30149;&#30340;&#27969;&#34892;&#29575;&#65292;&#24182;&#20801;&#35768;&#24555;&#36895;&#35745;&#31639;&#65292;&#26080;&#38656;&#21453;&#20107;&#23454;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;
Root causes of disease intuitively correspond to root vertices that increase the likelihood of a diagnosis. This description of a root cause nevertheless lacks the rigorous mathematical formulation needed for the development of computer algorithms designed to automatically detect root causes from data. Prior work defined patient-specific root causes of disease using an interventionalist account that only climbs to the second rung of Pearl's Ladder of Causation. In this theoretical piece, we climb to the third rung by proposing a counterfactual definition matching clinical intuition based on fixed factual data alone. We then show how to assign a root causal contribution score to each variable using Shapley values from explainable artificial intelligence. The proposed counterfactual formulation of patient-specific root causes of disease accounts for noisy labels, adapts to disease prevalence and admits fast computation without the need for counterfactual simulation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#27010;&#25324;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#31867;&#22411;&#21644;&#24212;&#29992;&#65292;&#27604;&#36739;&#20998;&#26512;&#20102;&#21508;&#20010;&#27169;&#22411;&#30340;&#32467;&#26500;&#12289;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#26377;&#21161;&#20110;&#36873;&#25321;&#21644;&#35774;&#35745;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.17473</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#27010;&#36848;&#19982;&#27604;&#36739;&#20998;&#26512;&#65306;CNN&#12289;RNN&#12289;LSTM&#12289;GRU&#12290;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Overview and Comparative Analysis on Deep Learning Models: CNN, RNN, LSTM, GRU. (arXiv:2305.17473v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#27010;&#25324;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#31867;&#22411;&#21644;&#24212;&#29992;&#65292;&#27604;&#36739;&#20998;&#26512;&#20102;&#21508;&#20010;&#27169;&#22411;&#30340;&#32467;&#26500;&#12289;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#26377;&#21161;&#20110;&#36873;&#25321;&#21644;&#35774;&#35745;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#24378;&#22823;&#23376;&#38598;&#65292;&#29305;&#21035;&#22312;&#22788;&#29702;&#38750;&#32467;&#26500;&#21270;&#21644;&#22823;&#22411;&#25968;&#25454;&#38598;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;ML&#26041;&#27861;&#12290;&#20854;&#24433;&#21709;&#36328;&#36234;&#21508;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#35821;&#38899;&#35782;&#21035;&#12289;&#21307;&#30103;&#20445;&#20581;&#12289;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#12289;&#32593;&#32476;&#23433;&#20840;&#12289;&#39044;&#27979;&#20998;&#26512;&#31561;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#21644;&#21160;&#24577;&#24615;&#32473;&#35774;&#35745;&#26377;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#20154;&#20204;&#24320;&#21457;&#20986;&#20102;&#20960;&#31181;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;&#19981;&#21516;&#30340;&#38382;&#39064;&#21644;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12289;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#12289;&#29983;&#25104;&#27169;&#22411;&#12289;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#21644;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#12290;&#25105;&#20204;&#32771;&#23519;&#20102;&#27599;&#20010;&#27169;&#22411;&#30340;&#32467;&#26500;&#12289;&#24212;&#29992;&#12289;&#22909;&#22788;&#21644;&#23616;&#38480;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19977;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) has emerged as a powerful subset of machine learning (ML) and artificial intelligence (AI), outperforming traditional ML methods, especially in handling unstructured and large datasets. Its impact spans across various domains, including speech recognition, healthcare, autonomous vehicles, cybersecurity, predictive analytics, and more. However, the complexity and dynamic nature of real-world problems present challenges in designing effective deep learning models. Consequently, several deep learning models have been developed to address different problems and applications. In this article, we conduct a comprehensive survey of various deep learning models, including Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Generative Models, Deep Reinforcement Learning (DRL), and Deep Transfer Learning. We examine the structure, applications, benefits, and limitations of each model. Furthermore, we perform an analysis using three publicly available dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#26234;&#33021;&#20013;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#27880;&#24847;&#21147;&#27169;&#24335;&#29702;&#35770;&#65288;AST&#65289;&#12290;&#20316;&#32773;&#21457;&#29616;&#23558;AS&#23454;&#29616;&#20026;&#19968;&#31181;&#24490;&#29615;&#20869;&#37096;&#25511;&#21046;&#30340;&#26234;&#33021;&#20307;&#25928;&#26524;&#26368;&#20339;&#65292;&#36825;&#19968;&#29702;&#35770;&#20026;&#24212;&#29992;&#19982;&#25913;&#36827;&#31070;&#32463;&#26234;&#33021;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2305.17375</link><description>&lt;p&gt;
&#31070;&#32463;&#26234;&#33021;&#20013;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Attention Schema in Neural Agents. (arXiv:2305.17375v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#26234;&#33021;&#20013;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#27880;&#24847;&#21147;&#27169;&#24335;&#29702;&#35770;&#65288;AST&#65289;&#12290;&#20316;&#32773;&#21457;&#29616;&#23558;AS&#23454;&#29616;&#20026;&#19968;&#31181;&#24490;&#29615;&#20869;&#37096;&#25511;&#21046;&#30340;&#26234;&#33021;&#20307;&#25928;&#26524;&#26368;&#20339;&#65292;&#36825;&#19968;&#29702;&#35770;&#20026;&#24212;&#29992;&#19982;&#25913;&#36827;&#31070;&#32463;&#26234;&#33021;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#24050;&#32463;&#25104;&#20026;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20013;&#30340;&#24120;&#35265;&#35201;&#32032;&#12290;&#23427;&#36890;&#36807;&#21152;&#20837;&#21160;&#24577;&#20449;&#24687;&#36873;&#25321;&#65292;&#25903;&#25345;&#38745;&#24577;&#30340;&#26435;&#37325;&#36873;&#25321;&#12290;&#21516;&#26679;&#22320;&#65292;&#25105;&#20204;&#21487;&#20197;&#24819;&#35937;&#22312;&#27880;&#24847;&#21147;&#20043;&#19978;&#26500;&#24314;&#19968;&#20010;&#26356;&#39640;&#38454;&#30340;&#20449;&#24687;&#36807;&#28388;&#22120;&#65306;&#27880;&#24847;&#21147;&#27169;&#24335;&#65288;AS&#65289;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#19968;&#20010;&#25551;&#36848;&#24615;&#21644;&#39044;&#27979;&#24615;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#12290;&#22312;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#20013;&#65292;&#27880;&#24847;&#21147;&#27169;&#24335;&#29702;&#35770;&#65288;AST&#65289;&#25903;&#25345;&#36825;&#31181;&#21306;&#20998;&#27880;&#24847;&#21147;&#21644;AS&#30340;&#24819;&#27861;&#12290;&#35813;&#29702;&#35770;&#30340;&#19968;&#20010;&#37325;&#35201;&#39044;&#27979;&#26159;&#65292;&#19968;&#20010;&#26234;&#33021;&#20307;&#21487;&#20197;&#20351;&#29992;&#33258;&#24049;&#30340;AS&#26469;&#25512;&#26029;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#27880;&#24847;&#29366;&#24577;&#65292;&#20174;&#32780;&#22686;&#24378;&#19982;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#21327;&#35843;&#12290;&#22240;&#27492;&#65292;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#20010;&#23454;&#39564;&#27979;&#35797;AST&#26377;&#25928;&#24615;&#30340;&#29702;&#24819;&#22330;&#26223;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#27880;&#24847;&#21147;&#21644;AS&#30456;&#20114;&#20316;&#29992;&#30340;&#19981;&#21516;&#26041;&#24335;&#65292;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;AS&#23454;&#29616;&#20026;&#19968;&#31181;&#24490;&#29615;&#20869;&#37096;&#25511;&#21046;&#30340;&#26234;&#33021;&#20307;&#25928;&#26524;&#26368;&#20339;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#36825;&#20123;&#23454;&#39564;&#20026;&#29702;&#35299;&#20197;&#21450;&#25913;&#36827;&#31070;&#32463;&#26234;&#33021;&#20013;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention has become a common ingredient in deep learning architectures. It adds a dynamical selection of information on top of the static selection of information supported by weights. In the same way, we can imagine a higher-order informational filter built on top of attention: an Attention Schema (AS), namely, a descriptive and predictive model of attention. In cognitive neuroscience, Attention Schema Theory (AST) supports this idea of distinguishing attention from AS. A strong prediction of this theory is that an agent can use its own AS to also infer the states of other agents' attention and consequently enhance coordination with other agents. As such, multi-agent reinforcement learning would be an ideal setting to experimentally test the validity of AST. We explore different ways in which attention and AS interact with each other. Our preliminary results indicate that agents that implement the AS as a recurrent internal control achieve the best performance. In general, these expl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23545;&#40784;&#35780;&#20272;&#26041;&#27861;A2EHV&#65292;&#37319;&#29992;&#24322;&#36136;&#20215;&#20540;&#31995;&#32479;&#65292;&#24182;&#22522;&#20110;&#20215;&#20540;&#21512;&#29702;&#24615;&#21644;&#31038;&#20250;&#20215;&#20540;&#23450;&#21521;&#26694;&#26550;&#35780;&#20272;&#20195;&#29702;&#20154;&#34892;&#20026;&#30340;&#31038;&#20250;&#20559;&#22909;&#65292;&#32467;&#26524;&#34920;&#26126;&#27604;&#20256;&#32479;&#23545;&#40784;&#26041;&#27861;&#26356;&#21512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.17147</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24322;&#36136;&#20215;&#20540;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Value Evaluation for Large Language Models. (arXiv:2305.17147v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23545;&#40784;&#35780;&#20272;&#26041;&#27861;A2EHV&#65292;&#37319;&#29992;&#24322;&#36136;&#20215;&#20540;&#31995;&#32479;&#65292;&#24182;&#22522;&#20110;&#20215;&#20540;&#21512;&#29702;&#24615;&#21644;&#31038;&#20250;&#20215;&#20540;&#23450;&#21521;&#26694;&#26550;&#35780;&#20272;&#20195;&#29702;&#20154;&#34892;&#20026;&#30340;&#31038;&#20250;&#20559;&#22909;&#65292;&#32467;&#26524;&#34920;&#26126;&#27604;&#20256;&#32479;&#23545;&#40784;&#26041;&#27861;&#26356;&#21512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#20351;&#24471;&#23558;&#23427;&#20204;&#30340;&#20215;&#20540;&#19982;&#20154;&#31867;&#20215;&#20540;&#23545;&#40784;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#23581;&#35797;&#23558;&#20854;&#19982;&#19968;&#31181;&#21516;&#36136;&#30340;&#20154;&#31867;&#20215;&#20540;&#23545;&#40784;&#65292;&#24182;&#38656;&#35201;&#20154;&#31867;&#39564;&#35777;&#65292;&#20294;&#32570;&#20047;&#23545;&#23545;&#40784;&#25152;&#38656;&#26041;&#38754;&#21644;&#28145;&#24230;&#30340;&#20849;&#35782;&#20197;&#21450;&#36896;&#25104;&#30340;&#20154;&#31867;&#20559;&#35265;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23545;&#40784;&#35780;&#20272;&#26041;&#27861;A2EHV&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#24322;&#36136;&#20215;&#20540;&#31995;&#32479;&#65292;&#65288;1&#65289;&#26159;&#33258;&#21160;&#21270;&#30340;&#65292;&#20197;&#26368;&#23567;&#21270;&#21333;&#20010;&#20154;&#31867;&#20559;&#35265;&#65292;&#24182;&#19988;&#65288;2&#65289;&#20801;&#35768;&#35780;&#20272;&#38024;&#23545;&#21508;&#31181;&#30446;&#26631;&#20540;&#30340;&#24322;&#36136;&#20195;&#29702;&#20154;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20215;&#20540;&#21512;&#29702;&#24615;&#30340;&#27010;&#24565;&#65292;&#23427;&#20195;&#34920;&#20102;&#20195;&#29702;&#20154;&#25191;&#34892;&#26368;&#33021;&#28385;&#36275;&#30446;&#26631;&#20215;&#20540;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;&#20215;&#20540;&#21512;&#29702;&#24615;&#30340;&#37327;&#21270;&#26159;&#36890;&#36807;&#31038;&#20250;&#24515;&#29702;&#23398;&#20013;&#30340;&#31038;&#20250;&#20215;&#20540;&#23450;&#21521;&#26694;&#26550;&#36827;&#34892;&#30340;&#65292;&#35813;&#26694;&#26550;&#23558;&#20215;&#20540;&#31354;&#38388;&#20998;&#20026;&#22235;&#20010;&#31867;&#21035;&#65292;&#20197;&#35780;&#20272;&#20195;&#29702;&#20154;&#34892;&#20026;&#30340;&#31038;&#20250;&#20559;&#22909;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19977;&#20010;&#27169;&#22411;&#30340;&#20215;&#20540;&#21512;&#29702;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;A2EHV&#26041;&#27861;&#27604;&#20256;&#32479;&#23545;&#40784;&#26041;&#27861;&#26356;&#21512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergent capabilities of Large Language Models (LLMs) have made it crucial to align their values with those of humans. Current methodologies typically attempt alignment with a homogeneous human value and requires human verification, yet lack consensus on the desired aspect and depth of alignment and resulting human biases. In this paper, we propose A2EHV, an Automated Alignment Evaluation with a Heterogeneous Value system that (1) is automated to minimize individual human biases, and (2) allows assessments against various target values to foster heterogeneous agents. Our approach pivots on the concept of value rationality, which represents the ability for agents to execute behaviors that satisfy a target value the most. The quantification of value rationality is facilitated by the Social Value Orientation framework from social psychology, which partitions the value space into four categories to assess social preferences from agents' behaviors. We evaluate the value rationality of e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Ghost in the Minecraft (GITM)&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22522;&#20110;&#25991;&#26412;&#30340;&#30693;&#35782;&#21644;&#35760;&#24518;&#65292;&#21019;&#36896;&#20102;&#19968;&#31181;&#22312;Minecraft&#20013;&#20855;&#22791;&#36890;&#29992;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#65292;&#21487;&#22312;&#20197;&#25991;&#26412;&#20026;&#22522;&#30784;&#30340;&#22797;&#26434;&#32534;&#31243;&#29615;&#22659;&#20013;&#29087;&#32451;&#23548;&#33322;&#12290;</title><link>http://arxiv.org/abs/2305.17144</link><description>&lt;p&gt;
Minecraft&#20013;&#30340;&#24189;&#28789;&#65306;&#21033;&#29992;&#22522;&#20110;&#25991;&#26412;&#30693;&#35782;&#21644;&#35760;&#24518;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#36890;&#29992;&#33021;&#21147;&#26234;&#33021;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ghost in the Minecraft: Generally Capable Agents for Open-World Enviroments via Large Language Models with Text-based Knowledge and Memory. (arXiv:2305.17144v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Ghost in the Minecraft (GITM)&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22522;&#20110;&#25991;&#26412;&#30340;&#30693;&#35782;&#21644;&#35760;&#24518;&#65292;&#21019;&#36896;&#20102;&#19968;&#31181;&#22312;Minecraft&#20013;&#20855;&#22791;&#36890;&#29992;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#65292;&#21487;&#22312;&#20197;&#25991;&#26412;&#20026;&#22522;&#30784;&#30340;&#22797;&#26434;&#32534;&#31243;&#29615;&#22659;&#20013;&#29087;&#32451;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;Minecraft&#29609;&#27861;&#21560;&#24341;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#20851;&#27880;&#65292;&#25104;&#20026;&#24320;&#21457;&#33021;&#22815;&#22312;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#20013;&#36816;&#34892;&#30340;&#26234;&#33021;&#20307;&#30340;&#20016;&#23500;&#24179;&#21488;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#29305;&#23450;&#30340;&#30446;&#26631;&#19978;&#65292;&#20363;&#22914;&#27969;&#34892;&#30340;&#8220;ObtainDiamond&#8221;&#20219;&#21153;&#65292;&#24182;&#19988;&#36824;&#27809;&#26377;&#26174;&#31034;&#20986;&#26377;&#25928;&#22320;&#25512;&#24191;&#21040;&#26356;&#24191;&#27867;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#8220;ObtainDiamond&#8221;&#20219;&#21153;&#30340;&#30446;&#21069;&#26368;&#39640;&#25104;&#21151;&#29575;&#21482;&#26377;&#32422;20&#65285;&#65292;&#20984;&#26174;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25511;&#21046;&#22120;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Ghost in the Minecraft (GITM)&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22522;&#20110;&#25991;&#26412;&#30340;&#30693;&#35782;&#21644;&#35760;&#24518;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#21019;&#24314;Minecraft&#20013;&#30340;&#36890;&#29992;&#33021;&#21147;&#26234;&#33021;&#20307;&#12290;&#36825;&#20123;&#20855;&#22791;LLM&#20013;&#30340;&#36923;&#36753;&#21644;&#24120;&#35782;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#21487;&#20197;&#29087;&#32451;&#22320;&#22312;&#20197;&#25991;&#26412;&#20026;&#22522;&#30784;&#30340;&#22797;&#26434;&#32534;&#31243;&#29615;&#22659;&#20013;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The captivating realm of Minecraft has attracted substantial research interest in recent years, serving as a rich platform for developing intelligent agents capable of functioning in open-world environments. However, the current research landscape predominantly focuses on specific objectives, such as the popular "ObtainDiamond" task, and has not yet shown effective generalization to a broader spectrum of tasks. Furthermore, the current leading success rate for the "ObtainDiamond" task stands at around 20%, highlighting the limitations of Reinforcement Learning (RL) based controllers used in existing methods. To tackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel framework integrates Large Language Models (LLMs) with text-based knowledge and memory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These agents, equipped with the logic and common sense capabilities of LLMs, can skillfully navigate complex, sparse-reward environments with text-based 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;93&#21517;NLP&#21021;&#23398;&#32773;&#30340;&#35843;&#26597;&#65292;&#21457;&#29616;&#30740;&#31350;&#20316;&#32773;&#25552;&#20379;&#23436;&#25972;&#25991;&#26723;&#12289;&#26356;&#22909;&#30340;&#20195;&#30721;&#23454;&#36341;&#21644;&#26356;&#26131;&#20110;&#33719;&#21462;&#30340;&#25968;&#25454;&#25991;&#20214;&#26159;&#21021;&#23398;&#32773;&#25104;&#21151;&#22797;&#29616;&#26368;&#36817;NLP&#35770;&#25991;&#32467;&#26524;&#30340;&#20851;&#38190;&#65292;&#24314;&#35758;NLP&#30740;&#31350;&#20154;&#21592;&#27880;&#37325;&#36825;&#20123;&#26041;&#38754;&#65292;&#26356;&#22909;&#22320;&#25903;&#25345;&#21021;&#23398;&#32773;&#12290;</title><link>http://arxiv.org/abs/2305.16579</link><description>&lt;p&gt;
&#20154;&#20154;&#21487;&#22797;&#29616;&#30340;NLP&#30740;&#31350;&#65306;&#21021;&#23398;&#32773;&#30340;&#38656;&#27714;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
NLP Reproducibility For All: Understanding Experiences of Beginners. (arXiv:2305.16579v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16579
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;93&#21517;NLP&#21021;&#23398;&#32773;&#30340;&#35843;&#26597;&#65292;&#21457;&#29616;&#30740;&#31350;&#20316;&#32773;&#25552;&#20379;&#23436;&#25972;&#25991;&#26723;&#12289;&#26356;&#22909;&#30340;&#20195;&#30721;&#23454;&#36341;&#21644;&#26356;&#26131;&#20110;&#33719;&#21462;&#30340;&#25968;&#25454;&#25991;&#20214;&#26159;&#21021;&#23398;&#32773;&#25104;&#21151;&#22797;&#29616;&#26368;&#36817;NLP&#35770;&#25991;&#32467;&#26524;&#30340;&#20851;&#38190;&#65292;&#24314;&#35758;NLP&#30740;&#31350;&#20154;&#21592;&#27880;&#37325;&#36825;&#20123;&#26041;&#38754;&#65292;&#26356;&#22909;&#22320;&#25903;&#25345;&#21021;&#23398;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#36817;&#24180;&#26469;&#24322;&#24120;&#28779;&#29190;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24613;&#20110;&#36827;&#20837;&#35813;&#39046;&#22495;&#65292;&#20294;&#30446;&#21069;&#30340;&#30740;&#31350;&#22797;&#29616;&#21162;&#21147;&#26159;&#21542;&#36275;&#20197;&#35753;&#36825;&#20123;&#21021;&#23398;&#32773;&#24212;&#29992;&#26368;&#26032;&#30340;&#36827;&#23637;&#36824;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#20102;&#35299;&#21021;&#23398;&#32773;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#20171;&#32461;&#24615;&#30340;NLP&#35838;&#31243;&#20013;&#24320;&#23637;&#20102;&#19968;&#39033;&#30740;&#31350;&#65292;&#35753;&#23398;&#29983;&#22797;&#29616;&#26368;&#36817;NLP&#35770;&#25991;&#30340;&#32467;&#26524;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20182;&#20204;&#30340;&#32534;&#31243;&#25216;&#33021;&#21644;&#23545;&#30740;&#31350;&#35770;&#25991;&#30340;&#29702;&#35299;&#23545;&#23436;&#25104;&#32451;&#20064;&#30340;&#20184;&#20986;&#20165;&#26377;&#38480;&#30340;&#24433;&#21709;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#30740;&#31350;&#20316;&#32773;&#30340;&#21487;&#35775;&#38382;&#24615;&#21162;&#21147;&#26159;&#25104;&#21151;&#30340;&#20851;&#38190;&#65292;&#21253;&#25324;&#23436;&#25972;&#30340;&#25991;&#26723;&#12289;&#26356;&#22909;&#30340;&#32534;&#30721;&#23454;&#36341;&#21644;&#26356;&#23481;&#26131;&#33719;&#21462;&#30340;&#25968;&#25454;&#25991;&#20214;&#12290;&#21069;&#36827;&#26102;&#65292;&#25105;&#20204;&#24314;&#35758;NLP&#30740;&#31350;&#20154;&#21592;&#23494;&#20999;&#20851;&#27880;&#36825;&#20123;&#24320;&#28304;&#24037;&#20316;&#30340;&#31616;&#21333;&#26041;&#38754;&#65292;&#24182;&#20351;&#29992;&#21021;&#23398;&#32773;&#30340;&#21453;&#39304;&#35265;&#35299;&#25552;&#20379;&#21487;&#25805;&#20316;&#30340;&#24819;&#27861;&#20197;&#26356;&#22909;&#22320;&#25903;&#25345;&#20182;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
As natural language processing (NLP) has recently seen an unprecedented level of excitement, and more people are eager to enter the field, it is unclear whether current research reproducibility efforts are sufficient for this group of beginners to apply the latest developments. To understand their needs, we conducted a study with 93 students in an introductory NLP course, where students reproduced the results of recent NLP papers. Surprisingly, we find that their programming skill and comprehension of research papers have a limited impact on their effort spent completing the exercise. Instead, we find accessibility efforts by research authors to be the key to success, including complete documentation, better coding practice, and easier access to data files. Going forward, we recommend that NLP researchers pay close attention to these simple aspects of open-sourcing their work, and use insights from beginners' feedback to provide actionable ideas on how to better support them.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31616;&#35201;&#27010;&#36848;&#20102;&#38271;&#25991;&#26412;&#30340;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#29616;&#29366;&#65292;&#20027;&#35201;&#21253;&#25324;&#25991;&#26723;&#20998;&#31867;&#21644;&#25688;&#35201;&#65292;&#28085;&#30422;&#20102;&#24773;&#24863;&#20998;&#26512;&#65292;&#21516;&#26102;&#36824;&#25506;&#35752;&#20102;&#38271;&#25991;&#26412;NLP&#30340;&#20027;&#35201;&#25361;&#25112;&#12289;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.16259</link><description>&lt;p&gt;
&#38271;&#25991;&#26412;&#30340;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65306;&#29616;&#29366;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Neural Natural Language Processing for Long Texts: A Survey of the State-of-the-Art. (arXiv:2305.16259v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31616;&#35201;&#27010;&#36848;&#20102;&#38271;&#25991;&#26412;&#30340;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#29616;&#29366;&#65292;&#20027;&#35201;&#21253;&#25324;&#25991;&#26723;&#20998;&#31867;&#21644;&#25688;&#35201;&#65292;&#28085;&#30422;&#20102;&#24773;&#24863;&#20998;&#26512;&#65292;&#21516;&#26102;&#36824;&#25506;&#35752;&#20102;&#38271;&#25991;&#26412;NLP&#30340;&#20027;&#35201;&#25361;&#25112;&#12289;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#37319;&#29992;&#26497;&#22823;&#22320;&#20419;&#36827;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#38271;&#25991;&#26412;&#20998;&#26512;&#30340;&#38656;&#27714;&#19982;&#30701;&#25991;&#26412;&#26377;&#24456;&#22823;&#19981;&#21516;&#65292;&#32780;&#32593;&#32476;&#19978;&#20256;&#36755;&#30340;&#25991;&#26723;&#22823;&#23567;&#19981;&#26029;&#22686;&#21152;&#65292;&#20351;&#38271;&#25991;&#26412;&#30340;&#33258;&#21160;&#29702;&#35299;&#25104;&#20026;&#19968;&#39033;&#20851;&#38190;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#30340;&#20004;&#20010;&#30446;&#26631;&#26159;&#65306;a&#65289;&#27010;&#36848;&#30456;&#20851;&#30340;&#31070;&#32463;&#26500;&#24314;&#27169;&#22359;&#65292;&#20316;&#20026;&#30701;&#25945;&#31243;&#65307;b&#65289;&#24635;&#32467;&#38271;&#25991;&#26412;NLP&#30340;&#29616;&#29366;&#65292;&#20027;&#35201;&#20851;&#27880;&#20004;&#20010;&#26680;&#24515;&#20219;&#21153;&#65306;&#25991;&#26723;&#20998;&#31867;&#21644;&#25991;&#26723;&#25688;&#35201;&#12290;&#24773;&#24863;&#20998;&#26512;&#20063;&#28085;&#30422;&#22312;&#20869;&#65292;&#22240;&#20026;&#23427;&#36890;&#24120;&#34987;&#35270;&#20026;&#25991;&#26723;&#20998;&#31867;&#30340;&#29305;&#20363;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#38271;&#25991;&#26412;NLP&#30456;&#20851;&#30340;&#20027;&#35201;&#25361;&#25112;&#12289;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;&#20171;&#32461;&#20102;&#30456;&#20851;&#30340;&#20844;&#24320;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#20197;&#20415;&#20419;&#36827;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The adoption of Deep Neural Networks (DNNs) has greatly benefited Natural Language Processing (NLP) during the past decade. However, the demands of long document analysis are quite different from those of shorter texts, while the ever increasing size of documents uploaded on-line renders automated understanding of long texts a critical area of research. This article has two goals: a) it overviews the relevant neural building blocks, thus serving as a short tutorial, and b) it surveys the state-of-the-art in long document NLP, mainly focusing on two central tasks: document classification and document summarization. Sentiment analysis for long texts is also covered, since it is typically treated as a particular case of document classification. Additionally, this article discusses the main challenges, issues and current solutions related to long document NLP. Finally, the relevant, publicly available, annotated datasets are presented, in order to facilitate further research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;DirecT2V&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23548;&#28436;&#65292;&#20174;&#19968;&#20010;&#25277;&#35937;&#30340;&#29992;&#25143;&#25552;&#31034;&#20013;&#29983;&#25104;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#30340;&#36830;&#36143;&#19988;&#36830;&#36143;&#30340;&#35270;&#39057;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;LLM&#23548;&#28436;&#23558;&#29992;&#25143;&#36755;&#20837;&#20998;&#20026;&#27599;&#19968;&#24103;&#30340;&#25552;&#31034;&#65292;&#36890;&#36807;&#20540;&#26144;&#23556;&#21644;&#21452;softmax&#36807;&#28388;&#22120;&#26469;&#20445;&#25345;&#26102;&#38388;&#19968;&#33268;&#21644;&#38450;&#27490;&#23545;&#35937;&#25240;&#21472;&#12290;</title><link>http://arxiv.org/abs/2305.14330</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#30340;&#24103;&#32423;&#23548;&#28436;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Frame-level Directors for Zero-shot Text-to-Video Generation. (arXiv:2305.14330v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;DirecT2V&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23548;&#28436;&#65292;&#20174;&#19968;&#20010;&#25277;&#35937;&#30340;&#29992;&#25143;&#25552;&#31034;&#20013;&#29983;&#25104;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#30340;&#36830;&#36143;&#19988;&#36830;&#36143;&#30340;&#35270;&#39057;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;LLM&#23548;&#28436;&#23558;&#29992;&#25143;&#36755;&#20837;&#20998;&#20026;&#27599;&#19968;&#24103;&#30340;&#25552;&#31034;&#65292;&#36890;&#36807;&#20540;&#26144;&#23556;&#21644;&#21452;softmax&#36807;&#28388;&#22120;&#26469;&#20445;&#25345;&#26102;&#38388;&#19968;&#33268;&#21644;&#38450;&#27490;&#23545;&#35937;&#25240;&#21472;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#30340;&#33539;&#24335;&#20013;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#28857;&#25918;&#22312;&#23558;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#25193;&#23637;&#21040;&#25991;&#26412;&#21040;&#35270;&#39057;&#65288;T2V&#65289;&#29983;&#25104;&#19978;&#12290;&#23613;&#31649;&#36825;&#20123;&#26694;&#26550;&#24456;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#30528;&#32500;&#25252;&#19968;&#33268;&#30340;&#21465;&#36848;&#21644;&#22788;&#29702;&#20174;&#21333;&#20010;&#29992;&#25143;&#25552;&#31034;&#20013;&#30340;&#24555;&#36895;&#22330;&#26223;&#32452;&#21512;&#25110;&#23545;&#35937;&#20301;&#32622;&#21464;&#21270;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;DirecT2V&#65292;&#23427;&#21033;&#29992;&#38024;&#23545;&#25351;&#20196;&#26657;&#20934;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20174;&#21333;&#20010;&#25277;&#35937;&#29992;&#25143;&#25552;&#31034;&#29983;&#25104;&#36880;&#24103;&#25551;&#36848;&#12290;DirecT2V&#21033;&#29992;LLM&#23548;&#28436;&#23558;&#29992;&#25143;&#36755;&#20837;&#20998;&#20026;&#27599;&#20010;&#24103;&#30340;&#21333;&#29420;&#25552;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#21253;&#21547;&#26102;&#38388;&#21464;&#21270;&#30340;&#20869;&#23481;&#21644;&#20415;&#20110;&#19968;&#33268;&#30340;&#35270;&#39057;&#29983;&#25104;&#12290;&#20026;&#20102;&#20445;&#25345;&#26102;&#38388;&#19978;&#30340;&#19968;&#33268;&#24615;&#21644;&#38450;&#27490;&#23545;&#35937;&#25240;&#21472;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20540;&#26144;&#23556;&#26041;&#27861;&#21644;&#21452;softmax&#36807;&#28388;&#22120;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;DirecT2V&#26694;&#26550;&#22312;&#38646;&#26679;&#26412;T2V&#29983;&#25104;&#20013;&#20135;&#29983;&#30340;&#35270;&#35273;&#36830;&#36143;&#21644;&#19968;&#33268;&#30340;&#35270;&#39057;&#29983;&#25104;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the paradigm of AI-generated content (AIGC), there has been increasing attention in extending pre-trained text-to-image (T2I) models to text-to-video (T2V) generation. Despite their effectiveness, these frameworks face challenges in maintaining consistent narratives and handling rapid shifts in scene composition or object placement from a single user prompt. This paper introduces a new framework, dubbed DirecT2V, which leverages instruction-tuned large language models (LLMs) to generate frame-by-frame descriptions from a single abstract user prompt. DirecT2V utilizes LLM directors to divide user inputs into separate prompts for each frame, enabling the inclusion of time-varying content and facilitating consistent video generation. To maintain temporal consistency and prevent object collapse, we propose a novel value mapping method and dual-softmax filtering. Extensive experimental results validate the effectiveness of the DirecT2V framework in producing visually coherent and consist
&lt;/p&gt;</description></item><item><title>&#38646;&#26679;&#26412;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#23481;&#26131;&#20986;&#29616;&#8220;&#31163;&#35889;&#38382;&#39064;&#8221;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#31639;&#27861;LAVS&#21487;&#20197;&#36890;&#36807;&#22686;&#21152;&#35821;&#35328;&#20043;&#38388;&#30340;KL&#20998;&#27495;&#26174;&#33879;&#38477;&#20302;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.10930</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#8220;&#31163;&#35889;&#38382;&#39064;&#8221;
&lt;/p&gt;
&lt;p&gt;
On the Off-Target Problem of Zero-Shot Multilingual Neural Machine Translation. (arXiv:2305.10930v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10930
&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#23481;&#26131;&#20986;&#29616;&#8220;&#31163;&#35889;&#38382;&#39064;&#8221;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#31639;&#27861;LAVS&#21487;&#20197;&#36890;&#36807;&#22686;&#21152;&#35821;&#35328;&#20043;&#38388;&#30340;KL&#20998;&#27495;&#26174;&#33879;&#38477;&#20302;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23427;&#20173;&#28982;&#23384;&#22312;&#8220;&#31163;&#35889;&#38382;&#39064;&#8221;&#65292;&#21363;&#23558;&#32763;&#35793;&#36755;&#20986;&#21040;&#38169;&#35823;&#30340;&#35821;&#35328;&#20013;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#38646;&#26679;&#26412;&#32763;&#35793;&#20219;&#21153;&#20013;&#26356;&#21152;&#26126;&#26174;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#24403;&#32534;&#30721;&#30446;&#26631;&#35821;&#35328;&#20449;&#21495;&#26102;&#22833;&#25928;&#65292;&#20250;&#23548;&#33268;&#31163;&#35889;&#38382;&#39064;&#65292;&#24182;&#19988;&#20004;&#31181;&#35821;&#35328;&#35789;&#27719;&#20043;&#38388;&#26356;&#25509;&#36817;&#30340;&#35789;&#27719;&#36317;&#31163;&#65288;&#21363;KL&#20998;&#27495;&#65289;&#19982;&#26356;&#39640;&#30340;&#31163;&#35889;&#29575;&#26377;&#20851;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#21457;&#29616;&#65292;&#20165;&#38548;&#31163;&#35299;&#30721;&#22120;&#20013;&#19981;&#21516;&#35821;&#35328;&#30340;&#35789;&#27719;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#31639;&#27861;Language Aware Vocabulary Sharing (LAVS)&#26469;&#26500;&#24314;&#22810;&#35821;&#35328;&#35789;&#27719;&#34920;&#65292;&#36890;&#36807;&#22686;&#21152;&#35821;&#35328;&#20043;&#38388;&#30340;KL&#20998;&#27495;&#65292;&#22823;&#22823;&#20943;&#36731;&#20102;&#32763;&#35793;&#27169;&#22411;&#30340;&#31163;&#35889;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;11&#31181;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#32763;&#35793;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;90&#20010;&#32763;&#35793;&#20219;&#21153;&#65292;&#37319;&#29992;LAVS&#30340;&#31163;&#35889;&#29575;&#38477;&#20302;&#20102;37&#65285;&#33267;90&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
While multilingual neural machine translation has achieved great success, it suffers from the off-target issue, where the translation is in the wrong language. This problem is more pronounced on zero-shot translation tasks. In this work, we find that failing in encoding discriminative target language signal will lead to off-target and a closer lexical distance (i.e., KL-divergence) between two languages' vocabularies is related with a higher off-target rate. We also find that solely isolating the vocab of different languages in the decoder can alleviate the problem. Motivated by the findings, we propose Language Aware Vocabulary Sharing (LAVS), a simple and effective algorithm to construct the multilingual vocabulary, that greatly alleviates the off-target problem of the translation model by increasing the KL-divergence between languages. We conduct experiments on a multilingual machine translation benchmark in 11 languages. Experiments show that the off-target rate for 90 translation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CORAL&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#26080;&#30417;&#30563;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#35302;&#21457;&#35813;&#36807;&#31243;&#24182;&#22686;&#37327;&#26356;&#26032;&#27169;&#22411;&#65292;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#37096;&#20998;&#65306;&#35302;&#21457;&#28857;&#26816;&#27979;&#65292;&#22686;&#37327;&#22240;&#26524;&#22270;&#23398;&#20064;&#21644;&#22522;&#20110;&#32593;&#32476;&#20256;&#25773;&#30340;&#26681;&#26412;&#21407;&#22240;&#23450;&#20301;&#12290;</title><link>http://arxiv.org/abs/2305.10638</link><description>&lt;p&gt;
&#22686;&#37327;&#22240;&#26524;&#22270;&#23398;&#20064;&#36827;&#34892;&#22312;&#32447;&#26080;&#30417;&#30563;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Incremental Causal Graph Learning for Online Unsupervised Root Cause Analysis. (arXiv:2305.10638v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CORAL&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#26080;&#30417;&#30563;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#35302;&#21457;&#35813;&#36807;&#31243;&#24182;&#22686;&#37327;&#26356;&#26032;&#27169;&#22411;&#65292;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#37096;&#20998;&#65306;&#35302;&#21457;&#28857;&#26816;&#27979;&#65292;&#22686;&#37327;&#22240;&#26524;&#22270;&#23398;&#20064;&#21644;&#22522;&#20110;&#32593;&#32476;&#20256;&#25773;&#30340;&#26681;&#26412;&#21407;&#22240;&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#65288;RCA&#65289;&#30340;&#20219;&#21153;&#26159;&#20998;&#26512;&#31995;&#32479;&#30417;&#25511;&#25968;&#25454;&#65292;&#20197;&#35782;&#21035;&#31995;&#32479;&#25925;&#38556;/&#22833;&#25928;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#26377;&#25928;&#30340;RCA&#21487;&#20197;&#22823;&#22823;&#21152;&#36895;&#31995;&#32479;&#25925;&#38556;&#24674;&#22797;&#65292;&#24182;&#20943;&#36731;&#31995;&#32479;&#25439;&#22833;&#25110;&#36130;&#21153;&#25439;&#22833;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#22823;&#22810;&#38598;&#20013;&#22312;&#24320;&#21457;&#31163;&#32447;RCA&#31639;&#27861;&#19978;&#65292;&#36825;&#36890;&#24120;&#38656;&#35201;&#25163;&#21160;&#21551;&#21160;RCA&#36807;&#31243;&#65292;&#38656;&#35201;&#22823;&#37327;&#26102;&#38388;&#21644;&#25968;&#25454;&#26469;&#35757;&#32451;&#31283;&#20581;&#30340;&#27169;&#22411;&#65292;&#28982;&#21518;&#38656;&#35201;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#26032;&#30340;&#31995;&#32479;&#25925;&#38556;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CORAL&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;RCA&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#35302;&#21457;RCA&#36807;&#31243;&#24182;&#22686;&#37327;&#26356;&#26032;RCA&#27169;&#22411;&#12290;CORAL&#21253;&#25324;&#35302;&#21457;&#28857;&#26816;&#27979;&#12289;&#22686;&#37327;&#35299;&#32544;&#22240;&#26524;&#22270;&#23398;&#20064;&#21644;&#22522;&#20110;&#32593;&#32476;&#20256;&#25773;&#30340;&#26681;&#26412;&#21407;&#22240;&#23450;&#20301;&#12290;&#35302;&#21457;&#28857;&#26816;&#27979;&#32452;&#20214;&#26088;&#22312;&#33258;&#21160;&#26816;&#27979;&#31995;&#32479;&#29366;&#24577;&#36716;&#25442;&#24182;&#36827;&#34892;&#20934;&#23454;&#26102;&#26816;&#27979;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;m&#30340;&#22312;&#32447;&#35302;&#21457;&#28857;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of root cause analysis (RCA) is to identify the root causes of system faults/failures by analyzing system monitoring data. Efficient RCA can greatly accelerate system failure recovery and mitigate system damages or financial losses. However, previous research has mostly focused on developing offline RCA algorithms, which often require manually initiating the RCA process, a significant amount of time and data to train a robust model, and then being retrained from scratch for a new system fault.  In this paper, we propose CORAL, a novel online RCA framework that can automatically trigger the RCA process and incrementally update the RCA model. CORAL consists of Trigger Point Detection, Incremental Disentangled Causal Graph Learning, and Network Propagation-based Root Cause Localization. The Trigger Point Detection component aims to detect system state transitions automatically and in near-real-time. To achieve this, we develop an online trigger point detection approach based on m
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;CNN&#35299;&#20915;&#20102;&#29031;&#29255;&#26041;&#21521;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#20351;&#29992;Guided Backpropagation&#33719;&#24471;&#20102;CNN&#26816;&#27979;&#26041;&#21521;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.10319</link><description>&lt;p&gt;
&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#21160;&#29031;&#29255;&#26041;&#21521;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Automatic Photo Orientation Detection with Convolutional Neural Networks. (arXiv:2305.10319v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;CNN&#35299;&#20915;&#20102;&#29031;&#29255;&#26041;&#21521;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#20351;&#29992;Guided Backpropagation&#33719;&#24471;&#20102;CNN&#26816;&#27979;&#26041;&#21521;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#26469;&#35299;&#20915;&#30830;&#23450;&#28040;&#36153;&#32773;&#29031;&#29255;&#27491;&#30830;&#26041;&#21521;(0&#176;, 90&#176;, 180&#176;&#21644;270&#176;)&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#23545;&#20110;&#27169;&#25311;&#29031;&#29255;&#30340;&#25968;&#23383;&#21270;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#22312;&#26356;&#22256;&#38590;&#30340;&#28040;&#36153;&#32773;&#29031;&#29255;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#20351;&#29992;&#24341;&#23548;&#21453;&#21521;&#20256;&#25773;(Guided Backpropagation)&#26469;&#33719;&#24471;&#20851;&#20110;CNN&#22914;&#20309;&#26816;&#27979;&#29031;&#29255;&#26041;&#21521;&#30340;&#35265;&#35299;&#65292;&#24182;&#35299;&#37322;&#20854;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
We apply convolutional neural networks (CNN) to the problem of image orientation detection in the context of determining the correct orientation (from 0, 90, 180, and 270 degrees) of a consumer photo. The problem is especially important for digitazing analog photographs. We substantially improve on the published state of the art in terms of the performance on one of the standard datasets, and test our system on a more difficult large dataset of consumer photos. We use Guided Backpropagation to obtain insights into how our CNN detects photo orientation, and to explain its mistakes.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;HOI&#26816;&#27979;&#25968;&#25454;&#25286;&#20998;&#65292;&#26088;&#22312;&#35780;&#20272;&#31995;&#32479;&#24615;&#27867;&#21270;&#12290;&#22312;&#26032;&#30340;&#25968;&#25454;&#25286;&#20998;&#19978;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;HOI&#26816;&#27979;&#27169;&#22411;&#23545;&#20110;&#26410;&#35265;&#36807;&#30340;&#23545;&#35937;&#21644;&#20132;&#20114;&#32452;&#21512;&#30340;&#27867;&#21270;&#21313;&#20998;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2305.09948</link><description>&lt;p&gt;
HICO-DET-SG&#21644;V-COCO-SG&#65306;&#26032;&#30340;&#25968;&#25454;&#25286;&#20998;&#29992;&#20110;&#35780;&#20272;&#20154;-&#29289;&#20132;&#20114;&#26816;&#27979;&#20013;&#30340;&#31995;&#32479;&#24615;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
HICO-DET-SG and V-COCO-SG: New Data Splits to Evaluate Systematic Generalization in Human-Object Interaction Detection. (arXiv:2305.09948v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;HOI&#26816;&#27979;&#25968;&#25454;&#25286;&#20998;&#65292;&#26088;&#22312;&#35780;&#20272;&#31995;&#32479;&#24615;&#27867;&#21270;&#12290;&#22312;&#26032;&#30340;&#25968;&#25454;&#25286;&#20998;&#19978;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;HOI&#26816;&#27979;&#27169;&#22411;&#23545;&#20110;&#26410;&#35265;&#36807;&#30340;&#23545;&#35937;&#21644;&#20132;&#20114;&#32452;&#21512;&#30340;&#27867;&#21270;&#21313;&#20998;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;-&#29289;&#20132;&#20114;&#26816;&#27979;&#26159;&#19968;&#31181;&#39044;&#27979;&#22270;&#20687;&#20013;&#20154;&#19982;&#29289;&#21697;&#20043;&#38388;&#20132;&#20114;&#30340;&#20219;&#21153;&#12290;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#38656;&#35201;&#23545;HOI&#26816;&#27979;&#27169;&#22411;&#36827;&#34892;&#31995;&#32479;&#24615;&#30340;&#27867;&#21270;&#65292;&#21363;&#27867;&#21270;&#21040;&#26032;&#30340;&#23545;&#35937;&#21644;&#20132;&#20114;&#32452;&#21512;&#19978;&#65292;&#22240;&#20026;&#35757;&#32451;&#25968;&#25454;&#20165;&#21487;&#33021;&#28085;&#30422;&#25152;&#26377;&#21487;&#33021;&#32452;&#21512;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#27809;&#26377;&#24320;&#25918;&#30340;&#22522;&#20934;&#27979;&#35797;&#25110;&#29616;&#26377;&#24037;&#20316;&#35780;&#20272;HOI&#26816;&#27979;&#20013;&#30340;&#31995;&#32479;&#24615;&#27867;&#21270;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;HICO-DET&#21644;V-COCO&#25968;&#25454;&#38598;&#21019;&#24314;&#20102;&#20004;&#20010;&#21517;&#20026;HICO-DET-SG&#21644;V-COCO-SG&#30340;&#26032;&#30340;HOI&#26816;&#27979;&#25968;&#25454;&#25286;&#20998;&#12290;&#25105;&#20204;&#22312;&#26032;&#30340;&#25968;&#25454;&#25286;&#20998;&#19978;&#35780;&#20272;&#20102;&#20195;&#34920;&#24615;&#30340;HOI&#26816;&#27979;&#27169;&#22411;&#65292;&#24182;&#35266;&#23519;&#21040;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#30456;&#27604;&#27979;&#35797;&#24615;&#33021;&#26377;&#24456;&#22823;&#30340;&#38477;&#20302;&#12290;&#36825;&#20010;&#32467;&#26524;&#34920;&#26126;&#31995;&#32479;&#24615;&#27867;&#21270;&#26159;HOI&#26816;&#27979;&#20013;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#26032;&#25968;&#25454;&#25286;&#20998;&#33021;&#22815;&#40723;&#21169;&#26356;&#22810;&#30340;&#30740;&#31350;&#26397;&#30528;&#36825;&#20010;&#30446;&#26631;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-Object Interaction (HOI) detection is a task to predict interactions between humans and objects in an image. In real-world scenarios, HOI detection models are required systematic generalization, i.e., generalization to novel combinations of objects and interactions, because it is highly probable that the train data only cover a limited portion of all possible combinations. However, to our knowledge, no open benchmark or existing work evaluates the systematic generalization in HOI detection. To address this issue, we created two new sets of HOI detection data splits named HICO-DET-SG and V-COCO-SG based on HICO-DET and V-COCO datasets. We evaluated representative HOI detection models on the new data splits and observed large degradation in the test performances compared to those on the original datasets. This result shows that systematic generalization is a challenging goal in HOI detection. We hope our new data splits encourage more research toward this goal.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#22522;&#20110;&#21608;&#26399;&#26102;&#38388;&#35774;&#32622;&#36229;&#21442;&#25968;&#65292;&#20351;&#24471;PPO&#21644;SAC&#22312;&#24191;&#27867;&#30340;&#24490;&#29615;&#26102;&#38388;&#33539;&#22260;&#20869;&#36827;&#34892;&#23398;&#20064;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#25509;&#36817;&#32791;&#26102;&#30340;&#22312;&#32447;&#36229;&#21442;&#25968;&#35843;&#25972;&#33719;&#24471;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.05760</link><description>&lt;p&gt;
&#38477;&#20302;&#29616;&#23454;&#31574;&#30053;&#20248;&#21270;&#20013;&#24490;&#29615;&#26102;&#38388;&#35843;&#25972;&#30340;&#20195;&#20215;
&lt;/p&gt;
&lt;p&gt;
Reducing the Cost of Cycle-Time Tuning for Real-World Policy Optimization. (arXiv:2305.05760v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#22522;&#20110;&#21608;&#26399;&#26102;&#38388;&#35774;&#32622;&#36229;&#21442;&#25968;&#65292;&#20351;&#24471;PPO&#21644;SAC&#22312;&#24191;&#27867;&#30340;&#24490;&#29615;&#26102;&#38388;&#33539;&#22260;&#20869;&#36827;&#34892;&#23398;&#20064;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#25509;&#36817;&#32791;&#26102;&#30340;&#22312;&#32447;&#36229;&#21442;&#25968;&#35843;&#25972;&#33719;&#24471;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#36890;&#24120;&#20351;&#29992;&#22266;&#23450;&#21608;&#26399;&#26102;&#38388;&#30340;&#31163;&#25955;&#27493;&#39588;&#36827;&#34892;&#25805;&#20316;&#12290;&#23454;&#36341;&#20013;&#38656;&#35201;&#20026;&#32473;&#23450;&#20219;&#21153;&#36873;&#25321;&#25805;&#20316;&#21608;&#26399;&#26102;&#38388;&#65292;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#26159;&#23398;&#20064;&#31639;&#27861;&#30340;&#36229;&#21442;&#25968;&#26159;&#21542;&#38656;&#35201;&#20026;&#27599;&#20010;&#21608;&#26399;&#26102;&#38388;&#37325;&#26032;&#35843;&#25972;&#65292;&#36825;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#26469;&#35828;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;--PPO&#21644;SAC--&#22312;&#19981;&#21516;&#30340;&#21608;&#26399;&#26102;&#38388;&#19979;&#20351;&#29992;&#22522;&#32447;&#36229;&#21442;&#25968;&#20540;&#30340;&#24773;&#20917;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#32447;&#36229;&#21442;&#25968;&#22312;&#19968;&#20010;&#22522;&#20934;&#20219;&#21153;&#20013;&#23637;&#31034;&#36825;&#20004;&#31181;&#31639;&#27861;&#34920;&#29616;&#33391;&#22909;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#36873;&#25321;&#19981;&#21516;&#20110;&#20219;&#21153;&#40664;&#35748;&#20540;&#30340;&#21608;&#26399;&#26102;&#38388;&#26102;&#65292;&#20351;&#29992;&#22522;&#32447;&#36229;&#21442;&#25968;&#30340;PPO&#26080;&#27861;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#24403;&#36229;&#21442;&#25968;&#29992;&#20110;&#27599;&#20010;&#21608;&#26399;&#26102;&#38388;&#26102;&#65292;&#22522;&#20110;&#22522;&#32447;&#30340;PPO&#21644;SAC&#34920;&#29616;&#22343;&#26126;&#26174;&#21155;&#20110;&#23427;&#20204;&#30340;&#35843;&#25972;&#20540;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21608;&#26399;&#26102;&#38388;&#35774;&#32622;&#36825;&#20123;&#36229;&#21442;&#25968;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;&#25105;&#20204;&#30340;&#20223;&#30495;&#26426;&#22120;&#20154;&#36816;&#21160;&#20219;&#21153;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;PPO&#21644;SAC&#22312;&#26497;&#20854;&#24191;&#27867;&#30340;&#21608;&#26399;&#26102;&#38388;&#33539;&#22260;&#20869;&#36827;&#34892;&#23398;&#20064;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#25509;&#36817;&#20110;&#32791;&#26102;&#30340;&#22312;&#32447;&#36229;&#21442;&#25968;&#35843;&#25972;&#33719;&#24471;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continuous-time reinforcement learning tasks commonly use discrete steps of fixed cycle times for actions. As practitioners need to choose the action-cycle time for a given task, a significant concern is whether the hyper-parameters of the learning algorithm need to be re-tuned for each choice of the cycle time, which is prohibitive for real-world robotics. In this work, we investigate the widely-used baseline hyper-parameter values of two policy gradient algorithms -- PPO and SAC -- across different cycle times. Using a benchmark task where the baseline hyper-parameters of both algorithms were shown to work well, we reveal that when a cycle time different than the task default is chosen, PPO with baseline hyper-parameters fails to learn. Moreover, both PPO and SAC with their baseline hyper-parameters perform substantially worse than their tuned values for each cycle time. We propose novel approaches for setting these hyper-parameters based on the cycle time. In our experiments on simu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#22270;&#36974;&#30422;&#33258;&#32534;&#30721;&#22120;&#30340;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#26292;&#38706;&#20986;&#24102;&#26377;&#36974;&#30422;&#30340;&#39033;&#30446;&#24207;&#21015;&#65292;&#33258;&#36866;&#24212;&#21160;&#24577;&#25552;&#21462;&#20840;&#23616;&#39033;&#30446;&#36716;&#25442;&#20449;&#24687;&#36827;&#34892;&#33258;&#30417;&#30563;&#22686;&#24378;&#65292;&#22312;&#20855;&#26377;&#36739;&#23569;&#26631;&#35760;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#22987;&#32456;&#27604;&#26368;&#20808;&#36827;&#30340;&#24207;&#21015;&#25512;&#33616;&#26041;&#27861;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#23545;&#25968;&#25454;&#25439;&#22351;&#21644;&#32570;&#22833;&#24773;&#20917;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.04619</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#24418;&#36974;&#30422;&#33258;&#32534;&#30721;&#22120;&#30340;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Graph Masked Autoencoder for Sequential Recommendation. (arXiv:2305.04619v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04619
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#22270;&#36974;&#30422;&#33258;&#32534;&#30721;&#22120;&#30340;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#26292;&#38706;&#20986;&#24102;&#26377;&#36974;&#30422;&#30340;&#39033;&#30446;&#24207;&#21015;&#65292;&#33258;&#36866;&#24212;&#21160;&#24577;&#25552;&#21462;&#20840;&#23616;&#39033;&#30446;&#36716;&#25442;&#20449;&#24687;&#36827;&#34892;&#33258;&#30417;&#30563;&#22686;&#24378;&#65292;&#22312;&#20855;&#26377;&#36739;&#23569;&#26631;&#35760;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#22987;&#32456;&#27604;&#26368;&#20808;&#36827;&#30340;&#24207;&#21015;&#25512;&#33616;&#26041;&#27861;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#23545;&#25968;&#25454;&#25439;&#22351;&#21644;&#32570;&#22833;&#24773;&#20917;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#19968;&#20123;&#24378;&#22823;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65288;&#20363;&#22914;Transformer&#12289;&#22270;&#31070;&#32463;&#32593;&#32476;&#65289;&#36890;&#36807;&#39640;&#38454;&#39033;&#20381;&#36182;&#24314;&#27169;&#22312;&#24207;&#21015;&#25512;&#33616;&#20013;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#22312;&#26631;&#31614;&#31232;&#32570;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#34920;&#24449;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#26631;&#31614;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#24050;&#32463;&#24341;&#36215;&#20102;&#36817;&#26399;&#30340;&#20851;&#27880;&#65292;&#36890;&#36807;&#23884;&#20837;&#23545;&#27604;&#26469;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#30340;&#25968;&#25454;&#22686;&#24378;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#23545;&#27604;&#35270;&#22270;&#29983;&#25104;&#31574;&#30053;&#30340;&#25163;&#24037;&#21046;&#23450;&#29305;&#24615;&#65292;&#29616;&#26377;&#30340;CL&#22686;&#24378;&#27169;&#22411;&#19981;&#20165;&#38590;&#20197;&#22312;&#19981;&#21516;&#30340;&#24207;&#21015;&#25512;&#33616;&#20219;&#21153;&#20013;&#20135;&#29983;&#19968;&#33268;&#30340;&#24615;&#33021;&#65292;&#36824;&#21487;&#33021;&#23545;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#22122;&#22768;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#37492;&#20110;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33258;&#36866;&#24212;&#20840;&#23616;&#20449;&#24687;&#25552;&#21462;&#30340;&#22270;&#36974;&#30422;&#33258;&#32534;&#30721;&#22120;&#22686;&#24378;&#30340;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#65288;MAERec&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23427;&#33258;&#28982;&#22320;&#36991;&#20813;&#20102;&#19978;&#36848;&#38382;&#39064;&#65292;&#24471;&#30410;&#20110;&#20854;&#29420;&#29305;&#30340;&#25968;&#25454;&#37325;&#26500;&#26426;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#26292;&#38706;&#20986;&#24102;&#26377;&#36974;&#30422;&#30340;&#39033;&#30446;&#24207;&#21015;&#65292;&#20351;&#34920;&#31034;&#19981;&#20165;&#21033;&#29992;&#26412;&#22320;&#39034;&#24207;&#20449;&#24687;&#65292;&#36824;&#21033;&#29992;&#39033;&#30446;&#20043;&#38388;&#30340;&#20840;&#23616;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20855;&#26377;&#36739;&#23569;&#26631;&#35760;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#22987;&#32456;&#27604;&#26368;&#20808;&#36827;&#30340;&#24207;&#21015;&#25512;&#33616;&#26041;&#27861;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#23545;&#25968;&#25454;&#25439;&#22351;&#21644;&#32570;&#22833;&#24773;&#20917;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While some powerful neural network architectures (e.g., Transformer, Graph Neural Networks) have achieved improved performance in sequential recommendation with high-order item dependency modeling, they may suffer from poor representation capability in label scarcity scenarios. To address the issue of insufficient labels, Contrastive Learning (CL) has attracted much attention in recent methods to perform data augmentation through embedding contrasting for self-supervision. However, due to the hand-crafted property of their contrastive view generation strategies, existing CL-enhanced models i) can hardly yield consistent performance on diverse sequential recommendation tasks; ii) may not be immune to user behavior data noise. In light of this, we propose a simple yet effective Graph Masked AutoEncoder-enhanced sequential Recommender system (MAERec) that adaptively and dynamically distills global item transitional information for self-supervised augmentation. It naturally avoids the abov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;GPT-3.5&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#65292;&#23454;&#29616;&#22312;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.01555</link><description>&lt;p&gt;
&#22914;&#20309;&#21457;&#25381;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?. (arXiv:2305.01555v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;GPT-3.5&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#65292;&#23454;&#29616;&#22312;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#25193;&#23637;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#24191;&#27867;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#20294;&#26159;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#36824;&#27809;&#26377;&#24471;&#21040;&#20840;&#38754;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#35814;&#32454;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#20351;&#29992;GPT-3.5&#36827;&#34892;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#30340;&#22522;&#26412;&#26041;&#27861;&#8212;&#8212;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#25968;&#25454;&#29983;&#25104;&#12290;&#20026;&#20102;&#22686;&#24378;&#23569;&#26679;&#26412;&#24615;&#33021;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#19982;&#20197;&#21069;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#32780;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#25512;&#21160;&#20197;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#22312;&#22235;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#23569;&#26679;&#26412;&#32467;&#26524;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#21487;&#20197;&#28608;&#21457;&#26410;&#26469;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#33021;&#21147;&#30340;&#30740;&#31350;&#12290;&#20195;&#30721;&#21487;&#20197;&#22312; \url{https://github.com/zjunlp/DeepKE/tree/main/example/llm} &#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scaling language models have revolutionized widespread NLP tasks, yet little comprehensively explored few-shot relation extraction with large language models. In this paper, we investigate principal methodologies, in-context learning and data generation, for few-shot relation extraction via GPT-3.5 through exhaustive experiments. To enhance few-shot performance, we further propose task-related instructions and schema-constrained data generation. We observe that in-context learning can achieve performance on par with previous prompt learning approaches, and data generation with the large language model can boost previous solutions to obtain new state-of-the-art few-shot results on four widely-studied relation extraction datasets. We hope our work can inspire future research for the capabilities of large language models in few-shot relation extraction. Code is available in \url{https://github.com/zjunlp/DeepKE/tree/main/example/llm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#30740;&#31350;&#20102;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#25913;&#36827;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#30340;&#26368;&#36817;&#30740;&#31350;&#65292;&#21253;&#25324;&#23545;&#21453;&#39304;&#30340;&#20840;&#38754;&#24418;&#24335;&#21270;&#12289;&#21453;&#39304;&#30340;&#26684;&#24335;&#21644;&#30446;&#30340;&#30340;&#25551;&#36848;&#65292;&#21644;&#25552;&#20986;&#20351;&#29992;&#21453;&#39304;&#30340;&#20004;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#29992;&#20110;&#20154;&#31867;&#21453;&#39304;&#25968;&#25454;&#25910;&#38598;&#30340;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#21644;&#30456;&#20851;&#30340;&#25285;&#24551;&#12290;</title><link>http://arxiv.org/abs/2305.00955</link><description>&lt;p&gt;
&#26550;&#36215;&#26725;&#26753;&#65306;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#25972;&#21512;&#65288;&#20154;&#31867;&#65289;&#21453;&#39304;&#30340;&#35843;&#26597;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation. (arXiv:2305.00955v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#30740;&#31350;&#20102;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#25913;&#36827;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#30340;&#26368;&#36817;&#30740;&#31350;&#65292;&#21253;&#25324;&#23545;&#21453;&#39304;&#30340;&#20840;&#38754;&#24418;&#24335;&#21270;&#12289;&#21453;&#39304;&#30340;&#26684;&#24335;&#21644;&#30446;&#30340;&#30340;&#25551;&#36848;&#65292;&#21644;&#25552;&#20986;&#20351;&#29992;&#21453;&#39304;&#30340;&#20004;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#29992;&#20110;&#20154;&#31867;&#21453;&#39304;&#25968;&#25454;&#25910;&#38598;&#30340;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#21644;&#30456;&#20851;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#30340;&#36827;&#23637;&#37117;&#26159;&#22522;&#20110;&#21033;&#29992;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#25968;&#25454;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33539;&#24335;&#21487;&#33021;&#20250;&#23548;&#33268;&#29983;&#25104;&#26377;&#23475;&#12289;&#19981;&#20934;&#30830;&#21644;&#26080;&#29992;&#20869;&#23481;&#30340;&#27169;&#22411;&#65292;&#32780;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#36890;&#24120;&#26080;&#27861;&#35782;&#21035;&#36825;&#20123;&#34892;&#20026;&#12290;&#38543;&#30528;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#24378;&#22823;&#65292;&#20154;&#31867;&#21453;&#39304;&#25104;&#20026;&#35780;&#20215;&#21644;&#25913;&#36827;&#27169;&#22411;&#30340;&#23453;&#36149;&#20449;&#21495;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#27010;&#36848;&#26368;&#36817;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#25913;&#36827;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#30340;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#23545;&#21453;&#39304;&#30340;&#20840;&#38754;&#24418;&#24335;&#21270;&#65292;&#24182;&#26681;&#25454;&#36825;&#31181;&#24418;&#24335;&#21270;&#23558;&#29616;&#26377;&#30740;&#31350;&#36827;&#34892;&#20998;&#31867;&#21644;&#32452;&#32455;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#21453;&#39304;&#21487;&#20197;&#36890;&#36807;&#20854;&#26684;&#24335;&#21644;&#30446;&#30340;&#26469;&#25551;&#36848;&#65292;&#24182;&#28085;&#30422;&#20102;&#25552;&#20986;&#20351;&#29992;&#21453;&#39304;&#30340;&#20004;&#31181;&#26041;&#27861;&#65288;&#29992;&#20110;&#35757;&#32451;&#25110;&#35299;&#30721;&#65289;&#65306;&#30452;&#25509;&#20351;&#29992;&#21453;&#39304;&#25110;&#35757;&#32451;&#21453;&#39304;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#29992;&#20110;&#20154;&#31867;&#21453;&#39304;&#25968;&#25454;&#25910;&#38598;&#30340;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#30456;&#20851;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many recent advances in natural language generation have been fueled by training large language models on internet-scale data. However, this paradigm can lead to models that generate toxic, inaccurate, and unhelpful content, and automatic evaluation metrics often fail to identify these behaviors. As models become more capable, human feedback is an invaluable signal for evaluating and improving models. This survey aims to provide an overview of the recent research that has leveraged human feedback to improve natural language generation. First, we introduce an encompassing formalization of feedback, and identify and organize existing research into a taxonomy following this formalization. Next, we discuss how feedback can be described by its format and objective, and cover the two approaches proposed to use feedback (either for training or decoding): directly using the feedback or training feedback models. We also discuss existing datasets for human-feedback data collection, and concerns 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GeneticFlow&#30340;&#22522;&#20110;&#33258;&#24341;&#22270;&#30340;&#23398;&#32773;&#21078;&#26512;&#24037;&#20855;&#65292;&#33021;&#22815;&#28385;&#36275;&#23398;&#32773;&#29305;&#24449;&#21078;&#26512;&#20013;&#30340;&#19977;&#20010;&#22522;&#26412;&#35201;&#27714;&#65292;&#21363;&#32467;&#26500;&#21270;&#32972;&#26223;&#12289;&#23398;&#32773;&#20026;&#20013;&#24515;&#21644;&#20016;&#23500;&#30340;&#36827;&#21270;&#65292;&#22312;&#31185;&#23398;&#22870;&#39033;&#25512;&#29702;&#30495;&#23454;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2304.12217</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#24341;&#22270;&#30340;&#38754;&#21521;&#24433;&#21709;&#21147;&#30340;&#23398;&#32773;&#29615;&#22659;&#21078;&#26512;
&lt;/p&gt;
&lt;p&gt;
Impact-Oriented Contextual Scholar Profiling using Self-Citation Graphs. (arXiv:2304.12217v2 [cs.DL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GeneticFlow&#30340;&#22522;&#20110;&#33258;&#24341;&#22270;&#30340;&#23398;&#32773;&#21078;&#26512;&#24037;&#20855;&#65292;&#33021;&#22815;&#28385;&#36275;&#23398;&#32773;&#29305;&#24449;&#21078;&#26512;&#20013;&#30340;&#19977;&#20010;&#22522;&#26412;&#35201;&#27714;&#65292;&#21363;&#32467;&#26500;&#21270;&#32972;&#26223;&#12289;&#23398;&#32773;&#20026;&#20013;&#24515;&#21644;&#20016;&#23500;&#30340;&#36827;&#21270;&#65292;&#22312;&#31185;&#23398;&#22870;&#39033;&#25512;&#29702;&#30495;&#23454;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23450;&#37327;&#22320;&#21078;&#26512;&#23398;&#32773;&#30340;&#31185;&#30740;&#24433;&#21709;&#21147;&#23545;&#20110;&#29616;&#20195;&#30740;&#31350;&#31038;&#20250;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#20351;&#29992;&#30340;&#25991;&#29486;&#35745;&#37327;&#25351;&#26631;&#65288;&#22914;h&#25351;&#25968;&#65289;&#12289;&#21015;&#34920;&#21644;&#32593;&#32476;&#22312;&#23398;&#32773;&#25490;&#21517;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#19981;&#25552;&#20379;&#23398;&#32773;&#30456;&#20851;&#12289;&#20998;&#26512;&#24615;&#20219;&#21153;&#30340;&#32467;&#26500;&#21270;&#32972;&#26223;&#65292;&#20363;&#22914;&#21078;&#26512;&#21644;&#29702;&#35299;&#23398;&#32773;&#29305;&#24449;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#23398;&#32773;&#21078;&#26512;&#24037;&#20855;&#8212;&#8212;GeneticFlow (GF)&#65292;&#28385;&#36275;&#19977;&#20010;&#22522;&#26412;&#35201;&#27714;&#65306;&#32467;&#26500;&#21270;&#32972;&#26223;&#12289;&#23398;&#32773;&#20026;&#20013;&#24515;&#21644;&#20016;&#23500;&#30340;&#36827;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#21253;&#25324;&#25968;&#30334;&#19975;&#23398;&#32773;&#30340;&#22823;&#35268;&#27169;&#23398;&#26415;&#25968;&#25454;&#28304;&#19978;&#35745;&#31639;GF&#65307;&#35813;&#26694;&#26550;&#21253;&#21547;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#23548;&#24072;-&#23398;&#29983;&#26816;&#27979;&#31639;&#27861;&#12289;&#19968;&#20010;&#20351;&#29992;&#21487;&#35299;&#37322;&#29305;&#24449;&#30340;&#31934;&#24515;&#35774;&#35745;&#30340;&#24341;&#29992;&#31867;&#22411;&#20998;&#31867;&#22120;&#65292;&#20197;&#21450;&#19968;&#20010;&#32463;&#36807;&#24494;&#35843;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;&#31185;&#23398;&#22870;&#39033;&#25512;&#29702;&#30340;&#30495;&#23454;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26368;&#20339;GF&#21078;&#26512;&#30340;F1&#24471;&#20998;&#26126;&#26174;&#20248;&#20110;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantitatively profiling a scholar's scientific impact is important to modern research society. Current practices with bibliometric indicators (e.g., h-index), lists, and networks perform well at scholar ranking, but do not provide structured context for scholar-centric, analytical tasks such as profile reasoning and understanding. This work presents GeneticFlow (GF), a suite of novel graph-based scholar profiles that fulfill three essential requirements: structured-context, scholar-centric, and evolution-rich. We propose a framework to compute GF over large-scale academic data sources with millions of scholars. The framework encompasses a new unsupervised advisor-advisee detection algorithm, a well-engineered citation type classifier using interpretable features, and a fine-tuned graph neural network (GNN) model. Evaluations are conducted on the real-world task of scientific award inference. Experiment outcomes show that the F1 score of best GF profile significantly outperforms altern
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26080;&#30417;&#30563;&#24490;&#29615;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21487;&#20197;&#39640;&#25928;&#12289;&#33410;&#33021;&#22320;&#36827;&#34892;&#22312;&#32447;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#24182;&#19988;&#36890;&#36807;&#38543;&#26426;&#24310;&#36831;&#23884;&#20837;&#21644;&#20351;&#29992;&#24213;&#23618;&#21160;&#24577;&#31995;&#32479;&#20013;&#20171;&#25968;&#20013;&#24515;&#24615;&#26368;&#39640;&#30340;&#31070;&#32463;&#20803;&#30340;&#33180;&#30005;&#20301;&#36827;&#34892;&#37325;&#26500;&#12290;</title><link>http://arxiv.org/abs/2304.04697</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#33041;&#21551;&#21457;&#30340;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#32447;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Brain-Inspired Spiking Neural Network for Online Unsupervised Time Series Prediction. (arXiv:2304.04697v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04697
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26080;&#30417;&#30563;&#24490;&#29615;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21487;&#20197;&#39640;&#25928;&#12289;&#33410;&#33021;&#22320;&#36827;&#34892;&#22312;&#32447;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#24182;&#19988;&#36890;&#36807;&#38543;&#26426;&#24310;&#36831;&#23884;&#20837;&#21644;&#20351;&#29992;&#24213;&#23618;&#21160;&#24577;&#31995;&#32479;&#20013;&#20171;&#25968;&#20013;&#24515;&#24615;&#26368;&#39640;&#30340;&#31070;&#32463;&#20803;&#30340;&#33180;&#30005;&#20301;&#36827;&#34892;&#37325;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#65292;&#29305;&#21035;&#26159;&#38656;&#35201;&#22522;&#20110;&#27969;&#25968;&#25454;&#19981;&#26029;&#26356;&#26032;&#30340;&#36793;&#32536;AI&#24212;&#29992;&#20013;&#65292;&#33021;&#22815;&#39640;&#25928;&#12289;&#33410;&#33021;&#22320;&#36827;&#34892;&#22312;&#32447;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#30417;&#30563;&#22312;&#32447;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#19988;&#26080;&#27861;&#22312;&#24213;&#23618;&#31995;&#32479;&#21457;&#29983;&#21464;&#21270;&#26102;&#24555;&#36895;&#36866;&#24212;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#20351;&#29992;&#20256;&#20837;&#25968;&#25454;&#36827;&#34892;&#25345;&#32493;&#30340;&#37325;&#26032;&#35757;&#32451;&#65292;&#20351;&#20854;&#39640;&#24230;&#20302;&#25928;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26080;&#30417;&#30563;&#24490;&#29615;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65288;CLURSNN&#65289;&#65292;&#20854;&#36890;&#36807;&#23574;&#23792;&#26102;&#24207;&#30456;&#20851;&#21487;&#22609;&#24615;&#65288;STDP&#65289;&#36827;&#34892;&#35757;&#32451;&#12290;CLURSNN&#36890;&#36807;&#20351;&#29992;&#22312;&#21487;&#37325;&#26500;RSNN&#30340;&#20855;&#26377;&#26368;&#39640;&#20171;&#25968;&#20013;&#24515;&#24615;&#30340;&#31070;&#32463;&#20803;&#30340;&#33180;&#30005;&#20301;&#26469;&#36827;&#34892;&#38543;&#26426;&#24310;&#36831;&#23884;&#20837;&#65292;&#20197;&#37325;&#26500;&#24213;&#23618;&#21160;&#24577;&#31995;&#32479;&#36827;&#34892;&#22312;&#32447;&#39044;&#27979;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;Wasserstein&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy and data-efficient online time series prediction for predicting evolving dynamical systems are critical in several fields, especially edge AI applications that need to update continuously based on streaming data. However, current DNN-based supervised online learning models require a large amount of training data and cannot quickly adapt when the underlying system changes. Moreover, these models require continuous retraining with incoming data making them highly inefficient. To solve these issues, we present a novel Continuous Learning-based Unsupervised Recurrent Spiking Neural Network Model (CLURSNN), trained with spike timing dependent plasticity (STDP). CLURSNN makes online predictions by reconstructing the underlying dynamical system using Random Delay Embedding by measuring the membrane potential of neurons in the recurrent layer of the RSNN with the highest betweenness centrality. We also use topological data analysis to propose a novel methodology using the Wasserstein Di
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#39046;&#22495;&#30693;&#35782;&#36827;&#34892;&#20010;&#24615;&#21270;&#25968;&#23383;&#20581;&#24247;&#34892;&#20026;&#21464;&#38761;&#24178;&#39044;&#30340;&#31995;&#32479;&#65292;&#20854;&#21033;&#29992;&#21453;&#20107;&#23454;&#20363;&#23376;&#36827;&#34892;&#29305;&#24449;&#25511;&#21046;&#20197;&#39044;&#27979;&#24178;&#39044;&#25928;&#26524;&#24182;&#20248;&#21270;&#24178;&#39044;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.03392</link><description>&lt;p&gt;
&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#39046;&#22495;&#30693;&#35782;&#20010;&#24615;&#21270;&#25968;&#23383;&#20581;&#24247;&#34892;&#20026;&#21464;&#38761;&#24178;&#39044;
&lt;/p&gt;
&lt;p&gt;
Personalizing Digital Health Behavior Change Interventions using Machine Learning and Domain Knowledge. (arXiv:2304.03392v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03392
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#39046;&#22495;&#30693;&#35782;&#36827;&#34892;&#20010;&#24615;&#21270;&#25968;&#23383;&#20581;&#24247;&#34892;&#20026;&#21464;&#38761;&#24178;&#39044;&#30340;&#31995;&#32479;&#65292;&#20854;&#21033;&#29992;&#21453;&#20107;&#23454;&#20363;&#23376;&#36827;&#34892;&#29305;&#24449;&#25511;&#21046;&#20197;&#39044;&#27979;&#24178;&#39044;&#25928;&#26524;&#24182;&#20248;&#21270;&#24178;&#39044;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#27491;&#22312;&#24320;&#21457;&#19968;&#31181;&#34394;&#25311;&#25945;&#32451;&#31995;&#32479;&#65292;&#24110;&#21161;&#24739;&#32773;&#22362;&#25345;&#34892;&#20026;&#21464;&#38761;&#24178;&#39044;&#65288;BCI&#65289;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#39044;&#27979;&#24739;&#32773;&#26159;&#21542;&#20250;&#25191;&#34892;&#30446;&#26631;&#34892;&#20026;&#65292;&#24182;&#20351;&#29992;&#21453;&#20107;&#23454;&#20363;&#23376;&#36827;&#34892;&#29305;&#24449;&#25511;&#21046;&#65292;&#20197;&#25351;&#23548;&#20010;&#24615;&#21270;BCI&#12290;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#19981;&#21516;&#21709;&#24212;&#27700;&#24179;&#30340;&#27169;&#25311;&#24739;&#32773;&#25968;&#25454;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We are developing a virtual coaching system that helps patients adhere to behavior change interventions (BCI). Our proposed system predicts whether a patient will perform the targeted behavior and uses counterfactual examples with feature control to guide personalizsation of BCI. We evaluated our prediction model using simulated patient data with varying levels of receptivity to intervention.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#22120;&#24341;&#23548;&#26041;&#27861;DOODL&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#38024;&#23545;&#30495;&#23454;&#29983;&#25104;&#30340;&#20687;&#32032;&#19978;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#26799;&#24230;&#20248;&#21270;&#25193;&#25955;&#28508;&#21464;&#26469;&#23454;&#29616;&#21363;&#25554;&#21363;&#29992;&#30340;&#25351;&#23548;&#65292;&#24182;&#22312;&#35745;&#31639;&#21644;&#20154;&#31867;&#35780;&#20272;&#24230;&#37327;&#19978;&#20248;&#20110;&#19968;&#27493;&#20998;&#31867;&#22120;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2303.13703</link><description>&lt;p&gt;
&#31471;&#21040;&#31471;&#25193;&#25955;&#28508;&#22312;&#20248;&#21270;&#25552;&#39640;&#20998;&#31867;&#22120;&#24341;&#23548;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-End Diffusion Latent Optimization Improves Classifier Guidance. (arXiv:2303.13703v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#22120;&#24341;&#23548;&#26041;&#27861;DOODL&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#38024;&#23545;&#30495;&#23454;&#29983;&#25104;&#30340;&#20687;&#32032;&#19978;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#26799;&#24230;&#20248;&#21270;&#25193;&#25955;&#28508;&#21464;&#26469;&#23454;&#29616;&#21363;&#25554;&#21363;&#29992;&#30340;&#25351;&#23548;&#65292;&#24182;&#22312;&#35745;&#31639;&#21644;&#20154;&#31867;&#35780;&#20272;&#24230;&#37327;&#19978;&#20248;&#20110;&#19968;&#27493;&#20998;&#31867;&#22120;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#22120;&#25351;&#23548;&#8212;&#8212;&#21033;&#29992;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#26799;&#24230;&#26469;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#8212;&#8212;&#26377;&#28508;&#21147;&#22823;&#24133;&#25193;&#23637;&#23545;&#22270;&#20687;&#29983;&#25104;&#21644;&#32534;&#36753;&#30340;&#21019;&#36896;&#24615;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20998;&#31867;&#22120;&#25351;&#23548;&#35201;&#20040;&#38656;&#35201;&#35757;&#32451;&#26032;&#30340;&#22122;&#22768;&#24863;&#30693;&#27169;&#22411;&#20197;&#33719;&#24471;&#31934;&#30830;&#30340;&#26799;&#24230;&#65292;&#35201;&#20040;&#20351;&#29992;&#19968;&#27493;&#21435;&#22122;&#30340;&#36817;&#20284;&#26368;&#32456;&#29983;&#25104;&#29289;&#65292;&#24182;&#23548;&#33268;&#26799;&#24230;&#19981;&#23545;&#40784;&#21644;&#27425;&#20248;&#25511;&#21046;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#36825;&#31181;&#36817;&#20284;&#30340;&#32570;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#23548;&#26041;&#27861;&#65306;&#30452;&#25509;&#20248;&#21270;&#25193;&#25955;&#28508;&#21464;&#65288;DOODL&#65289;&#65292;&#23427;&#36890;&#36807;&#38024;&#23545;&#30495;&#23454;&#29983;&#25104;&#30340;&#20687;&#32032;&#19978;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#26799;&#24230;&#20248;&#21270;&#25193;&#25955;&#28508;&#21464;&#26469;&#23454;&#29616;&#21363;&#25554;&#21363;&#29992;&#30340;&#25351;&#23548;&#65292;&#20351;&#29992;&#21487;&#36870;&#25193;&#25955;&#36807;&#31243;&#23454;&#29616;&#20869;&#23384;&#26377;&#25928;&#30340;&#21453;&#21521;&#20256;&#36882;&#12290;&#23637;&#31034;&#20102;&#26356;&#31934;&#30830;&#25351;&#23548;&#28508;&#21147;&#30340; DOODL &#22312;&#19981;&#21516;&#24418;&#24335;&#30340;&#25351;&#23548;&#30340;&#35745;&#31639;&#21644;&#20154;&#31867;&#35780;&#20272;&#24230;&#37327;&#19978;&#20248;&#20110;&#19968;&#27493;&#20998;&#31867;&#22120;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classifier guidance -- using the gradients of an image classifier to steer the generations of a diffusion model -- has the potential to dramatically expand the creative control over image generation and editing. However, currently classifier guidance requires either training new noise-aware models to obtain accurate gradients or using a one-step denoising approximation of the final generation, which leads to misaligned gradients and sub-optimal control. We highlight this approximation's shortcomings and propose a novel guidance method: Direct Optimization of Diffusion Latents (DOODL), which enables plug-and-play guidance by optimizing diffusion latents w.r.t. the gradients of a pre-trained classifier on the true generated pixels, using an invertible diffusion process to achieve memory-efficient backpropagation. Showcasing the potential of more precise guidance, DOODL outperforms one-step classifier guidance on computational and human evaluation metrics across different forms of guidanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Aux-Drop&#30340;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;&#22312;&#32447;&#23398;&#20064;&#65292;&#33021;&#22815;&#22788;&#29702;&#28151;&#20081;&#30340;&#36755;&#20837;&#29305;&#24449;&#12290;&#36890;&#36807;&#35843;&#25972;&#24120;&#35268;&#30340;&#20002;&#22833;&#27491;&#21017;&#21270;&#26041;&#26696;&#65292;&#30830;&#20445;&#26368;&#32456;&#36755;&#20986;&#21463;&#21040;&#36825;&#20123;&#29305;&#24449;&#28151;&#20081;&#20986;&#29616;&#30340;&#26368;&#23567;&#24433;&#21709;&#65292;&#26377;&#21161;&#20110;&#26500;&#24314;&#26356;&#20581;&#22766;&#26377;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2303.05155</link><description>&lt;p&gt;
Aux-Drop: &#20351;&#29992;&#36741;&#21161;&#20002;&#24323;&#22788;&#29702;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#28151;&#20081;&#36755;&#20837;
&lt;/p&gt;
&lt;p&gt;
Aux-Drop: Handling Haphazard Inputs in Online Learning Using Auxiliary Dropouts. (arXiv:2303.05155v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Aux-Drop&#30340;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;&#22312;&#32447;&#23398;&#20064;&#65292;&#33021;&#22815;&#22788;&#29702;&#28151;&#20081;&#30340;&#36755;&#20837;&#29305;&#24449;&#12290;&#36890;&#36807;&#35843;&#25972;&#24120;&#35268;&#30340;&#20002;&#22833;&#27491;&#21017;&#21270;&#26041;&#26696;&#65292;&#30830;&#20445;&#26368;&#32456;&#36755;&#20986;&#21463;&#21040;&#36825;&#20123;&#29305;&#24449;&#28151;&#20081;&#20986;&#29616;&#30340;&#26368;&#23567;&#24433;&#21709;&#65292;&#26377;&#21161;&#20110;&#26500;&#24314;&#26356;&#20581;&#22766;&#26377;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22522;&#20110;&#22312;&#32447;&#23398;&#20064;&#30340;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#31243;&#24207;&#29983;&#25104;&#30340;&#27969;&#24335;&#25968;&#25454;&#20855;&#26377;&#28151;&#20081;&#30340;&#29305;&#24615;&#65292;&#21363;&#21253;&#21547;&#32570;&#22833;&#30340;&#29305;&#24449;&#65292;&#22312;&#26102;&#38388;&#19978;&#21464;&#24471;&#36807;&#26102;&#65292;&#20197;&#21518;&#20986;&#29616;&#26032;&#29305;&#24449;&#24182;&#19988;&#32570;&#20047;&#23545;&#24635;&#36755;&#20837;&#29305;&#24449;&#25968;&#37327;&#30340;&#28165;&#26224;&#35748;&#35782;&#12290;&#36825;&#20123;&#25361;&#25112;&#20351;&#24471;&#20026;&#36825;&#20123;&#24212;&#29992;&#26500;&#24314;&#21487;&#23398;&#20064;&#31995;&#32479;&#21464;&#24471;&#22256;&#38590;&#65292;&#24182;&#19988;&#20960;&#20046;&#27809;&#26377;&#20851;&#20110;&#36825;&#20010;&#38382;&#39064;&#30340;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#24037;&#20316;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Aux-Drop&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#23398;&#20064;&#30340;&#36741;&#21161;&#20002;&#22833;&#27491;&#21017;&#21270;&#31574;&#30053;&#65292;&#20197;&#26377;&#25928;&#22788;&#29702;&#28151;&#20081;&#30340;&#36755;&#20837;&#29305;&#24449;&#12290;Aux-Drop&#20026;&#28151;&#20081;&#30340;&#36755;&#20837;&#29305;&#24449;&#31354;&#38388;&#35843;&#25972;&#24120;&#35268;&#30340;&#20002;&#22833;&#27491;&#21017;&#21270;&#26041;&#26696;&#65292;&#30830;&#20445;&#26368;&#32456;&#36755;&#20986;&#21463;&#21040;&#36825;&#20123;&#29305;&#24449;&#28151;&#20081;&#20986;&#29616;&#30340;&#26368;&#23567;&#24433;&#21709;&#12290;&#23427;&#26377;&#21161;&#20110;&#38450;&#27490;&#29305;&#21035;&#26159;&#36741;&#21161;&#21644;&#22522;&#30784;&#29305;&#24449;&#30340;&#20849;&#36866;&#24212;&#65292;&#24182;&#20943;&#23569;&#27169;&#22411;&#20013;&#20219;&#20309;&#36741;&#21161;&#36755;&#20837;&#23545;&#36755;&#20986;&#30340;&#24378;&#28872;&#20381;&#36182;&#12290;&#36825;&#26377;&#21161;&#20110;&#20026;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#31243;&#24207;&#26500;&#24314;&#26356;&#20581;&#22766;&#26377;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world applications based on online learning produce streaming data that is haphazard in nature, i.e., contains missing features, features becoming obsolete in time, the appearance of new features at later points in time and a lack of clarity on the total number of input features. These challenges make it hard to build a learnable system for such applications, and almost no work exists in deep learning that addresses this issue. In this paper, we present Aux-Drop, an auxiliary dropout regularization strategy for online learning that handles the haphazard input features in an effective manner. Aux-Drop adapts the conventional dropout regularization scheme for the haphazard input feature space ensuring that the final output is minimally impacted by the chaotic appearance of such features. It helps to prevent the co-adaptation of especially the auxiliary and base features, as well as reduces the strong dependence of the output on any of the auxiliary inputs of the model. This hel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;-&#35805;&#35821;&#27880;&#24847;&#21147;&#21644;&#32852;&#21512;&#24314;&#27169;&#30340;&#26597;&#35810;&#24863;&#30693;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#27169;&#22359;&#35745;&#31639;&#35805;&#35821;&#32423;&#21035;&#19982;&#26597;&#35810;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#23558;&#26631;&#35760;&#32423;&#21035;&#30340;&#26597;&#35810;&#20851;&#32852;&#24615;&#21644;&#35805;&#35821;&#32423;&#21035;&#30340;&#26597;&#35810;&#20851;&#32852;&#24615;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#29983;&#25104;&#19968;&#20010;&#26356;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#25688;&#35201;&#12290;&#32463;&#36807;&#23545;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#65292;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;QFMS&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.04487</link><description>&lt;p&gt;
&#22522;&#20110;&#26597;&#35810;-&#35805;&#35821;&#27880;&#24847;&#21147;&#21644;&#32852;&#21512;&#24314;&#27169;&#30340;&#26597;&#35810;&#28966;&#28857;&#20250;&#35758;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Query-Utterance Attention with Joint modeling for Query-Focused Meeting Summarization. (arXiv:2303.04487v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;-&#35805;&#35821;&#27880;&#24847;&#21147;&#21644;&#32852;&#21512;&#24314;&#27169;&#30340;&#26597;&#35810;&#24863;&#30693;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#27169;&#22359;&#35745;&#31639;&#35805;&#35821;&#32423;&#21035;&#19982;&#26597;&#35810;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#23558;&#26631;&#35760;&#32423;&#21035;&#30340;&#26597;&#35810;&#20851;&#32852;&#24615;&#21644;&#35805;&#35821;&#32423;&#21035;&#30340;&#26597;&#35810;&#20851;&#32852;&#24615;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#29983;&#25104;&#19968;&#20010;&#26356;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#25688;&#35201;&#12290;&#32463;&#36807;&#23545;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#65292;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;QFMS&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#28966;&#28857;&#20250;&#35758;&#25688;&#35201;&#65288;QFMS&#65289;&#26088;&#22312;&#26681;&#25454;&#32473;&#23450;&#30340;&#26597;&#35810;&#65292;&#20174;&#20250;&#35758;&#35760;&#24405;&#20013;&#29983;&#25104;&#25688;&#35201;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#36890;&#24120;&#23558;&#26597;&#35810;&#19982;&#20250;&#35758;&#35760;&#24405;&#25340;&#25509;&#36215;&#26469;&#65292;&#24182;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#38544;&#24335;&#22320;&#23545;&#26631;&#35760;&#32423;&#21035;&#30340;&#26597;&#35810;&#30456;&#20851;&#24615;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38271;&#26102;&#38388;&#30340;&#20250;&#35758;&#35760;&#24405;&#23548;&#33268;&#20851;&#38190;&#30340;&#26597;&#35810;&#30456;&#20851;&#20449;&#24687;&#34987;&#31232;&#37322;&#65292;&#22240;&#27492;&#21407;&#22987;&#30340;&#22522;&#20110;&#36716;&#25442;&#30340;&#27169;&#22411;&#19981;&#36275;&#20197;&#31361;&#20986;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#20851;&#38190;&#37096;&#20998;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;-&#35805;&#35821;&#27880;&#24847;&#21147;&#21644;&#32852;&#21512;&#24314;&#27169;&#30340;&#26597;&#35810;&#24863;&#30693;&#26694;&#26550;&#12290;&#23427;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#27169;&#22359;&#35745;&#31639;&#35805;&#35821;&#32423;&#21035;&#19982;&#26597;&#35810;&#30340;&#30456;&#20851;&#24615;&#12290;&#28982;&#21518;&#65292;&#23558;&#26631;&#35760;&#32423;&#21035;&#30340;&#26597;&#35810;&#20851;&#32852;&#24615;&#21644;&#35805;&#35821;&#32423;&#21035;&#30340;&#26597;&#35810;&#20851;&#32852;&#24615;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#26126;&#30830;&#30340;&#27880;&#24847;&#26426;&#21046;&#25972;&#21512;&#21040;&#29983;&#25104;&#36807;&#31243;&#20013;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#19981;&#21516;&#39063;&#31890;&#24230;&#30340;&#26597;&#35810;&#30456;&#20851;&#24615;&#26377;&#21161;&#20110;&#29983;&#25104;&#19968;&#20010;&#26356;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#25688;&#35201;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;QFMS&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Query-focused meeting summarization (QFMS) aims to generate summaries from meeting transcripts in response to a given query. Previous works typically concatenate the query with meeting transcripts and implicitly model the query relevance only at the token level with attention mechanism. However, due to the dilution of key query-relevant information caused by long meeting transcripts, the original transformer-based model is insufficient to highlight the key parts related to the query. In this paper, we propose a query-aware framework with joint modeling token and utterance based on Query-Utterance Attention. It calculates the utterance-level relevance to the query with a dense retrieval module. Then both token-level query relevance and utterance-level query relevance are combined and incorporated into the generation process with attention mechanism explicitly. We show that the query relevance of different granularities contributes to generating a summary more related to the query. Exper
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24341;&#23548;&#26041;&#27861;&#65292;&#36890;&#36807;&#24050;&#30693;&#30340;&#38598;&#21512;&#21457;&#29616;&#26032;&#30340;&#24182;&#34892;&#38170;&#28857;&#65292;&#20197;&#20811;&#26381;&#30456;&#23545;&#34920;&#31034;&#24335;&#20013;&#33719;&#21462;&#24182;&#34892;&#38170;&#28857;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#29992;&#20110;&#19981;&#21516;&#22495;&#20043;&#38388;&#30340;&#35821;&#20041;&#23545;&#24212;&#65292;&#21462;&#24471;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.00721</link><description>&lt;p&gt;
&#20026;&#30456;&#23545;&#34920;&#31034;&#24335;&#24341;&#20837;&#24182;&#34892;&#38170;&#28857;&#30340;&#24341;&#23548;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bootstrapping Parallel Anchors for Relative Representations. (arXiv:2303.00721v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00721
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24341;&#23548;&#26041;&#27861;&#65292;&#36890;&#36807;&#24050;&#30693;&#30340;&#38598;&#21512;&#21457;&#29616;&#26032;&#30340;&#24182;&#34892;&#38170;&#28857;&#65292;&#20197;&#20811;&#26381;&#30456;&#23545;&#34920;&#31034;&#24335;&#20013;&#33719;&#21462;&#24182;&#34892;&#38170;&#28857;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#29992;&#20110;&#19981;&#21516;&#22495;&#20043;&#38388;&#30340;&#35821;&#20041;&#23545;&#24212;&#65292;&#21462;&#24471;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#23545;&#34920;&#31034;&#27861;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#37117;&#26174;&#31034;&#20986;&#20102;&#20316;&#20026;&#28508;&#22312;&#23884;&#20837;&#27861;&#30340;&#28508;&#21147;&#65292;&#20363;&#22914;&#21551;&#29992;&#28508;&#22312;&#31354;&#38388;&#36890;&#20449;&#21644;&#27169;&#22411;&#25340;&#25509;&#12290;&#28982;&#32780;&#65292;&#30456;&#23545;&#34920;&#31034;&#20381;&#36182;&#20110;&#19968;&#23450;&#37327;&#30340;&#24182;&#34892;&#38170;&#28857;&#20316;&#20026;&#36755;&#20837;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#38590;&#20197;&#33719;&#21462;&#36825;&#20123;&#38170;&#28857;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#26041;&#27861;&#65292;&#20174;&#26377;&#38480;&#30340;&#24050;&#30693;&#38598;&#21512;&#65288;&#31181;&#23376;&#65289;&#20013;&#21457;&#29616;&#26032;&#30340;&#24182;&#34892;&#38170;&#28857;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#22312;&#19981;&#21516;&#22495;&#20043;&#38388;&#23547;&#25214;&#35821;&#20041;&#23545;&#24212;&#65292;&#23545;&#40784;&#23427;&#20204;&#30340;&#30456;&#23545;&#31354;&#38388;&#65292;&#24182;&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;&#21462;&#24471;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of relative representations for latent embeddings has shown potential in enabling latent space communication and zero-shot model stitching across a wide range of applications. Nevertheless, relative representations rely on a certain amount of parallel anchors to be given as input, which can be impractical to obtain in certain scenarios. To overcome this limitation, we propose an optimization-based method to discover new parallel anchors from a limited known set (seed). Our approach can be used to find semantic correspondence between different domains, align their relative spaces, and achieve competitive results in several tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21382;&#21490;&#27010;&#36848;&#20197;&#21450;ChatGPT&#32972;&#21518;&#30340;&#25216;&#26415;&#65292;&#24378;&#35843;&#20102;&#23427;&#22312;&#21307;&#30103;&#20445;&#20581;&#12289;&#25945;&#32946;&#21644;&#30740;&#31350;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#24182;&#25351;&#20986;&#20102;&#20854;&#38544;&#31169;&#21644;&#36947;&#24503;&#26041;&#38754;&#30340;&#25285;&#24551;&#20197;&#21450;&#24403;&#21069;&#29256;&#26412;&#30340;&#37325;&#35201;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2302.13817</link><description>&lt;p&gt;
&#35753;&#25105;&#20204;&#26469;&#32842;&#32842;&#21543;&#65281;&#19982;ChatGPT&#30340;&#23545;&#35805;&#65306;&#25216;&#26415;&#65292;&#24212;&#29992;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Let's have a chat! A Conversation with ChatGPT: Technology, Applications, and Limitations. (arXiv:2302.13817v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21382;&#21490;&#27010;&#36848;&#20197;&#21450;ChatGPT&#32972;&#21518;&#30340;&#25216;&#26415;&#65292;&#24378;&#35843;&#20102;&#23427;&#22312;&#21307;&#30103;&#20445;&#20581;&#12289;&#25945;&#32946;&#21644;&#30740;&#31350;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#24182;&#25351;&#20986;&#20102;&#20854;&#38544;&#31169;&#21644;&#36947;&#24503;&#26041;&#38754;&#30340;&#25285;&#24551;&#20197;&#21450;&#24403;&#21069;&#29256;&#26412;&#30340;&#37325;&#35201;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#27454;&#33021;&#22815;&#29983;&#25104;&#20687;&#20154;&#31867;&#19968;&#26679;&#30340;&#21477;&#23376;&#21644;&#20889;&#20986;&#36830;&#36143;&#25991;&#31456;&#30340;&#20154;&#24037;&#26234;&#33021;&#32842;&#22825;&#26426;&#22120;&#20154;ChatGPT&#30340;&#20986;&#29616;&#24341;&#36215;&#20102;&#19990;&#30028;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21382;&#21490;&#27010;&#36848;&#20197;&#21450;ChatGPT&#32972;&#21518;&#30340;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#36824;&#24378;&#35843;&#20102;ChatGPT&#22312;&#21508;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#21307;&#30103;&#20445;&#20581;&#65292;&#25945;&#32946;&#21644;&#30740;&#31350;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;&#23613;&#31649;&#26377;&#30528;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#65292;&#20294;&#26159;ChatGPT&#21608;&#22260;&#23384;&#22312;&#30528;&#19968;&#20123;&#38544;&#31169;&#21644;&#36947;&#24503;&#26041;&#38754;&#30340;&#25285;&#24551;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#24403;&#21069;&#29256;&#26412;ChatGPT&#30340;&#19968;&#20123;&#37325;&#35201;&#38480;&#21046;&#12290;&#25105;&#20204;&#36824;&#21521;ChatGPT&#25552;&#20986;&#20102;&#19968;&#20123;&#38382;&#39064;&#65292;&#20197;&#20415;&#23427;&#34920;&#36798;&#33258;&#24049;&#30340;&#30475;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of an AI-powered chatbot that can generate human-like sentences and write coherent essays has caught the world's attention. This paper discusses the historical overview of chatbots and the technology behind Chat Generative Pre-trained Transformer, better known as ChatGPT. Moreover, potential applications of ChatGPT in various domains, including healthcare, education, and research, are highlighted. Despite promising results, there are several privacy and ethical concerns surrounding ChatGPT. In addition, we highlight some of the important limitations of the current version of ChatGPT. We also ask ChatGPT to provide its point of view and present its responses to several questions we attempt to answer.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#32534;&#30721;&#20013;&#23398;&#20064;&#24471;&#21040;&#30340;&#22823;&#20110;&#23454;&#38469;&#23383;&#20856;&#30340;&#24773;&#20917;&#19979;&#65292;&#22122;&#22768;&#20250;&#23548;&#33268;&#26631;&#20934;&#30340;&#23383;&#20856;&#23398;&#20064;&#30446;&#26631;&#20989;&#25968;&#26080;&#27861;&#24674;&#22797;&#20986;&#23454;&#38469;&#23383;&#20856;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#36974;&#30422;&#25968;&#25454;&#30340;&#26041;&#27861;&#36827;&#34892;&#21487;&#38752;&#30340;&#23383;&#20856;&#24674;&#22797;&#36866;&#29992;&#20110;&#22810;&#31181;&#20449;&#21495;&#27169;&#24577;&#12290;</title><link>http://arxiv.org/abs/2302.12715</link><description>&lt;p&gt;
&#36974;&#30422;&#25968;&#25454;&#26377;&#24110;&#21161;&#65306;&#20851;&#20110;&#31232;&#30095;&#32534;&#30721;&#20013;&#36974;&#30422;&#30340;&#22909;&#22788;
&lt;/p&gt;
&lt;p&gt;
Hiding Data Helps: On the Benefits of Masking for Sparse Coding. (arXiv:2302.12715v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#32534;&#30721;&#20013;&#23398;&#20064;&#24471;&#21040;&#30340;&#22823;&#20110;&#23454;&#38469;&#23383;&#20856;&#30340;&#24773;&#20917;&#19979;&#65292;&#22122;&#22768;&#20250;&#23548;&#33268;&#26631;&#20934;&#30340;&#23383;&#20856;&#23398;&#20064;&#30446;&#26631;&#20989;&#25968;&#26080;&#27861;&#24674;&#22797;&#20986;&#23454;&#38469;&#23383;&#20856;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#36974;&#30422;&#25968;&#25454;&#30340;&#26041;&#27861;&#36827;&#34892;&#21487;&#38752;&#30340;&#23383;&#20856;&#24674;&#22797;&#36866;&#29992;&#20110;&#22810;&#31181;&#20449;&#21495;&#27169;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#32534;&#30721;&#34987;&#29992;&#22312;&#20449;&#21495;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#20197;&#21450;&#21307;&#23398;&#25104;&#20687;&#31561;&#39046;&#22495;&#65292;&#22312;&#36825;&#20123;&#24212;&#29992;&#20013;&#65292;&#20449;&#21495;&#34987;&#24314;&#27169;&#20026;&#29992;&#23398;&#20064;&#21040;&#30340;&#23383;&#20856;&#20013;&#30340;&#20803;&#32032;&#30340;&#31232;&#30095;&#32447;&#24615;&#32452;&#21512;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22823;&#22810;&#25968;&#30740;&#31350;&#20851;&#27880;&#23398;&#20064;&#21040;&#30340;&#23383;&#20856;&#19982;&#30495;&#23454;&#23383;&#20856;&#22823;&#23567;&#30456;&#21516;&#26102;&#30340;&#24773;&#20917;&#65292;&#24182;&#19988;&#20165;&#30740;&#31350;&#20102;&#27809;&#26377;&#22122;&#22768;&#30340;&#24773;&#26223;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25968;&#25454;&#36974;&#30422;&#26041;&#27861;&#65292;&#33021;&#22815;&#20445;&#35777;&#22312;&#36974;&#30422;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#21487;&#38752;&#30340;&#23383;&#20856;&#24674;&#22797;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#20449;&#21495;&#27169;&#24577;&#19979;&#23545;&#25105;&#20204;&#30340;&#36974;&#30422;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#65292;&#24471;&#21040;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse coding, which refers to modeling a signal as sparse linear combinations of the elements of a learned dictionary, has proven to be a successful (and interpretable) approach in applications such as signal processing, computer vision, and medical imaging. While this success has spurred much work on provable guarantees for dictionary recovery when the learned dictionary is the same size as the ground-truth dictionary, work on the setting where the learned dictionary is larger (or over-realized) with respect to the ground truth is comparatively nascent. Existing theoretical results in this setting have been constrained to the case of noise-less data. We show in this work that, in the presence of noise, minimizing the standard dictionary learning objective can fail to recover the elements of the ground-truth dictionary in the over-realized regime, regardless of the magnitude of the signal in the data-generating process. Furthermore, drawing from the growing body of work on self-superv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;K-SHAP&#30340;&#31639;&#27861;&#65292;&#26469;&#35299;&#20915;&#22810;&#20010;&#26234;&#33021;&#20307;&#20445;&#25345;&#21311;&#21517;&#19988;&#20165;&#26377;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#26234;&#33021;&#20307;&#20915;&#31574;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.11996</link><description>&lt;p&gt;
K-SHAP: &#19968;&#31181;&#29992;&#20110;&#21311;&#21517;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#31574;&#30053;&#32858;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
K-SHAP: Policy Clustering Algorithm for Anonymous State-Action Pairs. (arXiv:2302.11996v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;K-SHAP&#30340;&#31639;&#27861;&#65292;&#26469;&#35299;&#20915;&#22810;&#20010;&#26234;&#33021;&#20307;&#20445;&#25345;&#21311;&#21517;&#19988;&#20165;&#26377;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#26234;&#33021;&#20307;&#20915;&#31574;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#26234;&#33021;&#20307;&#34892;&#20026;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#25105;&#20204;&#23545;&#23427;&#20204;&#20915;&#31574;&#36807;&#31243;&#30340;&#29702;&#35299;&#65292;&#20174;&#32780;&#22686;&#24378;&#25105;&#20204;&#35299;&#37322;&#23427;&#20204;&#19982;&#29615;&#22659;&#21644;&#20854;&#20182;&#26234;&#33021;&#20307;&#20043;&#38388;&#20132;&#20114;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#25991;&#29486;&#20013;&#24050;&#32463;&#25552;&#20986;&#20102;&#22810;&#31181;&#23398;&#20064;&#25216;&#26415;&#65292;&#20294;&#36824;&#26377;&#19968;&#31181;&#29305;&#23450;&#30340;&#24773;&#20917;&#23578;&#26410;&#34987;&#25506;&#32034;&#65292;&#37027;&#23601;&#26159;&#26234;&#33021;&#20307;&#36523;&#20221;&#20445;&#25345;&#21311;&#21517;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#12290;&#20363;&#22914;&#65292;&#22312;&#37329;&#34701;&#24066;&#22330;&#20013;&#65292;&#26631;&#35760;&#25968;&#25454;&#36890;&#24120;&#26159;&#19987;&#26377;&#30340;&#65292;&#20165;&#20844;&#24320;&#22810;&#20010;&#24066;&#22330;&#21442;&#19982;&#32773;&#20132;&#20114;&#32780;&#20135;&#29983;&#30340;&#21311;&#21517;&#29366;&#24577;-&#21160;&#20316;&#23545;&#12290;&#22240;&#27492;&#65292;&#26234;&#33021;&#20307;&#34892;&#21160;&#24207;&#21015;&#19981;&#21487;&#35266;&#27979;&#65292;&#38480;&#21046;&#20102;&#29616;&#26377;&#24037;&#20316;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#32858;&#31867;&#31639;&#27861;K-SHAP&#65292;&#23427;&#23398;&#20064;&#26681;&#25454;&#26234;&#33021;&#20307;&#31574;&#30053;&#23545;&#21311;&#21517;&#29366;&#24577;-&#21160;&#20316;&#23545;&#36827;&#34892;&#20998;&#32452;&#12290;&#25105;&#20204;&#23558;&#35813;&#38382;&#39064;&#20316;&#20026;&#27169;&#20223;&#23398;&#20064;(IL)&#20219;&#21153;&#65292;&#23398;&#20064;&#19968;&#20010;w...
&lt;/p&gt;
&lt;p&gt;
Learning agent behaviors from observational data has shown to improve our understanding of their decision-making processes, advancing our ability to explain their interactions with the environment and other agents. While multiple learning techniques have been proposed in the literature, there is one particular setting that has not been explored yet: multi agent systems where agent identities remain anonymous. For instance, in financial markets labeled data that identifies market participant strategies is typically proprietary, and only the anonymous state-action pairs that result from the interaction of multiple market participants are publicly available. As a result, sequences of agent actions are not observable, restricting the applicability of existing work. In this paper, we propose a Policy Clustering algorithm, called K-SHAP, that learns to group anonymous state-action pairs according to the agent policies. We frame the problem as an Imitation Learning (IL) task, and we learn a w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#26234;&#33021;&#21452;&#36275;&#26426;&#22120;&#20154;&#36339;&#36291;&#25511;&#21046;&#26694;&#26550;&#65292;&#21487;&#20197;&#25191;&#34892;&#21508;&#31181;&#36339;&#36291;&#20219;&#21153;&#65292;&#20855;&#26377;&#22810;&#29992;&#36884;&#21644;&#31283;&#20581;&#24615;&#65292;&#19988;&#21487;&#30452;&#25509;&#36716;&#31227;&#21040;&#30495;&#23454;&#26426;&#22120;&#20154;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.09450</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#31283;&#20581;&#19988;&#22810;&#29992;&#36884;&#30340;&#21452;&#36275;&#36339;&#36291;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Robust and Versatile Bipedal Jumping Control through Reinforcement Learning. (arXiv:2302.09450v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#26234;&#33021;&#21452;&#36275;&#26426;&#22120;&#20154;&#36339;&#36291;&#25511;&#21046;&#26694;&#26550;&#65292;&#21487;&#20197;&#25191;&#34892;&#21508;&#31181;&#36339;&#36291;&#20219;&#21153;&#65292;&#20855;&#26377;&#22810;&#29992;&#36884;&#21644;&#31283;&#20581;&#24615;&#65292;&#19988;&#21487;&#30452;&#25509;&#36716;&#31227;&#21040;&#30495;&#23454;&#26426;&#22120;&#20154;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20351;&#25197;&#30697;&#25511;&#21046;&#30340;&#21452;&#36275;&#26426;&#22120;&#20154;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#36827;&#34892;&#31283;&#20581;&#19988;&#22810;&#29992;&#36884;&#30340;&#36339;&#36291;&#65292;&#23558;&#21452;&#36275;&#26426;&#22120;&#20154;&#30340;&#25935;&#25463;&#24615;&#25552;&#21319;&#21040;&#26497;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#20154;&#23436;&#25104;&#21508;&#31181;&#36339;&#36291;&#20219;&#21153;&#65292;&#22914;&#36339;&#21521;&#19981;&#21516;&#30340;&#20301;&#32622;&#21644;&#26041;&#21521;&#12290;&#20026;&#20102;&#25552;&#39640;&#22312;&#36825;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#32467;&#26500;&#65292;&#23427;&#32534;&#30721;&#20102;&#26426;&#22120;&#20154;&#30340;&#38271;&#26399;&#36755;&#20837;/&#36755;&#20986; (I/O) &#21382;&#21490;&#35760;&#24405;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#23545;&#30701;&#26399; I/O &#21382;&#21490;&#35760;&#24405;&#30340;&#30452;&#25509;&#35775;&#38382;&#12290;&#20026;&#20102;&#35757;&#32451;&#20855;&#26377;&#22810;&#29992;&#36884;&#36339;&#36291;&#31574;&#30053;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#22810;&#38454;&#27573;&#30340;&#35757;&#32451;&#26041;&#26696;&#65292;&#20854;&#20013;&#21253;&#25324;&#19981;&#21516;&#30446;&#26631;&#30340;&#19981;&#21516;&#35757;&#32451;&#38454;&#27573;&#12290;&#22312;&#22810;&#38454;&#27573;&#35757;&#32451;&#20043;&#21518;&#65292;&#35813;&#31574;&#30053;&#21487;&#20197;&#30452;&#25509;&#36716;&#31227;&#21040;&#30495;&#23454;&#30340;&#21452;&#36275; Cassie &#26426;&#22120;&#20154;&#19978;&#12290;&#23545;&#19981;&#21516;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#21644;&#25506;&#32034;&#26356;&#22810;&#26679;&#21270;&#30340;&#24773;&#26223;&#20250;&#23548;&#33268;&#39640;&#24230;&#31283;&#20581;&#30340;&#31574;&#30053;&#65292;&#36825;&#20123;&#31574;&#30053;&#21487;&#20197;&#21033;&#29992;&#25152;&#23398;&#20064;&#30340;&#21508;&#31181;&#26426;&#21160;&#26041;&#24335;&#26469;&#20174;&#25200;&#21160;&#25110;&#19981;&#33391;&#30528;&#38470;&#20013;&#24674;&#22797;&#36807;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work aims to push the limits of agility for bipedal robots by enabling a torque-controlled bipedal robot to perform robust and versatile dynamic jumps in the real world. We present a reinforcement learning framework for training a robot to accomplish a large variety of jumping tasks, such as jumping to different locations and directions. To improve performance on these challenging tasks, we develop a new policy structure that encodes the robot's long-term input/output (I/O) history while also providing direct access to a short-term I/O history. In order to train a versatile jumping policy, we utilize a multi-stage training scheme that includes different training stages for different objectives. After multi-stage training, the policy can be directly transferred to a real bipedal Cassie robot. Training on different tasks and exploring more diverse scenarios lead to highly robust policies that can exploit the diverse set of learned maneuvers to recover from perturbations or poor land
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29983;&#25104;Sokoban&#28216;&#25103;&#20851;&#21345;&#65292;&#24182;&#21457;&#29616;&#38543;&#30528;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;LLMs&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#24456;&#22823;&#30340;&#25552;&#39640;&#12290;&#26410;&#26469;&#24037;&#20316;&#30340;&#21069;&#26223;&#20063;&#34987;&#35752;&#35770;&#20102;&#12290;</title><link>http://arxiv.org/abs/2302.05817</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20851;&#21345;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Level Generation Through Large Language Models. (arXiv:2302.05817v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05817
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29983;&#25104;Sokoban&#28216;&#25103;&#20851;&#21345;&#65292;&#24182;&#21457;&#29616;&#38543;&#30528;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;LLMs&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#24456;&#22823;&#30340;&#25552;&#39640;&#12290;&#26410;&#26469;&#24037;&#20316;&#30340;&#21069;&#26223;&#20063;&#34987;&#35752;&#35770;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26159;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#33021;&#22815;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;&#35757;&#32451;&#20889;&#25925;&#20107;&#12289;&#29983;&#25104;&#20195;&#30721;&#21644;&#22238;&#31572;&#38382;&#39064;&#12290;&#20294;&#23427;&#20204;&#33021;&#21542;&#29983;&#25104;&#21151;&#33021;&#24615;&#30340;&#35270;&#39057;&#28216;&#25103;&#20851;&#21345;&#21602;&#65311;&#28216;&#25103;&#20851;&#21345;&#30001;&#20110;&#21151;&#33021;&#32422;&#26463;&#21644;&#22810;&#32500;&#31354;&#38388;&#20851;&#31995;&#30340;&#22797;&#26434;&#24615;&#65292;&#19982;LLM&#22312;&#35757;&#32451;&#26399;&#38388;&#36890;&#24120;&#30475;&#21040;&#30340;&#25968;&#25454;&#31181;&#31867;&#38750;&#24120;&#19981;&#21516;&#12290;&#28216;&#25103;&#20851;&#21345;&#30340;&#25968;&#25454;&#38598;&#20063;&#24456;&#38590;&#33719;&#24471;&#65292;&#21487;&#33021;&#20250;&#32791;&#23613;&#36825;&#20123;&#23545;&#25968;&#25454;&#26377;&#24378;&#28872;&#38656;&#27714;&#30340;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;LLMs&#29983;&#25104;Sokoban&#28216;&#25103;&#20851;&#21345;&#65292;&#24182;&#21457;&#29616;LLMs&#30830;&#23454;&#33021;&#22815;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#24182;&#19988;&#23427;&#30340;&#24615;&#33021;&#38543;&#30528;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#22823;&#24133;&#25552;&#39640;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#21021;&#27493;&#30340;&#23454;&#39564;&#26469;&#25511;&#21046;LLM&#20851;&#21345;&#29983;&#25104;&#22120;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#26377;&#21069;&#36884;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are powerful tools, capable of leveraging their training on natural language to write stories, generate code, and answer questions. But can they generate functional video game levels? Game levels, with their complex functional constraints and spatial relationships in more than one dimension, are very different from the kinds of data an LLM typically sees during training. Datasets of game levels are also hard to come by, potentially taxing the abilities of these data-hungry models. We investigate the use of LLMs to generate levels for the game Sokoban, finding that LLMs are indeed capable of doing so, and that their performance scales dramatically with dataset size. We also perform preliminary experiments on controlling LLM level generators and discuss promising areas for future work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#26356;&#22909;&#30340;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#23545;&#25239;&#35757;&#32451;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#37319;&#29992;&#26368;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#35757;&#32451;&#30340;&#27169;&#22411;&#20165;&#20351;&#29992;&#29983;&#25104;&#30340;&#25968;&#25454;&#23601;&#22312;RobustBench&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#22312;CIFAR-10&#21644;CIFAR-100&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#25552;&#39640;&#20102;$+4.58\%$&#21644;$+8.03\%$&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.04638</link><description>&lt;p&gt;
&#26356;&#22909;&#30340;&#25193;&#25955;&#27169;&#22411;&#36827;&#19968;&#27493;&#25913;&#36827;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Better Diffusion Models Further Improve Adversarial Training. (arXiv:2302.04638v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#26356;&#22909;&#30340;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#23545;&#25239;&#35757;&#32451;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#37319;&#29992;&#26368;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#35757;&#32451;&#30340;&#27169;&#22411;&#20165;&#20351;&#29992;&#29983;&#25104;&#30340;&#25968;&#25454;&#23601;&#22312;RobustBench&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#22312;CIFAR-10&#21644;CIFAR-100&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#25552;&#39640;&#20102;$+4.58\%$&#21644;$+8.03\%$&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#30001;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#20135;&#29983;&#30340;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#23545;&#25239;&#24615;&#35757;&#32451;&#12290;&#38543;&#30528;&#25193;&#25955;&#27169;&#22411;&#30340;&#36805;&#36895;&#21457;&#23637;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#26356;&#22909;&#30340;&#25193;&#25955;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#36827;&#19968;&#27493;&#25913;&#36827;&#23545;&#25239;&#35757;&#32451;&#65311;&#26412;&#25991;&#37319;&#29992;&#20102;&#26368;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#19982;DDPM&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#65288;&#32422;$\sim20$&#20010;&#37319;&#26679;&#27493;&#39588;&#65289;&#21644;&#26356;&#20302;&#30340;&#22270;&#20687;&#36136;&#37327;&#65288;&#26356;&#20302;&#30340;FID&#20998;&#25968;&#65289;&#65292;&#35777;&#26126;&#20102;&#26356;&#22909;&#30340;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#23545;&#25239;&#35757;&#32451;&#12290;&#25105;&#20204;&#35757;&#32451;&#30340;&#27169;&#22411;&#20165;&#20351;&#29992;&#29983;&#25104;&#30340;&#25968;&#25454;&#65288;&#27809;&#26377;&#22806;&#37096;&#25968;&#25454;&#38598;&#65289;&#22312;RobustBench&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;&#22312;$\ell_\infty$-norm&#23041;&#32961;&#27169;&#22411;&#19979;&#65292;&#24403;$\epsilon=8/255$&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;CIFAR-10&#21644;CIFAR-100&#19978;&#20998;&#21035;&#36798;&#21040;$70.69\%$&#21644;$42.67\%$&#30340;&#40065;&#26834;&#20934;&#30830;&#24230;&#65292;&#21363;&#20998;&#21035;&#27604;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#25552;&#39640;&#20102;$+4.58\%$&#21644;$+8.03\%$&#12290;&#22312;$\ell_2$-norm&#23041;&#32961;&#27169;&#22411;&#19979;&#65292;&#24403;$\epsilon=128/255$&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;CIFAR-10&#19978;&#21487;&#20197;&#36798;&#21040;$84.86\%$&#30340;&#20934;&#30830;&#29575;&#65292;&#25552;&#39640;&#20102;$+4.44\%$&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been recognized that the data generated by the denoising diffusion probabilistic model (DDPM) improves adversarial training. After two years of rapid development in diffusion models, a question naturally arises: can better diffusion models further improve adversarial training? This paper gives an affirmative answer by employing the most recent diffusion model which has higher efficiency ($\sim 20$ sampling steps) and image quality (lower FID score) compared with DDPM. Our adversarially trained models achieve state-of-the-art performance on RobustBench using only generated data (no external datasets). Under the $\ell_\infty$-norm threat model with $\epsilon=8/255$, our models achieve $70.69\%$ and $42.67\%$ robust accuracy on CIFAR-10 and CIFAR-100, respectively, i.e. improving upon previous state-of-the-art models by $+4.58\%$ and $+8.03\%$. Under the $\ell_2$-norm threat model with $\epsilon=128/255$, our models achieve $84.86\%$ on CIFAR-10 ($+4.44\%$). These results also beat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#19968;&#20123;&#25216;&#24039;&#29992;&#20110;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#25552;&#21462;&#65292;&#25552;&#20986;&#20102;&#22312;&#25991;&#26412;&#29983;&#25104;&#21644;&#25991;&#26412;&#25490;&#21517;&#20013;&#21487;&#20197;&#20351;&#29992;&#30340;&#25216;&#24039;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#25216;&#24039;&#23545;&#20110;&#25552;&#39640;&#35757;&#32451;&#25968;&#25454;&#25552;&#21462;&#30340;&#25928;&#26524;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2302.04460</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#25552;&#21462;&#30340;&#25216;&#24039;&#24635;&#32467;
&lt;/p&gt;
&lt;p&gt;
Bag of Tricks for Training Data Extraction from Language Models. (arXiv:2302.04460v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#19968;&#20123;&#25216;&#24039;&#29992;&#20110;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#25552;&#21462;&#65292;&#25552;&#20986;&#20102;&#22312;&#25991;&#26412;&#29983;&#25104;&#21644;&#25991;&#26412;&#25490;&#21517;&#20013;&#21487;&#20197;&#20351;&#29992;&#30340;&#25216;&#24039;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#25216;&#24039;&#23545;&#20110;&#25552;&#39640;&#35757;&#32451;&#25968;&#25454;&#25552;&#21462;&#30340;&#25928;&#26524;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#26029;&#30740;&#31350;&#65292;&#38544;&#31169;&#20445;&#25252;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#35757;&#32451;&#25968;&#25454;&#25552;&#21462;&#20316;&#20026;&#28508;&#22312;&#30340;&#35780;&#20272;&#38544;&#31169;&#27844;&#38706;&#30340;&#24037;&#20855;&#21464;&#24471;&#38750;&#24120;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#39033;&#20219;&#21153;&#30340;&#22256;&#38590;&#31243;&#24230;&#65292;&#30446;&#21069;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#20173;&#28982;&#19981;&#22815;&#26377;&#25928;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#25216;&#24039;&#29992;&#20110;&#25913;&#36827;&#35757;&#32451;&#25968;&#25454;&#25552;&#21462;&#65292;&#25105;&#20204;&#23545;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#30001;&#20110;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25552;&#21462;&#26041;&#27861;&#20351;&#29992;&#29983;&#25104;&#28982;&#21518;&#25490;&#24207;&#30340;&#27969;&#31243;&#65288;&#20363;&#22914;&#65292;&#29983;&#25104;&#28508;&#22312;&#30340;&#35757;&#32451;&#25968;&#25454;&#25991;&#26412;&#65292;&#28982;&#21518;&#26681;&#25454;&#29305;&#23450;&#30340;&#26631;&#20934;&#23545;&#23427;&#20204;&#36827;&#34892;&#25490;&#24207;&#65289;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#25991;&#26412;&#29983;&#25104;&#21644;&#25991;&#26412;&#25490;&#21517;&#30340;&#25216;&#24039;&#12290;&#65288;&#20363;&#22914;&#65292;&#37319;&#26679;&#31574;&#30053;&#21644;&#20196;&#29260;&#32423;&#26631;&#20934;&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#20123;&#20043;&#21069;&#34987;&#24573;&#35270;&#30340;&#25216;&#24039;&#23545;&#20110;&#35757;&#32451;&#25968;&#25454;&#25552;&#21462;&#30340;&#25104;&#21151;&#38750;&#24120;&#20851;&#38190;&#12290;&#22522;&#20110;GPT-Neo 1.3B&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#24039;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advance of language models, privacy protection is receiving more attention. Training data extraction is therefore of great importance, as it can serve as a potential tool to assess privacy leakage. However, due to the difficulty of this task, most of the existing methods are proof-of-concept and still not effective enough. In this paper, we investigate and benchmark tricks for improving training data extraction using a publicly available dataset. Because most existing extraction methods use a pipeline of generating-then-ranking, i.e., generating text candidates as potential training data and then ranking them based on specific criteria, our research focuses on the tricks for both text generation (e.g., sampling strategy) and text ranking (e.g., token-level criteria). The experimental results show that several previously overlooked tricks can be crucial to the success of training data extraction. Based on the GPT-Neo 1.3B evaluation results, our proposed tricks outperform the b
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#22686;&#24378;&#12289;&#32593;&#32476;&#26550;&#26500;&#21644;&#35757;&#32451;&#31639;&#27861;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#20026;SSL&#20174;&#19994;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#20123;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2302.02774</link><description>&lt;p&gt;
SSL&#30340;&#30456;&#20114;&#20316;&#29992;&#65306;&#22686;&#24378;&#12289;&#24402;&#32435;&#20559;&#24046;&#21644;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
The SSL Interplay: Augmentations, Inductive Bias, and Generalization. (arXiv:2302.02774v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#22686;&#24378;&#12289;&#32593;&#32476;&#26550;&#26500;&#21644;&#35757;&#32451;&#31639;&#27861;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#20026;SSL&#20174;&#19994;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#20123;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#26080;&#38656;&#30417;&#30563;&#21363;&#21487;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#23398;&#20064;&#34920;&#31034;&#30340;&#24378;&#22823;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#24037;&#31243;&#24072;&#38754;&#20020;&#30528;&#35843;&#25972;&#20248;&#21270;&#22120;&#21644;&#35757;&#32451;&#36807;&#31243;&#20013;&#34920;&#31034;&#22604;&#38519;&#31561;&#38382;&#39064;&#12290;&#36825;&#20123;&#25361;&#25112;&#20419;&#20351;&#25105;&#20204;&#38656;&#35201;&#19968;&#31181;&#29702;&#35770;&#26469;&#38416;&#26126;&#25968;&#25454;&#22686;&#24378;&#12289;&#32593;&#32476;&#26550;&#26500;&#21644;&#35757;&#32451;&#31639;&#27861;&#30340;&#36873;&#25321;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#29702;&#35770;&#21451;&#22909;&#30340;&#35774;&#32622;&#19979;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#24615;&#33021;&#36827;&#34892;&#31934;&#30830;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#36825;&#31181;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#24378;&#35843;&#20102;&#25105;&#20204;&#29702;&#35770;&#24471;&#20986;&#30340;SSL&#20174;&#19994;&#32773;&#30340;&#19968;&#20123;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has emerged as a powerful framework to learn representations from raw data without supervision. Yet in practice, engineers face issues such as instability in tuning optimizers and collapse of representations during training. Such challenges motivate the need for a theory to shed light on the complex interplay between the choice of data augmentation, network architecture, and training algorithm. We study such an interplay with a precise analysis of generalization performance on both pretraining and downstream tasks in a theory friendly setup, and highlight several insights for SSL practitioners that arise from our theory.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#21487;&#23398;&#20064;&#27604;&#20363;&#22240;&#23376;&#21152;&#21095;&#20102;&#26435;&#37325;&#25391;&#33633;&#65292;&#26412;&#25991;&#25552;&#20986;&#19977;&#31181;&#25216;&#26415;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.02210</link><description>&lt;p&gt;
&#20302;&#20301;&#35270;&#35273;&#21464;&#21387;&#22120;&#30340;&#26080;&#25391;&#33633;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Oscillation-free Quantization for Low-bit Vision Transformers. (arXiv:2302.02210v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02210
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#21487;&#23398;&#20064;&#27604;&#20363;&#22240;&#23376;&#21152;&#21095;&#20102;&#26435;&#37325;&#25391;&#33633;&#65292;&#26412;&#25991;&#25552;&#20986;&#19977;&#31181;&#25216;&#26415;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#24847;&#35782;&#35757;&#32451;&#30340;&#19968;&#20010;&#19981;&#33391;&#21103;&#20316;&#29992;&#26159;&#26435;&#37325;&#25391;&#33633;&#65292;&#20854;&#20013;&#37327;&#21270;&#26435;&#37325;&#32463;&#24120;&#22312;&#20004;&#20010;&#37327;&#21270;&#32423;&#21035;&#20043;&#38388;&#36339;&#21160;&#65292;&#23548;&#33268;&#35757;&#32451;&#19981;&#31283;&#23450;&#21644;&#23376;&#20248;&#21270;&#30340;&#26368;&#32456;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#21487;&#23398;&#20064;&#30340;&#27604;&#20363;&#22240;&#23376;&#8212;&#8212;&#22312;&#37327;&#21270;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;$\textit{de facto}$&#35774;&#32622;&#8212;&#8212;&#21152;&#21095;&#20102;&#26435;&#37325;&#25391;&#33633;&#12290;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#21487;&#23398;&#20064;&#27604;&#20363;&#22240;&#23376;&#19982;&#37327;&#21270;&#26435;&#37325;&#25391;&#33633;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20197;ViT&#20026;&#26696;&#20363;&#26469;&#35828;&#26126;&#21457;&#29616;&#21644;&#35299;&#20915;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#33258;&#27880;&#24847;&#21147;&#23618;&#20013;&#37327;&#21270;&#26435;&#37325;&#30340;$\textit{query}$&#21644;$\textit{key}$&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#23384;&#20351;ViT&#23481;&#26131;&#21463;&#21040;&#25391;&#33633;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30456;&#24212;&#22320;&#25552;&#20986;&#20102;&#19977;&#31181;&#25216;&#26415;&#65306;&#32479;&#35745;&#26435;&#37325;&#37327;&#21270;&#65288;$\rm StatsQ$&#65289;&#20197;&#25913;&#21892;&#37327;&#21270;&#40065;&#26834;&#24615;&#65292;&#19982;&#26222;&#36941;&#20351;&#29992;&#30340;&#21487;&#23398;&#20064;&#27604;&#20363;&#22240;&#23376;&#26041;&#27861;&#30456;&#27604;&#65307;&#32622;&#20449;&#24230;&#24341;&#23548;&#30340;&#36864;&#28779;&#65288;$\rm CGA$&#65289;&#22312;&#35757;&#32451;&#26399;&#38388;&#20923;&#32467;&#20855;&#26377;$\textit{&#39640;&#32622;&#20449;&#24230;}$&#30340;&#26435;&#37325;&#65292;&#20197;&#20943;&#23569;&#26435;&#37325;&#25391;&#33633;&#65307;&#20197;&#21450;&#30456;&#20114;&#20381;&#36182;&#26435;&#37325;&#30340;&#22343;&#34913;&#65288;$\rm IWEqual$&#65289;&#65292;&#20197;&#26377;&#25928;&#22788;&#29702;&#30456;&#20114;&#20381;&#36182;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#27604;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weight oscillation is an undesirable side effect of quantization-aware training, in which quantized weights frequently jump between two quantized levels, resulting in training instability and a sub-optimal final model. We discover that the learnable scaling factor, a widely-used $\textit{de facto}$ setting in quantization aggravates weight oscillation. In this study, we investigate the connection between the learnable scaling factor and quantized weight oscillation and use ViT as a case driver to illustrate the findings and remedies. In addition, we also found that the interdependence between quantized weights in $\textit{query}$ and $\textit{key}$ of a self-attention layer makes ViT vulnerable to oscillation. We, therefore, propose three techniques accordingly: statistical weight quantization ($\rm StatsQ$) to improve quantization robustness compared to the prevalent learnable-scale-based method; confidence-guided annealing ($\rm CGA$) that freezes the weights with $\textit{high confi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#20165;&#22788;&#29702;&#25991;&#26412;&#30340;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#20687;&#36827;&#34892;&#32852;&#31995;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#19982;&#26816;&#32034;&#22270;&#20687;&#20132;&#38169;&#30340;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#12290;&#35813;&#26041;&#27861;&#22312;&#29615;&#22659;&#30456;&#20851;&#30340;&#22270;&#20687;&#26816;&#32034;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#21313;&#20998;&#20248;&#24322;&#65292;&#26159;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#35270;&#35273;&#22330;&#26223;&#19979;&#20132;&#20114;&#38382;&#39064;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2301.13823</link><description>&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#20687;&#36827;&#34892;&#32852;&#31995;&#20197;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Grounding Language Models to Images for Multimodal Inputs and Outputs. (arXiv:2301.13823v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13823
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#20165;&#22788;&#29702;&#25991;&#26412;&#30340;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#20687;&#36827;&#34892;&#32852;&#31995;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#19982;&#26816;&#32034;&#22270;&#20687;&#20132;&#38169;&#30340;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#12290;&#35813;&#26041;&#27861;&#22312;&#29615;&#22659;&#30456;&#20851;&#30340;&#22270;&#20687;&#26816;&#32034;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#21313;&#20998;&#20248;&#24322;&#65292;&#26159;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#35270;&#35273;&#22330;&#26223;&#19979;&#20132;&#20114;&#38382;&#39064;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#20165;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#19982;&#35270;&#35273;&#39046;&#22495;&#32852;&#31995;&#36215;&#26469;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#19982;&#26816;&#32034;&#22270;&#20687;&#20132;&#38169;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#21033;&#29992;&#20174;&#22823;&#35268;&#27169;&#25991;&#26412;&#39044;&#35757;&#32451;&#20013;&#23398;&#21040;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#29983;&#25104;&#12290;&#25105;&#20204;&#20445;&#25345;&#35821;&#35328;&#27169;&#22411;&#20923;&#32467;&#65292;&#24182;&#24494;&#35843;&#36755;&#20837;&#21644;&#36755;&#20986;&#32447;&#24615;&#23618;&#20197;&#23454;&#29616;&#36328;&#27169;&#24577;&#20132;&#20114;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#65292;&#24182;&#29983;&#25104;&#19982;&#26816;&#32034;&#22270;&#20687;&#20132;&#38169;&#30340;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#12290;&#25105;&#20204;&#22312;&#29615;&#22659;&#30456;&#20851;&#30340;&#22270;&#20687;&#26816;&#32034;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#38646;-shot&#34920;&#29616;&#65292;&#24182;&#23637;&#31034;&#20102;&#24341;&#20154;&#20837;&#32988;&#30340;&#20132;&#20114;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#20219;&#20309;&#29616;&#25104;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#22312;&#35270;&#35273;&#22330;&#26223;&#19979;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#25928;&#19988;&#36890;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an efficient method to ground pretrained text-only language models to the visual domain, enabling them to process arbitrarily interleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.
&lt;/p&gt;</description></item><item><title>PDVN&#26159;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#35757;&#32451;&#31639;&#27861;&#65292;&#23427;&#22312;&#36870;&#21521;&#21512;&#25104;&#35268;&#21010;&#20013;&#21033;&#29992;&#21452;&#20215;&#20540;&#32593;&#32476;&#20248;&#21270;&#23436;&#25972;&#30340;&#36335;&#32447;&#65292;&#25104;&#21151;&#29575;&#21644;&#25928;&#29575;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.13755</link><description>&lt;p&gt;
&#21452;&#20215;&#20540;&#32593;&#32476;&#22312;&#36870;&#21521;&#21512;&#25104;&#35268;&#21010;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Retrosynthetic Planning with Dual Value Networks. (arXiv:2301.13755v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13755
&lt;/p&gt;
&lt;p&gt;
PDVN&#26159;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#35757;&#32451;&#31639;&#27861;&#65292;&#23427;&#22312;&#36870;&#21521;&#21512;&#25104;&#35268;&#21010;&#20013;&#21033;&#29992;&#21452;&#20215;&#20540;&#32593;&#32476;&#20248;&#21270;&#23436;&#25972;&#30340;&#36335;&#32447;&#65292;&#25104;&#21151;&#29575;&#21644;&#25928;&#29575;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#21521;&#21512;&#25104;&#26088;&#22312;&#20174;&#21830;&#19994;&#19978;&#21487;&#24471;&#30340;&#36215;&#22987;&#26448;&#26009;&#20013;&#25214;&#21040;&#21512;&#25104;&#30446;&#26631;&#20998;&#23376;&#30340;&#36335;&#32447;&#65292;&#26159;&#33647;&#29289;&#21457;&#29616;&#21644;&#26448;&#26009;&#35774;&#35745;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21333;&#27493;&#21453;&#24212;&#39044;&#27979;&#22120;&#19982;&#22810;&#27493;&#35268;&#21010;&#22120;&#30340;&#32452;&#21512;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#21333;&#27493;&#39044;&#27979;&#22120;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#26159;&#31163;&#32447;&#35757;&#32451;&#30340;&#65292;&#21482;&#20248;&#21270;&#21333;&#27493;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#19981;&#32771;&#34385;&#23436;&#25972;&#30340;&#36335;&#32447;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#35757;&#32451;&#31639;&#27861;PDVN&#65292;&#36890;&#36807;&#20351;&#29992;&#26641;&#24418;MDP&#26469;&#20248;&#21270;&#23436;&#25972;&#30340;&#36335;&#32447;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25913;&#21892;&#21333;&#27493;&#39044;&#27979;&#22120;&#12290;&#22312;PDVN&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#20004;&#20010;&#21333;&#29420;&#30340;&#20215;&#20540;&#32593;&#32476;&#65292;&#20998;&#21035;&#39044;&#27979;&#20998;&#23376;&#30340;&#21487;&#21512;&#25104;&#24615;&#21644;&#25104;&#26412;&#12290;&#20026;&#20102;&#20445;&#25345;&#21333;&#27493;&#39044;&#27979;&#22120;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#20998;&#25903;&#32593;&#32476;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrosynthesis, which aims to find a route to synthesize a target molecule from commercially available starting materials, is a critical task in drug discovery and materials design. Recently, the combination of ML-based single-step reaction predictors with multi-step planners has led to promising results. However, the single-step predictors are mostly trained offline to optimize the single-step accuracy, without considering complete routes. Here, we leverage reinforcement learning (RL) to improve the single-step predictor, by using a tree-shaped MDP to optimize complete routes. Specifically, we propose a novel online training algorithm, called Planning with Dual Value Networks (PDVN), which alternates between the planning phase and updating phase. In PDVN, we construct two separate value networks to predict the synthesizability and cost of molecules, respectively. To maintain the single-step accuracy, we design a two-branch network structure for the single-step predictor. On the widely
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#26032;&#20844;&#24179;&#24230;&#37327;&#26041;&#27861;&#65306;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#26354;&#32447;&#20043;&#38388;&#21306;&#22495;&#65288;ABPC&#65289;&#21644;&#32047;&#31215;&#23494;&#24230;&#20989;&#25968;&#26354;&#32447;&#20043;&#38388;&#21306;&#22495;&#65288;ABCC&#65289;&#65292;&#23454;&#29616;&#20154;&#21475;&#32479;&#35745;&#29305;&#24449;&#30340;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;</title><link>http://arxiv.org/abs/2301.13443</link><description>&lt;p&gt;
&#26032;&#30340;&#20998;&#24067;&#27700;&#24179;&#24230;&#37327;&#26041;&#27861;&#65306;&#24323;&#29992;$\Delta$DP&#65292;&#23454;&#29616;&#20154;&#21475;&#32479;&#35745;&#29305;&#24449;&#30340;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Retiring $\Delta$DP: New Distribution-Level Metrics for Demographic Parity. (arXiv:2301.13443v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#26032;&#20844;&#24179;&#24230;&#37327;&#26041;&#27861;&#65306;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#26354;&#32447;&#20043;&#38388;&#21306;&#22495;&#65288;ABPC&#65289;&#21644;&#32047;&#31215;&#23494;&#24230;&#20989;&#25968;&#26354;&#32447;&#20043;&#38388;&#21306;&#22495;&#65288;ABCC&#65289;&#65292;&#23454;&#29616;&#20154;&#21475;&#32479;&#35745;&#29305;&#24449;&#30340;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#21475;&#32479;&#35745;&#29305;&#24449;&#30340;&#24179;&#31561;&#23545;&#24453;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#24191;&#27867;&#35748;&#21487;&#30340;&#20844;&#24179;&#24230;&#37327;&#26631;&#20934;&#12290;&#20026;&#23454;&#29616;&#20154;&#21475;&#32479;&#35745;&#24179;&#31561;&#65292;&#35768;&#22810;&#30740;&#31350;&#33268;&#21147;&#20110;&#36861;&#27714;&#24120;&#29992;&#24230;&#37327;&#26041;&#27861;$\Delta DP$&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#25581;&#31034;&#20102;&#20844;&#24179;&#24230;&#37327;&#26041;&#27861;$\Delta DP$&#23384;&#22312;&#22266;&#26377;&#32570;&#38519;&#65306;i) &#38646;&#20540;$\Delta DP$&#19981;&#20445;&#35777;&#27665;&#26063;&#32479;&#35745;&#24179;&#31561;&#30340;&#38646;&#36829;&#35268;&#65292;ii) $\Delta DP$&#20540;&#38543;&#19981;&#21516;&#20998;&#31867;&#38408;&#20540;&#21464;&#21270;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#20844;&#24179;&#24230;&#37327;&#26041;&#27861;&#8212;&#8212;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#26354;&#32447;&#20043;&#38388;&#21306;&#22495;&#65288;ABPC&#65289;&#21644;&#32047;&#31215;&#23494;&#24230;&#20989;&#25968;&#26354;&#32447;&#20043;&#38388;&#21306;&#22495;&#65288;ABCC&#65289;&#65292;&#20197;&#31934;&#30830;&#27979;&#37327;&#19981;&#21516;&#27665;&#26063;&#32479;&#35745;&#32676;&#20307;&#39044;&#27979;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Demographic parity is the most widely recognized measure of group fairness in machine learning, which ensures equal treatment of different demographic groups. Numerous works aim to achieve demographic parity by pursuing the commonly used metric $\Delta DP$. Unfortunately, in this paper, we reveal that the fairness metric $\Delta DP$ can not precisely measure the violation of demographic parity, because it inherently has the following drawbacks: i) zero-value $\Delta DP$ does not guarantee zero violation of demographic parity, ii) $\Delta DP$ values can vary with different classification thresholds. To this end, we propose two new fairness metrics, Area Between Probability density function Curves (ABPC) and Area Between Cumulative density function Curves (ABCC), to precisely measure the violation of demographic parity at the distribution level. The new fairness metrics directly measure the difference between the distributions of the prediction probability for different demographic group
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#20195;&#29702;&#35821;&#35328;&#23398;&#20064;&#19982;&#36890;&#20449;&#26694;&#26550;&#65292;&#25104;&#21151;&#22797;&#21046;&#20102;&#35821;&#24207;/&#26684;&#26631;&#20132;&#25442;&#36825;&#19968;&#24191;&#27867;&#23384;&#22312;&#30340;&#35821;&#35328;&#26222;&#36941;&#35268;&#24459;&#65292;&#23454;&#29616;&#20102;&#26356;&#30495;&#23454;&#30340;&#35821;&#35328;&#28436;&#21464;&#27169;&#25311;&#65292;&#24182;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#24433;&#21709;&#35821;&#35328;&#26222;&#36941;&#35268;&#24459;&#30340;&#35748;&#30693;&#21644;&#31038;&#20250;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2301.13083</link><description>&lt;p&gt;
&#31070;&#32463;&#20195;&#29702;&#36890;&#20449;&#25512;&#21160;&#35821;&#35328;&#26222;&#36941;&#35268;&#24459;&#30340;&#20986;&#29616;&#65306;&#20197;&#35821;&#24207;/&#26684;&#26631;&#20132;&#25442;&#20026;&#20363;&#35777;
&lt;/p&gt;
&lt;p&gt;
Communication Drives the Emergence of Language Universals in Neural Agents: Evidence from the Word-order/Case-marking Trade-off. (arXiv:2301.13083v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#20195;&#29702;&#35821;&#35328;&#23398;&#20064;&#19982;&#36890;&#20449;&#26694;&#26550;&#65292;&#25104;&#21151;&#22797;&#21046;&#20102;&#35821;&#24207;/&#26684;&#26631;&#20132;&#25442;&#36825;&#19968;&#24191;&#27867;&#23384;&#22312;&#30340;&#35821;&#35328;&#26222;&#36941;&#35268;&#24459;&#65292;&#23454;&#29616;&#20102;&#26356;&#30495;&#23454;&#30340;&#35821;&#35328;&#28436;&#21464;&#27169;&#25311;&#65292;&#24182;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#24433;&#21709;&#35821;&#35328;&#26222;&#36941;&#35268;&#24459;&#30340;&#35748;&#30693;&#21644;&#31038;&#20250;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#31070;&#32463;&#20195;&#29702;&#30340;&#35821;&#35328;&#28436;&#21464;&#21644;&#21464;&#21270;&#30340;&#27169;&#25311;&#20013;&#65292;&#20154;&#24037;&#23398;&#20064;&#32773;&#30340;&#34892;&#20026;&#36890;&#24120;&#19982;&#20154;&#31867;&#23398;&#20064;&#32773;&#19981;&#21516;&#65292;&#36825;&#24120;&#34987;&#24402;&#22240;&#20110;&#36825;&#20123;&#23398;&#20064;&#32773;&#32570;&#20047;&#36866;&#24403;&#30340;&#35748;&#30693;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#20063;&#26377;&#20154;&#25552;&#20986;&#26356;&#33258;&#28982;&#30340;&#35821;&#35328;&#23398;&#20064;&#21644;&#20351;&#29992;&#29615;&#22659;&#21487;&#33021;&#23548;&#33268;&#26356;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#31181;&#21518;&#19968;&#31181;&#35828;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#35821;&#24207;/&#26684;&#26631;&#20132;&#25442;&#65292;&#19968;&#31181;&#34987;&#24191;&#27867;&#35777;&#26126;&#30340;&#35821;&#35328;&#26222;&#36941;&#35268;&#24459;&#65292;&#36825;&#31181;&#35268;&#24459;&#22312;&#27169;&#25311;&#20013;&#34987;&#35777;&#26126;&#23588;&#20854;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31070;&#32463;&#20195;&#29702;&#35821;&#35328;&#23398;&#20064;&#21644;&#36890;&#20449;&#26694;&#26550;&#65288;NeLLCom&#65289;&#65292;&#20854;&#20013;&#35828;&#35805;&#21644;&#21548;&#21462;&#30340;&#20195;&#29702;&#39318;&#20808;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#23398;&#20064;&#19968;&#31181;&#23567;&#35821;&#35328;&#65292;&#28982;&#21518;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23545;&#20854;&#36827;&#34892;&#20248;&#21270;&#20197;&#36827;&#34892;&#27807;&#36890;&#12290;&#32039;&#23494;&#36981;&#24490;&#26089;&#26399;&#20154;&#31867;&#23454;&#39564;&#30340;&#35774;&#32622;&#65292;&#25105;&#20204;&#25104;&#21151;&#22797;&#21046;&#20102;&#36825;&#31181;&#26032;&#26694;&#26550;&#19979;&#30340;&#20132;&#25442;&#65292;&#32780;&#19981;&#26159;&#22312;&#20195;&#29702;&#20013;&#30828;&#32534;&#30721;&#29305;&#23450;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#26159;&#21457;&#23637;&#26356;&#30495;&#23454;&#30340;&#35821;&#35328;&#28436;&#21464;&#27169;&#25311;&#21644;&#26356;&#22909;&#22320;&#29702;&#35299;&#24433;&#21709;&#35821;&#35328;&#26222;&#36941;&#35268;&#24459;&#30340;&#35748;&#30693;&#21644;&#31038;&#20250;&#22240;&#32032;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial learners often behave differently from human learners in the context of neural agent-based simulations of language emergence and change. A common explanation is the lack of appropriate cognitive biases in these learners. However, it has also been proposed that more naturalistic settings of language learning and use could lead to more human-like results. We investigate this latter account focusing on the word-order/case-marking trade-off, a widely attested language universal that has proven particularly hard to simulate. We propose a new Neural-agent Language Learning and Communication framework (NeLLCom) where pairs of speaking and listening agents first learn a miniature language via supervised learning, and then optimize it for communication via reinforcement learning. Following closely the setup of earlier human experiments, we succeed in replicating the trade-off with the new framework without hard-coding specific biases in the agents. We see this as an essential step to
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#24102;&#26377;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22312;Bradley-Terry-Luce&#21644;Plackett-Luce&#27169;&#22411;&#19979;&#25910;&#25947;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#22312;&#19968;&#23450;&#30340;&#35206;&#30422;&#20551;&#35774;&#19979;&#65292;&#22522;&#20110;&#24754;&#35266;&#20272;&#35745;&#30340;MLE&#25552;&#20379;&#20102;&#24615;&#33021;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;&#22312;&#35777;&#26126;&#20102;&#30495;&#23454;MLE&#21644;&#20197;&#25104;&#23545;&#27604;&#36739;&#24418;&#24335;&#26367;&#20195;&#30340;&#22791;&#36873;MLE&#37117;&#21487;&#20197;&#22312;PL&#27169;&#22411;&#19979;&#25910;&#25947;&#30340;&#21516;&#26102;&#65292;&#20063;&#34920;&#26126;&#20102;&#30495;&#23454;MLE&#30340;&#39640;&#25928;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#20026;RLHF&#31639;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#24182;&#32479;&#19968;&#20102;RLHF&#38382;&#39064;&#21644;IRL&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.11270</link><description>&lt;p&gt;
&#20351;&#29992;&#26469;&#33258;&#25104;&#23545;&#25110;$K$&#20803;&#27604;&#36739;&#30340;&#20154;&#31867;&#21453;&#39304;&#30340;&#35268;&#33539;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Principled Reinforcement Learning with Human Feedback from Pairwise or $K$-wise Comparisons. (arXiv:2301.11270v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11270
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#24102;&#26377;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22312;Bradley-Terry-Luce&#21644;Plackett-Luce&#27169;&#22411;&#19979;&#25910;&#25947;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#22312;&#19968;&#23450;&#30340;&#35206;&#30422;&#20551;&#35774;&#19979;&#65292;&#22522;&#20110;&#24754;&#35266;&#20272;&#35745;&#30340;MLE&#25552;&#20379;&#20102;&#24615;&#33021;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;&#22312;&#35777;&#26126;&#20102;&#30495;&#23454;MLE&#21644;&#20197;&#25104;&#23545;&#27604;&#36739;&#24418;&#24335;&#26367;&#20195;&#30340;&#22791;&#36873;MLE&#37117;&#21487;&#20197;&#22312;PL&#27169;&#22411;&#19979;&#25910;&#25947;&#30340;&#21516;&#26102;&#65292;&#20063;&#34920;&#26126;&#20102;&#30495;&#23454;MLE&#30340;&#39640;&#25928;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#20026;RLHF&#31639;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#24182;&#32479;&#19968;&#20102;RLHF&#38382;&#39064;&#21644;IRL&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#24102;&#26377;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#24403;&#30495;&#23454;&#22870;&#21169;&#20989;&#25968;&#20026;&#32447;&#24615;&#20989;&#25968;&#26102;&#65292;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#22312;Bradley-Terry-Luce&#65288;BTL&#65289;&#27169;&#22411;&#21644;Plackett-Luce&#65288;PL&#65289;&#27169;&#22411;&#19979;&#22343;&#25910;&#25947;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#22522;&#20110;&#23398;&#24471;&#30340;&#22870;&#21169;&#27169;&#22411;&#35757;&#32451;&#31574;&#30053;&#26102;&#65292;MLE&#20250;&#22833;&#36133;&#65292;&#32780;&#22522;&#20110;&#24754;&#35266;&#20272;&#35745;&#30340;MLE&#22312;&#19968;&#23450;&#30340;&#35206;&#30422;&#20551;&#35774;&#19979;&#25552;&#20379;&#24615;&#33021;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;PL&#27169;&#22411;&#19979;&#65292;&#30495;&#23454;MLE&#21644;&#23558;$k$&#20803;&#27604;&#36739;&#25286;&#20998;&#20026;&#25104;&#23545;&#27604;&#36739;&#30340;&#22791;&#36873;MLE&#37117;&#25910;&#25947;&#12290;&#32780;&#30495;&#23454;MLE&#26159;&#28176;&#36817;&#26356;&#20026;&#39640;&#25928;&#30340;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#39564;&#35777;&#20102;&#29616;&#26377;RLHF&#31639;&#27861;&#65288;&#22914;InstructGPT&#65289;&#30340;&#23454;&#39564;&#25104;&#21151;&#65292;&#24182;&#20026;&#31639;&#27861;&#35774;&#35745;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#32479;&#19968;&#20102;RLHF&#38382;&#39064;&#21644;&#26368;&#22823;&#29109;&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;(IRL)&#38382;&#39064;&#65292;&#24182;&#20026;&#20854;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a theoretical framework for Reinforcement Learning with Human Feedback (RLHF). Our analysis shows that when the true reward function is linear, the widely used maximum likelihood estimator (MLE) converges under both the Bradley-Terry-Luce (BTL) model and the Plackett-Luce (PL) model. However, we show that when training a policy based on the learned reward model, MLE fails while a pessimistic MLE provides policies with improved performance under certain coverage assumptions. Additionally, we demonstrate that under the PL model, the true MLE and an alternative MLE that splits the $K$-wise comparison into pairwise comparisons both converge. Moreover, the true MLE is asymptotically more efficient. Our results validate the empirical success of existing RLHF algorithms in InstructGPT and provide new insights for algorithm design. Furthermore, our results unify the problem of RLHF and max-entropy Inverse Reinforcement Learning (IRL), and provide the first sample complexity bound fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#21644;&#24635;&#32467;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#30340;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;ICL&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#26032;&#33539;&#24335;&#65292;&#25506;&#32034;ICL&#20197;&#35780;&#20272;&#21644;&#25512;&#24191;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#33021;&#21147;&#24050;&#25104;&#20026;&#19968;&#31181;&#26032;&#36235;&#21183;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ICL&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#24635;&#32467;&#20102;&#39640;&#32423;&#25216;&#26415;&#65292;&#26368;&#21518;&#35752;&#35770;&#20102;ICL&#30340;&#25361;&#25112;&#20197;&#21450;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2301.00234</link><description>&lt;p&gt;
&#20851;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on In-context Learning. (arXiv:2301.00234v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#21644;&#24635;&#32467;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#30340;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;ICL&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#26032;&#33539;&#24335;&#65292;&#25506;&#32034;ICL&#20197;&#35780;&#20272;&#21644;&#25512;&#24191;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#33021;&#21147;&#24050;&#25104;&#20026;&#19968;&#31181;&#26032;&#36235;&#21183;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ICL&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#24635;&#32467;&#20102;&#39640;&#32423;&#25216;&#26415;&#65292;&#26368;&#21518;&#35752;&#35770;&#20102;ICL&#30340;&#25361;&#25112;&#20197;&#21450;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#19981;&#26029;&#22686;&#24378;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#26032;&#33539;&#24335;&#65292;&#22312;&#20854;&#20013;LLM&#20165;&#22522;&#20110;&#21152;&#20837;&#23569;&#37327;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#36827;&#34892;&#39044;&#27979;&#12290;&#25506;&#32034;ICL&#20197;&#35780;&#20272;&#21644;&#25512;&#24191;LLM&#30340;&#33021;&#21147;&#24050;&#25104;&#20026;&#19968;&#31181;&#26032;&#36235;&#21183;&#12290;&#26412;&#25991;&#26088;&#22312;&#35843;&#26597;&#21644;&#24635;&#32467;ICL&#30340;&#36827;&#23637;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;ICL&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#28548;&#28165;&#20854;&#19982;&#30456;&#20851;&#30740;&#31350;&#30340;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32452;&#32455;&#21644;&#35752;&#35770;&#39640;&#32423;&#25216;&#26415;&#65292;&#21253;&#25324;&#35757;&#32451;&#31574;&#30053;&#12289;&#28436;&#31034;&#35774;&#35745;&#31574;&#30053;&#20197;&#21450;&#30456;&#20851;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;ICL&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#21487;&#20197;&#40723;&#21169;&#26356;&#22810;&#30340;&#30740;&#31350;&#65292;&#25581;&#31034;ICL&#30340;&#24037;&#20316;&#21407;&#29702;&#24182;&#25913;&#36827;ICL&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing ability of large language models (LLMs), in-context learning (ICL) has become a new paradigm for natural language processing (NLP), where LLMs make predictions only based on contexts augmented with a few examples. It has been a new trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, demonstration designing strategies, as well as related analysis. Finally, we discuss the challenges of ICL and provide potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;</title><link>http://arxiv.org/abs/2212.09597</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#20316;&#20026;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#30340;&#37325;&#35201;&#33021;&#21147;&#65292;&#21487;&#20197;&#20026;&#21307;&#30103;&#35786;&#26029;&#12289;&#35848;&#21028;&#31561;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#21518;&#31471;&#25903;&#25345;&#12290;&#26412;&#25991;&#23545;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#21512;&#35843;&#26597;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30740;&#31350;&#25104;&#26524;&#30340;&#27604;&#36739;&#21644;&#24635;&#32467;&#65292;&#24182;&#25552;&#20379;&#20102;&#31995;&#32479;&#36164;&#28304;&#20197;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#36164;&#28304;&#21487;&#22312; https://github.com/zjunlp/Prompt4ReasoningPapers &#19978;&#33719;&#21462;&#65288;&#23450;&#26399;&#26356;&#26032;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#30340; LB-RELAX &#21450;&#20854;&#21464;&#20307;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32447;&#24615;&#35268;&#21010;&#26494;&#24347;&#20540;&#26469;&#36873;&#25321;&#37051;&#22495;&#65292;&#21152;&#24555;&#20102;&#35745;&#31639;&#36895;&#24230;&#32780;&#20445;&#25345;&#20102;&#36739;&#20256;&#32479;&#30340; LB &#31639;&#27861;&#30456;&#21516;&#30340;&#27714;&#35299;&#36136;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#36798;&#21040;&#20102;&#26368;&#20339;&#30340;&#20219;&#24847;&#26102;&#38388;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.08183</link><description>&lt;p&gt;
&#29992;&#20110;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#30340;&#23616;&#37096;&#20998;&#25903;&#26494;&#24347;&#21551;&#21457;&#24335;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Local Branching Relaxation Heuristics for Integer Linear Programs. (arXiv:2212.08183v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#30340; LB-RELAX &#21450;&#20854;&#21464;&#20307;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32447;&#24615;&#35268;&#21010;&#26494;&#24347;&#20540;&#26469;&#36873;&#25321;&#37051;&#22495;&#65292;&#21152;&#24555;&#20102;&#35745;&#31639;&#36895;&#24230;&#32780;&#20445;&#25345;&#20102;&#36739;&#20256;&#32479;&#30340; LB &#31639;&#27861;&#30456;&#21516;&#30340;&#27714;&#35299;&#36136;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#36798;&#21040;&#20102;&#26368;&#20339;&#30340;&#20219;&#24847;&#26102;&#38388;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37051;&#22495;&#25628;&#32034;&#26159;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#19968;&#31181;&#27969;&#34892;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#35774;&#35745;&#29992;&#20110;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#30340;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#23616;&#37096;&#20998;&#25903;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861; LB-RELAX &#21450;&#20854;&#21464;&#20307;&#20351;&#29992;&#32447;&#24615;&#35268;&#21010;&#26494;&#24347;&#20540;&#26469;&#36873;&#25321;&#37051;&#22495;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340; LB &#31639;&#27861;&#65292;&#20854;&#35745;&#31639;&#36895;&#24230;&#26356;&#24555;&#20294;&#25928;&#26524;&#30456;&#21516;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LB-RELAX &#31639;&#27861;&#21450;&#20854;&#21464;&#20307;&#22312;&#25552;&#39640;&#27714;&#35299;&#36136;&#37327;&#30340;&#21516;&#26102;&#65292;&#20063;&#23454;&#29616;&#20102;&#26368;&#20339;&#30340;&#20219;&#24847;&#26102;&#38388;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Neighborhood Search (LNS) is a popular heuristic algorithm for solving combinatorial optimization problems (COP). It starts with an initial solution to the problem and iteratively improves it by searching a large neighborhood around the current best solution. LNS relies on heuristics to select neighborhoods to search in. In this paper, we focus on designing effective and efficient heuristics in LNS for integer linear programs (ILP) since a wide range of COPs can be represented as ILPs. Local Branching (LB) is a heuristic that selects the neighborhood that leads to the largest improvement over the current solution in each iteration of LNS. LB is often slow since it needs to solve an ILP of the same size as input. Our proposed heuristics, LB-RELAX and its variants, use the linear programming relaxation of LB to select neighborhoods. Empirically, LB-RELAX and its variants compute as effective neighborhoods as LB but run faster. They achieve state-of-the-art anytime performance on se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#34892;&#20026;&#32422;&#26463;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38381;&#21512;&#24418;&#24335;&#31574;&#30053;&#25913;&#36827;&#31639;&#23376;&#65292;&#35813;&#31639;&#23376;&#23558;&#34892;&#20026;&#31574;&#30053;&#24314;&#27169;&#20026;&#39640;&#26031;&#28151;&#21512;&#65292;&#21033;&#29992;LogSumExp&#30340;&#19979;&#30028;&#21644;Jensen&#19981;&#31561;&#24335;&#20811;&#26381;&#20102;&#20248;&#21270;&#22256;&#38590;&#65292;&#33021;&#26377;&#25928;&#22788;&#29702;&#23454;&#38469;&#25968;&#25454;&#38598;&#20013;&#30340;&#24322;&#26500;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2211.15956</link><description>&lt;p&gt;
&#22522;&#20110;&#38381;&#21512;&#24418;&#24335;&#31574;&#30053;&#25913;&#36827;&#31639;&#23376;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Offline Reinforcement Learning with Closed-Form Policy Improvement Operators. (arXiv:2211.15956v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#34892;&#20026;&#32422;&#26463;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38381;&#21512;&#24418;&#24335;&#31574;&#30053;&#25913;&#36827;&#31639;&#23376;&#65292;&#35813;&#31639;&#23376;&#23558;&#34892;&#20026;&#31574;&#30053;&#24314;&#27169;&#20026;&#39640;&#26031;&#28151;&#21512;&#65292;&#21033;&#29992;LogSumExp&#30340;&#19979;&#30028;&#21644;Jensen&#19981;&#31561;&#24335;&#20811;&#26381;&#20102;&#20248;&#21270;&#22256;&#38590;&#65292;&#33021;&#26377;&#25928;&#22788;&#29702;&#23454;&#38469;&#25968;&#25454;&#38598;&#20013;&#30340;&#24322;&#26500;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#20026;&#32422;&#26463;&#31574;&#30053;&#20248;&#21270;&#24050;&#34987;&#35777;&#26126;&#26159;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#19968;&#31181;&#25104;&#21151;&#30340;&#33539;&#24335;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;&#38381;&#21512;&#24418;&#24335;&#31574;&#30053;&#25913;&#36827;&#31639;&#23376;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34892;&#20026;&#32422;&#26463;&#33258;&#28982;&#22320;&#28608;&#21169;&#20102;&#20351;&#29992;&#19968;&#38454;&#27888;&#21202;&#36817;&#20284;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#31574;&#30053;&#30446;&#26631;&#30340;&#32447;&#24615;&#36817;&#20284;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23454;&#38469;&#25968;&#25454;&#38598;&#36890;&#24120;&#30001;&#24322;&#26500;&#31574;&#30053;&#25910;&#38598;&#32780;&#26469;&#65292;&#25105;&#20204;&#23558;&#34892;&#20026;&#31574;&#30053;&#24314;&#27169;&#20026;&#39640;&#26031;&#28151;&#21512;&#65292;&#24182;&#21033;&#29992;LogSumExp&#30340;&#19979;&#30028;&#21644;Jensen&#19981;&#31561;&#24335;&#20811;&#26381;&#20102;&#24341;&#36215;&#20248;&#21270;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#38381;&#24335;&#31574;&#30053;&#25913;&#36827;&#31639;&#23376;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26032;&#39062;&#31574;&#30053;&#25913;&#36827;&#31639;&#23376;&#26469;&#23454;&#20363;&#21270;&#31163;&#32447;RL&#31639;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Behavior constrained policy optimization has been demonstrated to be a successful paradigm for tackling Offline Reinforcement Learning. By exploiting historical transitions, a policy is trained to maximize a learned value function while constrained by the behavior policy to avoid a significant distributional shift. In this paper, we propose our closed-form policy improvement operators. We make a novel observation that the behavior constraint naturally motivates the use of first-order Taylor approximation, leading to a linear approximation of the policy objective. Additionally, as practical datasets are usually collected by heterogeneous policies, we model the behavior policies as a Gaussian Mixture and overcome the induced optimization difficulties by leveraging the LogSumExp's lower bound and Jensen's Inequality, giving rise to a closed-form policy improvement operator. We instantiate offline RL algorithms with our novel policy improvement operators and empirically demonstrate their e
&lt;/p&gt;</description></item><item><title>&#22312;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#20013;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#22122;&#22768;&#12289;&#33258;&#28982;&#21457;&#29983;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#21644;&#21327;&#21464;&#37327;&#28418;&#31227;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#33391;&#27169;&#24335;&#34987;&#38169;&#35823;&#22320;&#36171;&#20104;&#39640;&#22870;&#21169;&#20540;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#22870;&#21169;&#21338;&#24328;&#65292;&#38656;&#35201;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2211.08714</link><description>&lt;p&gt;
&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#22870;&#21169;&#21338;&#24328;
&lt;/p&gt;
&lt;p&gt;
Reward Gaming in Conditional Text Generation. (arXiv:2211.08714v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08714
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#20013;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#22122;&#22768;&#12289;&#33258;&#28982;&#21457;&#29983;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#21644;&#21327;&#21464;&#37327;&#28418;&#31227;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#33391;&#27169;&#24335;&#34987;&#38169;&#35823;&#22320;&#36171;&#20104;&#39640;&#22870;&#21169;&#20540;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#22870;&#21169;&#21338;&#24328;&#65292;&#38656;&#35201;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#30340;&#36755;&#20986;&#19982;&#25152;&#38656;&#34892;&#20026;&#30456;&#19968;&#33268;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#28857;&#22312;&#20110;&#20351;&#29992;&#20174;&#20154;&#31867;&#27880;&#37322;&#20013;&#23398;&#20064;&#30340;&#22870;&#21169;&#20989;&#25968;&#20197;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35757;&#32451;&#27169;&#22411;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19977;&#31181;&#24120;&#35265;&#24773;&#20917;&#65292;&#21363;&#30001;&#22122;&#22768;&#24341;&#36215;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12289;&#33258;&#28982;&#21457;&#29983;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#21644;&#21327;&#21464;&#37327;&#28418;&#31227;&#65292;&#20854;&#20013;&#39640;&#22870;&#21169;&#34987;&#38169;&#35823;&#22320;&#20998;&#37197;&#32473;&#19981;&#33391;&#27169;&#24335;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#21363;&#20351;&#23398;&#20064;&#21040;&#30340;&#24230;&#37327;&#22312;&#35757;&#32451;&#22870;&#21169;&#20989;&#25968;&#25152;&#20351;&#29992;&#30340;&#25968;&#25454;&#20998;&#24067;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#19981;&#33391;&#27169;&#24335;&#22312;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#30340;RL&#35757;&#32451;&#36807;&#31243;&#20013;&#20173;&#26377;&#21487;&#33021;&#34987;&#25918;&#22823;&#12290;&#23613;&#31649;RL&#25110;&#23433;&#20840;&#31038;&#21306;&#24050;&#32463;&#24320;&#22987;&#35752;&#35770;&#22870;&#21169;&#21338;&#24328;&#65292;&#20294;&#22312;&#36825;&#31687;&#35752;&#35770;&#20013;&#65292;&#25105;&#20204;&#24076;&#26395;&#20351;&#29992;&#20855;&#20307;&#30340;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#31034;&#20363;&#65292;&#37325;&#28857;&#20171;&#32461;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#31038;&#21306;&#20013;&#30340;&#22870;&#21169;&#21338;&#24328;&#65292;&#24182;&#35752;&#35770;&#21487;&#33021;&#30340;&#20462;&#22797;&#25514;&#26045;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
To align conditional text generation model outputs with desired behaviors, there has been an increasing focus on training the model using reinforcement learning (RL) with reward functions learned from human annotations. Under this framework, we identify three common cases where high rewards are incorrectly assigned to undesirable patterns: noise-induced spurious correlation, naturally occurring spurious correlation, and covariate shift. We show that even though learned metrics achieve high performance on the distribution of the data used to train the reward function, the undesirable patterns may be amplified during RL training of the text generation model. While there has been discussion about reward gaming in the RL or safety community, in this discussion piece, we would like to highlight reward gaming in the natural language generation (NLG) community using concrete conditional text generation examples and discuss potential fixes and areas for future work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#34920;&#26126;&#20102;&#31163;&#32447;&#25351;&#26631;&#19982;&#20154;&#24037;&#36830;&#32493;&#35780;&#20998;&#20043;&#38388;&#23384;&#22312;&#33391;&#22909;&#30340;&#30456;&#20851;&#24615;&#65292;&#21487;&#20197;&#22312;&#21516;&#26102;&#32763;&#35793;&#27169;&#24335;&#20013;&#21487;&#38752;&#22320;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#30340;&#35780;&#20272;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#22823;&#35268;&#27169;&#20154;&#24037;&#35780;&#20272;&#30340;&#38656;&#35201;&#12290;</title><link>http://arxiv.org/abs/2211.08633</link><description>&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#30340;&#34913;&#37327;&#25351;&#26631;&#19982;&#20154;&#24037;&#35780;&#20998;&#30456;&#31526;&#65292;&#22312;&#21516;&#26102;&#32763;&#35793;&#20013;&#20063;&#36866;&#29992;
&lt;/p&gt;
&lt;p&gt;
MT Metrics Correlate with Human Ratings of Simultaneous Speech Translation. (arXiv:2211.08633v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#34920;&#26126;&#20102;&#31163;&#32447;&#25351;&#26631;&#19982;&#20154;&#24037;&#36830;&#32493;&#35780;&#20998;&#20043;&#38388;&#23384;&#22312;&#33391;&#22909;&#30340;&#30456;&#20851;&#24615;&#65292;&#21487;&#20197;&#22312;&#21516;&#26102;&#32763;&#35793;&#27169;&#24335;&#20013;&#21487;&#38752;&#22320;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#30340;&#35780;&#20272;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#22823;&#35268;&#27169;&#20154;&#24037;&#35780;&#20272;&#30340;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#35780;&#20998;&#21644;&#31163;&#32447;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#25351;&#26631;&#65288;&#22914;BLEU&#12289;chrF2&#12289;BertScore&#21644;COMET&#65289;&#20043;&#38388;&#26377;&#20960;&#20010;&#20803;&#35780;&#20272;&#30740;&#31350;&#12290;&#36825;&#20123;&#25351;&#26631;&#24050;&#32463;&#29992;&#20110;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#65288;SST&#65289;&#30340;&#35780;&#20272;&#65292;&#20294;&#23427;&#20204;&#19982;&#26368;&#36817;&#25910;&#38598;&#30340;&#36830;&#32493;&#35780;&#20998;&#65288;CR&#65289;&#30340;SST&#30340;&#20154;&#24037;&#35780;&#20998;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#25552;&#20132;&#32473;IWSLT 2022&#24180;&#33521;&#24503;SST&#20219;&#21153;&#30340;&#20505;&#36873;&#31995;&#32479;&#30340;&#35780;&#20272;&#65292;&#24182;&#23545;CR&#21644;&#19978;&#36848;&#25351;&#26631;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30456;&#20851;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#31163;&#32447;&#25351;&#26631;&#19982;CR&#23384;&#22312;&#33391;&#22909;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#21487;&#20197;&#21487;&#38752;&#22320;&#29992;&#20110;&#35780;&#20272;&#21516;&#26102;&#32763;&#35793;&#27169;&#24335;&#19979;&#30340;&#26426;&#22120;&#32763;&#35793;&#65292;&#20294;&#23545;&#27979;&#35797;&#38598;&#22823;&#23567;&#26377;&#19968;&#23450;&#38480;&#21046;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#31216;&#65292;&#37492;&#20110;&#24403;&#21069;SST&#30340;&#36136;&#37327;&#27700;&#24179;&#65292;&#36825;&#20123;&#25351;&#26631;&#21487;&#20197;&#29992;&#20316;CR&#30340;&#20195;&#29702;&#65292;&#20943;&#36731;&#20102;&#22823;&#35268;&#27169;&#20154;&#24037;&#35780;&#20272;&#30340;&#38656;&#35201;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#35768;&#22810;&#25351;&#26631;&#30340;&#30456;&#20851;&#24615;&#22312;&#19981;&#21516;&#30340;&#27979;&#35797;&#38598;&#19979;&#21457;&#29983;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
There have been several meta-evaluation studies on the correlation between human ratings and offline machine translation (MT) evaluation metrics such as BLEU, chrF2, BertScore and COMET. These metrics have been used to evaluate simultaneous speech translation (SST) but their correlations with human ratings of SST, which has been recently collected as Continuous Ratings (CR), are unclear. In this paper, we leverage the evaluations of candidate systems submitted to the English-German SST task at IWSLT 2022 and conduct an extensive correlation analysis of CR and the aforementioned metrics. Our study reveals that the offline metrics are well correlated with CR and can be reliably used for evaluating machine translation in simultaneous mode, with some limitations on the test set size. We conclude that given the current quality levels of SST, these metrics can be used as proxies for CR, alleviating the need for large scale human evaluation. Additionally, we observe that correlations of the m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26367;&#20195;&#25216;&#26415;&#65292;&#29992;&#20110;&#35757;&#32451;&#33258;&#22238;&#24402;&#20302;&#24310;&#36831;&#35821;&#38899;&#22686;&#24378;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#22330;&#26223;&#19979;&#22343;&#33021;&#24102;&#26469;&#31283;&#23450;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2211.01751</link><description>&lt;p&gt;
&#36845;&#20195;&#33258;&#22238;&#24402;&#65306;&#25552;&#39640;&#20302;&#24310;&#36831;&#35821;&#38899;&#22686;&#24378;&#27169;&#22411;&#30340;&#26032;&#25216;&#24039;
&lt;/p&gt;
&lt;p&gt;
Iterative autoregression: a novel trick to improve your low-latency speech enhancement model. (arXiv:2211.01751v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26367;&#20195;&#25216;&#26415;&#65292;&#29992;&#20110;&#35757;&#32451;&#33258;&#22238;&#24402;&#20302;&#24310;&#36831;&#35821;&#38899;&#22686;&#24378;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#22330;&#26223;&#19979;&#22343;&#33021;&#24102;&#26469;&#31283;&#23450;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24335;&#27169;&#22411;&#26159;&#23454;&#26102;&#35821;&#38899;&#22686;&#24378;&#24037;&#20855;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#27969;&#24335;&#27169;&#24335;&#38480;&#21046;&#35821;&#38899;&#22686;&#24378;&#27169;&#22411;&#20165;&#33021;&#20351;&#29992;&#26497;&#23569;&#37327;&#26410;&#26469;&#20449;&#24687;&#20316;&#20026;&#19978;&#19979;&#25991;&#12290;&#22240;&#27492;&#65292;&#20302;&#24310;&#36831;&#27969;&#24335;&#35774;&#32622;&#36890;&#24120;&#34987;&#35270;&#20026;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36825;&#23545;&#27169;&#22411;&#30340;&#36136;&#37327;&#26377;&#30528;&#26174;&#33879;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#27969;&#24335;&#29983;&#25104;&#30340;&#39034;&#24207;&#24615;&#25552;&#20379;&#20102;&#33258;&#22238;&#24402;&#30340;&#33258;&#28982;&#21487;&#33021;&#24615;&#65292;&#21363;&#22312;&#36827;&#34892;&#24403;&#21069;&#39044;&#27979;&#26102;&#21033;&#29992;&#20197;&#21069;&#30340;&#39044;&#27979;&#12290;&#24120;&#35268;&#35757;&#32451;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#26041;&#27861;&#26159;&#25945;&#24072;&#24378;&#21046;&#65292;&#20294;&#20854;&#20027;&#35201;&#32570;&#28857;&#22312;&#20110;&#35757;&#32451;-&#25512;&#29702;&#19981;&#21305;&#37197;&#21487;&#33021;&#20250;&#23548;&#33268;&#22823;&#24133;&#24230;&#30340;&#36136;&#37327;&#38477;&#32423;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26367;&#20195;&#25216;&#26415;&#65292;&#29992;&#20110;&#35757;&#32451;&#33258;&#22238;&#24402;&#20302;&#24310;&#36831;&#35821;&#38899;&#22686;&#24378;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#22330;&#26223;&#19979;&#37117;&#33021;&#24102;&#26469;&#31283;&#23450;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Streaming models are an essential component of real-time speech enhancement tools. The streaming regime constrains speech enhancement models to use only a tiny context of future information. As a result, the low-latency streaming setup is generally considered a challenging task and has a significant negative impact on the model's quality. However, the sequential nature of streaming generation offers a natural possibility for autoregression, that is, utilizing previous predictions while making current ones. The conventional method for training autoregressive models is teacher forcing, but its primary drawback lies in the training-inference mismatch that can lead to a substantial degradation in quality. In this study, we propose a straightforward yet effective alternative technique for training autoregressive low-latency speech enhancement models. We demonstrate that the proposed approach leads to stable improvement across diverse architectures and training scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20013;&#28857; Mixup &#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#23398;&#20064;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#23398;&#20064;&#27599;&#20010;&#31867;&#21035;&#30340;&#25152;&#26377;&#29305;&#24449;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.13512</link><description>&lt;p&gt;
&#29992;&#20013;&#28857; Mixup &#22312;&#22810;&#35270;&#35282;&#25968;&#25454;&#20013;&#21487;&#35777;&#26126;&#23398;&#20064;&#22810;&#20803;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Provably Learning Diverse Features in Multi-View Data with Midpoint Mixup. (arXiv:2210.13512v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20013;&#28857; Mixup &#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#23398;&#20064;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#23398;&#20064;&#27599;&#20010;&#31867;&#21035;&#30340;&#25152;&#26377;&#29305;&#24449;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Mixup &#26159;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#20381;&#36182;&#20110;&#20351;&#29992;&#25968;&#25454;&#28857;&#21644;&#26631;&#31614;&#30340;&#38543;&#26426;&#20984;&#32452;&#21512;&#36827;&#34892;&#35757;&#32451;&#12290;&#36817;&#24180;&#26469;&#65292;Mixup &#24050;&#25104;&#20026;&#35757;&#32451;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#26631;&#20934;&#22522;&#20803;&#65292;&#22240;&#20026;&#23427;&#22312;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#27604;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#20174;&#29305;&#24449;&#23398;&#20064;&#30340;&#35282;&#24230;&#35299;&#37322;&#19968;&#20123;&#36825;&#31181;&#25104;&#21151;&#12290;&#25105;&#20204;&#20851;&#27880;&#30340;&#20998;&#31867;&#38382;&#39064;&#26159;&#65292;&#27599;&#20010;&#31867;&#21035;&#21487;&#33021;&#20855;&#26377;&#22810;&#20010;&#30456;&#20851;&#29305;&#24449;&#65288;&#25110;&#35270;&#22270;&#65289;&#65292;&#21487;&#29992;&#20110;&#27491;&#30830;&#39044;&#27979;&#31867;&#21035;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20855;&#26377;&#27599;&#31867;&#20004;&#20010;&#29305;&#24449;&#30340;&#19968;&#31867;&#38750;&#24179;&#20961;&#25968;&#25454;&#20998;&#24067;&#20013;&#65292;&#20351;&#29992;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#35757;&#32451; 2 &#23618;&#21367;&#31215;&#32593;&#32476;&#21487;&#33021;&#20250;&#23548;&#33268;&#20960;&#20046;&#25152;&#26377;&#31867;&#21035;&#21482;&#23398;&#20064;&#19968;&#20010;&#29305;&#24449;&#65292;&#32780;&#20351;&#29992; Mixup &#30340;&#29305;&#23450;&#23454;&#20363;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#25104;&#21151;&#22320;&#23398;&#20064;&#27599;&#20010;&#31867;&#21035;&#30340;&#20004;&#20010;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixup is a data augmentation technique that relies on training using random convex combinations of data points and their labels. In recent years, Mixup has become a standard primitive used in the training of state-of-the-art image classification models due to its demonstrated benefits over empirical risk minimization with regards to generalization and robustness. In this work, we try to explain some of this success from a feature learning perspective. We focus our attention on classification problems in which each class may have multiple associated features (or views) that can be used to predict the class correctly. Our main theoretical results demonstrate that, for a non-trivial class of data distributions with two features per class, training a 2-layer convolutional network using empirical risk minimization can lead to learning only one feature for almost all classes while training with a specific instantiation of Mixup succeeds in learning both features for every class. We also show
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#24322;&#26041;&#24046;&#22122;&#22768;&#27169;&#22411;&#65292;&#21457;&#29616;&#38500;&#29305;&#27530;&#24773;&#20917;&#22806;&#22240;&#26524;&#26041;&#21521;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;&#25552;&#20986;&#20102;&#20004;&#20010;&#20272;&#35745;&#22120;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2210.09054</link><description>&lt;p&gt;
&#20851;&#20110;&#22240;&#26524;&#20301;&#32622;-&#23610;&#24230;&#22122;&#22768;&#27169;&#22411;&#30340;&#21487;&#35782;&#21035;&#24615;&#21644;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
On the Identifiability and Estimation of Causal Location-Scale Noise Models. (arXiv:2210.09054v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#24322;&#26041;&#24046;&#22122;&#22768;&#27169;&#22411;&#65292;&#21457;&#29616;&#38500;&#29305;&#27530;&#24773;&#20917;&#22806;&#22240;&#26524;&#26041;&#21521;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;&#25552;&#20986;&#20102;&#20004;&#20010;&#20272;&#35745;&#22120;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#20301;&#32622;-&#23610;&#24230;&#25110;&#24322;&#26041;&#24046;&#22122;&#22768;&#27169;&#22411;&#65288;LSNMs&#65289;&#65292;&#22312;&#20854;&#20013;&#65292;&#25928;&#24212;$ Y $&#21487;&#20197;&#34987;&#20889;&#25104;&#26159;&#22240;&#26524;$ X $&#21644;&#19982;$ X $&#26080;&#20851;&#30340;&#22122;&#22768;&#28304;$ N $&#30340;&#20989;&#25968;&#65292;&#20294;&#21487;&#33021;&#34987;&#22240;&#26524;$ X $&#32553;&#25918;&#20026;&#19968;&#20010;&#27491;&#20989;&#25968;$ g&#65288;X&#65289;$&#65292;&#21363;$ Y = f&#65288;X&#65289;+ g&#65288;X&#65289;N $&#12290;&#23613;&#31649;&#27169;&#22411;&#31867;&#21035;&#38750;&#24120;&#24191;&#27867;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#38500;&#19968;&#20123;&#29305;&#27530;&#24773;&#20917;&#22806;&#65292;&#22240;&#26524;&#26041;&#21521;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;&#20026;&#20102;&#22312;&#23454;&#35777;&#19978;&#39564;&#35777;&#36825;&#20123;&#29702;&#35770;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;LSNMs&#30340;&#20272;&#35745;&#22120;&#65306;&#19968;&#20010;&#22522;&#20110;&#65288;&#38750;&#32447;&#24615;&#65289;&#29305;&#24449;&#26144;&#23556;&#30340;&#20272;&#35745;&#22120;&#21644;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20272;&#35745;&#22120;&#12290;&#20004;&#32773;&#23558;$ Y $&#32473;&#23450;$ X $&#30340;&#26465;&#20214;&#20998;&#24067;&#24314;&#27169;&#20026;&#30001;&#20854;&#33258;&#28982;&#21442;&#25968;&#21442;&#25968;&#21270;&#30340;&#39640;&#26031;&#20998;&#24067;&#12290;&#24403;&#29305;&#24449;&#26144;&#23556;&#34987;&#27491;&#30830;&#35268;&#23450;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#26159;&#32852;&#21512;&#20984;&#30340;&#65292;&#24182;&#19988;&#26159;&#22240;&#26524;&#25928;&#24212;&#35782;&#21035;&#20219;&#21153;&#30340;&#19968;&#33268;&#20272;&#35745;&#22120;&#12290;&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#27809;&#26377;&#32487;&#25215;&#36825;&#20123;&#20445;&#35777;&#65292;&#20294;&#23427;&#21487;&#20197;&#25311;&#21512;&#20219;&#24847;&#22797;&#26434;&#24615;&#30340;&#20989;&#25968;&#65292;&#24182;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the class of location-scale or heteroscedastic noise models (LSNMs), in which the effect $Y$ can be written as a function of the cause $X$ and a noise source $N$ independent of $X$, which may be scaled by a positive function $g$ over the cause, i.e., $Y = f(X) + g(X)N$. Despite the generality of the model class, we show the causal direction is identifiable up to some pathological cases. To empirically validate these theoretical findings, we propose two estimators for LSNMs: an estimator based on (non-linear) feature maps, and one based on neural networks. Both model the conditional distribution of $Y$ given $X$ as a Gaussian parameterized by its natural parameters. When the feature maps are correctly specified, we prove that our estimator is jointly concave, and a consistent estimator for the cause-effect identification task. Although the the neural network does not inherit those guarantees, it can fit functions of arbitrary complexity, and reaches state-of-the-art performance
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;TECO&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20854;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#35270;&#39057;&#29983;&#25104;&#30340;&#38271;&#26399;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;3&#20010;&#38590;&#24230;&#19981;&#21516;&#30340;&#35270;&#39057;&#25968;&#25454;&#38598;&#20197;&#35780;&#20272;&#35813;&#27169;&#22411;&#21644;&#29616;&#26377;&#27169;&#22411;&#22312;&#26102;&#38388;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.02396</link><description>&lt;p&gt;
&#26102;&#24577;&#19968;&#33268;&#30340;&#21464;&#25442;&#22120;&#29992;&#20110;&#35270;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Temporally Consistent Transformers for Video Generation. (arXiv:2210.02396v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;TECO&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20854;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#35270;&#39057;&#29983;&#25104;&#30340;&#38271;&#26399;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;3&#20010;&#38590;&#24230;&#19981;&#21516;&#30340;&#35270;&#39057;&#25968;&#25454;&#38598;&#20197;&#35780;&#20272;&#35813;&#27169;&#22411;&#21644;&#29616;&#26377;&#27169;&#22411;&#22312;&#26102;&#38388;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#29983;&#25104;&#20934;&#30830;&#30340;&#35270;&#39057;&#65292;&#31639;&#27861;&#24517;&#39035;&#29702;&#35299;&#19990;&#30028;&#20013;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#12290;&#30446;&#21069;&#30340;&#31639;&#27861;&#34429;&#28982;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#30701;&#26399;&#20869;&#30340;&#20869;&#23481;&#65292;&#20294;&#24448;&#24448;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#24403;&#29983;&#25104;&#30340;&#20869;&#23481;&#28040;&#22833;&#21518;&#20877;&#27425;&#20986;&#29616;&#26102;&#65292;&#27169;&#22411;&#20250;&#21457;&#26126;&#19981;&#21516;&#30340;&#20869;&#23481;&#12290;&#23613;&#31649;&#23384;&#22312;&#36825;&#31181;&#20005;&#37325;&#30340;&#38480;&#21046;&#65292;&#20294;&#30446;&#21069;&#23578;&#19981;&#23384;&#22312;&#22797;&#26434;&#25968;&#25454;&#30340;&#24050;&#24314;&#31435;&#22522;&#20934;&#26469;&#23545;&#20855;&#26377;&#38271;&#26399;&#26102;&#38388;&#20381;&#36182;&#24615;&#30340;&#35270;&#39057;&#29983;&#25104;&#36827;&#34892;&#20005;&#26684;&#35780;&#20272;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#28210;&#26579;&#36890;&#36807; 3D &#22330;&#26223;&#30340;&#27969;&#31243;&#36855;&#23467;&#65292;Minecraft &#19990;&#30028;&#21644;&#23460;&#20869;&#25195;&#25551;&#65292;&#31574;&#21010;&#20102;&#19977;&#20010;&#20855;&#26377;&#38271;&#31243;&#20381;&#36182;&#30340;&#35270;&#39057;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23545;&#24403;&#21069;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#35266;&#23519;&#21040;&#23427;&#20204;&#22312;&#26102;&#38388;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26102;&#24577;&#19968;&#33268;&#30340;&#21464;&#25442;&#22120;&#65288;TECO&#65289;&#65292;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#26174;&#33879;&#25552;&#39640;&#20102;&#38271;&#26399;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#20063;&#20943;&#23569;&#20102;&#37319;&#26679;&#26102;&#38388;&#12290;&#36890;&#36807;&#23558;&#20854;&#36755;&#20837;&#24207;&#21015;&#21387;&#32553;&#20026;&#26356;&#23569;&#30340;&#39033;&#65292;
&lt;/p&gt;
&lt;p&gt;
To generate accurate videos, algorithms have to understand the spatial and temporal dependencies in the world. Current algorithms enable accurate predictions over short horizons but tend to suffer from temporal inconsistencies. When generated content goes out of view and is later revisited, the model invents different content instead. Despite this severe limitation, no established benchmarks on complex data exist for rigorously evaluating video generation with long temporal dependencies. In this paper, we curate 3 challenging video datasets with long-range dependencies by rendering walks through 3D scenes of procedural mazes, Minecraft worlds, and indoor scans. We perform a comprehensive evaluation of current models and observe their limitations in temporal consistency. Moreover, we introduce the Temporally Consistent Transformer (TECO), a generative model that substantially improves long-term consistency while also reducing sampling time. By compressing its input sequence into fewer e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#23398;&#20064;&#30340;&#22810;&#26679;&#24615;&#38544;&#31169;&#20445;&#25252;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#21453;&#20107;&#23454;&#32422;&#26463;&#26465;&#20214;&#65292;&#24182;&#20026;&#31169;&#26377;&#35299;&#37322;&#27169;&#22411;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2209.13446</link><description>&lt;p&gt;
&#22522;&#20110;&#29305;&#24449;&#23398;&#20064;&#30340;&#22810;&#26679;&#24615;&#38544;&#31169;&#20445;&#25252;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Feature-based Learning for Diverse and Privacy-Preserving Counterfactual Explanations. (arXiv:2209.13446v5 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#23398;&#20064;&#30340;&#22810;&#26679;&#24615;&#38544;&#31169;&#20445;&#25252;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#21453;&#20107;&#23454;&#32422;&#26463;&#26465;&#20214;&#65292;&#24182;&#20026;&#31169;&#26377;&#35299;&#37322;&#27169;&#22411;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#33268;&#21147;&#20110;&#29702;&#35299;&#38271;&#26399;&#20197;&#26469;&#22240;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#32780;&#22768;&#21517;&#29436;&#34249;&#30340;&#22797;&#26434;&#40657;&#30418;&#23376;&#31995;&#32479;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#20854;&#20013;&#19968;&#31181;&#34028;&#21187;&#21457;&#23637;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#22914;&#20309;&#25913;&#21464;&#32467;&#26524;&#30340;&#24314;&#35758;&#12290;&#21453;&#20107;&#23454;&#26679;&#26412;&#19981;&#20165;&#24517;&#39035;&#21453;&#39539;&#40657;&#30418;&#20998;&#31867;&#22120;&#30340;&#21407;&#22987;&#39044;&#27979;&#65292;&#36824;&#24517;&#39035;&#28385;&#36275;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#20854;&#20013;&#22810;&#26679;&#24615;&#26159;&#20851;&#38190;&#32422;&#26463;&#20043;&#19968;&#20294;&#20173;&#36739;&#23569;&#35752;&#35770;&#12290;&#34429;&#28982;&#22810;&#26679;&#21270;&#30340;&#21453;&#20107;&#23454;&#24456;&#29702;&#24819;&#65292;&#20294;&#21516;&#26102;&#35299;&#20915;&#20854;&#20182;&#32422;&#26463;&#26465;&#20214;&#22312;&#35745;&#31639;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#20849;&#20139;&#21453;&#20107;&#23454;&#25968;&#25454;&#23384;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#23398;&#20064;&#30340;&#26694;&#26550;&#26469;&#26377;&#25928;&#22320;&#22788;&#29702;&#21453;&#20107;&#23454;&#32422;&#26463;&#26465;&#20214;&#65292;&#24182;&#20026;&#31169;&#26377;&#35299;&#37322;&#27169;&#22411;&#30340;&#26377;&#38480;&#36164;&#26009;&#24211;&#20570;&#20986;&#36129;&#29486;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#40657;&#30418;&#27169;&#22411;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#28789;&#27963;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretable machine learning seeks to understand the reasoning process of complex black-box systems that are long notorious for lack of explainability. One flourishing approach is through counterfactual explanations, which provide suggestions on what a user can do to alter an outcome. Not only must a counterfactual example counter the original prediction from the black-box classifier but it should also satisfy various constraints for practical applications. Diversity is one of the critical constraints that however remains less discussed. While diverse counterfactuals are ideal, it is computationally challenging to simultaneously address some other constraints. Furthermore, there is a growing privacy concern over the released counterfactual data. To this end, we propose a feature-based learning framework that effectively handles the counterfactual constraints and contributes itself to the limited pool of private explanation models. We demonstrate the flexibility and effectiveness of o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27979;&#35797;&#20102;&#35821;&#35328;&#26292;&#38706;&#20551;&#35828;&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20687;&#20154;&#31867;&#19968;&#26679;&#24863;&#30693;&#20070;&#38754;&#25991;&#26412;&#20013;&#35282;&#33394;&#30340;&#30693;&#35782;&#29366;&#24577;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35821;&#35328;&#27169;&#22411;&#34429;&#28982;&#36229;&#36807;&#20102;&#20598;&#28982;&#34892;&#20026;&#65292;&#20294;&#34920;&#29616;&#19981;&#22914;&#20154;&#31867;&#65292;&#26263;&#31034;&#30528;&#20165;&#38752;&#35821;&#35328;&#26292;&#38706;&#38590;&#20197;&#23436;&#20840;&#35299;&#37322;&#20154;&#31867;&#36825;&#31181;&#35748;&#30693;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2209.01515</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#20687;&#20154;&#31867;&#19968;&#26679;&#30693;&#36947;&#21035;&#20154;&#30340;&#20449;&#20208;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Large Language Models know what humans know?. (arXiv:2209.01515v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.01515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27979;&#35797;&#20102;&#35821;&#35328;&#26292;&#38706;&#20551;&#35828;&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20687;&#20154;&#31867;&#19968;&#26679;&#24863;&#30693;&#20070;&#38754;&#25991;&#26412;&#20013;&#35282;&#33394;&#30340;&#30693;&#35782;&#29366;&#24577;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35821;&#35328;&#27169;&#22411;&#34429;&#28982;&#36229;&#36807;&#20102;&#20598;&#28982;&#34892;&#20026;&#65292;&#20294;&#34920;&#29616;&#19981;&#22914;&#20154;&#31867;&#65292;&#26263;&#31034;&#30528;&#20165;&#38752;&#35821;&#35328;&#26292;&#38706;&#38590;&#20197;&#23436;&#20840;&#35299;&#37322;&#20154;&#31867;&#36825;&#31181;&#35748;&#30693;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#33021;&#22815;&#20102;&#35299;&#20182;&#20154;&#30340;&#20449;&#20208;&#12290;&#28982;&#32780;&#65292;&#23578;&#19981;&#28165;&#26970;&#36825;&#31181;&#33021;&#21147;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#26159;&#28304;&#20110;&#22825;&#29983;&#30340;&#29983;&#29289;&#31104;&#36171;&#65292;&#36824;&#26159;&#26469;&#28304;&#20110;&#20799;&#31461;&#21457;&#32946;&#36807;&#31243;&#20013;&#30340;&#32463;&#39564;&#31215;&#32047;&#65292;&#23588;&#20854;&#26159;&#36890;&#36807;&#25509;&#21463;&#25551;&#36848;&#20182;&#20154;&#24515;&#29702;&#29366;&#24577;&#30340;&#35821;&#35328;&#32780;&#33719;&#24471;&#30340;&#32463;&#39564;&#12290;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;&#26292;&#38706;&#20110;&#22823;&#37327;&#20154;&#31867;&#35821;&#35328;&#30340;&#27169;&#22411;&#26159;&#21542;&#26174;&#31034;&#23545;&#20070;&#38754;&#27573;&#33853;&#20013;&#35282;&#33394;&#26263;&#31034;&#30340;&#30693;&#35782;&#29366;&#24577;&#25935;&#24863;&#24615;&#26469;&#27979;&#35797;&#35821;&#35328;&#26292;&#38706;&#20551;&#35828;&#30340;&#21487;&#34892;&#24615;&#12290;&#22312;&#39044;&#27880;&#20876;&#30340;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#21521;&#20154;&#31867;&#21442;&#19982;&#32773;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-3&#25552;&#20379;&#20102;&#35821;&#35328;&#29256;&#26412;&#30340;&#35823;&#20449;&#20219;&#21153;&#12290;&#20004;&#32773;&#37117;&#25935;&#24863;&#20110;&#20182;&#20154;&#30340;&#20449;&#20208;&#65292;&#32780;&#35821;&#35328;&#27169;&#22411;&#26174;&#33879;&#36229;&#36807;&#20102;&#20598;&#28982;&#34892;&#20026;&#65292;&#20294;&#23427;&#30340;&#34920;&#29616;&#19981;&#22914;&#20154;&#31867;&#65292;&#24182;&#19988;&#27809;&#26377;&#35299;&#37322;&#20182;&#20204;&#34892;&#20026;&#30340;&#20840;&#37096;&#33539;&#22260;--&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#25509;&#21463;&#20102;&#27604;&#19968;&#20010;&#20154;&#19968;&#29983;&#20013;&#25509;&#21463;&#30340;&#35821;&#35328;&#26356;&#22810;&#30340;&#35821;&#35328;&#12290;&#36825;&#34920;&#26126;&#65292;&#34429;&#28982;&#20174;&#35821;&#35328;&#26292;&#38706;&#20013;&#36827;&#34892;&#30340;&#32479;&#35745;&#23398;&#20064;&#21487;&#33021;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#35299;&#37322;&#20102;&#20154;&#31867;&#22914;&#20309;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#20294;&#23427;&#24182;&#19981;&#33021;&#23436;&#20840;&#35299;&#37322;&#20154;&#31867;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans can attribute beliefs to others. However, it is unknown to what extent this ability results from an innate biological endowment or from experience accrued through child development, particularly exposure to language describing others' mental states. We test the viability of the language exposure hypothesis by assessing whether models exposed to large quantities of human language display sensitivity to the implied knowledge states of characters in written passages. In pre-registered analyses, we present a linguistic version of the False Belief Task to both human participants and a Large Language Model, GPT-3. Both are sensitive to others' beliefs, but while the language model significantly exceeds chance behavior, it does not perform as well as the humans, nor does it explain the full extent of their behavior -- despite being exposed to more language than a human would in a lifetime. This suggests that while statistical learning from language exposure may in part explain how huma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#35780;&#21028;&#32622;&#20449;&#24230;&#24341;&#23548;&#25506;&#32034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#29616;&#26377;&#30340;&#31070;&#35861;&#31574;&#30053;&#32435;&#20837;&#26631;&#20934;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#20197;&#25552;&#39640;&#25506;&#32034;&#25928;&#29575;&#12290;&#22312;&#19981;&#30830;&#23450;&#24615;&#39640;&#26102;&#65292;&#35813;&#26041;&#27861;&#20250;&#23558;&#31070;&#35861;&#31574;&#30053;&#30340;&#34892;&#21160;&#20316;&#20026;&#24314;&#35758;&#32435;&#20837;&#23398;&#20064;&#26041;&#26696;&#20013;&#65292;&#32780;&#22312;&#19981;&#30830;&#23450;&#24615;&#20302;&#26102;&#24573;&#30053;&#23427;&#12290;</title><link>http://arxiv.org/abs/2208.10533</link><description>&lt;p&gt;
&#19968;&#20123;&#30417;&#30563;&#26159;&#24517;&#39035;&#30340;&#65306;&#36890;&#36807;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#20837;&#31070;&#35861;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Some Supervision Required: Incorporating Oracle Policies in Reinforcement Learning via Epistemic Uncertainty Metrics. (arXiv:2208.10533v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#35780;&#21028;&#32622;&#20449;&#24230;&#24341;&#23548;&#25506;&#32034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#29616;&#26377;&#30340;&#31070;&#35861;&#31574;&#30053;&#32435;&#20837;&#26631;&#20934;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#20197;&#25552;&#39640;&#25506;&#32034;&#25928;&#29575;&#12290;&#22312;&#19981;&#30830;&#23450;&#24615;&#39640;&#26102;&#65292;&#35813;&#26041;&#27861;&#20250;&#23558;&#31070;&#35861;&#31574;&#30053;&#30340;&#34892;&#21160;&#20316;&#20026;&#24314;&#35758;&#32435;&#20837;&#23398;&#20064;&#26041;&#26696;&#20013;&#65292;&#32780;&#22312;&#19981;&#30830;&#23450;&#24615;&#20302;&#26102;&#24573;&#30053;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#22266;&#26377;&#38382;&#39064;&#26159;&#36890;&#36807;&#38543;&#26426;&#34892;&#21160;&#25506;&#32034;&#29615;&#22659;&#65292;&#20854;&#20013;&#24456;&#22823;&#19968;&#37096;&#20998;&#21487;&#33021;&#26159;&#26080;&#25928;&#30340;&#12290;&#30456;&#21453;&#65292;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#29616;&#26377;&#30340;&#65288;&#20808;&#21069;&#23398;&#20064;&#30340;&#25110;&#30828;&#32534;&#30721;&#30340;&#65289;&#31070;&#35861;&#31574;&#30053;&#12289;&#31163;&#32447;&#25968;&#25454;&#25110;&#28436;&#31034;&#26469;&#25913;&#21892;&#25506;&#32034;&#12290;&#20294;&#22312;&#20351;&#29992;&#31070;&#35861;&#31574;&#30053;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#26368;&#22823;&#21270;&#23398;&#20064;&#26679;&#26412;&#25928;&#29575;&#22320;&#23558;&#31070;&#35861;&#32463;&#39564;&#34701;&#20837;&#21040;&#23398;&#20064;&#31574;&#30053;&#20013;&#21487;&#33021;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35780;&#21028;&#32622;&#20449;&#24230;&#24341;&#23548;&#25506;&#32034;&#65288;Critic Confidence Guided Exploration&#65292;CCGE&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#36825;&#26679;&#30340;&#31070;&#35861;&#31574;&#30053;&#32435;&#20837;&#26631;&#20934;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24403;&#19981;&#30830;&#23450;&#24615;&#39640;&#26102;&#65292;CCGE&#20197;&#31070;&#35861;&#31574;&#30053;&#30340;&#34892;&#21160;&#20026;&#24314;&#35758;&#65292;&#24182;&#23558;&#27492;&#20449;&#24687;&#32435;&#20837;&#23398;&#20064;&#26041;&#26696;&#20013;&#65292;&#32780;&#24403;&#19981;&#30830;&#23450;&#24615;&#20302;&#26102;&#24573;&#30053;&#23427;&#12290;CCGE&#23545;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#19981;&#21152;&#21306;&#20998;&#65292;&#24182;&#19988;&#25105;&#20204;&#35777;&#26126;&#23427;&#19982;&#29616;&#26377;&#31639;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
An inherent problem of reinforcement learning is performing exploration of an environment through random actions, of which a large portion can be unproductive. Instead, exploration can be improved by initializing the learning policy with an existing (previously learned or hard-coded) oracle policy, offline data, or demonstrations. In the case of using an oracle policy, it can be unclear how best to incorporate the oracle policy's experience into the learning policy in a way that maximizes learning sample efficiency. In this paper, we propose a method termed Critic Confidence Guided Exploration (CCGE) for incorporating such an oracle policy into standard actor-critic reinforcement learning algorithms. More specifically, CCGE takes in the oracle policy's actions as suggestions and incorporates this information into the learning scheme when uncertainty is high, while ignoring it when the uncertainty is low. CCGE is agnostic to methods of estimating uncertainty, and we show that it is equa
&lt;/p&gt;</description></item><item><title>Claim-Dissector&#26159;&#19968;&#27454;&#32852;&#21512;&#37325;&#25490;&#21644;&#30495;&#23454;&#24615;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#30340;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;&#65292;&#21487;&#20197;&#35782;&#21035;&#19982;&#22768;&#26126;&#30456;&#20851;&#30340;&#35777;&#25454;&#65292;&#24182;&#30830;&#23450;&#22768;&#26126;&#30340;&#30495;&#23454;&#24615;&#12290;&#35813;&#31995;&#32479;&#30340;&#20010;&#20154;&#36129;&#29486;&#20197;&#21450;&#35777;&#25454;&#25152;&#25903;&#25345;&#25110;&#21453;&#39539;&#22768;&#26126;&#30340;&#36129;&#29486;&#37117;&#21487;&#20197;&#34987;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2207.14116</link><description>&lt;p&gt;
Claim-Dissector: &#19968;&#27454;&#32852;&#21512;&#37325;&#25490;&#21644;&#30495;&#23454;&#24615;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#30340;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Claim-Dissector: An Interpretable Fact-Checking System with Joint Re-ranking and Veracity Prediction. (arXiv:2207.14116v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.14116
&lt;/p&gt;
&lt;p&gt;
Claim-Dissector&#26159;&#19968;&#27454;&#32852;&#21512;&#37325;&#25490;&#21644;&#30495;&#23454;&#24615;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#30340;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;&#65292;&#21487;&#20197;&#35782;&#21035;&#19982;&#22768;&#26126;&#30456;&#20851;&#30340;&#35777;&#25454;&#65292;&#24182;&#30830;&#23450;&#22768;&#26126;&#30340;&#30495;&#23454;&#24615;&#12290;&#35813;&#31995;&#32479;&#30340;&#20010;&#20154;&#36129;&#29486;&#20197;&#21450;&#35777;&#25454;&#25152;&#25903;&#25345;&#25110;&#21453;&#39539;&#22768;&#26126;&#30340;&#36129;&#29486;&#37117;&#21487;&#20197;&#34987;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Claim-Dissector&#65292;&#19968;&#31181;&#38024;&#23545;&#20107;&#23454;&#26680;&#26597;&#21644;&#20998;&#26512;&#30340;&#26032;&#22411;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#32473;&#20986;&#19968;&#20010;&#22768;&#26126;&#21644;&#19968;&#32452;&#26816;&#32034;&#21040;&#30340;&#35777;&#25454;&#65292;&#32852;&#21512;&#23398;&#20064;&#35782;&#21035;&#65306;&#65288;i&#65289;&#19982;&#32473;&#23450;&#22768;&#26126;&#30456;&#20851;&#30340;&#35777;&#25454;&#65292;&#65288;ii&#65289;&#22768;&#26126;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#24314;&#35758;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#35299;&#24320;&#27599;&#20010;&#35777;&#25454;&#30340;&#30456;&#20851;&#24615;&#27010;&#29575;&#21450;&#20854;&#23545;&#26368;&#32456;&#30495;&#23454;&#24615;&#27010;&#29575;&#30340;&#24433;&#21709;-&#26368;&#32456;&#30495;&#23454;&#24615;&#27010;&#29575;&#19982;&#27599;&#20010;&#35777;&#25454;&#30456;&#20851;&#24615;&#27010;&#29575;&#30340;&#32447;&#24615;&#25972;&#21512;&#25104;&#27604;&#20363;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#27599;&#20010;&#35777;&#25454;&#23545;&#26368;&#32456;&#39044;&#27979;&#27010;&#29575;&#30340;&#20010;&#20154;&#36129;&#29486;&#12290;&#22312;&#27599;&#20010;&#35777;&#25454;&#30340;&#30456;&#20851;&#24615;&#27010;&#29575;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#21487;&#20197;&#36827;&#19968;&#27493;&#21306;&#20998;&#27599;&#20010;&#30456;&#20851;&#35777;&#25454;&#26159;&#25903;&#25345;&#65288;S&#65289;&#36824;&#26159;&#21453;&#39539;&#65288;R&#65289;&#22768;&#26126;&#12290;&#36825;&#26679;&#21487;&#20197;&#37327;&#21270;S/R&#27010;&#29575;&#23545;&#26368;&#32456;&#32467;&#35770;&#30340;&#36129;&#29486;&#25110;&#26816;&#27979;&#26377;&#24322;&#35758;&#30340;&#35777;&#25454;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#31995;&#32479;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#20294;&#22312;FEVER&#31454;&#36187;&#20013;&#65292;&#20854;&#32467;&#26524;&#19982;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Claim-Dissector: a novel latent variable model for fact-checking and analysis, which given a claim and a set of retrieved evidences jointly learns to identify: (i) the relevant evidences to the given claim, (ii) the veracity of the claim. We propose to disentangle the per-evidence relevance probability and its contribution to the final veracity probability in an interpretable way -- the final veracity probability is proportional to a linear ensemble of per-evidence relevance probabilities. In this way, the individual contributions of evidences towards the final predicted probability can be identified. In per-evidence relevance probability, our model can further distinguish whether each relevant evidence is supporting (S) or refuting (R) the claim. This allows to quantify how much the S/R probability contributes to the final verdict or to detect disagreeing evidence.  Despite its interpretable nature, our system achieves results competitive with state-of-the-art on the FEVER 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#20302;&#21151;&#32791;TinyML&#31995;&#32479;&#65292;&#37319;&#29992;&#24494;&#22411;&#39592;&#24178;&#32593;&#32476;&#26500;&#24314;&#39640;&#25928;CNN&#27169;&#22411;&#65292;&#20877;&#30001;&#29305;&#21035;&#35774;&#35745;&#30340;&#31070;&#32463;&#21327;&#22788;&#29702;&#22120;&#19982;MCU&#30456;&#36830;&#65292;&#23558;&#25152;&#26377;&#29305;&#24449;&#21644;&#26435;&#37325;&#20648;&#23384;&#22312;&#33455;&#29255;&#19978;&#65292;&#23436;&#20840;&#28040;&#38500;&#33455;&#29255;&#22806;&#23384;&#20648;&#22120;&#35775;&#38382;&#30340;&#24310;&#36831;&#21644;&#21151;&#32791;&#12290;&#27492;&#31995;&#32479;&#20855;&#26377;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#24182;&#23454;&#29616;&#20102;&#21019;&#32426;&#24405;&#30340;160mW&#36229;&#20302;&#21151;&#32791;&#12290;</title><link>http://arxiv.org/abs/2207.04663</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#23454;&#26102;&#35270;&#35273;&#22788;&#29702;&#30340;&#36229;&#20302;&#21151;&#32791;TinyML&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
An Ultra-low Power TinyML System for Real-time Visual Processing at Edge. (arXiv:2207.04663v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#20302;&#21151;&#32791;TinyML&#31995;&#32479;&#65292;&#37319;&#29992;&#24494;&#22411;&#39592;&#24178;&#32593;&#32476;&#26500;&#24314;&#39640;&#25928;CNN&#27169;&#22411;&#65292;&#20877;&#30001;&#29305;&#21035;&#35774;&#35745;&#30340;&#31070;&#32463;&#21327;&#22788;&#29702;&#22120;&#19982;MCU&#30456;&#36830;&#65292;&#23558;&#25152;&#26377;&#29305;&#24449;&#21644;&#26435;&#37325;&#20648;&#23384;&#22312;&#33455;&#29255;&#19978;&#65292;&#23436;&#20840;&#28040;&#38500;&#33455;&#29255;&#22806;&#23384;&#20648;&#22120;&#35775;&#38382;&#30340;&#24310;&#36831;&#21644;&#21151;&#32791;&#12290;&#27492;&#31995;&#32479;&#20855;&#26377;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#24182;&#23454;&#29616;&#20102;&#21019;&#32426;&#24405;&#30340;160mW&#36229;&#20302;&#21151;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
TinyML&#26159;&#22312;&#36164;&#28304;&#21644;&#21151;&#32791;&#20005;&#26684;&#38480;&#21046;&#30340;&#31995;&#32479;&#19978;&#25191;&#34892;AI&#24037;&#20316;&#36127;&#36733;&#30340;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26497;&#20854;&#24494;&#23567;&#30340;&#39592;&#24178;&#32593;&#32476;&#65292;&#29992;&#20110;&#26500;&#24314;&#29992;&#20110;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#30340;&#39640;&#25928;CNN&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#19968;&#31181;&#29305;&#21035;&#35774;&#35745;&#30340;&#31070;&#32463;&#21327;&#22788;&#29702;&#22120;&#65288;NCP&#65289;&#19982;MCU&#30456;&#36830;&#65292;&#26500;&#24314;&#20102;&#19968;&#31181;&#36229;&#20302;&#21151;&#32791;&#30340;TinyML&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#23558;&#25152;&#26377;&#29305;&#24449;&#21644;&#26435;&#37325;&#23384;&#20648;&#22312;&#33455;&#29255;&#19978;&#65292;&#24182;&#23436;&#20840;&#28040;&#38500;&#20102;&#33455;&#29255;&#22806;&#23384;&#20648;&#22120;&#35775;&#38382;&#30340;&#24310;&#36831;&#21644;&#21151;&#32791;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;&#29305;&#23450;&#30340;&#25351;&#20196;&#38598;&#65292;&#20197;&#23454;&#29616;&#25935;&#25463;&#24320;&#21457;&#21644;&#24555;&#36895;&#37096;&#32626;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22522;&#20110;&#25105;&#20204;&#30340;&#27169;&#22411;&#12289;NCP&#21644;&#25351;&#20196;&#38598;&#30340;TinyML&#31995;&#32479;&#22312;&#23454;&#29616;30FPS&#30340;&#30446;&#26631;&#26816;&#27979;&#21644;&#35782;&#21035;&#26102;&#20855;&#26377;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#21019;&#32426;&#24405;&#30340;160mW&#36229;&#20302;&#21151;&#32791;&#12290;&#28436;&#31034;&#35270;&#39057;&#22312;\url{https://www.youtube.com/watch?v=mIZPxtJ-9EY}&#19978;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tiny machine learning (TinyML), executing AI workloads on resource and power strictly restricted systems, is an important and challenging topic. This brief firstly presents an extremely tiny backbone to construct high efficiency CNN models for various visual tasks. Then, a specially designed neural co-processor (NCP) is interconnected with MCU to build an ultra-low power TinyML system, which stores all features and weights on chip and completely removes both of latency and power consumption in off-chip memory access. Furthermore, an application specific instruction-set is further presented for realizing agile development and rapid deployment. Extensive experiments demonstrate that the proposed TinyML system based on our model, NCP and instruction set yields considerable accuracy and achieves a record ultra-low power of 160mW while implementing object detection and recognition at 30FPS. The demo video is available on \url{https://www.youtube.com/watch?v=mIZPxtJ-9EY}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#28151;&#21512;&#36807;&#31243;&#30340;&#20551;&#35774;&#65292;&#21363;&#32467;&#26500;&#31232;&#30095;&#24615;&#65292;&#26469;&#23454;&#29616;&#38750;&#32447;&#24615;ICA&#30340;&#21487;&#35782;&#21035;&#24615;&#65292;&#26080;&#38656;&#36741;&#21161;&#21464;&#37327;&#12290;</title><link>http://arxiv.org/abs/2206.07751</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#29420;&#31435;&#20998;&#37327;&#20998;&#26512;&#30340;&#21487;&#36776;&#35782;&#24615;&#65306;&#31232;&#30095;&#24615;&#21450;&#20854;&#23427;
&lt;/p&gt;
&lt;p&gt;
On the Identifiability of Nonlinear ICA: Sparsity and Beyond. (arXiv:2206.07751v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#28151;&#21512;&#36807;&#31243;&#30340;&#20551;&#35774;&#65292;&#21363;&#32467;&#26500;&#31232;&#30095;&#24615;&#65292;&#26469;&#23454;&#29616;&#38750;&#32447;&#24615;ICA&#30340;&#21487;&#35782;&#21035;&#24615;&#65292;&#26080;&#38656;&#36741;&#21161;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#32447;&#24615;&#29420;&#31435;&#20998;&#37327;&#20998;&#26512;&#26088;&#22312;&#20174;&#20854;&#21487;&#35266;&#27979;&#30340;&#38750;&#32447;&#24615;&#28151;&#21512;&#20013;&#24674;&#22797;&#20986;&#28508;&#22312;&#29420;&#31435;&#20998;&#37327;&#12290;&#22914;&#20309;&#20351;&#38750;&#32447;&#24615;ICA&#27169;&#22411;&#21487;&#36776;&#35782;&#30452;&#21040;&#26576;&#20123;&#24179;&#20961;&#19981;&#30830;&#23450;&#24615;&#26159;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#38271;&#26399;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#31361;&#30772;&#26159;&#23558;&#28304;&#30340;&#26631;&#20934;&#29420;&#31435;&#24615;&#20551;&#35774;&#37325;&#26032;&#23450;&#20041;&#20026;&#22312;&#26576;&#20123;&#36741;&#21161;&#21464;&#37327;&#65288;&#20363;&#22914;&#31867;&#26631;&#31614;&#21644;/&#25110;&#22495;/&#26102;&#38388;&#32034;&#24341;&#65289;&#32473;&#23450;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#65292;&#20316;&#20026;&#24369;&#30417;&#30563;&#25110;&#24402;&#32435;&#20559;&#32622;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#26080;&#26465;&#20214;&#20808;&#39564;&#30340;&#38750;&#32447;&#24615;ICA&#26080;&#27861;&#20174;&#36825;&#20123;&#21457;&#23637;&#20013;&#21463;&#30410;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#26465;&#26367;&#20195;&#36335;&#24452;&#65292;&#24182;&#20165;&#32771;&#34385;&#28151;&#21512;&#36807;&#31243;&#30340;&#20551;&#35774;&#65292;&#20363;&#22914;&#32467;&#26500;&#31232;&#30095;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36825;&#20123;&#32422;&#26463;&#30340;&#20855;&#20307;&#23454;&#20363;&#19979;&#65292;&#29420;&#31435;&#30340;&#28508;&#22312;&#20998;&#37327;&#21487;&#20197;&#20174;&#20854;&#38750;&#32447;&#24615;&#28151;&#21512;&#20013;&#36776;&#35782;&#20986;&#26469;&#65292;&#36798;&#21040;&#38750;&#24179;&#20961;&#30340;&#38750;&#32447;&#24615;ICA&#21487;&#35782;&#21035;&#24615;&#65292;&#32780;&#26080;&#38656;&#36741;&#21161;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonlinear independent component analysis (ICA) aims to recover the underlying independent latent sources from their observable nonlinear mixtures. How to make the nonlinear ICA model identifiable up to certain trivial indeterminacies is a long-standing problem in unsupervised learning. Recent breakthroughs reformulate the standard independence assumption of sources as conditional independence given some auxiliary variables (e.g., class labels and/or domain/time indexes) as weak supervision or inductive bias. However, nonlinear ICA with unconditional priors cannot benefit from such developments. We explore an alternative path and consider only assumptions on the mixing process, such as Structural Sparsity. We show that under specific instantiations of such constraints, the independent latent sources can be identified from their nonlinear mixtures up to a permutation and a component-wise transformation, thus achieving nontrivial identifiability of nonlinear ICA without auxiliary variable
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AVDN&#30340;&#26080;&#20154;&#26426;&#23548;&#33322;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#26469;&#25511;&#21046;&#26080;&#20154;&#26426;&#30340;&#23548;&#33322;&#65292;&#20026;&#35299;&#20915;&#26080;&#20154;&#26426;&#25511;&#21046;&#30340;&#20154;&#20307;&#36127;&#25285;&#12289;&#22810;&#20219;&#21153;&#25805;&#20316;&#12289;&#20197;&#21450;&#27531;&#30142;&#20154;&#21644;&#25163;&#37096;&#21344;&#29992;&#32773;&#25511;&#21046;&#26080;&#20154;&#26426;&#30340;&#38590;&#39064;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2205.12219</link><description>&lt;p&gt;
&#31354;&#20013;&#23545;&#35805;&#19982;&#23548;&#33322;&#65306;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#26080;&#20154;&#26426;&#23548;&#33322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Aerial Vision-and-Dialog Navigation. (arXiv:2205.12219v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AVDN&#30340;&#26080;&#20154;&#26426;&#23548;&#33322;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#26469;&#25511;&#21046;&#26080;&#20154;&#26426;&#30340;&#23548;&#33322;&#65292;&#20026;&#35299;&#20915;&#26080;&#20154;&#26426;&#25511;&#21046;&#30340;&#20154;&#20307;&#36127;&#25285;&#12289;&#22810;&#20219;&#21153;&#25805;&#20316;&#12289;&#20197;&#21450;&#27531;&#30142;&#20154;&#21644;&#25163;&#37096;&#21344;&#29992;&#32773;&#25511;&#21046;&#26080;&#20154;&#26426;&#30340;&#38590;&#39064;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#19982;&#20154;&#31867;&#36827;&#34892;&#23545;&#35805;&#24182;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26159;&#26234;&#33021;&#26080;&#20154;&#26426;&#65288;&#21363;&#26080;&#20154;&#26426;&#65289;&#30340;&#20851;&#38190;&#33021;&#21147;&#12290;&#23427;&#21487;&#20197;&#20943;&#36731;&#20154;&#20204;&#19968;&#30452;&#25569;&#30528;&#25511;&#21046;&#22120;&#30340;&#36127;&#25285;&#65292;&#20801;&#35768;&#21516;&#26102;&#36827;&#34892;&#22810;&#39033;&#20219;&#21153;&#65292;&#20351;&#27531;&#30142;&#20154;&#25110;&#25163;&#37096;&#21344;&#29992;&#32773;&#26356;&#23481;&#26131;&#25511;&#21046;&#26080;&#20154;&#26426;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Aerial Vision-and-Dialog Navigation&#65288;AVDN&#65289;&#30340;&#26080;&#20154;&#26426;&#23548;&#33322;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#26469;&#25511;&#21046;&#26080;&#20154;&#26426;&#30340;&#23548;&#33322;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#24102;&#26377;&#36830;&#32493;&#36924;&#30495;&#29615;&#22659;&#30340;&#26080;&#20154;&#26426;&#27169;&#25311;&#22120;&#65292;&#24182;&#25910;&#38598;&#20102;&#19968;&#32452;&#26032;&#30340;AVDN&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#36229;&#36807;3,000&#20010;&#35760;&#24405;&#30340;&#23548;&#33322;&#36712;&#36857;&#20197;&#21450;&#25351;&#25381;&#23448;&#21644;&#36319;&#38543;&#32773;&#20043;&#38388;&#30340;&#24322;&#27493;&#20154;&#38469;&#23545;&#35805;&#12290;&#25351;&#25381;&#23448;&#25552;&#20379;&#21021;&#22987;&#23548;&#33322;&#25351;&#20196;&#65292;&#24182;&#26681;&#25454;&#38656;&#35201;&#25552;&#20379;&#36827;&#19968;&#27493;&#30340;&#25351;&#23548;&#65292;&#32780;&#36319;&#38543;&#32773;&#22312;&#27169;&#25311;&#22120;&#20013;&#23548;&#33322;&#26080;&#20154;&#26426;&#24182;&#22312;&#38656;&#35201;&#26102;&#25552;&#38382;&#12290;&#22312;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#20013;&#65292;&#36319;&#38543;&#32773;&#23545;&#26080;&#20154;&#26426;&#30340;&#35270;&#35273;&#35266;&#23519;&#20063;&#34987;&#35760;&#24405;&#19979;&#26469;&#12290;&#22522;&#20110;AVDN&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26080;&#20154;&#26426;&#23548;&#33322;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to converse with humans and follow natural language commands is crucial for intelligent unmanned aerial vehicles (a.k.a. drones). It can relieve people's burden of holding a controller all the time, allow multitasking, and make drone control more accessible for people with disabilities or with their hands occupied. To this end, we introduce Aerial Vision-and-Dialog Navigation (AVDN), to navigate a drone via natural language conversation. We build a drone simulator with a continuous photorealistic environment and collect a new AVDN dataset of over 3k recorded navigation trajectories with asynchronous human-human dialogs between commanders and followers. The commander provides initial navigation instruction and further guidance by request, while the follower navigates the drone in the simulator and asks questions when needed. During data collection, followers' attention on the drone's visual observation is also recorded. Based on the AVDN dataset, we study the tasks of aerial
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#38656;&#35201;&#30340;&#21333;&#35843;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#23618;&#20013;&#30340;&#19968;&#37096;&#20998;&#31070;&#32463;&#20803;&#20013;&#37319;&#29992;&#21407;&#22987;&#28608;&#27963;&#20989;&#25968;&#65292;&#21516;&#26102;&#22312;&#21478;&#19968;&#37096;&#20998;&#37319;&#29992;&#20854;&#28857;&#23545;&#31216;&#21453;&#23556;&#26469;&#35299;&#20915;&#26500;&#24314;&#21333;&#35843;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#30340;&#31934;&#24230;&#31526;&#21512;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2205.11775</link><description>&lt;p&gt;
&#21463;&#38480;&#21333;&#35843;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Constrained Monotonic Neural Networks. (arXiv:2205.11775v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#38656;&#35201;&#30340;&#21333;&#35843;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#23618;&#20013;&#30340;&#19968;&#37096;&#20998;&#31070;&#32463;&#20803;&#20013;&#37319;&#29992;&#21407;&#22987;&#28608;&#27963;&#20989;&#25968;&#65292;&#21516;&#26102;&#22312;&#21478;&#19968;&#37096;&#20998;&#37319;&#29992;&#20854;&#28857;&#23545;&#31216;&#21453;&#23556;&#26469;&#35299;&#20915;&#26500;&#24314;&#21333;&#35843;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#30340;&#31934;&#24230;&#31526;&#21512;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#21487;&#20197;&#36924;&#36817;&#20174;&#22024;&#26434;&#25968;&#25454;&#20013;&#24471;&#20986;&#30340;&#20219;&#24847;&#20989;&#25968;&#65292;&#20294;&#22312;&#25512;&#24191;&#36807;&#31243;&#20013;&#38656;&#35201;&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#24182;&#23545;&#23427;&#20204;&#26045;&#21152;&#39069;&#22806;&#30340;&#38480;&#21046;&#65292;&#20854;&#20013;&#21333;&#35843;&#24615;&#26159;&#26368;&#21463;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#38656;&#35201;&#30340;&#23646;&#24615;&#20043;&#19968;&#65292;&#24182;&#19988;&#26159;&#35813;&#35770;&#25991;&#30340;&#37325;&#28857;&#12290;&#26368;&#26089;&#26500;&#24314;&#21333;&#35843;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#26159;&#23558;&#20854;&#26435;&#37325;&#32422;&#26463;&#20026;&#38750;&#36127;&#65292;&#21516;&#26102;&#37319;&#29992;&#21333;&#35843;&#28608;&#27963;&#20989;&#25968;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#35813;&#26041;&#27861;&#26080;&#27861;&#19982;&#24120;&#29992;&#30340;&#38750;&#39281;&#21644;&#28608;&#27963;&#20989;&#25968;&#65288;&#22914;ReLU&#65292;ELU&#65292;SELU&#31561;&#65289;&#19968;&#36215;&#20351;&#29992;&#65292;&#22240;&#20026;&#23427;&#21482;&#33021;&#36924;&#36817;&#20984;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#23618;&#20013;&#30340;&#19968;&#37096;&#20998;&#31070;&#32463;&#20803;&#20013;&#37319;&#29992;&#21407;&#22987;&#28608;&#27963;&#20989;&#25968;&#65292;&#21516;&#26102;&#22312;&#21478;&#19968;&#37096;&#20998;&#37319;&#29992;&#20854;&#28857;&#23545;&#31216;&#21453;&#23556;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#24314;&#31435;&#21333;&#35843;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31934;&#24230;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#65292;&#21516;&#26102;&#28385;&#36275;&#21333;&#35843;&#24615;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are becoming increasingly popular in approximating arbitrary functions from noisy data. But wider adoption is being hindered by the need to explain such models and to impose additional constraints on them. Monotonicity constraint is one of the most requested properties in real-world scenarios and is the focus of this paper. One of the oldest ways to construct a monotonic fully connected neural network is to constrain its weights to be non-negative while employing a monotonic activation function. Unfortunately, this construction does not work with popular non-saturated activation functions such as ReLU, ELU, SELU etc, as it can only approximate convex functions. We show this shortcoming can be fixed by employing the original activation function for a part of the neurons in the layer, and employing its point reflection for the other part. Our experiments show this approach of building monotonic deep neural networks have matching or better accuracy when compared to ot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;DyHPO&#65292;&#23427;&#33021;&#22815;&#23398;&#20064;&#21160;&#24577;&#20915;&#23450;&#21738;&#20010;&#36229;&#21442;&#25968;&#37197;&#32622;&#22312;&#25152;&#26377;&#21487;&#34892;&#30340;&#37197;&#32622;&#20013;&#36827;&#34892;&#19979;&#19968;&#27493;&#35757;&#32451;&#12290;&#23454;&#39564;&#34920;&#26126;DyHPO&#30456;&#27604;&#20110;&#26368;&#20808;&#36827;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#26377;&#26174;&#33879;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2202.09774</link><description>&lt;p&gt;
&#30417;&#30563;&#22810;&#20445;&#30495;&#24230;&#36229;&#21442;&#25968;&#37197;&#32622;&#31454;&#36187;
&lt;/p&gt;
&lt;p&gt;
Supervising the Multi-Fidelity Race of Hyperparameter Configurations. (arXiv:2202.09774v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.09774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;DyHPO&#65292;&#23427;&#33021;&#22815;&#23398;&#20064;&#21160;&#24577;&#20915;&#23450;&#21738;&#20010;&#36229;&#21442;&#25968;&#37197;&#32622;&#22312;&#25152;&#26377;&#21487;&#34892;&#30340;&#37197;&#32622;&#20013;&#36827;&#34892;&#19979;&#19968;&#27493;&#35757;&#32451;&#12290;&#23454;&#39564;&#34920;&#26126;DyHPO&#30456;&#27604;&#20110;&#26368;&#20808;&#36827;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#26377;&#26174;&#33879;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#22810;&#20445;&#30495;&#24230;&#36229;&#21442;&#25968;&#20248;&#21270;&#25216;&#26415;&#65288;HPO&#65289;&#20316;&#20026;&#35843;&#25972;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#32780;&#20986;&#29616;&#65292;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#36229;&#21442;&#25968;&#37197;&#32622;&#30340;HPO&#39044;&#31639;&#20998;&#37197;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;DyHPO&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#20915;&#23450;&#22312;&#25152;&#26377;&#21487;&#34892;&#30340;&#37197;&#32622;&#20013;&#21160;&#24577;&#36187;&#36305;&#20013;&#36827;&#19968;&#27493;&#35757;&#32451;&#21738;&#20010;&#36229;&#21442;&#25968;&#37197;&#32622;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#39640;&#26031;&#36807;&#31243;&#20869;&#26680;&#65292;&#23427;&#23884;&#20837;&#20102;&#23398;&#20064;&#26354;&#32447;&#21160;&#21147;&#23398;&#65292;&#20197;&#21450;&#19968;&#20010;&#21253;&#21547;&#22810;&#39044;&#31639;&#20449;&#24687;&#30340;&#33719;&#24471;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#21253;&#21547;50&#20010;&#25968;&#25454;&#38598;&#65288;&#34920;&#26684;&#12289;&#22270;&#20687;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65289;&#21644;&#19981;&#21516;&#26550;&#26500;&#65288;MLP&#12289;CNN/NAS&#12289;RNN&#65289;&#30340;&#22823;&#35268;&#27169;&#23454;&#39564;&#35777;&#26126;&#20102;DyHPO&#30456;&#27604;&#20110;&#26368;&#20808;&#36827;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#30340;&#26174;&#33879;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-fidelity (gray-box) hyperparameter optimization techniques (HPO) have recently emerged as a promising direction for tuning Deep Learning methods. However, existing methods suffer from a sub-optimal allocation of the HPO budget to the hyperparameter configurations. In this work, we introduce DyHPO, a Bayesian Optimization method that learns to decide which hyperparameter configuration to train further in a dynamic race among all feasible configurations. We propose a new deep kernel for Gaussian Processes that embeds the learning curve dynamics, and an acquisition function that incorporates multi-budget information. We demonstrate the significant superiority of DyHPO against state-of-the-art hyperparameter optimization methods through large-scale experiments comprising 50 datasets (Tabular, Image, NLP) and diverse architectures (MLP, CNN/NAS, RNN).
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24402;&#32435;&#24335;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;MNCI&#65292;&#36890;&#36807;&#22312;&#26102;&#38388;&#32593;&#32476;&#20013;&#25366;&#25496;&#37051;&#22495;&#21644;&#31038;&#21306;&#24433;&#21709;&#26469;&#29983;&#25104;&#33410;&#28857;&#23884;&#20837;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2110.00267</link><description>&lt;p&gt;
&#36890;&#36807;&#25366;&#25496;&#37051;&#22495;&#21644;&#31038;&#21306;&#24433;&#21709;&#30340;&#26041;&#24335;&#65292;&#22312;&#26102;&#38388;&#32593;&#32476;&#20013;&#36827;&#34892;&#24402;&#32435;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Inductive Representation Learning in Temporal Networks via Mining Neighborhood and Community Influences. (arXiv:2110.00267v3 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.00267
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24402;&#32435;&#24335;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;MNCI&#65292;&#36890;&#36807;&#22312;&#26102;&#38388;&#32593;&#32476;&#20013;&#25366;&#25496;&#37051;&#22495;&#21644;&#31038;&#21306;&#24433;&#21709;&#26469;&#29983;&#25104;&#33410;&#28857;&#23884;&#20837;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#20026;&#32593;&#32476;&#20013;&#30340;&#27599;&#20010;&#33410;&#28857;&#29983;&#25104;&#23884;&#20837;&#65292;&#20854;&#21487;&#20419;&#36827;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#31561;&#19979;&#28216;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12290;&#30446;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#36716;&#23548;&#24335;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#65292;&#21363;&#29983;&#25104;&#22266;&#23450;&#33410;&#28857;&#23884;&#20837;&#65292;&#36825;&#19981;&#36866;&#21512;&#23454;&#38469;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24402;&#32435;&#24335;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;MNCI&#65292;&#36890;&#36807;&#22312;&#26102;&#38388;&#32593;&#32476;&#20013;&#25366;&#25496;&#37051;&#22495;&#21644;&#31038;&#21306;&#24433;&#21709;&#26469;&#29983;&#25104;&#33410;&#28857;&#23884;&#20837;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32858;&#21512;&#20989;&#25968;&#65292;&#23558;&#37051;&#22495;&#24433;&#21709;&#19982;&#31038;&#21306;&#24433;&#21709;&#30456;&#32467;&#21512;&#65292;&#20197;&#22312;&#20219;&#20309;&#26102;&#38388;&#29983;&#25104;&#33410;&#28857;&#23884;&#20837;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#24182;&#23558;MNCI&#19982;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21253;&#25324;&#33410;&#28857;&#20998;&#31867;&#21644;&#32593;&#32476;&#21487;&#35270;&#21270;&#31561;&#21508;&#31181;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MNCI&#27604;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network representation learning aims to generate an embedding for each node in a network, which facilitates downstream machine learning tasks such as node classification and link prediction. Current work mainly focuses on transductive network representation learning, i.e. generating fixed node embeddings, which is not suitable for real-world applications. Therefore, we propose a new inductive network representation learning method called MNCI by mining neighborhood and community influences in temporal networks. We propose an aggregator function that integrates neighborhood influence with community influence to generate node embeddings at any time. We conduct extensive experiments on several real-world datasets and compare MNCI with several state-of-the-art baseline methods on various tasks, including node classification and network visualization. The experimental results show that MNCI achieves better performance than baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35299;&#32544;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#31995;&#32479;&#21160;&#21147;&#23398;&#65292;&#23558;&#39046;&#22495;&#21442;&#25968;&#19982;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#21160;&#21147;&#23398;&#20998;&#31163;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#27867;&#21270;&#21644;&#38271;&#26399;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2108.11684</link><description>&lt;p&gt;
&#40065;&#26834;&#39044;&#27979;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#35299;&#32544;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Disentangled Generative Models for Robust Prediction of System Dynamics. (arXiv:2108.11684v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.11684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35299;&#32544;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#31995;&#32479;&#21160;&#21147;&#23398;&#65292;&#23558;&#39046;&#22495;&#21442;&#25968;&#19982;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#21160;&#21147;&#23398;&#20998;&#31163;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#27867;&#21270;&#21644;&#38271;&#26399;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21160;&#21147;&#31995;&#32479;&#39044;&#27979;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#20294;&#26159;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#27867;&#21270;&#21644;&#38271;&#26399;&#31283;&#23450;&#24615;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#23558;&#21160;&#21147;&#31995;&#32479;&#30340;&#39046;&#22495;&#21442;&#25968;&#35270;&#20026;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#21464;&#24322;&#22240;&#32032;&#65292;&#20511;&#37492;&#30417;&#30563;&#35299;&#32544;&#21644;&#22240;&#26524;&#20998;&#35299;&#30340;&#24605;&#24819;&#65292;&#26088;&#22312;&#23558;&#39046;&#22495;&#21442;&#25968;&#19982;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#21160;&#21147;&#23398;&#20998;&#31163;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#27169;&#25311;&#20102;&#30456;&#31354;&#38388;&#21644;&#35270;&#39057;&#24207;&#21015;&#30340;&#21160;&#24577;&#65292;&#24182;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#36229;&#20986;&#20998;&#24067;&#20215;&#20540;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35299;&#32544;&#30340; VAEs &#26356;&#36866;&#24212;&#20110;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#23384;&#22312;&#30340;&#39046;&#22495;&#21442;&#25968;&#31354;&#38388;&#12290;&#21516;&#26102;&#65292;&#35299;&#32544;&#21487;&#20197;&#25913;&#21892;&#35270;&#39057;&#24207;&#21015;&#20013;&#29616;&#26377;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#38271;&#26399;&#21644;&#36229;&#20986;&#20998;&#24067;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have become increasingly of interest in dynamical system prediction, but out-of-distribution generalization and long-term stability still remains challenging. In this work, we treat the domain parameters of dynamical systems as factors of variation of the data generating process. By leveraging ideas from supervised disentanglement and causal factorization, we aim to separate the domain parameters from the dynamics in the latent space of generative models. In our experiments we model dynamics both in phase space and in video sequences and conduct rigorous OOD evaluations. Results indicate that disentangled VAEs adapt better to domain parameters spaces that were not present in the training data. At the same time, disentanglement can improve the long-term and out-of-distribution predictions of state-of-the-art models in video sequences.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#23558;Rational Closure&#25972;&#21512;&#21040;RDFS&#20013;&#65292;&#25552;&#20986;&#20102;&#21487;&#25764;&#38144;&#30340;$\rho df_\bot$&#65292;&#20351;&#20854;&#20173;&#28982;&#20445;&#25345;&#19977;&#20803;&#35821;&#35328;&#30340;&#35821;&#27861;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#26032;&#35268;&#21017;&#36827;&#34892;&#31616;&#21333;&#30340;&#25193;&#23637;&#65292;&#20197;&#22768;&#26126;&#19981;&#20860;&#23481;&#30340;&#23454;&#20307;&#12290;</title><link>http://arxiv.org/abs/2007.07573</link><description>&lt;p&gt;
&#22522;&#20110;Rational Closure&#30340;&#21487;&#25764;&#38144;RDFS
&lt;/p&gt;
&lt;p&gt;
Defeasible RDFS via Rational Closure. (arXiv:2007.07573v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.07573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#23558;Rational Closure&#25972;&#21512;&#21040;RDFS&#20013;&#65292;&#25552;&#20986;&#20102;&#21487;&#25764;&#38144;&#30340;$\rho df_\bot$&#65292;&#20351;&#20854;&#20173;&#28982;&#20445;&#25345;&#19977;&#20803;&#35821;&#35328;&#30340;&#35821;&#27861;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#26032;&#35268;&#21017;&#36827;&#34892;&#31616;&#21333;&#30340;&#25193;&#23637;&#65292;&#20197;&#22768;&#26126;&#19981;&#20860;&#23481;&#30340;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38750;&#21333;&#35843;&#36923;&#36753;&#39046;&#22495;&#65292;Rational Closure&#65288;RC&#65289;&#30340;&#27010;&#24565;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#26041;&#27861;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#25551;&#36848;&#36923;&#36753;&#65288;DLs&#65289;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;RC&#22312; OWL 2 &#30340;&#20027;&#35201;&#25104;&#20998;&#65288;&#31867;&#21644;&#35282;&#33394;&#65289;&#30340;&#35821;&#20041;Web&#26631;&#20934;&#26412;&#20307;&#35821;&#35328;&#19979;&#24471;&#21040;&#20102;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;RC&#38598;&#25104;&#21040;&#19977;&#20803;&#35821;&#35328;RDFS&#20013;&#65292;RDFS&#26159;&#19982;OWL2&#19968;&#36215;&#26500;&#25104;&#20027;&#35201;&#30340;&#35821;&#20041;Web&#26412;&#20307;&#35821;&#35328;&#30340;&#20004;&#20010;&#26631;&#20934;&#20043;&#19968;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20174;RDFS&#32972;&#21518;&#30340;$\rho df$&#24320;&#22987;&#65292;&#28982;&#21518;&#23558;&#20854;&#25193;&#23637;&#20026;$\rho df_\bot$&#65292;&#20801;&#35768;&#22768;&#26126;&#20004;&#20010;&#23454;&#20307;&#26159;&#19981;&#20860;&#23481;&#30340;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#36890;&#36807;&#20856;&#22411;&#30340;RC&#26500;&#36896;&#25552;&#20986;&#20102;&#21487;&#25764;&#38144;&#30340;$\rho df_\bot$&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20027;&#35201;&#29305;&#28857;&#26159;&#65306;&#65288;i&#65289;&#19982;&#22823;&#22810;&#25968;&#22312;&#21333;&#35843;RDFS&#20043;&#19978;&#28155;&#21152;&#19968;&#20010;&#39069;&#22806;&#30340;&#38750;&#21333;&#35843;&#35268;&#21017;&#23618;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#21487;&#25764;&#38144;&#30340;$\rho df_\bot$&#20173;&#28982;&#26159;&#35821;&#27861;&#19978;&#30340;&#19977;&#20803;&#35821;&#35328;&#65292;&#24182;&#19988;&#26159;&#36890;&#36807;&#24341;&#20837;&#19968;&#20123;&#26032;&#35268;&#21017;&#26469;&#31616;&#21333;&#22320;&#25193;&#23637;$\rho df_\bot$&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of non-monotonic logics, the notion of Rational Closure (RC) is acknowledged as a prominent approach. In recent years, RC has gained even more popularity in the context of Description Logics (DLs), the logic underpinning the semantic web standard ontology language OWL 2, whose main ingredients are classes and roles. In this work, we show how to integrate RC within the triple language RDFS, which together with OWL2 are the two major standard semantic web ontology languages. To do so, we start from $\rho df$, which is the logic behind RDFS, and then extend it to $\rho df_\bot$, allowing to state that two entities are incompatible. Eventually, we propose defeasible $\rho df_\bot$ via a typical RC construction. The main features of our approach are: (i) unlike most other approaches that add an extra non-monotone rule layer on top of monotone RDFS, defeasible $\rho df_\bot$ remains syntactically a triple language and is a simple extension of $\rho df_\bot$ by introducing some n
&lt;/p&gt;</description></item></channel></rss>