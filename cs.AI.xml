<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#20256;&#25773;&#26694;&#26550;&#21644;&#20004;&#31181;&#26032;&#30340;&#33410;&#28857;&#33258;&#36866;&#24212;&#20256;&#25773;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#21487;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#35770;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#26681;&#25454;&#33410;&#28857;&#30340;&#25299;&#25169;&#20449;&#24687;&#33258;&#23450;&#20041;&#27599;&#20010;&#33410;&#28857;&#30340;&#26368;&#20339;&#20256;&#25773;&#28145;&#24230;&#65292;&#20174;&#32780;&#36991;&#20813;&#20887;&#20313;&#29305;&#24449;&#20256;&#25773;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#36229;&#21442;&#25968;&#28789;&#27963;&#22320;&#31649;&#29702;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#24310;&#36831;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.10998</link><description>&lt;p&gt;
&#20351;&#29992;&#33410;&#28857;&#33258;&#36866;&#24212;&#20256;&#25773;&#21152;&#36895;&#21487;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#35770;
&lt;/p&gt;
&lt;p&gt;
Accelerating Scalable Graph Neural Network Inference with Node-Adaptive Propagation. (arXiv:2310.10998v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#20256;&#25773;&#26694;&#26550;&#21644;&#20004;&#31181;&#26032;&#30340;&#33410;&#28857;&#33258;&#36866;&#24212;&#20256;&#25773;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#21487;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#35770;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#26681;&#25454;&#33410;&#28857;&#30340;&#25299;&#25169;&#20449;&#24687;&#33258;&#23450;&#20041;&#27599;&#20010;&#33410;&#28857;&#30340;&#26368;&#20339;&#20256;&#25773;&#28145;&#24230;&#65292;&#20174;&#32780;&#36991;&#20813;&#20887;&#20313;&#29305;&#24449;&#20256;&#25773;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#36229;&#21442;&#25968;&#28789;&#27963;&#22320;&#31649;&#29702;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#24310;&#36831;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#21151;&#25928;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#22270;&#30340;&#35268;&#27169;&#20351;&#24471;GNNs&#30340;&#23454;&#26102;&#25512;&#35770;&#38754;&#20020;&#24040;&#22823;&#25361;&#25112;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#21487;&#25193;&#23637;GNNs&#21033;&#29992;&#32447;&#24615;&#20256;&#25773;&#23545;&#29305;&#24449;&#36827;&#34892;&#39044;&#22788;&#29702;&#24182;&#21152;&#36895;&#35757;&#32451;&#21644;&#25512;&#35770;&#36807;&#31243;&#65292;&#20294;&#22312;&#23545;&#26410;&#30693;&#33410;&#28857;&#36827;&#34892;&#25512;&#35770;&#26102;&#20173;&#28982;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#22240;&#20026;&#29305;&#24449;&#39044;&#22788;&#29702;&#38656;&#35201;&#24050;&#30693;&#19988;&#22266;&#23450;&#30340;&#22270;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#21152;&#36895;&#36825;&#31181;&#24402;&#32435;&#35774;&#32622;&#19979;&#30340;&#21487;&#25193;&#23637;GNNs&#25512;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#32447;&#20256;&#25773;&#26694;&#26550;&#21644;&#20004;&#31181;&#26032;&#30340;&#33410;&#28857;&#33258;&#36866;&#24212;&#20256;&#25773;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#33410;&#28857;&#30340;&#25299;&#25169;&#20449;&#24687;&#33258;&#23450;&#20041;&#27599;&#20010;&#33410;&#28857;&#30340;&#26368;&#20339;&#20256;&#25773;&#28145;&#24230;&#65292;&#20174;&#32780;&#36991;&#20813;&#20887;&#20313;&#29305;&#24449;&#20256;&#25773;&#12290;&#36890;&#36807;&#31616;&#21333;&#30340;&#36229;&#21442;&#25968;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#31649;&#29702;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#24310;&#36831;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#34917;&#20607;&#25439;&#22833;&#30340;&#31934;&#24230;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#34917;&#20607;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#20801;&#35768;&#20256;&#25773;&#30340;&#23618;&#25968;&#36229;&#36807;&#25152;&#36873;&#25321;&#30340;&#28145;&#24230;&#65292;&#20197;&#25552;&#39640;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have exhibited exceptional efficacy in a diverse array of applications. However, the sheer size of large-scale graphs presents a significant challenge to real-time inference with GNNs. Although existing Scalable GNNs leverage linear propagation to preprocess the features and accelerate the training and inference procedure, these methods still suffer from scalability issues when making inferences on unseen nodes, as the feature preprocessing requires the graph to be known and fixed. To further accelerate Scalable GNNs inference in this inductive setting, we propose an online propagation framework and two novel node-adaptive propagation methods that can customize the optimal propagation depth for each node based on its topological information and thereby avoid redundant feature propagation. The trade-off between accuracy and latency can be flexibly managed through simple hyper-parameters to accommodate various latency constraints. Moreover, to compensate for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;Clifford&#30340;&#20960;&#20309;&#20195;&#25968;&#21644;&#20984;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20248;&#26435;&#37325;&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#26679;&#26412;&#30340;&#26964;&#31215;&#26469;&#33719;&#24471;&#65292;&#24182;&#19988;&#35757;&#32451;&#38382;&#39064;&#21487;&#20197;&#31616;&#21270;&#20026;&#23545;&#26964;&#31215;&#29305;&#24449;&#36827;&#34892;&#20984;&#20248;&#21270;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.16512</link><description>&lt;p&gt;
&#20174;&#22797;&#26434;&#21040;&#28165;&#26224;&#65306;&#36890;&#36807;Clifford&#30340;&#20960;&#20309;&#20195;&#25968;&#21644;&#20984;&#20248;&#21270;&#30340;&#20998;&#26512;&#34920;&#36798;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;
&lt;/p&gt;
&lt;p&gt;
From Complexity to Clarity: Analytical Expressions of Deep Neural Network Weights via Clifford's Geometric Algebra and Convexity. (arXiv:2309.16512v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;Clifford&#30340;&#20960;&#20309;&#20195;&#25968;&#21644;&#20984;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20248;&#26435;&#37325;&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#26679;&#26412;&#30340;&#26964;&#31215;&#26469;&#33719;&#24471;&#65292;&#24182;&#19988;&#35757;&#32451;&#38382;&#39064;&#21487;&#20197;&#31616;&#21270;&#20026;&#23545;&#26964;&#31215;&#29305;&#24449;&#36827;&#34892;&#20984;&#20248;&#21270;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#65288;Clifford&#65289;&#20195;&#25968;&#21644;&#20984;&#20248;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#20351;&#29992;&#26631;&#20934;&#27491;&#21017;&#21270;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20248;&#26435;&#37325;&#30001;&#35757;&#32451;&#26679;&#26412;&#30340;&#26964;&#31215;&#32473;&#20986;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#38382;&#39064;&#21487;&#31616;&#21270;&#20026;&#23545;&#26964;&#31215;&#29305;&#24449;&#36827;&#34892;&#20984;&#20248;&#21270;&#65292;&#22312;&#20854;&#20013;&#32534;&#30721;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#35813;&#32467;&#26500;&#20197;&#25968;&#25454;&#21521;&#37327;&#29983;&#25104;&#30340;&#19977;&#35282;&#24418;&#21644;&#24179;&#34892;&#20307;&#30340;&#26377;&#31526;&#21495;&#20307;&#31215;&#34920;&#31034;&#12290;&#20984;&#38382;&#39064;&#36890;&#36807;$\ell_1$&#27491;&#21017;&#21270;&#25214;&#21040;&#26679;&#26412;&#30340;&#19968;&#20010;&#23567;&#23376;&#38598;&#65292;&#20197;&#21457;&#29616;&#20165;&#30456;&#20851;&#30340;&#26964;&#31215;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#24037;&#20316;&#26426;&#21046;&#30340;&#26032;&#35270;&#35282;&#65292;&#24182;&#25581;&#31034;&#20102;&#38544;&#34255;&#23618;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel analysis of neural networks based on geometric (Clifford) algebra and convex optimization. We show that optimal weights of deep ReLU neural networks are given by the wedge product of training samples when trained with standard regularized loss. Furthermore, the training problem reduces to convex optimization over wedge product features, which encode the geometric structure of the training dataset. This structure is given in terms of signed volumes of triangles and parallelotopes generated by data vectors. The convex problem finds a small subset of samples via $\ell_1$ regularization to discover only relevant wedge product features. Our analysis provides a novel perspective on the inner workings of deep neural networks and sheds light on the role of the hidden layers.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20809;&#35889;&#29702;&#35770;&#65292;&#29992;&#20110;&#29702;&#35299;&#31070;&#32463;&#39044;&#27979;&#21644;&#23545;&#40784;&#38382;&#39064;&#12290;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#28608;&#27963;&#30340;&#20809;&#35889;&#20559;&#24046;&#21644;&#31070;&#32463;&#21709;&#24212;&#19982;&#21487;&#23398;&#20064;&#23376;&#31354;&#38388;&#30340;&#23545;&#40784;&#24773;&#20917;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#23646;&#24615;&#65292;&#24182;&#21306;&#20998;&#22312;&#31070;&#32463;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#30456;&#21516;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.12821</link><description>&lt;p&gt;
&#31070;&#32463;&#39044;&#27979;&#21644;&#23545;&#40784;&#30340;&#20809;&#35889;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Spectral Theory of Neural Prediction and Alignment. (arXiv:2309.12821v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12821
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20809;&#35889;&#29702;&#35770;&#65292;&#29992;&#20110;&#29702;&#35299;&#31070;&#32463;&#39044;&#27979;&#21644;&#23545;&#40784;&#38382;&#39064;&#12290;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#28608;&#27963;&#30340;&#20809;&#35889;&#20559;&#24046;&#21644;&#31070;&#32463;&#21709;&#24212;&#19982;&#21487;&#23398;&#20064;&#23376;&#31354;&#38388;&#30340;&#23545;&#40784;&#24773;&#20917;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#23646;&#24615;&#65292;&#24182;&#21306;&#20998;&#22312;&#31070;&#32463;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#30456;&#21516;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#31034;&#32463;&#24120;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#21709;&#24212;&#19982;&#29983;&#29289;&#31995;&#32479;&#27979;&#24471;&#30340;&#21709;&#24212;&#36827;&#34892;&#22238;&#24402;&#26469;&#36827;&#34892;&#27604;&#36739;&#12290;&#35768;&#22810;&#19981;&#21516;&#30340;&#20808;&#36827;&#31070;&#32463;&#32593;&#32476;&#20135;&#29983;&#20102;&#31867;&#20284;&#30340;&#31070;&#32463;&#39044;&#27979;&#65292;&#20294;&#20173;&#28982;&#19981;&#28165;&#26970;&#22914;&#20309;&#21306;&#20998;&#22312;&#39044;&#27979;&#31070;&#32463;&#21709;&#24212;&#26041;&#38754;&#34920;&#29616;&#30456;&#21516;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#28145;&#20837;&#20102;&#35299;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#36817;&#26399;&#25552;&#20986;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#22238;&#24402;&#20013;&#30340;&#27867;&#21270;&#35823;&#24046;&#19982;&#27169;&#22411;&#28608;&#27963;&#30340;&#20809;&#35889;&#20559;&#24046;&#21644;&#31070;&#32463;&#21709;&#24212;&#19982;&#21487;&#23398;&#20064;&#23376;&#31354;&#38388;&#20043;&#38388;&#30340;&#23545;&#40784;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#29702;&#35770;&#25193;&#23637;&#21040;&#20102;&#27169;&#22411;&#28608;&#27963;&#21644;&#31070;&#32463;&#21709;&#24212;&#20043;&#38388;&#30340;&#22238;&#24402;&#24773;&#20917;&#65292;&#24182;&#23450;&#20041;&#20102;&#25551;&#36848;&#35823;&#24046;&#23884;&#20837;&#20960;&#20309;&#24615;&#36136;&#30340;&#20960;&#20309;&#23646;&#24615;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#22823;&#37327;&#39044;&#27979;&#35270;&#35273;&#30382;&#23618;&#27963;&#21160;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#23637;&#31034;&#20102;&#22810;&#31181;&#20960;&#20309;&#24418;&#29366;&#23548;&#33268;&#20302;&#31070;&#32463;&#39044;&#27979;&#35823;&#24046;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
The representations of neural networks are often compared to those of biological systems by performing regression between the neural network responses and those measured from biological systems. Many different state-of-the-art deep neural networks yield similar neural predictions, but it remains unclear how to differentiate among models that perform equally well at predicting neural responses. To gain insight into this, we use a recent theoretical framework that relates the generalization error from regression to the spectral bias of the model activations and the alignment of the neural responses onto the learnable subspace of the model. We extend this theory to the case of regression between model activations and neural responses, and define geometrical properties describing the error embedding geometry. We test a large number of deep neural networks that predict visual cortical activity and show that there are multiple types of geometries that result in low neural prediction error as
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#20419;&#36827;&#24739;&#32773;&#21644;&#21307;&#29983;&#20043;&#38388;&#30340;&#24322;&#27493;&#36890;&#20449;&#65292;&#36890;&#36807;&#35775;&#35848;&#30740;&#31350;&#20102;&#35299;&#20102;&#20182;&#20204;&#23545;LLMs&#30340;&#38656;&#27714;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;Talk2Care&#30340;LLM&#39537;&#21160;&#30340;&#36890;&#20449;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.09357</link><description>&lt;p&gt;
Talk2Care: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20419;&#36827;&#24322;&#27493;&#24739;&#32773;-&#21307;&#29983;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Talk2Care: Facilitating Asynchronous Patient-Provider Communication with Large-Language-Model. (arXiv:2309.09357v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#20419;&#36827;&#24739;&#32773;&#21644;&#21307;&#29983;&#20043;&#38388;&#30340;&#24322;&#27493;&#36890;&#20449;&#65292;&#36890;&#36807;&#35775;&#35848;&#30740;&#31350;&#20102;&#35299;&#20102;&#20182;&#20204;&#23545;LLMs&#30340;&#38656;&#27714;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;Talk2Care&#30340;LLM&#39537;&#21160;&#30340;&#36890;&#20449;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26377;&#22823;&#37327;&#30340;&#36828;&#31243;&#21307;&#30103;&#24212;&#29992;&#31243;&#24207;&#26469;&#24110;&#21161;&#23478;&#24237;&#20013;&#30340;&#32769;&#24180;&#20154;&#21644;&#21307;&#30103;&#25552;&#20379;&#32773;&#65292;&#20294;&#22522;&#26412;&#30340;&#28040;&#24687;&#21644;&#30005;&#35805;&#20173;&#28982;&#26159;&#26368;&#24120;&#35265;&#30340;&#36890;&#20449;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#26377;&#38480;&#30340;&#21487;&#29992;&#24615;&#12289;&#20449;&#24687;&#20002;&#22833;&#21644;&#27969;&#31243;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;&#20419;&#36827;&#24739;&#32773;-&#21307;&#29983;&#36890;&#20449;&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21450;&#20854;&#24378;&#22823;&#30340;&#33258;&#28982;&#23545;&#35805;&#21644;&#25688;&#35201;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;LLMs&#22312;&#36890;&#20449;&#36807;&#31243;&#20013;&#30340;&#20316;&#29992;&#36824;&#23384;&#22312;&#26377;&#38480;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#20004;&#39033;&#35775;&#35848;&#30740;&#31350;&#65292;&#20998;&#21035;&#19982;&#32769;&#24180;&#20154;(N=10)&#21644;&#21307;&#30103;&#25552;&#20379;&#32773;(N=9)&#36827;&#34892;&#20102;&#20132;&#27969;&#65292;&#20197;&#20102;&#35299;&#20182;&#20204;&#22312;&#24739;&#32773;-&#21307;&#29983;&#24322;&#27493;&#36890;&#20449;&#20013;&#23545;LLMs&#30340;&#38656;&#27714;&#21644;&#26426;&#20250;&#12290;&#22522;&#20110;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;LLM&#39537;&#21160;&#30340;&#36890;&#20449;&#31995;&#32479;Talk2Care&#65292;&#24182;&#20026;&#20004;&#20010;&#32676;&#20307;&#35774;&#35745;&#20102;&#20132;&#20114;&#32452;&#20214;: (1) &#23545;&#20110;&#32769;&#24180;&#20154;&#65292;&#25105;&#20204;&#21033;&#29992;&#35821;&#38899;&#21161;&#25163;&#30340;&#20415;&#21033;&#24615;&#21644;&#26131;&#20110;&#33719;&#21462;&#24615;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;LLM&#39537;&#21160;&#30340;&#35821;&#38899;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
Despite the plethora of telehealth applications to assist home-based older adults and healthcare providers, basic messaging and phone calls are still the most common communication methods, which suffer from limited availability, information loss, and process inefficiencies. One promising solution to facilitate patient-provider communication is to leverage large language models (LLMs) with their powerful natural conversation and summarization capability. However, there is a limited understanding of LLMs' role during the communication. We first conducted two interview studies with both older adults (N=10) and healthcare providers (N=9) to understand their needs and opportunities for LLMs in patient-provider asynchronous communication. Based on the insights, we built an LLM-powered communication system, Talk2Care, and designed interactive components for both groups: (1) For older adults, we leveraged the convenience and accessibility of voice assistants (VAs) and built an LLM-powered VA i
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;GNN&#30340;Lipschitz&#30028;&#38480;&#34920;&#24449;&#20102;GNN&#23545;&#20844;&#24179;&#24615;&#31283;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#21644;&#22266;&#26377;&#20559;&#20506;&#30340;&#24773;&#20917;&#19979;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#23545;&#20110;&#38480;&#21046;GNN&#36755;&#20986;&#30340;&#25200;&#21160;&#20197;&#20445;&#38556;&#20844;&#24179;&#24615;&#35757;&#32451;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.03648</link><description>&lt;p&gt;
GNN&#23545;&#20844;&#24179;&#24615;&#31283;&#23450;&#24615;&#30340;Lipschitz&#29305;&#24615;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Characterizing Lipschitz Stability of GNN for Fairness. (arXiv:2309.03648v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03648
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;GNN&#30340;Lipschitz&#30028;&#38480;&#34920;&#24449;&#20102;GNN&#23545;&#20844;&#24179;&#24615;&#31283;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#21644;&#22266;&#26377;&#20559;&#20506;&#30340;&#24773;&#20917;&#19979;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#23545;&#20110;&#38480;&#21046;GNN&#36755;&#20986;&#30340;&#25200;&#21160;&#20197;&#20445;&#38556;&#20844;&#24179;&#24615;&#35757;&#32451;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Lipschitz&#30028;&#38480;&#26159;&#20174;&#40065;&#26834;&#32479;&#35745;&#23398;&#20013;&#20511;&#37492;&#30340;&#19968;&#31181;&#25216;&#26415;&#65292;&#21487;&#20197;&#38480;&#21046;&#36755;&#20986;&#30456;&#23545;&#20110;&#36755;&#20837;&#30340;&#26368;&#22823;&#21464;&#21270;&#65292;&#32771;&#34385;&#21040;&#30456;&#20851;&#30340;&#38750;&#20851;&#38190;&#20559;&#20506;&#22240;&#32032;&#12290;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35777;&#26126;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#26597;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20986;&#31283;&#23450;&#24615;&#65292;&#32780;&#19981;&#20250;&#22686;&#21152;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#26368;&#36817;&#65292;&#23545;&#20110;&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#19978;&#25805;&#20316;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#27809;&#26377;&#30740;&#31350;&#35843;&#26597;GNN&#30340;Lipschitz&#30028;&#38480;&#20197;&#25581;&#31034;&#27169;&#22411;&#36755;&#20986;&#30340;&#31283;&#23450;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#20855;&#26377;&#22266;&#26377;&#20559;&#20506;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#26102;&#12290;&#30001;&#20110;&#24120;&#35265;&#22270;&#24418;&#25968;&#25454;&#22312;GNN&#35757;&#32451;&#20013;&#23384;&#22312;&#22266;&#26377;&#20559;&#24046;&#65292;&#36825;&#32473;&#38480;&#21046;&#30001;&#36755;&#20837;&#20559;&#24046;&#24341;&#36215;&#30340;GNN&#36755;&#20986;&#25200;&#21160;&#65292;&#20174;&#32780;&#22312;&#35757;&#32451;&#26399;&#38388;&#20445;&#38556;&#20844;&#24179;&#24615;&#65292;&#24102;&#26469;&#20102;&#20005;&#23803;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#23613;&#31649;Lipschitz&#24120;&#25968;&#22312;&#25511;&#21046;&#27431;&#20960;&#37324;&#24471;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#23450;&#24615;&#26041;&#38754;&#26377;&#25152;&#24212;&#29992;&#65292;&#20294;&#31934;&#30830;Lipschitz&#24120;&#25968;&#30340;&#35745;&#31639;&#21313;&#20998;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Lipschitz bound, a technique from robust statistics, can limit the maximum changes in the output concerning the input, taking into account associated irrelevant biased factors. It is an efficient and provable method for examining the output stability of machine learning models without incurring additional computation costs. Recently, Graph Neural Networks (GNNs), which operate on non-Euclidean data, have gained significant attention. However, no previous research has investigated the GNN Lipschitz bounds to shed light on stabilizing model outputs, especially when working on non-Euclidean data with inherent biases. Given the inherent biases in common graph data used for GNN training, it poses a serious challenge to constraining the GNN output perturbations induced by input biases, thereby safeguarding fairness during training. Recently, despite the Lipschitz constant's use in controlling the stability of Euclideanneural networks, the calculation of the precise Lipschitz constant rem
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;&#23433;&#20840;&#20851;&#38190;&#30340;&#21327;&#21516;&#23454;&#20307;&#31995;&#32479;&#30340;&#25805;&#20316;&#36755;&#20986;&#29305;&#24615;&#30340;&#38543;&#26426;&#31526;&#21512;&#24615;&#65292;&#20174;&#32780;&#21457;&#29616;&#26410;&#30693;-&#26410;&#30693;&#24773;&#20917;&#24182;&#35780;&#20272;&#28508;&#22312;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2309.02603</link><description>&lt;p&gt;
&#20351;&#29992;&#32479;&#35745;&#23398;&#19982;&#29289;&#29702;&#24341;&#23548;&#30340;&#36807;&#31243;&#27169;&#22411;&#22312;&#21327;&#21516;&#23454;&#20307;&#31995;&#32479;&#20013;&#26816;&#27979;&#26410;&#30693;-&#26410;&#30693;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Detection of Unknown-Unknowns in Cyber-Physical Systems using Statistical Conformance with Physics Guided Process Models. (arXiv:2309.02603v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02603
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;&#23433;&#20840;&#20851;&#38190;&#30340;&#21327;&#21516;&#23454;&#20307;&#31995;&#32479;&#30340;&#25805;&#20316;&#36755;&#20986;&#29305;&#24615;&#30340;&#38543;&#26426;&#31526;&#21512;&#24615;&#65292;&#20174;&#32780;&#21457;&#29616;&#26410;&#30693;-&#26410;&#30693;&#24773;&#20917;&#24182;&#35780;&#20272;&#28508;&#22312;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#30693;-&#26410;&#30693;&#24773;&#20917;&#26159;&#25351;&#22312;&#35774;&#35745;&#21644;&#27979;&#35797;&#38454;&#27573;&#26410;&#32771;&#34385;&#21040;&#30340;&#21327;&#21516;&#23454;&#20307;&#31995;&#32479;&#20013;&#30340;&#25805;&#20316;&#22330;&#26223;&#12290;&#22312;&#26410;&#30693;-&#26410;&#30693;&#24773;&#20917;&#19979;&#65292;&#21327;&#21516;&#23454;&#20307;&#31995;&#32479;&#30340;&#25805;&#20316;&#34892;&#20026;&#19981;&#33021;&#20445;&#35777;&#28385;&#36275;&#36890;&#36807;&#36755;&#20986;&#36712;&#36857;&#19978;&#30340;&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#65288;STL&#65289;&#25351;&#23450;&#30340;&#23433;&#20840;&#24615;&#21644;&#26377;&#25928;&#24615;&#31561;&#35201;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#23433;&#20840;&#20851;&#38190;&#21327;&#21516;&#23454;&#20307;&#31995;&#32479;&#25805;&#20316;&#36755;&#20986;&#29305;&#24615;&#30340;&#38543;&#26426;&#31526;&#21512;&#24615;&#65292;&#21487;&#20197;&#21457;&#29616;&#26410;&#30693;-&#26410;&#30693;&#24773;&#20917;&#24182;&#35780;&#20272;&#28508;&#22312;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#21147;&#23398;&#35825;&#23548;&#30340;&#28151;&#21512;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;DiH-RNN&#65289;&#65292;&#29992;&#20110;&#25366;&#25496;&#29289;&#29702;&#24341;&#23548;&#30340;&#20195;&#29702;&#27169;&#22411;&#65288;PGSM&#65289;&#65292;&#24182;&#20351;&#29992;STL&#23545;&#27169;&#22411;&#31995;&#25968;&#36827;&#34892;&#27169;&#22411;&#19968;&#33268;&#24615;&#26816;&#26597;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#36890;&#36807;&#26410;&#30693;&#33008;&#23707;&#32032;&#21345;&#24102;&#38169;&#35823;&#23548;&#33268;&#20154;&#24037;&#33008;&#33146;(AP)&#25805;&#20316;&#21464;&#21270;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unknown unknowns are operational scenarios in a cyber-physical system that are not accounted for in the design and test phase. As such under unknown-unknown scenarios, the operational behavior of the CPS is not guaranteed to meet requirements such as safety and efficacy specified using Signal Temporal Logic (STL) on the output trajectories. We propose a novel framework for analyzing the stochastic conformance of operational output characteristics of safety-critical cyber-physical systems that can discover unknown-unknown scenarios and evaluate potential safety hazards. We propose dynamics-induced hybrid recurrent neural networks (DiH-RNN) to mine a physics-guided surrogate model (PGSM) which is used to check the model conformance using STL on the model coefficients. We demonstrate the detection of operational changes in an Artificial Pancreas(AP) due to unknown insulin cartridge errors.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19981;&#31934;&#30830;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#40065;&#26834;&#32479;&#35745;&#39564;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20027;&#21160;&#23398;&#20064;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#65292;&#21487;&#20197;&#22312;&#22823;&#37327;&#30340;&#20998;&#24067;&#19978;&#25552;&#20379;&#23545;&#40657;&#30418;&#31995;&#32479;&#34892;&#20026;&#30340;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.14815</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#31934;&#30830;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#40065;&#26834;&#32479;&#35745;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Distributionally Robust Statistical Verification with Imprecise Neural Networks. (arXiv:2308.14815v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19981;&#31934;&#30830;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#40065;&#26834;&#32479;&#35745;&#39564;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20027;&#21160;&#23398;&#20064;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#65292;&#21487;&#20197;&#22312;&#22823;&#37327;&#30340;&#20998;&#24067;&#19978;&#25552;&#20379;&#23545;&#40657;&#30418;&#31995;&#32479;&#34892;&#20026;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;AI&#23433;&#20840;&#39046;&#22495;&#65292;&#19968;&#20010;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#26159;&#22312;&#39640;&#32500;&#33258;&#20027;&#31995;&#32479;&#30340;&#34892;&#20026;&#19978;&#25552;&#20379;&#20445;&#35777;&#12290;&#20197;&#21487;&#36798;&#24615;&#20998;&#26512;&#20026;&#20013;&#24515;&#30340;&#39564;&#35777;&#26041;&#27861;&#26080;&#27861;&#25193;&#23637;&#65292;&#32780;&#32431;&#31929;&#30340;&#32479;&#35745;&#26041;&#27861;&#21463;&#21040;&#23545;&#37319;&#26679;&#36807;&#31243;&#30340;&#20998;&#24067;&#20551;&#35774;&#30340;&#38480;&#21046;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#40657;&#30418;&#31995;&#32479;&#30340;&#20998;&#24067;&#40065;&#26834;&#29256;&#26412;&#30340;&#32479;&#35745;&#39564;&#35777;&#38382;&#39064;&#65292;&#20854;&#20013;&#25105;&#20204;&#30340;&#24615;&#33021;&#20445;&#35777;&#36866;&#29992;&#20110;&#22823;&#37327;&#30340;&#20998;&#24067;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#26680;&#24515;&#37096;&#20998;&#26159;&#19968;&#31181;&#31216;&#20026;&#19981;&#31934;&#30830;&#31070;&#32463;&#32593;&#32476;&#30340;&#38598;&#25104;&#25216;&#26415;&#65292;&#23427;&#25552;&#20379;&#20102;&#19981;&#30830;&#23450;&#24615;&#20197;&#25351;&#23548;&#20027;&#21160;&#23398;&#20064;&#12290;&#20027;&#21160;&#23398;&#20064;&#20351;&#29992;&#20102;&#19968;&#31181;&#31216;&#20026;Sherlock&#30340;&#20840;&#38754;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#24037;&#20855;&#26469;&#25910;&#38598;&#26679;&#26412;&#12290;&#22312;openAI gym Mujoco&#29615;&#22659;&#20013;&#20351;&#29992;&#22810;&#20010;&#29289;&#29702;&#27169;&#25311;&#22120;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
A particularly challenging problem in AI safety is providing guarantees on the behavior of high-dimensional autonomous systems. Verification approaches centered around reachability analysis fail to scale, and purely statistical approaches are constrained by the distributional assumptions about the sampling process. Instead, we pose a distributionally robust version of the statistical verification problem for black-box systems, where our performance guarantees hold over a large family of distributions. This paper proposes a novel approach based on a combination of active learning, uncertainty quantification, and neural network verification. A central piece of our approach is an ensemble technique called Imprecise Neural Networks, which provides the uncertainty to guide active learning. The active learning uses an exhaustive neural-network verification tool Sherlock to collect samples. An evaluation on multiple physical simulators in the openAI gym Mujoco environments with reinforcement-
&lt;/p&gt;</description></item><item><title>InstructME&#26159;&#19968;&#20010;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#25351;&#23548;&#38899;&#20048;&#32534;&#36753;&#21644;&#28151;&#38899;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#23610;&#24230;&#32858;&#21512;&#21644;&#24341;&#20837;&#21644;&#24358;&#36827;&#23637;&#30697;&#38453;&#26469;&#20445;&#25345;&#32534;&#36753;&#30340;&#19968;&#33268;&#24615;&#21644;&#25552;&#39640;&#26059;&#24459;&#21644;&#35856;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.14360</link><description>&lt;p&gt;
InstructME:&#19968;&#20010;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#25351;&#23548;&#38899;&#20048;&#32534;&#36753;&#21644;&#28151;&#38899;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
InstructME: An Instruction Guided Music Edit And Remix Framework with Latent Diffusion Models. (arXiv:2308.14360v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14360
&lt;/p&gt;
&lt;p&gt;
InstructME&#26159;&#19968;&#20010;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#25351;&#23548;&#38899;&#20048;&#32534;&#36753;&#21644;&#28151;&#38899;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#23610;&#24230;&#32858;&#21512;&#21644;&#24341;&#20837;&#21644;&#24358;&#36827;&#23637;&#30697;&#38453;&#26469;&#20445;&#25345;&#32534;&#36753;&#30340;&#19968;&#33268;&#24615;&#21644;&#25552;&#39640;&#26059;&#24459;&#21644;&#35856;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#20048;&#32534;&#36753;&#20027;&#35201;&#28041;&#21450;&#21040;&#20462;&#25913;&#20048;&#22120;&#36712;&#36947;&#25110;&#25972;&#20307;&#28151;&#38899;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#25805;&#20316;&#20026;&#21407;&#22987;&#20316;&#21697;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#37325;&#26032;&#35808;&#37322;&#12290;&#36825;&#20123;&#38899;&#20048;&#22788;&#29702;&#26041;&#27861;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#38656;&#35201;&#28145;&#21402;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#34429;&#28982;&#23545;&#22270;&#20687;&#21644;&#38899;&#39057;&#20462;&#25913;&#26377;&#25928;&#65292;&#20294;&#30452;&#25509;&#24212;&#29992;&#20110;&#38899;&#20048;&#26102;&#23384;&#22312;&#38382;&#39064;&#12290;&#36825;&#24402;&#22240;&#20110;&#38899;&#20048;&#30340;&#29420;&#29305;&#25968;&#25454;&#24615;&#36136;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#30772;&#22351;&#38899;&#20048;&#30340;&#20869;&#22312;&#21644;&#35856;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;InstructME&#65292;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#25351;&#23548;&#38899;&#20048;&#32534;&#36753;&#21644;&#28151;&#38899;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#32534;&#36753;&#20043;&#21069;&#21644;&#20043;&#21518;&#36890;&#36807;&#22810;&#23610;&#24230;&#32858;&#21512;&#24378;&#21270;&#20102;U-Net&#20197;&#20445;&#25345;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21644;&#24358;&#36827;&#23637;&#30697;&#38453;&#20316;&#20026;&#26465;&#20214;&#20449;&#24687;&#65292;&#24182;&#23558;&#20854;&#34701;&#20837;&#35821;&#20041;&#31354;&#38388;&#20197;&#25552;&#39640;&#32534;&#36753;&#26102;&#30340;&#26059;&#24459;&#21644;&#35856;&#24615;&#12290;&#20026;&#20102;&#36866;&#24212;&#22806;&#37096;&#38656;&#27714;&#65292;&#25105;&#20204;&#36824;&#21019;&#26032;&#22320;&#25552;&#20986;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#25913;&#36827;&#28151;&#38899;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Music editing primarily entails the modification of instrument tracks or remixing in the whole, which offers a novel reinterpretation of the original piece through a series of operations. These music processing methods hold immense potential across various applications but demand substantial expertise. Prior methodologies, although effective for image and audio modifications, falter when directly applied to music. This is attributed to music's distinctive data nature, where such methods can inadvertently compromise the intrinsic harmony and coherence of music. In this paper, we develop InstructME, an Instruction guided Music Editing and remixing framework based on latent diffusion models. Our framework fortifies the U-Net with multi-scale aggregation in order to maintain consistency before and after editing. In addition, we introduce chord progression matrix as condition information and incorporate it in the semantic space to improve melodic harmony while editing. For accommodating ext
&lt;/p&gt;</description></item><item><title>INFLECT-DGNN&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#12289;&#38024;&#23545;&#22270;&#25968;&#25454;&#36866;&#24212;&#30340;&#21512;&#25104;&#23569;&#25968;&#36807;&#37319;&#26679;&#25216;&#26415;&#21644;&#28378;&#21160;&#31383;&#21475;&#31574;&#30053;&#65292;&#29992;&#20110;&#24433;&#21709;&#32773;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;RNN&#26469;&#32534;&#30721;&#26102;&#38388;&#23646;&#24615;&#21644;GNN&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.08131</link><description>&lt;p&gt;
INFLECT-DGNN: &#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24433;&#21709;&#32773;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
INFLECT-DGNN: Influencer Prediction with Dynamic Graph Neural Networks. (arXiv:2307.08131v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08131
&lt;/p&gt;
&lt;p&gt;
INFLECT-DGNN&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#12289;&#38024;&#23545;&#22270;&#25968;&#25454;&#36866;&#24212;&#30340;&#21512;&#25104;&#23569;&#25968;&#36807;&#37319;&#26679;&#25216;&#26415;&#21644;&#28378;&#21160;&#31383;&#21475;&#31574;&#30053;&#65292;&#29992;&#20110;&#24433;&#21709;&#32773;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;RNN&#26469;&#32534;&#30721;&#26102;&#38388;&#23646;&#24615;&#21644;GNN&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#65292;&#21033;&#29992;&#32593;&#32476;&#20449;&#24687;&#36827;&#34892;&#39044;&#27979;&#24314;&#27169;&#24050;&#32463;&#21464;&#24471;&#38750;&#24120;&#26222;&#36941;&#12290;&#22312;&#25512;&#33616;&#21644;&#23450;&#21521;&#33829;&#38144;&#39046;&#22495;&#20013;&#65292;&#24433;&#21709;&#32773;&#26816;&#27979;&#26159;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#21160;&#24577;&#32593;&#32476;&#34920;&#31034;&#22823;&#22823;&#21463;&#30410;&#30340;&#39046;&#22495;&#65292;&#21407;&#22240;&#26159;&#19981;&#26029;&#21457;&#23637;&#30340;&#23458;&#25143;-&#21697;&#29260;&#20851;&#31995;&#12290;&#20026;&#20102;&#38416;&#36848;&#36825;&#19968;&#24605;&#24819;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;INFLECT-DGNN&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#65292;&#38024;&#23545;&#22270;&#25968;&#25454;&#36866;&#24212;&#30340;&#21512;&#25104;&#23569;&#25968;&#36807;&#37319;&#26679;&#25216;&#26415;&#65288;SMOTE&#65289;&#20197;&#21450;&#31934;&#24515;&#35774;&#35745;&#30340;&#28378;&#21160;&#31383;&#21475;&#31574;&#30053;&#30340;&#26032;&#26694;&#26550;&#12290;&#20026;&#20102;&#35780;&#20272;&#39044;&#27979;&#24615;&#33021;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#21253;&#21547;&#19977;&#20010;&#22478;&#24066;&#32593;&#32476;&#30340;&#29420;&#29305;&#20225;&#19994;&#25968;&#25454;&#38598;&#65292;&#24182;&#21046;&#23450;&#20102;&#19968;&#20010;&#20197;&#21033;&#28070;&#20026;&#39537;&#21160;&#30340;&#24433;&#21709;&#32773;&#39044;&#27979;&#35780;&#20272;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;RNN&#26469;&#32534;&#30721;&#26102;&#38388;&#23646;&#24615;&#20197;&#21450;GNN&#26174;&#33879;&#25913;&#21892;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging network information for predictive modeling has become widespread in many domains. Within the realm of referral and targeted marketing, influencer detection stands out as an area that could greatly benefit from the incorporation of dynamic network representation due to the ongoing development of customer-brand relationships. To elaborate this idea, we introduce INFLECT-DGNN, a new framework for INFLuencer prEdiCTion with Dynamic Graph Neural Networks that combines Graph Neural Networks (GNN) and Recurrent Neural Networks (RNN) with weighted loss functions, the Synthetic Minority Oversampling TEchnique (SMOTE) adapted for graph data, and a carefully crafted rolling-window strategy. To evaluate predictive performance, we utilize a unique corporate data set with networks of three cities and derive a profit-driven evaluation methodology for influencer prediction. Our results show how using RNN to encode temporal attributes alongside GNNs significantly improves predictive perform
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#23558;&#20154;&#24037;&#26234;&#33021;&#38598;&#25104;&#21040;&#26426;&#26800;&#33218;&#30340;&#20849;&#20139;&#25511;&#21046;&#33539;&#24335;&#20013;&#65292;&#20197;&#24110;&#21161;&#36816;&#21160;&#21463;&#25439;&#20154;&#22763;&#23454;&#29616;&#26356;&#39640;&#31243;&#24230;&#30340;&#20010;&#20154;&#33258;&#27835;&#12290;</title><link>http://arxiv.org/abs/2306.13509</link><description>&lt;p&gt;
&#25506;&#32034;AI&#22686;&#24378;&#30340;&#21327;&#20316;&#25511;&#21046;&#23545;&#20110;&#36741;&#21161;&#26426;&#26800;&#33218;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring AI-enhanced Shared Control for an Assistive Robotic Arm. (arXiv:2306.13509v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#23558;&#20154;&#24037;&#26234;&#33021;&#38598;&#25104;&#21040;&#26426;&#26800;&#33218;&#30340;&#20849;&#20139;&#25511;&#21046;&#33539;&#24335;&#20013;&#65292;&#20197;&#24110;&#21161;&#36816;&#21160;&#21463;&#25439;&#20154;&#22763;&#23454;&#29616;&#26356;&#39640;&#31243;&#24230;&#30340;&#20010;&#20154;&#33258;&#27835;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36741;&#21161;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#36741;&#21161;&#26426;&#26800;&#33218;&#24050;&#32463;&#25104;&#20026;&#24110;&#21161;&#36816;&#21160;&#21463;&#25439;&#20154;&#22763;&#23454;&#29616;&#33258;&#20027;&#29983;&#27963;&#30340;&#21487;&#33021;&#24615;&#12290;&#36817;&#24180;&#26469;&#65292;&#36234;&#26469;&#36234;&#22810;&#36825;&#26679;&#30340;&#31995;&#32479;&#24050;&#32463;&#38754;&#21521;&#26368;&#32456;&#29992;&#25143;&#25552;&#20379;&#65292;&#20363;&#22914;Kinova Jaco&#26426;&#26800;&#33218;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22823;&#22810;&#38656;&#35201;&#22797;&#26434;&#30340;&#25163;&#21160;&#25511;&#21046;&#65292;&#36825;&#21487;&#33021;&#20250;&#20351;&#29992;&#25143;&#19981;&#22570;&#37325;&#36127;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#35753;&#36825;&#20123;&#26426;&#22120;&#20154;&#33258;&#20027;&#34892;&#21160;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#33267;&#23569;&#23545;&#20110;&#36825;&#20010;&#29305;&#23450;&#30340;&#29992;&#25143;&#32676;&#20307;&#26469;&#35828;&#65292;&#36825;&#31181;&#26041;&#27861;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#24466;&#21171;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#29992;&#25143;&#24076;&#26395;&#20445;&#25345;&#25511;&#21046;&#26435;&#20197;&#23454;&#29616;&#26356;&#39640;&#31243;&#24230;&#30340;&#20010;&#20154;&#33258;&#27835;&#65292;&#20294;&#33258;&#20027;&#30340;&#26426;&#22120;&#20154;&#19982;&#27492;&#30456;&#21453;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#38598;&#25104;&#21040;&#20849;&#20139;&#25511;&#21046;&#33539;&#24335;&#20013;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#20851;&#27880;&#20102;&#20154;&#19982;&#26426;&#22120;&#20154;&#20043;&#38388;&#30028;&#38754;&#30340;&#24517;&#35201;&#35201;&#27714;&#65292;&#20197;&#21450;&#22914;&#20309;&#22312;&#26174;&#33879;&#20943;&#23569;&#24515;&#29702;&#36127;&#25285;&#21644;&#25152;&#38656;&#30340;&#26426;&#21160;&#33021;&#21147;&#30340;&#21516;&#26102;&#20445;&#25345;&#20154;&#31867;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Assistive technologies and in particular assistive robotic arms have the potential to enable people with motor impairments to live a self-determined life. More and more of these systems have become available for end users in recent years, such as the Kinova Jaco robotic arm. However, they mostly require complex manual control, which can overwhelm users. As a result, researchers have explored ways to let such robots act autonomously. However, at least for this specific group of users, such an approach has shown to be futile. Here, users want to stay in control to achieve a higher level of personal autonomy, to which an autonomous robot runs counter. In our research, we explore how Artifical Intelligence (AI) can be integrated into a shared control paradigm. In particular, we focus on the consequential requirements for the interface between human and robot and how we can keep humans in the loop while still significantly reducing the mental load and required motor skills.
&lt;/p&gt;</description></item><item><title>DIFFender&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20301;&#21644;&#24674;&#22797;&#20004;&#20010;&#38454;&#27573;&#30340;&#25805;&#20316;&#65292;&#21033;&#29992;&#25991;&#26412;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#38450;&#24481;&#23545;&#25239;&#24615;Patch&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#25972;&#20307;&#38450;&#24481;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.09124</link><description>&lt;p&gt;
DIFFender&#65306;&#22522;&#20110;&#25193;&#25955;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#29992;&#20110;&#25269;&#24481;Patch&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
DIFFender: Diffusion-Based Adversarial Defense against Patch Attacks. (arXiv:2306.09124v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09124
&lt;/p&gt;
&lt;p&gt;
DIFFender&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20301;&#21644;&#24674;&#22797;&#20004;&#20010;&#38454;&#27573;&#30340;&#25805;&#20316;&#65292;&#21033;&#29992;&#25991;&#26412;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#38450;&#24481;&#23545;&#25239;&#24615;Patch&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#25972;&#20307;&#38450;&#24481;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#23588;&#20854;&#26159;Patch&#25915;&#20987;&#65292;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#38752;&#24615;&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#12290;&#24320;&#21457;&#21487;&#38752;&#30340;&#38450;&#24481;&#26041;&#27861;&#20197;&#25269;&#24481;Patch&#25915;&#20987;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#24403;&#21069;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#36824;&#19981;&#20196;&#20154;&#28385;&#24847;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DIFFender&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#25991;&#26412;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#38450;&#24481;&#23545;&#25239;&#24615;Patch&#12290;DIFFender&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#38454;&#27573;&#65306;Patch&#23450;&#20301;&#21644;Patch&#24674;&#22797;&#12290;&#22312;&#23450;&#20301;&#38454;&#27573;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#21033;&#29992;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#19968;&#20010;&#26377;&#36259;&#29305;&#24615;&#65292;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#23545;&#25239;&#24615;Patch&#30340;&#20301;&#32622;&#12290;&#22312;&#24674;&#22797;&#38454;&#27573;&#65292;&#25105;&#20204;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#37325;&#24314;&#22270;&#20687;&#20013;&#30340;&#23545;&#25239;&#24615;&#21306;&#22495;&#21516;&#26102;&#20445;&#25345;&#35270;&#35273;&#20869;&#23481;&#30340;&#23436;&#25972;&#24615;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#20004;&#20010;&#38454;&#27573;&#37117;&#21463;&#21040;&#32479;&#19968;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#31934;&#24515;&#24341;&#23548;&#65292;&#22240;&#27492;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#23427;&#20204;&#20043;&#38388;&#30340;&#32039;&#23494;&#30456;&#20114;&#20316;&#29992;&#26469;&#25552;&#39640;&#25972;&#20010;&#38450;&#24481;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks, particularly patch attacks, pose significant threats to the robustness and reliability of deep learning models. Developing reliable defenses against patch attacks is crucial for real-world applications, yet current research in this area is not satisfactory. In this paper, we propose DIFFender, a novel defense method that leverages a text-guided diffusion model to defend against adversarial patches. DIFFender includes two main stages: patch localization and patch restoration. In the localization stage, we find and exploit an intriguing property of the diffusion model to effectively identify the locations of adversarial patches. In the restoration stage, we employ the diffusion model to reconstruct the adversarial regions in the images while preserving the integrity of the visual content. Importantly, these two stages are carefully guided by a unified diffusion model, thus we can utilize the close interaction between them to improve the whole defense performance. Mor
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Q-Ensembles&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#21152;Q&#32593;&#32476;&#30340;&#25968;&#37327;&#65292;&#26080;&#32541;&#22320;&#36830;&#25509;&#31163;&#32447;&#39044;&#35757;&#32451;&#21644;&#22312;&#32447;&#24494;&#35843;&#65292;&#21516;&#26102;&#19981;&#38477;&#20302;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36866;&#24403;&#25918;&#23485;Q&#20540;&#20272;&#35745;&#30340;&#24754;&#35266;&#24615;&#65292;&#24182;&#23558;&#22522;&#20110;&#38598;&#21512;&#30340;&#25506;&#32034;&#26426;&#21046;&#34701;&#20837;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06871</link><description>&lt;p&gt;
&#25552;&#21319;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;Q-Ensembles&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Offline-to-Online Reinforcement Learning with Q-Ensembles. (arXiv:2306.06871v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06871
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Q-Ensembles&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#21152;Q&#32593;&#32476;&#30340;&#25968;&#37327;&#65292;&#26080;&#32541;&#22320;&#36830;&#25509;&#31163;&#32447;&#39044;&#35757;&#32451;&#21644;&#22312;&#32447;&#24494;&#35843;&#65292;&#21516;&#26102;&#19981;&#38477;&#20302;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36866;&#24403;&#25918;&#23485;Q&#20540;&#20272;&#35745;&#30340;&#24754;&#35266;&#24615;&#65292;&#24182;&#23558;&#22522;&#20110;&#38598;&#21512;&#30340;&#25506;&#32034;&#26426;&#21046;&#34701;&#20837;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#23398;&#20064;&#33539;&#24335;&#65292;&#20195;&#29702;&#26681;&#25454;&#22266;&#23450;&#30340;&#32463;&#39564;&#25968;&#25454;&#38598;&#36827;&#34892;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#20165;&#20174;&#38745;&#24577;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#33021;&#38480;&#21046;&#20102;&#24615;&#33021;&#65292;&#22240;&#20026;&#32570;&#20047;&#25506;&#32034;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#23558;&#31163;&#32447;&#39044;&#35757;&#32451;&#19982;&#22312;&#32447;&#24494;&#35843;&#32467;&#21512;&#36215;&#26469;&#30340;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#35753;&#20195;&#29702;&#19982;&#29615;&#22659;&#23454;&#26102;&#20132;&#20114;&#65292;&#36827;&#19968;&#27493;&#23436;&#21892;&#20854;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23384;&#22312;&#24615;&#33021;&#19979;&#38477;&#21644;&#22312;&#32447;&#38454;&#27573;&#25913;&#36827;&#32531;&#24930;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Q-Ensembles&#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#22686;&#21152;Q&#32593;&#32476;&#30340;&#25968;&#37327;&#65292;&#26080;&#32541;&#22320;&#36830;&#25509;&#31163;&#32447;&#39044;&#35757;&#32451;&#21644;&#22312;&#32447;&#24494;&#35843;&#65292;&#21516;&#26102;&#19981;&#38477;&#20302;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#21152;&#24555;&#22312;&#32447;&#24615;&#33021;&#25552;&#21319;&#65292;&#25105;&#20204;&#36866;&#24403;&#25918;&#23485;Q&#20540;&#20272;&#35745;&#30340;&#24754;&#35266;&#24615;&#65292;&#24182;&#23558;&#22522;&#20110;&#38598;&#21512;&#30340;&#25506;&#32034;&#26426;&#21046;&#34701;&#20837;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) is a learning paradigm where an agent learns from a fixed dataset of experience. However, learning solely from a static dataset can limit the performance due to the lack of exploration. To overcome it, offline-to-online RL combines offline pre-training with online fine-tuning, which enables the agent to further refine its policy by interacting with the environment in real-time. Despite its benefits, existing offline-to-online RL methods suffer from performance degradation and slow improvement during the online phase. To tackle these challenges, we propose a novel framework called Ensemble-based Offline-to-Online (E2O) RL. By increasing the number of Q-networks, we seamlessly bridge offline pre-training and online fine-tuning without degrading performance. Moreover, to expedite online performance enhancement, we appropriately loosen the pessimism of Q-value estimation and incorporate ensemble-based exploration mechanisms into our framework. Experiment
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102; ChatGPT &#21644; GPT-4 &#20004;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#30340;&#36816;&#29992;&#21644;&#24615;&#33021;&#34920;&#29616;&#65292;&#36890;&#36807;&#20197;&#20154;&#26426;&#20132;&#20114;&#39046;&#22495;&#30340;&#30740;&#31350;&#25361;&#25112;&#20026;&#20363;&#65292;&#32467;&#35770;&#26159; ChatGPT &#21644; GPT-4 &#30340;&#32452;&#21512;&#26159;&#20998;&#26512;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#19968;&#31181;&#38750;&#24120;&#39640;&#25928;&#19988;&#33410;&#30465;&#25104;&#26412;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.05036</link><description>&lt;p&gt;
HCI&#25361;&#25112;&#30340;&#26144;&#23556;&#65306;ChatGPT&#21644;GPT-4&#22312;&#25104;&#26412;&#25928;&#30410;&#38382;&#31572;&#20013;&#30340;&#24212;&#29992;&#19982;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Mapping the Challenges of HCI: An Application and Evaluation of ChatGPT and GPT-4 for Cost-Efficient Question Answering. (arXiv:2306.05036v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102; ChatGPT &#21644; GPT-4 &#20004;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#30340;&#36816;&#29992;&#21644;&#24615;&#33021;&#34920;&#29616;&#65292;&#36890;&#36807;&#20197;&#20154;&#26426;&#20132;&#20114;&#39046;&#22495;&#30340;&#30740;&#31350;&#25361;&#25112;&#20026;&#20363;&#65292;&#32467;&#35770;&#26159; ChatGPT &#21644; GPT-4 &#30340;&#32452;&#21512;&#26159;&#20998;&#26512;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#19968;&#31181;&#38750;&#24120;&#39640;&#25928;&#19988;&#33410;&#30465;&#25104;&#26412;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#21644;GPT-4&#27491;&#22312;&#24191;&#27867;&#24212;&#29992;&#20110;&#23454;&#38469;&#24773;&#20917;&#12290;&#20294;&#26159;&#65292;&#36825;&#20004;&#31181;LLM&#26159;&#38381;&#28304;&#30340;&#65292;&#24182;&#19988;&#24456;&#23569;&#26377;&#20851;&#20110;&#23427;&#20204;&#22312;&#23454;&#38469;&#20351;&#29992;&#26696;&#20363;&#20013;&#30340;&#24615;&#33021;&#30340;&#20102;&#35299;&#12290;&#22312;&#23398;&#26415;&#30028;&#20013;&#65292;LLM&#30340;&#24615;&#33021;&#36890;&#24120;&#26159;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#27979;&#37327;&#30340;&#65292;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#21487;&#33021;&#24050;&#27844;&#28431;&#21040;ChatGPT&#21644;GPT-4&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;ChatGPT&#21644;GPT-4&#24212;&#29992;&#20110;&#25104;&#26412;&#25928;&#30410;&#38382;&#31572;&#30340;&#23454;&#38469;&#20219;&#21153;&#65292;&#20197;&#20174;2023&#24180;&#20154;&#26426;&#20132;&#20114;&#20250;&#35758;&#65288;CHI&#65289;&#30340;&#35770;&#25991;&#38598;&#20013;&#25552;&#21462;&#20154;&#26426;&#20132;&#20114;&#39046;&#22495;&#30740;&#31350;&#20154;&#21592;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#23545;LLM&#22312;&#36825;&#20010;&#23454;&#38469;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65292;ChatGPT&#21644;GPT-4&#30340;&#32452;&#21512;&#26159;&#20998;&#26512;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#26497;&#20339;&#25104;&#26412;&#25928;&#30410;&#25163;&#27573;&#12290;&#25104;&#26412;&#25928;&#29575;&#23545;&#20110;&#21407;&#22411;&#30740;&#31350;&#24819;&#27861;&#21644;&#20174;&#19981;&#21516;&#35282;&#24230;&#20998;&#26512;&#25991;&#26412;&#35821;&#26009;&#24211;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as ChatGPT and GPT-4, are gaining wide-spread real world use. Yet, the two LLMs are closed source, and little is known about the LLMs' performance in real-world use cases. In academia, LLM performance is often measured on benchmarks which may have leaked into ChatGPT's and GPT-4's training data. In this paper, we apply and evaluate ChatGPT and GPT-4 for the real-world task of cost-efficient extractive question answering over a text corpus that was published after the two LLMs completed training. More specifically, we extract research challenges for researchers in the field of HCI from the proceedings of the 2023 Conference on Human Factors in Computing Systems (CHI). We critically evaluate the LLMs on this practical task and conclude that the combination of ChatGPT and GPT-4 makes an excellent cost-efficient means for analyzing a text corpus at scale. Cost-efficiency is key for prototyping research ideas and analyzing text corpora from different persp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26089;&#26399;&#26435;&#37325;&#24179;&#22343;&#21270;&#26041;&#27861;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36136;&#37327;&#30340;&#26377;&#25928;&#24615;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#21152;&#36895;&#25910;&#25947;&#19988;&#27979;&#35797;&#21644;&#38646;&#26679;&#26412;&#27867;&#21270;&#25928;&#26524;&#26174;&#33879;&#65292;&#21516;&#26102;&#26377;&#25928;&#32531;&#35299;&#20102;&#35757;&#32451;&#20013;&#30340;&#25439;&#22833;&#27874;&#21160;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.03241</link><description>&lt;p&gt;
&#29702;&#35299;&#26089;&#26399;&#26435;&#37325;&#24179;&#22343;&#23545;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Understanding the Effectiveness of Early Weight Averaging for Training Large Language Models. (arXiv:2306.03241v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26089;&#26399;&#26435;&#37325;&#24179;&#22343;&#21270;&#26041;&#27861;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36136;&#37327;&#30340;&#26377;&#25928;&#24615;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#21152;&#36895;&#25910;&#25947;&#19988;&#27979;&#35797;&#21644;&#38646;&#26679;&#26412;&#27867;&#21270;&#25928;&#26524;&#26174;&#33879;&#65292;&#21516;&#26102;&#26377;&#25928;&#32531;&#35299;&#20102;&#35757;&#32451;&#20013;&#30340;&#25439;&#22833;&#27874;&#21160;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#20215;&#39640;&#26114;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#35757;&#32451;&#33267;&#25910;&#25947;&#24182;&#19981;&#39640;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#24819;&#27861;&#65292;&#21363;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#27839;&#30528;&#36712;&#36857;&#36827;&#34892;&#26816;&#26597;&#28857;&#24179;&#22343;&#21270;&#65292;&#20197;&#22312;&#27169;&#22411;&#25910;&#25947;&#20043;&#21069;&#25552;&#39640;&#20854;&#36136;&#37327;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#35757;&#32451;&#25110;&#25512;&#29702;&#26399;&#38388;&#19981;&#20250;&#20135;&#29983;&#39069;&#22806;&#30340;&#25104;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20855;&#26377;10&#20159;&#21040;120&#20159;&#21442;&#25968;&#30340;Pythia LLM&#30340;&#35757;&#32451;&#36712;&#36857;&#65292;&#24182;&#35777;&#26126;&#29305;&#21035;&#26159;&#22312;&#35757;&#32451;&#30340;&#26089;&#26399;&#21644;&#20013;&#26399;&#38454;&#27573;&#65292;&#36825;&#31181;&#24819;&#27861;&#21487;&#20197;&#21152;&#36895;&#25910;&#25947;&#24182;&#25552;&#39640;&#27979;&#35797;&#21644;&#38646;&#26679;&#26412;&#27867;&#21270;&#25928;&#26524;&#12290;&#25439;&#22833;&#27874;&#21160;&#26159;LLM&#35757;&#32451;&#20013;&#20247;&#25152;&#21608;&#30693;&#30340;&#38382;&#39064;&#65307;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#36935;&#21040;&#20102;&#20004;&#31181;&#22522;&#30784;&#36712;&#36857;&#30340;&#36825;&#31181;&#24773;&#20917;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#24179;&#22343;&#21270;&#21487;&#20197;&#32531;&#35299;&#36825;&#20004;&#31181;&#24773;&#20917;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#19968;&#20010;&#25317;&#26377;69&#20159;&#21442;&#25968;&#30340;LLM&#65292;&#25105;&#20204;&#30340;&#26089;&#26399;&#26435;&#37325;&#24179;&#22343;&#21270;&#37197;&#26041;&#21487;&#20197;&#33410;&#30465;&#39640;&#36798;4200&#23567;&#26102;&#30340;GPU&#26102;&#38388;&#65292;&#36825;&#23545;&#20113;&#35745;&#31639;&#25104;&#26412;&#26469;&#35828;&#26159;&#26174;&#33879;&#30340;&#33410;&#32422;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training LLMs is expensive, and recent evidence indicates training all the way to convergence is inefficient. In this paper, we investigate the ability of a simple idea, checkpoint averaging along the trajectory of a training run to improve the quality of models before they have converged. This approach incurs no extra cost during training or inference. Specifically, we analyze the training trajectories of Pythia LLMs with 1 to 12 billion parameters and demonstrate that, particularly during the early to mid stages of training, this idea accelerates convergence and improves both test and zero-shot generalization. Loss spikes are a well recognized problem in LLM training; in our analysis we encountered two instances of this in the underlying trajectories, and both instances were mitigated by our averaging.  For a 6.9B parameter LLM, for example, our early weight averaging recipe can save upto 4200 hours of GPU time, which corresponds to significant savings in cloud compute costs.
&lt;/p&gt;</description></item><item><title>SPRING&#26159;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#24320;&#25918;&#19990;&#30028;&#28216;&#25103;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23427;&#36890;&#36807;&#38405;&#35835;&#28216;&#25103;&#30340;&#21407;&#22987;&#23398;&#26415;&#35770;&#25991;&#24182;&#20351;&#29992;&#25152;&#23398;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#24182;&#29609;&#28216;&#25103;&#12290;&#12290;</title><link>http://arxiv.org/abs/2305.15486</link><description>&lt;p&gt;
SPRING: GPT-4&#36890;&#36807;&#23398;&#20064;&#35770;&#25991;&#21644;&#25512;&#29702;&#22312;&#28216;&#25103;&#20013;&#34920;&#29616;&#36229;&#36807;RL&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
SPRING: GPT-4 Out-performs RL Algorithms by Studying Papers and Reasoning. (arXiv:2305.15486v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15486
&lt;/p&gt;
&lt;p&gt;
SPRING&#26159;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#24320;&#25918;&#19990;&#30028;&#28216;&#25103;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23427;&#36890;&#36807;&#38405;&#35835;&#28216;&#25103;&#30340;&#21407;&#22987;&#23398;&#26415;&#35770;&#25991;&#24182;&#20351;&#29992;&#25152;&#23398;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#24182;&#29609;&#28216;&#25103;&#12290;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#28216;&#25103;&#30001;&#20110;&#20854;&#22810;&#20219;&#21153;&#12289;&#28145;&#24230;&#25506;&#32034;&#21644;&#30446;&#26631;&#20248;&#20808;&#32423;&#35201;&#27714;&#65292;&#23545;AI&#31639;&#27861;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#35299;&#20915;&#28216;&#25103;&#26041;&#38754;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#20854;&#39640;&#26679;&#26412;&#22797;&#26434;&#24615;&#38480;&#21046;&#20102;&#23427;&#22312;&#20687;Crafter&#25110;Minecraft&#36825;&#26679;&#22797;&#26434;&#30340;&#24320;&#25918;&#19990;&#30028;&#28216;&#25103;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;SPRING&#65292;&#36890;&#36807;&#38405;&#35835;&#28216;&#25103;&#30340;&#21407;&#22987;&#23398;&#26415;&#35770;&#25991;&#24182;&#20351;&#29992;&#25152;&#23398;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#24182;&#29609;&#28216;&#25103;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#32473;&#23450;LaTeX&#28304;&#20316;&#20026;&#28216;&#25103;&#35821;&#22659;&#21644;&#20195;&#29702;&#24403;&#21069;&#35266;&#23519;&#30340;&#25551;&#36848;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;SPRING&#26694;&#26550;&#21033;&#29992;&#20855;&#26377;&#28216;&#25103;&#30456;&#20851;&#38382;&#39064;&#30340;&#23450;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#20316;&#20026;&#33410;&#28857;&#21644;&#20381;&#36182;&#20851;&#31995;&#20316;&#20026;&#36793;&#12290;&#36890;&#36807;&#25353;&#25299;&#25169;&#39034;&#24207;&#36941;&#21382;DAG&#24182;&#35745;&#31639;&#27599;&#20010;&#33410;&#28857;&#30340;LLM&#21709;&#24212;&#26469;&#30830;&#23450;&#22312;&#29615;&#22659;&#20013;&#37319;&#21462;&#30340;&#26368;&#20248;&#34892;&#21160;&#65292;LLM&#23545;&#26368;&#32456;&#33410;&#28857;&#30340;&#31572;&#26696;&#30452;&#25509;&#36716;&#21270;&#20026;&#29615;&#22659;&#34892;&#21160;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;
&lt;/p&gt;
&lt;p&gt;
Open-world survival games pose significant challenges for AI algorithms due to their multi-tasking, deep exploration, and goal prioritization requirements. Despite reinforcement learning (RL) being popular for solving games, its high sample complexity limits its effectiveness in complex open-world games like Crafter or Minecraft. We propose a novel approach, SPRING, to read the game's original academic paper and use the knowledge learned to reason and play the game through a large language model (LLM). Prompted with the LaTeX source as game context and a description of the agent's current observation, our SPRING framework employs a directed acyclic graph (DAG) with game-related questions as nodes and dependencies as edges. We identify the optimal action to take in the environment by traversing the DAG and calculating LLM responses for each node in topological order, with the LLM's answer to final node directly translating to environment actions. In our experiments, we study the quality
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;WiFi&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#25552;&#21462;&#20154;&#20307;&#31227;&#21160;&#21644;&#31354;&#38388;&#29305;&#24449;&#30340;&#26080;&#35774;&#22791;&#22810;&#25151;&#38388;&#20154;&#20307;&#23384;&#22312;&#26816;&#27979;&#31995;&#32479;&#65292;&#33021;&#22815;&#36890;&#36807;&#26102;&#38388;-selective&#29305;&#24449;&#25552;&#21462;&#31639;&#27861;&#21306;&#20998;&#26377;&#30452;&#35273;&#35270;&#32447;&#36335;&#24452;&#38459;&#22622;&#21644;&#26080;&#35270;&#32447;&#36335;&#24452;&#38459;&#22622;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2304.13107</link><description>&lt;p&gt;
&#22522;&#20110;WiFi CSI&#30340;&#26080;&#35774;&#22791;&#22810;&#25151;&#38388;&#20154;&#20307;&#23384;&#22312;&#26816;&#27979;&#30340;&#26102;&#38388;&#36873;&#25321;&#24615;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Time-Selective RNN for Device-Free Multi-Room Human Presence Detection Using WiFi CSI. (arXiv:2304.13107v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13107
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;WiFi&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#25552;&#21462;&#20154;&#20307;&#31227;&#21160;&#21644;&#31354;&#38388;&#29305;&#24449;&#30340;&#26080;&#35774;&#22791;&#22810;&#25151;&#38388;&#20154;&#20307;&#23384;&#22312;&#26816;&#27979;&#31995;&#32479;&#65292;&#33021;&#22815;&#36890;&#36807;&#26102;&#38388;-selective&#29305;&#24449;&#25552;&#21462;&#31639;&#27861;&#21306;&#20998;&#26377;&#30452;&#35273;&#35270;&#32447;&#36335;&#24452;&#38459;&#22622;&#21644;&#26080;&#35270;&#32447;&#36335;&#24452;&#38459;&#22622;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#23384;&#22312;&#26816;&#27979;&#26159;&#21508;&#31181;&#24212;&#29992;&#30340;&#37325;&#35201;&#25216;&#26415;&#65292;&#21253;&#25324;&#23478;&#23621;&#33258;&#21160;&#21270;&#12289;&#23433;&#20840;&#21644;&#21307;&#30103;&#20445;&#20581;&#12290;&#34429;&#28982;&#20256;&#32479;&#19978;&#37319;&#29992;&#22522;&#20110;&#25668;&#20687;&#26426;&#30340;&#31995;&#32479;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#65292;&#20294;&#20250;&#24341;&#21457;&#38544;&#31169;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#21830;&#29992;WiFi&#25509;&#20837;&#28857;&#25552;&#20379;&#30340;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;(CSI)&#26041;&#27861;&#65292;&#25552;&#20379;&#35814;&#32454;&#30340;&#20449;&#36947;&#29305;&#24449;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26102;&#38388;&#36873;&#25321;&#24615;&#26465;&#20214;&#21452;&#29305;&#24449;&#25552;&#21462;&#36882;&#24402;&#32593;&#32476;(TCD-FERN)&#30340;&#26080;&#35774;&#22791;&#22810;&#25151;&#38388;&#20154;&#20307;&#23384;&#22312;&#26816;&#27979;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#26088;&#22312;&#20351;&#29992;&#21160;&#24577;&#21644;&#38745;&#24577;(DaS)&#25968;&#25454;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#22312;&#26465;&#20214;&#20154;&#20307;&#29305;&#24449;&#19979;&#25429;&#25417;&#37325;&#35201;&#30340;&#26102;&#38388;&#29305;&#24449;&#65292;&#25552;&#21462;&#20154;&#30340;&#31227;&#21160;&#21644;&#31354;&#38388;&#29305;&#24449;&#65292;&#24182;&#21306;&#20998;&#26377;&#30452;&#25509;&#35270;&#32447;&#36335;&#24452;&#38459;&#22622;&#21644;&#26080;&#35270;&#32447;&#36335;&#24452;&#38459;&#22622;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#20943;&#23569;&#25151;&#38388;&#38548;&#26029;&#36896;&#25104;&#30340;&#29305;&#24449;&#34928;&#20943;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110; LSTM &#30340; NCoV-DaS &#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human presence detection is a crucial technology for various applications, including home automation, security, and healthcare. While camera-based systems have traditionally been used for this purpose, they raise privacy concerns. To address this issue, recent research has explored the use of channel state information (CSI) approaches that can be extracted from commercial WiFi access points (APs) and provide detailed channel characteristics. In this thesis, we propose a device-free human presence detection system for multi-room scenarios using a time-selective conditional dual feature extract recurrent Network (TCD-FERN). Our system is designed to capture significant time features with the condition on current human features using a dynamic and static (DaS) data preprocessing technique to extract moving and spatial features of people and differentiate between line-of-sight (LoS) path blocking and non-blocking cases. To mitigate the feature attenuation problem caused by room partitions,
&lt;/p&gt;</description></item><item><title>CornerFormer&#26159;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#19981;&#21516;&#24314;&#27169;&#31574;&#30053;&#20110;&#21333;&#20010;&#27169;&#22411;&#20013;&#34701;&#21512;&#35282;&#28857;&#26816;&#27979;&#21644;&#36793;&#32536;&#39044;&#27979;&#26469;&#25552;&#21319;&#31934;&#32454;&#32467;&#26500;&#37325;&#24314;&#30340;&#34920;&#29616;&#65292;&#24182;&#22312;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.07072</link><description>&lt;p&gt;
CornerFormer: &#25552;&#21319;&#35282;&#28857;&#34920;&#24449;&#20197;&#36827;&#34892;&#31934;&#32454;&#32467;&#26500;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
CornerFormer: Boosting Corner Representation for Fine-Grained Structured Reconstruction. (arXiv:2304.07072v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07072
&lt;/p&gt;
&lt;p&gt;
CornerFormer&#26159;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#19981;&#21516;&#24314;&#27169;&#31574;&#30053;&#20110;&#21333;&#20010;&#27169;&#22411;&#20013;&#34701;&#21512;&#35282;&#28857;&#26816;&#27979;&#21644;&#36793;&#32536;&#39044;&#27979;&#26469;&#25552;&#21319;&#31934;&#32454;&#32467;&#26500;&#37325;&#24314;&#30340;&#34920;&#29616;&#65292;&#24182;&#22312;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#37325;&#24314;&#26159;&#19968;&#31181;&#38750;&#24179;&#20961;&#30340;&#23494;&#38598;&#39044;&#27979;&#38382;&#39064;&#65292;&#23427;&#20174;&#26629;&#26684;&#22270;&#20687;&#20013;&#25552;&#21462;&#32467;&#26500;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#24314;&#31569;&#35282;&#28857;&#21644;&#36793;&#32536;&#65289;&#65292;&#28982;&#21518;&#30456;&#24212;&#22320;&#37325;&#24314;&#20026;&#20108;&#32500;&#24179;&#38754;&#22270;&#12290;&#19982;&#24120;&#35265;&#30340;&#20998;&#21106;&#25110;&#26816;&#27979;&#38382;&#39064;&#30456;&#27604;&#65292;&#23427;&#26174;&#33879;&#20381;&#36182;&#20110;&#21033;&#29992;&#25972;&#20307;&#20960;&#20309;&#20449;&#24687;&#36827;&#34892;&#32467;&#26500;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#30446;&#21069;&#65292;&#22522;&#20110;transformer&#30340;&#26041;&#27861;&#37319;&#29992;&#20004;&#38454;&#27573;&#26041;&#24335;&#35299;&#20915;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22312;&#31532;&#19968;&#20010;&#27169;&#22411;&#20013;&#26816;&#27979;&#35282;&#28857;&#65292;&#24182;&#22312;&#31532;&#20108;&#20010;&#27169;&#22411;&#20013;&#20998;&#31867;&#25311;&#35758;&#36793;&#32536;&#65288;&#35282;&#23545;&#65289;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23558;&#20004;&#20010;&#38454;&#27573;&#20998;&#24320;&#25104;&#19981;&#21516;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#21482;&#20849;&#20139;&#20027;&#24178;&#32534;&#30721;&#22120;&#12290;&#19982;&#29616;&#26377;&#30340;&#24314;&#27169;&#31574;&#30053;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#24378;&#30340;&#35282;&#28857;&#34920;&#31034;&#26041;&#27861;&#65306;1&#65289;&#36890;&#36807;&#22312;&#19981;&#21516;&#30340;&#31890;&#24230;&#20013;&#20849;&#20139;&#29305;&#24449;&#65292;&#23427;&#22312;&#35282;&#28857;&#26816;&#27979;&#21644;&#36793;&#32536;&#39044;&#27979;&#20043;&#38388;&#34701;&#21512;&#30693;&#35782;&#65307;2&#65289;&#35282;&#28857;&#20505;&#36873;&#32773;&#26681;&#25454;&#20854;&#26041;&#21521;&#20316;&#20026;&#22235;&#20010;&#28909;&#22270;&#36890;&#36947;&#25552;&#20986;&#12290;&#23450;&#24615;&#21644;&#23450;&#37327;&#35780;&#20272;&#22343;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;CornerFormer&#26126;&#26174;&#20248;&#20110;&#20197;&#21069;&#30340;transformer-based&#27169;&#22411;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structured reconstruction is a non-trivial dense prediction problem, which extracts structural information (\eg, building corners and edges) from a raster image, then reconstructs it to a 2D planar graph accordingly. Compared with common segmentation or detection problems, it significantly relays on the capability that leveraging holistic geometric information for structural reasoning. Current transformer-based approaches tackle this challenging problem in a two-stage manner, which detect corners in the first model and classify the proposed edges (corner-pairs) in the second model. However, they separate two-stage into different models and only share the backbone encoder. Unlike the existing modeling strategies, we present an enhanced corner representation method: 1) It fuses knowledge between the corner detection and edge prediction by sharing feature in different granularity; 2) Corner candidates are proposed in four heatmap channels w.r.t its direction. Both qualitative and quantita
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#36125;&#21494;&#26031;&#23398;&#20064;&#26041;&#27861;&#23545;&#39564;&#35777;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#20808;&#39564;&#30693;&#35782;&#21644;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#22788;&#29702;&#65292;&#20197;&#35745;&#31639;&#20854;&#26399;&#26395;&#30340;&#20107;&#20214;&#21457;&#29983;&#29575;&#33539;&#22260;&#65292;&#36827;&#32780;&#36827;&#34892;&#24378;&#20581;&#39564;&#35777;&#12290;&#35813;&#26694;&#26550;&#25152;&#29992;&#26041;&#27861;&#21487;&#22788;&#29702;&#24120;&#35268;&#20107;&#20214;&#21644;&#32597;&#35265;&#24773;&#20917;&#65292;&#33021;&#22788;&#29702;&#30495;&#23454;&#31995;&#32479;&#20869;&#22312;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.08476</link><description>&lt;p&gt;
&#33258;&#20027;&#26426;&#22120;&#20154;&#24378;&#20581;&#39564;&#35777;&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bayesian Learning for the Robust Verification of Autonomous Robots. (arXiv:2303.08476v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#36125;&#21494;&#26031;&#23398;&#20064;&#26041;&#27861;&#23545;&#39564;&#35777;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#20808;&#39564;&#30693;&#35782;&#21644;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#22788;&#29702;&#65292;&#20197;&#35745;&#31639;&#20854;&#26399;&#26395;&#30340;&#20107;&#20214;&#21457;&#29983;&#29575;&#33539;&#22260;&#65292;&#36827;&#32780;&#36827;&#34892;&#24378;&#20581;&#39564;&#35777;&#12290;&#35813;&#26694;&#26550;&#25152;&#29992;&#26041;&#27861;&#21487;&#22788;&#29702;&#24120;&#35268;&#20107;&#20214;&#21644;&#32597;&#35265;&#24773;&#20917;&#65292;&#33021;&#22788;&#29702;&#30495;&#23454;&#31995;&#32479;&#20869;&#22312;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#36816;&#34892;&#39564;&#35777;&#33258;&#20027;&#26426;&#22120;&#20154;&#36827;&#34892;&#20851;&#38190;&#20219;&#21153;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#39564;&#35777;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#20808;&#39564;&#30693;&#35782;&#21644;&#35266;&#27979;&#25968;&#25454;&#65292;&#23398;&#20064;&#20854;&#20107;&#20214;&#21457;&#29983;&#29575;&#30340;&#39044;&#26399;&#20540;&#30340;&#33539;&#22260;&#12290;&#35813;&#26694;&#26550;&#25903;&#25345;&#22312;&#31995;&#32479;&#25805;&#20316;&#26399;&#38388;&#24120;&#35268;&#35266;&#23519;&#21040;&#30340;&#20107;&#20214;&#20197;&#21450;&#37027;&#20123;&#32597;&#35265;&#25110;&#26159;&#28798;&#38590;&#24615;&#25925;&#38556;&#31561;&#22238;&#25253;&#24040;&#22823;&#30340;&#20107;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#20107;&#20214;&#21457;&#29983;&#29575;&#33539;&#22260;&#32452;&#35013;&#21306;&#38388;&#36830;&#32493;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65292;&#24182;&#24212;&#29992;&#23450;&#37327;&#39564;&#35777;&#26041;&#27861;&#26469;&#35745;&#31639;&#20851;&#38190;&#31995;&#32479;&#23646;&#24615;&#30340;&#39044;&#26399;&#21464;&#21270;&#21306;&#38388;&#12290;&#36825;&#20123;&#21306;&#38388;&#21453;&#26144;&#20102;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#31995;&#32479;&#30340;&#20869;&#22312;&#19981;&#30830;&#23450;&#24615;&#65292;&#33021;&#22815;&#22312;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#19979;&#24378;&#20581;&#39564;&#35777;&#20854;&#23450;&#37327;&#23646;&#24615;&#12290;&#26412;&#25991;&#23558;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#27700;&#19979;&#33258;&#20027;&#26426;&#22120;&#20154;&#20219;&#21153;&#39564;&#35777;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a novel Bayesian learning framework that enables the runtime verification of autonomous robots performing critical missions in uncertain environments. Our framework exploits prior knowledge and observations of the verified robotic system to learn expected ranges of values for the occurrence rates of its events. We support both events observed regularly during system operation, and singular events such as catastrophic failures or the completion of difficult one-off tasks. Furthermore, we use the learnt event-rate ranges to assemble interval continuous-time Markov models, and we apply quantitative verification to these models to compute expected intervals of variation for key system properties. These intervals reflect the uncertainty intrinsic to many real-world systems, enabling the robust verification of their quantitative properties under parametric uncertainty. We apply the proposed framework to the case study of verification of an autonomous robotic mission for underwater
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#22836;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#25277;&#35937;&#20195;&#25968;&#21644;&#23450;&#24615;&#27010;&#24565;&#30340;&#30456;&#20284;&#24615;&#27010;&#24565;&#65292;&#24182;&#23558;&#20854;&#36890;&#36807;&#27169;&#22411;&#35770;&#31867;&#22411;&#33258;&#28982;&#22320;&#23884;&#20837;&#21040;&#19968;&#38454;&#36923;&#36753;&#20013;&#12290;</title><link>http://arxiv.org/abs/2302.10096</link><description>&lt;p&gt;
&#22522;&#20110;&#27867;&#21270;&#30340;&#30456;&#20284;&#24615;
&lt;/p&gt;
&lt;p&gt;
Generalization-based similarity. (arXiv:2302.10096v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#22836;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#25277;&#35937;&#20195;&#25968;&#21644;&#23450;&#24615;&#27010;&#24565;&#30340;&#30456;&#20284;&#24615;&#27010;&#24565;&#65292;&#24182;&#23558;&#20854;&#36890;&#36807;&#27169;&#22411;&#35770;&#31867;&#22411;&#33258;&#28982;&#22320;&#23884;&#20837;&#21040;&#19968;&#38454;&#36923;&#36753;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#21644;&#21033;&#29992;&#30475;&#20284;&#19981;&#30456;&#20851;&#30340;&#23545;&#35937;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26159;&#31867;&#27604;&#25512;&#29702;&#30340;&#26680;&#24515;&#65292;&#32780;&#31867;&#27604;&#25512;&#29702;&#21448;&#26159;&#20154;&#24037;&#26234;&#33021;&#30340;&#26680;&#24515;&#12290;&#26412;&#25991;&#20174;&#22836;&#24320;&#22987;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#25277;&#35937;&#20195;&#25968;&#21644;&#23450;&#24615;&#27010;&#24565;&#30340;&#30456;&#20284;&#24615;&#27010;&#24565;&#65292;&#22522;&#20110;&#35266;&#23519;&#21040;&#27867;&#21270;&#38598;&#21512;&#21487;&#20197;&#32534;&#30721;&#20803;&#32032;&#30340;&#37325;&#35201;&#23646;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20197;&#36825;&#31181;&#26041;&#24335;&#23450;&#20041;&#30340;&#30456;&#20284;&#24615;&#20855;&#26377;&#21560;&#24341;&#20154;&#30340;&#25968;&#23398;&#23646;&#24615;&#12290;&#36890;&#36807;&#20174;&#22522;&#26412;&#27010;&#24565;&#20986;&#21457;&#26500;&#24314;&#30456;&#20284;&#24615;&#27010;&#24565;&#65292;&#24182;&#23558;&#20854;&#33258;&#28982;&#22320;&#23884;&#20837;&#21040;&#27169;&#22411;&#35770;&#31867;&#22411;&#20013;&#30340;&#19968;&#38454;&#36923;&#36753;&#20013;&#65292;&#25105;&#20204;&#20351;&#35835;&#32773;&#30456;&#20449;&#20854;&#21512;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting and exploiting similarities between seemingly distant objects is at the core of analogical reasoning which itself is at the core of artificial intelligence. This paper develops {\em from the ground up} an abstract algebraic and {\em qualitative} notion of similarity based on the observation that sets of generalizations encode important properties of elements. We show that similarity defined in this way has appealing mathematical properties. As we construct our notion of similarity from first principles using only elementary concepts of universal algebra, to convince the reader of its plausibility, we show that it can be naturally embedded into first-order logic via model-theoretic types.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#38024;&#23545;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#30340;&#23454;&#26102;&#22312;&#32447;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#21487;&#20197;&#22312;&#25152;&#38656;&#35201;&#30340;&#20301;&#32622;&#21046;&#36896;&#19981;&#23384;&#22312;&#30340;&#29289;&#20307;&#65292;&#25915;&#20987;&#25104;&#21151;&#29575;&#32422;&#20026;90\%&#65292;&#25581;&#31034;&#20102;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#30340;&#24369;&#28857;&#21644;&#23433;&#20840;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.01962</link><description>&lt;p&gt;
&#23545;&#25239;&#26816;&#27979;: &#23454;&#26102;&#25915;&#20987;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Adversarial Detection: Attacking Object Detection in Real Time. (arXiv:2209.01962v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.01962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#38024;&#23545;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#30340;&#23454;&#26102;&#22312;&#32447;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#21487;&#20197;&#22312;&#25152;&#38656;&#35201;&#30340;&#20301;&#32622;&#21046;&#36896;&#19981;&#23384;&#22312;&#30340;&#29289;&#20307;&#65292;&#25915;&#20987;&#25104;&#21151;&#29575;&#32422;&#20026;90\%&#65292;&#25581;&#31034;&#20102;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#30340;&#24369;&#28857;&#21644;&#23433;&#20840;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#26426;&#22120;&#20154;&#20381;&#36182;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#26469;&#24863;&#30693;&#29615;&#22659;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#23433;&#20840;&#24615;&#30340;&#36827;&#27493;&#65292;&#25581;&#31034;&#20102;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#23041;&#32961;&#12290;&#28982;&#32780;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20391;&#37325;&#20110;&#25915;&#20987;&#38745;&#24577;&#22270;&#20687;&#25110;&#31163;&#32447;&#35270;&#39057;&#12290;&#22240;&#27492;&#65292;&#20173;&#19981;&#28165;&#26970;&#27492;&#31867;&#25915;&#20987;&#26159;&#21542;&#20250;&#21361;&#21450;&#21160;&#24577;&#29615;&#22659;&#19979;&#23454;&#38469;&#30340;&#26426;&#22120;&#20154;&#24212;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#38024;&#23545;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#30340;&#39318;&#27425;&#23454;&#26102;&#22312;&#32447;&#25915;&#20987;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#31181;&#25915;&#20987;&#26041;&#24335;&#65292;&#21487;&#20197;&#22312;&#25152;&#38656;&#20301;&#32622;&#29983;&#25104;&#19981;&#23384;&#22312;&#23545;&#35937;&#30340;&#36793;&#30028;&#26694;&#12290;&#36825;&#20123;&#25915;&#20987;&#22312;&#32422;20&#27425;&#36845;&#20195;&#20869;&#36798;&#21040;&#32422;90\%&#30340;&#25104;&#21151;&#29575;&#12290;&#28436;&#31034;&#35270;&#39057;&#21487;&#22312;https://youtu.be/zJZ1aNlXsMU&#19978;&#35266;&#30475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent robots rely on object detection models to perceive the environment. Following advances in deep learning security it has been revealed that object detection models are vulnerable to adversarial attacks. However, prior research primarily focuses on attacking static images or offline videos. Therefore, it is still unclear if such attacks could jeopardize real-world robotic applications in dynamic environments. This paper bridges this gap by presenting the first real-time online attack against object detection models. We devise three attacks that fabricate bounding boxes for nonexistent objects at desired locations. The attacks achieve a success rate of about 90\% within about 20 iterations. The demo video is available at https://youtu.be/zJZ1aNlXsMU.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32771;&#34385;&#26381;&#20174;&#24230;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#26469;&#25429;&#33719;&#25512;&#33616;&#31574;&#30053;&#21644;&#23454;&#38469;&#25191;&#34892;&#31574;&#30053;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#36890;&#36807;&#20998;&#26512;&#37096;&#20998;&#26381;&#20174;&#24230;&#23545;&#26368;&#20339;&#24314;&#35758;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24573;&#30053;&#37096;&#20998;&#26381;&#20174;&#24230;&#21487;&#33021;&#23548;&#33268;&#20219;&#24847;&#20005;&#37325;&#30340;&#24615;&#33021;&#24694;&#21270;&#12290;</title><link>http://arxiv.org/abs/2209.01874</link><description>&lt;p&gt;
&#26368;&#20339;&#20915;&#31574;&#24182;&#19981;&#26159;&#26368;&#20339;&#24314;&#35758;&#65306;&#21046;&#23450;&#32771;&#34385;&#26381;&#20174;&#24230;&#30340;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
The Best Decisions Are Not the Best Advice: Making Adherence-Aware Recommendations. (arXiv:2209.01874v3 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.01874
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32771;&#34385;&#26381;&#20174;&#24230;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#26469;&#25429;&#33719;&#25512;&#33616;&#31574;&#30053;&#21644;&#23454;&#38469;&#25191;&#34892;&#31574;&#30053;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#36890;&#36807;&#20998;&#26512;&#37096;&#20998;&#26381;&#20174;&#24230;&#23545;&#26368;&#20339;&#24314;&#35758;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24573;&#30053;&#37096;&#20998;&#26381;&#20174;&#24230;&#21487;&#33021;&#23548;&#33268;&#20219;&#24847;&#20005;&#37325;&#30340;&#24615;&#33021;&#24694;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#39640;&#39118;&#38505;&#20915;&#31574;&#36981;&#24490;&#19987;&#23478;&#21442;&#19982;&#30340;&#32467;&#26500;&#65292;&#21363;&#20154;&#31867;&#25805;&#20316;&#21592;&#20174;&#31639;&#27861;&#20013;&#25509;&#25910;&#24314;&#35758;&#20294;&#26368;&#32456;&#26159;&#20915;&#31574;&#21046;&#23450;&#32773;&#12290;&#22240;&#27492;&#65292;&#31639;&#27861;&#30340;&#24314;&#35758;&#21487;&#33021;&#19982;&#23454;&#36341;&#20013;&#23454;&#38469;&#25191;&#34892;&#30340;&#20915;&#31574;&#19981;&#21516;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#31639;&#27861;&#24314;&#35758;&#26159;&#36890;&#36807;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#33719;&#24471;&#30340;&#65292;&#35813;&#38382;&#39064;&#20551;&#23450;&#24314;&#35758;&#23558;&#23436;&#32654;&#25191;&#34892;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#26381;&#20174;&#24230;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#20197;&#25429;&#33719;&#25512;&#33616;&#31574;&#30053;&#21644;&#23454;&#38469;&#25191;&#34892;&#31574;&#30053;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#20998;&#26512;&#37096;&#20998;&#26381;&#20174;&#24230;&#23545;&#26368;&#20339;&#24314;&#35758;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24573;&#30053;&#37096;&#20998;&#26381;&#20174;&#24230;&#29616;&#35937;&#65292;&#19982;&#30446;&#21069;&#22823;&#22810;&#25968;&#25512;&#33616;&#24341;&#25806;&#25152;&#20570;&#30340;&#65292;&#21487;&#33021;&#23548;&#33268;&#19982;&#24403;&#21069;&#20154;&#31867;&#22522;&#32447;&#24615;&#33021;&#21644;&#25512;&#33616;&#31639;&#27861;&#39044;&#26399;&#24615;&#33021;&#30456;&#27604;&#65292;&#20986;&#29616;&#20219;&#24847;&#20005;&#37325;&#30340;&#24615;&#33021;&#24694;&#21270;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36824;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#24037;&#20855;&#26469;&#20998;&#26512;&#32467;&#26500;&#21644;&#35745;&#31639;&#26368;&#20339;&#25512;&#33616;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many high-stake decisions follow an expert-in-loop structure in that a human operator receives recommendations from an algorithm but is the ultimate decision maker. Hence, the algorithm's recommendation may differ from the actual decision implemented in practice. However, most algorithmic recommendations are obtained by solving an optimization problem that assumes recommendations will be perfectly implemented. We propose an adherence-aware optimization framework to capture the dichotomy between the recommended and the implemented policy and analyze the impact of partial adherence on the optimal recommendation. We show that overlooking the partial adherence phenomenon, as is currently being done by most recommendation engines, can lead to arbitrarily severe performance deterioration, compared with both the current human baseline performance and what is expected by the recommendation algorithm. Our framework also provides useful tools to analyze the structure and to compute optimal recom
&lt;/p&gt;</description></item></channel></rss>