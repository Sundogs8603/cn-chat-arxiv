<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;ReEvo&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36229;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#19968;&#31181;&#27714;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#24605;&#35774;&#35745;&#26041;&#27861;&#21644;&#24378;&#22823;&#30340;&#28436;&#21270;&#25628;&#32034;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22312;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#19978;&#30340;&#25628;&#32034;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01145</link><description>&lt;p&gt;
ReEvo&#65306;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20855;&#26377;&#21453;&#24605;&#28436;&#21270;&#30340;&#36229;&#21551;&#21457;&#24335;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
ReEvo: Large Language Models as Hyper-Heuristics with Reflective Evolution
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;ReEvo&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36229;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#19968;&#31181;&#27714;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#24605;&#35774;&#35745;&#26041;&#27861;&#21644;&#24378;&#22823;&#30340;&#28436;&#21270;&#25628;&#32034;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22312;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#19978;&#30340;&#25628;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NP&#22256;&#38590;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#26222;&#36941;&#23384;&#22312;&#25512;&#21160;&#39046;&#22495;&#19987;&#23478;&#21442;&#19982;&#35797;&#38169;&#24335;&#21551;&#21457;&#24335;&#35774;&#35745;&#36807;&#31243;&#12290;&#35774;&#35745;&#33258;&#21160;&#21270;&#30340;&#38271;&#26399;&#21162;&#21147;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23835;&#36215;&#32780;&#33719;&#24471;&#26032;&#30340;&#21160;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#35821;&#35328;&#36229;&#21551;&#21457;&#24335;&#31639;&#27861;&#65288;LHHs&#65289;&#65292;&#23427;&#26159;&#36229;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#19968;&#31181;&#26032;&#21464;&#20307;&#65292;&#21033;&#29992;LLM&#36827;&#34892;&#21551;&#21457;&#24335;&#29983;&#25104;&#65292;&#20855;&#26377;&#26368;&#23567;&#30340;&#20154;&#24037;&#24178;&#39044;&#21644;&#24320;&#25918;&#24335;&#30340;&#21551;&#21457;&#24335;&#31354;&#38388;&#12290;&#20026;&#20102;&#22686;&#24378;LHHs&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21453;&#24605;&#28436;&#21270;&#65288;ReEvo&#65289;&#65306;&#19968;&#31181;&#36890;&#29992;&#30340;&#25628;&#32034;&#26694;&#26550;&#65292;&#27169;&#25311;&#20102;&#20154;&#31867;&#19987;&#23478;&#30340;&#21453;&#24605;&#35774;&#35745;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21487;&#25193;&#23637;&#30340;LLM&#25512;&#29702;&#12289;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#39046;&#22495;&#30693;&#35782;&#21644;&#24378;&#22823;&#30340;&#36827;&#21270;&#25628;&#32034;&#25216;&#26415;&#36828;&#36828;&#36229;&#36234;&#20102;&#20154;&#31867;&#30340;&#33021;&#21147;&#12290;&#22312;12&#20010;&#32452;&#21512;&#20248;&#21270;&#35774;&#32622;&#30340;&#35780;&#20272;&#20013;&#26174;&#31034;&#65306;1)&#28436;&#21270;&#30340;&#21475;&#22836;&#21453;&#24605;&#23548;&#33268;&#26356;&#24179;&#28369;&#30340;&#36866;&#24212;&#24230;&#22320;&#24418;&#12289;&#40657;&#30418;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#35774;&#32622;&#30340;&#26126;&#30830;&#25512;&#29702;&#20197;&#21450;&#26356;&#22909;&#30340;&#25628;&#32034;&#32467;&#26524;&#65307;2)ReEvo&#29983;&#25104;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#22312;&#20998;&#38047;&#32423;&#20248;&#21270;&#26102;&#38388;&#20869;&#33719;&#24471;&#20102;&#21487;&#38752;&#21644;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The omnipresence of NP-hard combinatorial optimization problems (COPs) compels domain experts to engage in trial-and-error heuristic design process. The long-standing endeavor of design automation has gained new momentum with the rise of large language models (LLMs). This paper introduces Language Hyper-Heuristics (LHHs), an emerging variant of Hyper-Heuristics that leverages LLMs for heuristic generation, featuring minimal manual intervention and open-ended heuristic spaces. To empower LHHs, we present Reflective Evolution (ReEvo), a generic searching framework that emulates the reflective design approach of human experts while far surpassing human capabilities with its scalable LLM inference, Internet-scale domain knowledge, and powerful evolutionary search. Evaluations across 12 COP settings show that 1) verbal reflections for evolution lead to smoother fitness landscapes, explicit inference of black-box COP settings, and better search results; 2) heuristics generated by ReEvo in mi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DIRECT&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#30830;&#23450;&#31867;&#21035;&#20998;&#21106;&#38408;&#20540;&#24182;&#26631;&#35760;&#26368;&#19981;&#30830;&#23450;&#19988;&#31163;&#20854;&#26368;&#36817;&#30340;&#31034;&#20363;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#32597;&#35265;&#31867;&#21644;&#23569;&#25968;&#31867;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#25209;&#27425;&#26631;&#35760;&#21644;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#23481;&#24525;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2312.09196</link><description>&lt;p&gt;
DIRECT: &#22788;&#29702;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DIRECT: Deep Active Learning under Imbalance and Label Noise
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.09196
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DIRECT&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#30830;&#23450;&#31867;&#21035;&#20998;&#21106;&#38408;&#20540;&#24182;&#26631;&#35760;&#26368;&#19981;&#30830;&#23450;&#19988;&#31163;&#20854;&#26368;&#36817;&#30340;&#31034;&#20363;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#32597;&#35265;&#31867;&#21644;&#23569;&#25968;&#31867;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#25209;&#27425;&#26631;&#35760;&#21644;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#23481;&#24525;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#31867;&#21035;&#19981;&#24179;&#34913;&#26159;&#19968;&#20010;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#36890;&#24120;&#20250;&#23548;&#33268;&#32597;&#35265;&#21644;&#23569;&#25968;&#31867;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20027;&#21160;&#23398;&#20064;&#21487;&#33021;&#26159;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26368;&#26377;&#25928;&#25216;&#26415;&#65292;&#23427;&#20174;&#26681;&#26412;&#19978;&#37319;&#38598;&#26356;&#24179;&#34913;&#21644;&#20855;&#26377;&#20449;&#24687;&#37327;&#30340;&#26631;&#35760;&#31034;&#20363;&#36827;&#34892;&#27880;&#37322;&#12290;&#26631;&#31614;&#22122;&#22768;&#26159;&#25968;&#25454;&#27880;&#37322;&#20219;&#21153;&#20013;&#21478;&#19968;&#20010;&#24120;&#35265;&#38382;&#39064;&#65292;&#23545;&#20110;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#26469;&#35828;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#20027;&#21160;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#31283;&#20581;&#22320;&#30830;&#23450;&#31867;&#21035;&#20998;&#21106;&#38408;&#20540;&#24182;&#26631;&#35760;&#26368;&#19981;&#30830;&#23450;&#19988;&#31163;&#20854;&#26368;&#36817;&#30340;&#31034;&#20363;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#31616;&#21270;&#20026;&#19968;&#32500;&#20027;&#21160;&#23398;&#20064;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;DIRECT&#33021;&#22815;&#21033;&#29992;&#32463;&#20856;&#30340;&#20027;&#21160;&#23398;&#20064;&#25991;&#29486;&#26469;&#35299;&#20915;&#25209;&#27425;&#26631;&#35760;&#21644;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#23481;&#24525;&#31561;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#22823;&#37327;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class imbalance is a prevalent issue in real world machine learning applications, often leading to poor performance in rare and minority classes. With an abundance of wild unlabeled data, active learning is perhaps the most effective technique in solving the problem at its root -- collecting a more balanced and informative set of labeled examples during annotation. Label noise is another common issue in data annotation jobs, which is especially challenging for active learning methods. In this work, we conduct the first study of active learning under both class imbalance and label noise. We propose a novel algorithm that robustly identifies the class separation threshold and annotates the most uncertain examples that are closest from it. Through a novel reduction to one-dimensional active learning, our algorithm DIRECT is able to leverage the classic active learning literature to address issues such as batch labeling and tolerance towards label noise. We present extensive experiments on
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#21644;&#25945;&#23398;&#20013;&#25552;&#20379;&#20102;&#20998;&#24067;&#24335;&#26426;&#26500;&#65292;&#24182;&#21487;&#33021;&#20351;&#27785;&#28024;&#24335;&#25216;&#26415;&#26356;&#24378;&#22823;&#21644;&#22810;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.20216</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#21644;&#25945;&#23398;&#20013;&#30340;&#20998;&#24067;&#24335;&#26426;&#26500;
&lt;/p&gt;
&lt;p&gt;
Distributed agency in second language learning and teaching through generative AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20216
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#21644;&#25945;&#23398;&#20013;&#25552;&#20379;&#20102;&#20998;&#24067;&#24335;&#26426;&#26500;&#65292;&#24182;&#21487;&#33021;&#20351;&#27785;&#28024;&#24335;&#25216;&#26415;&#26356;&#24378;&#22823;&#21644;&#22810;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20216v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;&#25688;&#35201;: &#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#20026;&#35821;&#35328;&#23398;&#20064;&#25552;&#20379;&#20102;&#37325;&#22823;&#26426;&#20250;&#12290;&#20687;ChatGPT&#36825;&#26679;&#30340;&#24037;&#20855;&#21487;&#20197;&#36890;&#36807;&#20070;&#38754;&#25110;&#21475;&#22836;&#24418;&#24335;&#30340;&#23545;&#35805;&#20026;&#31532;&#20108;&#35821;&#35328;&#25552;&#20379;&#38750;&#27491;&#24335;&#32451;&#20064;&#65292;&#23398;&#20064;&#32773;&#36890;&#36807;&#25552;&#31034;&#25351;&#23450;&#23545;&#35805;&#21442;&#25968;&#65292;&#22914;&#29087;&#32451;&#31243;&#24230;&#12289;&#35821;&#35328;&#39118;&#26684;&#21644;&#35752;&#35770;&#20027;&#39064;&#12290;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#34987;&#25351;&#23548;&#32473;&#20104;&#32416;&#27491;&#24615;&#21453;&#39304;&#65292;&#21019;&#24314;&#32451;&#20064;&#39064;&#65292;&#25110;&#21046;&#23450;&#25193;&#23637;&#23398;&#20064;&#35745;&#21010;&#12290;&#25945;&#24072;&#21487;&#20197;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#26500;&#24314;&#21508;&#31181;&#23186;&#20307;&#30340;&#23398;&#20064;&#21644;&#35780;&#20272;&#26448;&#26009;&#12290;&#20154;&#24037;&#26234;&#33021;&#21487;&#33021;&#20250;&#20351;&#27785;&#28024;&#24335;&#25216;&#26415;&#26356;&#24378;&#22823;&#21644;&#22810;&#21151;&#33021;&#65292;&#25670;&#33073;&#33050;&#26412;&#21270;&#30340;&#20114;&#21160;&#12290;&#23545;&#20110;&#23398;&#20064;&#32773;&#21644;&#25945;&#24072;&#32780;&#35328;&#65292;&#37325;&#35201;&#30340;&#26159;&#35201;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#23616;&#38480;&#24615;&#65292;&#36825;&#20123;&#23616;&#38480;&#24615;&#26469;&#33258;&#20110;&#23427;&#20204;&#23545;&#20154;&#31867;&#35821;&#35328;&#30340;&#32431;&#32479;&#35745;&#27169;&#22411;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22788;&#29702;&#35821;&#35328;&#20351;&#29992;&#20013;&#24494;&#22937;&#31038;&#20250;&#21644;&#25991;&#21270;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21019;&#36896;&#26041;&#24335;&#23384;&#22312;&#20262;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20216v1 Announce Type: cross  Abstract: Generative AI offers significant opportunities for language learning. Tools like ChatGPT can provide informal second language practice through chats in written or voice forms, with the learner specifying through prompts conversational parameters such as proficiency level, language register, and discussion topics. AI can be instructed to give corrective feedback, create practice exercises, or develop an extended study plan. Instructors can use AI to build learning and assessment materials in a variety of media. AI is likely to make immersive technologies more powerful and versatile, moving away from scripted interactions. For both learners and teachers, it is important to understand the limitations of AI systems that arise from their purely statistical model of human language, which limits their ability to deal with nuanced social and cultural aspects of language use. Additionally, there are ethical concerns over how AI systems are crea
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#31070;&#32463;&#32593;&#32476;&#35268;&#33539;&#21270;&#26041;&#27861;CB-Norm&#65292;&#36890;&#36807;&#24341;&#20837;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26799;&#24230;&#31283;&#23450;&#24615;&#21644;&#23398;&#20064;&#21152;&#36895;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.16798</link><description>&lt;p&gt;
&#22522;&#20110;&#32858;&#31867;&#30340;&#31070;&#32463;&#32593;&#32476;&#35268;&#33539;&#21270;&#23618;
&lt;/p&gt;
&lt;p&gt;
Cluster-Based Normalization Layer for Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16798
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#31070;&#32463;&#32593;&#32476;&#35268;&#33539;&#21270;&#26041;&#27861;CB-Norm&#65292;&#36890;&#36807;&#24341;&#20837;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26799;&#24230;&#31283;&#23450;&#24615;&#21644;&#23398;&#20064;&#21152;&#36895;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#20013;&#38754;&#20020;&#37325;&#35201;&#25361;&#25112;&#65292;&#21253;&#25324;&#20869;&#37096;&#21327;&#21464;&#37327;&#28418;&#31227;&#12289;&#26631;&#31614;&#28418;&#31227;&#12289;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#12289;&#36807;&#25311;&#21512;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#20256;&#32479;&#30340;&#35268;&#33539;&#21270;&#26041;&#27861;&#65292;&#22914;&#25209;&#26631;&#20934;&#21270;&#65292;&#26088;&#22312;&#35299;&#20915;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#65292;&#20294;&#36890;&#24120;&#20381;&#36182;&#20110;&#38480;&#21046;&#20854;&#36866;&#24212;&#24615;&#30340;&#20551;&#35774;&#12290;&#28151;&#21512;&#35268;&#33539;&#21270;&#22312;&#22788;&#29702;&#22810;&#20010;&#39640;&#26031;&#20998;&#24067;&#26102;&#38754;&#20020;&#35745;&#31639;&#38556;&#30861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#32858;&#31867;&#30340;&#35268;&#33539;&#21270;&#65288;CB-Norm&#65289;&#30340;&#20004;&#20010;&#21464;&#20307;&#8212;&#8212;&#30417;&#30563;&#24335;&#22522;&#20110;&#32858;&#31867;&#30340;&#35268;&#33539;&#21270;&#65288;SCB-Norm&#65289;&#21644;&#26080;&#30417;&#30563;&#24335;&#22522;&#20110;&#32858;&#31867;&#30340;&#35268;&#33539;&#21270;&#65288;UCB-Norm&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#19968;&#27493;&#35268;&#33539;&#21270;&#26041;&#27861;&#12290;CB-Norm&#21033;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#26469;&#19987;&#38376;&#35299;&#20915;&#19982;&#26799;&#24230;&#31283;&#23450;&#24615;&#21644;&#23398;&#20064;&#21152;&#36895;&#26377;&#20851;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16798v1 Announce Type: cross  Abstract: Deep learning faces significant challenges during the training of neural networks, including internal covariate shift, label shift, vanishing/exploding gradients, overfitting, and computational complexity. While conventional normalization methods, such as Batch Normalization, aim to tackle some of these issues, they often depend on assumptions that constrain their adaptability. Mixture Normalization faces computational hurdles in its pursuit of handling multiple Gaussian distributions.   This paper introduces Cluster-Based Normalization (CB-Norm) in two variants - Supervised Cluster-Based Normalization (SCB-Norm) and Unsupervised Cluster-Based Normalization (UCB-Norm) - proposing a groundbreaking one-step normalization approach. CB-Norm leverages a Gaussian mixture model to specifically address challenges related to gradient stability and learning acceleration.   For SCB-Norm, a supervised variant, the novel mechanism involves introduc
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23545;25&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#21644;7&#31181;&#30456;&#23545;&#36739;&#39640;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#21450;&#20854;&#36328;&#35821;&#35328;&#21464;&#20307;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#20351;&#29992;LLMs&#36827;&#34892;ICL&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#26367;&#20195;&#26041;&#27861;&#26597;&#35810;&#23545;&#40784;&#65292;&#24182;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;ICL&#25552;&#20379;&#20102;&#23453;&#36149;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.16512</link><description>&lt;p&gt;
LLMs&#26159;&#23569;&#26679;&#26412;&#24773;&#22659;&#20302;&#36164;&#28304;&#35821;&#35328;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
LLMs Are Few-Shot In-Context Low-Resource Language Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16512
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;25&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#21644;7&#31181;&#30456;&#23545;&#36739;&#39640;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#21450;&#20854;&#36328;&#35821;&#35328;&#21464;&#20307;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#20351;&#29992;LLMs&#36827;&#34892;ICL&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#26367;&#20195;&#26041;&#27861;&#26597;&#35810;&#23545;&#40784;&#65292;&#24182;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;ICL&#25552;&#20379;&#20102;&#23453;&#36149;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#25903;&#25345;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#21033;&#29992;&#30701;&#26102;&#30340;&#24773;&#22659;&#20449;&#24687;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#65292;&#36825;&#20026;&#32553;&#23567;&#39640;&#36164;&#28304;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#30340;&#24046;&#36317;&#25552;&#20379;&#20102;&#37325;&#35201;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#25506;&#35752;&#20102;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;ICL&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#38598;&#20013;&#22312;&#30456;&#23545;&#39640;&#36164;&#28304;&#30340;&#35821;&#35328;&#65292;&#27604;&#22914;&#27861;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;25&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#21644;7&#31181;&#30456;&#23545;&#36739;&#39640;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;ICL&#21450;&#20854;&#36328;&#35821;&#35328;&#21464;&#20307;&#65288;X-ICL&#65289;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#19981;&#20165;&#35780;&#20272;&#20102;LLMs&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#20351;&#29992;ICL&#30340;&#26377;&#25928;&#24615;&#65292;&#36824;&#21457;&#29616;&#20102;&#24773;&#22659;&#26631;&#31614;&#23545;&#40784;&#30340;&#32570;&#38519;&#65292;&#24182;&#24341;&#20837;&#20102;&#26356;&#26377;&#25928;&#30340;&#26367;&#20195;&#26041;&#27861;&#65306;&#26597;&#35810;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;ICL&#30340;&#21508;&#20010;&#26041;&#38754;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24635;&#32467;&#20102;&#23569;&#26679;&#26412;&#24773;&#22659;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16512v1 Announce Type: cross  Abstract: In-context learning (ICL) empowers large language models (LLMs) to perform diverse tasks in underrepresented languages using only short in-context information, offering a crucial avenue for narrowing the gap between high-resource and low-resource languages. Nonetheless, there is only a handful of works explored ICL for low-resource languages with most of them focusing on relatively high-resource languages, such as French and Spanish. In this work, we extensively study ICL and its cross-lingual variation (X-ICL) on 25 low-resource and 7 relatively higher-resource languages. Our study not only assesses the effectiveness of ICL with LLMs in low-resource languages but also identifies the shortcomings of in-context label alignment, and introduces a more effective alternative: query alignment. Moreover, we provide valuable insights into various facets of ICL for low-resource languages. Our study concludes the significance of few-shot in-cont
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21435;&#27602;&#21270;&#65292;&#22312;&#26500;&#24314;&#20102;SafeEdit&#22522;&#20934;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861; DINM&#65292;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;&#27169;&#22411;&#30340;&#27602;&#24615;&#65292;&#21516;&#26102;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.14472</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#32534;&#36753;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21435;&#27602;&#21270;
&lt;/p&gt;
&lt;p&gt;
Detoxifying Large Language Models via Knowledge Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21435;&#27602;&#21270;&#65292;&#22312;&#26500;&#24314;&#20102;SafeEdit&#22522;&#20934;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861; DINM&#65292;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;&#27169;&#22411;&#30340;&#27602;&#24615;&#65292;&#21516;&#26102;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#26469;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#21435;&#27602;&#21270;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;SafeEdit&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20061;&#31181;&#19981;&#23433;&#20840;&#31867;&#21035;&#65292;&#20855;&#26377;&#21508;&#31181;&#24378;&#22823;&#30340;&#25915;&#20987;&#25552;&#31034;&#65292;&#24182;&#37197;&#22791;&#20102;&#20840;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#32447;&#65292;&#32467;&#26524;&#34920;&#26126;&#30693;&#35782;&#32534;&#36753;&#26377;&#28508;&#21147;&#22312;&#23545;LLMs&#36827;&#34892;&#21435;&#27602;&#21270;&#26102;&#65292;&#22312;&#23545;&#19968;&#33324;&#24615;&#33021;&#30340;&#24433;&#21709;&#30456;&#23545;&#26377;&#38480;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22522;&#20934;&#32447;&#65292;&#31216;&#20026;&#36890;&#36807;&#26415;&#20013;&#31070;&#32463;&#30417;&#27979;&#21435;&#27602;&#21270;&#65288;DINM&#65289;&#65292;&#36890;&#36807;&#20165;&#19968;&#27425;&#23454;&#20363;&#30340;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;LLMs&#30340;&#27602;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#34920;&#26126;&#20808;&#21069;&#30340;&#26041;&#27861;&#22914;SFT&#21644;DPO&#21487;&#33021;&#20165;&#25233;&#21046;&#26377;&#27602;&#21442;&#25968;&#30340;&#28608;&#27963;&#65292;&#32780;DINM&#21017;&#20943;&#36731;&#26377;&#27602;&#21442;&#25968;&#30340;&#27602;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14472v1 Announce Type: cross  Abstract: This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs). We construct a benchmark, SafeEdit, which covers nine unsafe categories with various powerful attack prompts and equips comprehensive metrics for systematic evaluation. We conduct experiments to compare knowledge editing approaches with previous baselines, indicating that knowledge editing has the potential to efficiently detoxify LLMs with limited impact on general performance. Then, we propose a simple yet effective baseline, dubbed Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the toxicity of LLMs within a few tuning steps via only one instance. We further provide an in-depth analysis of the internal mechanism for various detoxify approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;LLMs&#27169;&#25311;&#19981;&#21516;&#35282;&#33394;&#36827;&#34892;&#35752;&#35770;&#65292;&#20197;&#36798;&#25104;&#23545;&#20195;&#30721;&#20013;&#28431;&#27934;&#23384;&#22312;&#21644;&#20998;&#31867;&#30340;&#20849;&#35782;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21021;&#27493;&#35780;&#20272;&#20013;&#23454;&#29616;&#20102;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#20998;&#25968;&#30340;&#26126;&#26174;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.14274</link><description>&lt;p&gt;
&#36890;&#36807;LLMs&#35752;&#35770;&#23454;&#29616;&#28431;&#27934;&#26816;&#27979;&#30340;&#22810;&#35282;&#20849;&#35782;
&lt;/p&gt;
&lt;p&gt;
Multi-role Consensus through LLMs Discussions for Vulnerability Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;LLMs&#27169;&#25311;&#19981;&#21516;&#35282;&#33394;&#36827;&#34892;&#35752;&#35770;&#65292;&#20197;&#36798;&#25104;&#23545;&#20195;&#30721;&#20013;&#28431;&#27934;&#23384;&#22312;&#21644;&#20998;&#31867;&#30340;&#20849;&#35782;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21021;&#27493;&#35780;&#20272;&#20013;&#23454;&#29616;&#20102;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#20998;&#25968;&#30340;&#26126;&#26174;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#31361;&#26174;&#20102;&#28431;&#27934;&#26816;&#27979;&#30340;&#28508;&#21147;&#65292;&#36825;&#26159;&#36719;&#20214;&#36136;&#37327;&#20445;&#35777;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20165;&#38480;&#20110;&#21333;&#19968;&#35282;&#33394;&#30340;&#35270;&#35282;&#65292;&#36890;&#24120;&#26159;&#27979;&#35797;&#20154;&#21592;&#65292;&#32570;&#20047;&#20856;&#22411;&#36719;&#20214;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#20013;&#19981;&#21516;&#35282;&#33394;&#30340;&#22810;&#20803;&#35266;&#28857;&#65292;&#21253;&#25324;&#24320;&#21457;&#20154;&#21592;&#21644;&#27979;&#35797;&#20154;&#21592;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;LLMs&#25198;&#28436;&#19981;&#21516;&#35282;&#33394;&#30340;&#26041;&#27861;&#65292;&#27169;&#25311;&#29616;&#23454;&#20195;&#30721;&#23457;&#26597;&#36807;&#31243;&#65292;&#36827;&#34892;&#35752;&#35770;&#20197;&#36798;&#25104;&#20851;&#20110;&#20195;&#30721;&#20013;&#28431;&#27934;&#23384;&#22312;&#21644;&#20998;&#31867;&#30340;&#20849;&#35782;&#12290;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#21021;&#27493;&#35780;&#20272;&#26174;&#31034;&#65292;&#31934;&#30830;&#29575;&#22686;&#21152;&#20102;4.73&#65285;&#65292;&#21484;&#22238;&#29575;&#22686;&#21152;&#20102;58.9&#65285;&#65292;F1&#20998;&#25968;&#22686;&#21152;&#20102;28.1&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14274v1 Announce Type: cross  Abstract: Recent advancements in large language models (LLMs) have highlighted the potential for vulnerability detection, a crucial component of software quality assurance. Despite this progress, most studies have been limited to the perspective of a single role, usually testers, lacking diverse viewpoints from different roles in a typical software development life-cycle, including both developers and testers. To this end, this paper introduces an approach to employ LLMs to act as different roles to simulate real-life code review process, engaging in discussions towards a consensus on the existence and classification of vulnerabilities in the code. Preliminary evaluation of the proposed approach indicates a 4.73% increase in the precision rate, 58.9% increase in the recall rate, and a 28.1% increase in the F1 score.
&lt;/p&gt;</description></item><item><title>STG-Mamba &#26159;&#39318;&#20010;&#21033;&#29992;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;&#26102;&#31354;&#22270;&#23398;&#20064;&#30340;&#30740;&#31350;&#65292;&#23558;STG&#32593;&#32476;&#35270;&#20026;&#31995;&#32479;&#65292;&#24182;&#37319;&#29992;&#22270;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22359;&#65288;GS3B&#65289;&#31934;&#30830;&#34920;&#24449;STG&#30340;&#21160;&#24577;&#28436;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.12418</link><description>&lt;p&gt;
STG-Mamba: &#36890;&#36807;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;&#26102;&#31354;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
STG-Mamba: Spatial-Temporal Graph Learning via Selective State Space Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12418
&lt;/p&gt;
&lt;p&gt;
STG-Mamba &#26159;&#39318;&#20010;&#21033;&#29992;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;&#26102;&#31354;&#22270;&#23398;&#20064;&#30340;&#30740;&#31350;&#65292;&#23558;STG&#32593;&#32476;&#35270;&#20026;&#31995;&#32479;&#65292;&#24182;&#37319;&#29992;&#22270;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22359;&#65288;GS3B&#65289;&#31934;&#30830;&#34920;&#24449;STG&#30340;&#21160;&#24577;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Spatial-Temporal Graph&#65288;STG&#65289;&#25968;&#25454;&#20855;&#26377;&#21160;&#24577;&#24615;&#12289;&#24322;&#36136;&#24615;&#21644;&#38750;&#24179;&#31283;&#24615;&#29305;&#28857;&#65292;&#23548;&#33268;&#26102;&#31354;&#22270;&#23398;&#20064;&#25345;&#32493;&#38754;&#20020;&#25361;&#25112;&#12290;&#36817;&#24180;&#26469;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#22522;&#20110;GNN&#30340;&#26041;&#27861;&#65292;&#20027;&#35201;&#38598;&#20013;&#20110;&#27169;&#25311;STG&#32593;&#32476;&#20013;&#33410;&#28857;&#20010;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24573;&#30053;&#20102;&#38543;&#26102;&#38388;&#23384;&#22312;&#30340;STG&#31995;&#32479;&#26412;&#36136;&#29305;&#24449;&#30340;&#24314;&#27169;&#37325;&#35201;&#24615;&#12290;&#30456;&#21453;&#65292;&#29616;&#20195;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSSMs&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;STG&#32593;&#32476;&#35270;&#20026;&#31995;&#32479;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#31934;&#24515;&#25506;&#32034;&#20102;STG&#31995;&#32479;&#22312;&#26102;&#38388;&#32500;&#24230;&#19978;&#30340;&#21160;&#24577;&#29366;&#24577;&#28436;&#21464;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Spatial-Temporal Graph Mamba&#65288;STG-Mamba&#65289;&#65292;&#20316;&#20026;&#39318;&#20010;&#21033;&#29992;&#24378;&#22823;&#30340;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;STG&#23398;&#20064;&#30340;&#30740;&#31350;&#65292;&#23558;STG&#32593;&#32476;&#35270;&#20026;&#31995;&#32479;&#65292;&#24182;&#37319;&#29992;&#22270;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22359;&#65288;GS3B&#65289;&#31934;&#30830;&#34920;&#24449;STG&#30340;&#21160;&#24577;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12418v1 Announce Type: cross  Abstract: Spatial-Temporal Graph (STG) data is characterized as dynamic, heterogenous, and non-stationary, leading to the continuous challenge of spatial-temporal graph learning. In the past few years, various GNN-based methods have been proposed to solely focus on mimicking the relationships among node individuals of the STG network, ignoring the significance of modeling the intrinsic features that exist in STG system over time. In contrast, modern Selective State Space Models (SSSMs) present a new approach which treat STG Network as a system, and meticulously explore the STG system's dynamic state evolution across temporal dimension. In this work, we introduce Spatial-Temporal Graph Mamba (STG-Mamba) as the first exploration of leveraging the powerful selective state space models for STG learning by treating STG Network as a system, and employing the Graph Selective State Space Block (GS3B) to precisely characterize the dynamic evolution of ST
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31995;&#32479;&#32423;&#29366;&#24577;&#35780;&#20272;&#27169;&#22411;&#21644;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#27169;&#22411;&#30340;&#22810;&#36793;&#21327;&#20316;&#35745;&#31639;&#23454;&#26102;&#35843;&#24230;&#22120; CoRaiS&#65292;&#20197;&#20248;&#21270;&#20998;&#21457;&#20998;&#24067;&#21040;&#36798;&#30340;&#35831;&#27714;&#65292;&#25552;&#39640;&#22810;&#36793;&#31995;&#32479;&#30340;&#21327;&#20316;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.09671</link><description>&lt;p&gt;
CoRaiS: &#36731;&#37327;&#32423;&#22810;&#36793;&#21327;&#20316;&#35745;&#31639;&#23454;&#26102;&#35843;&#24230;&#22120;
&lt;/p&gt;
&lt;p&gt;
CoRaiS: Lightweight Real-Time Scheduler for Multi-Edge Cooperative Computing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09671
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31995;&#32479;&#32423;&#29366;&#24577;&#35780;&#20272;&#27169;&#22411;&#21644;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#27169;&#22411;&#30340;&#22810;&#36793;&#21327;&#20316;&#35745;&#31639;&#23454;&#26102;&#35843;&#24230;&#22120; CoRaiS&#65292;&#20197;&#20248;&#21270;&#20998;&#21457;&#20998;&#24067;&#21040;&#36798;&#30340;&#35831;&#27714;&#65292;&#25552;&#39640;&#22810;&#36793;&#31995;&#32479;&#30340;&#21327;&#20316;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#36793;&#21327;&#20316;&#35745;&#31639;&#23558;&#22810;&#20010;&#36793;&#32536;&#30340;&#21463;&#38480;&#36164;&#28304;&#21512;&#24182;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#36164;&#28304;&#27744;&#65292;&#20855;&#26377;&#25552;&#20379;&#24040;&#22823;&#35745;&#31639;&#33021;&#21147;&#12289;&#25913;&#36827;&#21709;&#24212;&#26102;&#38388;&#21644;&#26356;&#22810;&#26679;&#21270;&#26381;&#21153;&#31561;&#28508;&#21147;&#24102;&#26469;&#24040;&#22823;&#22909;&#22788;&#12290;&#28982;&#32780;&#65292;&#22823;&#37327;&#24322;&#26500;&#36164;&#28304;&#32452;&#25104;&#21644;&#32570;&#20047;&#35843;&#24230;&#31574;&#30053;&#20351;&#24471;&#22810;&#36793;&#35745;&#31639;&#31995;&#32479;&#30340;&#24314;&#27169;&#21644;&#21327;&#20316;&#21464;&#24471;&#29305;&#21035;&#22797;&#26434;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#32423;&#29366;&#24577;&#35780;&#20272;&#27169;&#22411;&#65292;&#29992;&#20110;&#20445;&#25252;&#22797;&#26434;&#30340;&#30828;&#20214;&#37197;&#32622;&#24182;&#37325;&#26032;&#23450;&#20041;&#24322;&#26500;&#36793;&#32536;&#30340;&#19981;&#21516;&#26381;&#21153;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#27169;&#22411;&#65292;&#20197;&#20415;&#26368;&#20248;&#22320;&#35843;&#24230;&#20998;&#24067;&#24335;&#21040;&#36798;&#30340;&#35831;&#27714;&#12290;&#26368;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23398;&#20064;&#30340;&#36731;&#37327;&#32423;&#23454;&#26102;&#35843;&#24230;&#22120; CoRaiS&#12290;CoRaiS&#23884;&#20837;&#20102;&#22810;&#36793;&#31995;&#32479;&#30340;&#23454;&#26102;&#29366;&#24577;&#21644;&#35831;&#27714;&#20449;&#24687;&#65292;&#24182;&#23558;&#36825;&#20123;&#23884;&#20837;&#19982;&#31574;&#30053;&#32593;&#32476;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09671v1 Announce Type: cross  Abstract: Multi-edge cooperative computing that combines constrained resources of multiple edges into a powerful resource pool has the potential to deliver great benefits, such as a tremendous computing power, improved response time, more diversified services. However, the mass heterogeneous resources composition and lack of scheduling strategies make the modeling and cooperating of multi-edge computing system particularly complicated. This paper first proposes a system-level state evaluation model to shield the complex hardware configurations and redefine the different service capabilities at heterogeneous edges. Secondly, an integer linear programming model is designed to cater for optimally dispatching the distributed arriving requests. Finally, a learning-based lightweight real-time scheduler, CoRaiS, is proposed. CoRaiS embeds the real-time states of multi-edge system and requests information, and combines the embeddings with a policy netwo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36171;&#20104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;Agent&#32676;&#20307;&#20869;&#31038;&#20250;&#35268;&#33539;&#20986;&#29616;&#30340;&#29983;&#25104;&#24335;Agent&#26550;&#26500;CRSEC&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.08251</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;Agent&#31038;&#20250;&#20013;&#31038;&#20250;&#35268;&#33539;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Emergence of Social Norms in Large Language Model-based Agent Societies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08251
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36171;&#20104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;Agent&#32676;&#20307;&#20869;&#31038;&#20250;&#35268;&#33539;&#20986;&#29616;&#30340;&#29983;&#25104;&#24335;Agent&#26550;&#26500;CRSEC&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#35268;&#33539;&#30340;&#20986;&#29616;&#21560;&#24341;&#20102;&#31038;&#20250;&#31185;&#23398;&#12289;&#35748;&#30693;&#31185;&#23398;&#20197;&#21450;&#20154;&#24037;&#26234;&#33021;&#31561;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36171;&#20104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;Agent&#32676;&#20307;&#20869;&#31038;&#20250;&#35268;&#33539;&#20986;&#29616;&#30340;&#29983;&#25104;&#24335;Agent&#26550;&#26500;CRSEC&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#21253;&#25324;&#22235;&#20010;&#27169;&#22359;&#65306;Creation &amp; Representation&#12289;Spreading&#12289;Evaluation&#21644;Compliance&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#22788;&#29702;&#20102;&#20960;&#20010;&#20851;&#38190;&#26041;&#38754;&#30340;&#32039;&#24613;&#36807;&#31243;&#65306;(i)&#31038;&#20250;&#35268;&#33539;&#30340;&#26469;&#28304;&#65292;(ii)&#23427;&#20204;&#22914;&#20309;&#34987;&#27491;&#24335;&#34920;&#31034;&#65292;(iii)&#23427;&#20204;&#22914;&#20309;&#36890;&#36807;Agent&#30340;&#20132;&#27969;&#21644;&#35266;&#23519;&#20256;&#25773;&#65292;(iv)&#22914;&#20309;&#36890;&#36807;&#21512;&#29702;&#26816;&#26597;&#36827;&#34892;&#26816;&#26597;&#24182;&#22312;&#38271;&#26399;&#20869;&#36827;&#34892;&#32508;&#21512;&#65292;(v)&#22914;&#20309;&#34987;&#32435;&#20837;Agent&#30340;&#35745;&#21010;&#21644;&#34892;&#21160;&#20013;&#12290;&#25105;&#20204;&#22312;Smallville&#27801;&#30418;&#28216;&#25103;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26550;&#26500;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08251v1 Announce Type: cross  Abstract: The emergence of social norms has attracted much interest in a wide array of disciplines, ranging from social science and cognitive science to artificial intelligence. In this paper, we propose the first generative agent architecture that empowers the emergence of social norms within a population of large language model-based agents. Our architecture, named CRSEC, consists of four modules: Creation &amp; Representation, Spreading, Evaluation, and Compliance. Our architecture addresses several important aspects of the emergent processes all in one: (i) where social norms come from, (ii) how they are formally represented, (iii) how they spread through agents' communications and observations, (iv) how they are examined with a sanity check and synthesized in the long term, and (v) how they are incorporated into agents' planning and actions. Our experiments deployed in the Smallville sandbox game environment demonstrate the capability of our ar
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#34701;&#21512;&#21512;&#35268;&#21644;&#30417;&#30563;&#30340;AI&#23457;&#35745;&#29983;&#24577;&#31995;&#32479;&#65292;&#24378;&#35843;&#20102;DSA&#21644;AIA&#30417;&#31649;&#26694;&#26550;&#20013;&#23384;&#22312;&#30340;&#30417;&#31649;&#31354;&#30333;&#65292;&#24182;&#35201;&#27714;AIA&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#31038;&#20250;&#20844;&#27665;&#25552;&#20379;&#25968;&#25454;&#21644;&#27169;&#22411;&#35775;&#38382;&#26435;&#38480;</title><link>https://arxiv.org/abs/2403.07904</link><description>&lt;p&gt;
&#27491;&#35270;&#30417;&#31649;&#31354;&#30333;&#65306;&#36890;&#36807;&#32435;&#20837;&#31038;&#20250;&#20844;&#27665;&#25171;&#36896;&#36229;&#36234;AIA&#30340;&#27431;&#30431;AI&#23457;&#35745;&#29983;&#24577;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Addressing the Regulatory Gap: Moving Towards an EU AI Audit Ecosystem Beyond the AIA by Including Civil Society
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07904
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#34701;&#21512;&#21512;&#35268;&#21644;&#30417;&#30563;&#30340;AI&#23457;&#35745;&#29983;&#24577;&#31995;&#32479;&#65292;&#24378;&#35843;&#20102;DSA&#21644;AIA&#30417;&#31649;&#26694;&#26550;&#20013;&#23384;&#22312;&#30340;&#30417;&#31649;&#31354;&#30333;&#65292;&#24182;&#35201;&#27714;AIA&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#31038;&#20250;&#20844;&#27665;&#25552;&#20379;&#25968;&#25454;&#21644;&#27169;&#22411;&#35775;&#38382;&#26435;&#38480;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27431;&#27954;&#31435;&#27861;&#26426;&#26500;&#25552;&#20986;&#20102;&#25968;&#23383;&#26381;&#21153;&#27861;&#26696;&#65288;DSA&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#27861;&#26696;&#65288;AIA&#65289;&#26469;&#35268;&#33539;&#24179;&#21488;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20135;&#21697;&#12290;&#26412;&#25991;&#23457;&#26597;&#20102;&#31532;&#19977;&#26041;&#23457;&#35745;&#22312;&#36825;&#20004;&#39033;&#27861;&#24459;&#20013;&#30340;&#22320;&#20301;&#20197;&#21450;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#25552;&#20379;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#35775;&#38382;&#26435;&#38480;&#12290;&#36890;&#36807;&#32771;&#34385;&#23457;&#35745;&#29983;&#24577;&#31995;&#32479;&#20013;&#31532;&#19977;&#26041;&#23457;&#35745;&#21644;&#31532;&#19977;&#26041;&#25968;&#25454;&#35775;&#38382;&#30340;&#20215;&#20540;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#30417;&#31649;&#31354;&#30333;&#65292;&#21363;&#12298;&#20154;&#24037;&#26234;&#33021;&#27861;&#26696;&#12299;&#27809;&#26377;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#31038;&#20250;&#20844;&#27665;&#25552;&#20379;&#25968;&#25454;&#35775;&#38382;&#26435;&#38480;&#12290;&#25105;&#20204;&#23545;&#25991;&#29486;&#30340;&#36129;&#29486;&#21253;&#25324;&#65306;&#65288;1&#65289;&#23450;&#20041;&#20102;&#19968;&#20010;&#34701;&#21512;&#21512;&#35268;&#21644;&#30417;&#30563;&#30340;AI&#23457;&#35745;&#29983;&#24577;&#31995;&#32479;&#12290;&#65288;2&#65289;&#24378;&#35843;&#20102;DSA&#21644;AIA&#30417;&#31649;&#26694;&#26550;&#20013;&#23384;&#22312;&#30340;&#30417;&#31649;&#31354;&#30333;&#65292;&#38459;&#30861;&#20102;AI&#23457;&#35745;&#29983;&#24577;&#31995;&#32479;&#30340;&#24314;&#31435;&#12290;&#65288;3&#65289;&#24378;&#35843;&#30740;&#31350;&#21644;&#31038;&#20250;&#20844;&#27665;&#30340;&#31532;&#19977;&#26041;&#23457;&#35745;&#24517;&#39035;&#25104;&#20026;&#35813;&#29983;&#24577;&#31995;&#32479;&#30340;&#19968;&#37096;&#20998;&#65292;&#24182;&#35201;&#27714;AIA&#21253;&#25324;&#25968;&#25454;&#21644;&#27169;&#22411;&#35775;&#38382;&#26435;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07904v1 Announce Type: cross  Abstract: The European legislature has proposed the Digital Services Act (DSA) and Artificial Intelligence Act (AIA) to regulate platforms and Artificial Intelligence (AI) products. We review to what extent third-party audits are part of both laws and to what extent access to models and data is provided. By considering the value of third-party audits and third-party data access in an audit ecosystem, we identify a regulatory gap in that the Artificial Intelligence Act does not provide access to data for researchers and civil society. Our contributions to the literature include: (1) Defining an AI audit ecosystem that incorporates compliance and oversight. (2) Highlighting a regulatory gap within the DSA and AIA regulatory framework, preventing the establishment of an AI audit ecosystem. (3) Emphasizing that third-party audits by research and civil society must be part of that ecosystem and demand that the AIA include data and model access for ce
&lt;/p&gt;</description></item><item><title>GNN&#30340;&#26680;&#24515;&#26550;&#26500;&#26377;&#20004;&#20010;&#29256;&#26412;&#65292;&#31532;&#19968;&#20010;&#29256;&#26412;&#28040;&#24687;&#20165;&#21462;&#20915;&#20110;&#28304;&#39030;&#28857;&#30340;&#29366;&#24577;&#65292;&#32780;&#31532;&#20108;&#20010;&#29256;&#26412;&#28040;&#24687;&#21462;&#20915;&#20110;&#28304;&#39030;&#28857;&#21644;&#30446;&#26631;&#39030;&#28857;&#30340;&#29366;&#24577;&#12290;</title><link>https://arxiv.org/abs/2403.06817</link><description>&lt;p&gt;
&#30446;&#26631;&#20449;&#24687;&#26356;&#26377;&#25928;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Targeted Messages More Effective?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06817
&lt;/p&gt;
&lt;p&gt;
GNN&#30340;&#26680;&#24515;&#26550;&#26500;&#26377;&#20004;&#20010;&#29256;&#26412;&#65292;&#31532;&#19968;&#20010;&#29256;&#26412;&#28040;&#24687;&#20165;&#21462;&#20915;&#20110;&#28304;&#39030;&#28857;&#30340;&#29366;&#24577;&#65292;&#32780;&#31532;&#20108;&#20010;&#29256;&#26412;&#28040;&#24687;&#21462;&#20915;&#20110;&#28304;&#39030;&#28857;&#21644;&#30446;&#26631;&#39030;&#28857;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26159;&#29992;&#20110;&#22270;&#24418;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;&#26412;&#36136;&#19978;&#65292;GNN&#26159;&#19968;&#20010;&#20998;&#24067;&#24335;&#30340;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#65292;&#20854;&#21463;&#21040;&#20174;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#21442;&#25968;&#30340;&#25511;&#21046;&#12290;&#23427;&#22312;&#22270;&#30340;&#39030;&#28857;&#19978;&#25805;&#20316;&#65306;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#39030;&#28857;&#22312;&#27599;&#20010;&#20256;&#20837;&#36793;&#19978;&#25509;&#25910;&#19968;&#26465;&#28040;&#24687;&#65292;&#32858;&#21512;&#36825;&#20123;&#28040;&#24687;&#65292;&#28982;&#21518;&#26681;&#25454;&#23427;&#20204;&#24403;&#21069;&#30340;&#29366;&#24577;&#21644;&#32858;&#21512;&#30340;&#28040;&#24687;&#26356;&#26032;&#23427;&#20204;&#30340;&#29366;&#24577;&#12290;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#21487;&#20197;&#29992;&#24102;&#35745;&#25968;&#30340;&#19968;&#38454;&#36923;&#36753;&#30340;&#26576;&#20123;&#29255;&#27573;&#21644;Weisfeiler-Lehman&#31639;&#27861;&#26469;&#25551;&#36848;&#12290;GNN&#30340;&#26680;&#24515;&#26550;&#26500;&#26377;&#20004;&#20010;&#19981;&#21516;&#30340;&#29256;&#26412;&#12290;&#22312;&#31532;&#19968;&#20010;&#29256;&#26412;&#20013;&#65292;&#28040;&#24687;&#20165;&#21462;&#20915;&#20110;&#28304;&#39030;&#28857;&#30340;&#29366;&#24577;&#65292;&#32780;&#22312;&#31532;&#20108;&#20010;&#29256;&#26412;&#20013;&#65292;&#28040;&#24687;&#21462;&#20915;&#20110;&#28304;&#39030;&#28857;&#21644;&#30446;&#26631;&#39030;&#28857;&#30340;&#29366;&#24577;&#12290;&#23454;&#38469;&#19978;&#65292;&#36825;&#20004;&#20010;&#29256;&#26412;&#37117;&#34987;&#20351;&#29992;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;GNN&#30340;&#29702;&#35770;&#22823;&#22810;&#38598;&#20013;&#22312;&#31532;&#19968;&#20010;&#29256;&#26412;&#19978;&#12290;&#22312;&#36923;&#36753;&#26041;&#38754;&#65292;&#36825;&#20004;&#20010;&#29256;&#26412;&#23545;&#24212;&#30528;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06817v1 Announce Type: cross  Abstract: Graph neural networks (GNN) are deep learning architectures for graphs. Essentially, a GNN is a distributed message passing algorithm, which is controlled by parameters learned from data. It operates on the vertices of a graph: in each iteration, vertices receive a message on each incoming edge, aggregate these messages, and then update their state based on their current state and the aggregated messages. The expressivity of GNNs can be characterised in terms of certain fragments of first-order logic with counting and the Weisfeiler-Lehman algorithm.   The core GNN architecture comes in two different versions. In the first version, a message only depends on the state of the source vertex, whereas in the second version it depends on the states of the source and target vertices. In practice, both of these versions are used, but the theory of GNNs so far mostly focused on the first one. On the logical side, the two versions correspond to 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26679;&#24335;&#28508;&#22312;&#27969;&#36827;&#34892;&#28145;&#24230;&#20266;&#36896;&#35270;&#39057;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#26679;&#24335;&#28508;&#22312;&#21521;&#37327;&#22312;&#29983;&#25104;&#35270;&#39057;&#20013;&#30340;&#24322;&#24120;&#34892;&#20026;&#26469;&#26816;&#27979;&#20551;&#35270;&#39057;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30340;StyleGRU&#27169;&#22359;&#21644;&#26679;&#24335;&#27880;&#24847;&#27169;&#22359;&#30340;&#32452;&#21512;&#65292;&#33021;&#26377;&#25928;&#26816;&#27979;&#35270;&#35273;&#21644;&#26102;&#38388;&#24322;&#24120;&#12290;</title><link>https://arxiv.org/abs/2403.06592</link><description>&lt;p&gt;
&#21033;&#29992;&#26679;&#24335;&#28508;&#22312;&#27969;&#26469;&#25512;&#24191;&#28145;&#24230;&#20266;&#36896;&#35270;&#39057;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Exploiting Style Latent Flows for Generalizing Deepfake Detection Video Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06592
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26679;&#24335;&#28508;&#22312;&#27969;&#36827;&#34892;&#28145;&#24230;&#20266;&#36896;&#35270;&#39057;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#26679;&#24335;&#28508;&#22312;&#21521;&#37327;&#22312;&#29983;&#25104;&#35270;&#39057;&#20013;&#30340;&#24322;&#24120;&#34892;&#20026;&#26469;&#26816;&#27979;&#20551;&#35270;&#39057;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30340;StyleGRU&#27169;&#22359;&#21644;&#26679;&#24335;&#27880;&#24847;&#27169;&#22359;&#30340;&#32452;&#21512;&#65292;&#33021;&#26377;&#25928;&#26816;&#27979;&#35270;&#35273;&#21644;&#26102;&#38388;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#26512;&#26679;&#24335;&#28508;&#22312;&#21521;&#37327;&#21450;&#20854;&#22312;&#29983;&#25104;&#35270;&#39057;&#30340;&#26102;&#38388;&#21464;&#21270;&#20013;&#24322;&#24120;&#34892;&#20026;&#30340;&#26816;&#27979;&#20551;&#35270;&#39057;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#29983;&#25104;&#30340;&#38754;&#37096;&#35270;&#39057;&#22312;&#26679;&#24335;&#28508;&#22312;&#21521;&#37327;&#30340;&#26102;&#38388;&#21464;&#21270;&#20013;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#29420;&#29305;&#24615;&#65292;&#36825;&#22312;&#29983;&#25104;&#20855;&#26377;&#21508;&#31181;&#38754;&#37096;&#34920;&#24773;&#21644;&#20960;&#20309;&#21464;&#25442;&#30340;&#26102;&#38388;&#31283;&#23450;&#35270;&#39057;&#26102;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#20102;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30340;StyleGRU&#27169;&#22359;&#26469;&#34920;&#31034;&#26679;&#24335;&#28508;&#22312;&#21521;&#37327;&#30340;&#21160;&#24577;&#29305;&#24615;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26679;&#24335;&#27880;&#24847;&#27169;&#22359;&#65292;&#23558;StyleGRU&#29983;&#25104;&#30340;&#29305;&#24449;&#19982;&#22522;&#20110;&#20869;&#23481;&#30340;&#29305;&#24449;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#23545;&#35270;&#35273;&#21644;&#26102;&#38388;&#24322;&#24120;&#30340;&#26816;&#27979;&#12290;&#25105;&#20204;&#22312;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#30340;&#21508;&#31181;&#22522;&#20934;&#24773;&#26223;&#19979;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#36328;&#25968;&#25454;&#38598;&#21644;&#36328;&#25805;&#20316;&#24773;&#26223;&#20013;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06592v1 Announce Type: cross  Abstract: This paper presents a new approach for the detection of fake videos, based on the analysis of style latent vectors and their abnormal behavior in temporal changes in the generated videos. We discovered that the generated facial videos suffer from the temporal distinctiveness in the temporal changes of style latent vectors, which are inevitable during the generation of temporally stable videos with various facial expressions and geometric transformations. Our framework utilizes the StyleGRU module, trained by contrastive learning, to represent the dynamic properties of style latent vectors. Additionally, we introduce a style attention module that integrates StyleGRU-generated features with content-based features, enabling the detection of visual and temporal artifacts. We demonstrate our approach across various benchmark scenarios in deepfake detection, showing its superiority in cross-dataset and cross-manipulation scenarios. Through f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#25991;&#26412;&#20013;&#21477;&#23376;&#32423;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#21106;&#30340;&#20004;&#27493;&#39588;&#27969;&#31243;&#26469;&#26816;&#27979;&#21508;&#27573;&#33853;&#30340;&#19968;&#33268;&#20316;&#32773;&#21477;&#23376;&#12290;</title><link>https://arxiv.org/abs/2403.03506</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#19982;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#25991;&#26412;&#20013;&#30340;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Detecting AI-Generated Text within Human-AI Collaborative Hybrid Texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#25991;&#26412;&#20013;&#21477;&#23376;&#32423;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#21106;&#30340;&#20004;&#27493;&#39588;&#27969;&#31243;&#26469;&#26816;&#27979;&#21508;&#27573;&#33853;&#30340;&#19968;&#33268;&#20316;&#32773;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#25991;&#26412;&#20013;&#21477;&#23376;&#32423;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#20851;&#20110;&#28151;&#21512;&#25991;&#26412;&#20013;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#30740;&#31350;&#36890;&#24120;&#20381;&#36182;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#36890;&#24120;&#28041;&#21450;&#24102;&#26377;&#26377;&#38480;&#36793;&#30028;&#30340;&#28151;&#21512;&#25991;&#26412;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#26816;&#27979;&#28151;&#21512;&#25991;&#26412;&#20013;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#30740;&#31350;&#24212;&#35206;&#30422;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#29983;&#25104;&#30340;&#19981;&#21516;&#31867;&#22411;&#28151;&#21512;&#25991;&#26412;&#65292;&#20197;&#26356;&#22909;&#22320;&#25351;&#23548;&#23454;&#38469;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21033;&#29992;&#20102;CoAuthor&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#36890;&#36807;&#20154;&#31867;&#20316;&#32773;&#21644;&#26234;&#33021;&#20889;&#20316;&#31995;&#32479;&#20043;&#38388;&#30340;&#21327;&#20316;&#29983;&#25104;&#30340;&#22810;&#36718;&#20132;&#20114;&#20013;&#20135;&#29983;&#30340;&#22810;&#26679;&#21270;&#12289;&#30495;&#23454;&#30340;&#28151;&#21512;&#25991;&#26412;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#27493;&#20998;&#21106;&#20026;&#22522;&#30784;&#30340;&#27969;&#31243;&#65306;(i)&#26816;&#27979;&#32473;&#23450;&#28151;&#21512;&#25991;&#26412;&#20013;&#30340;&#21508;&#20010;&#27573;&#33853;&#65292;&#20854;&#20013;&#27599;&#20010;&#27573;&#33853;&#21253;&#21547;&#19968;&#33268;&#20316;&#32773;&#30340;&#21477;&#23376;&#65292;&#20197;&#21450;(ii)&#20998;&#31867;&#27599;&#20010;&#30830;&#23450;&#27573;&#33853;&#30340;&#20316;&#32773;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03506v1 Announce Type: cross  Abstract: This study explores the challenge of sentence-level AI-generated text detection within human-AI collaborative hybrid texts. Existing studies of AI-generated text detection for hybrid texts often rely on synthetic datasets. These typically involve hybrid texts with a limited number of boundaries. We contend that studies of detecting AI-generated content within hybrid texts should cover different types of hybrid texts generated in realistic settings to better inform real-world applications. Therefore, our study utilizes the CoAuthor dataset, which includes diverse, realistic hybrid texts generated through the collaboration between human writers and an intelligent writing system in multi-turn interactions. We adopt a two-step, segmentation-based pipeline: (i) detect segments within a given hybrid text where each segment contains sentences of consistent authorship, and (ii) classify the authorship of each identified segment. Our empirical 
&lt;/p&gt;</description></item><item><title>&#37117;&#24066;GPT&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#20855;&#26377;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#30340;&#26102;&#31354;&#27169;&#22411;&#65292;&#20511;&#37492;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#23601;&#12290;</title><link>https://arxiv.org/abs/2403.00813</link><description>&lt;p&gt;
UrbanGPT: &#26102;&#31354;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UrbanGPT: Spatio-Temporal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00813
&lt;/p&gt;
&lt;p&gt;
&#37117;&#24066;GPT&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#20855;&#26377;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#30340;&#26102;&#31354;&#27169;&#22411;&#65292;&#20511;&#37492;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#23601;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37117;&#24066;GPT&#26088;&#22312;&#39044;&#27979;&#24182;&#27934;&#23519;&#22478;&#24066;&#29615;&#22659;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#19978;&#19981;&#26029;&#21464;&#21270;&#30340;&#21160;&#24577;&#12290;&#20854;&#30446;&#30340;&#26159;&#39044;&#27979;&#37117;&#24066;&#29983;&#27963;&#21508;&#20010;&#26041;&#38754;&#30340;&#26410;&#26469;&#27169;&#24335;&#12289;&#36235;&#21183;&#21644;&#20107;&#20214;&#65292;&#21253;&#25324;&#20132;&#36890;&#12289;&#20154;&#21475;&#27969;&#21160;&#21644;&#29359;&#32618;&#29575;&#31561;&#12290;&#23613;&#31649;&#24050;&#32463;&#20184;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#24320;&#21457;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#20197;&#20934;&#30830;&#39044;&#27979;&#26102;&#31354;&#25968;&#25454;&#65292;&#20294;&#38656;&#27880;&#24847;&#21040;&#24456;&#22810;&#26041;&#27861;&#22312;&#29983;&#25104;&#31934;&#30830;&#30340;&#26102;&#31354;&#34920;&#31034;&#26102;&#20005;&#37325;&#20381;&#36182;&#20110;&#26377;&#36275;&#22815;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22312;&#23454;&#38469;&#37117;&#24066;&#24863;&#30693;&#22330;&#26223;&#20013;&#65292;&#25968;&#25454;&#31232;&#32570;&#26159;&#19968;&#20010;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#24314;&#31435;&#19968;&#20010;&#20855;&#26377;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#30340;&#26102;&#31354;&#27169;&#22411;&#36328;&#36234;&#22810;&#26679;&#26102;&#31354;&#23398;&#20064;&#22330;&#26223;&#26159;&#24517;&#35201;&#30340;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#21331;&#36234;&#25104;&#23601;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00813v1 Announce Type: cross  Abstract: Spatio-temporal prediction aims to forecast and gain insights into the ever-changing dynamics of urban environments across both time and space. Its purpose is to anticipate future patterns, trends, and events in diverse facets of urban life, including transportation, population movement, and crime rates. Although numerous efforts have been dedicated to developing neural network techniques for accurate predictions on spatio-temporal data, it is important to note that many of these methods heavily depend on having sufficient labeled data to generate precise spatio-temporal representations. Unfortunately, the issue of data scarcity is pervasive in practical urban sensing scenarios. Consequently, it becomes necessary to build a spatio-temporal model with strong generalization capabilities across diverse spatio-temporal learning scenarios. Taking inspiration from the remarkable achievements of large language models (LLMs), our objective is 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RAGFormer&#30340;&#26032;&#26694;&#26550;&#65292;&#21516;&#26102;&#23558;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#23884;&#20837;&#30446;&#26631;&#33410;&#28857;&#65292;&#20197;&#25913;&#36827;&#27450;&#35784;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.17472</link><description>&lt;p&gt;
&#36890;&#36807;&#34701;&#21512;&#20840;&#23616;&#21644;&#23616;&#37096;&#20851;&#31995;&#20132;&#20114;&#36827;&#34892;&#27450;&#35784;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Fraud Detection with Binding Global and Local Relational Interaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17472
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RAGFormer&#30340;&#26032;&#26694;&#26550;&#65292;&#21516;&#26102;&#23558;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#23884;&#20837;&#30446;&#26631;&#33410;&#28857;&#65292;&#20197;&#25913;&#36827;&#27450;&#35784;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#27450;&#35784;&#26816;&#27979;&#20855;&#26377;&#26377;&#25928;&#24615;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#22312;&#25972;&#20307;&#35270;&#35282;&#20013;&#32534;&#30721;&#33410;&#28857;&#20132;&#20114;&#21644;&#32858;&#21512;&#29305;&#24449;&#12290;&#26368;&#36817;&#65292;&#20855;&#26377;&#20986;&#33394;&#24207;&#21015;&#32534;&#30721;&#33021;&#21147;&#30340;Transformer&#32593;&#32476;&#22312;&#25991;&#29486;&#20013;&#20063;&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;GNN&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;GNN&#21644;&#22522;&#20110;Transformer&#30340;&#32593;&#32476;&#21482;&#32534;&#30721;&#25972;&#20010;&#22270;&#30340;&#19968;&#20010;&#35270;&#35282;&#65292;&#32780;GNN&#32534;&#30721;&#20840;&#23616;&#29305;&#24449;&#65292;Transformer&#32593;&#32476;&#32534;&#30721;&#23616;&#37096;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#24573;&#35270;&#20102;&#20351;&#29992;&#21333;&#29420;&#32593;&#32476;&#32534;&#30721;&#24322;&#26500;&#22270;&#30340;&#20840;&#23616;&#20132;&#20114;&#29305;&#24449;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;Relation-Aware GNN with transFormer&#65288;RAGFormer&#65289;&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#23558;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#21516;&#26102;&#23884;&#20837;&#30446;&#26631;&#33410;&#28857;&#20013;&#12290;&#36825;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#32593;&#32476;&#24212;&#29992;&#20102;&#19968;&#20010;&#20462;&#25913;&#21518;&#30340;GAGA&#27169;&#22359;&#65292;&#20854;&#20013;&#27599;&#20010;Transformer&#23618;&#21518;&#38754;&#37117;&#36319;&#30528;&#19968;&#20010;&#36328;&#20851;&#31995;&#32858;&#21512;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17472v1 Announce Type: cross  Abstract: Graph Neural Network has been proved to be effective for fraud detection for its capability to encode node interaction and aggregate features in a holistic view. Recently, Transformer network with great sequence encoding ability, has also outperformed other GNN-based methods in literatures. However, both GNN-based and Transformer-based networks only encode one perspective of the whole graph, while GNN encodes global features and Transformer network encodes local ones. Furthermore, previous works ignored encoding global interaction features of the heterogeneous graph with separate networks, thus leading to suboptimal performance. In this work, we present a novel framework called Relation-Aware GNN with transFormer (RAGFormer) which simultaneously embeds local and global features into a target node. The simple yet effective network applies a modified GAGA module where each transformer layer is followed by a cross-relation aggregation lay
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;API-BLEND&#65292;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#31995;&#32479;&#27979;&#35797;&#24037;&#20855;&#22686;&#24378;&#22411;LLMs&#30340;&#22823;&#22411;&#35821;&#26009;&#24211;&#65292;&#26088;&#22312;&#35299;&#20915;&#33719;&#21462;&#28041;&#21450;&#35843;&#29992;&#24037;&#20855;/API&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#24182;&#27169;&#25311;&#30495;&#23454;&#22330;&#26223;&#30340;API&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.15491</link><description>&lt;p&gt;
API-BLEND&#65306;&#29992;&#20110;&#35757;&#32451;&#21644;&#22522;&#20934;&#27979;&#35797;API LLM&#30340;&#32508;&#21512;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
API-BLEND: A Comprehensive Corpora for Training and Benchmarking API LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;API-BLEND&#65292;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#31995;&#32479;&#27979;&#35797;&#24037;&#20855;&#22686;&#24378;&#22411;LLMs&#30340;&#22823;&#22411;&#35821;&#26009;&#24211;&#65292;&#26088;&#22312;&#35299;&#20915;&#33719;&#21462;&#28041;&#21450;&#35843;&#29992;&#24037;&#20855;/API&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#24182;&#27169;&#25311;&#30495;&#23454;&#22330;&#26223;&#30340;API&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26377;&#25928;&#20351;&#29992;&#24037;&#20855;&#21644;&#22806;&#37096;&#24212;&#29992;&#31243;&#24207;&#32534;&#31243;&#25509;&#21475;&#65288;APIs&#65289;&#26469;&#35268;&#21010;&#21644;&#23436;&#25104;&#20219;&#21153;&#30340;&#38656;&#27714;&#26085;&#30410;&#22686;&#38271;&#65292;&#23545;&#21487;&#20197;&#33719;&#21462;&#28041;&#21450;&#35843;&#29992;&#24037;&#20855;/API&#30340;&#36275;&#22815;&#25968;&#37327;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#26041;&#27861;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#26412;&#25991;&#20851;&#27880;&#35782;&#21035;&#12289;&#25972;&#29702;&#21644;&#36716;&#21270;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#24182;&#20171;&#32461;API-BLEND&#65292;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#31995;&#32479;&#27979;&#35797;&#24037;&#20855;&#22686;&#24378;&#22411;LLMs&#30340;&#22823;&#22411;&#35821;&#26009;&#24211;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#27169;&#25311;&#28041;&#21450;API&#20219;&#21153;&#30340;&#30495;&#23454;&#22330;&#26223;&#65292;&#22914;API/&#24037;&#20855;&#26816;&#27979;&#12289;&#27133;&#22635;&#20805;&#20197;&#21450;&#26816;&#27979;&#21040;&#30340;API&#30340;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15491v1 Announce Type: cross  Abstract: There is a growing need for Large Language Models (LLMs) to effectively use tools and external Application Programming Interfaces (APIs) to plan and complete tasks. As such, there is tremendous interest in methods that can acquire sufficient quantities of train and test data that involve calls to tools / APIs. Two lines of research have emerged as the predominant strategies for addressing this challenge. The first has focused on synthetic data generation techniques, while the second has involved curating task-adjacent datasets which can be transformed into API / Tool-based tasks. In this paper, we focus on the task of identifying, curating, and transforming existing datasets and, in turn, introduce API-BLEND, a large corpora for training and systematic testing of tool-augmented LLMs. The datasets mimic real-world scenarios involving API-tasks such as API / tool detection, slot filling, and sequencing of the detected APIs. We demonstrat
&lt;/p&gt;</description></item><item><title>E2USD&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#29366;&#24577;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#21452;&#35270;&#22270;&#23884;&#20837;&#27169;&#22359;&#36827;&#34892;&#32534;&#30721;&#65292;&#20197;&#21450;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#28040;&#38500;&#20551;&#38452;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;SOTA&#20934;&#30830;&#24615;&#24182;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2402.14041</link><description>&lt;p&gt;
E2USD&#65306;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#39640;&#25928;&#32780;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#29366;&#24577;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
E2USD: Efficient-yet-effective Unsupervised State Detection for Multivariate Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14041
&lt;/p&gt;
&lt;p&gt;
E2USD&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#29366;&#24577;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#21452;&#35270;&#22270;&#23884;&#20837;&#27169;&#22359;&#36827;&#34892;&#32534;&#30721;&#65292;&#20197;&#21450;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#28040;&#38500;&#20551;&#38452;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;SOTA&#20934;&#30830;&#24615;&#24182;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;E2USD&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#32780;&#20934;&#30830;&#30340;&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#29366;&#24577;&#26816;&#27979;&#12290;E2USD&#21033;&#29992;&#22522;&#20110;&#24555;&#36895;&#20613;&#31435;&#21494;&#21464;&#25442;&#30340;&#26102;&#38388;&#24207;&#21015;&#21387;&#32553;&#22120;(FFTCompress)&#21644;&#20998;&#35299;&#30340;&#21452;&#35270;&#22270;&#23884;&#20837;&#27169;&#22359;(DDEM)&#65292;&#19968;&#36215;&#20197;&#20302;&#35745;&#31639;&#24320;&#38144;&#23545;&#36755;&#20837;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#32534;&#30721;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20551;&#38452;&#24615;&#21462;&#28040;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;(FNCCLearning)&#65292;&#20197;&#25269;&#28040;&#20551;&#38452;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#23454;&#29616;&#26356;&#21451;&#22909;&#30340;&#31751;&#23884;&#20837;&#31354;&#38388;&#12290;&#20026;&#20102;&#22312;&#27969;&#24335;&#35774;&#32622;&#20013;&#36827;&#19968;&#27493;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#38408;&#20540;&#26816;&#27979;(ADATD)&#12290;&#36890;&#36807;&#20351;&#29992;&#20845;&#20010;&#22522;&#32447;&#27169;&#22411;&#21644;&#20845;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;E2USD&#33021;&#22815;&#22312;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;SOTA&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/AI4CTS/E2Usd &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14041v1 Announce Type: cross  Abstract: We propose E2USD that enables efficient-yet-accurate unsupervised MTS state detection. E2USD exploits a Fast Fourier Transform-based Time Series Compressor (FFTCompress) and a Decomposed Dual-view Embedding Module (DDEM) that together encode input MTSs at low computational overhead. Additionally, we propose a False Negative Cancellation Contrastive Learning method (FNCCLearning) to counteract the effects of false negatives and to achieve more cluster-friendly embedding spaces. To reduce computational overhead further in streaming settings, we introduce Adaptive Threshold Detection (ADATD). Comprehensive experiments with six baselines and six datasets offer evidence that E2USD is capable of SOTA accuracy at significantly reduced computational overhead. Our code is available at https://github.com/AI4CTS/E2Usd.
&lt;/p&gt;</description></item><item><title>AgentScope&#26159;&#19968;&#20010;&#24320;&#21457;&#32773;&#20013;&#24515;&#30340;&#22810;&#20195;&#29702;&#24179;&#21488;&#65292;&#25552;&#20379;&#20102;&#20197;&#28040;&#24687;&#20132;&#25442;&#20026;&#26680;&#24515;&#36890;&#20449;&#26426;&#21046;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#24320;&#21457;&#21644;&#29702;&#35299;&#30340;&#38556;&#30861;&#65292;&#21516;&#26102;&#20855;&#22791;&#28789;&#27963;&#30340;&#23481;&#38169;&#26426;&#21046;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#22788;&#29702;&#30340;&#31995;&#32479;&#32423;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2402.14034</link><description>&lt;p&gt;
AgentScope: &#19968;&#20010;&#28789;&#27963;&#32780;&#21448;&#24378;&#22823;&#30340;&#22810;&#20195;&#29702;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
AgentScope: A Flexible yet Robust Multi-Agent Platform
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14034
&lt;/p&gt;
&lt;p&gt;
AgentScope&#26159;&#19968;&#20010;&#24320;&#21457;&#32773;&#20013;&#24515;&#30340;&#22810;&#20195;&#29702;&#24179;&#21488;&#65292;&#25552;&#20379;&#20102;&#20197;&#28040;&#24687;&#20132;&#25442;&#20026;&#26680;&#24515;&#36890;&#20449;&#26426;&#21046;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#24320;&#21457;&#21644;&#29702;&#35299;&#30340;&#38556;&#30861;&#65292;&#21516;&#26102;&#20855;&#22791;&#28789;&#27963;&#30340;&#23481;&#38169;&#26426;&#21046;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#22788;&#29702;&#30340;&#31995;&#32479;&#32423;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#22810;&#20195;&#29702;&#24212;&#29992;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#21327;&#35843;&#20195;&#29702;&#21512;&#20316;&#21644;LLMs&#30340;&#19981;&#31283;&#23450;&#24615;&#34920;&#29616;&#26041;&#38754;&#30340;&#22797;&#26434;&#24615;&#65292;&#32473;&#24320;&#21457;&#20581;&#22766;&#39640;&#25928;&#30340;&#22810;&#20195;&#29702;&#24212;&#29992;&#24102;&#26469;&#20102;&#26174;&#33879;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AgentScope&#65292;&#19968;&#20010;&#20197;&#28040;&#24687;&#20132;&#25442;&#20026;&#26680;&#24515;&#36890;&#20449;&#26426;&#21046;&#30340;&#38754;&#21521;&#24320;&#21457;&#32773;&#30340;&#22810;&#20195;&#29702;&#24179;&#21488;&#12290;&#25105;&#20204;&#30340;&#36890;&#20449;&#26426;&#21046;&#36830;&#21516;&#20016;&#23500;&#30340;&#21477;&#27861;&#24037;&#20855;&#12289;&#20869;&#32622;&#36164;&#28304;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;&#20132;&#20114;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#24320;&#21457;&#21644;&#29702;&#35299;&#30340;&#38556;&#30861;&#12290;&#20026;&#20102;&#23454;&#29616;&#20581;&#22766;&#21644;&#28789;&#27963;&#30340;&#22810;&#20195;&#29702;&#24212;&#29992;&#65292;AgentScope&#25552;&#20379;&#20102;&#20869;&#32622;&#21644;&#21487;&#23450;&#21046;&#30340;&#23481;&#38169;&#26426;&#21046;&#65292;&#21516;&#26102;&#36824;&#37197;&#22791;&#29992;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#29983;&#25104;&#12289;&#23384;&#20648;&#21644;&#20256;&#36755;&#30340;&#31995;&#32479;&#32423;&#25903;&#25345;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;actor&#30340;&#20998;&#21457;&#26694;&#26550;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14034v1 Announce Type: cross  Abstract: With the rapid advancement of Large Language Models (LLMs), significant progress has been made in multi-agent applications. However, the complexities in coordinating agents' cooperation and LLMs' erratic performance pose notable challenges in developing robust and efficient multi-agent applications. To tackle these challenges, we propose AgentScope, a developer-centric multi-agent platform with message exchange as its core communication mechanism. Together with abundant syntactic tools, built-in resources, and user-friendly interactions, our communication mechanism significantly reduces the barriers to both development and understanding. Towards robust and flexible multi-agent application, AgentScope provides both built-in and customizable fault tolerance mechanisms while it is also armed with system-level supports for multi-modal data generation, storage and transmission. Additionally, we design an actor-based distribution framework, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Smart Parallel Auto-Correct Decoding (SPACE)&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#21322;&#33258;&#22238;&#24402;&#25512;&#29702;&#21644;&#29468;&#27979;&#35299;&#30721;&#65292;&#23454;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#21152;&#36895;&#21644;&#24182;&#34892;&#29983;&#25104;&#39564;&#35777;&#20196;&#29260;&#30340;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11809</link><description>&lt;p&gt;
&#26234;&#33021;&#24182;&#34892;&#33258;&#21160;&#32416;&#38169;&#35299;&#30721;&#65306;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11809
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Smart Parallel Auto-Correct Decoding (SPACE)&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#21322;&#33258;&#22238;&#24402;&#25512;&#29702;&#21644;&#29468;&#27979;&#35299;&#30721;&#65292;&#23454;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#21152;&#36895;&#21644;&#24182;&#34892;&#29983;&#25104;&#39564;&#35777;&#20196;&#29260;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#21152;&#36895;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#26234;&#33021;&#24182;&#34892;&#33258;&#21160;&#32416;&#38169;&#35299;&#30721;&#8221;&#65288;SPACE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;LLMs&#30340;&#26080;&#25439;&#21152;&#36895;&#12290;&#36890;&#36807;&#38598;&#25104;&#21322;&#33258;&#22238;&#24402;&#25512;&#29702;&#21644;&#29468;&#27979;&#35299;&#30721;&#33021;&#21147;&#65292;SPACE&#29420;&#29305;&#22320;&#20351;&#33258;&#22238;&#24402;LLMs&#33021;&#22815;&#24182;&#34892;&#29983;&#25104;&#21644;&#39564;&#35777;&#20196;&#29260;&#12290;&#36825;&#26159;&#36890;&#36807;&#19987;&#38376;&#30340;&#21322;&#33258;&#22238;&#24402;&#30417;&#30563;&#24494;&#35843;&#36807;&#31243;&#23454;&#29616;&#30340;&#65292;&#35813;&#36807;&#31243;&#20351;&#29616;&#26377;LLMs&#20855;&#26377;&#21516;&#26102;&#39044;&#27979;&#22810;&#20010;&#20196;&#29260;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#19968;&#31181;&#33258;&#21160;&#32416;&#38169;&#35299;&#30721;&#31639;&#27861;&#20419;&#36827;&#20102;&#21333;&#20010;&#27169;&#22411;&#35843;&#29992;&#20869;&#20196;&#29260;&#24207;&#21015;&#30340;&#21516;&#26102;&#29983;&#25104;&#21644;&#39564;&#35777;&#12290;&#36890;&#36807;&#22312;&#19968;&#31995;&#21015;LLMs&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#65292;SPACE&#22312;HumanEval-X&#19978;&#34920;&#29616;&#20986;2.7&#20493;&#33267;4.0&#20493;&#30340;&#25512;&#29702;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11809v1 Announce Type: cross  Abstract: This research aims to accelerate the inference speed of large language models (LLMs) with billions of parameters. We propose \textbf{S}mart \textbf{P}arallel \textbf{A}uto-\textbf{C}orrect d\textbf{E}coding (SPACE), an innovative approach designed for achieving lossless acceleration of LLMs. By integrating semi-autoregressive inference and speculative decoding capabilities, SPACE uniquely enables autoregressive LLMs to parallelize token generation and verification. This is realized through a specialized semi-autoregressive supervised fine-tuning process that equips existing LLMs with the ability to simultaneously predict multiple tokens. Additionally, an auto-correct decoding algorithm facilitates the simultaneous generation and verification of token sequences within a single model invocation. Through extensive experiments on a range of LLMs, SPACE has demonstrated inference speedup ranging from 2.7x-4.0x on HumanEval-X while maintaini
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#33258;&#22238;&#24402;&#30340;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#25490;&#24207;&#25512;&#33616;&#65292;&#22312;&#22810;&#38454;&#27573;&#25512;&#33616;&#31995;&#32479;&#20013;&#25198;&#28436;&#20851;&#38190;&#35282;&#33394;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#25552;&#39640;&#25928;&#29575;&#21644;&#25928;&#26524;&#65292;&#24182;&#35299;&#20915;&#31232;&#30095;&#35757;&#32451;&#26679;&#26412;&#21644;&#21160;&#24577;&#20505;&#36873;&#39033;&#23545;&#27169;&#22411;&#25910;&#25947;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.06871</link><description>&lt;p&gt;
&#38750;&#33258;&#22238;&#24402;&#30340;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#25490;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Non-autoregressive Generative Models for Reranking Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#33258;&#22238;&#24402;&#30340;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#25490;&#24207;&#25512;&#33616;&#65292;&#22312;&#22810;&#38454;&#27573;&#25512;&#33616;&#31995;&#32479;&#20013;&#25198;&#28436;&#20851;&#38190;&#35282;&#33394;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#25552;&#39640;&#25928;&#29575;&#21644;&#25928;&#26524;&#65292;&#24182;&#35299;&#20915;&#31232;&#30095;&#35757;&#32451;&#26679;&#26412;&#21644;&#21160;&#24577;&#20505;&#36873;&#39033;&#23545;&#27169;&#22411;&#25910;&#25947;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#38454;&#27573;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#37325;&#26032;&#25490;&#24207;&#36890;&#36807;&#24314;&#27169;&#39033;&#30446;&#20043;&#38388;&#30340;&#20869;&#37096;&#30456;&#20851;&#24615;&#36215;&#21040;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#37325;&#26032;&#25490;&#24207;&#30340;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#22312;&#25490;&#21015;&#30340;&#32452;&#21512;&#31354;&#38388;&#20013;&#25506;&#32034;&#26368;&#20339;&#24207;&#21015;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#29983;&#25104;&#22120;-&#35780;&#20272;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#29983;&#25104;&#22120;&#29983;&#25104;&#22810;&#20010;&#21487;&#34892;&#24207;&#21015;&#65292;&#35780;&#20272;&#22120;&#22522;&#20110;&#20272;&#35745;&#30340;&#21015;&#34920;&#24471;&#20998;&#36873;&#25321;&#26368;&#20339;&#24207;&#21015;&#12290;&#29983;&#25104;&#22120;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#29983;&#25104;&#27169;&#22411;&#38750;&#24120;&#36866;&#21512;&#29983;&#25104;&#22120;&#20989;&#25968;&#12290;&#24403;&#21069;&#30340;&#29983;&#25104;&#27169;&#22411;&#37319;&#29992;&#33258;&#22238;&#24402;&#31574;&#30053;&#36827;&#34892;&#24207;&#21015;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#26102;&#24037;&#19994;&#31995;&#32479;&#20013;&#37096;&#32626;&#33258;&#22238;&#24402;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#25490;&#24207;&#25512;&#33616;&#65288;NAR4Rec&#65289;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#19982;&#31232;&#30095;&#35757;&#32451;&#26679;&#26412;&#21644;&#21160;&#24577;&#20505;&#36873;&#39033;&#23545;&#27169;&#22411;&#25910;&#25947;&#24615;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;m
&lt;/p&gt;
&lt;p&gt;
In a multi-stage recommendation system, reranking plays a crucial role by modeling the intra-list correlations among items.The key challenge of reranking lies in the exploration of optimal sequences within the combinatorial space of permutations. Recent research proposes a generator-evaluator learning paradigm, where the generator generates multiple feasible sequences and the evaluator picks out the best sequence based on the estimated listwise score. Generator is of vital importance, and generative models are well-suited for the generator function. Current generative models employ an autoregressive strategy for sequence generation. However, deploying autoregressive models in real-time industrial systems is challenging. Hence, we propose a Non-AutoRegressive generative model for reranking Recommendation (NAR4Rec) designed to enhance efficiency and effectiveness. To address challenges related to sparse training samples and dynamic candidates impacting model convergence, we introduce a m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21152;&#26435;&#23376;&#22270;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(WCE-GNN)&#23454;&#29616;&#20102;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;WCE-GNN&#20855;&#26377;&#20248;&#31168;&#30340;&#39044;&#27979;&#25928;&#26524;&#21644;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.05569</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Hypergraph Node Classification With Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21152;&#26435;&#23376;&#22270;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(WCE-GNN)&#23454;&#29616;&#20102;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;WCE-GNN&#20855;&#26377;&#20248;&#31168;&#30340;&#39044;&#27979;&#25928;&#26524;&#21644;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#26159;&#29992;&#26469;&#27169;&#25311;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#20851;&#38190;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#25104;&#21151;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#20855;&#26377;&#25104;&#23545;&#20132;&#20114;&#30340;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#36825;&#28608;&#21457;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#20855;&#26377;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#25968;&#25454;&#30340;&#24819;&#27861;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HyperGNNs&#65289;&#30340;&#21457;&#23637;&#12290;GNNs&#21644;HyperGNNs&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#19981;&#21516;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#34987;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#19981;&#21516;&#20960;&#20309;&#25299;&#25169;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#22312;&#33410;&#28857;&#20998;&#31867;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#22823;&#22810;&#25968;HyperGNNs&#21487;&#20197;&#20351;&#29992;&#24102;&#26377;&#36229;&#22270;&#30340;&#21152;&#26435;&#23376;&#22270;&#25193;&#23637;&#30340;GNN&#26469;&#36817;&#20284;&#12290;&#36825;&#23548;&#33268;&#20102;WCE-GNN&#65292;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#19968;&#20010;GNN&#21644;&#19968;&#20010;&#21152;&#26435;&#23376;&#22270;&#25193;&#23637;&#65288;WCE&#65289;&#65292;&#29992;&#20110;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#12290;&#23545;&#20110;&#20061;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;WCE-GNN&#19981;&#20165;&#20855;&#26377;&#20248;&#31168;&#30340;&#39044;&#27979;&#25928;&#26524;&#65292;&#32780;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypergraphs, with hyperedges connecting more than two nodes, are key for modelling higher-order interactions in real-world data. The success of graph neural networks (GNNs) reveals the capability of neural networks to process data with pairwise interactions. This inspires the usage of neural networks for data with higher-order interactions, thereby leading to the development of hypergraph neural networks (HyperGNNs). GNNs and HyperGNNs are typically considered distinct since they are designed for data on different geometric topologies. However, in this paper, we theoretically demonstrate that, in the context of node classification, most HyperGNNs can be approximated using a GNN with a weighted clique expansion of the hypergraph. This leads to WCE-GNN, a simple and efficient framework comprising a GNN and a weighted clique expansion (WCE), for hypergraph node classification. Experiments on nine real-world hypergraph node classification benchmarks showcase that WCE-GNN demonstrates not o
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RSCNet&#30340;&#21160;&#24577;CSI&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#21387;&#32553;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#26469;&#20943;&#23569;&#29289;&#32852;&#32593;&#35774;&#22791;&#21521;&#20113;&#26381;&#21153;&#22120;&#20256;&#36755;CSI&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;RSCNet&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21333;&#20803;&#21644;&#20248;&#21270;&#30340;CSI&#31383;&#21475;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#24863;&#30693;&#21644;CSI&#37325;&#24314;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23454;&#26102;&#30340;&#20113;&#22522;WiFi&#24863;&#30693;&#12290;</title><link>https://arxiv.org/abs/2402.04888</link><description>&lt;p&gt;
RSCNet&#65306;&#20113;&#22522;WiFi&#24863;&#30693;&#30340;&#21160;&#24577;CSI&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
RSCNet: Dynamic CSI Compression for Cloud-based WiFi Sensing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04888
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RSCNet&#30340;&#21160;&#24577;CSI&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#21387;&#32553;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#26469;&#20943;&#23569;&#29289;&#32852;&#32593;&#35774;&#22791;&#21521;&#20113;&#26381;&#21153;&#22120;&#20256;&#36755;CSI&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;RSCNet&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21333;&#20803;&#21644;&#20248;&#21270;&#30340;CSI&#31383;&#21475;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#24863;&#30693;&#21644;CSI&#37325;&#24314;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23454;&#26102;&#30340;&#20113;&#22522;WiFi&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
WiFi&#36830;&#25509;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#27491;&#22312;&#20174;&#32431;&#31929;&#30340;&#36890;&#20449;&#35774;&#22791;&#21457;&#23637;&#20026;&#21033;&#29992;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#25552;&#21462;&#33021;&#21147;&#30340;&#24863;&#30693;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#36164;&#28304;&#26377;&#38480;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#35201;&#27714;&#23558;CSI&#20256;&#36755;&#21040;&#20113;&#26381;&#21153;&#22120;&#36827;&#34892;&#24863;&#30693;&#12290;&#23613;&#31649;&#21487;&#34892;&#65292;&#20294;&#36825;&#20250;&#23548;&#33268;&#22823;&#37327;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#26102;&#24863;&#30693;&#21644;&#21387;&#32553;&#32593;&#32476;&#65288;RSCNet&#65289;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#21387;&#32553;CSI&#26469;&#23454;&#29616;&#24863;&#30693;&#65292;&#20174;&#32780;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#12290;RSCNet&#22312;&#30001;&#23569;&#37327;CSI&#24103;&#32452;&#25104;&#30340;CSI&#31383;&#21475;&#20043;&#38388;&#36827;&#34892;&#20248;&#21270;&#12290;&#19968;&#26086;&#20256;&#36755;&#21040;&#20113;&#26381;&#21153;&#22120;&#65292;&#23427;&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21333;&#20803;&#20174;&#20808;&#21069;&#30340;&#31383;&#21475;&#20013;&#25552;&#21462;&#25968;&#25454;&#65292;&#20174;&#32780;&#22686;&#24378;&#24863;&#30693;&#20934;&#30830;&#24615;&#21644;CSI&#37325;&#24314;&#12290;RSCNet&#24039;&#22937;&#22320;&#24179;&#34913;&#20102;CSI&#21387;&#32553;&#21644;&#24863;&#30693;&#31934;&#24230;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#23454;&#26102;&#20113;&#22522;WiFi&#24863;&#30693;&#65292;&#24182;&#20943;&#23569;&#20102;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
WiFi-enabled Internet-of-Things (IoT) devices are evolving from mere communication devices to sensing instruments, leveraging Channel State Information (CSI) extraction capabilities. Nevertheless, resource-constrained IoT devices and the intricacies of deep neural networks necessitate transmitting CSI to cloud servers for sensing. Although feasible, this leads to considerable communication overhead. In this context, this paper develops a novel Real-time Sensing and Compression Network (RSCNet) which enables sensing with compressed CSI; thereby reducing the communication overheads. RSCNet facilitates optimization across CSI windows composed of a few CSI frames. Once transmitted to cloud servers, it employs Long Short-Term Memory (LSTM) units to harness data from prior windows, thus bolstering both the sensing accuracy and CSI reconstruction. RSCNet adeptly balances the trade-off between CSI compression and sensing precision, thus streamlining real-time cloud-based WiFi sensing with redu
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21361;&#23475;&#25918;&#22823;&#29616;&#35937;&#24182;&#21457;&#23637;&#20102;&#37327;&#21270;&#21361;&#23475;&#25918;&#22823;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#27169;&#22411;&#36755;&#20986;&#30340;&#21361;&#23475;&#19982;&#29992;&#25143;&#36755;&#20837;&#30340;&#24773;&#22659;&#12290;&#25105;&#20204;&#36824;&#23454;&#35777;&#22320;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#37327;&#21270;&#20102;&#30001;&#21361;&#23475;&#25918;&#22823;&#24341;&#36215;&#30340;&#24615;&#21035;&#20043;&#38388;&#30340;&#24433;&#21709;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.01787</link><description>&lt;p&gt;
&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#21361;&#23475;&#25918;&#22823;
&lt;/p&gt;
&lt;p&gt;
Harm Amplification in Text-to-Image Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01787
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21361;&#23475;&#25918;&#22823;&#29616;&#35937;&#24182;&#21457;&#23637;&#20102;&#37327;&#21270;&#21361;&#23475;&#25918;&#22823;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#27169;&#22411;&#36755;&#20986;&#30340;&#21361;&#23475;&#19982;&#29992;&#25143;&#36755;&#20837;&#30340;&#24773;&#22659;&#12290;&#25105;&#20204;&#36824;&#23454;&#35777;&#22320;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#37327;&#21270;&#20102;&#30001;&#21361;&#23475;&#25918;&#22823;&#24341;&#36215;&#30340;&#24615;&#21035;&#20043;&#38388;&#30340;&#24433;&#21709;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687; (T2I) &#27169;&#22411;&#24050;&#25104;&#20026;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#28982;&#32780;&#65292;&#23384;&#22312;&#23433;&#20840;&#38382;&#39064;&#65292;&#21363;&#20351;&#29992;&#25143;&#36755;&#20837;&#30475;&#20284;&#23433;&#20840;&#30340;&#25552;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#20063;&#21487;&#33021;&#29983;&#25104;&#26377;&#23475;&#22270;&#20687;&#12290;&#36825;&#31181;&#29616;&#35937;&#31216;&#20026;&#21361;&#23475;&#25918;&#22823;&#65292;&#23427;&#27604;&#23545;&#25239;&#25552;&#31034;&#26356;&#20855;&#28508;&#22312;&#39118;&#38505;&#65292;&#20351;&#29992;&#25143;&#26080;&#24847;&#38388;&#36973;&#21463;&#20260;&#23475;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#21361;&#23475;&#25918;&#22823;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#24182;&#36827;&#19968;&#27493;&#36129;&#29486;&#20110;&#24320;&#21457;&#29992;&#20110;&#37327;&#21270;&#21361;&#23475;&#25918;&#22823;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#27169;&#22411;&#36755;&#20986;&#30340;&#21361;&#23475;&#19982;&#29992;&#25143;&#36755;&#20837;&#30340;&#24773;&#22659;&#12290;&#25105;&#20204;&#36824;&#32463;&#39564;&#24615;&#22320;&#30740;&#31350;&#20102;&#22914;&#20309;&#24212;&#29992;&#36825;&#20123;&#26041;&#27861;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#30340;&#37096;&#32626;&#22330;&#26223;&#65292;&#21253;&#25324;&#37327;&#21270;&#30001;&#21361;&#23475;&#25918;&#22823;&#24341;&#36215;&#30340;&#19981;&#21516;&#24615;&#21035;&#20043;&#38388;&#30340;&#24433;&#21709;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#20026;&#30740;&#31350;&#32773;&#25552;&#20379;&#24037;&#20855;&#21435;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image (T2I) models have emerged as a significant advancement in generative AI; however, there exist safety concerns regarding their potential to produce harmful image outputs even when users input seemingly safe prompts. This phenomenon, where T2I models generate harmful representations that were not explicit in the input, poses a potentially greater risk than adversarial prompts, leaving users unintentionally exposed to harms. Our paper addresses this issue by first introducing a formal definition for this phenomenon, termed harm amplification. We further contribute to the field by developing methodologies to quantify harm amplification in which we consider the harm of the model output in the context of user input. We then empirically examine how to apply these different methodologies to simulate real-world deployment scenarios including a quantification of disparate impacts across genders resulting from harm amplification. Together, our work aims to offer researchers tools to
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Scheduled Curiosity-Deep Dyna-Q (SC-DDQ)&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#39044;&#23450;&#23398;&#20064;&#21644;&#22909;&#22855;&#24515;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20195;&#29702;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.00085</link><description>&lt;p&gt;
&#39044;&#23450;&#22909;&#22855;&#24515;-&#28145;&#24230;&#21160;&#24577;-Q&#65306;&#23545;&#35805;&#31574;&#30053;&#23398;&#20064;&#30340;&#39640;&#25928;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Scheduled Curiosity-Deep Dyna-Q: Efficient Exploration for Dialog Policy Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Scheduled Curiosity-Deep Dyna-Q (SC-DDQ)&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#39044;&#23450;&#23398;&#20064;&#21644;&#22909;&#22855;&#24515;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20195;&#29702;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20195;&#29702;&#30340;&#22521;&#35757;&#26159;&#32791;&#26102;&#30340;&#65292;&#24182;&#38656;&#35201;&#19982;&#30495;&#23454;&#29992;&#25143;&#36827;&#34892;&#22823;&#37327;&#30340;&#20132;&#20114;&#12290;&#22914;&#20309;&#22312;&#26377;&#38480;&#30340;&#23545;&#35805;&#32463;&#39564;&#20013;&#25484;&#25569;&#23545;&#35805;&#31574;&#30053;&#20173;&#28982;&#26159;&#20351;&#20195;&#29702;&#22521;&#35757;&#36807;&#31243;&#26356;&#21152;&#39640;&#25928;&#30340;&#38556;&#30861;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#26694;&#26550;&#36890;&#36807;&#38543;&#26426;&#36873;&#25321;&#22521;&#35757;&#26679;&#26412;&#24320;&#22987;&#22521;&#35757;&#65292;&#36825;&#19982;&#20154;&#31867;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;&#25439;&#23475;&#20102;&#22521;&#35757;&#30340;&#25928;&#29575;&#21644;&#31283;&#23450;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#23545;&#35805;&#27169;&#22411;Deep Dyna-Q(DDQ)&#30340;&#39044;&#23450;&#22909;&#22855;&#24515;-&#28145;&#24230;&#21160;&#24577;-Q (SC-DDQ)&#30340;&#22909;&#22855;&#24515;&#39537;&#21160;&#35838;&#31243;&#23398;&#20064;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#21035;&#20026;SC-DDQ&#21644;DDQ&#35774;&#35745;&#20102;&#23398;&#20064;&#35745;&#21010;&#65292;&#36981;&#24490;&#20004;&#31181;&#30456;&#21453;&#30340;&#22521;&#35757;&#31574;&#30053;&#65306;&#32463;&#20856;&#35838;&#31243;&#23398;&#20064;&#21450;&#20854;&#36870;&#21521;&#29256;&#26412;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#24341;&#20837;&#39044;&#23450;&#23398;&#20064;&#21644;&#22909;&#22855;&#24515;&#65292;&#26032;&#26694;&#26550;&#22312;DDQ&#21644;Dee&#30340;&#22522;&#30784;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training task-oriented dialog agents based on reinforcement learning is time-consuming and requires a large number of interactions with real users. How to grasp dialog policy within limited dialog experiences remains an obstacle that makes the agent training process less efficient. In addition, most previous frameworks start training by randomly choosing training samples, which differs from the human learning method and hurts the efficiency and stability of training. Therefore, we propose Scheduled Curiosity-Deep Dyna-Q (SC-DDQ), a curiosity-driven curriculum learning framework based on a state-of-the-art model-based reinforcement learning dialog model, Deep Dyna-Q (DDQ). Furthermore, we designed learning schedules for SC-DDQ and DDQ, respectively, following two opposite training strategies: classic curriculum learning and its reverse version. Our results show that by introducing scheduled learning and curiosity, the new framework leads to a significant improvement over the DDQ and Dee
&lt;/p&gt;</description></item><item><title>Genixer&#26159;&#19968;&#31181;&#20026;&#29983;&#25104;&#39640;&#36136;&#37327;&#25351;&#23548;&#25968;&#25454;&#32780;&#35774;&#35745;&#30340;&#21019;&#26032;&#25968;&#25454;&#29983;&#25104;&#31649;&#36947;&#65292;&#21253;&#25324;&#20061;&#20010;&#20195;&#34920;&#24615;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;&#22235;&#20010;&#20851;&#38190;&#27493;&#39588;&#26469;&#25913;&#21892;&#25968;&#25454;&#29983;&#25104;&#30340;&#38590;&#24230;&#12290;</title><link>https://arxiv.org/abs/2312.06731</link><description>&lt;p&gt;
Genixer&#65306;&#23558;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#36171;&#33021;&#20026;&#24378;&#22823;&#30340;&#25968;&#25454;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Genixer: Empowering Multimodal Large Language Models as a Powerful Data Generator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06731
&lt;/p&gt;
&lt;p&gt;
Genixer&#26159;&#19968;&#31181;&#20026;&#29983;&#25104;&#39640;&#36136;&#37327;&#25351;&#23548;&#25968;&#25454;&#32780;&#35774;&#35745;&#30340;&#21019;&#26032;&#25968;&#25454;&#29983;&#25104;&#31649;&#36947;&#65292;&#21253;&#25324;&#20061;&#20010;&#20195;&#34920;&#24615;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;&#22235;&#20010;&#20851;&#38190;&#27493;&#39588;&#26469;&#25913;&#21892;&#25968;&#25454;&#29983;&#25104;&#30340;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06731v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442;-&#20132;&#21449;  &#25688;&#35201;: &#35843;&#20248;&#25968;&#25454;&#23545;&#20110;&#35757;&#32451;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;(MLLMs)&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#39640;&#36136;&#37327;&#35843;&#20248;&#25968;&#25454;&#30340;&#21019;&#24314;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#20808;&#21069;&#20381;&#36182;&#20110;GPT-4&#36827;&#34892;&#25968;&#25454;&#29983;&#25104;&#30340;&#26041;&#27861;&#19981;&#20165;&#25104;&#26412;&#39640;&#26114;&#65292;&#32780;&#19988;&#22312;&#22797;&#26434;&#20219;&#21153;&#65288;&#21363;&#22522;&#20110;&#29702;&#35299;&#30340;&#25512;&#29702;&#20219;&#21153;&#65289;&#20013;&#30340;&#24615;&#33021;&#20063;&#19981;&#23613;&#22914;&#20154;&#24847;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;Genixer&#65292;&#29992;&#20110;&#29983;&#25104;&#21508;&#31181;&#39640;&#36136;&#37327;&#30340;&#35843;&#20248;&#25968;&#25454;&#65292;&#21253;&#25324;&#20061;&#20010;&#20195;&#34920;&#24615;&#20219;&#21153;&#65292;&#20363;&#22914;&#24120;&#35265;&#30340;VQA&#65292;REC&#65292;REG&#21644;PointQ&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;Genixer&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#22235;&#20010;&#20851;&#38190;&#27493;&#39588;&#26469;&#32531;&#35299;&#25968;&#25454;&#29983;&#25104;&#30340;&#22256;&#38590;&#65306;(i)&#25351;&#23548;&#25968;&#25454;&#25910;&#38598;&#65292;(ii)&#25351;&#23548;&#27169;&#26495;&#35774;&#35745;&#65292;(iii)&#36171;&#33021;MLLM&#65292;&#21644;(iv)&#25968;&#25454;&#29983;&#25104;&#21644;&#36807;&#28388;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#30340;Genixer&#30340;&#21331;&#36234;&#30340;&#23450;&#24615;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;MLLM&#20855;&#26377;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06731v2 Announce Type: replace-cross  Abstract: Instruction tuning data is essential for training the Multimodal Large Language Models (MLLMs). However, the creation of high-quality instruction tuning data presents significant challenges. Prior methods that depended on GPT-4 for data generation were not only costly but also lacked satisfactory performance in complex tasks (i.e., grounding-based reasoning tasks). To address these issues, we developed an innovative data generation pipeline, Genixer, to generate various high-quality instruction tuning data, including nine representative tasks, e.g., Common VQA, REC, REG, and PointQ. Specifically, Genixer provides a unified solution with four key steps for alleviating the difficulty of data generation: (i) instruction data collection, (ii) instruction template design, (iii) empowering MLLM, and (iv) data generation and filtering. Subsequently, the superior qualitative results of our Genixer demonstrate that current MLLMs have a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#23558;&#21512;&#36866;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#24212;&#29992;&#20110;&#35299;&#20915;&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#20013;&#30340;&#25968;&#25454;&#38598;&#36716;&#31227;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;&#22312;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;EEG&#20449;&#21495;&#20998;&#31867;&#31995;&#32479;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2210.06554</link><description>&lt;p&gt;
&#36816;&#29992;XAI&#26041;&#27861;&#20110;&#22522;&#20110;EEG&#30340;&#31995;&#32479;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Toward the application of XAI methods in EEG-based systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.06554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23558;&#21512;&#36866;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#24212;&#29992;&#20110;&#35299;&#20915;&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#20013;&#30340;&#25968;&#25454;&#38598;&#36716;&#31227;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;&#22312;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;EEG&#20449;&#21495;&#20998;&#31867;&#31995;&#32479;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#25968;&#25454;&#38598;&#36716;&#31227;&#38382;&#39064;&#30340;&#19968;&#20010;&#26377;&#36259;&#26696;&#20363;&#26159;&#22312;&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#32972;&#26223;&#19979;&#23545;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#36827;&#34892;&#20998;&#31867;&#12290; EEG&#20449;&#21495;&#30340;&#38750;&#38745;&#27490;&#24615;&#21487;&#33021;&#23548;&#33268;BCI&#20998;&#31867;&#31995;&#32479;&#22312;&#19981;&#21516;&#20250;&#35805;&#20013;&#20351;&#29992;&#26102;&#24615;&#33021;&#27867;&#21270;&#24046;&#65292;&#29978;&#33267;&#26159;&#21516;&#19968;&#34987;&#35797;&#39564;&#12290; &#26412;&#25991;&#30340;&#20986;&#21457;&#28857;&#26159;&#65292;&#36890;&#36807;&#21033;&#29992;&#21512;&#36866;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#26469;&#23450;&#20301;&#21644;&#36716;&#25442;&#36755;&#20837;&#30340;&#30456;&#20851;&#29305;&#24449;&#65292;&#20174;&#32780;&#32531;&#35299;&#25968;&#25454;&#38598;&#36716;&#31227;&#38382;&#39064;&#12290; &#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#23545;&#20960;&#31181;XAI&#26041;&#27861;&#22312;&#22312;&#20856;&#22411;&#30340;&#29992;&#20110;&#24773;&#32490;&#35782;&#21035;&#30340;EEG&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;ML&#31995;&#32479;&#19978;&#29983;&#25104;&#30340;&#35299;&#37322;&#30340;&#23454;&#39564;&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;XAI&#26041;&#27861;&#25214;&#21040;&#30340;&#35768;&#22810;&#30456;&#20851;&#32452;&#20214;&#22312;&#20250;&#35805;&#20043;&#38388;&#26159;&#20849;&#20139;&#30340;&#65292;&#21487;&#20197;&#29992;&#26469;&#26500;&#24314;&#19968;&#20010;&#20855;&#26377;&#26356;&#22909;&#27867;&#21270;&#33021;&#21147;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.06554v3 Announce Type: replace-cross  Abstract: An interesting case of the well-known Dataset Shift Problem is the classification of Electroencephalogram (EEG) signals in the context of Brain-Computer Interface (BCI). The non-stationarity of EEG signals can lead to poor generalisation performance in BCI classification systems used in different sessions, also from the same subject. In this paper, we start from the hypothesis that the Dataset Shift problem can be alleviated by exploiting suitable eXplainable Artificial Intelligence (XAI) methods to locate and transform the relevant characteristics of the input for the goal of classification. In particular, we focus on an experimental analysis of explanations produced by several XAI methods on an ML system trained on a typical EEG dataset for emotion recognition. Results show that many relevant components found by XAI methods are shared across the sessions and can be used to build a system able to generalise better. However, re
&lt;/p&gt;</description></item><item><title>Marabou 2.0&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#31070;&#32463;&#32593;&#32476;&#24418;&#24335;&#20998;&#26512;&#22120;&#65292;&#20855;&#26377;&#21019;&#26032;&#30340;&#26550;&#26500;&#35774;&#35745;&#21644;&#24341;&#20837;&#30340;&#20027;&#35201;&#21151;&#33021;&#21644;&#32452;&#20214;&#12290;</title><link>http://arxiv.org/abs/2401.14461</link><description>&lt;p&gt;
Marabou 2.0: &#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#31070;&#32463;&#32593;&#32476;&#24418;&#24335;&#20998;&#26512;&#22120;
&lt;/p&gt;
&lt;p&gt;
Marabou 2.0: A Versatile Formal Analyzer of Neural Networks. (arXiv:2401.14461v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14461
&lt;/p&gt;
&lt;p&gt;
Marabou 2.0&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#31070;&#32463;&#32593;&#32476;&#24418;&#24335;&#20998;&#26512;&#22120;&#65292;&#20855;&#26377;&#21019;&#26032;&#30340;&#26550;&#26500;&#35774;&#35745;&#21644;&#24341;&#20837;&#30340;&#20027;&#35201;&#21151;&#33021;&#21644;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#20851;&#20110;Marabou&#26694;&#26550;2.0&#29256;&#26412;&#30340;&#32508;&#21512;&#31995;&#32479;&#25551;&#36848;&#65292;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24418;&#24335;&#20998;&#26512;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#24037;&#20855;&#30340;&#26550;&#26500;&#35774;&#35745;&#65292;&#24182;&#20171;&#32461;&#20102;&#33258;&#21021;&#22987;&#21457;&#24067;&#20197;&#26469;&#24341;&#20837;&#30340;&#20027;&#35201;&#21151;&#33021;&#21644;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper serves as a comprehensive system description of version 2.0 of the Marabou framework for formal analysis of neural networks. We discuss the tool's architectural design and highlight the major features and components introduced since its initial release.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#25945;&#23398;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36845;&#20195;&#26426;&#22120;&#25945;&#23398;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#22686;&#24378;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;&#26679;&#26412;&#23398;&#20064;&#22120;&#30340;&#27169;&#22359;&#21270;AI&#20195;&#29702;&#30340;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05467</link><description>&lt;p&gt;
&#22522;&#20110;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#27169;&#22359;&#21270;AI&#20195;&#29702;&#30340;&#26426;&#22120;&#25945;&#23398;
&lt;/p&gt;
&lt;p&gt;
Machine Teaching for Building Modular AI Agents based on Zero-shot Learners. (arXiv:2401.05467v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05467
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#25945;&#23398;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36845;&#20195;&#26426;&#22120;&#25945;&#23398;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#22686;&#24378;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;&#26679;&#26412;&#23398;&#20064;&#22120;&#30340;&#27169;&#22359;&#21270;AI&#20195;&#29702;&#30340;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#23548;&#33268;&#20102;&#35768;&#22810;&#27169;&#22359;&#21270;AI&#20195;&#29702;&#30340;&#21019;&#24314;&#12290;&#36825;&#20123;&#20195;&#29702;&#20351;&#29992;LLMs&#20316;&#20026;&#38646;&#26679;&#26412;&#23398;&#20064;&#22120;&#65292;&#22312;&#20154;&#31867;&#29992;&#25143;&#35774;&#23450;&#30340;&#22797;&#26434;&#20219;&#21153;&#20013;&#25191;&#34892;&#23376;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#22686;&#24378;&#21033;&#29992;LLMs&#20316;&#20026;&#38646;&#26679;&#26412;&#23398;&#20064;&#22120;&#30340;&#27169;&#22359;&#21270;AI&#20195;&#29702;&#30340;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#36845;&#20195;&#26426;&#22120;&#25945;&#23398;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#26377;&#38480;&#30340;&#20154;&#31867;&#21453;&#39304;&#19979;&#36880;&#28176;&#25945;&#23548;AI&#20195;&#29702;&#30340;&#39640;&#25928;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#38646;&#26679;&#26412;&#23398;&#20064;&#36136;&#37327;&#38480;&#21046;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20027;&#24352;&#21033;&#29992;&#21021;&#22987;&#37096;&#32626;&#30340;&#25968;&#25454;&#36861;&#36394;&#20197;&#21450;&#38646;&#26679;&#26412;&#23398;&#20064;&#22120;&#30340;&#36755;&#20986;&#25110;&#27880;&#37322;&#26469;&#35757;&#32451;&#26356;&#23567;&#19988;&#20219;&#21153;&#29305;&#23450;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#21487;&#20197;&#20943;&#23569;&#32463;&#27982;&#25104;&#26412;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26426;&#22120;&#25945;&#23398;&#36807;&#31243;&#21033;&#29992;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#26469;&#32416;&#27491;&#39640;&#27010;&#29575;&#35823;&#26631;&#27880;&#30340;&#31034;&#20363;&#12290;&#22312;&#19977;&#20010;&#24120;&#35265;&#23545;&#35805;AI&#20195;&#29702;&#20219;&#21153;&#19978;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#25509;&#36817;&#29702;&#24819;&#24615;&#33021;&#21487;&#20197;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advances in large language models (LLMs) have led to the creation of many modular AI agents. These agents employ LLMs as zero-shot learners to perform sub-tasks in order to solve complex tasks set forth by human users. We propose an approach to enhance the robustness and performance of modular AI agents that utilize LLMs as zero-shot learners. Our iterative machine teaching method offers an efficient way to teach AI agents over time with limited human feedback, addressing the limit posed by the quality of zero-shot learning. We advocate leveraging the data traces from initial deployments and outputs or annotations from the zero-shot learners to train smaller and task-specific substitute models which can reduce both the monetary costs and environmental impact. Our machine teaching process avails human expertise to correct examples with a high likelihood of misannotations. Results on three tasks, common to conversational AI agents, show that close-to-oracle performance can be 
&lt;/p&gt;</description></item><item><title>MoSECroT&#26159;&#19968;&#20010;&#32467;&#21512;&#38745;&#24577;&#35789;&#21521;&#37327;&#30340;&#27169;&#22411;&#25340;&#25509;&#26694;&#26550;&#65292;&#29992;&#20110;&#36328;&#35821;&#35328;&#38646;&#26679;&#20363;&#36801;&#31227;&#12290;&#23427;&#21033;&#29992;&#30456;&#23545;&#34920;&#31034;&#26500;&#24314;&#20102;&#28304;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#30446;&#26631;&#35821;&#35328;&#38745;&#24577;&#35789;&#21521;&#37327;&#30340;&#20849;&#20139;&#31354;&#38388;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#36890;&#36807;&#31616;&#21333;&#20132;&#25442;&#23884;&#20837;&#20174;&#28304;&#35821;&#35328;&#35757;&#32451;&#25968;&#25454;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#30446;&#26631;&#35821;&#35328;&#19978;&#36827;&#34892;&#38646;&#26679;&#20363;&#36801;&#31227;&#12290;</title><link>http://arxiv.org/abs/2401.04821</link><description>&lt;p&gt;
MoSECroT: &#20351;&#29992;&#38745;&#24577;&#35789;&#21521;&#37327;&#36827;&#34892;&#27169;&#22411;&#25340;&#25509;&#23454;&#29616;&#36328;&#35821;&#35328;&#38646;&#26679;&#20363;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
MoSECroT: Model Stitching with Static Word Embeddings for Crosslingual Zero-shot Transfer. (arXiv:2401.04821v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04821
&lt;/p&gt;
&lt;p&gt;
MoSECroT&#26159;&#19968;&#20010;&#32467;&#21512;&#38745;&#24577;&#35789;&#21521;&#37327;&#30340;&#27169;&#22411;&#25340;&#25509;&#26694;&#26550;&#65292;&#29992;&#20110;&#36328;&#35821;&#35328;&#38646;&#26679;&#20363;&#36801;&#31227;&#12290;&#23427;&#21033;&#29992;&#30456;&#23545;&#34920;&#31034;&#26500;&#24314;&#20102;&#28304;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#30446;&#26631;&#35821;&#35328;&#38745;&#24577;&#35789;&#21521;&#37327;&#30340;&#20849;&#20139;&#31354;&#38388;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#36890;&#36807;&#31616;&#21333;&#20132;&#25442;&#23884;&#20837;&#20174;&#28304;&#35821;&#35328;&#35757;&#32451;&#25968;&#25454;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#30446;&#26631;&#35821;&#35328;&#19978;&#36827;&#34892;&#38646;&#26679;&#20363;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#65292;&#32780;&#36825;&#20123;&#36164;&#28304;&#20960;&#20046;&#21482;&#26377;&#39640;&#36164;&#28304;&#35821;&#35328;&#25165;&#33021;&#33719;&#24471;&#12290;&#30456;&#21453;&#65292;&#38745;&#24577;&#35789;&#21521;&#37327;&#30340;&#35757;&#32451;&#26356;&#23481;&#26131;&#65292;&#21487;&#20197;&#26356;&#33410;&#30465;&#35745;&#31639;&#36164;&#28304;&#21644;&#25968;&#25454;&#37327;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MoSECroT&#65288;Model Stitching with Static Word Embeddings for Crosslingual Zero-shot Transfer&#65289;&#27169;&#22411;&#25340;&#25509;&#19982;&#38745;&#24577;&#35789;&#21521;&#37327;&#32467;&#21512;&#30340;&#26032;&#39062;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#23384;&#22312;&#38745;&#24577;&#35789;&#21521;&#37327;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21033;&#29992;&#30456;&#23545;&#34920;&#31034;&#26500;&#24314;&#28304;&#35821;&#35328;PLM&#23884;&#20837;&#21644;&#30446;&#26631;&#35821;&#35328;&#38745;&#24577;&#35789;&#21521;&#37327;&#20043;&#38388;&#30340;&#20849;&#20139;&#31354;&#38388;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#28304;&#35821;&#35328;&#35757;&#32451;&#25968;&#25454;&#35757;&#32451;PLM&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#22320;&#20132;&#25442;&#23884;&#20837;&#23436;&#25104;&#20174;&#28304;&#35821;&#35328;&#21040;&#30446;&#26631;&#35821;&#35328;&#30340;&#38646;&#26679;&#20363;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based pre-trained language models (PLMs) have achieved remarkable performance in various natural language processing (NLP) tasks. However, pre-training such models can take considerable resources that are almost only available to high-resource languages. On the contrary, static word embeddings are easier to train in terms of computing resources and the amount of data required. In this paper, we introduce MoSECroT Model Stitching with Static Word Embeddings for Crosslingual Zero-shot Transfer), a novel and challenging task that is especially relevant to low-resource languages for which static word embeddings are available. To tackle the task, we present the first framework that leverages relative representations to construct a common space for the embeddings of a source language PLM and the static word embeddings of a target language. In this way, we can train the PLM on source-language training data and perform zero-shot transfer to the target language by simply swapping th
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#40657;&#30418;&#38382;&#39064;&#30340;&#30333;&#30418;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;&#22522;&#20110;&#30456;&#20851;&#39046;&#22495;&#19968;&#33324;&#29702;&#35770;&#30340;&#30830;&#23450;&#24615;&#36923;&#36753;&#32454;&#32990;&#33258;&#21160;&#26426;&#30340;&#35268;&#21017;&#65292;&#35813;&#32454;&#32990;&#33258;&#21160;&#26426;&#23454;&#29616;&#33258;&#21160;&#24182;&#34892;&#36923;&#36753;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2401.03093</link><description>&lt;p&gt;
&#19968;&#31181;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#40657;&#30418;&#38382;&#39064;&#30340;&#30333;&#30418;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A white box solution to the black box problem of AI. (arXiv:2401.03093v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03093
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#40657;&#30418;&#38382;&#39064;&#30340;&#30333;&#30418;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;&#22522;&#20110;&#30456;&#20851;&#39046;&#22495;&#19968;&#33324;&#29702;&#35770;&#30340;&#30830;&#23450;&#24615;&#36923;&#36753;&#32454;&#32990;&#33258;&#21160;&#26426;&#30340;&#35268;&#21017;&#65292;&#35813;&#32454;&#32990;&#33258;&#21160;&#26426;&#23454;&#29616;&#33258;&#21160;&#24182;&#34892;&#36923;&#36753;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20154;&#24037;&#26234;&#33021;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#36879;&#26126;&#24615;&#65292;&#23545;&#20854;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#23384;&#22312;&#25285;&#24551;&#12290;&#36825;&#23601;&#26159;&#20154;&#24037;&#26234;&#33021;&#30340;&#40657;&#30418;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#31526;&#21495; AI &#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#31526;&#21495; AI &#20855;&#26377;&#36879;&#26126;&#30340;&#30333;&#30418;&#24615;&#36136;&#12290;&#31526;&#21495; AI &#30340;&#24191;&#27867;&#24212;&#29992;&#21463;&#21040;&#25968;&#23398;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#26415;&#35821;&#30340;&#19981;&#36879;&#26126;&#24615;&#12289;&#32570;&#20047;&#32479;&#19968;&#26412;&#20307;&#35770;&#20197;&#21450;&#25628;&#32034;&#36873;&#39033;&#30340;&#32452;&#21512;&#29190;&#28856;&#30340;&#38459;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#30340;&#40657;&#30418;&#38382;&#39064;&#24182;&#23454;&#29616;&#36890;&#29992;&#30340;&#31526;&#21495; AI&#65292;&#25105;&#20204;&#25552;&#35758;&#20351;&#29992;&#22522;&#20110;&#30456;&#20851;&#39046;&#22495;&#19968;&#33324;&#29702;&#35770;&#30340;&#30830;&#23450;&#24615;&#36923;&#36753;&#32454;&#32990;&#33258;&#21160;&#26426;&#30340;&#35268;&#21017;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#30456;&#20851;&#39046;&#22495;&#30340;&#19968;&#33324;&#29702;&#35770;&#36215;&#21040;&#20102;&#32454;&#32990;&#33258;&#21160;&#26426;&#25512;&#29702;&#30340;&#30693;&#35782;&#24211;&#30340;&#20316;&#29992;&#12290;&#32454;&#32990;&#33258;&#21160;&#26426;&#22312;&#22797;&#26434;&#31995;&#32479;&#30340;&#19977;&#20010;&#23618;&#27425;&#19978;&#23454;&#29616;&#33258;&#21160;&#24182;&#34892;&#36923;&#36753;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence based on neural networks has made significant progress. However, there are concerns about the reliability and security of this approach due to its lack of transparency. This is the black box problem of AI. Here we show how this problem can be solved using symbolic AI, which has a transparent white box nature. The widespread use of symbolic AI is hindered by the opacity of mathematical models and natural language terms, the lack of a unified ontology, and the combinatorial explosion of search options. To solve the AI black box problem and to implement general-purpose symbolic AI, we propose to use deterministic logic cellular automata with rules based on first principles of the general theory of the relevant domain. In this case, the general theory of the relevant domain plays the role of a knowledge base for the cellular automaton inference. A cellular automaton implements automatic parallel logical inference at three levels of organization of a complex system. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#23545;&#34920;&#26684;&#25968;&#25454;&#26597;&#35810;&#21644;&#21487;&#35270;&#21270;&#30340;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#27010;&#36848;&#65292;&#20171;&#32461;&#20102;&#35821;&#20041;&#35299;&#26512;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;Text-to-SQL&#21644;Text-to-Vis&#38382;&#39064;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2310.17894</link><description>&lt;p&gt;
&#23545;&#34920;&#26684;&#25968;&#25454;&#26597;&#35810;&#21644;&#21487;&#35270;&#21270;&#30340;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Natural Language Interfaces for Tabular Data Querying and Visualization: A Survey. (arXiv:2310.17894v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#23545;&#34920;&#26684;&#25968;&#25454;&#26597;&#35810;&#21644;&#21487;&#35270;&#21270;&#30340;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#27010;&#36848;&#65292;&#20171;&#32461;&#20102;&#35821;&#20041;&#35299;&#26512;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;Text-to-SQL&#21644;Text-to-Vis&#38382;&#39064;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#20986;&#29616;&#24443;&#24213;&#25913;&#21464;&#20102;&#29992;&#25143;&#19982;&#34920;&#26684;&#25968;&#25454;&#30340;&#20132;&#20114;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#20174;&#20256;&#32479;&#30340;&#26597;&#35810;&#35821;&#35328;&#21644;&#25163;&#21160;&#32472;&#22270;&#36716;&#21521;&#26356;&#30452;&#35266;&#12289;&#22522;&#20110;&#35821;&#35328;&#30340;&#30028;&#38754;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#21450;&#20854;&#21518;&#32487;&#32773;&#36827;&#19968;&#27493;&#25512;&#21160;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#26412;&#35843;&#26597;&#25552;&#20379;&#20102;&#20851;&#20110;&#34920;&#26684;&#25968;&#25454;&#26597;&#35810;&#21644;&#21487;&#35270;&#21270;&#30340;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#36825;&#20123;&#30028;&#38754;&#20801;&#35768;&#29992;&#25143;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#19982;&#25968;&#25454;&#36827;&#34892;&#20132;&#20114;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#36825;&#20123;&#30028;&#38754;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#25216;&#26415;&#65292;&#29305;&#21035;&#24378;&#35843;&#35821;&#20041;&#35299;&#26512;&#65292;&#36825;&#26159;&#23454;&#29616;&#20174;&#33258;&#28982;&#35821;&#35328;&#21040;SQL&#26597;&#35810;&#25110;&#25968;&#25454;&#21487;&#35270;&#21270;&#21629;&#20196;&#36716;&#21270;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#28982;&#21518;&#20174;&#25968;&#25454;&#38598;&#12289;&#26041;&#27861;&#35770;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#31995;&#32479;&#35774;&#35745;&#30340;&#35282;&#24230;&#28145;&#20837;&#25506;&#35752;&#20102;Text-to-SQL&#21644;Text-to-Vis&#38382;&#39064;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of natural language processing has revolutionized the way users interact with tabular data, enabling a shift from traditional query languages and manual plotting to more intuitive, language-based interfaces. The rise of large language models (LLMs) such as ChatGPT and its successors has further advanced this field, opening new avenues for natural language processing techniques. This survey presents a comprehensive overview of natural language interfaces for tabular data querying and visualization, which allow users to interact with data using natural language queries. We introduce the fundamental concepts and techniques underlying these interfaces with a particular emphasis on semantic parsing, the key technology facilitating the translation from natural language to SQL queries or data visualization commands. We then delve into the recent advancements in Text-to-SQL and Text-to-Vis problems from the perspectives of datasets, methodologies, metrics, and system designs. Thi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22240;&#26524;&#22270;&#25277;&#35937;&#20174;&#35266;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#35782;&#21035;&#24178;&#39044;&#24635;&#25928;&#24212;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#25193;&#23637;&#25688;&#35201;&#22240;&#26524;&#22270;&#20013;&#24635;&#25928;&#24212;&#24635;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25688;&#35201;&#22240;&#26524;&#22270;&#20013;&#24635;&#25928;&#24212;&#21487;&#35782;&#21035;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#30340;&#22270;&#24418;&#26465;&#20214;&#65292;&#24182;&#25552;&#20379;&#20102;&#35843;&#25972;&#38598;&#21512;&#20197;&#20272;&#35745;&#24635;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2310.14691</link><description>&lt;p&gt;
&#26102;&#24207;&#22240;&#26524;&#22270;&#30340;&#25277;&#35937;&#20013;&#24635;&#25928;&#24212;&#30340;&#21487;&#35782;&#21035;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Identifiability of total effects from abstractions of time series causal graphs. (arXiv:2310.14691v2 [math.ST] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22240;&#26524;&#22270;&#25277;&#35937;&#20174;&#35266;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#35782;&#21035;&#24178;&#39044;&#24635;&#25928;&#24212;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#25193;&#23637;&#25688;&#35201;&#22240;&#26524;&#22270;&#20013;&#24635;&#25928;&#24212;&#24635;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25688;&#35201;&#22240;&#26524;&#22270;&#20013;&#24635;&#25928;&#24212;&#21487;&#35782;&#21035;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#30340;&#22270;&#24418;&#26465;&#20214;&#65292;&#24182;&#25552;&#20379;&#20102;&#35843;&#25972;&#38598;&#21512;&#20197;&#20272;&#35745;&#24635;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20165;&#22522;&#20110;&#31995;&#32479;&#30340;&#22240;&#26524;&#22270;&#25277;&#35937;&#20174;&#35266;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#35782;&#21035;&#24178;&#39044;&#24635;&#25928;&#24212;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#25277;&#35937;&#65306;&#25193;&#23637;&#25688;&#35201;&#22240;&#26524;&#22270;&#23558;&#25152;&#26377;&#28382;&#21518;&#22240;&#26524;&#20851;&#31995;&#28151;&#28102;&#22312;&#19968;&#36215;&#65292;&#20294;&#21306;&#20998;&#28382;&#21518;&#21644;&#30636;&#26102;&#20851;&#31995;&#65307;&#32780;&#25688;&#35201;&#22240;&#26524;&#22270;&#21017;&#19981;&#25552;&#20379;&#20219;&#20309;&#20851;&#20110;&#22240;&#26524;&#20851;&#31995;&#28382;&#21518;&#30340;&#25351;&#31034;&#12290;&#25105;&#20204;&#35777;&#26126;&#25193;&#23637;&#25688;&#35201;&#22240;&#26524;&#22270;&#20013;&#24635;&#25928;&#24212;&#24635;&#26159;&#21487;&#35782;&#21035;&#30340;&#65292;&#24182;&#19988;&#25105;&#20204;&#25552;&#20379;&#20102;&#25688;&#35201;&#22240;&#26524;&#22270;&#20013;&#30340;&#21487;&#35782;&#21035;&#24615;&#25152;&#24517;&#38656;&#21644;&#20805;&#20998;&#30340;&#22270;&#24418;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#22312;&#24635;&#25928;&#24212;&#21487;&#35782;&#21035;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29992;&#20110;&#20272;&#35745;&#24635;&#25928;&#24212;&#30340;&#35843;&#25972;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of identifiability of the total effect of an intervention from observational time series only given an abstraction of the causal graph of the system. Specifically, we consider two types of abstractions: the extended summary causal graph which conflates all lagged causal relations but distinguishes between lagged and instantaneous relations; and the summary causal graph which does not give any indication about the lag between causal relations. We show that the total effect is always identifiable in extended summary causal graphs and we provide necessary and sufficient graphical conditions for identifiability in summary causal graphs. Furthermore, we provide adjustment sets allowing to estimate the total effect whenever it is identifiable.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KI-PMF&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20808;&#39564;&#30693;&#35782;&#65292;&#23545;&#20132;&#36890;&#21442;&#19982;&#32773;&#30340;&#26410;&#26469;&#34892;&#21160;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#65292;&#36981;&#24490;&#36710;&#36742;&#30340;&#36816;&#21160;&#32422;&#26463;&#21644;&#34892;&#39542;&#29615;&#22659;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#36890;&#36807;&#26465;&#20214;&#21270;&#32593;&#32476;&#20197;&#36981;&#24490;&#29289;&#29702;&#23450;&#24459;&#65292;&#21487;&#20197;&#33719;&#24471;&#20934;&#30830;&#21644;&#23433;&#20840;&#30340;&#39044;&#27979;&#65292;&#23545;&#20110;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#32500;&#25252;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#23433;&#20840;&#21644;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2310.12007</link><description>&lt;p&gt;
KI-PMF&#65306;&#30693;&#35782;&#32508;&#21512;&#30340;&#21512;&#29702;&#21160;&#20316;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
KI-PMF: Knowledge Integrated Plausible Motion Forecasting. (arXiv:2310.12007v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KI-PMF&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20808;&#39564;&#30693;&#35782;&#65292;&#23545;&#20132;&#36890;&#21442;&#19982;&#32773;&#30340;&#26410;&#26469;&#34892;&#21160;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#65292;&#36981;&#24490;&#36710;&#36742;&#30340;&#36816;&#21160;&#32422;&#26463;&#21644;&#34892;&#39542;&#29615;&#22659;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#36890;&#36807;&#26465;&#20214;&#21270;&#32593;&#32476;&#20197;&#36981;&#24490;&#29289;&#29702;&#23450;&#24459;&#65292;&#21487;&#20197;&#33719;&#24471;&#20934;&#30830;&#21644;&#23433;&#20840;&#30340;&#39044;&#27979;&#65292;&#23545;&#20110;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#32500;&#25252;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#23433;&#20840;&#21644;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#20132;&#36890;&#21442;&#19982;&#32773;&#30340;&#34892;&#21160;&#23545;&#22823;&#35268;&#27169;&#37096;&#32626;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#30340;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#20248;&#21270;&#29305;&#23450;&#24230;&#37327;&#30340;&#25439;&#22833;&#20989;&#25968;&#19978;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#39044;&#27979;&#19981;&#31526;&#21512;&#29289;&#29702;&#23450;&#24459;&#25110;&#36829;&#21453;&#22806;&#37096;&#32422;&#26463;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#32467;&#21512;&#26126;&#30830;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20351;&#32593;&#32476;&#33021;&#22815;&#39044;&#27979;&#26410;&#26469;&#36712;&#36857;&#65292;&#31526;&#21512;&#36710;&#36742;&#30340;&#36816;&#21160;&#32422;&#26463;&#21644;&#34892;&#39542;&#29615;&#22659;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38750;&#21442;&#25968;&#21098;&#26525;&#23618;&#21644;&#27880;&#24847;&#21147;&#23618;&#26469;&#25972;&#21512;&#23450;&#20041;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#30830;&#20445;&#20132;&#36890;&#21442;&#19982;&#32773;&#22312;&#22797;&#26434;&#21644;&#21160;&#24577;&#24773;&#20917;&#19979;&#30340;&#21040;&#36798;&#21487;&#36798;&#24615;&#20445;&#35777;&#12290;&#36890;&#36807;&#23558;&#32593;&#32476;&#26465;&#20214;&#21270;&#20026;&#36981;&#24490;&#29289;&#29702;&#23450;&#24459;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#20934;&#30830;&#21644;&#23433;&#20840;&#30340;&#39044;&#27979;&#65292;&#36825;&#23545;&#20110;&#22312;&#23454;&#38469;&#19990;&#30028;&#29615;&#22659;&#20013;&#32500;&#25252;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#23433;&#20840;&#21644;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately forecasting the motion of traffic actors is crucial for the deployment of autonomous vehicles at a large scale. Current trajectory forecasting approaches primarily concentrate on optimizing a loss function with a specific metric, which can result in predictions that do not adhere to physical laws or violate external constraints. Our objective is to incorporate explicit knowledge priors that allow a network to forecast future trajectories in compliance with both the kinematic constraints of a vehicle and the geometry of the driving environment. To achieve this, we introduce a non-parametric pruning layer and attention layers to integrate the defined knowledge priors. Our proposed method is designed to ensure reachability guarantees for traffic actors in both complex and dynamic situations. By conditioning the network to follow physical laws, we can obtain accurate and safe predictions, essential for maintaining autonomous vehicles' safety and efficiency in real-world settings
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#20154;&#31867;&#35760;&#24518;&#26426;&#21046;&#21551;&#21457;&#30340;&#31070;&#32463;&#27169;&#22359;&#65292;&#27169;&#25311;&#20154;&#31867;&#21644;&#26426;&#22120;&#22914;&#20309;&#23545;&#24403;&#21069;&#36755;&#20837;&#36827;&#34892;&#20851;&#32852;&#25512;&#29702;&#21644;&#38382;&#31572;&#65292;&#24182;&#23558;&#20854;&#19982;&#36807;&#21435;&#30340;&#35760;&#24518;&#32467;&#21512;&#36215;&#26469;&#12290;&#36890;&#36807;&#24863;&#30693;&#12289;&#35760;&#24518;&#21644;&#25512;&#29702;&#32452;&#20214;&#65292;&#35813;&#27169;&#22359;&#23454;&#29616;&#20102;&#24863;&#30693;&#26356;&#26032;&#12289;&#35760;&#24518;&#34701;&#21512;&#21644;&#20449;&#24687;&#26816;&#32034;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09297</link><description>&lt;p&gt;
&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#35748;&#30693;&#65306;&#21463;&#20154;&#31867;&#35760;&#24518;&#26426;&#21046;&#21551;&#21457;&#30340;&#25512;&#29702;&#31070;&#32463;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Understanding AI Cognition: A Neural Module for Inference Inspired by Human Memory Mechanisms. (arXiv:2310.09297v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09297
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#20154;&#31867;&#35760;&#24518;&#26426;&#21046;&#21551;&#21457;&#30340;&#31070;&#32463;&#27169;&#22359;&#65292;&#27169;&#25311;&#20154;&#31867;&#21644;&#26426;&#22120;&#22914;&#20309;&#23545;&#24403;&#21069;&#36755;&#20837;&#36827;&#34892;&#20851;&#32852;&#25512;&#29702;&#21644;&#38382;&#31572;&#65292;&#24182;&#23558;&#20854;&#19982;&#36807;&#21435;&#30340;&#35760;&#24518;&#32467;&#21512;&#36215;&#26469;&#12290;&#36890;&#36807;&#24863;&#30693;&#12289;&#35760;&#24518;&#21644;&#25512;&#29702;&#32452;&#20214;&#65292;&#35813;&#27169;&#22359;&#23454;&#29616;&#20102;&#24863;&#30693;&#26356;&#26032;&#12289;&#35760;&#24518;&#34701;&#21512;&#21644;&#20449;&#24687;&#26816;&#32034;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21644;&#26426;&#22120;&#22914;&#20309;&#23558;&#24403;&#21069;&#30340;&#36755;&#20837;&#19982;&#36807;&#21435;&#30340;&#35760;&#24518;&#32467;&#21512;&#36215;&#26469;&#65292;&#36827;&#34892;&#20851;&#32852;&#25512;&#29702;&#21644;&#38382;&#31572;&#65292;&#24182;&#23558;&#24863;&#30693;&#21040;&#30340;&#20449;&#24687;&#32622;&#20110;&#19978;&#19979;&#25991;&#20013;&#65292;&#36825;&#26159;&#35748;&#30693;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35868;&#39064;&#12290;&#21463;&#21040;&#20154;&#33041;&#35760;&#24518;&#31995;&#32479;&#21644;&#35748;&#30693;&#32467;&#26500;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#24863;&#30693;&#12289;&#35760;&#24518;&#21644;&#25512;&#29702;&#32452;&#20214;&#30340;PMI&#26694;&#26550;&#12290;&#29305;&#21035;&#22320;&#65292;&#35760;&#24518;&#27169;&#22359;&#21253;&#25324;&#24037;&#20316;&#35760;&#24518;&#21644;&#38271;&#26399;&#35760;&#24518;&#65292;&#20854;&#20013;&#21518;&#32773;&#20855;&#26377;&#26356;&#39640;&#38454;&#30340;&#32467;&#26500;&#26469;&#20445;&#30041;&#26356;&#22810;&#30340;&#32047;&#31215;&#30693;&#35782;&#21644;&#32463;&#39564;&#12290;&#36890;&#36807;&#21487;&#21306;&#20998;&#30340;&#31454;&#20105;&#20889;&#20837;&#35775;&#38382;&#65292;&#24403;&#21069;&#30340;&#24863;&#30693;&#26356;&#26032;&#24037;&#20316;&#35760;&#24518;&#65292;&#20043;&#21518;&#36890;&#36807;&#22806;&#31215;&#20851;&#32852;&#19982;&#38271;&#26399;&#35760;&#24518;&#34701;&#21512;&#65292;&#36991;&#20813;&#20869;&#23384;&#28322;&#20986;&#24182;&#26368;&#23567;&#21270;&#20449;&#24687;&#20914;&#31361;&#12290;&#22312;&#25512;&#29702;&#27169;&#22359;&#20013;&#65292;&#30456;&#20851;&#20449;&#24687;&#20174;&#20004;&#20010;&#21333;&#29420;&#30340;&#35760;&#24518;&#28304;&#26816;&#32034;&#24182;&#32467;&#21512;&#65292;&#20197;&#33719;&#24471;&#26356;&#20840;&#38754;&#21644;&#31934;&#30830;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
How humans and machines make sense of current inputs for relation reasoning and question-answering while putting the perceived information into context of our past memories, has been a challenging conundrum in cognitive science and artificial intelligence. Inspired by human brain's memory system and cognitive architectures, we propose a PMI framework that consists of perception, memory and inference components. Notably, the memory module comprises working and long-term memory, with the latter endowed with a higher-order structure to retain more accumulated knowledge and experiences. Through a differentiable competitive write access, current perceptions update working memory, which is later merged with long-term memory via outer product associations, averting memory overflow and minimizing information conflicts. In the inference module, relevant information is retrieved from two separate memory origins and associatively integrated to attain a more comprehensive and precise interpretatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35266;&#23519;&#25193;&#25955;&#21644;&#20449;&#24687;&#20998;&#35299;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#32454;&#31890;&#24230;&#20851;&#31995;&#65292;&#36827;&#19968;&#27493;&#35299;&#20915;&#20102;&#39640;&#32500;&#31354;&#38388;&#20013;&#20449;&#24687;&#25658;&#24102;&#21464;&#37327;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07972</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#20449;&#24687;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Interpretable Diffusion via Information Decomposition. (arXiv:2310.07972v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35266;&#23519;&#25193;&#25955;&#21644;&#20449;&#24687;&#20998;&#35299;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#32454;&#31890;&#24230;&#20851;&#31995;&#65292;&#36827;&#19968;&#27493;&#35299;&#20915;&#20102;&#39640;&#32500;&#31354;&#38388;&#20013;&#20449;&#24687;&#25658;&#24102;&#21464;&#37327;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#29992;&#20110;&#22797;&#26434;&#20851;&#31995;&#30340;&#26465;&#20214;&#29983;&#25104;&#21644;&#23494;&#24230;&#24314;&#27169;&#65292;&#22914;&#22270;&#20687;&#21644;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#21040;&#30340;&#20851;&#31995;&#30340;&#26412;&#36136;&#26159;&#19981;&#36879;&#26126;&#30340;&#65292;&#22240;&#27492;&#24456;&#38590;&#20934;&#30830;&#29702;&#35299;&#21333;&#35789;&#21644;&#22270;&#20687;&#37096;&#20998;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25110;&#32773;&#39044;&#27979;&#24178;&#39044;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#35266;&#23519;&#25193;&#25955;&#21644;&#20449;&#24687;&#20998;&#35299;&#20043;&#38388;&#30340;&#31934;&#30830;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#32454;&#31890;&#24230;&#20851;&#31995;&#12290;&#20114;&#20449;&#24687;&#21644;&#26465;&#20214;&#20114;&#20449;&#24687;&#30340;&#31934;&#30830;&#34920;&#36798;&#21487;&#20197;&#36890;&#36807;&#21435;&#22122;&#27169;&#22411;&#26469;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#20063;&#21487;&#20197;&#36731;&#26494;&#20272;&#35745;&#22312;&#29305;&#23450;&#22270;&#20687;&#21644;&#26631;&#39064;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36827;&#19968;&#27493;&#23545;&#20449;&#24687;&#36827;&#34892;&#20998;&#35299;&#65292;&#20197;&#29702;&#35299;&#39640;&#32500;&#31354;&#38388;&#20013;&#21738;&#20123;&#21464;&#37327;&#25658;&#24102;&#20449;&#24687;&#65292;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#23545;&#20110;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#38750;&#36127;&#20449;&#24687;&#20998;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion models enable conditional generation and density modeling of complex relationships like images and text. However, the nature of the learned relationships is opaque making it difficult to understand precisely what relationships between words and parts of an image are captured, or to predict the effect of an intervention. We illuminate the fine-grained relationships learned by diffusion models by noticing a precise relationship between diffusion and information decomposition. Exact expressions for mutual information and conditional mutual information can be written in terms of the denoising model. Furthermore, pointwise estimates can be easily estimated as well, allowing us to ask questions about the relationships between specific images and captions. Decomposing information even further to understand which variables in a high-dimensional space carry information is a long-standing problem. For diffusion models, we show that a natural non-negative decomposition of mutu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21487;&#22609;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#25968;&#25454;&#22686;&#24378;&#23545;&#20445;&#25345;&#21487;&#22609;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#35780;&#35770;&#32773;&#30340;&#21487;&#22609;&#24615;&#25439;&#22833;&#26159;&#39640;&#25928;&#35757;&#32451;&#30340;&#20027;&#35201;&#38480;&#21046;&#22240;&#32032;&#65292;&#24182;&#19988;&#26410;&#21450;&#26102;&#24674;&#22797;&#35780;&#35770;&#32773;&#30340;&#21487;&#22609;&#24615;&#23558;&#23548;&#33268;&#28798;&#38590;&#24615;&#32467;&#26524;&#12290;&#36825;&#20026;&#35299;&#20915;&#39640;&#37325;&#25918;&#27604;&#22256;&#22659;&#25552;&#20379;&#20102;&#26032;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.07418</link><description>&lt;p&gt;
&#37325;&#23457;&#35270;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21487;&#22609;&#24615;&#65306;&#25968;&#25454;&#12289;&#27169;&#22359;&#21644;&#35757;&#32451;&#38454;&#27573;
&lt;/p&gt;
&lt;p&gt;
Revisiting Plasticity in Visual Reinforcement Learning: Data, Modules and Training Stages. (arXiv:2310.07418v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21487;&#22609;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#25968;&#25454;&#22686;&#24378;&#23545;&#20445;&#25345;&#21487;&#22609;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#35780;&#35770;&#32773;&#30340;&#21487;&#22609;&#24615;&#25439;&#22833;&#26159;&#39640;&#25928;&#35757;&#32451;&#30340;&#20027;&#35201;&#38480;&#21046;&#22240;&#32032;&#65292;&#24182;&#19988;&#26410;&#21450;&#26102;&#24674;&#22797;&#35780;&#35770;&#32773;&#30340;&#21487;&#22609;&#24615;&#23558;&#23548;&#33268;&#28798;&#38590;&#24615;&#32467;&#26524;&#12290;&#36825;&#20026;&#35299;&#20915;&#39640;&#37325;&#25918;&#27604;&#22256;&#22659;&#25552;&#20379;&#20102;&#26032;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#22609;&#24615;&#65292;&#31070;&#32463;&#32593;&#32476;&#38543;&#26032;&#25968;&#25454;&#28436;&#36827;&#30340;&#33021;&#21147;&#65292;&#23545;&#20110;&#39640;&#24615;&#33021;&#21644;&#26679;&#26412;&#39640;&#25928;&#30340;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;(VRL)&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#37325;&#32622;&#21644;&#27491;&#21017;&#21270;&#31561;&#26041;&#27861;&#21487;&#33021;&#33021;&#22815;&#32531;&#35299;&#21487;&#22609;&#24615;&#25439;&#22833;&#65292;&#20294;VRL&#26694;&#26550;&#20869;&#21508;&#31181;&#32452;&#20214;&#23545;&#20195;&#29702;&#30340;&#21487;&#22609;&#24615;&#30340;&#24433;&#21709;&#20173;&#28982;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#32463;&#39564;&#24615;&#25506;&#32034;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#19977;&#20010;&#20027;&#35201;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#30340;&#26041;&#38754;&#65292;&#24182;&#24471;&#20986;&#20197;&#19979;&#26377;&#28145;&#20837;&#35265;&#35299;&#30340;&#32467;&#35770;&#65306;(1)&#25968;&#25454;&#22686;&#24378;&#23545;&#20110;&#20445;&#25345;&#21487;&#22609;&#24615;&#33267;&#20851;&#37325;&#35201;&#65307;(2)&#35780;&#35770;&#32773;&#30340;&#21487;&#22609;&#24615;&#25439;&#22833;&#26159;&#38459;&#30861;&#39640;&#25928;&#35757;&#32451;&#30340;&#20027;&#35201;&#29942;&#39048;&#65307;(3)&#22312;&#26089;&#26399;&#38454;&#27573;&#27809;&#26377;&#21450;&#26102;&#24178;&#39044;&#20197;&#24674;&#22797;&#35780;&#35770;&#32773;&#30340;&#21487;&#22609;&#24615;&#65292;&#20854;&#25439;&#22833;&#23558;&#21464;&#24471;&#28798;&#38590;&#24615;&#12290;&#36825;&#20123;&#35265;&#35299;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#23545;&#39640;&#37325;&#25918;&#27604;&#65288;RR&#65289;&#22256;&#22659;&#30340;&#26032;&#31574;&#30053;&#65292;&#20854;&#20013;&#21152;&#21095;&#30340;&#21487;&#22609;&#24615;&#25439;&#22833;&#22952;&#30861;&#20102;&#36890;&#36807;&#22686;&#21152;&#37325;&#25918;&#25968;&#37327;&#24102;&#26469;&#30340;&#26679;&#26412;&#25928;&#29575;&#30340;&#28508;&#22312;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Plasticity, the ability of a neural network to evolve with new data, is crucial for high-performance and sample-efficient visual reinforcement learning (VRL). Although methods like resetting and regularization can potentially mitigate plasticity loss, the influences of various components within the VRL framework on the agent's plasticity are still poorly understood. In this work, we conduct a systematic empirical exploration focusing on three primary underexplored facets and derive the following insightful conclusions: (1) data augmentation is essential in maintaining plasticity; (2) the critic's plasticity loss serves as the principal bottleneck impeding efficient training; and (3) without timely intervention to recover critic's plasticity in the early stages, its loss becomes catastrophic. These insights suggest a novel strategy to address the high replay ratio (RR) dilemma, where exacerbated plasticity loss hinders the potential improvements of sample efficiency brought by increased
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20174;Shapley Value&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#20102;Integrated Gradients&#30340;&#22522;&#32447;&#36873;&#25321;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#32447;&#26500;&#24314;&#26041;&#27861;&#21483;&#20570;Shapley Integrated Gradients (SIG)&#12290;&#22312;GridWorl&#19978;&#30340;&#27169;&#25311;&#23454;&#39564;&#34920;&#26126;&#65292;SIG&#33021;&#22815;&#29983;&#25104;&#26377;&#24847;&#20041;&#21644;&#26080;&#20559;&#30340;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.04821</link><description>&lt;p&gt;
&#20174;Shapley Value&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;Integrated Gradients&#30340;&#22522;&#32447;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Rethink Baseline of Integrated Gradients from the Perspective of Shapley Value. (arXiv:2310.04821v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04821
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20174;Shapley Value&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#20102;Integrated Gradients&#30340;&#22522;&#32447;&#36873;&#25321;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#32447;&#26500;&#24314;&#26041;&#27861;&#21483;&#20570;Shapley Integrated Gradients (SIG)&#12290;&#22312;GridWorl&#19978;&#30340;&#27169;&#25311;&#23454;&#39564;&#34920;&#26126;&#65292;SIG&#33021;&#22815;&#29983;&#25104;&#26377;&#24847;&#20041;&#21644;&#26080;&#20559;&#30340;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26041;&#27861;&#24050;&#32463;&#23581;&#35797;&#36890;&#36807;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#39044;&#27979;&#24402;&#22240;&#20110;&#20854;&#36755;&#20837;&#29305;&#24449;&#26469;&#35299;&#37322;DNN&#12290;&#20854;&#20013;&#19968;&#20010;&#30740;&#31350;&#20805;&#20998;&#30340;&#24402;&#22240;&#26041;&#27861;&#26159;Integrated Gradients&#65288;IG&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36873;&#25321;IG&#30340;&#22522;&#32447;&#26159;&#22312;&#19981;&#21516;&#24773;&#26223;&#19979;&#29983;&#25104;&#26377;&#24847;&#20041;&#21644;&#26080;&#20559;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#30340;&#20851;&#38190;&#32771;&#34385;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21033;&#29992;&#21333;&#19968;&#22522;&#32447;&#30340;&#20570;&#27861;&#26410;&#33021;&#23454;&#29616;&#36825;&#20010;&#24895;&#26395;&#65292;&#22240;&#27492;&#38656;&#35201;&#22810;&#20010;&#22522;&#32447;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;IG&#19982;&#22885;&#26364;&#8212;&#22799;&#26222;&#21033;&#65288;Aumann-Shapley&#65289;&#20215;&#20540;&#20043;&#38388;&#30340;&#20869;&#22312;&#32852;&#31995;&#24418;&#25104;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#35270;&#35282;&#65292;&#37325;&#26032;&#24605;&#32771;&#20102;&#22522;&#32447;&#30340;&#35774;&#35745;&#12290;&#22312;&#26576;&#20123;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20986;&#19968;&#32452;&#22522;&#32447;&#19982;Shapley Value&#20013;&#30340;&#32852;&#30431;&#30456;&#23545;&#24212;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#32447;&#26500;&#24314;&#26041;&#27861;&#65292;&#31216;&#20026;Shapley Integrated Gradients&#65288;SIG&#65289;&#65292;&#36890;&#36807;&#27604;&#20363;&#25277;&#26679;&#26469;&#25628;&#32034;&#19968;&#32452;&#22522;&#32447;&#65292;&#20197;&#37096;&#20998;&#27169;&#25311;Shapley Value&#30340;&#35745;&#31639;&#36335;&#24452;&#12290;&#22312;GridWorl&#19978;&#36827;&#34892;&#20102;&#27169;&#25311;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous approaches have attempted to interpret deep neural networks (DNNs) by attributing the prediction of DNN to its input features. One of the well-studied attribution methods is Integrated Gradients (IG). Specifically, the choice of baselines for IG is a critical consideration for generating meaningful and unbiased explanations for model predictions in different scenarios. However, current practice of exploiting a single baseline fails to fulfill this ambition, thus demanding multiple baselines. Fortunately, the inherent connection between IG and Aumann-Shapley Value forms a unique perspective to rethink the design of baselines. Under certain hypothesis, we theoretically analyse that a set of baseline aligns with the coalitions in Shapley Value. Thus, we propose a novel baseline construction method called Shapley Integrated Gradients (SIG) that searches for a set of baselines by proportional sampling to partly simulate the computation path of Shapley Value. Simulations on GridWorl
&lt;/p&gt;</description></item><item><title>&#35780;&#22996;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#35780;&#20272;&#26694;&#26550;&#21644;&#24037;&#20855;&#21253;&#65292;&#26088;&#22312;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#20013;&#19981;&#21516;&#31995;&#32479;&#21644;&#25351;&#26631;&#30340;&#35780;&#20272;&#25361;&#25112;&#65292;&#24182;&#25913;&#36827;&#24230;&#37327;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.02040</link><description>&lt;p&gt;
&#35780;&#22996;&#65306;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
Jury: A Comprehensive Evaluation Toolkit. (arXiv:2310.02040v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02040
&lt;/p&gt;
&lt;p&gt;
&#35780;&#22996;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#35780;&#20272;&#26694;&#26550;&#21644;&#24037;&#20855;&#21253;&#65292;&#26088;&#22312;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#20013;&#19981;&#21516;&#31995;&#32479;&#21644;&#25351;&#26631;&#30340;&#35780;&#20272;&#25361;&#25112;&#65292;&#24182;&#25913;&#36827;&#24230;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#25198;&#28436;&#30528;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#20316;&#20026;&#20219;&#20309;&#22522;&#20110;&#39044;&#27979;&#30340;&#31995;&#32479;&#30340;&#22522;&#26412;&#27169;&#22359;&#12290;&#28982;&#32780;&#65292;&#22823;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#21644;&#21508;&#31181;&#25351;&#26631;&#30340;&#21457;&#23637;&#23548;&#33268;&#20102;&#20351;&#29992;&#19981;&#21516;&#25351;&#26631;&#35780;&#20272;&#19981;&#21516;&#31995;&#32479;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#35780;&#22996;&#30340;&#24037;&#20855;&#21253;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35780;&#20272;&#26694;&#26550;&#21644;&#26631;&#20934;&#21270;&#32467;&#26500;&#65292;&#20197;&#36328;&#19981;&#21516;&#20219;&#21153;&#21644;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#12290;&#35780;&#22996;&#30340;&#30446;&#26631;&#26159;&#26631;&#20934;&#21270;&#21644;&#25913;&#36827;&#25152;&#26377;&#31995;&#32479;&#30340;&#24230;&#37327;&#35780;&#20272;&#65292;&#24182;&#24110;&#21161;&#31038;&#21306;&#20811;&#26381;&#35780;&#20272;&#20013;&#30340;&#25361;&#25112;&#12290;&#33258;&#35780;&#22996;&#30340;&#24320;&#28304;&#21457;&#24067;&#20197;&#26469;&#65292;&#24050;&#32463;&#21560;&#24341;&#20102;&#24191;&#22823;&#29992;&#25143;&#65292;&#21487;&#22312;https://github.com/obss/jury &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluation plays a critical role in deep learning as a fundamental block of any prediction-based system. However, the vast number of Natural Language Processing (NLP) tasks and the development of various metrics have led to challenges in evaluating different systems with different metrics. To address these challenges, we introduce jury, a toolkit that provides a unified evaluation framework with standardized structures for performing evaluation across different tasks and metrics. The objective of jury is to standardize and improve metric evaluation for all systems and aid the community in overcoming the challenges in evaluation. Since its open-source release, jury has reached a wide audience and is available at https://github.com/obss/jury.
&lt;/p&gt;</description></item><item><title>ComSD&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#21512;&#29702;&#30340;&#20114;&#20449;&#24687;&#20272;&#35745;&#21644;&#21160;&#24577;&#21152;&#26435;&#30340;&#20869;&#22312;&#22870;&#21169;&#26469;&#24179;&#34913;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#20013;&#30340;&#34892;&#20026;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.17203</link><description>&lt;p&gt;
ComSD: &#22312;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#20013;&#24179;&#34913;&#34892;&#20026;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
ComSD: Balancing Behavioral Quality and Diversity in Unsupervised Skill Discovery. (arXiv:2309.17203v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17203
&lt;/p&gt;
&lt;p&gt;
ComSD&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#21512;&#29702;&#30340;&#20114;&#20449;&#24687;&#20272;&#35745;&#21644;&#21160;&#24577;&#21152;&#26435;&#30340;&#20869;&#22312;&#22870;&#21169;&#26469;&#24179;&#34913;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#20013;&#30340;&#34892;&#20026;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#30340;&#29702;&#24819;&#26041;&#27861;&#33021;&#22815;&#22312;&#27809;&#26377;&#22806;&#37096;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#22810;&#26679;&#19988;&#21512;&#26684;&#30340;&#25216;&#33021;&#65292;&#21516;&#26102;&#21457;&#29616;&#30340;&#25216;&#33021;&#38598;&#33021;&#22815;&#20197;&#21508;&#31181;&#26041;&#24335;&#39640;&#25928;&#22320;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Contrastive multi-objectives Skill Discovery (ComSD)&#65292;&#36890;&#36807;&#26356;&#21512;&#29702;&#30340;&#20114;&#20449;&#24687;&#20272;&#35745;&#21644;&#21160;&#24577;&#21152;&#26435;&#30340;&#20869;&#22312;&#22870;&#21169;&#26469;&#20943;&#36731;&#21457;&#29616;&#30340;&#34892;&#20026;&#22312;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning diverse and qualified behaviors for utilization and adaptation without supervision is a key ability of intelligent creatures. Ideal unsupervised skill discovery methods are able to produce diverse and qualified skills in the absence of extrinsic reward, while the discovered skill set can efficiently adapt to downstream tasks in various ways. Maximizing the Mutual Information (MI) between skills and visited states can achieve ideal skill-conditioned behavior distillation in theory. However, it's difficult for recent advanced methods to well balance behavioral quality (exploration) and diversity (exploitation) in practice, which may be attributed to the unreasonable MI estimation by their rigid intrinsic reward design. In this paper, we propose Contrastive multi-objectives Skill Discovery (ComSD) which tries to mitigate the quality-versus-diversity conflict of discovered behaviors through a more reasonable MI estimation and a dynamically weighted intrinsic reward. ComSD proposes
&lt;/p&gt;</description></item><item><title>MaGNet&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#33021;&#22815;&#39034;&#24207;&#22320;&#25972;&#21512;&#19981;&#21516;&#39034;&#24207;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#26377;&#24433;&#21709;&#21147;&#30340;&#32039;&#20945;&#22270;&#32467;&#26500;&#25552;&#20379;&#26377;&#24847;&#20041;&#19988;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.13459</link><description>&lt;p&gt;
&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#25972;&#21512;&#23616;&#37096;&#21644;&#20840;&#23616;&#20449;&#24687;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Model-Agnostic Graph Neural Network for Integrating Local and Global Information. (arXiv:2309.13459v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13459
&lt;/p&gt;
&lt;p&gt;
MaGNet&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#33021;&#22815;&#39034;&#24207;&#22320;&#25972;&#21512;&#19981;&#21516;&#39034;&#24207;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#26377;&#24433;&#21709;&#21147;&#30340;&#32039;&#20945;&#22270;&#32467;&#26500;&#25552;&#20379;&#26377;&#24847;&#20041;&#19988;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#21508;&#31181;&#20197;&#22270;&#20026;&#37325;&#28857;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;GNN&#23384;&#22312;&#20004;&#20010;&#37325;&#35201;&#38480;&#21046;&#65306;&#30001;&#20110;&#40657;&#30418;&#29305;&#24615;&#65292;&#32467;&#26524;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65307;&#26080;&#27861;&#23398;&#20064;&#19981;&#21516;&#39034;&#24207;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MaGNet&#65289;&#26694;&#26550;&#65292;&#33021;&#22815;&#39034;&#24207;&#22320;&#25972;&#21512;&#19981;&#21516;&#39034;&#24207;&#30340;&#20449;&#24687;&#65292;&#20174;&#39640;&#38454;&#37051;&#23621;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#26377;&#24433;&#21709;&#21147;&#30340;&#32039;&#20945;&#22270;&#32467;&#26500;&#25552;&#20379;&#26377;&#24847;&#20041;&#19988;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;&#29305;&#21035;&#22320;&#65292;MaGNet&#30001;&#20004;&#20010;&#32452;&#20214;&#32452;&#25104;&#65306;&#22270;&#25299;&#25169;&#19979;&#22797;&#26434;&#20851;&#31995;&#30340;&#28508;&#22312;&#34920;&#31034;&#30340;&#20272;&#35745;&#27169;&#22411;&#21644;&#35782;&#21035;&#26377;&#24433;&#21709;&#21147;&#30340;&#33410;&#28857;&#12289;&#36793;&#21644;&#37325;&#35201;&#33410;&#28857;&#29305;&#24449;&#30340;&#35299;&#37322;&#27169;&#22411;&#12290;&#20174;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#36890;&#36807;&#32463;&#39564;Rademacher&#22797;&#26434;&#24230;&#24314;&#31435;&#20102;MaGNet&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have achieved promising performance in a variety of graph-focused tasks. Despite their success, existing GNNs suffer from two significant limitations: a lack of interpretability in results due to their black-box nature, and an inability to learn representations of varying orders. To tackle these issues, we propose a novel Model-agnostic Graph Neural Network (MaGNet) framework, which is able to sequentially integrate information of various orders, extract knowledge from high-order neighbors, and provide meaningful and interpretable results by identifying influential compact graph structures. In particular, MaGNet consists of two components: an estimation model for the latent representation of complex relationships under graph topology, and an interpretation model that identifies influential nodes, edges, and important node features. Theoretically, we establish the generalization error bound for MaGNet via empirical Rademacher complexity, and showcase its pow
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Auto-UI&#30340;&#22810;&#27169;&#24577;&#21160;&#20316;&#38142;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#30452;&#25509;&#19982;&#30028;&#38754;&#20132;&#20114;&#65292;&#36991;&#20813;&#20102;&#29615;&#22659;&#35299;&#26512;&#25110;&#20381;&#36182;&#20110;&#24212;&#29992;&#31243;&#24207;API&#30340;&#38656;&#35201;&#65292;&#24182;&#24341;&#20837;&#20102;&#21160;&#20316;&#38142;&#25216;&#26415;&#26469;&#24110;&#21161;&#27169;&#22411;&#36827;&#34892;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2309.11436</link><description>&lt;p&gt;
&#20320;&#20165;&#20851;&#27880;&#23631;&#24149;&#65306;&#22810;&#27169;&#24577;&#21160;&#20316;&#38142;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
You Only Look at Screens: Multimodal Chain-of-Action Agents. (arXiv:2309.11436v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Auto-UI&#30340;&#22810;&#27169;&#24577;&#21160;&#20316;&#38142;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#30452;&#25509;&#19982;&#30028;&#38754;&#20132;&#20114;&#65292;&#36991;&#20813;&#20102;&#29615;&#22659;&#35299;&#26512;&#25110;&#20381;&#36182;&#20110;&#24212;&#29992;&#31243;&#24207;API&#30340;&#38656;&#35201;&#65292;&#24182;&#24341;&#20837;&#20102;&#21160;&#20316;&#38142;&#25216;&#26415;&#26469;&#24110;&#21161;&#27169;&#22411;&#36827;&#34892;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#29992;&#25143;&#30028;&#38754;&#65288;UI&#65289;&#26426;&#22120;&#20154;&#26088;&#22312;&#36890;&#36807;&#19982;&#29992;&#25143;&#30028;&#38754;&#36827;&#34892;&#20132;&#20114;&#65292;&#23454;&#29616;&#20219;&#21153;&#33258;&#21160;&#21270;&#65292;&#26080;&#38656;&#25163;&#21160;&#24178;&#39044;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#65292;&#20197;&#22312;&#22810;&#26679;&#29615;&#22659;&#20013;&#26377;&#25928;&#21442;&#19982;&#12290;&#20026;&#20102;&#31526;&#21512;LLM&#30340;&#36755;&#20837;-&#36755;&#20986;&#35201;&#27714;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#27801;&#30418;&#29615;&#22659;&#20013;&#24320;&#21457;&#65292;&#20381;&#36182;&#20110;&#22806;&#37096;&#24037;&#20855;&#21644;&#24212;&#29992;&#31243;&#24207;&#29305;&#23450;&#30340;API&#23558;&#29615;&#22659;&#35299;&#26512;&#20026;&#25991;&#26412;&#20803;&#32032;&#65292;&#24182;&#35299;&#37322;&#39044;&#27979;&#30340;&#21160;&#20316;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#21463;&#21040;&#25512;&#29702;&#25928;&#29575;&#20302;&#21644;&#38169;&#35823;&#20256;&#25773;&#39118;&#38505;&#30340;&#22256;&#25200;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Auto-UI&#65292;&#19968;&#31181;&#22810;&#27169;&#24577;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#30452;&#25509;&#19982;&#30028;&#38754;&#20132;&#20114;&#65292;&#36991;&#20813;&#20102;&#23545;&#29615;&#22659;&#35299;&#26512;&#25110;&#20381;&#36182;&#20110;&#24212;&#29992;&#31243;&#24207;&#30456;&#20851;&#30340;API&#30340;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#20316;&#38142;&#25216;&#26415;&#65292;&#21033;&#29992;&#19968;&#31995;&#21015;&#20013;&#38388;&#20808;&#21069;&#21160;&#20316;&#21382;&#21490;&#21644;&#26410;&#26469;&#21160;&#20316;&#35745;&#21010;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#36827;&#34892;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous user interface (UI) agents aim to facilitate task automation by interacting with the user interface without manual intervention. Recent studies have investigated eliciting the capabilities of large language models (LLMs) for effective engagement in diverse environments. To align with the input-output requirement of LLMs, existing approaches are developed under a sandbox setting where they rely on external tools and application-specific APIs to parse the environment into textual elements and interpret the predicted actions. Consequently, those approaches often grapple with inference inefficiency and error propagation risks. To mitigate the challenges, we introduce Auto-UI, a multimodal solution that directly interacts with the interface, bypassing the need for environment parsing or reliance on application-dependent APIs. Moreover, we propose a chain-of-action technique -leveraging a series of intermediate previous action histories and future action plans -- to help the age
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#20248;&#21270;&#22823;&#32928;&#30340;&#20998;&#21106;&#32467;&#26524;&#65292;&#24182;&#32467;&#21512;&#20808;&#36827;&#30340;&#34920;&#38754;&#37325;&#26500;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#22823;&#32928;3D&#24418;&#29366;&#30340;&#31934;&#21270;&#24674;&#22797;&#12290;</title><link>http://arxiv.org/abs/2309.08289</link><description>&lt;p&gt;
&#21033;&#29992;&#28857;&#25193;&#25955;&#27169;&#22411;&#23545;&#22823;&#32928;&#30340;3D&#24418;&#29366;&#36827;&#34892;&#31934;&#21270;&#20197;&#29983;&#25104;&#25968;&#23383;&#24187;&#24433;
&lt;/p&gt;
&lt;p&gt;
Large Intestine 3D Shape Refinement Using Point Diffusion Models for Digital Phantom Generation. (arXiv:2309.08289v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#20248;&#21270;&#22823;&#32928;&#30340;&#20998;&#21106;&#32467;&#26524;&#65292;&#24182;&#32467;&#21512;&#20808;&#36827;&#30340;&#34920;&#38754;&#37325;&#26500;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#22823;&#32928;3D&#24418;&#29366;&#30340;&#31934;&#21270;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#24314;&#27169;&#20154;&#20307;&#22120;&#23448;&#22312;&#26500;&#24314;&#34394;&#25311;&#25104;&#20687;&#35797;&#39564;&#30340;&#35745;&#31639;&#20223;&#30495;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20174;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#20013;&#29983;&#25104;&#35299;&#21078;&#23398;&#19978;&#21487;&#20449;&#30340;&#22120;&#23448;&#34920;&#38754;&#37325;&#24314;&#20173;&#28982;&#23545;&#20154;&#20307;&#32467;&#26500;&#20013;&#30340;&#35768;&#22810;&#22120;&#23448;&#26469;&#35828;&#26159;&#20010;&#25361;&#25112;&#12290;&#22312;&#22788;&#29702;&#22823;&#32928;&#26102;&#65292;&#36825;&#20010;&#25361;&#25112;&#23588;&#20026;&#26126;&#26174;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#26469;&#20248;&#21270;&#22823;&#32928;&#20998;&#21106;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#22120;&#23448;&#34920;&#31034;&#20026;&#20174;3D&#20998;&#21106;&#25513;&#27169;&#34920;&#38754;&#37319;&#26679;&#24471;&#21040;&#30340;&#28857;&#20113;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20998;&#23618;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#33719;&#24471;&#22120;&#23448;&#24418;&#29366;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#20998;&#23618;&#28508;&#22312;&#31354;&#38388;&#20013;&#35757;&#32451;&#20004;&#20010;&#26465;&#20214;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#26469;&#36827;&#34892;&#24418;&#29366;&#31934;&#21270;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36824;&#32467;&#21512;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#34920;&#38754;&#37325;&#26500;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#24418;&#29366;&#30340;&#26356;&#22909;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate 3D modeling of human organs plays a crucial role in building computational phantoms for virtual imaging trials. However, generating anatomically plausible reconstructions of organ surfaces from computed tomography scans remains challenging for many structures in the human body. This challenge is particularly evident when dealing with the large intestine. In this study, we leverage recent advancements in geometric deep learning and denoising diffusion probabilistic models to refine the segmentation results of the large intestine. We begin by representing the organ as point clouds sampled from the surface of the 3D segmentation mask. Subsequently, we employ a hierarchical variational autoencoder to obtain global and local latent representations of the organ's shape. We train two conditional denoising diffusion models in the hierarchical latent space to perform shape refinement. To further enhance our method, we incorporate a state-of-the-art surface reconstruction model, allowin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;BEVTrack&#30340;&#31616;&#21333;&#21364;&#24378;&#22823;&#30340;&#22522;&#32447;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#28857;&#20113;&#20013;3D&#21333;&#29289;&#20307;&#36319;&#36394;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#23558;&#28857;&#20113;&#36716;&#25442;&#20026;&#40479;&#30640;&#22270;&#34920;&#31034;&#65292;&#24182;&#36827;&#34892;&#31616;&#21333;&#30340;&#36880;&#20803;&#32032;&#25805;&#20316;&#65292;BEVTrack&#33021;&#22815;&#32534;&#30721;&#31354;&#38388;&#37051;&#36817;&#24615;&#21644;&#25429;&#25417;&#36816;&#21160;&#32447;&#32034;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#30340;&#36319;&#36394;&#12290;</title><link>http://arxiv.org/abs/2309.02185</link><description>&lt;p&gt;
BEVTrack&#65306;&#19968;&#31181;&#38024;&#23545;&#40479;&#30640;&#22270;&#20013;3D&#21333;&#29289;&#20307;&#36319;&#36394;&#30340;&#31616;&#21333;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
BEVTrack: A Simple Baseline for 3D Single Object Tracking in Birds's-Eye-View. (arXiv:2309.02185v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;BEVTrack&#30340;&#31616;&#21333;&#21364;&#24378;&#22823;&#30340;&#22522;&#32447;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#28857;&#20113;&#20013;3D&#21333;&#29289;&#20307;&#36319;&#36394;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#23558;&#28857;&#20113;&#36716;&#25442;&#20026;&#40479;&#30640;&#22270;&#34920;&#31034;&#65292;&#24182;&#36827;&#34892;&#31616;&#21333;&#30340;&#36880;&#20803;&#32032;&#25805;&#20316;&#65292;BEVTrack&#33021;&#22815;&#32534;&#30721;&#31354;&#38388;&#37051;&#36817;&#24615;&#21644;&#25429;&#25417;&#36816;&#21160;&#32447;&#32034;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#30340;&#36319;&#36394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22806;&#35266;&#21464;&#21270;&#12289;&#24178;&#25200;&#29289;&#21644;&#28857;&#20113;&#30340;&#39640;&#31232;&#30095;&#24615;&#65292;&#28857;&#20113;&#20013;&#30340;3D&#21333;&#29289;&#20307;&#36319;&#36394;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#20013;&#65292;&#30446;&#26631;&#29289;&#20307;&#36890;&#24120;&#22312;&#36830;&#32493;&#24103;&#20013;&#20445;&#25345;&#31354;&#38388;&#37051;&#36817;&#24615;&#65292;&#20027;&#35201;&#27700;&#24179;&#31227;&#21160;&#12290;&#36825;&#31181;&#31354;&#38388;&#36830;&#32493;&#24615;&#20026;&#30446;&#26631;&#23450;&#20301;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#36319;&#36394;&#22120;&#36890;&#24120;&#37319;&#29992;&#28857;&#32423;&#34920;&#31034;&#65292;&#38590;&#20197;&#26377;&#25928;&#21033;&#29992;&#36825;&#31181;&#30693;&#35782;&#65292;&#22240;&#20026;&#36825;&#31181;&#34920;&#31034;&#30340;&#19981;&#35268;&#21017;&#26684;&#24335;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#38656;&#35201;&#31934;&#24515;&#35774;&#35745;&#24182;&#35299;&#20915;&#22810;&#20010;&#23376;&#20219;&#21153;&#26469;&#24314;&#31435;&#31354;&#38388;&#23545;&#24212;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;BEVTrack&#65292;&#19968;&#20010;&#31616;&#21333;&#20294;&#24378;&#22823;&#30340;&#29992;&#20110;3D&#21333;&#29289;&#20307;&#36319;&#36394;&#30340;&#22522;&#32447;&#26694;&#26550;&#12290;&#36890;&#36807;&#23558;&#36830;&#32493;&#30340;&#28857;&#20113;&#36716;&#25442;&#20026;&#24120;&#35265;&#30340;&#40479;&#30640;&#22270;&#34920;&#31034;&#65292;BEVTrack&#36890;&#36807;&#31616;&#21333;&#30340;&#36880;&#20803;&#32032;&#25805;&#20316;&#33258;&#28982;&#22320;&#32534;&#30721;&#20102;&#31354;&#38388;&#37051;&#36817;&#24615;&#65292;&#24182;&#28789;&#27963;&#22320;&#25429;&#25417;&#20102;&#36319;&#36394;&#30340;&#36816;&#21160;&#32447;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D single object tracking (SOT) in point clouds is still a challenging problem due to appearance variation, distractors, and high sparsity of point clouds. Notably, in autonomous driving scenarios, the target object typically maintains spatial adjacency across consecutive frames, predominantly moving horizontally. This spatial continuity offers valuable prior knowledge for target localization. However, existing trackers, which often employ point-wise representations, struggle to efficiently utilize this knowledge owing to the irregular format of such representations. Consequently, they require elaborate designs and solving multiple subtasks to establish spatial correspondence. In this paper, we introduce BEVTrack, a simple yet strong baseline framework for 3D SOT. After converting consecutive point clouds into the common Bird's-Eye-View representation, BEVTrack inherently encodes spatial proximity and adeptly captures motion cues for tracking via a simple element-wise operation and con
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37325;&#26032;&#35782;&#21035;&#21311;&#21517;&#20010;&#20154;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#12289;&#36755;&#20837;&#38271;&#24230;&#21644;&#25351;&#20196;&#35843;&#25972;&#26159;&#26368;&#37325;&#35201;&#30340;&#20915;&#23450;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2308.11103</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20877;&#35782;&#21035;&#33021;&#21147;&#65306;&#21311;&#21517;&#38754;&#20020;&#39118;&#38505;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language Models. (arXiv:2308.11103v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37325;&#26032;&#35782;&#21035;&#21311;&#21517;&#20010;&#20154;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#12289;&#36755;&#20837;&#38271;&#24230;&#21644;&#25351;&#20196;&#35843;&#25972;&#26159;&#26368;&#37325;&#35201;&#30340;&#20915;&#23450;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27431;&#30431;&#21644;&#29790;&#22763;&#65292;&#27861;&#38498;&#35009;&#20915;&#20013;&#33258;&#28982;&#20154;&#21644;&#27861;&#20154;&#30340;&#21311;&#21517;&#24615;&#26159;&#38544;&#31169;&#20445;&#25252;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#23545;&#20110;&#21311;&#21517;&#20154;&#21592;&#30340;&#22823;&#35268;&#27169;&#20877;&#35782;&#21035;&#30340;&#25285;&#24551;&#26085;&#30410;&#22686;&#38271;&#12290;&#26681;&#25454;&#29790;&#22763;&#32852;&#37030;&#26368;&#39640;&#27861;&#38498;&#30340;&#35201;&#27714;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#29790;&#22763;&#32852;&#37030;&#26368;&#39640;&#27861;&#38498;&#30340;&#23454;&#38469;&#27861;&#24459;&#25968;&#25454;&#26500;&#24314;&#20102;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#65292;&#26469;&#25506;&#35752;LLMs&#37325;&#26032;&#35782;&#21035;&#27861;&#38498;&#35009;&#20915;&#20013;&#20010;&#20154;&#30340;&#28508;&#21147;&#12290;&#22312;&#26368;&#21021;&#30340;&#23454;&#39564;&#20043;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#32463;&#36807;&#21311;&#21517;&#21270;&#22788;&#29702;&#30340;&#32500;&#22522;&#30334;&#31185;&#25968;&#25454;&#38598;&#65292;&#20316;&#20026;&#19968;&#20010;&#26356;&#20005;&#26684;&#30340;&#27979;&#35797;&#22330;&#22320;&#26469;&#36827;&#19968;&#27493;&#30740;&#31350;&#30740;&#31350;&#32467;&#26524;&#12290;&#36890;&#36807;&#24341;&#20837;&#24182;&#24212;&#29992;&#25991;&#26412;&#20013;&#20877;&#35782;&#21035;&#20154;&#21592;&#30340;&#26032;&#20219;&#21153;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#26032;&#30340;&#24615;&#33021;&#34913;&#37327;&#25351;&#26631;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#20998;&#26512;&#20102;&#24433;&#21709;&#25104;&#21151;&#20877;&#35782;&#21035;&#30340;&#22240;&#32032;&#65292;&#30830;&#23450;&#27169;&#22411;&#22823;&#23567;&#12289;&#36755;&#20837;&#38271;&#24230;&#21644;&#25351;&#20196;&#35843;&#25972;&#26159;&#26368;&#37325;&#35201;&#30340;&#20915;&#23450;&#22240;&#32032;&#20043;&#19968;&#12290;&#23613;&#31649;&#22312;&#21311;&#21517;&#21270;&#22788;&#29702;&#21518;&#65292;LLMs&#22312;&#37325;&#26032;&#35782;&#21035;&#19978;&#30340;&#25104;&#21151;&#29575;&#24456;&#39640;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20173;&#28982;&#23384;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anonymity of both natural and legal persons in court rulings is a critical aspect of privacy protection in the European Union and Switzerland. With the advent of LLMs, concerns about large-scale re-identification of anonymized persons are growing. In accordance with the Federal Supreme Court of Switzerland, we explore the potential of LLMs to re-identify individuals in court rulings by constructing a proof-of-concept using actual legal data from the Swiss federal supreme court. Following the initial experiment, we constructed an anonymized Wikipedia dataset as a more rigorous testing ground to further investigate the findings. With the introduction and application of the new task of re-identifying people in texts, we also introduce new metrics to measure performance. We systematically analyze the factors that influence successful re-identifications, identifying model size, input length, and instruction tuning among the most critical determinants. Despite high re-identification rates on
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#23884;&#20837;&#27169;&#22411;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#22240;&#31361;&#21464;&#22312;&#30284;&#30151;&#20013;&#30340;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#31561;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#20256;&#32479;&#21644;&#26368;&#26032;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.14361</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;LSTM&#12289;BiLSTM&#12289;CNN&#12289;GRU&#21644;GloVe&#30340;&#28151;&#21512;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#22522;&#22240;&#31361;&#21464;&#22312;&#30284;&#30151;&#20013;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
A Hybrid Machine Learning Model for Classifying Gene Mutations in Cancer using LSTM, BiLSTM, CNN, GRU, and GloVe. (arXiv:2307.14361v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#23884;&#20837;&#27169;&#22411;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#22240;&#31361;&#21464;&#22312;&#30284;&#30151;&#20013;&#30340;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#31561;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#20256;&#32479;&#21644;&#26368;&#26032;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#27169;&#22411;&#65292;&#23558;LSTM&#12289;BiLSTM&#12289;CNN&#12289;GRU&#21644;GloVe&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#22312;Kaggle&#30340;&#8220;&#20010;&#24615;&#21270;&#21307;&#23398;&#65306;&#37325;&#26032;&#23450;&#20041;&#30284;&#30151;&#27835;&#30103;&#8221;&#25968;&#25454;&#38598;&#20013;&#23545;&#22522;&#22240;&#31361;&#21464;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#19982;BERT&#12289;Electra&#12289;Roberta&#12289;XLNet&#12289;Distilbert&#20197;&#21450;&#23427;&#20204;&#30340;LSTM&#38598;&#25104;&#31561;&#30693;&#21517;&#36716;&#25442;&#22120;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#12289;F1&#20998;&#25968;&#21644;&#22343;&#26041;&#35823;&#24046;&#26041;&#38754;&#37117;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23427;&#36824;&#38656;&#35201;&#36739;&#23569;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#23454;&#29616;&#20102;&#24615;&#33021;&#21644;&#25928;&#29575;&#30340;&#23436;&#32654;&#32467;&#21512;&#12290;&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;&#38598;&#25104;&#27169;&#22411;&#22312;&#22522;&#22240;&#31361;&#21464;&#20998;&#31867;&#31561;&#22256;&#38590;&#20219;&#21153;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents an ensemble model combining LSTM, BiLSTM, CNN, GRU, and GloVe to classify gene mutations using Kaggle's Personalized Medicine: Redefining Cancer Treatment dataset. The results were compared against well-known transformers like as BERT, Electra, Roberta, XLNet, Distilbert, and their LSTM ensembles. Our model outperformed all other models in terms of accuracy, precision, recall, F1 score, and Mean Squared Error. Surprisingly, it also needed less training time, resulting in a perfect combination of performance and efficiency. This study demonstrates the utility of ensemble models for difficult tasks such as gene mutation classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#26131;&#20986;&#38169;&#27979;&#35797;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#27979;&#35797;&#20195;&#30721;&#26469;&#39044;&#27979;&#27979;&#35797;&#30340;&#20462;&#22797;&#31867;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;UniXcoder&#20248;&#20110;CodeBERT&#12290;</title><link>http://arxiv.org/abs/2307.00012</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#40657;&#30418;&#39044;&#27979;&#26131;&#20986;&#38169;&#27979;&#35797;&#20462;&#22797;&#31867;&#21035;
&lt;/p&gt;
&lt;p&gt;
Black-Box Prediction of Flaky Test Fix Categories Using Language Models. (arXiv:2307.00012v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#26131;&#20986;&#38169;&#27979;&#35797;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#27979;&#35797;&#20195;&#30721;&#26469;&#39044;&#27979;&#27979;&#35797;&#30340;&#20462;&#22797;&#31867;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;UniXcoder&#20248;&#20110;CodeBERT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26131;&#20986;&#38169;&#27979;&#35797;&#20250;&#22312;&#30456;&#21516;&#36719;&#20214;&#29256;&#26412;&#30340;&#27979;&#35797;&#19979;&#38750;&#30830;&#23450;&#24615;&#22320;&#36890;&#36807;&#25110;&#22833;&#36133;&#65292;&#24341;&#36215;&#28151;&#20081;&#24182;&#28010;&#36153;&#24320;&#21457;&#32773;&#26102;&#38388;&#12290;&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#34987;&#29992;&#20110;&#39044;&#27979;&#26131;&#20986;&#38169;&#24615;&#21450;&#20854;&#26681;&#26412;&#21407;&#22240;&#65292;&#20294;&#22312;&#25552;&#20379;&#20462;&#22797;&#25903;&#25345;&#26041;&#38754;&#20173;&#26377;&#36739;&#23569;&#24037;&#20316;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#20165;&#20998;&#26512;&#27979;&#35797;&#20195;&#30721;&#33258;&#21160;&#29983;&#25104;13&#20010;&#20462;&#22797;&#31867;&#21035;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#24182;&#35757;&#32451;&#27169;&#22411;&#26469;&#39044;&#27979;&#26131;&#20986;&#38169;&#27979;&#35797;&#30340;&#20462;&#22797;&#31867;&#21035;&#12290;&#34429;&#28982;&#22312;&#24403;&#21069;&#38454;&#27573;&#20934;&#30830;&#39044;&#27979;&#20462;&#22797;&#26412;&#36523;&#26159;&#19981;&#29616;&#23454;&#30340;&#65292;&#20294;&#36825;&#20123;&#31867;&#21035;&#25552;&#20379;&#20102;&#20851;&#20110;&#38656;&#35201;&#26816;&#26597;&#30340;&#27979;&#35797;&#20195;&#30721;&#37096;&#20998;&#30340;&#31934;&#30830;&#25351;&#23548;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#65292;&#21363;CodeBERT&#21644;UniXcoder&#65292;&#20854;&#36755;&#20986;&#32463;&#36807;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FNN&#65289;&#25110;&#22522;&#20110;&#23402;&#29983;&#32593;&#32476;&#30340;Few Shot Learning&#65288;FSL&#65289;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UniXcoder&#22312;&#27491;&#30830;&#39044;&#27979;&#22823;&#22810;&#25968;&#20462;&#22797;&#31867;&#21035;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;CodeBERT&#12290;
&lt;/p&gt;
&lt;p&gt;
Flaky tests are problematic because they non-deterministically pass or fail for the same software version under test, causing confusion and wasting developer time. While machine learning models have been used to predict flakiness and its root causes, there is less work on providing support to fix the problem. To address this gap, we propose a framework that automatically generates labeled datasets for 13 fix categories and train models to predict the fix category of a flaky test by analyzing the test code only. Though it is unrealistic at this stage to accurately predict the fix itself, the categories provide precise guidance about what part of the test code to look at. Our approach is based on language models, namely CodeBERT and UniXcoder, whose output is fine-tuned with a Feed Forward Neural Network (FNN) or a Siamese Network-based Few Shot Learning (FSL). Our experimental results show that UniXcoder outperforms CodeBERT, in correctly predicting most of the categories of fixes a dev
&lt;/p&gt;</description></item><item><title>MultiLegalPile&#26159;&#19968;&#20010;689GB&#30340;&#22810;&#35821;&#35328;&#27861;&#24459;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#26469;&#33258;17&#20010;&#21496;&#27861;&#31649;&#36758;&#21306;&#30340;24&#31181;&#35821;&#35328;&#30340;&#19981;&#21516;&#27861;&#24459;&#25968;&#25454;&#28304;&#65292;&#20801;&#35768;&#22312;&#20844;&#24179;&#20351;&#29992;&#19979;&#38024;&#23545;&#39044;&#35757;&#32451;NLP&#27169;&#22411;&#12290;&#35813;&#35821;&#26009;&#24211;&#20026;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25552;&#20379;&#20102;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#65292;&#24182;&#22312;LexGLUE&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2306.02069</link><description>&lt;p&gt;
MultiLegalPile&#65306;689GB&#30340;&#22810;&#35821;&#35328;&#27861;&#24459;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
MultiLegalPile: A 689GB Multilingual Legal Corpus. (arXiv:2306.02069v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02069
&lt;/p&gt;
&lt;p&gt;
MultiLegalPile&#26159;&#19968;&#20010;689GB&#30340;&#22810;&#35821;&#35328;&#27861;&#24459;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#26469;&#33258;17&#20010;&#21496;&#27861;&#31649;&#36758;&#21306;&#30340;24&#31181;&#35821;&#35328;&#30340;&#19981;&#21516;&#27861;&#24459;&#25968;&#25454;&#28304;&#65292;&#20801;&#35768;&#22312;&#20844;&#24179;&#20351;&#29992;&#19979;&#38024;&#23545;&#39044;&#35757;&#32451;NLP&#27169;&#22411;&#12290;&#35813;&#35821;&#26009;&#24211;&#20026;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25552;&#20379;&#20102;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#65292;&#24182;&#22312;LexGLUE&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#23545;&#20110;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20026;&#27490;&#65292;&#19987;&#19994;&#39046;&#22495;&#65288;&#22914;&#27861;&#24459;&#65289;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#24456;&#23569;&#65292;&#32780;&#19988;&#32463;&#24120;&#20165;&#38480;&#20110;&#33521;&#35821;&#12290;&#25105;&#20204;&#25972;&#29702;&#24182;&#21457;&#24067;&#20102;MultiLegalPile&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;17&#20010;&#21496;&#27861;&#31649;&#36758;&#21306;&#30340;24&#31181;&#35821;&#35328;&#30340;689GB&#35821;&#26009;&#24211;&#12290;MultiLegalPile&#35821;&#26009;&#24211;&#21253;&#25324;&#21508;&#31181;&#35768;&#21487;&#35777;&#30340;&#19981;&#21516;&#27861;&#24459;&#25968;&#25454;&#28304;&#65292;&#20801;&#35768;&#22312;&#20844;&#24179;&#20351;&#29992;&#19979;&#38024;&#23545;&#39044;&#35757;&#32451;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#27169;&#22411;&#65292;&#23545;&#20110;Eurlex Resources&#21644;Legal mC4&#23376;&#38598;&#25317;&#26377;&#26356;&#23485;&#26494;&#30340;&#35768;&#21487;&#35777;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;RoBERTa&#27169;&#22411;&#21644;&#19968;&#20010;&#22810;&#35821;&#35328;Longformer&#30340;&#39044;&#35757;&#32451;&#65292;&#24182;&#20998;&#21035;&#22312;&#27599;&#31181;&#29305;&#23450;&#35821;&#35328;&#23376;&#38598;&#19978;&#36827;&#34892;&#20102;24&#20010;&#21333;&#35821;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;LEXTREME&#19978;&#23545;&#23427;&#20204;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;LexGLUE&#19978;&#23545;&#33521;&#35821;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#22312;LEXTREME&#19978;&#21019;&#36896;&#20102;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;(SotA)&#65292;&#33521;&#35821;&#27169;&#22411;&#21017;&#22312;LexGLUE&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;&#25105;&#20204;&#23558;&#25968;&#25454;&#38598;&#12289;&#35757;&#32451;&#27169;&#22411;&#21644;&#20195;&#30721;&#20840;&#37096;&#37322;&#25918;&#22312;&#26368;&#24320;&#25918;&#30340;&#35768;&#21487;&#35777;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large, high-quality datasets are crucial for training Large Language Models (LLMs). However, so far, there are few datasets available for specialized critical domains such as law and the available ones are often only for the English language. We curate and release MultiLegalPile, a 689GB corpus in 24 languages from 17 jurisdictions. The MultiLegalPile corpus, which includes diverse legal data sources with varying licenses, allows for pretraining NLP models under fair use, with more permissive licenses for the Eurlex Resources and Legal mC4 subsets. We pretrain two RoBERTa models and one Longformer multilingually, and 24 monolingual models on each of the language-specific subsets and evaluate them on LEXTREME. Additionally, we evaluate the English and multilingual models on LexGLUE. Our multilingual models set a new SotA on LEXTREME and our English models on LexGLUE. We release the dataset, the trained models, and all of the code under the most open possible licenses.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#23398;&#20064;&#30340;&#33410;&#28857;&#25490;&#24207;&#26041;&#27861;&#65288;MGL2Rank&#65289;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#36947;&#36335;&#32593;&#32476;&#30340;&#20016;&#23500;&#29305;&#24449;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.14375</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#22270;&#34701;&#21512;&#30340;&#36947;&#36335;&#32593;&#32476;&#33410;&#28857;&#37325;&#35201;&#24615;&#25490;&#24207;&#26041;&#27861;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning to Rank the Importance of Nodes in Road Networks Based on Multi-Graph Fusion. (arXiv:2305.14375v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#23398;&#20064;&#30340;&#33410;&#28857;&#25490;&#24207;&#26041;&#27861;&#65288;MGL2Rank&#65289;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#36947;&#36335;&#32593;&#32476;&#30340;&#20016;&#23500;&#29305;&#24449;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22478;&#24066;&#35268;&#21010;&#39046;&#22495;&#20013;&#65292;&#35782;&#21035;&#20855;&#26377;&#24378;&#20256;&#25773;&#33021;&#21147;&#30340;&#37325;&#35201;&#33410;&#28857;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#35838;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35780;&#20272;&#33410;&#28857;&#37325;&#35201;&#24615;&#30340;&#26041;&#27861;&#20165;&#32771;&#34385;&#25299;&#25169;&#20449;&#24687;&#21644;&#20132;&#36890;&#27969;&#37327;&#65292;&#24573;&#30053;&#20102;&#36947;&#36335;&#32593;&#32476;&#30340;&#22810;&#26679;&#24615;&#29305;&#24449;&#65292;&#22914;&#36710;&#36947;&#25968;&#37327;&#21644;&#36947;&#36335;&#27573;&#30340;&#24179;&#22343;&#36895;&#24230;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#23398;&#20064;&#30340;&#33410;&#28857;&#25490;&#24207;&#26041;&#27861;&#65288;MGL2Rank&#65289;&#65292;&#23427;&#38598;&#25104;&#20102;&#36947;&#36335;&#32593;&#32476;&#30340;&#20016;&#23500;&#29305;&#24449;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#19968;&#20010;&#37319;&#26679;&#31639;&#27861;&#65288;MGWalk&#65289;&#65292;&#21033;&#29992;&#22810;&#22270;&#34701;&#21512;&#26469;&#24314;&#31435;&#22522;&#20110;&#23646;&#24615;&#30340;&#36947;&#36335;&#27573;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#23884;&#20837;&#27169;&#22359;&#65292;&#29992;&#20110;&#23398;&#20064;&#27599;&#20010;&#36947;&#36335;&#27573;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#26368;&#21518;&#65292;&#24471;&#21040;&#30340;&#33410;&#28857;&#34920;&#31034;&#29992;&#20110;&#23398;&#20064;&#36947;&#36335;&#27573;&#30340;&#37325;&#35201;&#24615;&#25490;&#24207;&#12290;&#25105;&#20204;&#22312;&#20013;&#22269;&#27784;&#38451;&#24066;&#21306;&#22495;&#36947;&#36335;&#32593;&#32476;&#19978;&#36827;&#34892;&#20102;&#20223;&#30495;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;MGL2Rank&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MGL2Rank&#22312;&#31934;&#24230;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#33410;&#28857;&#25490;&#24207;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying important nodes with strong propagation capabilities in road networks is a significant topic in the field of urban planning. However, existing methods for evaluating nodes importance consider only topological information and traffic volumes, ignoring the diversity of characteristics in road networks, such as the number of lanes and average speed of road segments, limiting their performance. To address this issue, this paper proposes a graph learning-based node ranking method (MGL2Rank) that integrates the rich characteristics of the road network. In this method, we first develop a sampling algorithm (MGWalk) that utilizes multi-graph fusion to establish association between road segments based on their attributes. Then, an embedding module is proposed to learn latent representation for each road segment. Finally, the obtained node representation is used to learn importance ranking of road segments. We conduct simulation experiments on the regional road network of Shenyang ci
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#25361;&#25112;&#8212;&#8212;&#33268;&#25964;&#31070;&#35805;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;(TOTAIC)&#65292;&#35813;&#31454;&#36187;&#22522;&#20110;&#12298;&#19978;&#21476;&#21367;&#36724;&#22312;&#32447;&#12299;&#20013;&#30340;&#19968;&#27454;&#21345;&#29260;&#28216;&#25103;&#12290;&#38500;&#20102;&#24212;&#23545;&#36890;&#24120;&#19982;CCG&#30456;&#20851;&#30340;&#25361;&#25112;&#22806;&#65292;&#35813;&#25361;&#25112;&#36824;&#38656;&#35201;&#38271;&#26399;&#35268;&#21010;&#21644;&#36866;&#24212;&#24615;&#12290;&#31454;&#36187;&#37319;&#29992;&#22810;&#31181;&#26041;&#27861;&#35299;&#20915;&#28216;&#25103;&#65292;&#22914;&#23545;&#25239;&#25628;&#32034;&#12289;&#21333;&#20154;&#35745;&#21010;&#21644;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#12290;&#31532;&#19968;&#23626;TOTAIC&#23558;&#22312;2023&#24180;&#30340;IEEE&#28216;&#25103;&#20250;&#35758;&#19978;&#20030;&#34892;&#12290;</title><link>http://arxiv.org/abs/2305.08234</link><description>&lt;p&gt;
&#24341;&#20837;&#33268;&#25964;&#31070;&#35805;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;
&lt;/p&gt;
&lt;p&gt;
Introducing Tales of Tribute AI Competition. (arXiv:2305.08234v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08234
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#25361;&#25112;&#8212;&#8212;&#33268;&#25964;&#31070;&#35805;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;(TOTAIC)&#65292;&#35813;&#31454;&#36187;&#22522;&#20110;&#12298;&#19978;&#21476;&#21367;&#36724;&#22312;&#32447;&#12299;&#20013;&#30340;&#19968;&#27454;&#21345;&#29260;&#28216;&#25103;&#12290;&#38500;&#20102;&#24212;&#23545;&#36890;&#24120;&#19982;CCG&#30456;&#20851;&#30340;&#25361;&#25112;&#22806;&#65292;&#35813;&#25361;&#25112;&#36824;&#38656;&#35201;&#38271;&#26399;&#35268;&#21010;&#21644;&#36866;&#24212;&#24615;&#12290;&#31454;&#36187;&#37319;&#29992;&#22810;&#31181;&#26041;&#27861;&#35299;&#20915;&#28216;&#25103;&#65292;&#22914;&#23545;&#25239;&#25628;&#32034;&#12289;&#21333;&#20154;&#35745;&#21010;&#21644;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#12290;&#31532;&#19968;&#23626;TOTAIC&#23558;&#22312;2023&#24180;&#30340;IEEE&#28216;&#25103;&#20250;&#35758;&#19978;&#20030;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#25361;&#25112;&#65292;&#21363;&#33268;&#25964;&#31070;&#35805;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;(TOTAIC)&#65292;&#35813;&#31454;&#36187;&#22522;&#20110;&#12298;&#19978;&#21476;&#21367;&#36724;&#22312;&#32447;&#12299;&#28023;&#20043;&#23707;&#31456;&#33410;&#21457;&#24067;&#30340;&#19968;&#27454;&#21452;&#20154;&#21345;&#29260;&#24314;&#35774;&#31867;&#28216;&#25103;&#12290;&#30446;&#21069;&#36824;&#27809;&#26377;&#20854;&#20182;&#28085;&#30422;&#25910;&#38598;&#21345;&#29260;&#28216;&#25103;(CCG)&#31867;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#65292;&#20063;&#20174;&#26410;&#26377;&#36807;&#38024;&#23545;&#24314;&#35774;&#21345;&#29260;&#28216;&#25103;&#30340;&#31454;&#36187;&#12290;&#22240;&#27492;&#65292;&#25104;&#21151;&#30340;&#26041;&#27861;&#38500;&#20102;&#35201;&#20811;&#26381;&#36890;&#24120;&#19982;CCG&#30456;&#20851;&#30340;&#38556;&#30861;&#65292;&#22914;&#38543;&#26426;&#24615;&#12289;&#38544;&#34255;&#20449;&#24687;&#21644;&#22823;&#30340;&#20998;&#25903;&#22240;&#32032;&#22806;&#65292;&#36824;&#38656;&#35201;&#38271;&#26399;&#35268;&#21010;&#21644;&#36866;&#24212;&#24615;&#12290;&#35813;&#28216;&#25103;&#21487;&#20197;&#37319;&#29992;&#22810;&#31181;&#26041;&#27861;&#36827;&#34892;&#35299;&#20915;&#65292;&#21253;&#25324;&#32463;&#20856;&#30340;&#23545;&#25239;&#25628;&#32034;&#12289;&#21333;&#20154;&#35745;&#21010;&#21644;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#31639;&#27861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#31454;&#36187;&#26694;&#26550;&#65292;&#25551;&#36848;&#20102;&#28216;&#25103;&#35268;&#21017;&#65292;&#24182;&#21576;&#29616;&#20102;&#31034;&#20363;AI&#20195;&#29702;&#20043;&#38388;&#30340;&#38182;&#26631;&#36187;&#32467;&#26524;&#12290;TOTAIC&#30340;&#31532;&#19968;&#23626;&#23558;&#22312;2023&#24180;IEEE&#28216;&#25103;&#20250;&#35758;&#19978;&#20030;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new AI challenge, the Tales of Tribute AI Competition (TOTAIC), based on a two-player deck-building card game released with the High Isle chapter of The Elder Scrolls Online. Currently, there is no other AI competition covering Collectible Card Games (CCG) genre, and there has never been one that targets a deck-building game. Thus, apart from usual CCG-related obstacles to overcome, like randomness, hidden information, and large branching factor, the successful approach additionally requires long-term planning and versatility. The game can be tackled with multiple approaches, including classic adversarial search, single-player planning, and Neural Networks-based algorithms. This paper introduces the competition framework, describes the rules of the game, and presents the results of a tournament between sample AI agents. The first edition of TOTAIC is hosted at the IEEE Conference on Games 2023.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#20262;&#29702;&#35780;&#20215;&#22810;&#27169;&#24577;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#21019;&#24314;&#22810;&#27169;&#24577;&#20262;&#29702;&#25968;&#25454;&#24211;&#26469;&#21327;&#21516;&#21028;&#26029;&#31995;&#32479;&#26159;&#21542;&#28385;&#36275;&#20262;&#29702;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2304.13765</link><description>&lt;p&gt;
&#36208;&#21521;&#20262;&#29702;&#22810;&#27169;&#24577;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Towards ethical multimodal systems. (arXiv:2304.13765v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20262;&#29702;&#35780;&#20215;&#22810;&#27169;&#24577;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#21019;&#24314;&#22810;&#27169;&#24577;&#20262;&#29702;&#25968;&#25454;&#24211;&#26469;&#21327;&#21516;&#21028;&#26029;&#31995;&#32479;&#26159;&#21542;&#28385;&#36275;&#20262;&#29702;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23545;&#25105;&#20204;&#31038;&#20250;&#30340;&#24433;&#21709;&#27491;&#22312;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#36895;&#24230;&#22686;&#38271;&#12290;&#20363;&#22914;&#65292;ChatGPT&#27491;&#22312;&#36827;&#34892;&#24515;&#29702;&#20581;&#24247;&#27835;&#30103;&#24212;&#29992;&#30340;&#27979;&#35797;&#65292;&#22914;Koko&#65292;Stable Diffusion&#29983;&#25104;&#30340;&#33402;&#26415;&#20316;&#21697;&#19982;&#20154;&#31867;&#33402;&#26415;&#23478;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#29978;&#33267;&#34920;&#29616;&#26356;&#22909;&#12290;&#23545;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#34892;&#20026;&#21644;&#24212;&#29992;&#30340;&#20262;&#29702;&#38382;&#39064;&#36817;&#24180;&#26469;&#19981;&#26029;&#22686;&#21152;&#65292;AI&#23545;&#40784;&#39046;&#22495;&#8212;&#8212;&#23558;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#34892;&#20026;&#24341;&#23548;&#21521;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30456;&#19968;&#33268;&#30340;&#26041;&#21521;&#8212;&#8212;&#26159;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#23376;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20262;&#29702;&#35780;&#20215;&#22810;&#27169;&#24577;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#21516;&#26102;&#20197;&#25991;&#26412;&#21644;&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#24182;&#36755;&#20986;&#25991;&#26412;&#30340;&#22810;&#27169;&#24577;&#31995;&#32479;&#65292;&#23436;&#25104;&#20316;&#20026;&#36755;&#20837;&#30340;&#21477;&#23376;&#25110;&#38382;&#39064;&#30340;&#22238;&#31572;&#12290;&#25105;&#20204;&#20998;&#20004;&#27493;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65306;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#22810;&#27169;&#24577;&#20262;&#29702;&#25968;&#25454;&#24211;&#30340;&#21019;&#24314;&#65292;&#28982;&#21518;&#20351;&#29992;&#36825;&#20010;&#25968;&#25454;&#24211;&#26469;&#21327;&#21516;&#21028;&#26029;&#31995;&#32479;&#26159;&#21542;&#28385;&#36275;&#20262;&#29702;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
The impact of artificial intelligence systems on our society is increasing at an unprecedented speed. For instance, ChatGPT is being tested in mental health treatment applications such as Koko, Stable Diffusion generates pieces of art competitive with (or outperforming) human artists, and so on. Ethical concerns regarding the behavior and applications of generative AI systems have been increasing over the past years, and the field of AI alignment - steering the behavior of AI systems towards being aligned with human values - is a rapidly growing subfield of modern AI. In this paper, we address the challenges involved in ethical evaluation of a multimodal artificial intelligence system. The multimodal systems we focus on take both text and an image as input and output text, completing the sentence or answering the question asked as input. We perform the evaluation of these models in two steps: we first discus the creation of a multimodal ethical database and then use this database to co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#22312;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#38480;&#21046;&#21644;&#23637;&#26395;&#12290;</title><link>http://arxiv.org/abs/2304.05832</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;: &#26041;&#27861;&#19982;&#25361;&#25112;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Few Shot Semantic Segmentation: a review of methodologies and open challenges. (arXiv:2304.05832v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#22312;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#38480;&#21046;&#21644;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#20998;&#21106;&#21487;&#20026;&#22270;&#20687;&#20013;&#30340;&#27599;&#20010;&#20687;&#32032;&#36171;&#20104;&#20998;&#31867;&#26631;&#31614;&#65292;&#20026;&#33258;&#21160;&#39550;&#39542;&#21644;&#26426;&#22120;&#20154;&#31561;&#39046;&#22495;&#30340;&#31361;&#30772;&#25552;&#20379;&#20102;&#22865;&#26426;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35821;&#20041;&#20998;&#21106;&#26041;&#38754;&#21462;&#24471;&#20102;&#39640;&#24230;&#20934;&#30830;&#24615;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#19968;&#20123;&#39046;&#22495;&#30001;&#20110;&#25968;&#25454;&#31232;&#32570;&#12289;&#38544;&#31169;&#38382;&#39064;&#21644;&#38656;&#35201;&#29087;&#32451;&#26631;&#27880;&#20154;&#21592;&#31561;&#21407;&#22240;&#65292;&#38590;&#20197;&#26500;&#24314;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#12290;&#23569;&#26679;&#26412;&#23398;&#20064;(FSL)&#24050;&#20316;&#20026;&#19968;&#31181;&#20801;&#35768;&#27169;&#22411;&#20174;&#23569;&#37327;&#26679;&#26412;&#20013;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#26032;&#30740;&#31350;&#27969;&#27966;&#32780;&#20986;&#29616;&#12290;&#26412;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#22312;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#21516;&#26102;&#25551;&#36848;&#20102;&#24403;&#21069;&#30340;&#38480;&#21046;&#21644;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic segmentation assigns category labels to each pixel in an image, enabling breakthroughs in fields such as autonomous driving and robotics. Deep Neural Networks have achieved high accuracies in semantic segmentation but require large training datasets. Some domains have difficulties building such datasets due to rarity, privacy concerns, and the need for skilled annotators. Few-Shot Learning (FSL) has emerged as a new research stream that allows models to learn new tasks from a few samples. This contribution provides an overview of FSL in semantic segmentation (FSS), proposes a new taxonomy, and describes current limitations and outlooks.
&lt;/p&gt;</description></item><item><title>RulE&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#23454;&#20307;&#12289;&#20851;&#31995;&#21644;&#36923;&#36753;&#35268;&#21017;&#32479;&#19968;&#34920;&#31034;&#22312;&#19968;&#20010;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#26377;&#25928;&#21033;&#29992;&#36923;&#36753;&#35268;&#21017;&#25552;&#21319;&#30693;&#35782;&#22270;&#25512;&#29702;&#12290;&#21516;&#26102;&#65292;RulE&#27880;&#20837;&#20808;&#21069;&#30340;&#36923;&#36753;&#35268;&#21017;&#20449;&#24687;&#65292;&#25913;&#36827;&#20102;&#23454;&#20307;/&#20851;&#31995;&#23884;&#20837;&#65292;&#20351;&#24471;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#20063;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2210.14905</link><description>&lt;p&gt;
RulE: &#20351;&#29992;&#35268;&#21017;&#23884;&#20837;&#30340;&#31070;&#32463;-&#31526;&#21495;&#30693;&#35782;&#22270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
RulE: Neural-Symbolic Knowledge Graph Reasoning with Rule Embedding. (arXiv:2210.14905v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14905
&lt;/p&gt;
&lt;p&gt;
RulE&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#23454;&#20307;&#12289;&#20851;&#31995;&#21644;&#36923;&#36753;&#35268;&#21017;&#32479;&#19968;&#34920;&#31034;&#22312;&#19968;&#20010;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#26377;&#25928;&#21033;&#29992;&#36923;&#36753;&#35268;&#21017;&#25552;&#21319;&#30693;&#35782;&#22270;&#25512;&#29702;&#12290;&#21516;&#26102;&#65292;RulE&#27880;&#20837;&#20808;&#21069;&#30340;&#36923;&#36753;&#35268;&#21017;&#20449;&#24687;&#65292;&#25913;&#36827;&#20102;&#23454;&#20307;/&#20851;&#31995;&#23884;&#20837;&#65292;&#20351;&#24471;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#20063;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#65288;KG&#65289;&#25512;&#29702;&#23545;&#20110;&#30693;&#35782;&#22270;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#26377;&#21407;&#21017;&#23450;&#20301;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;RulE&#65288;&#20195;&#34920;&#35268;&#21017;&#23884;&#20837;&#65289;&#65292;&#20197;&#26377;&#25928;&#21033;&#29992;&#36923;&#36753;&#35268;&#21017;&#26469;&#22686;&#24378;KG&#25512;&#29702;&#12290;&#19982;&#30693;&#35782;&#22270;&#23884;&#20837;&#65288;KGE&#65289;&#26041;&#27861;&#19981;&#21516;&#65292;RulE&#36890;&#36807;&#22312;&#32479;&#19968;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#32852;&#21512;&#34920;&#31034;&#23454;&#20307;&#12289;&#20851;&#31995;&#21644;&#36923;&#36753;&#35268;&#21017;&#65292;&#20174;&#29616;&#26377;&#19977;&#20803;&#32452;&#21644;&#19968;&#38454;&#35268;&#21017;&#20013;&#23398;&#20064;&#35268;&#21017;&#23884;&#20837;&#12290;&#22522;&#20110;&#23398;&#20064;&#21040;&#30340;&#35268;&#21017;&#23884;&#20837;&#65292;&#21487;&#20197;&#35745;&#31639;&#27599;&#20010;&#35268;&#21017;&#30340;&#32622;&#20449;&#24230;&#24471;&#20998;&#65292;&#21453;&#26144;&#20854;&#19982;&#35266;&#23519;&#21040;&#30340;&#19977;&#20803;&#32452;&#30340;&#19968;&#33268;&#24615;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#20197;&#36719;&#26041;&#24335;&#36827;&#34892;&#36923;&#36753;&#35268;&#21017;&#25512;&#29702;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#36923;&#36753;&#30340;&#33030;&#24369;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;RulE&#23558;&#20808;&#21069;&#30340;&#36923;&#36753;&#35268;&#21017;&#20449;&#24687;&#27880;&#20837;&#21040;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#20016;&#23500;&#21644;&#35268;&#33539;&#21270;&#23454;&#20307;/&#20851;&#31995;&#23884;&#20837;&#12290;&#36825;&#20063;&#20351;&#24471;&#20165;&#20351;&#29992;KGE&#30340;&#34920;&#29616;&#26356;&#22909;&#12290;RulE&#22312;&#27010;&#24565;&#19978;&#31616;&#21333;&#19988;&#22312;&#23454;&#39564;&#19978;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph (KG) reasoning is an important problem for knowledge graphs. In this paper, we propose a novel and principled framework called \textbf{RulE} (stands for {Rul}e {E}mbedding) to effectively leverage logical rules to enhance KG reasoning. Unlike knowledge graph embedding (KGE) methods, RulE learns rule embeddings from existing triplets and first-order {rules} by jointly representing \textbf{entities}, \textbf{relations} and \textbf{logical rules} in a unified embedding space. Based on the learned rule embeddings, a confidence score can be calculated for each rule, reflecting its consistency with the observed triplets. This allows us to perform logical rule inference in a soft way, thus alleviating the brittleness of logic. On the other hand, RulE injects prior logical rule information into the embedding space, enriching and regularizing the entity/relation embeddings. This makes KGE alone perform better too. RulE is conceptually simple and empirically effective. We conduct
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#23454;&#39564;&#65292;&#21457;&#29616;&#32467;&#26500;&#27169;&#22359;&#21270;&#24182;&#19981;&#19968;&#23450;&#33021;&#22815;&#30830;&#20445;&#21151;&#33021;&#19987;&#19994;&#21270;&#65292;&#22312;&#29305;&#23450;&#29615;&#22659;&#21644;&#36164;&#28304;&#38480;&#21046;&#19979;&#65292;&#25165;&#33021;&#22815;&#20986;&#29616;&#19987;&#19994;&#21270;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2106.02626</link><description>&lt;p&gt;
&#32422;&#26463;&#36164;&#28304;&#19979;&#31070;&#32463;&#27169;&#22359;&#19987;&#19994;&#21270;&#30340;&#21160;&#21147;&#23398;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Dynamics of specialization in neural modules under resource constraints. (arXiv:2106.02626v2 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.02626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#23454;&#39564;&#65292;&#21457;&#29616;&#32467;&#26500;&#27169;&#22359;&#21270;&#24182;&#19981;&#19968;&#23450;&#33021;&#22815;&#30830;&#20445;&#21151;&#33021;&#19987;&#19994;&#21270;&#65292;&#22312;&#29305;&#23450;&#29615;&#22659;&#21644;&#36164;&#28304;&#38480;&#21046;&#19979;&#65292;&#25165;&#33021;&#22815;&#20986;&#29616;&#19987;&#19994;&#21270;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#20154;&#20204;&#19968;&#30452;&#35748;&#20026;&#22823;&#33041;&#22312;&#32467;&#26500;&#21644;&#21151;&#33021;&#19978;&#39640;&#24230;&#27169;&#22359;&#21270;&#65292;&#20294;&#26368;&#36817;&#30340;&#35777;&#25454;&#20351;&#19968;&#20123;&#20154;&#23545;&#20004;&#31181;&#27169;&#22359;&#21270;&#30340;&#31243;&#24230;&#20135;&#29983;&#20102;&#24576;&#30097;&#12290;&#25105;&#20204;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26469;&#27979;&#35797;&#32467;&#26500;&#27169;&#22359;&#21270;&#26159;&#21542;&#36275;&#20197;&#20445;&#35777;&#21151;&#33021;&#19987;&#19994;&#21270;&#65292;&#24182;&#21457;&#29616;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#24182;&#19981;&#19968;&#23450;&#25104;&#31435;&#65292;&#38500;&#38750;&#22312;&#26497;&#31471;&#27700;&#24179;&#19978;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#27979;&#35797;&#20102;&#29615;&#22659;&#21644;&#32593;&#32476;&#30340;&#21738;&#20123;&#29305;&#24449;&#20250;&#23548;&#33268;&#19987;&#19994;&#21270;&#30340;&#20986;&#29616;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#29609;&#20855;&#29615;&#22659;&#12289;&#20219;&#21153;&#21644;&#32593;&#32476;&#65292;&#20197;&#31934;&#30830;&#25511;&#21046;&#26465;&#20214;&#65292;&#24182;&#34920;&#26126;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#20960;&#20010;&#19981;&#21516;&#30340;&#19987;&#19994;&#21270;&#24230;&#37327;&#25351;&#26631;&#32473;&#20986;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#65292;&#65288;1&#65289;&#19987;&#19994;&#21270;&#21482;&#33021;&#22312;&#29615;&#22659;&#20013;&#37027;&#20123;&#21487;&#20197;&#26126;&#30830;&#20998;&#31163;&#30340;&#29305;&#24449;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#20986;&#29616;&#65292;&#65288;2&#65289;&#19987;&#19994;&#21270;&#26356;&#23481;&#26131;&#22312;&#32593;&#32476;&#36164;&#28304;&#21463;&#21040;&#24378;&#28872;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#20986;&#29616;&#65292;&#65288;3&#65289;&#36825;&#20123;&#21457;&#29616;&#22312; qualitatively &#19978;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has long been believed that the brain is highly modular both in terms of structure and function, although recent evidence has led some to question the extent of both types of modularity. We used artificial neural networks to test the hypothesis that structural modularity is sufficient to guarantee functional specialization, and find that in general, this doesn't necessarily hold except at extreme levels. We then systematically tested which features of the environment and network do lead to the emergence of specialization. We used a simple toy environment, task and network, allowing us precise control, and show that in this setup, several distinct measures of specialization give qualitatively similar results. We further find that (1) specialization can only emerge in environments where features of that environment are meaningfully separable, (2) specialization preferentially emerges when the network is strongly resource-constrained, and (3) these findings are qualitatively similar ac
&lt;/p&gt;</description></item></channel></rss>