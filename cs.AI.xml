<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#32477;&#23545;&#31574;&#30053;&#20248;&#21270;&#65288;APO&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#22312;&#20445;&#35777;&#24615;&#33021;&#19979;&#30028;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#21644;Atari&#28216;&#25103;&#20013;&#30340;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13230</link><description>&lt;p&gt;
&#32477;&#23545;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Absolute Policy Optimization. (arXiv:2310.13230v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13230
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#32477;&#23545;&#31574;&#30053;&#20248;&#21270;&#65288;APO&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#22312;&#20445;&#35777;&#24615;&#33021;&#19979;&#30028;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#21644;Atari&#28216;&#25103;&#20013;&#30340;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#20449;&#20219;&#22495;&#30340;&#22312;&#32447;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#22312;&#35299;&#20915;&#22797;&#26434;&#25511;&#21046;&#20219;&#21153;&#21644;&#28216;&#25103;&#22330;&#26223;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#31867;&#21035;&#20013;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#20027;&#35201;&#24378;&#35843;&#23545;&#39044;&#26399;&#24615;&#33021;&#30340;&#25913;&#36827;&#65292;&#32570;&#20047;&#23545;&#26368;&#22351;&#24773;&#20917;&#19979;&#24615;&#33021;&#32467;&#26524;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#65307;&#36890;&#36807;&#20248;&#21270;&#35813;&#20989;&#25968;&#65292;&#21487;&#20197;&#30830;&#20445;&#36817;&#20046;&#24635;&#20307;&#24615;&#33021;&#26679;&#26412;&#30340;&#19979;&#30028;&#65288;&#32477;&#23545;&#24615;&#33021;&#65289;&#21576;&#29616;&#21333;&#35843;&#25913;&#36827;&#12290;&#32771;&#34385;&#21040;&#36825;&#19968;&#20855;&#26377;&#31361;&#30772;&#24615;&#30340;&#29702;&#35770;&#36827;&#23637;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#36817;&#20284;&#23545;&#36825;&#20010;&#29702;&#35770;&#22522;&#30784;&#31639;&#27861;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#24471;&#21040;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#31216;&#20026;&#32477;&#23545;&#31574;&#30053;&#20248;&#21270;&#65288;APO&#65289;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23558;&#20854;&#36866;&#29992;&#24615;&#25193;&#23637;&#21040;&#25484;&#25569;Atari&#28216;&#25103;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;APO&#22312;&#25552;&#39640;&#24615;&#33021;&#30340;&#21516;&#26102;&#20063;&#26174;&#33879;&#25913;&#21892;&#20102;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, trust region on-policy reinforcement learning has achieved impressive results in addressing complex control tasks and gaming scenarios. However, contemporary state-of-the-art algorithms within this category primarily emphasize improvement in expected performance, lacking the ability to control over the worst-case performance outcomes. To address this limitation, we introduce a novel objective function; by optimizing which, it will lead to guaranteed monotonic improvement in the lower bound of near-total performance samples (absolute performance). Considering this groundbreaking theoretical advancement, we then refine this theoretically grounded algorithm through a series of approximations, resulting in a practical solution called Absolute Policy Optimization (APO). Our experiments demonstrate the effectiveness of our approach across challenging continuous control benchmark tasks and extend its applicability to mastering Atari games. Our findings reveal that APO signifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#19988;&#26426;&#22120;&#23398;&#20064;&#20934;&#22791;&#30340;&#22810;&#27169;&#24577;&#32959;&#30244;&#23398;&#25968;&#25454;&#38598;(MINDS)&#26694;&#26550;&#65292;&#29992;&#20110;&#34701;&#21512;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#25506;&#32034;&#20851;&#31995;&#21644;&#26500;&#24314;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#30028;&#38754;&#12290;</title><link>http://arxiv.org/abs/2310.01438</link><description>&lt;p&gt;
&#26500;&#24314;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#19988;&#26426;&#22120;&#23398;&#20064;&#20934;&#22791;&#30340;&#22810;&#27169;&#24577;&#32959;&#30244;&#23398;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Building Flexible, Scalable, and Machine Learning-ready Multimodal Oncology Datasets. (arXiv:2310.01438v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#19988;&#26426;&#22120;&#23398;&#20064;&#20934;&#22791;&#30340;&#22810;&#27169;&#24577;&#32959;&#30244;&#23398;&#25968;&#25454;&#38598;(MINDS)&#26694;&#26550;&#65292;&#29992;&#20110;&#34701;&#21512;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#25506;&#32034;&#20851;&#31995;&#21644;&#26500;&#24314;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#30028;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#37319;&#38598;&#12289;&#23384;&#20648;&#21644;&#22788;&#29702;&#25216;&#26415;&#30340;&#36827;&#27493;&#23548;&#33268;&#20102;&#24322;&#36136;&#21307;&#23398;&#25968;&#25454;&#30340;&#24555;&#36895;&#22686;&#38271;&#12290;&#23558;&#25918;&#23556;&#23398;&#25195;&#25551;&#12289;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#21644;&#20998;&#23376;&#20449;&#24687;&#19982;&#20020;&#24202;&#25968;&#25454;&#25972;&#21512;&#26159;&#24320;&#21457;&#23545;&#30142;&#30149;&#26377;&#20840;&#38754;&#29702;&#35299;&#21644;&#20248;&#21270;&#27835;&#30103;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#22797;&#26434;&#30142;&#30149;&#65288;&#22914;&#30284;&#30151;&#65289;&#20013;&#65292;&#23558;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#30340;&#25968;&#25454;&#36827;&#34892;&#25972;&#21512;&#30340;&#38656;&#27714;&#26356;&#21152;&#31361;&#20986;&#65292;&#20197;&#23454;&#29616;&#31934;&#20934;&#21307;&#23398;&#21644;&#20010;&#24615;&#21270;&#27835;&#30103;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#32959;&#30244;&#25968;&#25454;&#31995;&#32479;&#65288;MINDS&#65289;-&#19968;&#31181;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#19988;&#32463;&#27982;&#39640;&#25928;&#30340;&#20803;&#25968;&#25454;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#26469;&#33258;&#20844;&#20849;&#26469;&#28304;&#65288;&#22914;&#30284;&#30151;&#30740;&#31350;&#25968;&#25454;&#20849;&#20139;&#24211;&#65289;&#30340;&#24322;&#26500;&#25968;&#25454;&#26377;&#25928;&#22320;&#34701;&#21512;&#21040;&#19968;&#20010;&#30456;&#20114;&#36830;&#25509;&#19988;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#20013;&#12290;MINDS&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20197;&#25506;&#32034;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#20043;&#38388;&#20851;&#31995;&#24182;&#26500;&#24314;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#30028;&#38754;&#12290;&#36890;&#36807;&#21327;&#35843;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;MINDS&#26088;&#22312;&#23454;&#29616;&#20419;&#36827;&#30740;&#31350;&#21019;&#26032;&#12289;&#31934;&#20934;&#21307;&#23398;&#21644;&#20010;&#24615;&#21270;&#27835;&#30103;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancements in data acquisition, storage, and processing techniques have resulted in the rapid growth of heterogeneous medical data. Integrating radiological scans, histopathology images, and molecular information with clinical data is essential for developing a holistic understanding of the disease and optimizing treatment. The need for integrating data from multiple sources is further pronounced in complex diseases such as cancer for enabling precision medicine and personalized treatments. This work proposes Multimodal Integration of Oncology Data System (MINDS) - a flexible, scalable, and cost-effective metadata framework for efficiently fusing disparate data from public sources such as the Cancer Research Data Commons (CRDC) into an interconnected, patient-centric framework. MINDS offers an interface for exploring relationships across data types and building cohorts for developing large-scale multimodal machine learning models. By harmonizing multimodal data, MINDS aims to pot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;S.T.A.R.-Track&#65292;&#19968;&#20010;&#37319;&#29992;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;Transformer&#26694;&#26550;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;3D&#29289;&#20307;&#36319;&#36394;&#12290;&#36890;&#36807;&#26032;&#39062;&#30340;&#28508;&#22312;&#36816;&#21160;&#27169;&#22411;&#21644;&#23398;&#20064;&#22411;&#36319;&#36394;&#23884;&#20837;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20934;&#30830;&#24314;&#27169;&#29289;&#20307;&#30340;&#20960;&#20309;&#36816;&#21160;&#21644;&#21464;&#21270;&#65292;&#24182;&#22312;nuScenes&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17602</link><description>&lt;p&gt;
S.T.A.R.-Track&#65306;&#33258;&#36866;&#24212;&#26102;&#31354;&#22806;&#35980;&#34920;&#31034;&#30340;&#31471;&#21040;&#31471;3D&#29289;&#20307;&#36319;&#36394;&#30340;&#28508;&#22312;&#36816;&#21160;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
S.T.A.R.-Track: Latent Motion Models for End-to-End 3D Object Tracking with Adaptive Spatio-Temporal Appearance Representations. (arXiv:2306.17602v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;S.T.A.R.-Track&#65292;&#19968;&#20010;&#37319;&#29992;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;Transformer&#26694;&#26550;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;3D&#29289;&#20307;&#36319;&#36394;&#12290;&#36890;&#36807;&#26032;&#39062;&#30340;&#28508;&#22312;&#36816;&#21160;&#27169;&#22411;&#21644;&#23398;&#20064;&#22411;&#36319;&#36394;&#23884;&#20837;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20934;&#30830;&#24314;&#27169;&#29289;&#20307;&#30340;&#20960;&#20309;&#36816;&#21160;&#21644;&#21464;&#21270;&#65292;&#24182;&#22312;nuScenes&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#36319;&#36394;-&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#22522;&#20110;Transformer&#30340;3D&#36319;&#36394;&#26694;&#26550;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#36319;&#36394;&#26041;&#27861;&#36890;&#36807;&#20960;&#20309;&#36816;&#21160;&#27169;&#22411;&#34701;&#21512;&#24103;&#20043;&#38388;&#30340;&#29289;&#20307;&#21644;&#33258;&#36816;&#21160;&#30340;&#20960;&#20309;&#25928;&#24212;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;S.T.A.R.-Track&#65292;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#28508;&#22312;&#36816;&#21160;&#27169;&#22411;&#26469;&#35843;&#25972;&#23545;&#35937;&#26597;&#35810;&#65292;&#20197;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#30452;&#25509;&#32771;&#34385;&#35270;&#35282;&#21644;&#20809;&#29031;&#26465;&#20214;&#30340;&#21464;&#21270;&#65292;&#21516;&#26102;&#26126;&#30830;&#24314;&#27169;&#20960;&#20309;&#36816;&#21160;&#12290;&#32467;&#21512;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#23398;&#20064;&#30340;&#36319;&#36394;&#23884;&#20837;&#65292;&#26377;&#21161;&#20110;&#24314;&#27169;&#36712;&#36857;&#30340;&#23384;&#22312;&#27010;&#29575;&#65292;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#36319;&#36394;&#26694;&#26550;&#65292;&#21487;&#20197;&#19982;&#20219;&#20309;&#22522;&#20110;&#26597;&#35810;&#30340;&#26816;&#27979;&#22120;&#38598;&#25104;&#12290;&#22312;nuScenes&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#23637;&#31034;&#20102;&#22522;&#20110;DETR3D&#30340;&#36319;&#36394;&#22120;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#20102;&#36712;&#36857;&#30340;&#36523;&#20221;&#36716;&#25442;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following the tracking-by-attention paradigm, this paper introduces an object-centric, transformer-based framework for tracking in 3D. Traditional model-based tracking approaches incorporate the geometric effect of object- and ego motion between frames with a geometric motion model. Inspired by this, we propose S.T.A.R.-Track, which uses a novel latent motion model (LMM) to additionally adjust object queries to account for changes in viewing direction and lighting conditions directly in the latent space, while still modeling the geometric motion explicitly. Combined with a novel learnable track embedding that aids in modeling the existence probability of tracks, this results in a generic tracking framework that can be integrated with any query-based detector. Extensive experiments on the nuScenes benchmark demonstrate the benefits of our approach, showing state-of-the-art performance for DETR3D-based trackers while drastically reducing the number of identity switches of tracks at the s
&lt;/p&gt;</description></item><item><title>MRFI&#26159;&#19968;&#20010;&#39640;&#24230;&#21487;&#37197;&#32622;&#30340;&#31070;&#32463;&#32593;&#32476;&#25925;&#38556;&#27880;&#20837;&#24037;&#20855;&#65292;&#29992;&#25143;&#21487;&#20197;&#20462;&#25913;&#29420;&#31435;&#30340;&#25925;&#38556;&#37197;&#32622;&#25991;&#20214;&#36827;&#34892;&#27880;&#20837;&#21644;&#28431;&#27934;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2306.11758</link><description>&lt;p&gt;
MRFI&#65306;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#30340;&#24320;&#28304;&#22810;&#20998;&#36776;&#29575;&#25925;&#38556;&#27880;&#20837;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MRFI: An Open Source Multi-Resolution Fault Injection Framework for Neural Network Processing. (arXiv:2306.11758v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11758
&lt;/p&gt;
&lt;p&gt;
MRFI&#26159;&#19968;&#20010;&#39640;&#24230;&#21487;&#37197;&#32622;&#30340;&#31070;&#32463;&#32593;&#32476;&#25925;&#38556;&#27880;&#20837;&#24037;&#20855;&#65292;&#29992;&#25143;&#21487;&#20197;&#20462;&#25913;&#29420;&#31435;&#30340;&#25925;&#38556;&#37197;&#32622;&#25991;&#20214;&#36827;&#34892;&#27880;&#20837;&#21644;&#28431;&#27934;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#30830;&#20445;&#21363;&#20351;&#22312;&#19981;&#21487;&#38752;&#30340;&#30828;&#20214;&#19978;&#20063;&#33021;&#36827;&#34892;&#26377;&#24377;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#65292;&#36890;&#24120;&#38656;&#35201;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#37096;&#32626;&#20043;&#21069;&#36827;&#34892;&#21508;&#31181;&#30828;&#20214;&#25925;&#38556;&#30340;&#20840;&#38754;&#21487;&#38752;&#24615;&#20998;&#26512;&#65292;&#24182;&#19988;&#38656;&#35201;&#39640;&#25928;&#30340;&#38169;&#35823;&#27880;&#20837;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25925;&#38556;&#27880;&#20837;&#24037;&#20855;&#20173;&#28982;&#23616;&#38480;&#20110;&#23545;&#31070;&#32463;&#20803;&#30340;&#22522;&#26412;&#25925;&#38556;&#27880;&#20837;&#65292;&#24182;&#26410;&#25552;&#20379;&#32454;&#31890;&#24230;&#28431;&#27934;&#20998;&#26512;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#25925;&#38556;&#27880;&#20837;&#24037;&#20855;&#20173;&#38656;&#35201;&#26356;&#25913;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24182;&#20351;&#25925;&#38556;&#27880;&#20837;&#19982;&#27491;&#24120;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#32039;&#23494;&#32806;&#21512;&#65292;&#36825;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#25925;&#38556;&#27880;&#20837;&#24037;&#20855;&#30340;&#20351;&#29992;&#38590;&#24230;&#24182;&#20943;&#24930;&#20102;&#25925;&#38556;&#27169;&#25311;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#21487;&#37197;&#32622;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22810;&#20998;&#36776;&#29575;&#25925;&#38556;&#27880;&#20837;&#24037;&#20855;MRFI&#12290;&#23427;&#20351;&#29992;&#25143;&#33021;&#22815;&#20462;&#25913;&#29420;&#31435;&#30340;&#25925;&#38556;&#37197;&#32622;&#25991;&#20214;&#65292;&#32780;&#19981;&#26159;&#20462;&#25913;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#25925;&#38556;&#27880;&#20837;&#21644;&#28431;&#27934;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
To ensure resilient neural network processing on even unreliable hardware, comprehensive reliability analysis against various hardware faults is generally required before the deep neural network models are deployed, and efficient error injection tools are highly demanded. However, most existing fault injection tools remain rather limited to basic fault injection to neurons and fail to provide fine-grained vulnerability analysis capability. In addition, many of the fault injection tools still need to change the neural network models and make the fault injection closely coupled with normal neural network processing, which further complicates the use of the fault injection tools and slows down the fault simulation. In this work, we propose MRFI, a highly configurable multi-resolution fault injection tool for deep neural networks. It enables users to modify an independent fault configuration file rather than neural network models for the fault injection and vulnerability analysis. Particul
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#29992;&#20110;&#25193;&#23637;&#38035;&#40060;&#37038;&#20214;&#25915;&#20987;&#65292;&#20316;&#32773;&#36890;&#36807;&#23454;&#35777;&#27979;&#35797;&#34920;&#26126;&#39640;&#32423;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25915;&#20987;&#30340;&#25928;&#29575;&#21644;&#25104;&#26412;&#25928;&#30410;&#12290;</title><link>http://arxiv.org/abs/2305.06972</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#29992;&#20110;&#26377;&#25928;&#25193;&#23637;&#38035;&#40060;&#37038;&#20214;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Can Be Used To Effectively Scale Spear Phishing Campaigns. (arXiv:2305.06972v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06972
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#29992;&#20110;&#25193;&#23637;&#38035;&#40060;&#37038;&#20214;&#25915;&#20987;&#65292;&#20316;&#32773;&#36890;&#36807;&#23454;&#35777;&#27979;&#35797;&#34920;&#26126;&#39640;&#32423;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25915;&#20987;&#30340;&#25928;&#29575;&#21644;&#25104;&#26412;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#23588;&#20854;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#24050;&#32463;&#20135;&#29983;&#20102;&#21151;&#33021;&#24378;&#22823;&#32780;&#36890;&#29992;&#30340;&#21452;&#37325;&#29992;&#36884;&#31995;&#32479;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22914;&#20309;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38035;&#40060;&#37038;&#20214;&#25915;&#20987;&#65292;&#36825;&#31181;&#27969;&#34892;&#30340;&#32593;&#32476;&#29359;&#32618;&#24418;&#24335;&#28041;&#21450;&#23558;&#30446;&#26631;&#20154;&#29289;&#35825;&#39575;&#25259;&#38706;&#25935;&#24863;&#20449;&#24687;&#12290;&#20316;&#32773;&#39318;&#20808;&#30740;&#31350;&#20102;LLMs&#22312;&#25104;&#21151;&#30340;&#38035;&#40060;&#25915;&#20987;&#30340;&#20390;&#23519;&#21644;&#20449;&#24687;&#29983;&#25104;&#38454;&#27573;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#20808;&#36827;&#30340;LLMs&#33021;&#22815;&#22312;&#36825;&#20123;&#38454;&#27573;&#26174;&#30528;&#25552;&#39640;&#32593;&#32476;&#32618;&#29359;&#30340;&#25928;&#29575;&#12290;&#20854;&#27425;&#65292;&#20316;&#32773;&#20351;&#29992;OpenAI&#30340;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#20026;&#36229;&#36807;600&#21517;&#33521;&#22269;&#35758;&#21592;&#21019;&#24314;&#20102;&#29420;&#29305;&#30340;&#38035;&#40060;&#37038;&#20214;&#30340;&#23454;&#35777;&#27979;&#35797;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#37038;&#20214;&#19981;&#20165;&#36924;&#30495;&#32780;&#19988;&#25104;&#26412;&#25928;&#30410;&#26174;&#33879;&#65292;&#27599;&#23553;&#30005;&#23376;&#37038;&#20214;&#20165;&#33457;&#36153;&#20960;&#20998;&#20043;&#19968;&#30340;&#32654;&#20998;&#21363;&#21487;&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in artificial intelligence (AI), particularly in the domain of large language models (LLMs), has resulted in powerful and versatile dual-use systems. Indeed, cognition can be put towards a wide variety of tasks, some of which can result in harm. This study investigates how LLMs can be used for spear phishing, a prevalent form of cybercrime that involves manipulating targets into divulging sensitive information. I first explore LLMs' ability to assist with the reconnaissance and message generation stages of a successful spear phishing attack, where I find that advanced LLMs are capable of meaningfully improving cybercriminals' efficiency during these stages. Next, I conduct an empirical test by creating unique spear phishing messages for over 600 British Members of Parliament using OpenAI's GPT-3.5 and GPT-4 models. My findings reveal that these messages are not only realistic but also remarkably cost-effective, as each email cost only a fraction of a cent to generate. N
&lt;/p&gt;</description></item><item><title>FlightBERT++&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#30340;&#22810;&#26102;&#22495;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26102;&#22495;&#24863;&#30693;&#19978;&#19979;&#25991;&#29983;&#25104;&#22120;&#35299;&#20915;&#20102;&#35823;&#24046;&#32047;&#31215;&#21644;&#20302;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.01658</link><description>&lt;p&gt;
FlightBERT++&#65306;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#22810;&#26102;&#22495;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FlightBERT++: A Non-autoregressive Multi-Horizon Flight Trajectory Prediction Framework. (arXiv:2305.01658v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01658
&lt;/p&gt;
&lt;p&gt;
FlightBERT++&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#30340;&#22810;&#26102;&#22495;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26102;&#22495;&#24863;&#30693;&#19978;&#19979;&#25991;&#29983;&#25104;&#22120;&#35299;&#20915;&#20102;&#35823;&#24046;&#32047;&#31215;&#21644;&#20302;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26159;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#21487;&#20197;&#24110;&#21161;&#31354;&#31649;&#21592;&#26356;&#23433;&#20840;&#39640;&#25928;&#22320;&#31649;&#29702;&#31354;&#22495;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#33258;&#22238;&#24402;&#26041;&#24335;&#25191;&#34892;&#22810;&#26102;&#22495;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#65292;&#23481;&#26131;&#20986;&#29616;&#35823;&#24046;&#32047;&#31215;&#21644;&#20302;&#25928;&#29575;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;FlightBERT++&#65292;&#20197;i&#65289;&#30452;&#25509;&#20197;&#38750;&#33258;&#22238;&#24402;&#26041;&#24335;&#39044;&#27979;&#22810;&#26102;&#22495;&#39134;&#34892;&#36712;&#36857;&#65292;&#21644;ii&#65289;&#25913;&#21892;FlightBERT&#26694;&#26550;&#20013;&#20108;&#36827;&#21046;&#32534;&#30721;&#65288;BE&#65289;&#34920;&#31034;&#30340;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#36890;&#36807;&#36890;&#29992;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#23454;&#29616;&#65292;&#20854;&#20013;&#32534;&#30721;&#22120;&#20174;&#21382;&#21490;&#35266;&#27979;&#20013;&#23398;&#20064;&#26102;&#31354;&#27169;&#24335;&#65292;&#32780;&#35299;&#30721;&#22120;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#27493;&#30340;&#39134;&#34892;&#29366;&#24577;&#12290;&#19982;&#20256;&#32479;&#26550;&#26500;&#30456;&#27604;&#65292;&#39069;&#22806;&#30340;&#26102;&#22495;&#24863;&#30693;&#19978;&#19979;&#25991;&#29983;&#25104;&#22120;&#65288;HACG&#65289;&#19987;&#38376;&#35774;&#35745;&#32771;&#34385;&#20808;&#21069;&#30340;&#26102;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Flight Trajectory Prediction (FTP) is an essential task in Air Traffic Control (ATC), which can assist air traffic controllers to manage airspace more safely and efficiently. Existing approaches generally perform multi-horizon FTP tasks in an autoregressive manner, which is prone to suffer from error accumulation and low-efficiency problems. In this paper, a novel framework, called FlightBERT++, is proposed to i) forecast multi-horizon flight trajectories directly in a non-autoregressive way, and ii) improved the limitation of the binary encoding (BE) representation in the FlightBERT framework. Specifically, the proposed framework is implemented by a generalized Encoder-Decoder architecture, in which the encoder learns the temporal-spatial patterns from historical observations and the decoder predicts the flight status for the future time steps. Compared to conventional architecture, an extra horizon-aware contexts generator (HACG) is dedicatedly designed to consider the prior horizon 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#31232;&#30095;&#21270;&#21644;&#24046;&#20998;&#38544;&#31169;&#30340;&#26080;&#32447;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#38543;&#26426;&#31232;&#30095;&#21270;&#31639;&#27861;&#32531;&#35299;DP&#24341;&#36215;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#20943;&#23569;&#19978;&#20256;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#32780;&#19981;&#25439;&#22833;&#25910;&#25947;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.04164</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#31232;&#30095;&#21270;&#21644;&#24046;&#20998;&#38544;&#31169;&#30340;&#39640;&#25928;&#26080;&#32447;&#32852;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Gradient Sparsification for Efficient Wireless Federated Learning with Differential Privacy. (arXiv:2304.04164v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#31232;&#30095;&#21270;&#21644;&#24046;&#20998;&#38544;&#31169;&#30340;&#26080;&#32447;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#38543;&#26426;&#31232;&#30095;&#21270;&#31639;&#27861;&#32531;&#35299;DP&#24341;&#36215;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#20943;&#23569;&#19978;&#20256;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#32780;&#19981;&#25439;&#22833;&#25910;&#25947;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#20351;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#22312;&#19981;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21327;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#19978;&#20256;&#27169;&#22411;&#32780;&#27844;&#28431;&#31169;&#26377;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#30001;&#20110;&#26377;&#38480;&#30340;&#20256;&#36755;&#24102;&#23485;&#65292;&#35757;&#32451;&#24310;&#36831;&#22686;&#21152;&#65292;&#21516;&#26102;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#20445;&#25252;&#26102;&#27169;&#22411;&#24615;&#33021;&#20250;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#31232;&#30095;&#21270;&#21644;&#24046;&#20998;&#38544;&#31169;&#30340;&#26080;&#32447;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#32780;&#19981;&#25439;&#22833;&#25910;&#25947;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#38543;&#26426;&#31232;&#30095;&#21270;&#31639;&#27861;&#65292;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#35757;&#32451;&#20013;&#20445;&#30041;&#19968;&#37096;&#20998;&#26799;&#24230;&#20803;&#32032;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;DP&#24341;&#36215;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#20943;&#23569;&#20102;&#26080;&#32447;&#20449;&#36947;&#19978;&#20256;&#36755;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#24314;&#27169;&#38750;&#20984;FL&#38382;&#39064;&#20998;&#26512;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#25910;&#25947;&#24230;&#30028;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#32852;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#20351;&#29992;Alternating Direction Method of Multipliers&#65288;ADMM&#65289;&#35299;&#20915;&#20854;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables distributed clients to collaboratively train a machine learning model without sharing raw data with each other. However, it suffers the leakage of private information from uploading models. In addition, as the model size grows, the training latency increases due to limited transmission bandwidth and the model performance degrades while using differential privacy (DP) protection. In this paper, we propose a gradient sparsification empowered FL framework over wireless channels, in order to improve training efficiency without sacrificing convergence performance. Specifically, we first design a random sparsification algorithm to retain a fraction of the gradient elements in each client's local training, thereby mitigating the performance degradation induced by DP and and reducing the number of transmission parameters over wireless channels. Then, we analyze the convergence bound of the proposed algorithm, by modeling a non-convex FL problem. Next, we formula
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21516;&#26102;&#22788;&#29702;&#23450;&#20215;&#21644;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#32771;&#34385;&#21830;&#19994;&#20915;&#31574;&#23545;&#26410;&#26469;&#30340;&#24433;&#21709;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26694;&#26550;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25340;&#36710;&#30340;&#25928;&#29575;&#21644;&#25928;&#30410;&#12290;</title><link>http://arxiv.org/abs/2302.10510</link><description>&lt;p&gt;
&#38754;&#21521;&#21487;&#25345;&#32493;&#24615;&#30340;&#21363;&#26102;&#25340;&#36710;&#30340;&#26410;&#26469;&#24863;&#30693;&#23450;&#20215;&#21644;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Future Aware Pricing and Matching for Sustainable On-demand Ride Pooling. (arXiv:2302.10510v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21516;&#26102;&#22788;&#29702;&#23450;&#20215;&#21644;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#32771;&#34385;&#21830;&#19994;&#20915;&#31574;&#23545;&#26410;&#26469;&#30340;&#24433;&#21709;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26694;&#26550;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25340;&#36710;&#30340;&#25928;&#29575;&#21644;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21363;&#26102;&#25340;&#36710;&#30340;&#21463;&#27426;&#36814;&#31243;&#24230;&#22312;&#20110;&#20026;&#39038;&#23458;&#65288;&#26356;&#20302;&#30340;&#20215;&#26684;&#65289;&#12289;&#20986;&#31199;&#36710;&#21496;&#26426;&#65288;&#26356;&#39640;&#30340;&#25910;&#20837;&#65289;&#12289;&#29615;&#22659;&#65288;&#30001;&#20110;&#26356;&#23569;&#30340;&#36710;&#36742;&#32780;&#20943;&#23569;&#30899;&#25490;&#25918;&#37327;&#65289;&#21644;Uber&#31561;&#32858;&#21512;&#20844;&#21496;&#65288;&#26356;&#39640;&#30340;&#25910;&#20837;&#65289;&#25552;&#20379;&#20102;&#22909;&#22788;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#25910;&#30410;&#65292;&#24517;&#39035;&#26377;&#25928;&#22320;&#35299;&#20915;&#20004;&#20010;&#20851;&#38190;&#32780;&#30456;&#20114;&#20851;&#32852;&#30340;&#25361;&#25112;&#65306;&#65288;a&#65289;&#23450;&#20215;&#8212;&#8212;&#20026;&#20986;&#31199;&#36710;&#39038;&#23458;&#35831;&#27714;&#35774;&#32622;&#20215;&#26684;&#65307;&#21644;&#65288;b&#65289;&#21305;&#37197;&#8212;&#8212;&#23558;&#25509;&#21463;&#20215;&#26684;&#30340;&#23458;&#25143;&#20998;&#37197;&#32473;&#20986;&#31199;&#36710;/&#27773;&#36710;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20004;&#20010;&#25361;&#25112;&#37117;&#26159;&#21333;&#29420;&#30740;&#31350;&#30340;&#65292;&#24182;&#19988;&#20351;&#29992;&#30701;&#35270;&#30340;&#26041;&#27861;&#65288;&#21482;&#32771;&#34385;&#24403;&#21069;&#35831;&#27714;&#65289;&#65292;&#32780;&#19981;&#32771;&#34385;&#24403;&#21069;&#21305;&#37197;&#23545;&#35299;&#20915;&#26410;&#26469;&#35831;&#27714;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#22788;&#29702;&#23450;&#20215;&#21644;&#21305;&#37197;&#38382;&#39064;&#65292;&#21516;&#26102;&#32771;&#34385;&#23450;&#20215;&#21644;&#21305;&#37197;&#20915;&#31574;&#23545;&#26410;&#26469;&#24433;&#21709;&#30340;&#24433;&#21709;&#12290;&#22312;&#30495;&#23454;&#20986;&#31199;&#36710;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#21305;&#37197;&#21644;&#23450;&#20215;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The popularity of on-demand ride pooling is owing to the benefits offered to customers (lower prices), taxi drivers (higher revenue), environment (lower carbon footprint due to fewer vehicles) and aggregation companies like Uber (higher revenue). To achieve these benefits, two key interlinked challenges have to be solved effectively: (a) pricing -- setting prices to customer requests for taxis; and (b) matching -- assignment of customers (that accepted the prices) to taxis/cars. Traditionally, both these challenges have been studied individually and using myopic approaches (considering only current requests), without considering the impact of current matching on addressing future requests. In this paper, we develop a novel framework that handles the pricing and matching problems together, while also considering the future impact of the pricing and matching decisions. In our experimental results on a real-world taxi dataset, we demonstrate that our framework can significantly improve re
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#20102;&#22721;&#38754;&#36793;&#30028;&#23618;&#28237;&#27969;&#20013;&#30340;&#36895;&#24230;&#22330;&#65292;&#24182;&#21033;&#29992;SHAP&#31639;&#27861;&#35780;&#20272;&#20102;&#30456;&#24178;&#32467;&#26500;&#23545;&#39044;&#27979;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#19968;&#36807;&#31243;&#25110;&#26377;&#21161;&#20110;&#35299;&#20915;&#28237;&#27969;&#30740;&#31350;&#20013;&#30340;&#38590;&#39064;&#65292;&#20026;&#28237;&#27969;&#27169;&#22411;&#30340;&#21457;&#23637;&#25552;&#20379;&#26032;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2302.01250</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#35299;&#37322;&#22721;&#38754;&#36793;&#30028;&#23618;&#28237;&#27969;
&lt;/p&gt;
&lt;p&gt;
Explaining wall-bounded turbulence through deep learning. (arXiv:2302.01250v2 [physics.flu-dyn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#20102;&#22721;&#38754;&#36793;&#30028;&#23618;&#28237;&#27969;&#20013;&#30340;&#36895;&#24230;&#22330;&#65292;&#24182;&#21033;&#29992;SHAP&#31639;&#27861;&#35780;&#20272;&#20102;&#30456;&#24178;&#32467;&#26500;&#23545;&#39044;&#27979;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#19968;&#36807;&#31243;&#25110;&#26377;&#21161;&#20110;&#35299;&#20915;&#28237;&#27969;&#30740;&#31350;&#20013;&#30340;&#38590;&#39064;&#65292;&#20026;&#28237;&#27969;&#27169;&#22411;&#30340;&#21457;&#23637;&#25552;&#20379;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22721;&#38754;&#36793;&#30028;&#23618;&#28237;&#27969;&#20316;&#20026;&#19968;&#20010;&#20855;&#26377;&#37325;&#22823;&#31185;&#23398;&#21644;&#25216;&#26415;&#24847;&#20041;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#23547;&#27714;&#26032;&#30340;&#35270;&#35282;&#26469;&#35299;&#20915;&#12290;&#26412;&#30740;&#31350;&#39318;&#27425;&#37319;&#29992;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30740;&#31350;&#20102;&#27969;&#22330;&#20013;&#30456;&#24178;&#32467;&#26500;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#28237;&#27969;&#36890;&#36947;&#20013;&#30340;&#30636;&#26102;&#36895;&#24230;&#22330;&#39044;&#27979;&#20102;&#26102;&#38388;&#20869;&#30340;&#36895;&#24230;&#22330;&#65292;&#28982;&#21518;&#21033;&#29992;SHapley Additive exPlanations&#65288;SHAP&#65289;&#31639;&#27861;&#23545;&#27599;&#20010;&#32467;&#26500;&#39044;&#27979;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#26412;&#30740;&#31350;&#32467;&#26524;&#19982;&#20808;&#21069;&#25991;&#29486;&#35266;&#23519;&#32467;&#26524;&#19968;&#33268;&#65292;&#24182;&#36890;&#36807;&#37327;&#21270;&#38647;&#35834;&#24212;&#21147;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#65292;&#25214;&#21040;&#20102;&#36825;&#20123;&#32467;&#26500;&#19982;&#27969;&#21160;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#21487;&#33021;&#26377;&#21161;&#20110;&#25581;&#31034;&#22721;&#38754;&#36793;&#30028;&#23618;&#28237;&#27969;&#30340;&#38271;&#26399;&#38382;&#39064;&#65292;&#24182;&#20026;&#28237;&#27969;&#27169;&#22411;&#30340;&#24320;&#21457;&#25552;&#20379;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite its great scientific and technological importance, wall-bounded turbulence is an unresolved problem that requires new perspectives to be tackled. One of the key strategies has been to study interactions among the coherent structures in the flow. Such interactions are explored in this study for the first time using an explainable deep-learning method. The instantaneous velocity field in a turbulent channel is used to predict the velocity field in time through a convolutional neural network. Based on the predicted flow, we assess the importance of each structure for this prediction using the game-theoretic algorithm of SHapley Additive exPlanations (SHAP). This work provides results in agreement with previous observations in the literature and extends them by quantifying the importance of the Reynolds-stress structures, finding a connection between these structures and the dynamics of the flow. The process, based on deep-learning explainability, has the potential to shed light on
&lt;/p&gt;</description></item></channel></rss>